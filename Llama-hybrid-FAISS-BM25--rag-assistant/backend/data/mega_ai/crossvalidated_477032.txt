[site]: crossvalidated
[post_id]: 477032
[parent_id]: 475768
[tags]: 
I don't have much to offer in terms of methods, I think the ones presented here (esp inverse variance weighted approaches) are good ones. What I can add is a small simulation study to prove that under the assumption of Gaussian errors in the regression, this process has good enough coverage set.seed(0) library(tidyverse) simulate_data $beta*results$ w)/sum(results$w) sig = sqrt(1/sum(results$w)) interval = tibble(lower = m - 1.96*sig, est = m, upper = m + 1.96*sig) interval } map_dfr(1:10000, simulate_procedure, .id = 'iter') %>% mutate(contains = (2 lower)) %>% summarise(mean(contains)) >>>0.922 So what does this mean? It means that were I to repeat this procedure to construct a 95% interval for the slope, the resulting interval would capture the true slope (here 2) only 92% of the time. So barring I didn't make a mistake (entirely possible) that seems to be good enough. How should the estimate of the slope be reported, including errors? Let's imagine I only have access to these values (and not the underlying data that was used for fitting to obtain these slopes). So I would compute $m$ and $\sigma^2$ as mentioned by Yair Daon. You don't need to access the data in order to do these. In your example, the $m$ would be 5.5, 5.5, 5.2. The variances are found by doing a little algebra on the confidence interval. Remember, confidence intervals look like $$m \pm 1.96 se $$ Here, $se$ is the standard error (or the standard deviation of the sampling distribution). You can find the variance by taking the difference between interval endpoints and then dividing by $3.92 = 2\times 1.96$ . Your sigmas (not squared) would then be 0.306, 0.102, 0.357. So your best estimate for $m$ from the example you've provided is 5.47, with an accompanying interval of 5.29 to 5.66. These were computed using the formulae provided by Yair.
