[site]: crossvalidated
[post_id]: 626017
[parent_id]: 
[tags]: 
Comparison of Squared Dice Loss vs. Standard Dice Loss

I've been diving into segmentation tasks and came across two variations of the Dice Loss that I'm considering for my neural network: the standard Dice Loss and the Squared Dice Loss. The Standard Dice Loss is formulated as: $$ \text{Dice Coefficient} = \frac{2 \times \text{intersection} + \text{smooth}}{\text{sum of predictions} + \text{sum of ground truth} + \text{smooth}} $$ And the loss is: $$ \text{Dice Loss} = 1 - \text{Dice Coefficient} $$ The Squared Dice Loss on the other hand is: $$ \text{Squared Dice Coefficient} = \frac{2 \times \text{intersection} + \text{smooth}}{\text{inputs}^2.\text{sum()} + \text{targets}^2.\text{sum()} + \text{smooth}} $$ And its corresponding loss: $$ \text{Squared Dice Loss} = 1 - \text{Squared Dice Coefficient} $$ I'm trying to understand the practical differences between the two. In what scenarios would one be preferred over the other? Are there any empirical studies or theoretical insights on which might be better for certain types of data or tasks? I am currently working with medical datasets so based on that , I think squared dice loss would be a better fit for my problem , along with that i would also use it with BCE loss , so it would be something like DICE + BCE loss which has shown to be better in experiments .
