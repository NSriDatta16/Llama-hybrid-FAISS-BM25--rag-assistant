[site]: crossvalidated
[post_id]: 264200
[parent_id]: 
[tags]: 
Random Forest: high variable importance but ignorable?

I've been trying to identify an optimal random forest model (regressor) by constructing a series of models each with different variable composition. After several experiments, i've noticed something that I couldnâ€™t explain. One model with independent variables A, B, C, D, E gave me an OOB score of 70%, with variable D having the highest variable importance (considerably higher than the others). However, after I removed D leaving only four variables in the model, the OOB score increased to 75%. I implemented this in both R (with randomForest package) and python (with RandomForestRegressor within the sklearn library) and got the similar results. Can someone please explain why?
