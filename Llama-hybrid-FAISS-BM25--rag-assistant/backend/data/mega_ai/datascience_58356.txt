[site]: datascience
[post_id]: 58356
[parent_id]: 47990
[tags]: 
You want to ensemble your two algorithms. The way to do that is to not just use e.g. sklearn's precision and recall metric functions, but to actually obtain probabilities for being positive (most models can output this with a simple function call), and take the average of those probabilities between your two (or more) binary classifiers. Then, test a range of probability thresholds beyond which you might label something positive, and evaluate the impact on recall and precision. This is a pretty common approach to obtaining better results through utilizing the strengths of different prediction algorithms. Just some pythonic pseudo-code to demonstrate: recall = [] precision = [] for threshold in [0.1, 0.2, 0.3, ..., 0.9]: df_of_mean_probabilities['label'] = df_of_mean_probabilities > threshold recall.append(TP/(TP + FN)) precision.append(TP/(TP + FP))
