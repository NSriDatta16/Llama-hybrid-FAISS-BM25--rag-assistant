[site]: crossvalidated
[post_id]: 297799
[parent_id]: 297795
[tags]: 
The multinomial regression model is $$ \Pr(Y = k \mid \mathbf{X}) = \frac{ \exp\big( \beta_k \mathbf{X} \big) }{ 1 + \sum_{i=1}^{K-1} \exp\big( \beta_i \mathbf{X} \big) } \tag{1} $$ What you want to estimate is $$ \Pr(Y = k \mid \mathbf{X}_k) = \frac{ \exp\big( \beta_k I(Y = k) \mathbf{X}_k \big) }{ 1 + \sum_{i=1}^{K-1} \exp\big( \beta_i I(Y = i) \mathbf{X}_i \big) } \tag{2} $$ So the first consequence to notice, is that you are not estimating the conditional probability of $Y \mid \mathbf{X}$, at least not in the sense that there is a linear dependence of $Y=k \mid X_k$. Now, since both terms $\alpha_k$ and $\beta_k \mathbf{X}_k $ are unique for each $k$ , then any one of them can, as well, be equal to zero and you would still have the model $$ \Pr(Y = k \mid \mathbf{X}_k) = \frac{ \exp\big( I(Y =k)\cdot \text{something}_{\,k} \big) }{ 1 + \sum_{i=1}^{K-1} \exp\big( I(Y =i)\cdot \text{something}_{\,i} \big) } $$ so you model reduces to $$ \Pr(Y = k) = \frac{ \exp\big( \alpha_k \big) }{ 1 + \sum_{i=1}^{K-1} \exp\big( \alpha_i \big) } $$ since you are not estimating any conditional probabilities, and for each $k$ the formula needs to return some constant that is equal to $\Pr(Y = k )$. Saying this otherwise, you would get the same result if $\beta_k I(Y = k) \mathbf{X}_k = \alpha_k$, since $\frac{ \exp( \beta_k I(Y = k) \mathbf{X}_k ) }{ 1 + \sum_{i=1}^{K-1} \beta_i I(Y = i) \mathbf{X}_i } $ needs to evaluate to a single value for each $k$. So $\beta_k I(Y=k) \mathbf{X}_k$ does not let you to estimate any linear dependence of $Y = k \mid \mathbf{X}_k$. Regarding your example, if you rename $\alpha X_1, \beta X_2$ to $a, b$, then you would clearly see that those values need to be constants, so there is no way how regression parameters would tell you anything meaningful in here. It is as if you had a logistic regression model and used it for data containing only successes (or failures).
