[site]: crossvalidated
[post_id]: 365432
[parent_id]: 
[tags]: 
Interpretation of conditional variance of estimator of intercept in linear regression

$Y_i=a+bX_i+e_i$. $Y_i$ and $X_i$ are scalar r.v. We have, $$ V(\hat b|X)=\frac{\sigma^2}{n\left(\bar{X^2}-\left[\bar{X}\right]^2\right)} $$ and, $$ V(\hat a|X)=\frac{\sigma^2 \bar{X^2}}{n\left(\bar{X^2}-\left[\bar{X}\right]^2\right)}. $$ Where $X$ is $X_1$ up to $X_n$ and $\sigma^2=V(e_i)$. We see that as $X$ becomes more variable, the variance of the estimator decreases. But why does the variance of the estimator of the intercept increase as $\bar{X}$ increase? The further away $X$ is from the origin on average, the larger the variance of the estimator. Why?
