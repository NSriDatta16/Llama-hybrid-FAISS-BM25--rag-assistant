[site]: crossvalidated
[post_id]: 636429
[parent_id]: 
[tags]: 
ROCAUC = average sensitivity across all thresholds according to IEEE TPAMI, yet my calculations show otherwise

Carrington et al (2023) make the claim that area under the receiver-operator characteristic curve is equal to the average sensitivity across all thresholds, and similarly for specificity (section 3), citing a book by Zhou, McClish, and Obuchowski (2002). Two other less commonly known interpretations of the AUC are that AUC equals average sensitivity across all thresholds, and AUC equals average specificity across all thresholds. I had never heard this and wrote a simulation to check it out. The results are about the same if the sample size N is increased to at least a million and the number of repetitions R to at least ten-thousand. library(pROC) set.seed(2024) N $sensitivities) specs[i] specificities) } # Are all three equal? # summary(aucs - senss) summary(aucs - specs) summary(senss - specs) The summary calls show that the AUC always differs (rather considerably) from both the average sensitivity and the average specificity. Further, the average sensitivity and average specificity are not even equal. It appears that the claim in the article is, then, incorrect. Is there something weird about how pROC does the calculations? Is there some narrower sense in which the claim would be correct? (Did I just write code with a bug?) REFERENCES Carrington, Andr√© M., et al. "Deep ROC analysis and AUC as balanced average accuracy, for improved classifier selection, audit and explanation." IEEE Transactions on Pattern Analysis and Machine Intelligence 45.1 (2022): 329-341. X.-H. Zhou, D. K. McClish, and N. A. Obuchowski, "Statistical Methods in Diagnostic Medicine." Hoboken, NJ, USA: Wiley, 2002.
