[site]: crossvalidated
[post_id]: 509838
[parent_id]: 472255
[tags]: 
This is standard behavior for VAEs. The thing that ultimately matters is that the total loss (reconstruction error plus KL divergence) is decreasing. If the model is good enough to solve your problem, it's a success. Sometimes a VAE will have the KL divergence swamp any improvement to the reconstruction. In that case, it can help to anneal the weight assigned to the KLD portion of the loss from 0 (KLD is ignored) to 1 (KLD is given full weight and the loss is the ordinary variational lower-bound). Samuel R. Bowman, Luke Vilnis. " Generating Sentences from a Continuous Space "
