[site]: crossvalidated
[post_id]: 544129
[parent_id]: 541868
[tags]: 
It seems that the hyperlink is broken, so for now, I will attempt to clarify some details without having read the paper and can answer the specifics later. I suspect that even though the agent has found a solution, it may not be the optimal solution and that finding the optimal solution may still require a whole bunch of extra exploration in both state and action space. Reward-shaping is one way of exploring efficiently, and useful priors such as fuel efficiency may allow the agent to find the optimal solution quicker even after having found the reward. That being said, I think that in the case of 2D navigation in a grid-like world, what you are proposing may work totally fine as long as the discount factor $\gamma \neq 1$ which would effectively be the same as fuel efficiency in the spacecraft game. Since you said that you're new to the field, here's a more detailed answer to make sure that we're on the same page re. your question. When the rewards are sparse and the state-space is very large, RL agents are often unable to learn because there is a very low probability of reaching the rewarding state(s) by random exploration in a finite amount of time. This problem is best known in the game Montezuma's revenge, where Deep RL agents without tweaks (vanilla DQN; Mnih et al 2015) have struggled to perform well (though recent attempts have fared well ). Ideally, an agent should learn to explore the state-space better, see whether you have visited some states before, and if not, go visit those to reduce your uncertainty about them and see whether they are rewarding. People use prioritized experience replays, reward shaping, uncertainty-based exploration, and a bunch of other tweaks to solve this. The point of these tweaks is to help the agent with the exploration problem such that it can. However, once the agent learns to find the correct area of the state-space, i.e. one that leads it to the rewarding locations which are sparse, you could remove the bonuses that you artificially constructed, for instance, by reward-shaping since the exploration problem is not as difficult because the agent knows where the reward is. If I understand correctly, this is the logic you are using when you framed your question.
