[site]: crossvalidated
[post_id]: 425127
[parent_id]: 425123
[tags]: 
Question 1: No, the optimization figures that out itself. You provide the constraint for every training example, and the support vectors are the ones that lie on the "gutters" (usually called lying on or within the margin); these are the ones that will have $\alpha_i > 0$ . For the non-support vectors, $\alpha_i = 0$ at the optimal solution, but the optimization algorithm determines which ones are zero and which aren't. Question 2: You're right that this is dependent on the choice of kernel and what the data looks like: For a linear kernel, the original problem, this is obviously not true. For something like a polynomial kernel, it will allow for linear separability for some kinds of data distributions, but not others; thinking through some examples of where it does and where it doesn't is a good exercise for yourself. For something like the kernel $k(x, y) = \exp(- \lVert x - y\rVert^2 / (2 \sigma^2))$ – which goes by many names including Gaussian, RBF (somewhat "improperly"), squared exponential, or exponentiated quadratic – it turns out that any dataset will always be linearly separable, as long as there are no two identical points with different labels. But even if a dataset is perfectly linearly separable, that doesn't mean that a soft-margin SVM will actually prefer that solution. There's a tradeoff between maximizing the margin and the regularization that you do in an SVM; you might be able to get a bigger margin by allowing a few points to violate it a bit, compared to having an exact but very small margin. Playing around with this a bit is also a good exercise for yourself.
