[site]: crossvalidated
[post_id]: 122609
[parent_id]: 122062
[tags]: 
I'm going to leave it as an exercise for the community to flesh this answer out, but in general the reason why shrinkage estimators will *dominate*$^1$ unbiased estimators in finite samples is because Bayes$^2$ estimators cannot be dominated$^3$, and many shrinkage estimators can be derived as being Bayes.$^4$ All of this falls under the aegis of Decision Theory. A exhaustive, but rather unfriendly reference is "Theory of point estimation" by Lehmann and Casella. Maybe others can chime in with friendlier references? $^1$ An estimator $\delta_1(X)$ of parameter $\theta \in \Omega$ on data $X$ is dominated by another estimator $\delta_2(X)$ if for every $\theta \in \Omega$ the Risk (eg, Mean Square Error) of $\delta_1$ is equal or larger than $\delta_2$, and $\delta_2$ beats $\delta_1$ for at least one $\theta$. In other words, you get equal or better performance for $\delta_2$ everywhere in the parameter space. $^2$ An estimator is Bayes (under squared-error loss anyways) if it is the the posterior expectation of $\theta$, given the data, under some prior $\pi$, eg, $\delta(X) = E(\theta | X)$, where the expectation is taken with the posterior. Naturally, different priors lead to different risks for different subsets of $\Omega$. An important toy example is the prior $$\pi_{\theta_0} = \begin{cases} 1 & \mbox{if } \theta = \theta_0 \\ 0 & \theta \neq \theta_0 \end{cases} $$ that puts all prior mass about the point $\theta_0$. Then you can show that the Bayes estimator is the constant function $\delta(X) = \theta_0$, which of course has extremely good performance at and near $\theta_0$, and very bad performance elsewhere. But nonetheless, it cannot be dominated, because only that estimator leads to zero risk at $\theta_0$. $^3$ A natural question is if any estimator that cannot be dominated (called admissible , though wouldn't indomitable be snazzier?) need be Bayes? The answer is almost. See "complete class theorems." $^4$ For example, ridge regression arises as a Bayesian procedure when you place a Normal(0, $1/\lambda^2$) prior on $\beta$, and random effect models arise as an empirical Bayesian procedure in a similar framework . These arguments are complicated by the fact that the vanilla version of the Bayesian admissibility theorems assume that every parameter has a proper prior placed on it. Even in ridge regression, that is not true, because the "prior" being placed on variance $\sigma^2$ of error term is the constant function (Lebesgue measure), which is not a proper (integrable) probability distribution. But nonetheless, many such "partially" Bayes estimators can be shown to be admissible by demonstrating that they are the "limit" of a sequence of estimators that are proper Bayes. But proofs here get rather convoluted and delicate. See "generalized bayes estimators".
