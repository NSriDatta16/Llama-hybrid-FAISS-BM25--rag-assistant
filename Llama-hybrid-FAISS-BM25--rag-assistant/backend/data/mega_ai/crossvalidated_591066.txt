[site]: crossvalidated
[post_id]: 591066
[parent_id]: 591065
[tags]: 
Tunning the hyperparameters does not have to lead to improvement. It can be the case that the default parameters were good enough or that different values of parameters do not play an important role in training (e.g. the data is especially good or bad). Maybe you simply reached the limit of what XGBoost can achieve here. XGBoost is not the only machine learning model, it is still possible that other models can have better performance. If "no improvement" means very bad results, it always can be the case that the problem cannot be solved with machine learning. Check your code for bugs, if there are no changes, maybe you are doing something incorrectly. Did you make sure that you are exploring a broad enough region of the hyperparameter space?
