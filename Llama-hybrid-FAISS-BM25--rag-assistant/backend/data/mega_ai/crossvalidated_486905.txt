[site]: crossvalidated
[post_id]: 486905
[parent_id]: 
[tags]: 
Overfit in aggregated models: boosting versus simple bagging

Let's fix a bagging setup, where several models are build independently and than somehow aggregated. It is intuitive that increasing the number of weak learners ( N ) does not lead to overfit ( in the sense that the overfitting properties do not worsen adding an arbitrary number of trees ) . This is also discussed here for random forest: https://datascience.stackexchange.com/questions/1028/do-random-forest-overfit I was wondering if the situation is completely the opposite when we aggregate through boosting. In the AdaBoost algorithm, for example https://en.wikipedia.org/wiki/AdaBoost , the parameters of the next weak learner are chosen so that it improves the prediction of the previous step. Does it mean that, given enough weak learners, one would (over)fit perfectly the training data-set and, a fortiori, cause bad generaliazion ? The question refers to the (theoretical) asymtptotic behavior for large N (the number of weak learners).
