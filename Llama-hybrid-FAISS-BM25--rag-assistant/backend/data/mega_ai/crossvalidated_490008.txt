[site]: crossvalidated
[post_id]: 490008
[parent_id]: 489894
[tags]: 
This answer focuses mainly on the computer vision side of things. Also I guess I have a bit of a bias towards academic research practices. It's probably uncommon to develop architectures completely from scratch, but there's also a wide spectrum between "from scratch" and "take an off-the-shelf model". For example, you might start off your design with all the "common practices" -- relu, batch/instance norm, residual blocks, avg pooling, etc. This is pretty much "from scratch" as far as most people would consider it, but you're still pulling heavily from prior knowledge. You could also incorporate architectural features known to be useful or helpful for the task at hand -- dilated / strided convs if a wide receptive field is needed, spectral norm if designing a discriminator, gated convs if the input isn't fully dense, U-net structure for raster outputs, etc. This is pretty far from designing "from scratch", but also pretty far from taking an off-the-shelf model. Finally, a common design pattern in computer vision -- particularly object detection and segmentation -- is to have a big "backbone network" which extracts a feature map, followed by a number of "auxiliary networks" "branches" or "heads", which take the output of the backbone as their input and make the final task-specific predictions. Backbone networks include ResNet, ResNeXt, DenseNet, etc, and can often be easily swapped out for each other. To finally get to your question: for "well-studied" problems such as classification, detection, and segmentation, the backbone approach is very common -- although this is not to say that the entire field consists of just tuning models -- there can be very interesting and novel questions about the design of the heads, which I wouldn't call tuning. Stuff like generative models are typically much tricker to design and train, architectures can differ vastly depending on the target dataset, the model type (VAE vs GAN vs flows, etc), so "off-the-shelf" models don't work so great, unless you're simply trying to retrain the same model on very similar data. So there's definitely more "from scratch" design here. Finally as Sycorax points out, there's many niche reasearch areas where designing new architectures from scratch (which hopefully perform better than currently known approaches) is the whole point! It doesn't seem logical for us to develop a model from scratch (as typically taught to students) which often times we have a feeling that it might not work so well, then gradually switch to more complicated architectures through a trial-and-error process of tuning, when you can directly start with the complicated architecture and do tuning on top of it. Another perspective on this is that in a lot of cases, we're trying to investigate or improve on a secondary aspect of the network -- such as how the input / outputs are parameterized or preprocessed, or some data augmentation scheme, etc. It's common that the effect of these secondary aspects persists across many different network architectures, so it makes sense to start by experimenting with a simpler architecture (even if it doesn't perform as well), because it's faster to train / experiment with, and you can be more confident you won't run into any architecture specific idiosyncrasies (training instabilities, out of memory problems, uses batch norm which doesn't play nice with your objective function, etc). Then once you've made some progress, you just switch to using the better architecture, and confirm if your newfound knowledge / improvements also transfer over. And if they don't, that can be just as interesting from an academic perspective -- a possible sign of more interesting phenomena to investigate.
