[site]: crossvalidated
[post_id]: 587007
[parent_id]: 586989
[tags]: 
One thing that comes to mind for exploratory data analysis (EDA), applicable for 1D distributions, is to plot the quantiles (related to the empirical cumulative distribution function) in various transformed axes: linear-vs-linear, log-vs-linear, linear-vs-log, log-vs-log, etc. Having done so, some hypotheses may be formed and/or tested. A specific example where “log of log” may appear is when the 1D data is distributed according to a Weibull distribution: The cumulative distribution function (CDF) of a Weibull distributed variable $x$ is: $ F(x) = 1 - \exp(-(x/\lambda)^k).$ Therefore $$-\log(1 - F(x)) = (x/\lambda)^k$$ $$\log(-\log(1 - F(x))) = k \log(x) - k \log(\lambda).$$ Note: $\log$ is the natural logarithm, sometimes denoted $\operatorname{ln}(x)$ . Now, we obtain a straight line in the transformed axes: the left-hand side is the “dependent variable” in a kind of strange double-logarithmic transformation, $k$ is the slope of a log transformation of $x$ , and the intercept is $-k\log(\lambda)$ . In the example above, you approximate $F(x)$ empirically using the rank $r$ of each $x$ data value (requires sorting your data), and use a common method guaranteeing $0 required for the formula above such as $$\hat{F}(r) = \frac{r - 0.3}{n + 0.4},$$ where $r$ is the rank of the data value and $n$ is the total number of data points. Since the OP has a tag "least squares", please be very careful with applying it due to its fragility; better use a robust fitting procedure, such as Theil-Sen. Specifically in the example above, although the result should be a straight line after transformation, any small deviation of the actual data from a straight line may be magnified by the nonlinear transformation, ruining any least squares fit, but probably not a robust method.
