[site]: crossvalidated
[post_id]: 582008
[parent_id]: 53078
[tags]: 
I arrive here after reading "Probabilistic Programming and Bayesian Methods for Hackers" . Disclaimer not sure if I'm committing a crime here, but as we have a "prediction" model with a classification problem, one way to evaluate and visualize its goodness is to apply some standard ML classification eval plots . I usually use AUC and AUCPR plots, but there's also DET curve as well. we base these plots on the posterior_probability given by the sampling procedure. Another alternative (the one that I like the most) is the "precision-recall threshold curves" you can clearly see here how "good" your prediction probability is, given a different probability threshold. ( I think is the closest to the one that is in the book and publication) if you also define some probability threshold you can evaluate the confusion matrix among other metrics. At least for me, it is clearer when I have the AUC score, but again, maybe I'm getting too ahead of myself using this method for a bayesian model. code here
