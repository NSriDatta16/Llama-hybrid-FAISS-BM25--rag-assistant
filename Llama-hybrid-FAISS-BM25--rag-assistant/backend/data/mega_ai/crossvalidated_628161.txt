[site]: crossvalidated
[post_id]: 628161
[parent_id]: 627767
[tags]: 
I don't know how it works, but I do have a suspicion. As it's unconfirmed, I would ask you don't choose this as the correct answer. But for the sake of furthering discussion, here's my guess, based on what I know of the networks and the feasibility of various training datasets: The earlier layers of a diffusion model tend to focus on coarse structure, while the later ones do details. The QR Code Monster model influences only the coarse structure. ControlNet models are trainable models that learn to inject values between layers of the frozen main diffusion model (like SD1.5, SDXL, and their derivatives), in order to guide them based on their own inputs and a desired training output. As you said, trying to collect or create a bunch of "hidden layout" images for training seems infeasible, if not impossible. So putting all those together, my suspicion is they simply trained the ControlNet model to learn how to override the main model in an autoencoder fashion. In other words, during training, when QR Code 1 goes in, then the expected output is just a perfect copy of QR Code 1, etc. Once that model is trained up, they can "lobotomize" it by setting all the layers after a certain point back to 0 (or straight-up pruning them). In doing so, the model that learned how to override every aspect of the diffusion to match your control image now can only override the earlier layers -- the coarse structure -- and then the main model can take over again and fill in all the details based on your prompt. The result is an image that fits your prompt, but has a hidden structure matching your control image. Again, totally unverified, but it makes sense to me that this would be a solid approach to making it work.
