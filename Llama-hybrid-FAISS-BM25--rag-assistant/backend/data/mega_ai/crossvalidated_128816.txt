[site]: crossvalidated
[post_id]: 128816
[parent_id]: 
[tags]: 
Why is the mixtures of conjugate priors important?

I have questions about the mixture of conjugate priors. I learned and saw the mixture of conjugate priors a couple of times when I am learning bayesian. I am wondering why this theorem is such important, how are we going to apply it when we are doing Bayesian analysis. To be more specific, one theorem from Diaconis and Ylivisaker 1985 illustrated a theorem as this: Given a sampling model $p(y|\theta)$ from an exponential family, any prior distribution can be approximated by a finite mixture of conjugate prior distributions. More specifically, given prior $p(\theta)=\int p(\theta|\omega)p(\omega)d\omega$ , we can derive the posterior: $p(\theta|Y)\propto\int p(Y|\theta)p(\theta|\omega)p(\omega)d\omega\propto\int \frac{p(Y|\theta)p(\theta|\omega)}{p(Y|\omega)}p(Y|\omega)p(\omega)d\omega\propto \int p(\theta|Y, \omega)p(Y|\omega)p(\omega)d\omega$ Therefore, $p(\theta|Y)=\frac{\int p(\theta|Y, \omega)p(Y|\omega)p(\omega)d\omega}{\int p(Y|\omega)p(\omega)d\omega}$
