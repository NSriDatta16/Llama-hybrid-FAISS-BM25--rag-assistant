[site]: crossvalidated
[post_id]: 509898
[parent_id]: 442773
[tags]: 
However, since I'm manually counting to get actual object count, I selected only 30 images. 30 is one of the two conditions of the Central Limit Theorem for a sample mean. If you sample more than 30 cases and there are no extreme outliers, the sampling distribution of the mean is expected to be nearly normal. In the following analysis, we assume that the frequentist methods would be satisfactory. In your case, I think you can transform the numerical outcome into a categorical binary variable: Correct or Incorrect . If your model detects all the relevant objects in an image correctly you label it as Correct , otherwise Incorrect . In this scenario, you need about 1068 cases with a 95% confidence level and margin of error smaller than 0.03 according to power analysis. That is, the sample proportion is within ±0.03 of the actual proportion in a 95% confidence interval. And here is the derivation(to take the population size into consideration please refer to this article ): $z^{*}\sqrt{\frac{p(1-p)}{n}} For a 95% confidence interval, the z score $z^{*}$ is 1.96 , and we obtain: $1.96 \times \sqrt{\frac{p(1-p)}{n}} We rearrange the inequality to this form: $n> p(1-p)(\frac{1.96}{0.03})^2$ If we take p as the worse case value 0.5 to make the right side the largest we get this: $n> .5(1-.5)(\frac{1.96}{0.03})^2=1067.11$ Will that sample be enough to make a conclusion that algorithm 1 is better than others in terms performance and accuracy? Since you would like to compare two proportions, let's work out how to check the hypothesis that the difference between the two proportions is 0.03. That's is if the model is better by 3% than the baseline. And again we set the confidence interval as 95%, meaning our z score is 1.96. Suppose that the proportion for your model is $p_1$ and the baseline proportion is $p_2$ and the cases for the two models are not paired. $\frac{p_1 - p_2 - .03}{\sqrt {\frac{p_1 (1- p_1)}{n}+\frac{p_2 (1- p_2)}{n}}} > 1.96$ After some arrangements we get this: $n > \frac{1.96^2 \times (p_1 + p_2 -P_1^2 - p_2^2)}{(p_1-p_2-.03)^2}$ We can see from the graph that the maximum of the right side is positive infinity, then let's guess that $p_1$ is .95 and $p_2$ is .9, and we get $n > \frac{1.96^2 \times (.95 + .9 -.95^2 - .9^2)}{(.95-.9-.03)^2} = 1320.55$ And the minimum sample size is 1321 for each model. The above is just for illustration and the real answer would be more complicated and you can refer to this book: Sample Sizes for Clinical, Laboratory and Epidemiology Studies or just use this online tool: Sample size calculation: comparison of two proportions . If you apply one test set to the two models you need the paired chi-square test: McNemar’s Test , and this would be also helpful: Statistical Significance Tests for Comparing Machine Learning Algorithms . Help this would be of any help to you.
