[site]: crossvalidated
[post_id]: 490750
[parent_id]: 490643
[tags]: 
I'd like to echo some of the points in the other answer with slightly different emphasis. To me the most important issue is that the Bayesian view of uncertainty/probability/randomness is the one that directly answers the questions we probably care about, whereas the Frequentist view of uncertainty directly answers other questions that are often somewhat besides the point. Bayesian inferences try to tell us what we (or an algorithm, machine, etc.) should believe given the data we have seen, or in other words "what can I learn about the world from this data?" Frequentist inferences try to tell us how different our results would be if the data that we actually saw were "re-generated" or "repeatedly sampled" an infinite number of times. Personally I sometimes find Frequentist questions interesting, but I can't think of a scenario where the Bayesian questions aren't what matter most (since at the end of the day I want to make a decision about what to believe or do now that I've seen new data). It's worth noting that often people (statisticians included) incorrectly interpret Frequentist analyses as answering Bayesian questions, probably betraying their actual interests. And while people get worried about the subjectivity inherent in Bayesian methods, I think of the Tukey line, "Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise." For what it's worth, Frequentist methods are also subjective, and arguably in ways that are less obvious and convenient to critique. Getting off my Bayesian high horse, you're right that answers to Frequentist questions (especially MLE) sometimes coincide closely (and in rare cases, exactly) with answers to Bayesian questions. However, large data is a vague notion in a few senses that can make Bayesian and Frequentist (MLE) answers remain different: Most results about large data are asymptotic as the sample size goes to infinity, meaning that they don't tell us when our sample size is actually large enough for the asymptotic result to be accurate enough (up to some known level of error). If you go through the trouble to do both Bayesian and Frequentist analyses of your data and find they're numerically similar then it doesn't matter so much. Often with "large" data (e.g. many observations) we also have a large number of questions or parameters of interest. This is basically Bernhard's point #4. A lot of large data sets are not perfectly designed and relate to our interests indirectly because of issues like measurement error or sampling bias. Treated honestly, these complications may not go away even asymptotically, meaning that the models that realistically relate the data to what we care about have non-identifiable sensitivity parameters that are most natural to deal with using priors and the Bayesian machinery. Of course, the flip-side of this question is "Why should I be Frequentist when my dataset is large?"
