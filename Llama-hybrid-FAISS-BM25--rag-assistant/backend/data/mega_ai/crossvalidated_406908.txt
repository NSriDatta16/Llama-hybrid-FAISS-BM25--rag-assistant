[site]: crossvalidated
[post_id]: 406908
[parent_id]: 406857
[tags]: 
Do not do anything to your sampling distribution. This is not a situation where we have strong class imbalance; there is no reason to perplex things. Do assess the classifier performance based on calibration plots . In general what we want is a well-calibrated model. I would suggest trying a Generalised Additive Model ( GAM ) so non-linear relations between the predictor variables and the response can be taken into account. Aside GAMs using a penalisation approach like elastic net , might be a good idea especially given the relatively small size available. To that extent, 150 samples do not offer a lot room of generalisation so it would be essentially to cross-validate your results. The methodology presented in Beleites et al. (2013) Sample size planning for classification models is a good starting point. Side-note for GBMs: A GBM while great, usually does not offer well-calibrated probabilities out of the box. The extra calibration step (i.e. Platt scaling , isotonic regression , beta calibration , etc.) requires a hold-out sample and with 150 samples to begin with, this is too expensive at this point. It is more prudent to focus on learners like GLM/GAM that are well-calibrated out-of-the-box (see the CV.SE thread on Why is logistic regression well calibrated, and how to ruin its calibration? , for more details).
