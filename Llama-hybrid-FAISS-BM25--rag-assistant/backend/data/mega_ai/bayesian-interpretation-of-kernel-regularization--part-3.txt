\displaystyle {\hat {f}}(\mathbf {x} ')=\mathbf {k} ^{\top }(\mathbf {K} +\lambda n\mathbf {I} )^{-1}\mathbf {Y} .} A Bayesian perspective The notion of a kernel plays a crucial role in Bayesian probability as the covariance function of a stochastic process called the Gaussian process. A review of Bayesian probability As part of the Bayesian framework, the Gaussian process specifies the prior distribution that describes the prior beliefs about the properties of the function being modeled. These beliefs are updated after taking into account observational data by means of a likelihood function that relates the prior beliefs to the observations. Taken together, the prior and likelihood lead to an updated distribution called the posterior distribution that is customarily used for predicting test cases. The Gaussian process A Gaussian process (GP) is a stochastic process in which any finite number of random variables that are sampled follow a joint Normal distribution. The mean vector and covariance matrix of the Gaussian distribution completely specify the GP. GPs are usually used as a priori distribution for functions, and as such the mean vector and covariance matrix can be viewed as functions, where the covariance function is also called the kernel of the GP. Let a function f {\displaystyle f} follow a Gaussian process with mean function m {\displaystyle m} and kernel function k {\displaystyle k} , f ∼ G P ( m , k ) . {\displaystyle f\sim {\mathcal {GP}}(m,k).} In terms of the underlying Gaussian distribution, we have that for any finite set X = { x i } i = 1 n {\displaystyle \mathbf {X} =\{\mathbf {x} _{i}\}_{i=1}^{n}} if we let f ( X ) = [ f ( x 1 ) , … , f ( x n ) ] ⊤ {\displaystyle f(\mathbf {X} )=[f(\mathbf {x} _{1}),\ldots ,f(\mathbf {x} _{n})]^{\top }} then f ( X ) ∼ N ( m , K ) , {\displaystyle f(\mathbf {X} )\sim {\mathcal {N}}(\mathbf {m} ,\mathbf {K} ),} where m = m ( X ) = [ m ( x 1 ) , … , m ( x N ) ] ⊤ {\displaystyle \mathbf {m} =m(\mathbf {X} )=[m(\mathbf {x} _{1}),\ldots ,m(\mathbf {x} _{N})]^{\top }} is the mean vector and K = k ( X , X ) {\displaystyle \mathbf {K} =k(\mathbf {X} ,\mathbf {X} )} is the covariance matrix of the multivariate Gaussian distribution. Derivation of the estimator In a regression context, the likelihood function is usually assumed to be a Gaussian distribution and the observations to be independent and identically distributed (iid), p ( y | f , x , σ 2 ) = N ( f ( x ) , σ 2 ) . {\displaystyle p(y|f,\mathbf {x} ,\sigma ^{2})={\mathcal {N}}(f(\mathbf {x} ),\sigma ^{2}).} This assumption corresponds to the observations being corrupted with zero-mean Gaussian noise with variance σ 2 {\displaystyle \sigma ^{2}} . The iid assumption makes it possible to factorize the likelihood function over the data points given the set of inputs X {\displaystyle \mathbf {X} } and the variance of the noise σ 2 {\displaystyle \sigma ^{2}} , and thus the posterior distribution can be computed analytically. For a test input vector x ′ {\displaystyle \mathbf {x} '} , given the training data S = { X , Y } {\displaystyle S=\{\mathbf {X} ,\mathbf {Y} \}} , the posterior distribution is given by p ( f ( x ′ ) | S , x ′ , ϕ ) = N ( m ( x ′ ) , σ 2 ( x ′ ) ) , {\displaystyle p(f(\mathbf {x} ')|S,\mathbf {x} ',{\boldsymbol {\phi }})={\mathcal {N}}(m(\mathbf {x} '),\sigma ^{2}(\mathbf {x} ')),} where ϕ {\displaystyle {\boldsymbol {\phi }}} denotes the set of parameters which include the variance of the noise σ 2 {\displaystyle \sigma ^{2}} and any parameters from the covariance function k {\displaystyle k} and where m ( x ′ ) = k ⊤ ( K + σ 2 I ) − 1 Y , σ 2 ( x ′ ) = k ( x ′ , x ′ ) − k ⊤ ( K + σ 2 I ) − 1 k . {\displaystyle {\begin{aligned}m(\mathbf {x} ')&=\mathbf {k} ^{\top }(\mathbf {K} +\sigma ^{2}\mathbf {I} )^{-1}\mathbf {Y} ,\\\sigma ^{2}(\mathbf {x} ')&=k(\mathbf {x} ',\mathbf {x} ')-\mathbf {k} ^{\top }(\mathbf {K} +\sigma ^{2}\mathbf {I} )^{-1}\mathbf {k} .\end{aligned}}} The connection between regularizatio