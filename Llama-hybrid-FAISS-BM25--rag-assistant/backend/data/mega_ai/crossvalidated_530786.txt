[site]: crossvalidated
[post_id]: 530786
[parent_id]: 530653
[tags]: 
Given that most neural networks are constructed from nodes that multiply and add (via the weights and biases of the matrics), how can such a system be a universal approximator for functions that include other mathematical forms such as ratios? I think what you’re asking for is a demonstration of why rational functions can be modeled as neural networks on an interval that does not include a singularity. Approximating a rational function such as $f(z)=z^{-1}$ in an interval “away from” the singularity can be done using piecewise linear functions. The simplest approximation uses 1 line, and has a high error; but we are “away from” the singularity, so the error is finite. We can reduce the high error by adding more linear basis functions, breaking the curve into smaller lines. In terms of neural networks, ReLU activation functions yield piecewise linear prediction functions. The main idea, breaking a curve into lines using ReLU, is outlined in How does the Rectified Linear Unit (ReLU) activation function produce non-linear interaction of its inputs? where the function being approximated is a simple quadratic. This construction shows how multiplication and addition can be composed with a common neural network activation to create an approximation to a rational function on an interval omitting the singularity.
