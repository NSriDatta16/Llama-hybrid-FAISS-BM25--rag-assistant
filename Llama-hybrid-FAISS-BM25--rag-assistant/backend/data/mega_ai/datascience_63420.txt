[site]: datascience
[post_id]: 63420
[parent_id]: 63079
[tags]: 
A feed-forward neural network looks like this: input -> hidden layer 1 -> hidden layer 2 -> ... -> hidden layer k -> output. Each layer may have a different number of neurons, but that's the architecture. An LSTM (long-short term memory cell) is a special kind of node within a neural network. It can be put into a feedforward neural network, and it usually is. When that happens, the feedforward neural network is referred to as an LSTM (confusingly!). So how does an LSTM work? Have a look at my answer here: Forget Layer in a Recurrent Neural Network (RNN) - tl;dr: an LSTM cell has three gates, each of which is used to modulate its input in some way: an input gate, a forget gate, and an output gate. It has a "memory" and an output that get modified by the gates. That is, within a single LSTM cell: (input & previous cell state) -> (input & forget gates) -> (update cell state) (input & previous cell state & updated cell state) -> (output gate) You can stack these cells in many different configurations. So there is no single "LSTM network" - rather a set of many possible architectures that can be built from these basic nodes. Hope that gets you started!
