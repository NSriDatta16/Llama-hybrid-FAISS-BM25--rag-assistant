[site]: crossvalidated
[post_id]: 621085
[parent_id]: 
[tags]: 
Is Fisher's discriminant analysis equivalent to the Bayes optimal LDA when the no. of classes is greater than two and covariances are all equal?

P.S. While I gave a brief background to make the question complete, informed readers can move to the questions 1 and 2 towards the end of this post, right after 'what are not clear to me are:' . Fisher's discriminant analysis for two classes: Let's assume that we've data in $\mathbb{R}^d$ from $2$ classes with means $\mu_0, \mu_1$ and covariances $\Sigma_0, \Sigma_1$ respectively, $d > 2$ and that we're performing Fisher's discriminant analysis (FDA for short, not a standard abbreviation) that finds the optimal line $\mathbb{R}^d$ passing through the origin, so that projections of the given data onto this subspace are 'optimally separated', i.e. the ratio of the between-class variance of the projection onto $w$ to the within-class variance of the projection onto $w$ is maximal; so mathematically copying from Wikipidea : is maximal w.r.t. $w,$ and it's well known that Now consider the homoscedasticity assumption, i.e. $\Sigma_0=\Sigma_1=\Sigma.$ In this case, we can see that $w\propto \Sigma^{-1}(\mu_1-\mu_0).$ Linear Discriminant Anslysis (LDA) for two classes: In this case, the goal is to perform a two-class classification and thus find a linear decision boundary between the two sets of data. We know that (see the section "LDA for two classes") in this case, we can have this decision boundary is given by: $$\{x\in \mathbb{R}^d: w^{T}x=c\}$$ where: Given the above analysis for the two-class case, we note that the decision boundary from LDA is orthogonal to the optimal direction of projection in FDA, and FDA and LDA can be deemed equivalent in this case . My question is: Are there any results that generalize this to the $K>2$ class FDA and LDA? A brief mathematical analysis for the same is given on the same Wiki page, or also in this book , P. 191-192, section 4.1.6, equation (4.51) where this time, instead of one direction, we look for a vector subspace of dimension $d' and the optimal solution is given by maximizing The paragraph after equation (4.51) explains why $d'\le k-1$ using a rank argument, which is clear. But what are not clear to me are: What's the formulation of Bayes' optimal multiclass LDA as a linear classifier under homoscedasticity ? Could you cite a resource, as I was unable to find it in neither the Wiki nor the book I mentioned above. Is there a subspace of dimension $d-d'$ arising from the multiclass Bayes' optimal LDA, that's perpendicular/orthogonal to the optimal subspace $W_{opt}$ of dimension $d'$ arising from multiclass FDA, mentioned above? I took a look at this as a resource, and on P.8, they derive the equation to the decision boundary between class $k, l, k\ne l,$ but this is of dimension $d-1 > d-d',$ so this isn't the subspace $W_{opt}.$ Thank you for any directions and resources!
