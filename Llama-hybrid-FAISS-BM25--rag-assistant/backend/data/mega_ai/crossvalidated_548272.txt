[site]: crossvalidated
[post_id]: 548272
[parent_id]: 
[tags]: 
Size of the training set versus complexity of the model during hyper-parameter tuning

In general, the higher is the complexity of a model (number of parameters) the larger should be the training size to try to avoid overfitting. With neural networks, one can have one or more hidden layers and each layer can have a certain number of neurons inside. Choosing the best network architecture is problem-specific and should be decided by cross-validating the different options by parameter tuning. When doing this, however, we are cross-validating the performance of networks with different complexities while using the same size of the training set. Smaller networks may receive an advantage by being trained with more data per parameter with respect to larger networks. In your opinion, can this have an effect on the overall result of the tuning process and best parameters choice? Should we then increase the size of the training set during the tuning process to maintain a similar number of training data points for each parameter (imagine a hypothetical case where we have infinite training data)?
