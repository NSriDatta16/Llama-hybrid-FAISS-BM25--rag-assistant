[site]: crossvalidated
[post_id]: 570912
[parent_id]: 570810
[tags]: 
I may have answered my own question using Support Vector Regression. The solutions seems to have worked well, but I'd welcome comments for improvement. (See my notes after the code block): import numpy as np import matplotlib.pyplot as plt import pandas as pd from sklearn.preprocessing import StandardScaler from sklearn.svm import SVR from sklearn.pipeline import make_pipeline dataset = pd.read_csv("touch.csv", header=0) dataset.head() x y x_raw y_raw z1_raw z2_raw 0 50.0 50.0 776 13364 3468 29436 1 50.0 50.0 7156 13780 3480 29264 2 50.0 50.0 6924 12444 3636 29152 3 50.0 50.0 6668 13288 3548 29180 4 50.0 50.0 6200 13048 3528 28872 y = dataset.iloc[:,0:1].values.astype(float).ravel() X = dataset.iloc[:,2:].values.astype(float) regressor = SVR(kernel='rbf') regressor.fit(X, y) y_ = regressor.predict(X) plt.plot(np.arange(0, len(y)), y, y_) plt.show() A few notes: There are large inaccuracies at the transitions, but in the target application, I can handle those by suppressing output until the output stabilizes. I'm aware that I'm committing a statistical faux pas by using the training data as test data, but my other datasets check out equally well. Since all the feature data is more or less in the same range (500 ... 32768) I didn't bother to normalize them.
