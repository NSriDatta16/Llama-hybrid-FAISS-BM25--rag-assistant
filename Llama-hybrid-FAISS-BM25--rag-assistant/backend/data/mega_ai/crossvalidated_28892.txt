[site]: crossvalidated
[post_id]: 28892
[parent_id]: 
[tags]: 
Correcting standard errors when the independent variables are autocorrelated

I have a question about how to correct standard errors when the independent variable has correlation. In a simple time series setting we can use Newey-West covariance matrix with a bunch of lags and that will take care of the problem of correlation in the residuals. What does one do in a panel data setting? Imagine the situation where you observe firms over time: $$ Y_{i,t} = a + b\Delta{X_{i,t}} + \epsilon_{i,t} $$ where the $\Delta{X_{i,t}} = X_{i,t} - X_{i,t-n}$. It seems that clustering standard errors on $i$ and on $t$ should fix this problem. Am I correct?
