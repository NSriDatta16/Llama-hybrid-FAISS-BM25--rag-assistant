[site]: crossvalidated
[post_id]: 444966
[parent_id]: 
[tags]: 
Variational autoencoder won't work for toy dataset (mixture of Gaussians)

I wish to use a Variational autoencoder (VAE) as a generator for a multivariate distribution which originates from a graphical model - e.g. samples from a Bayesian Network (I have my reasons...). I wanted to start with some toy problems, but got stuck on the very first one - getting a VAE to regenerate a mixture of Gaussians distribution. I generated a synthetic dataset of 16k samples from a mixture of 5, mostly non-overlapping, 2D gaussians, and trained a VAE to recreate this distribution (Tried many different architectures, all modeled after this official Pytoch VAE example with FC layers. Losses were MSE between the input and reconstructed input plus + $D_{KL}$ between the encoder's output and the prior). The train loss plateaus pretty fast, no matter what the learning rate is, and later at test time when feeding standard normal noise to the decoder, the output looks nothing like the original mixture. Question: Before I start a more serious debugging and/or architecture optimization effort than I already have, I'd like to know if my task is somehow ill defined or if this is expected to work? Training set - 16k 2D samples drawn from a mixture of 5 equiprobable uncorrelated (diagonal covariance matrices) Gaussians Test set - 64 samples from a standard normal distribution passed through the decoder
