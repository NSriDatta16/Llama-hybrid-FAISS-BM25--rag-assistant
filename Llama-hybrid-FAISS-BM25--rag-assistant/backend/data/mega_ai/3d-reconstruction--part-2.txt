ve 3D equivalent. Deep neural networks have shown to be highly effective for 3D reconstruction from a single color image. This works even for non-photorealistic input images such as sketches. Thanks to the high level of accuracy in the reconstructed 3D features, deep learning based method has been employed for biomedical engineering applications to reconstruct CT imagery from X-ray. Stereo vision Stereo vision obtains the 3-dimensional geometric information of an object from multiple images based on the research of human visual system. The results are presented in form of depth maps. Images of an object acquired by two cameras simultaneously in different viewing angles, or by one single camera at different time in different viewing angles, are used to restore its 3D geometric information and reconstruct its 3D profile and location. This is more direct than Monocular methods such as shape-from-shading. Binocular stereo vision method requires two identical cameras with parallel optical axis to observe one same object, acquiring two images from different points of view. In terms of trigonometry relations, depth information can be calculated from disparity. Binocular stereo vision method is well developed and stably contributes to favorable 3D reconstruction, leading to a better performance when compared to other 3D construction. Unfortunately, it is computationally intensive, besides it performs rather poorly when baseline distance is large. Problem statement and basics The approach of using Binocular stereo vision to acquire object's 3D geometric information is on the basis of visual disparity. The following picture provides a simple schematic diagram of horizontally sighted Binocular Stereo Vision, where b is the baseline between projective centers of two cameras. The origin of the camera's coordinate system is at the optical center of the camera's lens as shown in the figure. Actually, the camera's image plane is behind the optical center of the camera's lens. However, to simplify the calculation, images are drawn in front of the optical center of the lens by f. The u-axis and v-axis of the image's coordinate system O 1 u v {\displaystyle O_{1}uv} are in the same direction with x-axis and y-axis of the camera's coordinate system respectively. The origin of the image's coordinate system is located on the intersection of imaging plane and the optical axis. Suppose such world point P {\displaystyle P} whose corresponding image points are P 1 ( u 1 , v 1 ) {\displaystyle P_{1}(u_{1},v_{1})} and P 2 ( u 2 , v 2 ) {\displaystyle P_{2}(u_{2},v_{2})} respectively on the left and right image plane. Assume two cameras are in the same plane, then y-coordinates of P 1 {\displaystyle P_{1}} and P 2 {\displaystyle P_{2}} are identical, i.e., v 1 = v 2 {\displaystyle v_{1}=v_{2}} . According to trigonometry relations, u 1 = f x p z p {\displaystyle u_{1}=f{\frac {x_{p}}{z_{p}}}} u 2 = f x p − b z p {\displaystyle u_{2}=f{\frac {x_{p}-b}{z_{p}}}} v 1 = v 2 = f y p z p {\displaystyle v_{1}=v_{2}=f{\frac {y_{p}}{z_{p}}}} where ( x p , y p , z p ) {\displaystyle (x_{p},y_{p},z_{p})} are coordinates of P {\displaystyle P} in the left camera's coordinate system, f {\displaystyle f} is focal length of the camera. Visual disparity is defined as the difference in image point location of a certain world point acquired by two cameras, d = u 1 − u 2 = f b z p {\displaystyle d=u_{1}-u_{2}=f{\frac {b}{z_{p}}}} based on which the coordinates of P {\displaystyle P} can be worked out. Therefore, once the coordinates of image points is known, besides the parameters of two cameras, the 3D coordinate of the point can be determined. x p = b u 1 d {\displaystyle x_{p}={\frac {bu_{1}}{d}}} y p = b v 1 d {\displaystyle y_{p}={\frac {bv_{1}}{d}}} z p = b f d {\displaystyle z_{p}={\frac {bf}{d}}} The 3D reconstruction consists of the following sections: Image acquisition 2D digital image acquisition is the information source of 3D reconstruction. Commonly used 3D rec