[site]: datascience
[post_id]: 49588
[parent_id]: 49578
[tags]: 
I think it purely depends upon the model. For instance, if it is a Naive Bayes, as it deals with probabilities only, you can't use the negative values. In this case Normalization works! When you deal with geometry based algorithms such as SVM or Logistic Regression, it's better to standardize the data because due to (-1,1) symmetry in the data. The learning of training process happens very fast (due to symmetry points) when compared to Normalization. I believe Standardization mostly works for many algorithms. However, what I suggest you is do check the context of algorithm and loss function metric.
