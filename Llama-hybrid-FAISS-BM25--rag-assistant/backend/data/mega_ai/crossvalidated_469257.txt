[site]: crossvalidated
[post_id]: 469257
[parent_id]: 
[tags]: 
Integration factors in Variational Bayes' predictive distribution

Under the Variational Bayes framework, a posterior distribution $p({\bf Z}|{\bf X})$ for latent variables ${\bf Z}$ and observed variables ${\bf X}$ is approximated by finding the distribution $q$ that maximizes the lower bound $\mathcal L(q)$ given by $$ \mathcal{L}(q) = \int q({\bf Z})\log\left(\frac{p({\bf X}, {\bf Z})}{q({\bf Z})}\right) d {\bf Z} \tag{1} $$ Once maximized the lower bound, the predictive distribution for a new observation $x$ would be given by $$ \begin{aligned} p(x|{\bf X}) &= \int p(x | {\bf Z}) p({\bf Z} | {\bf X})d{\bf Z} \\ &\approx \int p(x | {\bf Z}) q({\bf Z})d{\bf Z} \end{aligned} \tag{2} $$ My question is about the predictive distribution for the variational linear regression and logistic regression presented in the book Pattern Recognition and Machine Learning by Christopher Bishop. In both cases, the author finds the approximate posterior distribution for the weights of the model, as well as the hyperparameters, by using $(1)$ . However, when finding the predictive distribution, he only integrates out the weight parameters and fixes the hyperparameters to their expected variational-approximate value. What is the reason behind fixing the hyperparameters instead of integrating them out as well? Is it merely to obtain a tractable integral?
