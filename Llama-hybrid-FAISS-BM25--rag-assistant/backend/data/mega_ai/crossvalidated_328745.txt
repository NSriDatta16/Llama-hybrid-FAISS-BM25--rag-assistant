[site]: crossvalidated
[post_id]: 328745
[parent_id]: 328703
[tags]: 
PCA and PLS-DA are mostly similar yet fundamentally different methods. PCA provides dimension reduction by penalizing directions of low variance. What is meant by that is you provide no class information whatsoever and deal only with variance in the independent variables. PLS-DA , on the other hand, again penalizes directions, but this time the directions are about covariance between independent and dependent variables(see this very good link for more on that). Oh, and in PLS-DA your dependent variables are simply class information. PCA + PLS-DA is quite uncommon(at least in my area, chemometrics). There are few justifications I can think of: The small direction of variances which are omitted via PCA can actually be useful in classification PLS-DA already provides dimension reduction and deals with multicollinearity in a very similar manner (if you have time, see the lovely NIPALS algorithm for PCA and PLS. It worked better than reading words for me) So, there might be instances where PCA + PLS-DA provides a good model but I suspect that is quite rare. Same applies to regression case; there is Principle Components Regression(PCR) and PLS alone, but I have never seen PLS after PCA. All in all, although not particularly wrong, I think using PCA prior to PLS-DA will probably introduce a risk of decreasing model performance while providing no significant advantage and using PLS-DA only is a better option. I don't know about Wilks lambda criterion, yet there are other methods for component selection which aim to yield highest prediction performance with minimal overfitting such as Leave-N-Out-Cross-Validation, Monte Carlo Cross Validation etc.. These methods are quite common and if you think your component selection is problematic, you can give these methods a try.
