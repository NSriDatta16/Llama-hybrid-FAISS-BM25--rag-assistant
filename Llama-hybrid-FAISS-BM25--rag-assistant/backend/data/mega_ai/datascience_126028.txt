[site]: datascience
[post_id]: 126028
[parent_id]: 42760
[tags]: 
I recommend that you change your point of view a bit: first decide on what forecast you want, then choose an appropriate error measure. Let me explain. Suppose we are not directly predicting a single value, but an entire (predictive) density $\hat{F}$ for our future unknown outcome, which we then want to summarize or condense into a single number. Assuming that the predictive density is a correct summary of the distribution of the outcome we are predicting, we can now ask which "one-number summary" yields the lowest expected error for our chosen error measure. And here, we should keep in mind that different error measures are minimized in expectation by different one-number summaries. The MSE and RMSE is minimized by the expectation of $\hat{F}$ . Showing this is a rather simple and fun calculus exercise. So is $1-R^2$ (let's work with minimizing this rather than maximizing $R^2$ , which is of course the same, but makes life easier), since $R^2$ is just a reformulation of MSE . The expected absolute error, your MAD, is minimized by the median of $\hat{F}$ . The hinge or quantile loss is minimized by the appropriate quantile of $\hat{F}$ , which is of course exactly why we use it in quantile regression, or in any way of predicting conditional quantiles. Thus, if we want conditional median predictions, we should use the MAD, and if we want conditional expectations, we should use the (R)MSE. (This is a reformulation of the common meme that "the MAD is less affected by outliers than the MSE", and the discussion above illustrates why I believe this is a mistaken way of looking at things.) If we want our model to yield expectation forecasts, using the MAD may give the misleading impression that we are not doing a good job. The difference of course depends on how asymmetric our conditional density is. Now, you might argue that we do not care about predictive densities, we only want single-number predictions. To which I would answer that even if we are not interested in densities, they are still very much interested in us . As long as we are dealing with noisy data, (error) distributions are still very relevant. Even if we never explicitly consider predictive densities, using the (R)MSE will reward predictions that are close to the conditional expectation, and the MAD will reward ones that are close to the conditional median, simply because calculating them deals with the distribution of the actuals on which we evaluate our predictions. And this is where my recommendation comes from: we should first decide on which "one number summary" ("functional") of the unknown future distribution we want (expectation, median, quantile), and then pick the error measure that elicits that functional (MSE, MAD, pinball loss). Kolassa (2020) , a commentary on the M4 forecasting competition, makes the point in somewhat more detail. And here I discuss the Mean Absolute Percentage Error (MAPE) in the same way: What are the shortcomings of the Mean Absolute Percentage Error (MAPE)? As an example, assume that conditional on our model and predictors, we believe that the future realization follows a lognormal distribution with log-mean $\mu=1$ and log-variance $\sigma^2=1$ . Below is a simulated example in time series format, together with the point predictions that minimize each expected error, based on $10^5$ simulations. Since the simulations are IID, the optimal point predictions will be a horizontal flat line. Note that minimizing the MAD, the MAE or the MSLE leads to predictions that are far below the expectation of $\exp\big(\mu+\frac{\sigma^2}{2}\big)=\exp 1.5\approx 4.48$ . (I hesitate to call them "biased", because per above, these error measures elicit exactly what they are eliciting, and the issue is just that this elicited functional is not the expectation.) Actually, we can look at different predictions and calculate the expected errors over our $10^5$ simulations. We can then deduce which prediction minimizes the expected error. See below for these plots - the colored vertical lines in each case give the optimal predictions (and correspond to the horizontal prediction lines above, because that is how I got them), while the vertical grey line gives the expectation at about 4.48. For a second example, we can simulate Poisson data with mean and variance $\lambda=0.5$ : Fun fact: trying to minimize the MAD or MAE incentivizes us to output a flat zero forecast. This is exactly as it should be, since the median of a Poisson(0.5) is zero, but it did make some people working in intermittent demand forecasting rather nervous ( Kolassa, 2016 ). Here are the expected errors against candidate predictions: R code for the plots: plot_result
