[site]: crossvalidated
[post_id]: 3276
[parent_id]: 
[tags]: 
Regularization and Mean Estimation

Suppose I have some i.i.d. data $x_1, \ldots, x_n \sim N(\mu, \sigma^2)$, where $\sigma^2$ is fixed and $\mu$ is unknown, and I want to estimate $\mu$. Instead of simply giving the MLE of $\mu = \bar{x}$, one could estimate (1) $\mu = \lambda \mu_0 + (1 - \lambda) \bar{x},$ for some "prior best guess" $\mu_0$. This also has a nice Bayesian interpretation: we place a prior $\mu \sim N(\mu_0, \sigma^2_0)$ on $\mu$, and $\lambda$ is the weighted precision. I seem to recall that this also has an explicit L2 regularization-interpretation (i.e., we choose some penalty and minimize the squared loss to get the above estimate), similar to things like lasso and ridge regression, but I can't remember how it goes. Can anyone explain what the L2 regularization-interpretation of (1) is? [More general answers, where the data isn't necessarily normal distributed, are welcome as well.]
