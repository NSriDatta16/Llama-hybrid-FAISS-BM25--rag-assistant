[site]: datascience
[post_id]: 56528
[parent_id]: 46312
[tags]: 
First a clarification: there is no masking at all in the [CLS] and [SEP] tokens. These are artificial tokens that are respectively inserted before the first sequence of tokens and between the first and second sequences. About the value of the embedded vectors of [CLS] and [SEP] : they are not filled with 0's but contain numerical representations like any of the other vectors in the embedding table. From the point of view of the model, there is nothing special about their embedded vectors, so they are trained normally like the others. About masking proper names: you should take into account that BERT is a subword-based model. This means that words get split into subwords that are part of the vocabulary. Sometimes a word maps to only one token, but other times a single word maps to a sequence of several tokens. Infrequent words tend to be sliced almost at the character level. Depending on the data the model is trained with, the proper nouns you are referring to may be split into multiple subwords. You should take this into account when designing your masking procedure by either: Before tokenizing, you replace the proper nouns with a text sequence that is included in the vocabulary and that is exclusive to them. After tokenizing the original sentence, you check if the text contained a proper noun and if so, you locate the sequence of subwords associated to them and replace each subsequence with a special proper noun token ID. Take into account that this is not similar to BERT's inherent masking, where the input token is masked but the loss forces the model to predict the original token. In your case, the original proper noun would be masked to never return.
