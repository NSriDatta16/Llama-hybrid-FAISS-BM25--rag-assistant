[site]: crossvalidated
[post_id]: 4606
[parent_id]: 
[tags]: 
Averaged continuous Kernel Density Estimates in lieu of a discrete Kernel Density Estimate in Monte Carlo Proceedure

I am thinking of using this code in a Monte Carlo routine to generate Kernel Density Estimates for subsequent use in a Naive Bayes Classifier (see this earlier post) . The author of the code states on the above linked page that it will "recognise that the data you have provided is perfectly discrete and since discrete data does not need smoothing, the selected bandwidth should be zero" in the case of ties. However, I do not want this but can envisage that, given the parameters I intend to use, multiple repetitions in a large Monte Carlo routine will very likely produce ties in the generated data. To prevent this I am thinking of limiting the number of iterations in the MC routine and employing checks to ensure non-discrete generated data for use in the Kernel Density Estimate code, and then repeating. Perhaps this is more clearly explained thus: instead of a 100,000 iteration MC, which is highly likely to produce unwanted ties, and then one KDE, do a 100 iteration MC and one KDE and repeat this 100 and 1 routine 1000 times. This will result in 1000 separate but similar continuous KDEs which can then be averaged to produce a single "unified" continuous KDE. Is this a valid approach? Are averaged KDEs a statistically sound methodology? The discussion section here would seem to imply that even if there are deficiencies in this approach, they wouldn't necessarily be too troublesome.
