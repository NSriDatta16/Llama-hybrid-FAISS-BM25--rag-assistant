[site]: crossvalidated
[post_id]: 605114
[parent_id]: 390309
[tags]: 
Thank you so much for making this post! I was looking to solve the same problem, and yours is the only one I found that satisfies both (1) compare two samples (2) The null hypothesis isn't just p1 = p2, but p1 Setup. X: The data. X1, X2 ~ Bin(n1, p2), Bin(n2, p2), where n1= 1000, n2=2000, and the sampled data are: successes = [20,30] respectively. H0: p1 > p2, H1: p1 Assume uniform priors for p1, p2. Author's procedure. Want: Compute Bayes factor $\frac{P(x | H1)}{P(x | H0)}$ . Since we assume uniform priors, then P(H0) = P(H1). The bayes factor is the same as computing the posterior odds ratio. Want: Compute posterior odds ratio $\frac{P(H1 | x)}{P(H0 | x)}$ . We note that the denominator is just 1 - numerator. So, let's just focus on the numerator. Want: Compute $P(H1 | x)$ . We expand this out to be: $P(p1 > p2 | x1, x2)$ The author approximates $P(p1 > p2 | x1, x2)$ by sampling the individual posteriors: Sample p1 from the individual posterior $P(p1 | x1)$ , which is a Beta distribution (See 3.1 from these notes on conjugate priors), similarly p2 from its posterior Estimate $P(p1 > p2 | x1, x2)$ to be: out of the (p1,p2) pairs sampled from the individual posteriors, what fraction of them has p1 > p2? Proof, and gaps. So far, the entire procedure makes sense to me. The only thing we're missing is the proof that your sampling method is valid. I lack the background knowledge to do this, so can only offer an intuitive explanation: "P(p1 > p2 | x1, x2) = integral p1>p2 joint_pdf(p1,p2 | x1,x2) = integral p1>p2 f(p1|x2) f(p2|x2)". This last term, corresponds to your sampling approach. -- A request for someone who has learned about MCMC to verify the intuition please :)
