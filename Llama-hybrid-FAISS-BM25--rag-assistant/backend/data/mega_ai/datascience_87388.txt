[site]: datascience
[post_id]: 87388
[parent_id]: 87185
[tags]: 
I replicate your code using a toy data set and I did not find anything wrong with your implementation: import numpy as np from sklearn.preprocessing import StandardScaler from sklearn.datasets import load_breast_cancer from sklearn.metrics import accuracy_score import matplotlib.pyplot as plt plt.style.use("seaborn-whitegrid") import warnings warnings.filterwarnings("ignore") X, y = load_breast_cancer(return_X_y= True) X_train,X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state = 42) sc = StandardScaler().fit(X_train, y_train) X_train = sc.transform(X_train) X_test = sc.transform(X_test) def sigmoid(x): return 1 / (1 + np.exp(-x)) def predict(X, W, b): return (sigmoid(X.dot(W) + b)) b = 2.0 W = np.repeat(1.0, X_train.shape[1]) m = X_train.shape[0] cost = list() lr = 0.001 num_epochs = 100 for epoch in range(num_epochs): Y = predict(X_train, W, b) W = W - lr * X_train.T.dot(Y - y_train.T) b = b - lr * np.sum(Y - y_train.T) loss = -1/m * np.sum(y_train * np.log(Y) + (1 - y_train) * np.log(1 - Y)) # print(W) # print(b) cost.append(loss) print(f"params are W:{W} and b:{b}") probs = predict(X_test, W, b) preds = np.where(probs > .5, 1,0) test_acc = accuracy_score(y_true = y_test, y_pred = preds) plt.plot(cost) plt.title(f"Accuracy score is: {round(test_acc,3)}"); Nonetheless, when using your data the results are inferior: data =pd.read_csv("https://raw.githubusercontent.com/abhiwalia15/500-Person-Gender-Height-Weight-Body-Mass-Index/master/500_Person_Gender_Height_Weight_Index.csv", error_bad_lines= False) data.drop(["Index"], axis = 1, inplace = True) X = data.drop(["Gender"], axis = 1) y = data.Gender.map({"Male":0,"Female":1}) X_train,X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state = 42) sc = MinMaxScaler().fit(X_train, y_train) X_train = sc.transform(X_train) X_test = sc.transform(X_test) b = 2.0 W = np.repeat(1.0, X_train.shape[1]) m = X_train.shape[0] cost = list() lr = 0.001 num_epochs = 100 for epoch in range(num_epochs): Y = predict(X_train, W, b) W = W - lr * X_train.T.dot(Y - y_train.T) b = b - lr * np.sum(Y - y_train.T) loss = -1/m * np.sum(y_train * np.log(Y) + (1 - y_train) * np.log(1 - Y)) # print(W) # print(b) cost.append(loss) print(f"params are W:{W} and b:{b}") probs = predict(X_test, W, b) preds = np.where(probs > .5, 1,0) test_acc = accuracy_score(y_true = y_test, y_pred = preds) plt.plot(cost) plt.title(f"Accuracy score is: {round(test_acc,3)}"); Going deeper on your data I see the problem as confirmed above, is not in your implementation of SGD but in the separateness of your data, this is not linearly separable: from sklearn.decomposition import PCA pca = PCA(n_components= 2).fit(X_train, y_train) X2D = pca.transform(X_test) ev = pca.explained_variance_.sum() plt.scatter(X2D[:,0], X2D[:,1], c = y_test, cmap = "RdYlBu") plt.colorbar() plt.title(f"PCA projection in 2D\nExplaneid variance is: [{round(ev,3)}]")
