[site]: crossvalidated
[post_id]: 242618
[parent_id]: 242595
[tags]: 
After some more thinking I will make an attempt to answer my own question. From Bishop's Pattern Recognition and Machine Learning, p. 296, I take rules for building new Kernels from valid Kernels. Let $k_1$ be a valid Kernel then $$ k(x_n,x_m) = f(x) k_1(x_n,x_m) f(x^T) $$ $$ k(x_n,x_m) = \exp(k_1(x_n,x_m)) $$ are again valid Kernels. Now we have $$\frac{\theta}{2} \lVert x_n-x_m \rVert^2 = \frac{\theta}{2} x_n^T x_n + \frac{\theta}{2} x_m^T x_m - \theta x_n^T x_m$$ So $$\exp (-\frac{\theta}{2} \lVert x_n-x_m \rVert^2)= \exp (-\frac{\theta}{2} x_n^T x_n) \exp (\theta x_n^T x_m) \exp (-\frac{\theta}{2} x_m^T x_m)$$ Hence by the second rule from above and since we know $x_n^T x_m$ is a valid kernel, $\exp (\theta x_n^T x_m) $ is a valid kernel if $\theta>0$, but it is not if $\theta 0$ but not if $\theta
