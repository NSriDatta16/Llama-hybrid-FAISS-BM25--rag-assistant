[site]: datascience
[post_id]: 126226
[parent_id]: 116705
[tags]: 
Is bigger always better? No. Otherwise, we'd always try to build the biggest possible networks. For instance, even if larger models always have a higher accuracy, it will come at a cost. Larger models typically require more data to achieve good performance, which takes longer, requires more computing, and might be more expensive. However Larger/deeper neural networks can model higher-dimensional functions and, therefore, more complex problems. That's just math and not up for debate. But in practice, we don't always know how "complex" our problems are and how large of a model would be necessary. Therefore, empirical evidence typically prevails.
