[site]: datascience
[post_id]: 22608
[parent_id]: 869
[tags]: 
I've tried the following 2 ways for trial-n-test implementation of neural networks with text. The latter one works fairly well, but with limitations. Create vocabulary using word2vect or NLTK/custom word tokens and assign an index to each word. It is this index which represents the word as number. Challenges: The indexes must be "normalized" using feature scaling. If the output of the neural network has even a slight variation, then the output may be an index to unexpected word (e.g. if expected output is 250; but NN outputs 249 or 251, then it might be a close output from numeric context; but they are indexes to different words). Recurrent NN to generate output index can be leveraged here. If new words are added to the vocabulary, then the token indexes should be re-scaled. The model trained with previously scaled values may become invalid and must be re-trained. Use identity matrix e.g. for "n" words use "n x n" or (n-1 x n-1) matrix where each row and column represents a word. Put "1" in intersection cell and "0" at rest places. ( reference ) Challenges: Every input and output value is "n x 1" vector. For a large sized vocabulary its a bulky computation and slower. If new words are added to the vocabulary, then the identity matrix (i.e. word vector) should be re-calculated. The model trained with previously calculated vectors may become invalid and must be re-trained.
