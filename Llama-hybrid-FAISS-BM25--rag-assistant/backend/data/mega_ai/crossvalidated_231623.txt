[site]: crossvalidated
[post_id]: 231623
[parent_id]: 
[tags]: 
Features selection: why does Boruta confirms all my features as "important"?

I want to run a feature selection study to select only the most important features, before running a machine learning classification. My data is 30,000 x 17 (Observed objects x Features). I use the R implementation of Boruta, with default parameters. My results is: all my 17 features are green (confirmed as "important"). It is suspicious because it is likely that some are not and should be dropped. When I only use a sub-set of observations (eg 100 randomly chosen observations among 30,000), the Boruta algo then changes drastically: 6 features are red (unimportant) and 11 are green (important). Why do I have such different results, is it overfitting? How should I perform to make sure I correctly identify the less and most relevant features among the initial set of 17?
