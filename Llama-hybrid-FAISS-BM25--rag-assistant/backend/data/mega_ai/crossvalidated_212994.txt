[site]: crossvalidated
[post_id]: 212994
[parent_id]: 
[tags]: 
Autoencoder with tied weights: bias?

For some unsupervised learning problem, I need to train an autoencoder, so that I only have to store the encoder afterwards. However, I am not sure on how and if the bias weights can be tied. To make myself more clear: I have input $x\in R^d$, being encoded to a code $h\in R^{d'}$ using the encoding: $h=\sigma(Wx+b)$. The code is then used for the recostructruction with $y=\sigma(W'h+b')$, where $W'=W^T$ (tied weights) and $||y-x||^2$ is used as the minimization criterion. And now my question: Is there a similar way to tie the bias $b'$ according to some function of $W$ and $b$, so that after the training of the autoencoder, I can store the encoder parameters only? Or do I have to store the bias weights of the decoder as well? Also, if there are more layers on the encoder/decoder, does the same apply to their bias weights as well?
