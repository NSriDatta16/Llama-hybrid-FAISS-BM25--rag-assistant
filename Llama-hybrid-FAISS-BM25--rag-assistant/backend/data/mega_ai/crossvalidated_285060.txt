[site]: crossvalidated
[post_id]: 285060
[parent_id]: 285058
[tags]: 
I'd say that it's only about perspective. In statistics you usually check for some assumptions in order to be sure that your model is going to give you good predictions in the data range. Also you choose which method you're going to use in your model . In machine learning all of these assumptions and checkings are more "empirical" in the sense that for example you could be checking you prediction accuracy after adding some feature engineering that perhaps include feature selection o even doing PCA over the X, so you 'll have independent variables, for example, if this is giving you the best accuracy over your test set. I think both are valid strategies and it only depends on several aspects: your knowledge about the data itself the amount of data you have and how good testing you can do with test data how many variables are there, 10? , 150? 900? Depending on this it would be more or less reasonable checking assumptions or doing "brute force" engeneering, etc
