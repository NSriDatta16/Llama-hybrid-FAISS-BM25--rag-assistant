[site]: datascience
[post_id]: 8210
[parent_id]: 8193
[tags]: 
The answers about Gibbs Sampling are misleading, in my opinion. The reason for this is that Gibbs Sampling leads to some error due to resampling. All Bayesian methods that use MCMC techniques face this issue, and it's really not that much of a problem, as under standard conditions, you can put bounds on your MCMC error. But this is not one of those standard conditions, as Latent Dirichlet Allocation leads to a multi-modal posterior! As such, standard sampling methods will typically find a single mode and sample around that single mode. The sampling techniques may well miss other modes, which may in fact have a higher posterior probability than the one found. Often, in these types of problems, several starts from different initial parameters are run and the solutions are compared. There's more advanced samplers that attempt to deal with this problem as well, such as a sampler that occasionally attempts to sample far from the current mode in the hopes of hitting another mode. Odds are that if you find a mode, it's probably a pretty good solution, even if not optimal, but that is certainly not insured! In regards to making results replicable, if you want get the same solution, you have to start from the same place, with the same random seed.
