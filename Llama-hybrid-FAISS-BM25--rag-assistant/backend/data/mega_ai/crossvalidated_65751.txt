[site]: crossvalidated
[post_id]: 65751
[parent_id]: 65699
[tags]: 
PCA and LSA are both analyses which use SVD. PCA is a general class of analysis and could in principle be applied to enumerated text corpora in a variety of ways. In contrast LSA is a very clearly specified means of analyzing and reducing text. Both are leveraging the idea that meaning can be extracted from context. In LSA the context is provided in the numbers through a term-document matrix. In the PCA you proposed, context is provided in the numbers through providing a term covariance matrix (the details of the generation of which probably can tell you a lot more about the relationship between your PCA and LSA). You may want to look here for more details. You are basically on track here. The exact reasons they are used will depend on the context and the aims of the person playing with the data. The answer will probably depend on the implementation of the procedure you are using. Carefully and with great art. Most consider the dimensions of these semantic models to be uninterpretable. Note that you almost certainly expect there to be more than one underlying dimension. When there is more than one dimension in factor analysis, we rotate the factor solution to yield interpretable factors. However, for some reason this is not typically done for these models. Your approach sounds like a principled way to start your art... although I'd be less than certain the scaling between dimensions is similar enough to trust a cluster analysis solution. If you want to play around with meaning, you might also consider a simpler approach in which the vectors have a direct relationship with specific words, e.g. HAL .
