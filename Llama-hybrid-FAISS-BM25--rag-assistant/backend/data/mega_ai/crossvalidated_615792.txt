[site]: crossvalidated
[post_id]: 615792
[parent_id]: 
[tags]: 
How should I understand the noise variance parameter(s) in multifidelity modeling using Emukit

I am learning the multi-fidelity modeling and have a question about Emukit's mixed_noise parameters, or more general, how should we determine the noise when the training data are simulation results, which can be assumed to be exact/noise-free according to the book Gaussian processes for machine learning. In Emukit's tutorial, it says: Note: The model implementation defaults to a MixedNoise noise likelihood whereby there is independent Gaussian noise for each fidelity. This can be modified upfront using the 'likelihood' parameter in the model constructor, or by updating them directly after the model has been created. In the example below, we choose to fix the noise to '0' for both fidelities in order to reflect that the observations are exact. and in the example: gpy_lin_mf_model.mixed_noise.Gaussian_noise.fix(0) gpy_lin_mf_model.mixed_noise.Gaussian_noise_1.fix(0) In my work, the high and low fidelity observations are both simulation results from two different mesh refinement levels. Should I simply fix both noise variance to be 0 like this tutorial example " in order to reflect that the observations are exact "? Or should I just fix the noise variance to be zero and try to quantify/determine the noise for the low fidelity? If yes, how should I derive a reasonable parameter? Also, I'm not sure if this mental image is correct: now I see the discrepancy term $\gamma(\mathbf{x})$ in linear multi-fidelity modeling $f_t(\mathbf{x}) = \rho f_{t-1}^* (\mathbf{x}) + \gamma(\mathbf{x}) $ is such a Gaussian process, it samples at the shared points between high and low fidelities (most methods assume nested DoE) and the target value is the difference among those shared points. Hence the two fidelities can both be exact right? In fact, it makes the prediction of the errors more confident, doesn't it? I appreciate your comments and insights!
