[site]: crossvalidated
[post_id]: 303047
[parent_id]: 
[tags]: 
Min-max stochastic function of few variables

I'm facing an optimization problem I'll have to run (with a variety of different hyperparameters) and I'd like some tips on how best to optimize it. I have a function $f$ which I can only evaluate in a noisy fashion, that is, there is a random variance to it. I have some 7 variables in group "A", and another 4 variables in group "B". I would like to maximize the expectation of $f$ with regards to the A variables, while simultaneously minimizing with regards to "B". Each evaluation of $f$ takes a few seconds up to a minute, so I'm happy to run a relatively complicated algorithm using data from all my previous evaluations, in order to converge in less calls to $f$. The only constraint is that the variables are all positive. Since they're also operating at very different scales (some are ~1, some are ~0.001, some are ~1000), so I'm making the problem a bit easier by taking log of all the parameters first. That is, I'm actually evaluating $f(\exp(\bf x))$, and tweaking my parameter vector $\bf x$. I expect that shifting my destination optimum will be within $\pm 3$ in each coordinate of $\bf x$, then. (So, within an order of magnitude of my original parameters.) This also means that the problem is essentially completely unconstrained now. Here is one idea I had: Start by evaluating $f$ at one initial point $\bf x_0$, and then evaluate $f$ at other values for $x_0$, with each coordinate $\pm 1$. Do a quadratic fit to these points (I already have a library set up to do that), and then inspect the quadratic term. If I had only group A variables, then I would check that the quadratic term is positive definite, and then run a test at the estimated global optimum of that quadratic. As I take more and more test points close to the optimum, the quadratic fit will begin to pay more attention to fitting correctly around there (since I have more samples), and so this should converge to the correct value. The downside is that it might end up weighted "incorrectly" by values far from the optimum for quite a while; and I'm not sure how I would handle when it's not positive definite (just run a random test point near an old expected optimum, maybe??); and I'm not sure how I would handle adding in the B-group variables would handle this. One possibility would be writing it as the sum of a quadratic A in and a quadratic in B, and then moving to the maximum of one term and minimum of the other. But this neglects the A/B variable cross terms, which I expect to matter here. Again, ideally, as more data accumulate around the true optimum, it should converge (i.e. all the B data is collected near the A optimum, and vice versa), but that convergence could be very slow. And I'm still not sure how to approach the positive definiteness. I could find some information on stochastic optimization algorithms, but they all seem to be much more designed for the case with very tight memory constraints, so that they avoid using earlier evaluations too much. For instance, this is the problem with the Python noisyopt algorithms. Since I'm min-maxing on different variables, also, I basically need to optimize with regards to A, and then holding A constant, optimize the other with regards to B, which isn't something that the most packages seem to like very much, so I think I need to roll my own algorithm here (and am quite happy to -- I just want these optimizations to be fast!) The final code where I run many such optimization problems, I expect to execute over the course of a month or so as part of a research project, so I'm really happy writing my own algorithm if I think it will be more stable and require fewer invocations.
