[site]: crossvalidated
[post_id]: 46986
[parent_id]: 43867
[tags]: 
The particular phenomenon that you see with the least squares solution in Bishops Figure 4.5 is a phenomenon that only occurs when the number of classes is $\geq 3$. In ESL , Figure 4.2 on page 105, the phenomenon is called masking . See also ESL Figure 4.3. The least squares solution results in a predictor for the middel class that is mostly dominated by the predictors for the two other classes. LDA or logistic regression don't suffer from this problem. One can say that it is the rigid structure of the linear model of class probabilities (which is essentially what you get from the least squares fit) that causes the masking. With only two classes the phenomenon does not occur $-$ see also Exercise 4.2 in ESL, page 135, for details on the relation between the LDA solution and the least squares solution in the two class case. Edit: Masking is perhaps most easily visualised for a two-dimensional problem, but it is also a problem in the one-dimensional case, and here the mathematics is particularly simple to understand. Suppose that the one-dimensional input variables are ordered as $$x_1 with the $x$'s from class 1, the $y$'s from class two and the $z$'s from class 3. Together with the coding scheme for the classes as three-dimensional binary vectors we have the data organized as follows $$\begin{array}{c|cccccccc} & 1 & \ldots & 1 & 0 & \ldots & 0 & 0 & \ldots & 0 \\ \mathbf{T}^T & 0 & \ldots & 0 & 1 & \ldots & 1 & 0 & \ldots & 0 \\ & 0 & \ldots & 0 & 0 & \ldots & 0 & 1 & \ldots & 1 \\ \hline \mathbf{x}^T & x_1 & \ldots & x_k & y_1 & \ldots & y_m & z_1 & \ldots & z_n \\ \end{array}$$ The least squares solution is given as three regressions of each of the columns in $\mathbf{T}$ on $\mathbf{x}$. For the first column, the $x$-class, the slope will be negative (all the ones are to the left above) and for the last column, the $z$-class, the slope will be positive. For the middle column, the $y$-class, the linear regression will have to balance the zeroes for the two outer classes with the ones in the middle class resulting in a rather flat regression line and a particularly poor fit of the conditional class probabilities for this class. As it turns out, the max of the regression lines for the two outer classes dominates the regression line for the middle class for most values of the input variable, and the middle class is masked by the outer classes. In fact, if $k = m = n{}$ then one class will always be masked completely, whether or not the input variables are ordered as above. If the class sizes are all equal the three regression lines all pass through the point $(\bar{x}, 1/3)$ where $$\bar{x} = \frac{1}{3k}\left(x_1 + \ldots + x_k + y_1 + \ldots + y_m + z_1 + \ldots + z_n\right).$$ Hence, the three lines all intersect in the same point and the max of two of them dominates the third.
