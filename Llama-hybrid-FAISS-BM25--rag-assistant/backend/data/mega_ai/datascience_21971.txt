[site]: datascience
[post_id]: 21971
[parent_id]: 21954
[tags]: 
OOB samples are a very efficient way to obtain error estimates for random forests. From a computational perspective, OOB are definitely preferred over CV. Also, it holds that if the number of bootstrap samples is large enough, CV and OOB samples will produce the same (or very similar) error estimates. Thus, if you perform many bootstrap samples, I would recommend performing cross-validation along the way with OOB samples until the OOB error converges.
