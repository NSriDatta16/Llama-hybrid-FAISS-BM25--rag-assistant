[site]: datascience
[post_id]: 75147
[parent_id]: 
[tags]: 
Don't know how to preprocess my dataset for image classification

I'm trying to do image classification using CNN. The exact model isn't important but I decided to try use AlexNet and I'm getting abysmal accuracy . I believe the issue might be with my data preprocessing. My dataset directory contains a Training and Test folder but no validation folder (I have to split the dataset myself) and they are layed out like this: Training ├── class0 │ ├── image1 │ ├── .... │ └── image20 │ ├── .... │ ├── image1 │ ├── .... │ └── image20 │ └── class9 ├── image1 ├── .... └── image20 Here is my AlexNet model, the code is below: model = tf.keras.models.Sequential([ #Conv_1 #original model was built for input shape of 224X224 tf.keras.layers.Conv2D(96, (11,11),strides=4, padding='valid', activation='relu', input_shape=(224, 224, 3)), # Pooling_1 tf.keras.layers.MaxPooling2D((2, 2), strides=(2,2),padding='valid'), # Batch Normalisation_1 tf.keras.layers.BatchNormalization(), # Conv_2 tf.keras.layers.Conv2D(256, (11,11),strides=1, padding='valid', activation='relu'), # Pooling_2 tf.keras.layers.MaxPooling2D((2, 2), strides=(2,2),padding='valid'), #Batch Normalisation_2 tf.keras.layers.BatchNormalization(), # Conv_3 tf.keras.layers.Conv2D(384, (3,3),strides=1, padding='valid', activation='relu'), # Batch Normalisation_3 tf.keras.layers.BatchNormalization(), # Conv_4 tf.keras.layers.Conv2D(384, (3,3),strides=1, padding='valid', activation='relu'), # Batch Normalisation_3 tf.keras.layers.BatchNormalization(), #conv_5 tf.keras.layers.Conv2D(256, (3,3),strides=1, padding='valid', activation='relu'), #pooling_3 tf.keras.layers.MaxPooling2D((2, 2), strides=(2,2),padding='valid'), #Batch Normalization_4 tf.keras.layers.BatchNormalization(), tf.keras.layers.Flatten(), #Dense layer_1 tf.keras.layers.Dense(4096, activation='relu'), tf.keras.layers.Dropout(0.5), tf.keras.layers.BatchNormalization(), #Dense layer_2 tf.keras.layers.Dense(4096, activation='relu'), tf.keras.layers.Dropout(0.5), tf.keras.layers.BatchNormalization(), #Dense layer_3 tf.keras.layers.Dense(1000, activation='relu'), tf.keras.layers.Dropout(0.5), tf.keras.layers.BatchNormalization(), tf.keras.layers.Dense(1, activation='sigmoid') ]) As for what I did to preprocess my data: train_dir = '/content/gdrive/My Drive/x/Training' test_dir = '/content/gdrive/My Drive/x/Test' train_datagen = ImageDataGenerator( rescale=1./255, rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest', ) test_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.4) train_generator = train_datagen.flow_from_directory( train_dir, target_size=(224, 224), batch_size=20, class_mode='binary') validation_generator = test_datagen.flow_from_directory( test_dir, target_size=(224, 224), batch_size=20, class_mode='binary', subset='validation') model.compile(loss='binary_crossentropy', optimizer=RMSprop(lr=1e-4), metrics=['acc']) history = model.fit_generator( train_generator, steps_per_epoch=100, epochs=10, validation_data=validation_generator, validation_steps=10, #callbacks = [checkpoint], verbose=2) Any and all help will be immensely appreciated! Thank you to anyone who may reply.
