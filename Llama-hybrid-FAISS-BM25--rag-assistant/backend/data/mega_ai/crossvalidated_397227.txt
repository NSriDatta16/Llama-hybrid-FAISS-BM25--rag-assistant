[site]: crossvalidated
[post_id]: 397227
[parent_id]: 
[tags]: 
Why feature transformation is needed in machine learning & statistics? Doesn't it affect the "interaction" between features?

Before feeding machine learning models, we can do data transformation and feature scaling depending on data distribution. For example, if a column is skewed, we can use Box-Cox transformation to reduce its skewness: https://machinelearningmastery.com/power-transform-time-series-forecast-data-python/ But do these feature transformation affect the interaction between features? And thus leading to bad predicting power of the ML models? After I performed box-cox transformation on several columns in a dataset, I found the correlation coefficients also changed. This gave me an "impression" that the data is altered way too much. But on the other hand, as long as I also perform the transformation on test data, the model learned with the transformed training data should also work on the test data. Standardization doesnot affect the correlations between features. http://rstudio-pubs-static.s3.amazonaws.com/318113_6581029a53064b988b700fc3eee55864.html#
