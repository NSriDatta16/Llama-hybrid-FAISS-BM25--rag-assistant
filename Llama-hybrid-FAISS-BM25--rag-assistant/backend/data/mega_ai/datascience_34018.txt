[site]: datascience
[post_id]: 34018
[parent_id]: 33814
[tags]: 
You have said in a comment that you use random forests and scikit-learn. The first step that you might take is to identify the features that have the greatest weight in deciding the outcome. As you can see here this is your classifier's property feature_importances_ which ranks the inputs by their importance. From my experience their importance will usually follow the power-law and you could set a threshold that will reduce the number of dimensions you are looking at significantly. From here it depends on what approach you would rather take - you can train a classifier that takes these features and outputs the changes you need to switch the resulting class (use regularization to minimize the changes) or you can use an optimization black-box to solve the problem on a per-case basis. Specifically you could use the scipy.optimize.minimize method to minimize the -(classifier confidence) with constraints. For example if you want to find $\bar{x}$ s.t. $sample * \bar{x}$ maximizes the classifier's confidence, you could do something on the lines of: def min_func(x): return -cls.fit(sample*x) x0 = [1.3, 0.7, 0.8, 1.9, 1.2] minimizing_xs = minimize(min_func, x0, method='SLSQP', constraints=cons) This optimization approach will naturally lend itself to a similar learning approach where you try to learn a generalized vector $\bar{x}$.
