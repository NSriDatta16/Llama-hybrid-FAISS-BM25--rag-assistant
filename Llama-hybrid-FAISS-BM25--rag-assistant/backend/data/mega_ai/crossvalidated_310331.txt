[site]: crossvalidated
[post_id]: 310331
[parent_id]: 
[tags]: 
Selecting parsimonious model after performing feature selection.

I am performing feature selection using 3 different techniques Lasso, Random forest and gradient boosting. I am comparing these 3 results based on Test MSE (Using CV). I pick the one that has the lowest MSE and get the important variables. The dataset I am using is standardized before performing feature selection. Of the variables that get picked as important ones, there is one that stands out to be the most important one compared to the rest. My question is, is it valid to perform subset selection after feature selection to find a model that has fewer parameters than the features that get selected by the algorithms above? If not, is there another technique to find a narrow set of selected features?
