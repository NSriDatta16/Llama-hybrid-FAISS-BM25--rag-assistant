[site]: datascience
[post_id]: 79730
[parent_id]: 76322
[tags]: 
A neural language model tries to predict a conditional probability $P (w_{i + 1} | w_1, \dots, w_i)$ . It approximates the probability with the following $P(w_{i+1} | s(w_1, \dots, w_i))$ , where $s$ is a state function. After that an LSTM looked at all the words $w_1, \dots, w_i$ , it has an updated state, so now it contains some useful information about all previous words. You've got an error in your code: you should take all words of a sentence, but the last. But you've taken all, but the last sentence. In language modeling a normal sentence $w_i, \dots w_n$ is usually augmented with 2 special tokens: -- begin of sequence, -- end of sequence. So your example "Hello my name is" should transform into " Hello my name is ". Now your source tokens are all except the last i.e. " Hello my name is" and the targets you want to predict are all expect the first i.e. "Hello my name is ". You feed tokens in your LSTM one at a time and try to predict the next token.
