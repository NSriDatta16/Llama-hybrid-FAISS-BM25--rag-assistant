[site]: crossvalidated
[post_id]: 591119
[parent_id]: 255301
[tags]: 
Your Claim: The VC Dimension of a SVM $f_K$ equipped with kernel $K$ can be defined as $d^{any}_K+1$ . Now, Just consider $K=(x.y)^2$ with transformation $\phi(x) = \begin{pmatrix} x_1^2 \\ \sqrt{2} x_1x_2 \\ x_2^2 \end{pmatrix} $ . Just check that, $\nexists x$ $(=(x_1, x_2))$ s.t $$ \phi(x) = \begin{pmatrix} x_1^2 \\ \sqrt{2} x_1x_2 \\ x_2^2 \end{pmatrix} = \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix} $$ So this alone makes the first case of proof The VC-dimension of the set of linear classifiers is d invalid Essentially, higher-dimensional space $\mathcal{H}$ generated through any mapping $\phi$ is a "restrictive" space, in the sense that not all points exists in that space. There needs to be a separate proof for VC-dimension in these "restricted" spaces and we may need to proof it for different kernels. (I hope you got my point, above $K$ and $\phi$ are just examples to get this point across)
