[site]: crossvalidated
[post_id]: 316909
[parent_id]: 
[tags]: 
How do I get a measure of accuracy when dealing with measurements across time for different proccesses without shared timestamps

I have a number of measurements of values across time, each of these measurements belong to a process which either remains constant or goes down discontinuously (step down), never up. Now I have 4 such processes and 25 measurements of each of them, each process produced between 300 and 15000 measurements and I'm interested in which process is best at any given point in time and what the chance is and some measurement of confidence that this actually holds true. Anybody know how to go about this? For example I might have one process produce the following 2 measurements t(s) 0 1 4 9 v(p) 10 9 9 2 t(s) 0 1 3 5 v(p) 5 2 1 0 which clearly shows quite a large variance within the same process verses say a second process like this: t(s) 0 3 4 6 v(p) 20 14 12 8 t(s) 0 1 3 5 v(p) 20 18 14 10 clearly any prediction based on the second series would be more likely be more accurate. Now I know that if I had time measurements for any moment in time I could easily calculate variances and get p-values but in this case I do not see how I can show that averaging the second across time will produce a more accurate result. In other words I have series of measurements of two processes and want to know if the null hypothesis that process 1 > process 2 holds true. However I only know the value of either process at somewhat random moments(whenever the system took a measurement). Here is a picture of the averages of all 4 processes and here is a picture of a single run I hope this makes sense and if not please leave a comment so I can explain better.
