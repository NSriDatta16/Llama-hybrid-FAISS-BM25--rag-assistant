[site]: crossvalidated
[post_id]: 580436
[parent_id]: 
[tags]: 
Poor reconstructions from using sigmoid in last layer of variational autoencoder

I have trained a variational autoencoder (VAE) using Pytorch Lightning to reproduce images. Without sigmoid , reproductions are good. However, some output image values are negative so need to be clipped. With sigmoid the reproductions are poor. Why is the sigmoid causing poor results? Using sigmoid to squash output into [0-1] seems a common approach but I just can't seem to get it working. Input Inputs to the pipeline are RGB image tensors with values in the range [0-1]. Output Outputs from the decoder are RGB image tensors (visualised below) with values in the range [-1 to 1] without sigmoid and [0 to 1] with sigmoid. self.kl_coefficient = 100 self.latent_dim = 8 self.input_image_height = 64 self.input_image_channels = 3 self.leaky_relu_negative_slope = 0.01 # Encoder / Decoder architecture self.encoder = nn.Sequential( nn.Conv2d(in_channels=self.input_image_channels, out_channels=8, kernel_size=4, stride=2), nn.LeakyReLU(negative_slope=self.leaky_relu_negative_slope), nn.Conv2d(in_channels=8, out_channels=16, kernel_size=4, stride=2), nn.LeakyReLU(negative_slope=self.leaky_relu_negative_slope), nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2), nn.LeakyReLU(negative_slope=self.leaky_relu_negative_slope), nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2), nn.LeakyReLU(negative_slope=self.leaky_relu_negative_slope), Flatten() ) self.decoder = nn.Sequential( nn.Linear(in_features=self.latent_dim, out_features=64), nn.LeakyReLU(negative_slope=self.leaky_relu_negative_slope), Unflatten(image_height=self.input_image_height), nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=5, stride=2), nn.LeakyReLU(negative_slope=self.leaky_relu_negative_slope), nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=5, stride=2), nn.LeakyReLU(negative_slope=self.leaky_relu_negative_slope), nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=6, stride=2), nn.LeakyReLU(negative_slope=self.leaky_relu_negative_slope), nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=6, stride=2), nn.LeakyReLU(negative_slope=self.leaky_relu_negative_slope), nn.Conv2d(in_channels=16, out_channels=3, kernel_size=1, stride=1), nn.Sigmoid() # Reproductions poor with sigmoid...but without the sigmoid, some of the pixel outputs go negative ) # Fully connected layers self.fc_pose_mu = nn.Linear(256, self.latent_dim) self.fc_pose_log_var = nn.Linear(256, self.latent_dim) def step(self, batch, batch_idx): reconstructed_images, q, p = self._run_step(batch) # 1. Reconstruction Loss (Mean Squared Error) reconstruction_loss = F.mse_loss(batch, reconstructed_images, reduction="sum") # 2. Training Stability Loss - latent distribution VS standard Gaussian (KL divergence) q_vs_standard_gaussian = torch.distributions.kl_divergence(q, p) training_stability_loss = q_vs_standard_gaussian.mean() training_stability_loss *= self.kl_coefficient # TOTAL loss return reconstruction_loss + training_stability_loss Results Top row are inputs, bottom left is without sigmoid and bottom right is with sigmoid (most pixels #000 or very close). Reconstruction loss when including the sigmoid , appears to converge to minimum: UPDATE #1 Batch norm (between the last Conv2D layer and the sigmoid layer) has helped the reconstructions. nn.BatchNorm2d(3) Although two further issues: There are some consistent artefacts (see image) in the reconstructions Background should be #000 black, but instead remains grey even after the object appears to be reconstructed well. Without batchnorm + sigmoid the background mostly goes to #000.
