[site]: crossvalidated
[post_id]: 340034
[parent_id]: 340027
[tags]: 
Normalizing batches do exist, and are commonplace in neural networks. Please see https://arxiv.org/pdf/1502.03167.pdf . As far as normalizing filters, the way it is defined in the link, it is actually a relaxation of regularization. Note that $\min_{||x||=1}f(x) \geq min_{||x||\leq1} f(x)$, where $f()$ i some notion of loss and $x$ is your weight vector The Lagrangian of the latter formulation is $f(x) + \lambda ||x-1||$ which is what we see as a varaint of regularization. The hyperparameter "tuning" is basically finding the dual variables by trial and error
