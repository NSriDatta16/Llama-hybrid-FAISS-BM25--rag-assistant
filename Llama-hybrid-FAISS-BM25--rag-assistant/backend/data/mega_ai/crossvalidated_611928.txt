[site]: crossvalidated
[post_id]: 611928
[parent_id]: 
[tags]: 
Are dropout layers equivalent to adding noise to training samples?

I'm doing reading around regularisation techniques for neural networks. My intuition is that dropout is essentially adding noise into the network by zeroing out activations according to a given probability. Can the same result be achieved by introducing noise into training examples/layers given the same probability?
