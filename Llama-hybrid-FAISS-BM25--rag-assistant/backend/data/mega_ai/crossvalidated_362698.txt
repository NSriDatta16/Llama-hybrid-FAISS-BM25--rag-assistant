[site]: crossvalidated
[post_id]: 362698
[parent_id]: 362194
[tags]: 
With a logistic regression, where you only put age and parch into the model as linear predictors, you will have the predicted probability of survival monotonically increasing or decreasing (or, of course, constant) in each variable. In fact, it would act linearly on the logit of the survival probability. Your assumption is that for passenger $i=1,2,\ldots$ you have $$\log \pi_i - \log(1-\pi_i) = \beta_0 + \beta_1 \times \text{age} + \beta_2 \times \text{parch},$$ where $\pi_i = P(\text{passenger } i \text{ survives})$. So, it would not be able to reflect it, if survival probability keeps going up the more age with increasing age, until you become elderly and a bit frail. It will just try to do a linear line on the logit scale. Within the logistic regression framework you can become more flexible in this respect by using regression splines to allow for non-linear relationships (and interactions with splines to address some of the other issues mentioned below). Even if it is a linear line on that scale, unless you are a parent, it would not be able to reflect that. A change in the age slope e.g. based on parch would be possible with an age by parch interaction, but again, if your belief is that the effect of parch changes exactly at age 18 years, then you probably actually want and interaction with the indicator for whether a passengers age is $\geq 18$, otherwise you try to approximate that step-function by a linear line (probably not the best idea). Picking/constructing variables/predictors that you believe will work well, or that you want to test out is "feature engineering". Algorithms like logistic regression take what you give them and the assumptions you (implicitly make such as linear relationships on some scale) and then do the best (with respect to the loss function) they can with that. If you did build really clever features based on your deep insights into shipping disasters, logistic regression may perform fantastically, if you give it terrible features, it will do poorly. If you want a model that figures this out on its own (at least given enough data), then you may want something like extreme gradient boosting ( xgboost ), random forest or deep neural network ("deep learning"). These can to some extent construct their own features/figure out what interactions occur may matter. However, they need a lot of data to do so. So, if you give them good features, they will need a lot less training data to achieve the same performance as you would get with poorly designed input features. Given an near infinite amount of data, your feature engineering will often be irrelevant, for small datasets like the Titanic one it will still matter a lot.
