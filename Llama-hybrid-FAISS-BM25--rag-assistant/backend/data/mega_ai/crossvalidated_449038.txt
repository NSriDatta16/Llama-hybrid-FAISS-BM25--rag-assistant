[site]: crossvalidated
[post_id]: 449038
[parent_id]: 
[tags]: 
Binary Classification Propensity Scoring: High Accuracy in Train/Validation/Test, but Low Accuracy on Production Data

Before I describe my problem, if anyone has material for propensity scoring I would love it if you posted some. I've done a lot of research on propensity, but I think I've bled myself dry on the net. If you guys have any hidden gems I would love for you to send them over. If you guys see anything wrong with how I'm going about predicting the propensity for the model, let me know. Task : The goal is to predict the probability that person A is a good option to be enrolled in a service/product they are currently not enrolled in. Approach : After a lot of research, I decided to create a XGBoost binary classification model and use predict_proba to determine propensity scores. To determine how well my model looks during training and testing, I'm using a confusion matrix and the ROC method. I run CV during training/testing. Feature Selection : I'm using a library I found online called Feature Selector, which prunes through input features for single value columns, high correlation, etc. Some Confusion : Since the goal here is to recommend people who are not currently enrolled in a service, wouldn't I almost be looking for the model to guess incorrectly? To clarify, I want the model to predict true for someone who is false, and grab the probability of how certain the model is about that suggestion. This could also mean the confusion matrix would look very skewed. Unbalanced Data : I'm working on a binary classification model that is relatively imbalanced. The data set is about 80,000 class 0, and 20,000 class 1. I chose to limit my training and test to 80% of the size of class 1 because it is the limiting feature, and down sample class 0 to match it for a balanced model. I ended up with a randomly sampled train/validation/test set of 16,000 class 1 and 16,000 class 0 (for a total of 32,000 in my train/validation/test set). The accuracy is around 70% in train/validation, and similar in test. I don't see any place where my test could have bled into the training, so I think I'm good on that end. However, when I try to apply my model to the rest of the data (remaining 68,000), the confusion matrix is suddenly very bad. I have some thoughts about why this would happen. It could just be that there is so much more class 0 than class 1 that the model could not have generalized well. My Thoughts : It could also be possible that the model is truly recommending people who are not currently enrolled to be enrolled. It would make sense then that accuracy would drop since the goal of this project is to find people who are not enrolled, and suggest them for enrollment. How would I best interpret my results? Any ideas what I can do here?
