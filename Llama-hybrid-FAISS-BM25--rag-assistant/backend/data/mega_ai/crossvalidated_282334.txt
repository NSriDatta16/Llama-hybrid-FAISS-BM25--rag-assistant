[site]: crossvalidated
[post_id]: 282334
[parent_id]: 279450
[tags]: 
I find the following a convenient way to think about SVM: solve for $y_i(\langle w, x_i \rangle + b) \geq K - \xi_i$ where $2K$ is our functional margin, and $\xi_i$ is the amount by which we haven't achieved that functional margin. The geometric margin will be $\frac{2K}{||w||}$. There can be additional error terms multiplied by some weights. We then have several different approaches to solving from an optimization perspective: Maximize the functional margin (or solve for $\min -K$ plus error terms). But, then we could keep multiply through $w$ and $K$ by some $\beta > 1$, and the objective would improve without changing our hyperplane direction. So we'd need to require that $||w||^2 \leq 1$ for example, to prevent run-away growth (i.e., objective of minus infinity). And then at the solution we know that we'll get $||w||^2 = 1$. Similar to step 1, but rather than constrain $||w||^2 \leq 1$ add on some regularization term to $||w||^2$ to the objective, where we now need a new constant to weigh the $K$ term relative to it: $\min \frac{||w||^2}{2} - \nu K$ plus error terms. This is the approach used by $\nu$-SVM. Fix $K$ to a constant, e.g. $K=1$. Then the geometric margin is maximized by minimizing $||w||$ or equivalently $\frac{||w||^2}{2}$. This is the approach taken by $C$-SVM.
