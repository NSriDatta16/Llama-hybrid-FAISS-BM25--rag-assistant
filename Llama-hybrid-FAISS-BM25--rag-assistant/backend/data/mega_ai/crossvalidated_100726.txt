[site]: crossvalidated
[post_id]: 100726
[parent_id]: 100126
[tags]: 
I'm going to assume that "KNN" means $k$-nearest neighbors employed as a 2-class classification algorithm wherein labelled data are points placed in some $n$-dimensional space, and the classes of new, un-labelled data are assigned to a label by a vote among the $k$ nearest points to the un-lablled point. Does KNN algorithm have a centroid as k-means? No. Each non-labeled datapoint is understood to be the "middle" of its $k$-nearest neighbors, but that isn't the same as being the centroid. On the other hand, any set of points has a centroid, but that answer is trivial. Is there a way to obtain the centroid for the classified data by KNN? I'm not sure what you mean. You could certainly treat all of your classes as their own "clouds" and compute the centroids of the clouds, but I'm not sure that necessarily makes the problem easier to understand. Suppose you have data in three dimensions. One class is uniformly distributed on the surface of a sphere with radius 2, centered at the origin. The second class is uniformly distributed on the surface of a sphere with radius 1, also centered at the origin. Your sample will both have very similar centroids, while in reality they both have centroids at the origin. Is there a way to compare SVM classification with KNN classification? Sure. I believe any of the usual metrics are valid: false positive rate, false negative rate, recall, precision, $F$-measure, etc.
