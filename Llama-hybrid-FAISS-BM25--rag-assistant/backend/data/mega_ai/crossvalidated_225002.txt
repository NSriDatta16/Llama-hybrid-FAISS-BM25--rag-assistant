[site]: crossvalidated
[post_id]: 225002
[parent_id]: 
[tags]: 
Are we frequentists really just implicit/unwitting Bayesians?

For a given inference problem, we know that a Bayesian approach usually differ in both form and results from a fequentist approach. Frequentists (usually includes me) often point out that their methods don't require a prior and hence are more "data driven" than "judgement driven". Of course, Bayesian's can point to non-informative priors, or, being pragmatic, just use a really diffuse prior. My concern, especially after feeling a hint of smugness at my fequentist objectivity, is that perhaps my purportedly "objective" methods can be formulated in a Bayesian framework, albeit with some unusual prior and data model. In that case, am I just blissfully ignorant of the preposterous prior and model my frequentist method implies ? If a Bayesian pointed out such a formulation, I think my first reaction would be to say "Well, that's nice that you can do that, but that's not how I think about the problem!". However, who cares how I think about it, or how I formulate it. If my procedure is statistically/mathematically equivalent to some Bayesian model, then I am implicitly ( unwittingly !) performing Bayesian inference. Actual Question Below This realization substantially undermined any temptation to be smug. However, I'm not sure if its true that the Bayesian paradigm can accommodate all frequentist procedures (again, provided the Bayesian chooses a suitable prior and likelihood) . I know the converse is false. I ask this because I recently posted a question about conditional inference, which led me to the following paper: here (see 3.9.5,3.9.6) They point out Basu's well-known result that there can be more than one ancillary statistic, begging the question as to which "relevant subset" is most relevant. Even worse, they show two examples of where, even if you have a unique ancillary statistic, it does not eliminate the presence of other relevant subsets. They go on to conclude that only Bayesian methods (or methods equivalent to them) can avoid this problem, allowing unproblematic conditional inference. It may not be the case that Bayesian Stats $\supset$ Fequentist Stats -- that's my question to this group here. But it does appear that a fundamental choice between the two paradigms lies less in philosophy than in goals: do you need high conditional accuracy or low unconditional error: High conditional accuracy seems applicable when we have to analyze a singular instance -- we want to be right for THIS particular inference, despite the fact that this method may not be appropriate or accurate for the next dataset (hyper-conditionality/specialization). Low unconditional error is appropriate when if we are willing make conditionally incorrect inferences in some cases, so long as our long run error is minimized or controlled. Honestly, after writing this, I'm not sure why I would want this unless I were strapped for time and couldn't do a Bayesian analysis...hmmm. I tend to favor likelihood-based fequentist inference, since I get some (asymptotic/approximate) conditionality from the likelihood function, but don't need to fiddle with a prior - however, I've become increasingly comfortable with Bayesian inference, especially if I see the prior a a regularization term for small sample inference. Sorry for the aside. Any help for my main problem is appreciated.
