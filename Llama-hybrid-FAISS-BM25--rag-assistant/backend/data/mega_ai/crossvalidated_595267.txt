[site]: crossvalidated
[post_id]: 595267
[parent_id]: 519422
[tags]: 
Although the OP clearly refers to the Bayesian framework, it may be worth mentioning that the expression marginal likelihood has been used for long outside of the Bayesian framework, with a different meaning. This frequentist concept is described in several famous books, especially in those by D.R. Cox and coauthors. A modern presentation is to be found in the book Statistical Models by A.C. Davison (Chap. 12) which inspired this answer. The book Principles of Statistical Inference by D.R. Cox discusses both the frequentist and the Bayesian concepts. Consider a random vector $\mathbf{Y}$ of observations with distribution depending on a vector of parameters $\boldsymbol{\theta}$ . If $\mathbf{Y}$ splits into two sub-vectors, say $\mathbf{V}$ and $\mathbf{W}$ , the likelihood is $$ L(\boldsymbol{\theta};\,\mathbf{y}) = f_{\mathbf{Y}}(\mathbf{y};\,\boldsymbol{\theta}) = f_{\mathbf{V}}(\mathbf{v}; \boldsymbol{\theta}) \, f_{\mathbf{W} \vert \mathbf{V}}(\mathbf{w} \, \vert\, \mathbf{v}; \, \boldsymbol{\theta}). \tag{1} $$ If instead the couple $[\mathbf{V}, \mathbf{W}]$ is a sufficient statistic for $\boldsymbol{\theta}$ , the same form holds, up to a multiplicative constant (w.r.t. $\boldsymbol{\theta}$ ). It may happen that the information on some specific component of interest in $\boldsymbol{\theta}$ is mainly conveyed by one of the two factors of the product on which we can therefore focus. As an example consider $\mathbf{Y} \sim_{\text{i.i.d.}} \text{Norm}(\mu,\, \sigma^2)$ , so that $\boldsymbol{\theta} := [\mu,\,\sigma^2]$ , and assume that we are interested in the inference on $\sigma^2$ only. It can be checked that $$ f_{\mathbf{Y}}(\mathbf{y}; \, \sigma^2,\,\mu) = f_{S^2} (s^2; \, \sigma^2) \, f_{\bar{Y}} (\bar{y}; \, \sigma^2,\, \mu) \, f_{\mathbf{Y}\vert \bar{Y},S^2}(\mathbf{y} \, \vert \, \bar{y}, \, s^2). $$ At the right-hand side the third term does not depend on the parameter: in other words, the sample mean $\bar{Y}$ and the sample variance $S^2$ are jointly sufficient statistics for $\boldsymbol{\theta}$ . It can be anticipated that the information on $\sigma^2$ in the likelihood is conveyed by the first term since the sample mean $\bar{Y}$ does not tell much about $\sigma^2$ . So the inference on $\sigma^2$ can be based on $f_{S^2} (s^2; \, \sigma^2)$ called the/a marginal likelihood for this specific case. More generally, the form (1) can have a special importance when the parameter vector $\boldsymbol{\theta} = [\boldsymbol{\psi},\, \boldsymbol{\lambda}]$ splits into two parts: a parameter of interest $\boldsymbol{\psi}$ and a nuisance parameter $\boldsymbol{\lambda}$ . We can call marginal likelihood a function which only depends on $\boldsymbol{\psi}$ and on the observations available, and which extracts most of the information on $\boldsymbol{\psi}$ that can be retrieved from the observations. If the joint density can be factored as $$ f_\mathbf{Y}(\mathbf{y}; \boldsymbol{\psi},\, \boldsymbol{\lambda}) = f_{\mathbf{V}}(\mathbf{v}; \boldsymbol{\psi}) \, f_{\mathbf{W} \vert \mathbf{V}}(\mathbf{w} \, \vert\, \mathbf{v}; \, \boldsymbol{\psi}, \,\boldsymbol{\lambda}), \tag{M} $$ then we can ignore the second term at the r.h.s. and use the following marginal likelihood (for $\boldsymbol{\psi}$ ) $L_{\text{M}}(\boldsymbol{\psi};\, \mathbf{y}) := f_{\mathbf{V}}(\mathbf{v}; \boldsymbol{\psi})$ . A different form of factorisation that can be used is $$ f_\mathbf{Y}(\mathbf{y}; \boldsymbol{\psi},\, \boldsymbol{\lambda}) = f_{\mathbf{V}}(\mathbf{v}; \boldsymbol{\psi},\, \boldsymbol{\lambda}) \, f_{\mathbf{W} \vert \mathbf{V}}(\mathbf{w} \, \vert \, \mathbf{v}; \, \boldsymbol{\psi}). \tag{C} $$ In this case we may base the inference on $\boldsymbol{\psi}$ on the so-called the conditional likelihood $L_{\text{C}}(\boldsymbol{\psi};\, \mathbf{y}) := f_{\mathbf{W} \vert \mathbf{V}}(\mathbf{w} \, \vert \, \mathbf{v}; \, \boldsymbol{\psi})$ . So, while in (M) the parameter of interest is isolated in the marginal part of the factorisation, in (C) it is isolated in the conditional part. As a typical example is provided by regression where the inference is usually conditional on the vector of covariates. So in its usual frequentist meaning, a marginal likelihood is a function of the parameter of interest $\psi$ that can be used as a likelihood to infer on $\psi$ disregarding the nuisance parameter. A striking point is that in some cases the inference based on the marginal likelihood is better than that based on the profile likelihood, which is mainly asymptotic. This is the case in the example above, which generalises to linear regression.
