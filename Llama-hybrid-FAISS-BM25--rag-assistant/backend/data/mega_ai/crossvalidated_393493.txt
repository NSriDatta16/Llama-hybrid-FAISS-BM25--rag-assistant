[site]: crossvalidated
[post_id]: 393493
[parent_id]: 393471
[tags]: 
The mean (or average) squared error is indeed defined as (1). Or (2). Indeed, the setting (and the meaning of the notations differ: In (1), the expectation is against the distribution of the random variable $X_i$ and $\hat\lambda_i=\hat\lambda_i(x_i)$ In (2), the expectation is against the distribution of the random vector $(X_1,\ldots,X_i,\ldots,X_M)$ and $\hat\lambda=\hat\lambda_i(x_1,\ldots,x_M)$ Maybe the confusion stems from perceiving $\hat\lambda$ as a vector made of the $\hat\lambda_i$ 's, which is not the case since they all estimate the same quantity and thus evolve in the same set $\Lambda$ . ...my final estimator is a constant: it is computed after the measurements... Both estimators $\hat\lambda_i$ and $\hat\lambda$ are "constant" once the sample is observed. But as non-constant functions of the random variable $X_i$ and of the sample $(X_1,\ldots,X_i,\ldots,X_M)$ they are random variables.
