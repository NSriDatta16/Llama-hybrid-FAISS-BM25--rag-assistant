[site]: crossvalidated
[post_id]: 263343
[parent_id]: 256908
[tags]: 
The two results are not guaranteed to be similar because the two approaches maximize different objective functions. However, they are related and the key difference is in the priors: In DPM (Bayesian approach), you infer cluster parameters ($\theta := \{\pi,\mu_{1:K},\Sigma_{1:K}\}$) by performing MCMC sampling of $\theta$ according to the posterior distribution $\theta \sim p(\theta \mid X)=p(X \mid \theta)p(\theta)$. The second approach is a maximum likelihood approach. That is, EM iterations output cluster parameters that maximize the likelihood objective function $\hat \theta = \arg\max_{\theta} p(X\mid\theta)$. Comparing the two methods, the main difference is that in DPM, you let the priors influence your cluster parameters (i.e., DP for $\pi$, Gaussian for $\mu$, Wishart for $\Sigma$), whereas in the EM approach there is no such prior terms. The prior can directly affect the resulting $K$. The most obvious example is that the hyperparameter, $\alpha$, that you set for DP prior directly controls $K$ (the larger $\alpha$, the bigger output $K$). Keep in mind that, since this is an unsupervised problem, there is no single "correct" $K$ (unless your data have very strong cluster separation and cohesion that the likelihood term simply dominates over the prior term, which can be far from true in real data.). Therefore, in practical, the two results can be quite different, depending on the prior hyperparameters $\alpha$ and $G_0$ that you provide.
