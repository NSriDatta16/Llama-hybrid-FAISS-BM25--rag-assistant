[site]: crossvalidated
[post_id]: 604635
[parent_id]: 604624
[tags]: 
Marginalization for Joint Distributions This video intuitively approaches the idea of marginalization for a joint distribution. It states that marginalization is when you remove features that you don't care about by aggregating their probability into the remaining features that you do care about. In the video, this is demonstrated with two features, which combine to create a joint distribution . That is, the probability of each feature combination is computed, and the probability of all the feature combinations must sum to 1. This is not the case for a model that uses a set of features to predict some output. But the idea is similar. Marginalization for a model From the English definition of the word: Marginalization: treatment of a person, group, or concept as insignificant or peripheral. Intuitively, marginalizing a model means treating some of its constituent features as insignificant or peripheral. Because you consider them insignificant, you average out their effect on the model and assign the average to the remaining feature(s). For example, say you have a two feature model that predicts the probability of an ice cream van selling over 10 ice creams in a given day. Your features are Temperature (Hot, Cold) , and Holidays (Yes, No) . This produces the following table of feature combinations with their associated probabilities. $0$ means the model predicts you definitely don't sell over 10 ice creams, and $1$ means the model predicts you definitely do. \begin{array}{l|l|l} & \text{Temperature: Hot} & \text{Temperature: Cold} \\ \hline \text{Holidays: Yes} & 0.98 & 0.65 \\ \hline \text{Holidays: No} & 0.78 & 0.1 \end{array} To interpret this table, take a look at the top left cell. This cells says that the model predicts there is a 98% chance that the ice cream van will sell over 10 ice creams a day when it is hot and the holidays. To explain marginalization of a model , let's marginalize Holidays out of the table. To do this, we look down each column in the table and take the average of its rows. What's left is a table of all the columns with a single row representing the model's average prediction probability for that particular column value. \begin{array}{l|l} \text{Temperature: Hot} & \text{Temperature: Cold} \\ \hline 0.815 & 0.375 \end{array} For illustration purposes, this model is simple and only has two values for the feature represented in the column. But, in general, there could be hundreds or thousands of values for this feature. The values can then be plotted along the x axis of a graph, and the probability along the y axis to give you the Partial Dependence Plot. Aside According to wikipedia , the term "marginalization", comes from summing up the rows of a column, or columns of a row, and putting the summation in the table's margin. The variable represented by the rows or columns that were summed across is then said to have been "marginalized out".
