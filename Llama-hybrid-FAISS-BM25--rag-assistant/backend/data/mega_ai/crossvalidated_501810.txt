[site]: crossvalidated
[post_id]: 501810
[parent_id]: 501570
[tags]: 
(B) should find the set of parameters a,b, c that minimizes the overall sum of squared errors (i.e. the overall squared error across all "time points" 1,2,..., n ). You don't have the same guarantee if you estimate the parameter separately and then average as you suggest in (A). Imho (A) instead seems justified only if there's reasons to expect that the "true" parameters take different value at each point 1,2,..., n . However in this case you wouldn't want to average them. By the way, if your measurement y is a percentage, bounded within 0% and 100%, least squares might not be the best estimation method. There are alternative approaches designed to work with bounded variables, for example have a look at beta-regression .
