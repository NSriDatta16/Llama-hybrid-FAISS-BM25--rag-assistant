[site]: crossvalidated
[post_id]: 97729
[parent_id]: 97725
[tags]: 
Once a linear regresion model is fitted, the output is simply a linear combination of the inputs (the coefficients are the fitted parameters of the regression model, $\hat{\beta}$.) In particular, the model prediction is $y=\hat{\beta}^{T}x$, or if a binary output is desired, we can apply a threshhold: $y=\left\{ \begin{array}{ll} 1 & \hat{\beta}x \geq c \\ 0 & \hat{\beta}x In logtistic regression, the prediction model is only slightly more complicated. The predicted probability is: $y=\mbox{logit}^{-1}(\hat{\beta}^{T}x)$ In many cases we make a binary yes or no prediction by applying a threshhold to the above formula. Since $\mbox{logit}^{-1}$ is monotone, this is just a threshhold on $\hat{\beta}x$: $y=\left\{ \begin{array}{ll} 1 & \hat{\beta}x \geq c \\ 0 & \hat{\beta}x A classical perceptron node computes $\hat{\beta}x$ and applies a threshhold, so a single perceptron node can be used to implement either a linear regression or logistic regression classifier.
