[site]: crossvalidated
[post_id]: 475622
[parent_id]: 475611
[tags]: 
In Logistic Regression, collinearity inflates the uncertainty in the learned estimates. Here is an example. I'll simulate 1000 datasets from the same phenomenon and fit a model. Then, I will plot the estimates of that model under the assumption that the features are independent and that they have correlation of 0.8 sim Note the scale in the subplots. With correlation, the estimates become more extreme. Note also that the estimates are negatively correlated; if one is large, the other is necessarily small. That is because they are fighting with one another to have an impact on the outcome. For decision trees, the correlation is likely to have a very large impact on predictions but only because the estimator is very high variance. Small changes in the data can lead to large changes in the predictions. Random forests are an attempt to correct for this, and correlation in the parameters is less of a problem. Because random forests allow the user to a) bootstrap the data to build a model and b) randomly select a subset of predictors to create the split, the correlation between features (and subsequently between trees in the forest) is combated. It would still be difficult to determine feature importance by say looking at split frequency or loss reduction when split on the feature. My intuition says that since the features are correlated, they would have similar feature importance, but this requires empirical validation.
