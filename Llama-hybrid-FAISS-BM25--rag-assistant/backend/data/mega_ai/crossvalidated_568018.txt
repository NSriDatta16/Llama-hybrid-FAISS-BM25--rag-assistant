[site]: crossvalidated
[post_id]: 568018
[parent_id]: 
[tags]: 
Is it wrong to run a Random Forest on high-dimensional, sparse, and unbalanced data?

I am learning about random forests, and I have been testing using R. I have doubts about whether I am doing something wrong given that my data are: sparse, high-dimensional, and unbalanced. Trying to understand the algorithm, I have been experimenting with a dataset of TF-IDF. The data frame is of dimension (1143, 2016). Each row represents a distinct document, and there are more than 2000 variables with the TF-IDF of the stems present in the corpus (the tf-idf tend to be sparse). I have a label L that says whether the document was written by a specific person. In summary, the data frame df looks like this: L tfidf_word1 .... tfidf_word2015 TRUE $x_{1,1}$ .... $x_{1,2015}$ FALSE $x_{2,1}$ .... $x_{2,2015}$ ... ... .... ... FALSE $x_{1143,1}$ .... $x_{1143,2015}$ These data are highly unbalanced, with the share of L that are TRUE around 5%. Therefore, I have constructed a weight variable as follows weight $L , $ \frac{1}{0.05} $, $ \frac{1}{0.95}$) I have ran the following random following random forest: library(ranger) rf And these are the results: Type: Classification Number of trees: 500 Sample size: 1143 Number of independent variables: 2016 Mtry: 44 Target node size: 1 Variable importance mode: impurity Splitrule: gini OOB prediction error: 0.00 % I am very worried about the OOB prediction error of 0%. Is there a formal reason to think that for data so sparse, unbalanced, or of high dimension there may be some sort of overfitting going on?
