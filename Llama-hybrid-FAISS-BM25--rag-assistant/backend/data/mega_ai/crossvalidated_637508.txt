[site]: crossvalidated
[post_id]: 637508
[parent_id]: 637505
[tags]: 
Here is my partial answer thus far. This kind of uses information-theoretic/Bayesian reasoning but I think you could probably reformulate it in frequentist terms. One can think about, for instance, how much information one gets from each query, on average. This is the entropy. So, it would make sense to try to choose queries that maximize the entropy. If the true proportion of glitch emojis is $\theta$ , then the entropy from querying only one is the binary entropy function $H_b(\theta) = -\theta \log \theta - (1-\theta) \log (1-\theta)$ . Now, suppose we query $N$ emojis at once. Well, we have another Bernoulli random variable, which will be $1$ (or "glitch") iff all of them are. So, the success probability is $\theta^N$ , and the entropy is simply $H_b(\theta^N)$ . Graphing all of these makes clear that the ideal batch size depends on $\theta$ : So, in general, as $\theta$ increases, it becomes more beneficial to do larger batches. For instance, if $\theta = 0.5$ , you have a 50/50 chance of success, and get one bit of entropy from querying a single emoji. But if $\theta = ~0.707$ , you now have a 50/50 chance of success from querying two emojis instead. And in general, whenever $\theta = 0.5^{1/N}$ , a query of $N$ emojis has the same entropy as just querying one if it's $0.5$ . So, at least one partial starting point is: Try to estimate $\theta$ from a small random sample (what batch size to use for this is another interesting question). Given that, choose the batch size accordingly to maximize the entropy. This is the value such that the success of the batch is as close to a 50/50 chance of success as possible. The initial query, at least, should be with that batch size. In particular, from this we can at least see that if $\theta$ is less than about $0.62$ , the entropy will be maximized for N=1 queries. So, if your estimate gives you a reasonable amount of certainty that this is true, it would seem that the optimal strategy really is just doing one emoji at a time. If you do have $\hat \theta > 0.62$ , then things seem a bit more interesting. Your first query should then be of the ideal batch size. But from there, I'm not quite sure what is best. Whenever you get a result of "true", things are easy - you have a simpler version of the original problem with a smaller search space. But for any "false" result, you now have a complex joint dependency between the emojis. What is the best way to deal with that? In principle, the answer seems to be to keep updating at each step and looking for whichever query maximizes the entropy, given everything you know so far. But, I'm not sure if there's a "fast" way to compute this for all $2^N$ possible queries. So one question is if there is a fast way, or if not, if there's some heuristic that's good enough. This is my partial solution. And, for what it's worth, though I've kind of used Bayesian terms above, I don't think that I've actually used any prior probability anywhere. So, in principle it seems one ought to be able to reformulate everything in a frequentist way, if one wants.
