[site]: crossvalidated
[post_id]: 512403
[parent_id]: 512399
[tags]: 
Neural network is basically a tensor function (tensors = scalars, vectors, matrices etc.). It takes stuff in (e.g. vector of some features), compoundly apply some operations like matrix multipication or activation functions, and produces stuff out (e.g. vector of probability distribution over some classes). The term neuron basically refers to a particular element of a tensor. Having 1 neuron in a layer = the layer would consist of a scalar value (or equivalently vector of size 1, matrix of size 1x1 etc.). Usually, it's good idea to have more dimensional tensors in a network. Example: You have network that takes an input of 10-dimensional vector, apply matrix multiplication (the matrix is a learnable parameter) to get a vector of 64-dimension, then apply elementwise activation function (e.g. RELU), then apply e.g. softmax classification to get probability distribution over 7 classes. The hidden layer have 64 neurons, which means that the vector have a dimensionality of 64. If it would have only a dimensionality of 1, the capacity (complexity) of the network would be much lower.
