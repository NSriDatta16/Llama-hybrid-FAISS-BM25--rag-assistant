[site]: datascience
[post_id]: 17222
[parent_id]: 17216
[tags]: 
I haven't used sklearn but I'll try to answer the question to the best of my ability. Assuming you have an $n \times f$ matrix (data), now reduced to $n\times d$ (reduced data). Say the 1st reduced data point was mapped to some cluster, wouldn't that correspond to the first data point in the reduced matrix being classified to that same cluster? (Each row denotes a data point) Basically, the ordering will not change. Alternatively, if you want to look at it more mathematically, assume $W=(w1,w2...wd)$ form the basis vectors of the subspace you want to map your feature vectors to (the first d eigenvectors of your data covariance matrix). Now every data point can be represented as a linear combination of these basis vectors: $x = \alpha1w1 + \alpha2w2 ... + \alpha dwd$ Here the vector $\alpha = (\alpha1, \alpha2..., \alpha d)$ is the representation of your data point in d dimensions, i.e. in the subspace, and your original data point is: $x = W^T\alpha$ I'm not so clear about what you are asking in part 2). You choose the number of components you want to keep in PCA depending on the amount of error you are willing to tolerate. Suppose you want the reduced data points to correspond to 95% of the variance, you select the first d eigenvectors such that their eigenvalues correspond to the above percentage of variance. $\frac {\sum\limits_{i=1}^d \lambda_i}{\sum\limits_{i=1}^n \lambda_i} \times 100 = 95\% $ Hope that helped!
