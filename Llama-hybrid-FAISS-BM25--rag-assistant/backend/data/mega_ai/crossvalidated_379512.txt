[site]: crossvalidated
[post_id]: 379512
[parent_id]: 379262
[tags]: 
For some general discussion of using AIC to select among a set of models, see this page . Frank Harrell's answer there says: Generally speaking, AIC works best if used to select a unique single parameter (e.g., shrinkage coefficient) or to compare 2 or 3 candidate models. In the specific case of logistic regression, however, comparing models based on different subsets of predictors as you propose can pose particular problems. Logistic regression has an inherent omitted-variable bias . If predictors that are related to class membership are not included in a model, the coefficients for other predictors will be biased. The answer shows in detail, for the related probit model, why this is so. In logistic regression, unlike linear regression, such bias can occur even if the omitted predictors are uncorrelated to the included predictors, and the bias will be toward lower absolute values of coefficients than their true values. Omitting predictors thus might make it more difficult for you to identify truly significant relations of predictors to class membership. In your case, with 13 predictors and 520 cases, you might be able simply to use a full model with all predictors and evaluate directly the significance of each predictor with respect to outcome. The rule of thumb is 10-20 cases of the least prevalent class per predictor evaluated, so if your smallest class has at least 25% prevalence, all predictors are continuous or binary, and you aren't including interactions, you should be OK. That approach should reduce or even eliminate further AIC-based comparisons. If for some reason you need a more parsimonious model that omits some of your 13 predictors, you could consider stepwise backward elimination from the full model or a penalized approach such as LASSO. Finally, you should be evaluating your entire model-building process by cross validation, not just the performance of your final selected model. See this answer for more details.
