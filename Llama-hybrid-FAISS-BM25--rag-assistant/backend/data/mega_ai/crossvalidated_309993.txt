[site]: crossvalidated
[post_id]: 309993
[parent_id]: 307592
[tags]: 
You may not be doing anything wrong, this happens sufficiently frequently that is covered in the Keras FAQ . The FAQ give at two common possibilities: Dropout (and other types of regularisation) are turned off during testing, or Training loss is calculated over the whole epoch while test loss is calculated only at the end. If neither of these explain your situation, there are some tips for debugging neural networks in this Github issue .
