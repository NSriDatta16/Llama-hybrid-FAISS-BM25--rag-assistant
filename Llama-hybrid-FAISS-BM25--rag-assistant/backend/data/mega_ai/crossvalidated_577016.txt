[site]: crossvalidated
[post_id]: 577016
[parent_id]: 
[tags]: 
Is there a probabilistic or bayesian interpretation of the kalman filter gain?

The Kalman filter makes sense to me as the repeated application of Bayes' theorem - if you correctly propagate the gaussian prior at each step and then update on new observation, you get a gaussian posterior, and now you can repeat the whole process. When I derive the mean and covariance of the posterior, I arrive at the correct update equations, and if I do some symbol shuffling I can factor out what's called the "optimal Kalman gain" matrix. The wikipedia article suggests that non-optimal gains are often used: It is common to discuss the filter's response in terms of the Kalman filter's gain. The Kalman-gain is the weight given to the measurements and current-state estimate, and can be "tuned" to achieve a particular performance. With a high-gain, the filter places more weight on the most recent measurements, and thus conforms to them more responsively. With a low gain, the filter conforms to the model predictions more closely. This raises the question - is there a bayesian interpretation of what an arbitrary gain matrix does? The filter isn't computing normal bayes updates anymore. There's no "update speed" parameter to Bayes' theorem as far as I know, so is this just probabilistically incoherent? Yet, the wikipedia article mentions that the posterior covariance can be computed for any gain, not just the optimal gain.
