[site]: crossvalidated
[post_id]: 490859
[parent_id]: 
[tags]: 
When transforming data, why use resampling to estimate transformed values?

I am working through a machine learning book and am using the caret package in R . There is a function, preProcess, that uses resampling (intended for use on a training set I think) to estimate transformed values (e.g., log-transformation). My questions are: Why would resampling be necessary? Why would you not just apply the transformation to your entire dataset? Why would you even need to estimate the transformed values? In my field, typically you would just apply whatever formula to your variables (e.g., calculate the log), and get newly-calculated variables that have undergone transformation. My hunch is that this has something to do with computational efficiency, but am curious what the rationale for this procedure is.
