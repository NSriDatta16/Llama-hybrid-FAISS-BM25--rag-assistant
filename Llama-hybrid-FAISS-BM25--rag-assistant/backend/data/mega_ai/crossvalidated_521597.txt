[site]: crossvalidated
[post_id]: 521597
[parent_id]: 514276
[tags]: 
Yes, your intuition is right, you definitely want to exploit known symmetries. You can try to do it with a clever model architecture, but one of the easiest ways to encode some symmetries in neural networks is to use them as data augmentations. I.e. if you have one record with one ordering of inputs, but a different ordering should give them same answers (just with re-ordered answers), then just mix this up for training. Either randomly, or feeding all the permutations you know the answer for into the network in each epoch. Does this lead to overfitting? No, as long as you regularize the NN appropriately (e.g. right amount of dropout and weight decay) with the extent chosen according to a proper cross-validation. "Proper" in this case means that you don't make the mistake of augmenting the data and then randomly splitting, but instead you split first and then augment (or in some other way make sure you do not accidentally put an original record and an augmented version in both the training and validation part of a CV-split). Using the same weights an initial layer or so for each pair of x s certainly sounds like something to try, even if you do the data augmentation as suggested above. Other ways to encode the symmetries via the model architecture is by using a transformer neural network. They are of course most famous for being used in language modeling where order matters, but people do in fact introduce positional encodings to ensure the model respects order, which is not what one would want here. However, they usually take categorical input, so this would not be a simple basic tweak in a standard situation. I guess you could also consider it a graph with 3 nodes that are all adjacent and use a graph neural network, but I don't have much intuition for those (so this is more speculative on my part). Other thoughts on architecture: Featuring all of fully connected (linear), BatchNorm, and drop-out layers seems to be relatively standard for neural networks for tabular data (see e.g. what the defaults of the fastai library are). So, perhaps an architecture that combines three separate inputs going through a layer or two with identical weigths, followed by some more linear-BN-drop-out layers could be an obvious try. Another architecture that has got some attention (pun intended) on Kaggle is TabNet that sort of tries to do boosting (just with neural networks). Regarding outputs summing to 1, if they additionally are all in 0 to 1, then applying a softmax to the outputs would seem like an obvious thing to do (you can also try not predicting one of them, but calculating the softmax with that one set to, say, 0). If they are anywhere in $(-\infty, \infty)$ , then only predicting 5 of the 6 and setting the 6th to $1-\sum_{i=1}^5 \hat{y}_i$ would seem to achieve this constraint. Another question is how to pre-process inputs for the network. I don't know much about what your inputs are, but standardizing them, or applying a rank-Gauss transformation has sometimes been useful for continuous data (while for categorical data, it's of course embeddings).
