[site]: crossvalidated
[post_id]: 600834
[parent_id]: 
[tags]: 
Linear positional encoding for Transformers with fixed sequence length

I am trying to use Transformers for a Time-series task (with fixed sequence length). Since it's sequence length is always the same, can I use a linear positional encoding? I read some articles about this, and in all of them, the reasoning behind using a sinosoidal is possibility of varying sequence length. Is this not the case? Are there any other reasoning for using sinosoidals instead of a simple liner encoding?
