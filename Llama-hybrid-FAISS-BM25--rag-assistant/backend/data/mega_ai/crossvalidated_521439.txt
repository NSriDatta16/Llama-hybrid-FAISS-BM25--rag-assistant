[site]: crossvalidated
[post_id]: 521439
[parent_id]: 
[tags]: 
Grid search yielding unexpected results

I am new to machine learning and having some trouble making sense of the results I am getting from grid search. I am working on a supervised binary classification problem with a small dataset (109 samples). I selected a few models to test and am using a cross-validated grid search to identify the hyperparameters that yield the greatest AUC. Wen I plot learning curves on the training data using the tuned models, I see that there is severe overfitting with a very good training score and much poorer cross validation score. The overfitting isn't too surprising given the small sample size. What I'm confused about is why the grid search didn't select parameters to reduce overfitting that were provided in the parameter grid. For example, in the case of MLP, the grid included a smaller hidden layer size and greater L2 penalty than what was selected. In the case of KNN, a relatively small K value was selected from the grid.
