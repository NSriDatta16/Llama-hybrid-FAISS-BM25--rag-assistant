[site]: crossvalidated
[post_id]: 176775
[parent_id]: 176771
[tags]: 
If you want to talk about classifier performance, we are typically interested in generalization capabilities, that is its performance on unseen data. Unseen in this case means data that was not used in fitting the model whatsoever, so that is essentially everything except training data (and optionally also excluding data used for tuning). Typically this is called test or validation data, depending on the context. Furthermore, accuracy is the fraction of instances classified correctly. While somewhat useful, this is not the best metric to evaluate classifiers. Better approaches include area under the ROC curve (AUROC) and log loss (only for probabilistic classifiers). Better yet would be to design a utility function yourself, based on what is important for your application (e.g. typically the cost varies between false positives and false negatives for any given application). As you mentioned SVM, I recommend using AUROC because proper scoring metrics like log loss are unavailable since SVMS are not inherently probabilistic (an SVM yield the signed distance of a new test instance to the separating hyperplane, which is in $[-\infty,\infty]$, instead of $[0,1]$).
