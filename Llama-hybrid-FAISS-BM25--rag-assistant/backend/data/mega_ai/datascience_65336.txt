[site]: datascience
[post_id]: 65336
[parent_id]: 
[tags]: 
Wiggle in the initial part of an LSTM prediction

I working on using LSTMs and GRUs to make time series predictions. For the most part the predictions are pretty good. However, there seems to be a wiggle (or initial up-then-down) before the prediction settles out similar to the left side of this figure from another question . In my case, it is also causing a slight offset. Does anyone have any idea why this might be the case? Below are the shapes of the training and test sets, as well as the current network structure. I've tried reducing the sequence lengths from 60 timesteps, switching between LSTMs and GRUs, and also having the inputs and outputs overlap slightly, all to no avail. Adding dropout does not seem to help either. The wiggles will not disappear! My sequence lengths are 60 inputs and 60 outputs, currently. Xtrain: (920, 60, 2) Ytrain: (920, 60, 2) Xtest: (920, 60, 2) Ytest: (920, 60, 2) def define_model(): model = Sequential() model.add(LSTM(64, return_sequences=True, input_shape=(None, 2))) model.add(LSTM(64, return_sequences=True)) model.add(LSTM(64, return_sequences=True)) model.add(TimeDistributed(Dense(2))) model.add(Activation('linear')) model.summary() return model
