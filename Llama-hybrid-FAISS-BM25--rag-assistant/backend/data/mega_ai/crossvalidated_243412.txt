[site]: crossvalidated
[post_id]: 243412
[parent_id]: 
[tags]: 
Understanding Gelman & Carlin "Beyond Power Calculations: ..." (2014)

I am reading Gelman & Carlin "Beyond Power Calculations: Assessing Type S (Sign) and Type M (Magnitude) Errors" (2014). I am trying to understand the main idea, the main takeway, but I am confused. Could anyone help distill me the essence? The paper goes something like this (if I understood it correctly). Statistical studies in psychology are often plagued by small samples. Conditional on a statistically significant result in a given study, (1) the true effect size is likely to be severely overestimated and (2) the sign of the effect may be opposite with high probability -- unless the sample size is large enough. The above is shown using a prior guess of the effect size in population, and that effect is typically taken to be small. My first problem is, why condition on the statistically significant result? Is it to reflect the publication bias? But that does not seem to be the case. So why, then? My second problem is, if I do a study myself, should I treat my results differently than I am used to (I do frequentist statistics, not very familiar with Bayesian)? E.g. I would take a data sample, estimate a model and record a point estimate for some effect of interest and a confidence bound around it. Should I now mistrust my result? Or should I mistrust it if it is statistically significant? How does any given prior change that? What is the main takeaway (1) for a "producer" of statistical research and (2) for a reader of applied statistical papers? References: Gelman, Andrew, and John Carlin. "Beyond Power Calculations: Assessing Type S (Sign) and Type M (Magnitude) Errors." Perspectives on Psychological Science 9.6 (2014): 641-651. P.S. I think the new element for me here is the inclusion of prior information, which I am not sure how to treat (coming from the frequentist paradigm).
