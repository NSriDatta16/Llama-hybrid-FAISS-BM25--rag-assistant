[site]: crossvalidated
[post_id]: 541138
[parent_id]: 541049
[tags]: 
This is already partially answered in the comment. With the standard attention, you basically multiply $V$ by a vector (or matrix) of probabilities $A$ , so that you pay more (higher probabilities) or less (lower probabilities) "attention" to particular values of $V$ $$ \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right)V = A V $$ What you seem to suggest is to have something like $$ \frac{1}{\sqrt{d_k}} A VW^T $$ in such a case, you not only attend but also normalize $\tfrac{1}{\sqrt{d_k}}$ and multiply $V$ by additional weights $W^T$ . Sure, you can do all kinds of algebraic operations inside a neural network, and such changes in many cases lead to creating new model architectures, but your suggestions go well beyond "just" attending to the values of $V$ . Moreover, in the first case, you only normalize the attention part with regard to the dimensionality $d_k$ of the key $K$ , while in the second version you normalize the output in terms of the dimensionality of the key. Those operations have completely different effects, why would the output directly depend on the dimensionality of the key? In the first case, the result is just a convex combination of values $V$ with weights such as $\sum A = 1$ . In the second case, you have no guarantees that the output would still be in the range of the values $V$ .
