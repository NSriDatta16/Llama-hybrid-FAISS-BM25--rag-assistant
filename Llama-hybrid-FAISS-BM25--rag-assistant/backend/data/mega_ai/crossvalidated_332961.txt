[site]: crossvalidated
[post_id]: 332961
[parent_id]: 332838
[tags]: 
It is no surprise that the learning curve highly depends on the capabilities of the learner and on the structure of the data set and prediction power of its features. It might be the case that there is only little variance in the combination of feature values (predictors) and labels (response). In this case even a small sample size can allow a capable learner to find all detectable patterns, resulting in a early high score. If not all patters can be detected, no perfect score can be achieved. Since the training score is slightly above the cv-score for 10000 samples, I'd expect that there is an even greater difference for less than 10000 samples. So I suggest to test that. If the the overlap and score remains (even for a small number of examples, let's say Edit regarding class imbalance The class distribution is y 0 77623 1 5436 so guessing the majority class leads to an score of ~ 0.935, which is exactly what we see in the learning curve. The scoring function used in the learning curve is one provided by the estimator, which is Accuracy in case of SVC (see SVC documentation ). The scoring function can be changed in learning curve (search for "scoring" in the learning_curve documentation , also needed to pass the parameter via plot_learning_curve ). Rerunning the code, but not going up to full train size (due to the time complexity of SVC) import numpy as np import matplotlib.pyplot as plt from sklearn.svm import SVC from sklearn.model_selection import learning_curve from sklearn.model_selection import ShuffleSplit import pickle def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=1, train_sizes=np.linspace(.01, 1.0, 5)): # calc curve first to avoid premature opening of figure train_sizes, train_scores, test_scores = learning_curve( estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes) train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) # now create the plot plt.figure(figsize = (13,9)) plt.title(title) if ylim is not None: plt.ylim(*ylim) plt.xlabel("Training examples") plt.ylabel("Score") plt.grid() plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color="r") plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color="g") plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training score") plt.plot(train_sizes, test_scores_mean, 'o-', color="g", label="Cross-validation score") plt.legend(loc="best") return plt y = np.array(pickle.load(open('path-to-test', 'rb'))) # https://stackoverflow.com/questions/11305790/pickle-incompatibility-of-numpy-arrays-between-python-2-and-3 X = None with open('path-to-train', 'rb') as f: u = pickle._Unpickler(f) u.encoding = 'latin1' X = np.array(u.load()) gamma = 0.001 C = 10000 title = "Learning Curves (SVM, RBF kernel, $\gamma=" + str(gamma) + ", C=" + str(C) + "$)" # SVC is more expensive so we do a lower number of CV iterations: cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42) estimator = SVC(kernel = 'rbf', C=C, gamma=gamma, class_weight='balanced') plot_learning_curve(estimator, title, X, y, cv=cv, n_jobs=6, train_sizes=np.linspace(0.015,0.15,5)) plt.show() leads to this graph so there might be a code issue, maybe at initial loading / preprocessing of X and y.
