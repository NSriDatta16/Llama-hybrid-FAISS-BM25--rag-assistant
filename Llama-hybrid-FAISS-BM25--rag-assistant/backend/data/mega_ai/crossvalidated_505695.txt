[site]: crossvalidated
[post_id]: 505695
[parent_id]: 
[tags]: 
Calculating neural network weights from decision boundary

I am was preparing for a pattern recognition exam and in a mock exam the question below was asked. I've also included my attempt, but I couldn't really figure out how to find the weights to the output layer. Could you please tell me whether what I've done is correct and guide me how to find the output layer weights? Question The neural network is supposed to give boundary decision between classes depicted in the diagram below. Please fill in missing neuron’s weights in the net schema at the bottom of the page. The network should produce 1 for ‘*’ class. Neurons have binary bipolar activation function (i.e. their output is -1 or 1). My attempt I thought that each line segment, which I will now refer as left, middle (the vertical one) and right line segments could be represented with one of the neurons from the network. So I took the decision boundary as, $$ \begin{align*} & w_1x_1 + w_2x_2 - b = 0 \tag 1 \\ & x_2 = - \frac{w_1}{w_2} x_1 + \frac{b}{w_2} \tag 2 \end{align*} $$ Left Line Segment (LLS) I've chosen 2 points from the Figure above that are on the left line segment so that I could calculate the slope of it which I will later use to get the slope of the weight vector. $$ \begin{align*} P_1 &= (\phantom- 2, 6)\\ P_2 &= (-6, 2) \\[1ex] m_{LLS} &= \frac{6 - 2}{2 - (-6)} = \frac{1}{2} \end{align*} $$ Since our weight vector is supposed to be perpendicular to the decision boundary I can then do, $$ m_{w_{LLS}} = - \frac{1}{m_{LLS}} = - \frac{2}{1} $$ which means that, $$ m_{w_{LLS}} = - \frac{2}{1} = \frac{w_{LLS_{2}} - 0}{w_{LLS_{1}} - 0} = \frac{w_{LLS_{2}}}{w_{LLS_{1}}} \\ $$ Since the weight vector should be pointing in the direction of the positive (red) class I took the $w_{LLS}$ vector as $$ \begin{align*} & w_{LLS_{1}} = \phantom- 1 \\ & w_{LLS_{2}} = -2 \end{align*} $$ Substituting the weight values and $P_1$ in Eq. (2) $$ \begin{align*} & 6 = \frac{1}{2} 2 + \frac{b}{2} \\[1ex] & b = 10 \end{align*} $$ In the end, for the left line segment, we have got a weight vector of $\begin{bmatrix} \phantom- 1 \\ -2 \end{bmatrix}$ and a bias of 10. Middle Line Segment (MLS) For the middle line segment, since it's a vertical line we know that $w_{MLS_{2}}$ should already be 0 so that we get a horizontal weight vector that is perpendicular to the MLS. Since the weight vector should point to the positive (red) samples, I thought that $w_{MLS_{1}}$ should be -1 so that we get a horizontal weight vector pointing to the positive samples. After figuring out $w_{MLS}$ we can calculate the bias by substituting weights and a point that's on MLS in Eq. 1. \begin{align} P &= (2, 6) \\[1ex] -1x_1 + 0x_2 - b &= 0 \\ -2 + 0 - b &= 0 \\ b &= -2 \end{align} In the end, for the left line segment, we have got a weight vector of $\begin{bmatrix}-1\\ \phantom- 0\end{bmatrix}$ and a bias of -2. Right Line Segment (RLS) Two points on the right line segment are, $$ \begin{align*} & P_1 = (2, -6) \\ & P_2 = (8, -2) \end{align*} $$ Slope of the RLS is, $$ m_{RLS} = \frac{-2 - (-6)}{8 - 2} = \frac{2}{3}\\ $$ Hence the slope of the weight vector $w_{RLS}$ should be $$ m_{w_{RLS}} = - \frac{3}{2} = \frac{w_{RLS_{2}} - 0}{w_{RLS_{1}} - 0} = \frac{w_{RLS_{2}}}{w_{RLS_{1}}} $$ Since the weight vector should point to the positive (red) samples we choose $w_{RLS_{2}}$ to be negative and $w_{RLS_{1}}$ to be positive so that we get $$ w_{RLS_{1}} = \phantom- 2 \\ w_{RLS_{2}} = -3 $$ It's shown in the network figure above that our $w_{RLS_{1}}$ should be 1 so we get a new scaled weight vector of $\begin{bmatrix}1\\ \frac{-3}{2}\end{bmatrix}$ . To find the bias we substitute weights and $P_1$ in Eq. (1) $$ 1 x_1 + \frac{-3}{2}x_2 - b = 0 \\ 2 + \frac{-3}{2}(-6) - b = 0 \\ b = 11 $$ Finally If I substitute all the values in the network Figure above, I get Where I am Stuck As I mentioned in the beginning, I couldn't figure out how to find the output layer weights. I don't know what I should be looking for. I would appreciate your help on that.
