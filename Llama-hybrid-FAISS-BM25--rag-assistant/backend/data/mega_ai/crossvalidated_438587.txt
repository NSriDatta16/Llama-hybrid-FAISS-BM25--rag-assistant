[site]: crossvalidated
[post_id]: 438587
[parent_id]: 438197
[tags]: 
Handling this type of data as predictors in any regression is tricky. However you treat the below-detection or extreme-outlier cases, you still are faced with the requirement that the outcome in the appropriate scale (log-odds of disease status in this case) must be linearly related to the predictor value (perhaps after some transformation). Furthermore, your approach might be leading you to miss some important relationships in your data. For the cases below detection limit, it's not clear why a lower detection limit is providing negative values. One way to proceed would be to set such undetectable values to the lower limit of detection. For the extreme-outlier cases, you should first determine whether these are technical errors (in which case they should be removed or handled by imputation ) or if they are cases with truly high values (in which case you need to keep them). The problems with linearity might be minimized by taking advantage of your large number of cases. Instead of simply using the untransformed predictor values or simple transformations (e.g., logarithmic) of them, use spline terms in the regression to find an empirical transformation that fits well. The rcs() function in the R rms package provides that functionality in a way that you can incorporate into your analysis. I question, however, the apparent restriction of your approach to 50 separate conditional logistic regressions , as indicated in a comment. That approach does not allow you to evaluate how values of any single measured compound are related to disease status when the values of the other compounds are also taken into account, whether as independent predictors or in interactions with the values of that compound. For example, a high level of Compound A might not matter for disease status unless Compound B is also high. Your approach will miss that type of situation. Also, you will have to decide how to handle the problem of multiple comparisons when you are evaluating so many models at once on the same data set. With the size of your data set you might be able to incorporate all 50 of your compounds into a single regression model, at least without interactions, while avoiding overfitting. (I would further recommend a regularization approach like LASSO as a start toward predictor selection without overfitting, but I'm not sure how readily standard implementations of LASSO would work with conditional logistic regression as implemented via Cox-type models in clogit() .) In what seems to be still the exploratory phase of your study, you might be better off using a different approach from conditional logistic regression. Your data seem nicely suited to initial analysis with boosted classification trees . That analysis requires no assumptions about the shape of the relationship between your predictor values and class membership, and a suitable choice of the size of the trees can allow for incorporation of multi-way interactions among the predictors. Implementations of this analysis can provide information about relative importance about predictors, which you could use to help focus on a subset of your compounds in future work. I don't know whether this analysis can take your matching directly into account, but I suspect that you can get useful results this way in any event.
