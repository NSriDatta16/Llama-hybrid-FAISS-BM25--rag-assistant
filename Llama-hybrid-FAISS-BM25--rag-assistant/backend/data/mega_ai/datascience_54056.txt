[site]: datascience
[post_id]: 54056
[parent_id]: 
[tags]: 
Minimizing cross entropy vs minimizing negative probabilities

The cross entropy loss can be written as $L_1 = -\sum_i\sum_c y_{ic}\log P_{ic}$ , where $i$ represents images and $c$ are the classes. $y_{ic}=1$ for the correct class. Instead of L_1 I can minimize the following $L_2 = -\sum_i\sum_c y_{ic}P_{ic}$ When I use these as the loss functions to CNN I found that for a particular problem $L_2$ performs significantly better than $L_1$ . However, I am unable to explain it. What could be the reason for this?
