[site]: crossvalidated
[post_id]: 597038
[parent_id]: 597031
[tags]: 
The logistic regression model is an instance of Generalized Linear Models (GLM) and arises as follows. Suppose we observe random variables that can take only zeros and ones, that is, let us have a sample $Y_1,\ldots, Y_n$ , where $Y_i$ is a binary random variable. Suppose also that the elements of the sample are independent of each other, and each of them follows a Bernoulli distribution with success probability $p_i$ . Mathematically this can be expressed by $$ Y_i\,\overset{\textrm{iid}}{\sim}\,\text{Ber}(p_i),\quad i=1,\ldots,n. $$ Now, for some good reason, you believe that the $p_i$ s may not necessarily be equal and that they depend on some features that you observe or that you have control over. Let these features be $X_i = (X_{i1},\ldots, X_{ip})$ , for $i=1,\ldots,n$ . Suppose that a linear relationship between the probabilities $p_i$ and the $X_i$ is fine, say $$ p_i = \psi(X_i^\top\beta),\tag{*} $$ where $\beta = (\beta_1,\ldots,\beta_p)$ are the unknown coefficients that determine such a relation and $\psi(x) = x$ is the identity function. However, you see right away that this representation is flawed because you have no guarantees that $X_i^\top \beta$ will be valid probabilities, i.e. will be in (0,1). To overcome this issue, you have to use a suitable $\psi$ in (*) that guarantees this condition. One choice is to take $\psi(x) = e^{x}/(1+e^x)$ and the relation becomes $$ p_i = \frac{e^{X_i\beta}}{1+e^{X_i\beta}}\, $$ or equivalently $$ \text{logit}(p_i) = \log\left(\frac{p_i}{1-p_i}\right) = X_i\beta\,\tag{**} $$ The equation (**), or its equivalent version, is one of the reasons why this model is called logistic regression. By the way, this specific $\psi(x)$ is the cumulative distribution function of a standard logistic random variable . Other choices for $\psi$ are possible (i.e. the probit function which leads to the probit regression ), although there are some special reasons for using the logit function. This is meant as an intuitive explanation but if you need more mathematical details, please refer to (the bible of) GLM, i.e. McCullagh & Nelder (1983) Generalized Linear Models , 2nd edition, CRC Press, doi ).
