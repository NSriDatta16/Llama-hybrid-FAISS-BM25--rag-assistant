[site]: crossvalidated
[post_id]: 500220
[parent_id]: 287046
[tags]: 
Multilayer Perceptron (MLP) are sensitive to outliers. MLP are universal approximators i.e. they can be used to approximate any target function. With such an expressive hypothesis space, MLP may risk overfitting by learning from noise (outliers). Outliers can also cause slow/no learning to take place because of the vanishing gradient problem . Activations saturate at either tail of 0 or 1, and gradients are near zero in these regions. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function (unable to learn from other features). Hence, as a preprocessing step, it is highly recommended to apply standardization on the training data to reduce outliers. References: LeCun, Y. A., Bottou, L., Orr, G. B., & MÃ¼ller, K. R. (2012). Efficient backprop. In Neural networks: Tricks of the trade (pp. 9-48). Springer, Berlin, Heidelberg. Pang-Ning Tan, Michael Steinbach, Anuj Karpatne, and Vipin Kumar. 2018. Introduction to Data Mining (2nd Edition) (2nd. ed.). Pearson. Chapter 5.4 Artificial Neural Networks
