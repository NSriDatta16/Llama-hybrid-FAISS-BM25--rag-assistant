[site]: crossvalidated
[post_id]: 320665
[parent_id]: 
[tags]: 
Why does feature engineering work ?

Recently I have learned that one of ways for finding better solutions for ML problems is by creation of features. One can do that by for example summing two features. For example, we possess two features "attack" and "defense" of some kind of hero. We then create additional feature called "total" which is a sum of "attack" and "defense". Now what appears to me strange is that even tough "attack" and "defense" are almost perfectly correlated with "total" we still gain useful information. What is the math behind that? Or is me reasoning wrong? Additionally, is that not a problem, for classificators such as kNN, that "total" will be always bigger than "attack" or "defense"? Thus, even after standarization we will have features containing values from different ranges?
