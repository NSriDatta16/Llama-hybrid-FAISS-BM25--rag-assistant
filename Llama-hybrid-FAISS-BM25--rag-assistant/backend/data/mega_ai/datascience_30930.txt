[site]: datascience
[post_id]: 30930
[parent_id]: 
[tags]: 
Accuracy and loss don't change in CNN. Is it over-fitting?

My task is to perform classify news articles as Interesting [1] or Uninteresting [0]. My training set has 4053 articles out of which 179 are Interesting . The validation set has 664 articles out of which 17 are Interesting . I have preprocessed the articles and converted to vectors using word2vec. The CNN architecture is as follows: sentence_length, vector_length = 500, 100 def create_convnet(img_path='../new_out/cnn_model_word2vec.png'): input_shape = Input(shape=(sentence_length, vector_length, 1)) tower_1 = Conv2D(8, (vector_length, 3), padding='same', activation='relu')(input_shape) tower_1 = MaxPooling2D((1,vector_length-3+1), strides=(1, 1), padding='same')(tower_1) tower_1 = Dropout(0.25)(tower_1) tower_2 = Conv2D(8, (vector_length, 4), padding='same', activation='relu')(input_shape) tower_2 = MaxPooling2D((1,vector_length-4+1), strides=(1, 1), padding='same')(tower_2) tower_2 = Dropout(0.25)(tower_2) tower_3 = Conv2D(8, (vector_length, 5), padding='same', activation='relu')(input_shape) tower_3 = MaxPooling2D((1, vector_length-5+1), strides=(1, 1), padding='same')(tower_3) tower_3 = Dropout(0.25)(tower_3) merged = concatenate([tower_1, tower_2, tower_3], axis=1) merged = Flatten()(merged) dropout1 = Dropout(0.5)(merged) out = Dense(1, activation='sigmoid')(dropout1) model = Model(input_shape, out) plot_model(model, to_file=img_path) return model some_model = create_convnet() some_model.compile(loss=keras.losses.binary_crossentropy, optimizer='adam', metrics=['accuracy']) The model predicts all articles in the validation set as Uninteresting [0]. The accuracy is 97.44% which is same as the ratio of Uninteresting articles in the validation set. I have tried variations of this architecture but still, the issue exists. For experimentation, I predicted on the training data itself, for that too, it predicts all as Uninteresting [0]. Here are the logs for 10 epochs: some_model.fit_generator(train_gen, train_steps, epochs=num_epoch, verbose=1, callbacks=callbacks_list, validation_data=val_gen, validation_steps=val_steps) Epoch 1/10 254/253 [==============================] - 447s 2s/step - loss: 0.7119 - acc: 0.9555 - val_loss: 0.4127 - val_acc: 0.9744 Epoch 00001: val_loss improved from inf to 0.41266, saving model to ../new_out/cnn_model_word2vec Epoch 2/10 254/253 [==============================] - 440s 2s/step - loss: 0.7099 - acc: 0.9560 - val_loss: 0.4127 - val_acc: 0.9744 Epoch 00002: val_loss did not improve Epoch 3/10 254/253 [==============================] - 440s 2s/step - loss: 0.7099 - acc: 0.9560 - val_loss: 0.4127 - val_acc: 0.9744 Epoch 00003: val_loss did not improve Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513. Epoch 4/10 254/253 [==============================] - 448s 2s/step - loss: 0.7099 - acc: 0.9560 - val_loss: 0.4127 - val_acc: 0.9744 Epoch 00004: val_loss did not improve Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05. Epoch 5/10 254/253 [==============================] - 444s 2s/step - loss: 0.7099 - acc: 0.9560 - val_loss: 0.4127 - val_acc: 0.9744 Epoch 00005: val_loss did not improve Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06. Epoch 6/10 254/253 [==============================] - 443s 2s/step - loss: 0.7099 - acc: 0.9560 - val_loss: 0.4127 - val_acc: 0.9744 Epoch 00006: val_loss did not improve Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07. Epoch 7/10 254/253 [==============================] - 443s 2s/step - loss: 0.7099 - acc: 0.9560 - val_loss: 0.4127 - val_acc: 0.9744 Epoch 00007: val_loss did not improve Epoch 00007: ReduceLROnPlateau reducing learning rate to 1e-07. Epoch 8/10 254/253 [==============================] - 443s 2s/step - loss: 0.7099 - acc: 0.9560 - val_loss: 0.4127 - val_acc: 0.9744 Epoch 00008: val_loss did not improve Epoch 00008: ReduceLROnPlateau reducing learning rate to 1e-07. Epoch 9/10 254/253 [==============================] - 444s 2s/step - loss: 0.7099 - acc: 0.9560 - val_loss: 0.4127 - val_acc: 0.9744 Epoch 00009: val_loss did not improve Epoch 00009: ReduceLROnPlateau reducing learning rate to 1e-07. Epoch 10/10 254/253 [==============================] - 440s 2s/step - loss: 0.7099 - acc: 0.9560 - val_loss: 0.4127 - val_acc: 0.9744 Epoch 00010: val_loss did not improve Epoch 00010: ReduceLROnPlateau reducing learning rate to 1e-07. Out[3]:
