[site]: crossvalidated
[post_id]: 282399
[parent_id]: 282387
[tags]: 
First off, principal component analysis is not necessarily what you want. PCA is a method of dimensionality reduction, meaning that it reduces the dimensions of your data. This may be good for you, but you should know that the algorithm will change the value of the components in each dimension. See this: http://setosa.io/ev/principal-component-analysis/ Your thought about distributions is a very good idea! So good, actually, that staticians have been doing something similar for a long time. Try computing the Kullback-Leibler divergence of the distributions of your two classes for each dimension in your data. Don't just do it, understand what it means: https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained or look at my own post https://github.com/abrhor/Momentum-Neural-Networks/blob/master/Distributions/Conditional%20Distribution%20of%20RSI%20Notebook.ipynb which is not as informative and also has some finance and a lot of code. KL divergence will give you information about your data, but won't help simplify the computation of the actual classifier like PCA does: Finally, just a convention, you don't mean computing the normal distributions. "Normal" is a term for a regular distribution/histogram if the plot is or is approximately symmetrical, because it has certain resulting statistical characteristics.
