[site]: crossvalidated
[post_id]: 424560
[parent_id]: 424558
[tags]: 
For a single example, the network takes a 784-element vector as its input. So rephrasing the problem in OP's post, they wish to learn the function $$ f(x) = Ix $$ where $I$ is the $784\times 784$ identity matrix. Perfect fit is impossible with this model The 1-layer network probably has an easier time because instead of attempting to "line up" four weight matrices through four nonlinearities, it only has to line up one, i.e. it is easier to find an approximation in $W_1, b_1$ for $$ Ix = g(W_1 x+b_1). $$ But even the simple expression $Ix = g(W_1 x+b_1)$ should be an obvious warning that attempting to find a perfect fit is a fool's errand, because it is trying to approximate a linear function with a nonlinear function. In particular, because of how ReLUs are defined, any $x is set to 0, so this model will never achieve 0 error when any elements of $x$ are negative. The UAT is an approximation theorem Indeed, for any choice of nonlinear activation $g$ , I can find an $x$ for which the error is positive. So then the interesting question becomes "Can we fit a model so that the error is at most $\epsilon$ for $x$ in some interval $\mathcal{I}$ ?" And this statement of the problem is more-or-less compatible with the caveats of the UAT. It also points us in a more profitable direction: instead of seeking 0 error, we wish to find minimal error when the inputs are in some interval. In other words, theorems about neural networks don't guarantee that you can achieve 0 error, they guarantee that you can bound error for inputs in some interval (subject to some terms and conditions). The UAT doesn't comment on whether it's easy to train any particular network. Actually finding the weights and biases which achieve the minimum error is a very challenging problem. In particular, we don't have much reason to believe that the choice of initialization, optimizer, learning rate and number of epochs, etc. in this code snippet are best for this task. This optimization problem is hard A four-layer network with ReLU activations $g(x)=\max\{0, x\}$ is given by $$ h(x)=g(W_4g(W_3g(W_2g(W_1x+b_1)+b_2)+b_3)+b_4). $$ So what you seek in your question is solutions $W_i, b_i$ such that $$ Ix = g(W_4g(W_3g(W_2g(W_1x+b_1)+b_2)+b_3)+b_4) $$ for all $x$ , where $W_i, b_i$ are have appropriate shape. This doesn't look particularly friendly to try and solve. Indeed, in light of my remarks about the UAT, we will have to restate this to bound the error and focus on an interval of inputs. Even if we restate the problem in this way, it is still challenging from the perspective of gradient descent because of the dying ReLU phenomenon , the weaknesses of gradient descent , and the poor conditioning of the optimization task due to the scale of the inputs. Tuning a neural network is the greater part of using neural networks. If you don't want to spend a lot of time changing hyper-paremters, then you should use a different model.
