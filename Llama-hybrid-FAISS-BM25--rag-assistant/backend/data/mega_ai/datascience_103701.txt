[site]: datascience
[post_id]: 103701
[parent_id]: 72589
[tags]: 
Well, I have found a lot of confusing materials on the Internet and even in papers. MLP and FC layers are kind of similar, but not the same actually. 1) Short answer: MLP - mini neural network with 3 fully-connected layers and an activation function. FC - a layer whose neurons have connections to every input neuron (from the previous layer). 2) Deep answer. FC: First, the fully-connected layer is just one layer with multiple 'neurons' in it, which are connected to every input 'neurons'. That is where its name 'fully-connected' comes from: everything is connected with everything. MLP: Next, MLP is a multi-layer perception, which is a combination of at least 3 fully-connected layers: input layer, hidden layer, output layer. MLP also has an activation function applied to every neuron's output of the hidden layer. Therefore, the pipeline is: fc_in -> fc_hid -> activation func -> fc_out. You can consider an MLP as a tiny neural network (actually MLP indeed was almost the first NN). Moreover, the original 'Perceptron' proposed in the original paper actually uses a specific non-differentiable activation function - Heaviside function (using it neurons may have only '0' or '1' as an output, and nothing in-between). However nowadays nobody uses that activation, since we need to calculate gradient through it. [Currently, we use ReLU or similar activation functions.] Therefore, at the moment you barely can find a paper that uses the actual 'MLP'. Conclusion: Unfortunately, due to misunderstanding, many people use the term 'MLP' as the synonym for the 'FC layer', however, this is not correct. They are not interchangeable and we can't use the term MLP to describe an FC layer. In my opinion, I would not recommend using the 'MLP' term at all, since now nobody actually uses the 'real' MLP implementation. Regarding papers, I noticed that authors usually use 'MLP' when they want to describe a combination of an FC layer and any activation functions following it: MLP: fc - > activation func (in -> MLP -> out). However, for the aforementioned reasons, this notation is not fully correct.
