[site]: crossvalidated
[post_id]: 197241
[parent_id]: 197217
[tags]: 
Following @Wayne's comments on parallelization, see Zhou et al. (2014), "A Reduction of the Elastic Net to Support Vector Machines with an Application to GPU Computing", arXiv:1409.1976 . They say The state-of-the-art single-core implementation for solving the Elastic Net problem is the glmnet package developed by Friedman. Mostly written in Fortran language, glmnet adopts the coordinate gradient descent strategy and is highly optimized. As far as we know, it is the fastest off-the-shelf solver for the Elastic Net. Due to its inherent sequential nature, the coordinate descent algorithm is extremely hard to parallelize. but come up with a different approach, showing the elastic net is equivalent a type of support vector machine (which can be parallelized): [...] we take inspiration from recent work on machine learning reductions [15, 19] and we reduce the Elastic Net to the squared hinge-loss SVM (without a bias term). We show that this reduction is exact and extremely efficient in practice. The resulting algorithm, which we refer to as Support Vector Elastic Net (SVEN), naturally takes advantage of the vast existing work on parallel SVMs, immediately providing highly efficient Elastic Net and Lasso implementations on GPUs, multi-core CPUs and distributed systems Note that hyper-parameter selection by cross-validation can of course be parallelized (and is implemented for the shrinkage hyper-parameter in cv.glmnet â€”see executing glmnet in parallel in R ). The Matlab code to fit a Support Vector Elastic Net is available at https://bitbucket.org/mlcircus/sven/ . You could give that a try!
