[site]: stackoverflow
[post_id]: 1755183
[parent_id]: 1750001
[tags]: 
I wonder, whether the execution time is taken by the join or by the data transfer. Assumed, the average data size in your Name column is 150 chars, you will actually have 300 bytes plus the other columns per record. Multiply this by 100 million records and you get about 30GB of data to transfer to your client. Do you run the client remote or on the server itself ? Maybe you wait for 30GB of data being transferred to your client... EDIT: Ok, i see you are inserting into Aux table. What is the setting of the recovery model of the database? To investigate the bottleneck on the hardware side, it might be interesting whether the limiting resource is reading data or writing data. You can start a run of the windows performance monitor and capture the length of the queues for reading and writing of your disks for example. Ideal, you should place the db log file, the input tables and the output table on separate physical volumes to increase speed.
