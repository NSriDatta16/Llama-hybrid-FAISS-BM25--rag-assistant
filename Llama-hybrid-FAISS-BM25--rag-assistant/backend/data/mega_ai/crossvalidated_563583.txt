[site]: crossvalidated
[post_id]: 563583
[parent_id]: 552195
[tags]: 
Eventually, I have found a comprehensive description of the algorithm for creating a calibration plot in J. Esarey, A. Pierce: "Assessing Fit Quality and Testing for Misspecification in Binary Dependent Variable Models." Political Analysis 20.4, pp. 480-500, 2012 The article compares it with classification based evaluation. Here is a summary of the ideas together with my comments and R code for creating a calibration plot. When comparing the probability predicted by the model with the "observed" probability, there is the problem that no probabilities are observed but only zeros or ones, i.e. (non-) occurences of the response. These values can be smoothed out to probabilities by a distance weighted average in the "neighborhood" of each value, e.g. with a LOESS local regression. The distance for establishing the "neighborhood" and the weights can be measured in different spaces. Two obvious possible choices are The distance on the link scale, i.e. on $\eta_i=\beta_0 + \langle\vec{\beta},\vec{x}_i\rangle$ , where $x_i$ are the predictor variable values for the $i$ -th observation, and $\beta$ are the model parameters. The distance on the probability scale, i.e. on $p_i=P(Y=1|\vec{x}_i) = 1 / (1+e^{-\eta_i})$ A LOESS fit through the points $(y_i,\eta_i)$ or $(y_i,p_i)$ will then yield an estimator $\hat{p}_i$ for each $y_i$ , which can be compared to the probability $p_i$ predicted by the model: There are two caveats, however: For degrees greater than zero, the LOESS fit can yield values outside [0,1]. For this reason, the first value is missing in both of the above plots: its estimated probability $\hat{p}_i$ is negative. This can be easily corrected by cutting off the probabilities at zero and one. LOESS only takes a certain percentage (parameter span ) of neighbors into account. The above plots have been created with the default span=0.75 . Esarey & Pierce suggest two different optimization methods and link to a reference implementation in a footnote, but that link is meanwhile stalled. I have therefore implemented a very simple optimization criterion: the minimum MSE between $\hat{p}_i$ and $p_i$ , i.e. $\sum_i(\hat{p}_i - p_i)^2$ . The result on the Challenger Space Shuttle O-Ring dataset can be seen here: I have also included the 95% prediction interval for $p_i$ as predicted by the model. Esarey & Pierce also compute the percentage of values that lie outside an 80% confidence interval by means of a parametric bootstrap, but this might easier be computed directly from the confidence intervals for $p_i$ . Here is the code to produce the calibration plot on the link level (right hand side): # Challenger Space Shuttle O-ring data: ok vs temp data $fit) x fit[sort.key] # prediction interval for probability plot(link.model $fit, data$ y, main="link level") p.lower $fit - qnorm(1-0.05/2) * link.model$ se.fit)[sort.key] p.upper $fit + qnorm(1-0.05/2) * link.model$ se.fit)[sort.key] polygon(c(x,rev(x)), c(p.lower, rev(p.upper)), col="#dddddd", border=NA) points(link.model $fit, data$ y) # replot overplotted points lines(x, plogis(x), col="red") # LOESS fit optim.span $y, x=link.model$ fit, p.model=p.model) span $minimum p.fit y, x=link.model$fit), family="gaussian", degree=1, span=span) p.cutfit 1]
