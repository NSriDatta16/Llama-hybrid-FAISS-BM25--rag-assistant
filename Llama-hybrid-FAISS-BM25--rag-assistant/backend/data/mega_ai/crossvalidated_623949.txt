[site]: crossvalidated
[post_id]: 623949
[parent_id]: 
[tags]: 
In Bayesian parameter estimation, prior update involves averaging posteriors of all examples?

I was reading Andriy Burkov's the hundred page machine learning book (section 2.5, check here: http://themlbook.com/wiki/doku.php ) I am a little bit confused by the way he computes the prior for the next iteration, which is highlighted in the screenshot below. In my understanding bayesian update should be incremental and the prior for the next iteration should just be the posterior that you derived from the current example, isn't it? The author seems to be computing the posteriors for all examples and averaging them in every iteration. Something I am missing here?
