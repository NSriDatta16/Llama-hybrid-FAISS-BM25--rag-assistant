[site]: stackoverflow
[post_id]: 1631175
[parent_id]: 
[tags]: 
Does varchar result in performance hit due to data fragmentation?

How are varchar columns handled internally by a database engine? For a column defined as char(100) , the DBMS allocates 100 contiguous bytes on the disk. However, for a column defined as varchar(100) , that presumably isn't the case, since the whole point of varchar is to not allocate any more space than required to store the actual data value stored in the column. So, when a user updates a database row containing an empty varchar(100) column to a value consisting of 80 characters for instance, where does the space for that 80 characters get allocated from? It seems that varchar columns must result in a fair amount of fragmentation of the actual database rows, at least in scenarios where column values are initially inserted as blank or NULL, and then updated later with actual values. Does this fragmentation result in degraded performance on database queries, as opposed to using char type values, where the space for the columns stored in the rows is allocated contiguously? Obviously using varchar results in less disk space than using char, but is there a performance hit when optimizing for query performance, especially for columns whose values are frequently updated after the initial insert?
