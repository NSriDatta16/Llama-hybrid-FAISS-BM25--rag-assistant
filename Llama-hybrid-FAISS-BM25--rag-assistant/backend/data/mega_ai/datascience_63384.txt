[site]: datascience
[post_id]: 63384
[parent_id]: 
[tags]: 
Audio dataset preprocessing to perform cry detection

I am building a neural network to perform cry detection (i.e., binary classification of cry/non-cry situations) when capturing sound in a house environment. To do so, I performed the following steps: Dataset building : I collected some samples in publicly available datasets, and enriched this sample set with some samples I registered with my own device (including speech, sounds from the home, sounds from the street, music...); I have a few samples (around 2500), which could be further enriched with new samples Feature extraction : I used librosa to extract MFCC descriptors from my samples Model creation and training : I built the model of my NN (using Keras, one recurrent layer, one dense layer and one final sigmoid node), and trained it on the features extracted at step 2. Alas, although the model is performing very well on test set (achieving ~98% of accuracy), in the real world (i.e., when I put the trained model in a mobile application, stream the audio through my device and use it to classify the sounds I capture in the home environment I am at that moment) the network seems to perform very poorly, giving more or less random "cry"/"non-cry" labels over time. I was wondering if this poor accuracy I achieve in the real world is related only to the small size of the dataset. If this is not the case, is there some preprocessing I can do on my dataset? I learned from other questions that for speech recognition one would apply some preprocessing to remove some unwanted situations (e.g., to reduce the effect of volume change or to remove noise). Can you suggest me some preprocessing to do on the dataset, before training, either on the raw audio or on the extracted MFCC coefficients, so that this processing is adapt to my case study? Thanks.
