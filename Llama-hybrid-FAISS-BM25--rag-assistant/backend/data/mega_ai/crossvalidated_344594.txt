[site]: crossvalidated
[post_id]: 344594
[parent_id]: 
[tags]: 
Artificial neurons based on modelling observed correlations and predicting from them?

Biological neurons are much more sophisticated than standard ANNs. Basing mostly on received impulses, they can learn long correlation chains between e.g. visual cortex, motor system and motivation system of a human newborn. There are approaches to re-engineer neocortex with simple local rules, for example hierarchical temporal memory , but mainly focused on neurons searching for sparse patterns and predicting them. The big question is what local rules are sufficient for such single neuron, which gets some inputs and can make some local predictions based on them. It turns out we can construct inexpensive artificial neurons directly learning correlations of inputs and using them for predictions - by modeling their joint probability distribution as a linear combination of orthonormal functions $\rho(x)=\sum a_f f(x)$, adaptation of these coefficients can be just: $a_f=(1-\lambda) a_f +\lambda f(x)$ for observation $x$ and some learning rate $\lambda$. Such artificial neuron can e.g. predict each input basing on the remaining ones ($E_{f,i}$ is expected value: $f$ with with constant on $i$-th variable): Here is some orthonormal basis for a pair of variables - $|C|=1$ functions independently model density for each variable, $C=\{1,2\}$ model corrections to density from their correlations, e.g. the upper-left function models that growing one variable grows or shrinks the second (next - concentrates/spreads): Are correlation-based artificial neurons considered in literature? Can they be useful for real-world problems? How to build practical neural-networks from them? Shallow supervised learning would just feed label to one of inputs, then ask for its prediction basing on the remaining inputs ... but what network architecture to use to get deep-learning level of predictions from them? Here is simple simulator allowing to play with this density estimation method. The closest model I was suggested so far is cascade correlations (CC) - where there are added new neurons corresponding to correlations - with frozen weights due to complex dependencies. The approach here uses single neuron working as entire CC network - its adding neurons corresponds to adding new type of correlation to the basis of considered functions $f$. Some advantages: Instead of heuristic modeling, it is based on $L^2$ optimization, which thanks to using orthonormal basis, allows to estimate coefficients independently: $a_f$ is just average of $f(x)$ over the sample. So in contrast to CC, we don't longer have to freeze anything - all coefficients can be continuously adapted, we can freely add/remove functions from the basis. Additionally, we can use a larger basis for modeling, and only a smaller one for actual prediction - which coefficients are significant, larger than their uncertainty (can be also estimated). Independence allows also to use these coefficients as inputs for further layers. It can naturally contain non-linear correlations, e.g. for 2 variables: function (1,1) models linear correlation: increase of the first variable, corresponds to increase or decrease of the second, function (1,2) has parabola in second variable: increase of first variable corresponds to squeeze or spreed of the second, finally they create complete basis, so theoretically could recreate any joint probability distribution, also for a larger number of variables.
