[site]: crossvalidated
[post_id]: 475321
[parent_id]: 
[tags]: 
Problem with bayesian implementation of a Time-lagged Linear Model in PyMC3

I am trying to build a GLM of a time-series y(t) with 2 predictor time series x1(t) and x2(t), where t is in days. But the second time-series influences y(t) with an unknown lag of l days. I was trying to model this in a Bayesian framework using PyMC3 in Python and fit the lag using the code below: from pymc3 import * import numpy as np from scipy.ndimage.interpolation import shift with Model() as model: # Define priors sigma = HalfCauchy('sigma', beta=10, testval=1.) intercept = Normal('Intercept', 0, sigma=20) x_coeff1 = Normal('x1', 0, sigma=20) x_coeff2 = Normal('x2', 0, sigma=20) lag = Poisson('lag',1) likelihood = Normal('y', mu=intercept + x_coeff1 * x1 + x_coeff2 * shift(x2,lag,cval=x2.mean()), sigma=sigma, observed=y) trace = sample(3000, cores=4) But it gives an error: TypeError: argument must be symbolic vector, got 'lag' TypeError: TensorType does not support iteration. Maybe you are using builtin.sum instead of theano.tensor.sum? (Maybe .max?) When instead of setting a prior for lag, I give it a numerical value (say 4) with likelihood = Normal('y', mu=intercept + x_coeff1 * x1 + x_coeff2 * shift(x2,4,cval=x2.mean()), sigma=sigma, observed=y) it works perfectly. What am I doing wrong? How else can i fit the lag?
