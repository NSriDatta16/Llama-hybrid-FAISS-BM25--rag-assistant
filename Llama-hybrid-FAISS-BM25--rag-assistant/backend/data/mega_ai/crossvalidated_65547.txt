[site]: crossvalidated
[post_id]: 65547
[parent_id]: 65543
[tags]: 
I can't make sense of that Wiki article at all. Here's an alternative stab at explaining it. A perceptron with one logistic output node is a classification network for 2 classes. It outputs $p$, the probability of being in one of the classes, with the probability of being in the other simply $1 - p$. A perceptron with two output nodes is a classification network for 3 classes. The two nodes each output the probability of being in a class $p_i$, and the probability of being in the third class is $1 - \sum_{i=(1,2)} p_i$. And so on; a perceptron with $m$ output nodes is a classifier for $m + 1$ classes. Indeed, if there is no hidden layer, such a perceptron is basically the same as a multinomial logistic regression model, just as a simple perceptron is the same as a logistic regression.
