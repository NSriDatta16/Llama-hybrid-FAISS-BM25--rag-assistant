[site]: crossvalidated
[post_id]: 572641
[parent_id]: 572640
[tags]: 
Would this be the right approach Nope. You need to evaluate the process of selecting hyperparameters. This is done via nested cross validation. I would encourage you to stop thinking of a single model . The real question we're trying to answer when we do cross validation is "how does my model creation process perform on unseen data". To that effect, nested cross validation is a better approach. Here is how nested cross validation works. Take your train data and split it into $k$ folds. Set 1 fold aside, let's call this set $X_v$ . Take the remaining $k-1$ folds and combine them. Call this $X_t$ . Use $X_t$ to performa grid search cross validation. This means you're going to split $X_t$ into $k$ folds, leave out one, use the remainder to fit a model with the specified parameters in your grid, etc etc. Once you've done grid search cross validation and selected a model, now use that model to predict on $X_v$ . Note $X_v$ was not used in the selection procedure so there is no risk of data leakage, implicit or explicit. If you're using sklearn, this very easily as shown here . Note that when you create an instance of GridSearchCV you create an estimator. So think of the process of selecting a model via grid search cross validation as a model in an of itself . This idea can help clarify when and how to use various validating schemes.
