[site]: datascience
[post_id]: 84202
[parent_id]: 
[tags]: 
Why Continous Variable Buckets Overfitting model

I have a continuous (high cardinal discrete) variable 'numInteractionPoints' in my dataset during training model - I binned this feature in order to avoid overffing , first top bar chart is from training and second bar chart is from testing for this binned feature. Both datasets has same distribution as per following bar graph this is how i created bin for this feature i converted variable 'numInteractionPoints' into bins using simple bin method based on visual analysis bins = [0, 6, 12, np.inf] names = ['0-6', '6-12','12+'] train_data['numInteractionPointsRange'] = pd.cut(train_data['numInteractionPoints'], bins, labels=names) train_data['numInteractionPointsRange']=train_data['numInteractionPointsRange'].astype('object') in both cases when i include raw variable numInteractionPoints without bin and with above defined bucket the overall F1 score of model decrease on test data from 0.47 to 0.33 for a class 1. On validation dataset it is giving 0.47 while on test it gives 0.33 with or without bins Any idea please what i am missing? This behavior is in both algo Xgboost and catboost, target class is highly imbalanced and i am controlling it Xgboost parameter scale_pos_weight. Why model overfitting due to defined buckets , as you can see above bar chart distribution of both dataset are same
