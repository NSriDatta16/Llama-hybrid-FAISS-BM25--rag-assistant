[site]: crossvalidated
[post_id]: 415159
[parent_id]: 414847
[tags]: 
After some more reading I came to following conclusion: Invertible NN are just neural networks that represent bijective functions $f$ . Normalizing flows are invertible NN $f$ that also have a tractable determinant of the Jacobian $D_x f$ as well as a tractable inverse $f^{-1}$ . This allows for following interpretation: Let $X \sim p_X, Z \sim p_Z$ be some random variable with $Z = f(X)$ . Then $$p_X(x) = p_Z(f(x)) \det D_x f .$$ Because $f$ has a tractable inverse $f^{-1}$ we can therefore easily sample from one of the two distributions $p_X, p_Z$ by sampling from the other one and using the transformation above. This could be applied in the following way (just as an example): We could train $f$ such that $p_X$ represents a distribution of images (e.g. represented by MNIST) and $p_Z$ a Gaussian. Then we can easily sample from the distribution of images by sampling $Z \sim p_Z$ (Gaussian) and just transforming it back to $X = f^{-1}(Z) \sim p_X$ .
