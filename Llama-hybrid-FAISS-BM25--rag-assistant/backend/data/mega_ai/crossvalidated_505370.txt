[site]: crossvalidated
[post_id]: 505370
[parent_id]: 503502
[tags]: 
A slightly different characterisation of the problem Instead of these separate variations/errors in $\alpha$ and $\beta$ you could describe the variance of $Y_i$ directly. A common way (which you see a lot on this site) is to describe a linear function like $$y_i = a+bx_i + \epsilon \quad \text{where} \quad \epsilon \sim N(0,\sigma^2)$$ or $$y_i|x_i \sim N(a+bx_i,\sigma^2)$$ The above is with normal distributed errors. But you can use other distributions too. In general you could describe the mean and variance for $Y_i$ . Conditional on $X_i$ it is often like (the case for homogeneous errors, independent of $x_i$ ) $$\begin{array}{rcl} \text{E}[y_i|x_i] &=& \alpha + \beta x_i \\ \text{Var}[y_i|x_i] &=& \sigma^2 \end{array}$$ (In the case of general linear models a description where $\text{Var}[y_i|x_i]$ is a function of $\text{E}[y_i|x_i]$ is also useful) Your case is very similar but now the variance of the error is not a constant $\sigma$ and it depends on $x_i$ . $$\begin{array}{rcl} \text{E}[y_i|x_i] &=& \alpha + \beta x_i \\ \text{Var}[y_i|x_i] &=& \sigma_{\alpha\alpha} + 2 x_i \sigma_{\alpha\beta} + {x_i}^2 \sigma_{\beta\beta} \end{array}$$ where we use $\sigma_{ij}$ to indicate the variance or covariance. In the case $\alpha = 5, \beta = 3, \sigma_{\alpha\alpha} = \sigma_{\beta\beta} = 0.1, \sigma_{\alpha\beta} = 0.05$ and $X \sim Unif(-1,1)$ it will look like: It is a linear relationship with heteroscedasticity. We can estimate the variance and covariance of $\alpha$ and $\beta$ based on this heteroscedastic dependency of the variance of the error (which we might approximate with the residuals). Method of moments The method that you used is the method of moments . You expressed the expectation of $\tilde X_i Y_i^2$ for the population in terms of coefficients. Then you replace in the expressions the expectation for the population by the average for the sample to obtain estimates of the coefficients. (In your particular execution there is a small mistake by assuming that the expectation of $X_i^2\tilde X_i$ is zero. This is only true when $X_i$ is distributed symmetrical around zero) Least squares method A simpler approach might be to model the expectation of the square of the errors as a linear function of terms of $X_i$ and estimate it with the least squares method applied to the residuals. (It is simpler because it is straightforward and it will help to generalize the problem) The errors are distributed as: $$E(\epsilon_i^2) = \text{Var}[y_i|x_i] = \sigma_{\alpha\alpha} + 2 x_i \sigma_{\alpha\beta} + {x_i}^2 \sigma_{\beta\beta}$$ library(MASS) fit Maximum Likelihood I guess that you might also maximize the likelihood function (or a quasi-likelihood function if you do not see a particular distribution and stick to a formulation with only known conditional mean and variance). But I can not find a closed solution for this. It can be done computationally. I leave this as a separate problem as writing a function that solves it might make this answer too cluttered. In addition, I am not sure whether it will be much faster or more accurate than solving it with the method of moments or fitting the square of the residuals. Generalising Your problem with two equations can be solved in the same way. Now we have two sets of residuals $r_{1i}$ and $r_{2i}$ whose expectation of the products depend on the covariance of the $\alpha_1$ , $\alpha_2$ , $\beta_1$ and $\beta_2$ . $$\begin{array}{rcl} \text{E}[r_{1i}r_{2i}|x_i] &=& \sigma_{\alpha_1\alpha_2} + x_i (\sigma_{\alpha_1\beta_2} + \sigma_{\alpha_2\beta_1}) + {x_i}^2 \sigma_{\beta_1\beta_2} \end{array}$$ You have indeed the term $(\sigma_{\alpha_1\beta_2} + \sigma_{\alpha_2\beta_1})$ whose terms can not be separated with this single equation. The dependency of $r_{1i}r_{2i}$ or $y_{1i}y_{2i}$ on $x_i$ is dependent on the sum but not the independent terms. If you would measure the $y_{1i}$ and ${y_{2i}}$ based on the same correlated $\alpha_1$ , $\alpha_2$ , $\beta_1$ and $\beta_2$ , but with different $x_i$ (say $x_{1i}$ and $x_{2i}$ ) then you could separate the variables $$\begin{array}{rcl} \text{E}[r_{1i}r_{2i}|x_i] &=& \sigma_{\alpha_1\alpha_2} + x_{2i} \sigma_{\alpha_1\beta_2} + x_{1i} \sigma_{\alpha_2\beta_1} + x_{1i}x_{2i} \sigma_{\beta_1\beta_2} \end{array}$$ For what it is worth, here's a code that would compute the covariances (based on the linear fit of the residual term): fit2 $residuals mod2 residuals ### model covariance tabel modr
