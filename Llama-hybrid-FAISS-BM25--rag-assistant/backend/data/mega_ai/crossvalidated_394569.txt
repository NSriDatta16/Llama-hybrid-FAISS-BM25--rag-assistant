[site]: crossvalidated
[post_id]: 394569
[parent_id]: 394504
[tags]: 
There are a couple of issues here: In classical statistics all the distributions used are implicitly conditional on $\theta$ , which is considered to be an "unknown constant". In Bayesian analysis there is no such thing as an unknown constant (anything unknown is treated as a random variable) and we instead use explicit conditioning statements for all probability statements. This means that, in Bayesian analysis, the sampling density $P(X|\theta)$ is the object $P_\theta(X)$ that you referred to in the classical case. (The likelihood function is just the sampling density treated as a function of the parameter $\theta$ with $X=x$ taken to be fixed.) It also means that the density $P(X)$ in the Bayesian analysis is not conditional on $\theta$ . It is the marginal density of the data, which is given by: $$P(X) = \int \limits_{\Theta} P(X|\theta) P(\theta) \ d \theta.$$ There are a few places in your question where you get a bit sloppy with conditioning statements, and you end up equivocating the conditional and marginal distributions of the data. That is not a big problem in classical statistics (since all probability statements are implicitly conditional on the parameter), but it will cause trouble for you in Bayesian analysis. The notation $P(X ; \theta)$ is usually used only in classical statistics, and it is used to denote the same thing as $P_\theta(X)$ ---i.e., it is implicitly the conditional density of the data given the parameter. It would be unusual (and confusing) to use this notation for the joint density. The Bayesian method whereby you maximise the posterior distribution with respect to the parameter is a point-estimation method called maximum a-posteriori (MAP) estimation . This is a point-estimation method that gives you a single point-estimate. You should bear in mind that Bayesians are usually concerned with also retaining the whole posterior density, since this contains more information than the MAP estimator.
