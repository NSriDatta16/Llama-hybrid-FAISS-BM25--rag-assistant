[site]: crossvalidated
[post_id]: 500392
[parent_id]: 
[tags]: 
Why does fast graph convolution need Chebyshev polynomials?

I'm reading the paper Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering and find it difficult to understand the motivation for using Chebyshev polynomials. With localized kernels, $g_{\theta}(\Lambda) = \sum_{k=0}^{K-1} \theta_{k}\Lambda^k$ , and the convolution $U g_{\theta}(\Lambda) U^Tf$ becomes $\sum_{k=0}^{K-1}\theta_{k}L^kf$ . The paper argues that $U g_{\theta}(\Lambda) U^Tf$ requires $O(n^2)$ operations and therefore proposes Chebyshev polynomials to accelerate the computation. My question is, although evaluating $U g_{\theta}(\Lambda) U^Tf$ requires $O(n^2)$ operations, an equivalent evaluation $\sum_{k=0}^{K-1}\theta_{k}L^kf$ only requires $O(K\mathcal{E})$ ( $\mathcal{E}$ is the number of edges). So why bother using Chebyshev polynomials ? How to evaluate $\sum_{k=0}^{K-1}\theta_{k}L^kf$ within $O(K\mathcal{E})$ operations: $u_o = f$ $u_1 = Lu_0$ ( $L$ is sparse and so it requires $O(\mathcal{E})$ operations) $u_2 = Lu_1$ ( $L$ is sparse and so it requires $O(\mathcal{E})$ operations) ...... $\sum_{k=0}^{K-1}\theta_{k}L^kf$ = $\sum_{k=0}^{K-1} u_k$ requires $O(K\mathcal{E})$ operations.
