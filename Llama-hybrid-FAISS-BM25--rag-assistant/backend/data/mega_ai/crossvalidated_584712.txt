[site]: crossvalidated
[post_id]: 584712
[parent_id]: 584706
[tags]: 
Data is king, so if it works in real life we can't argue with it. Having said that, I agree with you it's bad practice and will usually not end well. I can design a dataset where it would work though. Images for example are known for having correlation between close pixels, effectively reducing the number of features with information. Imagine for example a dataset of black balls on a white background and you need to classify images with a ball vs. no ball. In that case, most pixels will not have any information gain so the split will not happen in the tree. All the edge pixels will always be white, containing zero information. Combining that with regularization and image augmentation may leave you with a working model. Just for my curiosity, do you know if the images are generally similar? Another phenomenon worth mentioning is double descent . This is more of a NN phenomenon where over parameterized models where $P>>N$ achieve better performance as the number of parameters increases. But it's also been demonstrated in classical models like linear regression. In summary it's possible, but if I had to do it (with XGBoost) I'd do the following: Dimensionality reduction. Strong regularization. Most importantly - a robust, careful validation procedure.
