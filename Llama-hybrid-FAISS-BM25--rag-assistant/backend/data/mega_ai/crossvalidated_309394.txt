[site]: crossvalidated
[post_id]: 309394
[parent_id]: 
[tags]: 
When does larger network decrease performance?

I am training a deep neural network of mostly one-hot encoded features, which are very sparse (500 columns, mostly zeros). My network architecture of Dense 64 (w/ dropout)->128 (w/ dropout)->128 (w/ dropout)->256 (w/ dropout) performs significantly worse than a single layer Dense 128...why might this be? I doubt that the deeper network is getting caught in a local minimum, because the first few batches of the simpler network perform better than the 20th epoch of the deeper net. All activations are ReLU
