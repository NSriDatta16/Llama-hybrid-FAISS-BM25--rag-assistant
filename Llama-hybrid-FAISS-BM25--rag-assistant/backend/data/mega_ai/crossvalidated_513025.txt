[site]: crossvalidated
[post_id]: 513025
[parent_id]: 
[tags]: 
How to understand the difference between the mixture of same distributions vs. convolution (sum) of random variables of same distributions?

Before I ask the question, let me introduce how I came to this problem. Recently I learned about the linear regression. It was said, that the residuals of the model should be normally distributed . We were told, that many people confuse it with with the distribution of the response variable, which is completely wrong. In other words - what is normally distributed is the response conditionally to the predictor value. I was told that this rule applies to the logistic regression, gamma regression, beta regression and all the other cases of the generalized linear model. That is why in the R statistical package we specify, for example, "distribution = gamma" and the "gamma" doesn't refer to the distribution of the response itself, but, instead, to the conditional distribution of Y given the predictor. The distribution ( Y | X=x). If the model was matched well the data, the distributions of residuals all had mean = 0 and constant variance (homoscedasticity, no bias). When we summed up all those distributions of residuals per group (predictor), it resulted in a normal distribution of same mean and same variance. We were told, that, actually we assess the "overall distribution of residuals" (all residuals pooled together), but this is the same as taking all the conditional distributions and sum them up. This was called "a mixture of distributions" - and indeed, the sum of N(0, sigma) gave N(0, sigma) on the histogram. We can simulate it with this code: > a b mean(c(a, b)) [1] 4.968929 > var(c(a, b)) [1] 0.9398824 > shapiro.test(c(a,b)) Shapiro-Wilk normality test data: c(a, b) W = 0.99251, p-value = 0.3988 This also answered the question, why the response may look totally skewed and still be valid for the regression. We observe skewed response, because when we sum the conditional responses (NOT residuals), which form shifted distributions (different means) and therefore they together it makes a skewed shape. But a few lectures ago, we were told, that the sum of normally distributed random variables produce a new normally distributed normal variable N(sum(means), sum(variances)). Now I am confused. According to this, the distribution of residuals should be N(0+0 = 0, 1+1 = 2). But I don't obtain it. Also, in my example above, it should be N(5+5=10, 1+1=2) rather than N(5, 1). So, in other words, what is the difference between "sum of normally distributed random variables" and "sum of normally distributed data"? In my example above, the response together forms skewness, not normality. And neither test can confirm normality here, regardless of the sample size. But when we ask for "sum of normal random variables" (and my conditional distributions are normal), it should make another normal one with much more dispersion.
