[site]: crossvalidated
[post_id]: 300220
[parent_id]: 
[tags]: 
Why overfitting might not happen?

I am working with a pretty big dataset (800k samples) and I solve a classification problem. What puzzles me is that models (CNN and MLP) with ~3000 and 3000000 parameters have pretty much the same (and decent) performance. The bigger model does not overfit, learning curves (loss and AUC) look similar. What are the possible reasons for that? The problem is too simple?
