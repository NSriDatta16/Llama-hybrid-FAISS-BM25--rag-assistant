[site]: crossvalidated
[post_id]: 591891
[parent_id]: 
[tags]: 
Model comparison between inherently different machine-learning algorithms

For example, how to fairly compare the performance of XGBoost / LightGBM to the performance of a Feed-Forward Neural Network ? Especially regarding (1) Data Preprocessing and (2) Hyperparameter Tuning ? Both steps can differ greatly for the mentioned algorithms - so how to make sure the results aren't biased due to suboptimal data processing? I know that there won't be the one correct answer to this question since the problem is non-trivial. However, are there any best practices that could mitigate the risk to obtain biased results in said model comparison? Does anyone know of research papers that approach this issue in a reasonable way? Are there libraries that try to approach hyperparameter tuning objectively for all sorts of machine-learning models [...]?
