[site]: crossvalidated
[post_id]: 567426
[parent_id]: 
[tags]: 
What's the difference between stacked LSTM and encoder-decoder LSTM

I wanted to learn about encoder-decoder LSTM and after some digging around I get that the first LSTM layer in an encoder-decoder-LSTM outputs its hidden state and then the next LSTM layer uses that hidden state as its initial hidden state, I get this bit but what I don't understand is that what's the difference between this and stacked LSTM in terms of code? the code for a normal stacked LSTM like the one I'm currently using for a time series forecasting problem is: lstm = tf.keras.models.Sequential([ tf.keras.layers.LSTM(16, return_sequences=True), tf.keras.layers.LSTM(24, return_sequences=True), tf.keras.layers.Dense(units=1) ]) now what I dont understand is that what do I change here to make this an encoder decoder? some articles I read added something called a RepeatVector layer however that went be necessary for me since im returning sequences on the first layer right..? I apologise if this is a naive question, im new to DL and LSTMs
