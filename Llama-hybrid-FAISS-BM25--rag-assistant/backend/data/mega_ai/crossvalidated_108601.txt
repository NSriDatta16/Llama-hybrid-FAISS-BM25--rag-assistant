[site]: crossvalidated
[post_id]: 108601
[parent_id]: 108567
[tags]: 
My own thoughts on this aren't very collected, but here is a collection of points I'm aware of that might help. The Bayesian interpretation of AIC is that it is a bias-corrected approximation to the expected log pointwise predictive density, i.e. the out-of-sample prediction error. This interpretation is laid out nicely in Gelman, Hwang, and Vehtari (2013) and also discussed briefly on Gelman's blog . Cross-validation is a different approximation to the same thing. Meanwhile, BIC is an approximation to the " Bayes factor " under a particular prior (explained nicely in Raftery, 1999 ). This is almost the Bayesian analogue of a likelihood ratio. What's interesting about AIC and BIC is that penalized regression also has a Bayesian interpretation, e.g. LASSO is the MAP estimate of Bayesian regression with independent Laplace priors on the coefficients. A bit more info in this previous question and a lot more in Kyung, Gill, Ghosh, and Casella (2010) . This suggests to me that you might get some mileage, or at least a more coherent research design, by thinking and modeling in Bayesian terms. I know this is a bit unusual in a lot of applications like high-dimensional machine learning, and also somewhat removed from the (in my opinion) more interpretable geometric and loss-function interpretations of regularization. At the very least, I rely heavily on the Bayesian interpretation to decide between AIC and BIC and to explain the difference to laymen, non-statistically-oriented co-workers/bosses, etc. I know this doesn't speak much to cross-validation. One nice thing about Bayesian inference is that it produces approximate distributions of your parameters, rather than point estimates. This, I feel, can be used to sidestep the issue of measuring one's uncertainty about prediction error. However, if you're talking about using CV to estimate hyperparameters, e.g. $\lambda$ for LASSO, I again defer to Gelman : selecting a tuning parameter by cross-validation is just a particular implementation of hierarchical Bayes.
