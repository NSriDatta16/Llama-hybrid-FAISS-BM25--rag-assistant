[site]: crossvalidated
[post_id]: 169496
[parent_id]: 
[tags]: 
Prediction for non-negative data using PLS/alternative

I am currently using PLS (the set of predictors are quite highly-dimensional) to predict a particular variable, $age$, and I am using Caret's train implementation using the pls method: modelFit The above method does pretty well at predicting, although it does sometimes predict a negative value for $age$, which is nonsensical. To try to sort this problem, I have tried using $log(age)$ as my dependent variable, then exponentiating to retrieve a non-negative value of $age$. This seems to do ok, but the model does (predictably) exhibit considerable heteroscedasticity, where the error variance increases with $age$. This is because I am training a model on the log scale, and hence a prediction that does reasonably well on this scale will not on the non-logged scale. I am not sure whether this method is optimal; I can't help but think that training on the logged scale, when what I care about is on the non-logged scale is not idea. In particular, are there any machine learning methods that account for this type of dependent variable naturally? Are these preferential? Edit: to be clear, I know of models that handle this problem (such as the Tobit model) in econometrics. What I am after here is whether there are machine learning equivalents that handle this. Essentially, are there modifications to PLS or other dimensionality-reduction techniques that prevent non-negative predictions? Best, Ben
