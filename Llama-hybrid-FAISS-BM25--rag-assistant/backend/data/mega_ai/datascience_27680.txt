[site]: datascience
[post_id]: 27680
[parent_id]: 27672
[tags]: 
Your approach of adding negative examples to your data set, replacing the 10-output softmax final layer with an 11-output softmax, and retraining is certainly valid. You need these additional examples because the network can only learn to classify categories that it has seen before. Retraining the final layer only is a good first attempt because it is the cheapest, but if you don't get good results you can try fine tuning one or more additional layers at the end of the network. All these techniques fall under the topic of Transfer Learning . Regarding the second part of your question about how many negative examples to include - this is known as Class Imbalance Problem in Machine Learning. Whether your model will always predict the negative class if you include many negative examples depends on the data and the model, but it is certainly a possibility. The safest approach would be to undersample your negative examples and include only 7000 to match the frequency of other classes in the training set. This should be a good first try, but if it doesn't work you should consider a couple of questions: What is the expected distribution of my target population? What is the cost of misclassifying each type of example? And then make an informed decision about how to proceed, which may involve either sampling negative examples differently, or adjusting your performance metric to something like AUC (which can guide early-stopping , for example). Additional material: A systematic study of the class imbalance problem in convolutional neural networks
