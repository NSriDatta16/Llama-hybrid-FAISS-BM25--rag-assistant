[site]: crossvalidated
[post_id]: 483637
[parent_id]: 483604
[tags]: 
It's written as a transpose for linear algebraic reasons. Consider the trivial rank-one case $A = uv^T$ , where $u$ and $v$ are, say, unit vectors. This expression tells you that, as a linear transformation, $A$ takes the vector $v$ to $u$ , and the orthogonal complement of $v$ to zero. You can see how the transpose shows up naturally. This is generalized by the SVD, which tells you that any linear transformation is a sum of such rank-one maps, and, what's more, you can arrange for the summands to be orthogonal. Specifically, the decomposition $$ A = U\Sigma V^T = \sum_{i = 1}^k \sigma_i u_i v_i^T $$ says that, for any linear transformation $A$ on $\mathbb{R}^n$ for some $n$ (more generally, any compact operator on separable Hilbert space), you can find orthonormal sets $\{v_i\}$ and $\{u_i\}$ such that $\{v_i\}$ spans $\ker(A)^{\perp}$ . $A$ takes $v_i$ to $\sigma_i u_i$ , for each $i$ . A special case of this is the spectral decomposition for a positive semidefinite matrix $A$ , where $U = V$ and the $u_i$ 's are the eigenvectors of $A$ ---the summands $u_i u_i^T$ are rank-one orthogonal projections. For Hermitian $A$ , $U$ is "almost equal" to $V$ ---if the corresponding eigenvalue is negative, one has to take $u_i = -v_i$ so that $\sigma_i \geq 0$ .
