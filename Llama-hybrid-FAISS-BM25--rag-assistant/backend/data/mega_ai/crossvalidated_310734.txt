[site]: crossvalidated
[post_id]: 310734
[parent_id]: 
[tags]: 
Why is the mini batch gradient descent's cost function graph noisy?

I am doing the Deep Learning Specialization on Coursera , and in one of the videos I came forward to the following graph:- I could not understand the reason why the mini-batch gradient descent's cost function is noisy. Dr. Ng told in the video that the reason for this is that one set might be "easy to train" and the other might be "hard to train". What do these terms mean? And how do they affect the cost function of the mini batch gradient descent?
