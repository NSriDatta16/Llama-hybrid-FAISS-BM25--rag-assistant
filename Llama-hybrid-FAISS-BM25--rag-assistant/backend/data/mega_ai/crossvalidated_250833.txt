[site]: crossvalidated
[post_id]: 250833
[parent_id]: 250823
[tags]: 
One hidden layer is enough to achieve an arbitrarily low training error, as stated by the Universal approximation theorem : In the mathematical theory of artificial neural networks, the universal approximation theorem states that a feed-forward network with a single hidden layer containing a finite number of neurons (i.e., a multilayer perceptron), can approximate continuous functions on compact subsets of Rn, under mild assumptions on the activation function. The theorem thus states that simple neural networks can represent a wide variety of interesting functions when given appropriate parameters; however, it does not touch upon the algorithmic learnability of those parameters.
