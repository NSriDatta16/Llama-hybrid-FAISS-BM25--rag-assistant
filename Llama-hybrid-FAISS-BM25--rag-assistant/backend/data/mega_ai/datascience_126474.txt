[site]: datascience
[post_id]: 126474
[parent_id]: 
[tags]: 
Gradient boosting to forecast just one-step ahead

I'm training a gradient boost algorithm (trying both XGBoost and LightGBM) for cash flow forecasting. I was able to do it well separating my training and test sets using the default separation (80/20) and adding some useful variables after analysing the shap values and the feature importance. I also tuned the hyperparameters after that. The thing is that my true challenge is to predict only one step ahead. My first idea was to leave just one sample in the test set and apply the same algorithm that I trained before, but it's difficult to analyse the metrics in the test set with only one sample, and the best parameters/features also changed. Is there a specific way of running the algorithm when you need to predict just one-step ahead? I was thinking about running the algorithm many times (separating one sample for the test set and then including it in the training after each iteration) and get my metrics/features/hyperparameters from that, but I'm not sure if it will work (or how to do that).
