[site]: crossvalidated
[post_id]: 331448
[parent_id]: 331299
[tags]: 
There are two methods in my mind: (1) The manual way . Since you have $n$ classifiers and their predictions, if you did train-test-split to test the accuracy, you will have a sense of which classifier(s) are doing better and which are doing worse. You can use this accuracy as a weight assignment for your ensemble: more weights to better classifiers, less weights to worse performing classifiers. (2) Stacking . You can create another classifier (I usually choose Logistic Regression) with input being the predictions from your $n$ classifiers, and label being the true label for your training data. This way, this new classifier will determine the weight automatically. It usually gives you a better performance than models without ensembles. You can read more about it here: http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/
