[site]: datascience
[post_id]: 87512
[parent_id]: 87510
[tags]: 
Yes, you can employ different methods. You can employ deep learning models, but you should not train them from scratch. You should employ transfer learning. Due to the fact that your dataset is small, you should utilize a deep learning model that is already trained. Next, you should replace the last layer with another layer that has the same number of neurons as your classes. The connections of this newly added layer should have random weight at first. Finally, you will freeze all the weights except for the newly added layer. In this case, your model will have a nice capacity to learn your data, and it will not overfit it. You may want to see the following links: Pr. Ng's video Another helpful video You can also utilize SVM with soft-margin to have good generalization. About the number of samples, it cannot be said in advance. Moreover, for different tasks, it may be different. By the way, someone who looks at your data can easily figure out you have small dataset.
