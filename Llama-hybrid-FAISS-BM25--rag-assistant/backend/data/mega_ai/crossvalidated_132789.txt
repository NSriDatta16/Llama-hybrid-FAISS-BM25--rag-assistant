[site]: crossvalidated
[post_id]: 132789
[parent_id]: 24330
[tags]: 
I ran 4500 random forests over night with some random parameter-settings: Regression problem Ysignal = x1^2+sin(x2*pi) + x3 * x4 + x5 where any x are sampled independent from a normal distribution, sd=1, mean=1 Ytotal = Ysignal + Yerror where Yerror = rnorm(n.observations,sd=sd(Ysignal))*noise.factor theoretical.explainable.variance"TEV" = var(Ysignal= / var(Ytotal) randomForest.performance = explained.variance(OOB cross-validation) / TEV datasets were sampled from the regression problem and added noise n.obs was a random number between 1000 and 5000 n.extra.dummy.variables between 1 and 20 ntree always 1000 sample_replacement always true mtry is 5 to 25, limited by n.obs noise.factor between 0 and 9 samplesize.ratio a random number between 10% and 100%, the ratio size of each bootstrap all models were trained like rfo = randomForest(x=X, y=Ytotal, ) the randomForest.performance , its ability to explain the highest fraction of the TEV increases in general when samplesize lowers when the TEV is less than 50% and decrease when TEV is higher than 50%. Thus, if your randomForest-modelfit reports e.g. 15% explained variance by OOB-CV, and this is an acceptable model-precision for you, then you can probably tweak the performance a little higher by lowering sampsize to a third of numbers of observations, given ntree > 1000 . Morale : For very noisy data it is better to de-correlate trees than to lower bias by growing maximal size trees.
