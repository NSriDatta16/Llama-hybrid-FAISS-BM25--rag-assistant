[site]: crossvalidated
[post_id]: 408401
[parent_id]: 
[tags]: 
Gradient clipping just before averaging

A typical way of implementing mini batch learning is by calculating the gradients of every element within the mini batch and then average all of these element's gradients to come up with the final gradient. Gradient clipping occurs after we have averaged out all the gradients within the mini batch. My question is, what if we performed gradient clipping on every single gradient in the mini batch, and then average out the clipped gradients. Would this make any sense? From a bias-variance point of view would it be the same as the typical way of doing gradient clipping?
