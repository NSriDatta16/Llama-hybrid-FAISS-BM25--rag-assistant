[site]: crossvalidated
[post_id]: 94618
[parent_id]: 29578
[tags]: 
Python: import numpy as np # @author: jonathanfriedman def jsd(x,y): #Jensen-shannon divergence import warnings warnings.filterwarnings("ignore", category = RuntimeWarning) x = np.array(x) y = np.array(y) d1 = x*np.log2(2*x/(x+y)) d2 = y*np.log2(2*y/(x+y)) d1[np.isnan(d1)] = 0 d2[np.isnan(d2)] = 0 d = 0.5*np.sum(d1+d2) return d jsd(np.array([0.5,0.5,0]),np.array([0,0.1,0.9])) Java: /** * Returns the Jensen-Shannon divergence. */ public static double jensenShannonDivergence(final double[] p1, final double[] p2) { assert (p1.length == p2.length); double[] average = new double[p1.length]; for (int i = 0; i * *Note*: If any value in p2 is 0.0 then the * KL-divergence is infinite . Limin changes it to zero instead of * infinite. */ public static double klDivergence(final double[] p1, final double[] p2) { double klDiv = 0.0; for (int i = 0; i
