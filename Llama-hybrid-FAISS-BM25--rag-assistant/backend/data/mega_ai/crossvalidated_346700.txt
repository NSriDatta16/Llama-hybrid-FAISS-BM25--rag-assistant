[site]: crossvalidated
[post_id]: 346700
[parent_id]: 346599
[tags]: 
By default, most SVM implementations are soft-margin SVM, which allows a point to be within the margin, or even on the wrong side of the decision boundary, even if the data is linearly separable. However, there is a penalty associated with each point which violates the traditional SVM constraints. If we set the coefficient on the penalty to some very large value, we get something closer to hard-margin SVM which does what you'd expect: >>> clf = svm.SVC(kernel='linear', C = 10000.) >>> clf.fit(X, Y) >>> w = clf.coef_[0] >>> b = clf.intercept_[0] >>> print([w.dot(X0[i])+b for i in range(len(X0))]) # negative class [-2.9999999999999987, -0.9999999999999987, -0.9999999999999991, -1.1999999999999988] >>> print([w.dot(X1[i])+b for i in range(len(X1))]) # positive class [5.000000000000002, 1.0000000000000004, 1.0000000000000009] As you can see, the margins are now equal on both sides.
