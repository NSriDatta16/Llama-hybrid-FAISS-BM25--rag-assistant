[site]: crossvalidated
[post_id]: 246758
[parent_id]: 246718
[tags]: 
You could start by defining what is normal and then assess the data from that perspective. E.g. build a model and see if some of the data happen to be outliers in the context of that model. E.g. look at $m$-day moving averages and see if a given day is far away from that. Subject-matter knowledge could be helpful in assessing whether a given deviation from the "normal" behaviour is "large" or "small". Alternatively, you could do unsupervised clustering that would identify groups of similar data points. If time-adjacent data points belong to different clusters, a change might have happened between these time points. You would of course have to think hard how to measure distance between points for this to work well (e.g. you might or might not want to consider series of adjacent points rather than single points, this way including or excluding "history" of a given data point).
