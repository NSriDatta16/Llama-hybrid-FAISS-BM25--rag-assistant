[site]: datascience
[post_id]: 33307
[parent_id]: 33304
[tags]: 
For the problem of an imbalanced dataset, you can look into stratified sampling , or stratified-cross-validation (as mentioned here ). One idea might be to create stratified batches from the data. I would probably make all attempts to get train/val/test splits, because you will otherwise face issues when claiming a final test accuracy, as the model might well have seen your entire dataset. One could imagine splitting to have e.g. 300/9750 (pos/neg) in the training dataset, and during training, you create stratified batches from those 1050 images, so each batch e.g. of 50 images, might contain, 10 positives and 40 negatives. This is still somewhat imbalanced, but you are pushing the balance into a more favourable direction in that the model should be able to learn more efficiently. In medical research it is often the case that there are too few samples (in addition to class imabalnces), and so there is usually a huge effort that goes into data augmentation, which you might also be able to make use of it. Here is some related literature (extremely fresh - edited one week ago!). Here is another approach , whereby the authors (William Fithian & Trevor Hastie) devise a subsampling method, which uses the features of the samples to accept/reject them. They design it for the simplist case (logistic regression), but perhaps it might give you ideas: ... using a pilot estimate to preferentially select examples whose responses are conditionally rare given their features. Something to be especially aware of when using the ideas I mentioned above is overfitting . Cross-validation is probably what can best help you out in this respect.
