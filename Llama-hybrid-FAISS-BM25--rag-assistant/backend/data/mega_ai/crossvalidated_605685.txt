[site]: crossvalidated
[post_id]: 605685
[parent_id]: 605371
[tags]: 
In the original formulation of Goodfellow et al. article , the Generator G is trained to minimize $$\mathbb{E}_{z \sim q(z)} [\log (1-D(G(z))]$$ . This is the formulation which is used to describe the issue in Arjovsky et al, 2017. It is easy to see that if the discriminator is perfect, i.e. $D(G(z)) = 0 \; \forall z$ , the loss is also 0. A better formulation for the generator loss is the non-saturating version described in the 2016 tutorial , which has better convergence properties $$-\mathbb{E}_{z \sim q(z)}[\log D(G(z))]$$ The authors state that this formulation is also present in the original GAN article, but I only found it in (Eq. 13) of the 2016 paper. This is further emphasized in the article Many paths to equilibrium: GANs do not need to decrease a divergence at every step , where the statement of Arjovsky's paper is criticized This figure is used by Arjovsky et al. (2017) to show that a model they call the “traditional GAN” suffers from vanishing gradients in the areas where D(x) is flat. This plot is correct if “traditional GAN” is used to refer to the minimax GAN, but it does not apply to the non-saturating GAN. (Right) A plot of both generator losses from the original GAN paper, as a function of the generator output. Even when the model distribution is highly separated from the data distribution, non-saturating GANs are able to bring the model distribution closer to the data distribution because the loss function has strong gradient when the generator samples are far from the data samples, even when the discriminator itself has nearly zero gradient While it is true that the $\frac{1}{2}\log(1-D(x))$ loss has a vanishing gradient on the right half of the plot, the original GAN paper instead recommends implementing $-\frac{1}{2}\log D(x)$ . This latter, recommended loss function has a vanishing gradient only on the left side of the plot. It makes sense for the gradient to vanish on the left because generator samples in that area have already reached the area where data samples lie. As a perfect D would predict all images to be 0 and the loss would be computed by comparing its answers to predicting all 1s, we'd have a high loss. In the formulations I provided the Generator learns only with the Discriminator 1s (i.e the generator classifies a generated image as real). So in this case the loss would be zero in both formulations.
