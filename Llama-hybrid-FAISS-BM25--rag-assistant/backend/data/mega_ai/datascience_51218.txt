[site]: datascience
[post_id]: 51218
[parent_id]: 51215
[tags]: 
It comes down to overfitting as you scale. Decision trees tend to overfit as they grow deep. After every split there will be fewer and fewer samples for the next split to work with. Fewer samples means that risk of splitting on noise increases. Random forest avoids the overfitting problem of decision trees by instead scaling by adding more trees instead of building one big tree. Averaging the outputs of the trees in the forest means that it does not matter as much if the individual trees are overfitting. Regarding your update. No, they will not score the same. Random forest will not have just one decision tree. It has several and divides the features into random subsets for each tree to be trained on. So even if the size of the decision trees in random forest would be the same as a single decision tree, the features they are trained on would not be. But if you ask what happens if we take a random forest, only use one tree and train it on the same features as a single decision tree of the same size, then yes they would be one and the same.
