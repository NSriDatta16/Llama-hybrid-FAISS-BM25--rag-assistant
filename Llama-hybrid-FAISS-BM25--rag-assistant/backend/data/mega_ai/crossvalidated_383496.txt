[site]: crossvalidated
[post_id]: 383496
[parent_id]: 
[tags]: 
CNN can't learn - "oscillating" loss function and classifier that marks every sample with same label

I would be really thankful for any hint as I have no idea what to do next. I am trying to create a working convolutional neural network for an image classification task. There is a 96x96 RGB image as an input and the net is supposed to determine one of three classes. The cross entropy loss and Adam model are used. The problem is that my classifier labels all the samples with one class. The loss function also doesn't look good and I can't find a way to fix this. On the graph it looks almost like a periodic function. Take a look at the example values during the training: This is my PyTorch CNN: class Network(nn.Module): def __init__(self): super(Network, self).__init__() self.conv_seq1 = nn.Sequential( nn.Conv2d(3, 8, 3, stride=1, padding=1), nn.ReLU() ) self.conv_seq2 = nn.Sequential( nn.Conv2d(8, 16, 3, stride=1, padding=1), nn.ReLU() ) self.conv_seq3 = nn.Sequential( nn.Conv2d(16, 32, 3, stride=1, padding=1), nn.ReLU(), nn.MaxPool2d(2, stride=2) ) n = 32 * 48 * 48 self.linear_seq = nn.Sequential( nn.Linear(n, int(n / 64)), nn.Linear(int(n / 64), 3) ) def forward(self, data): output = self.conv_seq1(data) output = self.conv_seq2(output) output = self.conv_seq3(output) output = output.reshape(output.size(0), -1) output = self.linear_seq(output) return output What I've tried: Increasing and decreasing learning rate value Using small and big datasets (from 60 to 7500 samples) Adding weights to the classes (but I did it kinda blindly just to check) Using minibatches of different sizes And yet nothing helped. Is there anything more I could do?
