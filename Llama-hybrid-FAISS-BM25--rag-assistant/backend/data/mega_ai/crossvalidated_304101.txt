[site]: crossvalidated
[post_id]: 304101
[parent_id]: 303735
[tags]: 
I found one paper that uses the JS-Divergence of multiple distributions to estimate the hardness of a query (in the area of Information Retrieval). The paper can be found here and they themselves refer a paper called "Divergence measures based on the shannon entropy (1991)". They also give a little bit different mathematical expression for it. As for the interpretation, they explain it as follows: Given a set of distributions thus obtained, we employ the well known Jensen-Shannon divergence [8] to measure the diversity of the distributions corresponding [...] So the Jensen-Shannon divergence can be seen to measure the overall diversity between all the probability distributions. As for the Python code, I couldn't find any package that implements the JSD for more than two distributions. But there is already one quite straightforward code example on crossvalidated (see here ) .
