[site]: datascience
[post_id]: 15966
[parent_id]: 15358
[tags]: 
Like one of the commenters, I also question the utility of these scatterplots in this situation. What you've described is a standard multi-class classification problem . Your outcomes are labeled (clubs, spades, etc.), and you have seven features (f2-f8) with which to predict the outcomes. So, simply try putting all of the features into a standard classification model (e.g., logistic regression), and see what comes out. If you believe that some features are unimportant, you can eliminate those features from the model, and examine the cross-validation score (accuracy, f1-score, etc.) to see if the feature was truly predictive or not. For a linear model, looking at the coefficients and their standard errors can also be useful in this regard. Another approach for feature selection for generalized linear models is to examine the deviance . Hill/Gelman write in their book (Data Analysis using Regression and Multilevel/Hierarchical Models) that "when an informative predictor is added to a model, we expect the deviance to decrease by more than 1. When $k$ predictions are added to a model, we expect the deviance to decrease by more than $k$". What you should not do is to examine scatter plots to determine which features are important. The fundamental problem is that even though the data might be well-separated in seven dimensions (i.e., using all seven features), this separation might not show up well in a scatter plot since this plot is only two-dimensional.
