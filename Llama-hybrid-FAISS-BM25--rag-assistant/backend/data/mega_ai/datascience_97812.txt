[site]: datascience
[post_id]: 97812
[parent_id]: 97780
[tags]: 
I remember reading long, long ago that neural networks can in theory (assuming infinitely large network) reproduce any algorithm - therefore it stands to reason that a theoretical neural network could replicate a hash function. This is the universal approximation theorem and it does apply to learning hash functions. However, the theorem says nothing about generalisation, and that is a major problem when learning any pseudo-random functions. Any hash function which has the following properties: Output is hard or impossible to reverse back to any original input, output is highly sensitive to input, such that a single bit changing in the input will (pseudo-randomly) change all bits in the output. will be impossible to approximate and generalise using statistical techniques. Cryptographic hashing algorithms have these as a design goal, and attempt to meet the ideal of a random oracle . If you could train a neural network to generalise better than 50% accuracy (bit for bit, and measured over a suitably large test data set to establish significance) on unseen inputs, that would be an indication that the hash was broken for cryptographic use. The chances are that you would not be able to achieve this even for known broken hashes such as MD5. If we limit our scope to looking at cryptographic hashes only and assume we can train using input/output pairs is it reasonable to train a network for this? You are very unlikely to create any useful reusable model doing this. It may still be a worthwhile learning exercise for practicing machine learning, with the main benefit being it is very easy to put together training and test datasets. Your expectation of not being able to find anything could be used as a test of statistical knowledge, and for bug hunting. Your first suspicion if you get positive results would be to check your code and assumptions. However, there are other learning scenarios where you have a higher expectation of making a useful model, and those are IMO going to be more rewarding.
