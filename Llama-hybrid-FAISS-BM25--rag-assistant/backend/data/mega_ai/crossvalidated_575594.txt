[site]: crossvalidated
[post_id]: 575594
[parent_id]: 575380
[tags]: 
I feel a bit bad by overcrowding the comments section with pedantic notes and questions about the origins of the noise in the data. So I will make up for it with an answer that is a bit more decent than those little comments. This answer will demonstrate how a straightforward (non linear) least squares fit (which can also be done with glm*) will be overestimating the significance of the difference between the two compounds. The reason that this overestimation happens is because the noise is not homogeneous whereas the model that is used to compute the p-value assumes that all points have the same noise. Effectively it is mostly the few points around the midpoint that make a large difference between models. Consequently, the effective degrees of freedom is much less than what the computations assume. This makes the permutation test from the answer by @jbowman a more robust method. The problem of the permutation test is that you need a sufficient amount of relevant data points in order for the test to be powerful. In addition the permutation test is not immune to the situation where the datapoints are correlated due to particular experimental procedures that make the errors not independent. Some other method like using a Bayesian model that models the entire process and incorporates all errors could be better. The bottomline is that there is no single solution that can be applied to al these type of curves. You can not give a set of data to a statistician along with a function and no other information. It is important to know where the data comes from and how it has been generated. This needs to be incorporated into the statistical model. In this example we generate data points, not by adding noise to the curve, but by changing the parameters for each data point. $$a \sim \text{Unif}(4,6) \qquad b = 3 \\ y = 1- \frac{1}{1+(x/a)^b}$$ then we would get a plot like this for generating two times 80 datapoints. The variability in the curve is not like noise added to some 'true' value but but is because of variations in the curves themselves being different each time. This could correspond with variations in the (bacterial?) cells that are used. Each of them is different and may behave according to a different coefficient $a$ and $b$ . The consequence is that the noise is not homogeneous. It is mostly around the middle $x=5$ where the largest variation occurs. If we repeat this simulation $10^4$ times and compute the p-values with a glm model or non-linear least squares fit, then the distribution of p-values look like this So you do not get a homogeneous distribution of p-values and the models overestimate the probability that a certain discrepancy occurs. The reason is because the variation mostly occurs in a few point whereas the computation of the p-value assumes that it occurs evenly in all points. This overestimates the degrees of freedom. Especially the inclusion of the point $x=0$ is completely useless because a change in the coefficients has no effect on the value of the outcome. Sample code: sigmoid = function(x,a=5,b=3) { 1-(1+(x/a)^b)^-1 } simulate_test = function(plot = TRUE) { ### create data x = rep(0:15,10) y = sigmoid(x, a = runif(length(x),4,6)) compound = c(rep(0,16*5), rep(1,16*5)) ### make a plot if desired if (plot == TRUE) { plot(x, y, pch = 21, col = 1, bg = 0 + compound * 2, cex = 0.7) } ### Perform fitting ### ### we can do the fitting with nls but the lines below shows that glm works as well ### For glm points with x=0 need to be removed because we use log(x), but these do not add information anyway modnls = nls(y ~ sigmoid(x,a+c*compound,b), start = c(a=5,b=3,c=0), control = nls.control(minFactor = 10^-4,warnOnly = TRUE)) modglm = glm(y[-which(x==0)] ~ log(x[-which(x==0)]) + compound[-which(x==0)], family = gaussian(link = "logit")) #lines below demonstrate how you would get the (same) coefficients with the two methods glm vs nls #coefficients(modnls) #coefficients(modglm) #exp(-modglm $coefficients[1]/modglm$ coefficients[2]) #exp((-modglm $coefficients[1]-modglm$ coefficients[3])/modglm $coefficients[2])-exp(-modglm$ coefficients[1]/modglm$coefficients[2]) return(list(nls_p = summary(modnls) $coefficients[3,4], glm_p = summary(modglm)$ coefficients[3,4])) } set.seed(1) x = replicate(10^4, as.numeric(simulate_test(plot = FALSE))) x = list(nls_p = x[1,], glm_p = x[2,]) hist(x $glm_p, breaks = seq(0,1,0.01)) hist(x$ nls_p, breaks = seq(0,1,0.01)) simulate_test(plot = TRUE) *The fit can also be done with with a generalized linear model because we can find a link function such that the converted values are a linear function of the regressors: $\log \left(\frac{y}{1-y}\right) = b \log(x) - b \log(a)$ . This is demonstrated in the code. There is a slight difference between the coefficients and estimates because of computational errors, and of p-values because the glm model has to use $\log(x)$ and exclude the value $x=0$ . But these values wit $x=0$ add no information anyway and make the glm model actually more precise than the nls model.
