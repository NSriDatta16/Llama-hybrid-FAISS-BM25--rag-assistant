[site]: crossvalidated
[post_id]: 559332
[parent_id]: 559314
[tags]: 
Seasonal differencing needs to be motivated rather than taken for granted. It is only relevant under special circumstances, namely, when the time is seasonally integrated. (This leads to some fairly unintuitive behavior such as trajectories of the different seasons diverging from each other. Try simulating from a SARIMA(0,0,0)(0,1,0) process and you will see for yourself. This suggests seasonal integration may not be a very common phenomenon in reality, despite the suggestions of many time series manuals that should know better...) In all other cases, it leads to overdifferencing. SARIMA without seasonal differencing can be relevant when there is autocorrelation at the seasonal frequency and its integer multiples, as then the process can be approximated by a SARIMA without seasonal differencing. I still not understand at which point it should be recognized (despite the clear signs of seasonality like ACF spikes) that no seasonal differencing is required, as it turned out to be the very first step based on several ARIMA manuals. So, reworded, my question is how SARIMA is able to cope with seasonality without seasonal differencing tag. and how can it be distinguished if "time is seasonally integrated" or "there is autocorrelation at the seasonal frequency". Until now I thought that two are quite the same. The ACF at seasonal frequencies declines very slowly for a seasonally integrated process. It declines faster for those that can be approximated by SAR and SMA without seasonal unit roots. Again, you can simulate from SARIMA(0,0,0)(0,1,0) vs. SARIMA(0,0,0)(p,0,0) for several values of $p$ vs. SARIMA(0,0,0)(0,0,1) for several values of $q$ to see the difference in the patterns.
