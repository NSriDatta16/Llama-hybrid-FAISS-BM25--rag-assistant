[site]: crossvalidated
[post_id]: 522617
[parent_id]: 
[tags]: 
Bayesian comparison for two linear models with same response variable but different predictors?

I ran several separate linear models with the same response $y$ , but with $n$ different predictors, i.e.: $y = \alpha + \beta x_0$ , $y = \alpha + \beta x_1$ , ..., $y = \alpha + \beta x_n$ . All parameters were specified the same priors across models. I ran all these models in a Bayesian setting and have the joint posterior distribution. I would like to somehow select between choosing either $x_0$ , $x_1$ , ... or $x_n$ as the "best" predictor for $y$ . I guess that the best way would be to do Bayesian variable selection, but this is just a small part of a much larger and complex model which makes it hard to run several variables at once (or perform variable selection). Bayes factors would be also hard to implement because of the computation limitations in the actual application. Could I compare somehow the resulting posterior probabilities as a measure of "goodness of fit" (analogous to AIC or so in a likelihood setting?). Thanks!
