[site]: crossvalidated
[post_id]: 298803
[parent_id]: 292317
[tags]: 
EDIT: Apparently tensorflow has a "Lazy Adam Optimizer" that only updates the gradient for variables whose indices appear in the current batch. Lazy_Adam_Optimizer This may be a good idea for very sparse data like language models. Otherwise, here is my original response Original: In a general case you do not know which parts of the input, if any, are sparse. So assuming they are not makes the algorithm more generally applicable. Furthermore, the momentum serves the purpose of "remembering" previous gradients. Since stochastic gradient descent trains on different examples at each step the momentum helps smooth out the updates. So if an input is only rarely present than the momentum will help update the weights corresponding to it more often. Otherwise these weights would take a lot longer to converge.
