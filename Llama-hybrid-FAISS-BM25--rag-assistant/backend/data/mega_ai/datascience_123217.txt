[site]: datascience
[post_id]: 123217
[parent_id]: 123194
[tags]: 
One approach that could work in this scenario is unsupervised translation. You can train your model without direct translations, which is especially beneficial for languages with limited available translation resources, such as conlangs. You can find one very interesting post about the subject here and the accompanying implementation here . Edit: According to the researchers: • Their MT system creates a bilingual dictionary by generating word embeddings for every word in each language, capturing semantic structures and context. • Since word embeddings across different languages share similar structure, the system can align them through rotation, enabling basic word-by-word translation. • For translating sentences, this basic translation is enhanced by local edits using a language model trained on monolingual data. This is further refined using a technique called back translation.
