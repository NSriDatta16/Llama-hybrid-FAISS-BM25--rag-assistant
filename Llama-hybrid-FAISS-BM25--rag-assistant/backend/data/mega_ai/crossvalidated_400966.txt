[site]: crossvalidated
[post_id]: 400966
[parent_id]: 
[tags]: 
Equation 10.6 in Bishop book

This is referring to equation 10.6 in Pattern Recognition and Machine Learning by Bishop: $$ L(q) = \int \prod_{i}q_{i} \left[\ln p(X,Z) - \sum \ln q_{i}\right] dZ $$ $$ =\int q_{j}\left[\int \ln p(X,Z) \prod_{i\neq j}q_{i} dZ_{i}\right] dZ_{j} - \int q_{j}\ln(q_{j})dZ_{j} + const $$ $$ =\int q_j \ln\tilde{p}(X,Z_{j})dZ_{j} - \int q_{j} \ln(q_{j}) dZ_{j} + const $$ where $\ln(\tilde{p}(X,Z_{j}) = \mathbb{E}_{i \neq j}[\ln(p(X,Z)] + const$ where the expectation is with respect to the q distributions over all variables $z_{i}$ for $i \neq j$ Since $\prod_{i\neq j}q_{i}$ multiplies both the terms within { } in the first part of equation 10.6 shouldn't the last equation of 10.6 read as follows? $$\int q_j\ln\tilde{p}(X,Z_{j})dZ_{j} - (\prod_{i\neq j}q_{j}) \int q_{j} \ln(q_{j}) dZ_{j} + const$$ This would then no longer be the negative KL divergence between $q_{j}(Z_{j})$ and $\tilde{p}(X,Z_{j})$ as is claimed later in the text. What am i missing?
