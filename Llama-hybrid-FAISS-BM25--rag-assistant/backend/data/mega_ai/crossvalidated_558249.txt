[site]: crossvalidated
[post_id]: 558249
[parent_id]: 
[tags]: 
how to prove if central limit theorem would apply to a process?

I'm very new to stats so apologies in advance for not using the right language and terms. I have just learned about the central limits thm can show up in places other than taking a sample distribution of mean, but in cases where a large number of factors all contribute to an outcome. An example is the chest-size measurement: https://www.britannica.com/science/central-limit-theorem To see this for myself, I wrote some "arbitrary" function where a large number of random factors contribute (deterministically) to an outcome, code below: def sample_number(): N = 1234 As = np.random.rand(N) Bs = np.random.rand(N) ret = 0 for i in range(N): if As[i] > 0.1: ret += Bs[i] / As[i] else: ret += np.log(0.01 + As[i] ** Bs[i]) return ret lo and behold, as N becomes bigger, the distribution of the random variable "sample_number" looks more and more like a normal distribution once plotted. with N = 4 with N = 400 however this really isn't a proof, so I'm looking for if there's some general statement one can make about these kinds of problems, i.e. some function class (polynomials of finite degrees, or neural networks) mapping N random variables to a single numerical outcome, in such a way that it "uses" all the information of these N random variables (perhaps we can demand that the outcome has nonzero mutual information to any individual r.v.) will result in a normal distribution as N goes to infinity. the closest thread is this one: Does the central limit theorem apply to these probability density functions? , but I am not sure if the same analysis will work for the code I have wrote, as I purposefully made it annoying to manipulate analytically. thanks in advance.
