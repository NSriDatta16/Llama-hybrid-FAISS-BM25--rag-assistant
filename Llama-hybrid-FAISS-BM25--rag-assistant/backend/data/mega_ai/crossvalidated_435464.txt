[site]: crossvalidated
[post_id]: 435464
[parent_id]: 435452
[tags]: 
If X is n by p matrix, then the absolutely standard interpetation is that one has n observations of p variables. Therefore $\Sigma V^*$ is also a nxp matrix, that is what one has done is created new variables that are linear combinations of the previous variables. That makes good sense. If I have two variables x,y, then $x+y$ and $x-y$ also make sense as variables. What you suggest $U\Sigma$ , is taking linear combinations of the observations. One can do it, but for what purpose. The point of SVD is to find a new basis for the set of variables one is observing that is 1) Orthogonal and 2) listed in 'order of importance' as measured by the diagonal entries of $\Sigma$ . The usual reason for doing so is to throw away variables that are less important. In many situations it may turn out to be the case that a small number of variables in the new basis capture most of the information in the data as measured by variance. Two examples of this come to mind right away. mp3 encoding of music is exactly that. Variables are frequencies measured and the observations are the values at various points in time. Similarly if one represents a picture (black and white for simplicity), the the picture can be thought of a matrix where each pixel is a matrix entry between 0 and 1 where 0 = white and 1 = black. Then pca gives one a way to capture the picture with less information. Google is your friend for more details on either topic. Oh and a third. If one has time series data of various stocks, the doing a pca will almost always produce a situation in which the first principle component is about 60%-80% of the variability and 3-5 components will capture 95% or more of the variance. Incidentally the first component is easily seen to be market direction.
