[site]: crossvalidated
[post_id]: 87838
[parent_id]: 
[tags]: 
How to set the step size for stochastic gradient descent such that its provable it will converge

Recall stochastic gradient descent (for regression): $\theta = 0 $ $ \text{Randomly select } t \in [1,n]\{\\ \quad \theta^{(k+1)} = \theta^{k} + \eta_{k}(y^{(t)} - \theta \cdot x^{(t)})x^{(t)}\\ \}$ I was following some notes and it said that to have stochastic gradient descent converge it would be sufficient to set the learning rate to: $$\eta_{k} = \frac{1}{k+1}$$ I normally post my attempts to a question, but I honestly don't know how to prove that setting that value will make stochastic gradient descent converge. If someone knows of a prove it would be awesome! Also an answer that contains a proof, even if its for some special case, say "convex function" or in the case of linear regression or whatever the case is, but that provides some insight (or a proof) why this result holds for different setting of stochastic gradient descent, would be good answers! I am aware that finding global minimum solutions is really hard anyway, so restricting your answer should be ok if it contains a good proof/argument and/or if it contains an example in machine learning. Also, if you do not remember the proof but instead maybe have a good intuition on why that value is a good for stochastic gradient descent, that would also be a very useful response for me! :) Thanks in advance!
