[site]: crossvalidated
[post_id]: 584368
[parent_id]: 583941
[tags]: 
The multilingual pre-trained models are in principle capable of so-called zero-shot language transfer, i.e., after being fine-tuned in one language, it should work in other languages covered by the representation as well. This, however, depends largely on the task: transfer in sentiment analysis seems to be easy, for more challenging tasks such as hate speech detection or fact-checking , the results are still very poor (in mid-2022). When you have at least some data for Arabic, it is always better to use them. If you have unlabeled Arabic data for the task, you can use some data augmentation techniques . BLOOM is a generative model and it is huge. If you are interested in closed-class categorization (rather than a generation), you should consider using encoder-only models such as mBERT , XLM-R , or RemBERT , which are considerably smaller are suitable for classification.
