[site]: datascience
[post_id]: 120708
[parent_id]: 
[tags]: 
Calculating the importance metric in random forest: Why don't we remove the variable instead of permutating its values?

The importance metric in random forests is a way to determine the significance of a predictor variable in a model. It does this by randomly permutating the values of one predictor variable at a time and observing how much it affects the model's performance. Instead, why don't we remove the variable , retrain the model and measure the effect? I am guessing it may be because removing the variable from the model reduces its "complexity" (measured by the number of predictors) and this may have an effect on its performance. But I am not sure if this is the reason, or if there are other reasons besides this one.
