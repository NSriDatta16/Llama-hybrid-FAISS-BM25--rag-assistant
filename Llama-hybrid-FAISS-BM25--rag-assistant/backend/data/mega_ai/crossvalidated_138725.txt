[site]: crossvalidated
[post_id]: 138725
[parent_id]: 
[tags]: 
How to pass the sequential var. length data for the NN?

The main task is from Inductive Logic Programming (ILP) area. The task related to ANN is inspired by paper below but is applied to more complex case. Learning an approximation to clause evaluation function The main task is the search of the best theory (hypothesis) that subsumes bottom theory (most specific hypothesis), that covers as many positive examples as possible, and covers as few negative examples as possible and has minimal description length. The input data consist from the several records. The records represent the set of clauses (theory/hypothesis). Every record represents the single clause which is the set of literals (possible empty). The clause is encoded via bit vector where every bit indicates if the literal is dropped or active. Bottom clause's bit vector is filled with 1's. A record contain numeric vector is #'s of occurrences of a literal in the clause. The vector is associated with bit vector. A record's length (i.e. bit vector and numeric vector length) may vary. Every record contain also the fixed number of numeric values whose representation is trivial. The output is 2 numbers. Their values (both in the range [0.0, 1.0]) are coverages for positive and negative examples. So, my question is: How to represent/normalize these records for the NN? In particular, is there a (tricky) way to represent the bit vector compactly? Its length can be up to a few thousands. Also, I'd like to mention about learning mode(s). The NN may learn fitness function approximation in 'online' mode from real data, i.e. in parallel with main searching. In this mode all the training samples have the same clauses length schema. It may learn in 'offline' mode from abstract (generated) datasets, and, finally, in both modes. It's unclear for me is it necessary to consider 2nd and 3rd cases (I think they are significantly complicated input data format to the NN).
