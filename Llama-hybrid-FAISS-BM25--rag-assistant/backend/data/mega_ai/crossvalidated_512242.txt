[site]: crossvalidated
[post_id]: 512242
[parent_id]: 
[tags]: 
Why does transformer has such a complex architecture?

In this pretty neat article https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html I saw the claim that one of the reasons of transformer architecture to be popular is the fact that it's reading all input at once as O(1) (obviously amount of work is M x M x d where M x d is input size) but according picture only Encoder part can do so. The Decoder is clearly working in a recurrent way reading its own output from previous steps, so its overall amount of work is actually O( M x M x M x d ). So Decoder part make much more work (by factor of M ) than Encoder. Transformer basically (if we forget its blowing complexity as argument of input length) is RNN model with build-in attention. Or I'm missing something? Also, please explain why does Transformer have so complex architecture? Can the Transformer Encoder part do the same thing in case of the output has the same size as the input?
