[site]: crossvalidated
[post_id]: 24713
[parent_id]: 24300
[tags]: 
The question has many valid interpretations. The comments--especially the one indicating permutations of 15 or more elements are needed (15! = 1307674368000 is getting big)--suggest that what is wanted is a relatively small random sample, without replacement, of all n! = n*(n-1)*(n-2)*...*2*1 permutations of 1:n. If this is true, there exist (somewhat) efficient solutions. The following function, rperm , accepts two arguments n (the size of the permutations to sample) and m (the number of permutations of size n to draw). If m approaches or exceeds n!, the function will take a long time and return many NA values: it is intended for use when n is relatively big (say, 8 or more) and m is much smaller than n!. It works by caching a string representation of the permutations found so far and then generating new permutations (randomly) until a new one is found. It exploits R's associative list-indexing ability to search the list of previously-found permutations quickly. rperm 1000) { # 1000 is arbitrary; adjust to taste p The nature of replicate is to return the permutations as column vectors; e.g. , the following reproduces an example in the original question, transposed : > set.seed(17) > rperm(6, size=4) [,1] [,2] [,3] [,4] [,5] [,6] [1,] 1 2 4 4 3 4 [2,] 3 4 1 3 1 2 [3,] 4 1 3 2 2 3 [4,] 2 3 2 1 4 1 Timings are excellent for small to moderate values of m, up to about 10,000, but degrade for larger problems. For example, a sample of m = 10,000 permutations of n = 1000 elements (a matrix of 10 million values) was obtained in 10 seconds; a sample of m = 20,000 permutations of n = 20 elements required 11 seconds, even though the output (a matrix of 400,000 entries) was much smaller; and computing sample of m = 100,000 permutations of n = 20 elements was aborted after 260 seconds (I didn't have the patience to wait for completion). This scaling problem appears to be related to scaling inefficiencies in R's associative addressing. One can work around it by generating samples in groups of, say, 1000 or so, then combining those samples into a large sample and removing duplicates. R experts might be able to suggest more efficient solutions or better workarounds. Edit We can achieve near linear asymptotic performance by breaking the cache into a hierarchy of two caches, so that R never has to search through a large list. Conceptually (although not as implemented), create an array indexed by the first $k$ elements of a permutation. Entries in this array are lists of all permutations sharing those first $k$ elements. To check whether a permutation has been seen, use its first $k$ elements to find its entry in the cache and then search for that permutation within that entry. We can choose $k$ to balance the expected sizes of all the lists. The actual implementation does not use a $k$ -fold array, which would be hard to program in sufficient generality, but instead uses another list. Here are some elapsed times in seconds for a range of permutation sizes and numbers of distinct permutations requested: Number Size=10 Size=15 Size=1000 size=10000 size=100000 10 0.00 0.00 0.02 0.08 1.03 100 0.01 0.01 0.07 0.64 8.36 1000 0.08 0.09 0.68 6.38 10000 0.83 0.87 7.04 65.74 100000 11.77 10.51 69.33 1000000 195.5 125.5 (The apparently anomalous speedup from size=10 to size=15 is because the first level of the cache is larger for size=15, reducing the average number of entries in the second-level lists, thereby speeding up R's associative search. At some cost in RAM, execution could be made faster by increasing the upper-level cache size. Just increasing k.head by 1 (which multiplies the upper-level size by 10) sped up rperm(100000, size=10) from 11.77 seconds to 8.72 seconds, for instance. Making the upper-level cache 10 times bigger yet achieved no appreciable gain, clocking at 8.51 seconds.) Except for the case of 1,000,000 unique permutations of 10 elements (a substantial portion of all 10! = about 3.63 million such permutations), practically no collisions were ever detected. In this exceptional case, there were 169,301 collisions, but no complete failures (one million unique permutations were in fact obtained). Note that with large permutation sizes (greater than 20 or so), the chance of obtaining two identical permutations even in a sample as large as 1,000,000,000 is vanishingly small. Thus, this solution is applicable primarily in situations where (a) large numbers of unique permutations of (b) between $n=5$ and $n=15$ or so elements are to be generated but even so, (c) substantially fewer than all $n!$ permutations are needed. Working code follows. rperm max.failures) { p
