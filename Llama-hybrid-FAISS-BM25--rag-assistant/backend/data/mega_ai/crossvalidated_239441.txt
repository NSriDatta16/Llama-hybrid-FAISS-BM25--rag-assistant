[site]: crossvalidated
[post_id]: 239441
[parent_id]: 239076
[tags]: 
I think there are a couple of things confusing you, so first things first. Given a signal $x[n]$, and a kernel (also called a filter) $h[n]$, then the convolution of $x[n]$ with $h[n]$ is written as $y[n] = (x \star h)[n]$, and is computed via a sliding dot-product, mathematically given by: $$ y[n] = \sum_{m=-\infty}^{\infty}x[m] \ h[n-m] $$ The above if for one-dimensional signals, but the same can be said for images, which are just two-dimensional signals. In that case, the equation becomes: $$ I_{new}[r,c] = \sum_{u=-\infty}^{\infty}\sum_{v=-\infty}^{\infty} I_{old}[u,v] \ k[r-u, c-v] $$ Pictorially, this is what is happening: At any rate, the thing to keep in mind, is that the kernel , in actually learned during training a Deep Neural Network (DNN). A kernel is just going to be what you convolve your input with. The DNN will learn the kernel, such that it brings out certain facets of the image (or previous image), that are going to be good for lowering the loss of your target objective. This is the first crucial point to understand: Traditionally people have designed kernels, but in Deep Learning, we let the network decide what the best kernel should be. The one thing we do specify however, is the kernel dimensions. (This is called a hyperparameter, for example, 5x5, or 3x3, etc).
