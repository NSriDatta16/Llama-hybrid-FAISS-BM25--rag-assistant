[site]: datascience
[post_id]: 46559
[parent_id]: 
[tags]: 
Why do recurrent layers work better than simple feed-forward networks?

On a time series problem that we try to solve using RNNs , the input usually has the shape $input features \times timesteps \times batchsize$ and we then feed this input into recurrent layers. An alternative would be to flatten the data so that the shape is $(input features \times timesteps) \times batchsize$ and use a fully connected layer for our time series task. This would clearly work and our dense network would be able to find dependencies between the data at different timesteps as well. So what is it that makes recurrent layers more powerful? I would be very thankful for an intuitive explanation.
