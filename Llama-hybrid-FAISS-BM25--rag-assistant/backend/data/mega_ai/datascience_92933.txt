[site]: datascience
[post_id]: 92933
[parent_id]: 92856
[tags]: 
I should add, since you mention FastText, that FastText uses subword information to build its word vectors. Subword information is not tied to any specific word and can therefore be used to create vectors for OOV or rare words (the authors of the FastText algorithm specifically mention the ability to cater to rare word vectors not encountered). BERT, GPT,etc are the latest set of pre-trained models that have refined the vectors for their words by basically studying the words across very large and very numerous contexts e.g sentences, documents, etc in which the word could possibly be found and applying specialized attention mechanisms (transformers) to expand the context of the word as much as possible to refine the embedding of the word (BERT looks at upto 512 tokens at once in the context to determine the vector representation). BERT also uses subword information / a sentencepiece byte pair encoding algorithm to cater to OOV words. BERT is especially meant to be used in a way that you can lift the embedding from pre-trained models for the words in the corpus you are studying and use those for your NLP tasks. These pre-trained embeddings are very powerful and have typically taken days, if not weeks, to fine tune across very large contexts. If you take a look at , for example a toolkit like HuggingFace ( https://huggingface.co/ ), they have a lot of models with embeddings catered for specific tasks e.g one set for classification, one set for sequence labeling, etc.
