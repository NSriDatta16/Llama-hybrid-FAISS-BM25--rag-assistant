[site]: crossvalidated
[post_id]: 521746
[parent_id]: 
[tags]: 
Data augmentation for traditional machine learning algorithms

Data augmentation suffices multiple purposes, I would list a few here: Increasing dataset size: The data is just fragment/stand-in trying to represent reality, having more data should thus result in a better representation of it Effective against overfitting: If the sample space becomes more complex, then the features also do Algorithms are stupid, can't deal with symmetries: If the classifier can recognize a cat in the top left, but not in the bottom right corner of an image, then the classifier is called stupid. Unfortunately lots of these exists and one way counteracting this is by augmenting the data, so that they can deal with that. I am now mostly interested in this 3rd point as in Deep Learning as some layers were particularly created to deal with these symmetries. E.g. CNNs are translation equivariant, GNNs are combinatorical equivariant and so on. However, how do traditional Machine Learning algorithms fare here? In particular I am interested in, if there has also been an evolution of algorithms like the SVMs, Random Forrests etc. to deal with symmetries. My impression is that most data is pre-processed in a way to get rid of this symmetry problem and simply the Random Forrest already highly abstracted features. But of course, I can be wrong here. Thus I was wondering, why does this topic not seem to pop up for traditional methods as much as for Deep Learning? Also: Have there been any traditional meta-learning algorithms to discover and automatically deal with symmetries similar to deep learning algorithms?
