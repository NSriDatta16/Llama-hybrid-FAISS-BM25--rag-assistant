[site]: crossvalidated
[post_id]: 535473
[parent_id]: 
[tags]: 
log trick on message passing in factor graphs

I'm reading Barbers book on Bayesian reasoning and Machine learning http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/200620.pdf page 90 To give context this is a proof of using the log trick for the Factor graph messages. I have two questions 1. Is the derivation incorrect? Using the definition that $e^{\lambda_{f \rightarrow x}(x)} = \mu_{f \rightarrow x}(x)$ . Substituting into the top equation we get $$e^{\lambda_{f \rightarrow x}(x)} = \sum_{\mathcal{X_f} \setminus x} \phi_{f}(\mathcal{X}_f) \prod_{y \in ne(f) \setminus x} e^{\lambda_{y \rightarrow f}(y)}$$ Now let $n = |ne(f) \setminus x|$ We can write the above as $$e^{\lambda_{f \rightarrow x}(x)} = \sum_{\mathcal{X_f} \setminus x} \phi_{f}(\mathcal{X}_f) e^{\sum_{y \in ne(f) \setminus x} \lambda_{y \rightarrow f}(y)}$$ Which is equivalent to $$ e^{\lambda_{f \rightarrow x}(x)} = \sum_{\mathcal{X_f} \setminus x} \phi_{f}(\mathcal{X}_f) e^{n \lambda^{*}_{y \rightarrow f}} e^{\sum_{y \in ne(f) \setminus x} \lambda_{y \rightarrow f}(y) - \lambda^{*}_{y \rightarrow f}} $$ which is equivalent to $$ e^{\lambda_{f \rightarrow x}(x)} = e^{n \lambda^{*}_{y \rightarrow f}}\sum_{\mathcal{X_f} \setminus x} \phi_{f}(\mathcal{X}_f) e^{\sum_{y \in ne(f) \setminus x} \lambda_{y \rightarrow f}(y) - \lambda^{*}_{y \rightarrow f}} $$ Finally taking logs we get $$\lambda_{f \rightarrow x}(x) = n \lambda^{*}_{y \rightarrow f} + \mathrm{log}\left(\sum_{\mathcal{X_f} \setminus x} \phi_{f}(\mathcal{X}_f) e^{\sum_{y \in ne(f) \setminus x} \lambda_{y \rightarrow f}(y) - \lambda^{*}_{y \rightarrow f}}\right)$$ So mine differes by a factor of n in the first term. Am i correct or have i made a mistake? 2. How would we calculate $\lambda^{*}_{y \rightarrow f} = \mathrm{max}_{y \in \mathrm{ne}(f) \setminus x } \lambda_{y \rightarrow f} $ given that technically $\lambda_{y \rightarrow f}$ is a vector? I was thinking perhaps you pick the vector with the largest element but since we do difference elementwise that wouldn't necessarily ensure differences are negative.
