[site]: crossvalidated
[post_id]: 457582
[parent_id]: 
[tags]: 
Kernel Mean Embedding relationship to regular kernel functions

I am struggling to understand kernel mean embeddings and how it relates to typical kernel functions. Review of Kernel Basics: Basically, a kernel function maps points (or vectors) from one feature space to another space. The idea is that this new representation of data facilitates analysis in some way. For example, it is now easier to classify the data in this new feature space than its original feature space. Other examples include Kernel PCA, SVMs, etc. Let $\mathcal{X}$ be a non-empty set. A kernel function $k: \mathcal{X} \times \mathcal{X} \rightarrow\mathbb{R}$ exists if there is a Hilbert space, $\mathcal{H}$ , with corresponding map $\phi:\mathcal{X} \rightarrow\mathcal{H}$ such that: $$k(x,y) = \langle\phi(x), \phi(y) \rangle_{\mathcal{H}}$$ where $x,y \in \mathcal{X} $ (just two elements in this set). This can also be interpreted as the distance between $x$ and $y$ in this new feature space, $\mathcal{H}$ . Note usually $k$ is selected to make $\mathcal{H}$ a reproducing kernel Hilbert space (RKHS). Kernel Mean Embeddings: Kernel embeddings are a sort of generalization of the basic kernel transformation shown above. It uses the same premise ( $\mathcal{X}$ , $\mathcal{H}$ ) & tools (kernel functions) but instead of mapping vectors, they map distributions to a new feature space. From Wikipedia , let $X$ denote a random variable with domain $ \Omega $ and distribution $P$ . Given a kernel $k$ on $\Omega \times \Omega$ , the kernel embedding of the distribution $P$ in $\mathcal {H}$ (also called the kernel mean or mean map ) is given by: $$\mu_{X}:=\mathbb{E}[k(X, \cdot)]=\mathbb{E}[\phi(X)]=\int_{\Omega} \phi(x) \mathrm{d} P(x)$$ Given $n$ training examples ${\{x_{1},\ldots ,x_{n}\}}$ drawn independently and identically distributed (i.i.d.) from $P$ , the kernel embedding of $P$ can be empirically estimated as: $$\hat{\mu}_{X}=\frac{1}{n} \sum_{i=1}^{n} \phi\left(x_{i}\right)$$ My questions are: Is there some kind of analogy one could use to help visualize what is going when a kernel mean embedding is applied? Why are mapping distributions called " embeddings " when mapping basic vectors aren't called "embeddings"? How are we mapping a distribution if in the end, we just average a kernel mapping of sampled points? Doesn't seem to be a representation of the distribution in a new feature space, just it's average in a new feature space. Why do we need to map into a RKHS? Can't it just be a regular Hilbert space? Is $\mu_X$ (or $\hat{\mu}_{X}$ ) a function or a real number? How is a kernel mean embedding different from a parzen window (KDE) ?
