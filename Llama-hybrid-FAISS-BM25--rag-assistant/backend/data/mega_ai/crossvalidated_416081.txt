[site]: crossvalidated
[post_id]: 416081
[parent_id]: 416007
[tags]: 
What precisely is implied by omitting the intercept? @gung and @kjetil have explained in their answers how for this kind of model, with a factor variable included as a predictor, when asked to omit the intercept R, meaning lm() , changes its parameterisation. The model fitted is really another version of the same model. How well this is documented and how widely it is understood are separate issues. Here I first give the data in a form suitable for reading into Stata, not because I assume that other readers use Stata, but because I guess a copy here might be as useful as the version on pastebin, which in any case may not remain accessible indefinitely. They are close to a form that could be read into many different programs with minor editing. clear input byte(age sex college income crimes) 19 0 2 56 13 19 1 2 59 16 20 0 2 55 13 21 0 2 60 13 20 0 2 52 14 24 0 3 54 14 25 0 3 55 13 25 0 3 59 16 27 1 4 56 16 28 1 4 52 14 38 0 4 59 20 29 1 4 63 25 30 1 4 55 19 21 1 3 29 8 21 1 2 35 11 20 0 2 33 10 19 0 2 27 6 21 0 3 24 7 21 1 2 53 15 16 1 2 63 23 18 1 2 72 25 18 1 2 75 22 18 0 2 61 16 19 1 2 65 19 19 1 2 70 19 20 1 2 78 18 19 0 2 76 16 18 0 2 53 12 31 0 4 59 23 32 1 4 62 25 32 1 4 55 22 31 0 4 57 25 30 1 4 46 17 29 0 4 35 14 29 0 4 32 12 28 0 4 30 10 27 0 4 29 8 26 0 4 28 7 25 0 4 25 5 24 0 3 33 9 23 0 3 26 7 23 1 3 28 9 22 0 3 38 10 22 0 3 24 4 22 0 3 28 6 end Now to the heart of the matter. I note that here there is one indicator for sex which from the magnitude of the coefficient just below I infer to be the same as male which the OP doen't give. Unsurprisingly, Stata can reproduce a plain regression with intercept. . regress crimes income age sex college Source | SS df MS Number of obs = 45 -------------+---------------------------------- F(4, 40) = 52.26 Model | 1319.7758 4 329.943951 Prob > F = 0.0000 Residual | 252.535307 40 6.31338267 R-squared = 0.8394 -------------+---------------------------------- Adj R-squared = 0.8233 Total | 1572.31111 44 35.7343434 Root MSE = 2.5126 ------------------------------------------------------------------------------ crimes | Coef. Std. Err. t P>|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- income | .2933889 .0302029 9.71 0.000 .2323465 .3544313 age | .3777469 .1974061 1.91 0.063 -.0212256 .7767194 sex | 2.293912 .8495281 2.70 0.010 .5769517 4.010872 college | .1793265 1.18586 0.15 0.881 -2.217386 2.576039 _cons | -10.30475 2.277652 -4.52 0.000 -14.90805 -5.701441 ------------------------------------------------------------------------------ So, this is a just a model with systematic part $\text{crimes} = \beta_0 + \beta_1 \text{income} + \beta_2 \text{age} + \beta_3 \text{sex} + \beta_4 \text{college}$ Here $\text{sex}$ is 0 and 1 and need not be declared as a factor variable in Stata (although that would do no harm). I note that $\text{college}$ is treated as measured, not as a categorical variable, as in the question. The quotation given by the OP It can also used to remove the intercept term: when fitting a linear model y ~ x - 1 specifies a line through the origin. A model with no intercept can be also specified as y ~ x + 0 or y ~ 0 + x. led me -- in comments now deleted -- to suppose that R would fit the following model, with systematic part $\text{crimes} = \beta_1 \text{income} + \beta_2 \text{age} + \beta_3 \text{sex} + \beta_4 \text{college}$ As said, that is not what R does. But it is an interpretation of what omitting the intercept means. It's not, in my view, a defensible model for these data. There are no grounds for postulating a model fit that goes through the origin, which here as often is not only way outside the data but also impossible as a data point (no student with age 0, for a start). That's nevertheless easy to do in Stata. Here for what they are worth (nothing) are the results: . regress crimes income age sex college, noconstant Source | SS df MS Number of obs = 45 -------------+---------------------------------- F(4, 41) = 280.95 Model | 10464.2351 4 2616.05876 Prob > F = 0.0000 Residual | 381.764941 41 9.31134003 R-squared = 0.9648 -------------+---------------------------------- Adj R-squared = 0.9614 Total | 10846 45 241.022222 Root MSE = 3.0514 ------------------------------------------------------------------------------ crimes | Coef. Std. Err. t P>|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- income | .2312656 .0326698 7.08 0.000 .1652876 .2972437 age | .0497223 .2229822 0.22 0.825 -.4005994 .500044 sex | 2.534587 1.029674 2.46 0.018 .455119 4.614056 college | .4111346 1.438808 0.29 0.777 -2.494596 3.316865 ------------------------------------------------------------------------------ To spell it out, this is a quite different model with one fewer parameter and different coefficient estimates. As before sex is 0 and 1 and need not be declared as a factor variable in Stata (although that would do no harm). I confirm that the same numerical results are obtained in Stata in either case if sex is declared as a factor variable. By the way, nothing of interest really follows from the difference in $R^2$ here. The first $R^2$ measures how much better the model is than predicting the mean of crimes; the second measures how much better the model is than predicting zero crimes, which I can't see as a benchmark of statistical or social science or policy interest. That this is possible in Stata is not a bug or bogus. The main reason for the option noconstant in regress in Stata is the same main reason as explained in the quotation above from R documentation. Occasionally there are good grounds for forcing a model fit through the origin. The most common example in my experience is fitting $y = bx$ rather than $y = a + bx$ (after all many elementary laws of physics are of the first kind). Discussing what Stata does is here just a way of checking results and of underlining that different interpretations of omitting the intercept exist. What other software would or could do by way of omitting the intercept is equally germane, in my view. Dataset given in the example: How to analyse? Does it even make sense? The OP gave their dataset. I now make some partial remarks on the different but interesting question of how to analyse their data. It is evident that income is the most important linear predictor: see the scatter plot below. The response is bounded [0, 25]. The upper limit for response of 25 is attained in the data, so a linear functional form appears less suitable than say fractional logit. I scale to [0, 1], fit a logit model and plot a model summary in terms of the original scale. . gen crimes2 = crimes/25 . fracreg logit crimes2 income age i.sex college Iteration 0: log pseudolikelihood = -29.235193 Iteration 1: log pseudolikelihood = -26.141125 Iteration 2: log pseudolikelihood = -26.12196 Iteration 3: log pseudolikelihood = -26.121956 Iteration 4: log pseudolikelihood = -26.121956 Fractional logistic regression Number of obs = 45 Wald chi2(4) = 146.02 Prob > chi2 = 0.0000 Log pseudolikelihood = -26.121956 Pseudo R2 = 0.1490 ------------------------------------------------------------------------------ | Robust crimes2 | Coef. Std. Err. z P>|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- income | .0528822 .0065389 8.09 0.000 .0400662 .0656982 age | .0786006 .0469061 1.68 0.094 -.0133338 .1705349 1.sex | .456912 .1565497 2.92 0.004 .1500802 .7637437 college | .0104632 .2507818 0.04 0.967 -.4810602 .5019865 _cons | -4.28356 .5641885 -7.59 0.000 -5.389349 -3.177771 ------------------------------------------------------------------------------ Although in principle quite different, the indications about predictors seem similar. income is the biggest deal followed by sex . Yet further analysis might start by noting the correlation between age and college , which takes the values 2, 3, 4. Closer scrutiny reveals a puzzling pattern, here shown by a plot with stem-and-leaf flavour: On average I would expect people to be about 1 or 2 years older for each year in college, taking note of part-timers, etc. Here the medians are 19, 23, 29 years! I suspect that the data are invented on this and other grounds: is it plausible that 45 students know their parental income as implied?
