[site]: crossvalidated
[post_id]: 431775
[parent_id]: 
[tags]: 
Transformers for non-NLP tasks?

I'm looking into using Transformer model for non-NLP generative tasks. Specifically, Events in a sequence semantically happen at the same time unordered; Events are in high-dimensional vectors that cannot be tokenized naively as in most NLP applications. The same for targets. (1) seems to break the basic assumption of Positional Encoding ; (2) seems to break most of the implementations, and although I can change the lead-in and lead-out of the model I'm not sure whether the decoding layer will handle this correctly in practice. Do any of my concern stand? Are Transformers for my task?
