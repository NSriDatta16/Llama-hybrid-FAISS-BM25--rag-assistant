[site]: datascience
[post_id]: 9370
[parent_id]: 
[tags]: 
Extremely dominant feature?

I'm new to datascience. I was wondering how one should treat an extremely dominant feature. For example, one of the features is "on"/"off", and when it's "off", none of the other features matter and the output will just always be 0. So should I drop all rows where it's "off" in my train/test data sets? I feel like I would get a better fit that way. If I delete those rows, I'm concerned about how I would handle those rows in the test set. For example, I'd have to write code to loop through the data and put a 0 in the prediction column for those rows, as well as make sure everything else lines up. (This is all Kaggle related, so the training set is several columns of features and a y_column, whereas the test set doesn't have the y_column and we're supposed to predict it.) I'm using Python and Scikit Learn's random forest, if that matters.
