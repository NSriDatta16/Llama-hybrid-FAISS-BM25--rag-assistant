[site]: crossvalidated
[post_id]: 268561
[parent_id]: 
[tags]: 
Example of backpropagation for neural network with softmax and sigmoid activation

I am trying to produce a NN algorithm to classify the species of Iris into three species (versicolor, virginica, setosa) - preferably in R. The scaffolding / source is this code in R with ReLU activation of the hidden unit and softmax. The question is code-neutral, and an alternative source is this post in Python , probably by the same authors. I want to solve the backpropagation algorithm with sigmoid activation (as opposed to ReLU) of a 6-neuron single hidden layer without using packaged functions (just to gain insight into backpropagation). As in the linked posts the architecture is as follows: The compiled code for the links is here for R and here for Python . Here is the ready to paste (now working) code reproduced in the answer.
