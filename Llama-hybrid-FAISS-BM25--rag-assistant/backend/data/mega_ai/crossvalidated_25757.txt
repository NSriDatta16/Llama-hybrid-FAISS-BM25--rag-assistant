[site]: crossvalidated
[post_id]: 25757
[parent_id]: 25727
[tags]: 
If you have a known distribution, Gumbel or not, with distribution function $F$ and an observation $x$ a simple one-sided probability of how extreme the observation is simply the probability $$1-F(x)$$ of an observation larger than the actual observation $x$. If $F$ is given in terms of a scale-location transformation of a distribution with distribution function $G$ with scale parameter $\sigma$ and location parameter $\mu$ then the probability is $$1 - F(x) = 1 - G\left( \frac{x - \mu}{\sigma}\right).$$ For the Gumbel distribution with $G(x) = \exp(-\exp(-x))$ this amounts to computing $$1 - \exp\left(-\exp\left(-\frac{x-\mu}{\sigma}\right)\right).$$ Note that in this parametrization of a location-scale transformation of $G$ the location parameter $\mu$ and the scale parameter $\sigma$ are not the mean and standard deviation of the distribution. With a single, known distribution the probability could have a Bayesian or a frequency interpretation. If the parameters $\mu$ and $\sigma$ are estimated based on a data set a Bayesian approach could be to integrate the formula above over the posterior on $(\mu, \sigma)$ whereas a typical plug-in frequency approach is to simply plug in the estimates of $\mu$ and $\sigma$. In the latter case, a confidence interval on the estimated tail probability is preferred to be able to judge the uncertainty in the estimated probability. In some applications, e.g. in biological sequence analysis, the probability is used to judge a very large number of observations, and though the computation is the same, we do face a multiple testing problem, which should be accounted for in the evaluation of the computed probabilities. Edit The following was added to answer a question in a comment by the OP. If there are two known distributions, or models, with densities $f_1$ and $f_2$, respectively, and we assign prior probabilities $\pi_1$ and $\pi_2$ to these the conditional probability of model 1 given $x$ is $$\frac{\pi_1 f_1(x)}{\pi_1 f_1(x) + \pi_2 f_2(x)}.$$ This easily generalizes to a finite number of models, but we have to specify these other models in addition to the prior probabilities to carry out the computation of the posterior probability of model 1.
