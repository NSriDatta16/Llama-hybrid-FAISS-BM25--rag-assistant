[site]: crossvalidated
[post_id]: 512306
[parent_id]: 
[tags]: 
Should I use different types of normalization on the same dataset when preprocessing for machine learning

I am working to preprocess a dataset where half of it is already normalized between 0 and 1. I was planning on using z-score to normalize the rest of the dataset but I was wondering if that was a bad idea. Should I use a different normalization function that keeps it between 0 and 1? Or perhaps use a z-score on the data that is normalized between 0 and 1. I know that using different normalizations is do able but I am looking for the optimal way of doing things. Also, the data set has some binary features it that makes a difference. Note: I have already one-hot encoded the categorical features. Edit: the problem is based off the the KDD Cup 1999 dataset for analyzing normal connection verse malicious connections ( link to the data set here ). The features are different, if you observe the dataset some of the features are "rates" so they are "normalized" between 0 and 1. Other features such as the number of failed connections are not normalized in any way. I was thinking of normalizing the number of failed connections using their z-score but I was unclear if that was a good idea based off the fact that some of the data seem to already be "normalized" since it is a rate. Let me know if you have further questions.
