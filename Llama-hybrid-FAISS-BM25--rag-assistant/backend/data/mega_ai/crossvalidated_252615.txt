[site]: crossvalidated
[post_id]: 252615
[parent_id]: 252610
[tags]: 
Whether you have variables that are "roughly" on the same scale, or you have one (volume) or more that are far larger/smaller, it is common-practice and recommended to standardize. There are cases in which I would recommend not standardizing (specifically, not scaling ), for example when the variables are in the same units and represent the same things. Anyway, there are many ways to standardize, usually by first centering and then scaling. I'll list three common methods. Let $x \in \mathbb{R}^n$ be a vector of measurements (a feature) and let $\tilde{x}$ be its standardized version. $\ell_2$ standardization works well when your data is normal or platykurtic and not too skewed: $$\tilde{x} = \frac{x - \mathbf{mean}(x)}{\|x - \mathbf{mean}(x)\|_2}$$ In this case $\mathbf{mean}(\tilde{x}) = 0$ while $\|\tilde{x}\|_2 = 1$. $\ell_1$ standardization is a robust alternative to $\ell_2$ and works better when your data is highly leptokurtic, is corrupted by outliers, or is very skewed: $$\tilde{x} = \frac{x - \mathbf{median}(x)}{\|x - \mathbf{median}(x)\|_1}$$ In this case $\mathbf{median}(\tilde{x}) = 0$ while $\|\tilde{x}\|_1 = 1$. This is the one I tend to use when working with financial data like returns and volume. $\ell_\infty$ standardization is typically used with uniform distribution or others where the data is known to have a bounded support: $$\tilde{x} = \frac{x - \mathbf{min}(x)}{\|x - \mathbf{min}(x)\|_\infty}$$ In this case $\mathbf{min}(\tilde{x}) = 0$ and $\mathbf{max}(\tilde{x}) = 1$. The comments I added when explaining when each methods works best are purely anecdotal and it's quite common for practitioners to try different methods and judge using objective metrics like out-of-sample forecasting error. I know, for example, that $\ell_\infty$ standardization is a favorite in the deep learning literature, even when the sampling distribution is approximately normal.
