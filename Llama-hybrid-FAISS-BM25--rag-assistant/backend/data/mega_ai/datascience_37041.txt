[site]: datascience
[post_id]: 37041
[parent_id]: 37021
[tags]: 
Another way of looking at what dropout does is that it is like a slab-and-spike prior for the coefficient for a covariate (that is some complex interaction term of the original covariates with some complicated functional transformations) in a Bayesian model. This is the interpretation proposed by Yarin Gal in his thesis (see his list of publications ). Here is a brief hand-waving argument for why this is so: In those batches, where a neuron is eliminated, the coefficient for feature/covariate (constructed by connection in the neural network going into the neuron) is zero (spike at zero). In those batches, where the neuron is present, the coefficient is unrestricted (improper flat prior = slab). Averaged across all batches, you get a spike-and-slab prior. Why would we want a slab-and-spike prior? It induces a Bayesian model averaging between a neutral network without that neuron and one with it in. In other words, it lets us express uncertainty about whether the neutral network really needs to have its full possible complexity and appropriately takes this uncertainty into account in the predictions. This addresses the major issue of neutral networks being able to overfit to data (though of course it is not the only possible way to achieve that).
