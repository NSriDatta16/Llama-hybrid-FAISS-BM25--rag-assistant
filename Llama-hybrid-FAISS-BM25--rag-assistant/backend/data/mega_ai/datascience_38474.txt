[site]: datascience
[post_id]: 38474
[parent_id]: 
[tags]: 
What does the one function $\mathbf{1}_{i,y^{(t)}}$ exactly mean in backward propagation of RNN in the book "Deep learning" of Bengio

It confused me for a long time what is $\mathbf{1}_{i,y^{(t)}}$ exactly mean in (10.18) below. It is in the Chapter 10 on RNN of the book LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. "Deep learning." nature 521.7553 (2015): 436. Because the ${i,y^{(t)}}$ doesn't seem to be a condition while the author has introduced a condition function as $\mathbf{1}_\text{condition} = 1$ if condition is true or $=0$ if the condition is false. It is computing the gradient of the log-likelihood with backward propagation in recurrent neural network. Instead of $\hat{y}_i^{(t)}− \mathbf{1}_{i,y^{(t)}}$ , I found my solution might be $$ (\hat{y}^{(t)}_i-y_i^{(t)})\dfrac{\exp(o_i^{(t)})\sum_j\exp(z_j)-(\exp(z_i))^2}{(\sum_j\exp(z_j))^2} $$ It is based on the prior assumption of Gaussian distribution with $\sigma=1$ Below is the equation I am asking about: The gradient $\nabla_{o^{(t)}}L$ on the outputs at time step $t$ , for all $i$ , $t$ , is as follows: $$ (\nabla_{o(t)}L)_i = \dfrac{\partial L}{\partial o_i^{(t)}} = \dfrac{\partial L}{\partial L^{(t)}}\dfrac{\partial L^{(t)}}{\partial o_i^{(t)}} = \hat{y}_i^{(t)}− \mathbf{1}_{i,y^{(t)}}. \qquad\qquad (10.18) $$ While, the corresponding notes are: $$ \begin{align*} \boldsymbol{a}^{(t)} &= \boldsymbol{b} +\boldsymbol{Wh}^{(t−1)} + \boldsymbol{Ux}^{(t)} &(10.8)\\ \boldsymbol{h}^{(t)} &= \tanh(\boldsymbol{a}^{(t)}) &(10.9)\\ \boldsymbol{o}^{(t)} &= \boldsymbol{c} + \boldsymbol{V h}^{(t)} &(10.10)\\ \hat{\boldsymbol{y}}^{(t)} &= \text{softmax}(\boldsymbol{o}^{(t)}) &(10.11) \end{align*} $$ And I wonder which version of softmax function is used here? $$ \text{softmax}(\boldsymbol{z})=\dfrac{\exp(z_i)}{\sum_j\exp(z_j)} $$ or $$ \text{softmax}(\boldsymbol{z})=\dfrac{\exp(z_i-\max_kz_k)}{\sum_j\exp(z_j-\max_kz_k)} $$ ? Thank you very much!
