[site]: crossvalidated
[post_id]: 397218
[parent_id]: 397174
[tags]: 
A model that cannot really capture the structure of the data, i.e. a model that actually underfits , can still memorize something internally, that results in bad generalization. Although it is typically not considered as overfitting since your model is not able to capture the training data well, it actually emulates overfitting by spending its resources to memorize (maybe some portion of) your data. In such a case, you should stop the learning process, e.g. early-stopping for neural networks. But, more importantly, you should either update your model or your training algorithm (e.g. gradient descent method).
