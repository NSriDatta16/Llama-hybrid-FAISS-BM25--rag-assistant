[site]: datascience
[post_id]: 109075
[parent_id]: 
[tags]: 
"Steps" in training T5

I am trying to reproduce the results of paper that fine-tunes T5 (a deep learning language model) on a dataset. In, the paper they say they fine-tune all their models for 10k steps. For the base-version of T5, they use a batch size of 128, for the large version of T5 they use a batch size of 32, and for the XL version of T5 they use a batch size of 16. Does this mean they are fine tuning the base version for 8 times more epochs than the XL version of T5?
