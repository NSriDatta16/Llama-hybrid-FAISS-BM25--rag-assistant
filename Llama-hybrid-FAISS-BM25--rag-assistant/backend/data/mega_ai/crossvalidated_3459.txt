[site]: crossvalidated
[post_id]: 3459
[parent_id]: 3458
[tags]: 
I think it would be worth giving a try to Random Forests ( randomForest ); some references were provided in response to related questions: Feature selection for “final” model when performing cross-validation in machine learning ; Can CART models be made robust? . Boosting/bagging render them more stable than a single CART which is known to be very sensitive to small perturbations. Some authors argued that it performed as well as penalized SVM or Gradient Boosting Machines (see, e.g. Cutler et al., 2009). I think they certainly outperform NNs. Boulesteix and Strobl provides a nice overview of several classifiers in Optimal classifier selection and negative bias in error rate estimation: an empirical study on high-dimensional prediction (BMC MRM 2009 9: 85). I've heard of another good study at the IV EAM meeting , which should be under review in Statistics in Medicine , João Maroco , Dina Silva, Manuela Guerreiro, Alexandre de Mendonça. Do Random Forests Outperform Neural Networks, Support Vector Machines and Discriminant Analysis classifiers? A case study in the evolution to dementia in elderly patients with cognitive complaints I also like the caret package: it is well documented and allows to compare predictive accuracy of different classifiers on the same data set. It takes care of managing training /test samples, computing accuracy, etc in few user-friendly functions. The glmnet package, from Friedman and coll., implements penalized GLM (see the review in the Journal of Statistical Software ), so you remain in a well-known modeling framework. Otherwise, you can also look for association rules based classifiers (see the CRAN Task View on Machine Learning or the Top 10 algorithms in data mining for a gentle introduction to some of them). I'd like to mention another interesting approach that I plan to re-implement in R (actually, it's Matlab code) which is Discriminant Correspondence Analysis from Hervé Abdi. Although initially developed to cope with small-sample studies with a lot of explanatory variables (eventually grouped into coherent blocks), it seems to efficiently combine classical DA with data reduction techniques. References Cutler, A., Cutler, D.R., and Stevens, J.R. (2009). Tree-Based Methods , in High-Dimensional Data Analysis in Cancer Research , Li, X. and Xu, R. (eds.), pp. 83-101, Springer. Saeys, Y., Inza, I., and Larrañaga, P. (2007). A review of feature selection techniques in bioinformatics . Bioinformatics, 23(19): 2507-2517.
