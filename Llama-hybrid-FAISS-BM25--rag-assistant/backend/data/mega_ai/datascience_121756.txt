[site]: datascience
[post_id]: 121756
[parent_id]: 
[tags]: 
How to monitor training of text generation models?

I'm finetuning a pretrained Huggingface model based on Transformers for a downstream Text Generation task, but I have doubts on how the fine-tuning process should be monitored: In classification, I usually calculate the loss and other metrics (e.g accuracy) on a validation set to for early stopping and to save the checkpoint of the best epoch, but for Text Generation tasks I see these additional issues: (Causal) text generation is slow , while training can be more parallelized with teacher forcing. Does it make sense to perform validation and generate text after every training epoch? Text generation depends on an additional set of hyperparameters which greatly condition the quality of generated text, such as the decoding technique (greedy vs sampling-based or other techniques), number of beams, temperature, maximum length, etc. All these parameters are not actually used during training. I suppose also find the best combination after training, but how can I monitor the training? HuggingFace generation API does not provide the loss during prediction, i.e I cannot generate text and calculate the cross-entropy loss (at least out-of-the-box) during validation. To calculate loss I could either Create a custom generation procedure which includes loss. Perform two passes on all data during validation (one with model.generate and one with model.forward ) Both these alternatives are suboptimal and this made me think that it is not common to calculate validation loss in text generation tasks, is it true? What is the common way to monitor training/fine-tuning of text generation models?
