[site]: crossvalidated
[post_id]: 539541
[parent_id]: 
[tags]: 
The equality condition of Jensen's inequality in derivation of variational sparse gaussian process

I am trying to understand the derivation of Variational Learning of Inducing Variables in Sparse Gaussian Processes and how Eq 8: $$ F_V(X_m, \phi)=\int p(f|f_m)\phi(f_m)\log \frac{p(y|f)p(f_m)}{\phi(f_m)}dfdf_m $$ can be "maximized" to Eq 9: $$ F_V(X_m) = \log[N(y|0,\sigma^2I+Q_{nn})]-\frac{1}{2\sigma^2}Tr(\tilde{K}) $$ The paper said that the detail can be found in the technical report ( Variational Model Selection for Sparse Gaussian Process Regression ), in that I found following steps in page 14: Merge the logs $$ F_V(X_m, \phi(f_m))=\int_{f_m}\phi(f_m){\log \frac{G(f_m,y)p(f_m)}{\phi(f_m)}}df_m $$ Reverse Jensen's inequality to maximize wrt $\phi(f_m)$ : $$ F_V(X_m) = \log \int_{f_m} G(f_m, y) p(f_m) df_m $$ While I can try to fill the gap by: $$ \begin{align*} F_V(X_m,\phi(f_m)) &= E_{f_m\sim\phi(f_m)} \log \frac{G(f_m,y)p(f_m)}{\phi(f_m)} \\ &\le \log E_{f_m\sim\phi(f_m)}\frac{G(f_m,y)p(f_m)}{\phi(f_m)} \\ &= \log \int_{f_m} \phi(f_m)\frac{G(f_m,y)p(f_m)}{\phi(f_m)} \\ &= \log \int_{f_m} G(f_m,y) p(f_m)df_m \end{align*} $$ However, I can't see how the equality can holds here (like the derivation of EM algorithm?). It's obvious that $\log$ is not linear and $\frac{G(f_m,y)p(f_m)}{\phi(f_m)}$ is not constant and $\phi(f_m)$ is not a "delta distribution". Does Titsias just maximize an "upper bound" which is not ensured to increase like in Bayesian Optimization?
