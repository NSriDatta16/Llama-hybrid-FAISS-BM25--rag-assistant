[site]: datascience
[post_id]: 72482
[parent_id]: 68865
[tags]: 
I am not sure if I understand your thinking, but my understanding is following - maybe it can help you see it from another perspective. The multi-label classification problem with n possible classes can be seen as n binary classifiers. If so, we can simply calculate AUC ROC for each binary classifier and average it. This is a bit tricky - there are different ways of averaging, especially: 'macro': Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account. 'weighted': Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label). as explained here . There is another option (which may be the closest to how you see it): 'micro': Calculate metrics globally by considering each element of the label indicator matrix as a label. which is implemented here . There is also the fourth option, which I don't really understand: 'samples': Calculate metrics for each instance, and find their average. I have been using myself weighted averaging.
