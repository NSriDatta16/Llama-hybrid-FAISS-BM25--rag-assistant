[site]: crossvalidated
[post_id]: 620757
[parent_id]: 229376
[tags]: 
Model collapse and catastrophic forgetting can both happen to some neural networks. Setting aside these somewhat niche scenarios, the trajectory of SGD can "make hops back and forth" while making excruciatingly slow progress towards the minimum. See: https://stats.stackexchange.com/a/367459/22311 Tuning the learning rate and using momentum and preconditioning can ameliorate this somewhat.
