[site]: crossvalidated
[post_id]: 287932
[parent_id]: 
[tags]: 
Simplification of case-based logistic regression cost function

In Andrew Ng's Coursera Machine Learning class , the cost function for logistic regression is defined as: $$\begin{align*} J(\theta) &= \dfrac{1}{m} \sum_{i=1}^m \mathrm{Cost}(h_\theta(x^{(i)}),y^{(i)}) \\[2ex] \mathrm{Cost}(h_\theta(x),y) &= \begin{cases} -\log(1-h_\theta(x)) \; & \text{if y = 0} \\ -\log(h_\theta(x)) \; & \text{if y = 1} \\ \end{cases} \end{align*}$$ It is said that this simplifies to: $$\require{cancel} J(\theta) = - \frac{1}{m} \displaystyle \sum_{i=1}^m \left[y^{(i)}\log \big(h_\theta (x^{(i)})\big) + (1 - y^{(i)})\log \big(1 - h_\theta(x^{(i)})\big)\right] \\[6pt]$$ How is this simpler? Why are the cases multiplied by $y^{(i)}$ and $1-y^{(i)}$ respectively?
