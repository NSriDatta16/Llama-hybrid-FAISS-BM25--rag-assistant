[site]: crossvalidated
[post_id]: 540607
[parent_id]: 
[tags]: 
Relationship between L1 penalty and margin in SVM

Expanding on " Why aren't there there two regularization terms in SVC? " and " Meaning of penalty and loss in LinearSVM ": It appears that LinearSVC in Python also supports $l_1$ regularisation ("penalty") of the class boundary vector $\mathbf{w}$ , in addition to the usual $l_2$ regularisation. The documentation claims (and it is in line with what we know about LASSO) that $l_1$ regularisation leads to a sparse vector of coefficients. But I wonder what effect does it have on the margin and on the generalisation properties of the classifier. I'd appreciate links to publications discussing this question. Also, is there a deep, mathematical reason why LinearSVC wouldn't support the combination of penalty='l2' and loss='hinge' (the standard combination in SVC) when dual=False (i.e. when optimising the primal), or is it simply a design choice by the authors?
