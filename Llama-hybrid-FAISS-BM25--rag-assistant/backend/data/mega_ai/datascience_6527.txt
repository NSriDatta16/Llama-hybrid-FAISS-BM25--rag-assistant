[site]: datascience
[post_id]: 6527
[parent_id]: 6510
[tags]: 
One key difference is that cross validation ensures all samples will appear in the training and test sets, so 100% of your data gets used at some point for training and for testing. Depending on the size of your dataset the bootstrapping , sampling with replacement, occurring in the random forest will not guarantee the splits the trees see will contain all instances. If you have enough trees in your forest the OOB estimate should asymptotically converge towards the best OOB estimate value. The accuracy for both methods will to some degree be data dependent so it may be prudent to compare both methods on the particular data you have in front of you and see if CV and RF OOB estimates give similar values. If they do not, then it would be worth exploring further estimates of the true error rate, perhaps by much higher vales of K in CV.
