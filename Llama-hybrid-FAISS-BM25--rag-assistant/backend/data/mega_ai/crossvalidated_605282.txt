[site]: crossvalidated
[post_id]: 605282
[parent_id]: 604728
[tags]: 
Yes, we are likely overfitting because we get "45%+ more error" moving from the training to the validation set. That said , overfitting is properly assessed by using a training, validation and a testing set. That is because we can still overfit the validation set, CV.SE has a very enlightening thread on Overfitting the validation set if one wants to look at this further. In addition to the point above and more specific to this post: what we are missing is information about the variability of the performance metric used. For example, if our validation set performance is 80% and our test performance is 78% but we have a variability of $\pm 6\%$ in our performance metric, then we do not overfit. If we have variability of $\pm 0.6\%$ we do overfit. In order to get some idea of this variability we could use some sort of resampling technique (e.g. $k$ -fold cross-validation is the simplest form). That is because simply put overfitting is "learning the noise" and resampling allows us to ameliorate noise's influence. In terms of references, a very accessible yet thorough discussion can be found in Roberts et al. (2016) Cross‚Äêvalidation strategies for data with temporal, spatial, hierarchical, or phylogenetic structure , it deals with ecological data but the learnings are transferable across all domains. More particular to ML, Raschka (2018) Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning is a great short monograph on the comparison of ML algorithms and their variance due to data sampling.
