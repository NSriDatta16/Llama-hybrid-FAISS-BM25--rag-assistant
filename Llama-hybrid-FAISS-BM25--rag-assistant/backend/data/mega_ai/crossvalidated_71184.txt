[site]: crossvalidated
[post_id]: 71184
[parent_id]: 
[tags]: 
Cross-validation or bootstrapping to evaluate classification performance?

What is the most appropriate sampling method to evaluate the performance of a classifier on a particular data set and compare it with other classifiers? Cross-validation seems to be standard practice, but I've read that methods such as .632 bootstrap are a better choice. As a follow-up: Does the choice of performance metric affect the answer (if I use AUC instead of accuracy)? My ultimate goal is to be able to say with some confidence that one machine learning method is superior to another for a particular dataset.
