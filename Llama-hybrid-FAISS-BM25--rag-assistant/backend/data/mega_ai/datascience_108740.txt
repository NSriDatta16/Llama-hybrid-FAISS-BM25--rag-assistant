[site]: datascience
[post_id]: 108740
[parent_id]: 
[tags]: 
BERT - The purpose of summing token embedding, positional embedding and segment embedding

I read the implementation of BERT inputs processing (image below). My question is why the author chose to sum up three types of embedding (token embedding, positional embedding and segment embedding)?
