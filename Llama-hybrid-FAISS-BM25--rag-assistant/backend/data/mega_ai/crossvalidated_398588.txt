[site]: crossvalidated
[post_id]: 398588
[parent_id]: 
[tags]: 
Principal Components' relation with variables having lower variance

This is a philosophical question about PCA, and not a direct coding question. I understand that PCA is a dimensionality reduction technique which results in a certain set of PCs, each PC being a linear combination of all the variables in the dataset. In my use case, I have n variables in my dataset, and I'm reducing these n variables to k PCs, such that, n > k . For each of the k PCs, I'm extracting the top 2 original variables (with the largest absolute magnitude of loadings) and assuming that those 2 variables (for each PC) contribute the most to that individual PC. Now, taking each of the top 2 original variables from each PC, I take a DISTINCT on the resulting 2k variables and end up with m variables, such that, n > m . For my use case, I would like to figure out what other variables (not contained in m variables) are correlated to each of the m variables. For example: Say I have a dataset of users and their ratings on 6 movies M1,M2,M3,M4,M5 and M6 . The movies M1 through M6 will form the 6 original variables ( n=6 ) in question. Now say I reduce this to 3 PCs ( k=3 ), and after taking DISTINCT of the top 2 original variables ( M1 through M6 ) on each PC, say I end up with 2 variables - M2 and M4 ( m=2 ). Now I would like to figure out - if a user doesn't like the movie M2 , what other movies (out of M1,M3,M5,M6 , the variables not having highest magnitudes for any PC) is the same user less likely to appreciate. I could just look at the correlation of M2 with each of M1,M3,M5 and M6 and the variables with higher correlations would be the answer. Is there any other statistically sound way to figure this out? I'm doing this in Python + Pandas + sklearn. Any help would be hugely appreciated.
