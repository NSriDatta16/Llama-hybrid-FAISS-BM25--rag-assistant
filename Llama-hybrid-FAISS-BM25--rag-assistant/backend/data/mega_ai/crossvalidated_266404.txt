[site]: crossvalidated
[post_id]: 266404
[parent_id]: 266178
[tags]: 
Let's you denote your data as $X$ and imagine that you have some probabilistic model that describes your data in terms of the likelihood of observing your data given some parameter $\theta$. The parameter $\theta$ is unknown and is to be estimated from your data. To estimate your parameter you could use many different approaches, e.g. use maximum likelihood estimation to find such value of $\theta$ that maximizes the likelihood, or use Bayesian approach. In Bayesian approach to estimate the parameter we need one more thing, a prior distribution for $\theta$. If you take those things together, you can use Bayes theorem to obtain the posterior distribution of $\theta$ (i.e. your estimate): $$ \underbrace{p(\theta|X)}_\text{posterior} \propto \underbrace{p(X|\theta)}_\text{likelihood} \, \underbrace{p(\theta)}_\text{prior} $$ So to catch up: prior is the distribution of $\theta$ that is assumed by you before seeing the data. You do not "know" it, it is something that you assume. likelihood is the conditional distribution of data given the prior , it defines your model, posterior is the "estimated" distribution of parameter $\theta$ given data and the prior , posterior predictive distribution is the distribution of data that is "predicted" by your model given your data and the prior , it describes your predictions from the model. For example, in the classical beta-binomial model (see beta-binomial for multiple examples and more details) we have $$ X \mid \theta \sim \mathrm{Binom}(n, \theta) \\ \theta \sim \mathrm{Beta}(\alpha, \beta) $$ so binomial distribution parametrized by $\theta$ is our likelihood , beta distribution with hyperparameters $\alpha,\beta$ is our prior and by using conjugacy we can obtain a closed-form solution for posterior $$ \theta \mid X \sim \mathrm{Beta}(x+\alpha, n-x+\beta) $$ and posterior predictive distribution $$ \tilde X \mid X,\theta \sim \mathrm{BetaBinom}(n, x+\alpha, n-x+\beta) $$ where $\tilde X$ are the $X$'s predicted by our model. If this all is still unclear for you you can check the multiple threads tagged as bayesian on this site, or multiple handbooks on this topic.
