[site]: crossvalidated
[post_id]: 199937
[parent_id]: 
[tags]: 
Does the Support Vector Machine favor datasets with fewer features?

I am a bit concerned, as there are so many questions asked and so few answers given. I take it, machine learning has become quite common to use, but only little is really understood about their nature. That's problematic, but on the other hand - yes, I am one of those people. Not enough knowledge, but bound to use them and now puzzling over my results which seem quite chaotic to me. This is the problem: I use Support Vector Algorithms for regression of remote sensing data (satellite data in pixels). I want to compare the capability of one sensor to predict paramaterts like e.g. chlorophyll content of plants. I have the sensor data and the parameters, so I can calibrate and validate the model. Now one sensor has 13 bands (13 features), the other one has 242 bands. They cover the same spectrum, only one has a higher spectral resolution. In theorey I'd suppose: the more features available, the better my result . Turns out, this is not the case. The 13 features deliver way more accurate parameters! The more features I add to the algorithm, the worse it gets. Another disturbing fact is that a feature selection showed me, there was a constant improvement of the results when I used 100 features and more. But I never reach the goodnes of fit for the 13 band sensor. Is that a problem of the SVR or am I mistaking something? Thanks for your help all, very much appreciated!
