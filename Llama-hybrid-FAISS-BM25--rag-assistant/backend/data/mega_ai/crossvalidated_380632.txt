[site]: crossvalidated
[post_id]: 380632
[parent_id]: 380621
[tags]: 
The obvious case is when your classifier returns zeros and ones and has perfect accuracy, then the predictions and target values are the same $y_i = \hat{y}_i \;\forall\, i$ , then also $\tfrac{\sum_{i=1}^n y_i}{n} = \tfrac{\sum_{i=1}^n \hat{y}_i}{n}$ . When it woundn't have perfect accuracy, this won't have to be the case. We say that estimator is unbiased , when on average the predictions are equal to predicted values $E[\hat{\theta}_n] = \theta$ . If you are aiming at predicting the probabilities ( $\theta$ ) rather then the labels ( $y$ ), then probabilities on average equal to empirical proportion is in fact statement about bias of the estimator. If the classifier returns probability-like scores, it is also related to probabilities being well- calibrated . If they are, then they adequately predict individual probabilities. When talking about averaging the predicted probabilities $\hat{p}(y|\mathbf{X})$ , you most likely mean averaging them given the features $\mathbf{X}$ distributed as $f$ : $$ E_{\mathbf{x} \sim f}\big[\,\hat{p}(y|\mathbf{X})\,\big] = \sum_i \,\hat{p}(y|\mathbf{X}_i) \;f(\mathbf{X}_i) $$ Notice that in case of perfectly calibrated probabilities, i.e. $p(y|\mathbf{X}) = \hat{p}(y|\mathbf{X})$ , by the law of total probability we obtain $$ \sum_i \,\hat{p}(y|\mathbf{X}_i) \;f(\mathbf{X}_i) = p(y) $$ We want the predicted probabilities to be consistent with the true probabilities (and so, with empirical probabilities), that is why plotting predicted probabilities vs binned probabilities is a popular diagnostic plot for predictive models. See also Properties of logistic regressions and Should predicted probabilities from Logistic Regression correspond with percentages? .
