[site]: crossvalidated
[post_id]: 601056
[parent_id]: 600410
[tags]: 
These uncentered moments are sometimes called "raw" or "crude" moments: https://mathworld.wolfram.com/RawMoment.html One practical difference between centered moments and uncentered (raw) ones is their estimation from data. For example, estimating $E[X^2]$ is trivially done in an unbiased manner via: $\frac{1}{n} \sum_i^n x_i^2$ . However estimating the 2nd centered moment, ie. $\text{Variance}[X]$ , involves first estimating the expectation to do the centering. To estimate the variance in an unbiased manner, one has to account for the fact that the expectation was also estimated from data, leading to a subtle difference in the denominator term: why sample variance has has n-1 in the denominator? The raw moments are intimately related to the Characteristic function and Moment-generating function of the distribution: https://en.wikipedia.org/wiki/Characteristic_function_(probability_theory)#Moments Characteristic functions are very useful for studying theoretical properties of distributions, cf: What is the purpose of characteristic functions? Similar representations of distributions like characteristic functions have many practical uses in Machine learning: https://en.wikipedia.org/wiki/Kernel_embedding_of_distributions Regarding centered or raw moments, it is not the case that one should inherently prefer one over the other, it all depends on what you want to use them for. The centered moments have more common uses than the raw moments. For instance, the second centered moment is the variance, which used to quantify the spread of a distribution. The third centered moment (technically a normalized version thereof) is the skewness, which is commonly used to quantify the lopsidedness of a distribution (how asymmetric it is). The fourth centered moment is the kurtosis, which is commonly used to quantify how heavy-tailed a distribution is. Centered moments are useful to measure properties of distributions in a location-invariant manner. Random variable $X$ and $X + c$ (for any constant $c$ ) have the same centered moments. Also the $k$ -th centered moment of $c \cdot X$ is equal to $c^k$ times the $k$ -th centered moment of $X$ . Finally for independent random variables $X$ and $Y$ , the $k$ th moment of $X+Y$ is equal to the $k$ th moment of $X$ + the $k$ th moment of $Y$ -- but only for $k = 1, 2, or 3$ (not higher moments).
