[site]: crossvalidated
[post_id]: 265617
[parent_id]: 265038
[tags]: 
A sample from an nCRP is an $L$-depth path through the infinitely branching, infinitely deep tree: Each element of $\textbf{c}_d$ refers to a metaphorical restaurant. From Blei et al 2010: More formally, consider the infinite tree defined by the nCRP and let $\textbf{c}_d$ denote the path through that tree for the $d$th customer (i.e., document). Each of those restaurants has an infinite number of tables referring to other restaurants aren't shared across restaurants. This answers the question of how dependence is encoded, though I find this clearer when reading from Blei et al 2003 , emphasis mine: Suppose that there are an infinite number of infinite-table Chinese restaurants in a city. One restaurant is determined to be the root restaurant and on each of its infinite tables is a card with the name of another restaurant. On each of the tables in those restaurants are cards that refer to other restaurants, and this structure repeats infinitely. Each restaurant is referred to exactly once ; thus, the restaurants in the city are organized into an infinitely-branched tree. [...] Let $c_1$ be the root restaurant. For each level $\ell \in \left\{2, . . . , L\right\}$: (a) Draw a table from restaurant $c_{\ell - 1}$ using Eq. (1). Set $c_{\ell}$ to be the restaurant referred to by that table. Draw an $L$-dimensional topic proportion vector $\theta$ from Dir($\alpha$). For each word $n \in \left\{1, . . . , N\right\}$: (a) Draw $z \in \left\{1, . . . , L\right\}$ from Mult($\theta$). (b) Draw $w_n$ from the topic associated with restaurant $c_z$. Put another way, you can think of each restaurant as being associated with two distributions: A distribution over words, i.e. a "topic." A distribution over an infinite number of other restaurants, none of which are referred to by other restaurants' distributions. I myself find it more intuitive to think through the second point using the topic intuition. Take this path through the tree in Fig. 5 of Blei et al 2003 for example: The top node is English function words; the right node of the second level are terms related to machine learning; the right-most on the third level holds reinforcement learning terms. In that example, you can only get to the topic of reinforcement learning by going through that of machine learning. Much like how it's difficult to imagine writing a document about reinforcement learning without using terms from machine learning.
