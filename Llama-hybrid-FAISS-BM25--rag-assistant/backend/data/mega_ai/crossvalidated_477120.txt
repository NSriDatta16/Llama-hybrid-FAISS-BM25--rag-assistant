[site]: crossvalidated
[post_id]: 477120
[parent_id]: 476170
[tags]: 
Let $q$ be the density of your true data-generating process and $f_\theta$ be your model-density. Then $$KL(q||f_\theta) = \int q(x) log\left(\frac{q(x)}{f_\theta(x)}\right)dx = -\int q(x) \log(f_\theta(x))dx + \int q(x) \log(q(x)) dx$$ The first term is the Cross Entropy $H(q, f_\theta)$ and the second term is the (differential) entropy $H(q)$ . Note that the second term does NOT depend on $\theta$ and therefore you cannot influence it anyway. Therfore minimizing either Cross-Entropy or KL-divergence is equivalent. Without looking at the formula you can understand it the following informal way (if you assume a discrete distribution). The entropy $H(q)$ encodes how many bits you need if you encode the signal that comes from the distribution $q$ in an optimal way. The Cross-Entropy $H(q, f_\theta)$ encodes how many bits on average you would need when you encoded the singal that comes from a distribution $q$ using the optimal coding scheme for $f_\theta$ . This decomposes into the Entropy $H(q)$ + $KL(q||f_\theta)$ . The KL-divergence therefore measures how many additional bits you need if you use an optimal coding scheme for distribution $f_\theta$ (i.e. you assume your data comes from $f_\theta$ while it is actually generated from $q$ ). This also explains why it has to be positive. You cannot be better than the optimal coding scheme that yields the average bit-length $H(q)$ . This illustrates in an informal way why minimizing KL-divergence is equivalent to minimizing CE: By minimzing how many more bits you need than the optimal coding scheme (on average) you of course also minimize the total amount of bits you need (on average) The following post illustrates the idea with the optimal coding scheme: Qualitively what is Cross Entropy
