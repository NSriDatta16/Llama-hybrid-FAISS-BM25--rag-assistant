[site]: crossvalidated
[post_id]: 515186
[parent_id]: 
[tags]: 
why increasing diversity in ensemble method cause lower variance?

I am reading hands on machine learning . Bootstrapping introduces a bit more diversity in the subsets that each predictor is trained on, so bagging ends up with a slightly higher bias than pasting, but this also means that predictors end up being less correlated so the ensembleâ€™s variance is reduced. I think in this situation model do not have many unique instances and it cause for raising bias because it can not predict best parammetr but I dont have any Idea why variance decreases. I know about bias-variance trade off but I specially want to know what is happening in this situation.
