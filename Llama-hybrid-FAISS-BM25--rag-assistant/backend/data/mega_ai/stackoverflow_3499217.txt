[site]: stackoverflow
[post_id]: 3499217
[parent_id]: 3496011
[tags]: 
Naive Bayes probabilistic classifiers are commonly-used in text categorization. The basic idea is to use the joint probabilities of words and categories to estimate the probabilities of categories given a document. The naive part of such a model is the assumption of word independence. The simplicity of this assumption makes the computation of the Naive Bayes classifier far more efficient than the exponential complexity of non-naive Bayes approaches because it does not use word combination as predictors. If the task is to classify a test document into a single class, then the class with the highest posterior probability is selected. Here is one reference: [1] Tom Mitchell, "Machine Learning", McGraw-Hill, 1997. (Section 6.10) If you assume each question category as a text type then you can use text categorization. Naive Bayes classifier is based on Bayes theorem where you assume that all the features(or attribute) are independent. It's very easy to implement. You can find many software package with the implementation. e1071 Package in R implements it. Here is the sample code in R which uses naive bayes classifier: N
