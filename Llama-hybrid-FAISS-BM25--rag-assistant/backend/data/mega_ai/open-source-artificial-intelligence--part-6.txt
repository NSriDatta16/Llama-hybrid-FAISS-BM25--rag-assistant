 AI models to get AI to help develop more devastating terrorist schemes. The main barrier to developing real-world terrorist schemes lies in stringent restrictions on necessary materials and equipment. Furthermore, the rapid pace of AI advancement makes it less appealing to use older models, which are more vulnerable to attacks but also less capable. In July 2024, the United States released a presidential report saying it did not find sufficient evidence to restrict revealing model weights. Equity, social, and ethical implications There have been numerous cases of artificial intelligence leading to unintentionally biased products. Some notable examples include AI software predicting higher risk of future crime and recidivism for African-Americans when compared to white individuals, voice recognition models performing worse for non-native speakers, and facial-recognition models performing worse for women and darker-skinned individuals. Researchers have also criticized open-source artificial intelligence for existing security and ethical concerns. An analysis of over 100,000 open-source models on Hugging Face and GitHub using code vulnerability scanners like Bandit, FlawFinder, and Semgrep found that over 30% of models have high-severity vulnerabilities. Furthermore, closed models typically have fewer safety risks than open-sourced models. The freedom to augment open-source models has led to developers releasing models without ethical guidelines, such as GPT4-Chan. Data quality There are numerous systemic problems that may contribute to inequitable and biased AI outcomes, stemming from causes such as biased data, flaws in model creation, and failing to recognize or plan for the possibility of these outcomes. As highlighted in research, poor data quality—such as the underrepresentation of specific demographic groups in datasets—and biases introduced during data curation lead to skewed model outputs. A study of open-source AI projects revealed a failure to scrutinize for data quality, with less than 28% of projects including data quality concerns in their documentation. This study also showed a broader concern that developers do not place enough emphasis on the ethical implications of their models, and even when developers do take ethical implications into consideration, these considerations overemphasize certain metrics (behavior of models) and overlook others (data quality and risk-mitigation steps). Transparency and "black boxes" Another key concern with many AI systems with respect to issues such as safety and bias is their lack of transparency. Many open-source AI models operate as "black boxes", where their decision-making process is not easily understood, even by their creators. This lack of interpretability can hinder accountability, making it difficult to identify why a model made a particular decision or to ensure it operates fairly across diverse groups. Furthermore, when AI models are closed-source (proprietary), this can facilitate biased systems slipping through the cracks, as was the case for numerous widely adopted facial recognition systems. These hidden biases can persist when those proprietary systems fail to publicize anything about the decision process which could help reveal those biases, such as confidence intervals for decisions made by AI. Especially for systems like those used in healthcare, being able to see and understand systems' reasoning or getting "an [accurate] explanation" of how an answer was obtained is "crucial for ensuring trust and transparency". Frameworks for improvement Efforts to counteract these challenges have resulted in the creation of structured documentation frameworks that guide the ethical development and deployment of AI: Model Cards: Introduced in a Google research paper, these documents provide transparency about an AI model's intended use, limitations, and performance metrics across different demographics. They serve as a standardized tool to highlight ethical considerations a