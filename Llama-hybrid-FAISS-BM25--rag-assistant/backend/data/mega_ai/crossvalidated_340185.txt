[site]: crossvalidated
[post_id]: 340185
[parent_id]: 340175
[tags]: 
The main reason that $t$-SNE is not used in classification models is that it does not learn a function from the original space to the new (lower) dimensional one. As such, when we would try to use our classifier on new / unseen data we will not be able to map / pre-process these new data according to the previous $t$-SNE results. There is work on training a deep neural network to approximate $t$-SNE results (e.g., the "parametric" $t$-SNE paper) but this work has been superseded in part by the existence of (deep) autoencoders . Autoencoders are starting to be used as input / pre-processors to classifiers (especially DNN) exactly because they get very good performance in training as well as generalise naturally to new data. $t$-SNE can be potentially used if we use a non-distance based clustering techniques like FMM (Finite Mixture Models ) or DBSCAN ( Density-based Models ). As you correctly note, in such cases, the $t$-SNE output can quite helpful. The issue in these use cases is that some people might try to read into the cluster placement and not only the cluster membership. As the global distances are lost, drawing conclusions from cluster placement can lead to bogus insights. Notice that just saying: " hey, we found all the 1 s cluster together " does not offer great value if cannot say what they are far from. If we just wanted to find the 1 's we might as well have used classification to begin with (which bring us back to the use of autoencoders).
