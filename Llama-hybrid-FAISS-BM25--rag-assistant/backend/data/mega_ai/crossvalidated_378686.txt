[site]: crossvalidated
[post_id]: 378686
[parent_id]: 
[tags]: 
Use internal representation of autoencoder for anomaly detection

I've trained an autoencoder to recognize 'positive' time series (the network is a simple fully connected network, no recurrent layers). The problem is that from what my advisor says, I should try to detect anomalies using some statistics on the latent space (like difference between histograms of latent space between good and outlier data), but when I predict time series with outliers I get the same internal representation as with the good data. I believe this is due to the fact that my network can only reproduce the normal data. Do you have any hints? Thanks
