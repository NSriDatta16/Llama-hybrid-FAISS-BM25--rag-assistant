[site]: datascience
[post_id]: 93366
[parent_id]: 93344
[tags]: 
Technically BOW includes all the methods where words are considered as a set, i.e. without taking order into account. Thus TFIDF belongs to BOW methods: TFIDF is a weighting scheme applied to words considered as a set. There can be many other options for weighting the words in a set. Compared to regular TF-weighted BOW, the TFIDF weighting scheme gives more weight to words which appear in fewer documents and less weight to words which appear in many documents. The rationale is that a word which appears in many documents is unlikely to be relevant since it doesn't help selecting the most similar document. Typically the most frequent words are grammatical words (also called stop words, e.g. determiners, pronouns, etc.), but in a corpus made of sci-fi books for example some words such as "robot" or "planet" will also be very common. On the contrary a word like "elephant" would be very rare in a sci-fi context, so it is given more weight because it's more discriminative. This is meaningful in Information Retrieval tasks where the goal is to find a document similar to a query, and by extension it's useful in most tasks where the goal is to compare text documents by their semantic similarity. It is not meaningful and often counter-productive in classification tasks related to the style of text, as opposed to its semantic content. Note that Okapi BM25 is a similar weighting scheme which is not as famous as TFIDF but has been proved to work better in most applications.
