[site]: crossvalidated
[post_id]: 561462
[parent_id]: 
[tags]: 
Backpropagation final layer δ term

I'm trying to understand the calculation for the gradient of the blue weight shown in the NN below. In Andrew Ng's Machine Learning coursera module, the δ term for the final layer of the NN is: However, in other sources , the δ term for the final layer is: where the red box has been added to the formula (the derivative of the sigmoid activation function). Both sources then multply δ by the activation of the previous node to get the gradient: What is the reason for the discrepancy between these two calculations? Could it be because Andrew assumes the activation function of the final layer is g(z) = z?
