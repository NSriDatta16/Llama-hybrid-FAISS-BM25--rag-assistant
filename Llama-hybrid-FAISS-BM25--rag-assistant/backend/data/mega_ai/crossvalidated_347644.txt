[site]: crossvalidated
[post_id]: 347644
[parent_id]: 347623
[tags]: 
Reinforcement learning does not itself require normalised state or action data. However, the RL context does not change neural network behaviour in this respect. Neural networks work better with normalised data. So, yes, the advice should be to normalise the data. You could either do that as part of state representation, or just before any input to the neural network model. What will work for you depends on the framework you are using. I suspect if you are using OpenAI's baselines that you will need to adjust the state representation. Although I am not sure, I have not used it, I took a quick look at baselines/ppo1/mlp_policy.py but there is too much abstraction for me to quickly assess how the state representation is processed before getting to the NNs. It is unlikely at the outset that you will know the population of state and action representations under an optimal policy well enough to normalise by mean and standard deviation, so you may need to pick some other range adjusting approaches, or simply guess suitable multipliers and offsets. Normalisation does not need to be perfect, for NNs you just want to avoid being out by a significant factor. If a specific input scales over a few orders of magnitude, and this is an important factor, then maybe work with (a multiple and offset of) the log of that input.
