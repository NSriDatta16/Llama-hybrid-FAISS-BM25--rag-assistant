[site]: crossvalidated
[post_id]: 631559
[parent_id]: 631540
[tags]: 
It seems that Robert has already addressed the assumptions required in regression and some of the theoretical underpinnings of regression. Along with Robert's useful answer, I would like to highlight some additional reasons, which are similarly related to each other: Regression predicts rather than describes . The whole reason regression was invented was to understand sweet pea genetics . If Galton could predict with some reliability how the plants would grow, he could determine what would be useful and what would be wasteful. The reason this method became useful was due to it's ability to model in variation. A naive estimate of the predicted value of an outcome is generally considered the mean (but could be other values). A regression makes this idea conditional upon variation based off different inputs. This was one of the beginning problems that Galton faced, which was taking rather uninformative descriptive data like the mean and transforming it into a modeling tool which makes the mean conditional (see plots of the mean and conditional mean regression below). If we consider this a prediction problem, we can see how the first plot is bad and the second plot is at least in some part better. Let's say we are trying to predict $y$ here. If we just guess that it will be the mean of $y$ , we will obviously miss by a lot, but it won't be totally wrong...any given $y$ here should hover around the mean. However, if we make that conditional on $x$ , we can make better guesses. For example, we can probably assume that $x=1$ should yield an outcome of $y=2$ . This leads into my next point. Fitting is not the same as overfitting . I gather from the question that part of your issue is with overfitting , where any naive line is fit to data without consideration of the data generating process (DGP). Lets say we have an archer who is shooting arrows at a target: Visually inspecting this relationship, we should predict to a degree that the relationship has a downward-facing parabolic function...the arrow should go up in height, then go down in height. For simplicity, we pretend the height starts and ends at zero feet from the ground, and hits a maximum around 7 feet: We could just fit any line here: But you can see both of these are poor approximations of the DGP (I made a more extreme version here than my original answer for a better illustration). The first linear fit would make for terrible predictions. We would expect the arrow to shoot downward and never stop. The other is overfit and makes similarly bad predictions (its a 20th order polynomial which interpolates badly), so where we predict the arrow is all over the place. However, if we simply understand that this is a downward parabola, we can fit a much simpler and reasonable relationship that makes for better predictions: Our assumptions allow these predictions to be better. Part of your question was about which conditions allow for a regression to function well. Robert already noted some, but I'll highlight that linearity is an important one which you can see from my example (note that we wish for linearity in the parameters, even with nonlinear data). Another example is heterogeneity of variance. Here you can see from the Engel data from Koenker & Hallock, 2001 that the dispersion of data points is rather unequal. The data first clusters to the bottom and then fans out across the scatterplot. Once again, we fit a linear regression: Because our data doesn't meet the heterogeneity of variance assumption, predictions are once again potentially poor. We can guess with considerable certainty what low values of income should give us in terms of food expenditure, but that becomes a lot more uncertain as we move to the top right. Thus our predictions are only as good as our regressions meet the minimum standards for fitting. Edit I just saw that you also asked about what math is necessary to learn the intuition behind regression. A lot can be learned from basic algebra (such as what a "function" is), calculus (how things like the slope work for linear and nonlinear functions), linear algebra (learning things like how the coefficients are constructed), probability (understanding hypothesis testing in regression) and of course statistics. At a higher level, you may need to take classes that combine all this math in something like a linear models class. However, much of this can also be simply understood by experience with data and simulation. In fact, much of what I know now about regression is a combination of math I have learned and simulation based around that math.
