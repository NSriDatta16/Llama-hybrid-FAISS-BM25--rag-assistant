[site]: crossvalidated
[post_id]: 164519
[parent_id]: 
[tags]: 
Find the unique MVUE

This question is from Robert Hogg's Introduction to Mathematical Statistics 6th Version problem 7.4.9 at page 388. Let $X_1,...,X_n$ be iid with pdf $f(x;\theta)=1/3\theta,-\theta zero elsewhere, where $\theta>0$ . (a) Find the mle $\hat{\theta}$ of $\theta$ (b) Is $\hat{\theta}$ a sufficient statistics for $\theta$ ? Why ? (c) Is $(n+1)\hat{\theta}/n$ the unique MVUE of $\theta$ ? Why ? I think I can solve (a) and (b), but I am confused by (c). For (a): Let $Y_1 be the order statistics. $L(\theta;x)=\frac{1}{3\theta}\times\frac{1}{3\theta}\times...\times\frac{1}{3\theta}=\frac{1}{(3\theta)^n}$ when $-\theta and $y_n ;elsewhere $L(\theta;x)=0$ $\frac{dL(\theta;x)}{d\theta}=-n(3\theta)^{n-1}$ , since $\theta>0$ , we can see this derivative is negative, so likelihood function $L(\theta;x)$ is decreasing. From $(-\theta and $ y_n , $\Rightarrow$ $(\theta>-y_1 $ and $\theta>y_n/2), \Rightarrow \theta>max(-y_1,y_n/2)$ $L(\theta,x)$ is decreasing, so when $\theta$ has the samllest value the likelihood function will achieve maximum, since $\theta>max(-y_1,y_n/2)$ , when $\theta=max(-y1,y_n/2)$ , the likelihood function will achieve the maximum value. $\therefore$ mle $\hat{\theta}=max(-y_1,y_n/2)$ For (b): $f(x_1;\theta)f(x_2;\theta)...f(x_n;\theta)=\frac{1}{(3\theta)^n}\prod_{i}^{n} I(-\theta $\therefore$ By factorization theorem of Neyman, $y_n=max(x_i)$ is a sufficient statistic for $\theta$ . Therefore, $y_n/2$ is also a sufficient statisitc Samely, $f(x_1;\theta)f(x_2;\theta)...f(x_n;\theta)=\frac{1}{(3\theta)^n}\prod_{i}^{n} I(-\theta -\theta)\times 1$ $\therefore$ By factorization theorem of Neyman, $y_1=min(x_i)$ is a sufficient statistic for $\theta$ . Therefore, $-y_1$ is also a sufficient statisitc. For (c): First, we find the CDF of $X$ $F(x)=\int_{-\theta}^{x}\frac{1}{3\theta}dt=\frac{x+\theta}{3\theta},-\theta Next, we can find pdf for both $Y_1$ and $Y_n$ from the formula of the book for the order statistics. $f(y_1)=\frac{n!}{(1-1)!(n-1)!}[F(y_1)]^{1-1}[1-F(y_1)]^{n-1}f(y_1)=n[1-\frac{y_1+\theta}{3\theta}]^{n-1}\frac{1}{3\theta}=n\frac{1}{(3\theta)^n}(2\theta-y_1)^{n-1}$ Samely, $f(y_n)=n(\frac{y_n+\theta}{3\theta})^{n-1}\frac{1}{3\theta}=n\frac{1}{(3\theta)^n}(y_n+\theta)^{n-1}$ Next, we show the completeness of family of pdf for $f(y_1)$ and $f(y_n)$ $E[u(Y_1)]=\int_{-\theta}^{2\theta}u(y_1)n\frac{1}{(3\theta)^n}(2\theta-y_1)^{n-1}dy_1=0 \Rightarrow \int_{-\theta}^{2\theta}u(y_1)(2\theta-y_1)dy_1=0$ . By $FTC$ (derivate the integral) we can show $u(\theta)=0$ for all $\theta>0$ . Therefore, family of pdf $Y_1$ is complete.. Samely, still by $FTC$ , we can show that family of pdf $Y_n$ is complete. The problem is now we need to show that $\frac{(n+1)\hat{\theta}}{n}$ is unbiased. When $\hat{\theta}=-y_1$ $E(-y_1)=\int_{-\theta}^{2\theta}(-y_1)\frac{n}{(3\theta)^n}(2\theta-y_1)^{n-1}dy_1=\frac{1}{(3\theta)^n}\int_{-\theta}^{2\theta}y_1d(2\theta-y_1)^n$ We can solve the integral by integrate by parts $E(-y_1)=\frac{1}{(3\theta)^n}[y_1(2\theta-y_1)^n\mid_{-\theta}^{2\theta}-\int_{-\theta}^{2\theta}(2\theta-y_1)^ndy_1]=\frac{1}{(3\theta)^n}[\theta (3\theta)^n-\frac{(3\theta)^{n+1}}{n+1}]=\theta-\frac{3\theta}{n+1}=\frac{(n-2)\theta}{n+1}$ $\therefore E(\frac{(n+1)\hat{\theta}}{n})=\frac{n+1}{n}E(-y_1)=\frac{n+1}{n}\frac{(n-2)\theta}{n+1}=\frac{n-2}{n}\theta$ Therefore, $\frac{(n+1)\hat{\theta}}{n}$ is not an unbiased estimator of $\theta$ when $\hat{\theta}=-y_1$ When $\hat{\theta}=y_n/2$ $E(Y_n)=\int_{-\theta}^{2\theta}y_n\frac{n}{(3\theta)^n}(y_n+\theta)^{n-1}dy_n=\frac{1}{(3\theta)^n}\int_{-\theta}^{2\theta}y_nd(y_n+\theta)^n=\frac{1}{(3\theta)^n}[y_n(y_n+\theta)^n\mid_{-\theta}^{2\theta}-\int_{-\theta}^{2\theta}(y_n+\theta)^ndy_n]=\frac{1}{(3\theta)^n}[2\theta(3\theta)^-\frac{(3\theta)^{n+1}}{n+1}]=2\theta-\frac{3\theta}{n+1}=\frac{2n-1}{n+1}\theta$ $\therefore E(\frac{(n+1)\hat{\theta}}{n})=\frac{n+1}{n}E(Y_n/2)=\frac{n+1}{2n}E(Y_n)=\frac{n+1}{2n}\frac{2n-1}{n+1}\theta=\frac{2n-1}{2n}\theta$ Still, $\frac{(n+1)\hat{\theta}}{n}$ is not an unbiased estimator of $\theta$ when $\hat{\theta}=y_n/2$ But the book's answer is that $\frac{(n+1)\hat{\theta}}{n}$ is an unique MVUE. I don't understand why it is a MVUE if it is a biased estimator. Or my calculations are wrong, please help me to find the mistakes, I can give you more detailed calculations.
