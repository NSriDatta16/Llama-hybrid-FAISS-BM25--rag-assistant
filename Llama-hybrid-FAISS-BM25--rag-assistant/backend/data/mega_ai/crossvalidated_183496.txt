[site]: crossvalidated
[post_id]: 183496
[parent_id]: 183236
[tags]: 
It is common to whiten data before using k-means. The reason is that k-means is extremely sensitive to scale, and when you have mixed attributes there is no "true" scale anymore. Then you have to normalize, standardize, or whiten your data. None is perfect, but whitening will remove global correlation which can sometimes give better results. PCA/whitening is $O(n\cdot d^2 + d^3)$ since you operate on the covariance matrix. To my understanding, the relationship of k-means to PCA is not on the original data . It is to using PCA on the distance matrix (which has $n^2$ entries, and doing full PCA thus is $O(n^2\cdot d+n^3)$ - i.e. prohibitively expensive, in particular compared to k-means which is $O(k\cdot n \cdot i\cdot d)$ where $n$ is the only large term), and maybe only for $k=2$. K-means is a least-squares optimization problem, so is PCA. k-means tries to find the least-squares partition of the data. PCA finds the least-squares cluster membership vector. The first Eigenvector has the largest variance, therefore splitting on this vector (which resembles cluster membership, not input data coordinates!) means maximizing between cluster variance . By maximizing between cluster variance, you minimize within-cluster variance, too. But for real problems, this is useless. It is only of theoretical interest.
