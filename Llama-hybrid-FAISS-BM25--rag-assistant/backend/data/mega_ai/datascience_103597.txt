[site]: datascience
[post_id]: 103597
[parent_id]: 103576
[tags]: 
The ever-present danger with high-dimensional data is overfitting. When there are a lot of features ( p ) and relatively few examples ( n ), it is easy for models to find spurious relationships between features and target. There are two generic solutions to this problem: dimensionality reduction and regularization. Dimensionality reduction reduces the number of features prior to training. Regularization penalizes the model for adding complexity. For example, L1 or L2 regularization are commonly used in linear models to penalize the size of coefficients. This encourages models to "ignore" certain features by reducing their coefficient to zero. So to your question directly: the reason that SVMs work well with high-dimensional data is that they are automatically regularized, and regularization is a way to prevent overfitting with high-dimensional data.
