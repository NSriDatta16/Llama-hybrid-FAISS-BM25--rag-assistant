[site]: datascience
[post_id]: 39946
[parent_id]: 39929
[tags]: 
For neural networks, there is another reason. Sigmoid function provides values between 0 and 1; if the task is binary classification, you'd use a sigmoid function at the output. For another task, you might have used tanh function at some layer, a centered input to that neuron works computationally well. The reason is as follows; outside the range of some input values, the derivatives of the activation functions will be close to zero. At those points, the gradient descent steps will be extremely slow due to the small weight updates. Directly think of the 2-D function graph of the related activation function and think of the range of input values where the derivatives are close to zero. This is why Rectified Linear Unit (ReLU) or Leaky Rectified Linear Unit (Leaky ReLU) outperforms the others in most of the tasks, with the constant derivative of 1, when the input is greater than 1. Also refer to: https://stats.stackexchange.com/questions/51012/must-i-normalize-inputs-into-a-perceptron-that-uses-a-sigmoid-activation-functio
