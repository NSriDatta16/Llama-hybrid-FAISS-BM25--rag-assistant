[site]: crossvalidated
[post_id]: 192134
[parent_id]: 192132
[tags]: 
First off, there are basic probability axioms: you have a sample space of all possible observable values that arise from conducting an experiment, like the face of a thrown die. Put loosely, the probability of events must sum to 1 (among some other considerations). Standardizing (summing over some weights, and dividing by the sum) is a frequently used way of calculating probabilities. In your experiment, are events A, B, C, and D mutually exclusive in the sense that your sample space is {A, B, C, D}? If the weights represent an unstandardized marginal frequencies, standardization is fine. Or is each calculated sequentially so you observe {{}, A, AB, ABC, ABCD, B, BC, BCD, C, CD, D} possibly and the frequencies correspond to marginal probabilities? In that case, C might really show up in 90% of trials and B in 50%. (if these were mutually exclusive outcomes, that would violate probability axioms since their probability sum is 140%). Second, the question "what makes a probability" is philosophical in nature. Bayesians take probability to mean a degree of belief whereas frequentists take it to mean a frequency in the sense of a thought experiment involving independent replications of a particular experiment or data collection procedure.
