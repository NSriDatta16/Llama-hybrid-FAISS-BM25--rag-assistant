[site]: datascience
[post_id]: 49282
[parent_id]: 49276
[tags]: 
In general,there are two ways for finding document-document similarity TF-IDF approach Make a text corpus containing all words of documents . You have to use tokenisation and stop word removal . NLTK library provides all . Convert the documents into tf-idf vectors . Find the cosine-similarity between them or any new document for similarity measure. You can use libraries like NLTK , Scikit learn ,Gensim for Tf-Idf implementation . Gensim provides many additional functionality . See : https://www2.cs.duke.edu/courses/spring14/compsci290/assignments/lab02.html Word Embedding Google's Doc2Vec ,which is available in Gensim library can be used for document similarity .Additonaly,teh Doc2Vec model itself can compute the similarity score ( no cosine or anything needed her ) . You just need the vectorise the docs by tokenizing ( use NLTK ) and make a Doc2vec model using gensim and fins similarity and many using Gensim inbuilt methods like model.n_similarity for similarity between two documents . Other Additionally,since your aim is to cluster documents,you can try Topic Modelling using LDA ( Latent Dirichet Allocation) or LSI ( Latent Semantic Indexing ) .
