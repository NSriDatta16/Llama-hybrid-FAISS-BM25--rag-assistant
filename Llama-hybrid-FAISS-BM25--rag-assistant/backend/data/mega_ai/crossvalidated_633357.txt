[site]: crossvalidated
[post_id]: 633357
[parent_id]: 
[tags]: 
If you drop the "1-" in the formula of R2 and calculate the adjusted R2, does that metric mean anything?

First off, sorry for the convoluted title, but I didn't manage to come up with a shorter one. Background: I have some 2D particle tracks that I would like to fit with polynomials. The tracks are parameterized by time, so we can fit their coordinates $x_i(t)$ and $y_i(t)$ independently (I believe). For simplicity let's just focus on $x_i(t)$ . I would like to get an idea which polynomial order is the most appropriate for my data, so I use the adjusted coefficient of determination, $R^2_{adj}$ , as a metric. I didn't know how to calculate $R^2_{adj}$ though, so asked ChatGPT and it actually provided the correct formula: $$R^2_{adj} = 1 - \left[\frac{(1-R^2)(n-1)}{n-k-1}\right], \tag{1}$$ where $n$ is the number of data points, $k$ the polynomial order, and $$R^2 = 1 - \frac{\sum_i^n(x_i - f_i)^2}{\sum_i^n(x_i-\bar{x})^2}, \tag{2}$$ is the coefficient of determination, where $f_i$ are the values predicted by the polynomial fit, and $\bar{x}$ is the mean of the observed data. Now at first, I didn't believe it of course, so I double checked with other resources on the internet, and came across this post . Unfortunately, the variable names are somewhat inconsistent in this answer, which makes it a bit confusing, but the main point is that they calculate their $R^2$ as $$R^2 = \frac{\sum_i^n(f_i-\bar{x})^2}{\sum_i^n(x_i-\bar{x})^2}, \tag{3}$$ which I found really confusing. But apparently, this is also valid in case of linear regression (correct?). This may seem irrelevant, but now I come to my main point: Main issue: To see which polynomial order might be the most appropriate, I fitted 2nd, 3rd, and 4th order polynomials to my data, calculated the median $R^2_{adj}$ over all tracks, and also plotted the resulting $R^2_{adj}$ . What I found surprised me: The median $R^2_{adj}$ were extremely similar for all orders, and very close to 1: 2nd order: 0.9998917060044747 3rd order: 0.9999378575929398 4th order: 0.9999525950358665 And the corresponding plot looked like this: This looks very fishy to me, since I know that at least the third-order polynomials generally fit my tracks much better than the second-order polynomials do. Am I doing something wrong? Is there maybe something like a 2D $R^2_{adj}$ that I should rather look at than the $R^2_{adj}$ for the $x_i$ and $y_i$ independently? But aside from that, here is why I brought up that second way of calculating $R^2$ (Eq. 3), and what I refer to in the title. When I tested this other way, I also experimented a little, and found that if I drop the $-1$ in Equation (2) (or add it in Equation (3)), I get this for the $R^2_{adj}$ plot instead: Now I can actually see a noticeable difference between the different polynomial orders, more like what I acutally expected. But I wonder if this "metric" means anything. Can I learn anything from this? Finally for reference, here is the Python code that ChatGTP provided for the 1D case, which I also use: import numpy as np # Example data x = np.array([1, 2, 3, 4, 5]) y = np.array([2, 4, 5, 4, 5]) # Fit a polynomial of degree 2 (you can change the degree as needed) coefficients = np.polyfit(x, y, 2) # Calculate R-squared p = np.poly1d(coefficients) y_pred = p(x) r_squared = 1 - np.sum((y - y_pred)**2) / np.sum((y - np.mean(y))**2) # Calculate adjusted R-squared n = len(y) k = len(coefficients) - 1 # Degree of the polynomial adjusted_r_squared = 1 - ((1 - r_squared) * (n - 1)) / (n - k - 1) print(f"R-squared: {r_squared}") print(f"Adjusted R-squared: {adjusted_r_squared}")
