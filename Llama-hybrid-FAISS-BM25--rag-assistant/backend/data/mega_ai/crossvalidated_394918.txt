[site]: crossvalidated
[post_id]: 394918
[parent_id]: 
[tags]: 
Determining error between two surfaces given same discrete inputs?

Apologies if this isnt the best SE forum to ask on, but it seems relevant here. I have, as an output of a machine learning algorithm, a surface in z, which has known increments along x and y. These points along x and y match exactly to a surface which I am comparing the output of my algorithm against in order to get a metric of fit, or error. I have been struggling to find an optimal way of calculating this, and can't find any good resources on different options that I have. I have tried simple pointwise subtraction of the surfaces, which I take the absolute value and summation of, and I have tried squared versions of this, as well as calculating MSE, RMSE, and R2, but each of these encounters different problems. The best version I have tried so far is the simple subtraction squared, and the other version tend to give "flatter", more spread out surfaces. I suspect this may be because the values in z of my surfaces are much smaller than 1, on the order of 0.001. I have thus tried adding 1 and then calculating the different metrics, but this seemed to make the problem worse. I was wondering if any of you knew of any better ways of fitting, or any good resources on different options and which of these work in different situations. Thanks!
