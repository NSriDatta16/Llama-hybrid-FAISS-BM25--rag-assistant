[site]: datascience
[post_id]: 118273
[parent_id]: 
[tags]: 
Specifics about ChatGPT's Architecture

Does anyone know of reliable sources that have written about the architecture of OpenAI's ChatGPT - specifically regarding the following?: Number of hidden layers Number of attention heads Dimensions of its hidden layers Sequence length in terms of number of tokens Number of parameters in the version of the model currently free for public preview. My understanding is there are multiple versions of ChatGPT. Please note: I'm familiar with GPT-3 already. However, I am not certain that ChatGPT's architecture is the same. Open to contrary evidence, if any.
