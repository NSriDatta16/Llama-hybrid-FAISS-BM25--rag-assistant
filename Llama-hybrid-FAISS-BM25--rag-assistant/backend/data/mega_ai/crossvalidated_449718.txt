[site]: crossvalidated
[post_id]: 449718
[parent_id]: 
[tags]: 
Why does the first forward pass in a neural-network (NN) classification model computes to zero for all classes before final activation?

Suppose weights of NN are Gaussian spread initialization then forward pass for all the inputs will evaluate to zero which computes to 0.69 ( $-\log_{e}0.5 \approx 0.69 $ , since sigmoid (0) = 0.5) average loss at least in the first pass. For three classes it would be $1.0986$ ( $-\log_{e}\frac{1}{3}$ ) and so on. I understand when weights are zero-initialized, but why does this happen when weights are Normally distributed to start with?
