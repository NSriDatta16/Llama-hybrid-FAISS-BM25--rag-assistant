[site]: crossvalidated
[post_id]: 56626
[parent_id]: 56618
[tags]: 
I think the frequentist analogues are that of estimating equations to posterior mean and maximum likelihood to posterior mode. They are not equivalent by any means, but have some important similarities. When you estimate a posterior mode, you're doing Bayesian "maximum likelihood". The posterior mode is not often preferred because the sampling distribution of this value can be very irregular. That's for two reasons: the posterior may have many local maximae and mode estimation is very inefficient except when making strong assumptions. These points are moot when doing exact Bayes, in which case the posterior is known to fall into a parametric family. But doing Gibbs Sampling all higgeldy piggeldy will not guarantee that the posterior falls into any "known" family of distributions. In basic probability problems, it's easy to obtain exact expressions for posteriors when there are constraining assumptions made about the distribution of sample data and the specification of the prior. In practice, this is rarely the case and posteriors in finite (small) samples can be bumpy, ugly things. The sampling distribution of the posterior mode does have some convergence properties, like any estimator. But none so well understood and explored as those of the posterior mean. It's so often the efficient estimator in frequentist problems, little wonder it is preferred in the Bayesian world as well.
