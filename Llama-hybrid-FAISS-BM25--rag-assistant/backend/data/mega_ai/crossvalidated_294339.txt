[site]: crossvalidated
[post_id]: 294339
[parent_id]: 
[tags]: 
Are there posterior predictive check "point-values" in Bayesian modeling?

One standard for evaluating goodness-of-fit in Bayesian modeling is posterior predictive checks. This means simultaneously visualizing the empirical distribution of the data along with the posterior predictive distribution, correct? Do people use some more quantitative measure, like KL divergence between the empirical and posterior predictive distribution? Some context: I am working with hierarchical model in Stan. My stakeholders want a quantity of goodness-of-fit, like R-squared. I know one thing people do is cross-validation, but this KL-divergence idea seems to me to be something that people must have an opinion on, so I thought I'd ask.
