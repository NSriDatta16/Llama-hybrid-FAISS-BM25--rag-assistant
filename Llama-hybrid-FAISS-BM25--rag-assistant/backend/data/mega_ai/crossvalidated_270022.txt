[site]: crossvalidated
[post_id]: 270022
[parent_id]: 
[tags]: 
How do we know that our estimate of the mid-season batting average is actually good?

This question is based of David Robinson's excellent illustration of using the Beta distribution to estimate a (baseball) batter's season batting average in the middle of a season. To recap, the proportion of hits to at bats (aka batting average) is assumed to be (conditionally) Beta distributed with parameter values $\alpha = 81$ and $\beta = 219$, estimated using maximum likelihood with respect to the final observed batting averages in the Lahman dataset*. Then, an estimate of a batter's batting ability in the middle of the season is given by calculating the posterior distribution based on updating the Beta prior with these parameters: $$\mbox{Beta}(\alpha_0+\mbox{hits}, \beta_0+\mbox{misses}),$$ so that, if a batter in midseason had 100 hits out of 300 at bats, their season batting average $\mu$ could be approximated by the expected value of the posterior by: $$\hat{\mu} = \frac{81 + 100}{81 + 100 + 219 + 200} = 0.303.$$ My question is, on what basis do we know that this approximation is good/valid? As far as I can tell, the original estimates of the Beta prior's parameters at least have some guarantee of 'goodness' via the MLE method, but I am not sure what other metrics apply to our mid-season prediction. Take this to the extreme, one can apply this update methodology right up to after the second last game of the season. I think that this is unlikely to produce a better prediction than just using the batter's empirical average this season after this game. * In his excellent book , David shows how to write down regression equations for $\alpha$ and $\beta$ so that the estimated parameters, conditional on covariates, maximise the beta log-likelihood with respect to the final historical batting averages. The parameters estimated are different but I use the values from the old CV post for consistency.
