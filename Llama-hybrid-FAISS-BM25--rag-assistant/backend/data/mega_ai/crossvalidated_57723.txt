[site]: crossvalidated
[post_id]: 57723
[parent_id]: 57710
[tags]: 
The easiest solution would be to use a linear SVM, in which case the 'support vectors' can be combined together in a single weight vector. For kernel SVMs, you might want to look at budget SVMs, which focus on strategies to reduce the number of support vectors kept during the optimization. Here is one interesting reference with source code : Z. Wang, K. Crammer, and S. Vucetic, “Breaking the Curse of Kernelization : Budgeted Stochastic Gradient Descent for Large-Scale SVM Training,” Journal of Machine Learning Research, vol. 13, pp. 3103–3131, 2012. ( pdf )( source code )
