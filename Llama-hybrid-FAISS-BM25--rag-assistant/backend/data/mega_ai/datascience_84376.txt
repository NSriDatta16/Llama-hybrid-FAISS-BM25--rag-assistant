[site]: datascience
[post_id]: 84376
[parent_id]: 84372
[tags]: 
Catboost and LightGBM can handle categorical features. They're based on Decision trees (Random Forest is based on decision trees too), so you can use them (they're usually better than Random Forest), but they use more computational power comparing to Random Forest, yet you still can fine tune them (it's very easy with Catboost, LightGBM needs a little bit of practice), and still benefit from their encoding features, and you'll have less complicated pipeline.
