[site]: crossvalidated
[post_id]: 565741
[parent_id]: 325776
[tags]: 
We have to distinguish between Shallow Neural Networks (one hidden layer) and Deep Neural Networks (more than one hidden layer) since there is a difference. What I write below can also be found on the Wikipedia page Universal Approximation Theorem . Shallow Neural Networks: Pinkus showed in 1999 that Shallow Neural Networks with a continuous activation function have the universal approximation property on a compact set $K\subseteq \mathbb{R}$ if and only if the activation function is non-polynomial. The same article mentions that some discontinuous functions can also be used as activation function while preserving the universal approximation property for the networks. Deep Neural Networks: There are multiple different results. One of them is by Kidger and Lyon and is from 2020 . Here they show that Deep Neural Networks have the universal approximation property on a compact set $K\subseteq\mathbb{R}$ when their activation function is: Continuous Nonaffine (i.e. not a multivariate polynomial of degree less than 2) Differentiable with a continuous derivative which is different from 0 at at least one point (the exact technical formulation is slightly more strict). This shows one of the differences between Deep and Shallow Neural Networks, namely that Deep Neural Networks still have the universal approximation property when their activation function is a (nonaffine) polynomial. In the article, Kidger and Lyon extend the result in multiple ways. For instance, they show that the result still holds for some activation functions which are continuous but nowhere differentiable.
