[site]: crossvalidated
[post_id]: 584638
[parent_id]: 584567
[tags]: 
The short version is that there are two different reasons why we might use importance sampling. First, importance sampling is useful if we don't know how to sample from $p_z(z)$ . Second, it is also useful in the situation you draw, where $f(z)$ has very large peaks in the tail of $p_z(z)$ ; it gives us a more accurate estimate of the expectation. These are two different motivations why we might use importance sampling. Both are valid. I'll explain below in depth. Before I jump into the details of your question, let me start with some background, namely, the standard setting that motivates importance sampling. The standard setting is that we wish to estimate $$\mathbb{E}_{z \sim p_z(z)}[z],$$ but we do not have a way to sample from $p_z(z)$ . Since we don't have any way to sample from $p_z(z)$ , there is no way to use the standard estimator for the mean (drawing a few samples from the distribution and then computing the sample average), so we need some other way to estimate this expectation. Importance sampling gives us a solution to this problem. Importance sampling allows us to pick a distribution $q(z)$ that we do know how to sample from, and then estimate the expectation above by choosing samples distributed according to $z$ . In particular, it uses the fact that $$\mathbb{E}_{z \sim p_z(z)}[z] = \mathbb{E}_{z \sim q(z)}\left[{z p_z(x) \over q(z)}\right].$$ Notice that we do have a way to estimate the right-hand side using standard methods, by drawing samples from $q(z)$ , as long as we have a way to compute $p_z(z)$ for any given $z$ . There are many situations where we do know how to compute $p_z(z)$ for any given $z$ of interest, but we don't know how to efficiently draw a random sample from the distribution $z \sim p_z(z)$ . Now, your question asks about a slightly different situation. The above exposition talks about computing $\mathbb{E}[z]$ , but your question asks about $\mathbb{E}[f(z)]$ , which is slightly different. So now let's address your specific question. Why do we use importance sampling for this task? The answer is: it depends. There are two different reasons why we might use importance sampling, for this task. First situation: If you don't have a way to sample from $p_z(z)$ , but you do have a way to compute $p_z(z)$ , then all of the above discussion still applies, and we can use importance sampling in this generalized setting. In particular, we use the equality $$\mathbb{E}_{z \sim p_z(z)}[f(z)] = \mathbb{E}_{z \sim q(z)}\left[{f(z) p_z(x) \over q(z)}\right],$$ and we use standard methods to estimate the right-hand side by drawing several samples from $q(z)$ . So, in this setting, the reason we use importance sampling is because there is no other choice: we can't use simpler methods, so we're forced to use this more sophisticated method. Second situation: If we do have a way to sample from $p_z(z)$ , then we don't have to use importance sampling, but it might still be to our benefit to do so. In particular, your picture gives exactly the sort of situation that can occur, but you've drawn the wrong conclusion, because your intuition has led you astray. To help you improve your intuition, I suggest drawing the same picture, but now imagine that the height of the peak of $f(x)$ is $1000\times$ higher than the peak of $p_z(z)$ . Then it is hopefully clear that sampling from $p_z(z)$ is sub-optimal: most samples will be from the center of $p_z(z)$ , where $f(z)$ is close to zero. However, the expectation will be dominated by the area in the tail where $f(z)$ is large. Those samples will occur rarely if you sample from $p_z(z)$ , but their value will be so huge that they will dominate. The result is that naive sampling is inefficient: you need a lot of samples to make sure you obtain a few that are in the tail. The way that this shows up is that the variance of the standard estimator is very high, and the estimator will be unbiased in theory but in practice if you don't have a very large sample size, the estimator will often give values that are very wrong. Importance sampling offers a better method. Suppose we can find a distribution $q(z)$ that is approximately proportional to $p_z(z) f(z)$ , and suppose we can sample from $q(z)$ . Then it turns out that using importance sampling with $q(z)$ yields a better estimator than standard sampling from $p_z(z)$ . In particular, the variance of the importance sampling estimator will be lower (its estimates will tend to be more accurate). Formally, the optimal (most efficient) estimator sets $q(z)$ to be exactly proportional to $p_z(z) f(z)$ , and any distribution $q(z)$ that is close to that will be close to optimal. Hopefully, it is intuitive why this might be so. If the peak of $f(z)$ is $1000\times$ higher than you drew it here, it's best to bias your sampling towards the area where $f(z)$ is large, and then you want to correct for that bias by dividing by some appropriate constant. That's exactly what importance sampling does, if you use a distribution $q(z)$ that is proportional to $p_z(z) f(z)$ . At this point, you might be wondering, why not simply define $q^*(z) = p_z(z) f(z)$ , and then sample from $q^*(z)$ ? Well, two reasons. First, this is not a distribution; we need to renormalize it so it sums to one. So, the correct definition is $q^*(z) = p_z(z) f(z) / \int_t p_z(t) f(t) \; dt$ . This brings us to the second reason: we might not have any efficient way to sample from the resulting $q^*(z)$ . For starters, we might not have any efficient way to compute $\int_t p_z(t) f(t) \; dt$ , and in addition, just because you can compute $q^*(z)$ for any $z$ of your choice is not enough to let you sample from such a distribution of your choice. Importance sampling provides a solution to all of these challenges. It doesn't require us to be able to compute the optimal distribution $q^*(z)$ or to be able to sample from it. It lets us use any convenient distribution that is close to $q^*(z)$ .
