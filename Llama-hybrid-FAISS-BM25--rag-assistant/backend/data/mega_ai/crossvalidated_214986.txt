[site]: crossvalidated
[post_id]: 214986
[parent_id]: 214980
[tags]: 
When you compare likelihoods of models, you are saying the observed data is more likely to have been generated by model A than by model B. However, this is often not the goal in Machine Learning. In Machine Learning, the goal is typically to optimize on out of sample predictive accuracy. When this is the case, why* use some proxy (like model likelihood) to measure this when you can cut to the chase and try to directly measure this predictive accuracy? Techniques such as cross-validation (CV) and it's variants (k-fold CV, leave-one-out CV, test-training CV) directly measure this by building a model on a subset of your data and predicting the results on the remaining data. Additionally, many Machine Learning algorithms (such as SVM and CART) are non-parametric and thus have no likelihoods associated with them. In summary, if you have a Machine Learning algorithm that has a likelihood associated with it, you can use it. But chances are it's not what you're looking for. *There are some situations where you have to use some sort of proxy for cross-validation. For example, small sample size.
