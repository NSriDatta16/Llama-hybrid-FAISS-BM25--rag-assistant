[site]: crossvalidated
[post_id]: 222810
[parent_id]: 221715
[tags]: 
You can use doc2vec similar to word2vec and use a pre-trained model from a large corpus. Then use something like .infer_vector() in gensim to construct a document vector. The doc2vec training doesn't necessary need to come from the training set. Another method is to use an RNN, CNN or feed forward network to classify. This effectively combines the word vectors into a document vector. You can also combine sparse features (words) with dense (word vector) features to complement each other. So your feature matrix would be a concatenation of the sparse bag of words matrix with the average of word vectors. https://research.googleblog.com/2016/06/wide-deep-learning-better-together-with.html Another interesting method is to use a similar algorithm to word2vec but instead of predicting a target word, you can predict a target label. This directly tunes the word vectors to the classification task. http://arxiv.org/pdf/1607.01759v2.pdf For more ad hoc methods, you might try weighing the words differently depending on syntax. For example, you can weigh verbs more strongly than determiners.
