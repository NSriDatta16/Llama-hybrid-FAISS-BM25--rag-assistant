[site]: datascience
[post_id]: 104980
[parent_id]: 
[tags]: 
Fractional Differencing/Differentiation for Non-Time based Model; Look-ahead bias?

I have time-series data, but instead of using a time-based model like RNN, I've decided to approach my classification problem using an lgbm classifier. To do so, I have modified the data, such that the previous n rows are appended as new columns to the "present" row(turning the past n day's features into present features), using the function in this post . As I'm trying to do feature engineering, I'm questioning if a method like fractional differencing is still allowed. I believe that fractional differencing depends on the entirety of the data chosen, and normally, this wouldn't be a problem under the assumption that I use all available data(without splitting) to train for a time-based model. However, for my lgbm approach, can I fractional-difference the entire dataset, or would I have to split the data, fit on the train set, and then transform the validation set? I guess the question reduces down to a fundamental understanding of look-ahead bias when it comes to feature engineering using different models.
