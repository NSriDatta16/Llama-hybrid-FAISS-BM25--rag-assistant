[site]: datascience
[post_id]: 63535
[parent_id]: 63533
[tags]: 
I have tackled this exact issue using GlobalAveragePooling1D to flatten the output from those multivariate Embeddings and then concatenated them all along with the 1D embeddings later, which is how Youtube treats such things in their own recommendation engine : So in your place, I would go with your second approach. When I did it, the model structure looked something like this: Model: "model_8" __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== history-input (InputLayer) (None, 3063) 0 __________________________________________________________________________________________________ manufacturer-history-input (Inp (None, 3063) 0 __________________________________________________________________________________________________ history-embedding (Embedding) (None, 3063, 32) 538144 history-input[0][0] __________________________________________________________________________________________________ manufacturer-history-embedding (None, 3063, 32) 32224 manufacturer-history-input[0][0] __________________________________________________________________________________________________ average-history-embedding (Glob (None, 32) 0 history-embedding[0][0] __________________________________________________________________________________________________ manufacturer-average-history-em (None, 32) 0 manufacturer-history-embedding[0] __________________________________________________________________________________________________ numeric-inputs (InputLayer) (None, 49) 0 __________________________________________________________________________________________________ concatenate (Concatenate) (None, 113) 0 average-history-embedding[0][0] manufacturer-average-history-embe numeric-inputs[0][0] __________________________________________________________________________________________________ dense-512 (Dense) (None, 1024) 116736 concatenate[0][0] __________________________________________________________________________________________________ dropout_13 (Dropout) (None, 1024) 0 dense-512[0][0] __________________________________________________________________________________________________ dense-256 (Dense) (None, 512) 524800 dropout_13[0][0] __________________________________________________________________________________________________ dropout_15 (Dropout) (None, 512) 0 dense-256[0][0] __________________________________________________________________________________________________ target (Dense) (None, 16817) 8627121 dropout_15[0][0] ================================================================================================== Total params: 9,839,025 Trainable params: 9,839,025 Non-trainable params: 0
