[site]: crossvalidated
[post_id]: 172215
[parent_id]: 
[tags]: 
Find the joint distribution of $X_1$ and $\sum_{i=1}^n X_i$

This question is from Robert Hogg's Introduction to Mathematical Statistics 6th version question 7.6.7. The problem is : Let a random sample of size $n$ be taken from a distribution with the pdf $$f(x;\theta)=(1/\theta)\exp(-x/\theta)\mathbb{I}_{(0,\infty)}(x)$$ Find the MLE and the MVUE of $P(X \le 2)$ . I know how to find the MLE. I think the idea to find the MVUE is to use Rao-Blackwell and Lehmann and Scheffe. First we find an unbiased estimator of $P(X \le 2)$ which can be $\mathbb{I}_{(0,2)}(X_1)$ , and we know $Y=\sum_{i=1}^n X_i$ a sufficient statistic. Then $\mathbb{E}[I_{(0,2)}(X_1)\mid Y]$ will be the MUVE. To find the expectation, we need to the joint distribution of $X_1$ and $Y=\sum_{i=1}^n X_i$ I am stuck here. The book has a solution, but I don't understand the solution. The solution says let us find the joint distribution of $Z=X_1$ and $Y$ but first letting $V=X_1+X_2$ and $U=X_1+X_2+X_3+...$ the Jacobian is one then we integrating out those other variables. How comes the Jacobian is equal to one? The answer for the joint distribution is $$g(z,y;\theta)=\frac{(y-z)^{n-2}}{(n-2)!\theta^n}e^{-y/\theta}$$ How do we get this? Update: As suggested by Xi'an(the book suggested transformation is confusing), let us do the transformation by the following way: Let \begin{align} Y_1 & =X_1, \\Y_2 & =X_1+X_2,\\ Y_3 & =X_1+X_2+X_3, \\Y_4 & =X_1+X_2+X_3+X_4, \\ & \quad \vdots \\Y_n & =X_1+X_2+X_3+X_4+\cdots+X_n \end{align} then \begin{align} X_1 & =Y_1, \\ X_2 & =Y_2-Y_1,\\ X_3 & =Y_3-Y_2,\\X_4 & =Y_4-Y_3,\\ & \,\,\,\vdots \\ X_n & =Y_n-Y_{n-1} \end{align} and the corresponding Jacobian is: $$\left | J \right |=\begin{vmatrix} \frac{\partial x_1}{\partial y_1} &\frac{\partial x_1}{\partial y_2} &\frac{\partial x_1}{\partial y_3} &\cdots &\frac{\partial x_1}{\partial y_n} \\ \frac{\partial x_2}{\partial y_1} &\frac{\partial x_2}{\partial y_2} &\frac{\partial x_2}{\partial y_3} &\cdots &\frac{\partial x_2}{\partial y_n} \\ \frac{\partial x_3}{\partial y_1} &\frac{\partial x_3}{\partial y_2} &\frac{\partial x_3}{\partial y_3} &\cdots &\frac{\partial x_3}{\partial y_n} \\ \vdots&\vdots & \vdots & &\vdots \\ \frac{\partial x_n}{\partial y_1} &\frac{\partial x_n}{\partial y_2} &\frac{\partial x_n}{\partial y_3} &\cdots &\frac{\partial x_n}{\partial y_n} \end{vmatrix}=\begin{array}{r} 1& 0 &0 & \cdots &0 &0 \\ -1& 1 & 0 & \cdots & 0&0\\ 0&-1 & 1 & \cdots &0 &0\\ \vdots & \vdots & \vdots & & \vdots & \vdots \\ 0&0 &0 & \cdots & -1&1 \end{array}=1$$ Since $X_1,X_2,\ldots,X_n$ are i.i.d $\Gamma(1,\theta)$ [or $\mathcal{E}(1/\theta)$ ], the joint density of $x_1,x_2,\ldots,x_n $ is : $$f(x_1,x_2,\ldots,x_n)=\frac{1}{\theta}\exp(-x_1/\theta) \times\frac{1}{\theta} \exp(-x_2/\theta)\times \cdots\times\frac{1}{\theta}\exp(-x_n/\theta) \mathbb{I}_{x_1\ge 0} \cdots\mathbb{I}_{x_n\ge 0}$$ Therefore, the joint pdf of $(Y_1,Y_2,\ldots, Y_n)$ is \begin{align*}h(y_1,y_2,\ldots,y_n)&=\frac{1}{\theta^n}\exp(-y_1/\theta)\exp[-(y_2-y_1)/\theta]\exp[-(y_3-y_2)/\theta]\cdots\exp[-(y_n-y_{n-1})/\theta]\left |J \right |\mathbb{I}_{y_1\ge 0}\mathbb{I}_{y_2-y_1\ge 0}\cdots\mathbb{I}_{y_n-y_{n-1} \ge 0}\\&=\frac{1}{\theta^n}\exp(-y_n/\theta)\mathbb{I}_{y_1\ge 0} \mathbb{I}_{y_2\ge y_1}\cdots\mathbb{I}_{y_n\ge y_{n-1}}\end{align*} Next, we can integrate out $y_2,y_3,\ldots,y_{n-1}$ to get the joint pdf $y_1$ and $y_n$ Thanks to suggestions from Xi'an, now I can solve the problem, I will give detailed calculations below \begin{align} g(y_1,y_n) = {} &\int_{y_1}^{y_n}\int_{y_2}^{y_n} \cdots \int_{y_{n-3}}^{y_n} \int_{y_{n-2}}^{y_n} \frac{1}{\theta^n}\exp(-y_n/\theta)dy_{n-1}dy_{n-2} \cdots dy_3\,dy_2\\ = {} & \frac{1}{\theta^n}\exp(-y_n/\theta)\\ & \int_{y_1}^{y_n}\int_{y_2}^{y_n}\cdots\int_{y_{n-3}}^{y_n}\int_{y_{n-2}}^{y_n} \, dy_{n-1}\,dy_{n-2}\cdots dy_3\,dy_2 \\ = {} & \frac{1}{\theta^n}\exp(-y_n/\theta) \int_{y_1}^{y_n}\int_{y_2}^{y_n}\cdots\int_{y_{n-4}}^{y_n}\int_{y_{n-3}}^{y_n}(y_n-y_{n-2})\,dy_{n-2}\,dy_{n-3}\cdots dy_3\,dy_2 \\ = {} & \frac{1}{\theta^n}\exp(-y_n/\theta) \int_{y_1}^{y_n} \int_{y_2}^{y_n} \cdots \int_{y_{n-5}}^{y_n}\int_{y_{n-4}}^{y_n}\frac{(y_n-y_{n-3})^2}{2}dy_{n-3} \,dy_{n-4}\cdots dy_3 \, dy_2 \\ = {} & \frac{1}{\theta^n} \exp(-y_n/\theta) \int_{y_1}^{y_n }\int_{y_2}^{y_n} \cdots \int_{y_{n-6}}^{y_n} \int_{y_{n-5}}^{y_n} \frac{(y_n-y_{n-4})^3}{2 \times 3} \, dy_{n-4} \, dy_{n-5} \cdots dy_3\,dy_2\\ = {} & \frac{1}{\theta^n} \exp(-y_n/\theta) \int_{y_1}^{y_n} \int_{y_2}^{y_n} \cdots \int_{y_{n-7}}^{y_n} \int_{y_{n-6}}^{y_n} \frac{(y_n-y_{n-5})^4}{2 \times 3 \times 4} \, dy_{n-5} \, dy_{n-4} \cdots dy_3\,dy_2\\ = {} & \cdots \\ = {} & \frac{1}{\theta^n}\exp(-y_n/\theta)\frac{(y_n-y_1)^{n-2}}{(n-2)!} \end{align} Change to the book's notation, $y=y_n, z=y_1$ , we get $$g(z,y;\theta)=\frac{(y-z)^{n-2}}{\theta^n(n-2)!}e^{-y/\theta}.$$ This solves the problem.
