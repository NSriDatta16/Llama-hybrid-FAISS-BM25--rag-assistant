[site]: crossvalidated
[post_id]: 393326
[parent_id]: 393308
[tags]: 
You mentioned in the comments that your goal is to determine if GRE or SAT does better at predicting. Given that this is your goal, and that all other variables in your model appear to be identical, I'd suggest the following approach. Simply test your GRE model on a hold-out sample of data and compute the mean squared predicted error (MSPE). Do the same thing with the SAT model. Then, select the model that has the lowest mean squared predicted error as this model does the best job of predicting. You could even perform cross-validation or jackknife estimates with these methods and compare the averaged MSPEs. That being said, in the real world, rarely do school admissions offices receive only a GRE or only and SAT (I believe). As you've provided in your example, both factors likely contribute to the admission decision. As a result, I think you are better off creating a composite index of both GRE and SAT (perhaps by using a PCA score of GRE and SAT) and placing it in your model.
