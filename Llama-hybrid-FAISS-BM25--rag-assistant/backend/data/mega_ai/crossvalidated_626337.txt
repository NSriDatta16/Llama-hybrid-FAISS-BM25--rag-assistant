[site]: crossvalidated
[post_id]: 626337
[parent_id]: 463870
[tags]: 
Long and old question. Still quite relevant to others, I think. Question 1: Possibility 1 or 2 "Possibility 1" is clearly the more concise one, especially since you want to compete XGBoost against other algorithms. Selecting the optimal number of boosting rounds actively makes use of "eval_set". BTW: the same applies for the other big round-based algorithm: neural nets. How do I retrain with train and val? You would fit the model to the combined data without early stopping, but using the optimal number of rounds from the best early-stopping model. There is no "parameter" for it. Cross-validation Ideally, you combine early-stopping and CV. XGBoost has a "cv" method that boosts parallel over all folds, and evaluates the average performance over folds after each boosting round. The less concise alternative is that you boost each fold separately. Then you get an optimal number of boosting rounds per fold. You can choose how to combine them (take the average, maximum, minimum etc.). There is no official rule.
