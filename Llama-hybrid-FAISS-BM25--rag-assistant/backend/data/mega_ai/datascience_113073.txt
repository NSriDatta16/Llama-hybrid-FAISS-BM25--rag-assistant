[site]: datascience
[post_id]: 113073
[parent_id]: 
[tags]: 
Gradient descent vs stochastic gradient descent vs mini-batch gradient descent with respect to working step/example

I am trying to understand the working of gradient descent, stochastic gradient descent and mini-batch gradient descent. In case of gradient descent, gradient is computed on the entire dataset at each step. So I imagine this is like multiple tasks, where-in each task looks at 1 item from the dataset. Eventually the result of the best task is selected. Where-as stochastic gradient descent gets a new random sample at each step. So unlike the multiple tasks example above, there is only 1 task, and at each step of the task a random sample is selected. I assume the new random sample that is selected must be at a lower location then the existing sample? In case of mini-batch gradient descent, I imagine this like a single main task, where-in each step results in multiple tasks, where-in each task will perform the gradient loss calculation and an average will be taken. Then the same process will repeat in the next step.
