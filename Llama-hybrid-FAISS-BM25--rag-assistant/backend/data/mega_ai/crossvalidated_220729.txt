[site]: crossvalidated
[post_id]: 220729
[parent_id]: 
[tags]: 
How are hidden layer weights computed in a multilayer neural network?

More specifically, given a typical neural network with a single hidden layer $Z_m$ where $m = 1,...,M$ (see specifications/notation below drawn from p. 392 of Elements of Statistical Learning (Hastie, Tibshirani, Friedman, 2008)), wouldn't the weights for each of the derived features $Z_m$ converge to the same values since each feature $Z_m$ in the hidden layer is derived from the same collection of input observations $X_p$ where $p = 1,...,P$? To my untrained eyes it looks like the hidden layer contain redundant features. What am I missing? For K-class classiﬁcation, there are K units at the top, with the kth unit modeling the probability of class k. There are K target measurements $Y_k$, $k = 1,...,K$, each being coded as a 0−1 variable for the kth class. Derived features $Z_m$ are created from linear combinations of the inputs, and then the target $Y_k$ is modeled as a function of linear combinations of the $Z_m$, $Z_m = σ(α_{0m} + α^T_mX),$ where $m = 1,...,M,$ $T_k = β_{0k} + β^T_k Z,$ where $k = 1,...,K,$ $f_k(X) = g_k(T),$ where $k = 1,...,K,$ where $Z = (Z_1,Z_2,...,Z_M),$ and $T = (T_1,T_2,...,T_K).$
