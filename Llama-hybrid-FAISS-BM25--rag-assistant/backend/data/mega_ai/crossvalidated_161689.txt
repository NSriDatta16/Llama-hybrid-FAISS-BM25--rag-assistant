[site]: crossvalidated
[post_id]: 161689
[parent_id]: 86991
[tags]: 
The Elements of Statistical Learning by Hastie et al. define ridge regression as follows (Section 3.4.1, equation 3.41): $$\hat \beta{}^\mathrm{ridge} = \underset{\beta}{\mathrm{argmin}}\left\{\sum_{i=1}^N(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j)^2 + \lambda \sum_{j=1}^p \beta_j^2\right\},$$ i.e. explicitly exclude the intercept term $\beta_0$ from the ridge penalty. Then they write: [...] notice that the intercept $\beta_0$ has been left out of the penalty term. Penalization of the intercept would make the procedure depend on the origin chosen for $Y$ ; that is, adding a constant $c$ to each of the targets $y_i$ would not simply result in a shift of the predictions by the same amount $c$ . Indeed, in the presence of the intercept term, adding $c$ to all $y_i$ will simply lead to $\beta_0$ increasing by $c$ as well and correspondingly all predicted values $\hat y_i$ will also increase by $c$ . This is not true if the intercept is penalized: $\beta_0$ will have to increase by less than $c$ . In fact, there are several nice and convenient properties of linear regression that depend on there being a proper (unpenalized) intercept term. E.g. the average value of $y_i$ and the average value of $\hat y_i$ are equal, and (consequently) the squared multiple correlation coefficient $R$ is equal to the coefficient of determination $R^2$ : $$(R)^2 = \text{cor}^2(\hat {\mathbf y}, \mathbf y) = \frac{\|\hat{\mathbf y}\|^2}{\|\mathbf y\|^2} = R^2,$$ see e.g. this thread for an explanation: Geometric interpretation of multiple correlation coefficient $R$ and coefficient of determination $R^2$ . Penalizing the intercept would lead to all of that not being true anymore.
