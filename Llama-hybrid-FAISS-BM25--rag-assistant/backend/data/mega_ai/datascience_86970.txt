[site]: datascience
[post_id]: 86970
[parent_id]: 86951
[tags]: 
I understand that the fact that MLP allows for arbitrary decision boundaries derives from the fact that an MLP is a universal approximator. The theoretical area addressing the capability of neural networks to be able to approximate any function are the different flavors of the universal approximation theorem , for which there are two directions: the arbitrary width case (which you refer to when you assume that the ability comes from the three layers), and the arbitrary depth case. The wikipedia page contains the mathematical formulation of both directions, as well as reference to their respective proof for different assumptions.
