[site]: datascience
[post_id]: 126401
[parent_id]: 
[tags]: 
Learning the gradient descent stepsize with RL

Problem statement: I've been working on a project to accelerate the convergence of gradient descent using reinforcement learning (RL). I want to learn a policy that can map the current state of gradient descent to an optimal action, which is the stepsize in this case. As a reminder: the gradient descent iterations are given by x_k+1 = x_k - gamma*grad(f), with gamma the stepsize. Currently, I'm only considering convex quadratic functions of the form f(x) = x'Qx. I want to train the policy on a distribution of functions, such that at prediction time it can generalize to all the functions in this distribution, also those it hasn't seen during training. With the policy predicting an optimal stepsize in every iteration, the goal is that gradient descent converges in less iterations for all functions within this distribution. Current approach: Currently, I have been using model-free RL algorithms like Soft Actor-Critic (SAC) and Twin Delayed Deep Deterministic Policy Gradient (TD3) to train a policy, but I found the amount of memory and compute needed extremely high, even for the simple case when you are just overfitting on one particular function. Also when you are overfitting (training and evaluating on the same function), you would expect the reward to converge to some value. As shown in the figure, the reward does increase, but at some point the agent just completely forgets what it has learned. I used the sparse reward: 0 when converged and -1 when not converged in every iteration. Probably it would be better to have a reward that says something about the decrease in residual (=norm of the gradient) in every iteration so the agent not only receives information at the end of an episode. For the state, I tried different things, but just including the gradient in the current iterate x_k seems to work more or less. Probably, it would be better to include something like the Lipchitz constant to add more information about the function. The algorithm I used was SAC, which seemed to be faster then TD3. The actor and critic were both parametrized by neural networks with 3 hidden layers and 128 nodes each. I used the implementation of Stable-Baselines 3. My questions: Is model-free RL the right approach for this problem? It is computationally very expensive. Would there be a better approach like model-based RL or some kind of policy search? In the figure, why does the reward suddenly decrease? Does it have something to do with the size of the replay buffer? Currently I can allocate 120Gb of memory, which is already quite a lot. RL theory is often based on a Markov Proces. So it assumes the Markov property that the current state is completely independent of the previous states. However, it would be better to add some information about previous gradients to add momentum (something like Nesterov acceleration). Is this possible in this framework? Would an architecture like RNN work better for this kind of problem? Does this transform the Markov Decision Process into an Partially Observable Markov Decision Process?
