[site]: datascience
[post_id]: 63573
[parent_id]: 
[tags]: 
What are advantages of oversampling over changing threshold for unbalanced classes?

Let's say that I have unbalanced data set that has two classes, and I am using Random Forest to make my predictions. Random forest will be biased towards the majority class, which will cause low recall and high precision on minority class predictions that I am interested in, and it might not be desirable. One approach is to plot recall, precision and f1 score on the y-axis and threshold on the axis, and use this plot to select the proper threshold. Another approach is to use oversampling, let's say SMOTE, for the minority class. My questions is: What advantages would oversampling approach offer? So far I only see disadvantages: 1) We have to be very careful, that our minority class is in one cluster, otherwise SMOTE will put points between the clusters, which is not desired at all. To avoid that, we should create different labels for different clusters, which is much more complicated. 2) We are creating extra data, that increases run time and memory usage. 3) Even after we have done oversampling, we still might not have ideal recall, threshold balance, and it feels redundant to still have to adjust the threshold. I know that oversampling is a commonly used technique, what am I missing?
