[site]: crossvalidated
[post_id]: 562228
[parent_id]: 561748
[tags]: 
Neural networks are not per se expensive to train, but as you can arbitrarily / without end stack layers and layers, you can end up with quite a large network. A famous NLP network, GPT-3, is quite large: GPT-3 comes in eight sizes, ranging from 125M to 175B parameters Further, for achieving good results with NN you usually use huge amounts of training data, such as in this case Notice GPT-2 1.5B is trained with 40GB of Internet text, which is roughly 10 Billion tokens (conversely assuming the average token size is 4 characters) That even is a problem with storage, because The 175 Billion parameters needs 175 Ã— 4 = 700 G B memory to store in FP32 (each parameter needs 4 Bytes). The point I want to make is that successful NN are often deep and trained with lots of data, that does not mean that the class of algorithms per se is expensive to train. All quotes are from https://lambdalabs.com/blog/demystifying-gpt-3/
