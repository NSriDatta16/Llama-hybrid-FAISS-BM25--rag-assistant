[site]: crossvalidated
[post_id]: 163700
[parent_id]: 163579
[tags]: 
The KL divergences can be seen as a product of a weighting function $w(x)$ and a penalty function $g(x)$, i.e. $KL(q||p) = \sum_x w(x)g(x)$ with $w(x) = q(x)$ and $g(x) = \log\frac{q(x)}{p(x)}$ in the case of the reverse KL divergence. Whenever the weighting function is close to zero, i.e. $w(x) \approx 0$, the product of $w(x)g(x)$ is also close to zero and the value of the penalty function $g(x)$ does not contribute to the KL divergence no matter how large it is. Consider first the case of reverse KL divergence. When $q(x)$ is close to zero, the penalty term $g(x) = \log\frac{q(x)}{p(x)}$ is ignored and therefore the reverse KL divergence ''ignores'' the portion of $p(x)$, which is not covered by $q(x)$. On the other hand, when $q(x)$ has significant mass and $p(x)$ is close to zero the penalty term $g(x)$ will be large. Therefore the reverse KL divergence discourages situations where $q(x)$ is high and $p(x)$ is small leading to the ''zero-forcing''-effect. We can now make a similar analysis of the ''forward'' KL divergence. Now the weighting function corresponds to the target distribution $p$, i.e. $w(x) = p(x)$. Thus, when $p(x) \approx 0$ the value of the penalty $g(x) = \log\frac{p(x)}{q(x)}$ is largely ignored. Thus, there is almost no cost of having large $q(x)$ when $p(x)$ is small. On the other hand we see that when $p(x)$ has significant mass and $q(x)$ is small the contribution to the KL divergence is large. Combining these two properties leads to the ''zero-avoiding'' property.
