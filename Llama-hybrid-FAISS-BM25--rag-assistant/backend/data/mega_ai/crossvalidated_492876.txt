[site]: crossvalidated
[post_id]: 492876
[parent_id]: 
[tags]: 
Why does normalising a conditional probability sufficient to ensure ensure that the resulting quantity is a distribution?

The following is said in David Barber's Bayesian Reasoning and Machine Learning : "The relation between the conditional p(A=a | B=b) and the joint p(A=a, B=b) is just the normalisation constant since $p(A=a, B=b)$ is not a distribution in A - in other words, $\sum_a P(A=a, B=b)\not = 1$ . To make it a distribution (in A), we need to divide: $p(A=a, B=b)/ \sum_a p(A=a,B=b) $ which when summed over a does sum to 1. Indeed, this is just the definition of $p(A=a|B=b)$ " What is the point of this excerpt? I have two questions, please. I understand that P(A=a, B=b) is of course not a probability distribution for A, but isn't this obvious? If the point is that P(A=a, B=b) is not by itself a probability distribution for A, and that dividing by $\sum_a P(A=a, B=b)$ makes is a distribution as now $$\sum_a\frac{(p(A=a, B=b)}{ \sum_a p(A=a,B=b)}=1$$ . My question is why is this this normalising condition sufficient to ensure that $\frac{(p(A=a, B=b)}{ \sum_a p(A=a,B=b)}$ is a distribution in A?
