[site]: crossvalidated
[post_id]: 344724
[parent_id]: 343420
[tags]: 
Overfitting means the model works well on the training set but performs poorly on test set. IMHO, it comes from two sources: the data and the model we use (or our subjectivity). Data is probably the more important factor. With whatever models/approaches we use, we implicitly assume the our data is representative enough, that is what we obtain from our (training) data can be also generalized to the population. In practice it is always not the case. If the data is not iid then standard $k$-fold CV makes no sense in avoiding overfitting. As a result, if we are frequentist then the source of overfitting comes from MLE. If we are Bayesian then this comes from the (subjective) choice of prior distribution(and of course the choice of likelihood)). So even if you use posterior distribution/mean/median, you already overfitted from the beginning and this overfitting is carried along. The proper choice of prior distribution and likelihood will help but they are still the models, you can never avoid overfitting completely.
