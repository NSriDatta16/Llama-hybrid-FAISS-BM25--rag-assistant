s}}\end{cases}}} Kalman filter The very well-known Kalman filters are a special case of Bayesian filters. They are defined by the following Bayesian program: P r { D s { S p ( π ) { V a : S 0 , ⋯ , S T , O 0 , ⋯ , O T D c : { P ( S 0 ∧ ⋯ ∧ O T | π ) = [ P ( S 0 ∧ O 0 | π ) ∏ t = 1 T [ P ( S t | S t − 1 ∧ π ) × P ( O t | S t ∧ π ) ] ] F o : { P ( S t ∣ S t − 1 ∧ π ) ≡ G ( S t , A ∙ S t − 1 , Q ) P ( O t ∣ S t ∧ π ) ≡ G ( O t , H ∙ S t , R ) I d Q u : P ( S T ∣ O 0 ∧ ⋯ ∧ O T ∧ π ) {\displaystyle Pr{\begin{cases}Ds{\begin{cases}Sp(\pi ){\begin{cases}Va:\\S^{0},\cdots ,S^{T},O^{0},\cdots ,O^{T}\\Dc:\\{\begin{cases}&P\left(S^{0}\wedge \cdots \wedge O^{T}|\pi \right)\\=&\left[{\begin{array}{c}P\left(S^{0}\wedge O^{0}|\pi \right)\\\prod _{t=1}^{T}\left[P\left(S^{t}|S^{t-1}\wedge \pi \right)\times P\left(O^{t}|S^{t}\wedge \pi \right)\right]\end{array}}\right]\end{cases}}\\Fo:\\{\begin{cases}P\left(S^{t}\mid S^{t-1}\wedge \pi \right)\equiv G\left(S^{t},A\bullet S^{t-1},Q\right)\\P\left(O^{t}\mid S^{t}\wedge \pi \right)\equiv G\left(O^{t},H\bullet S^{t},R\right)\end{cases}}\end{cases}}\\Id\end{cases}}\\Qu:\\P\left(S^{T}\mid O^{0}\wedge \cdots \wedge O^{T}\wedge \pi \right)\end{cases}}} Variables are continuous. The transition model P ( S t ∣ S t − 1 ∧ π ) {\displaystyle P(S^{t}\mid S^{t-1}\wedge \pi )} and the observation model P ( O t ∣ S t ∧ π ) {\displaystyle P(O^{t}\mid S^{t}\wedge \pi )} are both specified using Gaussian laws with means that are linear functions of the conditioning variables. With these hypotheses and by using the recursive formula, it is possible to solve the inference problem analytically to answer the usual P ( S T ∣ O 0 ∧ ⋯ ∧ O T ∧ π ) {\displaystyle P(S^{T}\mid O^{0}\wedge \cdots \wedge O^{T}\wedge \pi )} question. This leads to an extremely efficient algorithm, which explains the popularity of Kalman filters and the number of their everyday applications. When there are no obvious linear transition and observation models, it is still often possible, using a first-order Taylor's expansion, to treat these models as locally linear. This generalization is commonly called the extended Kalman filter. Hidden Markov model Hidden Markov models (HMMs) are another very popular specialization of Bayesian filters. They are defined by the following Bayesian program: Pr { D s { S p ( π ) { V a : S 0 , … , S T , O 0 , … , O T D c : { P ( S 0 ∧ ⋯ ∧ O T ∣ π ) = [ P ( S 0 ∧ O 0 ∣ π ) ∏ t = 1 T [ P ( S t ∣ S t − 1 ∧ π ) × P ( O t ∣ S t ∧ π ) ] ] F o : { P ( S 0 ∧ O 0 ∣ π ) ≡ Matrix P ( S t ∣ S t − 1 ∧ π ) ≡ Matrix P ( O t ∣ S t ∧ π ) ≡ Matrix I d Q u : max S 1 ∧ ⋯ ∧ S T − 1 [ P ( S 1 ∧ ⋯ ∧ S T − 1 ∣ S T ∧ O 0 ∧ ⋯ ∧ O T ∧ π ) ] {\displaystyle \Pr {\begin{cases}Ds{\begin{cases}Sp(\pi ){\begin{cases}Va:\\S^{0},\ldots ,S^{T},O^{0},\ldots ,O^{T}\\Dc:\\{\begin{cases}&P\left(S^{0}\wedge \cdots \wedge O^{T}\mid \pi \right)\\=&\left[{\begin{array}{c}P\left(S^{0}\wedge O^{0}\mid \pi \right)\\\prod _{t=1}^{T}\left[P\left(S^{t}\mid S^{t-1}\wedge \pi \right)\times P\left(O^{t}\mid S^{t}\wedge \pi \right)\right]\end{array}}\right]\end{cases}}\\Fo:\\{\begin{cases}P\left(S^{0}\wedge O^{0}\mid \pi \right)\equiv {\text{Matrix}}\\P\left(S^{t}\mid S^{t-1}\wedge \pi \right)\equiv {\text{Matrix}}\\P\left(O^{t}\mid S^{t}\wedge \pi \right)\equiv {\text{Matrix}}\end{cases}}\end{cases}}\\Id\end{cases}}\\Qu:\\\max _{S^{1}\wedge \cdots \wedge S^{T-1}}\left[P\left(S^{1}\wedge \cdots \wedge S^{T-1}\mid S^{T}\wedge O^{0}\wedge \cdots \wedge O^{T}\wedge \pi \right)\right]\end{cases}}} Variables are treated as being discrete. The transition model P ( S t ∣ S t − 1 ∧ π ) {\displaystyle P\left(S^{t}\mid S^{t-1}\wedge \pi \right)} and the observation model P ( O t ∣ S t ∧ π ) {\displaystyle P\left(O^{t}\mid S^{t}\wedge \pi \right)} are both specified using probability matrices. The question most frequently asked of HMMs is: max S 1 ∧ ⋯ ∧ S T − 1 [ P ( S 1 ∧ ⋯ ∧ S T − 1 ∣ S T ∧ O 0 ∧ ⋯ ∧ O T ∧ π ) ] {\displaystyle \max _{S^{1}\wedge \cdots \wedge S^{T-1}}\left[P\lef