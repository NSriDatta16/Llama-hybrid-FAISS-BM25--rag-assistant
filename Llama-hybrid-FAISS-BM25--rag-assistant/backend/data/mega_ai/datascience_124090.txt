[site]: datascience
[post_id]: 124090
[parent_id]: 123924
[tags]: 
For datasets: To the best of my knowledge, the IMDB dataset is serving as the "Hello World" of NLP, especially for LLM-related matters, and can be used for either binary classification or generative text. Another well-known dataset is the BookCorpus which was used to train GPT, BERT, and other LLMs. If you're looking to scale up, the Amazon review dataset is also well-known, and there are many existing subsets which are more tractable than the full dataset. Regarding modeling: If you're starting from scratch, you probably want to take a look at Karpathy's minGPT and there are now many "Transformers from Scratch" type posts / videos / tutorials out there You want to work with a foundational model to get started (BERT, T5, or GPT) and since you specifically mentioned decoders, would most likely be GPT. If you really want to go back to fundamentals, you can take a look at the original Attention is All You Need Paper and code Hope this is helpful!
