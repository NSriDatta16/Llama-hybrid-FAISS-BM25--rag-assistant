[site]: crossvalidated
[post_id]: 89522
[parent_id]: 89513
[tags]: 
It sounds like your concern is that of model misspecification. You're interested in determining if the intervention lead to an incremental improvement in the risk of outcome comparing treated to control over time. It sounds like, in particular, you are worried that the log-linear term for the relationship between odds of outcome comparing groups differing by 1 unit in time may not be adequate. There are two solutions to this, but firstly note: "all models are wrong, some models are useful" - George Box. We ask: what's the risk of getting the time effect wrong? (say it's quadratic) Well, if both groups are measured consistently across time, there's actually no difference. This is the value of balanced design. Adjusting for time improves precision when there is imbalance and the specified model is correct. If you are willing to assume that the specified time effect is "close to correct" (maybe there is a weakly non-linear trend), then using robust standard errors ensures that the inference is correct on the actual intervention effect. The interpretation of the parameter is a "time averaged" effect as a consequence of that. Another solution is to use a more granular effect of time. Rather than assuming a linear increase, you can test nested models with categorical effects of time. Assume, for instance, there are four time points: two pre-intervention, and two post-intervention. The categorical model would then be: $$\mbox{logit} (Y | X, T) = \alpha + \beta_1 X + \gamma_1 T_2 + \gamma_2 T_3 + \gamma_3 T_4$$ against the full model $$\mbox{logit} (Y | X, T) = \alpha + \beta_1 X + \gamma_1 T_2 + \gamma_2 T_3 + \gamma_3 T_4 + \eta X T_4 $$ And the simultaneous test of all post-by-treatment parameters (one $\eta$ for each extra unit time after treatment... accounting for Hawthorne effect with $\beta_1$) will account for a categorical effect-by-time interaction.
