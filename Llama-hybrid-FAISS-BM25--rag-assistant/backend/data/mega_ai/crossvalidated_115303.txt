[site]: crossvalidated
[post_id]: 115303
[parent_id]: 
[tags]: 
MLE vs MAP vs conditional MLE with regards to logistic regression

We have some set of iid RV's: $(X_i, Y_i), \; i=1,\ldots n$. We believe each to be distributed as $P(X_i, Y_i | \theta)$. So that $$ P(X,Y | \theta) = \prod_i P_i(X_i, Y_i | \theta) $$ Now using Baye's rule: $$ P(\theta|X,Y) = \frac{P(X,Y|\theta)P(\theta)}{P(X,Y)} = \frac{P(\theta)\prod_i P_i(X_i, Y_i | \theta)}{P(X,Y)} $$ As I understand it, MLE, MAP, and conditional MLE all attempt to find the best parameters, $\theta$, given the data by maximizing the left hand side by maximizing a subset of terms on the right. For MLE, we maximize the likelihood term, $\prod_i P_i(X_i, Y_i | \theta)$. For MAP, we maximize all of the numerator, $P(\theta)\prod_i P_i(X_i, Y_i | \theta)$. For conditional MLE (as in logistic regression), we have $$ \frac{P(\theta)\prod_i P_i(X_i, Y_i | \theta)}{P(X,Y)} = \frac{P(\theta) \left( \prod_i P_i(Y_i | X_i, \theta) \right) \left( \prod_i P(X_i|\theta) \right) }{P(X,Y)} $$ Conditional MLE maximizes only the $\prod_i P_i(Y_i | X_i, \theta)$ term. Is this correct? I have seen regularized logistic regression amounting to maximizing the prior, $P(\theta)$. Would modeling the third distribution for conditional MLE, $\prod_i P(X_i|\theta)$, be something different entirely? I understand that logistic regression is discriminative model. Is this a result of this? Would modeling $P(X_i|\theta)$ then give us a generative model? Thanks for any pointers.
