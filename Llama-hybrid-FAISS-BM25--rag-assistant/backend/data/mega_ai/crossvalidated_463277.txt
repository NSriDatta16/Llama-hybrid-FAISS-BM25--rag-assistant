[site]: crossvalidated
[post_id]: 463277
[parent_id]: 
[tags]: 
In this case, no problem for initializing weights in deep learning networks to 0

Deep learning textbooks say that initializing all weights of neural networks to 0 will be problematic as it breaks symmetry . I tried with a simple 1-layer neural network but found such is not the case, as shown below. Can anyone help clarify? Consider a DL network composed of an input layer and an output layer, Suppose that a simplified loss function looks like $$L= (w_1 x_1 + w_2 x_2 - y)^2 + (w_1' x_1 + w_2' x_2 - y')^2$$ where $y$ and $y'$ are expected results for the two units in the output layer. We ignored activation functions. We got $\partial L/\partial w_1 = 2 y x_1$ , and $\partial L/\partial w_1' = 2 y' x_1$ So, there does not seem to have "breaking symmetry" issue in this case, unless $y=y'$ . Where am I wrong, or is this case not general enough?
