[site]: crossvalidated
[post_id]: 337919
[parent_id]: 337889
[tags]: 
Yes, given only a kernel matrix, you can visualize the data using kernel PCA (KPCA), which is a kernelized version of classical PCA. KPCA is equivalent to performing classical PCA after mapping the data into the feature space defined implicitly by the kernel function. Like SVMs, it uses the kernel trick to avoid computing this mapping explicitly. Given a kernel matrix, KPCA first performs a centering operation, which ensures the data are centered in feature space. It then finds the projection of the data along each principal component in feature space, using an eigendecomposition of the centered kernel matrix. The data can be visualized by plotting the projections for the top two or three components, which capture the most variance in feature space. You can use the KPCA implementation included in scikit-learn, which allows custom kernels. References: Sch√∂lkopf et al. (1998). Nonlinear component analysis as a kernel eigenvalue problem.
