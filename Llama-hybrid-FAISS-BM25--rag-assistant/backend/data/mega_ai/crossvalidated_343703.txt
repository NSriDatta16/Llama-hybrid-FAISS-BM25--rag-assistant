[site]: crossvalidated
[post_id]: 343703
[parent_id]: 
[tags]: 
UCB: where is delta and standard deviation?

One of the most popular multi-armed bandit is the Upper Confidence Bound (UCB) line of algorithms (see references below). My understanding is that we try to find the largest plausible estimate of the mean of arm $i$: $$\hat\mu_i(t-1) + \sqrt{\frac{2\sigma}{n_i(t-1)}\log\frac{1}{\delta}},$$ where $\hat\mu_i(t)$ is the average reward of arm $i$ until time $t$ and $n_i(t)$ is the number of times arm $i$ has been chosen. The second term of the formula is the confidence tail. We assume that rewards are $\sigma^2$-subgaussian which (by definition) means we have the upper-bound $\text P(\hat\mu\geq\varepsilon)\leq\exp(-\frac{n\varepsilon^2}{2\sigma})$. By setting $\delta=\exp(-\frac{n\varepsilon^2}{2\sigma})$, we have that $\text P(\hat\mu\geq\sqrt{\frac{2\sigma}{n}\log\frac{1}{\delta}}) \leq \delta$. In graphical terms, I believe (at each iteration) we want to find the arm that maximizes: My question is two-fold: Isn't this sensitive to the magnitude of the mean and deviation? What if I don't know the deviation $\sigma$ of my rewards? Most authors seem to assume $\sigma=1$, but this just seems wrong. In my particular case, UCB gives bad results because of this (I think). Should I just try different values instead of $\sigma$? Should I try somehow to estimate it? Likewise, I am not completely sure if my $\mu=0$ -- is this a problem? Why do so many people replace $\frac{1}{\sigma}$ by something like $1/\delta=f(t)=1+t\log^2(t)$? I could not find this from the original paper (which I found hard to read) or any of the references. I guess my question is: why do I see so many implementations like this? formula = mu + np.sqrt(2*np.log(it)/n) Why do $\sigma$ and $\delta$ disappear? References: A Survey of Online Experiment Design with the Stochastic Multi-Armed Bandit (2015) Bandit Algorithms blog
