[site]: crossvalidated
[post_id]: 555213
[parent_id]: 555182
[tags]: 
I'm running a performance analysis of the two programs and hence need to compare how much time on average it took for queries to run on different versions of the program. Evaluating "how much time on average" the versions differ doesn't require a "percentage difference" calculation. A simple evaluation of the paired differences could be all that you need. If you expect that one version will tend to be a certain percentage faster than the other, instead of a certain number of seconds faster on all tasks, then you can work with times in a logarithmic scale. For your setup, examine the paired $\log x_i - \log y_i= \log (x_i/y_i)$ values. The average over all $i$ is the "average log ratio" Frank Harrell recommended in a comment. That maintains symmetry between the $x_i$ values in List A and the $y_i$ values in List B. Section 14.5 of his BBR Notes nicely summarizes the problems with using "percent change" instead. A visual evaluation with a Tukey mean-difference plot , again with times on a log scale, could help evaluate if there are systematic differences between the versions as a function of the average computational time of the queries.
