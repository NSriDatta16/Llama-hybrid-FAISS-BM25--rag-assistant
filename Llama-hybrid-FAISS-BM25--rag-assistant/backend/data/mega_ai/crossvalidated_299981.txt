[site]: crossvalidated
[post_id]: 299981
[parent_id]: 184657
[tags]: 
The difference between Off-policy and On-policy methods is that with the first you do not need to follow any specific policy, your agent could even behave randomly and despite this, off-policy methods can still find the optimal policy. On the other hand on-policy methods are dependent on the policy used. In the case of Q-Learning, which is off-policy, it will find the optimal policy independent of the policy used during exploration, however this is true only when you visit the different states enough times. You can find in the original paper by Watkins the actual proof that shows this very nice property of Q-Learning. There is however a trade-off and that is off-policy methods tend to be slower than on-policy methods. Here a link with other interesting summary of the properties of both types of methods
