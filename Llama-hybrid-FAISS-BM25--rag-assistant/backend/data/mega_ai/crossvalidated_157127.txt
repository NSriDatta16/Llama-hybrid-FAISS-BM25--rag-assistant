[site]: crossvalidated
[post_id]: 157127
[parent_id]: 
[tags]: 
The derivation of $\delta_j = \frac{\partial E_n}{ \partial a_j}$ errors for hidden units in back propagation for neural networks with the chain rule

I was trying to understand the derivation for back propagation for multi-layer neural networks from Bishop's Pattern Recognition and Machine Learning book. Specifically I was reading section 5.3.1 from page 242 to 244. The equation that is specifically confusing me is equation 5.55 $$ \delta_j \equiv \frac{\partial E_n}{ \partial a_j} = \sum_k \frac{\partial E_n}{ \partial a_k} \frac{\partial a_k}{ \partial a_j} $$ where Bishop goes on to say: where the sum runs over all units $k$ to which $j$ sends connections. furthermore, what confuses me is the use of the chain rule and how the connections the affect the partial derivative. To make this discussion easier recall the definition of $E_n$ (the error of a the neural network for the nth training point, equation 5.46): $$E_n = \frac{1}{2} \sum_k (y_{nk} - t_{nk} )^2 = \frac{1}{2} \|y_n - t_n \|^2$$ where $y_n $ is the vector of outputs of our neural and $t_n$ is the true target output we are trying to learn. My confusion is specifically how he applied the multivariable chain rule. Usually the way I think the multivariable chain rule is as follows; given a function $f(x_1(t), ..., x_N(t) )$, if we want its derivative wrt to t then we get: $$\frac{df}{dt} = \sum^N_{k=1} \frac{\partial f}{\partial x_k} \frac{d x_k}{d t}$$ However, I am having some difficulties understanding how that equations was applied in the context of deriving $\frac{\partial E_n}{ \partial a_j}$ for the hidden units of the neural network. In particular, I thought $E_n$ is a function of all the $a_j$'s, at every layer, so why wouldn't all the $a_j$'s from each layer be part of the summation but only the ones from one layer before? Also, the part $ \frac{\partial a_k}{ \partial a_j} $ from equation 5.55 is never zero, right? because the $a_j$ vs $a_k$ are from different nodes in the network, right? Wouldn't it be easier/better to indicate this dependence with superscripts indicating the layers? Is somebody knows how to explain how that equation come about, I would be super grateful! For reference I will paste the relevant section of the chapter as reference:
