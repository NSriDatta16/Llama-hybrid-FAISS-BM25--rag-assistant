[site]: crossvalidated
[post_id]: 598319
[parent_id]: 
[tags]: 
Understanding likelihood vs conditional joint pdf

My course notes (3rd-year module in Bayesian Statistics, unpublished) have a paragraph, Gaussian data with known variance Suppose we have $\textbf{x}=\{x_1,\dots,x_n\}$ iid given $\theta$ and $\sigma^2$ as N $(x_i\,|\,\theta,\sigma^2)$ , and assume $\theta$ unknown but $\sigma^2$ known. Given these data you are interested in the unknown expected value $\theta$ . We can write down the likelihood function as $$f(\textbf{x}\,|\,\theta)\propto\prod_{i=1}^n\sqrt{\frac{1}{2\pi\sigma^2}}\exp\left(-\frac{1}{2\sigma^2}(x_i-\theta)^2\right).$$ Would I be right in thinking that calling $f$ "the likelihood function" is actually an abuse of notation/terminology (albeit a conventional one), and that it would be more accurate to say the following? We can write down the likelihood function as $$\mathcal{L}(\theta\,|\,\textbf{x})\propto f(\textbf{x}\,|\,\theta)=\prod_{i=1}^n\sqrt{\frac{1}{2\pi\sigma^2}}\exp\left(-\frac{1}{2\sigma^2}(x_i-\theta)^2\right),$$ where $f(\textbf{x}\,|\,\theta)$ is the joint pdf of the data $\textbf{x}$ conditional on $\theta$ . ETA: I asked a related question a while ago, and accepted an answer . I've spent most of today staring at the image included in that answer.
