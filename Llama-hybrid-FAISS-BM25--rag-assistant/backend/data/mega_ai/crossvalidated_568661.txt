[site]: crossvalidated
[post_id]: 568661
[parent_id]: 568653
[tags]: 
You can use a variant of Sigmoid function to fit your distribution, as: Although, you need to adjust the output of the fitting function according to PDF range, as: I have tried a simple python code to verify it, you can try it and tune some of its variables to achieve desired output. import numpy as np import matplotlib.pyplot as plt data = np.zeros(60) data[0:40] = 6 data[40:60] = 2 x = np.arange(0,60) c2 = 39.5 # function decreasing point c1 = 8 # strictness constant datapdf = data/np.sum(data) # Normalizing distribution to get PDF ffunc = abs(1.0 - 1/(1+np.exp(-c1*(x-c2)))) # variant of sigmoid function PDFfunc = (ffunc - min(ffunc))/(max(ffunc)-min(ffunc))*(max(datapdf)-min(datapdf))+min(datapdf) # To keep same probability range as PDF # Visualizing PDF and fitting function plt.plot(datapdf) plt.plot(PDFfunc, '--') plt.xlabel('X') plt.ylabel('Probability') plt.legend(('PDF','Fitting function')) plt.xticks(np.arange(0, 1+len(datapdf), 10)) plt.show() However, I would advise you to go further than fitting a function, as for the most practical probabilistic models, the exact inference is intractable*. You can try Kernel Density Estimation (KDE) and/or Monte Carlo Sampling. *Christopher M. Bishop, and Nasser M. Nasrabadi. Pattern recognition and machine learning, Springer Science+Business Media,New York, USA, 2006; pp. 523â€“556.
