[site]: crossvalidated
[post_id]: 552951
[parent_id]: 135014
[tags]: 
The reference prior improves upon Jeffrey's prior technique for finding a multiparameter prior by decomposing the problem into a series of conditional lower dimensional problems, for which reasonable noninformative priors can be computed. The goal is to obtain a noninformative prior. It requires further proof to show that the reference prior for a specific setting results in a proper posterior distribution ( https://www.jstor.org/stable/3085905 ). I will give you an example in terms of a linear regression model with a patterned variance-covariance matrix and normal errors. Let $\boldsymbol{\theta} = (\boldsymbol{\beta}, \boldsymbol{\phi})$ , where $\boldsymbol{\beta}$ represents the parameters in the mean function and $\boldsymbol{\phi}$ represents the parameters in the variance function. Given $\boldsymbol{\phi}$ , a noninformative prior for $\boldsymbol{\beta}$ would be proportional to 1 (i.e. uniform over the real line). Thus we can decompose the prior and construct the reference prior, $\pi^R\left(\boldsymbol{\theta}\right)=\pi^R\left(\boldsymbol{\beta}|\boldsymbol{\phi}\right)\pi^R\left(\boldsymbol{\phi}\right),$ where $\pi^R\left(\boldsymbol{\beta}|\boldsymbol{\phi}\right)=1$ . Next, $\pi^R\left(\boldsymbol{\phi}\right)$ is computed using the Jefferys-rule prior, but for the marginal model defined via the integrated likelihood \begin{eqnarray*} L^1 \left(\boldsymbol{\phi}\right) = \int_{\mathbb{R}^p} L(\boldsymbol{\theta}) \pi^R\left(\boldsymbol{\beta}|\boldsymbol{\phi}\right) d \boldsymbol{\beta}. \end{eqnarray*} Note, that a closed-form solution for $L^1 \left(\boldsymbol{\phi}\right)$ exists for this model so that obtaining the reference prior is not difficult. The difficulty is in proving that the posterior density will always be proper and hence you have an "automatic" noninformative prior at your disposal to perform Bayesian analysis.
