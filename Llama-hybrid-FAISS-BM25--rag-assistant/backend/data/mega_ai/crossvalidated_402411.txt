[site]: crossvalidated
[post_id]: 402411
[parent_id]: 
[tags]: 
pre-activation batch normalization in modern neural nets

Some of the more modern neural network architectures (Densenets, for example) use pre-activation batch normalization i.e batch normalization -> activation -> Convolution rather than the usual Convolution -> batch normalization -> activation. The resulting network has the same number of parameters but most of the papers report that pre-activation in these networks is better. I am struggling to figure out why this is the case. Is there any intuitive or principled reasoning behind it?
