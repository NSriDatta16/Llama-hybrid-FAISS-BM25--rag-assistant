[site]: datascience
[post_id]: 49661
[parent_id]: 
[tags]: 
Measuring distance preservation in dimensionality reduction

I am looking to compare the distance preserved during dimension reductions for several techniques. I have read some papers on similar topics here and here . For example, I would like to use the Euclidean Distance to measure the distance preserved during PCA's dimension reduction. However, my point of confusion what are $X$ and $Y$ in $$d(X, Y) = \sqrt{\sum^n_{i=1}\left(x_i - y_i\right)^2}$$ I understand how to calculate $d\left(X, Y\right)$ given two vectors/matrices, but I don't understand with context to PCA. Let me try to explain. Let $W_{d \times k}$ be the matrix of $k$ leading eigenvectors, $X_{d\times n}$ be the original data, and $Z_{k\times n}$ be the projection of $X$ onto the reduced subspace. $Z = W^{T}X$ Back to calculating $d\left(X, Y\right)$ . My guess is that the PCA's $X$ correspond to $X$ and $Y$ can correspond to $Z$ . But how does this work since $X$ and $Y$ have different dimensions? I have to be oblivious to something here. Also, I am not concerned if a Euclidean Distance measure is not a good choice for measuring PCA's distance preservation (unless they are incompatible). This is simply exploration. Edit: For example, if I have $$X_{d\times n} = \begin{bmatrix} x_{11} & \dots & x_{1n} \\ x_{21} & \dots & x_{2n} \\ x_{31} & \dots & x_{3n} \\ \end{bmatrix} $$ and say I choose to retain $k = 2$ principal components which are then projected onto $$Z_{k \times n} = \begin{bmatrix} z_{11} & \dots & z_{1n} \\ z_{21} & \dots & z_{2n} \\ \end{bmatrix} $$
