[site]: crossvalidated
[post_id]: 125894
[parent_id]: 124681
[tags]: 
Thought: I think eigenfaces is a decent way to convert what can be million-dimensional spaces to a few tens of dimensions. Premise: So lets assume that you are using a decent eigenfaces tool, or one that: does preprocessing to align appropriate features handles colors in an appropriate way makes sure the pictures used are all the same size This means you don't have "pictures" as much as you have vectors of length O(n=50) elements in size where the elements are weights for each eigen-face comprising the basis. Analysis: First I would create 150-element vectors (concatenation of weight) as inputs and 1-element vectors (elements of closest match) as outputs. If element 1 and 2 were closest then the output value would be "12". If elements 1 and 3 were closest then the output would be "13". If elements 2 and 3 were closest then output would be "23". Given that there are only 3 unique outputs, I could re-map them to case 1 for "12", case 2 for "13" and case 3 for "23. Second I would want to throw away as much meaningless data as possible. This means that I would try to use something like random forests to determine which of the ~150 columns were not informative. There is also a "random evil twin method" but I don't have it at my fingertips the way R gives me with random forests. (If you know a good R library for this, I invite you to put it in the comments). Third, in my personal experience, if you have decent sample sizes, and decent basis a random forest can usually drop you down to the ~30 variables of interest, even from as much as 15k columns. This is where you have to consider what is the general form of the answer. You could try a dozen breeds of transforms of these variables to map the reduced inputs to the outputs: you could train an RF of the reduced inputs and call it good. you could train a NN on the reduced inputs if you wanted better smooth interpolation and generalization than an RF you could use some sort of linear transformation on the inputs there are a few dozen other ML hammers to hit it with, but when you are a hammer every problem looks like a nail. More thoughts: I would be curious about which of the eigenfaces the reduced set references. I would just like to see that data and let it talk to me. I'm quite curious about your sample sizes and the nature of your variation. If you are looking at 3 rows, then having 150 columns is not going to be too productive. If you have a few thousand rows then you might be in great shape. A few hundred rows and you might be average. I would hope that you accounted for all sources of variation in terms of ethnicity, facial shape, and such. Dont be afraid of looking through simple models first. They can be good. Their interpretation and applicability are easily evaluated. Their execution can be tested and confirmed with substantially less effort then complex and highly sensitive methods. UPDATE: The "random evil twin" tool is "Boruta". ( link )
