[site]: crossvalidated
[post_id]: 485068
[parent_id]: 485011
[tags]: 
Do we treat $(X_i,Y_i)$ 's as random variables? In a regression model $Y=X\beta+\epsilon$ , $\epsilon$ is a random variable and therefore $Y$ , a transformation of a random variable, is itself a random variable. The explanatory variables may be random or fixed. Tipically they are fixed when the researcher "controls" or "sets" the values of the explanatory variables. In experimental studies "the individuals or material investigated, the nature of the treatments or manupulations under study and the measurement procedure used are all selected, in their important features at least, by the investigator" (Cox and Reid, The Theory of the Design of Experiments , CRC, 2000, p. 1). For example, in a clinical study drugs and their doses are decided by the researcher, are fixed and known quantities, not random variables. However, one can also think of stratified sampling , with the values of $X$ defining the strata, or subpopulations. "For example, if $X$ denotes gender, a researcher may decide to collect a sample consisting of 50 men, followed by 25 women. If so, the sample values of $X$ are nonstochastic as required, but the researcher has not controlled, set, or manipulated the gender of any individual in the population" (Arthur Goldberger, A Course in Econometrics , Harvard University Press, 1991, p. 148). In stratified sampling $X$ may be random, but $n$ values are specified, they define $n$ subpopulations, and are mantained in repeated sampling, so the expectation of each $Y_i$ will depend only on $i$ (Goldberger, p. 172). In random sampling from a multivariate population both $Y$ and $X$ are random variables. This often happens in observational studies, where the researcher observes several subjects, measures several variables together, looks for their joint dependence. A typical example is econometrics (Bruce Hansen, Econometrics , ยง1.4). Do we treat $\beta_0$ and $\beta_1$ as random variables? In "classical" statistical inference, parameters are just unknown quantities. (In bayesian inference parameters are random variables.) Do we treat $\hat\beta_0$ and $\hat\beta_1$ as random variables? In "classical" statistical inference estimators are random variables. What can have an expected value and what can't (what gets treated as a constant when finding expected values) and why? If $X$ is nonrandom, then you assume $E[\epsilon]=0$ and look for $E[Y]=X\beta$ . If $X$ is random, then you also assume $E[\epsilon\mid X]=0$ , and look for $E[Y\mid X]$ .
