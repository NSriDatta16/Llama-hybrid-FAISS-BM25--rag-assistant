[site]: crossvalidated
[post_id]: 421681
[parent_id]: 420212
[tags]: 
I think your understanding is fine but it has not crystallised how the estimates $\text{Pr}(Y=\{-1,+1\}|x)$ came to be and their relation to the expected exponential loss $E\{l(x)\}\}$ . We use $l(x) = \exp\{-Yf(x)\}$ exactly because it is minimised at $\frac{1}{2} \log\frac{\text{Pr}(Y=+1|x)}{\text{Pr}(Y=-1|x)}$ . We know by definition that: $E\{ \exp\{-Yf(x)\}\}$ $=$ $\text{Pr}(Y=+1|x)\exp\{-f(x)\}$ $+$ $\text{Pr}(Y=-1|x)\exp\{f(x)\}$ . Taking the derivative of this is simply: $-\text{Pr}(Y=+1|x)\exp\{-f(x)\}$ $+$ $\text{Pr}(Y=-1|x)\exp\{f(x)\}$ so setting this to zero directly leads to $\text{Pr}(Y=+1|x) = \frac{\exp\{f(x)\}}{ \exp\{-f(x)\} + \exp\{f(x)\}}= \frac{1}{1+ \exp\{-2f(x)\}}$ , i.e. the one-half of the log-odds of $\text{Pr}(Y=\{-1\}|x)$ , which (as Efron & Hastie (2016), (pp. 343) put it): " a perfectly reasonable (and symmetric) model for a probability ". i.e. we use $l(x)$ because through $f(x)$ defines to re-express our classification task in probabilistic terms; $f(x)$ independently is not interpretable as sample-wide loss function. In case you have not come across it yet, Friedman et al. (2000) Additive Logistic Regression: A statistical view of boosting is non-trivial good reference on the matter. We can focus on the Sect. 4 " AdaBoost: an additive logistic regression model " (and Sect. 1 "Introduction"), it should be enough to clear things fully on this matter. A more conversational approach can be found in: Efron & Hastie (2016) Computer Age Statistical Inference , Chapt. 17 " Random Forests and Boosting ".
