[site]: crossvalidated
[post_id]: 636224
[parent_id]: 
[tags]: 
Why does my neural network consider different features important compared to my decision tree?

I built a neural network and a decision tree using very similar data sets (the only difference was the randomness of selecting the training vs testing set). The variables with the highest shapley values in my neural network are completely different from the top node variables in my decision tree. How could this possibly be? Why would these two different models consider different sets of variables to be important?
