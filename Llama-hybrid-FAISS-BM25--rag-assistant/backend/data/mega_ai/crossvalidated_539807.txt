[site]: crossvalidated
[post_id]: 539807
[parent_id]: 539795
[tags]: 
Well, one does not just analyze their original dataset with missing values. If you do not choose a missing data technique, R chooses one for you. The typical approach is a complete-case analysis - that is when all rows with missing values are excluded from the analyses. It's a valid (unbiased) way to handle these cases, even for Bayesian estimation and inference. Of course when you write up the analysis, you have to explain how many rows of data were excluded, what were the missingness patterns (i.e. which variable(s) are missing), and the univariate characteristics of the non-missing variables among the excluded rows. Additionally, you need to consider the possibility of non-informative missingness; no analyses necessary, but some simple consideration of the design and the population: what might give rise to non-informative missingness, and how might follow-up studies assess this? Missing data methods might be needed if the number of complete cases is too few to provide any kind of precise estimation. Using multiple imputation in a Bayesian analysis makes it somewhat hard to wrap ones mind around; usually MI (like MICE) focuses on frequentist inference where for each multiply imputed dataset, the estimation is performed, and then the estimates and standard errors are combined using Rubin's rules. The Bayesian analysis outputs an entire posterior, and I'm not sure any method is in place to combine estimates of the posterior. However, one can use Bayesian methods to integrate the missing values, and handle the missing data at the likelihood level, rather than at the estimate level. In frequentist inference, one simply uses the EM algorithm to integrate over the likelihood of the missing values predicted by the non-missing values.
