[site]: crossvalidated
[post_id]: 478044
[parent_id]: 478017
[tags]: 
My mental model is that NN loss surfaces are narrow valleys: they have steep sides, but the bottom of the valley shows a shallow decline. In particular, the steepness of the sides can mean that the steepest direction tends to be dominated by the sides, instead of the shallow decline at the bottom. So a learning rate which is too large will tend to move by jumping from one side of the valley to the other, but can also make less-pronounced progress towards the minimum at the same time -- moving mostly from side-to-side, while also moving in the direction of the shallow decline. Moreover, you've only reported the results of an epoch's end, but not progress within an epoch. My hypothesis is that within an epoch, the training loss is fluctuating widely, but using the mean discards information about those fluctuations. As further evidence, there's a hint that when the validation accuracy is low, the training loss tends to be lower also (but not as low). This is consistent with my hypothesis. When we observe a large value of validation loss, we're just seeing the "snapshot" corresponding to wherever the parameters are at that time. While the mean of the training loss suppresses this fluctuation, the validation loss exposes it because the parameters aren't changing, so we're not averaging over many different parameter values. Tracking the training loss within epochs could confirm or disconfirm this hypothesis. (As an aside, measuring training statistics every mini-batch could consume too much memory if you have a large dataset and/or small mini-batch size. So instead, every $k > 1$ mini-batches, record two pieces of data: the most recent loss value and the mean of the most recent $k$ mini-batches. Choose the smallest $k$ that doesn't consume too much memory. ) My hypothesis is that lowering the learning rate will allow smoother progress over the loss surface. Instead of jumping around the steep sides of the narrow valley, the optimizer will be closer to the valley floor, and make steadier progress.
