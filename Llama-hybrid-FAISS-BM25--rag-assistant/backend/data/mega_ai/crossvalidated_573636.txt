[site]: crossvalidated
[post_id]: 573636
[parent_id]: 
[tags]: 
Prior in variational autoencoders

I am currently dealing with variational autoencoders where I've read the original paper "An introduction to variational Bayes" from Kingma and Welling. I am currently still a little confused about the choice of the prior $p(z)$ . For my understanding the variational autoencoder has the three possible distribution families we can choose: The family of distributions for the encoder $q_\phi(z|x)$ parametrised by $\phi$ . The family of distribtuions for the decoder $p_\theta(x|z)$ parametrised by $\theta$ . The family of prior distributions $p_\theta(z)$ . Now for my understanding and also according to this notation used, the parameters for the decoder and the prior have to be the same, since overall we want to compute $p_\theta(x,z)$ with a neural network right? Then, how is it possible to choose $p_\theta(z)=\mathcal{N}(0,I)$ while at the same time optimising the parameters $\theta$ computed by the decoder network? As a third question: if not restricting the prior to be a standard normal distribution, how do we learn the parameters of the prior $p_\theta(z)$ ? Edit: I am aware, that in practice we choose the prior distribution to be standard normal. But why are we allowed to do so? For my understanding, the whole model (looking at the variational autoencoder as one model) we want to fit $p_\theta(x,z)=p_\theta(x|z) p_\theta(z)$ meaning the parameters $\theta$ are coming from the same parameter space for the decoder and the prior distribution. But when on the one hand fixing the prior distribution and on the other hand computing the parameters $\theta$ for the decoder, then we are kind of violating this assumption aren't we?
