2^{-{\textrm {length}}(q)}} should be interpreted as a mixture (in this case, a sum) over all computable environments (which are consistent with the agent's past), each weighted by its complexity 2 − length ( q ) {\displaystyle 2^{-{\textrm {length}}(q)}} . Note that a 1 … a m {\displaystyle a_{1}\ldots a_{m}} can also be written as a 1 … a t − 1 a t … a m {\displaystyle a_{1}\ldots a_{t-1}a_{t}\ldots a_{m}} , and a 1 … a t − 1 = a < t {\displaystyle a_{1}\ldots a_{t-1}=a_{<t}} is the sequence of actions already executed in the environment by the AIXI agent. Similarly, o 1 r 1 … o m r m = o 1 r 1 … o t − 1 r t − 1 o t r t … o m r m {\displaystyle o_{1}r_{1}\ldots o_{m}r_{m}=o_{1}r_{1}\ldots o_{t-1}r_{t-1}o_{t}r_{t}\ldots o_{m}r_{m}} , and o 1 r 1 … o t − 1 r t − 1 {\displaystyle o_{1}r_{1}\ldots o_{t-1}r_{t-1}} is the sequence of percepts produced by the environment so far. Let us now put all these components together in order to understand this equation or definition. At time step t, AIXI chooses the action a t {\displaystyle a_{t}} where the function ∑ o t r t … max a m ∑ o m r m [ r t + … + r m ] ∑ q : U ( q , a 1 … a m ) = o 1 r 1 … o m r m 2 − length ( q ) {\displaystyle \sum _{o_{t}r_{t}}\ldots \max _{a_{m}}\sum _{o_{m}r_{m}}[r_{t}+\ldots +r_{m}]\sum _{q:\;U(q,a_{1}\ldots a_{m})=o_{1}r_{1}\ldots o_{m}r_{m}}2^{-{\textrm {length}}(q)}} attains its maximum. Parameters The parameters to AIXI are the universal Turing machine U and the agent's lifetime m, which need to be chosen. The latter parameter can be removed by the use of discounting. Optimality AIXI's performance is measured by the expected total number of rewards it receives. AIXI has been proven to be optimal in the following ways. Pareto optimality: there is no other agent that performs at least as well as AIXI in all environments while performing strictly better in at least one environment. Balanced Pareto optimality: like Pareto optimality, but considering a weighted sum of environments. Self-optimizing: a policy p is called self-optimizing for an environment μ {\displaystyle \mu } if the performance of p approaches the theoretical maximum for μ {\displaystyle \mu } when the length of the agent's lifetime (not time) goes to infinity. For environment classes where self-optimizing policies exist, AIXI is self-optimizing. It was later shown by Hutter and Jan Leike that balanced Pareto optimality is subjective and that any policy can be considered Pareto optimal, which they describe as undermining all previous optimality claims for AIXI. However, AIXI does have limitations. It is restricted to maximizing rewards based on percepts as opposed to external states. It also assumes it interacts with the environment solely through action and percept channels, preventing it from considering the possibility of being damaged or modified. Colloquially, this means that it doesn't consider itself to be contained by the environment it interacts with. It also assumes the environment is computable. Computational aspects Like Solomonoff induction, AIXI is incomputable. However, there are computable approximations of it. One such approximation is AIXItl, which performs at least as well as the provably best time t and space l limited agent. Another approximation to AIXI with a restricted environment class is MC-AIXI (FAC-CTW) (which stands for Monte Carlo AIXI FAC-Context-Tree Weighting), which has had some success playing simple games such as partially observable Pac-Man. See also Gödel machine References "Universal Algorithmic Intelligence: A mathematical top->down approach", Marcus Hutter, arXiv:cs/0701125; also in Artificial General Intelligence, eds. B. Goertzel and C. Pennachin, Springer, 2007, ISBN 9783540237334, pp. 227–290, doi:10.1007/978-3-540-68677-4_8.