[site]: datascience
[post_id]: 126414
[parent_id]: 
[tags]: 
Integrating MLFlow and SageMaker for a More Robust ML Model Deployment Pipeline

I'm seeking advice on enhancing the deployment pipeline of a machine learning model that's accessed via a FastApi in production. My goal is to replace the existing setup with a more robust and efficient system that includes built-in model monitoring . I'm considering using MLFlow Registry in conjunction with an AWS SageMaker endpoint. Current Pipeline: Custom transformers (e.g., OneHot encoder, imputers, business-specific transformers) are implemented as scikit-learn transformers. An scikit-learn regressor serves as the predictor. The entire process, including transformers and predictor, is serialized into multiple pickle files as an scikit-learn pipeline. Model metrics are logged in MLFlow during training. A FastApi server endpoint validates JSON input and uses these pickle files for transformation and prediction. Intended Approach : Fit transformers and model > Log artifacts in MLFlow > Deploy using SageMaker endpoint. However, I'm unsure about this approach, especially regarding the integration of custom transformers and the scikit-learn pipeline with SageMaker. Specific Questions : Is the above approach feasible, particularly in terms of integrating custom transformers and a scikit-learn pipeline? Is the above approach recommended? Do you have another suggestion to achieve my goal- model monitoring, more officiant predictions, and robust deployment? I've previously considered SageMaker pipelines, but they seemed overly complex for my specific use case and unflexible. Here are two resources I've been referring to: Saving an XGBoost Model in MLFlow Format Deploying an ML Model with MLFlow and AWS SageMaker
