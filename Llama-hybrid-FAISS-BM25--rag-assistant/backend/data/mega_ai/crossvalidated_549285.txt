[site]: crossvalidated
[post_id]: 549285
[parent_id]: 
[tags]: 
Do Statistical Models (Automatically) Allow For "Ranking" Predictions?

I have always had the following question: Do statistical models (e.g. regression models, decision trees, random forest, etc.) automatically allow for ranking the predictions? I.e. If a random forest (e.g. supervised binary classification task) predicts that one new observation will belong to "Class 0" with probability 0.7 and another new observation will belong to "Class 1" with probability 0.75 - (assuming that we trust the model), can we say that the second prediction is "stronger" than the first prediction? To give some context to my question, I present the following example: I saw the following definition of a (classification) decision tree: Where: $f(\theta, x_0)$ is the prediction made by the decision tree for a new observation $x_0$ " $\theta$ " is the set of variables used by the decision tree "I" is a indicator variable (can only take values "0" or "1") "R_theta" is the terminal node to which the decision tree assigned the new observation x_0 "x_i, y_i" are the covariates and response values for individual observations "N_theta" is the number of points in the terminal node "R_theta" Thus, the decision tree takes the average value of the response variable corresponding to all observations within the terminal node that the new observation "x_0" was assigned to : and uses this average value as the predicted response for "x_0". A random forest can be defined as "randomly" combining (via bootstrap aggregation) many such decision trees together (Breiman et al 2001 showed the statistical advantages of doing this): In the case of classification, it is important to note that the random forest algorithm (as well as many predictive algorithms/statistical models) will predict the "probability" that a new observation belongs to each of the classes of the response variable. For the random forest, this probability can be written as follows (i.e. the probability that a new observation "x_0" is predicted as "Class 1"): Where "K(x_0, x_i)" is a kernel function, e.g. Question: Suppose there are 2 new points "x_a and x_b". The random forest predicts the probability for both of these points as: P(y = 1 | x_a) = 0.7 and P(y = 1 | x_b) = 0.8. Can we say that the prediction of "x_b" is more reliable than "x_a"? Demonstration: I also demonstrate this with an example (supervised binary classification where we predict whether insurance claims will be "accepted" or "rejected") in the R programming language where the prediction probabilities made by a random forest model are used to assess the "relative reliability" of the predictions (i.e. rank them): #load libraries library(caret) library(randomForest) library(rpart) #generate data set.seed(123) property_damages_in_dollars Now, we get ready to train the model #split data into train set and test set index = createDataPartition(mydata$response, p=0.7, list = FALSE) train = mydata[index, ] test = mydata[-index, ] #create random forest statistical model rf = rpart(response ~ property_damages_in_dollars + car_damages_in_dollars + other_damages_in_dollars, data=train) Now, we make some predictions and extract the prediction probabilities: #have the model predict the test set pred = predict(rf, test, type = "prob") labels = as.factor(ifelse(pred[,2]>0.5, "approved", "rejected")) confusionMatrix(labels, test$response) #manipulate the prediction frame pred = data.frame(pred) pred $what_model_predicted = labels pred$ truth = test$response pred $prob = pmax(pred$ approved, pred $rejected) pred$ val = ifelse(pred $what_model_predicted == pred$ truth,1,0) pred $id = test$ ID Now, if we look at the results: head(pred) approved rejected what_model_predicted truth prob val id 1 0.1632653 0.8367347 approved rejected 0.8367347 0 1 16 0.2105263 0.7894737 approved approved 0.7894737 1 16 18 0.1632653 0.8367347 approved rejected 0.8367347 0 18 23 0.2163265 0.7836735 approved approved 0.7836735 1 23 24 0.2163265 0.7836735 approved approved 0.7836735 1 24 "Observation 1" was predicted to be "approved" with 0.83 probability and "Observation 16" was predicted to be "approved" with 0.78 probability. Would it have been reasonable to infer that the model was more "confident" about the prediction it made for "Observation 1" vs "Observation 16"? Statistically speaking, does it make sense to rank the predictions made by the random forest in order of descending probability and interpret this order as an indication of "reliability"? Note: In this example, the actual random forest model performs poorly Extra : For practicality, we can rank the predictions in descending order of "reliability": head(pred[order(-pred$prob), ]) approved rejected what_model_predicted truth prob val id 173 0.0000000 1.0000000 approved rejected 1.0000000 0 173 296 0.0000000 1.0000000 approved rejected 1.0000000 0 296 1 0.1632653 0.8367347 approved rejected 0.8367347 0 1 18 0.1632653 0.8367347 approved rejected 0.8367347 0 18 144 0.1632653 0.8367347 approved rejected 0.8367347 0 144 163 0.1632653 0.8367347 approved approved 0.8367347 1 163 References https://arxiv.org/pdf/1812.05792.pdf
