[site]: datascience
[post_id]: 98391
[parent_id]: 
[tags]: 
PyTorch backwards() call on loss function

Can someone confirm that a call to loss.backward() given loss defined with nn.MSELoss() if called in a loop like this: for j in range(nb_epochs): for i in range(TrainingSetSize): ... loss = myMSELoss(myNNOutput_i,myTargetForOutput_i) loss.backward() #this is done after the inner loop has looped over all tr set of course with torch.no_grad(): for p in model.parameters(): p -= learning_rate * p.grad model.zero_grad() does really just summing the gradients when repeatedly called as shown above (after first also computing them via backprop algo of course) for each parameter of my neural network? i.e., something equivalent to (the following pseudo-code): for p in model.parameters(): p.grad += curr_p.grad ... As far as I understand repeatedly calling backward() must be just summing (cummulating) the gradients , - until we possibly reset them with e.g. zero_grad(). (Of course backward() also computes the gradients, I know, but I am talking about repeatedly calling it as in the above code, to be clear). I would really like someone to confirm because although my code works, I am surprised that a "fully manually implemented" i.e. with coding myself the backprop equations etc backprop does not behave at all in the same way as this i.e. it takes 10 or 20 times more epochs to reach a similar error and when using backward() although the error drops far quicker the amount of memory used for a (very small nn, 2 linear layers of 784x50 and 50x10 ) takes a gigantic amount of RAM after about 100 iter (i.e. RAM is full (8GB+ after 100 iter!)) which seems crazy .. P.S. this is for testing stuff, just curious to know, it is of course not for a "real implementation" where one can use all the pyTorch functions as optimizers etc. from which I am aware of but is not the purpose here.
