[site]: crossvalidated
[post_id]: 561883
[parent_id]: 112144
[tags]: 
I agree with @Nir Friedman that a Bayesian approach would be a good fit here, so I went ahead and implemented it in Python. Since the uniform prior is conjugate to the multinomial distribution, we can implement it without any fancy MCMC/HMC stuff. First things first, I imported a few libraries and defined a function to calculate entropy: import numpy as np import seaborn as sns from matplotlib import pyplot as plt def entropy(x): return np.sum( -x*np.log2(x) , axis=-1) Then, I took Monte-Carlo samples from the posterior distribution of the entropy. This was done in two steps: First, notice that the posterior distribution for the true multinomial proportions is a Dirichlet. We can sample from it in Python in a single line of code: np.random.dirichlet(counts_die+1, 1000000) Calculate the entropy for each sample from that Dirichlet distribution The code for this is as follows: counts_die_1 = np.array([6,7,3,5,2,1]) counts_die_2 = np.array([3,4,2,1,1,2]) entropy_die_1 = entropy(np.random.dirichlet(counts_die_1+1, 1000000)) entropy_die_2 = entropy(np.random.dirichlet(counts_die_2+1, 1000000)) Then, we can plot the distribution for the difference between the entropies: sns.kdeplot(entropy_die_1-entropy_die_2, fill=True) plt.axvline(0, ls="--", c="k") # changing plot aesthetics plt.gca().set(yticklabels=[], ylabel="", xlabel="Difference in entropy, in bits") plt.gca().tick_params(left=False) sns.despine(left=True) The result looks like this: We don't see evidence for a difference in entropies, and we can be fairly certain (>99%) that any such difference is less than half a bit. We can get the probability of die 1 being less random than die 2 like this: (entropy_die_1 This gives us 0.512942: very close to 0.50 meaning that we have little to no evidence of a die being more random than another. Hope it was helpful!
