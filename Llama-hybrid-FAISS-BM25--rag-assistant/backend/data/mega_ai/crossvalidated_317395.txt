[site]: crossvalidated
[post_id]: 317395
[parent_id]: 317377
[tags]: 
In usual terminology, if we have a regression model $$ Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2 + \varepsilon $$ then $Y$ is called the dependent, target, or predicted variable, while $X_1,X_2$ are independent variables, features, predictors, or just "variables". $X_1 X_2$ is the interaction term and sometimes we talk about "terms" in regression models. $\beta_0, \beta_1, \beta_2, \beta_3$ are regression parameters, or coefficients, where $\beta_0$ is called the intercept, or bias term, and $\varepsilon$ is the noise, or error term. Calling the variables "parameters" would be very confusing for most of the statistical audience. Even if your variables describe things like parameters of some machine that are fixed before the experiment, or parameters of the experiment that are manipulated by the researcher, then using descriptive names for them rather than calling them "parameters" would seem more appropriate to me. The main reason is that in most statistical/machine learning problems we talk about estimating parameters of models: sometimes we could have parameters that are known in advance and the ones to be estimated, but "estimating the parameters" is a big part of statistics.
