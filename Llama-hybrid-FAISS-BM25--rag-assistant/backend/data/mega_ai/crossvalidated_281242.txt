[site]: crossvalidated
[post_id]: 281242
[parent_id]: 281167
[tags]: 
I think that you may be misunderstanding how cross-validation works, as well as what it is used to do. Additional detail on your research question and methodology would help. But TL;DR: the average of the cross-validation accuracy measures (generally mean squared error or successful classification) approximates the accuracy of your model on test data (a held-out sample or new data set). How well it approximates that depends on your model, and whether or not that's even what you're interested in depends on what you are trying to determine with cross-validation. For how it works, k-fold CV randomly divides your data into k subsections and then tests the model fit using fold k as a validation set against the remaining k-1 folds. This procedure is repeated k times, and since each repetition uses different folds for the validation/test sets it is unsurprising (and normal) that different results are produced. As to the data split ratios I don't know enough to comment. Some clarification on what you are referring to by "data-set split ratio(70:30)" would be valuable, as well as the ratios you observed when using 5 vs. 10 folds. Are you using a software package to do the data-splitting and cross-validating, and if so, how are you calling it?
