[site]: crossvalidated
[post_id]: 96083
[parent_id]: 
[tags]: 
Discrete features are less important

I am trying to train a random forest classifier. As predictors, I keep both discrete features and continuous features (the discrete ones including booleans, counters, etc., and the continuous contains floats). Training the model both in R and in Python's scikit.learn , I get that the discrete features are always less important (using the importance feature of the model). I wonder if that's a characteristic of my data or perhaps there's some innate characteristic of discrete features. I am not even sure it makes sense to mix discrete and continuous features in the same predictors matrix. Some numbers: a current test with about 30 features (not much, it'll grow larger) yields mean importance 0.035; the continuous features importances range from 0.037 to 0.06 (mean > 0.04) and the discrete features importances range from 0.005 to 0.03 (mean To sum, I have two questions: is there something "inherent" in discrete features that makes them "less important" (according to the algorithm for importance calculation) does it make sense to mix the feature types?
