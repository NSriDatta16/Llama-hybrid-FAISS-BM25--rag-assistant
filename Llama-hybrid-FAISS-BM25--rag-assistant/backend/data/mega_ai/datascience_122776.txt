[site]: datascience
[post_id]: 122776
[parent_id]: 
[tags]: 
Overfitting in the trained model

For my project on the classification problem of predicting churn customers, I trained various base models using k-fold validation on the training dataset and out of which random forest gave the best result but during prediction it is giving lower score than it gives on the test data which is apparently showing overfitting, even SVC was showing the same thing. How can I correct my code so that it gives similar results to that of the training data set? Python code: models = [] models.append(('LR', LogisticRegression())) models.append(('KNN', KNeighborsClassifier(n_neighbors=5))) models.append(('TREES', DecisionTreeClassifier(max_depth=20))) models.append(('FOREST', RandomForestClassifier(max_depth=15, n_estimators=30))) models.append(('SVM', SVC())) models names = [] for name, model in models: kfold = StratifiedKFold(n_splits=5, random_state=1, shuffle=True) cv_results = cross_val_score(model, X_train_scaled, Y_train, cv=kfold, scoring='roc_auc') print(cv_results) mean_score = np.mean(cv_results) print(f"Average ROC_AUC Score for {name} is: {mean_score:.3f}") output : [0.8627102 0.84906579 0.84063598 0.82388159 0.83202681] Average ROC_AUC Score for FOREST is: 0.842* rf_model=RandomForestClassifier(max_depth=15,n_estimators=30) rf_model.fit(X_train_scaled,Y_train) y_pred=rf_model.predict(X_test_scaled) auc_score= roc_auc_score(Y_test,y_pred) print(f'AUC Score for test dataset is: {auc_score}') Output: AUC Score for test dataset is: 0.7205451341221849
