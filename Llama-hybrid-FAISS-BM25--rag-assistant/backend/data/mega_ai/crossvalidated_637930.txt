[site]: crossvalidated
[post_id]: 637930
[parent_id]: 637917
[tags]: 
You can see discussion about this in any textbook eg Elements of Statistical Learning. I would say those heuristics are hard to justify (ridge regression + dropping least significant) roughly speaking ridge regression and lasso have different prior beliefs about the data. ridge regression: the inputs are correlated,common signal + independent identically distributed noise. Averaging the inputs (as is done by ridge regression) therefore increases the signal to noise ratio. It should be clear that dropping variables is a bad idea, since the average of more variables is more accurate than one with less. an example might be exam results. However, if some inputs have more noise than others there will be a diminishing return from adding more variables to the average. lasso regression: the high dimensional data have a fraction of important variables and the rest are noise. I believe this regularisation arose from gene microarray studies where only a handful of genes impact a given disease. Another example might be hierarchical data (eg school district-> school -> class), where assuming a given level of the hierarchy is relevant to the dependent variable, lasso will select that, whereas ridge will have some combination with the lower levels too. Which of these best describe your data set will affect which of the 2 regularisations perform better in the ideal case, but often there is little difference in practise.
