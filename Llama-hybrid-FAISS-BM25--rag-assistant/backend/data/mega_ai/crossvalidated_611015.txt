[site]: crossvalidated
[post_id]: 611015
[parent_id]: 610975
[tags]: 
There are many ways how this could be done and which exactly is the best depends on the details of your problem, that you didn't give us. If you simply want to identify unique words per document or common words between documents, this needs treating documents as sets of words and calculating set intersection (common words) or difference (unique words). Since set can be implemented as a hash map, those operations are quite cheap computationally . But are you sure you need to find the unique words? If you have large enough collection of documents, it can be as well the case that there is no unique words, or very few of them, so finding them would not make much sense. Much simpler solution might be to just code the word occurrences as one-hot vectors and match them by encoding the query and finding the document with the most similar encoding. If you care about size of the data, this could be efficiently implemented as sparse vectors. More sophisticated approach could be to use a language model to encode the document as latent vectors and calculate the similarities in terms of the latent vectors rather than the raw one-hot encodings. If you care about representing the documents in the most compressed form, this might be one way of doing so. Finally, there are many out-of-the box solutions available (like ElasticSearch ) and starting with those is often a much better idea than re-inventing the wheel yourself.
