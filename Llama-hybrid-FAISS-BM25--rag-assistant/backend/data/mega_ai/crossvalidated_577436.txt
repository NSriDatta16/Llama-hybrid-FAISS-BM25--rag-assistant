[site]: crossvalidated
[post_id]: 577436
[parent_id]: 561153
[tags]: 
The transformation of the formula from (5) in the paper to the formula at the bottom of page 9 is a consequence of the Law of the Unconscious Statistician (LOTUS) -see (28) in Monte Carlo Gradient Estimation in Machine Learning for this form - and is a special case of the reparameterization trick described there applied to standardizing a Gaussian distribution. The proof of the LOTUS effectively involves showing that the Jacobian determinant ( $|\det J_{S^{-1}}|$ ) arising in the modified density for $\eta$ as a result of the change of variables cancels out with the Jacobian determinant arising in the integral corresponding to the expectation as a result of the same change of variables (i.e. we need to replace $d\zeta$ by $|\det J_S| d\eta$ ). This doesn't happen in integrals in general, but happens in expectations because of the co-occurence of the density and the differential. So I agree that the Jacobian is not one; however, the result specified is correct.
