[site]: crossvalidated
[post_id]: 433654
[parent_id]: 146317
[tags]: 
I realise that this is an old question and has some very good answers. I would like to share some practical personal experience. When working with generative machine learning techniques, you are usually working with some sort of probabilities. An example may be the mixture probabilities of the $k$ components in a mixture model. They have the following constraints: All probabilities must be positive. All elements of the probability set must sum up to one This is actually asking a lot. With gradient descent one usually deals with constraints via a penalty function. Here it will not work. As soon as a value violates one of these constraints, your code will typically raise a numerical error of sorts. So one has to deal with the constraints by never actually allowing the optimisation algorithm to traverse it. There are numerous transformations that you can apply to your problem to satisfy the constraints in order to allows gradient descent. However, if you are looking for the easiest and most lazy way to implement this then coordinate descent is the way to go: For each probability $p_i$ : $p_i^{k+1} = p_i^{k} - \eta \frac{\partial J}{\partial p_i}$ $p_i = \min(\max(p_i,0),1)$ Update all p_i: $\mathbf{P}^{j+1} = \mathbf{P}^j \cdot \frac{1}{\sum_{i=1}^n p_i} $ For someone like me that work in Python, this usually means that I have to use an additional for-loop which impacts performance quite negatively. Gradient descent allows me to use Numpy which is performance optimised. One can get very good speed with it, however, this is not achievable with coordinate descent so I usually use some transform technique. So the conclusion really is that coordinate descent is the easiest option to deal with very strict constraints such as the rate parameter in the Poisson distribution. If its becomes negative, you code complains etc. I hope this has added a bit of insight.
