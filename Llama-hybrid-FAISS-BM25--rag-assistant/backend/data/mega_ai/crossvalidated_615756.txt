[site]: crossvalidated
[post_id]: 615756
[parent_id]: 443486
[tags]: 
Logistic regression was invented by a statistician, for statisticians. SVMs are a true ML algorithm. Random Forests are a statistician's take on Machine Learning. Since you explicitly ask about " ML algorithms ", I suppose you are not interested in inference, probabilities, confidence intervals and all other things statistics can offer to facilitate human understanding of the problem. The next question to clarify is how much do you know about the data generating process, how many observations (points) you have and how many variables (dimensions) per observation. Logistic regression tries to predict class probabilities and model them by the logistic function. This works best when the classes are normally distributed, with same covariance matrices $^*$ . The more your data deviate from that assumption $-$ and that's more likely with high-dimensional data, $-$ the poorer the performance of logistic regression. SVMs, on the other hand, simply attempt to make good binary predictions, without caring about probabilities at all. They will try to find a classification rule which is most likely to produce correct predictions on new data, without making assumptions about the process behind them. They tend to work well on high-dimensional data (your images of cats and dogs, for example). The disadvantage is that they are resource-hungry: the memory and computational costs rise more than quadratically with the dataset size. My experience with Random Forests is limited. A colleague of mine (a statistician) has chosen it for a project because of the human-interpretability of its results, but that's not really the point if you are only interested in the algorithm's performance. So, my highly subjective and personal rule of the thumb would be: Low dimensional data, compact (ideally likely normally distributed) classes, interpretability is desired $\rightarrow$ logistic regression Low dimensional data, unknown and/or complex distribution, interpretability is desired $\rightarrow$ Random Forest Small-to-moderate, high-dimensional dataset ( $\rightarrow$ SVM Everything else $\rightarrow$ neural networks *It is possible to construct artificial cases which deviate from this rule, e.g. having Poisson-distributed classes, but I have never encountered such a constellation in practice.
