[site]: crossvalidated
[post_id]: 266863
[parent_id]: 
[tags]: 
large training dataset for a neural network: splitting the data into chunks and pass fitted weights from one chunk to another

My data constitutes a few million observations, and am not able to load the entire data all at once on the memory (which seems to be the common practice with most packages such as scikit learn and Keras require). As a workaround, I was thinking to load data in smaller chunks, fit the model for that chunk (with the iteration number set at 1) get the weights, use the weights as the initial weight for the next chunk. Continue this until all data is used. Repeat this whole process many times (this would be the actual number of iteration). Does this process make sense? My second question may be off topic, but does either of scikit learn or Keras (or some other libraries) allow us to set the initial weights for a neural network?
