[site]: stackoverflow
[post_id]: 1333230
[parent_id]: 1332527
[tags]: 
You could use a map from a representative element to a list/vector/deque of other elements. This requires relatively fewer comparisons in insertion into the container and means that you can iterate through the resulting groups without having to perform any comparisons. This example always inserts the first representative element into the mapped deque storage as it makes the subsequent iteration through the group logically simple, but if this duplication proves a problem then it would be easy to only perform the push_back only if (!ins_pair.second) . typedef std::map > Storage; void Insert(Storage& s, const Type& t) { std::pair ins_pair( s.insert(std::make_pair(t, std::deque ())) ); ins_pair.first->second.push_back(t); } Iterating through the groups is then (relatively) simple and cheap: void Iterate(const Storage& s) { for (Storage::const_iterator i = s.begin(); i != s.end(); ++i) { for (std::deque ::const_iterator j = i->second.begin(); j != i->second.end(); ++j) { // do something with *j } } } I performed some experiments for comparison and object counts. In a test with 100000 objects in random order forming 50000 groups (i.e. and average of 2 objects per group) the above method cost the following number of comparisons and copies: 1630674 comparisons, 443290 copies (I tried bringing the number of copies down, but only really managed to at the expense of comparisons which seem to be the higher cost operation in your scenario.) Using a multimap, and retaining the previous element in the final iteration to detect the group transitions cost this: 1756208 comparisons, 100000 copies Using a single list and popping the front element and performing a linear search for other group members cost: 1885879088 comparisons, 100000 copies Yes, that's ~1.9b comparisons compared to ~1.6m for my best method. To get the list method to perform anywhere near an optimal number of comparisons it would have to be sorted and this is going to cost a similar number of comparisons as building an inherently ordered container would in the first place. Edit I took your posted code and ran the implied algorithm (I had to make some assumptions about the code as there as some assumed definitions) over the same test data set as I used before and I counted: 1885879088 comparisons, 420939 copies i.e. exactly the same number of comparisons as my dumb list algorithm, but with more copies. I think that that means we using essentially similar algorithms for this case. I can't see any evidence of an alternative sort order, but it looks like you want a list of the groups which contain more than one equivalent elements. This can be simply achieved in my Iterate function by adding in if (i->size > 1) clause. I still can't see any evidence that building a sorted container such as this map of deques isn't a good (even if not optimal) strategy.
