[site]: datascience
[post_id]: 102525
[parent_id]: 100515
[tags]: 
The bit of this code that surprised me matches the tutorial you linked to, but they seem to get very poor performance. Their val_loss at the (best) epoch 105 is 0.3232 ; whereas in the RNN tutorial they had a best val_loss of 0.0895 (epoch 376). The bit that surprised me was using Conv1D(filters=4, kernel_size=1) as the feed-forward network. I looked a couple of times for a link to a paper where this had come from, or some justification/explanation, but didn't spot anything. More normal, for a Transformer, is a simple fully-connected network. E.g. this keras tutorial does it as: self.ffn = keras.Sequential( [layers.Dense(ff_dim, activation="relu"), layers.Dense(embed_dim),] ) which is then used as: out1 = self.layernorm1(inputs + attn_output) ffn_output = self.ffn(out1) ffn_output = self.dropout2(ffn_output, training=training) Typically ff_dim is 4x the size of embed_dim ; I've done experiments with trying a smaller multiplier, but I think you want it to be at least 2x. BTW, I'm having trouble working out what your embed_dim is; but maybe it is only 10? In which case a head_dim of 256 is inappropriate. (E.g. if embed_dim was 256, you might have 4 heads each of size 64.) Taking a step back, the way you've described your data sounds like it can be rephrased in NLP terms as you have a batch of 75575 sentences , each with exactly 168 tokens . In NLP a token is a string of unicode characters, and the embedding layer will turn them into e.g. 256 floating point values, usually each randomly initialized to be roughly -1 to +1, but then they can be learned; whereas you have a set of 10 floating-point (?) features, that haven't been normalized to that kind of range (?). In summary, I'd first follow a tutorial that shows it managed an improvement over an RNN; and second I'd work on the data representation.
