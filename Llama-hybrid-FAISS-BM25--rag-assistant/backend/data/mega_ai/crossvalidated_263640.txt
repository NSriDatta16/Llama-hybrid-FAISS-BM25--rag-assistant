[site]: crossvalidated
[post_id]: 263640
[parent_id]: 
[tags]: 
When does asymptotic normality of the Bayesian posterior (Bernstein-von Mises) fail?

Consider the posterior density function given (as usual) by $$ \pi(\theta) \prod_{i=1}^n f(x_i;\theta),$$ with prior density $\pi$ and distribution $f(\cdot;\theta)$ of the $n$ observations $x_1, \dots, x_n$, conditional on the parameter value $\theta$. Under certain conditions, the posterior distribution is asymptotically normal (a result which is known as the Bernstein-von Mises theorem, see e.g. v.d Vaart, Asymptotic Statistics , Section 10.2, for rigorous arguments, or Young & Smith, Essentials of Statistical Inference , Section 9.12, for an informal discussion.) Are there any (hopefully elementary) examples in which the Bayesian posterior is not asymptotically normal? In particular are there examples where $\pi$ and $f$ are continuously differentiable with respect to $\theta$? $\pi(\theta) > 0$ for all $\theta$? One example I noted in the literature is that where $X_1, \dots, X_n$ are independent Cauchy random variables with location parameter $\theta$. In this case, with positive probability there exist multiple local maxima of the likelihood function (See Young & Smith, Example 8.3). Perhaps this may present a problem in the B-vM theorem although I am not sure. Update: Sufficient conditions for BvM are (as stated in v.d Vaart, Section 10.2): data is obtained from distribution with fixed parameter $\theta_0$ experiment is `differentiable in quadratic mean' at $\theta_0$ with non-singular Fisher information matrix $I(\theta_0)$ the prior is absolutely continuous in a region around $\theta_0$ the model is continuous and identifiable there exists a test which separates $H_0 : \theta = \theta_0$ from $H_1 : \|\theta -\theta_0\| \geq \varepsilon$ for some $\varepsilon > 0$
