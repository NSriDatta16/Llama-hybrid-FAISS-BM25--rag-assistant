[site]: datascience
[post_id]: 107289
[parent_id]: 
[tags]: 
Awful predictions of RNN while MSE is very low

I have encountered a strange situation where the predictions of RNN are just awful despite the fact that NN has found a minimum of loss function at 0.002 for training and 0.0013-0.0015 for validation data. My data set consists of 12470 samples with 60 time steps each. I've used sklearn shuffling method for every prepared dataset (x_train, y_train etc) like this: x_train, y_train, x_valid, y_valid, x_test, y_test = shuffle(x_train, y_train, x_valid, y_valid, x_test, y_test, random_state=0 ) The model looks like this: n_neurons = 64 epochs = 150 lr = 0.0001 initializer = tf.keras.initializers.HeNormal() model = keras.models.Sequential([ keras.layers.SimpleRNN(n_neurons, activation='elu', return_sequences=True, input_shape=[None, 1], kernel_regularizer='l2', bias_regularizer='l2', kernel_initializer=initializer, bias_initializer=tf.keras.initializers.RandomNormal), keras.layers.Dropout(0.2), keras.layers.SimpleRNN(n_neurons, activation='tanh', return_sequences=True, kernel_regularizer='l2', bias_regularizer='l2', kernel_initializer=initializer, bias_initializer=tf.keras.initializers.RandomNormal), keras.layers.Dropout(0.2), keras.layers.SimpleRNN(n_neurons, activation='selu', return_sequences=True, kernel_regularizer='l2', bias_regularizer='l2', kernel_initializer=initializer, bias_initializer=tf.keras.initializers.RandomNormal), keras.layers.Dropout(0.2), keras.layers.SimpleRNN(n_neurons, kernel_regularizer='l2', bias_regularizer='l2', kernel_initializer=initializer, bias_initializer=tf.keras.initializers.RandomNormal), keras.layers.Dropout(0.2), keras.layers.Dense(1, activation='linear', kernel_regularizer='l2', bias_regularizer='l2', kernel_initializer=initializer, bias_initializer=tf.keras.initializers.RandomNormal) ]) np.random.randn(42) tf.random.set_seed(42) optimizer = keras.optimizers.Adam(learning_rate=lr) model.compile(loss='mse', optimizer=optimizer) history = model.fit(x_train, y_train, epochs=epochs, batch_size=64, validation_data=(x_valid, y_valid)) I am aware of the fact that He initialization is not optimal for tanh, I was just changing activation functions during testing of my model, to see which one will fit. Now I've tested learning rates from range (0.0001 to 0.0009) with steps 0.0001. But after x epochs models with these learning rates were stuck at 0.03 mse loss and than changed to NaN. Learning rate of 0.0001 appears to be the best (in this very moment I am testing lower learning rates). As I wrote above this learning rate appears to be the best due to two facts: has found a minimum the loss curve is ideal But as you see the prediction is awful, the NN only kind of get's the 'range' it should be in. More examples below: What is causing such a bad performance of the RNN? How could I fix this?
