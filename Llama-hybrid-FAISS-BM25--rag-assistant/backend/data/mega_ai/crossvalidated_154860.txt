[site]: crossvalidated
[post_id]: 154860
[parent_id]: 
[tags]: 
Convolutional neural networks: shared weights?

In some literature the convolution layers of convolutional neural networks have shared weights (e.g. see "shared weights" at deeplearning.net tutorial ) but I already read some papers where the weights were non-shared and rather calculated like in "normal MLPs". So, what is the advantage of shared weights for convolutional neural networks? Are there any pros and cons? Or is it just an architecture-based decision, like "we noticed, our network works better with shared weights"?
