[site]: crossvalidated
[post_id]: 274822
[parent_id]: 
[tags]: 
What information should be released to characterize a dataset for text classification?

I am releasing a dataset for text classification. ​What information would a researcher in natural language processing or machine learning may want to have about this dataset? Here some some information I thought about: Dataset itself: texts, and text ID (the ID makes it easier to refer to a specific text, e.g. it could be inferred from the filename if each text is in a separate file). ​labels, and for each label annotator ID + annotation timestamp​ Meta information + superficial descriptive statistics + baselines (ideally with the source code allowing to produce them when applicable): ​ number of texts number of ​different ​​labels​ explanation on how the dataset is stored (flat files, MySQL database, etc.) and, if needed, what code one has to run to create/download/prepare/etc. the dataset. where do the texts come from, and how they were chosen how large are the texts (e.g., boxplot of text length expressed in number of tokens, or histogram) how diverse the vocabulary is (e.g., simply stating the vocabulary size) how imbalanced the data set is, i.e. the percentage of positive samples, for each label cooccurrence matrix of labels (i.e., in how many texts both label X and label Y are present) and label correlations. instructions given to annotators (e.g., definition for each label) tool(s) used to annotate how the annotators​ were chosen​, what their backgrounds are (e.g., medical student specialized in orthopedics) how many annotations each of the annotators did. were annotation disagreements arbitrated, if so how? (e.g., a third annotator with more experience) time spend by annotator to annotate each text inter-rater agreement a.k.a. inter-annotator agreement ( Cohen's kappa​ if each text was labeled by exactly 2 annotators, otherwise Fleiss' kappa is there more than 2 annotators​​, or Krippendorff's alpha if the number of annotators​ is variable​ ) intra-rater agreement a.k.a. intra-annotator agreement (if the same annotator was asked to annotate the same text twice) ​what is the precision/recall​/F1-score of each annotator​​ (i.e., human performance metric against which one can directly compare NLP algorithms) ​what is the precision/recall​/F1-score of each annotator​​ (i.e., human performance metric against which one can directly compare NLP algorithms) whether annotation policies/quality have fluctuated over time are there any missing annotations? what is the classification performance of some baseline systems? what is the official training/validation/test split? (it is best to provide one so that different publications based on the same data set can be more easily compared against) Point of contact if willing to answer any questions.​ (e.g., raising issues on the GitHub repository)​ ​Checksum​ of ​the dataset (not really needed if we place it on GitHub or if we just release some code to construct the dataset from a larger dataset) Encoding​ of the texts ​ Language​(s)​ of the texts, and if there is more than one language, what the language distribution is. Example​s​ of ​the data​ (a.k.a. samples) License price ​DOI​ Name of the dataset What other information should I release to ​best present the dataset to researchers in natural language processing or machine learning? Example of the dataset for text classification: a set of patient notes annotated with which medical condition(s) the patient has, amongst 10 medical conditions. I.e., each sample has 10 binary labels, annotated by humans. I have crossposted the question at: https://redd.it/66mraj http://qr.ae/TwK247
