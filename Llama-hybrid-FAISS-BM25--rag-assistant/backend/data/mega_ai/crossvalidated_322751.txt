[site]: crossvalidated
[post_id]: 322751
[parent_id]: 308534
[tags]: 
I finally found the answer in Frank Harrell's book "Regression Modeling Strategies" [1] in Section 5.2.4 (Improvements on Data-Splitting: Resampling.). "The randomization method" is presented as an interesting method to estimate optimism through random permutations of the response, especially in combination with variable selection (as in the example in the OP). He refers, among others, to [2] for related ideas. The method is very simple: Let's say your complicated modelling strategy involves forward-/backward (and sideway) selection and your data set is too small to have a clean train/validation/test split. Furthermore, you might not fully trust cross-validation as it always mean to discard a certain proportion of the data within fold. How can you judge if your R-squared of 0.7 is valid or if it mostly a result of overfitting? The randomization method works as follows (here we talk about R-squared but it can be any performance measure of interest). If your strategy is unbiased, then you would expect the R-squared to be close to 0 if repeated on a data set with randomly permuted response variable. Let's say you get an average R-squared of 0.6 instead of 0 after 20 permutations. So you know that the original R-squared of 0.7 is probably not much more than the result of overfitting. A more honest estimate of the "true" R-squared would be 0.7-0.6 = 0.1 (small). So you have shown how badly your strategy overfits. Advantages of the method Very simple You always use the full data set Disadvantages include The estimate of optimism does not seem to be very accurate The method is not well known in contrast to cross-validation or bootstrap validation. [1] Frank Harrell, "Regression Modeling Strategies", 2001. Springer. [2] R. Tibshirani and K. Knight. The covariance inflation criterion for adaptive model selection. JRSS B, 61:529-546, 1999.
