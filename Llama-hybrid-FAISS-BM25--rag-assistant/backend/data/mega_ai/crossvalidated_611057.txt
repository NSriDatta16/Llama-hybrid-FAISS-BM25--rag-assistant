[site]: crossvalidated
[post_id]: 611057
[parent_id]: 
[tags]: 
Replacing the KL-divergence term in a VAE with parameter regularization

When training a VAE, one aim to optimize function $\mathcal{L}$ , defined as: $$\mathcal{L}\left(\theta,\phi; \mathbf{x}^{(i)}\right) = - D_{KL}\left(q_\phi(\mathbf{z}|\mathbf{x}^{(i)}) || p_\theta(\mathbf{z})\right) + \mathbb{E}_{q_\phi\left(\mathbf{z}|\mathbf{x}^{(i)}\right)}{\log p_\theta\left(\mathbf{x}^{(i)}| \mathbf{z}\right)},$$ where $q_\phi\left(\mathbf{z}|\mathbf{x}^{(i)}\right) = \mathcal{N}_J(\mathbf{\mu}^{(i)},\sigma^{(i)}\mathbb{1})$ and $p_\theta(\mathbf{z}) = \mathcal{N}_J(\mathbf{0},\mathbb{1})$ . The term $D_{KL}\left(q_\phi(\mathbf{z}|\mathbf{x}^{(i)}) || p_\theta(\mathbf{z})\right)$ may be viewed as a regularizer. Since normality is already imposed on $q_\phi\left(\mathbf{z}|\mathbf{x}^{(i)}\right)$ , then the KL-divergence term aims solely to turn $\mathbf{\mu}^{(i)}=0$ and $\mathbf{\sigma}^{(i)}=1$ . Thus, wouldn't it be equivalent (and simpler) to replace the $D_{KL}$ term by (something along the lines of) $\left(\mu^{(i)}\right)^2 + \left(\sigma^{(i)}-1\right)^2$ ? That is, to use the following function $\tilde{\mathcal{L}}$ instead: $$\tilde{\mathcal{L}}\left(\theta,\phi; \mathbf{x}^{(i)}\right) = -\sum_{j=1}^J\left(\left(\mu^{(i)}_j\right)^2 + \left(\sigma^{(i)}_j-1\right)^2\right) + \mathbb{E}_{q_\phi\left(\mathbf{z}|\mathbf{x}^{(i)}\right)}{\log p_\theta\left(\mathbf{x}^{(i)}| \mathbf{z}\right)}.$$
