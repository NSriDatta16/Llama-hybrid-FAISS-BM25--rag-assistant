[site]: crossvalidated
[post_id]: 309534
[parent_id]: 
[tags]: 
understanding lstm results

I'm following an introduction to LSTM Neural Nets from this repo - https://github.com/jaungiers/LSTM-Neural-Network-for-Time-Series-Prediction The author has a few ways to plot and test the predictions. The one that confuses me his method in LSTM.py called predict_sequences_multiple. The code reads like this: def predict_sequences_multiple(model, data, window_size, prediction_len): #Predict sequence of 50 steps before shifting prediction run forward by 50 steps prediction_seqs = [] for i in range(int(len(data)/prediction_len)): import pdb curr_frame = data[i*prediction_len] predicted = [] for j in range(prediction_len): predicted.append(model.predict(curr_frame[newaxis,:,:])[0,0]) curr_frame = curr_frame[1:] curr_frame = np.insert(curr_frame, [window_size-1], predicted[-1], axis=0) prediction_seqs.append(predicted) return prediction_seqs So essentially he looking at a sliding window of 50 ticks in the data, predicting next value in the sequence, sliding the window to the right by one and then including the predicted value as the newest item in the sliding window which he then again tests the model on. This way his predictions from the model are being fed into his test data. I'm pretty new to Data Sci and time series predictions but I don't understand why anyone would do this as you'd be feeding any error in your model back into it's self. Is there any specific reason one would analyse the results this way?
