[site]: crossvalidated
[post_id]: 273210
[parent_id]: 272569
[tags]: 
Generating synthetic data for neural-network has indeed become a common practice lately, especially in the field of computer vision. However, it's important to differentiate between the practice (creating synthetic images) and the purpose, which can differ tremendously. Let's consider the task of face recognition using a deep neural network. Then two example purposes ("use-cases", if you wish) for this case are: robustness : For example, you'd want the network to recognize Brad Pit if he's staring straight to the camera, but also if the image is flipped, or rotated 180 degrees. In order to "help" the network learn that a face is a face even though it's rotated, a common practice is to manually introduce rotated faces to the training set. This is what is known as "data augmentation", and it's very common since it's essentially free - all you need to do is apply a simple pre-processing stage. accuracy : In some scenarios, getting labeled data is incredibly hard or time consuming. For example, in order to train a network to perform face recognition well enough, an order of at least half a million labeled faces are required (Google and Facebook, which have by far the best products, use order of millions of images). Without this order of data, any classifier will pay in accuracy. Note that generating new labeled faces is a task far more difficult than just flipping your training images. However, there have been some very nice results in this field using CG tools. For example, in "Frankenstein: Learning Deep Face Representations using Small Data" the authors suggest a very nice pipeline for generating new face images, thus reaching STOA performance with as few as 10,000 training images. Now, let's move away from vision to the field you've asked about, NLP. Generally in text, the practice of generating synthetic data is much less evolved. One reason I can point to relates to my comment in the beginning regarding the "why" factor. For example, in document classification, most methods common today are unsupervised, thus they don't require labeled data; un-labled data is so abundant, that trying to synthetically generate data really doesn't make sense. Of course, I'm not saying this can't at all be useful for NLP. There are supervised algorithms which could potentially benefit from a stage of "pre-training" on some artificially large corpus before "fine-tuning" them on a smaller amount of real data. But again, this will really depend on your application and what you're trying to achieve. Simply "getting more data" isn't a goal in itself, not even in deep-learning :)
