[site]: crossvalidated
[post_id]: 526013
[parent_id]: 525980
[tags]: 
For gradient boosting I am assuming you mean gradient boosting a linear model. The issue with this is that you are 'regularizing' your coefficients. So essentially you shrink them. Causal interpretation of regularized coefficients from a traditional POV is pretty controversial for well defined regularization techniques like LASSO and Ridge and I have not heard of any efforts to do it with boosting. Most practitioners would just do Ridge from a bayesian perspective and I would assume the model would be fairly similar or better performing than the boosted one. With that said, it is possible to carry around the coefficients and standard errors and adjust them after each boosting round by simply adding the coefficients and updating your standard error. This will get you a p-value in the end but I doubt it is technically correct. Some old code I have actually does this in a time-series setting where I combine decomposition (splitting a time series into trend + seasonality + extra factors) with boosting and I provide a normal ols summary for the extra factors if given. See the 'Dealing with Exogenous Variables' section to see this: But like I said, if you want to use regularization I would just stick with Ridge and go from there.
