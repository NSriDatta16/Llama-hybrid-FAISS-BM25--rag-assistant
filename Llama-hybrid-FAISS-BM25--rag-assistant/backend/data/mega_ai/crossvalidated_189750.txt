[site]: crossvalidated
[post_id]: 189750
[parent_id]: 188903
[tags]: 
I am tempted here to give a purely intuitive answer to your question. Rephrasing what you say, the KL divergence is a way to measure to the distance between two distributions as you would compute the distance between two data sets in a Hilbert space, but some caution should be taken. Why? The KL divergence is not a distance as that you may use usually, such as for instance the $L_2$ norm. Indeed, it is positive and equal to zero if and only if the two distributions are equal (as in the axioms for defining a distance). But as mentioned, it is not symmetric. There are ways to circumvent this, but it makes sense for it to not be symmetric. Indeed, the KL divergence defines the distance between a model distribution $Q$ (that you actually know) and a theoretical one $P$ such that it makes sense to handle differently $KL(P, Q)$ (the "theoretical" distance of $P$ to $Q$ assuming the model $P$) and $KL(Q, P)$ (the "empirical" distance of $P$ to $Q$ assuming the data $Q$) as they mean quite different measures.
