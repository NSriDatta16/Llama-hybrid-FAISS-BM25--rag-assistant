[site]: crossvalidated
[post_id]: 72193
[parent_id]: 71929
[tags]: 
Based on your statement that "the samples $\hat \mu$ are also Gaussian", I think what you are saying is that samples from the posterior of $\mu$ appear normally distributed, and this inspires you to think about the bias (mean) and variance of these draws as if they were estimates of $\mu$. Certainly we do this kind of analysis of estimation algorithms in statistical signal processing. However, I think the mistake in your thinking happens when you treat individual draws from the posterior, what you call $\hat \mu$, as actual estimates of $\mu$, when in fact these draws, as a collective, define the posterior distribution itself (as much as draws from a distribution can be said to "define" it). Here's how I think you'd get 5 for a sample mean and 0.2 as a sample standard deviation (you said variance but I think you mean standard deviation). First, construct an explicit estimator for the mean $\mu$ given data $\mathbb x$, e.g., obtain 50'000 MCMC samples and average them. (Note how this estimator collapses the entire MCMC run into a single scalar estimate.) Then apply this estimator on repeated Monte Carlo draws of $\mathbb x$, i.e., generate a bunch of new data and each time apply this estimator. You should find the sample mean of these Monte Carlo estimates to be close to 5 and their sample deviation to be small. I would not expect any inferences indicating a mean of 5 and standard deviation of 0.2 by modeling the mean $\mu$ random variable itself as a draw from another parent distribution. As @fonnesbeck explains, the structure of your data can't inform this higher-level hierarchy. In summary, the draws you see from the posterior of $\hat \mu$ are different than the random variable $\mu$ itself.
