[site]: crossvalidated
[post_id]: 45862
[parent_id]: 
[tags]: 
Detect trend in time series

Hypothesis: time series has an inverted-U shape. How do we test this numerically? My idea is to take the first difference of the variable and fit a linear model using the differentiated variable as endogenous variable and the time variable as exogenous. $\Delta y_t = \beta_1 + \beta_2 t_t + \epsilon_t$ If the hypothesis is true, $\beta_2$ should be significantly less than zero. If we try this approach with computer generated data, it can be seen that it works well. Call: lm(formula = dy ~ dt) Residuals: Min 1Q Median 3Q Max -1.219e-15 -2.520e-16 -2.218e-17 1.827e-16 1.241e-15 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 1.210e-01 1.118e-16 1.082e+15 However, if a slight amount of noise is added to the data, this method falls apart catastrophically: Call: lm(formula = dy ~ dt) Residuals: Min 1Q Median 3Q Max -0.96480 -0.21802 0.00826 0.24701 0.93200 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 0.114305 0.087548 1.306 0.195 dt -0.001922 0.001758 -1.093 0.277 Residual standard error: 0.3907 on 82 degrees of freedom Multiple R-squared: 0.01437, Adjusted R-squared: 0.002345 F-statistic: 1.195 on 1 and 82 DF, p-value: 0.2775 So, what is the alternative? Edit R code to generate the series and the plots: t
