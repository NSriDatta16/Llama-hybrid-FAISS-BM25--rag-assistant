[site]: stackoverflow
[post_id]: 689865
[parent_id]: 689827
[tags]: 
As you say, you may have 100 steps but each of those steps will take a different amount of time depending on what they do. One approach would be to group tasks by what they're doing (deleting, changing registry values, downloading, copying files, etc) and for each group, assigning some key properties: Which monitorable metrics apply (copy speed, unpacking speed, etc)? What's an average worst-case rate for that process? Then you need to build a list of what you're going to be doing for the whole job, eg: Unpacking a 100meg file (group: unpacking, value:100) Copying out 120megs (group: copy, value:120) Setting registry values (group: registry, value:25) Clean up (group: deletion, value:100) So from that you can work out an overall "estimate" based on your preset average worst-case values but the key to accuracy is updating each metric multiplier as you learn how fast that system can do each task. It took Microsoft a decade to get it right so don't be too distressed if it doesn't work at first =)
