[site]: crossvalidated
[post_id]: 124890
[parent_id]: 124406
[tags]: 
In case you are interested in a linear model, here's direction that hasn't been mentioned in the comments / other answers: Let $x \in R^{30}$ be the feature vector, and you want to learn a linear mapping $y = w\cdot x$ that maps $x$ to a value (that can be transformed into a probability using logistic regression , from which you can do anything). I assume that there's a cost involved in obtaining many features / using all of them for prediction (otherwise, why not use all of them). What you can do is to regularize the learning process with $L_1$ norm. It is known that $L_1$ regularization results in sparser models (i.e. a weight vector $w$ with lots of zeros in it), which is what you want. Specifically, you would minimize the following: $$ \textrm{RegularizedLoss}(w,\lambda) = \textrm{Loss}(w) + \lambda \cdot \|w\|_1,$$ where $\textrm{Loss}(w)$ is the logistic loss. There are efficient ways to do this (for example, vowpal wabbit can handle $L_1$ regularization). The specific value of $\lambda$ controls the tradeoff between accuracy and sparsity, i.e. how much loss in accuracy are you willing to take in return for a model that uses less features. HTH.
