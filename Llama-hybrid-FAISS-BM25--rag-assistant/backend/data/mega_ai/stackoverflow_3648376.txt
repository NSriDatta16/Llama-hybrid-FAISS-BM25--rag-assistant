[site]: stackoverflow
[post_id]: 3648376
[parent_id]: 3647101
[tags]: 
View functions ( map and reduce ) are purely functional. Side-effects such as setting a global variable are not supported. (When you move your application to BigCouch , how could multiple independent servers with arbitrary subsets of the data know what pageIndex is?) Therefore the answer will have to involve a traditional map function, perhaps keyed by timestamp. function(doc) { if (doc.type == 'log') { emit(doc.timestamp, null); } } How can you get every 50th document? The simplest way is to add a skip=0 or skip=50 , or skip=100 parameter. However that is not ideal (see below). A way to pre-fetch the exact IDs of every 50th document is a _list function which only outputs every 50th row. (In practice you could use Mustache.JS or another template library to build HTML.) function() { var ddoc = this, pageIndex = 0, row; send("["); while(row = getRow()) { if(pageIndex % 50 == 0) { send(JSON.stringify(row)); } pageIndex += 1; } send("]"); } This will work for many situations, however it is not perfect. Here are some considerations I am thinking--not showstoppers necessarily, but it depends on your specific situation. There is a reason the pretty URLs are discouraged. What does it mean if I load page 1, then a bunch of documents are inserted within the first 50, and then I click to page 2? If the data is changing a lot, there is no perfect user experience, the user must somehow feel the data changing. The skip parameter and example _list function have the same problem: they do not scale. With skip you are still touching every row in the view starting from the beginning: finding it in the database file, reading it from disk, and then ignoring it, over and over, row by row, until you hit the skip value. For small values that's quite convenient but since you are grouping pages into sets of 50, I have to imagine that you will have thousands or more rows. That could make page views slow as the database is spinning its wheels most of the time. The _list example has a similar problem, however you front-load all the work, running through the entire view from start to finish, and (presumably) sending the relevant document IDs to the client so it can quickly jump around the pages. But with hundreds of thousands of documents (you call them "log" so I assume you will have a ton) that will be an extremely slow query which is not cached. In summary, for small data sets, you can get away with the page=1 , page=2 form however you will bump into problems as your data set gets big. With the release of BigCouch, CouchDB is even better for log storage and analysis so (if that is what you are doing) you will definitely want to consider how high to scale.
