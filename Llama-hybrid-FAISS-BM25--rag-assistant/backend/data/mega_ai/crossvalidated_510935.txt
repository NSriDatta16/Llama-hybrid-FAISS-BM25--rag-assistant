[site]: crossvalidated
[post_id]: 510935
[parent_id]: 510927
[tags]: 
In short, they're the same thing—up to a sign difference. Minimizing cross entropy is equivalent to minimizing the negative log-likelihood, which is equivalent to maximizing the likelihood. I mean this in the most complete of senses: the formulas for cross entropy and average negative log-likelihood are identical, except that one is divided by the number of examples. To avoid reinventing the wheel, I'll link to the Wikipedia article on cross-entropy . It outlines the equivalence formulaically. ~~The equation you've shown at the top, though, is slightly wrong—it's missing a $\log$ . It should be a double summation of log-probabilities.~~ As of Feb 24, 2021, the OP has fixed the formula.
