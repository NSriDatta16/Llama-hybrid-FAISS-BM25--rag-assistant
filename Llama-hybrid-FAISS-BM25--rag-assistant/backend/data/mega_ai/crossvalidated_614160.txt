[site]: crossvalidated
[post_id]: 614160
[parent_id]: 614159
[tags]: 
In a standard application of logistic regression, you have one weight (or coefficient) per input feature, plus one intercept for the whole model (in ML sometimes referred to as the "bias term"). So the number of weights = number of inputs + 1. Technically an intercept is not needed, but exclusion is rare and usually not a good idea. We can have more features if we include the option of parameter expansion. As a very simple example, if we want to include a quadratic feature for $x$ , then the input feature is just $x$ , but then we create another term $x^2$ which gets it's own weight, so in a sense we have two weights that came from one input feature. Other examples of parameter expansion include interaction effects (features $x, y$ expanded to $x, y, x \times y$ ) and splines (piecewise continuous polynomials).
