[site]: crossvalidated
[post_id]: 354320
[parent_id]: 
[tags]: 
Systematic negative bias in auto-correlations of random time-series

I generated completely random time-series (using MATLAB "rand" function or Python "numpy.random.rand" function) and computed the auto-correlation of these time-series using either MATLAB xcorr, or Python numpy.correlate or just the definition of correlation coefficient. Before, computing the correlation I subtracted the signal mean from it. For a random time-series we expect to have zero correlations; however, when I generate 1000 random arrays each with 10000 samples and compute the average auto-correlation for each of them, the distribution of average correlation of trials is always biased toward negative values. I tried this with both MATLAB and Python and even for more or longer trails but the negative bias is always there (although reduces a bit by increasing the system size). I was wondering if anyone knows where this bias is coming from and how I can remove it?
