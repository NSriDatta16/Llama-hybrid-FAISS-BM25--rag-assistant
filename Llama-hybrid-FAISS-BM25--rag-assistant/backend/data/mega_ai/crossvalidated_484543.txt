[site]: crossvalidated
[post_id]: 484543
[parent_id]: 484360
[tags]: 
You're asking a question about methods here, but I would rather start an answer from your data and what you want to know. Here's a version of your data that may be useful to people who don't routinely use R; the opening and closing lines are specifically for Stata, but users of other software should be able to adapt according to need. The periods are Stata's generic code for numeric missings and correspond to NA in R. clear input byte(id group baseline outcome coverage) 1 1 28 28 28 2 1 28 0 28 3 1 28 28 28 4 1 28 0 28 5 1 28 0 28 6 1 28 . 28 7 1 28 . 28 8 1 28 16 28 9 1 28 28 28 10 1 28 10 28 11 1 12 12 28 12 1 28 0 28 13 1 28 28 28 14 1 28 12 28 15 1 28 0 28 16 1 28 0 28 17 1 28 28 28 18 1 24 8 28 19 1 28 0 28 20 1 28 28 28 21 1 28 28 28 22 1 28 0 28 23 1 28 4 28 24 1 28 . 28 25 0 28 . 28 26 0 28 0 28 27 0 20 . 28 28 0 28 28 28 29 0 28 . 28 30 0 24 20 28 31 0 24 1 28 32 0 28 3 28 33 0 28 28 28 34 0 28 26 28 35 0 28 . 28 36 0 28 0 28 37 0 28 20 28 38 0 28 16 28 39 0 24 16 28 40 0 28 0 28 41 0 28 . 28 42 0 28 3 28 43 0 28 0 28 44 0 28 1 28 45 0 16 20 28 46 0 28 0 28 end The core of the problem is comparing mean outcome for two values of group . A distraction is that baseline varies and it seems to be simplest at least at the outset to ignore cases that are not 28 days for baseline . It is not obvious to me that adding baseline as a predictor is the best way to adjust for varying baseline ; an alternative is to scale outcome to fraction of baseline , but that is likely to be confusing too. Comparing means can naturally be re-cast as a regression problem. Here is a graph with the regression line superimposed for the regression of outcome on group for baseline 28 days. With this simplification, the line just connects the two group means. I am rotating your histograms and treating the data as what they are, data points in a regression problem comparing means. Stacking of identical outcomes is a graphical convention only and does not affect the regression results. Although you refer to "Gaussian regression" the ideal condition of Gaussian or normal errors is the least important aspect of linear regression. The recent text by Gelman and friends https://www.cambridge.org/core/books/regression-and-other-stories even advises against normal quantile plots of residuals as a waste of time. I wouldn't go that far, but it is a serious point of view. A glance at the graph and regression results points up a difference of 2.9 days; my lay guess is that a difference of that magnitude would be clinically or scientifically interesting, but the regression results show that the sample is too small to confirm it as significant at conventional levels. If you are worried that such an indication is over-dependent on the implicit assumption of normal errors, some bootstrapping of those regression results implies that a difference of zero is well inside just about any confidence interval for the difference of means. Retreat to Kruskal-Wallis seems to me a counsel of despair; why use 1950s technology when 1970s technology (bootstrap) is available and allows you to focus on the difference of means that is of prime interest? In general, it is a really good idea to be sensitive to whether your data are counted or measured; to think about their conditional distributions; and to note whether an outcome is necessarily bounded. In this particular case, these plain regression results imply that it hardly matters what you assume or is what is assumed or ideal for the methods used. The difference between means looks interesting but is not conventionally significant and that indication is robust to whatever you do by way of analysis. However, if I try to match your binomial regression, but focusing on baseline equal to 28, I find similarly that it is enough to flip the difference to conventionally significant. I didn't at first understand why there is such a big difference in indication. But we should worry about what is assumed about distributions. I note that binomials can't be U-shaped. I first doubted whether that was the issue, but such thinking was visceral, not logical. If you repeat the analysis with robust (Eicker-Huber-White) standard errors, then the significance evaporates. In short, in applying binomial regression rather than plain regression you're replacing a distributional assumption that doesn't bite -- even though it seems quite wrong -- with a distributional assumption that does bite! That's my diagnosis. FWIW, the use of days here as an integer count is partly natural (people have daily rhythms they follow, sometimes rigidly and sometimes loosely) and partly a convention (in principle the data might be based on times of day too, yielding fractional days). Finally, comparison of means is not the only game in town. I note that in group 0 just 2 out of 13 but in group 1 7 out of 19 people reported the full 28 days. Those differences naturally affected the means, but the detail may be important too. Nitty-gritty follows as Stata output. R people expect that we're smart enough to decode R output if we're perverse enough not to use it (not to use it routinely, in my case) and I return the compliment. The minimalism of R output is admirable, except that not showing the sample size used in even the default summary is puzzling to me. . set seed 2803 . quietly bootstrap diff=_b[1.group], reps(1000) : regress outcome i.group if baseline == 28 (running regress on estimation sample) Linear regression Number of obs = 32 Replications = 1,000 command: regress outcome i.group diff: _b[1.group] ------------------------------------------------------------------------------ | Observed Bootstrap Normal-based | Coef. Std. Err. z P>|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- diff | 2.910931 4.409327 0.66 0.509 -5.731191 11.55305 ------------------------------------------------------------------------------ . estat bootstrap, percentile normal bc Linear regression Number of obs = 32 Replications = 1000 command: regress outcome i.group diff: _b[1.group] ------------------------------------------------------------------------------ | Observed Bootstrap | Coef. Bias Std. Err. [95% Conf. Interval] -------------+---------------------------------------------------------------- diff | 2.9109312 .1026972 4.4093271 -5.731191 11.55305 (N) | -5.055556 11.84828 (P) | -5.582857 11.58442 (BC) ------------------------------------------------------------------------------ (N) normal confidence interval (P) percentile confidence interval (BC) bias-corrected confidence interval . glm outcome i.group baseline, f(binomial coverage) Iteration 0: log likelihood = -530.29406 Iteration 1: log likelihood = -516.62802 Iteration 2: log likelihood = -516.61552 Iteration 3: log likelihood = -516.61552 Generalized linear models Number of obs = 38 Optimization : ML Residual df = 35 Scale parameter = 1 Deviance = 980.8498432 (1/df) Deviance = 28.02428 Pearson = 748.2307194 (1/df) Pearson = 21.37802 Variance function: V(u) = u*(1-u/coverage) [Binomial] Link function : g(u) = ln(u/(coverage-u)) [Logit] AIC = 27.34819 Log likelihood = -516.615519 BIC = 853.5343 ------------------------------------------------------------------------------ | OIM outcome | Coef. Std. Err. z P>|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- 1.group | .2522059 .1263387 2.00 0.046 .0045866 .4998252 baseline | -.038664 .0188569 -2.05 0.040 -.0756228 -.0017053 _cons | .5471053 .5090758 1.07 0.283 -.4506649 1.544875 ------------------------------------------------------------------------------ . glm outcome i.group if baseline == 28, f(binomial coverage) Iteration 0: log likelihood = -485.92872 Iteration 1: log likelihood = -481.53913 Iteration 2: log likelihood = -481.53724 Iteration 3: log likelihood = -481.53724 Generalized linear models Number of obs = 32 Optimization : ML Residual df = 30 Scale parameter = 1 Deviance = 931.0323116 (1/df) Deviance = 31.03441 Pearson = 708.6313527 (1/df) Pearson = 23.62105 Variance function: V(u) = u*(1-u/coverage) [Binomial] Link function : g(u) = ln(u/(coverage-u)) [Logit] AIC = 30.22108 Log likelihood = -481.5372359 BIC = 827.0602 ------------------------------------------------------------------------------ | OIM outcome | Coef. Std. Err. z P>|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- 1.group | .4368407 .1406668 3.11 0.002 .1611389 .7125425 _cons | -.6481498 .1103816 -5.87 0.000 -.8644938 -.4318058 ------------------------------------------------------------------------------ . predict predicted (option mu assumed; predicted mean outcome) . tabdisp group, c(predicted) -------------------------------- group | predicted ----------+--------------------- 0 | 9.615385 1 | 12.52632 -------------------------------- . glm outcome i.group if baseline == 28, f(binomial coverage) robust Iteration 0: log pseudolikelihood = -485.92872 Iteration 1: log pseudolikelihood = -481.53913 Iteration 2: log pseudolikelihood = -481.53724 Iteration 3: log pseudolikelihood = -481.53724 Generalized linear models Number of obs = 32 Optimization : ML Residual df = 30 Scale parameter = 1 Deviance = 931.0323116 (1/df) Deviance = 31.03441 Pearson = 708.6313527 (1/df) Pearson = 23.62105 Variance function: V(u) = u*(1-u/coverage) [Binomial] Link function : g(u) = ln(u/(coverage-u)) [Logit] AIC = 30.22108 Log pseudolikelihood = -481.5372359 BIC = 827.0602 ------------------------------------------------------------------------------ | Robust outcome | Coef. Std. Err. z P>|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- 1.group | .4368407 .6659552 0.66 0.512 -.8684075 1.742089 _cons | -.6481498 .5129588 -1.26 0.206 -1.653531 .357231 ------------------------------------------------------------------------------
