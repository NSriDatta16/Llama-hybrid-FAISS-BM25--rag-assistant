[site]: datascience
[post_id]: 112687
[parent_id]: 112611
[tags]: 
During recent years, the most successful feature attribution method from has been the SHAP values. SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions One of the most famous implementations is Tree Explainer Tree SHAP is a fast and exact method to estimate SHAP values for tree models and ensembles of trees The paper originally appeared in Nature ( here in Arxiv): Explainable AI for Trees: From Local Explanations to Global Understanding. In case you want to extend and see more related literature you can have a look at the appendix "Methods 5 Previous Global Explanation Methods for Tree". They provide the previous state of the art of feature-relevance methods. The exact algorithm that answers your question is in the Method 10, it explains how TreeShap is computed.
