[site]: datascience
[post_id]: 39671
[parent_id]: 39647
[tags]: 
There is a lot of information that might be useful for a more precise answer in your case, but is hard to extract from your question description. I think I can understand the following: You are training to minimise a cost instead of maximise a reward (this is unusual in RL, but not a problem). Random agents score variably, anything from 120 to 400 average cost. After many training steps, your agent is scoring 6.1 average cost, measured over the most recent 100 time steps. In testing, the agent scores better, an average cost of 5.2. The usual way of testing a DQN agent (or any off-policy agent, of which Q Learning or DQN are one type) is to stop exploring actions. I am assuming you do this too. During training, your agent was not always taking the actions that it determined were best. In tests, you are interested to see what the agent thinks is "best" and how good that is. So it should be expected that test scores better than recent training. If you were training using $\epsilon$ -greedy action selection, then your scores would be consistent with $\epsilon \approx 0.1$ However, your measurements have limited accuracy in any case. It might be worth calculating your standard error on your test data, to get a sense of how accurate your measure really is. The easiest way is to run your test multiple times, create an array of scores and calculate the standard deviation, mean and standard error as usual using the test results as samples of a random variable. Could we say our DQN is OK? That depends entirely on the problem. It seems that it does far better than acting randomly, which shows at least it has learned something. In order to get a sense of how well it has done, you need to compare it with something meaningful related to the problem: For simple problems, such as toy problems used to prove DQN works, it is possible to calculate an optimal answer and see how close the DQN can get. For problems that humans can attempt too, you can compare the performance of an agent to a human. Ideally a human who is expert at the task. For problems for which there are some publications, you can compare with state-of-the-art. This is true for many Atari 2600 games for instance. For problems which have a practical purpose, you can compare with a goal, such as would the agent be good enough to make money or reduce loss compared to current practice. For some problems you could compare to a simple rules-based agent, coded to perform the specific task. The more time steps should be more stable?? What is the effect of time steps? Up to a limit, the more time and samples that a DQN trains on, the better its performance will be. The best thing is to plot this - number of timesteps vs average cost. You may see sudden increases or even "catastrophic forgetting" (where the agent does no better than random, or even worse) at some times. But if the agent has trained successfully there should be a general improvement over time that reaches a limit as the DQN approaches the maximum capability of the agent.
