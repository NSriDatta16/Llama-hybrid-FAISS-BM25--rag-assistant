[site]: crossvalidated
[post_id]: 624335
[parent_id]: 623556
[tags]: 
My intuition is that this is not going to be a good idea, however at the moment I can only offer a possible example where it will give you an obviously wrong answer. Say the targets are independent of the input attributes, and there are an equal number of "A", "B", "neither" and "both" then the conditional mean everywhere (which is what the model predicts) will be a constant (1/4, 1/2, 1/2) which is a value the output can't represent because of the softmax activation function (using horrible neural network terminology ;o) as it sums to something greater than one. I think in this situation we would like the output first output to be half the value of the other two (as "neither" happens 1/4 of the time, but the outputs for "A" is lit half the time, either when it is an "A" or because it is a "both", and likewise for the "B" output). I think the closest you could get would be (1/5, 2/5, 2/5) for it to sum to one. That gives the probability of "neither" as being 1/5 when it should be 1/4. So I'd say it isn't robust because the "sum to one" constraint is not appropriate for this method of encoding the classes. Caveat lector: I've only tried this on my addled brain, not a computer, so I could be writing nonsense again.
