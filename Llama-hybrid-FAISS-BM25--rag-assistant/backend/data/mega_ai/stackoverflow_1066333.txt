[site]: stackoverflow
[post_id]: 1066333
[parent_id]: 1066264
[tags]: 
Depending on what exactly you need to do with the data prior to inserting it your best options in terms of speed are: Parse the file in java / do what you need with the data / write the "massaged" data out to a new CSV file / use "load data infile" on that. If your data manipulation is conditional (e.g. you need to check for record existence and do different things based on whether it's an insert or and update, etc...) then (1) may be impossible. In which case you're best off doing batch inserts / updates. Experiment to find the best batch size working for you (starting with about 500-1000 should be ok). Depending on the storage engine you're using for your table, you may need to split this into multiple transactions as well - having a single one span 1.8M rows ain't going to do wonders for performance.
