[site]: crossvalidated
[post_id]: 595793
[parent_id]: 
[tags]: 
Is data random in Bayesian inference?

I am trying to understand the Bayesian method. From Wasserman, my understanding is that the process for parametric inference is roughly: Choose a statistical model $\mathcal{F} = \{F_\Theta\}$ . Construct prior distribution $f(\theta)$ for $\Theta$ giving our beleif for how likely $\Theta$ is to take certain values. Observe data $X_1, \ldots, X_n$ . Compute the posterior $f(\theta|X_1, \ldots, X_n)$ . Here I do not make any assumptions about where my data came from. So there is no randomness in my data. To me this feels like using Bayes theorem to derive our rule for updating the posterior is on shaky ground, since the data is never random. It also means I cannot evaluate how well this works. Is there an assumption that $X_1, \ldots, X_n \sim F_\Theta$ where $\Theta$ is distributed according to my prior? On the other hand, I could Choose a statistical model $\mathcal{F} = \{F_\Theta\}$ . Construct prior distribution $f(\theta)$ for $\Theta$ giving our beleif for how likely $\Theta$ is to take certain values. Observe data $X_1, \ldots, X_n\sim F_{\theta}$ for some fixed unknown $\theta$ . Compute the posterior $f(\theta|X_1, \ldots, X_n)$ . Now the data are random variables, and I could analyze how well this approach performs over repeated samples. In particular, I can understand how my prior impacts the quality of the posterior. I could even go one step further than instead of having $\theta$ fixed, have it come from some fixed unknown prior distribution (different than my prior). Wasserman says "In general, Bayesian methods provide no guarantees on long run performance.", so I'm confused about whether this approach is still the "Bayesian method" (any keywords for further reading would be nice)? It seems to inherent a number assumptions that Wasserman characterizes as frequentest assumptions (i.e. that parameters are fixed unknown values).
