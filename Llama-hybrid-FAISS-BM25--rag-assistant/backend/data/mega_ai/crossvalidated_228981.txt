[site]: crossvalidated
[post_id]: 228981
[parent_id]: 
[tags]: 
Time Series Forecast - Evaluating accuracy

I am very new to R and the forecast package authored by Rob Hyndman. I am working on a time series with 24 samples per hour. I trained a random forest regressor to forecast 6 hour ahead values and am using MAPE(Mean Absolute Percentage Error) on a held out duration as the accuracy metric. I want to compare its accuracy with standard time series methods like ARMA and ARIMA models. Time Series Sample Here is what I have currently. data.csv has 576*16(16 days worth) samples and I wish to measure forecast accuracy on last 3 days. library(forecast) pv_data = read.csv("data.csv", header=FALSE) pv_ts This gives me MAPE which is average of h = 1 to 576*3 samples ahead point forecast absolute errors. Question: How to find the average of h=144 ahead forecast absolute percent error of the estimates of samples in test_ts ? Specifically, how to calculate $ \frac {\sum\limits_{T=576*13-h}^{576*16-h} \left\lvert \tfrac{\hat{e}_{T+h}}{y_{T+h}}\right\rvert }{576*3}$ with h=144 where $\hat{e}_{T+h}=\hat{y}_{T+h | T}-y_{T+h}$?
