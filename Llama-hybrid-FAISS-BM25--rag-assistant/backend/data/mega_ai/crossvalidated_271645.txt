[site]: crossvalidated
[post_id]: 271645
[parent_id]: 271522
[tags]: 
Similar to Auto-encoders, the objective of a Variational Auto-encoder is to reconstruct the input. The only difference is that AEs have direct links between encoder and decoder parts, but VAEs have a sampling layer which samples form a distribution (usually a Gaussian) and then feeds the generated samples to the decoder part. Here are some examples from different auto encoders as generative models. You can easily see how the networks are able to capture the data distribution and generate samples very similar to the original ones by only using random observations as an input. On the top, there's the random input and on the bottom, there's the reconstructed image. The models are trained on MNIST. If you have a look at this paper , you will find the answer to your question:
