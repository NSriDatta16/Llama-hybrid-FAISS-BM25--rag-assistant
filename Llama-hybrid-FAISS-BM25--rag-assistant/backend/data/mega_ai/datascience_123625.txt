[site]: datascience
[post_id]: 123625
[parent_id]: 
[tags]: 
XGBRegressor underestimates sum of regression target (insurance application)

I'm trying to learn how to apply Boostig Algorithms (e.g. XGBoost) to insurance applications (premium calculation). As a starting point I used this tutorial from the scikit-learn website. tldr: The authors use a data set from openml to train two types of models. A Tweedie Regression to directly calculate the so called PurePremium and a Frequency Severity model to separately calculate the claim frequency (Poisson Regression) and claim severity (Gamma Regression). With the product frequency*severity = PurePremium. They evaluate the models on different aspects: performance metrics (mean squared error, mean absolute error, mean tweedie deviances with various power parameters) observed/predicted total claim amount ranking performance from lowest to highest expected claim amount I downloaded the notebook and added XGBoost regression with a tweedie objective. from xgboost import XGBRegressor # XGBoost can handle non linear relationships between features and target. # Furthermore it can handle categorical features, therefore we will use # the untransformed features. df_train_xgb = df_train.copy() df_test_xgb = df_test.copy() categorical_features = ["VehBrand", "VehPower", "VehGas", "Region", "Area"] numeric_features = ["BonusMalus", "VehAge", "DrivAge", "Density"] df_train_xgb[categorical_features] = df_train_xgb[categorical_features].astype( "category" ) df_test_xgb[categorical_features] = df_test_xgb[categorical_features].astype("category") xgb_pure_premium = XGBRegressor( objective="reg:tweedie", tweedie_variance_power=1.9, enable_categorical=True, tree_method="gpu_hist", max_cat_to_onehot=10, random_state=42, ) xgb_pure_premium.fit( df_train_xgb[categorical_features + numeric_features], df_train_xgb["PurePremium"], sample_weight=df_train["Exposure"], ) The default parameters didn't perform well on any of the above mentioned aspects, therefore I did some hyperparameter optimization on the train data (preprocessing is obtained from the original notebook). from functools import partial, update_wrapper import logging import sys import time import numpy as np import optuna from optuna.samplers import TPESampler from sklearn.datasets import fetch_openml from sklearn.metrics import mean_tweedie_deviance from sklearn.model_selection import KFold, train_test_split from xgboost import XGBRegressor # freMTPL2freq dataset from https://www.openml.org/d/41214 df_freq = fetch_openml(data_id=41214, as_frame=True, parser="pandas").data df_freq["IDpol"] = df_freq["IDpol"].astype(int) df_freq.set_index("IDpol", inplace=True) # freMTPL2sev dataset from https://www.openml.org/d/41215 df_sev = fetch_openml(data_id=41215, as_frame=True, parser="pandas").data # sum ClaimAmount over identical IDs df_sev = df_sev.groupby("IDpol").sum() df = df_freq.join(df_sev, how="left") df["ClaimAmount"].fillna(0, inplace=True) # unquote string fields for column_name in df.columns[df.dtypes.values == object]: df[column_name] = df[column_name].str.strip("'") # Note: filter out claims with zero amount, as the severity model # requires strictly positive target values. df.loc[(df["ClaimAmount"] == 0) & (df["ClaimNb"] >= 1), "ClaimNb"] = 0 # Correct for unreasonable observations (that might be data error) # and a few exceptionally large claim amounts df["ClaimNb"] = df["ClaimNb"].clip(upper=4) df["Exposure"] = df["Exposure"].clip(upper=1) df["ClaimAmount"] = df["ClaimAmount"].clip(upper=200000) # XGBoost can handle non linear relationships between features and target. # Furthermore it can handle categorical features, therefore we will use # the untransformed features. categorical_features = ["VehBrand", "VehPower", "VehGas", "Region", "Area"] numeric_features = ["BonusMalus", "VehAge", "DrivAge", "Density"] df[categorical_features] = df[categorical_features].astype("category") # Insurances companies are interested in modeling the Pure Premium, that is # the expected total claim amount per unit of exposure for each policyholder # in their portfolio: df["PurePremium"] = df["ClaimAmount"] / df["Exposure"] # This can be indirectly approximated by a 2-step modeling: the product of the # Frequency times the average claim amount per claim: df["Frequency"] = df["ClaimNb"] / df["Exposure"] df["AvgClaimAmount"] = df["ClaimAmount"] / np.fmax(df["ClaimNb"], 1) df_train, df_test = train_test_split(df, random_state=0) X_train = df_train[categorical_features + numeric_features] X_test = df_test[categorical_features + numeric_features] kf = KFold(n_splits=5, shuffle=True, random_state=0) def objective(trial): # define fixed parameters and search space trial.set_user_attr("objective", "reg:tweedie") trial.set_user_attr("tweedie_variance_power", 1.9) trial.set_user_attr("enable_categorical", True) trial.set_user_attr("tree_method", "gpu_hist") trial.set_user_attr("max_cat_to_onehot", 10) trial.set_user_attr("random_state", 42) trial.set_user_attr("early_stopping_rounds", 20) params = { "lambda": trial.suggest_float("lambda", 1e-3, 10.0, log=True), "alpha": trial.suggest_float("alpha", 1e-3, 10.0, log=True), "colsample_bytree": trial.suggest_float("colsample_bytree", 0.3, 1.0), "subsample": trial.suggest_float("subsample", 0.4, 1.0), "eta": trial.suggest_float("learning_rate", 0.001, 0.1, log=True), "n_estimators": trial.suggest_int("n_estimators", 100, 1000), "max_depth": trial.suggest_categorical( "max_depth", [3, 5, 7, 9, 11, 13, 15, 17, 20] ), "min_child_weight": trial.suggest_int("min_child_weight", 1, 300), } # callbacks=[XGBoostPruningCallback(trial, "validation_0-tweedie-nloglik@1.01")] xgb = XGBRegressor() # set fixed parameters and trial parameters xgb.set_params( **{ "objective": trial.user_attrs["objective"], "tweedie_variance_power": trial.user_attrs["tweedie_variance_power"], "random_state": trial.user_attrs["random_state"], "enable_categorical": trial.user_attrs["enable_categorical"], "tree_method": trial.user_attrs["tree_method"], "max_cat_to_onehot": trial.user_attrs["max_cat_to_onehot"], "early_stopping_rounds": trial.user_attrs["early_stopping_rounds"], } ) xgb.set_params(**params) # evaluate model test_scores = [] for train_index, test_index in kf.split(X_train): X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index] y_train_fold, y_test_fold = ( df_train["PurePremium"].values[train_index], df_train["PurePremium"].values[test_index], ) weights_train_fold, weights_test_fold = ( df_train["Exposure"].values[train_index], df_train["Exposure"].values[test_index], ) eval_metric = update_wrapper( partial( mean_tweedie_deviance, power=1.9, sample_weight=weights_test_fold, ), mean_tweedie_deviance, ) xgb.set_params(eval_metric=eval_metric) xgb.fit( X_train_fold, y_train_fold, sample_weight=weights_train_fold, eval_set=[(X_test_fold, y_test_fold)], # eval_metric=eval_metric, verbose=False, ) y_pred_test = xgb.predict(X_test_fold) test_scores.append( mean_tweedie_deviance( y_test_fold, y_pred_test, sample_weight=weights_test_fold, power=1.9 ) ) return np.mean(test_scores) def model(db, end_time): """ Parameters ---------- data_db : sqlalchemy.engine.Engine Database engine with the data used for training. """ optuna.logging.get_logger("optuna").addHandler(logging.StreamHandler(sys.stdout)) study_name = "sklearn_xgboost_20230908" # Unique identifier of the study. storage_name = db sampler = TPESampler() study = optuna.create_study( study_name=study_name, storage=storage_name, load_if_exists=True, sampler=sampler, ) while time.time() I applied the "best" set of parameters to the XGBRegressor and rerun the notebook {'alpha': 0.8018296364250094, 'colsample_bytree': 0.8814562072113403, 'lambda': 0.08141446031734288, 'learning_rate': 0.061811999878165116, 'max_depth': 3, 'min_child_weight': 289, 'n_estimators': 538, 'subsample': 0.8432648125897754} I obtained the following results for the three aspects. Performance metrics: Total Claim Amount: Ranking Performance: The Boosting model does well in Performance Metrics (slightly overfitting on the train data) and Ranking Performance, but completely underestimates the Total Claim Amount. I am trying to understand why that is happening (and how it can be prevented), but so far I couldn't find any research with similar problems. Can anyone enlighten me?
