[site]: crossvalidated
[post_id]: 376861
[parent_id]: 
[tags]: 
Dealing with excessive number of zeros

ipdb> np.count_nonzero(test==0) / len(ytrue) * 100 76.44815766923736 I have a datafile counting 24000 prices where I use them for a time series forecasting problem. Instead of trying predicting the price, I tried to predict log-return, i.e. $\log(\frac{P_t}{P_{t-1}})$ . I have applied the log-return over the prices as well as all the features. The prediction are not bad, but the trend tend to predict zero. As you can see above, ~76% of the data are zeros. Now the idea is probably to "look up for a zero-inflated estimator : first predict whether it's gonna be a zero; if not, predict the value". In details, what is the perfect way to deal with excessive number of zeros? How zero-inflated estimator can help me with that? Be aware originally I am not probabilist. P.S. I am working trying to predict the log-return where the units are "seconds" for High-Frequency Trading study. Be aware that it is a regression problem (not a classification problem). Update That picture is probably the best prediction I have on the log-return, i.e $\log(\frac{P_t}{P_{t-1}})$ . Although it is not bad, the remaining predictions tend to predict zero. As you can see in the above question, there is too many zeros. I have probably the same problem inside the features as I take the log-return on the features as well, i.e. if $F$ is a particular feature, then I apply $\log(\frac{F_t}{F_{t-1}})$ . Here is a one day data, log_return_with_features.pkl , with shape (23369, 30, 161) . Sorry, but I cannot tell what are the features. As I apply $\log(\frac{F_t}{F_{t-1}})$ on all the features and on the target (i.e. the price), then be aware I added 1e-8 to all the features before applying the log-return operation to avoid division by 0.
