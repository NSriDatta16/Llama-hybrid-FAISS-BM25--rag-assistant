[site]: datascience
[post_id]: 121569
[parent_id]: 
[tags]: 
MobileNet validation loss not decreasing over time

I am trying to train a MobileNetV2 on a custom dataset, to image Classification task. Cardinality is 864 images, split in 70%/20%/10%, balanced between the 3 different classes. Weights are pre-loaded from imagenet, I froze the net and I added to the bottom of the net a GlobalAveragePooling, a Dropout (with 50% drop probability), and a Dense layer with 3 classes and softmax as activation function, since i want the output layer to give me an output like (1,0,0) if the inference image is from the first class, and so on. image size: 96x96 (I normalized, too) batch_size: 32 Learning rate: 0.001 trainable params: 3843 optimizer: sgd ('adam' doesn't improve my accuracy) loss: categorical cross entropy metrics: accuracy Training for 20 epochs gives me these results: After that I decided to try some fine-tuning, by freezing only the first 100 layers of the net. Trained again for 10 epochs, that's what I get: My net is overfitting, but I don't know why it's happening and what am I expected to do in order to improve my accuracy. Edit: I also tried increasing dataset images with some source images or even with some data augmentation, up to more than 3K images, but it didn't work out at any rate.
