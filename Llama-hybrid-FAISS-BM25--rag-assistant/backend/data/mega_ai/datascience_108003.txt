[site]: datascience
[post_id]: 108003
[parent_id]: 93124
[tags]: 
It seems to me that you simply want to use a fixed validation set for hyperparameter tuning. This is a very cardinal procedure, especially in the scenario as you mentioned, that one wants to ensure that the results are comparable across models. What I would try, if there aren't a whole lot of hyperparameters, is to manually set grids for those parameters and perform brute-force grid search. Sklearn has a nice feature sklearn.model_selection.ParameterGrid that helps you iterate all possible combinations of hyperparmaters. If there are indeed a lot of hyperparameters, or the values should be sampled from a continuous distribution, then I would suggest using hyperparameter techniques practiced in deep learning field. Essentially, try using Weights & Biases, which is very versatile yet easy to use. See this SO question ( https://stackoverflow.com/questions/54126811/order-between-using-validation-training-and-test-sets ) also if you want some clarifications on hyperparam tuning strategies.
