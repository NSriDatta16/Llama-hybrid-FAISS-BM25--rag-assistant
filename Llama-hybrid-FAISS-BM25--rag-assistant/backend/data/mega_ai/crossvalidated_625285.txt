[site]: crossvalidated
[post_id]: 625285
[parent_id]: 625203
[tags]: 
Given that the opponent is operating continuously, it sounds like you want an iterative procedure (e.g., you want to maximize the probability of class 1 over time , as opposed to just after performing a single perturbation). Without further assumptions (e.g., treating the classification model as a black box), this looks like a typical reinforcement learning problem. Here: The action space $\mathcal{A}$ contains the set of possible perturbations you can apply to $x$ (and perhaps a "do nothing" action). The state space $\mathcal{S}$ is the set of possible values of $x$ . The reward is the probability that $x$ belongs to class 1 (as estimated by the classification model). At timestep $t$ , you select an action from $\mathcal{A}$ . By timestep $t+1$ , the opponent may have further perturbed $x$ , and you can evaluate the classification model here to estimate the reward. Then repeat for $t = 0,1,...$ In general, this is quite a difficult problem, depending on the sizes of $\mathcal{A}$ and $\mathcal{S}$ , how skilled/influential the opponent is, the complexity/structure of the reward function, how much data collection ("exploration") you can perform before you need the algorithm to perform well, etc. There are lots of algorithms and approaches to reinforcement learning. Some starting points might be: Reinforcement Learning: An Introduction by Barto and Sutton Reinforcement Learning and Optimal Control by Bertsekas (with video lectures) Hope that helps.
