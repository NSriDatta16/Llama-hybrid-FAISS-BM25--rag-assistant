[site]: crossvalidated
[post_id]: 291787
[parent_id]: 
[tags]: 
Backward propagation without a loop over training examples

At this point in a Coursera Machine Learning lecture on implementing neural networks, Andrew Ng says: To implement back-prop, usually we will do that with a for -loop over the training examples. Some of you may have heard of frankly very advanced vectorization methods where you don't have a for -loop over the $m$ training examples The language used in the course is Octave / MATLAB. How does one implement vectorised back-prop without a loop over the training examples?
