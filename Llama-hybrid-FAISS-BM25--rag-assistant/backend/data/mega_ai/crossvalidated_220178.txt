[site]: crossvalidated
[post_id]: 220178
[parent_id]: 
[tags]: 
Gradient descent vs Contrastive Divergence

What are the differences (if any) between gradient descent and Contrastive Divergence? I understand how gradient descent is used to train neural networks via back-propogation, but I've just started learning about Restricted Boltzmann Machines (RBMs) and I keep coming across references to Contrastive Divergence, a concept that I'm not familiar with. This reference here ( http://blog.echen.me/2011/07/18/introduction-to-restricted-boltzmann-machines/ ) suggests that Contrastive Divergence is like an "approximate gradient descent", but I'm not sure what that means.
