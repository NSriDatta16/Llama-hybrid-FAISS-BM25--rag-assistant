[site]: stackoverflow
[post_id]: 1553486
[parent_id]: 1460294
[tags]: 
One last try… It is clear that there are too many locks hold for too long. Once the system starts slowing down due to too many locks there is no point in starting more transactions. Therefore you should benchmark the system to find out the optimal number of currant transaction , then use some queue system (or otherwise) to limit the number of currant transaction. Sql Server may have some setting (number of active connections etc) to help, otherwise you will have to write this in your application code. Oracle is good at allowing reads to bypass writes , however SqlServer is not as standared... Therefore I would split the stored proc to use two transactions, the first transaction should just: be a SNAPSHOT (or READ UNCOMMITTED) transaction find the “Id” of the rows for the seats you wish to sell. You should then commit (or abort) this transaction, and use a 2nd (hopefully very short) transaction that Most likcly is READ COMMITTED, (or maybe SERIALIZABLE) Selects each row for update (use a locking hint ) Check it has not been sold in the mean time (abort and start again if it has) Set the “IsSold” flag on the row (You may be able to the above in a single update statement using “in”, and then check that the expected number of rows were updated) Sorry sometimes you do need to understant what each time of transaction does and how locking works in detail. If the table is smaller, then the update is shorter and the locks are hold for less time. Therefore consider splitting the table: so you have a table that JUST contains “AllocationId” and “IsSold”. This table could be stored as a single btree (index organized table on AllocationId) As all the other indexes will be on the table that contrains the details of the seat, no indexes should be locked by the update.
