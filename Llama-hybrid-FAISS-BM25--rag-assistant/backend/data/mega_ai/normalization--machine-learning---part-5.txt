. For example, in a DiT, the conditioning information (such as a text encoding vector) is processed by a multilayer perceptron into γ , β {\displaystyle \gamma ,\beta } , which is then applied in the LayerNorm module of a transformer. Weight normalization Weight normalization (WeightNorm) is a technique inspired by BatchNorm that normalizes weight matrices in a neural network, rather than its activations. One example is spectral normalization, which divides weight matrices by their spectral norm. The spectral normalization is used in generative adversarial networks (GANs) such as the Wasserstein GAN. The spectral radius can be efficiently computed by the following algorithm: INPUT matrix W {\displaystyle W} and initial guess x {\displaystyle x} Iterate x ↦ 1 ‖ W x ‖ 2 W x {\displaystyle x\mapsto {\frac {1}{\|Wx\|_{2}}}Wx} to convergence x ∗ {\displaystyle x^{*}} . This is the eigenvector of W {\displaystyle W} with eigenvalue ‖ W ‖ s {\displaystyle \|W\|_{s}} . RETURN x ∗ , ‖ W x ∗ ‖ 2 {\displaystyle x^{*},\|Wx^{*}\|_{2}} By reassigning W i ← W i ‖ W i ‖ s {\displaystyle W_{i}\leftarrow {\frac {W_{i}}{\|W_{i}\|_{s}}}} after each update of the discriminator, we can upper-bound ‖ W i ‖ s ≤ 1 {\displaystyle \|W_{i}\|_{s}\leq 1} , and thus upper-bound ‖ D ‖ L {\displaystyle \|D\|_{L}} . The algorithm can be further accelerated by memoization: at step t {\displaystyle t} , store x i ∗ ( t ) {\displaystyle x_{i}^{*}(t)} . Then, at step t + 1 {\displaystyle t+1} , use x i ∗ ( t ) {\displaystyle x_{i}^{*}(t)} as the initial guess for the algorithm. Since W i ( t + 1 ) {\displaystyle W_{i}(t+1)} is very close to W i ( t ) {\displaystyle W_{i}(t)} , so is x i ∗ ( t ) {\displaystyle x_{i}^{*}(t)} to x i ∗ ( t + 1 ) {\displaystyle x_{i}^{*}(t+1)} , thus allowing rapid convergence. CNN-specific normalization There are some activation normalization techniques that are only used for CNNs. Response normalization Local response normalization was used in AlexNet. It was applied in a convolutional layer, just after a nonlinear activation function. It was defined by: b x , y i = a x , y i ( k + α ∑ j = max ( 0 , i − n / 2 ) min ( N − 1 , i + n / 2 ) ( a x , y j ) 2 ) β {\displaystyle b_{x,y}^{i}={\frac {a_{x,y}^{i}}{\left(k+\alpha \sum _{j=\max(0,i-n/2)}^{\min(N-1,i+n/2)}\left(a_{x,y}^{j}\right)^{2}\right)^{\beta }}}} where a x , y i {\displaystyle a_{x,y}^{i}} is the activation of the neuron at location ( x , y ) {\displaystyle (x,y)} and channel i {\displaystyle i} . I.e., each pixel in a channel is suppressed by the activations of the same pixel in its adjacent channels. k , n , α , β {\displaystyle k,n,\alpha ,\beta } are hyperparameters picked by using a validation set. It was a variant of the earlier local contrast normalization. b x , y i = a x , y i ( k + α ∑ j = max ( 0 , i − n / 2 ) min ( N − 1 , i + n / 2 ) ( a x , y j − a ¯ x , y j ) 2 ) β {\displaystyle b_{x,y}^{i}={\frac {a_{x,y}^{i}}{\left(k+\alpha \sum _{j=\max(0,i-n/2)}^{\min(N-1,i+n/2)}\left(a_{x,y}^{j}-{\bar {a}}_{x,y}^{j}\right)^{2}\right)^{\beta }}}} where a ¯ x , y j {\displaystyle {\bar {a}}_{x,y}^{j}} is the average activation in a small window centered on location ( x , y ) {\displaystyle (x,y)} and channel i {\displaystyle i} . The hyperparameters k , n , α , β {\displaystyle k,n,\alpha ,\beta } , and the size of the small window, are picked by using a validation set. Similar methods were called divisive normalization, as they divide activations by a number depending on the activations. They were originally inspired by biology, where it was used to explain nonlinear responses of cortical neurons and nonlinear masking in visual perception. Both kinds of local normalization were obviated by batch normalization, which is a more global form of normalization. Response normalization reappeared in ConvNeXT-2 as global response normalization. Group normalization Group normalization (GroupNorm) is a technique also solely used for CNNs. It can be understood as the LayerNorm for CN