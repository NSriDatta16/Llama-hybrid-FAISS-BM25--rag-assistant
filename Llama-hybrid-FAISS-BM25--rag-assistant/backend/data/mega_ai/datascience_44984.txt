[site]: datascience
[post_id]: 44984
[parent_id]: 44980
[tags]: 
You could try reducing number of features by assessing their importance. This can be achieved by training XGBoost on the data and then analyzing how each feature split improved gini score. More on this and the code you can find: here . You could get the sense of how many features are enough for the prediction for a small information loss. Another idea is to perform PCA (Principal component analysis) to extract non-correlated features explaining a certain percentage of the target variable variance. You could then run any classification algorithm.
