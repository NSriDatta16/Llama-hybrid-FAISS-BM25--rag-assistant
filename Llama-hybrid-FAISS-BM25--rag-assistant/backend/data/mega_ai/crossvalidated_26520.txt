[site]: crossvalidated
[post_id]: 26520
[parent_id]: 
[tags]: 
Adjusting the classification threshold of Naive Bayes

I've been involved in a machine learning project recently and am now in the process of writing the project up for a paper submission. We used the naive bayes classifier on the project and developed a method for adjusting the classification of datapoints depending on how exactly we want the classifier to perform, ie. increase or decrease recall on one class in a binary classification problem. This is similar to adjusting the classification threshold but is sensitive the feature counts unlike just adjusting the classification threshold. My question is if anyone knows of any previous reserach on methods relating to adjusting the decision boundary in any other way than just changing the classification threshold. update (as per steffen's question) thanks steffen for your reply 1) by feature counts I mean feature counts, as in the counts of the features (or tokens/words if you will) in the unseen document. I don't mean the number of articles seen, but the number of features (words) in those articles. 2) I am aware that adjusting the threshold is a common way to tweak the classifier to some particular case, what I would like references to research that mention some other way of achieving this tweaking. By adjusting the classification threshold I mean that instead of treating an unseen document whose total sum of log probabilities (and the prior) is below 0 as a case from class A, we say that the score needs to be (say) below 0.5. Presto we've changed the classification of a bunch of documents. There are however other methods for tweaking the classifications, like for instance weighting the features according to their importance.
