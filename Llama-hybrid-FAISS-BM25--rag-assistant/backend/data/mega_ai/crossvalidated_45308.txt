[site]: crossvalidated
[post_id]: 45308
[parent_id]: 43753
[tags]: 
How LDA does dimension reducion has same methodology with PCA. When you get J(W) at LDA (W is result that which minimizes within class scatter matrix but maximizes between class scatter matrix) W = inv(Sw)*Sb When you get W , you can get result with: V is eigenvectors of W final_result = V'*your_data before that the trick plays role. As like at PCA, you can get eigenvectors and eigenvalues of W and you can discard some eigenvectors that has smallest corresponding eigenvalues. Then if you multiply your data with that eliminated eigenvector matrix ( V ) you get a lower dimension.
