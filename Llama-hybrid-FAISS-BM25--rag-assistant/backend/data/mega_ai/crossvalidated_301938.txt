[site]: crossvalidated
[post_id]: 301938
[parent_id]: 44181
[tags]: 
I have the same issue where I want to compute per-item reliability, and as you said it looks like e.g., Krippendorf's alpha is not workable (as far as I can tell the expected disagreement is estimated across all units, so with only one unit the expected disagreement cancels the observed disagreement exactly). My plan is to just fall back on raw percent agreement, averaged across all possible pairs of raters (see this article for a good example and breakdown of the following methods: percent agreement, Scott's Pi and Cohens Kappa, Fleiss' Kappa, and Krippendorf's Alpha). I think that for your specific application simple percent agreement would work fine.
