[site]: datascience
[post_id]: 112149
[parent_id]: 
[tags]: 
Why we do random sampling when we select the training set?

The usual workflow when building a machine learning model starts with random splitting the data set into training and test set. What I can't understand is why we do this. For example lets say we have a labeled data set of 100 data points: $$\{x_i, y_i\}_{i=1}^{100}$$ and the $x$ values are distributed in the following way: $$x \in[0, 5) \rightarrow 80 \, \, \text{samples}$$ and $$x \in[5, 10) \rightarrow 20 \, \, \text{samples}$$ Then if we random sample 80 data points to generate our training how do we know that we won't pick the 80 samples from the first interval? I mean it is completely possible as all data points are equiprobable during sampling. And if that happens then our model wouldn't be able to see "trends" in regions outside of $[0, 5)$ . Why then random sampling is the way to go when we do train-test split?
