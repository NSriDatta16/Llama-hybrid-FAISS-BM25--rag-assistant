[site]: crossvalidated
[post_id]: 291730
[parent_id]: 
[tags]: 
Differentiating cross entropy w.r.t the bias term

In the 2-layer neural network with cross entropy loss for softmax hidden layer, why do I get 2 different answers when applying chain rule in 2 different ways? Please help me correct method#2; I am going crazy: Given: $$ C = \sum_iy_ilog(\hat{y_i}) $$ $$ \hat{y} = softmax(z) $$ $$ z=xW+b $$ Method # 1 - Gives correct, texbook answer Noticing $$ \frac{\partial C}{\partial b}=\frac{\partial C}{\partial{z}} \frac{\partial{z}}{\partial b} $$ and we know $$ \frac{\partial C}{\partial z} = y-\hat{y}, $$ $$ \frac{\partial z}{\partial b} = 1 $$ $$ \frac{\partial C}{\partial b}=y-\hat{y} $$ Method # 2 - Gives wrong answer, but why?: $$ \frac{\partial C}{\partial b}=\sum_{i} y_i\frac{\partial log(\hat{y_i})}{\partial b} $$ So then, $$ log(\hat{y}) = log(softmax(z))=log(\frac{exp(xW+b)}{\sum_k exp(x_kW_k+b)}) $$ $$=xW+b-log(\sum_k exp(x_kW_k+b)) $$ Therefore $$ \frac{\partial log(\hat{y})}{\partial b} = \frac{\partial}{\partial b}(xW+b)-\frac{\partial{log(\sum_k exp(x_kW_k+b))}}{\partial b} $$ $$ =1-\frac{1}{\sum_{l=1}^N exp(x_lW_l+b)}\frac{\sum_{k=1}^N\partial{exp(x_kW_k+b)}}{\partial b} $$ $$ = 1-\frac{1}{\sum_{l=1}^N exp(x_lW_l+b)}\sum_{k=1}^N\exp(x_kW_k+b) = 1-1 =0 $$ Obviously I messed it up in method 2, but cant figure what's wrong. I tried writing it out with all subscripts but still came to the same answer 0.
