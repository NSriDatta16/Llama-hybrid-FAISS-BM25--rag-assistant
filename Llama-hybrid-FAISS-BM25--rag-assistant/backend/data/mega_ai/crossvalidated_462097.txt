[site]: crossvalidated
[post_id]: 462097
[parent_id]: 462076
[tags]: 
Before answerinig you quesion, let's first make clear the Bayesian mindset: All about Bayesian is to use posterior distribution to summarize everything we know about a certain random entity (such as $\theta$ in your case). Where "everything we know" contains two parts: part1 : the prior information about $\theta$ , which is represented by the prior distribution $p(\theta)$ . part2 : the information from the observations , which is represented by the likelihood $p(y|\theta)$ To combine two parts of information, of course you need to assign a weight to each of them. There has to be one that is more pronouncing than the other. And as you have noticed, sample size $n$ plays as the information weight for part2, which make sense because the more samples you observe, the more convincing the samples say about $\theta$ , thus they should play a bigger role in the posterior (keep in mind that posterior is just a way to represent the combined information of part1 and part2). As for part1, there will always be a prior strength been assigned when defining the prior (for example when using inverse-wishart distribution as the prior of a covariance matrix, the degree-of-freedom of the inverse-wishart distribution is the prior strength). And prior strength plays as the information weight for part1 . What if you assigin weight $1/n$ to the observations, by multiplying a $1/n$ to the log-likelihood as you did in the question. Then the weight for part1 will not increae with new data been observed, which means no matter how many samples you observe, your total knowledge about $\theta$ won't change. For example: If I want to model a coin toss, and using $Beta(2,2)$ as the prior distribution of the probability of getting a head(here the prior strength is $2$ ). After 10 observations, say there are 4 heads and 6 tails, then the posterior distribution will be $$ \theta \sim Beta(4+2,6+2) $$ The variance of $\theta$ is $Var(\theta) = 0.0163$ . The variance shows how uncertain you are about the distribution of $\theta$ , the MAP estimate in this case is $\theta=0.4167$ . If you increase your sample size to 1000, there are 400 heads and 600 tails, then the posterior will be $$ \theta \sim Beta(400+2,600+2) $$ The variance is $\theta$ is $Var(\theta) = 0.0002$ , the MAP estimate in this case is $\theta=0.3998$ . You can see that the variance is much smaller with more samples been observed. This is because increasing samples will increase the amount of information you gained in the posterior, the more information result in less uncertainty . Also, the MAP esitmate will be closer to the MLE when sample size is increased, this is because observations plays more weight in the posterior, thus has more say to the estimate. But if you set weight to $1/n$ , for no matter 10 or 1000 samples, the posterior will be the same: $$ \theta \sim Beta(0.4+2,0.6+2) $$ The variance is $Var(\theta)=0.0416$ and won't reduce when more samples are observed.The MAP esitimate will be $0.4667$ and won't show a tendency to MLE.
