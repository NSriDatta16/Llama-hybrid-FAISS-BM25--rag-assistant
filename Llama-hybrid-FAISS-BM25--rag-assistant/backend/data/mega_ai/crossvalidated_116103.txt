[site]: crossvalidated
[post_id]: 116103
[parent_id]: 116033
[tags]: 
Intriguing question, I had this question for a while,. Here is my findings Short Answer You can create any number of classifier you want, but the point is, you can only prove a few of them to be Bayes/universally-consistent! ( Bayes consistency means that classifier is asymptotically optimal, i.e. with infinite data its risk limits Bayes risk, which is optimal risk) The consistency of a classifier, depends on loss function and (inverse)-link function (i.e. mapping from [0 1] probability space to $\mathbb{R}$ , and vice versa.) Long answer First, according to Tong's great paper all the (consistent) classifiers are equivalent! except in that they are minimizing different loss functions, and almost every difference between classifiers is a consequence of their loss functions. In fact, he showed that minimizing every loss function leads to optimal decision function (technically, inverse-link function), which is completely function of probabilities (even for SVMs!). His result is summarized in this table (by Hamed ): Despite of this unified view over all the classifiers, they are different in their outputs: Probability-Calibrated: for these class the classifiers (e.g. Logistic Regression), output is DIRECTLY within a probability measure, which this in turn not only answers yes/no question of the classifier, but also gives confidence of the of the decision. Not-probability-Calibrated: Other classifiers (e.g. SVM) are real-valued-output classifiers, which you can use some link functions to calibrate the to enforce outputs to be probabilities. Conclusion It really depend on loss-function, link-function, calibration. For example, first line of the table says that, least-squares regression and classification are the same,(if your classifier output is calibrated-probabilities $\eta$ , and using the corresponding inverse link function)
