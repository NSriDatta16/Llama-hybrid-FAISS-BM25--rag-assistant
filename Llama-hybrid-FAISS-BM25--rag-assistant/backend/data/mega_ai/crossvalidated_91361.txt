[site]: crossvalidated
[post_id]: 91361
[parent_id]: 
[tags]: 
normalizing dataset for extrapolation - sample or population mean and standard deviation?

I am currently fitting models that are intended to be used for extrapolating from a limited sample to a large population. For a specific example, one model is predicting water temperature in rivers based on attributes of the river. The sample includes data from approximately 1,000 river segments (a river segment is a unique length of a river), which will be used to extrapolate to over 100,000 river segments. This is a neural network model, but I have the same question for a different model using hierarchical logistic regression models, so the question is not statistical method specific. I have read and have found through experience that normalizing my modeling dataset can improve model fit. However, I am unsure in this instance because the goal is extrapolating to the population. Should the predictors be normalized [(y - mean)/stdev] based on their mean and sdev in the sample or in the population of river segments? I have taken the opinion that I should normalize based on the population to help ensure that the population variability is represented adequately in the modeling dataset. I apologize if there is a cross posting, as I have searched for this answer thoroughly and have not seen a similar question or discussion anywhere. So I am asking this here. Please point me to an answer if one exists elsewhere.
