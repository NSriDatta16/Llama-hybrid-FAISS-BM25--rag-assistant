[site]: crossvalidated
[post_id]: 354629
[parent_id]: 345570
[tags]: 
User @shimao writes in comments: input data needs to be normalized/standardized in order to properly train. this explains why adding 500 caused everything to stop working. also learning rate of 0.5 is way too high. try 0.01 or 0.001 neural networks are quite difficult to optimize for a variety of reasons, which has mostly been "solved" by clever initialization schemes, more computation power, better variants of gradient descent, etc. often, all of these details are tuned with the assumption that the data is normalized, so that the gradients aren't too large/small and you don't run into numerical problems. Therefore normalizing is necessary. I want to clarify that I've this comment as a community wiki answer because the comment is, more or less, an answer to this question. We have a dramatic gap between answers and questions. At least part of the problem is that some questions are answered in comments: if comments which answered the question were answers instead, we would have fewer unanswered questions. Please review Are we seeing a dramatic drop in answers per question? Comments that are actually answers
