[site]: crossvalidated
[post_id]: 636511
[parent_id]: 
[tags]: 
Leave-one-out performance variance on validation dataset

I'm having good performance on leave-one-out validation (not leaving one out, leaving 2 out, one for each class) but when trying in my validation set, the performance is not that good, from 85% precision to 60%. Context: I read some of the answers made before but, I have some differents aboarding, I'm working on a dataset that's like a time-series. I'm trying to classify on binary tags, I tried on some algorithms like knn, svm, and decision trees, but none of it works, so we started trying on Neural Net (Keras) on this. I got that performance, on k-fold cross-validation, the curves of learning overfit quickly an loss validation breaks easily, don't know what else to try, any recommendations for the next post are welcome, thanks. Some clarifications, my datasets arent so big having 300 samples and are not exactly balanced, so when overfit go straight to predict only one class, already try to balance the dataset and get better results
