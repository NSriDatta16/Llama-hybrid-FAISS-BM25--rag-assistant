[site]: crossvalidated
[post_id]: 49328
[parent_id]: 49313
[tags]: 
Try to analyze the differences of the clusters first. K-means is an odd algorithm. Sometimes it just works very well, and in other situations it fails just very very badly. It has a tendency to just split your data set along a single axis. So you may actually find out that your clustering result is something like this: Cluster A contains all documents that contain "apple" Cluster B contains all documents that contain "banana" Cluster C contains all documents that contain "cocoa" Cluster D contains all the others You need to double check your clustering results! There are several reasons for this. One is the way clusters look to k-means . They're Voronoi cells, separated by orthogonal hyperplanes. The other big effect here is the sparsity of your data set . The mean vectors computed by k-means will be much less sparse, usually. In fact, the average distance between the mean vectors will likely be lower than the distances from your data objects to the closest mean. The latter probably is a good test: What is the average distance between to cluster centers, and what is the average distance of an observation to the nearest cluster center? Clearly, objects should on average be closer to their cluster center than two cluster centers to each other. But this may actually not hold for sparse data.
