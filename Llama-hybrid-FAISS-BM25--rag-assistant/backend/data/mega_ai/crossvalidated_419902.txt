[site]: crossvalidated
[post_id]: 419902
[parent_id]: 
[tags]: 
Does padding ensure that the sizes of input and output of a CNN match?

I was looking at the equations for having this true and I believe its possible. For simplicity everything is squared. Say we have a square image of size $n$ . If we have filters of kernel size $f$ the we have a resulting feature vector of size: $$ n - f + 1 $$ when we padd we get an image of size $n + 2p$ . So we we do convolution on that we get: $$ n+2p-f +1 $$ So what we want is: $$ n+2p-f+1 = n$$ which results in: $$ p = \frac{f-1}{2} $$ and use $f$ being odd (odd size filters). Seems that as long as the padding always satisfies this no matter what deep of the network we are (e.g. middle layers vs input) we always have inputs of size $n$ . Is this true? I really want my feature vectors to never downsample.
