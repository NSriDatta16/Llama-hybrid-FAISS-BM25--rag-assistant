[site]: crossvalidated
[post_id]: 451290
[parent_id]: 350321
[tags]: 
I am giving an implementation of the above theory here @ https://haphazardmethods.wordpress.com/2017/06/29/chapter-3-kullback-leibler-divergence/ , so that the OP is better able to grasp the idea behind it and implement it in real life. They take a good sample range to explain the concepts and make a graph similar to the book's Figure 3.6 What they are doing is calculating a min KL divergence, between two functions if you want to send a piece of encoded information or train a neural network with KL for variational autoencoders, multiclass classification scenarios, or replacing least-square minimizations. Refer to this post for more info. and comparisons between similar entropy methods/least-squares/etc @ Intuition on the Kullback-Leibler (KL) Divergence
