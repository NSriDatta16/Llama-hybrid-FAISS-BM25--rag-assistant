[site]: crossvalidated
[post_id]: 261050
[parent_id]: 261038
[tags]: 
You're astute in sensing that there may be conflict between classical assumptions of ordinary least squares linear regression and the serial dependence commonly found in the time series setting. Consider Assumption 1.2 (Strict Exogeneity) of Fumio Hayashi's Econometrics . $$ \mathrm{E}[\epsilon_i \mid X] = 0 $$ This in turn implies $\mathrm{E}[\epsilon_i \mathbf{x}_j] = \mathbf{0}$, that any residual $\epsilon_i$ is orthogonal to any regressor $\mathbf{x}_j$. As Hayashi points out, this assumption is violated in the simplest autoregressive model .[1] Consider the AR(1) process: $$y_{t} = \beta y_{t-1} + \epsilon_t$$ We can see that $y_t$ will be a regressor for $y_{t+1}$, but $\epsilon_t$ isn't orthogonal to $y_t$ (i.e. $\mathrm{E}[\epsilon_ty_t]\neq0$). Since the strict exogeneity assumption is violated, none of the arguments that rely on that assumption can be applied to this simple AR(1) model! So we have an intractable problem? No, we don't! Estimating AR(1) models with ordinary least squares is entirely valid, standard behavior. Why can it still be ok? Large sample, asymptotic arguments don't need strict exogeniety. A sufficient assumption (that can be used instead of strict exogeneity) is that the regressors are predetermined , that regressors are orthogonal to the contemporaneous error term. See Hayashi Chapter 2 for a full argument. References [1] Fumio Hayashi, Econometrics (2000), p. 35 [2] ibid., p. 134
