[site]: crossvalidated
[post_id]: 266954
[parent_id]: 266937
[tags]: 
There are a couple issues with the expressions in this question. I'll address these, then answer the question down below. Issues with expressions In your expression, $p(x \mid \theta)$ will always be 0 if $x=1$ . It should be $\theta^{x_d} (1 - \theta)^{1-x_d}$ rather than $\theta^{x_d} (1 - \theta^{1-x_d})$ . No need to subscript the thetas; the data are i.i.d. so $\theta$ is a single, scalar value that's shared for all data points. If performing gradient descent you'd work with the negative log likelihood (otherwise you'd be minimizing the likelihood rather than maximizing it). Use the log likelihood if using gradient ascent . The negative log likelihood should be: $$L(\theta) = -\log \prod_{d=1}^{n} \theta^{x_d} (1 - \theta)^{1-x_d}$$ The log of a product is a sum of logs so: $$L(\theta) = -\sum_{d=1}^{n} \log \left ( \theta^{x_d} (1 - \theta)^{1-x_d} \right )$$ Differentiating w.r.t. $\theta$ , we have: $$\frac{d}{d\theta} L(\theta) = -\sum_{d=1}^{n} \left ( \frac{x_d}{\theta} + \frac{x_d-1}{1 - \theta} \right )$$ Can gradient descent yield invalid parameters? Can gradient descent ever set $\theta$ to be less than 0 or greater than 1? One thing that will tend to prevent this is that, when the data set contains a mix of zeros and ones, the negative log likelihood approaches infinity as $\theta$ approaches 0 or 1. This discourages gradient descent from approaching or exceeding these values; the gradient will pull $\theta$ back into a more reasonable range. For example, here's the negative log likelihood and gradient for some points sampled i.i.d. from a Bernoulli distribution with true $\theta=0.7$ : But, say we set the step size too large. For example, say the current $\theta$ is 0.5 (so gradient descent will step in the positive direction), and the step size is some large value. We can overshoot the optimum ( $\theta=0.7$ ), and even exceed the valid parameter range; $\theta$ could end up greater than 1. If this happens, the expression for the negative log likelihood will return a complex value because we'd be taking the log of a negative number. This breaks the optimization. The case is different when the data set contains all zeros or all ones. For example, here's the negative log likelihood and gradient for 100 values that are all one: The true value of $\theta$ is 1, which has a negative log likelihood of 0. But, looking at the expressions above, the gradient is -100. This means gradient descent will keep stepping in the positive direction. And, in this case, the expression for the negative log likelihood will produce increasingly negative values. So, gradient descent will continue to increase $\theta$ without bound. Fixing the problem The issue is that gradient descent is an unconstrained optimization algorithm, but this problem requires that $\theta \in [0, 1]$ . Solving the problem correctly requires imposing this constraint. Of course, this particular problem can be solved by simply calculating the frequency of ones in the data (no need for iterative optimization). But, for the sake of illustration, there are a couple approaches that can work. One way is to reparameterize the problem. For example, we could define $\theta$ as a sigmoid function of some real-valued parameter $\alpha$ . That is, $\theta = \frac{1}{1 + e^{-\alpha}}$ . So, $\alpha$ can take any value and $\theta$ will always always lie between 0 and 1. We can then use an unconstrained optimization algorithm to minimize the negative log likelihood w.r.t. $\alpha$ . Another approach is to use an optimization algorithm that lets us impose bound constraints, which will force $\theta$ to lie in the correct range. There are dedicated solvers that can implement many kinds of constraints, and the best choice will depend on the particular problem. A simple example in this case would be to modify gradient descent to clip $\theta$ to the allowed range after each step: $\theta \leftarrow \min(1, \max(0, \theta))$ . This is an example of a 'gradient projection method'. I mention it because this question is focused on gradient descent, but there are other constrained optimization algorithms with faster convergence.
