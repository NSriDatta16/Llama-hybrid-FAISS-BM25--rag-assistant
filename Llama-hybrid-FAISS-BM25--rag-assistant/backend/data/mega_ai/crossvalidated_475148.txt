[site]: crossvalidated
[post_id]: 475148
[parent_id]: 
[tags]: 
Can normalization decrease model performance of ensemble methods?

Normalization e.g. z-scoring is a common preprocessing method in Machine Learning. I am analyzing a dataset and use ensemble methods like Random Forests or the XGBOOST framework. Now I compare models using non normalized features z-scored features Using crossvalidation I observe in both cases that with higher max_depth parameter the training error decreases. For the 1. case the test error also decreases and saturates at a certain MAE: For the z-scored features however the test error is non-decreasing at all. In the question Would you recommend feature normalization when using boosting trees? was discussed that normalization is not necessary for tree based methods. But the example above shows that it has a severe effect. So I have two questions regarding this: Does it imply that overfitting with ensemble based methods is possible even when the test error decreases? Should normalization like z-scoring always be common practice when working with ensemble methods? Is it possible that normalization methods decrease the model performance?
