[site]: crossvalidated
[post_id]: 261008
[parent_id]: 
[tags]: 
Deep learning : How do I know which variables are important?

In terms of neural network lingo (y = Weight * x + bias) how would I know which variables are more important than others? I have a neural network with 10 inputs, 1 hidden layer with 20 nodes, and 1 output layer which has 1 node. I'm not sure how to know which input variables are more influential than other variables. What I'm thinking is that if an input is important then it will have a highly weighted connection to the first layer, but the weight might be positive or negative. So what I might do is take the absolute value of the input's weights and sum them. The more important inputs would have higher sums. So for example, if hair length is one of the inputs, then it should have 1 connection to each of the nodes in the next layer, so 20 connections (and therefore 20 weights). Can I just take the absolute value of each weight and sum them together?
