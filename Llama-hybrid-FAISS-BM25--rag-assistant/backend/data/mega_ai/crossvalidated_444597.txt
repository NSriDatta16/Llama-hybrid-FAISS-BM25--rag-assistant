[site]: crossvalidated
[post_id]: 444597
[parent_id]: 
[tags]: 
How to model transition probability if action does not lead to a state change (in MDP)?

An MDP (markov decision process) is defined as a set of states $S$ , actions space $A$ , Transition Probabilities $T$ and Rewards $R$ . An action $a$ in a state $s$ usually result in a change of state and the probability is defined as $p(s'|s,a)$ . I have a question if an action $a$ in a state $s$ does not result in a change of state then how to define transition probabilities $p(s'|s)$ ? How to define state transitions from $s$ to $s'$ when state transitions do not occur after taking an action $a$ in a state $s$ ?
