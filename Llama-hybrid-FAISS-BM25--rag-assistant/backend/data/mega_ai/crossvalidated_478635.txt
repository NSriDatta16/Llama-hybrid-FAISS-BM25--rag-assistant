[site]: crossvalidated
[post_id]: 478635
[parent_id]: 478629
[tags]: 
There isn't a "correct" order of the model. There is only a "most likely" order of the model, and what "most likely" means depends on the criterion you use to evaluate likelihood. There is a good paper that overviews the methods for determining model order, but it won't help unless you understand the fundamentals of Bayesian statistics. But looking at the paper for the mclust package by Scrucca et al we learn a basic difference between the criteria we are using (p. 297): BIC tends to select the number of mixture components needed to reasonably approximate the density, rather than the number of clusters as such...ICL penalises the BIC through an entropy term which measures clusters overlap Scrucca et al give a simplified formula for ICL as: where BIC_{M,G} is the BIC for model M with G components, and the term on the right is the entropy term referenced above. That term sums over observations 1...n and clusters 1...G; c_{ik} an indicator function that is 1 if the ith observation is assigned to cluster G, and z_{ik} is the conditional probability that the observation arises from the kth mixture component. Since we have log(z_{ik}) this term gets smaller (i.e. more negative) with the more clusters we assign each observation to and the lower the probability of assignment of each cluster. Since we are selecting the model that maximizes the ICL, the effect is to favour solutions where points are more clearly assigned to particular clusters (i.e., more separated solutions). What can we make of BIC vs ICL? Baudry et al (2010:333): Biernacki, Celeux, and Govaert (2000) argued that the goal of clustering is not same as that of estimating the best approximating mixture model, and so BIC may not the best way of determining the number of clusters, even though it does perform well in selecting the number of components in a mixture model. Instead they proposed the ICL criterion, whose purpose is to assess the number of mixture components that leads to best clustering. This turns out to be equivalent to BIC penalized by the entropy of the corresponding clustering. We argue here that the goal of selecting the number of mixture components for estimating the underlying probability density is well met by BIC, but that the goal of selecting the number of clusters may not be. Even when a multivariate Gaussian mixture model is used for clustering, the number of mixture components is not necessarily the same as number of clusters. This is because a cluster may be better represented by a mixture normals than by a single normal distribution. The last line makes sense if you think about trying to to approximate a bivariate lognormal distribution using bivariate normals. If your data came from one bivariate normal distribution you'd have to fit at least two bivariate normals to get a reasonable approximation of the density (one for the "spike" and one for the "tail"). So how do we interpret these results (assuming we don't know the DGP)? Look at the data and recognize we have highly overlapping components. If modelling via GMM still seems reasonable, recognize that recovering the parameters of the underlying Gaussians is possible but unambiguous assignment of observations to clusters isn't. (We could still assign observations to the most likely cluster if we want to go on and gather more data about the resulting "groups"). Because we have highly overlapping clusters, we shouldn't use the ICL criterion. It's designed to favour separation, which we don't have. Using the BIC criterion, we identify the VVE model with three components as best. Happily the LRT confirms this. Consequently we interpret these results as indicating an order 3 VVE model as the most likely (Gaussian) mixture given our observed data. It's reasonable to study the characteristics of these components, but we should be wary of making strong claims (beyond, say, "exploratory analysis"). Any application that requires unambiguously assigning an observation to a particular cluster is clearly a bad idea.
