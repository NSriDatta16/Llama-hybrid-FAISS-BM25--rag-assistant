[site]: crossvalidated
[post_id]: 346431
[parent_id]: 
[tags]: 
A2C Loss Function Explosion

I am training OpenAI's implementation of the A2C algorithm found here. Based on the mean episode reward graph below we can see it is in fact learning the policy function up until roughly 2000 updates: However, after roughly 1000 updates we see the overall loss explode as well as the value function loss: Overall Loss: Value Function Loss: Also note the explosion of mean value across states: Is the sudden "explosion" of overall loss a normal characteristic of A2C? Most of this significant jump seems to be attributed to the value function loss. Note: Based on domain knowledge I know our value function is relatively hard to approximate and thus why we chose the policy gradient method A2C.
