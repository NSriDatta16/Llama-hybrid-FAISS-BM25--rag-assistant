[site]: crossvalidated
[post_id]: 48602
[parent_id]: 
[tags]: 
Recurrence definition for a Markov chain

We define a state i to be recurrent if $\sum\limits_{n=0}^\infty P(X_n=i,X_k \neq i$ for $1\leq k Why do we take infinite series over the probability? Why don't we define recurrent just as: $P(X_n=i,X_k \neq i$ for $1\leq k Doesn't this already mean, that if we start in s, we will with probability 1 (almost surely) return to s, after n steps? Edit: Also, i is said to be recurrent if: $\sum\limits_{n=0}^\infty p_{ii}^{(n)}= \infty$. How does this fit logically with the other definition? Why would the series of the transition probabilities be unbounded?
