[site]: crossvalidated
[post_id]: 10277
[parent_id]: 
[tags]: 
Advantages of approaching a problem by formulating a cost function that is globally optimizable

This is a rather general question (i.e. not necessarily specific to statistics), but I have noticed a trend in the machine learning and statistical literature where authors prefer to follow the following approach: Approach 1 : Obtain a solution to a practical problem by formulating a cost function for which it is possible (e.g. from a computational standpoint) to find a globally optimal solution (e.g. by formulating a convex cost function). rather than: Approach 2 : Obtain a solution to the same problem by formulating a cost function for which we may not be able to obtain a globally optimal solution (e.g. we can only get a locally optimal solution for it). Note that rigorously speaking the two problems are different; the assumption is that we can find the globally optimal solution for the first one, but not for the second one. Other considerations aside (i.e. speed, ease of implementation, etc.), I am looking for: An explanation of this trend (e.g. mathematical or historical arguments) Benefits (practical and/or theoretical) for following Approach 1 instead of 2 when solving a practical problem.
