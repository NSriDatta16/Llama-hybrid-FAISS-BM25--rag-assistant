[site]: crossvalidated
[post_id]: 428956
[parent_id]: 406471
[tags]: 
I'm inclined to agree with your reviewer that this is a bad idea, for two reasons. Firstly, you haven't explained why it is reasonable to split your continuous variable (pretest score) into a binary one. It is almost always a bad idea to bin (i.e. split) continuous data, as it involves throwing away useful information. Binning into two blocks is an extreme case of it. Secondly, even though you may have done this with the best of intentions, this is bad use of p-values and the null hypothesis statistical testing framework. What you have done goes by many names: HARKing (Hypothesizing After the Results are Known), p-hacking, or more broadly and less pejoratively, the garden of forking paths . Essentially, after finding non-significant results one way, you changed a few things, ran a new analysis, and then found a significant result in one subgroup. There are a huge number of ways that one could tweak an analysis, and if you only stop when you find a significant result, you're essentially guaranteeing that you will find something significant. This massively biases any p-values you obtain and renders them meaningless. This is likely a major contributing factor to the replication crisis in science . These two sites allow you to play with simulations and real data to get a feel for how a basic version of this process biases results and inferences (note that the general idea described above is a bit more broad than the issues you can explore in these). However, it is possible to learn from data in a way that avoids these pitfalls. You can switch to a Bayesian framework and avoid p-values altogether (although a similar issue arguably arises with the choice of prior); combining this with multilevel modelling may prevent many such erroneous inferences. preregister your analyses before your project, so that any p-values you obtain are not guided by decisions you make after examining your data. distinguish between hypothesis testing and data exploration, with p-value calculations avoided entirely for the latter. Data exploration can (and ideally should) then be used to inspire follow-up hypothesis tests with new datasets; the p-values in the new analyses will be more reliable.
