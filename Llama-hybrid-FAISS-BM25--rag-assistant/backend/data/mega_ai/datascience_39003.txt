[site]: datascience
[post_id]: 39003
[parent_id]: 
[tags]: 
Memory error when using more layers in CNN model

On my dell core i7 - 16GB RAM - 4gb 960m GPU laptop, I am working on a project to the classify lung CT images using 3d CNN. I'm using the CPU version of tensorflow. The images are prepared as numpy array size (25,50,50). My CNN model had 2 conv layers, two maxpool layer, one FC layer and output layer. With this architecture I could train the model with approximately (5000 to 6000) samples. After adding more layers my model now has 6 conv layers, 3 max-pool layers, FC and output layer. My problem is after changing the architecture with just more than 1000 samples my memory gets filled and I get memory error. I tried to make smaller batches, but every time getting same error. I have two questions: Why by adding more layers the model needs more memory? Is there any way to deal with this type of problem?
