[site]: crossvalidated
[post_id]: 621720
[parent_id]: 
[tags]: 
How exactly do I create a partial regression plot to view the true effect of x on true y?

When creating a scatterplot by plotting each variable in a linear regression against y, we are essentially treating all regressors to be linearly independent. If regressors have multicollinearity, for each scatterplot, the x and y values will include the effect of other regressors on both x and y. For my purposes, I want to create a partial regression plot, which isolates the effect of other variables from both x and y and captures the sole effect of x on y . Chat GPT says to isolate the effects of those other variables from both x and y, I must subtract from x the predicted x of other variables on x and subtract from y the predicted y of other variables except x on y. Here is an example: def partial_corrwith(df: pd.DataFrame, y, method='pearson'): partial_model = StatLinearRegression() # Create an empty dataframe to store the residuals residuals_df = pd.DataFrame() # Create an empty series to store the partial correlations partial_corrs = pd.Series(dtype=float) # Create an empty series to store the VIFs vifs = pd.Series(dtype=float) # For each column in df for col in df.columns: # Define the other columns that are not `col` other_cols = [c for c in df.columns if c != col] # Regress `col` on all other predictors and get the residuals partial_model.fit(df[other_cols], df[col]) #Variance Inflation Factor(VIF) # Compute the VIF for `col` and add to the VIFs series vif = variance_inflation_factor(df.values, df.columns.get_loc(col)) vifs[col] = vif #Should I take the absolute value here? residuals = df[col] - partial_model.predict(df[other_cols]) # Standardize the residuals residuals_standardized = (residuals - residuals.mean()) / residuals.std() # Rescale the standardized residuals back to the original scale residuals_rescaled = residuals_standardized * df[col].std() + df[col].mean() residuals_df[col] = residuals_rescaled # Regress `y` on all other predictors except `col` and get the residuals partial_model.fit(df[other_cols], y) #This residual computation is required to remove the effect of other variables on y when analyzing x residuals_y = y - partial_model.predict(df[other_cols]) # Standardize the residuals of y residuals_y_standardized = (residuals_y - residuals_y.mean()) / residuals_y.std() # Rescale the standardized residuals of y back to the original scale residuals_y_rescaled = residuals_y_standardized * y.std() + y.mean() # Compute the correlation of the residuals for `col` and `y` and add to the partial correlations series partial_corrs[col] = residuals_df[col].corr(residuals_y, method=method) return partial_corrs, residuals_df, residuals_y_rescaled, vifs The problem is when I plot out the partial scatterplot, it displays negative values for x, which is very difficult to interpret in my context, because I know for a fact that none of my regressors have negative values, both numeric ones and string regressors target encoded by y. Please let me know whether this is the proper method of generating a partial regression plot in the first place and if so, teach me how to be able to maintain all valuable information of the partial regression plot while transforming negative values of x to positive. If this is not the proper method of doing a partial regression please teach me.
