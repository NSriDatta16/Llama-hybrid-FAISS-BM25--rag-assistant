[site]: crossvalidated
[post_id]: 342279
[parent_id]: 342268
[tags]: 
My understanding is that this choice is much like many other choices made in deep learning research: often there is some intuition driving these choices but everything ultimately comes down to what increases model accuracy and generalization to new data samples. In this case the intuition is that by stacking more fully connected layers, you are imbuing the model with greater representational capacity and it can thus learn more complex functional mapping’s between the output of the previous layers (a set of convolutional feature maps) and the final classification label. This capacity is balanced by a general tendency of deep networks to “overfit” to the training data; meaning that the model has too much capacity and begins to “memorize” training samples rather than learn high-level relationships. This ultimately hurts performance on the test/validation datasets, and eventually diminishes the accuracy on unlabeled data when used in production (which is a hidden problem since you rarely acquire the true labels for these samples to know if your model is performing well or not). You must remember that adding linear/fully connected layers allows you to model relationships between all of the input variables to the label but this comes at a great cost of a large number of additional fitting parameters in the model. Developing and successfully training a neural network is a complex engineering problem that can be guided by our intuition but ultimately comes down to following design decisions that achieve the main goals, increasing test set accuracy and simplifying the training process. So you see, the choice of how many FC layers to place at the end of a conv-net is determined by trying each number out and sticking with the “best” one.
