[site]: crossvalidated
[post_id]: 355936
[parent_id]: 159657
[tags]: 
I recently had to choose a metric for evaluating multilabel ranking algorithms and got to this subject, which was really helpful. Here are some additions to stpk's answer, which were helpful for making a choice. MAP can be adapted to multilabel problems, at the cost of an approximation MAP does not need to be computed at k but the multilabel version might not be adapted when the negative class is preponderant MAP and (N)DCG can both be rewritten as weigthed average of ranked relevance values Details Let us focus on average precision (AP) as mean average precision (MAP) is just an average of APs on several queries. AP is properly defined on binary data as the area under precision-recall curve, which can be rewritten as the average of the precisions at each positive items. (see the wikipedia article on MAP ) A possible approximation is to define it as the average of the precisions at each item. Sadly, we lose the nice property that the negative examples ranked at the end of the list have no impact on the value of AP. (This is particularly sad when it comes to evaluating a search engine, with far more negative examples than positive examples. A possible workaround is to subsample the negative examples, at the cost of other downsides, e.g. the queries with more positive items will become equally difficult to the queries with few positive examples.) On the other hand, this approximation has the nice property that it generalizes well to the multilabel case. Indeed, in the binary case, the precision at position k can be also interpreted as the average relevance before position k, where the relevance of a positive example is 1, and the relevance of a negative example is 0. This definition extends quite naturally to the case where there are more than two different levels of relevance. In this case, AP can also be defined as the mean of the averages of the relevances at each position. This expression is the one chosen by the speaker of the video cited by stpk in their answer. He shows in this video that the AP can be rewritten as a weighted mean of the relevances, the weight of the $k$-th element in the ranking being $$w_k^{AP} = \frac{1}{K}\log(\frac{K}{k})$$ where $K$ is the number of items to rank. Now we have this expression, we can compare it to the DCG. Indeed, DCG is also a weighted average of the ranked relevances, the weights being: $$w_k^{DCG} = \frac{1}{\log(k+1)}$$ From these two expressions, we can deduce that - AP weighs the documents from 1 to 0. - DCG weighs the documents independently from the total number of documents. In both cases, if there are much more irrelevant examples than relevant examples, the total weight of the positive can be negligible. For AP, a workaround is to subsample the negative samples, but I'm not sure how to choose the proportion of subsampling, as well as whether to make it dependent on the query or on the number of positive documents. For DCG, we can cut it at k, but the same kind of questions arise. I'd be happy to hear more about this, if anybody here worked on the subject.
