[site]: crossvalidated
[post_id]: 615523
[parent_id]: 
[tags]: 
Which loss function to use if entries in a prediction span multiple orders of magnitude?

I am modeling the rates of chemical reactions with a ML-model (specifically a neural network with a lowish number of parameters). However, the rates (or technically the formation rates), the predictions, span multiple orders of magnitude in absolute values with differing signs and including zero. Each prediction consists of a vector of rates. So one sample from a batch of typical training data might look like $$\begin{bmatrix} 1.3\mathrm{e}{-12} & -3.9\mathrm{e}{-8} & 7.6\mathrm{e}{-5} & 0.00 \end{bmatrix}.$$ That means that inside one target vector the values differ by orders of magnitude. However, the values between two targets are (mostly) in the same order of magnitude. My big problem is to accurately and robustly determine the 'error' of one of my predictions to ensure they are right, even across multiple orders of magnitude. My attempts so far include absolute error e.g. L1/L2-norm: They fail for obvious reasons. The absolute biggest term in the prediction dictates the error and predictions for small values can be wrong by orders of magnitudes. relative error e.g. MRE: They fail for the presence of zeros in my predictions so are not applicable as is but have to be extended with a threshold for example. thresholded relative error e.g. $|\frac{\hat{y}_i-y_i}{\max(|y_i|,t_i)}|$ : fixes the zero issue but the choice of threshold can be arbitrary and does not guarantee good performance. logarithmic scaling for error calculation : same problem as with relative errors except worse because predictions might be negative. So far I have not been able to find a good compromise and I see the consequences in finding good approximations for the more dominant rates and very poor results for the smaller ones. Are there any more reliable/meaningful error measures I am missing?
