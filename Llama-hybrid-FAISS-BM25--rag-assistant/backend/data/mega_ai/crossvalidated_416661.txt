[site]: crossvalidated
[post_id]: 416661
[parent_id]: 
[tags]: 
Reward attribution in deep q learning and texas holdem poker

I’m having issues with reward attribution in poker using deep q learning. Multiple actions will yield one reward, but the reward is only known at the end of the hand, not before. I have built a gym environment for Texas Hold’em, with the idea to use deep q agents to learn how to outperform random poker players: www.gitnub.com/dickreuter/neuron_poker One of the difficulties I’m facing is the correct reward attribution after each move. The reward will be known at the end of the hand only but before I could only approximate it with a montecarlo simulation that would be subject to a number of assumptions and most probably not precise enough. My question is therefore: would it be a valid approach to only give the agent only the reward after it becomes known what the outcome of the hand is? Specifically, an action at pre flop (no cards yet on the table), then another one at the flop (3 cards are put in the table) and finally at turn and river will collectively yield a single reward that is only known when the hand is over. Shouldn’t all actions therefore have the same reward that is then being used for experience replay? Any suggestions on how the reward calculation is best addressed would be appreciated. It seems an essential part of the problem.
