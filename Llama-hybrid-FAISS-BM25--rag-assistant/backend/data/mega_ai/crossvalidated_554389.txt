[site]: crossvalidated
[post_id]: 554389
[parent_id]: 
[tags]: 
Why do transformers use separate K and V networks

In transformers, two separate networks $\phi_k, \phi_q$ compute the keys and queries. The attention weight is then the dot product of the keys and queries. (i.e. $score = \phi_k(z_1) \cdot \phi_q(z_2)$ ) This seems overly complex. Why don't we use a single neural network $\phi$ to compute the attention scores directly (i.e. $scores = \phi( concatenate(z_1, z_2) )$ ). The latter approach seems simpler and likely uses less parameters than the former. Is there something I'm missing for why the first approach is preferred?
