[site]: crossvalidated
[post_id]: 115554
[parent_id]: 
[tags]: 
Confidence Intervals Around a Mean: biased (non-centered) confidence interval? (an exercise using R)

I've been playing around with the module "Confidence Intervals Around a Mean" (meanCI) from statsTeachR (www.statsteachr.org), authored by Eric A Cohen (unfortunately, author's contact information was not available). It "focuses on understanding and calculating confidence intervals around a sample mean. This includes understanding the concept of a confidence interval; understanding when using either the normal or the t-distribution (or neither) is appropriate; writing R code to calculate confidence intervals; and writing code to test coverage of the true population mean by confidence intervals" (www.statsteachr.org/modules/19). Building on this module, I produced this plot: It shows the average number of "mistakes" (confidence intervals that do not contain the population mean) as the sample size (n) increases from 2 to 1000 (the population size N), distinguishing between "below mistakes" and "above mistakes" (above mistake: the lower limit of the CE is above the population mean; below mistake: the upper limit of the CE is below the population mean). The figure has 4 panels: Top left: When the population follows a chi square distribution, and confidence interval using the normal distribution. Top right: When the population follows a chi square distribution, and confidence interval using the t distribution. Bottom left: When the population is normally distributed, and confidence interval using the normal distribution. Bottom right: When the population is normally distributed, and confidence interval using the t distribution. My question is, why there seems to be some sort of bias, with below mistakes consistently higher than the above mistakes when the population follows a chi squared distribution and above mistakes consistently higher than below mistakes when the population is normally distributed? Is it really like that?, I mean, is this a standard result?, or it is just an error in my code? (I swear I've checked my code many many times, and I think everything is ok). If there is no error in my code, doesn't it mean that we could improve confidence interval's performance trying to correct for such 'bias'? I think I've been doing my homework and I cannot reconcile what I find in my exercise using R with the theory. Checking out my introductory statistics book, it doesn't say anything about non-centered confidence interval. In fact, it says that the mean estimator is consistent and thus, it gets closer to the true population mean as the sample size increases, and given that the confidence interval has the mean estimator in its center, it seems to me that the above and below mistakes should be relatively similar. I also searched here in CrossValidated and found this VERY interesting post-answers ( What, precisely, is a confidence interval? ), but nobody there discussed anything about whether the confidence interval is indeed centered or not. Any thought is more than welcome. Here's my code: There are three core functions as follows. The first two (CIs_normal_dist, CIs_t_dist) calculate "by hand" the confidence interval using the normal approximation and the t distribution respectively. Note that both functions assume the population variance is unknown and thus, estimate the standard error using the sample values (as discussed here Confidence intervals for mean, when variance is unknown and here http://www.r-tutor.com/elementary-statistics/interval-estimation/interval-estimate-population-mean-unknown-variance ). The third function produces sets of CIs and calculates how many mistakes (population mean not in the CI) there are in a set, and repeat that many times to calculate the average number of mistakes. # BASED ON: # statsTeachR module: Confidence Intervals around a Mean # Author: Eric A. Cohen # Following statsTeachR module, this function takes many (num_CIs) samples # from the data and calculates the sample mean and the confindence interval # around it, using the normal approximation. CIs_normal_dist 3){ #print(z.test(x=sample_i, conf.level=1-alpha, sigma.x=sample_sd)$conf.int) #} } CIs } # Following statsTeachR module, this function takes many (num_CIs) samples # from the data and calculates the sample mean and the confindence interval # around it, using t distribution CIs_t_dist population_mean) #above mistake mistakes[i,3] Now, I just use those functions for the exercise and summarize the results in a plot: rm(list = ls(all = TRUE)) source("CE_bias_func.R") #I saved the functions above in this file library(BSDA) library(Hmisc) # set the parameters for the exercise pop_mean UPDATE: After very useful comments from @swmo and @heropup I re-run the exercise, now taking samples from the whole distribution instead of samples from a finite population, so as to have truly independent samples. And this is the result: pretty much what you would expect in the first place. i) no considerable differences in above and below mistakes and ii) total number of CIs that do not include the true parameter (above+below mistakes) close to 5%. Here's my code (for now, I only did it for a normal dist and using a t.test, but very easily could be adapted to cover the 4 cases in my original post): # A function that takes 100 samples from the distribution, # given the sample size (n) received as argument # then calculate the CI for each of them # and calculate the number of mistakes (above and below) for each of them # It returns a data.frame with one row and three variables for sample size, # above mistakes and below mistakes trial 2) 1 else 0) below_mistakes
