[site]: crossvalidated
[post_id]: 345442
[parent_id]: 345439
[tags]: 
In theory, that's feasible, since the encoder just summarizes the input text into 1 vector in a typical sequence to sequence model, and the decoder is pretty much a conditioned language model. (I'm aware that more advanced sequence to sequence models use attention, copy mechanism, and so on.) In practice, you'll need a training set, and given the current state-of-the-art you're probably better off doing some information retrieval to retrieve blog posts of interest given the keywords.
