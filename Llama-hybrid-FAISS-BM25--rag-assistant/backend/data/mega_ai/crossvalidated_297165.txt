[site]: crossvalidated
[post_id]: 297165
[parent_id]: 
[tags]: 
Taking average reward is not optimal when estimating state value in MDP?

In policy evaluation process in MDPs, the state value function is calculated as $V(s) = E(r+V(s'))$. Now suppose there are 7 states in an MDP, where state 1 is the start state and state 7 is the terminal state. Some transitions and their rewards according to some policy are shown in the follows: start -> end, reward 1 -> 2, 0 1 -> 3, 10 1 -> 4, 25 2 -> 5, 0 3 -> 5, 0 3 -> 6, 0 4 -> 6, 40 4 -> 6, 20 5 -> 7, 20 5 -> 7, 30 6 -> 7, 10 6 -> 7, 20 According to the policy evaluation formula, we can calculate the state value for each state: 42, 25, 20, 45, 25, 15, 0 respecively for state 1-7. However, I don't think the state values is a good fit for the transitions. If we define loss as $Loss = (V(s) - (V(s')+r))^2$ which is the loss used when we adopt function approximators for value functions,we will find that such set of state values gives a loss of 1567. However, by simply multiplying each state value by 0.9, it can give a better loss of about 1472. It is reasonable because there are lots of zero rewards in the transitions. So my question is: what is the objective function that taking the expectation of rewards optimizes? Is there anything wrong in my reasoning above? If we use a function approximator, it will definately underestimate the state values, what is the root cause for this?
