[site]: crossvalidated
[post_id]: 498556
[parent_id]: 498544
[tags]: 
I hope what I write is helpful. When you don't have information about the parameter of interest, in your case the parameter $\theta$ , you want to use a prior, that will let the data drive the inference. Such priors are called non informative. For your case in particular: Lets assume that we have $n$ iid data points, $T_{1},T_{2},...,T_{n}$ then the likelihood function of these data points, the function driven mostly by the data, is of the form: $L(\theta|T_{1},...,T_{n})=L(T_{1},T_{2},...,T_{n};\theta)=\prod_{i=1}^{n}\theta e^{-\theta T_{i}}=\theta^{n}e^{-\theta \sum_{i=1}^{n}T_{i}}$ Also, let us consider as prior distribution for the parameter $\theta$ the $Gamma(a,b).$ The ultimate goal in Bayesian inference is to calculate the posterior distribution: $p(\theta|T_{1},T_{2},...,T_{n})\propto L(\theta|T_{1},T_{2},...,T_{n})p(\theta) \propto\theta^{n}e^{-\theta \sum_{i=1}^{n}T_{i}} \theta^{a-1}e^{-b\theta}= \theta^{n+a-1}e^{-\theta(\sum_{i=1}^{n}T_{i}+b)}$ . If you notice in the case where you choose the parameter of the prior distribution of $\theta$ as $a=0.001$ and $b=0.001$ , then $n+0.001-1\approx n$ and $\sum_{i=1}^{n}T_{i}+0.001\approx \sum_{i=1}^{n}T_{i} $ . Generally speaking this means that the posterior distribution $p(\theta|T_{1},T_{2},...,T_{n})$ will be close to the likelihood function, i.e will be close to the function that is driven mostly by your data. Hence, I think that a Gamma distribution of the form $Gamma(0.001,0.001)$ is not a bad choice. Also, not that the parameters of a Gamma distribution cannot be zero. Lastly, if you want to can check Jeffreys Prior, which is invariant under parametrization and can be also non informative, https://en.wikipedia.org/wiki/Jeffreys_prior .
