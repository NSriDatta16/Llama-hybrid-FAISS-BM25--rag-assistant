[site]: crossvalidated
[post_id]: 614429
[parent_id]: 
[tags]: 
Generalizing Inverse Variance Weighting?

Consider a generalization of inverse-variance weighting , where we choose normalized weights $w_i = \dfrac{\sigma_i^{-p}}{W_p}$ for some $p\geq 1$ , where $W_p \equiv \sum\sigma_i^{-p}$ . Then $p=2$ corresponds to inverse-variance weighting. If we have $n$ data points and denote $r_i \equiv \sigma_i^{-1} \in \mathbb{R}$ , then we can write $w_i = \sigma_i^{-p}/W_p = \left(\dfrac{r_i}{||\mathbf{r}||_p}\right)^p$ , where $||\cdot||_p$ is the $p$ -norm. Thus, this generalization just reflects a potential choice of norm for $\mathbb{R}^n$ , your "inverse error space". Question (see below for background): Does anyone know how using $p=1$ might be theoretically justified, or if there are other more general methods that would be related to choosing $p=1$ ? The responses to this question seem to indicate "no". Background: I have some data about a system and its subcomponents (full context at bottom of page). The system AB has subcomponents A and B, both of which have subcomponents A+, A-, B+ and B-. Thus I have 7 data points $x_i \pm \sigma_i$ (scalars) for (AB, A, B, A+, A-, B+, B-). Qualitatively, each component seems to be a weighted average of its subcomponents, with the lower-error subcomponents carrying more weight (i.e. weight $w_i$ is inversely related to $\sigma_i$ in some way). My naive thinking (being unaware of inverse-variance weighting at the time) was to try that generalized inverse-error weighting above. I tried both $p=1$ and $p=2$ (the latter corresponding to inverse-variance weighting), and found that $p=1$ gave very good agreement in all cases: the weighted averages of both (A, B) and (A+, A-, B+, B-) both agree well with $x_{AB}$ , while the weighted averages of (A+, A-) and (B+, B-) respectively agree well with $x_A$ and $x_B$ . For $p=2$ , I only found agreement with (A, B) adding up to AB; in every other case, the subcomponents underestimate their combined component, due to the lower-error A- and B- components being overweighted. Looking into inverse-variance weighting, the logic of its use doesn't seem to apply to this scenario, since the $x_i$ 's are presumably not independent. When I talked to a professor of mine about the above, they essentially said I shouldn't have even bothered with $p=1$ and that my result isn't worth a mention -- I should just mention the qualitative "weighted-averageness" of the data -- seemingly because I have no theoretical justification for $p=1$ . However, the approach seems perfectly reasonable to me given the qualitative look of the data, and the empirical result of all the components being the (weighted) "sum of their parts" feels too good to dismiss like that. My logic was essentially to start with inverse errors, then choose a norm on my inverse-error space. In that sense, it seemed logical to try both $p=1$ (inverse errors weighted equally to determine $w_i$ ) and $p=2$ (inverse errors weighted by magnitude to determine $w_i$ ) and see what came of it. Context: Signal processing in astronomy. I have two very similar high-resolution spectra of a particular galaxy, taken at different times. I am cross-correlating them to look for a small shift, which would correspond to bulk acceleration of gas. I cut up the spectra into different velocity components, to look for a differential shift as a function of velocity. The full spectrum AB shows two broadened transition lines A and B, each of which have a high (+) and low (-) velocity component (A+, A-, B+, B-). I cross correlated AB and all of its subcomponents separately, for a total of 7 cross-correlations (AB, A, B, A+, A-, B+, B-). The data point $x_i$ is the lag at which the cross-correlation function peaks for component $i$ . Errors $\sigma_i$ are estimated by creating 100 "randomized" versions of both spectra, then correspondingly doing 10000 cross-correlations and generating a distribution of maximizing shifts for each component. We then take the standard deviation of the distribution to be the error in the shift. To randomize a spectrum, we take the flux at a given wavelength, $f_\lambda \pm \sigma_\lambda$ , and assign a new flux $f_{sim} \sim N(f_\lambda,\sigma_\lambda^2)$ , where $N(\mu,\sigma^2)$ is the usual normal distribution.
