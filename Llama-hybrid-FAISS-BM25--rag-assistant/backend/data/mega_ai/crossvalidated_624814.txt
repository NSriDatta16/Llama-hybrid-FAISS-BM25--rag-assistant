[site]: crossvalidated
[post_id]: 624814
[parent_id]: 495658
[tags]: 
In this answer, I will be using the example given in dariober's answer. There are two variables x1 and x2 that differ from each other only by a small noise ( sd= 0.0001 ). The response is also correlated to the two, although by a somewhat larger noise (no sd specified, defaults to 1). Now, we would expect that the two variables to give somewhat equal contributions to the linear model. Ideally, it will be around 0.5 each. What we get instead are very large coefficients that are similar in magnitude but with opposing signs. This is because they are trying to fit the noise in the response, and the difference between the variables, which is noise, may have some correlations with the noise in the response. By applying PCA, the highly correlated component of the two variables are put into PC1, while the uncorrelated component are put into PC2. By applying regression on these two, you now see the true contribution of the highly correlated component, while the uncorrelated component has a component with very large magnitude and very large standard error. Now, if you use all the PCs then you have not solved the problem with multicollinearity. The next step is to remove the problematic PC2 and recover the coefficients for the original variables by rotating the coefficients back (though there's only one of them now). After removal of PC2, the final coefficients for x1 and x2 were 0.5718034 and 0.5718014 respectively, which is in agreement to the original intuition we originally had in the first paragraph.
