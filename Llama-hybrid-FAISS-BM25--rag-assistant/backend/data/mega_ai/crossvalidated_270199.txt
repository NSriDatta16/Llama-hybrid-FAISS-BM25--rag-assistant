[site]: crossvalidated
[post_id]: 270199
[parent_id]: 
[tags]: 
Bayesian: Joint Smoothing Recursion

Note: I am dealing with a state space model/hidden Markov model, where process $x_t$ is only dependent on $x_{t-1}$ and $y_t$ is only dependent on $x_t$. \begin{equation} \begin{split} p(x_{0:t}|y_{0:t})& = \frac{p(y_t|x_{0:t},y_{0:t-1}) p(x_{0:t}|,y_{0:t-1}) }{p(y_t|y_{0:t-1})} ~~~\mbox{(bayes)}\\ & = \frac{p(y_t|x_{0:t},y_{0:t-1}) p(x_{t}|x_{0:t-1},y_{0:t-1}) p(x_{0:t-1}|y_{0:t-1})}{p(y_t|y_{0:t-1})}~~~\mbox{(bayes)} \\ &=\frac{p(y_t|x_{t}) p(x_{t}|x_{t-1}) p(x_{0:t-1}|y_{0:t-1})}{p(y_t|y_{0:t-1})}~~~\mbox{(conditional independence)} \end{split} \end{equation} Question: What is $p(y_t|y_{0:t-1})$ as an integral? I saw here that they wrote $$p(y_t|y_{0:t-1})=\int p(y_t|x_t)p(x_t|x_{t-1})p(x_{t-1}|y_{0:t-1}) dx_{t-1:t} \,.$$ How do they get to that: can someone write a step-by- step derivation (actively stating what steps they used to simplify)? I find it unusual how they simplified $p(x_{0:t-1}|y_{0:t-1})$ to $p(x_{t-1}|y_{0:t-1})$ and how they knew to integrate away $dx_{t-1:t}$. If I were to guess what $p(y_t|y_{0:t-1})$ was I would have written $$p(y_t|y_{0:t-1})=\int p(y_t|x_{0:t},y_{0:t-1})p(x_{0:t}|,y_{0:t-1}) dx_{0:t}$$ However, I do not know how to continue...
