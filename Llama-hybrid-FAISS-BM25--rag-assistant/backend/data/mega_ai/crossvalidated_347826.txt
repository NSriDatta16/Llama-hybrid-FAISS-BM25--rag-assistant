[site]: crossvalidated
[post_id]: 347826
[parent_id]: 
[tags]: 
Using Autoencoders to generate equal feature length for variable length data

Autoencoders can be used to generate a fixed-length feature representation of the input data. Now consider the case that your input data (e. g. videos) has unequal length. So you would train autoencoders for your individual data points in order to engineer the features of your input data. Is that a "valid" approach in terms of stochastic properties? Because I am wondering whether this reduction from e. g. 2 minutes material to a fixed number of features compared to a video of 30 seconds is valid to generate a reasonable model. What would be alternatives to this approach?
