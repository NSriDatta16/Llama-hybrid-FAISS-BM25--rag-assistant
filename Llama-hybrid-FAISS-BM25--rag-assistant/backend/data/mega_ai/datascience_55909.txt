[site]: datascience
[post_id]: 55909
[parent_id]: 
[tags]: 
does smaller training set always lead to better training accuracy?

Does anyone know any existing research or have observed some experimental results in deep learning about the following: For a fixed data set, if you subset the dataset from 50% to 90% as training set, using the same number of epochs, will the training error be bigger as the training set becomes bigger? would be great if anyone could give some concrete experimental results or paper about this? For example, this one https://arxiv.org/abs/1901.04169v1 and this one https://www.researchgate.net/publication/333671691_Resampling-based_Assessment_of_Robustness_to_Distribution_Shift_for_Deep_Neural_Networks but both papers have not drawn any conclusions on this issue.
