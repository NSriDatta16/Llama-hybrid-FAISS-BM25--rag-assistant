[site]: crossvalidated
[post_id]: 238801
[parent_id]: 215637
[tags]: 
Basically when a deep learning neural network runs, you want to optimize the weights on each neuron given the data and model . These "ideal weights" are very hard to compute exactly, so instead you run gradient descent to update the weights some on each iteration and get a good approximation. The more iterations you do, the better your performance, but you're never going to do better than your model+data would with an optimal choice of weights. So yes, you can improve the performance of Word2Vec some by increasing the number of iterations. But in practice, I haven't seen much gain beyond 20-30 iterations, and there's a hard upper bound on performance imposed by the quality and quantity of data you have.
