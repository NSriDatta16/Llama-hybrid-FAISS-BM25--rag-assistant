[site]: datascience
[post_id]: 128132
[parent_id]: 
[tags]: 
Does Google DeepMind's Gemma 7B models specs have inconsistent dimensions?

In Google DeepMind's Gemma technical paper ( https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf ), the 7B Gemma model specs are given as d_model = 3072, num_heads = 16 and head_size = 256 for the 7B model. They don't seem consistent (16 * 256 != 3072). Since the dimension is distributed across h heads, I think this should hold true - #heads * #head_size = d_model This is also explained in the original Transformers paper, "Attention Is All You Need". This equation holds for the specs provided for Gemma 2B model in the same paper. Am I missing something with the 7B Gemma specs? Or does this paper have an error?
