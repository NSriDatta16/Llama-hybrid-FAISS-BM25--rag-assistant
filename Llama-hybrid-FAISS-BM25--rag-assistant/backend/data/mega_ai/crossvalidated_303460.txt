[site]: crossvalidated
[post_id]: 303460
[parent_id]: 303362
[tags]: 
You could do that, depending on the size of the problem and the type of data. The Apriori algorithm comes to mind, but the complexity can get pretty rough. This would be appropriate if you were looking at sets of interactions, and wanted numbers like confidence on a given relationship. However, since you're dealing with things like “liking an item” probably not appropriate. The fabled Top-N would be would be appropriate here if your matrix [or matrices] was sparse and binary or numeric of size users*items. If you can subset the users or item space it would make it more computationally friendly. More or less, you would use some similarity measure like dotproduct or cosine to compute the similarity of users or items and rank those accordingly to determine likely interactions among similar users [or items]. Also, you can do the same for items, and find similar items to recommend in a serendipitous way based on whether a user “liked” a similar item, or a similar user liked a similar item, or a similar user liked that item but ultimately bought another similar item. I'd make the assumption that removals from carts are generally motivated by either too much cost [so recommend cheaper items] or just a distinct lack of interest, so the recommendations there should be short lived temporally. For example. Let's assume that you have n users and m items, and you wanted to get the user similarity. You would be doing comparisons of length items, users * users-1 times. So, the worst case complexity is O(mn^2). This isn't too bad if your data isn't hugantic ginormic [that is your users aren't huge], but you'd want to pick a similarity measure that's appropriate and quick to evaluate. E.g dotproduct for binary or cosine for numeric. There are lots of similarity measures out there [I think last I checked there were like 83 for this one algorithm] but every mathematical operation counts when you're dealing with nonlinear complexity. Then, you'd simply say, in ranked order, your user is most likely to do next what ranked most similar user did and so on. But, if there is a temporal quality to the behavior, this won't do very well. You can still find similar users, but you'd need to account for the time variable in some clever way. Naively, I'd just say to find another user that is at least x% similar that did a thing later and assume your user may do that. Or, use the serendipity from the set of options previously discussed. 10000 items isn't bad. It's 100 million comparisons of users length [to find similar items]. But, we should keep in mind that you only have to update as often as you'd like, and this type of matrix operation is intrinsically parallelizable. I wrote a pretty lengthy paper on this, and it's not thoroughly clean, but you can also compress the vectors into the spectral domain to get a 4–16x speedup with a slight loss in resolution and accuracy. But for something that size, I'd break it up and grind it out the ways the smarter guys and gals did before I made a variation on the theme. The proper way sounds much better unless you're really crunched for cpu cycles. As for dealing with the added information, it sounds fun and doable, but you'd have to make a few assumptions and rules to get it going.
