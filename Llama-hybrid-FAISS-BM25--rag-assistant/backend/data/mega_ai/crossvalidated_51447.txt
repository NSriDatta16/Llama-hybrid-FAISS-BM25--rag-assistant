[site]: crossvalidated
[post_id]: 51447
[parent_id]: 51441
[tags]: 
When performing cross-validation, you use part of the data (say nine tenths of the observations) to train the model ant the remaining tenth to compute a goodness-of-fit statistic like R2 or whatever you choose. The distinctive idea in cross valiation is that ALL data are used to both train and test. Assume, for instance, you have N=1000 observations. You would set aside observations 1 to 100 to test and train on all others, then set aside observations 101 to 200 and train on all others, etc. Thus, you would fit your model ten times and average the results of R2 or whatever you choose. It is common (and usually sufficient) to split the data in 5 or 10 fragments and perform (5-fold, 10-fold) cross-validation as indicated. In linear regression you may, at really no extra cost, perform the most extreme variety of cross-validation, leave-one-out, in which you leave aside one observation each time. If you are using R, I would suggest you to look at function lm.ridge in package MASS.
