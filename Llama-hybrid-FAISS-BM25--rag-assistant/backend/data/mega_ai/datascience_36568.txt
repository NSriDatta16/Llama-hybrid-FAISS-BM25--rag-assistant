[site]: datascience
[post_id]: 36568
[parent_id]: 
[tags]: 
Is deduction, genetic programming, PCA, or clustering machine learning according to Tom Mitchells definition?

Tom M. Mitchell defines machine learning as A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E. For some algorithms that are usually counted towards the field of machine learning, I heavily doubt that they are learning algorithms according to the definition. Interestingly, all of them are part of Tom Mitchell's book "Machine Learning". Deduction The "symbolist tribe" uses inverse deduction according to Pedro Domingos (see The 5 tribes of ML ). While I'm not too sure what inverse deduction is, the idea of having a set of true statements and inference rules like Modus Ponens to create more/other true statements is clear. Comming to the definition: What is experience here? How is performance measured? What does a typical task look like? Genetic programming From Wikipedia: Genetic programming (GP) is a technique whereby computer programs are encoded as a set of genes that are then modified (evolved) using an evolutionary algorithm (often a genetic algorithm, "GA"). What is experience in this case? Maybe "iterations" / "epochs"? (I'm not sure about the terminology) Tasks: What are the typical use cases of Genetic Programming? PCA PCA and some other dimensionality reduction algorithms are in some way "fixed". The algorithm does not change with more data. It does not have parameters. Thinking about it, one might argue that it estimates the projection in a better way with more data. But then: What is better? What is the performance measure P here? Clustering I don't see a clear performance measure for Clustering (e.g. k-means ). Also, the only kind of experience I can see is the number of samples.
