[site]: crossvalidated
[post_id]: 319749
[parent_id]: 319744
[tags]: 
Sure you can. Think of this as of a missing data problem, you have partial data with some missing cases. To estimate the full model you need first to estimate the parameters given the non-missing data cases, then make some predictions for the missing data and use it in your model. This is how algorithms like EM work and it can be incorporated in a Bayesian model. Since the missing data becomes something to be estimated, in Bayesian scenario you can, and will, additionally assume some priors for the unseen values of the data. Writing down the actual Stan code for such case would be tricky, so after you write down the full model, you could ask for help on the Stan users mailing list . To give you an example, say that you have two variables $X$ and $Y$ and you use a simple regression model $E(Y) = \beta_0 + \beta_1 X$. Your total sample has size $n$ and for $1,...,k$ first cases you have data on $X$, where for the $k+1,...,n$ you don't (let's call them $\tilde X$). So you can think of your model as of consisting of two "parts". First, you'd estimate $\beta_0,\beta_1$ for the submodel $E(Y_{[1,\dots,k]}) = \beta_0 + \beta_1 X_{[1,\dots,k]}$. Second, given the already known values of $\hat \beta_0, \hat \beta_1$, you estimate $\tilde X_{[k+1,\dots,n]}$ in the submodel $E(Y_{[k+1,\dots,n]}) = \hat\beta_0 + \hat\beta_1 \tilde X_{[k+1,\dots,n]}$. Using the EM algorithm, you would do this iteratively, while in the case of Bayesian model you could define a model where $\beta_0,\beta_1,\tilde X_{[k+1,\dots,n]}$ are the "parameters" to be estimated, where you assume some priors for them. The likelihood for the full data would consists of two "parts" for the missing and non-missing cases: $$ L(\beta_0, \beta_1, \tilde X_{[k+1,\dots,n]}) = \prod_{i=1}^k \mathcal{N}(y_i | \beta_0 + \beta_1 x_i,\; \sigma^2) \times \prod_{i=k+1}^n \mathcal{N}(y_i | \beta_0 + \beta_1 \tilde x_i,\; \sigma^2) $$ Such approach does not have to introduce much "bias" in your model. It makes your model use all the data you have (in the above examples, all $Y$'s and the $1,\dots,k$ $X$'s), rather then using only the non-missing cases. Obviously, you know less about the cases with missing data, then about the rest of your data, so the model would less precisely learn something about this part of your data. Also you need to remember that what you would assume about the missing data would depend on what you know about the non-missing data (imagine extreme case where you have only single non-missing $X$ and $99$ missing cases, then your best guess about the missing cases would be to assume that they are like the single non-missing value, what could lead to strongly biased results). The above example is very simple, but imagine that you have multivariate model, then the data in the non-missing variables would possibly make your guesses about the missing data in the missing-data variables more precise. As you can imagine, $E(Y) = \square + \square \square$ is a worse case in terms of identifiability, then $E(Y) = \square + \square X_1 + \square \square + \square X_3 + \square X_4 $, where "$\square$" stands for something (partially) unknown to be guessed.
