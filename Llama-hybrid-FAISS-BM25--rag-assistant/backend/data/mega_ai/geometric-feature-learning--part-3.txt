with high probability. The learning algorithm aims to predict whether the learned target concept f T ( X ) {\displaystyle \textstyle f_{T}(X)} belongs to a class, where X is the instance space consists with parameters and then test whether the prediction is correct. Evaluation framework After learning features, there should be some evaluation algorithms to evaluate the learning algorithms. D. Roth applied two learning algorithms: 1.Sparse Network of Winnows(SNoW) system SNoW-Train Initial step: initial the set of features F t = ϕ {\displaystyle \textstyle F_{t}=\phi } which linked to target t for all t ∈ T {\displaystyle \textstyle t\in T} . T is a set of object targets whose elements are t 1 {\displaystyle \textstyle t_{1}} to t k {\displaystyle \textstyle t_{k}} If each target object in set T belongs to a list of active features, link feature to target and set initial weight at the same time. Evaluate the targets : compare targets ∑ i ∈ e w i t {\displaystyle \textstyle {\underset {i\in e}{\sum }}w_{i}^{t}} with θ t {\displaystyle \textstyle \theta _{t}} , where w i t {\displaystyle \textstyle w_{i}^{t}} is the weight on one position connecting the features i to target t. \theta_{t} is the threshold for the target not t. Update weight according to the result of evaluation. There are two cases: predicted positive on negative example ( ∑ i ∈ e w i t > θ t {\displaystyle \textstyle {\underset {i\in e}{\sum }}w_{i}^{t}>\theta _{t}} and targets are not in the list of active features) and predicted negative on positive example( ∑ i ∈ e w i t ≤ θ t {\displaystyle \textstyle {\underset {i\in e}{\sum }}w_{i}^{t}\leq \theta _{t}} and targets are in the list of active features). SNoW-Evaluation Evaluate the each target using same function as introduced above Prediction: Make a decision to select the dominant active target node. 2. support vector machines The main purpose of SVM is to find a hyperplane to separate the set of samples ( x i , y i ) {\displaystyle \textstyle (x_{i},y_{i})} where x i {\displaystyle \textstyle x_{i}} is an input vector which is a selection of features x ∈ R N {\displaystyle \textstyle x\in R^{N}} and y i {\displaystyle \textstyle y_{i}} is the label of x i {\displaystyle \textstyle x_{i}} . The hyperplane has the following form: f ( x ) = sgn ( ∑ i = 1 l y i α i ⋅ k ( x , x i ) + b ) = { 1 , positive inputs − 1 , negative inputs {\displaystyle \textstyle f(x)={\text{sgn}}\left(\sum _{i=1}^{l}y_{i}\alpha _{i}\cdot k(x,x_{i})+b\right)=\left\{{\begin{matrix}1,\ {\text{positive inputs}}\\-1,\ {\text{negative inputs}}\end{matrix}}\right.} k ( x , x i ) = ϕ ( x ) ⋅ ϕ ( x i ) {\displaystyle \textstyle k(x,x_{i})=\phi (x)\cdot \phi (x_{i})} is a kernel function Both algorithms separate training data by finding a linear function. Applications Landmarks learning for topological navigation Simulation of detecting object process of human vision behaviour Learning self-generated action Vehicle tracking == References ==