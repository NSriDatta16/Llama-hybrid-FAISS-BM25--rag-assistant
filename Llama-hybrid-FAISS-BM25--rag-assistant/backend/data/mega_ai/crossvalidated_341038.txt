[site]: crossvalidated
[post_id]: 341038
[parent_id]: 341027
[tags]: 
After performing some additional research: After demonstrating the significant improvements of Experience Replay when training deep neurel networks in this paper , DeepMind wanted to remove the limiting architecture Experience Replay required. Instead, in they're next generation (A3C) ( paper here ) they decided to utilize asynchronous parellel agents to achieve the same purpose of Experience Replay (autocorrelation removal). Without Experience Replay they were free to implement N-Step returns. The following is explained in the paper Asynchronous Methods for Deep Reinforcement Learning 2 : Instead of experience replay, we asynchronously execute multiple agents in parallel, on multiple instances of the environment. This parallelism also decorrelates the agentsâ€™ data into a more stationary process, since at any given time-step the parallel agents will be experiencing a variety of different states. This simple idea enables a much larger spectrum of fundamental on-policy RL algorithms, such as Sarsa, n-step methods, and actorcritic methods, as well as off-policy RL algorithms such as Q-learning, to be applied robustly and effectively using deep neural networks.
