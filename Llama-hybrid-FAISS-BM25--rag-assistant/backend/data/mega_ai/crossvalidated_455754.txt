[site]: crossvalidated
[post_id]: 455754
[parent_id]: 455278
[tags]: 
I will first try to answer your first question. To show that we can estimate the variance $\text{Var}(\hat{\theta}_{n})$ of maximum likelihood estimator $\hat{\theta}_{n}$ with second log likelihood derivative, we can try to derive asymptotic relationship between second log likelihood derivative $l''(\theta)$ and Fisher information $I_{n}(\theta)^{-1}$ . The negative of second log likelihood derivative $-l''(\theta)$ is called observed Fisher information . Let $X_{1}, ..., X_{n}$ form a random sample from a distribution for which the p.d.f. (or p.f.) is $f(x \mid \theta)$ then the likelihood is a product $$ L_{n}(\theta) = \prod_{i = 1}^{n} f(x_{i} \mid \theta) $$ then the log likelihood and its derivatives are the sums \begin{align*} l_{n}(\theta) &= \sum_{i = 1}^{n} \log f(x_{i} \mid \theta) \\ l'_{n}(\theta) &= \sum_{i = 1}^{n} \frac{\partial}{\partial \theta} \log f(x_{i} \mid \theta) \\ l''_{n}(\theta) &= \sum_{i = 1}^{n} \frac{\partial^2}{\partial \theta^2} \log f(x_{i} \mid \theta) \end{align*} Fisher information in the entire sample is defined as $$ I_{n}(\theta) = -E_{\theta}\left[ l''_{n}(\theta) \right] $$ Since data are i.i.d.\ we also have $$ I_{n}(\theta) = n \cdot I_{1}(\theta). $$ Let $\hat{\theta}_{n}$ be the maximum likelihood estimator (MLE) of $\theta$ . Based on the asymptotic normality of MLE the distribution of $\hat{\theta}_{n}$ is approximately $$ \mathcal{N}(\theta, I_{n}(\theta)^{-1}) $$ so we can estimate the variance $Var(\hat{\theta}_{n})$ with Fisher information $I_{n}(\theta)^{-1}$ . Since $n$ random variables $X_{1}, ..., X_{n}$ are i.i.d., the $n$ random variables $l''(X_{1} \mid \theta), ..., l''(X_{n} \mid \theta) $ are also i.i.d., where we define $l''(x \mid \theta) = \frac{\partial^2}{\partial \theta^2} \log f(x \mid \theta)$ . Then dividing above equation by $n$ we get average of i.i.d. random variables $$ \frac{l''_{n}(\theta)}{n} = \frac{1}{n} \sum_{i = 1}^{n} \frac{\partial^2}{\partial \theta^2} \log f(x_{i} \mid \theta) $$ and we can calculate the expectation $$ E_{\theta} \left[ l''_{1}(\theta) \right] = -I_{1}(\theta) $$ and now we can apply (weak) law of large numbers (LLN) to get convergence in probability $$ \frac{l''_{n}(\theta)}{n} \overset{p}{\to} -I_{1}(\theta). $$ which is why we can approximate $I(\theta)$ with the second derivative of the log likelihood $l''(\theta)$ . To show this in your case of Cox regression model in a more rigorous way we need to generalize this from random variables to martingales.
