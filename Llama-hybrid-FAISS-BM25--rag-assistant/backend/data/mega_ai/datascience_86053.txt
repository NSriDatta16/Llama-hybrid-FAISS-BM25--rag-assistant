[site]: datascience
[post_id]: 86053
[parent_id]: 
[tags]: 
Attention for time-series in neural networks

Neural networks in many domains (audio, video, image text/NLP) can achieve great results. In particular in NLP using a mechanism named attention (transformer, BERT) have achieved astonishing results - without manual preprocessing of the data (text documents). I am interested in applying neural networks to time-series. However, in this domain, it looks like most people apply manual feature engineering by either: transposing the matrix of events to hold columns for each time observation and a row for each thing (device, patient, ...) manually generating sliding windows and feeding snippets to an RNN/LSMTM. Am I overlooking something? Why can't I find people using attention? Wouldn't this be much more convenient (automated)?
