[site]: crossvalidated
[post_id]: 409546
[parent_id]: 406501
[tags]: 
I thought this may answer your question: We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. But they found that temperature scaling can effectively calibrate predictions and in a new study some researchers found ways to calibrate the results of reinforcement learning. Since modern neural nets seem be trained all in batches so the aforementioned two papers are all about batch based training. Reference: On Calibration of Modern Neural Networks
