[site]: crossvalidated
[post_id]: 58957
[parent_id]: 46915
[tags]: 
You can use the 1-hot encoding - for a single tree, each example is represented by a vector containing 1 with the selected leaf, and combine these vectors for a forest (either concatenated or OR'ed). This gives you an intermediate representation. Another option is to use the proximity measure [1] to compute an unsupervised sparse feature representation- a matrix M where M_ij = #times examples i,j terminated in the same leaf (over the entire forest). This matrix is sparse and large but you can reduce its size. Do either of those give a useful intermediate representation? I don't know of any attempts at deep learning with random forests.. [1] http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#prox
