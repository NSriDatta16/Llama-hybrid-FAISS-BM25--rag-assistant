[site]: datascience
[post_id]: 64638
[parent_id]: 64631
[tags]: 
To be clear, RNN are not necessary as there exist other options. However, RNN are useful since they implicitly deal with temporal relationships between inputs. Regular neural networks are idempotent, meaning that the same input can be passed multiple times and the same output will be computed (obviously if the network is not updated in the meantime). This idempotence means that regular neural nets cannot implicitly "see" temporal patterns. To do so, they would need to be provided with temporal information as you suggest. This is actually what transformers achieve! By providing temporal embeddings on top of the input, it always a regular neural net to learn from temporal information.
