[site]: stackoverflow
[post_id]: 4976934
[parent_id]: 4976765
[tags]: 
There are several different ways of detecting/deleting duplicates: Nested loops Take the next value in sequence, then scan until the end of the list to see if this value occurs again. This is O(n 2 ) -- although I believe the bounds can be argued lower? -- but the actual performance may be better as only scanning from i to end (not 0 to end ) is done and it may terminate early. This does not require extra data aside from a few variables. (See Christoph's answer as how this could be done just using a traversal of the linked list and destructive "appending" to a new list -- e.g. the nested loops don't have to "feel" like nested loops.) Sort and filter Sort the list (mergesort can be modified to work on linked lists) and then detect duplicate values (they will be side-by-side now). With a good sort this is O(n*lg(n)). The sorting phase usually is/can be destructive (e.g. you have "one copy") but it has been modified ;-) Scan and maintain a look-up Scan the list and as the list is scanned add the values to a lookup. If the lookup already contains said values then there is a duplicate! This approach can be O(n) if the lookup access is O(1). Generally a "hash/dictionary" or "set" is used as the lookup, but if only a limited range of integrals are used then an array will work just fine (e.g. the index is the value). This requires extra storage but no "extra copy" -- at least in the literal reading. For small values of n, big-O is pretty much worthless ;-) Happy coding.
