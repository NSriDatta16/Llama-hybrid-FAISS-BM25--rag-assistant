[site]: crossvalidated
[post_id]: 609803
[parent_id]: 609795
[tags]: 
It's certainly a common idea to reconstruct the data of a study from reported summary statistics. That may or may not provide useful insights. Obviously, often there could be many datasets that could be consistent with what's reported and drawing samples of such datasets given plausible prior assumptions (basically something like Approximate Bayesian Computation aka "ABC" or similar approaches like Bayesian aggregation of average data ) and then treating these datasets like multiple imputations is one way of dealing with that. You're actually in a really good situation, if you have mean vector and covariance matrix, because those are sufficient statistics, if you are willing to assume bivariate normality. So, you could simulate data until you get samples that match the reported summary statistics (up to the reported decimal places for each study). Sure, it might take a while to exactly match, but it is likely pretty doable. I don't think that naively pooling the data would necessarily be appropriate. What would you do if you had the raw data from all the studies (presumably something that allows for some differences between studies and perhaps trying to explain why these differences exist?)? Whatever that analysis, it would also be a serious candidate for what to do with reconstructed synthetic data. Other approaches might include taking characteristics of the studies (or the samples used in the study) and looking at via meta-regression, whether these differences explain different correlations.
