[site]: crossvalidated
[post_id]: 283830
[parent_id]: 66304
[tags]: 
Consider your 2-dimensional SOM artificial neurons units as aiming to have values equal to those of your high-dimensional data. It attains this through the learning process- where a sample (a row of your data) is taken from the data, compared for similarity with each of the units on the map. The unit that comes closer in terms of similarity to the sample becomes the winner of that sample. Then to effect the "learning", the value on the unit is adjusted to be closer to that of the sample it has just won. Units near this winner have their values adjusted too, but with smaller amounts than that of the winner. That adjustments of the units values make the learning to occur. The process is repeated for all samples from the data. At the end of the learning process, you have a learned SOM with units that come closer to resembling your data values. Note that your data values remain intact, they were only read and assisted in conducting the learning process. Now concentrate on the values carried by each unit at the end of the learning process. Each unit may have won several samples from the data and they are now "clustered" around it. That is, several samples from your data can be comfortably represented by one unit of the SOM - this brings in the dimensionality reduction idea! Your 10-dimensinal data can now be visualized as 2-dimensional since similar data in the original dataset can be respresented by one unit of the SOM. For a deeper understanding check out here
