[site]: datascience
[post_id]: 46975
[parent_id]: 46921
[tags]: 
If ImageNet doesn't include the class then it won't be able to do it "out-of-the-box". But of course you can train it to do whatever you want. Essentially, you have used ImageNet for "pretraining": instead of starting from random weights, you start from weights that have been learned from some other tasks. This is called "transfer learning" in the literature. The simplest way this can be done is by (1) training an CNN (which consists of $n$ convolutional layers, followed by $m$ FC layers) on some random task (like ImageNet), (2) throwing away the $m$ FC layers and (optionally) "freezing" the weights of the $n$ conv layers, (3) adding new FC layers onto the end of network and training them on the task you are actually interested in. The reason this is done is that the conv filters learned from doing task 1 have been empirically found to be useful for general tasks ; that is, the output of the pretrained conv layers is a good vector representation of image, which can be used for other tasks. The FC layers on top then learn to do whatever task you want, using that representation. So when you ask Will the network be able to train based on these custom classes ? I would say yes, assuming you are using the paradigm above (i.e., you are fine-tuning VGG), under the assumption that the conv representation is good. As for that question, see part 2. Your question is missing a few potentially useful details. You wrote: I see that, from ```tensorboard`` that the training seems to be normal. However the model fails during inference with new images. Does "the training seems to be normal" just mean the training loss is decreasing nearly monotonically? Do you test on a validation set as well? Are the test (inference) images from the same set as the training images? Are these images similar to ImageNet? One potentially problematic case (where, for example, your pretraining might not be useful) is due to domain shift . This is when the test set is drawn from a different set/distribution than the training set. A multitude of algorithms have been designed to correct for this, called domain adaptation methods. If the training (or even pretraining) images are too different from the images at inference time, such methods are necessary for maintaining good performance. The documentation for flow_from_directory say yes . (Editing based on question update) Yes that seems reasonable. You can try some variations: for example, allow the conv layer weights to change, but use a small learning rate (aka fine-tuning). Or use a more powerful FC multilayer network. Honestly, a difference of 0.03 in accuracy between training and testing is rather minute. I would always expect the network to do at least a little worse on the test set compared to the training set. It also depends on the sizes of the sets. PAC bounds and all that :) I'm not sure what your loss is, but if it's cross-entropy-like, then you can think of the worse loss as the network being "less sure" about its responses (at least informally speaking). I'm not sure what you mean by During Inference if I give the images from training set, model is not able to classify it correctly. The accuracy value suggests it is doing reasonably well, and similar to the training set. If the images are really way different from ImageNet, then fine-tuning the conv weights may be more useful. If your dataset is also very small, looking into the domain adaptation and transfer learning literature may also be helpful.
