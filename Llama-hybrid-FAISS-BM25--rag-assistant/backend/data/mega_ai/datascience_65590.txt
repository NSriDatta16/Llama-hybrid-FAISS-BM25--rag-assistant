[site]: datascience
[post_id]: 65590
[parent_id]: 65577
[tags]: 
When using a RNN, you don't feed all the data at once, you usually have a seq2seq model. The models are created with an encoder-decoder architecture. The LSTM is used in the encoding phase. So, let's say you have a text of 78 words. You will feed the embedding vector (size 300 ) of those 78 words, 1-by-1 into your LSTM and in the end you will get a hidden vector which represents your sentence. Then, you can take this hidden vector and use it for classification (with a feedforward neural network, for example). So, it doesn't matter that you have 4293 unique words in your data. You need to feed your LSTM a sequence of size [ Ã— 300] .
