[site]: crossvalidated
[post_id]: 618730
[parent_id]: 
[tags]: 
Aggregate machine learning results for significance testing

I have a question that deals with assessing significant differences in the predictive performance of different machine learning algorithms. I am predicting different variables with different models and different feature selection strategies (using single variables, using single variables with feature selection, using theoretically derived aggregates of variables). This means that conducting all possible tests would lead to problems of multiple testing. Generally, I would use the corrected paired t-test by Nadeau & Bengio (2000) for comparing algorithm performance, because all algorithms are trained on the same dataset and thus the independence assumption of classical statistical tests do not hold. I think that this is the most accepted procedure for this kind of task (see here for details https://medium.com/analytics-vidhya/using-the-corrected-paired-students-t-test-for-comparing-the-performance-of-machine-learning-dc6529eaa97f ) My question is, would it be valid to aggregate the prediction results for some of the configurations to reduce the number of statistical tests? E.g. if I am just interested in comparing the performance of the algorithms, could I investigate this by averaging the prediction results across e.g. the feature selection strategy for each algorithmn. This would mean that in the corrected paired t-test, I don't use the prediction results of a certain CV-fold but the average of this CV-fold across the 3 feature selection strategies? Would it be a problem, if the subsamples in the folds that are averaged differ? This is something that one can fix. And more general, would this aggregation step be a valid strategy? Greetings, Ole
