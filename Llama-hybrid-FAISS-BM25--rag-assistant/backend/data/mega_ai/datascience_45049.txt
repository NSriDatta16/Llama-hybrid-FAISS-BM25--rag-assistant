[site]: datascience
[post_id]: 45049
[parent_id]: 44880
[tags]: 
Is it true that linear classifiers differ only in the Learning algorithm, but do they do the same during Prediction y = w1*x1 + w2*x2 + ... + c? Yes, all parametric linear classifiers try to predict the weights for the same equation $y = w1*x1 + w2*x2 + ... + c$ and differ in how they try to optimize the cost function. If I used one method for Training (for example SVM with linear kernel function), then can I use other method for prediction (for example Perceptron) with the same output result? You will not be able to achieve this using standard libraries. But, if you code your models from scratch and extract weights from one model after training and use those weights to make a prediction with a different model, you'll get a sufficiently close answer. This is due to the fact that once you have learned the weights $(w1, w2,...., wn)$ , you know the nature of data and can predict the target variable using the equation $y = w1*x1 + w2*x2 + ... + c$ . Note that, this will be true only in the case of parametric linear models as only then the above equation would be applicable. And can I do the same, if I used SVR (Support Vector Regression with linear kernel function) for Training, then can I use Perceptor as linear regression method for Prediction value? Well, yes again with the same restrictions as above. This is because you will get a decision boundary as the output which depends on the data. Note that, there will be minor changes in the output as the error of different models is different. Also, any other differences in the output will be due to the difference in the learning algorithm and how it operates on the data. You can verify this by training two models on the same data and extracting the weights after training. If the weights are close enough (which means you have learned the correct underlying pattern), you can use any of the two models on test data to predict and you will get similar values.
