[site]: crossvalidated
[post_id]: 541439
[parent_id]: 
[tags]: 
Bayesian regression dependencies

I am revising Bayesian linear regression from Bishop's book and the following material . In page 275 it provides the formula for the posterior distribution, which, in Bishop's book is the following: $$ \underbrace{p(\theta|D)}_{\text{posterior}} =\frac{\overbrace{p(\theta)}^{\text{prior}}\overbrace{p(D|\theta)}^{\text{likelihood}}}{\underbrace{p(D)}_{\text{evidence}}}=\frac{\overbrace{p(\theta)}^{\text{prior}}\overbrace{p(D|\theta)}^{\text{likelihood}}}{\underbrace{\int p(\theta)p(D|\theta)d\theta}_{\text{evidence}}} $$ which D represents our data. In the hyperlinked material the same equation it is phrased as: $$p(\mathbf \theta |Y,X) = \frac{p(Y|X,\mathbf{\theta}) p(\mathbf{\theta})}{p(Y|X)}$$ where $\mathbf{x}_{n} \in \mathbb{R}^{D}$ is the one of the vector for the matrix $X$ with the correspondent label to be $y_{n} \in \mathbb{R}$ and $\mathbf{\theta}$ the regression parameters. I guess roughly $D = \{X, Y\}$ . My question is that I am struggling to understand the likelihood in the second equation. Why is it $p(Y|X,\mathbf{\theta})$ and not $p(Y, X| \mathbf{\theta}$ ). The same for the evidence. Also what exactly is the interpretation of $p(Y|X)$ and $p(X|Y)$ .
