[site]: crossvalidated
[post_id]: 271983
[parent_id]: 271979
[tags]: 
The normalization you carry out doesn't affect information loss. What affects the amount of information loss is the number of principal components your create. Assume you have N=100 variables. when you perform PCA, you are attempting to reduce N to n such that n When choosing n, an important consideration is the amount of variance it explains of the original set. For example, with n=10, you could potentially capture 85% of the variance, for n=20, capture 93% of variance and for n=50 capture 95% of variance At this point, you would conclude, that after achieving 93% with the first 20 components, there was very little additional information in the following 30 components. This is where the information loss happens. In exchange for reducing your dimensions from 100 to 20, you sacrifice the final 7% of variance. The good thing however, is that you get to control how much you lose by choosing how many components you want. Hope that makes sense.
