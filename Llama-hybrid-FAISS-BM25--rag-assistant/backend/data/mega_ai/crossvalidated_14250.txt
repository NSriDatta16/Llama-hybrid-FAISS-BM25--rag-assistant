[site]: crossvalidated
[post_id]: 14250
[parent_id]: 14219
[tags]: 
As requested in a comment, here are some pointers for processing steps. A number of tools may be found at the CRAN Task View for Natural Language Processing . You may also want to look at this paper on the tm (text mining) package for R . Prior to processing, consider normalization of the word tokens. openNLP (for which there is an R package) is one route. For text processing, a common pre-processing step is to normalize the data via tf.idf -- term frequency * inverse document frequency - see the Wikipedia entry for more details. There are other more recent normalizations, but this is a bread and butter method, so it's important to know it. You can easily implement it in R: just store (docID, wordID, freq1, freq2) where freq1 is the count of times the word indexed by wordID has appeared in the given document and freq2 is the # of documents in which it appears. No need to store this vector for words that don't appear in a given document. Then, just take freq1 / freq2 and you have your tf.idf value. After calculating the tf.idf values, you can work with the full dimensionality of your data or filter out those words that are essentially uninformative. For instance, any word that appears in only 1 document is not going to give much insight. This may reduce your dimensionality substantially. Given the small # of documents being examined, you may find that reducing to just 1K dimensions is appropriate. I wouldn't both recentering the data (e.g. for PCA), but you can store the data now in a term matrix (where entries are now tf.idf values) with ease, using the sparse matrices, as supported by the Matrix package. At this point, you have a nicely pre-processed dataset. I would recommend proceeding with the tools cited in the CRAN task view or the text mining package. Clustering the data, for instance by projecting onto the first 4 or 6 principal components, could be very interesting to your group when the data is plotted. One other thing: you may find that dimensionality reduction along the lines of PCA (*) can be helpful when using various classification methods, as you are essentially aggregating the related words. The first 10-50 principal components may be all that you need for document classification, given your sample size. (*) Note: PCA is just a first step. It can be very interesting for someone just starting out with text mining and PCA, but you may eventually find that it is a bit of a nuisance for sparse data sets. As a first step, though, take a look at it, especially via the prcomp and princomp functions. Update: I didn't state a preference in this answer - I recommend prcomp rather than princomp .
