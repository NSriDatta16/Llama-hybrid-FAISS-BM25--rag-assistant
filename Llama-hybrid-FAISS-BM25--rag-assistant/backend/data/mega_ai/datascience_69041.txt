[site]: datascience
[post_id]: 69041
[parent_id]: 
[tags]: 
How do Bahdanau - Luong Attentions use Query, Value, Key vectors?

In the latest TensorFlow 2.1 , the tensorflow.keras.layers submodule contains AdditiveAttention() and Attention() layers, implementing Bahdanau and Luong's attentions, respectively. (docs here and here .) These new type of layers require query , value and key inputs (the latest is optional though). However, Query, Value, Key vectors are something I've always read referred to Transformer architectures. What do these vectors represent, when it comes to Bahdanau and Luong attention? For example, if I want to train an RNN model for a common task (let's say time series forecast), what would these inputs represent? EDIT: I'm thinking about a seq2seq to make forecasts. The input would be a series of given length, and a series of external variables. The output would be the series shifted forward of n steps.
