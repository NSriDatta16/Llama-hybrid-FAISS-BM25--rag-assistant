[site]: crossvalidated
[post_id]: 220245
[parent_id]: 
[tags]: 
Why does Geoffrey Hinton say in his Coursera course that gradient magnitudes can vary widely when training Neural Networks?

I was watching his coursera course video on RMSProp, and he said in a paraphrase: Gradient magnitudes vary widely. I was wondering, why is it that they vary widely? I had a guess but wanted to understand what he meant on the video . My guess is: Stochastic Gradient Descent gives it a variance because of its randomness. The activation functions for some reason gave it this property too (not sure why or which ones do and which ones don't) I was wondering if those were the reasons, and even if they were, why and how they contributed to this issue, or were there additional explanations that I was overlooking?
