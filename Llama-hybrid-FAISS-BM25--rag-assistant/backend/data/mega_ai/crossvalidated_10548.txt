[site]: crossvalidated
[post_id]: 10548
[parent_id]: 10497
[tags]: 
You are not using Naive Bayes, you are actually using something I'd call "Multiplicative Decision Stump"-Classifer ;). You can do that, but I'd recommend in this case to calculate the micro or macro-average across all words in the sentence (instead of multiplying them). E.g. macro-average: $p(Positive|sentence)=\frac{1}{n}\sum_{word\in sentence}p(Positive|word)$ I'd set $p$ to $\frac{Positive}{Positive+Negative}$ for calculating the $p(Positive|.)$ and $\frac{Negative}{Positive+Negative}$ for calculating the $p(Negative|.)$ respectively $m$ is the weight of the prior meanwhile the word-count is the weight of the occuring word. The lower $m$ the more importance the probability calculated by word-frequency gets and vice versa. Let's say for example $m$=8 and word-count=2 (for a certain word), than the resulting score will contain of 80% prior-information and 20% non-prior-information. Hence I'd always compare $m$ to the word-count of important words (i.e. those whose yes/no-probability differs strongly from the prior). Unfortunately, there is no "golden hammer"-value for this variable, so I suggest to play around a little bit. If you want to try out Naive Bayes, I suggest the section "Documentation classification" in the wiki-article about Naive Bayes
