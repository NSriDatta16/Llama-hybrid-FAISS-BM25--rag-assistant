[site]: crossvalidated
[post_id]: 522589
[parent_id]: 
[tags]: 
Applying Statistical Inference To Partial Dependence Values -- Is this Valid?

I'm using a Tree based model to understand an outcome, and will likely need to report my results in such a way that it conforms to the types of reporting typically done with linear regression -- ie, p-values for different columns, confidence intervals, etc. So I'm looking into different ways of explaining my results. I'm planning on using permutation feature importance and am looking at SHAP values as well, but I have an unanswered question: is it valid to take partial dependence values and then calculate effect sizes and p-values for them to report the statistical significance and confidence intervals behind the importance of individual values within the dataset? Off the top of my head, I'm inclined to answer yes, but I do not see anyone else doing this and I'm wondering if there is a particular reason why. Let's take an example: Suppose I have a dataset with 3000 observations, and a column for the day of the week with values Monday, Tuesday, Wednesday, Thursday, etc. After running PDP analysis, I find the expected average outcome for each individual day is 22.14, 23.59, 26.14, 21.28, etc. Once we have those values, can't we just then treat these as group means and calculate confidence intervals from the values, standard deviations, and t-stats about the differences between them? The only reason I can think of not to is that the error term from the predictions used to arrive at the PDP values might not be i.i.d. I'm inclined to try this out, but am curious if anyone has thoughts on the issue.
