[site]: datascience
[post_id]: 96982
[parent_id]: 
[tags]: 
XgBoost given targets its only feature but fails when test targets are outside the range of training targets?

I'm learning to use XgBoost, and I'm doing an exercise involving predicting prices. However I'm noticing some weird behavior where XgBoost's predictions deviate from the target value even if I'm passing the target as the model's only feature. This appears to happen when the test prices are significantly higher than the training set prices. This is rather counter intuitive, as I'd imagine the model would always output the feature, but somehow it limits itself. Was wondering if anyone knows why this happens, and if there is someway around this. params = {'gamma': 0.05, 'learning_rate': .05, 'max_depth': 8, 'n_estimators': 100, 'random_state': 10} model = xgb.XGBRegressor(**params, objective='reg:squarederror') model.fit(X_train, y_train, verbose=False) y_pred = model.predict(X_test)
