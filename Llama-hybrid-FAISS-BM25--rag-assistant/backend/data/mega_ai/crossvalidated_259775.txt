[site]: crossvalidated
[post_id]: 259775
[parent_id]: 
[tags]: 
Neural Net for multivariate regression

I need to build a model (M) that converts a 10 dimensional space of inputs (A) into a 20 dimensional space of outputs B. Both the inputs and outputs are analog, so this is not a classification problem but rather a regression one. A * M = B In order to allow some bias I included a last row of 1s in A: $A = \begin{bmatrix} A & 1 \end{bmatrix}$ My first approach was to just to left multiply by the pseudoInverse of A to obtain M: $M=A^{-1}B$ However this is not getting good results. Analyzing what I did, I think it's equivalent to using a one layer neural network with bias but without activation function and without regularization. Now, to improve my model I want to make it more complex using neural networks, but some questions arise. I find a lot of examples in internet using neural networks for classification, where each output neuron represents a class and it's output is associated to the probability of the input belonging to that class. However my problem has nothing to do with classification (think about it as an operator that takes some location in a 10 dimensional space and "moves" it to a new location in a higher dimensional space). Is there some specific neural network topology (type of activation function, regularization, training method) that is specific for this problem? How would you approach such a problem? Thanks a lot.
