[site]: crossvalidated
[post_id]: 474264
[parent_id]: 
[tags]: 
What is GloVe model's loss function actually minimizing?

I am struggling with exactly what is being minimized in the GloVe model. I've read every single blog post, watched every single Youtube video, and some papers that cited GloVe (and of course, read GloVe's paper) and I just couldn't figure out what is actually being minimized in the loss function. For example, most of the references I have found parrots the original paper by writing, Let $X_{ij}$ be a co-occurrence matrix, then the GloVe model learns two word vectors $w_i, w_j$ , by minimizing $$ J = \sum\limits_{i,j = 1}^n f(X_{ij}) (w_i^T w_j + b_i + b_j - \log(X_{ij}))^2 $$ where $b_i, b_j$ are bias and $f$ is a weighting function. First of all, this is an optimization problem, where our loss function is $J$ . But unlike a standard optimization problem, $\min_{x \in \mathbb{R}^n} f(x)$ , no argument of $J$ is supplied, so that we do not know is our objective variable (nor the domain of these parameters, which seems to be crucial, as we need to know the dimension of our embedding space). What are the parameters that are known? By inspection, $X_{ij}$ and $f$ are the only things that are known. Does it mean we are optimizing with respect to all the rest of the parameters $w_i, w_j, b_i, b_j$ ? i.e. taking the gradient wrt each parameters in the gradient descent routine? To solve this problem, how are the parameters initialized for the optimization routine? This is not specified in the original paper What is the interpretation of the optimizers should we find them? I know that $w_i, w_j$ should be the new embedding of the words, but what would an optimal $b_i, b_j$ mean in this context? Is this problem convex at all? Why are we sure that gradient descent will work? Can someone resolve some of these issues for me?
