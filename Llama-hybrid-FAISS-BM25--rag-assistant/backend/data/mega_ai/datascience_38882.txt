[site]: datascience
[post_id]: 38882
[parent_id]: 
[tags]: 
Decent ROC, but horrible Precision-Recall curve

I was working on a model with following process: Split to training/validation/test sets Try a series of different models like GBM, RF, Logistic Regressions Optimize hyper-params on them using GridSearchCV with roc_auc as scoring metric I applied Oversampling methods and was careful that only training folds were oversampled (Imbalance is around 99:1) Tested model on validation set In this example, I'm showing the result of my Random Forest model with SMOTE oversampling, but the result holds for the other models as well. The AUC is decent at around 0.72 and the ROC looks like following: However, the Precision-Recall curve looks horrible: I'm not sure why I'm running into this issue, but would appreciate any insight or suggestions on how I could change my approach or refine it.
