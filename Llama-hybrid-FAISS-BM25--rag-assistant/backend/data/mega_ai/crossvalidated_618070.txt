[site]: crossvalidated
[post_id]: 618070
[parent_id]: 
[tags]: 
Why the positional embeddings for a specific position i, for each embedding element is different

for this formulas PE(pos,2i)=sin(pos/(10000^(2i/modelDimension))) and PE(pos,2i+1)=cos(pos/(10000^(2i/modelDimension))) we know PE for position i follows: [sin(i/denominator[0]),cos(i/denominator[0]), sin(i/denominator[1]),cos(i/denominator[1]), sin(i/denominator[2]),cos(i/denominator[2]),...] . note each element here is a proposed positional embedding element for an embedding element. My question is why the positional embeddings for a specific position i, for each embedding element is different? how does it make sense to shift each embedding value to a different place in embedding space, for a specific position i? in other words, why they treat each positional embedding along embeddings dimension differently ? they all belong to same position , so why they are not assigned to the same value (note the denominator elements (denominator[0],denominator[1],...) are different for each embedding element) I also understand that these are learnable parameters, which I think they shouldnt be, because this way may lead to overfitting, but that other whole story, but anyway we should try to make first meaningful assignments to them, rather expect the network to learn it by itself. in general are there better positional embeddings which make more reasonable sense?
