[site]: crossvalidated
[post_id]: 544506
[parent_id]: 544499
[tags]: 
With Normalization If we start with the assumption that an object can belong to exactly one class, I think we still can compute the OR probability. Let's normalize the probabilities first, to have their sum as $1$ . probs = np.array([0.31, 0.45, 0.89, 0.02, 0.43, 0.21, 0.96, 0.32]) probs = probs / probs.sum() probs # array([0.08635097, 0.12534819, 0.24791086, 0.00557103, 0.11977716, # 0.05849582, 0.26740947, 0.08913649]) s.t., $P(C_0 \cup C_1) = P(C_0) + P(C_1)$ , and the events that an observation belongs to the output classes are mutually exclusive (so that an observation can belong to exactly one class) and $P(C_0 \; \text{OR} \; C_1)$ can be found with probs[0] + probs[1] # 0.2116991643454039 and $P(C_i \; \text{AND} \; C_j) = P(C_i \cap C_j) = 0, \quad i \neq j$ Multilabel Classification With Label Powerset This is a classic approach to deal with multi-label classification, which is used to transform a multi-label problem to multi-class problem. In this approach, each possible combinations of outputs is treated as a single unique label, i.e., using a mapping from the power set of $C$ to the indices, $2^C \to \{1,2,\ldots,2^{|C|}\}$ , where $C=\{C_1,C_2,\ldots,C_k\}$ is set of all possible class labels. This method allows to compute $P(C_i \cap C_j)$ directly. The following example demonstrates the idea, using skmultilearn 's implementation of LabelPowerset . # generate a sample multilabel dataset from sklearn.datasets import make_multilabel_classification from skmultilearn.problem_transform import LabelPowerset from sklearn.ensemble import RandomForestClassifier import numpy.random as random from scipy import sparse random.seed(123) X, y = make_multilabel_classification(n_classes=8, n_labels=2, # average no. of labels per instance allow_unlabeled=False, random_state=1) y[0:2] # array([[0, 0, 0, 1, 0, 0, 0, 1], # [0, 1, 0, 1, 0, 0, 1, 1]]) classifier = LabelPowerset( classifier = RandomForestClassifier(), require_dense = [False, True] ) classifier.fit(X, y) classifier.unique_combinations_ # {'1': 0, # '7': 1, # '1,3,7': 2, # '0,6': 3, # '3': 4, # '0': 5, # '6': 6, # '0,1,7': 7, # '1,4,7': 8, # '0,4': 9, # '4': 10, # '1,3,4,7': 11, # '1,6': 12, # '0,5': 13, # '1,3': 14, # '3,4': 15, # '1,4': 16, # '1,7': 17, # '0,1': 18, # '0,1,3': 19, # '1,3,5': 20, # '3,6,7': 21, # '5,7': 22, # '0,3': 23, # '1,6,7': 24, # '0,6,7': 25, # '3,5': 26, # '3,7': 27} def get_OR_prob(X, classes=None): pred = classifier.classifier.predict_proba(X.reshape(1,-1))[0] for combination_id in range(len(pred)): if sorted(classes) == sorted(classifier.reverse_combinations_[combination_id]): return pred[combination_id] return 0 Compute $P(C_0 \cap C_6)$ and $P(C_0 \cap C_1 \cap C_6)$ : get_OR_prob(X[2], [0,6]) # 0.19 get_OR_prob(X[2], [0,1,6]) # 0
