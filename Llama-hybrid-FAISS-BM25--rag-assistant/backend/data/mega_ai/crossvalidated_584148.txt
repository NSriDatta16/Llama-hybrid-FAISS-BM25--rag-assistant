[site]: crossvalidated
[post_id]: 584148
[parent_id]: 
[tags]: 
What is the correct way of normalizing/standardizing image-like data?

I have image-like data (e.g. H x W x C), where each channel contains quite different information. You can think of it being a 2D map (H x W) with information like elevation, wind velocity, temperature as channels (C). This data is fed into a neural network to make predictions. IMHO it is common sense that (for most cases) the input should follow a Gaussian with mean zero and unit variance. We see that in every MNIST example where we standardize img = (img - mean) / std . The channels in my data follow quite different distributions, some of which carry exact meaning, e.g. wind velocity is given in [m/s]. Some of them are also correlated, e.g. velocities in x and y direction. Just feeding in everything without any normalization/standardization does not yield any results I can be proud of, yet normalizing them in a naive way should not work well. What is the best way to normalize these data? The approaches on top of my head: Standardize per channel. Might destroy some correlations due to slightly different distributions. Standardize per "logical units", e.g. one operation for temperature, one for velocities in x and y (calculate mean,std over both channels). Some channels might not follow a zero mean and unit variance then. Standardize over entire data. Sounds like the worst of all worlds, but what do I know. Instead of input standardization, use LayerNorm in the first network layer(s?). Here we have at least two options again (see 1./2.).
