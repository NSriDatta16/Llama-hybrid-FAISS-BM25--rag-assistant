[site]: crossvalidated
[post_id]: 226626
[parent_id]: 
[tags]: 
Bayesian nonparametrics vs model selection using Minimum Message Length

As we know mixture models are important tools in density estimation and in general in statistical machine learning. I have always used nonparametric Bayesian mixture models to avoid the problem of finding the optimal number of mixture ecomponents. Considering the fact that mixture models with nonparametric priors are quite old and well known, why is that I still see papers that use finite mixture models and then use a model selection method to find the best model. Do these methods offer other advantages over nonparametric models? In particular, could anyone please let me know if the Minimum Message Length method have an advantage over using a nonparametric prior such as Dirichlet process?
