[site]: crossvalidated
[post_id]: 533915
[parent_id]: 
[tags]: 
Bootstrap and semantics: selecting samples before combining them

I have datasets (1D float arrays) from numerical model runs. This model has some internal variability, and I run it several times (let's say $n_1$ ) to assess that. Now what I'm really interested in is the combinations of model results, where the model is run with different configurations, assembled with determined weights. Let's say I have $n_2$ of these different configurations. The number of elements in a dataset varies slightly across the $n_1$ realisations of a specific configuration, but may vary over orders of magnitude for different configurations. I compute the statistics I'm interested in by randomly selecting one output among the $n_1$ same-configuration runs for each of the $n_2$ different-configuration runs, before combining. I can do that, say, $n_3$ times, to get a distribution of the statistics. Would that fall under the category of bootstrap methods? Are there some theoretical results to back that up? Recommendations on picking $n_3$ with respect to $n_1$ and $n_2$ ? I understand that a "proper" bootstrap would be more in the lines of randomly re-sampling among one of my $n_1$ runs before combining; I would still need a way to select that one run, though. A way to bypass that could be to flatten my $n_1$ arrays into one large array, and sample a given number of realisations (e.g. the average number across the $n_1$ arrays) from it. I would take advice on that as well, or any other suggestions.
