[site]: datascience
[post_id]: 96285
[parent_id]: 
[tags]: 
Struggling to understand/implement Transformer Decoder

I'm struggling to understand the decoder in a Transformer model, specifically with regards to some aspects of its architecture as well as how it actually handles the data during training. What I have right now is a working implementation of the encoder which outputs some calculated self-attention scores (using scaled dot-product for my attention mechanism). In my decoder, I understand that the correct "target" output is first embedded & positionally encoded (which I have implemented). I then added a layer of self-attention to the decoder. From what I understand, what follows is the encoder-decoder attention blocks. Firstly, should the self-attention layer be included in the decoder block to be repeated alongside the encoder-decoder attention layer, or should this just occur once after the target is passed into the decoder? Next, for the actual encoder-decoder attention, I am a bit uncertain as to how the query, key, and value vectors are calculated. From my understanding, the output of the encoder is passed through a feedforward layer to calculate the key and value vectors, whereas the output of the previous block of the decoder is passed through a feedforward layer to calculate the query vector. Thus, my implementation is as follows: x = self.encoder(x) y = self.decoder_self_attention(y) for decoder_block in self.decoder_blocks: y = decoder_block(x, y) This makes mathematical sense to me (the desired output should have the same embedding dimensionality as the input to the decoder block, so the query vector has to be from the decoder as it is the leftmost matrix in the dot-product attention calculation). However, I fail to understand how this allows it to handle recurrent data. This is especially confusing as the final layer is a feedforward layer, and I fail to understand how this would handle sequences rather than say, an LSTM. In addition, I have read very conflicting information on when the masking operation occurs. Some papers perform masking on the embedding directly, whereas some perform it after positional encoding, whereas some implement it in the attention mechanism. My model simply adds the mask to the cross product of the query and key vectors during the cross product calculation (before softmax), but I'm unsure how to verify this is correct. z = (q @ k.transpose(-2, -1)) * scale # Q K^T / sqrt(d) z += self.mask # before softmax, set everything above diagonal to -inf Lastly, although I am certain I'm failing to understand some aspect of the decoder causing a bug, I am unable to get my model to fit onto data of different modalities. My input embedding is of shape $(T_a, E_a)$ where $T_a$ is the number of tokens and $E_a$ is the embedding dimension. However, my output dimension is of shape $(T_b, E_b)$ where $T_b \neq T_a$ and $E_a \neq E_b$ . While mathematically this should be fine with respect to the attention mechanism, using the default PyTorch Encoder and Decoder layers throws a multitude of errors unless $T_a = T_b$ and $E_a = E_b$ , which leads me to believe there may be issues when the input and output sequence are of different modalities. Although my model does converge loss-wise, when I pass in training data and translate the output back into tokens, the output is absolutely nonsensical. I have checked the exact same dataset against a more traditional LSTM auto-encoder and gotten sensible results, which leads me to believe I may simply have misunderstood some pieces of the transformer architecture.
