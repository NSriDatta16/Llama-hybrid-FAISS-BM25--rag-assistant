[site]: crossvalidated
[post_id]: 110981
[parent_id]: 
[tags]: 
why is adaboost predicting probabilities with so little standard deviation?

I'm using several algorithms to predict a binary target. So far I tried Gradient Boosting, Random Forest, Extra Random Trees and adaboost from scikit learn. All of these algorithms appear to predict probabilities ranging from close to zero to close to 1 with a very similar standard deviation. adaboost is the only one whose predictions are mostly compressed in the 0.4 to 0.6 range with only a minority falling outside of that range. This is not the first time I notice this behavior from this algorithm. Why is that the case? Secondly, if I wanted to blend these models (i.e average the probabilities), how would I account for the fact that adaboost probabilities's standard deviation is so different from any other algorithm? Should I rescale all predicted values from each algorithm to have similar mean and variation?
