[site]: crossvalidated
[post_id]: 497636
[parent_id]: 497622
[tags]: 
However, I never came across it in the literature. There is a lot of literature on fitting distributions. Think for instance about Pearson's method of moments and chi-squared test which is already more than a hundred years old. Fitting by optimizing the likelihood, the maximum likelihood , is also a method that is (just) more than a hundred years old. In addition Pearson's chi-squared test is finding replacement by the G-test , which is based on likelihood. Regularised maximum likelihood methods are neither uncommon. Model selection methods that use values like BIC or AIC can be considered regularised likelihood regression (where the regularisation parameter is $\Vert \beta \Vert_0$ ). Thus it might be a matter of terminology that you do not read much about regularised maximum likelihood methods . Another related concept is Bayesian regression. The maximum a posteriori estimate could be considered as a regularised maximum likelihood. Your second reason, computational complexity, might also be a reason why maximum likelihood is not always used. The same is true for unregularised regression where using a simple linear estimator is often preferred (which is optimised when minimizing least squares residuals). The first reason might also be true. An advantage of least squares is indeed that it works independent of the underlying distribution. But both these reasons are reasons why regularised likelihood may be not often used. But they are not reasons why you can not find anything about it in the literature.
