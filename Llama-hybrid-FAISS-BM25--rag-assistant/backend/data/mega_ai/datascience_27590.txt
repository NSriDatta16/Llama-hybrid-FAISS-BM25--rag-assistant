[site]: datascience
[post_id]: 27590
[parent_id]: 27422
[tags]: 
This question will invite a lot of opinions and it will be very specific to the problem that you are looking at. But there's no harm in laying out the popular options - 1. Start with the data a. Images - If you are working with images and plan to use a classifier like ConvNet , PCA probably isnt a very good idea. You should just resize the image using popular libs like opencv b. Unstructured Text - I have personally looked at a lot of free text problems, and my goto method is using Cosine Distance after doing a TruncatedSVD (dimensionality reduction) on the bag-of-words data (or any way of structuring for that matter) c. Structured - Usually PCA is the go-to solution, but only if you are ok with losing the interpretability in your downstream predictive models. Scree Plots are a good way to evaluate how good a PCA exercise was / how many Principal Components you need. 2. Methods a. Auto-encoders - These are neural-network based dimensionality reduction technique and I haven't looked at a single production instance where in a predictive modelling workflow Auto-encoders outperformed PCA. I guess we just have to wait for the right use-case b. Distance metrics - Euclidean is no-doubt the most popular one, but Cosine catches up well to free text problems. You also have Hamming distance, Mahalanobis distance and countless others but its really upto you to try that. I haven't come across a case where these were performing better. While we are talking about distance metrics, also look at DBScan for clustering. Also, Silhouette Distance is another effective way of measuring how good your clusters did.
