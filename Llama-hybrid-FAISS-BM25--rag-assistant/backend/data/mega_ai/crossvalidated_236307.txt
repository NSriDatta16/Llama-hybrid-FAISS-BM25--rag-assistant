[site]: crossvalidated
[post_id]: 236307
[parent_id]: 
[tags]: 
Processing data with different number of features

I have this classification/regression task but the most interesting thing is that the number of features for each record is different. Features are already extracted and already prepared thus the context of the data is unknown, and the values of the features fluctuate from -10 to 10. There are records with more than 200 features, likewise there are records with the amount of features lower than 20. The dataframe df has two columns: ID and ATTRIBUTES and the output looks like this: ID ATTRIBUTES 0 1 1.1 2.1 3.3 4.4 5.5 6.6 ... 99.9 100.0 101.1 102.2 1 2 1.1 2.1 3.3 4.4 5.5 6.6 ... 45.0 46.0 47.0 49.0 2 3 1.1 2.1 3.3 4.4 5.5 6.6 ... 9.0 10.0 11.0 12.0 3 4 1.1 2.1 3.3 4.4 5.5 6.6 ... 70.0 71.0 72.0 73.0 4 5 1.1 2.1 3.3 4.4 5.5 6.6 ... 131.0 132.0 134.0 135.0 I have split column ATTRIBUTES into separate columns: df['ATTRIBUTES'].str.split(' ', expand=True).astype(float) Now df looks like this: 0 1 2 3 4 5 6 7 8 9 ... 131 132 133 134 135 0 1.1 2.1 3.3 4.4. 5.5. 6.6. 7.7 8.8 9.9 ... NaN NaN NaN NaN NaN 1 1.1 2.1 3.3 4.4. 5.5. 6.6. 7.7 8.8 9.9 ... NaN NaN NaN NaN NaN 2 1.1 2.1 3.3 4.4. 5.5. 6.6. 7.7 8.8 9.9 ... NaN NaN NaN NaN NaN 3 1.1 2.1 3.3 4.4. 5.5. 6.6. 7.7 8.8 9.9 ... NaN NaN NaN NaN NaN 4 1.1 2.1 3.3 4.4. 5.5. 6.6. 7.7 8.8 9.9 ... 131.0 132.0 133.0 134.0 135.0 Let's say record1 has 102 features, rec2 - 49, rec3- 12, rec4-73, rec5 - 135. After a split operation records rec1, rec2, rec3, rec4 were populated with NaN values to fill dataframe. The label ( answer ) column is a single column of real values (float) which variate from -10 to 10: >>> answer 0 8.8 1 -5.3 2 8.8 3 5.4 4 9.7 Iâ€™ve used LabelEncoder() to label answer column: from sklearn.preprocessing import LabelEncoder lbl_Encoder = LabelEncoder() lbl_Encoder.fit(answer) answer_labeled = lbl_Encoder.transform(answer) >>> answer_labeled array([1, 2, 1, 3, 4]) After some googling I've came up with following ideas: First thought was to change NaN values with meaningful features using Imputer ; Discard records that have less than 20 (40, 60 etc.) features. For classification I had chosen RandomForest. The baseline performance is approximately 0.4117 whilst validating 10% of training set (using scikit's train_test_split). Despite everything I had tried: Feature scaling - standardisation Dimensionality reduction via Principal Component Analysis (PCA) Tree-based feature selection using For baseline performance I used following: from sklearn.dummy import DummyClassifier clf_dummy = DummyClassifier(strategy='most_frequent',random_state=SEED) clf_dummy.fit(X_train, y_train) clf_dummy.score(X_test, y_test) >>> clf_dummy.score(X_test, y_test) 0.33062330623306235 The performance of RandomForest scored approximately 0.4. I have tried to apply various configurations to RF, but the score has not been improved, therefore additional data processing should be applied, probably. So my question is - how would one should proceed with the lack of features for particular records?
