[site]: crossvalidated
[post_id]: 224409
[parent_id]: 224200
[tags]: 
This is the solution for the problem above: In brief, for my case, the heteroskedasticity is caused by at least two different sources: Group's differences which OLS and all the family of "mono-level" regression model hardly can account for; Wrong specification of the model functional form: more in detail (as suggested by @Robert Long in first place) the relation between the DV and the covariates is not linear. For what concerns the group differences causing heteroskedasticity it has been of great help running analysis on truncated data for single groups, and acknowledge from the BP-test that heteroskedasticity was gone almost in all groups when considered singularly. By fitting a random intercept model the error structure has improved, but as noted by the commentors above heteroskedasticity could still be detected. Even after including a variable in the random part of the equation which has been able to improve the error structure even more, the problem could not be considered solved. (This key variable, coping strategies, well describes habits of household in case of food shortages, indeed these habits usually vary much across geographical regions and ethnic groups.) Here comes the second point, the most important. The relation between DV (as it is originally) and covariates is not linear. More options are available at this stage: Use a non linear model to explicitly take into account for the issue; Transform the DV, if you can find a suitable transformation. In my case square root of DV. Try using models that do not make any assumption on the distribution of the error term (glm family). In my view, the first option complicates a bit the interpretation of the coefficients (is a personal project-dependent observation just because I want to keep things simple for this article) and at least from my (recent) experiences, needs more computational power which for complicated models with many random coefficients and observations could bring R to crash. Transforming the DV is surely the best solution, if it works and if you are more lucky than me. What do I mean? In case of log transformed DV the interpretation would be in terms of percentage, but what about the square root transformation? How can I compare my results with other studies? Maybe a standardization of the transformed variable could help in interpreting the results in z-scores. In my opinion is just too much. About the glm or glmm models I cannot say much, in my case none of those worked, glm does not properly account for random differences between groups and the output of glmm reported convergence problems. Note that for my model the transformation of the DV does not work with OLS either for the same reason regarding glm above. However, there is at least one option left: assigning weights to the regression in order to correct for the heteroskedasticity without transforming the DV. Ergo: simple interpretation of the coeff.s. This is the result obtained by weigthing with DV_sqrt while using the un-transformed DV in a random coefficient model. At this stage I can compare my cofficients' standard errors with their counterpart from the robust estimator. Regarding the direct use of robust estimators in case as mine without trying understanding the source of the problem, I would like to suggest this reading: G. King, M. E. Roberts (2014), "How Robust Standard Errors Expose Methodological Problems They Do Not Fix, and What to Do About It".
