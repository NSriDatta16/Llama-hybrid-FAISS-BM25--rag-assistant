[site]: crossvalidated
[post_id]: 260718
[parent_id]: 260711
[tags]: 
Notationally: $P(\cdot)$ is a functional to denote probability of events. The same notation is used when speaking of probability in a frequentist framework (e.g. probability is a frequency of events observed in infinite replications of the universe, or counterfactual probability) as in a Bayesian framework (e.g. probability is a degree of belief). The history about $p$-values seems to date back to De Moivre in the 18th century according to Wikipedia. Pearson used the capital $P$ to denote a measure of inconsistency of an observed set of data from a hypothesis which is to be tested using a test statistic which assumes a known distribution when that hypothesis is true. Modern usage has reverted to lower case $p$ more often than not, I find, because the $p$ value is not a random variable, a type of distinction which is also somewhat antiquated in modern probability theory. I think you may find for submitting statistical research that most journals use lowercase $p$ but there may be instances of $P$, the only recommendation is to agree on one usage and be consistent. There is no $p$ for Bayesian statistics. Bayesian testing is a controversial subject, but all agree that the $p$-value should immediately be bucked when doing a Bayesian analysis. My personal preference is to report the results of a Bayesian analysis using credible intervals which provide a range of plausible (believable) effects for the parameter. Bayes factors can summarize tests in a manner similar to $p$-values, but $p$-values suck, why would you use them if you're doing a Bayesian analysis?
