[site]: crossvalidated
[post_id]: 640003
[parent_id]: 
[tags]: 
Neural Network with 1 hidden layer with ReLU modeling capabilities

I've been doing Andrew Ng's Machine Learning Specialization on Coursera, there's a lab in which he uses 1 hidden layer with ReLU to show how it enables models to stitch together linear segments to model complex non-linear functions, and this graph got me thinking: Let's say you have a function like this one but with just the last 3 segments It is possible to model it with the next function? f(x) = g(w1*x + b1) + g(w2*x + b2) + g(w3*x + b3) g(x) = max(0,x) [ReLU function] I’m specifically referring to the parts where the slope decreases, I don’t think it can be done because you’re doing addition, and ReLU makes all functions that you’re adding always >= 0 , it makes me think that you can only increase the slope, or keep it constant, with the consecutive functions that you’re adding, am I missing something? (I already asked in the Course's forum with no successful response)
