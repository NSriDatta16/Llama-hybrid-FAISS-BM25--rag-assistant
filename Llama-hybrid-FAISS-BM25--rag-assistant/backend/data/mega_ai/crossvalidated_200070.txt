[site]: crossvalidated
[post_id]: 200070
[parent_id]: 
[tags]: 
In general, does normalization mean to normalize the samples or features?

I'm just getting into machine learning, and I have seen two conflicting practices for normalization. To be concrete, let's suppose that we have a $n \times d$ matrix containing our training data, where $n$ is the number of samples and $d$ is the number of features. When people say that they normalize their data before running whatever algorithm, I have seen that they do one of the following things: normalize the columns of the data matrix so that $A_{1,i}^2 + A_{2, i}^2 + \cdots + A_{n, i}^2 = 1$ for each feature $i$ normalize the rows of the matrix so that each sample vector has the same norm In general , when someone refers to normalization of data, which of the following are they referring to? I was under the impression that it was the first one (seems to make the most sense to me), but looking at the documentation for sklearn's preprocessing library, it appears that the default behavior is the second one. This doesn't make sense to me.
