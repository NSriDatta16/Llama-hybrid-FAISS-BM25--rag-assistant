[site]: crossvalidated
[post_id]: 88982
[parent_id]: 88975
[tags]: 
The O($N^3$) training complexity involves $n^2$ dot products and $n^3$ inverse of kernel matrix ( A.Bordes et al Fast Kernel Classiﬁers with Online and Active Learning ). However, it is also shown that the runtime of linear SVM optimization may decrease as the training size $N$ increases ( Shai Shalev-Shwartz et al. SVM Optimization: Inverse Dependence on Training Set Size ). Mostly people view the training complexity independent of feature number, yet in SVM with RBF kernel, the training complexity is regarded as $O(dN^2)$ or $O(dN^3)$ ,where $d$ is the feature number (dimensionality)( Sreekanth Vempati et al.Generalized RBF feature maps for Efﬁcient Detection ). While reducing the feature number might help in reducing the training complexity, the aim of feature selection is mainly to remove the additional features that may not help separate the classes in the feature space (sometimes may even at the risk of learning the noise in the data) ( J. Weston et al Feature Selection for SVMs ).
