[site]: crossvalidated
[post_id]: 279736
[parent_id]: 278916
[tags]: 
It would help to know what's the nature of your data, to be able to say whether your featurespace is too large or not. For example, for simple object classification tasks, 32 is quite a high number of features (also known as the dimension of the featurespace), but for image data, this may not be true. In general 41 is not enough data points for anything but very simple classification tasks. Read about the Curse of Dimensionality . The short version is that while a higher dimensionality captures more fine-grained data, it also needs more data points to learn from, otherwise it ends up overfitting to the training set. Secondly, it is good to know what the best-first search strategy does (from this page ): Searches the space of attribute subsets by greedy hillclimbing augmented with a backtracking facility. Setting the number of consecutive non-improving nodes allowed controls the level of backtracking done. Best first may start with the empty set of attributes and search forward, or start with the full set of attributes and search backward, or start at any point and search in both directions (by considering all possible single attribute additions and deletions at a given point). It does not compute all possible subsets, but selects features in a greedy manner, adding them to a subset (or deleting them from the set) if it improves accuracy. You can try setting the feature selection method to Exhaustive, to perform a complete search of all feature subsets. Since you performed 10-fold cross validation, and if you are indeed reporting the average error across all 10 folds, it doesn't immediately indicate overfitting. But then again, you have very few data points, and it is possible the splits were not well done. Let us assume for the moment that it is not overfitting. Then why might you see a low info gain/correlation score? Here is an example of a simple classification task: Outlook Temperature Humidity Windy Play Sunny Hot High False No Sunny Hot Normal True No Rainy Mild High False Yes Rainy Hot High True No Rainy Cool Normal True No You can quickly eyeball this and observe that Windy's value doesn't have much to do with whether you go out to Play or not. In this case, the info gain of Windy as a feature may be abysmally low. But taken in conjunction with Rainy, you see that when it's rainy and windy, you don't go out to play. Essentially, when the metrics (correlation/info gain) report utility values of the features, they do that for each feature independently. Sometimes the features on their own are not useful but combined with other features, can be. This explains why you observe a low info gain value, yet a high accuracy values with those features. This article is a good read on feature selection using Weka. What you want to do is select the features that give a high score on these metrics on their own. But even then, since different metrics give different results, you will need to perform some trial and error to ensure that you have the right set of features. Maybe you can choose the features that all the metrics agree upon, and then try adding other features one by one, to see whether that improves the accuracy or not.
