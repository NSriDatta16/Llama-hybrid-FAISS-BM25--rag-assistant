[site]: datascience
[post_id]: 32460
[parent_id]: 
[tags]: 
NN embedding layer

Several neural network libraries such as tensorflow and pytorch offer an Embedding layer. Having implemented word2vec in the past, I understand the reasoning behind wanting a lower dimensional representation. However, it would seem the embedding layer is just a linear layer. All other things being equal, would an embedding layer not just learn the same weights as the equivalent linear layer? If so, then what are the advantages of using an embedding layer? In the case of word2vec, the lower dimensional representation can be used for other tasks (the infamous king/queen example). However, if your embedding layer will never be used for alternative tasks, what would be its purpose?
