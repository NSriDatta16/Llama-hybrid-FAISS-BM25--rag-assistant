[site]: crossvalidated
[post_id]: 436497
[parent_id]: 436327
[tags]: 
Your confusion might stem from the fact that for KNNs the workload shifts from training to predicting: Training a KNN model means basically just loading the training data (as long as you do not do anything fancy like creating a hash table for more efficient neigbor lookup later). There is no optimization, no gradient descent, no weight adjustments etc. However, when doing a prediction you need to solve an optimization problem to find the $K$ nearest neighbors! So that is where the "magic" is happening. Using a method like logistic regression has the opposite workload distribution: Training the model means solving an optimization problem to define your weights. However, doing the prediction is pretty simple. Just plug in the numbers and calcuate the result. Nevertheless both methods have a training and a prediction phase. And accordingly there is conceptually no difference when using cross validation. You can do it to compare KNNs for different levels of $K$ just as you would do it for different levels of $max depth$ with a decision tree. And you can use cross validation to compare your KNN to that decision tree or the logistic regression model you have trained.
