[site]: datascience
[post_id]: 102192
[parent_id]: 102172
[tags]: 
A few comments: I don't know this dataset but it seems to be a difficult one to classify since the performance is not much better than a random baseline (the random baseline in binary classification gives 50% accuracy, since it guesses right half the time). If I'm not mistaken the majority class (class 1) has 141 instances out of 252, i.e. 56% (btw the numbers are not easily readable in the matrices). This means that a classifier which automatically assigns class 1 would reach 56% accuracy. This is called the majority baseline, this is usually the minimal performance one wants to reach with a binary classifier. The LR and LDA classifiers are worse than this, so practically they don't really work. The k-NN classifier appears to give better results indeed, and importantly above 56% so it actually "learns" something useful. It's a bit strange that the first 2 classifers predict class 0 more often than class 1. It looks as if the training set and test set don't have the same distribution. the k-NN classifier correcly predicts class 1 more often, and that's why it works better. k-NN is also much less sensitive to the data distribution: in case it differs between training and test set, this could explain the difference with the first 2 classifiers. However it's rarely meaningful for the $k$ in $k$ -NN to be this high (125). Normally it should be a low value, like one digit only. I'm not sure what this means in this case. Suggestion: you could try some more robust classifiers like decision trees (or random forests) or SVM.
