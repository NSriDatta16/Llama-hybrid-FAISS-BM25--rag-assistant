[site]: crossvalidated
[post_id]: 207555
[parent_id]: 207484
[tags]: 
I have figured out how to perform the required conversion. Assume that the inputs $X$ are standardised (zero mean, unit variance) and the responses $y$ are centered. We know that the modified LARS algorithm provides the full LASSO regularisation path, cf. original paper by Efron et al . This means that, at each iteration $k$, the former algorithm finds an optimal couple $(\beta^*, \lambda^*)$ minimising the regularised loss function: \begin{align} (\beta^*, \lambda^*) &= \text{argmin}_{(\beta,\lambda)} L(\beta,\lambda) \\ L(\beta,\lambda) &= \Vert y-X\beta \Vert_2^2 + \lambda \Vert \beta \Vert_1 \\ &= \sum_{i=1}^N \left(y_i - \sum_{j=1}^p \beta_j X_{ij}\right)^2 + \lambda \sum_{j=1}^p \vert \beta_j \vert \end{align} For all active components $a=\{1,...,q\}$ in the active set $\mathcal{A}_k$ at the end of step $k$, applying the KKT stationarity condition gives \begin{align} 0 &= \frac{\partial L}{\partial \beta_a}(\beta^*,\lambda^*) \\ &= -2 \sum_{i=1}^N X_{ia} \left(y_i - \sum_{j=1}^q \beta_j^* X_{ij}\right) + \lambda^*\ \text{sign}(\beta_a^*) \end{align} In other words $$ \lambda^* = 2 \frac{\sum_{i=1}^N X_{ia} \left(y_i - \sum_{j=1}^q \beta_j^* X_{ij}\right)}{\text{sign}(\beta_a^*)} $$ or in matrix notations (noting that dividing/multiplying by $\text {sign}(x)$ is the same) the following equation is satisfied for any active component $a$ : $$ \lambda^* = 2 \ \text{sign}(\beta_a^*) X_a^T r $$ In the original paper, authors mention that for any solution to the LASSO problem, the sign of an active regression weight ($\beta_a^*$) should be identical to the sign of the corresponding active predictor's correlation with the current regression residual ($X_a^T r$), which is only logic since $\lambda^*$ must be positive. Thus we can also write: $$ \lambda^* = 2 \vert X_a^T r \vert $$ In addition, we see that at the final step $k$ (OLS fit, $\beta^* = (X^TX)^{-1}X^T y $), we get $\lambda^* = 0$ due to the orthogonality lemma. The use of the median in the MATLAB implementation I found IMHO seems like an effort to 'average out' numerical errors over all the active components: $$ \lambda^* = \text{median}( 2 \vert X_{\mathcal{A}_k}^T r_{\mathcal{A}_k} \vert ),\ \ \ \forall k > 1$$ To compute the value of $\lambda$ when there are no active components (step $k=1$), one can use the same trick as above but in the infinitesimal limit where all regression weights are zero and only the sign of the first component $b$ to become active (at step $k=2$) matters. This yields: $$ \lambda^* = 2 \ \text{sign}(\beta_b^*) X_b^T y $$ which is strictly equivalent to $$ \lambda^* = \max(2 \vert X^T y \vert), \text { for } k=1$$ because (i) same remark as earlier concerning the sign of regression weights; (ii) the LARS algorithm determines the next component $b$ to enter the active set as the one which is the most correlated with the current residual , which at step $k=1$ is simply $y$.
