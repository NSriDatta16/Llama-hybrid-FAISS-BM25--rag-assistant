[site]: datascience
[post_id]: 47160
[parent_id]: 47142
[tags]: 
No. Gradient descent is used in optimization algorithms that use the gradient as the basis of its step movement. Adam , Adagrad , and RMSProp all use some form of gradient descent, however they do not make up every optimizer. Evolutionary algorithms such as Particle Swarm Optimization and Genetic Algorithms are inspired by natural phenomena do not use gradients. Other algorithms, such as Bayesian Optimization , draw inspiration from statistics. Check out this visualization of Bayesian Optimization in action: There are also a few algorithms that combine concepts from evolutionary and gradient-based optimization. Non-derivative based optimization algorithms can be especially useful in irregular non-convex cost functions, non-differentiable cost functions, or cost functions that have a different left or right derivative . To understand why one may choose a non-derivative based optimization algorithm. Take a look at the Rastrigin benchmark function . Gradient based optimization is not well suited for optimizing functions with so many local minima.
