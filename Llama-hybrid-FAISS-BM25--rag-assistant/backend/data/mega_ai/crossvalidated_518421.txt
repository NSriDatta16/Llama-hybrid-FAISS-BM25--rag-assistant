[site]: crossvalidated
[post_id]: 518421
[parent_id]: 427331
[tags]: 
At first, we need to explore basics of DL-based voice synthesis models. A typical end-to-end speech synthesis task is split into 2 subtasks: text -> features representation and features -> speech , the latest one is called vocoder in the literature. Tacotron2 is an example of model that addresses the first subtask. It maps character embeddings to mel-scale spectrograms and has around 13M parameters with default setting ( source ) . WaveNet is a vocoder. It could synthesize time-domain waveforms from spectrograms made by Tacotron2 . The original paper does not mention details such as number of layers, stacks were used in their experiments. Number of parameters may differ depending on setup. I used this wavenet_vocoder implementation with following hyperparameters to calculate the approximate number of parameters (~3.7M): out_channels=10 * 3 layers=24 stacks=4 residual_channels=128 gate_channels=256 skip_out_channels=128 from train import build_model model = build_model() ... count_parameters(model) 3720932 DeepVoice3 consists of encoder, decoder and converter. At first, it encodes text into per-timestep key and value vectors for an attention-based decoder. The decoder uses these vectors to predict mel-spectrograms that correspond to the audio output. A converter predicts the vocoder parameters for waveform synthesis. (see full explanation ). This implementation contains ~13.3M parameters (calculated as above).
