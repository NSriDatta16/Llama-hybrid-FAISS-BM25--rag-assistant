[site]: crossvalidated
[post_id]: 220062
[parent_id]: 219873
[tags]: 
This is more a suggestion than an answer ATM, but too long for a comment. Intuitively, this seems somewhat similar to obtaining multiple results from different cross validation partitions and repeats$^1$. Therefore, I would try calculating statistics across your different RMSE and CI results in similar manner, then compare different models/select a suitable model based on those. Statistics usually taken into account would e.g. be a mean/median, sd/mad - or one boxplot per model ordered by median - which together give you an idea about the average performance and performance spread for each model. This should then help in selecting models/model parametrizations to take a closer look at. $^1$ The difference is that changes in error over time could occur with your models and data, which is not addressed with looking at results from classic CV partition and repeats. You might want to think about a way of adding this information in your metrics in some way.
