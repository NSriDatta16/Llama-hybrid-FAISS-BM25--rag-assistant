[site]: crossvalidated
[post_id]: 266393
[parent_id]: 266392
[tags]: 
You're talking about L1 norm and L2 norm. Both work for neural networks. However, they are different: L1 norm is better for sparsity and robust against outliers L2 norm is more sensitive to large errors (square those large errors) Their first derivative is very different. I don't want to repeat what someone has already written. Look at https://stats.stackexchange.com/a/159379/34623 . How those error functions update your weight is different (gradient). This has significant impact on your convergence in stochastic gradient decent (or something like that). http://www.chioka.in/differences-between-the-l1-norm-and-the-l2-norm-least-absolute-deviations-and-least-squares/ Without more information, I can't comment on how L2 norm is better (or worse) for your problem.
