[site]: datascience
[post_id]: 53948
[parent_id]: 
[tags]: 
Should I back-calculate soft labels (probabilities) of my training set when I use probabilistic classifiers?

I have 2 datasets (D1, D2) to train 2 models (M1, M2). M1 is a probabilistic classifier, which outputs soft labels (probabilities of a sample belonging to each class) for a binary classification problem, realized by a sklearn.svm.SVC(probability=True) . M2 is a generative model, which generates samples that belong to the desired class. The input to this model is the soft class label and some other properties. D1 has samples, their properties and their binary labels (either 0 or 1) which serve as ground truth to train M1. D2 has unlabeled samples and their properties. Therefore, the trained M1 model is used to label them and only then they are used to train M2. My problem: I want to use D1 as input to my generative model M2. However I am unsure whether I should use the ground truth "hard" labels (integers 0 or 1) along with the other properties as inputs to M2, or firstly use M1 to back-calculate the real-valued soft labels of D1 and feed them to M1. I hope the diagram sheds some light on my task.
