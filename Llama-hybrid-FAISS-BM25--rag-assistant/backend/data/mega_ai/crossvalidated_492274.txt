[site]: crossvalidated
[post_id]: 492274
[parent_id]: 
[tags]: 
How to make sense of rescaling time series of counts?

I'd like to forecast time series of counts : sold items. Each time series represents monthly sales. I also believe that there are clusters within the series, with low, medium and high count items. Such that : low -> lots of zeros and sold items which don't exceed 10 items per month or so. medium -> some zeros but mostly monthly counts between 20 and 100. high -> No zeros and lots of items sold, in the thousands. What I would like to do is cluster the series and create a model for each cluster (without forgetting the train/(validation)/test splits in the beginning). Should I scale the time series beforehand ? In the Python library tslearn, the examples often use mean and variance scaling for the series. I believe it's mostly for numerical stability reasons. Importantly, they note that : " The assumption here is that the range of a given time series is uninformative and one only wants to compare shapes in an amplitude-invariant manner ". My goal here is to use Neg. Bin. or Poisson processes for the different clusters, the amplitude does matter for the parameters if my understanding is correct. Should I still : Scale => Cluster => Model the scaled time series or should I rather Scale => Cluster => Model the unscaled time series or, Cluster => Model the unscaled time series Intuitively, 3. makes more sense to me but I'm not positive it's the best way to go about it. Is there literature or commented benchmarks on what one should do? Edit : for context, first I'd like to make a proof of concept with a small dataset of around 80-100 series with 120 time steps but it'll end up being used with maybe 10 000 series with between 36 and 60 time steps, since this is monthly sales. Many series but few time steps.
