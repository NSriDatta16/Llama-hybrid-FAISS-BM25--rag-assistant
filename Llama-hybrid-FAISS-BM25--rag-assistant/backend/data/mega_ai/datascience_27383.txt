[site]: datascience
[post_id]: 27383
[parent_id]: 27378
[tags]: 
I want to understand if I must use exactly the same Q function (and policy) to get A and A'. If I update Q function in each iteration, it follows that the next action in a subsequent iteration will be derived using the latest Q updated, while the previous action was obtained using the former Q.\ On the other hand, I really can make A and A' with exactly the same Q, and only after that update the Q. So I will always consider A and A' derived using the same function. Which is more orthodox / correct? The algorithm pseudocode given is more orthodox, since in order to revise the value of $A$ you would have to "roll back" the environment and see where the newly-sampled $A$ would take you from state $S$ . To make this clearer, you can see that: select action a' using a policy based on Q could be re-phrased: select action a' by sampling epsilon-greedy function over Q(s',*) . . . you cannot do that unless you have the value of $S'$ , and you may only have that value if you have already taken action $A$ when in state $S$ . Changing $A$ at that stage therefore means going back in time . . . In practice it doesn't matter much, even if you have capability to roll back (in a simulator, or in a planning algorithm). If your policy is based on e.g. $\epsilon$ -greedy over the current Q values, then you are performing SARSA for optimal control (as opposed to prediction). In that case, changing Q means changing the policy. "On-policy" in SARSA for control must allow for the non-stationarity of the policy. Occasionally that means that the $A'$ value you just chose would have been chosen with a lower probability in a more optimal policy. But you chose it anyway this time, and the agent should choose it less often in future . The learning-rate based updates will remove estimation bias due to earlier poor/too-frequently-sampled choices over time. Revising a single step "mistake" is possible, but not common practice in a purely online algorithm. I have not seen it in planning look-ahead or offline algorithms either that I have studied. I don't know for certain, but I suspect the occasional boost to learning you might get from revising the immediate part of the trajectory is too small to be worth the loss of generality of the algorithm. You may find it does help sometimes though, and worth an experiment to review whether it is helpful, provided you are working with a simulator/planner where rolling back state is relatively easy. Note that the way you are thinking does turn up again when using function approximators (e.g. linear functions or neural networks) in semi-gradient versus "true gradient" methods, where instead of this being an issue with which Q values to use, it is an issue with calculating gradients due to the TD error, when your TD target is based on the same parameters that you are are taking the gradient for. In semi-gradient methods, this issue is ignored, and the methods still work OK. However, the "true gradient" methods are more theoretically correct.
