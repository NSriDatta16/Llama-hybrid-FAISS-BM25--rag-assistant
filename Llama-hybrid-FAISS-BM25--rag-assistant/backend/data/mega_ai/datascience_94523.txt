[site]: datascience
[post_id]: 94523
[parent_id]: 
[tags]: 
Network overfitting after adding batch normalization

After I added BN between my model's hidden layers, my neural network started to overfit faster and because of this it didn't find optimal solution for the problem. My theory is BN speeded up the training and made neural network converge to training global minimum faster before exploring mininum which fits validation set the best. But BN should also regularise model a little bit which doesn't happen in this case. I have FFNN which is designed to predict football (soccer) outcomes and what bets are most profitable to take. Because I need odds in some metrics I had to create wrapper for categorical loss and crossentropy but it's the same as keras' ones underneath. Model is setup in code like this: factor = 0.002 rate = 0.4 model = tf.keras.models.Sequential() model.add(keras.layers.BatchNormalization()) model.add(keras.layers.Dropout(rate)) model.add(keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(factor), kernel_initializer=tf.keras.initializers.he_normal())) model.add(keras.layers.BatchNormalization()) model.add(keras.layers.Dropout(rate)) model.add(keras.layers.Dense(128, activation='relu', kernel_regularizer=l2(factor), kernel_initializer=tf.keras.initializers.he_normal())) model.add(keras.layers.BatchNormalization()) model.add(keras.layers.Dropout(rate)) model.add(keras.layers.Dense(64, activation='relu', kernel_regularizer=l2(factor), kernel_initializer=tf.keras.initializers.he_normal())) model.add(keras.layers.Dense(3, activation='softmax', kernel_initializer=tf.keras.initializers.he_normal())) model.compile(loss=categorical_crossentropy_with_bets, optimizer=keras.optimizers.Adam(learning_rate=0.0003), metrics=[categorical_acc_with_bets]) And here's loss and accuracy of network with BN: and here's without bn between hidden layers:
