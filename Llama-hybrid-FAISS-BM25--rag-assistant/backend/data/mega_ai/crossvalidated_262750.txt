[site]: crossvalidated
[post_id]: 262750
[parent_id]: 
[tags]: 
Why is it hard to train deep neural networks?

I'm fairly new to the field of ML and I often see in the intro of papers a sentence on how deep networks are hard to train or that it's difficult for the training signal to reach through multiple hidden layers. Is there some source that actually rigorously explains the problem and analyses the situation and how the chosen activation function affects this? I would assume that it has something do with vanishing gradients, but would actually want to see a rigorous analysis of this and how the number of layers impacts the gradients. I'm a math Ph.D. so I'm looking for something more precise and less handwavy.
