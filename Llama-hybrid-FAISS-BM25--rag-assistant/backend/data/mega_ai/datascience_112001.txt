[site]: datascience
[post_id]: 112001
[parent_id]: 
[tags]: 
What is meant by averaging inhibits it in the paper 'Attention is All You Need'?

Could anyone explain to me about the sentence below? What is meant by averaging inhibits it? Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this. Edit:
