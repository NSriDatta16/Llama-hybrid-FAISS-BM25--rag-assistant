[site]: crossvalidated
[post_id]: 548471
[parent_id]: 
[tags]: 
Do you need large amounts of data to estimate parameters in extreme value distributions?

There is probably not a hard answer for this, but I am wondering if you need to collect more data when trying to estimate the parameters of generalized pareto distribution well? The reason I ask is because I am trying to estimate the parameters of a generalized pareto distribution using Bayesian estimation, and my parameter estimates seem to be very good when I have lots of data (say, 1000+ data points), but when I drop the data size to say 100 then the estimates can be very poor. For example, if I have a generalized pareto distribution with true parameters $\mu=0$ , $\sigma=1.2$ , and $\xi=0.8$ , and I sample $N=1000$ observations then (running my Bayesian algorithm) I get estimates of $\hat\sigma=1.27$ 95% CI: $(1.12, 1.46)$ and $\hat\xi=0.83$ 95% CI: $(0.72, 0.98)$ . However, if I drop the same size to $N=100$ I get $\hat\sigma=0.87$ 95% CI: $(0.55, 1.24)$ and $\hat\xi=0.94$ 95% CI: $(0.61, 1.38)$ . If I keep decreasing $N$ , the point estimates of $\sigma$ and $\xi$ only get worst. Is there a rule of thumb about how much data is need for extreme value distributions? In most cases, 100 data points would be sufficient for modeling a distribution that doesn't have too extreme of values (say, normal, exponential, gamma, etc.). In my application I will always be dealing with less than 100 data points and so is it a bad idea to use the generalized pareto distribution? Here is an example of code doing what I am trying to explain: # log-likelihood likelihood
