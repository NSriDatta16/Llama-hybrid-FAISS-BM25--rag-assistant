[site]: crossvalidated
[post_id]: 578620
[parent_id]: 
[tags]: 
Does average or max pooling actually summarise the sentence?

I am working on an multi-label text classification problem at work and adapted model architecture from this notebook of Toxic Comment Classification challenge on Kaggle. I have trained the model, a person at work, after looking at the predictions, has said that "the model is concentrating only on some set of keywords in the sentence and ignoring much of it.". fyi, sentences that originally has at least 6-8 labels have only had 1-3 labels predicted(with value of each sigmoid at the output > 0.5) and I presume that's what has led to this person to believe so. I had learned from Anderw Ng's deep-learning specialisation that pooling 'summarises' features with image data, but does pooling also 'remove' the effect of some of the words from the input and led the model during training to focus only on some words in the sentence? more information: Stopword removal or lemmatisation has not been done on the sentences, they are actually long paragraphs. Unlike the challenge linked, my task was to predict themes/tags for a sentence and those tags/themes in some cases can directly come from the sentences and in some cases a theme/tag(s) can be learned only from the summary of the sentence. I cannot share any example data.
