[site]: datascience
[post_id]: 69607
[parent_id]: 69603
[tags]: 
Logistic Regression is a large margin loss not classifier. (in literal sense, meaning there is a margin with the loss function not the classification boundary There are two terms in the softmax loss, true values minus the exp or log of the observations (roughly and depending on the variant of loss) this just implies that distance of observed example from its true decision boundary needs to beat the log sum of the distances from all of the decision boundaries. Because the softmax function is a probability distribution, the largest the log softmax can be is 0, so the log softmax returns a negative value (i.e. a penalty) that approaches 0 as the probability of the true class under the softmax function approaches 1.
