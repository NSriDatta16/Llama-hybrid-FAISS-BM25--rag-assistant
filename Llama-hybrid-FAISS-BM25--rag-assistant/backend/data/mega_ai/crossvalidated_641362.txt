[site]: crossvalidated
[post_id]: 641362
[parent_id]: 580779
[tags]: 
In general, if $\theta$ is our parameter(s) of interest and $D$ is the data, Bayes theorem gives us $P(\theta|D)\propto P(\theta)\cdot P(D|\theta)$ where $P(\theta)P(D|\theta)$ is the prior and $P(D|\theta)$ is the likelihood. Now the optimal Bayesian estimator for $\theta$ (in terms of BMSE) is $E[\theta|D]$ . That is, in order to find the Bayesian estimator we derivate the posterior probability (or its log). While taking a concave prior (such as the normal) usually guarantees unimodality and getting the global extremum point, sometimes our prior isn't concave. In such cases, the optimality of the Bayes estimator indeed relies on the concavity of the the likelihood function. For example, consider the IRT 3PL model of answering correctly a binary-scored question: The question has parameters $a,b,c$ and given examinee ability $\theta$ the said probability is $$P(y=1|a,b,c,\theta)=c+(1-c)\frac{\exp(a(\theta-b))}{1+\exp(a(\theta-b))}$$ In a similar manner the probability of the same examinee to answer the same question incorrectly is $$Q=P(y=0|a,b,c,\theta)=1-P(y=1|a,b,c,\theta)=\frac{1-c}{1+\exp(a(\theta-b))}$$ Given the responses $D_j=\{y_{1j},...,y_{nj}\}$ of the $j^{th}$ examinee to $n$ questions, the likelihood function is $$P(D_j|\theta_j)=\prod_{i=1}^n P_i^{y_{ij}}Q_i^{(1-y_{ij})}$$ Let $e_{ij}=\exp(a_i(\theta_j-b_i))$ , the log-likelihood is $$\sum_i\left(y_{ij}\log(c_i+e_{ij})+(1-y_{ij})\log(1-c_i)-\log(1+e_{ij})\right)$$ The concavity of this function ensures the optimality of the Bayes estimator whether the prior $P(\theta)$ for modelling examinee ability is concave (e.g. normal) or not (which happens sometimes, especially in heterogenous populations where the empirical prior is multimodal). In other cases, if the likelihood isn't concave, we might force a concave prior to ensure Bayes optimality.
