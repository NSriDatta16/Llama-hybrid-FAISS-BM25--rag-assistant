[site]: datascience
[post_id]: 67936
[parent_id]: 
[tags]: 
What is an appropriate approach to sampling for probability of default using a classification model?

If we have a loan book and want to train the data to predict the probability of default, what is an appropriate way to sample the historical data to train the model, given that each account is open over a period of time and not just at a single snapshot? For example - for predicting defaults of new customers on a loan book, it is easy to select data to train a model at an equivalent point in time - i.e. when each customer is new, we may look at the FICO score, age of customer, home State etc at the point of loan application. But once a loan has been made and we wish to build a model for likelihood of default given ongoing repayment history, what is the statistically appropriate sample to take? A random sample within the life to date of the loan picking each record at a single random point in time, choosing all loans at a specified point in time (e.g. at exactly 6 months in from origination) or a sample of dates covering a period? Or does it require something else - is a classification model actually appropriate for this type of scenario (assuming a binary outcome), or should some sort of time series or survival model be considered?
