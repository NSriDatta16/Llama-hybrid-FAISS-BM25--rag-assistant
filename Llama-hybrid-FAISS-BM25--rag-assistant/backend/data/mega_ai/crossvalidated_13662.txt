[site]: crossvalidated
[post_id]: 13662
[parent_id]: 13655
[tags]: 
Both Confidence Intervals and Credible Intervals represent our knowledge about an unknown parameter given the data and other assumptions. When using lay interpretations the 2 intervals are pretty similar (though I may have just made frequentists and Bayesians have common ground in being offended by my statement). The tricky part comes when getting into exact definitions. The Bayesians can talk about the probability of the parameter being in the interval, but they have to use the Bayesian definition of probability which is basically that probability represents our knowledge about an unknown parameter (look familiar?). Note that I am not a Bayesian, so they may want to give a better definition than mine. This does not work if you try to use a frequentist understanding of probability. The frequentist definition of probability talks about the frequency that an outcome will occure if repeated a bunch of times at random. So once the randomness is over we cannot talk about probability any more, so we use the term confidence to represent the idea of amount of uncertainty after the event has occured (frequentist confidence is similar to Bayesian probability). Before I flip a fair coin I have a probability of 0.5 of getting heads, but after the coin has been flipped and landed or been caught it either shows a heads or a tails so the probability is either 0% or 100%, that is why frequentists don't like saying "probability" after the random piece is over (Bayesians don't have this problem since probability to them represents our knowledge about something, not the proportion of actual outcomes). Before collecting the sample from which you will compute your confidence interval you have a 95% chance of getting a sample that will generate a Confidence Interval that contains the true value. But once we have a Confidence Interval, the true value is either inside of that interval or it is not, and it does not change. Imagine that you have an urn with 95 white balls and 5 black balls (or a higher total number with the same proportion). Now draw one ball out completely at random and hold it in your hand without looking at it (if you are worried about quantum uncertainty you can have a friend look at it but not tell you what color it is). Now you either have a white ball or a black ball in your hand, you just don't know which. A Bayesian can say that there is a 95% probability of having a white ball because their definition of probability represents the knowledge that you drew a ball at random where 95% were white. The frequentists could say that they are 95% confident that you have a white ball for the same reasoning, but neither would claim that if you open your hand 100 times and look at the ball (without drawing a new ball) that you will see a black ball about 5 times and a white about 95 times (which is what would happen if there was a 95% frequentist probability of having a white ball). Now imagine that the white balls represent samples that would lead to a correct CI and black balls represent samples that would not. You can see this through simulation, either using a computer to simulate data from a known distribution or using a small finite population where you can compute the true mean. If you take a bunch of sample and compute Confidence Intervals and Credible Intervals for each sample, then compute the true mean (or other parameter) you will see that about 95% of the intervals contain the true value (if you have used reasonable assumptions). But if you concentrate on a single interval from a single sample, it either contains the true value or it does not, and no matter how long you stare at that given interval, the true value is not going to jump in or out.
