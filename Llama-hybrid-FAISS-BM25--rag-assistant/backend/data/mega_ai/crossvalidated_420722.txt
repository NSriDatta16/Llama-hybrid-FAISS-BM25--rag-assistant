[site]: crossvalidated
[post_id]: 420722
[parent_id]: 367777
[tags]: 
I would use pretrained word embeddings with a neural network as Sycorax mentioned (recurrent or transformer) if you're looking for a good performance as state of the art models for sentiment analysis are nn-based nowadays. You need to use the lookup index (word2idx) of the pretrained word embeddings to transform each word into an index then feed it into an embedding layer nn.Embedding in the case of pytorch for example. This layer would be initialized with the pretrained word vectors. You can also have a look at using a pretrained language model like Bert for your use case which could help you overcome the small size of your dataset https://github.com/huggingface/pytorch-transformers
