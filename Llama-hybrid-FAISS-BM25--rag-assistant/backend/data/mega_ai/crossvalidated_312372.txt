[site]: crossvalidated
[post_id]: 312372
[parent_id]: 285745
[tags]: 
In the answer you reference, the goal is to calculate $w_k$, one of the weights in one filter in a (convolutional) neural network. The update formulas for any parameter in a neural network, using gradient descent, is quite simple if you just express it as a derivative of the cost function $J$: $\Delta w_k = -\eta \frac{\partial J}{\partial w_k}$ But, computing $\frac{\partial J}{\partial w_k}$ is where things get tricky. In a convolutional neural network, this weight is shared, so you need to calculate $\frac{\partial J}{\partial w_k} = \sum \frac{\partial J}{\partial w_l}$ where the sum is taken over all the occurrences of that weight. So, if you have a 2x2 filter and an 8x8 image (stride 1), the filter is applied 7x7=49 times and in each application of this filter the top left weight is used. So, the sum is over these 49 values. For the top right weight, however, the sum is still over 49 values, but it's now a different set of values and so the update is different.
