[site]: crossvalidated
[post_id]: 48885
[parent_id]: 48877
[tags]: 
You should be able to use KDR to estimate a Dirichlet type distribution, you just need to ensure you capture the constraints correctly. First off, drop the dimensionality by 1. As you correctly point out, in the 2D case, you have a beta distribution, hence there is only 1 degree of freedom. In this case the correct thing to do would be to estimate the beta shaped pdf of one of the two variables. In higher dimensions, once you have $n-1$ of your probabilities, you know the last one. Now you only have a boundary problem. For the 2D case, this is just the edges at 0 and 1. If you use the normal Kernel it will "spill over" beyond this which will have 2 effects The support will be incorrect, if you truncate it you won't sum to zero, if you rescale you will over estimate the centre, underestimate the edges. The pdf estimate at the edges will tend to be pulled towards the middle - there will be no values to the left of $0$ or to the right of $1$ to balance the kernel smooth. I.e. imagine that the distribution rises to a point at 1, then the kernel will lower that point by averaging with values at 0.95, biasing the estimate at 1 downwards. The second of these is difficult (impossible?) to deal with in generality, although there have been many attempts. Given that your reason for choosing this route was for simply implementation, we shall ignore those. The first problem can be mitigated by simply using a kernel that truncates at 0 and 1. One way of doing this is to use a beta distribution as your kernel, with quite large parameters. This approximates a normal away from 0 and 1, but then "bunches up" when at the edges. Moving on to higher dimensions, in 3D you have support on a right angled triangle from (0,0) - (0,1) - (1,0). By analogy with the above, you could use a dirichlet kernel. This behaves like a normal distribution in the middle of the triangle, but bunches up at the boundaries. The same then clearly applies in higher dimensions.
