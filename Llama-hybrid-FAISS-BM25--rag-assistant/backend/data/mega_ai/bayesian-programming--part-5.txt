 "what is the probability for a given text to be spam knowing which words appear and don't appear in this text?" It can be formalized by: P ( Spam ∣ w 0 ∧ ⋯ ∧ w N − 1 ) {\displaystyle P({\text{Spam}}\mid w_{0}\wedge \cdots \wedge w_{N-1})} which can be computed as follows: P ( Spam ∣ w 0 ∧ ⋯ ∧ w N − 1 ) = P ( Spam ) ∏ n = 0 N − 1 [ P ( w n ∣ Spam ) ] ∑ Spam [ P ( Spam ) ∏ n = 0 N − 1 [ P ( w n ∣ Spam ) ] ] {\displaystyle {\begin{aligned}&P({\text{Spam}}\mid w_{0}\wedge \cdots \wedge w_{N-1})\\={}&{\frac {\displaystyle P({\text{Spam}})\prod _{n=0}^{N-1}[P(w_{n}\mid {\text{Spam}})]}{\displaystyle \sum _{\text{Spam}}[P({\text{Spam}})\prod _{n=0}^{N-1}[P(w_{n}\mid {\text{Spam}})]]}}\end{aligned}}} The denominator appears to be a normalization constant. It is not necessary to compute it to decide if we are dealing with spam. For instance, an easy trick is to compute the ratio: P ( [ Spam = true ] ∣ w 0 ∧ ⋯ ∧ w N − 1 ) P ( [ Spam = false ] ∣ w 0 ∧ ⋯ ∧ w N − 1 ) = P ( [ Spam = true ] ) P ( [ Spam = false ] ) × ∏ n = 0 N − 1 [ P ( w n ∣ [ Spam = true ] ) P ( w n ∣ [ Spam = false ] ) ] {\displaystyle {\begin{aligned}&{\frac {P([{\text{Spam}}={\text{true}}]\mid w_{0}\wedge \cdots \wedge w_{N-1})}{P([{\text{Spam}}={\text{false}}]\mid w_{0}\wedge \cdots \wedge w_{N-1})}}\\={}&{\frac {P([{\text{Spam}}={\text{true}}])}{P([{\text{Spam}}={\text{false}}])}}\times \prod _{n=0}^{N-1}\left[{\frac {P(w_{n}\mid [{\text{Spam}}={\text{true}}])}{P(w_{n}\mid [{\text{Spam}}={\text{false}}])}}\right]\end{aligned}}} This computation is faster and easier because it requires only 2 N {\displaystyle 2N} products. Bayesian program The Bayesian spam filter program is completely defined by: Pr { D s { S p ( π ) { V a : Spam , W 0 , W 1 … W N − 1 D c : { P ( Spam ∧ W 0 ∧ … ∧ W n ∧ … ∧ W N − 1 ) = P ( Spam ) ∏ n = 0 N − 1 P ( W n ∣ Spam ) F o : { P ( Spam ) : { P ( [ Spam = false ] ) = 0.25 P ( [ Spam = true ] ) = 0.75 P ( W n ∣ Spam ) : { P ( W n ∣ [ Spam = false ] ) = 1 + a f n 2 + a f P ( W n ∣ [ Spam = true ] ) = 1 + a t n 2 + a t Identification (based on δ ) Q u : P ( Spam ∣ w 0 ∧ … ∧ w n ∧ … ∧ w N − 1 ) {\displaystyle \Pr {\begin{cases}Ds{\begin{cases}Sp(\pi ){\begin{cases}Va:{\text{Spam}},W_{0},W_{1}\ldots W_{N-1}\\Dc:{\begin{cases}P({\text{Spam}}\land W_{0}\land \ldots \land W_{n}\land \ldots \land W_{N-1})\\=P({\text{Spam}})\prod _{n=0}^{N-1}P(W_{n}\mid {\text{Spam}})\end{cases}}\\Fo:{\begin{cases}P({\text{Spam}}):{\begin{cases}P([{\text{Spam}}={\text{false}}])=0.25\\P([{\text{Spam}}={\text{true}}])=0.75\end{cases}}\\P(W_{n}\mid {\text{Spam}}):{\begin{cases}P(W_{n}\mid [{\text{Spam}}={\text{false}}])\\={\frac {1+a_{f}^{n}}{2+a_{f}}}\\P(W_{n}\mid [{\text{Spam}}={\text{true}}])\\={\frac {1+a_{t}^{n}}{2+a_{t}}}\end{cases}}\\\end{cases}}\\\end{cases}}\\{\text{Identification (based on }}\delta )\end{cases}}\\Qu:P({\text{Spam}}\mid w_{0}\land \ldots \land w_{n}\land \ldots \land w_{N-1})\end{cases}}} Bayesian filter, Kalman filter and hidden Markov model Bayesian filters (often called Recursive Bayesian estimation) are generic probabilistic models for time evolving processes. Numerous models are particular instances of this generic approach, for instance: the Kalman filter or the Hidden Markov model (HMM). Variables Variables S 0 , … , S T {\displaystyle S^{0},\ldots ,S^{T}} are a time series of state variables considered to be on a time horizon ranging from 0 {\displaystyle 0} to T {\displaystyle T} . Variables O 0 , … , O T {\displaystyle O^{0},\ldots ,O^{T}} are a time series of observation variables on the same horizon. Decomposition The decomposition is based: on P ( S t ∣ S t − 1 ) {\displaystyle P(S^{t}\mid S^{t-1})} , called the system model, transition model or dynamic model, which formalizes the transition from the state at time t − 1 {\displaystyle t-1} to the state at time t {\displaystyle t} ; on P ( O t ∣ S t ) {\displaystyle P(O^{t}\mid S^{t})} , called the observation model, which expresses what can be observed at time t {\displaystyle t} when 