[site]: crossvalidated
[post_id]: 4286
[parent_id]: 
[tags]: 
Principal Component Analysis among matrices

I have the following setting. I have n Hermitian Positive Semidefinite (HPSD) matrices, and a metric induced by a matrix norm. I am primarily interested in the Frobenius norm and the operator norm. I want to extract the principal "principal component" for this set of observations, i.e., a 1-dimensional subspace in HPSD such that that the sum of squares of the minimum distances between the matrices with this subspace is minimized. I am not interested in second, third etc components, since they are not even well defined in this space, since I have not defined a scalar product for simplicity. In the case of the Frobenius norm, the problem can be reduced to traditional PCA, by using as input vectors the stacked versions of the input matrices. But in the case of the operator norms, I can't find a strategy to attack the problem. Questions: Has anyone seen this specific problem before? Recommendations and references are highly appreciated. Has anyone dealt with computation of PCA in the case of non-euclidean distances?
