[site]: crossvalidated
[post_id]: 642695
[parent_id]: 
[tags]: 
I screwed-up model selection but ended-up with a very good model; am I ok?

In a recent experiment, I made an oversight: I divided my data into training and testing sets and conducted cross-validation for model selection and hyperparameter tuning after having applied Boruta (feature importance with Random Forest) to select features on the entire training set. I want to ensure I fully grasp the risks associated with this methodological error. It appears to me that the model selection has been compromised in two ways. First, I belive that the error introduced a selection bias in favor of tree-based methods (resulting in XGBoost being the chosen model), as the set of most important features varies with the method chosen. Additionaly, I suspect that the cross-validation scores are optimistically biased due to the leakage of the test folds to the feature selection procedure. Nonetheless, the evaluation of the model on the test data (assuming it is sufficiently representative and sizable) should remain unaffected. Furthermore, given that Gradient Boosting is typically well-suited for tabular datasets, I'm questioning whether there are any genuine issues with the experiment's outcomes, apart from, perhaps, overlooking a more performant interpretable model. In summary, I believe I screwed-up model selection, but ended up with a very good model nonetheless, so I should be ok. What do you think?
