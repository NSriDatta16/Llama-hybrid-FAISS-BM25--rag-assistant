[site]: crossvalidated
[post_id]: 199639
[parent_id]: 
[tags]: 
Which statistical assumptions are still important when fitting a GLM to > 1 million observations?

I have previously only fit GLM models to small/medium sized data (up to several thousand points, maybe tens of thousands). I always try to be meticulous about checking that GLM assumptions hold where appropriate. These assumptions include but are not limited to: Normality of error (Q-Q plot, Jarque-Bera, Kolmorogov-Smirnov) Existence of high leverage outliers (visual inspection, Cook's distance) Multi-collinearity (correlation matrix, variance inflation factor) Heteroskedasticity (visual inspection, Breusch-Pagan) I am currently speccing out an interface to construct arbitrary GLM models using Spark (specifically mllib). This platform will potentially be fitted on much larger data sets than I am used to when using GLMs (millions and millions). I don't have a good intuition around the extent to which each one of these assumptions becomes more or less important as the amount of data gets larger. This is important to know, because the methods for checking these assumptions (especially the ones that involve visual inspection), don't necessarily scale very well. I'm planning on doing some simulations to see for myself, but thought I would seek expert opinion to make up for my lack of experience in this area.
