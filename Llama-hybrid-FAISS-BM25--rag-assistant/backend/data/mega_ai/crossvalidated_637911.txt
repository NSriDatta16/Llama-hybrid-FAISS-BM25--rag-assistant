[site]: crossvalidated
[post_id]: 637911
[parent_id]: 637906
[tags]: 
Regarding your first point: However, the problem is that when I add more than one variable (with any combination), the results become non-significant. I performed backward regression, and the two remaining variables are also non-significant. You need to step back for a second and forget about statistical significance. Rather than using $p$ values to determine your beliefs, you should instead be looking at your data and how it may/may not match expectation. These are some more important questions to ask yourself: What does the data tell me visually ? Is there an actual effect? Are there hidden relationships? (hint: you need to plot your data and see what's going on). Does past research or my theory explain this in some way? Is some causal pathway or otherwise spurious relationship influencing the results? Are there problems with my data (e.g. poor reliability of measures, erroneous outliers) that effect the results? In the comments of another answer here you noted: Basically, I need a way to prove that they are significant, but for a reason (point #1) they are not significant in multivariate regression, OR they are actually non-significant, and I need to know how to prove this. Let us say that you do not uncover any serious errors, visualization shows that the effects are what you expect, and the results support/refute your theory in a logical way. Surprising findings can be just as important and interesting if you have a good justification for why. What does this mean for your research? Does it challenge some old ways of thinking? I would approach it from this perspective, though doing so cautiously due to sample size constraints (if you are dedicated to $p$ values at least). Some other important takeaways/solutions: Never use backwards regression or any form of stepwise regression. It capitalizes on chance findings. You can just search this site for the many warnings about this technique. A useful paper on this topic can be found here . I agree with Linus that it would be helpful to check the VIF rather than the correlations to see if that is part of the issue. There is a quick overview of VIF in this paper . The performance package in R allows you to check this easily by running check_collinearity(fit) , as shown here . I disagree with Linus on the point of just gathering more observations after inspecting your data. This is considered by a good number of people a questionable research practice (QRP) and should be avoided. In short, it allows you to p-hack by collecting more people until your results look the way you want, which is unscientific. The opposite is just as true (selective stopping) when you stop collecting data as soon as your results are statistically significant. I would be very wary of the results here if I was a reviewer for a scientific journal given the tiny sample size. One approach you could consider is using a Bayesian regression instead, paying very careful attention to using well-calibrated priors. The book Statistical Rethinking by McElreath has a very accessible R-based approach to this topic. In short, any sample size is valid for Bayes, though with smaller sample sizes one must be especially cautious of the priors (otherwise Bayes can be just as destructive as NHST).
