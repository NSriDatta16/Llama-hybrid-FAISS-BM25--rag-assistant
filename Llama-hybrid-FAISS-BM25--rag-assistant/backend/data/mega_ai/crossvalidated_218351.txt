[site]: crossvalidated
[post_id]: 218351
[parent_id]: 
[tags]: 
What happens if I flip targets and predictions in cross-entropy?

When we compute the cross-entropy within the machine learning context, we use the following formula: $$ CE(t, p) = -\sum_{i=1}^{N} t_i \ \log(p_i) $$ Where $t$ is the target class probability, and $p$ is the predicted class probability. My question is, this metric obviously gives us different loss values if the $t$ and $p$ were reversed in the above formula. So, what is the rationale, and reason(s), for the the targets $t$ being outside the log, and the predictions $p$ being inside the log? Why this configuration, and not the opposite?
