[site]: crossvalidated
[post_id]: 331111
[parent_id]: 331102
[tags]: 
Both POMDPs and their fully observed counterparts -- MDPs -- aren't used for obtaining policies. Rather, they're mathematical formalisms for describing RL environments. So just as it doesn't make sense to ask if supervised learning will give an optimal classifier, it doesn't make sense to ask if POMDPs can be used to obtain an optimal policy. If you are asking whether there is an algorithm which extract a policy which can act optimally in a POMDP environment (compared to an optimal policy which has access to the entire state), then it's dependent on the exact POMDP and environment. It's easy to construct a POMDP where all actions give equal reward and every policy is optimal. It's also easy to construct a POMDP where the observations are useless and don't tell you anything, in which case you won't be able to learn an optimal policy. In practice, if the environment is nontrivial, such as an Atari game, then the learned policy is probably not going to be optimal whether the state is fully or partially observed.
