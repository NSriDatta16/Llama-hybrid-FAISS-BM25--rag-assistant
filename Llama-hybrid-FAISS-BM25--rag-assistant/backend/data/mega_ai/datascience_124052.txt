[site]: datascience
[post_id]: 124052
[parent_id]: 
[tags]: 
Why are 1/n, 2/n, 3/n ... 2048/n not good positional encodings to be concatenated to the word vectors in transformers?

The transformer architecture has no sense of the relative positions of the word and hence we need to pass that information apriori to the along with the word embeddings to the model The positional encoding currently implemented in the transformers uses trig enccoding Why is a simple positional encoding scheme like concatenating 1/n, 2/n, ... 2048/n (for gpt-4) to the input words not chosen? What are the arguments against using it?
