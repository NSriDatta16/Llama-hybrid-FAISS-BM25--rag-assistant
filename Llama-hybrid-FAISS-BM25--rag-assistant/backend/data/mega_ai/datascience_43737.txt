[site]: datascience
[post_id]: 43737
[parent_id]: 17701
[tags]: 
This wildml blog post has a very clear explanation of how to use 1D convolution on text. And Debo, DS at x.ai, provided some example Keras code to classify text using a character-based model (input documents are sequences of one-hot encoded characters rather than words or POS tags): from keras.models import Model from keras.layers import Input, Dense, Dropout, Flatten from keras.layers.convolutional import Convolution1D, MaxPooling1D inputs = Input(shape=(maxlen, vocab_size), name='input', dtype='float32') conv = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[0], border_mode='valid', activation='relu', input_shape=(maxlen, vocab_size))(inputs) conv = MaxPooling1D(pool_length=3)(conv) conv1 = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[1], border_mode='valid', activation='relu')(conv) conv1 = MaxPooling1D(pool_length=3)(conv1) conv2 = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[2], border_mode='valid', activation='relu')(conv1) conv3 = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[3], border_mode='valid', activation='relu')(conv2) conv4 = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[4], border_mode='valid', activation='relu')(conv3) conv5 = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[5], border_mode='valid', activation='relu')(conv4) conv5 = MaxPooling1D(pool_length=3)(conv5) conv5 = Flatten()(conv5) z = Dropout(0.5)(Dense(dense_outputs, activation='relu')(conv5)) z = Dropout(0.5)(Dense(dense_outputs, activation='relu')(z)) pred = Dense(n_out, activation='softmax', name='output')(z) model = Model(input=inputs, output=pred) model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) The last 3 lines are important. You can't use softmax on your output and you can't use 'categorical_crossentropy' for multi-label tagging (your problem). Your text tagging problem should be broken down into multiple binary classification problems, or you need to use a different loss function like 'binary_crossentropy' . And for binary_crossentropy , use a sigmoid activation function rather than softmax on the output. See this SO answer for details on multi-label tagging problems in keras and TF. If you want a more thorough explanation, check out Chapter 7 in my book, NLP In Action.
