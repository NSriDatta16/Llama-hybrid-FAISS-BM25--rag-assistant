[site]: crossvalidated
[post_id]: 415616
[parent_id]: 
[tags]: 
Bayesian imputation for reject inference?

I'm analyzing a dataset to predict whether a customer defaults on a loan. The problem is, the dataset only contains observations on customers who were offered a loan and accepted (ie. there is no data on those who were refused a loan). The problem here is that when applying the model to new data, there is an assumption that all of the customers are approved, which is not the case with the new data. The sampling literature calls this "unit non-response" and methods for dealing with the problem are called "reject inference." While the literature suggests gathering more data, I'm wondering if it is possible adjust for unit non-response using stochastic Bayesian process. The model works like this: there are three variables: c_default (whether the individual defaults), credit (credit score), and accept (whether the loan was accepted). The model is trying to predict the probability of default. It is a logistic regression run through Stan using the wrappers in the rethinking package. The simulation of the data makes the assumption that those who are rejected (who we never see) have a credit score that is 30% lower. The criteria for rejection, regardless of group, is a credit score below 80. library(rethinking) approval The model is then trained on the simulated data with adaptive priors. #logit b_model We can see that approval is a crucial factor in the model. At this point, we can take actual data from approved customers only and predict whether they default. This data is simulated, but all observations come from the "approved" sample with mean credit score of 100. #test with new data, all approved approval .5, 1, 0) truth table(pred, truth) truth pred 0 1 0 94 0 1 0 6 The model nails prediction under these assumptions. In short, the model is built under two assumptions: rejected applicants have a credit score that is 30% less than those accepted, and there is a default threshold of 80%. Thoughts?
