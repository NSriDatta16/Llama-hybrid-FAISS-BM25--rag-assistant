[site]: crossvalidated
[post_id]: 603975
[parent_id]: 603911
[tags]: 
Formulating the Markov chain: Although you didn't explicitly specify it, I will assume that the gambler instantly cashes in their tokens for a dollar as soon as they have $R$ tokens (i.e., they don't save their tokens when they could cash them in). Under this assumption the state space for the model can be described by a single integer state variable: $$s = R x_\text{Dollars} + x_\text{Tokens},$$ where $x_\text{Dollars} = \lfloor s/R \rfloor$ is the number of dollars they have and $x_\text{Tokens} = s - R \times \lfloor s/R \rfloor$ is the number of tokens they have. For all allowable states with $s_t \geqslant R$ the transition probabilities for the Markov chain are: $$p(s_{t+1} | s_t) = \begin{cases} 1-p & & & \text{for } s_{t+1}=s_t-R, \\[6pt] p & & & \text{for } s_{t+1}=s_t+1, \\[8pt] 0 & & & \text{otherwise}, \\[6pt] \end{cases}$$ and every state with $0 \leqslant s_t is an absorbing state that constitutes ruin. (The gambler may still have some tokens left, but not enough to trade in for a dollar.) Expressing the Markov chain in matrix form: To facilitate analysis, let $\mathbf{I}_*$ and $\mathbf{I}_{**}$ be $R \times R$ matrices with elements given as follows: $$\mathbf{I}_{*} = \begin{bmatrix} 0 & 1 & 0 & \cdots & 0 \\ 0 & 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & 0 & \cdots & 1 \\ 0 & 0 & 0 & \cdots & 0 \\ \end{bmatrix} \quad \quad \quad \quad \quad \mathbf{I}_{**} = \begin{bmatrix} 0 & 0 & 0 & \cdots & 0 \\ 0 & 0 & 0 & \cdots & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & 0 & \cdots & 0 \\ 1 & 0 & 0 & \cdots & 0 \\ \end{bmatrix}$$ (Observe that when we put together $[\mathbf{I}_{*}, \mathbf{I}_{**}]$ we get a block that contains an identity matrix that has been shifted to the right by one unit.) The transition probability matrix for the Markov chain can be written up to any desired dimension (depending on the upper bound of the number of games you want to use) in block form (where all the blocks are $R \times R$ matrices) as: $$\mathbf{P} = \begin{bmatrix} \mathbf{I} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \cdots \\ (1-p)\mathbf{I} & p\mathbf{I}_{*} & p\mathbf{I}_{**} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \cdots \\ \mathbf{0} & (1-p)\mathbf{I} & p\mathbf{I}_{*} & p\mathbf{I}_{**} & \mathbf{0} & \mathbf{0} & \cdots \\ \mathbf{0} & \mathbf{0} & (1-p)\mathbf{I} & p\mathbf{I}_{*} & p\mathbf{I}_{**} & \mathbf{0} & \cdots\\ \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots \\ \end{bmatrix}$$ (We label the states corresponding to the rows and columns by $s=0,1,2,3,...$ so the first row/column is labelled as zero.) Now, if you take some upper bound $T$ on the number of games you are looking at, you can then determine that the maximum possible state $\bar{s} = RK + T$ , which allow you to form a finite transition probability matrix $\mathbf{P}_T$ up to that maximum state. You can then take the appropriate powers of the transition probability matrix to get the marginal distributions of the state variable after each game. It is then simple to find the probability of ruin after $t$ games by summing all the transition probabilities from the starting state to the ruin states: $$\mathbb{P}(\text{Ruin after } t \text{ games}| s_0 = K) = \sum_{s=0}^{R-1} [\mathbf{P}_T^t]_{KR, s}.$$
