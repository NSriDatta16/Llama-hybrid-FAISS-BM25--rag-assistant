[site]: crossvalidated
[post_id]: 446038
[parent_id]: 
[tags]: 
Connection between Bayesian model selection and linear regression objective function

I'm currently studying machine learning using the book Introduction to Machine Learning (Alpaydin, 2014) and had a question. More specifically, this question is regarding a part of Chapter 4.8: Model Selection . The passage that brought up the question is as follows (the exact passage is modified to omit unnecessary details): Bayesian model selection is used when we have some prior knowledge about the appropriate class of approximating functions. Given the data and assuming a model, we can calculate $P(\text{model}|\text{data})$ using Bayes' theorem: $$P(\text{model}|\text{data}) = \dfrac{P(\text{data}|\text{model})P(\text{model})}{P(\text{data})}$$ If we take the log of this equation, we get: $$\log\left({P(\text{model}|\text{data})}\right) = \log\left({P(\text{data}|\text{model})}\right) + \log\left({P(\text{model})}\right) - c$$ The log likelihood of the data is the training error and the log of the prior is the penalty term. For example, if we have a regression model and use the prior $P(\mathbf{w}) \sim \mathcal{N}(0, 1/\lambda)$ , we minimize: $$E = \sum_t \left(r^t - g(x^t|\mathbf{w})\right)^2 + \lambda\sum_i w_i^2$$ (Here, $r^t$ refers to the true values and $g(x^t|\mathbf{w})$ refers to the predictions that our discriminant function outputs for data point $x^t$ given parameters $\mathbf{w}$ .) I'm having trouble how the author's relating the standard linear regression objective function $E$ with the Bayesian model selection approach. It seems rather out of the blue that we would suddenly get that error function. What is the relationship between the two? Any tips are appreciated. Thanks in advance.
