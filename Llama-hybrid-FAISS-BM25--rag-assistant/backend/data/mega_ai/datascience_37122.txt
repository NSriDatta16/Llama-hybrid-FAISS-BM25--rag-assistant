[site]: datascience
[post_id]: 37122
[parent_id]: 
[tags]: 
Handling large word embedding matrix in Python

I have a pre trained Glove word embedding matrix (U) of dimension (400000 * 50). Now for the purpose of query expansion I need to perform the operation matmul(U*U.T). This is the term by term similarity matrix. But this leads to a matrix size of (400000*400000) and Python is throwing me memoryError. Is there a work around to this problem other than using a better machine ?
