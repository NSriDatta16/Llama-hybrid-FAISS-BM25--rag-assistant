[site]: crossvalidated
[post_id]: 431732
[parent_id]: 219001
[tags]: 
Suppose one die $X$ can produce the values $x_1 \lt x_2 \lt \cdots \lt x_m$ with probabilities $p_1,p_2,\ldots, p_m$ (respectively) and the other die $Y$ can produce values $y_1\lt y_2 \lt \cdots \lt y_n$ with probabilities $q_1, q_2, \ldots, q_n$ (respectively). Consider a game in which $X$ and $Y$ are independently tossed. For independent throws, the chance $X$ exceeds $Y$ is $$\Pr(X\gt Y) = \sum_{i=1}^m \sum_{j\mid x_i \gt y_j}^n p_iq_j.$$ From your (many) observations you can estimate the chances $p_i$ and $q_j$ and thereby estimate this probability. To assess the uncertainty in the probability estimate, consider bootstrapping the data. One bootstrap iteration consists of constructing dice $X^{*}$ and $Y^{*}$ having the observed probabilities and rolling the pair ten thousand times--that is, for as many times as you have observations--and tallying the frequencies of the events $X^{*} \gt Y^{*},$ $X^{*} \lt Y^{*},$ or $X^{*}=Y^{*}.$ After several bootstrap iterations--50 may do, 500 is good, and an efficient algorithm with a modern computer will permit many more--you can study the distributions of these frequencies. They reveal the likely uncertainties in the inferences you will be drawing from your data. Allow me to illustrate with a smaller dataset having more uncertainty. I have divided your counts by 100, producing two sets of 100 observations. Here are the results of 10,000 bootstrap iterations: For example, the lefthand plot headed " $P(x \gt y)$ " shows the two bootstrapped dice tended to tie about 26% of the time, but the rates of ties varied from 15% to 40%. Proceeding to the right, we see $X^{*}$ was less than $Y^{*}$ about 41% of the time ( $X^{*}$ "lost" and $Y^{*}$ "won"), but this frequency varied from 25% to 55%; and $X^{*}$ won about 33% of the time, varying from 25% to 44%. Finally, at the far right is a histogram of the difference in winning frequencies. This difference averaged about -0.15 but extended from -0.4 to +0.1 within the bootstrap. The vertical red line is the threshold of zero: only 3% of all the results are positive. Put another way, in only 3% of the bootstrap iterations did $X^{*}$ win more often than $Y^{*}.$ The 3% number is a bootstrap "p-value" for testing whether $X$ has a lower rate of winning than $Y.$ As such, it is "one sided." At the outset you likely wanted to test only whether $X$ and $Y$ have different winning rates. That p-value for that "two sided" test is obtained by doubling the one-sided p-value to 6%. The two-sided bootstrap p-value of 6% is small enough to give some evidence that $X$ will tend to win less often than $Y.$ It often would not be considered "significant" evidence where only values of 5% or less are considered sufficient evidence. With the large amount data you actually have, the results are clear: on average $X^{*}$ wins 26.5% of the time, $Y^{*}$ wins 40.5% of the time, (and the remaining times they tie). The p-value is less than $10^{-4},$ which is so tiny we may infer with high confidence that $Y$ wins more often than $X:$ about 14% more often. (Notice the nature of this inference: from observed behaviors of $X^{*}$ and $Y^{*}$ we are making deductions about $X$ and $Y.$ That's the essential idea of the bootstrap.) In this particular bootstrap calculation (again with 10,000 iterations) the difference in winning frequencies ranges from -17% to -11%, reflecting some remaining uncertainty about how much inferior $X$ is to $Y:$ but inferior it is. Finally, note that the right hand histogram provides information not afforded by, say, a chi-squared test: its quantitative display of plausible alternative estimates shows you the amount of risk you undertake when making an inference about winning rates from your data. The following R code performed the bootstrapping and (after first dividing x and y by 100) created the figure. x `) x.lt.y y)`=sum(p[x.gt.y]), `P(x y)-P(x y)", ] - bootstrap["P(x y)-P(x 0))
