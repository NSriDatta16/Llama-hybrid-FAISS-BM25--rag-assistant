[site]: crossvalidated
[post_id]: 448236
[parent_id]: 447605
[tags]: 
If we're talking about standard GP regression where you're predicting a scalar output and assuming Gaussian noise, the posterior predictive distribution for a matrix of $n$ test points $X^* \in \mathbb R^{n \times d}$ is an $n$ dimensional multivariate Gaussian with mean and covariance given by $$\boldsymbol{\mu} = K(X^*,X) \left(K(X,X) + \sigma^2 I\right)^{-1} \boldsymbol{y}$$ $$\Sigma = K(X^*,X^*) - K(X^*,X) \left(K(X,X) + \sigma^2 I\right)^{-1}K(X,X^*)$$ $\boldsymbol{y}$ is the vector of training outputs, $X$ is the matrix of training input data, $\sigma^2$ is the noise variance. $K(X,Y)$ is the kernel matrix with $(i,j)^{th}$ entry $k(x_i,y_j)$ , where $k$ is your GP kernel. As with any multivariate Gaussian, you can get the marginal posterior predictive distributions (univariate Gaussians) for single datapoints by picking out the corresponding element of $\boldsymbol{\mu}$ and the corresponding diagonal element of $\Sigma$ . The posterior probability that a single prediction $f_i$ is less than some value then comes directly from the univariate normal cdf with mean $\mu_i$ and variance $\Sigma_{ii}$ . The best reference for GP-related questions is C. E. Rasmussen & C. K. I. Williams, Gaussian Processes for Machine Learning, MIT Press, 2006 . It's currently available in full, free of charge here .
