[site]: crossvalidated
[post_id]: 395488
[parent_id]: 20520
[tags]: 
I agree with the excellent answer by Xi'an , pointing out that there is no single prior that is "uninformative" in the sense of carrying no information. To expand on this topic, I wanted to point out that one alternative is to undertake Bayesian analysis within the imprecise probability framework (see esp. Walley 1991 , Walley 2000 ). Within this framework the prior belief is represented by a set of probability distributions , and this leads to a corresponding set of posterior distributions. That might sound like it would not be very helpful, but it actually is quite amazing. Even with a very broad set of prior distributions (where certain moments can range over all possible values) you often still get posterior convergence to a single posterior as $n \rightarrow \infty$ . This analytical framework has been axiomatised by Walley as its own special form of probabilistic analysis, but is essentially equivalent to robust Bayesian analysis using a set of priors, yielding a corresponding set of posteriors. In many models it is possible to set an "uninformative" set of priors that allows some moments (e.g., the prior mean) to vary over the entire possible range of values, and this nonetheless produces valuable posterior results, where the posterior moments are bounded more tightly. This form of analysis arguably has a better claim to being called "uninformative", at least with respect to moments that are able to vary over their entire allowable range. A simple example - Bernoulli model: Suppose we observe data $X_1,...,X_n | \theta \sim \text{IID Bern}(\theta)$ where $\theta$ is the unknown parameter of interest. Usually we would use a beta density as the prior (both the Jeffrey's prior and reference prior are of this form). We can specify this form of prior density in terms of the prior mean $\mu$ and another parameter $\kappa > 1$ as: $$\begin{equation} \begin{aligned} \pi_0(\theta | \mu, \kappa) = \text{Beta}(\theta | \mu, \kappa) = \text{Beta} \Big( \theta \Big| \alpha = \mu (\kappa - 1), \beta = (1-\mu) (\kappa - 1) \Big). \end{aligned} \end{equation}$$ (This form gives prior moments $\mathbb{E}(\theta) = \mu$ and $\mathbb{V}(\theta) = \mu(1-\mu) / \kappa$ .) Now, in an imprecise model we could set the prior to consist of the set of all these prior distributions over all possible expected values , but with the other parameter fixed to control the precision over the range of mean values. For example, we might use the set of priors: $$\mathscr{P}_0 \equiv \Big\{ \text{Beta}(\mu, \kappa) \Big| 0 \leqslant \mu \leqslant 1 \Big\}. \quad \quad \quad \quad \quad$$ Suppose we observe $s = \sum_{i=1}^n x_i$ positive indicators in the data. Then, using the updating rule for the Bernoulli-beta model, the corresponding posterior set is: $$\mathscr{P}_\mathbf{x} = \Big\{ \text{Beta}\Big( \tfrac{s + \mu(\kappa-1)}{n + \kappa -1}, n+\kappa \Big) \Big| 0 \leqslant \mu \leqslant 1 \Big\}.$$ The range of possible values for the posterior expectation is: $$\frac{s}{n + \kappa-1} \leqslant \mathbb{E}(\theta | \mathbb{x}) \leqslant \frac{s + \kappa-1}{n + \kappa-1}.$$ What is important here is that even though we started with a model that was "uninformative" with respect to the expected value of the parameter (the prior expectation ranged over all possible values), we nonetheless end up with posterior inferences that are informative with respect to the posterior expectation of the parameter (they now range over a narrower set of values). As $n \rightarrow \infty$ this range of values is squeezed down to a single point, which is the true value of $\theta$ .
