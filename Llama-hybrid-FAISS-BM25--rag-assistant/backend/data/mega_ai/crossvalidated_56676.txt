[site]: crossvalidated
[post_id]: 56676
[parent_id]: 
[tags]: 
Smoothing algorithm for irregular time interval

I have various sets of irregular interval time series data to which I want to apply some sort of smoothing algorithm to produce a good fit. I have attempted various methods which all were unsatisfactory. Loess - Too much of a tendency to overshoot/overreact to outliers Moving Average - The lag is unacceptable Example Dataset : I have read about the "Improved Holt Method for Irregular Time Series", but the paper was too difficult for me to understand and implement in C#. Can someone point me to a good method / algorithm which produces good smoothing? The method must be able to calculate the smoothed point at time $t$, without requiring $t+1$, etc., data. It also must be capable of dealing with multiple $y$ values for a given $x$ time.
