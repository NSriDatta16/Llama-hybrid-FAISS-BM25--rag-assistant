[site]: datascience
[post_id]: 26751
[parent_id]: 
[tags]: 
Why positive-unlabeled learning?

Machine learning can be divided into several areas: supervised learning, unsupervised learning, semi-supervised learning, learning to rank, recommendation systems, etc, etc. One such area is PU Learning , where only Positive and Unlabeled instances are available. There are many publications about this, usually involving a lot of mathematics... When looking at the literature, I was expecting to see methods similar to self-training (from semi-supervised learning), where labels are adjusted gradually according to the classifier margins. I don't think these is what practitioners from the area do, and I was unable to navigate the mathematics or to find a survey on PU learning. Could someone from the area perhaps clarify what said practitioners do? Why can they not just use a binary classifier where the negative class=unlabeled? Can negative labels exist among the unlabeled data? What is the goal and what metrics exist to evaluate said goal?
