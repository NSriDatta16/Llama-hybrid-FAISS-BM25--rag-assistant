[site]: crossvalidated
[post_id]: 548828
[parent_id]: 
[tags]: 
How to encode variable-length unordered data

I am trying to learn an encoding of data, but I want to do so in a way that doesn't depend on the order of one of the inputs. The data is given by $\{(X_i,y_i)\}_{i=1}^{n}$ where $n$ is the number of samples, and each $X_i \in \mathbb R^{m \times p_i}$ and $y_i \in \mathbb R^{p_i}$ . Note that the sizes of $X_i$ and $y_i$ are allowed to vary between samples. ( $p$ can be thought of as the number of reading from some sensor, with small values of $p$ corresponding to poor sampling conditions.) I want to encode each sample $(X_i,y_i)$ as a low-dimensional vector $z_i \in \mathbb R^\ell$ , then learn a decoding function $D$ . Unlike the traditional autoencoder approach, however, I want $D$ to be "vectorized" over $X_i$ , that is to say, $D : \mathbb R^m \times \mathbb R^\ell \to \mathbb R$ and satisfies $D(x_{ij}, z_i) \approx y_{ij}$ , where $x_{ij}$ denotes the $j$ -th row of $X_i$ and $y_{ij}$ is the $j$ -th element of $y_i$ . We can then reconstruct $y_i$ as $$\hat{y_i}=\left[\begin{aligned}D(x_{i1},z_i)\\D(x_{i2},z_i)\\\vdots\\D(x_{ip_i},z_i)\end{aligned}\right].$$ The reason for wanting such a $D$ is that we can now take a given latent vector $z_i$ belonging to sample $i$ , and some $x^*$ which was unseen in the dataset, and predict its corresponding $y$ value. My best approach so far relies on a recurrent neural network (RNN) and a heuristic, probabilistic argument. Suppose we sample $z_0$ from a prior distribution on the latent variable. For the encoding step, we could try to learn a function $\phi : \mathbb R^\ell \times \mathbb R^m \times \mathbb R \to \mathbb R^\ell$ which maps triples $(z,x,y)$ to some new latent vector $z'$ . $\phi$ can be regarded as an RNN, in particular, we can define the sequence $\{ z_i^{(k)} | 1 \leq k \leq p_i \}$ with $z_i^{0} = z_0$ by $$ z_i^{(k)} = \phi(z_i^{(k-1)}, x_{ik}, y_k) $$ Two important conditions need to hold: $z_i^{(p_i)} = z_i$ , and Order invariance: for any $z,x_1,x_2,y_1,y_2$ , we have $$\phi(\phi(z,x_1,y_1),x_2,y_2))=\phi(\phi(z,x_2,y_2),x_1,y_1))$$ so that the order in which readings are processed by $\phi$ does not matter. The issue is that the second condition is not satisfied in general by recurrent neural networks. I believe I need to consider some other type of architecture for $\phi$ (and for the encoder in general).
