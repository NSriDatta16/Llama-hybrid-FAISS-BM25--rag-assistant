[site]: crossvalidated
[post_id]: 490480
[parent_id]: 490222
[tags]: 
The Embedding layer is basically just a table that stores the embeddings. It can look up embeddings for provided indices and compute the gradients in the back-ward pass. The layer does not care where the gradients come from. You indeed can implement the word2vec algorithms in Keras, but this not how the Embedding layer is typically used. It is typically part of a more complex model (e.g., used as an input into an RNN) that is trained for some task (e.g., classification), and training signal comes from that task. It gets back-propagated through the entire network up to the embeddings, which is how the embeddings get the gradient to be updated with.
