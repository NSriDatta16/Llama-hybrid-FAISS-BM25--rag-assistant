[site]: crossvalidated
[post_id]: 249841
[parent_id]: 249175
[tags]: 
Regular DQN always has two networks since that's the way Q learning works and this provides more stable results than a Bellman Residual Minimization based approach (i.e. simply taking the gradient of the temporal difference error). Double Q learning is a variation of Q learning that keeps two estimates of Q and switches between them. But when used in the context of deep RL, it usually works a little bit different. The essential insight is that, when computing the target Q values, it can be beneficial to use different estimates of Q to select the next action and to determine the value of that action. When used with DQN, those two estimates may just be the online network and the target network but perhaps this is what you mean. van Hasselt, Hado, Guez, Arthur, and Silver, David. Deep Reinforcement Learning with Double Qlearning. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, 2016. http://arxiv.org/abs/1509.06461 .
