[site]: datascience
[post_id]: 107536
[parent_id]: 66703
[tags]: 
Convnets and dependency Convnets are structured in a way that spatial dependency is encoded in the model. They do not explicitly "reduce amount of dimensions". This means that while a fully contacted layer will consider relationships between all features, a convolutional layer will assume that neighbouring features are connected between them and whereas distant features are not dependent with each other. We could feed an image into a fully connected layer by lining up all pixels one under the other in a huge column and pass it to the neural network. However, that would be a very expensive and redundant computation, given we know that "links" between distant features have a very low probability of being inter-dependent. Network depth and summarisation As correctly stated in your question, a feed-forward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function an arbitrary precision (Universal approximation theorem). But, let's have a look at what a single hidden layer cannot do. 1. Non-linear modelling $\neq$ non-linear relationships Non-linear transformations or curvature (e.g. polynomial regression, logistic unit etc.) is often misread as non-linearity in model parameters (non-linear models). As an example, in the following neural network, the output of the first layer neuron would be: $$a_1=f(+) \ (1)$$ If our activation function $f(x)$ is linear, this is in essence linear regression: $$a_1 = wx + b\ (2)$$ If we added another layer with one neuron as in the image below and with linear activations, the output would be: $$a_1 = w_1x + b_1 \ (3), \ (first\ layer)$$ $$_2 = _2a_1+2 \ (4), \ (second\ layer)$$ This reduces to, $$a_2 = (w_2*w_1)x + w_2*b_1 + b_2\ (5)$$ The $(w_2*w_1)$ and $w_2*b_1$ terms in essence that the model is "non-linear in its parameters" but still "linear in the variables ( $x$ ). In essence, despite linear activation functions the 2-layer neural network is a nonlinear model of a linear relationship . Non-linearity in both the variables and parameters are important because you cannot replicate this two layer neural network with a single regression model and capture all effects of the model. Activation function (per layer): if non-linear, introduces non-linear relationships in the variables. Helpful for altering and/or expanding the solution space. Network depth (manier layers/connections): introduce non-linearities in the parameters which are necessary for non-degenerate solutions going beyond regression. In essence, the use of multiple hidden layers allows construction of hierarchical features at different levels of resolution [ 1 ] 2. Parametric nonlinear regression $\neq$ neural network On a similar point, it is common to depict neural networks as parametric nonlinear regression, multiple regression etc. Deep neural networks are a particular beast when the two different elements of nonlinearities as described above are combined; DNNs allow fitting models for an insane amount of parameters, impossible to fit with a Levenberg-Marquard for say nonlinear least squares. Although theoretically "parametric", in practise very few (if any) hyperparameters of a DNN are fixed a-prior and in fact the entirety of its modelling pipeline (from preprocessing to architecture and regularisation) are part of model tuning which usually results in fitting a model from a much bigger class of models. DNNs scale to huge datasets. Anyway, hope this helps. See also:\ Elements of Statistical learning https://stats.stackexchange.com/a/33891/110383 https://stackoverflow.com/a/61619406/11545502 https://stats.stackexchange.com/a/345065/110383
