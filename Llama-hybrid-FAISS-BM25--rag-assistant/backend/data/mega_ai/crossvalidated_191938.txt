[site]: crossvalidated
[post_id]: 191938
[parent_id]: 191935
[tags]: 
The process that generated data is the "true" model. I.e., if you had perfect knowledge about the world, this would be the "equation" you'd come up with to describe the interactions and processes that cause (truly unequivocally cause ) your dependent variable. Feature selection usually involves applying some sort of "domain" knowledge (what you know about the nature of the problem, theoretically). So, for example, if you were studying some medical problem, you could use a doctor's help in trying to pick the "features" (markers, test results, measurements, diagnostic history, genetics...) that the medical science (the theory) would "blame" for the outcome. So that's how it could aid in recovering the true process - by careful feature selection, you can weed out the irrelevant variables and focus on what truly matters. However, it does have a flip side: the theory could be wrong. And you could miss out on important variables that you wouldn't consider just because the theory of the day doesn't account for it. (There are other approaches to feature selection obviously, from hierarchical, step-wise approaches, to factoring / PDA, to deep learning, where the features are learned by the model... )
