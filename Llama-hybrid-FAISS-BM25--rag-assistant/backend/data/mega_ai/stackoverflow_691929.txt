[site]: stackoverflow
[post_id]: 691929
[parent_id]: 691922
[tags]: 
Here's a quick algorithm. While (points_left > 0) { Select a random point that is not already clustered Add point and all points within x distance that aren't already clustered to a new cluster. } Alternatively, read the wikipedia page . K-means clustering is a good choice: The K-means algorithm assigns each point to the cluster whose center (also called centroid) is nearest. The center is the average of all the points in the cluster â€” that is, its coordinates are the arithmetic mean for each dimension separately over all the points in the cluster. The algorithm steps are: * Choose the number of clusters, k. * Randomly generate k clusters and determine the cluster centers, or directly generate k random points as cluster centers. * Assign each point to the nearest cluster center. * Recompute the new cluster centers. * Repeat the two previous steps until some convergence criterion is met (usually that the assignment hasn't changed). The main advantages of this algorithm are its simplicity and speed which allows it to run on large datasets. Its disadvantage is that it does not yield the same result with each run, since the resulting clusters depend on the initial random assignments. It minimizes intra-cluster variance, but does not ensure that the result has a global minimum of variance. Another disadvantage is the requirement for the concept of a mean to be definable which is not always the case. For such datasets the k-medoids variant is appropriate.
