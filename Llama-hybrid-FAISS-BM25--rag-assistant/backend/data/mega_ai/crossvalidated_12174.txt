[site]: crossvalidated
[post_id]: 12174
[parent_id]: 
[tags]: 
Time taken to hit a pattern of heads and tails in a series of coin-tosses

Inspired by Peter Donnelly's talk at TED , in which he discusses how long it would take for a certain pattern to appear in a series of coin tosses, I created the following script in R. Given two patterns 'hth' and 'htt', it calculates how long it takes (i.e. how many coin tosses) on average before you hit one of these patterns. coin The summary statistics are as follows, hth htt Min. : 3.00 Min. : 3.000 1st Qu.: 4.00 1st Qu.: 5.000 Median : 8.00 Median : 7.000 Mean :10.08 Mean : 8.014 3rd Qu.:13.00 3rd Qu.:10.000 Max. :70.00 Max. :42.000 In the talk it is explained that the average number of coin tosses would be different for the two patterns; as can be seen from my simulation. Despite watching the talk a few times I'm still not quite getting why this would be the case. I understand that 'hth' overlaps itself and intuitively I would think that you would hit 'hth' sooner than 'htt', but this is not the case. I would really appreciate it if someone could explain this to me.
