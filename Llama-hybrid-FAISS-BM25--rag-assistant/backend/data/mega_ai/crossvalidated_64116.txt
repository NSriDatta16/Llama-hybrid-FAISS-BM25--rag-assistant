[site]: crossvalidated
[post_id]: 64116
[parent_id]: 64038
[tags]: 
Generally, people use error on a holdout set as a proxy for generalization error. I think a fair response is to say if using an l1 or l2 penalty reduced error on the holdout test, then whatever you were doing was probably overfitting. Now, as to why it works: for regression, you can consider a l2 penalty as a normal prior on the parameters. That is, it's direct to show that $$ \underset{ \boldsymbol{w} }{\operatorname{argmax}} \sum_{i=1}^{N} \log \mathcal{N}( y_{i} | \boldsymbol{w^{T}x_{i}}, \sigma^2) + \sum_{i} \log \mathcal{N}(w_j | 0, \tau^2) $$ is a MAP estimate. So the improvement from an l2 norm can be considered as the win from going from a mle to a map estimate. There are also some deeper connections to pca that I don't want to try to type in this box, but essentially, this is a shrinkage estimator that shrinks the directions we are most uncertain about $ \boldsymbol{w}$ more. One intuition about why a lasso may improve a model is if you have groups of highly correlated explanatory variables, lasso may help you drop some of them.
