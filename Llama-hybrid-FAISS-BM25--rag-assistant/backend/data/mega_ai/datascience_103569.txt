[site]: datascience
[post_id]: 103569
[parent_id]: 62658
[tags]: 
For anyone coming to this question from Google, I'll share my experience with building sentence embeddings. With a standard Bert Model you have three options: CLS: You take the first vector of the hidden_state , which is the token embedding of the classification [CLS] token Mean pooling: Take the average value across each dimension in the 512 hidden_state embeddings, making sure to exclude [PAD] embeddings Max pooling: Take the max value across each dimension in the 512 hidden_state embeddings, again exclude [PAD] If you're using the standard BERT, mean pooling or CLS are your best bets, both have worked for me in the past. However, there are BERT models that have been fine-tuned specifically for creating sentence embeddings. They're called sentence transformers and one of the easiest ways to use one of these is via the sentence-transformers library. Generally these models use the mean pooling approach, but have been fine-tuned to produce good sentence embeddings, and they far outperform anything a standard Bert Model could do. If you wanted to fine-tune your own BERT/other transformer, most of the current state-of-the-art models are fine-tuned using Multiple Negatives Ranking loss (ps I wrote that article). For this the model learns to distinguish between similar sentence pairs, and after a pretty short training session (just over an hour for me on RTX 3090) you can produce a good quality sentence transformer model. That being said, there are already many great pretrained models out there, there's a list of some of the better models here , although it isn't fully up to date - for example, flax-sentence-embeddings/all_datasets_v3_mpnet-base performs better on benchmarks than any of those listed.
