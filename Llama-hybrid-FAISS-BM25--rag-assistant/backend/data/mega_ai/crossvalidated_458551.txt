[site]: crossvalidated
[post_id]: 458551
[parent_id]: 
[tags]: 
Batch normalization leads to unstable validation loss

I'm working on a regression problem, and I'm trying to solve it using a simple multilayer perceptron with batch normalization. However, there are uncomfortably large fluctuations in the validation loss (see the first figure below). Interestingly, without the batch normalization the learning curve becomes much smoother (the second figure). What could be the reason for the fluctuations? I'm providing further details below. Dataset There is around 60 features and one target. Features are standardized. The training set contains 6M examples, and there are 300k examples in the validation set. Architecture The neural network consists of 5 blocks with the following composition: fully connected layer with 1024 neurons, ReLU activation, batch normalization (with default parameters from Keras). In the end a fully connected layer with a single neuron and linear activation is added. He uniform initialization is used for the weights. I tried varying the number of blocks and/or the number of neurons per hidden layer. The fluctuations seem to decrease for smaller networks. Training The training is done with Adam algorithm. The initial learning rate is 10 -4 , and it gets reduced by factor 5 when the validation loss fails to improve (these events are marked with dashed lines in the figures above). The batch size is 128. The relatively small initial learning rate and large batch size were an attempt to improve stability of the training. With larger learning rates and/or smaller batch sizes the fluctuations in the validation loss become larger. Observations Both when training with and without the batch normalization, the weights and biases in the resulting networks seem healthy: all values are well within the range (-1, 1), with some spread within each layer. In the batch normalization layers, β parameters stay around 0, while γ parameters are around 1 except for the last batch normalization layer, where a second (smaller) peak appears at γ ≈ 0. I think the latter just means that a number of neurons from that layer are not really used for the prediction. The moving means and variances in the batch normalization layers is the only place where I see something suspicious. The typical moving mean grows with layer index; for the last batch normalization layer its distribution is quite flat and spans a range (0, 8). The typical moving variance also grows; in the last layer it has a flattish distribution on a range (20, 1200).
