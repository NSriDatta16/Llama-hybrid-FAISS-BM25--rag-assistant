[site]: crossvalidated
[post_id]: 420381
[parent_id]: 420373
[tags]: 
To me, this question sounds like a constrained, dynamic, real-time dimension reduction problem. There are many possible methods which would address this and the choice is a matter for subjective judgement, predisposition, skill and training. Basing that choice on pairwise correlations alone would not be on my list of options. PCA (principal components analysis) is a classic approach to dimension reduction although it doesn't scale well to lots and lots of features (thousands? you didn't specify the quantity). Vowpal Wabbit is a machine learning method for fast, real-time, scalable analysis. It is worth a look as it is open-source and free to use. https://en.wikipedia.org/wiki/Vowpal_Wabbit Another approach would be to employ partial differential equations, an area which has seen a huge amount of developmental work, particularly wrt real-time, big data. One person doing some of the best work using PDEs is Nathan Kutz at the Univ of Washington. His published work is voluminous (see his Google Scholar citations) but this article is recent and looks like it would be relevant to your question, Data-driven multiscale decompositions for forecasting and model discovery https://arxiv.org/pdf/1903.12480.pdf Along with his articles, he also has many tutorial videos posted on Youtube, e.g., https://www.youtube.com/watch?feature=youtu.be&v=Oifg9avnsH4&app=desktop Tensors can be described as matrices of matrices , i.e., matrices which compress a larger set of matrices, a classic objective of dimension reduction. Google's Tensorflow is one open-source algorithm for implementing tensors. Their evangelists tout TF as being 'so easy to use, it frees you up from worrying about the details of programming, allowing you to think more about what the results mean' -- a questionable claim. Regardless, TF algorithms scale well. For instance, I've heard about a TF algorithm with 5 billion parameters but that would probably not be necessary for your purposes. Not having used TF for your specific objectives, I don't know how well it integrates with dynamic, real-time analysis but it should be on your list of options. Like Kutz, there is a TF Youtube channel with tutorials. Finally, you mention an issue with boundary conditions or constraints on the strength of the association. I foresee only problems with this rule. Why are you imposing it? First of all, correlation usually means Pearson correlation -- a measure of strictly linear association which is not at all representative of nonlinear relationships. Other metrics are available which do capture nonlinear dependence such as entropy, maximal mutual information criterion, distance correlations, and so on.
