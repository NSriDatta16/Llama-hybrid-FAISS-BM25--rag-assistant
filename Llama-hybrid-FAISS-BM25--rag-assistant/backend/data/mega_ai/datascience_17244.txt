[site]: datascience
[post_id]: 17244
[parent_id]: 
[tags]: 
Feature reduction convenience

In the field of machine learning, I'm wondering about the interest of applying feature selection techniques. I mean, I often read articles or lectures speaking about how to reduce the number of feature (dimensionality reduction, PCA), how to select the best features (feature selection etc). I'm not sure of the main purpose of this: Does feature reduction techniques always improve accuracy of the learned model? Or is it just a computational cost purpose? I would like to understand when it is necessary to reduce the number of features and when it is not, in order to improve interpretability or accuracy. Thanks!
