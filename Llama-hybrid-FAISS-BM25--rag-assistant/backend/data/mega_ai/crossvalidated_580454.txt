[site]: crossvalidated
[post_id]: 580454
[parent_id]: 580253
[tags]: 
You said: scoring and using an evaluation metric is the same But I guess I would question this idea and suggest that, at least in a typical XGBoost workflow, the concepts behind 'scoring' and 'evaluation metric' are slightly different. But only slightly. The evaluation metric is used to monitor the training process and watch for overtraining (for example). I think it does make sense for this to be something that reflects the magnitude of the error or bias, and RMSE is a good choice because it has the same units as the thing you're trying to predict. On the other hand, scoring is something we do after the model is fit (it's still an evaluation metric, but in the XGBoost API, I think this is the intended role for 'scoring'). We might use the score to compare between different hyperparameter choices, or between algorithms. Personally I like RMSE for scoring too, but I know lots of people like $R^2$ and perhaps expect it. Either way, I think makes sense to use a variety of metrics in most cases. Remember not to get fixated on scores (see Anscombe's quartet ) and to check the distribution of your residuals! Most people seem to forget this bit...
