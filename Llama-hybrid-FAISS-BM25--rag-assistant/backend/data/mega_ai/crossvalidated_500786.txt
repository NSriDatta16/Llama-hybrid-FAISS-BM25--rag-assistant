[site]: crossvalidated
[post_id]: 500786
[parent_id]: 500311
[tags]: 
To quote from Ronald Fisher himself (and intersecting with the answer by NicoFish as well as the answer by WHuber to the earlier question ): when defining the likelihood function (as a function of the parameter $\theta$ ), Fisher (in his 1912 undergraduate memoir ) warns against integrating it w.r.t. the parameter: “the integration with respect to [the mean parameter] $m$ is illegitimate and has no definite meaning with respect to inverse probability. [The likelihood is] a relative probability only, suitable to compare point with point, but incapable of being interpreted as a probability distribution over a region, or of giving any estimate of absolute probability.” And again in 1922: “[the likelihood] is not a differential element, and is incapable of being integrated: it is assigned to a particular point of the range of variation, not to a particular element of it”. He introduced the very term “likelihood” especially t o avoid the confusion with a probability density and to separate it and himself from Bayesian analysis: “I perceive that the word probability is wrongly used in such a connection: probability is a ratio of frequencies, and about the frequencies of such values we can know nothing whatever (…) I suggest that we may speak without confusion of the likelihood of one value of $p$ being thrice the likelihood of another (…) likelihood is not here used loosely as a synonym of probability, but simply to express the relative frequencies with which such values of the hypothetical quantity $p$ would in fact yield the observed sample”. [Which I understand as a defense of the relativity of the likelihood values, which connects with the point that it is only defined up to a multiplicative constant, as e.g. when comparing the likelihood for the entire sample and for a sufficient statistic.] Another point he makes repeatedly (both in 1912 and 1922) is the lack of invariance of the probability measure obtained by attaching a d $θ$ to the likelihood function $L(θ)$ and normalising it into a density: while the likelihood “is entirely unchanged by any [one-to-one] transformation” , this definition of a probability distribution is not. Fisher actually distanced himself from a Bayesian “uniform prior” throughout the 1920’s and opting for a uniform (improper) prior is indeed making an arbitrary choice of a particular prior. [Which connects with my critical answer that to turn the likelihood into a density, a dominating measure must first be chosen and that there is no compelling argument to make the Lebesgue measure as a default choice.] Fisher however made things more complicated when introducing the fiducial distribution in the 1950's since, to quote from Dennis Lindley (1958): according to Fisher (1956) [the fact that the fiducial distribution is consistent under sequential updating] is true, though no formal proof is given, for he says (p. 51) "The concept of probability involved [in the fiducial argument] is entirely identical with the classical probability of the early writers, such as Bayes". Again he says (p. 125) "This fiducial distribution supplies information of exactly the same sort as would a distribution given a priori". Although this is not exactly related with the original question, but for a uniform prior being used in both instances, let me point out that Lindley (1958) establishes that "therefore the fiducial argument is only consistent in the case of a single sufficient statistic when the distribution is of the gamma or normal forms, or transformable thereto."
