[site]: datascience
[post_id]: 12404
[parent_id]: 12402
[tags]: 
Given the definition for mutual information $$I(X;Y) = \sum_{y \in Y} \sum_{x \in X} p(x,y) \log{ \left(\frac{p(x,y)}{p(x)\,p(y)} \right) },$$ it follows from rearrangement of the summands $$I(Y; X) = \sum_{x \in X} \sum_{y \in Y} p(x,y) \log{ \left(\frac{p(x,y)}{p(x)\,p(y)} \right) }.$$ Hence $I(X; Y) = I(Y; X)$. Edit "$I(X;Y)$ measures the average reduction in uncertainty of $X$ that results from knowing $Y$" If you interpret $H(X) - H(X|Y)$ where $H(X)$ is the marginal entropy of $X$ and $H(X|Y)$ the conditional entropy of $X$ given $Y$ as the reduction in uncertainty, then it is equal to $H(Y) - H(Y|X)$ and $I(X; Y)$ (see Wikipedia ).
