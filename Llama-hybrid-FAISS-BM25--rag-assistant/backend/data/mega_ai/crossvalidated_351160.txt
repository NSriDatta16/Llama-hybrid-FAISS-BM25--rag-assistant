[site]: crossvalidated
[post_id]: 351160
[parent_id]: 349631
[tags]: 
Here are three intuitive ways to solve the problem: First normalize the feature importance of the features for each model to belong to 0-1 and then average the normalized feature importance values across the three models. Do the same as above, but instead of averaging perform weighted averaging of the feature importance. The weights in this case can be the performance of the models on your hold-out set. That way, you put more weight on your better performing models. In case you are interested in just ranking the features and you are not interested in their relative importance you can rank the features for each model and then average (or even weight-average) the corresponding ranks. For instance, the most important features has rank 1, the second most important feature rank 2 etc.. You do this across the three models and then you average the ranks. Of course, lower values suggest higher feature importance.
