[site]: crossvalidated
[post_id]: 351945
[parent_id]: 351105
[tags]: 
0) and 2) Moving Average Models. Suppose we are given nothing else than the following time series data time y 1: 0 -12.070657 2: 1 4.658008 3: 2 14.604409 4: 3 -17.835538 5: 4 11.751944 ... which looks like this: for the purpose of this toy example it is essentially sin(time) + disturbance (you can find the R code below). What do I mean by this weird moving average or most simple time series model? I talk about adding the past values of y as new columns. Lets take $k=3$ for example, then at every time $t$ we add $y_{t-1}, y_{t-2}, y_{t-3}$ as new columns: time y y_past_1 y_past_2 y_past_3 1: 0 -12.070657 NA NA NA 2: 1 4.658008 -12.070657 NA NA 3: 2 14.604409 4.658008 -12.070657 NA 4: 3 -17.835538 14.604409 4.658008 -12.070657 5: 4 11.751944 -17.835538 14.604409 4.658008 6: 5 14.331069 11.751944 -17.835538 14.604409 consider $t=3$. For this, $t-1 = 2$ and the value of y_past_1 (the value of y in the point in time just before the current $t=3$) is the value $y_{t-1} = y_2 = 14.604409$. Analogously, $t-2=1$ and the value of y_past_2 (the value of y two timesteps before the current $t=3$) is $y_{t-2} = y_1 = 4.658008$. Now what people do as a first shot is to compute a (linear) model with $y$ as a target variable and `y_past_1, ..., y_past_k$ as input features. These are also called 'lagged' variables because they are the same as the target variable just with a little lag in the time component. Now let us compute a linear model. What I get is essentially y ~ 0.4320*y_past_1 + 0.2457*y_past_2 + y_past_3*0.2361 + 0.3070 Huh, how can it be that we computed a linear model but the outcome is not linear? This happens as the function time -> y_time is not linear, i.e. the linear model is applied to the 'non linear pairs of values' (y_past_1, y_past_2, y_past_3) but nevertheless, adds these up linearly. That is what I mean by simple time series model: Take the past of some variable as input for the prediction of the new state. NB: We did not discuss the role of $K$. This parameter works as a smoothing factor, i.e. in terms of time series it determines how much the prediction will be a so-called high pass filter (K small, do not filter out sudden movements, i.e. high frequencies, K big then the predictions follows the sin function more smoothly and is not 'fooled' so much by sudden movements of the target variable: K=1: K=10: ) 1) I mean the following. Let us say that we consider the product PIZZA. We have two users, A and B. During the last year we have sent 20 advertising emails for pizza to each of these users. User A did respond 15 times by buying a pizza and user B did not respond at all. Now let us say that during the current day again we see user A and user B and we have the trigger to send them an ad email. We iterate through all our products and we arrive at the product pizza. Should we send A an ad for pizza? How about B? [of course we should send A the email because it had a high respond rate but we should probably not send B a pizza ad because apparently he/she does simply not like either our advertisement for pizza or pizza or has some other reason not to respond]. In that way for every poiont in time $t$ we should include the past $t-1, ..., t-K$ for each request and each user. That means that we do not have a "single past" for each user but for training but for every request in the training set we have a new unique 'past'... as in the example above: for every $t$, y_past_1 has a unique value, namely $y_{t-1}$. However, in your example, we do not simply take $y_{t-1}, ..., y_{t-K}$ into account but some function of it like so: For every request given by user $u$ at week $t$ we iterate over every product $p$ and for each product $p$ we go back $K=52$ weeks and check how often we sent the user $u$ an ad email for product $p$ (number sent ) and we count how often $u$ responded positively by buying the advertised product within one week after receiving the email (number positiveResponses ) and then we compute affinity = positiveResponses/sent and include that as a column for the current request. In that way the model should come up with the rule like 'if affinity for this product is high then I should send an ad for this product'. In that sense: you do not use a column for each week in the past but for each product you go back 52 weeks. 3) You seemed to be worried that the model could not figure out a certain rule like 'only if the values of column X and of column Y are high then predict TRUE, else predict FALSE'. However, whichever model you choose a model from the "first league of complexity" (i.e. anything other than linear models like neural nets, tree boosting methods like random forest, gradient boosting, geometric methods like SVM, ...) these models can figure out arbitrarily complicated regions if only the data tells them to do so (provably!!!). For example: for NN with just ONE HIDDEN LAYER(!) this [I believe] is the celebrated Stone Weierstrass theorem ( https://en.wikipedia.org/wiki/Stone%E2%80%93Weierstrass_theorem , NNs with just one hidden layer form an algebra). Example: SVM. Navigate your webbrowser to https://www.csie.ntu.edu.tw/~cjlin/libsvm/ . Scroll down to the java applet and place two different sets of colored points and play around with the hyperparameter C and g a little bit and you will see results like this: That means that even (much more complicated!) rules like the ones you formulated above will eventually be captured by the model. That's the reason I would not worry about that too much. EDIT: R code: library(data.table) set.seed(1234) dt = data.table(time = 0:250) dt = dt[, y := sin(time/100*2*pi)*30 + rnorm(dt[, .N], mean=0, sd = 10)] plot(dt$time, dt$y, type="l") lag = function(x, k, fillUp = NA) { if (length(x) > k) { fillUpVector = x[1:k] fillUpVector[1:k] = fillUp return(c(fillUpVector, head(x, length(x)-k))) } else { if (length(x) > 0) { x[1:length(x)] = fillUp return(x) } else { return(x) } } } K = 3 for (k in 1:K) { eval(parse(text=paste0("dt = dt[, y_past_", k, " := lag(y, k)]"))) } train = copy(dt) train = train[K:dt[, .N]] train = train[, time := NULL] model = lm(y ~ ., data = train) pred = predict.lm(object = model, newdata = train, se.fit = F) train = train[, PREDICTION := pred] plot(dt$time, dt$y, type="l") train = train[, time := K:dt[, .N]] lines(train$time, train$PREDICTION, col="red", lwd=2) Regards, FW
