[site]: crossvalidated
[post_id]: 363705
[parent_id]: 
[tags]: 
Clarification on the hypothesis space of a model

I'm self-learning machine learning and deep-learning with the Deep Learning Book from Goodfellow et al. but I'm struggling to grasp the concept of hypothesis space on a simple example. Let's consider a simple regression model where having inputs x we want to predict y. So: y = Xw where X is the design matrix and w a parameter we want to evaluate. In this case, we can say that the hypothesis space of our model is the set of degree 1 polynomial of x. Now, let say that instead of predicting y, we want to predict p(y|x). Let say our pmodel (our model probability) that we use follows a normal distribution: N(y|x, ^y(x,w), sigma) where y^ is prediction of the mean - our parameter - and sigma the variance (hyperparameter). If we use the maximum likelihood estimation, we end up to something like: y* = argmin ||^y-y||2. Once again, if we consider that ^y(x,w) is linear in x, we can replace ^y with Xw as mentioned on the first equation. My question is, what is the hypothesis space in this case ? In the first case, it was polynomial of degree 1, but now since we have a combination of parameters (i.e. one parameter depending on another parameter) I'm not really sure how this can be interpreted in terms of model hypothesis space.
