[site]: crossvalidated
[post_id]: 548384
[parent_id]: 
[tags]: 
Step-size adaptation of NUTS within Gibbs

I am trying to solve a hierarchical problem with a Gibbs sampler. I do not have closed-form expressions for the conditionals, thus I have to use another MCMC method within the Gibbs scheme to sample them. Since some of the conditionals are high-dimensional and heavy-tailed, I am using the NUTS variant of the Hamiltonian Monte Carlo method. Say $N$ and $N_b$ are the total number of samples and burn-in used in Gibbs, while $N_{\rm wG}$ is the number of samples used for the NUTS algorithim within Gibbs. Typically, $N_{\rm wG}\ll N$ since Markov chains associated to the individual conditionals are not required to reach stationarity at each Gibbs iteration (even $N_{\rm wG}=1$ ). However, I am using $N_{\rm wG}\approx 20$ as burn-in length for the NUTS within Gibbs so it can adapt and find a better step size $\eta$ at each Gibbs iteration. I consider the following options: Keep adaptating $\eta$ for the whole simulation. Stop the adaptation when reaching $N_b$ , and fix $\eta$ to the value of the previous Gibbs iteration. I show results for a conditional that is one-dimensional in the following Figure ( $N_b=5000$ ). In blue, the results for option 1 and in black for option 2. The problem with option 1 is that the resulting Gibbs samples (after burn-in) are highly correlated, while for option 2 they are almost independent (as one expects from a Gibbs scheme). However, the problem for option 2 is that the chain might get stuck and not doing a good exploration (see the chain in option 1, which still does not show stationarity). I am unsure if option 1 is somehow changing the convergence properties of the Gibbs algorithm, so I feel inclined towards option 2. But I am still puzzled by the results. I am wondering if anyone has experience with this? Which of the options feels "more correct"? Thanks in advance for the input !
