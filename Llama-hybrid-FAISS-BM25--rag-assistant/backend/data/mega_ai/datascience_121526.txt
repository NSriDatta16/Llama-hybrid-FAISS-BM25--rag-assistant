[site]: datascience
[post_id]: 121526
[parent_id]: 
[tags]: 
Transformers doubt

Basically here the $Q$ , $K$ and $V$ are passed through a linear layer to obtain the actual $Q$ , $K$ and $V$ for self attention mechanism and then we concatenate all of it. My doubt is, I thought the $Q$ , $K$ and $V$ were obtained through the input embedding $X$ . $$Q=XW_q$$ $$K=XW_k$$ $$V=XW_v$$ How come we are using the $Q$ , $K$ and $V$ and linearly projecting them to again get back $Q$ , $K$ and $V$ . Sorry if my doubt is stupid!
