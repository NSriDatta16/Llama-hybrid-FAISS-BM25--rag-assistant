[site]: crossvalidated
[post_id]: 626487
[parent_id]: 615573
[tags]: 
Each token of the sentence is embedded into a vector of dimension $d_{model}$ . When you do multi-head attention with $n$ heads, usually the key vector dimension is computed as $d_k = d_{model} // n$ . It is done this way so that increasing the number of heads will not increase the parameters of the model so that you can compare different configurations. If you have too many heads compared to the model dimension $d_{model}$ , then your key vector dimension could be too small. Thus, all of the tokens will have to be compressed to this low dimensional space, so your model will not perform well. On the other hand, if you have too few heads your model will again not perform well. Of course, there is some number of heads $n$ for which the model works good, but still each head will project the $d_{model}$ -dimensional embeddings into $d_k$ -dimensional queries, keys and values, with $d_k . Is this a reasonable thing to do? Well, yes, it is. Query and key embeddings donâ€™t have to be in the large $d_{model}$ -dimensional space, because they are only used to compute the attention score (a single number) between each of the tokens. Thus, a smaller space could easily do the job, and it would prevent the model from overfitting. And value embeddings are concatenated at the end to produce the final output of the layer, building back a $d_{model}$ -dimensional embeddings for each token.
