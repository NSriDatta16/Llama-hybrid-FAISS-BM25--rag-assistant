[site]: datascience
[post_id]: 120647
[parent_id]: 120604
[tags]: 
The main drawback of batch renormalization is that there are extra hyperparameters to tune. If your data meets the conditions where batch normalization works, then you will get no advantage from batch renormalization. In the example used in Figure 1 of Ioffe's paper Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models , the batch normalisation/renormalisation results are almost identical for a batch size of 32, so it appears that at least in some cases, you don't need a very large batch size before the benefit of batch renormalisation disappears. At this point, any effort/computational resources you spend tuning the batch renormalization hyperparameters are wasted. Where batch renormalization does make a difference it's still a trade-off. In Timon Ruban blog A Refresher on Batch (Re-)Normalization he says "As most things in life it is a trade-off between your time and your model’s performance. If you have outsourced your hyperparameter tuning to things like Bayesian optimization, it’s at least still a trade-off between computing resources and performance."
