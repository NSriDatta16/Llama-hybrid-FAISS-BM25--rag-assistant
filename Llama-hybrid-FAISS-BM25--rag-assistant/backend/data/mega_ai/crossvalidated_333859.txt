[site]: crossvalidated
[post_id]: 333859
[parent_id]: 333139
[tags]: 
TLDR: I think you can disentangle in principle, but in practice you would likely need a lot of data to achieve any reasonable precision/reliability. Mixed models are IMHO the way to go. Long answer: To me the bigger problem is not disentangling between-day and between-time variability but distinguishing between-day variability and other residual variability not accounted for. If you don't mind mixing the latter two, there is some hope. In the following I will use between-day variability to mean the total variability left after accounting for between-person and between-time variability. I think mixed models are the way to go. The key is to think generatively - how do you expect your data to have been created? One way to think about the probelm you describe is that there is a per-person true strength $\alpha_i$ which is modified by a Gaussian noise with sd of $\sigma$ independently for each day. Then there is a separate effect for each time point $\beta_t$ which I will assume is the same for all participants (it doesn't have to). $Y_{i,j}$ is the j-th measurement of subject i. Then $Y_{i,j} \sim N(\alpha_i + \beta_{time(j)} + \gamma, \sigma)$ $\alpha_i \sim N(0,\tau)$ $\beta_t \sim N(0,\kappa)$ Here $\gamma$ is the overall intercept and $\tau$ is the between-person variability, $\kappa$ the between time variability and $\sigma$ combines the measurement error, between-day variability. All of those are treated as model parameters. Since measurement error of your device is likely to be known and quite small and you are actually measuring the variable of interest (not a proxy), you can compute between-day variability simply by subtracting the known measurement error from $\sigma$. However, your data are only weakly informative about $\kappa$ and $\sigma$ individually (you only have one data point per subject to spot the difference). It might be prudent to use a fully Bayesian approach to quantify this uncertainty, as methods such as lme4 may not be reliable. The above model should be expressible in both INLA and brms which both should handle the uncertainty well (the former is computationally more efficient, while the latter is more flexible and IMHO better documented). You can also improve your ability to estimate $\beta_t$ by assuming that they are a continuous function of time (e.g. that $\beta_7$ is correlated with $\beta_{10}$ but less bound to $\beta_{21}$). Using splines or Gaussian process or autoregressive models (there is support for some of those in the aforementioned packages). With this assumption, you could probably even work with person-specific between-time variability. EDIT: It will also help if you have some form of horseshoe prior on the per-time and per-person coefficients e.g.: $\tau \sim HalfNormal(0,1)$ $\kappa \sim HalfNormal(0,1)$ The priors should obviously be modified to reflect the scale of the values you work with ($HalfNormal(0,1)$ should be OK if your data are somewhere between -5 and +5). Alternatively, you could assume a covariance strucutre on those parameters (once again, this is supported in INLA and brms out of the box, not sure about lme4).
