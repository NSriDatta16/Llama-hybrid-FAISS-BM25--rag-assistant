[site]: crossvalidated
[post_id]: 303211
[parent_id]: 303149
[tags]: 
What you had as performance measure is conceptually similar to an F-measure (also called F1 score). You took one score regarding the positive records and one other score regarding the negative records and then computed the geometrical mean of them. (Technically the square of the geometrical mean, but it doesn't matter for your comparison if you compare algorithms on your score or the square root of your score which would be the geometric mean.) This geometric mean penalizes situations where both measures diverge whereas the arithmetic mean would let one measure compensate for poor performance on the other. The F-measure takes the harmonic mean of two quantities, which penalizes differences even more than the geometric mean. The F-measure also averages not exactly the same quantities that you look at but precision and recall instead. What you did is unconventional there, but if you know that those two were the quantities of interest for you, why not. Cohen's $\kappa$ has the advantage that you can use it for multi-class problems in ways that your initial score, F-measure and also ROC AUC cannot be used out of the box. (There are extensions though.) You can just say that you are using Cohen's $\kappa$ for classification. Max Kuhn does it in his excellent book Applied predictive modelling . The same author also created the caret package in R. The issue with using $\kappa$ in classification is $p_e$ and how to compute it. In your case, the most prevalent class is the true class with $(45+25)/100$ records. A "classifier" that does nothing but assign all records to the majority class would already be right $70\%$ of the time. Therefore, it is not reasonable to use Cohen's definition of $p_e=0.544$ in your case since a much higher baseline agreement of $p_e=0.7$ can be reached effortlessly. That's the baseline you should compare your observed agreement $p_o$ to. (There are also discussions whether $p_e$ when based on the most prevalent class should be estimated over the training set, test set or the entire data set.) Some more general remarks: Only use special performance metrics if you know you have asymmetric costs of misclassification. If any error is as costly as any other, accuracy is the correct metric to compare performance on and it works out of the box for multi-class problems. If you have asymmetric costs of misclassification but you know all the concrete costs, optimize that objective function instead of any other metric. When comparing 200 algorithms (more likely many variants and differing parameters per algorithm for much fewer than 200 actual algorithms), be sure to compare not only point estimators of performance but some statistical test of performance difference and deal adequately with the problems arising from multiple comparisons (which is not at all easy with 200 of them).
