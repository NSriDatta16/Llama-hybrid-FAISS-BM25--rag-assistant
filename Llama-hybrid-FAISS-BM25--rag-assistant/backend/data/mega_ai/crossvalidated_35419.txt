[site]: crossvalidated
[post_id]: 35419
[parent_id]: 35358
[tags]: 
Additional to the procedures suggested by @Mark Hall there also is the work of Loh e.g. LOTUS and GUIDE as well as of Zeileis et al. MOB . As compared to functional trees and LMT, these algorithms come from the statistics literature and use ideas of statistical inference for tree induction. Their distinguishing features are that they are more or less "unbiased" in split variable selection ("biased" here means that if there is no real association between the response and the predictors, predictors with more possible split points have a higher probability to be selected for the split. This bias appears in algorithms like C4.5 and CART and derivatives like LMT and Gama's trees). Unbiasedness is desirable if the tree is to be interpreted, rather than just used as a machine for predictions. See this paper or this thesis for a comparison of some of the mentioned algorithms and what they can do or what they are good at. Now, what to use? I'm not exactly sure what you mean by "$x$ is a numerical attribute which corresponds to a probability of belonging to a particular class at a particular node", but if you want to regress given relative frequencies or estimated probabilities in the nodes, you can use a MOB with a beta regression model in the nodes, if you have a binary/k-ary outcome to be regressed you can use either LMT (I would not recommend it due to experience), LOTUS or MOB with binary/multinomial logistic regression.
