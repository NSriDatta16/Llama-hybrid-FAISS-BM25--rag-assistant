[site]: crossvalidated
[post_id]: 507739
[parent_id]: 507715
[tags]: 
Hi: When dealing with lagged dependent variables in time series models, things can become clearer if you write the model out this way: ( at first you used $u$ and then later you used $\epsilon$ . I will use $\epsilon$ ). First, use the lag operator: $ Y_t(1- \beta L) = \epsilon_t$ Then, divide both sides by $(1-\beta L)$ : $ Y_t = \sum_{i=0}^{\infty} \beta^{i} \epsilon_i$ So, $Y_t$ is dependent on the current $\epsilon$ and all past $\epsilon$ but $\epsilon$ is the only thing on the RHS so the OLS orthogonality condition doesn't really apply because there aren't any regressors for $\epsilon$ to be correlated with. Does that make sense ? $\epsilon$ is the only regressor in the model really. Note: This AR(1) model still cannot be estimated by OLS because the $\beta$ estimate will be biased but its not due to the condition you mentioned. The biased-ness arises in a complicated way and I can't remember how at the moment. There was another thread where someone explained how it arises but I forget the name of the thread. maybe search for "Why is AR(1) OLS estimate biased ?". UPDATE: Below is not what I was looking for but still quite useful-interesting. It explains the biasedness using your argument but the dependence is on PAST $\epsilon$ rather than the current $\epsilon$ . That dependence also violates the OLS condition. So, if you want to think of the reason as stemming from the argument below, that might be more suitable for you since it's closer to yours. Still, there's another argument for how the biasedness arises that focuses on the formula for $\hat{\beta}$ in OLS but I failed to find it. Anyway, sorry for confusion and maybe below is the best way to think about it since I failed to find the other argument !!!!! What's wrong if I fit the auto-regression with OLS?
