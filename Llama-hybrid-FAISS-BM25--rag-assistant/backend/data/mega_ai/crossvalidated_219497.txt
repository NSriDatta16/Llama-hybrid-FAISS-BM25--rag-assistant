[site]: crossvalidated
[post_id]: 219497
[parent_id]: 219487
[tags]: 
The short answer is that normalizing the coefficients will not affect the predictions, but it will mess up the estimated class probabilities. Don't do it. The coefficients don't represent the odds ratios but rather the feature weights . They can be negative. If a coefficient is strongly positive, it means that the corresponding feature is very much correlated with the positive class. If it is strongly negative, then its means that the feature is strongly correlated with the negative class. If the coefficient is close to zero, then it means that the feature is not correlated much with either the positive or the negative class. So if you want to compare the importance of each feature, you should compare the absolute values of the coefficients (and you can normalize them just for convenience, if you want, but don't use these normalized absolute coefficients to make predictions, only use them to compare feature importance). ( Edit : this assumes that the features have been normalized prior to training) This is probably all you need to know. Read on if you want to understand what would happen if you tried to normalize the coefficients. The decision function for logistic regression is: $h_\mathbf{\theta}(\mathbf{x}) = \sigma(\sum\limits_{i=0}^{n}\theta_i x_i)$ where $\sigma(t) = \dfrac{1}{1 + \exp(-t)}$ (the logistic function ) and $\mathbf{\theta}$ is the parameter vector, and $\mathbf{x}$ is the feature vector (including a bias term $x_0 = 1$) and $n$ is the number of features. The model's prediction $\hat{y}$ for the instance $\mathbf{x}$ is given by: $ \hat{y} = \begin{cases} 0 & \text{ if }h_\mathbf{\theta}(\mathbf{x}) Notice that $\sigma(t) \ge 0.5$ when $t \ge 0$, and $\sigma(t) $ \hat{y} = \begin{cases} 0 & \text{ if }\sum\limits_{i=0}^{n}\theta_i x_i If you normalize the feature vector, you get the new parameter vector $\bar{\mathbf{\theta}} = \dfrac{\mathbf{\theta}}{K} $. Since the coefficients can be negative, it would not make sense to divide them by the sum of coefficients (the sum could be negative or zero). So instead, let's define $K$ as the range of values (anyway, even if you choose another method for normalization, it does not change what follows). $K = \underset{i}\max(\theta_i) - \underset{i}\min(\theta_i)$ Look at what happens to the sum used for predictions: $ \sum\limits_{i=0}^{n}\bar{\theta}_i x_i = \sum\limits_{i=0}^{n}\dfrac{\theta_i}{K} x_i = \dfrac{1}{K}\sum\limits_{i=0}^{n}\theta_i x_i $ Everything just got multiplied by the constant $\dfrac{1}{K}$. If $K > 0$, the predictions don't change a bit, since $\dfrac{1}{K} \sum\limits_{i=0}^{n}\theta_i x_i$ has the same sign as $\sum\limits_{i=0}^{n}\theta_i x_i$. If $K = 0$ (which can only happen if all coefficients are equal), then $K$ is not defined (you can't normalize the coefficients). If you use another normalization technique, and end up with $K So normalizing the coefficients (by dividing them by their range of values) will not affect predictions. However, it will mess up the decision function $h_\mathbf{\theta}$. This function is used to estimate the probability of the positive class. By multiplying all coefficients by $\dfrac{1}{K}$, you will end up making the same predictions (since the sign is not affected), but the estimated probability will be higher or lower depending on the value of $K$. For example, if $K = 2$ then: $h_\mathbf{\bar{\theta}}(\mathbf{x}) = \sigma(\sum\limits_{i=0}^{n}\bar{\theta}_i x_i) = \sigma\left(\dfrac{1}{2} \sum\limits_{i=0}^{n}\theta_i x_i\right)$ This can't be simplified much: if you plot the curve of $\sigma(t)$ and compare it to $\sigma(\frac{1}{2}t)$, you will find that this updated model will be much less confident about its predictions. For no good reason.
