[site]: crossvalidated
[post_id]: 637861
[parent_id]: 555135
[tags]: 
My experience regarding your questions comes from training an NLP model for predicting similarity between two sentences, which is a value in [0,5]. To output a score between [0,5], the model uses a dense layer with output dimension of 2. The scores are normalized using Softmax. I used regular MSE for loss, where it's computed between probability of label 1 and target labels, which are normalized to the range [0,1]. For overfitting, in my case some 2000 iterations on a single batch of 8 examples were enough (around 3 minutes). To answer your questions: Train the model to predict the probability of a single category, and when computing loss (and in evaluation), normalize the target price range to 0,1. It probably depends on your model. I recommend plotting the loss after some number of iterations to see if it reduces as expected, and then continuing for as long as needed.
