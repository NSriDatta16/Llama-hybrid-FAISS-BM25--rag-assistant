[site]: crossvalidated
[post_id]: 632661
[parent_id]: 632523
[tags]: 
Model Comparison for GAMMs: AIC Both ways can be achieved in GAMMs, but I will note how they differ compared to other models, starting first with AIC. It is important to note how AIC is derived in mgcv , as it is slightly different from a typical AIC score. The way that AIC is derived in mgcv is detailed more specifically in Wood et al., 2016 , but in a nutshell, there were originally two types of AIC used for GAM models, the marginal AIC and the conditional AIC. Per Simon Wood's canonical text on GAMs: Marginal AIC is based on the (frequentist) marginal likelihood of the model: that is on the likelihood obtained by treating all penalized coefficients as random effects and integrating them out of the joint density of response data and random effects. The number of coefficients to use for the AIC penalty is then just the number of fixed effects plus the number of variance and smoothing parameters. Conditional AIC is based on the likelihood of all the coefficients at their maximum penalized likelihood (MAP) estimates. The number of coefficients in the penalty then has to be based on some estimate of the effective number of parameters, in order to account for the fact that the coefficient estimates are penalized. The problem with marginal AIC is that it underestimates the variance components and "oversmooths" so that it often favors simpler models to an extreme. Conditional AIC has the problem of neglecting smoothing parameter uncertainty, which leads to bias towards larger models. With respect to GAMMs, they are even more problematic per Wood et al., 2016: Greven and Kneib (2010) showed that this is overly likely to select complex models, especially when the model contains random effects: the difficulty arises because τ0 neglects the fact that the smoothing parameters have been estimated and are, therefore, uncertain (a marginal AIC based on the frequentist marginal likelihood, in which unpenalized effects are not integrated out, is equally problematic, partly because of underestimation of variance components and consequent bias toward simple models). Interestingly, BIC is not usually estimated in modern GAM models. As highlighted in the Wood paper: When viewing smoothing from a Bayesian perspective, the smooths have improper priors (or alternatively vague priors of convenience) corresponding to the null space of the smoothing penalties. This invalidates model selection via marginal likelihood comparison. Thus while AIC operates similarly in practice to how you normally think of it in other regression contexts, it is defined differently for GAMMs. In any case, the AIC calculated in mgcv accounts for these issues by adding a simple correction to the effective degrees of freedom when obtaining the AIC score and is straightforwardly calculated using AIC(fit) on the respective models (by extracting the corrected version of the log-likelihood and inputting it into the AIC function). Model Comparison for GAMMs: P-Values I'm not particularly a giant fan of $p$ -values as a means for comparing models, and this is particularly problematic for GAMs with random effects. The issue with a typical LRT is noted in Wood, 2017: It is tempting to try to compare GAMs using a generalized likelihood ratio test (appendix A.5, p. 411). One possibility is to use the frequentist marginal likelihood, counting the number of fixed effects plus number of smoothing parameters and variance parameters in order to obtain appropriate degrees of freedom. An alternative is to use the (conditional) likelihood along with effective degrees of freedom. Neither approach works for testing whether a random effect is needed, since in the marginal case the null model is restricting the variance parameters to the edge of the feasible parameter space, and in the conditional case we can not really view effective degrees of freedom as representing the number of unpenalized coefficients needed to approximate the penalized model. Simon Wood then shows some simulations which calculate AIC models on different types of GAMs and notes: As expected, the test is clearly useless for comparing models differing in random effect structure. For other comparisons the test seems to provide a reasonable approximation provided the smoothing parameter uncertainty correction is applied, which in practice requires use of REML or ML smoothing parameter selection. A comparison of the QQ plots shows just how bad the LRT is for random effects compared to fixed main effects and interactions: Thus the AIC score is a clearer contender for proper model selection for GAMMs compared to GAMs. Computational Details You also noted that you fit your model with the gamm function, which operates differently from the gam function (which also fits random effects). As noted by Gavin Simpson , the AIC from these models can vary considerably due to the way the AIC is calculated in each. I'm not as privy to the differences in AIC between both functions, but know that if you compare models, the models should be fitted to the same function. References Wood, S. N. (2017). Generalized additive models: An introduction with R (2nd ed.). CRC Press, Taylor and Francis Group. Wood, S. N., Pya, N., & Säfken, B. (2016). Smoothing parameter and model selection for general smooth models. Journal of the American Statistical Association, 111(516), 1548–1563. https://doi.org/10.1080/01621459.2016.1180986
