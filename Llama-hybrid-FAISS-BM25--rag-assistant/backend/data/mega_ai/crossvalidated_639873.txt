[site]: crossvalidated
[post_id]: 639873
[parent_id]: 
[tags]: 
Consequences of evaluating cross-validation goodness-of-fit *jointly* on all test folds

When performing a k -fold cross-validation, the standard method is to evaluate the error on each test fold and take the arithmetic average of the statistic. What if the test statistic is sensitive to sample size? Would it be statistically defensible to join the predicted values from the k test folds and evaluate the test statistic on this joined data (which will be the same size as the original data)? A simple illustrative example: We want to perform leave-one-out cross-validation on a series of data. We will evaluate the performance using the Nash-Sutcliffe Efficiency (NSE), which is a statistic frequently used in hydrology: $$NSE = 1-\frac{\sum_{t=1}^{T}(Q_o^t-Q_m^t)^2}{\sum_{t=1}^{T}(Q_o^t-\bar{Q}_o)^2}$$ Q is the discharge (our dependent variable), $t\in[1,2,...,T]$ are the timesteps, the $_o$ subscript indicates observed data and the $_m$ subscript indicates modelled data. The denominator is just the variance of the observed data. We are doing leave-one-out cross-validation, so $T=1$ , and thus the statistic is undefined. However, I could keep track of all the $Q_m$ and compute the NSE on the full time series. Is this ever done? Are there any theoretical problems with this?
