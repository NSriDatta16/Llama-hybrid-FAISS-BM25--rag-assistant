[site]: crossvalidated
[post_id]: 386981
[parent_id]: 386075
[tags]: 
This at most will be a partial answer (or no answer at all). First thing to note is that I agree with @dsaxton completely: all models "discriminate" (at least in some definitions of discrimination) as that is their function. The issue is that models work on summaries and averages and they assign things based on averages. Single individuals are unique and might be completely off the prediction. Example: consider a simple model that predicts the mentioned five star ranking based on one variable - age . For all people with the same age (say 30) it will produce the same output. However that is a generalisation. Not every person aged 30yr will be the same. And if the model produces different ranks for different ages - it is already discriminating people for their age. Say it gives a rank of 3 for 50 year olds and a rank of 4 for 40 year olds. In reality there will be many 50 year old people that are better at what they do than 40 year old. And they will be discriminated against. Should I use the gender (or any data correlated to it) as an input and try to correct their effect, or avoid to use these data? If you want the model to return the same outcome for otherwise equal men and women then you should not include gender in the model. Any data correlated to gender should probably be included. By excluding such covariates you can be making at least 2 types of errors: 1) assuming all men and women are equally distributed across all covariates; 2) if some of those gender-correlated covariates are both relevant to the rating and correlated with gender at the same time - you might vastly reduce the performance of your model by excluding them. How do I check the absence of discrimination against gender? Run the model on exactly the same data twice - one time using "male" and another time using "female". If this comes from a text document maybe some words could be substituted. How do I correct my model for data that are statistically discriminant but I don't want to be for ethical reasons? Depends on what you want to do. One brutal way to force equality between genders is to run the model on men applicants and women applicants separately. And then choose 50% from one group and 50% from another group. Your prediction will most likely suffer - as it is unlikely the best set of applicants will include exactly half men and half women. But you would probably be OK ethically? - again this depends on the ethics. I could see an ethical declaration where this type of practice would be illegal as it would also discriminate based on gender but in another way.
