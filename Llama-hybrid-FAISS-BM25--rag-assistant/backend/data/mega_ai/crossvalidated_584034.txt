[site]: crossvalidated
[post_id]: 584034
[parent_id]: 
[tags]: 
Why is there a difference between training and validation accuracy when both of them are pointing to the same subset?

I was training a model and I accidentally pointed the training and testing set to the same dataset. I was surprised by the fact the validation and training accuracy are not the same. What could be the cause of this? And, Why is the validation accuracy jumping around while the training accuracy is relatively stable? train_dir = 'dataset/train' test_dir = 'dataset/train' #should have been dataset/val train_ds = tf.keras.preprocessing.image_dataset_from_directory( train_dir, seed=1337, image_size=image_size, batch_size=16, color_mode='rgb', label_mode='categorical', ) val_ds = tf.keras.preprocessing.image_dataset_from_directory( test_dir, seed=1337, image_size=image_size, batch_size=16, color_mode='rgb', label_mode='categorical', ) The model is VGG16 with a modified top of dense and pooling layers. It does not have dropout, batch norm, or other regularization layers that would have behaved differently in the training and testing phases. model = VGG16(input_tensor=inputs,input_shape=input_shape,include_top=False) y = model.output y = layers.Dense(64, activation='relu')(y) y = layers.GlobalAveragePooling2D()(y) outputs = layers.Dense(num_classes, activation="softmax")(y) model =keras.Model(inputs, outputs) model.summary() Model: "model" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 224, 224, 3)] 0 block1_conv1 (Conv2D) (None, 224, 224, 64) 1792 block1_conv2 (Conv2D) (None, 224, 224, 64) 36928 block1_pool (MaxPooling2D) (None, 112, 112, 64) 0 block2_conv1 (Conv2D) (None, 112, 112, 128) 73856 block2_conv2 (Conv2D) (None, 112, 112, 128) 147584 block2_pool (MaxPooling2D) (None, 56, 56, 128) 0 block3_conv1 (Conv2D) (None, 56, 56, 256) 295168 block3_conv2 (Conv2D) (None, 56, 56, 256) 590080 block3_conv3 (Conv2D) (None, 56, 56, 256) 590080 block3_pool (MaxPooling2D) (None, 28, 28, 256) 0 block4_conv1 (Conv2D) (None, 28, 28, 512) 1180160 block4_conv2 (Conv2D) (None, 28, 28, 512) 2359808 block4_conv3 (Conv2D) (None, 28, 28, 512) 2359808 block4_pool (MaxPooling2D) (None, 14, 14, 512) 0 block5_conv1 (Conv2D) (None, 14, 14, 512) 2359808 block5_conv2 (Conv2D) (None, 14, 14, 512) 2359808 block5_conv3 (Conv2D) (None, 14, 14, 512) 2359808 block5_pool (MaxPooling2D) (None, 7, 7, 512) 0 dense (Dense) (None, 7, 7, 64) 32832 global_average_pooling2d (G (None, 64) 0 lobalAveragePooling2D) dense_1 (Dense) (None, 3) 195 ================================================================= Total params: 14,747,715 Trainable params: 14,747,715 Non-trainable params: 0 _________________________________________________________________ This the training result
