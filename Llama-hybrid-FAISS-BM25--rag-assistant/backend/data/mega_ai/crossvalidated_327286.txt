[site]: crossvalidated
[post_id]: 327286
[parent_id]: 
[tags]: 
How is PCA applied to new data?

I understand the basic intuition behind PCA: reducing the dimensionality of data by finding the eigenvectors along which there is most variance in the data, and projecting the data along these eigenvectors (the principal components). What I don't understand is the following: How are the eigenvectors found? A standard eigenvector equation is given by $Av=\lambda v$, where $\lambda$ and $v$ are the eigenvalues and eigenvectors respectively. So what is the $A$ matrix - the data itself, or the covariance matrix of the data... or something else? (If the data matrix isnt square then this equation doesn't hold.) Once PCA has been performed / trained on a data set, can it be applied to reduce the dimensionality of new unseen data? For this to be true, I suppose a mapping would need to be output by PCA, and this mapping could be applied to the new data, say in the form of matrix multiplication. What are the outputs of PCA? How are the outputs applied to new data, if at all?
