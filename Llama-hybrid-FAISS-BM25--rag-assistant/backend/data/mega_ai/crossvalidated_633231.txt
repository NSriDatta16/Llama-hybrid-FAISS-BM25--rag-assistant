[site]: crossvalidated
[post_id]: 633231
[parent_id]: 632935
[tags]: 
I'd like to add two reasons. One is that for the very first estimate of an effect, the p-value is useful for deciding what to do next. Do I bother collecting more data or running another study? Budgets are finite, after all. But you seem to be limiting your question to situations where prior estimates from prior studies exist to be compared with the current finding. With respect to that context, there is another reason, As I understand it, the idea of null-hypothesis significance testing came in an era of one-off, small samples. You needed something to tell you what would happen if you were to collect more data or draw more samples, because you could not (or it was the norm to not) do those things. For example, in my field, classic studies report results from ANOVAs with 10 observations to a cell. p-values really mattered for those contexts, but don't ask me why they didn't just get larger samples or replicate their finding like we do now â€“ I do not know the history of that. One ironic limitation of the p-value is that it tends to get smaller as the sample size gets larger, meaning that with a large enough sample, you pretty much always (definitely always?) end up with "statistical significance". In my field, it is especially frustrating to me that journals are requiring larger samples sizes and more within-study replications, yet still will not publish your work without p-values for each and every estimate. Do they not understand how p-values work? Probably. Or it's just inertia, I truly do not know. If a paper reports 3 studies with large samples, along with 3 successful replications (in the sense of predictions about estimates failing to be falsified), but the effect sizes were all trivially greater than zero, the p-values would still be significant if the samples were large enough. If the effect sizes were reasonably large, then frankly the p-values just get in the way - they are going to be significant still, making them redundant with the effect sizes. In studies like these, they at best add nothing, and at worst allow researchers to say "statistically significant" for effects that are clearly not significant in any other meaningful sense. Too long/didn't read: Why don't we rely on authors to do bayesian updating themselves? 1) it's useful for deciding which of two novel studies to follow up one. 2) it used to be useful for research limited to one-off, small samples (and still is, e.g., for animal testing where the priority is limiting the number animals that need to be sacrificed), and 3) excluding 1 and 2: I don't know! P.S. As one commenter replied, not everyone is an expert and therefore may not have a prior in their head that helps them put the current finding into context. For scholarly research, I might suggest that this could be easily overcome by e.g., journals requiring authors to report the estimates from prior literature, perhaps in a table, to give the reader that context. I personally would find this an improvement over the current practice of citing prior work verbally but then, in terms of the estimates themselves, functionally deceiving the reader into thinking that the current study is the first ever test of a given hypothesis by forcing them to use the p-value to decide whether or not to believe an effect is meaningful. Personally I would rather see the previous 5 estimates myself over the p-value of the current estimate. But that's just me!
