[site]: crossvalidated
[post_id]: 378392
[parent_id]: 226923
[tags]: 
One important thing to point out is that ReLU is idempotent. Given that ReLU is $\rho(x) = \max(0, x)$ , it's easy to see that $\rho \circ \rho \circ \rho \circ \dots \circ \rho = \rho$ is true for any finite composition. This property is very important for deep neural networks, because each layer in the network applies a nonlinearity. Now, let's apply two sigmoid-family functions to the same input repeatedly 1-3 times: You can immediately see that sigmoid functions "squash" their inputs resulting in the vanishing gradient problem: derivatives approach zero as $n$ (the number of repeated applications) approaches infinity.
