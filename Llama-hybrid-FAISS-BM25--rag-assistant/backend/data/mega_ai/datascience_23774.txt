[site]: datascience
[post_id]: 23774
[parent_id]: 
[tags]: 
Median versus Average, how to choose?

I want to test how long it takes to run an algorithm . So here is what I am doing: close all the other un-needed applications, run my algorithms alone considering some unstable computer system factors, run multiple times As you can tell, I can get a set of running time t1, t2, t3, t4, ... , tn , so I'm asking: What to choose from t1, t2, t3, t4, ... , tn as my final result, median or average? Is this the right way to do this? btw: my algorithm is actually a rendering algorithm, so it would be more complicated.
