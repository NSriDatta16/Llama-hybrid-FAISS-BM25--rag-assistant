[site]: crossvalidated
[post_id]: 428441
[parent_id]: 428316
[tags]: 
The reward can be continuously distributed although the action space is finite and the state space is finite. Actually, one of the first examples in Reinforcement Learning is the multi armed bandit. That is a Markov Decision Process based on a Markov Decision Automata with the following properties: There is only one single state $s_0$ . There are finitely many actions $a_1, ..., a_N$ . Associated to each transition from $s_0$ into $s_0$ with action $a_j$ is a normally distributed reward $p(\cdot|s_0, a_j) \sim \mathcal{N}(\mu_j, \sigma_j)$ . The single state is just there because we need a state for formulating this problem in the language of Markov Decision Processes. Choosing an action $a_j$ corresponds to 'pulling the $j$ -th arm' at this bandit. This example is a little unfortunate because this single state makes the situation very artificial and the best policy seems to be very clear, namely just pull the arm $a_j$ such that $\mu_j$ is maximal. However, unlike in the situation of chess, pong, go, ... we do not know these rewards and the goal is not to 'express' the best policy but to actually find it given that we do not know the rewards (i.e. their parameters)! The usual algorithms to tackle such a prpblem is a balanced mixture between 'exploit' (i.e. pull the arm for which we have a certainty that it gives a lot of reward) and 'explore' (i.e. at a few points in time, pull a random arm and risk to fall short on reward for the benefit of determining the parameters better and reducing uncertainty). NOTE: in the most clean and natural setting, many of the things are probabilistic, i.e. in the very basic definition of a Markov Decision Process, the state variables $S_t$ and action variables $A_t$ and the reward variables $R_t$ are not functions but they are all random variables ! Also the transition function from one state $s_t$ into a new one $s_{t+1}$ is probabilistic (but the whole problem might be 'degenerate' in the sense that $p(s_{t+1}|s_t,a_t)=0$ for all but one value of $s_{t+1}$ like it is the case in Tic Tac Toe, chess, go, ... because given a current situation and given that a player has chosen to put his 'x' mark at the top left, the next state in Tic Tac Toe will have an 'x' at the top left (deterministically and surely). However, in other examples like the game 'Icy Lake', it is actually senseful to have these as probabilities because the lake is icy and even if the player wants to go 'down', there is only a certain chance that the player will actually go down...
