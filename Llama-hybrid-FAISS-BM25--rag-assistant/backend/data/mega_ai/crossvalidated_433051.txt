[site]: crossvalidated
[post_id]: 433051
[parent_id]: 
[tags]: 
Advantage term in A2C

I am trying to understand A2C algorithm by an article Understanding Actor Critic Methods and A2C by Chris Yoon An author provides the following formula to compute an Advantage term: \begin{equation} A(s_t, a_t) = r_{t+1} + \gamma*V(s_{t+1}) - V(s_{t}) \end{equation} I understand that formula as: an advantage of taking an action $a$ in a state $s$ equals to a reward received in the next state plus discounted value of a next state minus a value of a current state . Let's say an episode consists of 5 states and discount factor $\gamma = 0.9$ : An array of rewards for each state: rewards = [10, 11, 10, 9, -12] An array of values for each state: values = [7, 13, 14, 10, -11] An array of done flags for each state: dones = [False, False, False, False, True] \begin{equation} A(s_1, a_1) = 11 + 0.9*13 - 7 \\ A(s_2, a_2) = 10 + 0.9*14 - 13 \\ A(s_3, a_3) = 9 + 0.9*10 - 14 \\ A(s_4, a_4) = -12 + 0.9*(-11) - 10 \\ \end{equation} First of all, is my calculation correct ? How to calculate an advantage of taking an action in a terminal state $A(s_5, a_5)$ ? Update: I populate rewards and values array in a following fashion: rewards = [] values = [] dones = [] state = env.reset() done = False while not done: action, value = agent.get_action(state) state, reward, done = env.step(action) rewards.append(reward) values.append(value) dones.append(done)
