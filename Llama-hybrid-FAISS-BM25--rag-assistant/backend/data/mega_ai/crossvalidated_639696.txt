[site]: crossvalidated
[post_id]: 639696
[parent_id]: 639628
[tags]: 
Sometimes it is easier to look at a visual representation to better understand the as extreme or more part of the definition of the p-value, or the p-value itself. Suppose you repeat an experiment over and over again under the exact same conditions. You measure 10 samples and calculate the sample means, standard errors, and t-statistics for each experiment. The t-statistic in its simplest form is just the difference of the sample mean and some hypothesized parameter value (let's just for simplicity say it's zero) divided by the standard error of the mean of the respective sample: $$t=\frac{\bar{x}-\mu_{0}}{s/\sqrt{n}}$$ Then you plot the t-statistics in a histogram or density plot (see purple line in the plot below). This distribution is also known as a sampling distribution, which fully characterizes your experiments with respect to the underlying unknown parameter. The sampling distribution will eventually converge to Student's t-distribution whose area under the curve will sum up to 1 (shaded blue and red areas in the plot below). Now let's forget about the sampling distribution, which we never directly observe anyway, and instead focus on Student's t-distribution which serves as a very good approximation of the sampling distribution (provided the assumption of independent and random sampling has been satisfied). The shape of the distribution only depends on the degrees of freedom of your sample ( $n-1$ ). Now you can ask for example which values will I observe on average 95% of the time given this experiment. This would be the blue-shaded area and all values that fall within this area would be in agreement with the null hypothesis that those values were generated from the same data generating process (experimental condition). But we still have to account for the remaining 5% under the curve. We can shove those into the tails, i.e. 2.5% left and 2.5% right, which are the red-shaded areas. Any values that would fall into those extremes (extreme in the sense of far away from the mean of the distribution), would not be in agreement with the null hypothesis and we would reject the null hypothesis, meaning that we believe that those values have likely originated from a different experimental population. This would be a significant result and the probability of observing these values is 0.05 or smaller than that. Remember areas under the curves represent probabilities, so if you fall into the red extremes, the area is small and hence the probability or p-value is small. From that you can also see that the p-value is always a probability statment about the observed data and not a probability about the null or alternative hypothesis being true. A small p-value indicates that the observed data is unlikely (but not improbable) under the null hypothesis provided the null hypothesis is true. Here's the code I used, just in case you were wondering. You can change those values and play around with it. Simulation is very helpful in understanding these concepts. # required package library(tidyverse) # create a simulation function for 95% confidence intervals and p-values simulation % bind_rows() %>% mutate(experiment_id = 1:length(sim)) -> draws ggplot(data = draws_colored, aes(x = t_statistic)) + geom_area(fun = dt, args = list(df = n-1), stat = "function", fill = "red") + geom_area(fun = dt, args = list(df = n-1), stat = "function", fill = "steelblue", xlim = c(qt(p = 0.025, df = n-1), qt(p = 0.975, df = n-1))) + stat_function(fun = dt, args = list(df = n-1), color = "steelblue", linewidth = 1) + geom_density(color = "purple", linewidth = 1.5) + scale_x_continuous(limits = c(-5,5)) + labs(x = "t-values", y = "Probability density", title = paste("t-distribution with",n-1,"degrees of freedom"))
