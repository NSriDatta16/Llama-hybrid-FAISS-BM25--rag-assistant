[site]: crossvalidated
[post_id]: 3510
[parent_id]: 3458
[tags]: 
It's important to bear in mind that there's no one algorithm that's always better than others. As stated by Wolpert and Macready, "any two algorithms are equivalent when their performance is averaged across all possible problems." (See Wikipedia for details.) For a given application, the "best" one is generally one that is most closely aligned to your application in terms of the assumptions it makes, the kinds of data it can handle, the hypotheses it can represent, and so on. So it's a good idea to characterise your data according to criteria such as: Do I have a very large data set or a modest one? Is the dimensionality high? Are variables numerical (continuous/discrete) or symbolic, or a mix, and/or can they be transformed if necessary? Are variables likely to be largely independent or quite dependent? Are there likely to be redundant, noisy, or irrelevant variables? Do I want to be able to inspect the model generated and try to make sense of it? By answering these, you can eliminate some algorithms and identify others as potentially relevant, and then maybe end up with a small set of candidate methods that you have intelligently chosen as likely to be useful. Sorry not to give you a simple answer, but I hope this helps nonetheless!
