[site]: crossvalidated
[post_id]: 499359
[parent_id]: 499348
[tags]: 
For your model $y_t = \beta_0 + \beta_1 z_t + u_t$ , the formulas for the OLS coefficients estimates $\hat{\beta_0}$ and $\hat{\beta_1}$ are: $\hat{\beta_1} = \frac{\widehat{Cov}(y,z)}{\widehat{Var}(z)}$ , as you correctly pointed out, where $\widehat{Cov}$ and $\widehat{Var}$ are the sample covariances and variances respectively; in R: cov(y,z), var(y) where y and z are vectors; $\hat{\beta_0}=\bar{y}-\hat\beta_1\cdot \bar{z}$ , where $\bar y$ and $\bar z$ are the sample averages; in R: mean(y) and mean(z) where y and z are vectors; These values are found by minimizing $\sum_{i=1}^{n}{(y_i-(\hat\beta_0+\hat \beta_1 z_i))^2}$ with respect to $\hat\beta_0$ and $\hat\beta_1$ (i.e. set partial derivatives to 0 and solve). . Extra content: I would suggest using the matrix form though, since it generalizes to any number of coefficient estimates: $\hat\beta_{OLS} = (X^TX)^{-1}X^Ty$ , where $X$ is the regressor matrix, i.e. first column is just 1s, second column is regressor 1 (here: vector z), third column regressor 2 etc. $y$ is the response vector. The rows are the observations. This operation would yield a vector $\hat\beta_{OLS}$ with the first element corresponding to $\hat \beta_0$ , second element to $\hat \beta_1$ etc. In R: beta_OLS = solve(t(X)%*%X)%*%t(X)%*%y
