[site]: datascience
[post_id]: 115440
[parent_id]: 
[tags]: 
Why is it useful to use different word splitting with different tokenizers?

I have a problem. I have a NLP classification problem. There are different methods to decompose sentences into tokens, for example in whole words or in characters. Then there are different tokenizers like: TF-IDF Binary Frequency Count My question now aims, why should one make the effort and use a different word division (word or character) and then check this with the different tokenizers?
