[site]: crossvalidated
[post_id]: 290902
[parent_id]: 224009
[tags]: 
You may follow the derivation of dLdV in this link: https://github.com/go2carter/nn-learn/blob/master/grad-deriv-tex/rnn-grad-deriv.pdf . For the derivative w.r.t. $U$ (and similarly $W$): $$\begin{aligned} \frac{\partial E_t}{\partial U} &= \sum\limits_{k=0}^{t} \frac{\partial E_t}{\partial z_k}\frac{\partial z_k}{\partial U}\\ &= \sum\limits_{k=0}^{t} x_k \frac{\partial E_t}{\partial z_k}\end{aligned}$$ in which $z_k = Ux_k + Ws_{k-1}$. This is because of the fact that $U$ contributes to all $z_k$'s up to $k=t$. The second equation is a little subtle; please take a look at this link: https://math.stackexchange.com/questions/1621948/derivative-of-a-vector-with-respect-to-a-matrix . delta_t in your code is precisely the $\frac{\partial E_t}{\partial z_k}$, which needs to be updated for each decrement of $k$. The trick here is that: $$ \frac{\partial E_t}{\partial z_{k-1}} = \frac{\partial E_t}{\partial z_k} \frac{\partial z_k}{\partial z_{k-1}} $$ So we only need to compute $\frac{\partial z_k}{\partial z_{k-1}}$ for each decrement of $k$ and use it to update delta_t. Please let me know if it's still confusing.
