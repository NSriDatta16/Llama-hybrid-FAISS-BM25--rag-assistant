[site]: crossvalidated
[post_id]: 295582
[parent_id]: 232969
[tags]: 
This question has been answered here: With neural networks, should the learning rate be in some way proportional to hidden layer sizes? Should they affect each other? Short answer is yes, there is a relation. Though, the relation is not this trivial, all I can tell you that what you see is because the optimization surface becomes more complex as the the number of hidden layers increase, therefore smaller learning rates are generally better. While stucking in local minima is a possibility with low learning rate, it's much better than complex surface and high learning rate.
