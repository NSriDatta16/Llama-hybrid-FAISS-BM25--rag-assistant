[site]: datascience
[post_id]: 13900
[parent_id]: 13897
[tags]: 
Keras does not provide pre-trained word embeddings out of the box. Do you need to avoid using the Embedding layer entirely? If not, you can use that layer as an input for a matrix of pre-trained word vectors. You can load in your pre-trained embeddings as the initial layer weights using the weights property of the layer. If you set trainable=False then your word embeddings will not shift when you run your model. Here's a snippet of code from an example using pre-trained word vectors from the Keras Github repo: embedding_layer = Embedding(nb_words + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False) If you do need to avoid the Embedding layer entirely, then @Jan's probably gives you what you need.
