[site]: crossvalidated
[post_id]: 571541
[parent_id]: 571539
[tags]: 
Pedantic technical comment: I'm not convinced we can achieve independent samples from our MC algorithms in all cases. I think we can realistically aim for unautocorrelated samples. Aim for $\text{ESS} = N$ where $N$ is the number of (retained) iterations in your Markov chain. Remember that ESS = "Effective sample size". In an ideal world, the effective size of a sample should be the actual size of the sample. If $\text{ESS} then you should consider Thinning your chain (keeping only every $k$ -th sample). This reduces the effect of autocorrelation on the samples. Increasing the amount of burn-in (discarding the first $B$ samples). In many circumstances, the MC has to "wander towards" the region of high posterior density, if you keep this burn-in phase, you will decrease the ESS of your chain. Using a different MC algorithm. For example, changing the proposal mechanism of the MCMC scheme. Some algorithms are better suited to some tasks than others. Impossible to say what will work better without any additional information. You can also perform other diagnostics to check for uncorrelated samples. For instance, plot/compute autocorrelation coefficients of each parameter. If the plotted coeffs are all $\approx 0$ , it is likely you have achieved an unautocorrelated sample.
