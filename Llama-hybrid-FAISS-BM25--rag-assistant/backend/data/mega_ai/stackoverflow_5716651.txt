[site]: stackoverflow
[post_id]: 5716651
[parent_id]: 
[tags]: 
C++, How floating-point arithmetic operations get optimized?

I observed a surprising behavior when testing simple arithmetic operations at limit cases, on an x86 architecture: const double max = 9.9e307; // Near std::numeric_limits ::max() const double init[] = { max, max, max }; const valarray myvalarray(init, 3); const double mysum = myvalarray.sum(); cout (Tested with MSVC in release mode, as well as gcc through Codepad.org. MSVC's debug mode sets average (2) to #INF .) I expected average (2) to be equal to average (1), but it seems to me the C++ built-in division operator got optimized by the compiler and somehow prevented the accumulation to reach #INF . In short: The average of big numbers doesn't yields #INF . I observed the same behavior with an std algorithm on MSVC: const double mysum = accumulate(init, init+3, 0.); cout (3); cout (3); cout (This time however, gcc set average (2) to #INF : http://codepad.org/C5CTEYHj .) Would someone care to explain how this "effect" was achieved? Is that a "feature"? Or can I consider this an "unexpected behavior" instead of simply "surprising"? Thanks
