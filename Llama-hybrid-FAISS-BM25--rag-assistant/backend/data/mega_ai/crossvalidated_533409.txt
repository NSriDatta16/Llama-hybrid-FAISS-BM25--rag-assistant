[site]: crossvalidated
[post_id]: 533409
[parent_id]: 
[tags]: 
More Tractable Approach To Causal Path EDA Than Exhaustive Posterior Probability of Models?

I've a dataset of 310 ecological variables aggregated at the State level (and normalized to per capita, etc.) on which I would like to do a causal path exploratory data analysis. In an initial, acausal, EDA I tried just averaging the r^2 for each variable correlated against all the others . (This after expanding the dataset to 930 by including the log and sqrt of each variable so as permit better normalization of variables that may have had exponential or polynomial distributions.) Such an r^2 average, I hoped, would provide a heuristic for rapidly pruning the correlation matrix to maximum parsimony during acausal (merely explanatory) path EDA. However, since I was most interested in doing causal path EDA, I looked around for an alternative to r^2 and found posterior probability of models as implemented in BMLiNGAM . After upgrading the Python implementation to use the in-development version of PyMC3 that relies on Aesara , I started the process running to do the entire matrix. As an example, here is one, directed, pair's result: { "2 * log(p(M)) - log(p(M_rev))": "4.504483502070855", "Infered causality": "log_SuicidesPercapita1990 -> AIDSPerHIVPositiveTest2001" } However, each such result takes nearly 2 minutes, resulting in projected completion time of 3 years. Eliminating the sqrt and log versions would reduce that to 4 months but this still seems exorbitant. Moreover, given the superior normality of many of these adjustments, it is unwise to preemptively prune them. It seems some sort of heuristic along the lines I had originally envisioned with r^2 coupled with the correlation matrix could radically speed up search for a parsimonious causal graph. What is a method that does this sort of search optimization?
