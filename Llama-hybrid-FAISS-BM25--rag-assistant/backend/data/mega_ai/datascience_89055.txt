[site]: datascience
[post_id]: 89055
[parent_id]: 89024
[tags]: 
There's a lot to say on this topic. First of all, outside of academia, there is an unofficial classification (more slanted toward business types) of machine learning systems (and thus neural networks), which splits them into three categories in order of increasing utility/autonomy: descriptive, predictive and prescriptive. A descriptive system summarizes data, a predictive one predicts future trends (or anything else not explicitly present in the data) and a prescriptive one recommends courses of action. Neural networks certainly do more than summarize data, so most of them fall in the predictive category, but with additional techniques they can and do reach the prescriptive category. To support decision making, as you mentioned, there are a number of desirable properties and capabilities a system should have, but the one I will focus on since you emphasized it is model interpretability, which will bring us further down the line to neural networks as methods of data analysis. Neural networks, as is, are eminently uninterpretable, black-box models: often, this leads people to adopt simpler models, such as decision trees or random forests. However, this isn't a fact of life: a lot of useful information can be extracted from what a neural network has learned, information that humans can understand. In the case of networks for simple tabular data (e.g. house prices), [https://eli5.readthedocs.io/en/latest/blackbox/index.html] offers permutation important to estimate the predictive power of features (e.g. surface area matters, color of walls not so much). It also offers a more complex method, LIME, which builds a surrogate linear model of the neural network (which is much more easily interpreted). LIME can also be used for CNNs. Other interpretability methods include partial plots and SHAP . Since we're on the topic of CNNs, other methods tailored specifically for them exist, such as the (basically industry standard by now) Grad-CAM, which highlights the contribution of parts of an image to the output of a CNN. If you consult literature review papers on explainable AI (XAI) , you will notice that one of the main goals is knowledge extraction from trained models. On top of all that, in my opinion, plain neural networks, without any fancy explanation method applied, can be considered data analysis methods as is. Image segmentation, semantic segmentation, autoencoding, for instance, sound like the definition you cited for data analysis, and they are all done with neural networks.
