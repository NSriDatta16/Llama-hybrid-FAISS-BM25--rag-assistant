[site]: crossvalidated
[post_id]: 319674
[parent_id]: 111957
[tags]: 
I also find Andrew Ng's notes confusing because there is a subtle point that isn't explained. What they say is that the noise $\epsilon$ has Gaussian distribution. This ends up being essential. If you look at the equation of a Gaussian: $$ Gau(x,y,\theta) = \frac{1}{\sqrt{2 \pi} \sigma} exp\left( \frac{(y - \theta^T x)^2}{2 \sigma^2}\right)$$ its just a function of 3 variables. When you have a value for all of the variables (i.e. its inputs are "fixed") it outputs some value. However, how you "fix" a value in probability matters a lot. If you "fix" a variable due to conditioning then it means that you do an integral and re-normalization, but simply fixing because you are querying what the probability of a certain value is observed does not change the form of the equation while condition does. In other words, they have (assume $\theta$ is not a r.v. nor bayesian for simplicity): $$ p(\epsilon) = p(y - \theta^Tx) = p(x,y;\theta) = p(x \mid y; \theta) = p(y \mid x; \theta) = \frac{1}{\sqrt{2 \pi} \sigma} exp\left( \frac{(y - \theta^T x)^2}{2 \sigma^2}\right) $$ i.e. they are all literally (and in analytic form) the same equations and return the same numbers. However, this only happens because things are Gaussian. If the noise were something else this would not happen. i.e. what happens for them is: $$ g(x,y;\theta) = p(x,y;\theta) = \frac{ p(x,y;\theta)}{ \int_y p(x,y; \theta) dy } = \frac{p(x,y;\theta)}{\int_x p(x,y ; \theta) dx} $$ which is not generally true (I only know it true for Gaussian distributions). For example, notice that the distribution for the noise $\epsilon$ is just a function of three variables (2 r.v./random and 1 not r.v./random) $x,y$ and $\theta$ . Depending on what type of condition and values $x,y$ have you will get different values. For example consider the following counter example I cooked up: this example just shows that conditioning does indeed change things quite a bit. In fact, conditioning tells you which table to choose $X \mid Y$ or $Y \mid X$ or simply the joint $X,Y$ . The value $\theta$ could hypothetically choose a different set of tables (obviously not shown). So the main points are: things work so nicely in Ng's example because things remain Gaussian because they started off Gaussian The way you fix things matters a lot. Just wondering what the probability is that you observe $Y=y$ is not the same as conditioning on the event $Y=y$ happening and then asking something else. Fixing due to conditioning changes the table, just fixing due to a query fixes a value but to does not change the which table your looking at. Appendix: Some other points I thought would be interesting is that in the context of MLE we are usually searching for some good value of $\theta$ . Thus, we want to get the $\theta$ we care most according to our objective. In this case since in practice usually we observe $x$ and then want to predict $y$ it makes sense to take $\theta$ that optimizes such a value i.e: $$ \theta \in arg \max_{ \theta } P( \cap^N_{n} Y = y_n \mid X=x_n ; \theta) $$ this matters because as I showed in the previous table, optimizing $p(x,y;\theta)$ or $p(x \mid y ; \theta)$ might be tractable (or not) but they are probably don't result in the same $\theta$ 's, since they might be optimizing different equations. So at Glen_b said in the question its because: Because you're trying to predict y from x ? seems obvious but it seems they might even result in different estimators if your not careful.
