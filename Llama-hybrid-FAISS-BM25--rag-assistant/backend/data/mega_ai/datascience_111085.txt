[site]: datascience
[post_id]: 111085
[parent_id]: 111078
[tags]: 
A standard decision tree (or random forest) predicts a probability for the instance to belong to the positive class (I'm assuming binary classification). This probability is based exactly on the same idea: given the features values leading to this leaf of the node, if the proportion of positive instances in this leaf (i.e. with these conditions on the features) is $p$ then a new instance is assigned $p$ as a probability to be positive. So basically you just have to obtain the predicted probability (instead of the class), and if this probability is close enough to 0.5 (e.g. between 0.4 and 0.6) you can predict 'not sure'. Naturally this probability is based on the training data. If the training data is not representative enough or a test instance is too different from the training data, then the probability would be meaningless.
