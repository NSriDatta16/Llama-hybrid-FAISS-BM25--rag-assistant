[site]: crossvalidated
[post_id]: 624278
[parent_id]: 621183
[tags]: 
There are few things that could be possibly negatively affecting your model's performance: Your confusion matrix shows a 120:49 negative:positive labels among your test data, which is relatively even for a class imbalance problem. It is possible that by oversampling you are introducing too much noise into your training data. Instead, I would change your implementation of the 'cross-validator' object ( cv=kfold in your code) to be a stratified K-fold. That way, your model is seeing an equal amount of each label within each iteration of the cross validation. I would also add scale_pos_weight to your parameter search, as that will affect how the model learns from a positive vs negative label. For example, you may want to have scale_pos_weight>1 so that your model is more sensitive to predicting the minority class correctly. Finally, if your code begins to take too long to run, you might be able to cut "n_estimators" . [EDIT] It is possible for boosting methods to overfit with too many trees, as opposed to something like Random Forests, but this is rare. See ISL , page 347. As @seanv507 pointed out, you can also incorporate the early_stopping_rounds method to avoid any possible overfitting.
