[site]: datascience
[post_id]: 56539
[parent_id]: 56537
[tags]: 
In your question, it's pretty hard to answer specifically what is going on as you don't share any details of what you're trying to do. However, I'll try to address the general case: In GANs (and other adversarial models) an increase of the loss functions on the generative architecture could be considered preferable because it would be consistent with the discriminator being better at discriminating. As the discriminator is trained, it actually changes the loss landscape of the generator, which in turn increases the loss of said generator. This is probably why. Another reason could be that the discriminator is getting too good and the generator is getting stuck, which often can be fixed by adjusting learning rates in the optimizers or increase the number of iterations of the adversarials. In this particular case I'd try decreasing the discriminator learning rate or increase the number of steps the generator is trained. I'd suggest reading the original Generative Adversarial Nets paper and perhaps also the Improved Techniques for Training GANs paper.
