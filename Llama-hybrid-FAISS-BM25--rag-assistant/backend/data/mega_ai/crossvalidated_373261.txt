[site]: crossvalidated
[post_id]: 373261
[parent_id]: 
[tags]: 
How can I remove the effect of one independent variable so I can standardize and compare the values of my dependent variable?

I have a table of television viewership data with each row being one series and the columns being various data about that series, e.g. name, time the series is on TV, length of an episode, how many households watched the show, and the average percent of the show that was watched among people who did watch it. There is a clear relationship between episode length and average percent views, with longer shows having lower average percent views and shorter shows having higher. I have included a screenshot of these plotted against each other. My question is, I want to compare average percent viewed between episodes without the effect of episode length to create an index with 1 being average and values above or below indicating better or worse to a standardized degree. My currents ideas are either... 1) Group series by episode time, find the average "average percent watched" for each group and index on that. 2) Create a linear regression of average percent views and episode length and compare individual episode APV scores to that line, being higher or lower. How should I go about this? Are either of those ideas statistically sound? Is there something else I should be doing? I am using R to work with this data, if that is relevant. I apologize if this is too vague, I'm struggling a bit with this and it has been some time since I have worked with this kind of data.
