[site]: crossvalidated
[post_id]: 50178
[parent_id]: 50086
[tags]: 
Partial answer to question 1: I have not really checked but I think that one simply obtains a conjugate family for model $(**)$ by considering the normal distributions on the within subspace (the range of the $X$ matrix) as prior distributions (conditionally to $\Sigma$) for the within parameter ${\boldsymbol \beta}$. Practically it suffices to project the posterior distributions of model $(*)$ on the within subspace, and that should be the Jeffreys prior of model $(**)$ when taking the Jeffreys prior for model $(*)$. Question 1bis : I am not entirely satisfied by the previous answer. What about the posterior distribution of $\Sigma$ ? (I will think later about this question because I'm too hungry now) Partial answer to question 1bis : Indeed, the partial answer to question 1 seems to be wrong. The Jeffreys posterior for the covariance matrix the multinormal model is inverse-Wishart with a mean related to the frequentist estimate $\hat\Sigma$ of $\Sigma$ in model $(*)$. Here we should have instead the variation around the fitted values of each response (see question 2bis), which are not the means of each response when there is a within-design. Partial answer to question 2: Since the within parameters are simply estimated by applying a linear projection to the estimated parameters of model $(*)$, the least-squares theory of model $(**)$ is inherited from the least-squares theory of model $(*)$. No ? And about $\hat\Sigma$ in model $(**)$ it should be $\frac{1}{n-1}({\boldsymbol y} - {\boldsymbol 1}_n \otimes X\hat{\boldsymbol \beta}){({\boldsymbol y} - {\boldsymbol 1}_n \otimes X\hat{\boldsymbol \beta})}'$ where $X \hat{\boldsymbol \beta} = P \hat{\boldsymbol \mu}^*$, $\hat{\boldsymbol \mu}^*$ is the estimate of ${\boldsymbol \mu}$ in model $(*)$, and $P$ is the projection on the range of $X$. In the example below, gls() returns $\frac{1}{n}({\boldsymbol y} - {\boldsymbol 1}_n \otimes X\hat{\boldsymbol \beta}){({\boldsymbol y} - {\boldsymbol 1}_n \otimes X\hat{\boldsymbol \beta})}'$ as the maximum likelihood estimate of $\Sigma$. Below is a R code for the Bayesian analysis using the bayesm package, with the timepoints example of my OP. ################################ #### SIMULATED DATA #### ################################ library(mvtnorm) m # compare expected posterior means with true means: > rowMeans(Beta.sims, dims = 2) [,1] [,2] [,3] [1,] 1.053459 1.956234 2.861015 > Mean [1] 1 2 3 > # theoretically these are also the frequentist estimates: > colMeans(Y) response.1 response.2 response.3 1.045015 1.956583 2.858293 > # compare posterior median Sigma with true Sigma: > apply(Sigma.sims, c(1, 2), quantile, probs = 0.5) [,1] [,2] [,3] [1,] 2.5455707 0.6033634 0.9018785 [2,] 0.6033634 1.4969095 0.6485045 [3,] 0.9018785 0.6485045 1.0620879 > Sigma [,1] [,2] [,3] [1,] 2.0 0.5 0.5 [2,] 0.5 1.0 0.5 [3,] 0.5 0.5 1.0 ############################################## ## PROJECTIONS OF THE POSTERIOR SIMULATIONS ## ## ON THE WITHIN-DESING SUBSPACE ## ############################################## # projection matrix: X # posterior summaries > summary(data.frame(alpha=sims["alpha",], beta=sims["beta",])) alpha beta Min. :-1.5637 Min. :0.3387 1st Qu.:-0.1171 1st Qu.:0.8134 Median : 0.1472 Median :0.9038 Mean : 0.1493 Mean :0.9038 3rd Qu.: 0.4106 3rd Qu.:0.9953 Max. : 2.0783 Max. :1.5786 > # frequentist estimates > P%*%colMeans(Y) [,1] alpha 0.1400198 beta 0.9066386 # note: maximum likelihood estimates from gls() differ from "frequentist estimates" library(nlme) fit coef(fit) (Intercept) timepoint 0.1430734 0.9054123 > ## the ML estimate of Sigma is the mean variation around the fitted values: > # calculate this variation > alpha.f beta.f fitted.frequentist crossprod(Y-rep(1,n)%*%t(fitted.frequentist)) response.1 response.2 response.3 response.1 54.45126 13.18208 19.65210 response.2 13.18208 32.16598 14.20923 response.3 19.65210 14.20923 22.76616 > # compare: > getVarCov(fit) * n Marginal variance covariance matrix [,1] [,2] [,3] [1,] 54.451 13.182 19.652 [2,] 13.182 32.166 14.209 [3,] 19.652 14.209 22.766 Standard Deviations: 7.3791 5.6715 4.7714 Update After these thoughts, I finally conjecture that the Jeffreys posterior is given by: \begin{align*} \Sigma \mid {\boldsymbol y} & \sim {\cal IW}_m(n, \Omega_n^{-1}) \\ {\boldsymbol \mu} \mid \Sigma, {\boldsymbol y} & \sim {\cal N}_{n \times m}\left(X\hat{\boldsymbol \beta}, \Sigma \otimes J_n^{-1} \right) \end{align*} with \begin{align*} J_n & = {(1,1\ldots,1)}'(1,1\ldots,1) \\ \Omega_n & = ({\boldsymbol y} -{\boldsymbol 1}_n \otimes X\hat{\boldsymbol \beta}){({\boldsymbol y} - {\boldsymbol 1}_n \otimes X\hat{\boldsymbol \beta})}' \end{align*} And then the posterior distributions of the within parameters are obtained by applying the projection $P$ on the within subspace to ${\boldsymbol \mu}$. When there is no within design this is the Jeffreys posterior for the multinormal model. Now I don't have the courage to check my conjecture.
