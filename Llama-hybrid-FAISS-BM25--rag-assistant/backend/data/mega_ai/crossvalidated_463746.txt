[site]: crossvalidated
[post_id]: 463746
[parent_id]: 463706
[tags]: 
They both fall into the general domain of graphical models . As you've pointed out, they are very similar to each other, for they both have hidden layers and both require iterative methods to perform inference tasks. But they are proposed on different initial ideas. "Neural network" was originally proposed by the connectionists and is now very active in the machine learning community , while "mixture model", or more general "latent variable models", is a category of classical models in the statistics community . Neural network (in machine learning) focus mainly on minimizing the prediction error, as long as the prediction error is minimized, it doesn't matter how you interpret the mathematic equations, or how many hidden layer/nodes you used in the model . On the other hand, mixture model (in statistics) focus mainly on maximizing the marginal likelihood, and every hidden layer and node matters because each of the hidden node or layer must have a corresponding real world explanation. The difference in initial purpose lead to some minor differences in the math equations and terms. For example the "activation function" in neural networks plays the same role as "conditional probability distribution function" in mixture models. Nowadays there's a tendency to unify the terms in different community with graphical model language. For example from graphical model perspective, no matter it's "activation function" or "conditional probability distribution function", they are all called "factors" .
