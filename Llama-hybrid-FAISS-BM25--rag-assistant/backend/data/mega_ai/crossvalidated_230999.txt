[site]: crossvalidated
[post_id]: 230999
[parent_id]: 176794
[tags]: 
This is why it's probably a better idea to use PReLU, ELU, or other leaky ReLU-like activations which don't just die off to 0, but which fall to something like 0.1*x when x gets negative to keep learning. It seemed to me for a long time that ReLUs are history like sigmoid, though for some reason people still publish papers with these. Why? I don't know. Dmytro Mishkin and other guys actually tested a network with plenty of different activation types, you should look into their findings on performance of different activation functions and other stuff . Some functions, like XOR, though, are better learnt with plain ReLU. Don't think about any neural stuff in dogma terms, because neural nets are very much work in progress. Nobody in the world actually knows and understands them well enough to tell the divine truth. Nobody. Try things out, make your own discoveries. Mind that using ReLU itself is a very recent development and for decades all the different PhD guys in the field have used over-complicated activation functions that we now can only laugh about. Too often "knowing" too much can get you bad results. It's important to understand that neural networks aren't an exact science. Nothing in maths says that neural networks will actually work as good as they do. It's heuristic. And so it's very malleable. FYI even absolute-value activation gets good results on some problems, for example XOR-like problems. Different activation functions are better suited to different purposes. I tried Cifar-10 with abs() and it seemed to perform worse. Though, I can't say that "it is a worse activation function for visual recognition", because I'm not sure, for example, if my pre-initialization was optimal for it, etc. The very fact that it was learning relatively well amazed me. Also, in real life, "derivatives" that you pass to the backprop don't necessarily have to match the actual mathematical derivatives. I'd even go as far as to say we should ban calling them "derivatives" and start calling them something else, for example, error activation functions to not close our minds to possibilities of tinkering with them. You can actually, for example, use ReLU activation, but provide a 0.1, or something like that instead of 0 as a derivative for x As for what's generally used, for tanH(x) activation function it's a usual thing to pass 1 - x² instead of 1 - tanH(x)² as a derivative in order to calculate things faster. Also, mind that ReLU isn't all that "obviously better" than, for example, TanH. TanH can probably be better in some cases. Just, so it seems, not in visual recognition. Though, ELU, for example, has a bit of sigmoid softness to it and it's one of the best known activation functions for visual recognition at the moment. I haven't really tried, but I bet one can set several groups with different activation functions on the same layer level to an advantage. Because, different logic is better described with different activation functions. And sometimes you probably need several types of evaluation. Note that it's important to have an intialization that corresponds to the type of your activation function. Leaky ReLUs need other init that plain ReLUs, for example. EDIT: Actually, standard ReLU seems less prone to overfitting vs leaky ones with modern architectures. At least in image recognition. It seems that if you are going for very high accuracy net with a huge load of parameters, it might be better to stick with plain ReLU vs leaky options. But, of course, test all of this by yourself. Maybe, some leaky stuff will work better if more regularization is given.
