[site]: crossvalidated
[post_id]: 419485
[parent_id]: 419479
[tags]: 
In the diagram you provided, there is a little box drawn on a region of the input image. That region is called the receptive field. In a convolutional layer of CNN, we apply multiple kernel filters (with different values) to that receptive field, which results in a separate feature map generated by each kernel filter. This is done so that each filter learns different features of the input image. As for the height and width of feature maps getting smaller after each layer, this may or may not happen in a convolutional layer, depending on whether padding is applied. Here is what happens with a hypothetical example without padding: Input image size: 32 x 32 Kernel filter size: 3 x 3 Resulting feature map size (from one filter): 30 x 30 If padding was applied above, then the resulting feature map size would still be 32 x 32. Your diagram also shows two pooling layers. Pooling is performed to reduce the dimensionality (to help with training time and overfitting issues). One type of pooling is max pooling where we just take the maximum value in a pooling window. If the input to the pooling layer has the dimensionality 32 x 32 x 10 (where 10 stands for 10 feature maps), then with a 2 x 2 pooling filter, we would reduce the input to 16 x 16 x 10, because we would take one maximum value for each 2 x 2 region of each feature map.
