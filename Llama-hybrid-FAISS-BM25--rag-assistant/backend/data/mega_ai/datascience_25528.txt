[site]: datascience
[post_id]: 25528
[parent_id]: 25482
[tags]: 
What you are looking for is feature contributions to the final score of an observation having a positive or negative price. Feature importances from a decision tree or random forest is not going to help you because a feature's importance is fixed across all observations (wines). They tell a story about the overall model, but nothing about the individual observations. So if you want to know what caused a wine having a negative or positive price, you should look at feature contributions. Let's elaborate a little bit. Let's forget about decision trees, and go for ensemble of them, say random forest, for all the good reasons. (I highly recommend you to read why ensemble models uniformly beat the base learner, and how they help with the high bias - high variance problem base learners might have.) The first step would be to label your dependent variable, price, as $1$ and $0$, depending on the price being positive and negative ($y = 1$ iff $price > 0$). After the typical exploratory analysis (where you can look at boxplots and stuff as a beginning study) and feature engineering, you can fit your random forest to predict a wine having a positive or negative price (i.e. $y$ being $1$ or $0$). Once you do that, you will have an estimated probability for each and every wine, representing the classification model's estimation of that wine's price being positive or negative. Something like this: Wine Prob A 0.4 B 0.7 C 0.9 D 0.3 These probabilities actually consist of the summation of a bias term and the individual contributions of all features to that observation only : $$P(y_i = 1) = bias + \sum_{i=1}^{m}(contributionOfFeature_i),$$ where $m$ is the total number of features. This is how you can analyze each and every feature's contribution on the final probability of a particulat wine's price being positive or negative. Regarding how to code this, if you are using scikit, you can use this very easy and convenient tool: https://github.com/andosa/treeinterpreter
