[site]: crossvalidated
[post_id]: 580307
[parent_id]: 
[tags]: 
Reinforcement Learning - what causes a plateau in the performance of the model during training?

I am working on a Reinforcement Learning problem, but since the underlying model is a neural network, I think this might have similarities to a supervised learning problem. Below is a screenshot of my model's mean performance over 5 million training steps: You can see that around 1-1.5M steps, it reaches a certain level of performance and just oscillates around that level for the rest of its training. I'm wondering, what can cause this behavior? I thought that if I keep training a neural network, it will keep overfitting more and more to my data. I realize that Reinforcement Learning is somewhat different, but my RL environment comes from a tabular dataset, so there's not an infinite amount of data/randomness/stochasticity. Why would the performance plateau like this? From knowing the data, I know for certain that it is possible to achieve a higher score on the data (maybe not on a test set, but for a model fitting itself to the training set, absolutely).
