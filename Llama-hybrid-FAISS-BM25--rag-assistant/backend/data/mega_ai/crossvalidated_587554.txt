[site]: crossvalidated
[post_id]: 587554
[parent_id]: 
[tags]: 
Predicted y on IPW (Inverse Probability Weighted) logistic regression

I am performing a logistic regression where the original data has unbalanced classes, 5% 1s and 95% 0s. Using IPW (either by calculating the weights myself or by choosing class_weight='balanced' in sklearn's LogisticRegression), I get in-sample predictions that, on average, are below 40%. More details on the calculation of weights: In this example, 1s are weighted by 1/0.05 and 0s are weighted by 1/0.95. So for both, 1s and 0s, the weight is inversely proportional to their sample frequency. The results are identical to using class_weight='balanced' in SKLearn's LogisticRegression (explained there as: The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y))) https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html Should I be expecting an average y-hat of around 50% and does not getting that point to model deficiencies? As a note, if I repeat the same regression without weighting the observations, accuracy statistics such as Area under Curve perform -slightly- worse.
