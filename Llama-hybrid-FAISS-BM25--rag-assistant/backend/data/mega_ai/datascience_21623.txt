[site]: datascience
[post_id]: 21623
[parent_id]: 21622
[tags]: 
The convolution neural networks take into consideration that an image already has a two-dimensional structure. This is a domain knowledge provided by humans and not something the neural network can pick up. For example, consider if I would give you the raw pixels shuffled in a flattened array (or any other signal for that matter). Would you be able to reconstruct its 2D structure? Probably this would be almost impossible (unless someone builds an algorithm to solve jigsaw puzzles at the pixel level!) Applying convolution with various strides we are looking for patterns in the patches of the image and not on the individual pixels. Using different strides we are explicitly expressing the amount of resolution that we are investigating the patterns of the image. So given that these convolution filters are trainable we are able to have an architecture that works well with images. In fact, it would work well with any signal that would have a 2D structure.
