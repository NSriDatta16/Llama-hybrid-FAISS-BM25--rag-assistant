[site]: crossvalidated
[post_id]: 507876
[parent_id]: 112148
[tags]: 
There's already many good points by others e.g. about sparse feature spaces, when we know a step-function will not work well/when you don't have a good representation of the data (some of these may of course be a matter of creating better features first), as well as non-tabular data types where other approaches are known to work better especially for how to represent the input data (as mentioned by others: images, text, audio). However, here are some additional situations that are non-ideal for RF (in approximately descending order of my strength of concern about using RF). Several of these are not specifically an issue with RF, but rather apply more broadly to many prediction modelling approaches. When we know mechanistically what underlies the system we model. E.g. you can often define a set of biologically sensible differential equations that govern the blood levels of a drug in the human body. You are much better off to set-up a non-linear model based on these, while any modelling approach that ignores this understanding of the biology is going to be a lot less efficient - especially for small datasets -, and likely terrible at extrapolation (see below). I would not ever seriously consider basic RF (or other "default" prediction models like gradient boosted decision trees) here. When you need to extrapolate beyond your training data. The fitted step-functions will remain constant outside the training feature value range, which will often be an implausible thing to assume. Extrapolation is generally a big challenge, but e.g. mechanistic models from the previous bullet point would be a much, much better bet for extrapolation than RF. RF is also not so great for multivariate outputs. Even something as simple as many correlated multi-label binary classification tasks can be more efficient with neural networks. It becomes even clearer, when the desired outputs is e.g. an image, a series of values such as a sentence of words, or a variable number of bounding boxes with class assignments. When you want to causally interpret the effect of a feature. This may be a really silly and obvious thing to point out, but I've seen too many people saying things like:"My RF predicts that being in a hospital is a predictor of a higher risk of death! Don't go to the hospital, if you get sick!" or similar other misinterpretations. Obviously, you need a different approach than the standard prediction set-up in order to try to approach a causal question (for which there are approaches that try to use RF). When there are high cardinality categorical features. E.g. user ID when there's hundreds or thousands of users (in a sense see sparse inputs), products that are being sold etc. and so on. This is not necessarily a case where RF should not be used, but rather where other approaches (such as neural networks with embeddings) might outperform them (of course, you could use neural network embeddings as inputs). When you need to win a data science competition on tabular data, RF is also not necessarily the first thing you'd try. Gradient boosting (xgboost, LightGBM, catboost etc.) tends to outperform RF once properly tuned. It's not necessarily by much and may not be of relevance for many applications unless every little improvement in prediction performance matters.
