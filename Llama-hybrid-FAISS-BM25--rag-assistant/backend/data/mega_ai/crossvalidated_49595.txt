[site]: crossvalidated
[post_id]: 49595
[parent_id]: 
[tags]: 
Puzzled by derivation of time series prediction based on its log

From Introductory Time Series with R : If the random variation is modelled by a multiplicative factor and the variable is positive, an additive decomposition model for $\log(x_t)$ [where $x_t$ is the observation at time $t$] can be used: $\log(x_t) = m_t + s_t + z_t$ [where $m_t$, $s_t$ and $z_t$ are, respectively, terms for the trend, seasonal effect and an error term at time $t$] Some care is required when the exponential function is applied to the predicted mean of $\log(x_t)$ to obtain a prediction for the mean value $x_t$, as the effect is usually to bias the predictions. If the random series $z_t$ are normally distributed with mean 0 and variance $\sigma^2$ , then the predicted mean value at time $t$ based on Equation (1.4) is given by $\hat{x}_t = e^{m_t + s_t} e^{\frac{1}{2}Ïƒ^2}$ The discussion about the caution that is necessary with this sort of model that follows makes sense and, unless it is necessary, I will omit it. What is confusing me is why $e^{\frac{1}{2}\sigma^2}$ is used to represent the error term. There is apparently some point about mathematical statistics that I'm not aware of, but would like to know. Thank you.
