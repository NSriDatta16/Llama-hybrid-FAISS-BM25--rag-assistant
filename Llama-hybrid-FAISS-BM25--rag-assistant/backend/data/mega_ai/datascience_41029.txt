[site]: datascience
[post_id]: 41029
[parent_id]: 41027
[tags]: 
The questions you're asking are empirical questions. The only answer anyone can give is to try all of them and see which works better. You have three options: Impute data Throw away data Use a classifier that can handle missing data, e.g. xgboost . See this answer . xgboost is a powerful classifier. So, if you're not tuning very hard for performance, xgboost is a great way to get a good v0. Some other points: The pattern of missing values is important, and can influence the choice of algorithm. If your dataset is noisy, imputing may only amplify the noise. If you can afford to drop those 2k rows, try that, or train both with and without that data and see if the combination performs better. Regarding software, there are many options: Scikit Learn has some imputation functions MICE, Multiple Imputation through Chained Equations works well for random data. Available in fancyimpute, and also in statsmodels. You will find many resources if you search
