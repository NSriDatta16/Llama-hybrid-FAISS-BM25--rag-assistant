[site]: crossvalidated
[post_id]: 163399
[parent_id]: 
[tags]: 
Metropolis Hastings Algorithm - Prior vs Proposal vs Numerator of Bayes Theorem

I've been using this technique in 'black-box' form for a little while as a physics student. I have been struggling to understand what's happening under the hood for some time and I think I almost have it - but I'm mixed up about several things. I'm trying to fit together the prior distribution, proposal distribution, and Bayes Theorem What is the difference between the prior $\mathcal{P}$ and the jumping distribution $\mathcal{Q}$ in the simple MH algorithm? Are these the same thing? I know the jumping distribution is used to draw a candidate point $X_c$ that is 'near' the current point $X_i$. Often this is something like a Gaussian function centered at $X_i$, therefore producing $X_c$ that is usually locally 'close' to $X_i$. That's what I keep finding, using resources such as this youtube video . Elsewhere, like in this amazing tutorial , I see that the prior is a surface above the parameter space in question shaped in a way that makes sense given the problem, and this surface is used to weight the posterior in conjunction with the appropriate likelihood function. For instance maybe we believe in advance that scores on an particular exam are Gaussian distributed, with known mean. So the posterior is weighted by this assumption and the MCMC will return samples with a higher density in that region for that particular parameter. That makes sense with Bayes Theorem, as the posterior is proportional to the product of the prior and the likelihood. $P(A|B) \propto P(B|A)P(A)$ I am not seeing where this happens into the MH algorithm. I know these three things (prior, proposal, and Bayes Theorem) are all wrapped up in the 'acceptance ratio' part of the algorithm. Can someone break it down for me like I'm five years old and show me exactly where and how those pieces fit together? Finally I'm having a lot of trouble understanding how we define likelihood for an arbitrary statistical model. One explanation from the first video is it's an exponential function that literally evaluates the difference between the proposed fit and the real data: $\mathcal{L} = e^{-(Data - Model)}$ Okay, so this function takes a higher value in regions of parameter space that produce better fits. But how can we say this is exactly the actual likelihood (i.e. the $P(B|A)$ part of Bayes Theorem)? This doesn't seem like it's guaranteed to be at all points proportional to what we want... Any help, guidance, or resources greatly appreciated! Thanks so much for reading to this point and bearing with me.
