[site]: datascience
[post_id]: 6384
[parent_id]: 
[tags]: 
What is the actual output of Principal Component Analysis?

I'm trying to understand PCA, but I don't have a machine learning background. I come from software engineering, but the literature I've tried to read so far is hard for me to digest. As far as I understand PCA, it will take a set of datapoints from an N dimensional space and translate them to an M dimensional space, where N > M. I don't yet understand what the actual output of PCA is. For example, take this 5 dimensional input data with values in the range [0,10): // dimensions: // a b c d e [[ 4, 1, 2, 8, 8], // component 1 [ 3, 0, 2, 9, 8], [ 4, 0, 0, 9, 1], ... [ 7, 9, 1, 2, 3], // component 2 [ 9, 9, 0, 2, 7], [ 7, 8, 1, 0, 0]] My assumption is that PCA could be used to reduce the data from 5 dimensions to, say, 1 dimension. Data details: There are two "components" in the data. One component has mid a levels, low b and c levels, high d , and nondeterministic e levels. The other component has high a and b levels, low c and d levels, and nondeterministic e levels. This means that the two components are most differentiated by b and d , somewhat differentiated by a , and negligibly differentiated by c and e . Outputs? I'm making this up , but say the (non-normalized) linear combination with the highest differentiating power is something like 5*a + 10*b + 0*c + 10*d + 0*e The above input data translated along that single axis is: [[110], [105], [110], ...etc Is that linear combination (or a vector describing it) the output of PCA? Or is the output the actual reduced dataset? Or something else entirely?
