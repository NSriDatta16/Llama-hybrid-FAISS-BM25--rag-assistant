[site]: crossvalidated
[post_id]: 316872
[parent_id]: 316870
[tags]: 
My Guess as to why you are getting the results you are getting is that your model is over-parameterized due to faulty model identification . As to precisely why,I suggest that you contact the author and present your data and ask why . The standard error of the acf is based VERY ROUGHLY on 1/SQRT(# OF OBSERVATIONS) thus if your sample is large you have VERY VERY ROUGHLY a very small standard error and thus a large t value ...that is one possibility ... A second possibility is that the software you are using is just not working for that time series as it may have embedded deterministic activity such as a quarterly effect for the most recent half of the series OR change in error variance all of which may be beyond the "pay grade" of your softwares of choice. If you wish to post your data I will try and help you further . I have been modelling time series for a little over 50 years and have never seen a data set that requires the model you are finding/using ...so I suspect the model may be probably inadequate unless you have a large sample size and are over-believing the apparently significant t values for the acf of the errors. EDITED AFTER RECEIPT OF DATA: I used 140 monthly values from 2006/3 and ob tained the following reasonable model that used a log transform , three pulses and a seasonal pulse that commenced 2011/1 and ARIMA structure using AUTOBOX my tool of choice. It generated the following acf of the residuals with 140 values the se of the acf is approx 1/12 or .07 Note that my reflections of a large sample size , the need for a power transform and the onset of unusual seasonal activity starting at period 59 of 140 came to pass. Just lucky I guess ! or maybe not so ! . This is the model summary and forecast plot EDITED TO RESPOND TO YOUR VERY GOOD QUESTION/OBSERVATION ABOUT THE NEED FOR A LOG XFORM The decision to take a log transform is based upon the linkage between the expected value and the variance of the residuals. When (and why) should you take the log of a distribution (of numbers)? . Thus after forming a model including any necessary deterministic structure one conducts the Box-Cox test to investigate the need for a power transform given that the variance change didn't occur deterministically over time . Following is the results of that test . . Early researchers (without the benefit of our computing power) thought and taught that the test should be conducted on the original data which of course assumes a model with only a constant removed whereas the Box-Cox approach was based upon the variance of the residuals NOT the variance around the mean. I have seen cases where one can detect increasing variance in the observed series so while it is true that this occurs it is not necessarrily the basis for the test but rather often coincidental to the way the variance of the errors changes. http://autobox.com/pdfs/vegas_ibf_09a.pdf slide 14 may help you in this regard where the variance around the mean leads to a bad conclusion about the need for a transform due to anomalies at the higher levels of the series. As an aside I present here the ACTUAL and CLEANSED data and to my eyes there is a DEFINITE increase in variance at the higher levels suggesting a possible need for a power transform like logs as the variance (around the expected value) increases with time. The graph suggests a point in time that the variance of the errors deterministically changed. AUTOBOX tested for that but the resultant test was inconclusive. http://docplayer.net/12080848-Outliers-level-shifts-and-variance-changes-in-time-series.html
