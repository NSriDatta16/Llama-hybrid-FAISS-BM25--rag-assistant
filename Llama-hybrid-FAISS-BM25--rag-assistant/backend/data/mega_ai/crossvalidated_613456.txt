[site]: crossvalidated
[post_id]: 613456
[parent_id]: 
[tags]: 
Why are some models performing better with a larger data set, and why are some models performing worse?

Background I am doing a project where I am comparing 5 models. It is a regression problem, where the model uses the 3D structure data of a protein and a drug molecule bound to it (specifically, the Cartesian coordinates of atoms) to predict the binding affinity of the protein-drug complex. My training data set has 3847 examples. My testing data set contains 209 examples and is a representative and diverse set of such protein-drug complexes that is used as a standard benchmark for this regression problem 1 . I am not using an 80-20 train-test split. I trained the following models, with these hyperparamters that I tuned with a on the same training set with a 10-fold CV grid search. Linear regression (no hyperparameters) MARS (penalty = 5, max degree = 1) kNN regression (n_neighbors=13, p=1) Support vector regression (kernel = 'rbf', gamma = 0.125, epsilon=0.25, C=1) Random forest (n_estimators=2000, max features=2) The problem I was told that bigger data sets will usually improve the generalization ability of models. To test this I did the following: I defined a set of subset sizes = {384, 768, 1152, .. 3847} i.e subsets that were 10%, 20% .. 100% of the training set. For each subset size $n$ , I took that $n$ examples (without replacement) from the training set, trained the above models with the same hyperparameters, tested them on the above test set and computed the RMSE. I repeated this 100 times for each subset size. In the end, I calculated the mean RMSE for each subset size Fig: (a) Linear regression (b) Random forest (c) MARS (d) SVM regression (e) kNN Linear regression and RF seem to perform better as the training set approaches its full possible size. The complete opposite trend is observed for SVM and MARS, and kNN appears to perform best for an intermediate size of the training set. I don't know how to explain this pattern. One piece of information that came to mind is the composition of the training set. Here is a few examples from the training set: As you can see, some proteins are over represented in this set (each of these repeated proteins is bound to different drug molecules, so a single protein-drug pair is not repeated, even if specific drugs or proteins are repeated). My guess is that in smaller subsets of the training set, the data set is more diverse and balanced, but becomes more biased as it approaches its full size, and hence some models are overfitting to these repeated examples. If that is the case, it makes sense that the linear regression model is not affected as it is a high bias model, but I can't make sense of the remaining observations. EDIT: The question previously stated that there were 3500 training examples. This was corrected.
