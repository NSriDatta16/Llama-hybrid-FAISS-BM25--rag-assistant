[site]: crossvalidated
[post_id]: 390051
[parent_id]: 
[tags]: 
Using regression weights when $Y$ might be measured with nonzero-mean measurement error

Suppose we observe data $Y, X$ and would like to fit a regression model for $\mathbf{E}[Y \,|\, X]$ . Unfortunately, $Y$ is sometimes measured with errors whose mean is nonzero. Let $Z \in \left\{\text{unbiased}, \text{biased}\right\}$ indicate whether $Y$ is measured with classical zero mean errors or nonzero-mean errors, respectively. We would like to estimate $\mathbf{E}[Y \,|\, X, Z = \text{unbiased}]$ . Unfortunately, $Z$ is generally not observed, and $\mathbf{E}[Y \,|\, X, Z = \text{unbiased}] \neq \mathbf{E}[Y \,|\, X]$ . If we fit a regression of $Y$ on $X$ , we'll get biased predictions. Suppose we cannot generally observe $Z$ , but have access to a model for $\Pr[Z \,|\, X,Y]$ (because we manually learned $Z$ on a small training set and fit a classification model with $Z$ as the target variable). Does fitting a regression of $Y$ on $X$ using $\Pr[Z = \text{unbiased} \,|\, X,Y]$ as regression weights produce an unbiased estimate of $\mathbf{E}[Y \,|\, X, Z = \text{unbiased}]$ (or, failing that, a less biased estimate than we would get without using weights)? Is this method used in practice, and does it have a name? Clarification: the goal is to fit a model that minimizes mean squared error on unseen data (test data) where $Z = \text{unbiased}$ . The optimal predictor for that objective is $\mathbf{E}[Y \,|\, X, Z = \text{unbiased}]$ , so that is the function we are trying to estimate. Methods for solving this problem should be ranked in terms of how well they achieve that objective. Small example in R with df$y_is_unbiased playing the role of $Z$ and df$y_observed playing the role of $Y$ : library(ggplot2) library(randomForest) set.seed(12345) get_df $y_unbiased epsilon ## Value of Y if measured incorrectly df $y_biased y_unbiased + sample(mismeasurement, size=n_obs, replace=TRUE) ## Y is equally likely to be measured correctly or incorrectly df $y_is_unbiased y_observed $y_is_unbiased, df$ y_unbiased, df$y_biased) return(df) } ## True coefficients constant $string_y_is_unbiased y_is_unbiased) ## Notice that Pr[Y | Z = biased] differs from Pr[Y | Z = unbiased] ggplot(df, aes(x=y_observed)) + geom_histogram(color="black", fill="grey", binwidth=0.5) + facet_wrap(~ string_y_is_unbiased, ncol=1) ## Recover true constant and beta (plus noise) when using y_unbiased summary(lm(y_unbiased ~ x1 + x2, data=df)) ## Biased estimates when using y_biased (constant is biased downward) summary(lm(y_biased ~ x1 + x2, data=df)) ## Also get biased estimates when using y_observed (constant is biased downward) summary(lm(y_observed ~ x1 + x2, data=df)) ## Now image that we "rate" subset of the data (manually check/research whether y was measured with or without bias) n_rated $y_is_unbiased y_is_unbiased) model_pr_unbiased In this example, the model $\Pr[Z = \text{unbiased} \,|\, X,Y]$ is a random forest with formula=y_is_unbiased ~ y_observed + x1 + x2 . If this model were perfectly accurate, it would generate weights of 1.0 where $Y$ is unbiased, 0.0 where $Y$ is biased, and the weighted regression would clearly be unbiased. What happens when the model for $\Pr[Z = \text{unbiased} \,|\, X,Y]$ has test precision and recalls that aren't perfect ( $Y$ on $X$ ? Slightly more complex example in which $\Pr[Z = \text{unbiased} \,|\, X]$ varies with $X$ (as opposed to the simpler example I posted above, where $\Pr[Z = \text{unbiased} \,|\, X] = \frac{1}{2} \; \forall X$ ): library(ggplot2) library(randomForest) set.seed(12345) logistic $y_unbiased epsilon ## Value of Y if measured incorrectly df $y_biased y_unbiased + sample(mismeasurement, size=n_obs, replace=TRUE) ## Note: in this example, Pr[ Z = biased | X ] varies with X ## In the first (simpler) example I posted, Pr[ Z = biased | X ] = 1/2 was constant with respect to X df $y_is_unbiased x1, df$x2) df $y_observed y_is_unbiased, df $y_unbiased, df$ y_biased) return(df) } ## True coefficients constant $string_y_is_unbiased y_is_unbiased) ## Notice that Pr[Y | Z = biased] differs from Pr[Y | Z = unbiased] ggplot(df, aes(x=y_observed)) + geom_histogram(color="black", fill="grey", binwidth=0.5) + facet_wrap(~ string_y_is_unbiased, ncol=1) ## Recover true constant and beta (plus noise) when using y_unbiased summary(lm(y_unbiased ~ x1 + x2, data=df)) ## Biased estimates when using y_biased (constant is biased downward) summary(lm(y_biased ~ x1 + x2, data=df)) ## Also get biased estimates when using y_observed ## Note: the constant is biased downward _and_ the coefficient on x2 is biased upward! summary(lm(y_observed ~ x1 + x2, data=df)) ## Now image that we "rate" subset of the data (manually check/research whether y was measured with or without bias) n_rated $y_is_unbiased y_is_unbiased) model_pr_unbiased In this example, the weighted regression of $Y$ on $X$ looks less biased than the unweighted regression. Is that true in general? I also tried shabbychef's suggestion (see answer below) on this example, and it appears to do worse than the weighted regression. For those who prefer Python to R, here's the second simulation in Python: import numpy as np import pandas as pd from sklearn.ensemble import RandomForestClassifier from sklearn.linear_model import LinearRegression def logistic(x): return 1 / (1 + np.exp(-x)) def pr_y_is_unbiased(x1, x2): # This function returns Pr[ Z = unbiased | X ] return logistic(x1 + 2*x2) def get_df(n_obs, constant, beta, sd_epsilon, mismeasurement): df = pd.DataFrame({ 'x1': np.random.normal(size=n_obs), 'x2': np.random.normal(size=n_obs), 'epsilon': np.random.normal(size=n_obs, scale=sd_epsilon), }) df['y_unbiased'] = constant + np.dot(np.array(df[['x1', 'x2']]), beta) + df['epsilon'] # Note: df['y_biased'].mean() will differ from df['y_unbiased'].mean() if the mismeasurements have a nonzero mean df['y_biased'] = df['y_unbiased'] + np.random.choice(mismeasurement, size=n_obs) df['y_is_unbiased'] = np.random.uniform(size=n_obs)
