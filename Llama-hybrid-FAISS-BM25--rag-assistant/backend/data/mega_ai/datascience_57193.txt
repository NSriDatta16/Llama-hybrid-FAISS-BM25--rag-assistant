[site]: datascience
[post_id]: 57193
[parent_id]: 57192
[tags]: 
Precision is the proportion of predictions of that class that are true. So 98% of the predictions for each of your classes are actually of the predicted class, and 2% are actually of the opposite class. Recall is the proportion of the true positives that are identified as such. This means that your model is correctly identifying 100% of the class 0s, but only 72% of the class 1s. F1-Score is a kind of average of the two; it's an attempt to provide a unified figure of the model's performance, but personally I consider it less useful than the separate figures. It's calculated via the formula 2 x ((precision x recall) / (precision + recall)) . Wikipedia's pages on these metrics are comprehensive: https://en.wikipedia.org/wiki/Precision_and_recall https://en.wikipedia.org/wiki/F1_score
