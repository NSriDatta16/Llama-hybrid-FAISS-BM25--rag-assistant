[site]: crossvalidated
[post_id]: 179784
[parent_id]: 179774
[tags]: 
Romeo, for such small data sets it can be hard to build good classifiers while avoiding overfitting. Also the domain you mentioned often deals with poor quality data / lots of noise. My first guess would be that you simply run into the "curse of dimensionality" (with 116 features vs. 126 data points): https://en.wikipedia.org/wiki/Curse_of_dimensionality (in high dimensional data spaces many ML algorithms and distance metrics (e.g. euclidean distance) perform poorly.) So you might not get there by simply tweaking one particular algorithm but a.) look at preprocessing/dimension reduction and b.) ensemble methods. Things you could try: Try some feature selection/dimensionality reduction, e.g. PCA Think about data cleansing. Can you identify outliers? are these obviously wrong data entries, or maybe actually the info you're looking for? Use ensemble methods (like stacked generalizations, boosting/bagging) to build different models, then build a meta-classifier Maybe that helps. - Florian
