[site]: crossvalidated
[post_id]: 223948
[parent_id]: 223818
[tags]: 
The geometric interpretation of ordinary least squares regression provides the requisite insight. Most of what we need to know can be seen in the case of two regressors $x_1$ and $x_2$ with response $y$ . The standardized coefficients, or "betas," arise when all three vectors are standardized to a common length (which we may take to be unity). Thus, $x_1$ and $x_2$ are unit vectors in a plane $E^2$ --they are located on the unit circle--and $y$ is a unit vector in a three dimensional Euclidean space $E^3$ containing that plane. The fitted value $\hat y$ is the orthogonal (perpendicular) projection of $y$ onto $E^2$ . Because $R^2$ simply is the squared length of $\hat y$ , we needn't even visualize all three dimensions: all the information we need can be drawn in that plane. Orthogonal regressors The nicest situation is when the regressors are orthogonal, as in the first figure. In this and the rest of the figures I will consistently draw the unit disk in white and the regressors as black arrows. $x_1$ will always point directly to the right. The thick red arrows depict the components of $\hat y$ in the $x_1$ and $x_2$ directions: that is, $\beta_1 x_1$ and $\beta_2 x_2$ . The length of $\hat y$ is the radius of the gray circle on which it lies--but remember that $R^2$ is the square of that length. The Pythagorean Theorem asserts $$R^2 = |\hat y|^2 = |\beta_1 x_1|^2 + |\beta_2 x_2|^2 = \beta_1^2(1)+\beta_2^2(1) = \beta_1^2 + \beta_2^2.$$ Because the Pythagorean Theorem holds in any number of dimensions, this reasoning generalizes to any number of regressors, yielding our first result: When the regressors are orthogonal, $R^2$ equals the sum of the squares of the betas. An immediate corollary is that when there is just one regressor--univariate regression-- $R^2$ is the square of the standardized slope. Correlated Negatively correlated regressors meet at angles greater than a right angle. It is visually apparent in this image that the sum of squares of the betas is strictly greater than $R^2$ . This can be proven algebraically using the Law of Cosines or by working with matrix solution of the Normal Equations. By making the two regressors almost parallel, we can position $\hat y$ near the origin (for an $R^2$ near $0$ ) while it continues to have large components in the $x_1$ and $x_2$ direction. Thus, there is no limit to how small $R^2$ might be. Let's memorialize this obvious result, our second generality: When regressors are correlated, $R^2$ may be arbitrarily smaller than the sum of squares of the betas. However, this is not a universal relation, as the next figure demonstrates. Now $R^2$ strictly exceeds the sum of squares of the betas. By drawing the two regressors close together and keeping $\hat y$ between them, we may make the betas both approach $1/2$ , even when $R^2$ is close to $1$ . Further analysis may require some algebra: I take that up below. I leave it to your imagination to construct similar examples with positively correlated regressors, which thereby meet at acute angles. Notice that these conclusions are incomplete: there are limits to how much less $R^2$ may be compared to the sum of squares of the betas. In particular, by examining the possibilities carefully, you may conclude (for a regression with two regressors) that When the regressors are positively correlated and the betas have a common sign, or when the regressors are negatively correlated and the betas have different signs, $R^2$ must be at least as large as the sum of the squares of the betas. Algebraic results Generally, let the regressors be (column vectors) $x_1, x_2, \ldots, x_p$ and the response be $y$ . Standardization means (a) each is orthogonal to the vector $(1,1,\ldots,1)^\prime$ and (b) they have unit lengths: $$|x_i|^2 = |y|^2 = 1.$$ Assemble the column vectors $x_i$ into an $n\times p$ matrix $X$ . The rules of matrix multiplication imply that $$\Sigma = X^\prime X$$ is the correlation matrix of the $x_i$ . The betas are given by the Normal Equations, $$\beta = (X^\prime X)^{-1} X^\prime y = \Sigma^{-1} (X^\prime y).$$ Moreover, by definition, the fit is $$\hat y = X \beta = X (\Sigma ^{-1} X^\prime y).$$ Its squared length gives $R^2$ by definition: $$R^2 = |\hat y|^2 = \hat y^\prime \hat y = (X\beta)^\prime (X\beta) = \beta^\prime (X^\prime X)\beta = \beta^\prime \Sigma\beta. $$ The geometric analysis suggested we look for inequalities relating $R^2$ and the sum of squares of the betas, $$\sum_{i=1}^p \beta_i^2 = \beta^\prime \beta.$$ The $L_2$ norm of any matrix $A$ is given by the sum of squares of its coefficients (basically treating the matrix as a vector of $p^2$ components in a Euclidean space), $$|A|_2^2 = \sum_{i,j} a_{ij}^2 = \operatorname{tr}(A^\prime A) = \operatorname{tr}(AA^\prime).$$ The Cauchy-Schwarz Inequality implies $$R^2 = \operatorname{tr}(R^2) = \operatorname{tr}(\beta^\prime \Sigma \beta) = \operatorname{tr}(\Sigma \beta \beta^\prime) \le |\Sigma|_2 | \beta\beta^\prime|_2 = |\Sigma|_2 \beta^\prime \beta.$$ Since squared correlation coefficients cannot exceed $1$ and there are just $p^2$ of them in the $p\times p$ matrix $\Sigma$ , $|\Sigma|_2$ cannot exceed $\sqrt{1\times p^2} = p$ . Therefore $$R^2 \le p\, \beta^\prime \beta.$$ The inequality is attained, for instance, when all the $x_i$ are perfectly positively correlated. There is an upper limit on how large $R^2$ may be. Its average value per regressor, $R^2/p$ , cannot exceed the sum of squares of the standardized coefficients. Conclusions What may we conclude in general? Evidently, information about the correlation structure of the regressors as well as the signs of the betas could be used either to bound the possible values of $R^2$ or even to compute it exactly. Absent that full information, little can be said beyond the obvious fact that when the regressors are linearly independent, a single nonzero beta implies $\hat y$ is nonzero, demonstrating $R^2$ is nonzero. One thing we can definitely conclude from the output in the question is that the data are correlated: because the sum of squares of the betas, equal to $1.1301$ , exceeds the maximum possible value of $R^2$ (namely $1$ ), there must be some correlation. Another thing is that since the largest beta (in size) is $-0.83$ , whose square is $0.69$ --far exceeding the reported $R^2$ of $0.20$ --we may conclude that some of the regressors must be negatively correlated. (In fact, $\text{VO}_{2\,\text{max}}$ is likely strongly negatively correlated with age, weight, and fat in any sample that covers a wide range of values of the latter.) If there were only two regressors, we could deduce a great deal more about $R^2$ from knowledge of high regressor correlations and inspection of the betas, because this would enable us to draw an accurate sketch of how $x_1$ , $x_2$ , and $\hat y$ must be situated. Unfortunately, the additional regressors in this six-variable problem complicate things considerably. In analyzing any two of the variables, we have to "take out" or "control for" the other four regressors (the "covariates"). In so doing, we shorten all of $x_1$ , $x_2$ , and $y$ by unknown amounts (depending on how all three of those are related to the covariates), leaving us knowing almost nothing about the actual sizes of the vectors we are working with.
