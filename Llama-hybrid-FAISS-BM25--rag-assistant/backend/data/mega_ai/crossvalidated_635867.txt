[site]: crossvalidated
[post_id]: 635867
[parent_id]: 
[tags]: 
Cross validation + model stacking with hyperparameter tuning while sharing data?

Let's say we want to stack 2 base models: an XGBoost regressor and a deep neural network by linearly combining their predictions as w1 * XgBoost prediction + w2 * DNN prediction We not only have to learn these models (the weights of the DNN and the stacked linear model, tree splits for the XGBoost model) but also the hyperparameters for each. It's quite confusing to put all this together into a cross-validation setup without any data leakage and still use as much data as possible for training models. The way k-fold cross validation for training a model is by training the model with one fold left out as a "hold-out" fold and then averaging the results for all possible hold-out folds. We repeat this whole process for various hyperparameter configurations and then choose the best performing one (the one with the best average metric). In the end, when we have our hyperparameter configuration, we actually go ahead and train on all the k folds as the last step. So in principle, it seems like whenever we use k-fold cross validation on some data, all of that data is "seen" by the model. So when we train a base model, all of that data is completely unusable by the stacked model (to prevent data leakage). So it makes it necessary to split the data initially into 2 parts: one for training the base models and the other for training the stacked models. Is there no way around this? ( This question is related but it doesn't talk about the hyperparameter tuning part)
