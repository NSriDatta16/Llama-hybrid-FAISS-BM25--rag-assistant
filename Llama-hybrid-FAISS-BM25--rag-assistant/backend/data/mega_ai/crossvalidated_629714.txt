[site]: crossvalidated
[post_id]: 629714
[parent_id]: 
[tags]: 
Does training time increase more if I add a layer at the beginning of a neural network or at the end?

Let's consider a fixed NN architecture, dataset and hardware. We add a layer, either at the beginning or at the end of the NN. In which case the training time will increase more? Intuitively, I imagine the answer may depend on a few factors: whether the NN is a autoencoder (input has same dimension as output), an encoder (input is way higher-dimensional than output), etc. which type of layer I'm adding: for the sake of argument, let's limit ourselves to very common layers, such as Conv, Dense, GCNConv or Self-Attention Thus, I'm looking for an answer that considers at least two cases (say, autoencoder and encoder), but clearly explains the logic behind the answer, so that I can adapt it to other cases. Ideally, the answer would consider the time complexity of training a neural network by backpropagation.
