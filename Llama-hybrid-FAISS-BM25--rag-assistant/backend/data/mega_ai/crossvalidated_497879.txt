[site]: crossvalidated
[post_id]: 497879
[parent_id]: 
[tags]: 
Why use sinusoidal along embedding dimension in positional encoding in transformers?

Why do we need sinusoidal function along the embedding dimension in positional encoding in transformers? Shouldn't sinusoidal function along time dimension be enough? This question is derived from this answer as I cannot comment on it yet. As mentioned in the linked answer, suppose a word is embedded with a vector: $e_1, e_2, ..., e_d$ . If the same word occurs at two different positions, we would like to encode that information using the positional encoding. I understand adding sinusoidal function along the position/time dimension but what is the rationale behind varying the positional encoding along $e_1, e_2, ..., e_d$ . Say the positional encoding for two positions is $p_1, p_2, ..., p_d$ and $p^{'}_1, p^{'}_2, ..., p^{'}_d$ . Why can't $p_1 = p_3 ... = \sin(f(t))$ and $p_2 = p_4 ... = \cos(f(t))$ where $f(t)$ is a function of time step? and similarly for $p^{'}$ Here is a related question
