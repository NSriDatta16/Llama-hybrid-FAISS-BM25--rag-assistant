[site]: crossvalidated
[post_id]: 495048
[parent_id]: 264239
[tags]: 
Yes, I think your interpretation is correct. An important point is that the parameter vector $\pi$ in the model is assumed to follow a Dirichlet distribution. These parameters represents the probability that an observation ends up in each bin of the histogram, meaning that $\pi_h$ would be the probability of an observations $y_i$ being in the interval $[\xi_{h-1}, \xi_{h}]$ . The hyperparameters $a_i$ can be seen as pseudo-counts, meaning how many observations we believe should be in the histogram bins before we seen any data. The update is done as you described, and we get the posterior distribution for the parameter vector. The posterior distribution is also a Dirichlet distribution since the Dirichlet distribution is the conjugate prior for the likelihood function in the model. Then we can sample parameter vectors from the posterior, and do whatever analysis we want on that sample. If you are reading the 3rd edition of "Bayesian Data Analysis" I suggest you check out the section 3.4 on the Multinomial model in chapter 3, which also uses a Dirichlet distribution as prior and works in a similar way. Source: Bayesian Data Analysis 3rd edn A. Gelman, J. B. Carlin, H. S. Stern, D. B. Dunson, A. Vehtari and D. B. Rubin, 2013 Boca Raton, Chapman and Hall–CRC 676 pp., ISBN 1‐439‐84095‐4
