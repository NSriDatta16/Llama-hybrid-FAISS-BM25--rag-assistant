[site]: crossvalidated
[post_id]: 559057
[parent_id]: 558446
[tags]: 
No overfitting I search a linear function that only depends only on one of my 20 features There is not much overfitting when you search for only a linear function $$y_i = a + b x_{i}$$ Overfitting is not a problem in this case because you do not have hyperparameters that you can tune. Your problem is more straightforward and only about picking the best feature. The question of overfitting, and the bias-variance tradeoff, is not relevant here because there is no bias that can make the model less flexible and less variable (except for such techniques as shrinking or Bayesian modeling which could also constrain a one-dimensional model). Overfitting would be more worrisome if you would be fitting more complex functions of your features (e.g. fit polynomial models). In that case, the hyperparameter would be the order of the polynomial and you can change this. Or, overfitting could occur if you would allow the use of multiple features, in which case the hyperparameter is in the number of features that you include in the model. (but your case seems to have been restricted to only one single feature) So in your case, You can simply perform the fitting repeatedly with different features $x_i$ and select whichever is the best-performing feature (for this you need to have some measure of performance, and ideally also have an idea about the distribution in order to estimate the significance, or use some part of the data to estimate the significance). Whether this is a good approach is a different question, but if this is the constraint of your question (find a linear function with only one single feature) then there is not much better solution possible than simply selecting the feature that fits your data the best.
