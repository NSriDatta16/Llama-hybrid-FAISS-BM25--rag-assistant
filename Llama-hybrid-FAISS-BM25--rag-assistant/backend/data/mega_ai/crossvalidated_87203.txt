[site]: crossvalidated
[post_id]: 87203
[parent_id]: 78240
[tags]: 
It depends what you want to show, what is the variable: categorical variable - it's fine discrete by ordinal - it's a bit tricky e.g. on 1-5 scale it is something different to have the same probabilities for 1 and 5, and for 3 and 4 continuos variable - it's even more tricky the previous argument the choice of coordinates matter (good coordinates are ones respecting symmetries (and they not always exist)) changing bin size scales entropy So, I will mostly focus of the categorical variant. Typical quantity you can use is Kullback-Leibler divergence , which means how different is your probability distribution $Y$ with respect to some initial one $X$. $$ D_{KL}(Y||X) = \sum_x P(Y=x) \log \left(\frac{P(Y=x)}{P(X=x)} \right) $$ It can be interpreted as information gain - expecting probability distribution $X$ how much information you gained when you measured probability $Y$. If $X$ is uniform, then the KL divergence is just entropy of $X$ minus entropy of $Y$. As an example, when you expect a coin to be fair $X=(\tfrac{1}{2}, \tfrac{1}{2})$, you toss it and get heads (and you are sure) $Y=(1,0)$, you learn exactly one bit of information. When it comes to setting "uninformed" probability - it depends on the problem. For discrete case, just take maximum entropy distribution given the constraints . If there are no constraints, it is simply uniform probability. For linear constraints (that is, that some averages are fixed) there is a simple recipe to compute such distribution. If there are a few different models, you can compare them measuring against the same $X$. The same can work for some ad hoc assumptions (for example uniform on some set, zero elsewhere). If you have to normalize it, divide by entropy of the uninformed probability distribution $X$. EDIT: If you want to just tell how concentrated is the distribution, just use entropy of $Y$ (comparing it to entropy of $X$). In this case, lower is better.
