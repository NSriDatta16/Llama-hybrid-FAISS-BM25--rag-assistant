[site]: datascience
[post_id]: 37774
[parent_id]: 37767
[tags]: 
Welcome to Data Science SE! As a short answer has been given, I'll give a longer overview of the problem, starting with a analogous human approach. But first, a formal definition of machine learning from Mitchell: "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E." This essentially says: "Do something, get it wrong, *perform magic* , try again, repeat until happy" It doesn't detail the magic part... and I guess that is what you are interested in. Well, one main inspiration for that magic within machine learning (and artificial intelligence) is humans - and how we learn. We generally follow the phrase "learning by doing". Let's use a stupid human example to begin with: GOAL: you want to get a cookie from a jar STEPS: you naïvely stretch out your arms, but you can't reach the jar... no cookie - you are unhappy you learn a trick e.g. getting a chair to stand on. Now you can reach the jar - you are happy you look in the jar and see 3 different cookies - you want chocolate chip but pick a raisin cookie... don't like the taste - unhappy you pick a different cookie... chocolate chip - happy In this example above, steps 1 and 2 sound like reinforcement learning .... iteratively learning how to complete a task, exploring possibilities. Steps 3 and 4 are most like image classification ... classifying and labelling images (types of cookies). Comparing the main steps of learning - first the human brain view, then the machine learning equivalent as the second point: Initialisation Your brain is the model, holding lots of information about your experience and how happy your are. It is combined to tell you what to do based on what you sense (input). a model consisting of a combination of weights/coefficients define conditional behaviour. Inputs are combined and weighted with these to produce an output. Doing You get feedback (cookie or not) - this tells you how to adjust your brain's model of the poblem at hand. The machine learning model measures a numerical error based on its output and the expected result - putting a number on how bad was it's attempt was. Learning the (average) human cannot now further explain how it improves. It tries something different... goes for a cookie with smaller dark coloured dots, the bigger cookie, etc. This is the magic part that gets into neuroscience. an ML model can be explained. It uses an optimisation method to make sense of those errors... it changes the weights/coefficients in a way it thinks will reduce the error it measures. We generally speak about gradient descent ( more help ) as a way to train the model - to help it learn . It is called gradient descent because we imagine the errors/cost of the model are like a hill that we want to descend i.e. we want to get to the bottom of it: This curve shows how the iterative process of computing our error, updating the weights and repeating, takes us down the gradient of the error/cost. We do this iteratively, in little steps. The weights of the model are stored between stages - the weights are persistent between iterations. Once the model is happy, measuring small/acceptable errors, we are finished training and can use that hold-out test dataset to validate the model's performance. Gradient descent (and e.g. backpropagation) are then really where learning happens. They're the methods by which we update the weights. They basically close the loop from (1) getting an error –how unhappy we are– to (2) being able to adjust those weights slightly in a way that would make us better at that task next time. This really equates to adding or subtracting a small amount from the existing weights. Exactly how much we add/subtract is controlled using the learning rate parameter. If it is big, we add/subtract big amounts - so learning fast. But that has its own problems - we cannot learn consistently and can get completely lost, so it is usually a small number ~ 0.001. The weights we start with in a neural network are most commonly just random numbers. We ask the model to perform a task, then use gradient descent with the errors to slowly improve those random weights with tiny alterations. At the end, it is these weights that we save to disk on the computer. They are intellgient combinations of simply decimal numbers. To get your head around machine learning and what it means, at the highest level, you might want to read an article like this . I would suggest reading Michael Nielsen's eBook as a nice intuitive introduction - is a great place to start.
