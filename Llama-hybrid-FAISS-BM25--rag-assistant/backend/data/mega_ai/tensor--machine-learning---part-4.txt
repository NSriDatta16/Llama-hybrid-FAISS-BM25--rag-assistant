kernel, data and inverse transform of the kernel. Y = A [ ( C g ) ⊙ ( B d ) ] , {\displaystyle {\mathcal {Y}}={\mathcal {A}}[(Cg)\odot (Bd)],} where A , B {\displaystyle {\mathcal {A}},{\mathcal {B}}} and C {\displaystyle {\mathcal {C}}} are the inverse transform, data and kernel. The derivation is more complex when the filtering kernel also includes a non-linear activation function such as sigmoid or ReLU. The hidden weights of the convolution layer are the parameters to the filter. These can be reduced with a pooling layer which reduces the resolution (size) of the data, and can also be expressed as a tensor operation. Tensor factorization An important contribution of tensors in machine learning is the ability to factorize tensors to decompose data into constituent factors or reduce the learned parameters. Data tensor modeling techniques stem from the linear tensor decomposition (CANDECOMP/Parafac decomposition) and the multilinear tensor decompositions (Tucker). Tucker decomposition Tucker decomposition, for example, takes a 3-way array X ∈ R I × J × K {\displaystyle {\mathcal {X}}\in \mathbb {R} ^{I\times J\times K}} and decomposes the tensor into three matrices A , B , C {\displaystyle {\mathcal {A,B,C}}} and a smaller tensor G {\displaystyle {\mathcal {G}}} . The shape of the matrices and new tensor are such that the total number of elements is reduced. The new tensors have shapes A ∈ R I × P , {\displaystyle {\mathcal {A}}\in \mathbb {R} ^{I\times P},} B ∈ R J × Q , {\displaystyle {\mathcal {B}}\in \mathbb {R} ^{J\times Q},} C ∈ R K × R , {\displaystyle {\mathcal {C}}\in \mathbb {R} ^{K\times R},} G ∈ R P × Q × R . {\displaystyle {\mathcal {G}}\in \mathbb {R} ^{P\times Q\times R}.} Then the original tensor can be expressed as the tensor product of these four tensors: X = G × A × B × C . {\displaystyle {\mathcal {X}}={\mathcal {G}}\times {\mathcal {A}}\times {\mathcal {B}}\times {\mathcal {C}}.} In the example shown in the figure, the dimensions of the tensors are X {\displaystyle {\mathcal {X}}} : I=8, J=6, K=3, A {\displaystyle {\mathcal {A}}} : I=8, P=5, B {\displaystyle {\mathcal {B}}} : J=6, Q=4, C {\displaystyle {\mathcal {C}}} : K=3, R=2, G {\displaystyle {\mathcal {G}}} : P=5, Q=4, R=2. The total number of elements in the Tucker factorization is | A | + | B | + | C | + | G | = {\displaystyle |{\mathcal {A}}|+|{\mathcal {B}}|+|{\mathcal {C}}|+|{\mathcal {G}}|=} ( I × P ) + ( J × Q ) + ( K × R ) + ( P × Q × R ) = 8 × 5 + 6 × 4 + 3 × 2 + 5 × 4 × 2 = 110. {\displaystyle (I\times P)+(J\times Q)+(K\times R)+(P\times Q\times R)=8\times 5+6\times 4+3\times 2+5\times 4\times 2=110.} The number of elements in the original X {\displaystyle {\mathcal {X}}} is 144, resulting in a data reduction from 144 down to 110 elements, a reduction of 23% in parameters or data size. For much larger initial tensors, and depending on the rank (redundancy) of the tensor, the gains can be more significant. The work of Rabanser et al. provides an introduction to tensors with more details on the extension of Tucker decomposition to N-dimensions beyond the mode-3 example given here. Tensor trains Another technique for decomposing tensors rewrites the initial tensor as a sequence (train) of smaller sized tensors. A tensor-train (TT) is a sequence of tensors of reduced rank, called canonical factors. The original tensor can be expressed as the sum-product of the sequence. X = G 1 G 2 G 3 . . G d {\displaystyle {\mathcal {X}}={\mathcal {G_{1}}}{\mathcal {G_{2}}}{\mathcal {G_{3}}}..{\mathcal {G_{d}}}} Developed in 2011 by Ivan Oseledts, the author observes that Tucker decomposition is "suitable for small dimensions, especially for the three-dimensional case. For large d it is not suitable." Thus tensor-trains can be used to factorize larger tensors in higher dimensions. Tensor graphs The unified data architecture and automatic differentiation of tensors has enabled higher-level designs of machine learning in the form of tensor graphs. This leads to