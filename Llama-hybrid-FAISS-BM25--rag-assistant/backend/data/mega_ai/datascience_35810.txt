[site]: datascience
[post_id]: 35810
[parent_id]: 35807
[tags]: 
A neural network is probably a perfectly reasonable approach here. But this is an extremely unbalanced dataset and you're going to have to handle that somehow. The network is learning that the best way for it to reduce the loss is to always guess neutral - if you were taking a standardized test and you realized that you'd get 90% right if you always guessed 'C', you certainly wouldn't answer 'A' and 'B' in equal measure. There are a few standard ways to deal with this, and probably depend on how big your dataset is (how many examples 6% and 4% come out to be): Oversample the minority classes to even things out (so you'd show the net the same example of the positive / negative classes multiple times for every time it saw each neutral example) Undersample the majority class (only show the net a random subset of the neutral samples during each epoch) Set a class weight / sample weight - proportionally weight down the contribution to the loss function by examples of the majority class. I'd probably try 3 first because it's super easy. Check out the documentation of the Cross Entropy Loss . It takes a tensor of weights as a parameter and weights the classes according to the entries in that tensor. So just say the positive (4%) class gets weight 1, negative(6%) gets 2/3, and 90% gets 2/45 (if I did the math right...). That would even out the total contributions of each class in the dataset. Hopefully you'll see better results that way. You'll probably want to look at precision / recall to evaluate the results (because random guessing gets you 90% if you're using accuracy).
