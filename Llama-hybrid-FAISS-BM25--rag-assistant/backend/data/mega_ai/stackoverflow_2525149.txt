[site]: stackoverflow
[post_id]: 2525149
[parent_id]: 2524608
[tags]: 
This is essentially a sequence prediction problem, so you want Recurrent neural networks or hidden Markov models. If you only have a fixed time to look back, time window approaches might suffice. You take the sequence data and split it into overlapping windows of length n. (eg. you split a sequence ABCDEFG into ABC, BCD, CDE, DEF, EFG). Then you train a function approximator (e.g. neural network or linear regression) to map the first n-1 parts of that window onto the nth part. Your predictor will not be able to look back in time longer than the size of your window. RNNs and HMMs can do so in theory, but are hard to tune or sometimes just don't work. (State of the art RNN implementations can be found in PyBrain http://pybrain.org ) Update: Here is the pybrain code for your problem. (I haven't tested it, there might be some typos and stuff, but the overall structure should work.) from pybrain.datasets import SequentialDataSet from pybrain.supervised.trainers import BackpropTrainer from pybrain.tools.shortcuts import buildNetwork from pybrain.structure import SigmoidLayer INPUTS = 4 HIDDEN = 10 OUTPUTS = 4 net = buildNetwork(INPUTS, HIDDEN, OUTPUTS, hiddenclass=LSTMLayer, outclass=SigmoidLayer, recurrent=True) ds = SequentialDataSet(INPUTS, OUTPUTS) # your_sequences is a list of lists of tuples which each are a bitmask # indicating the event (so 1.0 at position i if event i happens, 0.0 otherwise) for sequence in your_sequences: for (inpt, target) in zip(sequence, sequence[1:]): ds.newSequence() ds.appendLinked(inpt, target) net.randomize() trainer = BackpropTrainer(net, ds, learningrate=0.05, momentum=0.99) for _ in range(1000): print trainer.train() This will train the recurrent network for 1000 epochs and print out the error after every epochs. Afterwards you can check for correct predictions like this: net.reset() for i in sequence: next_item = net.activate(i) > 0.5 print next_item This will print an array of booleans for every event.
