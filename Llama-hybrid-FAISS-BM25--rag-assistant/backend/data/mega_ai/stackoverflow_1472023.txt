[site]: stackoverflow
[post_id]: 1472023
[parent_id]: 1471002
[tags]: 
You are unnecessarily storing a full copy of the original file in @contents_of_the_file and -- if the amount of duplication is low relative to the file size -- nearly two other full copies in %unique_contents_of_the_file and @unique_contents_of_the_file . As ire_and_curses noted, you can reduce the storage requirements by making two passes over the data: (1) analyze the file, storing information about the line numbers of non-duplicate lines; and (2) process the file again to write non-dups to the output file. Here is an illustration. I don't know whether I've picked the best module for the hashing function ( Digest::MD5 ); perhaps others will comment on that. Also note the 3-argument form of open() , which you should be using. use strict; use warnings; use Digest::MD5 qw(md5); my (%seen, %keep_line_nums); my $in_file = 'data.dat'; my $out_file = 'data_no_dups.dat'; open (my $in_handle, ' ', $out_file) or die $!; while ( defined(my $line = ) ){ my $hashed_line = md5($line); $keep_line_nums{$.} = 1 unless $seen{$hashed_line}; $seen{$hashed_line} = 1; } seek $in_handle, 0, 0; $. = 0; while ( defined(my $line = ) ){ print $out_handle $line if $keep_line_nums{$.}; } close $in_handle; close $out_handle;
