[site]: crossvalidated
[post_id]: 631095
[parent_id]: 631080
[tags]: 
Logistic regression is not strictly a classification method. It produces a linear predictor, a function of all the predictor values, that estimates the log-odds of a binary outcome, which then can be translated into outcome probability. The path along the ROC curve is equivalent to a path along the values of that linear predictor (or its associated predicted probabilities). Using a high predicted probability as a cutoff, you get a low fraction of true positives and a low fraction of false positives (bottom left of the curve). Using a low predicted probability as a cutoff, you get high proportions of both true and false positives (top right of the curve). The R ROCR package has a nice way of visualizing the predicted probabilities along the ROC curve, as illustrated in a vignette and in this online text . There might be a similar display available in SPSS, but I don't use that software. So to answer your second question, for a multiple logistic regression you use the desired value of the linear predictor (or predicted probability), based on all the predictors together, that puts you where you want along the ROC curve. To answer your first question, do not fall into the trap of always using p = 0.5 (the usual hidden software default) or the "top left" of the ROC curve ("best sensitivity/specificity") as the probability cutoff. The proper choice of cutoff (if one is really necessary) should take into account the relative costs of false-negative and false-positive classifications. See for example this page and its links, particularly those from this answer . Briefly, the optimal probability cutoff is determined by the ratio of misclassification costs. That's part of the application of the logistic regression model, not of the model itself. Added in response to comments In general with regression, and particularly with binary (e.g., logistic) regression, it's a mistake to do separate models for each predictor. Even in ordinary linear regression, a model that leaves out a predictor that is associated both with outcome and with an included predictor leads to omitted-variable bias . It's even worse in binary regression, as leaving out any outcome-associated predictor inherently leads to bias, even if the omitted predictor is uncorrelated with the included predictors. See this page . So to continue the answer to the second question, performing separate regressions on each predictor to get separate ROC curves for each predictor is a bad idea. If you really need to have an ROC curve, base it on a complete model that incorporates all predictors together. With respect to the first question, I don't know that there is a generally reliable way to translate sensitivity and specificity values from an ROC curve to the modeled probability, except in the types of ideal circumstances described by Sextus Empiricus in a comment on the question. If you are going to follow best practice and choose a probability cutoff (if needed) based on relative misclassification costs, you don't need the ROC curve anyway. You just use the probability value that minimizes net cost. The display provided for example by the ROCR package can then show you where that probability lines up on your ROC curve, if you wish. Finally, one problem with classification tables is that they put too much emphasis on accuracy, sensitivity and specificity, which are not good measures of model performance and ignore the importance of a proper cost-based application of the model. Frank Harrell has written extensively on the problems with such measures; this post is one useful introduction to the issues.
