[site]: datascience
[post_id]: 36178
[parent_id]: 36105
[tags]: 
Spark (naively) uses average of vectors for all words in the document as representation of the document. Check the API documentation little carefully. "The Word2VecModel transforms each document into a vector using the average of all words in the document ; this vector can then be used as features for prediction, document similarity calculations, etc." If you are interested specifically in vector for a word (and not document), you can check getVectors method which will return data-frame of word and vector. The API behaviour does give rise to confusion though as it assumes that everyone wants to use averaging by default.
