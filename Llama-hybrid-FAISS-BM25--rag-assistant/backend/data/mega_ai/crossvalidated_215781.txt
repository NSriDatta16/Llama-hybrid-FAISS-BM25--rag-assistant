[site]: crossvalidated
[post_id]: 215781
[parent_id]: 
[tags]: 
intraclass correlation (ICC) for level of comprehension of individual items by different raters

I'm currently doing a translation and adaptation of a questionnaire composed of 36 items. After finishing the translation, synthesis and back-translation steps, 40 people randomly selected from a population of undergraduates were asked to rate how well the could understand what was being asked, with responses being given on a scale of 1 to 6. So what I'm testing is the clarity of the instrument, namely how comprehensible the questions are. This is my hypothesis: if the instrument has good comprehensibility, the average score for item comprehension will be high and the correlation of scores given by raters to each item will be low (indicating that when there is misunderstanding, it is more probably due to random noise than to bad question design). So, I'm interested in knowing both the average scores and the level of agreement among raters. For that, I'll run 2 tests: (1) for each question, calculate for the mean of ratings given by all judges. (if mean rating is high, probably the question has good comprehensibility, or there is some other confounder like social desirability). (2) calculate the Intraclass Correlation Coefficient (ICC) for the consistency presented by the raters, using a two-way random effects average-measures model. (if ICC is low, probably there's no agreement; if ICC is high, probably there is good agreement) My doubt here is with the hypothesis testing: If I understand well, a high ICC (alternative hypothesis, r > 0) would indicate that raters generally agree with how well they can understand each item. If that is the case, it would be plausible that the low rating is due to the item itself and not som random variation. On the other hand, a low ICC (null hypothesis, r = 0) would indicate that raters disagree on their measures and if there are variations in the scores given by the items, we could not say that it is due to item or rater characteristics. Thus we might be seeing no more than random variations. Would that interpratation of the ICC be incorrect? Here's the data I got from irr::icc in R: Single Score Intraclass Correlation Model: twoway Type : agreement Subjects = 36 Raters = 33 ICC(A,1) = -0.000293 F-Test, H0: r0 = 0 ; H1: r0 > 0 F(35,1084) = 0.977 , p = 0.507 95%-Confidence Interval for ICC Population Values: -0.005 And here the means and std. deviation for each question: Mean Std. Dev 5.84 .495 5.79 .528 5.87 .414 5.89 .388 5.87 .578 5.81 .569 5.95 .324 5.82 .563 5.76 .634 5.68 .904 5.82 .865 5.82 .865 5.97 .162 5.74 .724 5.87 .811 5.87 .811 5.86 .536 5.97 .162 5.79 .875 5.87 .529 5.89 .649 5.97 .162 5.87 .811 5.84 .594 5.87 .811 5.89 .388 5.79 .875 5.82 .865 5.87 .811 5.67 1.195 5.68 1.068 5.84 .823 5.82 .652 5.92 .359 5.89 .649 5.84 .718
