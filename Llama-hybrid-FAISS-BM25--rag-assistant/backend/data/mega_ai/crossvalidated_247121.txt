[site]: crossvalidated
[post_id]: 247121
[parent_id]: 
[tags]: 
In propensity score analysis, what are options to deal with very small or large propensities?

$\newcommand{\P}{\mathbb{P}}$I am concerned with observational data in which the treatment assignment can be explained exceedingly well. For example, a logistic regression of $$\P(A =1 |X) = (1+ \exp(-(X\beta)))^{-1}$$ wehre $A$ treatment assignment and $X$ covariates has very good fit with very high test $AUC >.80$ or even $>.90$. This is good news for the accuracy of the propensity model, but it leads to propensity score estimates $$\hat{\pi} = (1+ \exp(-(X \hat{\beta})))^{-1}$$ close to $0$ or $1$. These in turn lead to large inverse probability weights $\hat{\pi}^{-1}$ and $(1-\hat{\pi})^{-1}$ used in estimators such as the inverse probability weighted estimator of expectation of outcome $Y_1$ (observation under treatment): $$n^{-1} \sum_i \hat{\pi_i}^{-1} A_i Y_{1i}.$$ This, I suspect, turns the estimates' variances very large. It seems like a vicious circle that very discriminative propensity score models lead to extreme weights. My question : what are available option to make this analysis more robust? Are there alternatives to fit the propensity score model or how to deal with large weights after the model has been fit?
