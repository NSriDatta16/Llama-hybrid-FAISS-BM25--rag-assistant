[site]: crossvalidated
[post_id]: 565732
[parent_id]: 
[tags]: 
Distance metric that is robust to collinearity

I'm trying to find a distance metric that takes into account the correlation between vectors. That is, suppose we have matrix $M$ of dimensions $n \times k$ , and we take the pairwise distance between all $n$ rows of $M$ , yielding a distance matrix $D_0$ of dimensions $n \times n$ . Now suppose we append a new column to $M$ , which is highly correlated with an existing column, say $cor(M_{*1}, M_{*k+1}) = 0.99$ . We then calculate the new pairwise distance between the $n$ rows taking into account this new vector, yielding a new distance matrix $D_1$ . What I'm trying to find is a distance metric such that $D_0 \approx D_1$ . Basically the intuition is that adding a bunch of correlated variables shouldn't affect the distance between entries. So suppose we were looking at differences between people's traits, and we measure height, weight, cholesterol, blood pressure, etc. on $n$ people, and then calculate their differences in trait space. If we also measure BMI, which is highly correlated to height and weight, it shouldn't contain much new information, and so the distance measures should be relatively unchanged whether or not we include this. I've looked at PCA distance, Mahalanobis distances, and a bunch of others. Certainly distances like Chebyshev's maximum metric ( $d_{ij} = \max_k |M_{ki} - M_{kj}|$ ) are by definition robust, but they also loose a ton of information. Are there any standard solutions to this problem, either via selection of a distance metric or dimensionality reduction procedures that account for this? EDIT: Here's some code to illustrate how the Mahalanobis goes wrong: set.seed(1) # generate a random vector x Ideally what I'm looking for is a metric where these two distance calculates are exactly on the 1-to-1 line. Here, the correlation between the distances is 0.66, despite the vectors being nearly identical (all pairwise correlations > 0.999). EDIT 2: As @Noah pointed out below, if we remove any vector (doesn't matter which one) we see a near perfect correlation: # get the pairwise Mahalanobis distance, removing the 10th vector dst Any intuition as to why this is the case? The solution can't be to always remove a vector at random (as the vector might have low correlation with others). Is there a way to generalize this?
