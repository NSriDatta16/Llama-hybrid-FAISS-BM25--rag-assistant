[site]: crossvalidated
[post_id]: 147581
[parent_id]: 
[tags]: 
Regularization in Neural networks

One way to regularize a neural network is "early stopping" , meaning that I don't let the weights get to their optimal values (based on the cost function calculated on the training data) but stop the gradient descent process before they do. I understand why it's true when using batch gradient descent because I just stop the optimization process on all samples after i.e 10 iterations instead of 50. What I don't understand is how can it be true when using online gradient descent? It means that I stop the process before using all of my samples. It seems like a paradox that when wanting to regularize in order to solve a high variance problem I stop the learning process before using all samples,when in order to solve a high variance problem I would actually want to use as many samples as possible.
