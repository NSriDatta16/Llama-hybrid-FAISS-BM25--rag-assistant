[site]: crossvalidated
[post_id]: 440115
[parent_id]: 
[tags]: 
$R^2$ of Log transformed data is positive, however that of reversed transformed data is negative

I am running an XGBoost model with a continuous target variable. With ~200 features I am getting a Test $R^2$ of 0.54. By looking at the distribution of the target variable, it appears it's highly left skewed. So, I took log transformation on the target variable and reran the model which gave me an $R^2$ of 0.68, which is significantly better than the non-transformed model. Then, I reverse transformed the predicted values (antilog) and calculated $R^2$ using original values of the target variable. The $R^2$ is -0.02. I am having a difficult time wrapping my head around such results. I understand taking antilog (or exponent) will significantly shoot up the error $log(y) = f(x) + error$ $y = exp(f(x) + error)$ I am trying to understand which results should I trust and if $R^2$ is the correct accuracy metric to look at? Case 1: Use the model with original non-transformed data ( $R^2 = 0.54$ ) Case 2: Use the model with log-transformed target variable ( $R^2 = 0.68$ , but when reverse transformed $R^2 = -0.02$ )
