[site]: crossvalidated
[post_id]: 370179
[parent_id]: 
[tags]: 
Why binary crossentropy can be used as the loss function in autoencoders?

I was wondering why binary crossentropy can be used as the loss function in autoencoders trained on (normalized) images, e.g. here or this paper ? I know that binary crossentropy can be used in binray classification problems where the ground-truth labels (i.e. $y$ ) are either 0 or 1 and therefore when predictions (i.e. $p$ ) are correct, in both cases, the loss value would be zero: $$ BCE(y,p) = -y.log(p) - (1-y).\log{(1-p)} $$ $$ BCE(0,0) = 0, BCE(1,1) = 0 $$ However, binary crossentropy does not have a value of zero when neither of its arguments are both zero or one, which is the case for an autoencoder with ground-truth labels in range $[0,1]$ (i.e. assuming the input data has been normalized in this range). I thought a regression loss function such as mean squared error or mean absolute error must be used instead, which have a value of zero when labels and predictions are the same. What am I missing here?
