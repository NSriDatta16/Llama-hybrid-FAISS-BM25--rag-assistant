[site]: datascience
[post_id]: 94357
[parent_id]: 
[tags]: 
Predicting Y Values Properly in a Regression Task using Scaled Values (Random Forest & MLP)

I have a supervised learning regression task: I am trying to forecast demand for a product based on sales in past years. Data description: Samples (rows) - Demand for a certain product (at a certain month, in a certain year). Number of rows: 2,400 X Variables (columns) - Explanatory variables. Most of them are categorical (which I turned into dummy variables), 2 of them are continuous ("Price Per Unit", "GDP"). Number of X variables: ~20 Y Variable (target): Demand in units. I decided to use Random Forest Regression and MLP Regression for the task, using Python & scikit-learn. For both models, I used StandardScalar to scale the continuous values, and then used RandomSearch for hyper-parameter tuning to find the optimal model. My question: After finding the optimal parameters, do I need to fit the model again (with the same optimal parameters) but on the original, un-scaled values? (Is the scaling meant to be only for FINDING the optimal parameters?) Also, when predicting values for new data: do I need to scale the values again, or do I just use the original, un-scaled data? (If I scale them, the model will return scaled Y predictions, which I cannot interpret as a solid demand in meters...)
