[site]: crossvalidated
[post_id]: 529290
[parent_id]: 
[tags]: 
Truncated data in Neural Network regression

I'm working on a Neural Network regression problem and the variable of interest (trade flows) that I'm trying to predict is skewed and truncated at 0. I first tried to fit the model without prior transformation of the trade flows, only by scaling it in order to normalise the loss function. This lead to the network only predicting zeros. I'm using a ReLu activation in the output layer so as not to have negative values for the prediction. After scaling the data by $log(x +1 )$ I got rid of the skewness, but I'm still left with 15% zeros. Is there a way to somehow penalise the zeros? Might it be that I'm just not training long enough, that the network has thus not started to converge and I'm still underfitting? My hyperparameters are set via a genetic algorithm. It might be that I need to crank up the upper bound of steps available for training.
