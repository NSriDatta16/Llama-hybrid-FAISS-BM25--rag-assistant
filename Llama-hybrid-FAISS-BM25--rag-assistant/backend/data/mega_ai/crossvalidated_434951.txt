[site]: crossvalidated
[post_id]: 434951
[parent_id]: 434946
[tags]: 
In short, yes; I would recommend it. Cross-validation is usually used for tuning hyperparameters (e.g. penalty term in lasso/ridge regression; tree depth and fraction of parameters per split in random forest; etc.). Once you use CV for tuning, it can no longer reliably be used to assess how well your method will perform on out-of-sample data. That's what the hold-out/test set is for, although it is admittedly suboptimal to use only a single sample to evaluate performance; some kind of hierarchical cross-validation procedure might be preferable. e.g. from the scikit-learn documentation on cross-validation : [When using CV a] test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. (The second clause refers to splitting a data set, a single time, into train/validate/test components.)
