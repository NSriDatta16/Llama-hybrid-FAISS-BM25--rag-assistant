[site]: crossvalidated
[post_id]: 81679
[parent_id]: 81197
[tags]: 
The point being made in section 2.3 of the book (where this quote come from) is that if the source of the data is from Scenario 1, there is nothing better you can do than a linear division (as in figure 2.1). Any finer tuning is actually self-delusion: you should then expect to get worse results predicting cases outside the training data if you do not use the optimal linear division. However, if the source of the data is from Scenario 2, you can reasonably expect the low variance of each of the $10$ source distributions to make it more likely that data points of the same colour will tend to cluster together in a non-linear manner, and so a non-linear approach may be more skilful. The example the book gives is that of looking at the colours of nearest neighbours: figure 2.2 shows the classification boundary if you look at the 15 nearest neighbours (a fairly smooth non-linear boundary) while figure 2.3 looks at the boundary if you look at just 1 nearest neighbour (a very jagged boundary). I suspect that the point being made is that the value of statistical or machine learning techniques depends on the source of the data, and that some techniques are better in some circumstances, and others in others. But it is also possible to generalise ideas from different methods and come up with further techniques, as section 2.4 and figure 2.5 do with what the book calls the "Bayes classifier".
