[site]: datascience
[post_id]: 97332
[parent_id]: 
[tags]: 
Traditional alternatives to Caputure Words Sequence information in NLP

What were the traditional/earlier methods in which NLP researchers captured the word sequence information through feature engineering? I know the current methods which rely on deep learning models like roBERT and BERT and work well with capturing sequence information. I also know about embeddings like word2vec, but they fail to capture the sequence information. For example, I would like a feature which can differentiate between "cat ran after the dog." and "dog ran after the cat."
