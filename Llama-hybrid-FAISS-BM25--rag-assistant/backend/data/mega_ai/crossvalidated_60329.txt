[site]: crossvalidated
[post_id]: 60329
[parent_id]: 
[tags]: 
Test error correlation with label frequency

For a university project I wrote a SVM classifier and during the benchmark phase I'm getting some results I'd like to better understand from the theoretical side. My original dataset contains a multi-labeled examples but I'm buiding a binary classifier so I pick a feature A and I replace all the labels with +1 if A is present or -1 if it isn't. For instance: label1 label2 label3 label4 = feature1 feature2 feature3 feature4 label2 label4 = feature1 feature2 feature3 feature4 label3 label4 = feature1 feature2 feature3 feature4 If choose label3 the dataset becomes: 1 = feature1 feature2 feature3 feature4 -1 = feature1 feature2 feature3 feature4 1 = feature1 feature2 feature3 feature4 If choose label2 the dataset becomes: 1 = feature1 feature2 feature3 feature4 1 = feature1 feature2 feature3 feature4 -1 = feature1 feature2 feature3 feature4 Now, my original dataset contains 20k rows and: If I choose a label which is in 9k rows I get a average test error of 13% If I choose a label which is in 5k rows I get a average test error of 7% I was wondering why this is happening. My guesses are: In the second case the separating hyperplane has a larger margin In this the first case the is overfitting
