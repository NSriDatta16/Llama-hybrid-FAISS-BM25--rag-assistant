[site]: crossvalidated
[post_id]: 588846
[parent_id]: 
[tags]: 
Resizing input vectors of different lengths

I have a set of data taken from a number of machines. A simplified version of the dataset is shown below: id time1 time2 time3 time4 time5 time6 time7 failure 0 1 1.824 3.107 9.695 0.944 0.151 2.365 NaN 0 1 2 3.573 0.728 3.001 5.470 4.078 3.713 0.761 1 2 3 5.064 6.090 8.290 NaN NaN NaN NaN 0 3 4 8.636 4.538 5.866 5.294 9.988 NaN NaN 0 4 5 3.487 4.915 1.860 2.911 NaN NaN NaN 1 In this sample dataset, there are 5 machines, each with a unique id value. Each machine has a sensor that takes and records a measurement every few weeks. Each sensor reading is recorded in one of the timeN columns. Some machines are newer than others, meaning that newer machines have recorded fewer sensor measurements than older machines. The failure column shows a value of 1 if a machine failed/broke down, and a 0 if the machine is still operational. I want to use the sensor measurement vectors as input data , and the failure state as my target value to train a predictive model. My problem is that the sensor measurement vectors are not all the same size. In the real dataset, the size of the input vectors can be anywhere between 200 and 800 measured values, and there are around 1000 machines. My question is , what are some ways I can resize all my input vectors to be of equal lengths, while preserving any signals that might be in the data? Some things I am considering are Some method of random sampling to downsample the larger vectors Take incremental averages of sequential groups of sensor values (i.e. instead of [ time1[value] , time2[value] , time3[value] , time4[value] , ...], use [ mean(time1[value],time2[value],time3[value]) , mean(time2[value],time3[value],time4[value]) , ...]) to shorten longer vectors. I'm intending to use something like XGBoost coded using Python. What other methods of resizing my input vectors would be recommended for the purposes described above?
