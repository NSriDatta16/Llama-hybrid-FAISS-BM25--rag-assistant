[site]: crossvalidated
[post_id]: 47071
[parent_id]: 
[tags]: 
Confusion related to calculation of likelihood

I was reading this paper related to Learning from multiple annotator using Gaussian processes. The idea is if we don't have the actual ground truth of a certain data, but only the labels from some noisy experts, then how can we learn a model from this data from multiple noisy experts and predict on future data Basically, what the guys have done is taken the weighted average of the labels from individual predictors, based upon their precision, to get a single label for each point. Then they have used GP to assume that the data points belong to a Gaussian distribution. However, I have this question/confusion when they have derived the likelihood of $p(Y)$ where $Y$ is the set of labels given by the noise experts. I have added the screenshot as well I know the one who can answer this question has to go through the paper and all, but I have already spend significant time trying to figure out how this $-\log p(Y)$ turned out like that but couldn't. I would really appreciate if someone could give me some pointers
