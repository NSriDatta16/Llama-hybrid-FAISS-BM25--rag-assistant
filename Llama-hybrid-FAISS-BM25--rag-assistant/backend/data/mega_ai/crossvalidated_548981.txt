[site]: crossvalidated
[post_id]: 548981
[parent_id]: 548974
[tags]: 
I generally agree that when an experiment is performed, there isn't really sufficient prior information to be useful (if there was, why perform that experiment and not another?). However, Empirical Bayes approaches might be interesting to you - they can be useful when there is no prior information available, yet the goal of the analysis is to predict several somethings (as opposed to estimating things). One classic example is described by Brad Efron here . The data available was the batting average of a team early in the 1970 season, and the goal was to predict their final batting averages at the end of the season. To do this, they used a shrinkage estimator to shrink the maximum likelihood estimate for each player toward the grand average of all 18 players. At the end of the season, they demonstrated that the mean squared error for their shrunken estimates was smaller than the mean squared error of the maximum likelihood estimator. Using a shrinkage estimator can be thought of as using a prior - but here, they didn't set the prior before looking at the data. Rather, they looked at the data, set the prior, and still managed to get a lower MSE than the MLE approach. Basically, you don't need actual prior knowledge for a Bayesian approach to pay off. Now, as a caveat, using a shrinkage estimator can easily be justified from a frequentist perspective (as long as you're attempting to predict >2 things, unbiased estimators are usually dominated by some kind of regularized approach).
