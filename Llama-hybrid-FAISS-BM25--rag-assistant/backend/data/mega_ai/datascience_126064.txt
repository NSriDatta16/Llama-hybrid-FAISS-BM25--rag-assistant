[site]: datascience
[post_id]: 126064
[parent_id]: 
[tags]: 
PyTorch input shape for text classification using LSTM

I have three sentiment classes: POSITIVE, NEGATIVE, and NEUTRAL, along with a dataset consisting of 3000 sentences and their corresponding sentiment labels (POSITIVE, NEGATIVE, or NEUTRAL). Each sentence is represented as a $100$ -dimensional vector. As a newcomer to LSTMs, I am seeking advice on the preferable approach for developing an LSTM model in PyTorch. Both approaches have a batch size of 64, with batch_first = True. Approach - 1: The input tensor should be of shape $(64,100,1)$ where each batch contains 64 sentences and each of the sentences consists of 100 timesteps where each timestep is a $1$ -d vector. Basically each dimension of the sentence - vector is a timestep. Approach - 2: The input tensor should be of shape $(64,1,100)$ where each batch contains 64 sentences and each of the sentences consists of 1 timestep which has $100$ dimensions. It's worth mentioning that the sentence vectors were generated by averaging the word embeddings of the tokens of each sentence. Although Approach - 1 seems more intuitive, I am unsure if the choice between the two approaches is influenced by experimental results.
