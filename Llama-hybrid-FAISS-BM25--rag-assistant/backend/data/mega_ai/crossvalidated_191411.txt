[site]: crossvalidated
[post_id]: 191411
[parent_id]: 191396
[tags]: 
To generate ROC curves (= Receiver Operating Characteristic curves): Assume we have a probabilistic, binary classifier such as logistic regression. Before presenting the ROC curve, the concept of confusion matrix must be understood. When we make a binary prediction, there can be 4 types of errors: We predict 0 while we should have the class is actually 0: this is called a True Negative , i.e. we correctly predict that the class is negative (0). For example, an antivirus did not detect a harmless file as a virus . We predict 0 while we should have the class is actually 1: this is called a False Negative , i.e. we incorrectly predict that the class is negative (0). For example, an antivirus failed to detect a virus. We predict 1 while we should have the class is actually 0: this is called a False Positive , i.e. we incorrectly predict that the class is positive (1). For example, an antivirus considered a harmless file to be a virus. We predict 1 while we should have the class is actually 1: this is called a True Positive , i.e. we correctly predict that the class is positive (1). For example, an antivirus rightfully detected a virus. To get the confusion matrix, we go over all the predictions made by the model, and count how many times each of those 4 types of errors occur: In this example of a confusion matrix, among the 50 data points that are classified, 45 are correctly classified and the 5 are misclassified. Since to compare two different models it is often more convenient to have a single metric rather than several ones, we compute two metrics from the confusion matrix, which we will later combine into one: True positive rate ( TPR ), aka. sensitivity, hit rate , and recall , which is defined as $ \frac{TP}{TP+FN}$. Intuitively this metric corresponds to the proportion of positive data points that are correctly considered as positive, with respect to all positive data points. In other words, the higher TPR, the fewer positive data points we will miss. False positive rate ( FPR ), aka. fall-out , which is defined as $ \frac{FP}{FP+TN}$. Intuitively this metric corresponds to the proportion of negative data points that are mistakenly considered as positive, with respect to all negative data points. In other words, the higher FPR, the more negative data points we will missclassified. To combine the FPR and the TPR into one single metric, we first compute the two former metrics with many different threshold (for example $0.00; 0.01, 0.02, \dots, 1.00$) for the logistic regression, then plot them on a single graph, with the FPR values on the abscissa and the TPR values on the ordinate. The resulting curve is called ROC curve: In this figure, the blue area corresponds to the Area Under the curve of the Receiver Operating Characteristic (AUROC). The dashed line in the diagonal we present the ROC curve of a random predictor: it has an AUROC of 0.5. The random predictor is commonly used as a baseline to see whether the model is useful. If you want to get some first-hand experience: Python: http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html MATLAB: http://www.mathworks.com/help/stats/perfcurve.html
