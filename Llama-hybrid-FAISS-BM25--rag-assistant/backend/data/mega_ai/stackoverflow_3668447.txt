[site]: stackoverflow
[post_id]: 3668447
[parent_id]: 3663605
[tags]: 
I'm not sure if this is exactly what you are asking, but people often use cross-validation to break a single set of data into multiple training/testing subsets to better evaluate learning performance. The basic idea (for e.g. 10-fold cross-validation) is to: randomly split your data into training and testing sets train a classifier on the training set evaluate its performance on the testing set repeat steps 1-3 nine more times with different random training/testing splits The overall performance of the classifier is its average performance on all 10 testing sets. I looked around a bit and found some examples of how to perform cross-validation programmatically or via the Weka UI .
