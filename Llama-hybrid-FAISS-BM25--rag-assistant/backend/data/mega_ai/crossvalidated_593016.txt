[site]: crossvalidated
[post_id]: 593016
[parent_id]: 592995
[tags]: 
As with many neural network questions, the answers are ultimately kind of unsatisfying: It would mean that the dimension of the latent space is larger than 784. Conceptually this doesn't really make sense, since part of the motivation of the VAE is to compress the data into a simpler representation, but there is no mathematical or architectural reason why you couldn't do it. It depends exactly what results you care about. If you care a lot about reconstruction of the data, probably the results will be better with a larger latent space. If instead you care a lot about compressing the data along meaningful dimensions, then probably they will be better with a smaller latent space. But ultimately there is not really any principled way for predicting ahead of time what size latent space will be best for a given dataset, and you either have to just look at what people have done in the past or just try a lot of values for yourself and see what works best.
