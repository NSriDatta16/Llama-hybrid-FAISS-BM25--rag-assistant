[site]: crossvalidated
[post_id]: 468718
[parent_id]: 468688
[tags]: 
Your numbers are for Normal distributions. Other distributions yield other relative efficiencies. I will explain how to find these values -- but ultimately, for Normal distributions, you have to perform numerical integration. Setting and definitions We are contemplating taking a sample. This is modeled as $n$ identically distributed, independent random variables with common distribution $F.$ With this sample we will estimate the central tendency of $F.$ Two estimators are under consideration: The sample mean is a random variable with variance $\sigma^2/n$ where $\sigma^2$ is the variance of $F.$ This follows from basic properties of the variance. The sample median also is a random variable. When $F$ is a continuous distribution with density $f$ and $n=2k-1$ is an odd number, the density of the sample median is that of the $k^\text{th}$ order statistic given by $$f_{k}(x) = \binom{n}{k-1,1,n-k} F^{k-1}(x)\, f(x)\, (1-F(x))^{n-k}$$ The variance of the sample median is $\mu_{2} - \mu_{1}^2$ where, as usual, the moments are given as the integrals $$\mu_{m} = \int_{\mathbb{R}} x^m\, f_{k}(x)\,\mathrm{d}x.\tag{*}$$ The efficiency of an estimator is the reciprocal of its sampling variance. The relative efficiency of two estimators (hopefully of the same quantity, for otherwise this makes little sense) is the ratio of their efficiencies. Why is this important? Because the variance of an estimator typically is directly proportional to the size of a sample needed to detect a given effect with a given confidence and power. Relative efficiency, therefore, is the relative cost of your sample when you use one estimator instead of another. For instance, when $F$ is Normal and $n$ is large, the sample mean's efficiency is approximately $2/\pi\approx 0.637$ relative to the sample median for estimating the central tendency. Thus, when your objective is to estimate this tendency precisely and you are certain $F$ really has a Normal distribution, then by using the sample mean you will need only $63.7\%$ as large a sample than by using the sample median for your estimator. Application When $F$ is a Normal distribution and $n$ exceeds $1,$ computing the moments $(*)$ requires numerical integration -- but this isn't hard to do, because $F$ is well understood and can be quickly and accurately computed. Moreover, we may assume with no loss of generality that $F$ is the standard Normal distribution because all Normal distributions are just $F$ but with a different unit of measurement for the variable and changing the unit won't change the relative efficiency. To illustrate, I wrote code to perform the integrals in $(*)$ and plotted the relative efficiency of the sample mean compared to the sample median for three distributions: Normal (which could be any Normal distribution), Uniform (which could be any uniform distribution on an interval), and Student t with $3$ degrees of freedom. (The calculations took no measurable time.) The values in the "Normal" plot at left are every other value quoted in the question, because I have analyzed the situation only for odd values of $n.$ Namely, these values are (from left to right) 1, 0.743, 0.697, 0.679, 0.669, 0.663, 0.659, 0.656. Notice how the sample mean is even more (relatively) efficient for the Uniform distribution (shown in the middle plot). This is because outliers cannot occur: uniform distributions are bounded between two values and samples tend to be spread evenly between those bounds. The sample mean is worse than the sample median for the Student t distribution (notice the values on the vertical axis in the right hand plot). This distribution has "heavy" tails that frequently produce huge outliers. The sample mean is sensitive to these outliers, much more so than the median. This illustrates the risk you take in using the sample mean when you think you might have a Normal distribution but really you have something like a Student t distribution governing your data. It explains why the median might be preferred as an estimator even though (under ideal assumptions) it appears to be more efficient. Remarks Why focus on odd $n$ ? Only because the median for even $n$ is not uniquely defined: there is no definite middle value in an even sample. The usual rule of averaging the two values closest to the middle results in a more complicated expression than $(*)$ and I just didn't want to deal with it here, since it adds little insight. Why perform numerical integration? Another solution is via simulation: generate a gazillion samples, compute the mean and median of each one, and compare the variances of those gazillion statistics. Although that's easily done, (1) the formula $(*)$ is amenable to mathematical analysis, which can reveal far more than any bunch of simulations, and (2) the numerical integrals can be computed far more quickly and with far greater accuracy than you will ever obtain through simulation. Here is the R code that produced the figure. It was written to apply to any distribution you want--but it includes no safety checks to make sure the integration succeeded, so be careful! # # Use numerical integration to compute moments of the kth order statistic from a # distribution with log cdf `f`, log pdf `ff`, and logsurvival function `sf`. # Provide hints about the domain of integration (if possible) by supplying # values for the limits `lower` and `upper`. # moment.order $value }) } # # Create a list of distributions to supply to `moment.order`. # NB: Rather than asking the code to compute the variance of each distribution, # we simply stipulate it. # distros variance / n # # Plot relative efficiency. # plot(n, phi / v, pch=21, bg="Gray", ylab="Relative Efficiency", main=s) } par(mfrow=c(1,1))
