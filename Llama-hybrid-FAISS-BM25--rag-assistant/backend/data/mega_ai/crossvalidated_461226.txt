[site]: crossvalidated
[post_id]: 461226
[parent_id]: 
[tags]: 
Are there two types of cross validation: with and without test set?

I am a bit confused about cross validation. In some papers, cross validation is done after train/test split via splitting the test set into folds. In other papers, cross validation is done without any other splits. If I got the right idea, the first type is to set hyperparameters and the second one is to evaluate model with already selected hyperparameters, is that right? Let's say I have a CNN model that tends to overfit after some time (100+ epochs) -> I need to monitor val_loss to stop him in right stage to achieve best model. Would in these case be okay to use the first type of cross validation for model selection (split test would be used as validation) or is this considered hacky?
