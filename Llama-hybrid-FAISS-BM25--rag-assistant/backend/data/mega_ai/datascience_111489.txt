[site]: datascience
[post_id]: 111489
[parent_id]: 111454
[tags]: 
It seems to me that your model is currently completly underfitting due to the too deep architecture of your LSTM. To solve your problem, your first objective will be to obtain a model that is good on the training data. Then, in a second time, you will search to obtain a model that is good on the validation data. Solving the underfitting with RNN-LSTM Here is what you can do to solve this underfitting problem : Remove the second and third LSTM layers. If you only have a single LSTM layer remaining, make sure to turn off return_sequences to False . Lower the number of LSTM neurons in your first layers. Maybe a number between 10 and 20 would be a better choice to start and reduce underfitting. Increase the batch size to a more consistent number such as 32, 64 or 128. Increase the number of epochs until your performance on the training set is good. Scale your input features. Once you obtain a good score on the training set, you can try to obtain the best score as possible on your validation data.
