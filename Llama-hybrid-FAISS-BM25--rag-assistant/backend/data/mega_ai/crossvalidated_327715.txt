[site]: crossvalidated
[post_id]: 327715
[parent_id]: 327703
[tags]: 
First, from your description I suppose that you misunderstand BN. Basically normalization is done along the batch axis, not within any dimensions of a sample. (Actually it is possible but this is a special case for convolution layers). Imagine that a neural network has to know the original value of some inputs to get a job done. It is hard to imagine, as NN does not have any notion of unit (at least for inputs). It was shown that inputs normalized to mean 0 and variance 1 ease learning. Anyway, after first activation all the original values are gone, e.g. if activation is tanh, values will be in (-1, 1).
