[site]: datascience
[post_id]: 43512
[parent_id]: 
[tags]: 
How to train the generator in a recurrent GAN (Keras)

I am trying to train a Recurrent GAN that is meant to generate geospatial movement data (sequences of 3-tuples of latitude, longitude and time). You may simply consider it a sequences of vectors with 3 features. I am using Keras on top of TensorFlow for this at the moment. The following are the two networks I have come up with. I know the hyper-parameters are probably terrible. I just want to get the general architecture working and then tune them later. Generator: generator = tf.keras.Sequential([ tf.keras.layers.GRU(256, return_sequences=True, stateful=True, input_shape=(None, 3), batch_size=1), tf.keras.layers.TimeDistributed(tf.layers.Dense(3)) ]) Discriminator: discriminator = tf.keras.Sequential([ tf.keras.layers.GRU(128, input_shape=(None, 3)), tf.keras.layers.Dense(1, activation='sigmoid') ]) For the sake of completeness, here is what my combined adversarial model looks like with discriminator.trainable = False : adversarial = tf.keras.Sequential([ generator, discriminator ]) The following is my training loop. Part 1 is for training the discriminator. I grab some real sequences and I run a loop to generate some fake ones using the generator, then I create labels for them and train the discriminator on both no problem. I struggle to figure out how to train the generator, though. As you can see, I need to call the generator in a loop in order to generate the vectors of the sequence one by one. I can't really use train_on_batch with that. for epoch in range(EPOCHS): for batch in range(n_batches): # --- PART 1: Train the discriminator ---------------------------------- # Use the generator to generate a half a batch full of fake data fake_data = [] for i in range(BATCH_SIZE // 2): length = df_train.sample(1).squeeze().shape[0] start = data_train[np.random.randint(data_train.shape[0])] generated = np.array([start]) generator.reset_states() for i in range(1, length): input = generated[-1:] input = np.array([input]) prediction = model.predict(input, batch_size=1) prediction = np.squeeze(prediction, axis=0) generated = np.concatenate([generated, prediction]) fake_data.append(generated) fake_labels = np.zeros((BATCH_SIZE // 2, 1)) # Get half a batch of real data real_data = df_train.iloc[batch * BATCH_SIZE:(batch + 1) * BATCH_SIZE].tolist() real_labels = np.ones((BATCH_SIZE // 2, 1)) # Train the discriminator on half a batch of real and half a batch of fake data d_loss_fake = discriminator.train_on_batch(fake_data, fake_labels) d_loss_real = discriminator.train_on_batch(real_data, real_labels) d_loss = 0.5 * np.add(d_loss_fake, d_loss_real) # --- PART 2: Train the generator -------------------------------------- g_loss = # HERE, I AM STRUGGLING I'm open to any ideas, even changes to the entire approach, if you believe RNNs aren't the way go to about this. I decided to use RNNs because I need to generate sequences, so they naturally fit, obviously. Also, the sequences need to be of variable length, so using some sort of fully connected network would be possible but rather awkward. I really hope someone can help, as I'm trying to figure this out for days now. Please ask, if there is any information you are missing.
