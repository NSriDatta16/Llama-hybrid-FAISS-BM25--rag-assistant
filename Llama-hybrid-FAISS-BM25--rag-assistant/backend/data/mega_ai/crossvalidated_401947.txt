[site]: crossvalidated
[post_id]: 401947
[parent_id]: 253057
[tags]: 
Though Underminer gives a perfectly good answer, I thought I would take the time to provide a Bayesian estimate. In your problem, you don't explicitly say you have no reason to believe the data are normal, only that the mean and the variance are unknown. The likelihood function is thus $$ y \sim \mathcal{N}(\mu, \sigma^2)$$ Here, $\mu, \sigma$ must be estimated. I'll place an uninformative prior on both of these parameters $$ p(\mu, \sigma^2) \propto (\sigma^2)^{-1} \>.$$ In Bayesian Data Analysis 3 (p.g. 64) Gelman goes through this example and derives an analytic posterior density. The marginal posterior for the variance is $$\sigma^{2} | y \sim \operatorname{Inv}-\chi^{2}\left(n-1, s^{2}\right)$$ with $$ s^{2}=\frac{1}{n-1} \sum_{i=1}^{n}\left(y_{i}-\overline{y}\right)^{2}$$ while the marginal posterior for the mean conditioned on the variance and the data is $$\mu | \sigma^{2}, y \sim \mathrm{N}\left(\overline{y}, \sigma^{2} / n\right)$$ We can sample from these densities very easily with python and compute credible intervals. from scipy.stats import norm, chi2 import numpy as np import matplotlib.pyplot as plt population = norm(loc = 50, scale = 10) n = 10 samp = population.rvs(n) ybar = samp.mean() s = samp.var(ddof = 1) var_posterior = (n-1)*s/chi2(df =n-1).rvs(100_000) mu_posterior = norm(loc = ybar, scale = np.sqrt(var_postrior/n)).rvs() #Now for credible intevals print(np.percentile(mu_posterior, [2.5, 97.5])) print(np.percentile(np.sqrt(var_posterior), [2.5, 97.5])) fig, ax = plt.subplots() ax.hist(mu_posterior, bins = 25) fig,ax = plt.subplots() ax.hist(np.sqrt(var_posterior), bins = 25) plt.show() The benefit from this method is that you get uncertainty estimates of the variance for cheap, whereas Underminer's method only gives you uncertainty estimates for the mean. It actually turns out that the posterior is the student-t density (go figure), so all this is more or less equivalent to Underminer's approach. However, that is only because we assumed we knew nothing about the mean and the variance before hand. If we knew something about those parameters (for instance, that negative values are impossible, or that they were larger than some number) then we could do much more by means of MCMC.
