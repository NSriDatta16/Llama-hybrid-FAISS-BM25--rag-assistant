[site]: datascience
[post_id]: 58230
[parent_id]: 58026
[tags]: 
Let me explain one thing very clearly "NEURAL NETWORK ARE VERY SIMPLE STRUCTURES AND YET VERY EFFICIENT ALGORITHM WHICH GIVES AMAZINGLY GREAT RESULTS AND IF YOU SEE CLOSELY NEURAL NETWORKS ARE ACTUALLY BIG COMPOSITE FUNCTIONS WHICH REQUIRE YOU TO CALCULATE GRADIENTS SO THAT YOU CAN OBSERVE HOW THE OUTPUT VARIES ACCORDING TO YOUR INPUT WEIGHTS(That is what is backpropagation achieved using gradient descent) THAT IS SIMPLY THE GOAL I.E. TO CALCULATE DERIVATIVES OF THE TWO EQUATIONS YOU HAVE MENTIONED (2) AND (3)". Now as you stated you can calculate its value and reuse it i.e. eq(1) because eq (1) here gives you the error at a neuron j in layer 'l' with respect to that neuron's weighted sum of inputs it is useful and necessary for calculation of weights when the loss is calculated at the'Lth layer(last layer)' it is then backpropagated to the L-1 th layer where gradient is calculated and then weights are updated it is the eq(1) that allows you to compute the gradients of a neuron j in layer 'l' and then the weight matrix of that neuron j is updated so essentially eq(1) allows you to use eq(2) and eq(3) to optimize your weights in order to minimize your loss function. One more thing if you calculate its derivative that is of eq(1) its partial derivative it will be: first-rate of change of cost function w.r.t to activation function * rate of change of activation function w.r.t to weighted sum of inputs * rate of change of weighted sum of inputs w.r.t to weights of your neuron so this is essentially a chain in itself which gets longer and longer as the loss is backpropagated through l-1,l-2,l-3.... layers and so on, thus eq(1) is actually calculated using the first two in the chain stated above which is then used to calculate the rate of change of weights w.r.t to your loss function. This is why whenever you read about backprop this equation is bound to be seen as it is kind of connection mechanism that allows you to relate your loss with neurons in the subsequent layers during backpropagation and subsequently update their weights. Hence according to me, eq(1) in itself is the true essence of Backpropagation using gradient descent.
