[site]: crossvalidated
[post_id]: 486078
[parent_id]: 486060
[tags]: 
Both classical and Bayesian parametric methods will assume a "model form" that manifests in a likelihood function parameterised by some unknown parameters. Both assume the structure of the likelihood function, so they make the same structural "model" assumptions. Unlike Bayesian statistics, classical statistics treats the parameters as "unknown constants". This methodology proceeds by looking at statistical properties that are conditional on the true parameter values ---e.g., bias, consistency, MSE, etc. This analysis does not assume that the true parameters (and therefore the true distribution) is accessible. Instead, the classical methodology analyses statistical properties over the class of possible values of the parameters, and tries to formulate methods that work well regardless of the true value of the underlying parameter . In this sense, the methods used in classical statistics tend to be quite robust, at least within the assumed model form. Perhaps an example will assist here. Suppose you have a parametric model for data $\mathbf{x}$ that depends on an unknown parameter $\theta \in \Theta$ . One of the concepts formed in classical statistics is the idea of "consistency" of an estimator. Broadly speaking, for an estimator $\hat{\theta}$ of the parameter $\theta$ , this means that $\hat{\theta} \rightarrow \theta$ (in some appropriate probabilistic sense) as $n \rightarrow \infty$ . In order for the estimator to be considered "consistent", this property must hold for all $\theta \in \Theta$ . Thus, so long as the assumed model form for the likelihood function is correct, a consistent estimator will tend to estimate well in the long run --- this is guaranteed regardless of the unknown parameter value. (Note here that the theoretical derivation of the consistency property involves looking at the behaviour of an estimator under an assumed true parameter value, and varying the assumed true value over the entire parameter space. However, once a particular estimator is known to be consistent within a particular model, for applied work we do not assume that the true parameter is accessible.) The above discussion is focused on the case where the assumed model form is correct. Both Bayesian and classical statistics also have sub-fields where they look at what happens when the assumed model form is wrong, and this is the field of "mis-specification". Classical statistics has a number of well-known results for estimators in mis-specified models (see e.g., a related question here ). Generally speaking, all discussions of parametric models assume that the specified parametric model is correct, and if you want to know what happens when it is not correct you go further out into the field of mis-specification. Obviously it is desirable to look at both of these issues, and it is useful when you have methods that work well for a parametric model that you think is correct, but are also robust to mis-specification. As to your specific questions, these are all extremely broad questions that are essentially asking for a full account of the field of mis-specified models. The answer to your questions depends on which specific models you are talking about. There is a large statistical literature on mis-specification, and this has been examined for many parametric models. The "validity" of parametric methods under mis-specification varies depending on the original parametric model, the estimation method used, and the degree to which the true model departs from the originally assumed form. If you would like to know more about this, I suggest you search the statistical literature on "mis-specification". Some important papers to get you started are White (1982) , McGuirk, Driscoll and Alwang (1993) , Gustafson (2001) and Mayo and Spanos (2004) .
