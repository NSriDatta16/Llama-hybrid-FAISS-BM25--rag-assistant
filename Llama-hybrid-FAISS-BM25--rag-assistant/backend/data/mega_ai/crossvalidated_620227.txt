[site]: crossvalidated
[post_id]: 620227
[parent_id]: 620224
[tags]: 
All statistical models are idealisations. They assume randomness, yet what is expressed in these models is never perfectly fulfilled in reality. Even the best attempt at random sampling can be criticised (you never have a perfect random sample if there are missing responses, and anyway even random number generators are not in fact really random), and there are other assumptions such as normality that will not be fulfilled. Formal assumptions "live in the world of mathematics", which is not the real world. It is still relevant to think about how well the mathematics used for running the analysis is connected to reality, and ultimately indeed there can be problems and conclusions may be invalid because of issues with the data collection such as taking a convenience sample. To what extent this is a problem always depends on the specific situation, it can be discussed, and problematic implications for the analysis may be discovered. So it is a good idea to try hard to do something that is "close" to idealised random sampling in the sense that you can convince yourself and others that results are reliable, and you are right criticising work where the data collection has a clear potential to produce misleading results. But we need to acknowledge that this is not a binary issue. Model assumptions do not "have to be fulfilled and otherwise an analysis would be invalid or useless", as if this were true, we couldn't even do anything valid. Pragmatic compromises have to be made and ultimately reality is too complex for granting formal assumptions. So in any case, this requires case-specific thinking and discussion. I should by the way add that the term "random sampling" and its relation to the formal model assumptions is not straightforward. For example, regression and ANOVA can be applied to data from designed experiments, and although there may (and often should) be randomisation , random sampling is not normally applied. Also how exactly random sampling is carried out will depend on the population to which generalisations are desired, and in many situations (such as measurements in physical experiments) the population is infinite and hypothetical, and the term "random sampling" doesn't really apply (what you have learnt may be connected to a specific application field, maybe in the social sciences, but these models are applied in a large variety of situations and issues with data collection may play out in a variety of ways). More precisely, random sampling is not really about running the model but rather about generalisation of the results. The regression line will summarise the sample without any random sampling; the issue with random sampling is rather whether results allow general statements about a bigger population from which the sample has been drawn (including assessment of the uncertainty of results due to sampling). Such statements may be compromised by issues with random sampling. The issue is with the interpretation of the results.
