[site]: crossvalidated
[post_id]: 337645
[parent_id]: 226555
[tags]: 
I do not think this is a valid ML approach. I will give a simple example to convince you - suppose there are two classifiers c1 and c2. Say c1 is doing good on one set of independent variables x1 and c2 is doing well on another set of independent variables x2, such that x1 U x2 = x i.e. your training set. If you average the scores - both classifiers will start loosing the points where they are good at because the other classifier will bring the score down. What I would instead suggest is : say you get one output from the trained RandomForests - o1 and another output from the trained XGBoost - o2 . You can train a simple LinearRegression on top of these to learn your final latitudes/longitudes. This will ensure that the weights are learnt in way that minimize the loss from each classifier.
