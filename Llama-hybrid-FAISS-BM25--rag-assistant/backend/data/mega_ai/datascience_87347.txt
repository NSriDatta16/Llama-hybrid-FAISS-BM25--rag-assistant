[site]: datascience
[post_id]: 87347
[parent_id]: 87244
[tags]: 
I think the main problem is how you are using BERT, as you are processing your text sentence by sentence. Instead, you should be feeding the input to the model in mini-batches: Neural networks for NLP are meant to receive not only one sentence at a time, but multiple sentences. The sentences are stacked together in a single tensor of integer numbers (token IDs) with dimensions number of sentences $\times$ sequence length. As sentences have different lengths, normally the batch has as sequence length the length of the longest sequence in it, and all the sentences that are shorter than that, are filled with padding tokens. You can have a look at batching in huggingface's library here . Using batch-oriented processing would allow you to profit from parallel processing of all the sentences in the batch. The problem is that, while HuggingFace Transformer supports batch-oriented for training, it does not support batch-oriented inference in some cases. For instance, FeatureExtractionPipeline , which extracts token embeddings like you want, does not support batch processing (unlike TableQuestionAnsweringPipeline which has the sequential parameter). This way, in order to have batch-oriented inference you would need to feed the data manually instead of relying on the pipeline API. You can find examples on how to do that in this thread . If you try using GPU instead of CPU, using batch-oriented processing would also be key to enable performance gains. If you decide to stay on CPU, ensure that your Pytorch build is using MKL-DNN, which is a major performance booster in CPU. You can check this thread on how to do it. If you are not using it, install a newer version that includes it.
