[site]: crossvalidated
[post_id]: 305775
[parent_id]: 305772
[tags]: 
In general you calculate gradient of some error (which most reasonably will be some kind of average of per-row errors) with respect to weights. Yes, but the error is not accumulated, only averaged. When using full dataset you calculate average over the whole dataset. In minibatch SGD you do this for the minibatch, so you can treat this error and gradient respectively as estimators of 'real' error and gradient. No, but if your estimate of error is good enough you might not care, since using minibatches can be much faster. For a concrete example see this notebook (minibatch SGD plot has oscillations, since the samples might not be representative of the whole distribution).
