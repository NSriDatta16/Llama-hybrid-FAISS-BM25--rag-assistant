[site]: datascience
[post_id]: 32358
[parent_id]: 32355
[tags]: 
As you mentioned, each decision tree is trained on p (sometimes sqrt(p) random features). This ensures that each tree is “grown” (trained) differently so that the model 1. Does not overfit the training data (reduces variance) and 2. Generalizes better to new data (reduces bias). Therefore we don’t weigh the trees differently, as this would be similar to having all tress trained on the same features. You can change the voting threshold however from the standard 50% to anything you want (e.g 40%, 70%, 90%) which will change the precision and recall accuracies of the model. EDIT: changing the voting threshold means changing the number of trees needed to make a classification. For example, in binary classification most standard random forests require 50% or more of the trees to vote for a class for that class to “win” (be predicted to be that class). But if you change this threshold, to say 70%, then 70% or more of the trees need to vote for the same class for that class to win.
