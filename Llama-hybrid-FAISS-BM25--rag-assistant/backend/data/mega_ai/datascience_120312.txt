[site]: datascience
[post_id]: 120312
[parent_id]: 
[tags]: 
Why the label is not explicitly involved in the loss function of skip-gram?

I am recently learning word embedding myself. When learning skip-gram from the paper https://arxiv.org/pdf/1310.4546.pdf[Distributed Representations of Words and Phrases and their Compositionality], I am stuck in understanding the loss function. $$ -\frac{1}{T}\sum_{t=1}^{T}\sum_{-c\le j\le c}\log p(w_{t+j}|w_t) $$ It is so-called conditional likelihood or something, a little bit like cross entropy, but it is not. In the first forward propagation, we feed the word pairs to the neural network. For example, input $w_1$ as the focus word and $w_2$ as the context word. However, I cannot see how $w_2$ works in the loss. Traditionally, the loss function for a supervised learning should contain label and the predicted value, but here $p(w_{t+j}|w_t)$ is just a predicted value calculated by applying softmax function on the output layer. If there is no label in the loss function, how can we reduce the loss by backward propagation.
