[site]: crossvalidated
[post_id]: 296778
[parent_id]: 296773
[tags]: 
Your data are modeled with error. If you are using a standard regression, then this error is assumed to be symmetrically distributed around the expected value. The key thing to observe is that regression aims at modeling and predicting the expected value . (Actually, this is not only the case for regression, but for almost all statistical or machine learning models, except for quantile or density prediction methods.) So, assuming that you predicted a value that turned out to be very high, i.e., with a high $y$ coordinate in your plot. This value almost certainly had a high expectation, but - given that the observation is high - it also had a large positive error or noise term. The prediction was the expectation, this is the red dotted line. It's still high, but given the high error, the prediction is systematically too low. The same holds the other way around for the very low observations. Thus, what you are seeing is not a shortcoming of your algorithm. It is simply a consequence of modeling expectations but observing with errors. Any prediction will underforecast very large and overforecast very small actuals. Put differently: conditional on observing a high actual, your prediction will be biased low, and vice versa. There is nothing you can do. Unless of course there are not-yet modeled influences, but even if you do model these, the same problem will remain. The concept of regression toward the mean is related.
