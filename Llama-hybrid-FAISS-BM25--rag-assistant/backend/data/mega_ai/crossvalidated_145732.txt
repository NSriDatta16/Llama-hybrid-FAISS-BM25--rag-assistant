[site]: crossvalidated
[post_id]: 145732
[parent_id]: 
[tags]: 
What methods can I use to aid in modeling a smaller data set when I have a significantly larger data set with fewer variables?

I currently have a data set with about 4,000 rows. The current model I have established for it is not very good, and I am going to receive more data for about 150 of these points, and I'm hoping that there will be a very high correlation between the new variables and the response variable. Normally I would opt to use imputation or data removal, but it is very wasteful to throw out 95% of the data, especially when parameter estimates can still be useful, and imputing that large of a portion of a data set seems like bad practice. My goal is to build a predictive model (most likely just a linear regression) using the variables. Is there any way I can use the majority of the data set to aid in this modeling? Edit: I am using cross-validation and mean-squared error to measure predictive capability, since I am not using time series data.
