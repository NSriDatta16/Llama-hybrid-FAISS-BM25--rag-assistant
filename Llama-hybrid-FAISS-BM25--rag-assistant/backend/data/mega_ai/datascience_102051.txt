[site]: datascience
[post_id]: 102051
[parent_id]: 102050
[tags]: 
I'm not sure that these terminologies are universal, but the xgboost documentation appears to be considering a "decision tree" to specifically mean that the predictions made are hard class predictions (the mode of the classes among training data in a leaf), not probability predictions, and therefore not usable for regression tasks. Regression trees on the other hand generally average the target values in each leaf, and that leads to a useful "soft" classifier version of classification trees as well. Random forests and AdaBoosting may use either hard or soft voting, but gradient boosting requires each learner to be a regressor (fitting to pseudo-residuals), and so XGBoost and LightGBM both use those.
