[site]: crossvalidated
[post_id]: 400873
[parent_id]: 
[tags]: 
Intuition behind MA(q) (moving average) time series forecasting model (i.e. 'MA' part of ARIMA) and implementation

The $AR(n)$ part of ARIMA makes sense to me. If $$x_{t+1}=\sum_{i=0}^n a_ix_{t-i}$$ then we are making the intuitive assumption that the next time step will somehow depend on the previous time steps. However, what is the assumption behind $$x_{t+1}=\sum_{i=0}^n a_iw_{t-i}$$ where all the $w_i$ are normally distributed error terms? From what I read in textbooks, errors are not to be confused with residuals, these $w_i$ signify actual errors in measurement. If that is the case, why is it reasonable to assume that the next time step will somehow depend on the white noise error measurements at previous time steps? If the $w_i$ are centered at zero, doesn't this equation imply that the expectation of $x_{t+1}$ is just $\sum_{i=0}^n a_i$ ? Alternatively, the $w_i$ actually stand for residuals. In that case, would the assumption behind this model be that the next time step only depends on how large/small our forecasting errors (i.e. residuals) were in the past. In this case, would the intuition be that we want a model that adjusts to how badly it performed in the previous time steps? Perhaps adjusts is the wrong word. In either case, we can't actually calculate this explicitly, so how does one go about writing an algorithm to implement this? Can you point me to a source of algorithms for $MA(q)$ , perhaps seeing that will shed some light? I have seen derivations where an $MA(q)$ model can be represented as an infinite $AR(\infty)$ model, do implementations just cut off this $AR(\infty)$ at some finite point and use that finite $AR$ process as an estimator for $MA$ ?
