[site]: crossvalidated
[post_id]: 367777
[parent_id]: 
[tags]: 
Word embedding as input or raw text?

I'm trying to implement a neural network for text recognition and I'm a little bit confused about text inputs. The goal of the network is to classify a comment, toy example: I like that -> GOOD I don't like that -> BAD For now I'm tokenizing my comments as: [i, like, that] [i, dont, like, that] and my questions are: Should I input that list of words into the network? how?? one-hot encoding? I have read and tried Word2Vec but once I have my model should I translate the comment as a list of vectors? I mean: In my Word2Vec model imagine that my ' i ' is [0.23, 0.45, 0.1] , ' like ' is [0.67, 0.15, 0.98] and ' that ' is [0.43, 0.25, 0.72] , My first input ( which were [i, like, that] ) should be now [[0.23, 0.45, 0.1],[0.67, 0.15, 0.98],[0.43, 0.25, 0.72]] ? In that case, I should be using Word2Vec to reduce dimensionality, right? so if with one-hot enconding I had a matrix of WordNumber * WordNumber now I should create vectors in order to have WordNumber * Vectorlength , right?
