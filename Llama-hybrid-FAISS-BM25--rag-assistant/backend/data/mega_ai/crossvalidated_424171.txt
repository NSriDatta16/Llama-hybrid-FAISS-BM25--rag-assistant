[site]: crossvalidated
[post_id]: 424171
[parent_id]: 424119
[tags]: 
Question 1 This is good summary of how iterative gradient updates work, but this definition leaves out some classes of models, so I'm hesitant to agree that it is comprehensive. For example, $k$ -NN classifier does not minimize a loss. It doesn't even "learn" anything: at the time that you'd like to classify some new observation, it does a nearest neighbors search. In contrast to logistic regression or neural networks, there's no equation to evaluate. In contrast to a decision tree, there's no tree. All it $k$ -NN does is measure distances. Each time you have a query, it starts measuring distances anew. In this sense, a $k$ -NN doesn't generalize anything about the training set to some other abstraction (a tree, a formula). Additionally, there's no training procedure, so nothing about the $k$ -NN classifier is iteratively updated. In another example, random forest proceeds by always making greedy splits. While it's true that each of these splits is an optimization (maximize information gain), the total loss of the model is never evaluated during training, nor are previous trees updated. So in that sense, it's not correcting itself, even if some of its trees usually make wrong predictions. The whole point is that, on average, the ensemble will do well. Question 2 I don't agree with your definition. Additionally, I think that we can make some meaningful distinctions between the iterative gradient updates of SVMs and neural networks. SVMs were expressly designed to be strongly convex optimization problems, while neural networks are non-convex optimization problems. So although both of them are using iterative gradient updates to improve model fitness, we can have much more confidence that a trained SVM is optimal (at least wrt training data), because a strongly convex problem has a unique minimum. Question 3 UAT is an interesting theorem, but in applied settings, practitioners tend to get hung up on the word "universal" and neglect the hypotheses which are necessary for UAT to apply. As a practical matter, one might want to ask Does the specific problem that I'm working on satisfy the requirements of UAT? Is the function continuous? Do we only care about a compact subset of reals? Is it possible to find the ideal network weights in a reasonable amount of time? Do we have enough data to learn the ideal network (sufficient number of neurons)? UAT assumes we can tolerate $\epsilon$ amount of error. Is it possible to train the model to achieve sufficiently small error given the fixed amount of data that we have? Stated another way, does our model over- or under-fit the data? In applied settings, similar sentiments can be applied to Turing completeness of a particular network. In a setting where you do not know the underlying function you wish to approximate, it will be challenging to answer these questions. Neural networks are very general and flexible, but that generality and flexibility comes with costs. Tuning and training a neural network is a very expensive proposition, and it may take a lot of work to find a network which can out-perform a simpler model. In most tabular problems (problems where the data is a single matrix where rows are observations and columns are feature values), a random forest is an extremely powerful "default" model which will be hard to beat without investing lots of R&D time. Moreover, there are lots of ways to approximate smooth functions on a compact interval which are much simpler than a multi-layer neural network. For example, spline regression is a very flexible tool that is not much more complicated than a linear regression. The Stoneâ€“Weierstrass theorem provides a similar result to the UAT in the case of polynomial regression (thanks, @Sean507). These observations are not intended as criticisms of theorists who demonstrate that some models have these sophisticated properties, but if one's interest is primarily in applications and not theory, then a fixation on generality or provability can be a distraction from more pressing concerns, such as deadlines and customer satisfaction .
