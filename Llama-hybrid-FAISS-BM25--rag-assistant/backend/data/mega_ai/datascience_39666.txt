[site]: datascience
[post_id]: 39666
[parent_id]: 23789
[tags]: 
I would add a small addition to the good answers. The main problem is overfitting. As soon as you have more than one parameter and also add non-linear functions, all the algorithms start to overfit. They see something in the data that does not exist. Like when it is dark or the fog is strong people tend to see things in the darkness/fog that do not exist. Almost all the computational algorithms do more overfitting than humans do. Even linear regressions start to show strange coefficients when the variables are highly correlated. If there was no overfitting then usual decision trees, on which those algorithms are based on, would have been better than Random Forest or XGBoost. And there is no exact science why overfitting occurs and why some algorithms are better than the others. In theory ARIMA models are very sound, but practice shows that using exponential smoothing techniques is better and ARIMA cannot even differentiate variables that behave according to ARIMA but with different parameters. Some neural networks and especially convolutional neural networks appear to have low overfitting. At the same time the original idea of fully connected neural networks fails with high number of neurons because of overfitting. The main possibilities to fight overfitting are: random sampling averaging across multiple models randomizing the model (random dropping of neurons while training neural networks) If I understand the algorithms correctly both Random Forest and XGBoost do random sampling and average across multiple models and thus manage to reduce overfitting. In ImageNet image recognition competition the best model for 2016 (Shao et al) was a combination of several really good models. Some of them won the competition in previous years. This model had a 20% less error than any of the models it was based on. This is how averaging across multiple models could be strong in fighting with overfitting.
