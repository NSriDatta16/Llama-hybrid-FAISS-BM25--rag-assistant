[site]: datascience
[post_id]: 60424
[parent_id]: 60296
[tags]: 
In general, the cost function of a neural network is non-convex, so it's mathematically okay that you're converging at a local minima. (To convince you that it's non convex, take any permutation of the set of weights and it would also yield you to the same point in space). As a suggestion, try using a linear classifier, or a multilayer perceptron and compare your results.
