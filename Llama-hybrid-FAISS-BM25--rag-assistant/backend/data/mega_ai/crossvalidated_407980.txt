[site]: crossvalidated
[post_id]: 407980
[parent_id]: 
[tags]: 
BERT Classification fine tuning for Q & A

I want to fine-tune BERT for Q & A in a different way than the SQuAD mission: I have pairs of (question, answer) Part of them are the correct answer (Label - 1) Part of them are the incorrect answer (Label - 0) I want to fine-tune BERT to learn the classification mission: Given a pair of (q, a), predict if a is a correct answer for q. What is the best way to do it? Which model should I use? What is this mission? It's not a classic classification because the run_classifier demands only the text and the label, and I have the answer as well. It's also not classic q&a like SQuAD... What should I do? Thanks
