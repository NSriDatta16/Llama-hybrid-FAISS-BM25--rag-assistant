[site]: crossvalidated
[post_id]: 300178
[parent_id]: 299354
[tags]: 
I may worth to try a neural network for classification, specifically an LSTM is doing quite what you would like to achieve. It could be used as follows: LSTM need input sequences to be the same length. This could be solved by padding the data by adding leading characters. The padding character should be not 0 or 1. Another solution is to use batch size 1 without resetting the status after each batch. Once padded data should be encoded using the one hot method. You can use 3 categories: 0, 1 and the padding character. The last binary value of each sequence should be used as a target, the rest as input. Of course the padding character will never be the target because we padded on the front. Stacking 2 LSTM will do a better job Your network could look like: Input->LSTM->LSTM->Dense->Dense->Output The more data you have the better it will learn the patterns. Such a network will learn very easily that the padding character is never the output, so don't worry about it.
