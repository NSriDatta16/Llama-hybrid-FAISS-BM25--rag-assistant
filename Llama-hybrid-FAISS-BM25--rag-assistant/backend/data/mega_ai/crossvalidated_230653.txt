[site]: crossvalidated
[post_id]: 230653
[parent_id]: 
[tags]: 
Is it possible to over-train a classifier?

Context: I'm constructing a CNN classifier for text categorization. I have a dataset with 20 different classes and approximately 20,000 labeled features (the 20 News Group dataset for those interested). I'm wondering if I'm training my model on too many epochs, which would make it really good at categorizing the features from my training dataset, but unable to adapt to new / slightly different inputs. Is that what we call "overfitting"? The term is not clear to me. Also I would like to clarify the term "convergence" of a neural network. Is this convergence attained when the accuracy starts plateauing? Or is it related to the loss value?
