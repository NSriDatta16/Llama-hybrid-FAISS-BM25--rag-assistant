[site]: crossvalidated
[post_id]: 36046
[parent_id]: 35971
[tags]: 
Yes, sampling is relevant and will remain relevant. Bottom line is that the accuracy of a statistical estimate is generally a function of the sample size, not the population to which we want to generalize. So a mean or an average proportion computed from a sample of 1,000 respondents will yield an estimate of a certain accuracy (with respect to the entire population from which we sampled), regardless of the size of the population (or “how big” the “big data” are are). Having said that: There are specific issues and challenges that are relevant and should be mentioned: Taking a good probability sample is not always easy. Theoretically, every individual in the population to which we want to generalized (about which we want to make inferences) must have a known probability of being selected; ideally that probability should be the same (equal probability sample or EPSEM – Equal Probability of Selection). That is an important consideration and one should have a clear understanding of how the sampling process will assign selection probabilities to the members of the population to which one wants to generalize. For example, can one derive from Twitter feeds accurate estimates of overall sentiments in the population at large, including those individuals without twitter accounts? Big data may contain very complex details and information; put another way, the issue is not sampling, but (micro-) segmentation, pulling out the right details for a small subset of observations that are relevant. Here the challenge is not sampling, but to identify the specific stratification and segmentation of the big data that yields the most accurate actionable information that can be turned into valuable insights. Another general rule of opinion measurement is that non-sampling errors and biases are usually much bigger than the sampling error and biases. Just because you process 1 hundred gazillion records of respondents expressing an opinion doesn’t make the results more useful if you only have data of a 1000 person subsample, in particular if the questions for the respective survey were not written well and induced bias. Sometimes sampling is required: For example, if one were to build a predictive model from all data, how would one validate it? How would one compare the accuracy of different models? When there are “big data” (very large data repositories) then one can build multiple models and modeling scenarios for different samples, and validate them (try them out) in other independent samples. If one were to build one model for all data – how would one validate it? You can check out our 'Big Data Revolution' here.
