[site]: crossvalidated
[post_id]: 541250
[parent_id]: 541200
[tags]: 
Cool project! Regarding inputs: It depends on how you want to treat the signal. Putting wavenumbers as channels would result essentially in a fully connected Linear layer. In a deep learning convolution, each filter will have different weights for each channel. So if you orient the spectrum so that wavenumbers lie along the channels axis, your convolution will just be computing a dense dot product. If you're using convolutions, then you probably don't want that. (Otherwise you'd just explicitly model as a Linear layer). But, regarding architecture and nan s: Are you sure you want convolutions for this problem? Is there a reason to think that there are local patterns that repeat at different wavenumbers? Patterns that repeat within a signal are where convolutions provide modeling utility. But if the goal is something like fingerprinting, I would think a Linear layer is closer to what you want. (Have not looked at the citations, though - maybe they provide good justification for the convolutions!) Ditto pooling - if you're not using convolutions, might not be very useful. I'd also reconsider whether batch norm is the ideal normalization for this problem - I would think layer normalization would be sufficient and generally easier to work with, especially at smaller batch sizes like 32. As far as nan s, these in my experience usually result from optimization issues, not the model. If you're using Adam or something else complicated, I'd try switching to SGD with momentum and lowering the learning rate. I'd also check your data and confirm what the model is receiving during training is what you expect.
