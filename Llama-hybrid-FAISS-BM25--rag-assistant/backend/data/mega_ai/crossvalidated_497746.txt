[site]: crossvalidated
[post_id]: 497746
[parent_id]: 417129
[tags]: 
I think the Gaussian kernel does suffer from the curse of dimensionality. In the machine learning setting, especially the regression setting, we are trying to recover a $d$ -dimension function from the noise data. But we can not just search within the whole space of $L_2(\mathbb{R}^d)$ : it is just so big that the bias-variance trader-off is ruined (and I doubt if there is any procedures can direct do this). We usually need to search within a sub-space of $L_2(\mathbb{R}^d)$ : we can search within the $d$ -dimension Sobolev space (which itself is an RKHS), which is a space that is big enough so that usually the true function indeed lies in this space. Or we can search within the RKHS related to the Gaussian kernel, by using Gaussian kernel as the kernel function in your favorite machine learning algorithm. However, this space is surprisingly small: which is smaller than any $d$ -dimension Sobolev space, i.e. the function in this RKHS is super smooth. Therefore, if you assume \textbf{your regression function indeed lie in this super small function space} and try to learn it from data, you would get a pretty nice dependency on the dimension. However, if your true regression does not lie in this space, you need to suffer a huge approximation error, which potentially would cause more trouble. It sounds like you have basic familiarity with high-dimensional statistics: the Wainwright 2019 book on high-dimensional statistics would be a nice starting point. You can read his discussion in chapter 12 on RKHS: pay attention to the eigenvalue decay rate.
