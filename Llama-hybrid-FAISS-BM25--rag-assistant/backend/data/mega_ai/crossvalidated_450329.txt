[site]: crossvalidated
[post_id]: 450329
[parent_id]: 450321
[tags]: 
Traditional unsupervised outlier detection algorithms include principal component analysis , support vector machine and Isolation Forest . Clustering, for example, k-means, is very related to GMM and hence to autoencoder. Autoencoder is the core technique for almost all unsupervised anomaly detection algorithms(8.2.3 in the reference paper). I thought we can try to embed the inputs and then add an autoencoder for the embeddings. To tell if the coming input in test time is an outlier we can compare the embedding before the autoencoder and the reconstruction output after the autoencoder by calculating their difference, cosine distance for instance. We can detect the outliers according to the Interquartile Range Rule(IQR) . Since GMM is a soft-clustering algorithm we can add that atop of the autoencoder as described in this paper: Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection , or just an GMM is enough. Reference: Deep Learning for Anomaly Detection: A Survey
