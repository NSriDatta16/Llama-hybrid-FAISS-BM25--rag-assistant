[site]: crossvalidated
[post_id]: 384702
[parent_id]: 384355
[tags]: 
The issue is that a CNN by definition will have an ever-growing receptive field as you keep going deeper. At some point you'll apply a final global pooling, and this somewhat captures the attention aspect of the image. On the one hand this tends to light up regions of the image that are relevant. On the other hand, each piece of the global pooling layer has a rather large receptive field. For example if the global pooling gives you a 10x10 output, then the top-left value of this corresponds to 1/10 of the image (from the top left). You could take your current loss function and add a term that minimizes the L1 norm of the output of the global pooling layer. This will somewhat mimic trying to force the model to use as little of the image as possible for the final classification part.
