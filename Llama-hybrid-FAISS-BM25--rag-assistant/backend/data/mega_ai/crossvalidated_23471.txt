[site]: crossvalidated
[post_id]: 23471
[parent_id]: 23470
[tags]: 
Sparsity and linear dependence are two different things. Linear dependence implies that some of the feature vectors are simple multiples of other feature vectors (or the same applied to examples). In the setup you have described I think linear dependence is unlikely (it implies two terms have the same frequency (or multiples thereof) across all documents). Simply having sparse features does not present any problem for the SVM. One way to see this is that you could do a random rotation of the co-ordinate axes, which would leave the problem unchanged and give the same solution, but would make the data completely non-sparse (this is in part how random projections work). Also it appears that you are talking about the SVM in the primal . Note that if you use the kernel SVM, just because you have a sparse dataset does not mean that the kernel matrix will be sparse. It may, however, be low rank. In that case you can actually take advantage of this fact for more efficient training (see for example Efficient svm training using low-rank kernel representations ).
