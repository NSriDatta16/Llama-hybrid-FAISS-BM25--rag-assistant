[site]: crossvalidated
[post_id]: 20735
[parent_id]: 
[tags]: 
Arithmetic for updating likelihoods using Bayes theorem

This may be an elementary question which is why I have not been able to find it on Stackexchange or Mathoverflow however I am having problems with the arithmetic involved in updating likelihoods using Bayes theorem for a problem I am working on. Background: I am attempting to give likelihood forecasts to future events which have no or few precedents. Unlike most of the literature and texts on Bayes which use previously known distributions to give likelihoods on future events within similar parameters - my situation is founded on expert opinion only with few or no reasonable distributions to reference. Example: GM announced they are developing a new car but didn't say when it would be released. The Production Manager for KIA needs to know when they will be ready to release it so that they can release their new car around the same time. KIA knows that that the new car needs the following components in order to be ready for release (1) engine, (2) transmission, (3) body, (4) Wheels and Suspension. KIA's experienced engineers state that for a new project like this they are 90% confident that it can be completed in two years. KIA also found out that GM did a test with the new transmission in another SUV and it worked as designed with a 95% success rate. The same engineers stated that given this transmission test a car can be completed within that time frame 70% of the time. The way I have it, at this point KIA can start the Bayesian calculation with the initial sample as below: A = GM will release the new car in two years B1 = GM will successfully test a new transmission P(A) = Prior Probability that GM will release the new car in two years P(B1) = Probability that GM will successfully test a new transmission P(B1|A) = Likelihood that given a successful transmission test, the car will be released within 2 years Assigning values as follows P(A) = .9 P(B1) = .95 P(B1|A) = .7 $$ P(A|B_1) = \frac {P(A)P(B_1|A)}{P(A)P(B_1|A)+P(\bar{A})P(B_1|\bar{A})} $$ $$ .9545 = \frac {.9*.7}{(.9*.7)+(.1*.3)} $$ Shortly after the KIA statistics department gave this update, GM announced that they had tested their new engine and it had a 98% success rate over all its tests. The KIA engineers said that typically if there is a successful engine test that there is a 80% likelihood that a car will be completed on time - but that they did not know what the likelihood on the overall completion time was given both and engine and a transmission test was. Values now for our second bit of evidence, which should be noted are independent for this case - but are not in all cases for example the body must go on after the suspension: P(B2) = .98 P(B2|A) = .8 So here is where I am having trouble: arithmetically integrating the posterior P(A|B1) into the calculation for P(A|B1,B2), given that priors should stay constant. As I mentioned, some events within {$B_1...B_n$} are independent, others are conditional. I have seen the wikipedia entry which describes three event bayes extention: $$ P(A|B_1,B_2) = \frac {P(B_2|A,B_1)P(B_1|A)P(A)}{P(B_2|B_1)P(B_1)} $$ however what about a fourth and fifth extension? Most of the books and online resources I have do not show the steps for updating priors in any fashion that I can discriminate. It could be that I am too far removed from my undergraduate calculus days to interpret it, but my fear is that I need to have significant experience in set theory and graduate level maths in order to do what would appear to be a simple calculation. This exchange is the closest I could find and even it does not step through it. The fact that I have not after a week of searching found a basic tutorial on the mechanics of updating Bayes theorem (not mind you on what Bayes theorem is and how it works - there are more that enough of those) beyond the first implementation, makes me think it is not a trivial calculation. Is there a straightforward way to do this updating without graduate level mathematics? Note: I am aware of the irony related to the inherent difficulty of the "updating problem" WRT Bayes as Yudkowski has gone on about it for some time. I was assuming, perhaps incorrectly, that those working on it were referencing much more complex iterations, however I am aware that it could be the case I am running into that issue.
