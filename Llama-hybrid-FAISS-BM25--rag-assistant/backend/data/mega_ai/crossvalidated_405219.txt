[site]: crossvalidated
[post_id]: 405219
[parent_id]: 403086
[tags]: 
Answering the three comments on the first answer: I do not understand what you mean by 'RL': This approach (using RNN' is one way of solving an RL problem. The RNN-training consists of two steps, yes. In the case of the bracket language it would consist of teaching it in a somewhat intermediate mode to generate only valid bracket expressions and then fine tune it towards producing valid bracket words with desirable properties (like having many brackets and/or a's and so forth). The intermediate part looks like this: We need a fixed training set of 'valid' expressions, i.e. for example . Then we still cycle through the word using the hidden states and the output distributions and so forth but we do not sample a single character but we use cross entropy in order to measure 'how far off' the produced distribution over characters is from the one that actually comes next in the word. Then we do the usual thing: gradient descent. This is intermediate because we still cycle through the word like in the RL-step but it is also supervised because we know the 'actual' final word that the RNN is supposed to sample. In the second step, we actually use an RL-like prodcedure as described above and let it produce new words and punish it if it produced words with no desirable properties and we encourage it if it produces words with desirable properties. This is also "supervised" (at least I would not call it unsupervised) because you know the rewards! Let us assume that the functions $T,S$ only look at the very last character. Then the automata sketched in the answer actually models the problem well. This can still be considered as a reinforcement learning problem but it is a very simple one: You do not need any fancy RNN or so... Just sample every character once, see what reward you get and then exclusively produce words that consist of the character giving the highest reward. In RL, people tend to use this example as a toy example nevertheless. This is called multi armed bandit. The multi armed bandit is nevertheless interesting however, because the rewards are probabilistic (i.e. random variables). However, if the functions take into considerations the different interactions between characters, then it is about the "path that you went" in order to arrive at this particular word rather than just about the character that you just appended and the problem becomes more complicated but also interesting. A Markov Decision Process (MDP) is a collection of random variables $(S_t, A_t, R_t)_{t \in T}$ where $T$ is either all of $\mathbb{N}_0$ or a finite set $\{0,1,2,...,M\}$ such that the random variables satisfy some properties (for example: at least every finite vector of them like $(S_0, A_0, R_0, S_1, A_1, R_1, ..., S_N, A_N, R_N, S_{N+1})$ needs to have a common density). Then we call $\pi_t(a_t|s_t) := p(a_t|s_t)$ the policy at time $t$ , $\Delta_t(s_{t+1}|s_t,a_t)$ the transition probabilities and $\text{rew}_t(r_t|s_{t+1},s_t,a_t) := p(r_t|s_{t+1}, a_t, s_t)$ the rewards. One major assumption is that $\Delta_t$ actualy does not depend on $t$ and also that $\text{rew}_t$ does not depend on $t$ . When we talk about RL in a very (the only in fact) clean, mathematical way, we talk about manipulating the policy $\pi = (\pi_t)_{t \in T}$ in such a way that some quantity gets maximized. Usually, that quantity is as follows: Given $\gamma \in (0,1)$ , we want to maximize $$v_0(s) := E[\sum_{k=0}^\infty \gamma^k R_k | S_0=s]$$ A Markov Decision Automata (MDA) is an automata with actions, reward distributions and transition probabilities on the edges. What is the relationship between MDPs and MDAs? Can we ever construct (i.e. does it exist) something complicated like an MDP, a potentially infinite sequence of random variables that satisfy a lot of properties? Turns out that we can and that MDPs are exactly the trajectories of runs along MDAs. What do I mean by that? Well, let us assume that we are given an MDA together with a policy. Then we can actually create an MDP from that: First we construct an infinite sequence of iid random variables $\Delta_{a_t, s_t}$ that are distributed as given by the edge from the state $s_t$ using action $a_t$ . We also create iid random variables $R_{s_{t+1}, a_t, s_t}$ that are distributed as given on the edge leading from state $s_t$ to $s_{t+1}$ using action $a_t$ . We also create random variables $P_{t, s_t}$ distributed as $\pi_t(\cdot|a_t)$ describes. We also assume that we are given the initial distribution (i.e. random variable) for $S_0$ . Given the random variable $S_t$ we define $S_{t+1}, A_t, R_t$ as follows: Then we put $A_t(\omega) = P_{t, S_t(\omega)}(\omega)$ , $S_{t+1}(\omega) := \Delta_{\pi_t(A_t(\omega), S_t(\omega))}(\omega)$ , $R_t(\omega) := R_{S_{t+1}(\omega), A_t(\omega), S_t(\omega)}(\omega)$ . It can be shown that this weird collection of random variables then indeed is an MDP. For a sketch of the proof you might want to check out the simpler case of Markov Processes: https://mathoverflow.net/questions/292942/markov-processes-construction-of-the-state-variables . That is what I mean by 'the MDP emerging from the MDA'. Given an MDP, a policy $\pi$ is called markovian iff. $p(a_t|s_t, a_{t-1}, r_{t-1}, s_{t-1}, ..., r_0, a_0, s_0) = p(a_t|s_t)$ , i.e. iff. the choice of the next action only depends on the current state $s_t$ but not on the past. If we try to use the simple MDA and then we nevertheless want to use an RNN then due to the hidden state, the policy $\pi_t$ at time $t$ depends on the whole word sampled so far and not only on the current state (which is the non informative state $s_0$ all the time!). So it actually depends on all the actions (i.e. chosen characters) $a_0, a_1, ..., a_{t-1}, a_t$ in order to make the decision about the next character $a_{t+1}$ , i.e. it is non-markovian. To understand 3 completely you must have some basic knowledge about probability theory (what is the precise definition of a random variable? Does every random variable have a density? Why do we describe discrete random variables with integrals instead of sums, etc) and you should definitely read Puterman, Markov Decision Processes, Chapter 6
