[site]: crossvalidated
[post_id]: 125562
[parent_id]: 125542
[tags]: 
The first point of $\geq$ is that the hypothesis space is topologically closed within the whole parameter space. Without considering randomness, this can be a useful convention if you have some assertion about a converging sequence of parameters belonging to the hypothesis because then you would know that the limit does not suddenly belong to the alternative. Now considering the probability distributions, they are (usually) right-continuous. That means that the mapping of the closed hypothesis space to the $[0,1]$ interval is closed again. That's why confidence intervals are also closed by convention. This enhances mathematics. Imagine, you would construct a confidence interval for the location parameter of an asymmetric probability distribution. There, you would have to trade the length to the upper tail for the length to the lower tail. The probability in both tails should sum up to $\alpha$. To have the CI as informative as possible, you would have to shorten the CI's length such that its coverage probability is still $\geq 1-\alpha$. This is a closed set. You can find an optimal solution there by some iterative algorithm, e.g. Banach's fixed point theorem. If it were an open set, you cannot do this.
