[site]: crossvalidated
[post_id]: 209661
[parent_id]: 209646
[tags]: 
Yes, feedforward neural networks (FFNN) are networks without loops. The source of confusion seems to be that Wikipedia (as well as other literature) uses it more or less as a synonym for Perceptrons and Multi-Layer Perceptrons (MLP). But technically, RBFNs are FFNNs too, by definition, since information flows only in one direction. The differences between MLPs and RBFNs are: MLP: uses dot products (between inputs and weights) and sigmoidal activation functions (or other monotonic functions) and training is usually done through backpropagation for all layers (which can be as many as you want); RBF: uses Euclidean distances (between inputs and weights) and Gaussian activation functions, which makes neurons more locally sensitive. Also, RBFs may use backpropagation for learning, or hybrid approaches with unsupervised learning in the hidden layer (they have just 1 hidden layer).
