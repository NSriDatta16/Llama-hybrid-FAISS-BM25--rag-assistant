[site]: crossvalidated
[post_id]: 533657
[parent_id]: 533495
[tags]: 
This type of model comparison might actually be leading to your apparent loss of proportional hazards (PH). There are problems when you omit any outcome-associated predictor from a Cox model, as there are in logistic regression . At the least, that can lead to bias in the coefficient estimates of the retained predictors. The situation can get exacerbated in Cox models evaluating events over time, particularly if predictors are correlated. Say, for example, that your predictor A is associated with a faster development of disease than is predictor B while A and B are associated with each other. Then if you omit A from a model that still contains B , those with the A marker will preferentially develop disease at earlier times and be lost from the risk set at later times. So in a model that omits A it might seem that the association of B with outcome decreases over time as the higher-risk cases (marked by A ) leave the risk set: an apparent loss of PH. If you keep A in your model, the model can take the A status into account and you might find that the combination of A and B maintains PH adequately. In general, the more outcome-associated predictors you include in your model without overfitting, the better predictive performance will be. As your predictor C is the combination of A and B , one would expect the C model to be superior. Comparing hazard ratios (HRs), however, is not the best way to compare models. HRs tell you how quickly the disease develops on average, but they don't necessarily demonstrate the model's overall ability to discriminate event times among cases or to reliably gauge the probability of disease development by any given time. You might, for example, have a predictor that works very well with a large HR on a small subset of cases but doesn't work well over the entire population of interest. Provided that the models are fit to the same data set, overall model comparison can be done by anova() for nested models, or by the Akaike Information Criterion (AIC). There are tools for evaluating Cox model discrimination and calibration in, for example, the rms package in R. If PH doesn't hold, then there aren't well-defined HRs in any event--they necessarily change over time. Without PH, comparisons among HRs thus make even less sense. The best way to proceed to to grapple directly with PH, as the lack of proportionality might be of substantive interest. For example, use time-varying coefficients, as explained in the R survival package time-dependence vignette . Something as simple as a step function could both solve the PH problem and indicate different associations of a marker with earlier versus later events. Or try an accelerated failure time model that doesn't follow PH, like a log-normal or log-logistic model. If you nevertheless still want to compare HRs among Cox models that don't meet PH, then you have to choose what type of "average" HR you want to compare. Absent PH, the standard Cox model provides a type of event-averaged HR. That average thus depends on the distributions of event and censoring times. The weighting procedures in coxphw try to to provide more of a time-averaged HR less dependent on the specific event and censoring times. According to the article describing the package , the procedures weight the contributions of each event time to the partial likelihood according to the product of the survivor function and the inverse cumulative probability of follow-up at that time. Absent PH, however, those HR values from coxphw aren't time-constant HRs in the sense envisioned in a Cox model. Those HR values can be interpreted in terms of odds of concordance, a useful measure of a single model's performance, but concordance is not sensitive for distinguishing among models . That continues to argue against using HR values to compare among models. Absent PH, comparing HRs between standard Cox models and those from coxphw is "comparing apples to pears," as you fear. So choose one or the other type of model for such comparisons. If all models are fit on the same data, my sense is that the coxphw correction will be similar for all models, so that inter-model comparisons should be similar with either standard or coxphw models even if the absolute HR values differ between those model types. One might argue that the coxphw approach for all models will be the most generally reliable, but I have no practical experience with it.
