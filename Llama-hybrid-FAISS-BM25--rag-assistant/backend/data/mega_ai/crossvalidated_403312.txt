[site]: crossvalidated
[post_id]: 403312
[parent_id]: 
[tags]: 
Understanding the infinite sum of random variables

I am doing a course on time series analysis, and am struggling with this definition: We call a weakly stationary process $\{X_t\}$ invertible with respect to a white noise $\{\epsilon_t\}$ if there exist real numbers $(\phi_j)_{j \in > \mathbb{N_0}}$ with $\sum_{j=0}^{\infty}|\phi_j| and $\epsilon_t = \sum_{j=1}^{\infty}\phi_j X_{t-j}$ for all $t \in > \mathbb{Z}$ . What exactly does it mean for a random variable, $\epsilon_t$ in this case, to be equal to an infinite series of random variables, $\sum_{j=1}^{\infty}\phi_j X_{t-j}$ ? For some real sequence $\{x_t\}$ , we write $L = \sum_{j=0}^{\infty}x_j$ if $\lim_{n \to \infty} s_n$ exists and equals $L$ , where $s_n = \sum_{j=0}^{n} x_j$ , but I cannot see how this definition extends to a random sequence?
