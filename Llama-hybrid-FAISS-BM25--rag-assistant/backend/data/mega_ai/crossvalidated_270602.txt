[site]: crossvalidated
[post_id]: 270602
[parent_id]: 270587
[tags]: 
A related idea is used in the context of Approximate Bayesian Computation, where people have an original data set, simulate subsequent data sets from the model, and check whether they are "similar" to the original one. This is done by using: (i) a set of summary statistics $S$ (such as moments, quantiles, and etcetera), (ii) a distance between the summary statistics $\delta$, and (iii) a tolerance $\epsilon>0$. This, is, $Data_2$ is accepted as an approximate "replication" (in a loose sense) of $Data_1$ if $\delta(S(Data_1),S(Data_2)) https://en.wikipedia.org/wiki/Approximate_Bayesian_computation However, since you seem to be doing clustering, there must be some machine learning techniques to do it in a more "formal way" (not that machine learnists are formal), in which you classify the new observations according to the components of the previously fitted mixture.
