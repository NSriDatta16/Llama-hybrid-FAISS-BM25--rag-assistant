[site]: datascience
[post_id]: 84126
[parent_id]: 17710
[tags]: 
An empirical answer to that question woud be to look at public kaggle competitions / notebooks (see here ), where xgboost is heavily used as state of the art for tabular data problems . The answer is yes without a doubt. Notably in competitions, feature engineering is the main way to make a difference (followed maybe by parameter tuning) with everyone else. If everyone was dumping the same dataset in the same xgboost model they would have the same results. I think this can be restated as a Data Science "no free lunch theorem" . It is fairly easy to install R / Python with the associated XGBoost library. But, if it was that easy to deal with Data Science problems, any one would be able to do it and there would not be so much people training or working in Data Science. Genereally speaking that means Data Science has hard parts you need to deal with. Feature engineering is one of those hard parts of Data Science that has no universal solution.
