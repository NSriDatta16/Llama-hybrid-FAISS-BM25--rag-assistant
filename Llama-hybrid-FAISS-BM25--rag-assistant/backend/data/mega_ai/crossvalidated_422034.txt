[site]: crossvalidated
[post_id]: 422034
[parent_id]: 422005
[tags]: 
For starters, I want to make sure that I understand correctly that when you say "deriving," you mean "differentiating". What you are asking is called "differentiating under the integral sign" and is covered in basic calculus classes. If you still have your calculus textbook, you might want to do a quick review before going on in machine learning. I don't mean that condescendingly, just a suggestion. The method is called the Leibnitz Integration Rule . You can read about it here with examples, and an interesting anecdote from Richard Feynmann. Working through the steps: Notice that $EPE$ is a function of $\beta$ only because the dependence on $x$ and $y$ is being integrated out. $$ \begin{align} \frac{d EPE}{d \beta} & = \frac{d}{d \beta} \int (y - x^T\beta)^2Pr(dx,dy) \\ & = \int \frac{\partial}{\partial \beta} (y - x^T\beta)^2 Pr(dx,dy) ]] \\ & \text{using the chain rule...} \\ & = \int -2(y - x^T\beta)Pr(dx,dy) \\ & = -2 \int (y - x^T\beta)Pr(dx,dy) \end{align} $$ The partial derivative is needed inside the integral because we have not yet integrated out $x$ and $y$ , so the function we are differentiating is still a function of those variables.
