[site]: crossvalidated
[post_id]: 466863
[parent_id]: 466834
[tags]: 
In the context of PCA the variance is understood as a portion of variance in the original data retained in PCs. Suppose you have the data set with P input series $x_i$ . The total variance of the data set is sum of variances of all variables, i.e. $\Omega=\sum_{i=1}^P \operatorname {Var}[x_i]$ . After PCA you convert these variables into P factors $f_i$ . The sum of variances is retained, of course, $\sum_{i=1}^P \operatorname {Var}[f_i]=\Omega$ . However, when PCA works it will rearrange the variance in such a way that first few factors, say $K\ll P$ , will retain most of the total variance $$\frac{\sum_{i=1}^K\operatorname {Var}[f_i]}{\Omega}\sim 1$$ That's what's meant by maximizing variance in the context of PCA. I guess your confusion comes from the use of the word variance which has other meanings. For instance, large error variance would suggest that there's a lot of noise and amount of information is smaller relative to noise. In this context more variance would not be good for gathering information. In the context of PCA the more variance has a specific meaning reflecting the value of PCA for your data set in terms of retaining information in small number of factors.
