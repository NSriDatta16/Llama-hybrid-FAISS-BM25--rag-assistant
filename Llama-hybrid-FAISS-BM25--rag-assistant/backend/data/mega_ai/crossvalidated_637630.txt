[site]: crossvalidated
[post_id]: 637630
[parent_id]: 636896
[tags]: 
Your testing procedures look okay. The method you are using is Wald CI of binomial proportions. Some errors are The null hypothesis is "the control and experiment groups have the same probability of return." Hypotheses must be regarding population parameters (i.e. probability) instead of sample estimates (i.e. proportion). Do not associate statistical significance with hypotheses. Here I am confused what return means and whether the percentages have probability interpretation. Do they actually mean percentage increases instead, so an increase from an average score of 80 to 88 leads to 10% return? If so, testing binomial proportions may not be correct. The standard error of the difference should be $\text{SE} =\sqrt{p_0(1 - p_0)/n_0 + p_1(1 - p_1)/n_1}$ . The formula you used assumes equal variances between two groups, which is not appropriate in most cases. Using a two-sided test, the correct conclusion is that "the control and experiment groups have different probabilities of return" or "the return probability in the experiment group significantly differs from that in the control group." Again, we must talk about population parameters that are not observed but inferred. Should I use a one-tailed test? You can but do no have to. One-tailed tests correspond to one-sided CI (the other side is the theoretical limit, here +1 or -1 for a probability). Is there better tests or approaches to that? Is there any functions in Python or R that can be used to this kind of experiments? Yes, in R you should use DescTools::BinomDiffCI(x1, n1, x2, n2, conf.level = 0.95, sides = c("two.sided","left","right"), method = c("ac", "wald", "waldcc", "score", "scorecc", "mn", "mee", "blj", "ha", "hal", "jp")) . As it shows, there are multiple methods to construct the confidence interval. You can check out the details by sending ??BinomDiffCI in the console and check out the script for formula by Ctrl+click BinomDiffCI() in a line in your script editor. Since your sample size is quite large, they will differ by a tiny amount. If the task is hypothesis testing instead of confidence intervals, you can use (1) chi-square test of independence in chisq.test() (2) permutation test of independence with resampling coin::chisq_test() . Bootstrapped confidence intervals for proportions is possible but not very meaningful. See https://cran.r-project.org/web/packages/confintr/vignettes/confintr.html . In theory t.test() is not appropriate for binomial proportions although in your case it will still give very similar confidence intervals as DescTools::BinomDiffCI() does.
