ility distribution, which is the network's prediction of class probability distribution. This is the one used by the original ViT and Masked Autoencoder. Global average pooling (GAP) does not use the dummy token, but simply takes the average of all output tokens as the classification token. It was mentioned in the original ViT as being equally good. Multihead attention pooling (MAP) applies a multiheaded attention block to pooling. Specifically, it takes as input a list of vectors x 1 , x 2 , … , x n {\displaystyle x_{1},x_{2},\dots ,x_{n}} , which might be thought of as the output vectors of a layer of a ViT. It then applies a feedforward layer F F N {\displaystyle \mathrm {FFN} } on each vector, resulting in a matrix V = [ F F N ( v 1 ) , … , F F N ( v n ) ] {\displaystyle V=[\mathrm {FFN} (v_{1}),\dots ,\mathrm {FFN} (v_{n})]} . This is then sent to a multiheaded attention, resulting in M u l t i h e a d e d A t t e n t i o n ( Q , V , V ) {\displaystyle \mathrm {MultiheadedAttention} (Q,V,V)} , where Q {\displaystyle Q} is a matrix of trainable parameters. This was first proposed in the Set Transformer architecture. Later papers demonstrated that GAP and MAP both perform better than BERT-like pooling. Graph neural network pooling In graph neural networks (GNN), there are also two forms of pooling: global and local. Global pooling can be reduced to a local pooling where the receptive field is the entire output. Local pooling: a local pooling layer coarsens the graph via downsampling. Local pooling is used to increase the receptive field of a GNN, in a similar fashion to pooling layers in convolutional neural networks. Examples include k-nearest neighbours pooling, top-k pooling, and self-attention pooling. Global pooling: a global pooling layer, also known as readout layer, provides fixed-size representation of the whole graph. The global pooling layer must be permutation invariant, such that permutations in the ordering of graph nodes and edges do not alter the final output. Examples include element-wise sum, mean or maximum. Local pooling layers coarsen the graph via downsampling. We present here several learnable local pooling strategies that have been proposed. For each cases, the input is the initial graph is represented by a matrix X {\displaystyle \mathbf {X} } of node features, and the graph adjacency matrix A {\displaystyle \mathbf {A} } . The output is the new matrix X ′ {\displaystyle \mathbf {X} '} of node features, and the new graph adjacency matrix A ′ {\displaystyle \mathbf {A} '} . Top-k pooling We first set y = X p ‖ p ‖ {\displaystyle \mathbf {y} ={\frac {\mathbf {X} \mathbf {p} }{\Vert \mathbf {p} \Vert }}} where p {\displaystyle \mathbf {p} } is a learnable projection vector. The projection vector p {\displaystyle \mathbf {p} } computes a scalar projection value for each graph node. The top-k pooling layer can then be formalised as follows: X ′ = ( X ⊙ sigmoid ( y ) ) i {\displaystyle \mathbf {X} '=(\mathbf {X} \odot {\text{sigmoid}}(\mathbf {y} ))_{\mathbf {i} }} A ′ = A i , i {\displaystyle \mathbf {A} '=\mathbf {A} _{\mathbf {i} ,\mathbf {i} }} where i = top k ( y ) {\displaystyle \mathbf {i} ={\text{top}}_{k}(\mathbf {y} )} is the subset of nodes with the top-k highest projection scores, ⊙ {\displaystyle \odot } denotes element-wise matrix multiplication, and sigmoid ( ⋅ ) {\displaystyle {\text{sigmoid}}(\cdot )} is the sigmoid function. In other words, the nodes with the top-k highest projection scores are retained in the new adjacency matrix A ′ {\displaystyle \mathbf {A} '} . The sigmoid ( ⋅ ) {\displaystyle {\text{sigmoid}}(\cdot )} operation makes the projection vector p {\displaystyle \mathbf {p} } trainable by backpropagation, which otherwise would produce discrete outputs. Self-attention pooling We first set y = GNN ( X , A ) {\displaystyle \mathbf {y} ={\text{GNN}}(\mathbf {X} ,\mathbf {A} )} where GNN {\displaystyle {\text{GNN}}} is a generic permutation equivariant GNN layer (e.g., GCN, GA