[site]: crossvalidated
[post_id]: 591120
[parent_id]: 591106
[tags]: 
There is a history of answers to this kind of question following Horn's 1965 article on parallel analysis in principal component/s analysis (which has a confusing history of being called factor analysis, which Horn did). At any rate, in the 1960s Horn was interested in accounting for "sampling error and least-squares bias" in PCAs of the correlation matrix $\mathbf{R}$ . Horn's solution was to make a quantitative, not analytic, estimation through PCA on $k$ matrices of random data of size $N\times d$ , and to average the resulting $1^{\text{st}}$ , $2^{\text{nd}}$ , … $d^{\text{th}}$ eigenvalues (which he then subtracted from the eigenvalues of the PCA of the observed $\mathbf{R}$ ). This should also work for $\mathbf{\Sigma}$ . Because generating (pseudo) random numbers and because eigen-decomposition were both computationally expensive until about the mid 90s (and perhaps later, depending on the size of $N \times d$ ), there's a literature until about the turn of the millennium on estimating these average eigenvalues from $1$ to $d$ given $N$ and $d$ using computationally cheap formulas, while still using his parallel analysis approach to component retention decisions (Humphries and Montanelli, 1976; Allen and Hubbard, 1986; Lautenschlager, 1989; Longman, et al., 1989; Keeling, 2000). These methods were generally considered to be pretty lousy for the purposes of actual parallel analysis (e.g., see Zwick and Velicer, 1986; Hayton, et al., 2004), however, if, for your purposes, $\mathbf{\Sigma}$ is reasonably close to $\mathbf{R}$ , you could probably use one of them. There's also been a bit of a literature (Hayton, et al., 2004; Peres-eto, et al., 2005) about resampling/rerandomizing to generate the random data for parallel analysis, but this is unnecessary, since eigen-decomposition is not of the data per se , but of $\mathbf{R}$ , and is insensitive to the distributional form of the data (Dinno, 2009). References Allen, S. J., & Hubbard, R. (1986). Regression equations for the latent roots of random data correlation matrices with unities on the diagonal. Multivariate Behavioral Research , 21(3), 393–396. Dinno, A. (2009). Exploring the Sensitivity of Horn’s Parallel Analysis to the Distributional Form of Simulated Data . Multivariate Behavioral Research , 44(3), 362–388. Hayton, J. C., Allen, D. G., & Scarpello, V. (2004). Factor Retention Decisions in Exploratory Factor Analysis: A Tutorial on Parallel Analysis . Organizational Research Methods , 7(2), 191–205. Horn, J. L. (1965). A rationale and test for the number of factors in factor analysis. Psychometrika , 30(2), 179–185. Humphreys, L. G., & Montanelli, R. G. (1976). Latent roots of random data correlation matrices with squared multiple correlations on the diagonal: A Monte Carlo study. Psychometrika , 41(3), 341–348. Keeling, K. B. (2000). A Regression Equation for Determining the Dimensionality of Data. Multivariate Behavioral Research , 35(4), 457–468. Lautenschlager, G. J. (1989). A comparison of alternatives to conducting Monte Carlo analyses for determining parallel analysis criteria . Multivariate Behavioral Research, 24(3), 365–395. Longman, R. S., Cota, A. A., Holden, R. R., & Fekken, G. C. (1989). A regression equation for the parallel analysis criterion in principal components analysis: Mean and 95th percentile eigenvalues. Multivariate Behavioral Research , 24(1), 59–69. Montanelli, R. G., & Humphreys, L. G. (1976). Latent roots of random data correlation matrices with squared multiple correlations on the diagonal: A Monte Carlo study. Psychometrika, 41(3), 341–348. Peres-Neto, P. R., Jackson, D. A., & Somers, K. M. (2005). How many principal components? Stopping rules for determining the number of non-trivial axes revisited . Computational Statistics & Data Analysis , 49(4), 974–997. Zwick, W. R., & Velicer, W. F. (1986). A comparison of five rules for determining the number of factors to retain . Psychological Bulletin , 99(3), 432–442.
