[site]: datascience
[post_id]: 43592
[parent_id]: 
[tags]: 
RL - Weighthing negative rewards

Let's consider that I give an agent a reward of -1 (minimum reward) every time it performs an action which leads to the premature end of the episode (i.e., the agent dies). Besides, I also give a negative reward in the [-1, 0) interval when the agent performs an action that I want to avoid to repeat too much. However, these actions do not terminate an episode. Is it possible that the agent will learn to take the "-1" action given that it is possible to collect many negative rewards which in the end will sum less than -1? This is, the agent will prefer to commit suicide rather than experience many negative rewards. PD: the agent also receives positive rewards in the (0, 1] interval, which tend to be of higher magnitude than the negative rewards but the agent receives them less often when it is exploring.
