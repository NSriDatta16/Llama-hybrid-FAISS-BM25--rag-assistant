[site]: crossvalidated
[post_id]: 288762
[parent_id]: 2350
[tags]: 
I thought I would add an intuitive explanation for this pattern. In each decision tree comprising the random forest, the data are iteratively split along single dimensions. Crucially, this procedure involves 1) considering only a small, randomly-selected subset of all the explanatory variables, and 2) selecting the most strongly associated explanatory variable within this randomly-selected variable subset to split the data along. Therefore, the probability of the n most important variables being selected at any particular node decreases as the number of explanatory variables increases. Therefore, if one adds in a large number of variables that contribute little-to-no explanatory power, it automatically leads to an increase in the forest's error rate. And conversely, choosing only the most important variables for inclusion will very likely lead to a decrease in the error rate. Random forests are quite robust to this and it typically requires a very large addition of these 'noise' parameters to meaningfully reduce performance.
