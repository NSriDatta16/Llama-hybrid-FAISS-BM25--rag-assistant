[site]: crossvalidated
[post_id]: 384656
[parent_id]: 
[tags]: 
LSTM - random and always-different time between data measurements?

I am working with a time series problem where the time between two data measurements is random, and I am trying, without luck, to find an LSTM architecture that can handle this. A very simplified example of what this data may look like is below: index of | time of | time between | value of | value of data | measurement | measurements | input | output ----------------------------------------------------------- 0 | 0sec | | 5 | 0 1 | 12sec | 12sec | 3 | 1 2 | 13sec | 1sec | 2 | 1 3 | 16sec | 3sec | 4 | 0 4 | 25sec | 9sec | 10 | 1 I have found ideas that deal with inconsistently timed measurements, such as phase LSTMs, however those are not the same because because they deal with measurement that are different yet consistently timed. From what I understand they deal with a situation like "one sequence or type of data is all measured at a rate of X while another is all at a rate of Y". My situation is more along the lines of "the time between two measurements of data is determined by some unknown probability distribution". I have methods in-mind that can work-around this problem, however I was curious if there are any that directly handle/account for this? If so, do they generally work well on out-of-sample data? If relevant, I have theoretical and empirical reason to believe that there is some pattern, although still stochastic, to the time between measurements. This wikipedia link presents a similar idea (not the same because my problem does not have a strictly increasing pattern)
