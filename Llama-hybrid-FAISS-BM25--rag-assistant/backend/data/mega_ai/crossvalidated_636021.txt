[site]: crossvalidated
[post_id]: 636021
[parent_id]: 
[tags]: 
Poor fitted vs. actual values

I'm using a BART model (Bayesian additive regression tree) to predict the relative risk of an outcome (21,384 observations) controlling for 388 features and I'm getting a really poor actual vs. fitted value plot unlike any I've gotten before in other models with different outcomes. DV = logged relative risk + 1 of renter eviction rate at the census tract level. My first model (not shown) used the raw relative risk (risk is the tract eviction rate compared to all other tracts in the state) but I had a poor fit so this last round I transformed the DV into the log(RR+1) where +1 was added to all the RR values before logging to avoid infinity because of lots of zeros. IVs = All but one of the 388 variables is continuous. Variables are on the subjects of housing markets, demographics, built environment, local policies, and other variables related to eviction. Goal = fit a predicted value of relative risk in the rest of the country where there isn't data based on the controls in the respective tract. Process = I start with a baseline number of controls (388 being the max included), run a cross-validated BART model to get optimal hyper-parameters, then loop 20 BART models using those parameters each with a different seed to find the most important variables, reduce the variables after looking at several variable importance factors. I then repeat this process till I find a satisfactory model set. This last of several model iteration's R^2 is about .64 and the RMSE is about .28. Problem How can I improve this model? EDIT: clarify plot. These are credible interval points. Blue means it falls within the credible range while red falls outside. Here we have 45.06% coverage. The worst I've gotten before is 95% (plot looks mostly blue).
