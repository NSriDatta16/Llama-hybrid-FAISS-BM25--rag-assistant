[site]: crossvalidated
[post_id]: 345053
[parent_id]: 
[tags]: 
RNNs: backprop loss from just the last time step or every single one?

Consider a simple task that predict the next alphabets based on previous ones using RNNs. That is, during model inference, we would like the model to output y1_hat (hopefully 'b') given 'a', and y2_hat given ['a', y1_hat ], etc. in a recursive manner. In order to do this, I can think of two simple training settings which are different in data preparation and loss calculation (thus the gradient/update during backprop through time) preparing the data pair as x, y = ['a', 'b', 'c'], ['d'] ; preparing the data pair as x, y = ['a', 'b', 'c'], ['b', 'c', 'd'] . Note that I consider three time steps, for illustration. Please see the figure below for clarity; the top and bottom refer to the first and later setting, respectively, and they are the unrolled RNNs. We can substitute x1 , x2 and x3 in the figure with 'a', 'b' and 'c'; y1 , y2 and y3 with 'b', 'c' and 'd'. Black and red arrows represent forward and backward propagation, respectively. One of the essential differences between the two configurations would be the loss computation for each training sample: for the first, only the loss derived from the last time step is used for gradient and weight update; while in the second, the total loss equals to the sum of losses derived from all the three time steps. Regardless of such a difference, we can achieve the model inference scenario described earlier with both training configurations. However, I suggest there might be another differences in terms of, e.g., predicting performance; maybe the first setting would need longer input (longer than three in this example) in order to output meaningful prediction, while the second can more successfully capture both long- and short-term relation? Any thought is appreciated, thanks!
