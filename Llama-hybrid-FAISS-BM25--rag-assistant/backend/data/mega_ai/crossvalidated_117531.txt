[site]: crossvalidated
[post_id]: 117531
[parent_id]: 117527
[tags]: 
There are several mistakes with the statement you give. I think a better resource for anyone interested in Machine Learning these days would be Tibshirani Hastie "Elements of Statistical Learning." The correct statement of the CLT is that, given $X_1, X_2, \ldots, X_n \sim iid (\mu, \sigma^2)$, $$\sqrt{n} \left(\bar{X} - \mu \right) \rightarrow_d \mathcal{N}(0, \sigma^2) $$ (note that several of these "assumptions" can be relaxed, ref Lindeberg Feller CLT) A consequence is that the distribution of the sample mean can be approximated using a $\mathcal{N}(\bar{X}, \sigma^2/n)$ distribution. The cumulative sum $S_n$ is given by $n \bar{X}$ which would have an approximating distribution of $\mathcal{N}(n\mu, n\sigma^2)$ (recall $\mbox{Var}(aX) = a^2 \mbox{Var}(X)$). The last problem with your statement is that you confuse the probability density function for a normal curve with a cumulative DF. The $Pr(S_n The range in which you refer to the accuracy of the approximation is complete bunk. The CLT is certainly based on a Taylor series expansion... but making a rule of thumb about it is useless. More $n$ implies better approximating distribution, that's all we can say about it. From what I gather, Barry-Essen has nothing to do with any of this. All in all, please disregard the note.
