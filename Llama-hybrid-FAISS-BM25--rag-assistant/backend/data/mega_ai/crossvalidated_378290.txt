[site]: crossvalidated
[post_id]: 378290
[parent_id]: 
[tags]: 
Validation data considerably worse performance than stratified kfold cross validation

I've been working on a binary classifer for a NBA spread dataset and am running into an issue where the validation data has considerablly lower performance than the test/train cross validation. I am using the XGBoost Classifer on a total of 15266 games starting in the 06 season and running until last month. I know that the data sample size is pretty small but for this domain it is literally all of the data that I think is available which contains the spread/moneylines or at least all of the data I was able to find/scrape The cross validation results are better than I would have expected considering the difficulty of the task but the validation data(last 2500x data points) is considerably worse than the cross validation results. Below are the classification reports for each fold(Stratified Kfold): Fold #1 precision recall f1-score support 0 0.57 0.80 0.67 1320 1 0.63 0.36 0.46 1234 micro avg 0.59 0.59 0.59 2554 macro avg 0.60 0.58 0.57 2554 weighted avg 0.60 0.59 0.57 2554 Fold #2 precision recall f1-score support 0 0.58 0.81 0.67 1320 1 0.64 0.38 0.47 1233 micro avg 0.60 0.60 0.60 2553 macro avg 0.61 0.59 0.57 2553 weighted avg 0.61 0.60 0.58 2553 Fold #3 precision recall f1-score support 0 0.57 0.80 0.67 1320 1 0.63 0.36 0.46 1233 micro avg 0.59 0.59 0.59 2553 macro avg 0.60 0.58 0.57 2553 weighted avg 0.60 0.59 0.57 2553 Fold #4 precision recall f1-score support 0 0.58 0.81 0.68 1320 1 0.64 0.37 0.47 1233 micro avg 0.60 0.60 0.60 2553 macro avg 0.61 0.59 0.57 2553 weighted avg 0.61 0.60 0.58 2553 Fold #5 precision recall f1-score support 0 0.59 0.84 0.69 1320 1 0.68 0.37 0.48 1233 micro avg 0.61 0.61 0.61 2553 macro avg 0.64 0.61 0.59 2553 weighted avg 0.63 0.61 0.59 2553 Below is the xgboost scores for each fold and the mean scores for the test and validaton data kfold train scores mean 0.6256264979362992 kfold train scores [0.6171171171171171, 0.6441789875648684, 0.6263585626162734, 0.6257710760795065, 0.6147067463037306] kfold test score mean 0.5979952340069461 kfold test scores [0.5908379013312451, 0.5977281629455542, 0.5898942420681551, 0.5981198589894242, 0.6133960047003525] val scores mean 0.5119199999999999 val scores [0.5168, 0.504, 0.5156, 0.506, 0.5172] Here are the scores/classification reports for the model trained on the full dataset and the validation data train score 0.6145811573010236 test score 0.5902255639097744 validation score 0.5196 test report: precision recall f1-score support 0 0.57 0.82 0.67 1633 1 0.65 0.35 0.45 1559 micro avg 0.59 0.59 0.59 3192 macro avg 0.61 0.58 0.56 3192 weighted avg 0.61 0.59 0.57 3192 val report: precision recall f1-score support 0 0.52 0.99 0.68 1292 1 0.65 0.01 0.02 1208 micro avg 0.52 0.52 0.52 2500 macro avg 0.59 0.50 0.35 2500 weighted avg 0.58 0.52 0.36 2500 In the validation data classification report the recall/f1-score for the positive class is a pretty big red flag to me. What could be the possible issue/issues here with the model? Over fitting, Under fitting, etc? What would be some possible solutions?
