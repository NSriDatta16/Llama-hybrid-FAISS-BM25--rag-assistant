[site]: crossvalidated
[post_id]: 601527
[parent_id]: 601302
[tags]: 
Asymptotic inference (i.e., the variance of the estimator) for TMLE using influence functions requires the nuisance models--the models for the expected potential outcomes $E[Y|A,X]$ and propensity scores $E[A|X]$ --to converge to the truth (i.e., for the predicted values to converge to the true values) at a certain rate. Different models converge to the truth at desired rates under a variety of assumptions; for example, if the true propensity score model is a logistic model, maximum likelihood estimated (MLE) logistic regression converges quickly to the truth, but if the true propensity score model is a specific more complicated function, a gradient boosting machine (GBM) may converge at some rate, and the logistic regression won't converge at all. In practice, we don't know what the true model is, so we don't know if we are in a scenario where MLE logistic regression will converge at the required rate or if GBM will. SuperLearner takes on the fastest convergence rate of its candidate models, which means that to have the best chance of approaching the required converge rate, SuperLearner asymptotically does as well or better than any of its candidate models when the true model is unknown. Using the example above, if the true propensity score model is logistic and MLE logistic regression and GBM are used as candidate libraries for SuperLearner, then the resulting SuperLearner predictions converge to the truth at the same rate as the MLE logistic regression alone would. But if the GBM converges to the truth at a certain rate and the MLE logistic model is wrong, then that same SuperLearner as above will converge at the same rate as the GBM, even with the logistic regression included in the library. So, there is a kind of "multiple robustness" in the SuperLearner in that if any of the candidate models converge to the truth at the required rate, the SuperLearner containing those models does, too. We know that a machine learning method called "highly adaptive lasso" (HAL) alone converges to the truth at the required rate; so technically, it is the only learner required for asymptotic inference with TMLE to be valid. But if the true propensity score or outcome model is a simple model captured well by MLE logistic regression, we would want predictions from such a model to help steer the predictions to the truth at an even faster rate than HAL would do alone in order to improve precision and arrive at approximately valid inference with a smaller sample size. Including both MLE logistic regression and HAL (and other models) would improve the performance of the resulting predictions while guaranteeing the asymptotic consistency properties imparted by HAL. Also note that the models are cross-validated and, ideally, cross-fit, which reduces the problems of overfitting that you mention. I believe the convergence rates for a given model refer to their cross-validated rates. I know others may be able to provide more technical explanations for this, but this is my understanding at an intuitive level of why SuperLearner is so important to TMLE (and AIPW and all other doubly-robust methods).
