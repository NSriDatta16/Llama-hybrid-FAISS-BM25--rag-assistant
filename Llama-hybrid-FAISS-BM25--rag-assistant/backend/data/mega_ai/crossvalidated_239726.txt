[site]: crossvalidated
[post_id]: 239726
[parent_id]: 239704
[tags]: 
Lets express your problem statement as the following relationship: Healthy ~ carbohydrates + fiber + fat + protein Since you've decided this should be posed as a classification problem, our first objective is to find out the decision boundary between healthy=1 and healthy=0. To do this, you could use a variety of popular machine learning algorithms such as Random Forest, Logistic Regression, Linear regression, Boosted trees, etc. Once you have built the best model using the data you have, you can then proceed to use it to identify which of the independent variables it considers most important. Algorithm implementations such as Random Forest and Gradient Boosting Machines generate this "importance" listing as part of the model training process and you can easily extract these. But you may find out the the order of importance varies depending upon which algorithm you used to build the model. This is because each algorithm determines the classification boundary in a different way. The random forest algorithm fits multiple trees, each tree in the forest is built by randomly selecting different features from the dataset. The nodes of each tree are built up by choosing and splitting to achieve maximum variance reduction. While predicting on the test dataset, the individual trees output is averaged to obtain the final output. Each variable is permuted among all trees and the difference in out-of-sample error of before and after permutation is calculated. The variables with highest difference are considered most important, and ones with lower values are less important. A gradient boosted machine calculates a gain - this implies the relative contribution of the corresponding variable to the model calculated by taking each variable's contribution for each tree in the model. A higher value of this metric when compared to another variable implies it is more important for generating a prediction. Variable importance is very different for a linear regression model as compared to a random forest or a GBM model. A popular approach for linear regression models is to decompose $R^2$ into contributions attributed to each variable. But variable importance is not straightforward in linear regression since variables might be correlated. Refer to the document describing the PMD method (Feldman, 2005) in the references below. Another popular approach is averaging over orderings (LMG, 1980). The LMG works like this: Find the semi-partial correlation of each predictor in the model, e.g. for variable a we have: $SS_a/SS_{total}$. It implies how much would $R^2$ increase if variable $a$ were added to the model. Calculate this value for each variable for each order in which the variable gets introduced into the model, i.e. {$a,b,c$} ; {$b,a,c$} ; {$b,c,a$} Find the average of the semi-partial correlations for each of these orders. This is the average over orderings. There isn't much consensus on the best method to determine relative importance of variables in logistic regression models. But for practical purposes (applying the same analogy as the linear regression model's $R^2$) , you could use adjusted $R^2$ to determine how much would $R^2$ increase if each particular variable were added to the model. References: https://en.wikipedia.org/wiki/Decision_tree_learning https://en.wikipedia.org/wiki/Random_forest Relative importance of Linear Regressors in R Relative Importance and Value, Barry Feldman (PMD method)
