[site]: crossvalidated
[post_id]: 558645
[parent_id]: 
[tags]: 
SVM function with kernel trick

The linear SVM function can be expressed as: $$h(x)=w^{T}x+w_{0}$$ where $x=({{x}}_{1},\dots,{{x}}_{N})^{T}$ is one observation and $w=({{w}}_{1},\dots,{{w}}_{N})^{T}$ are the coefficients. We can do the calculation of the dot product in the following way: $$w^{T}x=x_1w_1+\dots+x_Nw_N$$ Now with the kernel trick, the function $h$ is expressed as: $$h(x)=w^{T}\phi (x)+w_{0}$$ And my question is : for a given observation $x$ , how the calculation is done ? since, $\phi$ is always expressed as: $$K(x_{i},x_{j})=\phi (x_{i})^{T}\cdot \phi (x_{j})$$ With some example of $K$ Polynomial kernel : $K(x_{i},x_{j})=(x_{i}^{T}\cdot x_{j}+1)^{d} $ Gaussian kernel: $ {\displaystyle K(\mathbf {x} ,\mathbf {y} )=\exp \left(-{\frac {\|\mathbf {x} -\mathbf {y} \|^{2}}{2\sigma ^{2}}}\right)}$ PS: with the dual form, we can express $h$ as: $$h(x)=\sum _{{k=1}}^{p}\alpha _{k}^{*}l_{k}K(x_{k},x)+w_{0}$$ But I try to understand how to write the function $\phi$ in an explicit way.
