[site]: crossvalidated
[post_id]: 348579
[parent_id]: 348574
[tags]: 
Though SO seems to be a better fit for this question, there is a theoretical knowledge of the algorithm to keep in mind. On Wikipedia there is a specific paragraph related to large samples. Just like (kernel) SVM, you need to compute the whole matrix $K(x_i, x_j)$. Where $x_i$'s are your sample points. You have 170 000 of them, so 170 000 ^ 2 terms to compute (and store) in the matrix $K$. Even with enough memory, I doubt the calculation would end. An approach could be (from wikipedia) : One way to deal with this is to perform clustering on the dataset, and populate the kernel with the means of those clusters. Since even this method may yield a relatively large K, it is common to compute only the top P eigenvalues and eigenvectors of K. Or to look for streaming implementations of KPCA.
