[site]: crossvalidated
[post_id]: 578457
[parent_id]: 578425
[tags]: 
[Context] @Henry and @JonathanLew firmly pointed out errors in my original answer, which argued that the statement "the exact value of any likelihood is meaningless" is glib and that you can't prove a logical claim about all likelihoods by providing specific examples where it's safe to compute the likelihood up to a constant (which admittedly is often the case). Since I first posted my answer I've learned that continuous likelihoods have (theoretical) units given by 1/(units of the data) from @apdnu's answer to Units for likelihoods and probabilities . I've come across examples where likelihood function should be be computed exactly to get the correct answer. These examples teach me that I should be careful with my likelihood calculations and don't presume that I can safely ignore normalizing constants. And I've discovered that (a version of) this question has been asked and answered before: What does "likelihood is only defined up to a multiplicative constant of proportionality" mean in practice? Example #1: Comparing a model with normal errors to a model with Cauchy errors This example is from Chapter 6, Y. Pawitan In All Likelihood: Statistical Modelling And Inference Using Likelihood (2013). We want to model Y in terms of X; there are a few unusual values in the data (outliers). We propose two models with the same mean structure E(Y) = β 0 + β 1 X but different error structure: in one model the errors are iid Normal(0, σ 2 ), in the other model the errors are iid Cauchy(0, γ). We fit the models by maximizing the likelihoods and next we use AIC = -2 $\log$ L + 2k (where k is the number of model parameters) to choose the "better" model. Both the Normal density and the Cauchy density have constant terms that are usually save to ignore: (2) -1/2 for the Normal and -1 for the Cauchy. These are not the same constant for both models, so no parts of the likelihood functions can be dropped. Example #2: Mixture of Bernoullis for latent class analysis This example is from Chapter 9 of C. M. Bishop. Pattern Recognition and Machine Learning (2006). We want to model a dataset of binary observations as a mixture of $K$ Bernoulli components with parameters $\{\mu_k\}$ and mixing proportions $\pi_k$ . The log likelihood is: $$ \ln p(\mathbf{X}|\mathbf{\mu},\mathbf{\pi}) = \sum_{n=1}^N\ln\left\{\sum_{k=1}^K\pi_kp(\mathbf{x}_n|\mathbb{\mu}_k)\right\} $$ Since there is a summation inside a logarithm, the math doesn't simplify but the maximum likelihood solution can be found with the EM algorithm. Example #3: Bayesian $t$ -test This example is from Chapter 4 of K. P. Murphy. Machine Learning: A Probabilistic Perspective (2012). We want to test the hypothesis $\mu > \mu_0$ for some known value of $\mu_0$ . The p-value for an one-side t-test is an integral over the likelihood: $$ \begin{aligned} p(\mu>\mu_0|\text{data}) = \int_{\mu_0}^\infty p(\mu|\text{data})d\mu \end{aligned} $$ We can't omit any terms inside the integral or we won't compute the p-value correctly. In summary, there are both theory and examples to illustrate that the exact value of the likelihood function can be meaningful. [Original answer, with corrections following comments] The statement "the exact value of any likelihood is meaningless" is abstract and imprecise at the same time. So let's start with the definition of likelihood. In the spirit of this question, the definition isn't mathematically rigorous. We take a probabilistic model f(x,θ) for data x with parameter θ . As a function of the data x , f(x,θ) is a probability density/mass function. [pdf if x is continuous; pmf if x is discrete.] As a function of the parameter θ , f(x,θ) is the likelihood. It's true that the likelihood doesn't integrate to 1. Many functions don't, yet we don't conclude that their exact value is meaningless. ∫ x f(x,θ) dx = 1 [replace the integral with a summation if x is discrete] ∫ θ f(x,θ) dθ = constant that depends on the model f and the data x A common theme running through the answers is that likelihood computations often simplify. The logical argument seems to go something like this: in many computations a term in the likelihood is constant or behaves like a constant so we can simplify the math by dropping that term; ergo the exact value of a likelihood function is meaningless. However, the likelihood has more uses than maximizing it to find the MLE or performing a likelihood ratio test. And a likelihood term that can be ignored in one computation is important to keep track of in another.
