[site]: crossvalidated
[post_id]: 186984
[parent_id]: 167448
[tags]: 
The model in the deeplearning.net tutorial learns an embedding of the words, which maps from an integer word index (your X above) to a scalar vector for each word. These embedded words are the input to the LSTM. In lstm.py , take a look at the build_model() function, which has the following line: emb = tparams['Wemb'][x.flatten()].reshape([n_timesteps, n_samples, options['dim_proj']]) Here the model parameter Wemb is the embedding, and the variable x contains the integer word indexes from your data. The result is the scalar vectors emb , one vector of dim_proj scalars for each word. Hopefully it is clear what is going on with flatten() and reshape() . The word embedding approach is explained in Recurrent Neural Networks with Word Embeddings .
