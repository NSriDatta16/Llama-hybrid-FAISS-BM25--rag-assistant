[site]: datascience
[post_id]: 16295
[parent_id]: 
[tags]: 
Normal equation result simplification

The derivation of the normal equation can be noted $\theta = (X^TX)^{-1}(X^T)y$, where $X^{-1}$ is the inverse of $X$ and can also be written $inv(X)$. But why can't we write $inv(X^T X)$ as $inv(X)inv(X^T)$, and also use the definition of inverse $inv(X^T)(X^T) = I$? Then $\theta = inv(X^T X)(X^T)y = inv(X)inv(X^T)(X^T)y = inv(X)Iy = inv(X)y$ Why isn't the final result of normal equation in regression in machine learning written as $\theta = inv(X) y$ after simplifying?
