[site]: datascience
[post_id]: 75936
[parent_id]: 75001
[tags]: 
Since no one answered my question, I'll describe what I found here for others. Similar to what I described above in the question, you go over pre-processing, then look for the terms in the ontology and replace them with the whole branches from the ontology. After that, you train your model. Notes: Since you have the ontology with all the relevant terms, you can leave the stopwords in while pre-processing. Stopword removal can be performed via lists or using sklearn or Spacy. In addtion, Spacy performs lemmatization, which can be done via ontologies too (more accurate, but probably slower). Use SPARQL for working with ontologies and pattern matching. Spacy does patter matching too; also, Lucene and several other tools. However, I haven't used them with ontologies. Similarity algorithms can also be used. I found Random Forests to work best among traditional supervised methods. However, probably using neural networks might produce better results. Please keep in mind you can use alternative methods for text classification without ontologies, such as word embedding and topic models. Spacy seems to be a good tool for text classification too. It has nice documentation.
