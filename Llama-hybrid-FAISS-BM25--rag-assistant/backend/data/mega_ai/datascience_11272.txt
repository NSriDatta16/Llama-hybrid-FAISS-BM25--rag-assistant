[site]: datascience
[post_id]: 11272
[parent_id]: 
[tags]: 
Is boosting resistant to overfitting for both number of iterations and number of features?

Boosting methods (such as the popular xgboost ) do not tend to overfit when we use many iterations - Schapire and Freund . Are they also resistant to overfitting when we feed them with a large number of features (where some of the features are not very useful?) If so, is there a theoretic connection between these two desirable properties? (related to https://stats.stackexchange.com/questions/35276/svm-overfitting-curse-of-dimensionality )
