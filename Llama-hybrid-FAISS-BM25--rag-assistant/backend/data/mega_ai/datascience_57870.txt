[site]: datascience
[post_id]: 57870
[parent_id]: 57861
[tags]: 
Welcome to the site and thanks for the great question! I recently led an NLP project that dealt with a lot of laws. While I have to obfuscate my actual work, here's a general view: The laws themselves may not be the best source data. It would take a massively transformed recordset in order to make most laws actionable for modeling. I'm talking about big rooms, full of lawyers providing an annotated version of laws in order to create a recordset that can actually be useful The above assumes that the laws have been digitized in some easy to digest format. That may not always be the case. In a lot of instances, you are referring back to classic OCR approaches as part of your data prep and I don't know anyone that likes working with OCR :-) The human-in-the-loop requirements are very high. So you have an algorithm, now what? That's not something you can just put out on Mechanical Turk for the layman to verify. You need more lawyers to help with the verification of your approach and correct mistakes that are happening Finally, you must get very sophisticated with your embedding layers in how you create and apply them. That's not an easy thing to do and very processor intensive - a GPU is highly recommended and not a lot of grassroot efforts are going to have this processing power Good luck!
