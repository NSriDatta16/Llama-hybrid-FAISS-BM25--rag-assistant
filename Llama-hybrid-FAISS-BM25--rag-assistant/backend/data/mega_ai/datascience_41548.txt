[site]: datascience
[post_id]: 41548
[parent_id]: 34209
[tags]: 
In xgboost 0.7.post3 : XGBRegressor.feature_importances_ returns weights that sum up to one. XGBRegressor.get_booster().get_score(importance_type='weight') returns occurrences of the features in splits. If you divide these occurrences by their sum, you'll get Item 1. Except here, features with 0 importance will be excluded. xgboost.plot_importance(XGBRegressor.get_booster()) plots the values of Item 2: the number of occurrences in splits. XGBRegressor.get_booster().get_fscore() is the same as XGBRegressor.get_booster().get_score(importance_type='weight') Method get_score returns other importance scores as well. Check the argument importance_type . In xgboost 0.81 , XGBRegressor.feature_importances_ now returns gains by default, i.e., the equivalent of get_score(importance_type='gain') . See importance_type in XGBRegressor . So, for importance scores, better stick to the function get_score with an explicit importance_type parameter. Also, check this question for the interpretation of the importance_type parameter: "weight", "gain", and "cover".
