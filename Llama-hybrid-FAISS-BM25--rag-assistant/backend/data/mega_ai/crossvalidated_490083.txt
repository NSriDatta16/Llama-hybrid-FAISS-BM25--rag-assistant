[site]: crossvalidated
[post_id]: 490083
[parent_id]: 490072
[tags]: 
That's a great question - I'm not sure what the best way to answer this, but in a statistical framework, I believe the differences are a bit more clearly cut. I'll be curious to see how others answer this from a purer ML/DL perspective. I think one way in which they differ is that parameters (at last from a statistical standpoint) are something on which you can make inference on, whereas a hyper-parameter is an element of the algorithm that is tuned to optimize it. For a concrete example, say you are running a LASSO-type penalty for a linear regression model. The $\beta$ weights/coefficients are parameters as one can make a decision on the estimated values and determine relevance or directionality (i.e., check which coefficients are not 0 in a LASSO procedure, or which "protect agaisnt" vs. "increase" risk). Using the same LASSO example, the $\alpha$ weight on a penalty function can be considered a hyper parameter, since the actual value of the $\alpha$ would not provide any insight into the model/post-hoc analysis. This is a bit of a "statistical" perspective of a difference b/w what is a parameter vs. a hyper-parameter, though that's one option with how to differentiate. With non-parametric algorithms, decision trees, and neural networks, this is where I think there are more gray areas.
