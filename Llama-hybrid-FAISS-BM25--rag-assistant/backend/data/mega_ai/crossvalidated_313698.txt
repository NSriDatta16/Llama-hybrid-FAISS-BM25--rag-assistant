[site]: crossvalidated
[post_id]: 313698
[parent_id]: 
[tags]: 
How is the dropout method different than Random Forests?

I've come across something called a dropout method that involves setting a threshold parameter $p$ and then for each predictor in your training set, generate a uniform random number. If that uniform random number is less than p, drop it. Basically, for each predictor in the training sample, set the predictor values to 0 with probability $p$. This for $B$ trials to create $B$ separate training sets. Fit decision tree models $\hat{h}^1(x), \ldots, \hat{h}^B(x) \in \{0,1\}$ to the $B$ training sets. Combine the decision tree models into a single classifier by taking a majority vote: $$ \hat{H}_{maj}(x) \,=\, majority\Big(\hat{h}^1(x), \ldots, \hat{h}^B(x)\Big). $$ I am wondering how this is different than Random Forests. Are they the same? Thanks.
