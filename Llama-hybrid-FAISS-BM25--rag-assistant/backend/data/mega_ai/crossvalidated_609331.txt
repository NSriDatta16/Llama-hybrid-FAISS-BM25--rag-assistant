[site]: crossvalidated
[post_id]: 609331
[parent_id]: 599113
[tags]: 
Intuitively, having more data will tell the neural network where to turn, by how much, and in what direction (up/down, left/right, combinations, extensions in high-dimension spaces, etc). Imagine your true function to be a parabola. However, you only have two data points. You have no way to capture the curvature. You cannot figure out if the parabola opens up or down. You cannot figure out how wide the parabola is. When you add a third point, you can start to figure out some of this. However, that assumes you know the shape to be a parabola. If you do not know the function, how can you distinguish that from something like an absolute value function that uses straight lines? By having more data, you provide more opportunities to penalize the network for turning incorrectly, even if the fit on fewer points is perfect. This sounds like resolution, and I suspect that there is a way to tie this notion to the Nyquist rate and Nyquistâ€“Shannon sampling theorem in signal processing (or some generalization to higher-dimension spaces) to bring full mathematical rigor.
