[site]: crossvalidated
[post_id]: 553114
[parent_id]: 553059
[tags]: 
One interpretation is that regularization minimizes the "description length" of your model + data (the number of bits you'd need to encode your model + data). Sparsity is one way to do this -- obviously you need fewer bits if you have fewer parameters, but other types of regularization also accomplish this via the "bits back" argument. But why prefer shorter descriptions? It turns out, following this idea, you can arrive at an inference procedure which is "optimal" in some very general senses. For example, if the world was deterministic, you'd only ever make a finite amount of errors in any prediction task. If the world was random, you'd only make about as many errors than if you knew its "true nature". You might find Shane Legg's primer on Solomonoff Induction helpful for a more rigorous description of this. The fact that less regularization is sometimes better still agrees with the minimum description length principle: we're interested in minimizing the length of data and model, not just the model. Models like GPT-3 have very long descriptions, but they are powerful enough to reduce the length of the data by enough to compensate for this, and achieve an overall shorter description.
