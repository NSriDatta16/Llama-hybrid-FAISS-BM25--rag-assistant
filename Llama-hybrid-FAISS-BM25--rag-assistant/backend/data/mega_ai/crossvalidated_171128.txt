[site]: crossvalidated
[post_id]: 171128
[parent_id]: 171093
[tags]: 
Exactly this question is answered by Schapire and Freund, et. al. in their publication Boosting the Margin: A New Explanation for the Effectiveness of Voting Methods Robert E. Schapire, Yoav Freund, Peter Bartlett and Wee Sun Lee The Annals of Statistics Vol. 26, No. 5 (Oct., 1998), pp. 1651-1686 The takeaway from all their math is, that even when the training error is zero, the margin (= sample distance to decision boundary) is still improved by further boosting iterations. Also, among other things they show that there are lower upper bounds on the generalization error for a classifier with larger margin, which at least supports what you are seeing.
