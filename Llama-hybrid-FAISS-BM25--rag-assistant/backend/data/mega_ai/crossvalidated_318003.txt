[site]: crossvalidated
[post_id]: 318003
[parent_id]: 
[tags]: 
Binary Sparse Coding

In this binary sparse coding paper referenced in the Goodfellow/Bengio/Courville deep learning book ( https://fias.uni-frankfurt.de/~bornschein/papers/HennigesEtAl_lva2010.pdf ), the parameter $\pi=p(s_i = 1)$ is learned and not a hyperparameter. As far as I can tell, there is no prior imposed on the value of $\pi$. My question is: why should we expect the learned codes to be sparse? Couldn't $\pi$ be quite large? I suppose if $\pi$ is close to 1.0, then we can consider the "opposite" code as being sparse. But I don't see why $\pi$ shouldn't want to be 0.50. If there is no reason to expect that $\pi$ is far from 0.50, I don't understand why the scheme is called Binary Sparse Coding or why the codes values are mostly 0s or mostly 1s. Thank you humans of CrossValidated for your help!
