[site]: crossvalidated
[post_id]: 531163
[parent_id]: 
[tags]: 
Inverse Neural Networks

Suppose there is a series of transformation applied to the random variable $z_0$ such that $$ z_M = f_{\theta_{M}} \circ f_{\theta_{M-1}} \circ \ldots \circ f_{\theta_{1}}(z_0) =: f_{\theta}(z_0). $$ Additionally, these transformations $f_{\theta_k}, \ k = 1,\ldots,M$ , are all diffeomorphisms, meaning they are invertible and both $f$ and $f^{-1}$ are continuous. Suppose this network of finite transformations is trained such that the KL divergence between $p_M(z_M;\theta)$ and some desired distribution $p_{M}^*(z_M)$ is minimized, i.e., $$ \theta^* = \textrm{argmin}_{\theta} \ L(\theta) := \mathbb{D}_{\textrm{KL}}\left[p_M(z_M;\theta) \ || \ p_M^*(z_M)\right]. $$ Given these optimal parameters $\theta^*$ , and since it is guaranteed that $f^{-1}$ exists, I am interested in computing the inverse function at some sampled point $z_M$ . Since this is a composition of $M$ different transformations, there is no analytical way to compute the inverse, but I was wondering if there may be numerical techniques. Thank you!
