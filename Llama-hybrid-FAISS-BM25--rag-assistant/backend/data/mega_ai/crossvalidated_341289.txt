[site]: crossvalidated
[post_id]: 341289
[parent_id]: 
[tags]: 
Applying timer series analysis to machine learning data

First, I am a computer scientist and only know the very basic of statistics. I am visualizing the learning behaviour of different machine learning architectures (VGG, ResNet, etc.) by using the CIFAR-10 dataset. I do that by training the models for multiple epochs and print the misclassification rate after every 5th epoch in a line plot. For example, the misclassification rate of cat images which were falsely predicted as dog images over 195 epochs for a VGG and a ResNet model looks like this: The blue line is VGG and the orange is ResNet. As you can see for example, the confusion rate for ResNet is often higher (e.g. epoch 58 and epoch 120) compared to the VGG model as ResNet mispredicts more cat images as dog images. Another example is the precision of both models: The ResNet seems to start (epoch 0) with a lower misclassification rate compared to VGG. So my actual question: I want to analyze these learning statistics more deeply. I want to find out if there are any patterns or characteristics in the line graphs. I have been reading about "time series analysis" in the last couple of days. I think this could be a good idea but I am still not sure if it made sense to apply models like autoregressive models, ARMA, ARIMA to this kind of data. Can you give me advice please?
