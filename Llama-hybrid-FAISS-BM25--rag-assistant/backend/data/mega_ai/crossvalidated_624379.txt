[site]: crossvalidated
[post_id]: 624379
[parent_id]: 624360
[tags]: 
I would think twice before approaching this problem without more data. In your comment to Dave, you mention Since random forest generates the feature importance metric, we can look at which features are important. Karandeep Singh has a very insightful twitter thread demonstrating that feature importance scores can sometimes not yield the most important feature . For this reason, I doubt this method of feature selection (or any other for that matter, but we'll get there) will work. It sounds like your feature selection procedure will inform some downstream decision. To me, that would mean you want to do stable feature selection (meaning that you would find the right variables each time, regardless of the sample obtained). I have a very simple simulation showing that even when all assumptions about the data generating process are met , feature selection can't reliably find the right variables. Mind you this is with stepwise regression, but I don't have much confidence in other methods. Can you elaborate on what you intend to use the selected variables for? What is this experiment you're planning? What have you captured data on and what are your goals? From your comment, it sounds like you'd like to: Identify the genes that are associated with disease, and Use those genes to predict future disease status. I have no doubt you can create a model with the data you have. Personally, I think performing stable feature selection is a fools errand here. You have 60, 000 genes and only 1000 samples, there are many such combinations of features which could lead to reasonable prediction accuracy, even if no relationship truly exists. I think it would be a better idea to use all data and reduce the dimension of the problem using something like PCA. If there is any signal, you could be able to use the PCs to predict outcomes which would fulfill your second goal.
