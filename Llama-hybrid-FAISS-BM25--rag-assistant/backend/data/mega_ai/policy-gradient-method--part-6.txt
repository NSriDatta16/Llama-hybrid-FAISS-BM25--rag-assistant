}{x^{T}Fx}}}x,\;\theta _{i}+\alpha {\sqrt {\frac {2\epsilon }{x^{T}Fx}}}x,\;\theta _{i}+\alpha ^{2}{\sqrt {\frac {2\epsilon }{x^{T}Fx}}}x,\;\dots } until it finds one that both satisfies the KL constraint D ¯ K L ( π θ i + 1 ‖ π θ i ) ≤ ϵ {\displaystyle {\bar {D}}_{KL}(\pi _{\theta _{i+1}}\|\pi _{\theta _{i}})\leq \epsilon } and results in a higher L ( θ i + 1 , θ i ) ≥ L ( θ i , θ i ) {\displaystyle L(\theta _{i+1},\theta _{i})\geq L(\theta _{i},\theta _{i})} . Here, α ∈ ( 0 , 1 ) {\displaystyle \alpha \in (0,1)} is the backtracking coefficient. Proximal Policy Optimization (PPO) A further improvement is proximal policy optimization (PPO), which avoids even computing F ( θ ) {\displaystyle F(\theta )} and F ( θ ) − 1 {\displaystyle F(\theta )^{-1}} via a first-order approximation using clipped probability ratios. Specifically, instead of maximizing the surrogate advantage max θ L ( θ , θ t ) = E s , a ∼ π θ t [ π θ ( a | s ) π θ t ( a | s ) A π θ t ( s , a ) ] {\displaystyle \max _{\theta }L(\theta ,\theta _{t})=\mathbb {E} _{s,a\sim \pi _{\theta _{t}}}\left[{\frac {\pi _{\theta }(a|s)}{\pi _{\theta _{t}}(a|s)}}A^{\pi _{\theta _{t}}}(s,a)\right]} under a KL divergence constraint, it directly inserts the constraint into the surrogate advantage: max θ E s , a ∼ π θ t [ { min ( π θ ( a | s ) π θ t ( a | s ) , 1 + ϵ ) A π θ t ( s , a ) if A π θ t ( s , a ) > 0 max ( π θ ( a | s ) π θ t ( a | s ) , 1 − ϵ ) A π θ t ( s , a ) if A π θ t ( s , a ) < 0 ] {\displaystyle \max _{\theta }\mathbb {E} _{s,a\sim \pi _{\theta _{t}}}\left[{\begin{cases}\min \left({\frac {\pi _{\theta }(a|s)}{\pi _{\theta _{t}}(a|s)}},1+\epsilon \right)A^{\pi _{\theta _{t}}}(s,a)&{\text{ if }}A^{\pi _{\theta _{t}}}(s,a)>0\\\max \left({\frac {\pi _{\theta }(a|s)}{\pi _{\theta _{t}}(a|s)}},1-\epsilon \right)A^{\pi _{\theta _{t}}}(s,a)&{\text{ if }}A^{\pi _{\theta _{t}}}(s,a)<0\end{cases}}\right]} and PPO maximizes the surrogate advantage by stochastic gradient descent, as usual. In words, gradient-ascending the new surrogate advantage function means that, at some state s , a {\displaystyle s,a} , if the advantage is positive: A π θ t ( s , a ) > 0 {\displaystyle A^{\pi _{\theta _{t}}}(s,a)>0} , then the gradient should direct θ {\displaystyle \theta } towards the direction that increases the probability of performing action a {\displaystyle a} under the state s {\displaystyle s} . However, as soon as θ {\displaystyle \theta } has changed so much that π θ ( a | s ) ≥ ( 1 + ϵ ) π θ t ( a | s ) {\displaystyle \pi _{\theta }(a|s)\geq (1+\epsilon )\pi _{\theta _{t}}(a|s)} , then the gradient should stop pointing it in that direction. And similarly if A π θ t ( s , a ) < 0 {\displaystyle A^{\pi _{\theta _{t}}}(s,a)<0} . Thus, PPO avoids pushing the parameter update too hard, and avoids changing the policy too much. To be more precise, to update θ t {\displaystyle \theta _{t}} to θ t + 1 {\displaystyle \theta _{t+1}} requires multiple update steps on the same batch of data. It would initialize θ = θ t {\displaystyle \theta =\theta _{t}} , then repeatedly apply gradient descent (such as the Adam optimizer) to update θ {\displaystyle \theta } until the surrogate advantage has stabilized. It would then assign θ t + 1 {\displaystyle \theta _{t+1}} to θ {\displaystyle \theta } , and do it again. During this inner-loop, the first update to θ {\displaystyle \theta } would not hit the 1 − ϵ , 1 + ϵ {\displaystyle 1-\epsilon ,1+\epsilon } bounds, but as θ {\displaystyle \theta } is updated further and further away from θ t {\displaystyle \theta _{t}} , it eventually starts hitting the bounds. For each such bound hit, the corresponding gradient becomes zero, and thus PPO avoid updating θ {\displaystyle \theta } too far away from θ t {\displaystyle \theta _{t}} . This is important, because the surrogate loss assumes that the state-action pair s , a {\displaystyle s,a} is sampled from what the agent would see if the agent runs the policy π θ t {\displaystyle \pi _{\theta _{t}}} , b