[site]: datascience
[post_id]: 93651
[parent_id]: 
[tags]: 
Reason for adding 1 to word index for sequence modeling

I notice in many of the tutorials 1 is added to the word_index . For example considering a sample code snippet inspired from Tensorflow's tutorial for NMT https://www.tensorflow.org/tutorials/text/nmt_with_attention : import tensorflow as tf sample_input = ["sample sentence 1", "sample sentence 2"] lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='') lang_tokenizer.fit_on_texts(sample_input) vocab_inp_size = len(lang_tokenizer.word_index)+1 I dont understand the reason for adding 1 to the word_index dictionary . Wont adding a random 1 affect the prediction. Any suggestions will be helpful
