[site]: datascience
[post_id]: 112189
[parent_id]: 
[tags]: 
How does compute required scale with number of model parameters?

GPT-3 has 175 billion parameters, required ~ $3.114 * 10^{23}$ FLOPS, and took approximately one month to train on ~10k Tesla V100 GPUs. It seems commonly stated that the brain has the equivalent of ~100 trillion parameters. I was wondering what kind of compute would be required for training a transformer of this size. Would it simply be ~ $10^3$ times more FLOPs? In general, how does compute required scale with respect to model parameters for transformers, neural networks, CNNs, and other popular deep learning models?
