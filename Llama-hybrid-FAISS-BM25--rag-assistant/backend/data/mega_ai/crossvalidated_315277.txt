[site]: crossvalidated
[post_id]: 315277
[parent_id]: 315262
[tags]: 
Here is the relevant excerpt from my book Bayesian Choice (2001, Chapter 2): Consider $x\sim{\mathcal B}(n,\theta)$ when $\theta$ is to be estimated under the quadratic loss, $$ \mathrm{L}(\theta,\delta) = (\delta-\theta)^2. $$ Bayes estimators are then given by posterior expectations (see Section 2.5) and, when $\theta \sim{\mathcal B}e \left({\sqrt{ n} \over 2}, {\sqrt{ n} \over 2} \right)$, the posterior mean is $$ \delta^ \ast (x) = {x+ \sqrt{n}/2 \over n+ \sqrt{ n}}. $$ Moreover, this estimator has constant risk , $\mathrm{R}(\theta,\delta^*) = 1/4(1+\sqrt{n})^2$. Therefore, integrating out $\theta$, $\mathrm{r}(\pi) = \mathrm{R}(\theta,\delta^*)$ and $\delta^*$ is minimax according to Lemma 2.9. Notice the difference with the MLE, $\delta_0(x) = x/n$, for the small values of $n$, and the unrealistic concentration of the prioraround $0.5$ for larger values of $n$.
