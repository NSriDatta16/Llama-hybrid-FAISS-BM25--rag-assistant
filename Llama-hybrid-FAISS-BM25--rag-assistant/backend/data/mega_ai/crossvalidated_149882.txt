[site]: crossvalidated
[post_id]: 149882
[parent_id]: 149875
[tags]: 
Random forest has several hyperparameters that need to be tuned. To do this correctly, you need to implement a nested cross validation structure. The inner CV will measure out-of-sample performance over a sequence of hyperparameters. The outer CV will characterize performance of the procedure used to select hyperparameters, and can be used to get unbiased estimates of AUC and so forth. The hyperparameters that you may tune include ntree , mtry and tree depth (either maxnodes or nodesize or both). By far, the most important is mtry . The default mtry for $p$ features is $\sqrt{p}$. Increasing mtry may improve performance. I recommend trying a grid over the range $\sqrt{p}/2$ to $3\sqrt{p}$ by increments of $\sqrt{p}/2$. Tuning ntree is basically an exercise in selecting a large enough number of trees so that the error rate stabilizes. Because each tree is i.i.d., you can just train a large number of trees and pick the smallest $n$ such that the OOB error rate is basically flat. By default, randomForest will build trees with a minimum node size of 1. This can be computationally expensive for many observations. Tuning node size/tree depth might be useful for you, if only to reduce training time. In Elements of Statistical Learning , the authors write that they have only observed modest gains in performance to be had by tuning trees in this way.
