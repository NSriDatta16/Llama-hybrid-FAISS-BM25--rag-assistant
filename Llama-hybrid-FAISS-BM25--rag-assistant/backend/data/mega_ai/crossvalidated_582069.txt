[site]: crossvalidated
[post_id]: 582069
[parent_id]: 582061
[tags]: 
You are correct that overfitting is a rampant problem in health research, just as it is in all other fields in which sample sizes are not huge. One of the biggest mistakes being made in recent years is to assume that machine learning algorithms somehow fix this problem. While algorithms can be tuned with cross-validation to not overfit, many such as random forests typically result in massive overfitting. It is a not correct to use one method of supervised learning to select features to promote for use in another method. The second method has lost the context and does not know how to apply the proper amount of shrinkage. In addition, the first method has a very low chance of finding the "right" features. For example many practitioners think that lasso finds the right features when it fact it usually fails miserably in that task. I go into many of these issues at length in RMS and BBR . The most general-purpose, safest, and interpretable solution is heavy use of unsupervised learning (sparse principal components; regular principal components after doing variable clustering, etc.). This allows either traditional or ML supervised learning to be used on the reduced, combined, features with much stability and without much overfitting. Data are not capable of informing us about which variables are important, so we should stop trying to use data to do that. A simple simulation in RMS shows that. With a limited number of candidate features, a high signal:noise ratio, and zero collinearity, stepwise variable selection still has a very low chance of selecting the right variables. Same with methods such as lasso . If it can't work in an ideal setting it can't work on real data.
