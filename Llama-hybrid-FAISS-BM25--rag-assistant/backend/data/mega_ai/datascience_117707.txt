[site]: datascience
[post_id]: 117707
[parent_id]: 117670
[tags]: 
Long Short Term Memory (LSTM) can take a long time to train because of the complexity of the architecture. If you think the size of the embedding space is also slowing down training, you can reduce the size of the vocabulary by only taking the most frequently occurring words. Since it appears you are using PyTorch, you can do this within the torchtext.vocab.GloVe class with the max_vectors argument which limits the size of the loaded set.
