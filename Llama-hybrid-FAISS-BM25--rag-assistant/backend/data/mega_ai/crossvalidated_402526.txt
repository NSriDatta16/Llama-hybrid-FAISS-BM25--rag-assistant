[site]: crossvalidated
[post_id]: 402526
[parent_id]: 396567
[tags]: 
https://wayve.ai/blog/dreaming-about-driving-imagination-rl is describing a single network. They are not describing a procedure where the prediction is used as input for further prediction. the data they have is a series of observed driving conditions lets call it $$DC={DC_1,DC_2,DC_3,...DC_N}$$ Note their statement: We train the encoder and prediction model on real-world data. At no point are predictions used as inputs to train the neural network. Training is performed in this fashion DC_K -> [neural network] -> [prediction] ------| |->[reward to neural network] DC_{K+1} --------------------------------------| the prediction is not being used as input into the neural network it is only being used as comparison with DC_{K+1} which is being used as the input to the neural network. DC_K -> [neural network] -> [prediction] ------| |->[reward to neural network] DC_{K+1} --------------------------------------| | ---> [neural network] -> [prediction] ------| |->[reward to neural network] DC_{K+2} --------------------------------------| | ---> [neural network] -> [prediction] ------| |->[reward to neural network] DC_{K+3} --------------------------------------| | ... | ---> [neural network] -> [prediction] ------| |->[reward to neural network] DC_{N} ----------------------------------------| notice that at no point are the predictions being used as inputs to the neural network, this is not being done in a sequence of training operations as might be inferred from their diagrams but rather the rewards are all applied in a single stage as would be expected from a normal stage of neural network training. The only reason they are making it look like the outputs are being used as inputs is to explain the serial nature of their training data, that DC_{K} is both input and target. In the explanation for training their driving policy they do use the prediction output as the input to the next stage of prediction. However at this point the prediction model is not being trained, and the output of the driving policy is just steering and speed information. The paper leaves out how they calculate the reward given to the policy network based on the predictions. I would assume they have another network that has been trained to evaluate how "off the road" a given state vector is. Another, less likely option, is that they just take the difference of the predicted state vectors vs actual state vectors given for each possible DC_1 . Since if DC is based on proper driving behavior then each predicted DC_k should match the original real world data if the driving policy is matching the proper driving behavior. I doubt they are doing this.
