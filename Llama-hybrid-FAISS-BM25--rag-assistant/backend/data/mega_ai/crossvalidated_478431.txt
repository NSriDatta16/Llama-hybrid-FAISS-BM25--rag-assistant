[site]: crossvalidated
[post_id]: 478431
[parent_id]: 478428
[tags]: 
I am almost certain that your logistic regression does not predict only one outcome , i.e., a probability of $\hat{p}_i=0$ or $\hat{p}_i=1$ for the target class for all instances $i$ . Rather, it predicts some $\hat{p}_i\in[0,1]$ , which you then compare to a threshold $\theta$ , which you chose in some way. Possibly, you use $\theta=0.5$ . You then label instance $i$ as "target class" or "non-target class" based on $\hat{p}_i$ and $\theta$ . And it happens that $\hat{p}_i\geq\theta$ for all $i$ (or, equivalently, $\hat{p}_i\leq\theta$ for all $i$ ). The solution to your conundrum is not to use a threshold and hard classification at all, but to deal directly with the probabilistic classification given by $\hat{p}$ . More information can be found at Reduce Classification Probability Threshold . I also recommend Why is accuracy not the best measure for assessing classification models? , because every criticism leveled there at accuracy applies equally to precision, recall etc.
