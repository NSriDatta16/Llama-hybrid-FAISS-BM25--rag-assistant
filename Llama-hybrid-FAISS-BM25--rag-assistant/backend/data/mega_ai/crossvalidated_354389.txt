[site]: crossvalidated
[post_id]: 354389
[parent_id]: 65699
[tags]: 
Just some extension to russellpierce's answer. 1) Essentially LSA is PCA applied to text data. When using SVD for PCA, it's not applied to the covariance matrix but the feature-sample matrix directly, which is just the term-document matrix in LSA. The difference is PCA often requires feature-wise normalization for the data while LSA doesn't. There's a nice lecture by Andrew Ng that illustrates the connections between PCA and LSA. 2/3) Since document data are of various lengths, usually it's helpful to normalize the magnitude. Here sample-wise normalization should be used not the feature-wise normalization. In practice I found it helpful to normalize both before and after LSI. If the clustering algorithm metric does not depend on magnitude (say cosine distance) then the last normalization step can be omitted. 4) It think this is in general a difficult problem to get meaningful labels from clusters. Some people extract terms/phrases that maximize the difference in distribution between the corpus and the cluster. Another way is to use semi-supervised clustering with predefined labels.
