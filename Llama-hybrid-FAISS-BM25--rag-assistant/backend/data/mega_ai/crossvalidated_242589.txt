[site]: crossvalidated
[post_id]: 242589
[parent_id]: 242117
[tags]: 
Good question! I've actually tried to get my mind around the same for quite some time, so I'll just share my experiences. I'm still a novice in this area, so I hope I haven't made any mistakes in notation. always , and I mean always , standardise your continous variables to have mean=0 and sd=1 (or even sd=2). Look into some of Andrew Gelman's blog posts or articles about this. Just google Andrew Gelman standardization, there are a lot of good papers and posts. Think of your coefficients as log(odds-ratios) (in reference to a reference category, explanation follows). For an in-depth discussion, see this answer . Andrew Gelman also has some recommendations on priors, such as the cauchy , or normal(0,1) . His papers are about logistic regression, but I find that these recommendations also extend to multi-outcome regression. The dimension of the prior indeed depends on the number of outcomes. If you have three outcomes, you have these three linear dependencies: $ y_1 = \beta_{0,1} + \sum_i\beta_{i,1}*x_{i,1} $ and $ y_2 = \beta_{0,2} + \sum_i\beta_{i,2}*x_{i,2} $ and $ y_3 = \beta_{0,3} + \sum_i\beta_{i,3}*x_{i,3} $ The second subscript denotes the outcome. Normally you would put a prior on each of these coefficients. I'll illustrate with the intercept, $\beta_{0,k}$, $\beta_{0,1} \sim \mathcal{N}(0,1)$ and $\beta_{0,2} \sim \mathcal{N}(0,1)$ and $\beta_{0,3} \sim \mathcal{N}(0,1)$ Just to complete the math, the probability of outcome $k$ is, $p_k = \frac{y_k}{\sum_k y_k}$ and the likelihood is $y \sim \text{Multinomial}(p)$ Please note that you would often force one of the outcomes to be the "reference" outcome due to identifiability issues. Wiki has a detailed description of why. Practically, this means you force the coefficients of the reference category to be 0, and thus $p_{reference} = 1$, since $exp(0) = 1$. I have never used MCMCglmm so I cannot answer anything specific to that, I'm afraid.
