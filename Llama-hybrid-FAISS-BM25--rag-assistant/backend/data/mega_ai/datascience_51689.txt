[site]: datascience
[post_id]: 51689
[parent_id]: 
[tags]: 
Reading a visualization of word embeddings

For my Masters Thesis, I created a Word2Vec model. I wanted to show this image to clarify the result. But how does the mapping works to display the words in this 2D space? All words are represented by a vector of 300 dim. How are they mapped on this 2D image? What are the x & y scales? Code: documents = [_text.split() for _text in df_train.text] w2v_model = gensim.models.word2vec.Word2Vec(size=W2V_SIZE, window=W2V_WINDOW, min_count=W2V_MIN_COUNT, workers=8) w2v_model.build_vocab(documents) words = w2v_model.wv.vocab.keys() vocab_size = len(words) print("Vocab size", vocab_size) w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH) tokenizer = Tokenizer() tokenizer.fit_on_texts(df_train.text) vocab_size = len(tokenizer.word_index) + 1 print("Total words", vocab_size) x_train = pad_sequences(tokenizer.texts_to_sequences(df_train.text), maxlen=SEQUENCE_LENGTH) x_test = pad_sequences(tokenizer.texts_to_sequences(df_test.text), maxlen=SEQUENCE_LENGTH) labels = df_train.target.unique().tolist() labels.append(NEUTRAL) encoder = LabelEncoder() encoder.fit(df_train.target.tolist()) y_train = encoder.transform(df_train.target.tolist()) y_test = encoder.transform(df_test.target.tolist()) y_train = y_train.reshape(-1,1) y_test = y_test.reshape(-1,1) embedding_matrix = np.zeros((vocab_size, W2V_SIZE)) for word, i in tokenizer.word_index.items(): if word in w2v_model.wv: embedding_matrix[i] = w2v_model.wv[word] print(embedding_matrix.shape) embedding_layer = Embedding(vocab_size, W2V_SIZE, weights=[embedding_matrix], input_length=SEQUENCE_LENGTH, trainable=False)
