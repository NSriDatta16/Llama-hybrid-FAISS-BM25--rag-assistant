[site]: crossvalidated
[post_id]: 419065
[parent_id]: 418779
[tags]: 
I believe it may be a case of gradient descent not being able to converge. When we are doing ML with a Logistic Regression we are essentially defining a cost function, trying "something", checking what direction the gradient that reduces the cost function is pointing, and following that direction in our next attempt. Ideally you want something like the image on the right: However if your features are on very different scales, you will have a problem like the one on the left for Gradient Descent to solve. It will take a step, overshoot, take a step, overshoot, etc. You can technically find the minimum, but it's a much harder problem. So in summary, when you normalise your features, you are helping gradient descent find the optimum. And gradient descent is your friend. Le sauce of the image
