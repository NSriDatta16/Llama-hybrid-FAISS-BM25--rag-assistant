[site]: stackoverflow
[post_id]: 4254303
[parent_id]: 4254230
[tags]: 
Not to undermine your question but 28k records is really not all that many. Are you perhaps optimizing prematurely? I would first stick to using 'regular' indices on a DB table. The harshing heuristics they use are typically very efficient and not trivial to beat (or if you can is it really worth the effort in time and are the gains large enough?). Also depending on where you actually do the tag query, is the user really noticing the 200ms time gain you optimized for? First measure then optimize :-) EDIT Without a DB I would probably have a master table holding all tags together with an ID (if possible hold it in memory). Keep a regular sorted list of IDs together with each post. Not sure how much storage based on commonality would help. A sorted list in which you can do a regular binary search may prove fast enough; measure :-) Here you would need to iterate all posts for every tag query though. If this ends up being to slow you could resort to storing a pocket of post identifiers for each tag. This data structure may become somewhat large though and may require a file to seek and read against. For a smaller table you could resort to build one based on a hashed value (with duplicates). This way you could use it to quickly get down to a smaller candidate list of posts that need further checking to see if they match or not.
