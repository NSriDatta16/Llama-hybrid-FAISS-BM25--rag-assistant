[site]: crossvalidated
[post_id]: 273940
[parent_id]: 273436
[tags]: 
Transforming individual variables would not help in random forest, because you can transform distances, but order of observations would remain the same, and problem of inability to distinguish classes would remain. In fact in most decision trees distances does not matter at all and only order of observations matter. Unfortunately, if for some groups of cases your data does not differ between different classes, it is impossible to predict them correctly. However, there is a hope. Although, your classes does not differ on any single variable, there is a chance, that they differ in multidimensional space of those variables. Try using some algorithms that are multidimensional (does not use single variable splits like decision trees in random forest, but many variables at once) or uses distances. I would recommend trying: -k-means (this of course uses distances on multidimensional space, you should try with transforming variables with PCA before modelling and experiment with 5+ centroids), -multinominal logistic regression with quadratic terms on every variable (this method gives score basing on many variables at once, so maybe it would help), -support vector machine (quite complicated to explain it here, but it also uses kind of multidimensional distances). If your problem is only the imbalance of predicted classes you can retrieve votes of every tree from your model and decide yourself about some custom rules what class should be predicted with certain votes share or you can weight votes for different classes. Some random forest implementations also allow to assign weights to model. If you would weight underrepresented classes with higher weights your model will predict them more often.
