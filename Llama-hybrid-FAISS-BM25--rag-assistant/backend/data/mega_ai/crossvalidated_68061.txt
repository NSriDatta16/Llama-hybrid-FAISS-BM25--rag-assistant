[site]: crossvalidated
[post_id]: 68061
[parent_id]: 68054
[tags]: 
The 'E' is typically referred to as the 'expectation' step. It refers to the fact that with fixed $\theta$ and fixed data, one can (usually) find the conditional expectation for the hidden states conditional on the current value of the parameters. In step 2, you consider the hidden states fixed (and data of course) and update the current value of the parameters by maximizing the conditional likelihood. And you repeat until the parameters converge. So to answer your question, the expectation step considers the parameters fixed at a particular value, in which case the hidden states are conditionally computable as a function of the current parameters. In the Bayesian context, this value would form a member of the posterior probability density. Note, however, that the traditional EM algorithm is really built to find the mode (MLE or MAP), and does so by hunting around the posterior. However, it's not fully Bayesian as you do not search, nor try to search, through the entire posterior space and don't attain a posterior PDF as a result. The Bayesian extension is known as variational Bayes.
