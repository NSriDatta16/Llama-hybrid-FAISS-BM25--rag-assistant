[site]: crossvalidated
[post_id]: 541626
[parent_id]: 
[tags]: 
Questions about MNIST samples training multilayer perceptron with the structure 784x2500x2000x1500x1000x500x10

According to the MNIST database , the MLP with the configuration 784 input neurons, 2500, 2000, 1500, 1000, 500 neurons in the hidden layers, and 10 output neurons can be trained with a learning rate of 0.35% (=0.0035). The network trains by using stochastic gradient descent (without (mini-)batches) with backpropagation, and the sigmoid function is used as activation function for every neuron in every layer. The weights and biases are both initialised randomly between a range of -1 and 1, is this correct? How many epochs (1 epoch = all samples being trained once) are needed for the 60000 image dataset to get that accuracy close to 99.65% which is claimed by Cire≈üan, Meier, Gambardella and Schmidhuber (2010) (arXiv) ? My self-built neural network does 100 iterations (100 iterations = training 100 samples) in 20 seconds with this network configuration on a 8 GB RAM computer, using Java. Is this considered as average speed, or is Tensorflow faster, if so how many times?
