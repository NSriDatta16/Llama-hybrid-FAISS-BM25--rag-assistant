[site]: crossvalidated
[post_id]: 628990
[parent_id]: 
[tags]: 
Solving an exercise about admissible coefficient values for a MA(1) process

I'm studying "Principles of system identification : Theory and Pratice" by Arun K. Tangirala and well... I've just entered the part about moving averages and I'm confused. I don't understand some assumptions around white noise, stationary processes and autoregressive models. I'll illustrate my problems on the following exercise: The problem is exercise E8.3 p 203 and it states : "Show that for a MA(1) process $v[k]=e[k]+c_1 e[k-1]$ , the ACVF sequence $\sigma_{vv}(l)$ is non negative definite whenever $\vert c_1\vert \geq \frac{1}{2}$ " Here is what I did so far : from a computation made somewhere else in the book around the same problem, which seems to work if you assume $\mu_v=0$ (more on that later) we have, $e$ being a white noise process : $$ \begin{align} \sigma_{vv}(l) &= E[(v[k]-\mu_v)(v[k-l]-\mu_v)]\\ &=E[(e[k]+c_1e[k-1])(e[k-l]+c_1e[k-l-1])]\\ &= E[e[k]e[k-l]] + c_1E[e[k]e[k-l-1]]\\ &+c_1E[e[k-1]e[k-l]] + c_1^{2}E[e[k-1]e[k-l-1]]\\ &=\sigma_{ee}[l] + c_1\sigma_{ee}[l+1] + c_1\sigma_{ee}[l-1]+c_1^{2}\sigma_{ee}[l] \end{align} \label{eq1}\tag{1}$$ Now if we agree with this we might proceed and (try to?) solve the exercise. According to the result of \ref{eq1} the variance-autocovariance matrix is then : $$\sigma_{e}^{2}\underbrace{\begin{bmatrix}1+c_1^{2} & c_1\\c_1&1+c_1^{2}\end{bmatrix}}_{A}\label{eq2}\tag{2}$$ A quick examination of $A$ can show that $\begin{bmatrix}1\\1\end{bmatrix}$ and $\begin{bmatrix}1\\-1\end{bmatrix}$ are eigenvectors of $A$ for the respective eigenvalues $\lambda_1(c_1) = c_1^{2} + c_1 + 1$ and $\lambda_2(c_1) = c_1^{2} - c_1 + 1$ $\lambda_1$ and $\lambda_2$ reach their minimum respectively for $c_1 = -\frac{1}{2}$ and $c_1 = \frac{1}{2}$ . Once plugged-in they both give $\lambda_{1_{min}} = \frac{3}{4}$ and $\lambda_{2_{min}} = \frac{3}{4}$ . This means that $\lambda_1$ and $\lambda_2$ only have complex roots and will remain strictly positive for any $c_1$ . To my knowledge this means that A will be definite positive no matter the (real) value of $c_1$ . So either my reasoning or the exercise is broken. Here are my questions : Is my calculation right? How am I supposed to find a condition on $c_1$ ? In general is that a thing to show that AR coefficients have to compel to the positivity of the variance-autocovariance matrix? Between the first and second line of \ref{eq1} it is assumed that $\mu_v=0$ which happens if $\mu_e=0$ . The author seems to do that a lot in he book without explanation. In this book white noise is only defined as a stationary process $e$ verifying $$\sigma_{e}^{2}[l] = \delta_{l0}$$ so there is no mention of zero mean. Is it a forgotten assumption? Does the stationarity of $v$ imply the zero mean of $e$ (if so, how?)? Do white noises always have zero mean? In essence why exactly do we have $E[v[k]]=0$ in general?
