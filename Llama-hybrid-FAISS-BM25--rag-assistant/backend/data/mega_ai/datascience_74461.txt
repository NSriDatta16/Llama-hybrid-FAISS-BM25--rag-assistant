[site]: datascience
[post_id]: 74461
[parent_id]: 74447
[tags]: 
LSTMs have memory, so it matters in what order the model sees your samples. From the answer you linked: The model's internal parameters are changing and persisting with each new example it sees. The current prediction depends on the last prediction. Recurrent neural networks have memory, so order matters. If you're worried that your estimate of loss is inaccurate, you can obtain a more stable estimate of validation_loss by using time-series cross-validation . If you are instead worried that the performance of your model is insufficient, keep in mind that hyperparameter tuning will likely be important for this problem. No need to fix the size of the historical window at 3; perhaps a larger window size is better? Other hyperparameters such as the size of the hidden state and the number of recurrent layers can also have a big impact on performance.
