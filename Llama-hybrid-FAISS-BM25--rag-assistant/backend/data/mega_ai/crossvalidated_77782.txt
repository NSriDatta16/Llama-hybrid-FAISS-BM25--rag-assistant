[site]: crossvalidated
[post_id]: 77782
[parent_id]: 77770
[tags]: 
Artificial neural network: computational power (Wikipedia) : The multi-layer perceptron (MLP) is a universal function approximator , as proven by the Cybenko theorem . However, the proof is not constructive regarding the number of neurons required or the settings of the weights. Work by Hava Siegelmann and Eduardo D. Sontag has provided a proof that a specific recurrent architecture with rational valued weights (as opposed to full precision real number-valued weights) has the full power of a Universal Turing Machine using a finite number of neurons and standard linear connections. They have further shown that the use of irrational values for weights results in a machine with super-Turing power. Cybenko, G.V. (1989). Approximation by Superpositions of a Sigmoidal function , Mathematics of Control, Signals, and Systems, Vol. 2 pp. 303–314. Siegelmann, H.T. and Sontag, E.D. (1994). Analog computation via neural networks , Theoretical Computer Science, v. 131, no. 2, pp. 331–360.
