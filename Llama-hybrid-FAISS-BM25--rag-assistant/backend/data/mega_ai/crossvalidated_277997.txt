[site]: crossvalidated
[post_id]: 277997
[parent_id]: 277251
[tags]: 
Basically, yes. Normal priors are common and your approach to selecting the s.d./variance is perfectly reasonable. I'd recommend reading any of Andrew Gelman's stuff about prior distributions. His book Bayesian Data Analysis is a very good and approachable resource for (almost) all things Bayesian. Essentially what I am doing is setting up a distribution that the values for the parameters (the coefficients for the constant and predictor) will be drawn from, correct? In a sense, yes, but I don't think this is the best way to think of priors. Priors are there to dampen/accentuate the likelihood in certain regions of parameter space, i.e. to weight the likelihood in some way. If we choose improper uniform priors (i.e. completely uninformative), the posterior distribution is just a normalised likelihood, which is very sensitive to the data. The mode of this distribution is the estimate you would get from running any common linear regression procedure ( lm in R for example). This is fine if we have complete faith in our data -- we're sure it reflects the population distribution pretty well. If we have reason to be suspicious of the data, it makes sense to include a prior. The effect of this is that if the data happen to result in lots of likelihood at some unreasonable parameter value, our prior should cancel it out by attaching a very small weight to that value; remember we multiply the prior and the likelihood (pointwise) to get the posterior distribution. So you aren't really drawing parameter values from the prior, rather you're expressing a restriction on the parameter values you're willing to accept as reasonable by weighting the likelihood. As a very loose approximation, we're just modifying the likelihood before we run lm to find its maximum to make sure our estimates don't come out as being unreasonable/at odds with our prior understanding of the process.
