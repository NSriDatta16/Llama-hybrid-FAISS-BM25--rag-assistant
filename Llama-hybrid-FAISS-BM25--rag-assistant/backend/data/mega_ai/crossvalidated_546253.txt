[site]: crossvalidated
[post_id]: 546253
[parent_id]: 546252
[tags]: 
This is just an obvious consequence of creating splits in the tree based on what improves the fit the most (when you look at the data before the split). E.g. let's say you have 3 categories and thee are mean outcomes for the different combinaton f category levels: Variables Variable 1 X X X X Y Y Y Y Variable 2 A A B B A A B B Variable 3 1 2 1 2 1 2 1 2 Mean outcome 0 10 10 0 0.1 10.1 10.1 0.1 When you greedily build a try, you'd split by Variable 1 on X vs. Y to create groups separated by 0.1 in their predictions (assuming that's a big enough lift to be meaningful to you/based on the hyperparameters you set). Then you would stop, because you cannot find another split that improves predictions. Obviously, there's ways of building trees that will split in other ways / on other variables that modern tree based algorithms use, and once you've split on either Variable 2 or 3, splitting on the other one will be obvious. If you create the tree in "holistic" way, you might really want to split on Variable 2 and 3 first (creates groups that are 10 apart in their predictions) and only then on Variable 1 (small additional improvement). You can think of more sophisticated versions of this, but that's one basic challenge with trees. See also e.g. random forest, where you build lots of trees, each of which may not be all that good, but in combination they are pretty good.
