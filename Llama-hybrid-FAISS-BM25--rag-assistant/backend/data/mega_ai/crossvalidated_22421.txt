[site]: crossvalidated
[post_id]: 22421
[parent_id]: 21502
[tags]: 
I read a clear example from Introduction to Data Mining by Tan et al. The example claims that if you are combining your classifiers with a voting system, that is classify a record with the most voted class, you obtain better performance. However, this example uses directly the output label of classifiers, and not the predictions (I think you meant probabilities). Let's have 25 independent classifiers that have generalization error $e = 1 - \mbox{accuracy} = 0.35$. In order to misclassify a record at least half of them have to misclassify it. Everything can be modeled with random variables, but you just have to compute the probability that at least 13 of them misclassify the record $$\sum_{i=13}^{25}\binom{25}{i}e^i(1-e)^{(25-i)} = 0.06$$ where each term of the summation means that $i$ classifier get the record class correctly and $25-i$ get it wrong. Using directly predictions and using as a combination method an average, I think that it could be a bit more difficult to show the improvment in ensemble performance. However, focusing only on predictions and without caring at the output label of the ensemble, averaging more predictions can be seen as an estimator of the real probability. Therefore, adding classifiers should improve the predictions of the ensemble technique.
