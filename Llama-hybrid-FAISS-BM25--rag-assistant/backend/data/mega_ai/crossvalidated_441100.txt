[site]: crossvalidated
[post_id]: 441100
[parent_id]: 
[tags]: 
How to estimate the error of the model in the best way?

I have many samples in my data and I want to fit many models which should be tuned by bayesian optimization. What is the best way for estimation of error, in statistical point of view? I mean, some estimates are biased, some have too high variance, I want to find some good way. I have some ideas and I would appreciate a discussion: 1) Split train data to train and validation set and use it in this setting for all estimations of hyperparameters and model selection. 2) Split the data ti training and validation set each time randomly when I will use new method, but keep the setting during hyperparameter tunning. 3) Split the data randomly to train and validation set in each step of bayesian optimization, for each method (this should lead to low bias probably, as the results will not be biased only towards the only split as mentioned in 1) ) 4) Do the cross validation in a way, that I split the data to $k$ folds once in the beginning and do the cross validation in bayesian optimization and when the model is selected, I can train the final model on the whole training set 5) Split the data to $k$ folds in each iteration of bayesian optimization, i.e. compute CV-error with current parameters, choose new parameters, split the data randomly to $k$ folds again and compute CV-error with current parameters etc... This should lead to lower bias towards the split ? Or any other idea? I would really appreciate also some justification and some discussion, I assume there is no "general" answer, but everything has its pluses and minuses. Or some good literature, what I have found so far does not answer my question. Thanks to whoever is interested !
