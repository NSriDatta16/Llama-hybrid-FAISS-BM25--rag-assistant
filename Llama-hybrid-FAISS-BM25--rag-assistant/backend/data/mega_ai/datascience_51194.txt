[site]: datascience
[post_id]: 51194
[parent_id]: 51188
[tags]: 
A possible explanation is this: When the order of the columns differ, there is a little difference in the procedure. What LightGBM, XGBoost, CatBoost, amongst other do is to select different columns from the features in your dataset in every step in the training. The selections of these columns is done randomly: Let's say your dataset has 20 columns. The root node selects the features 1st, 3rd and 18th , on both datasets the 1st, 3rd and 18th features are different in both possible datasets. This is repeatedly done and in every step there is randomness affecting your ultimate result.
