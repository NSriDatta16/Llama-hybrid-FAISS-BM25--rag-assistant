[site]: crossvalidated
[post_id]: 81842
[parent_id]: 72930
[tags]: 
Akaike's Information Criterion (AIC): Formally, AIC is defined as $2 \log L_k+ 2k$ where $L_k$ is the maximized log likelihood and $k$ is the number of parameters in the model. For the normal regression problem, AIC is an estimate of the Kullback-Leibler discrepancy between a true model and a candidate model. AIC, Bias Corrected (AICc): A corrected form, suggested by Sugiura (1978), and expanded by Hurvich and Tsai (1989), can be based on small-sample distributional results for the linear regression model Bayesian Information Criterion (BIC): is also called the Schwarz Information Criterion (SIC), for an approach yielding the same statistic based on a minimum description length argument. Various simulation studies have tended to verify that BIC does well at getting the correct order in large samples, whereas AICc tends to be superior in smaller samples where the relative number of parameters is large. Example: sarima(gnpgr, 1, 0, 0) # AR(1) $AIC: -8.294403 $AICc: -8.284898 $BIC: -9.263748 sarima(gnpgr, 0, 0, 2) # MA(2) $AIC: -8.297693 $AICc: -8.287854 $BIC: -9.251711 The AIC and AICc both prefer the MA(2) fit, whereas the BIC prefers the simpler AR(1) model. It is often the case that the BIC will select a model of smaller order than the AIC or AICc. It would not be unreasonable in this case to retain the AR(1) because pure autoregressive models are easier to work with.
