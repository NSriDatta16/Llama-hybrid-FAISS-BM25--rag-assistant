[site]: datascience
[post_id]: 120027
[parent_id]: 118910
[tags]: 
The animation is pretty! I think with a fully-connected network and a single hidden layer you would not expect to see any strong patterns of neuron responsibility emerge. But if you used two or three hidden layers, you might start to see some patterns in the 3rd layer. Additionally, if you use a high L1 regularization when training, then very small contributions will go to 0. This encourages the model to make each decision with fewer, more dedicated, neurons. So you might see more distinct patterns. If you train a CNN then the lower-levels will be discovering features, such as straight lines and curves. Higher-levels might combine these to discover loops, etc. You would then expect to see trends in the final linear layer. E.g. all the zeroes are reacting to discovering one large loop. The 6s and 9s are reacting to discovering small loops and a line. The 1s and 7s don't react to any of the loop features. Etc.
