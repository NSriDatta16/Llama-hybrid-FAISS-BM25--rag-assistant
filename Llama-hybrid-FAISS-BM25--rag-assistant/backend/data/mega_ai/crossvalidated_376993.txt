[site]: crossvalidated
[post_id]: 376993
[parent_id]: 
[tags]: 
Why does dropout increase the training time per epoch in a neural network?

I'm training an MLP neural network with one hidden layer and batch gradient descent using Keras/Tensorflow. Applying dropout to the input layer increased the training time per epoch by about 25 %, independent of the dropout rate. That dropout increases the number of epochs needed to reach a validation loss minimum is clear, but I thought that the training time per epoch would decrease by dropping out units. Does anyone know the reason?
