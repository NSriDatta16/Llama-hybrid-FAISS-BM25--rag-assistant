[site]: crossvalidated
[post_id]: 242785
[parent_id]: 
[tags]: 
Is this test/method known? (testing mean $ = 0$ when all observations have different variances)

I am sorry, if you can suggest a better title please do ... Here is the problem. Assume I have variables $X_i$ ( $i = 1,\dots,n$ ) with mean $\mu_i \ge 0$ and variance $\sigma_i^2$ . I want to test the null hypothesis $H_0$ : $\mu_1 = \dots = \mu_n = 0$ vs the general alternative "for some $i$ , $\mu_i > 0$ ". It is natural to consider the statistic $$ \sum_i X_i$$ but if the $\sigma_i^2$ are unknown, we are in a delicate situation. A possible solution would be to use $\sum X_i^2$ as an estimate of $\sum_i \sigma_i^2$ under the null hypothesis, which leads to the test statistic $$ {\sum_i X_i \over \sqrt{\sum_i X_i^2}}$$ which should asymptotically be $\mathcal N(0,1)$ under the null hypothesis (assume that the $\sigma_i^2$ are bounded). However under the alternative the variance will be overestimated, and one gets the feeling that this test is not optimal. Now here is the (strange) idea. Define a log-likelihood-like function $$ \ell( \theta ) = \sum_i \log( 1 + \theta X_i ). $$ The first derivative (a score-like function) is $$U(\theta) = \sum_i {X_i \over 1 + \theta X_i} $$ and the opposite of the second derivative (an observed information-like function) is $$J(\theta) = \sum_i {X_i^2 \over (1 + \theta X_i)^2}. $$ For small $\theta$ , $U(\theta) \simeq \sum_i (X_i - \theta X_i^2)$ . Under the null hypothesis, $E( U(\theta) ) \simeq -\theta \sum_i \sigma_i^2$ , which is null for $\theta = 0$ . We have $E(J(0)) > 0$ so its a maximum. Under the alternative, for small deviations from the null, the maximum likelihood should be around $\theta = { \sum_i \mu_i \over \sum_i \mu_i^2 + \sigma^2 } > 0$ . Finally, $\ell(\theta)$ seems to behave exactly like a brave and honest log-likelihood, the expected maximum of which is $\theta = 0$ under the null, and $\theta > 0$ under the alternative. All proofs based on Taylor developments should still hold for this particular function. It is important to note that the expected value of $J(0)$ (under the null hypothesis) is $\sum_i \sigma_i^2$ which is the variance of $U(0)$ . So if we treat this function like a real likelihood, and we look on the score test for $\theta = 0$ . It is $$ {\sum_i X_i \over \sqrt{\sum_i X_i^2}},$$ which should be familiar. Alternatively, a LRT can be used, with a classical trick for getting a one-sided test: $$ LRT = \text{sign}\left(\hat \theta\right) \sqrt{ 2 \ell\left(\hat\theta\right)- 2 \ell(0) } = \text{sign}\left(\hat \theta\right) \sqrt{ 2 \ell\left(\hat\theta\right)}.$$ Under the null, this statistic is equivalent to the above written one, but is (on average) higher under the alternative, thus leading to a higher power test (cf numerical experiment below). Now my question is: is this method known, what's its name? (I am used to endless wheel reinvention ...) Any good reference? Some concrete applications? (the idea to write a log-likelihood like function can certainly be used for other problems ...) Some numerical experiments. N $par, lrt = sign(opt$ par)*sqrt(2*opt$value), score = sum(X)/sqrt(sum(X**2))) } # under the null x # under the alternative x
