[site]: crossvalidated
[post_id]: 347831
[parent_id]: 347727
[tags]: 
I think when considering your question it helps if you try to keep the goal/selling points of null-hypothesis significance testing (NHST) in mind; it's just one paradigm (albeit a very popular one) for statistical inference, and the others have their own strengths as well (e.g., see here for a discussion of NHST relative to Bayesian inference). What's the big perk of NHST?: Long-run error control . If you follow the rules of NHST (and sometimes that is a very big if), then you should have a good sense of how likely you are to be wrong with the inferences you make, in the long run. One of the persnickety rules of NHST is that, without further alteration to your testing procedure, you only get to take one look at your test of interest. Researchers in practice often ignore (or are not aware of) this rule (see Simmons et al., 2012), conducting multiple tests after adding waves of data, checking their $p$-values after adding/removing variables to their models, etc. The problem with this is that researchers are rarely neutral with respect to outcome of NHST; they are keenly aware that significant results are more likely to be published than are non-significant results (for reasons that are both misguided and legitimate; Rosenthal, 1979). Researchers are therefore often motivated to add data/amend models/select outliers and repeatedly test until they "uncover" a significant effect (see John et al., 2011, a good introduction). A counterintuitive problem is created by the above practices, described nicely in Dienes (2008): if researchers will keep adjusting their sample/design/models until significance is achieved, then their desired long-run error rates of false-positive findings (often $\alpha =.05$) and false-negative findings (often $\beta =.20$) will each approach 1.0 and 0.0, respectively (i.e., you will always reject $H_0$, both when it's false and when it's true). In the context of your specific questions, researchers use two-tailed tests as a default when they don't want to make particular predictions with respect to the direction of the effect. If they are wrong in their guess, and run a one-tailed test in the direction of the effect, their long-run $\alpha$ will be inflated. If they look at descriptive statistics and run a one-tailed test based on their eyeballing of the trend, their long-run $\alpha$ will be inflated. You might think this isn't a huge problem, in practice, that the $p$-values lose their long-run meaning, but if they don't retain their meaning, it begs the question of why you are using an approach to inference that prioritizes long-run error control. Lastly (and as a matter of personal preference), I would have less of a problem if you first conducted a two-tailed test, found it non-significant, then did the one-tailed test in the direction the first test implied, and found it to be significant if (and only if) you performed a strict confirmatory replication of that effect in another sample, and published the replication in the same paper. Exploratory data analysis--with error-rate inflating flexible analysis practice--is fine, as long as you are able to replicate your effect in a new sample without that same analytic flexibility. References Dienes, Z. (2008). Understanding psychology as a science: An introduction to scientific and statistical inference . Palgrave Macmillan. John, L. K., Loewenstein, G., & Prelec, D. (2012). Measuring the prevalence of questionable research practices with incentives for truth telling. Psychological science , 23(5), 524-532. Rosenthal, R. (1979). The file drawer problem and tolerance for null results. Psychological bulletin , 86(3), 638. Simmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological science , 22(11), 1359-1366.
