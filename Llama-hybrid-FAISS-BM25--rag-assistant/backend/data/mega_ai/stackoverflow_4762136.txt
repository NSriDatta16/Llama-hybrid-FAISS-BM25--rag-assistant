[site]: stackoverflow
[post_id]: 4762136
[parent_id]: 
[tags]: 
Using robots.txt to block /?param=X

I have created a website using wordpress, and the first day it was full of dummy content until I uploaded mine. Google indexed pages such as: www.url.com/?cat=1 Now these pages doesn't exists, and to make a removal request google ask me to block them on robots.txt Should I use: User-Agent: * Disallow: /?cat= or User-Agent: * Disallow: /?cat=* My robots.txt file would look something like this: User-agent: * Disallow: /cgi-bin Disallow: /wp-admin Disallow: /wp-includes Disallow: /wp-content Disallow: /wp-login.php Disallow: /wp-register.php Disallow: /author Disallow: /?cat= Sitemap: http://url.com/sitemap.xml.gz Does this look fine or can it cause any problem with search engines? Should I use Allow: / along with all the Disallow:?
