[site]: crossvalidated
[post_id]: 485682
[parent_id]: 485266
[tags]: 
estimate the variance of the estimator using the usual CLT-based approach. ... Can I use this information to produce estimates with smaller confidence intervals? Yes, you can. (This is true in general. In many cases, you can do better than a normal approximation, especially when the distribution is not really a normal distribution but just approximately) How you are gonna do it exactly will depend on the situation. It seems like you want to compute the average of the distribution of $x$ by taking a sample. Classically your estimate will be based on a sample of size $n$ like $x_1, \dots x_n$ , and then you compute the mean and standard error. If the distribution of $x$ is assumed to be Gaussian (or approximately Gaussian, like most sample means are anyway), then you would use: $$\begin{array}{} \hat{\mu} &=& \bar{x} &=& \frac{1}{n} \sum_{i=1}^n x_i\\ \hat{\sigma}_\mu & =& \frac{1}{\sqrt{n}} s &=& \frac{1}{\sqrt{n}} \sqrt{\frac{1}{n-1}\sum_{i=1}^n (x_i-\bar{x})^2} \end{array}$$ But instead of the classical estimate of the error of the mean, you want to use some information about a special property of the data sampling which is that some of the items may occur multiple times. The exact approach will depend on the type of joint distribution of $x_k,\pi_k$ . But here we will show by means of two examples that indeed the estimates and the confidence interval can be treated differently. Binomial distribution case You might have a situation where there is only two items. Then the estimate of the mean all boils down to estimation of the probability $p$ for the 1st item (and $1-p$ for the second item). And the estimate of the mean becomes $$\hat{\mu} = x_1 \hat{p} + x_2 (1-\hat{p}) = x_2 + \hat{p} (x_1 - x_2)$$ Where the estimate $\hat{p}$ relate to estimation of the parameter of a binomial distribution whose estimate of the standard error is different from the estimate of the standard error of mean. In fact there is a large variety of approaches ( https://en.m.wikipedia.org/wiki/Binomial_proportion_confidence_interval ). In this example you know all the $x_k$ because you assume that there are only two items. In reality you may have something more complex like $\pi$ being some parametric probability function/density/mass $f(x)$ telling you how probable a certain value (or range) $x$ is. And your estimate of the average of $x$ will boil down to being an estimate of the average of the distribution/function $\pi$ . Depending on the type of distribution $\pi$ you will get different types of estimates and confidence intervals. Independent $\pi$ and $x$ It could be that the items are distributed with $\pi$ and $x$ independently. Your sample could have some item $k$ occuring multiple times, but this will be partly random/noisy behaviour that tells you little about the true weighted mean. Because of the independence of $\pi$ and $x$ you will only be interested in the distribution of $x$ and not the $\pi$ . So you can estimate the mean by only considering the $m$ unique items in the sample and not all the $n$ items (ie. you ignore multiplicity) $$\begin{array}{} \hat{\mu} &=& \bar{x} &=& \frac{1}{m} \sum_{i=1}^m x_i\\ \hat{\sigma}_\mu & =& \frac{1}{\sqrt{m}} s &=& \frac{1}{\sqrt{m}} \sqrt{\frac{1}{m-1}\sum_{i=1}^m (x_i-\bar{x})^2} \end{array}$$ Example computation Let $x_k \sim N(\mu,\sigma^2)$ and independent relative frequencies $y_k \sim Uniform(a,b)$ from which we compute the normalised frequencies $\pi_k = \frac{y_k}{\sum y_k}$ . Say we have 10 000 items according to this distribution and in order to estimate $\sum_{i=1}^{10000} x_i\pi_i$ we sample 5 000 times an item (with repetition). With a simulation we can see that there can be a difference in the error with the classical estimate and the alternative estimate, with the latter being closer to zero (see the sharper distribution): ### number of repetitions r $x*example$ p) ### sample 5000 items k $p) unique k) ### traditional estimate est1 $x[k]) ### alternative estimate est2 x[unique]) ### store results v_mu[trial] $mids,(h2$ density),type="l", log = "", xlab = "error of estimate", ylab = "density", xlim = c(-1,1)*0.15) lines(h1 $mids,(h1$ density),lty = 2) legend(-0.15,25, c("with repetitions","without repetitions"), lty = c(2,1),cex = 0.7) Note that this effect will depend a lot on the particular distribution of $\pi$ . In this example $\pi \sim U(1,1.1)$ , which is not much variation between the different $\pi_k$ and the variance of duplicity is more noise than reflecting a true difference in $\pi_k$ . You can change it a bit (e.g. use $\pi \sim U(0,1)$ or an entirely different distribution) and then the effect becomes less pronounced, or even negative. Anyway, the example in this answer shows that there will be differences in estimators and potential improvements can be made (but it will depend a lot on the knowledge of the particular underlying distribution how you are gonna approach the estimation).
