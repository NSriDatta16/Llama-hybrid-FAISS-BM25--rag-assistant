[site]: crossvalidated
[post_id]: 591435
[parent_id]: 
[tags]: 
Ask for rationale of finding the corresponding prior from regularizer by taking exponential of negative regularizer

In equation (5.112) of textbook "Pattern Recognition and Machine Learning" by Christopher M. Bishop, the simple regularizer takes the form $\frac{\lambda}{2}{\bf w}^T{\bf w}$ . The author says in the following that this regularizer corresponds to a zero-mean Gaussian prior distribution which is the exponential of the negative regularizer. This relation is elaborated in section 3.3.1 of the book and in particular equation (3.52). Later in the next section, the author says in page 259 that the regularizer which is invariant to re-scaling of the weights $$\frac{\lambda_1}{2}\sum\limits_{w\in\mathcal W_1}w^2+\frac{\lambda_2}{2}\sum\limits_{w\in\mathcal W_2}w^2\tag{5.121}$$ corresponds to a prior of the form $$p({\bf w}|\alpha_1,\alpha_2)\propto\exp\left(-\frac{\alpha_1}{2}\sum\limits_{w\in\mathcal W_1}w^2-\frac{\alpha_2}{2}\sum\limits_{w\in\mathcal W_2}w^2\right)\tag{5.122}.$$ Here the prior (5.122) is again obtained by taking exponential of negative regularizer, the same way as the relation regarding simple regularizer. Here comes my question. It takes a page (page 153) to address the relation between the simple regularizer and the corresponding prior, if we don't count preceding derivations of formulas related to Gaussian distribution. But here in equation (5.122), the author claims immediately that the corresponding prior is exponential of negative regularizer. I tried to compute the posterior distribution and then to verify that I can get the regularizer (5.121), but of no avail. Is taking exponential of negative regularizer a theorem to tell immediately what the corresponding prior is from the regularizer? If so, where is this theorem in the book? If not, how does the author tell immediately that regularizer (5.121) corresponds to a prior of the form (5.122)? Thanks.
