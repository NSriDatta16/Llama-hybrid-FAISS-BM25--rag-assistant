[site]: datascience
[post_id]: 53036
[parent_id]: 53033
[tags]: 
My opinion : You should try to increase the learning rate of your model (or even other parameters of your optimizer - e.g. momentum). To answer your questions : Why the network is still learning after so many epochs (and so slowly)? It is a reasonable behaviour? Do I need to run the model for 2000, or even 3000 epochs to get the best macro f1 score? It risks to overfit, doesn't it? Yes from what I can tell your model is still learning (evident by the fact that your validation loss is still dropping). It definitely needs more epochs to get the best performance. My network is fed by word embeddings and position embeddings only, so I'm wondering if this behaviour can be motivated by (i) a network architecture that is too complex for the task, (ii) a network architecture that is too simple to model the complexity for the task, (iii) the inputs are not so informative to discriminate the classes, so the network learns with difficulty (and thus slowly). My network architecture is not deep: it has an embedding layer + conv layer + max-pool layer + softmax. Generally speaking, larger networks need more time to converge. However the model you describe is pretty small so it shouldn't take too long. A side question: suppose 1000 epochs are enough. When I need to compare performance of many models (or to compare different hyper-parameter combinations), which score I need to pick and compare? The "last best" f1 score (say at epoch 980) or the last reported score (i.e., the one at epoch 1000)? If you want to compare performance, you should compare the best f1 for each model. If you want to compare convergence speed then you could compare the f1 at X epochs.
