[site]: crossvalidated
[post_id]: 392194
[parent_id]: 316464
[tags]: 
To add to Curtis White's answer (and adding some more references): Yes SGD works as a type of regularization. This is important because otherwise, it's hard to explain why DNNs do not always overfit, because they can . The reason, as I understand, is that SGD causes 'hopping around' in parameter space, so during training the parameters cannot stay in a narrow minimum, only in (or close to) wider ones. And these wider ones apparently [1] generalize better (aka, less overfitting). More references: Here's [2] another paper that formalizes this (or tries to, I didn't follow everything through, check for yourself!) This paper [3] claims that there is a phase of "stochastic relaxation, or random diffusion" where the stochasticity inherent in SGD leads to "maximiz[ation of] the conditional entropy of the layer" . Both sort of say that SGD corresponds to an entropy regularization term. There could definitely be other ways in which batch size influences convergence; this is the one I know of. Editing in another reference, to show where people in practice observe that a smaller batch size improves generalization: [5] [1] Example: "A Bayesian Perspective on Generalization and Stochastic Gradient Descent", Smith, Le, 2018. From the abstract: "We propose that the noise introduced by small mini-batches drives the parameters towards minima whose evidence is large." [2] "Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks", Chaudhari, Soatto 2017 [3] "Opening the black box of Deep Neural Networks via Information" Schwartz-Ziv, Tishby, 2017 [4] "Understanding deep learning requires rethinking generalization", C. Zhang etc. 2016 [5] "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima", N. S. Keskar et al 2016
