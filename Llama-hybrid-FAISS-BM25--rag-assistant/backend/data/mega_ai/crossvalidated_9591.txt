[site]: crossvalidated
[post_id]: 9591
[parent_id]: 9590
[tags]: 
You haven't specified what "modeling" you plan on, but it sounds like you're asking about how to select independent variables among $A$, $B$, and $C$ for the purpose of (say) regressing a fourth dependent variable $W$ on them. To see that this approach can go wrong, consider three independent Normally distributed variables $X$, $Y$, and $Z$ with unit variance. For the true, underlying model choose a small constant $\beta \ll 1$, a really tiny constant $\epsilon \ll \beta$, and let the (dependent variable) $W = Z$ (plus a little bit of error independent of $X$, $Y$, and $Z$). Suppose the independent variables you have are $A = X + \epsilon Y$, $B = X - \epsilon Y$, and $C = \beta Z$. Then $W$ and $C$ are strongly correlated (depending on the variance of the error), because each is close to a multiple of $Z$. However, $W$ is uncorrelated with either of $A$ or $B$. Because $\beta$ is small, the first principal component for $\{A, B, C\}$ is parallel to $X$ with eigenvalue $2 \gg \beta$. $A$ and $B$ load heavily on this component and $C$ loads not at all because it is independent of $X$ (and $Y$). Nevertheless, if you eliminate $C$ from the independent variables, leaving only $A$ and $B$, you will be throwing away all information about the dependent variable because $W$, $A$, and $B$ are independent! This example shows that for regression you want to pay attention to how the independent variables are correlated with the dependent one; you can't get away just by analyzing relationships among the independent variables.
