[site]: crossvalidated
[post_id]: 235694
[parent_id]: 235687
[tags]: 
Decision Trees are pretty good at finding the most important features, they consider all features and create a split on the one that is separating class labels the best (in terms of entropy). If you use Random Forests it's even better, because some implementations (like scikit-learn's) are capable of sampling the features and use only a subset of it. Also in general Random Forests are more robust than decision trees. If you want, you can compute Information Gain before using a Decision Tree to see how much information a particular feature contains regarding the Label: https://en.wikipedia.org/wiki/Information_gain_in_decision_trees
