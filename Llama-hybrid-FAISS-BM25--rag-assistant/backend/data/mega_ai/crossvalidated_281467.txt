[site]: crossvalidated
[post_id]: 281467
[parent_id]: 275956
[tags]: 
In principle, as long as you have a reward signal, yes. You could consider mechanical mechanism design as the case of making sequential decisions about the parameters of each component. However, you don't observe rewards (the return) until the end of an "episode" (the sequence of decisions that fully specifies your mechanism). This is fine, it just makes the "credit assignment" problem harder (which actions led to which rewards). If the action space is large (many parameters required to fully specify a design), you could use policy-gradient reinforcement learning to approach this problem. Policy-gradient reinforcement learning uses the observed returns and "pushes" the policy to make the actions that yielded higher returns more likely. If the evaluation of each mechanism is efficient, then the method should work well - it can quickly obtain experience. You could try using a Recurrent Neural Network-based policy gradient method, and will have to design 1) the action representation 2) the observations the policy sees at each decision step 3) the reward signal
