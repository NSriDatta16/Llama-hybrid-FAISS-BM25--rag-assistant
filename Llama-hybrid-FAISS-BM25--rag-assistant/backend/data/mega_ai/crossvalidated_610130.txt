[site]: crossvalidated
[post_id]: 610130
[parent_id]: 
[tags]: 
Calculating Cohen's Kappa for a dataset when categories differ between examples

Cohen's Kappa is a measure of inter-annotator agreement that is traditionally applied to a setting where you have two raters who each classify $N$ items into $C$ mutually exclusive categories. For example, in a 4-class classification dataset with 1000 examples, $N = 1000$ and $|C| = 4$ . In some settings, $|C|$ differs across the items. For example, suppose you have a dataset where each example is a (question, paragraph) pair, and the annotators are asked to select a substring of the paragraph that answers the question. In this case, the $C$ for each example is the set of all possible substrings in the paragraph (and the paragraph differs in each example). Does it make sense to calculate Cohen's Kappa in this setting? While it is possible to calculate the Cohen's kappa on a per-example basis, is there a reasonable way of computing an aggregated Cohen's kappa across the whole dataset (the $N = 1000$ examples)? One could imagine taking a simple average, or doing a weighted average based on the prior probability of chance agreement, etc. If you have any pointers to techniques or past literature, that'd be much appreciated! Thanks in advance!
