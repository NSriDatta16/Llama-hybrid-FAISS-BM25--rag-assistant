[site]: crossvalidated
[post_id]: 600263
[parent_id]: 281304
[tags]: 
This is an important question to ask. I think you've hit a bit of a wall with it because of the common misconception that to draw a line between "sampling from a probability distribution" and "sampling from a population" is to create a false dichotomy. Let's look at a simple Bayesian example: You flip a coin $n$ times and record $X$ heads. The number of heads in $n$ flips is given by the binomial distribution: $$P(x|\theta)= \binom nx \theta^x (1-\theta)^{n-x}$$ We're interested in finding out plausible values for $\theta$ , the probability of the coin landing Heads. We are interested in the posterior distribution of $\theta$ : $$p(\theta|x)=\frac{p(x|\theta)*p(\theta)}{A}$$ $A$ is just a normalizing constant, and yet it is often too complicated to solve for. This means we want to be able to sample from unnormalized distributions (distributions that do not add up to 1). So, instead of sampling from $p(\theta|x)$ we are interested in sampling from the unnormalized distribution $p(x|\theta)*p(\theta)$ . Finally, we want to sample under different priors (different assumptions about the prior distribution $p(\theta)$ ). If we assume 3 different priors for $\theta$ , we want to be able to sample from all 3 unnormalized probability distributions: $$p(x|\theta)*p(\theta)=p(x|\theta)*prior_1$$ $$p(x|\theta)*p(\theta)=p(x|\theta)*prior_2$$ $$p(x|\theta)*p(\theta)=p(x|\theta)*prior_3$$ What this example tried to illustrate is that we need the ability to sample from any nonnegative function that we may want to write down on paper; I don't call it a probability distribution because some may think this refers to sampling from named distributions only (Normal, Binomial, Poisson, etc.); yet in practice we are sampling from some unnamed complicated nonnegative function. The other reason I do not call it a probability distribution is so that we do not mistake our mission for that of sampling from normalized distributions only; very often we need to sample from unnormalized distributions. Now, finally, I am ready to actually answer your question. The reason it is important to learn how to sample from any nonnegative function we may write down (unnormalized probability distribution) is that real life phenomena are very complex. We model that complexity with a statistical model. Whether with a Monte Carlo simulation or a Bayesian statistical model (e.g. Hierarchical Model), at the end of the day we are interested in some random variable that more often than not has a very complicated probability distribution. To learn about our random variable of interest, we need to be able to sample from its complicated (likely unnormalized) probability distribution. And that is why sampling methods such as Metropolis-Hastings are so important to all of science. Important enough that it was named one of the 10 most important algorithms of the 20th century.
