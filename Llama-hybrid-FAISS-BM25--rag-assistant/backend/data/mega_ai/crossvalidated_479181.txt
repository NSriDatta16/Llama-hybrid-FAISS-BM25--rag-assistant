[site]: crossvalidated
[post_id]: 479181
[parent_id]: 46368
[tags]: 
Hyperparameters optimization or parameters tuning is used to find the best hyperparameters sklearn hyperparameters optimization that are parameters that are not directly learnt within estimators. They are passed as arguments to the constructor of the estimator classes. Typical examples include C, kernel and gamma for Support Vector Classifier, alpha for Lasso, etc. Model selection or model comparison model selection is to search the best model with high generalization ability (low generalization error) for your datasets. For large datasets, it usually divide datasets into two parts: training data and test data(it is similar to Kaggle competition ). So, the training data is fitting by the model to learning the pattern, and test data is used to evaluate the model generalization ability. However , hyperparameters are very important to the model learning ability (like XGBoost). It needs to search the optimal hyperparameters combination. It is time to use K-fold cross validation to find the optimal hyperparameters( GridsearchCV , RandomSearchCV ), beacause the one fold in cross validation can be used as validation data to evaluate the model in corresponding hyperparameters combination. Therefore, the training data is used to tune hyperparameters and fit the modle, the test data is used to calculate the generalization ability in the optimal hyperparameters combination and compare different models. for small datasets (the large datasets or small datasets refers to the size of samples from your study field), the training data and test data are not advisable. To calculate or compare the generalization ability of each model. It is recommanded to use k-fold cross validation which can compare the generalization ability in the whole datasets. However, the question is how to choose the optimal hyperparameters. The nested cross validation is to use. It is understood that the K-1 fold data is as training data and left fold is as test data. to find the optimal hyperparameters. the k-1 fold data (training data like 3. ) is used to hyperparameters optimazition and left fold to compare the models. It is called nested cross validation. I think the result of K-fold cross validation can be stored with the predicted value and corresponding observed value in the .csv and then handle them for your task.
