[site]: crossvalidated
[post_id]: 556274
[parent_id]: 
[tags]: 
Computing standard deviation from bursts of data

This is my first question on CrossValidated Exchange! I seek help and pointers on proper terminology or other references, with respect to the following challenge I have. I have a time series of data that represents distance measurements in meters (taken with a laser). The data is produced with a mechanism that is outside of my control. This means that I do not have knowledge of the sampling time between each measurement. However, I know that the data is collected in "bursts"; a sample of the data is provided in the chart below: From the data we observe, it is clear that each burst is centered around a different mean value, but the standard deviation of the measurements within each burst seems to be more or less the same for all bursts. Question: How can I compute the standard deviation of all the measurements by removing the "bias" that the mean value of each burst is centered about? Would it be scientifically valid to compute the mean of the standard deviations of each burst? If yes, how can I properly group those burst in order to compute the "local" standard deviation of each burst and then average them out? How is this problem called? My background is in aerospace engineering, but my statistics may be rusty at least, Thank you in advace for your help! Kind regards, Emmanuel
