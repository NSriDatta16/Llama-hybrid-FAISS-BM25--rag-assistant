[site]: crossvalidated
[post_id]: 600337
[parent_id]: 
[tags]: 
Can there be 3 initial weights for 2 inputs in a backpropagation network?

I am fairly new to machine learning and Neural Network. I was given a scenario where there is a 2-input single unit backpropagation Neural Network has 3 initial weights. The inputs are x and y. The weights are WO, Wa, Wb. To my prior knowledge, each weight is connected to each input. Thus, a 2-input neural network often has x connected to Wa and y connected to Wb. This makes the net, Z = x * Wa + y * Wb + bias. Can there be three weights (W0, Wa, Wb) with two inputs (x, y)? If there is, how would the network architecture and the mathematics behind it look like?
