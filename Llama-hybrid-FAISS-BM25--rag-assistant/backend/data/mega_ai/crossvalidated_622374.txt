[site]: crossvalidated
[post_id]: 622374
[parent_id]: 
[tags]: 
Least squares with multiple outputs but one coefficient per example

According to Elements of Statistical Learning Ch 8.8, we can apply least sqaures at the population level to show that for a regression ensemble $f_1(x), f_2(x), \ldots , f_M(x)$ where $f_j: \mathbb{R}^p \to \mathbb{R}$ the full ensemble has smaller error than any single model: $$ \mathbb{E}[y - \sum_{j=1}^{M}\hat{w}_jf_j(x)]^2 \leq \mathbb{E}[y - f_j(x)]^2 \hspace{0.25cm} \forall \hspace{0.25cm} j.$$ Where this is shown my simply solving the standard least squares problem that $$ \hat{w} = \mathop{\arg \min}\limits_w \mathbb{E} [ y - \sum_{j=1}^{M} w_{j} f_{j}(x) ] ^2. $$ Suppose now we are interested in the multi-class problem where $y \in \mathbb{R}^k$ and $f(x) \in \mathbb{R}^p$ but we still have only $M$ weights $w_1, \ldots, w_M$ . Does an equivalent result hold? Could one derive a similar result for average least squares or negative log likelihood losses?
