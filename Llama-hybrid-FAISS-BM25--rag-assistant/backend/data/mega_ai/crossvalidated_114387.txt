[site]: crossvalidated
[post_id]: 114387
[parent_id]: 114375
[tags]: 
Some quick answers... It means basically nothing to compare the values of the regression coefficients unless the predictors are standardized and the model is specified correctly, especially when the predictors are inter-correlated (which is definitely the case - look at the warning at the bottom of the output). Just see what happens to the coefficients if you drop one of the predictors from the model. Chances are, one or more of them changes radically, possibly even changing sign. Generally speaking, the coefficients tell you about the additional contribution of each variables, given the others in the model. You can assess the strength of each variable's contribution by the absolute value of each $t$ statistic. The one with the greatest $\lvert t\rvert$ makes the greatest contribution. It can be shown that the $t$ statistic, squared, is equal to the $F$ statistic based on the model-r eduction principle, whereby you remove one predictor from the model and measure how much the $SSE$ increases. If it increases a lot, then that predictor must be pretty important because including it accounts for a lot of unexplained variation. The F statistics are all proportional to those $SSE$ changes, so the one with the biggest $|t|=\sqrt{t^2} = \sqrt{F}$ is the one that makes the most difference. You haven't dropped anything; you have just chosen a parameterization. You will obtain exactly the same predictions regardless of which indicator is dropped. The interpretation of each regression coefficient is that it is the amount by which the prediction changes from the prediction obtained for the category whose indicator was dropped. To get a better idea of relative weights, I suggest using, instead of $k-1$ indicators, the variables $x_1=I_1-I_k, x_2=I_2-I_k,...,x_{k-1}=I_{k-1}-I_k$ where the $I_i$ are the indicators. The coefficient $b_i$ of of $x_i$ is then an estimate of the effect of the $i$ th category minus the average of all $k$ of them; and you can obtain the analogous effect for the $k$ th level by the fact that $b_1+b_2+\cdots+b_k=0$ , thus $b_k=-(b_1+b_2+\cdots b_{k-1})$ . The variables $x_i$ are called sum-to-zero contrasts (in R, you get them using "contr.sum" , but it doesn't look like that's what you're using).
