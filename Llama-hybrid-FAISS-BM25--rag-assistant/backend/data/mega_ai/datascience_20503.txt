[site]: datascience
[post_id]: 20503
[parent_id]: 18572
[tags]: 
A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. This assumption is weak and in many cases may not hold. For example, imagine that we have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. You can find a lot of methods in the literature. Some of them are trying to tackle the difference in the marginal distributions p(X) of the train and test set, while others are trying to tackle the conditional distributions p(y|x). Some methods are taking into account both differences among the train and test set distributions. Almost all methods are trying to bring the two distributions closer, using weights on the train instances (importance sampling) so that the new weighted train set will be close to the test set. Other methods transform the feature spaces so that the distributions the train and test sets will be close enough. For more information about this topic you can check the survey Pan, Sinno Jialin, and Qiang Yang. "A survey on transfer learning." IEEE Transactions on knowledge and data engineering 22, no. 10 (2010): 1345-1359.
