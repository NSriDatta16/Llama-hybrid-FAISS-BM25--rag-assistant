[site]: crossvalidated
[post_id]: 562170
[parent_id]: 562161
[tags]: 
a) what should I do and what are the pitfalls/points that I should make sure to keep them aware? In my opinion, you should do what they want. b) Is there anything important that I should highlight them? It's a bad idea to focus on accuracy. From what you said in the comments, it seems that they want a measure of how well the model ranks cases, in which case you should use AUC or a similar measure. Accuracy is usually misleading anyway. c) Should this project still be dropped if business is okay to be with 50% acc? Can we continue to use this model as long as business is fine with it? Why not? It's not your decision. It's up to the business. d) Any real time experience from your model deployment decisions? It sounds like this business might have been burned by data scientists before. They hire someone who treats the task like a Kaggle competition and takes weeks to build the best model they can. Or, they hire a statistician, who spends the entire time whining that the data isn't good enough (following Fisher's maxim about "what the experiment died of") and doesn't do anything at all. Most businesses don't really want that. They want to get a minimal working prototype into production as quickly as possible. Then it can be improved later. Or maybe the situation is hopeless, in which case they can make the decision to give up quickly. I worked with one guy who spent his time building an extremely complicated Hidden Markov Model. That's what he did. I worked with him for five years. In that time, as far as I am aware, he didn't put a single model into production. He just came to the office every day and added nodes to his model. Now he's in another company, managing data science teams. Please don't be like him.
