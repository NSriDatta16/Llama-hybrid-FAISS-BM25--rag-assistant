[site]: crossvalidated
[post_id]: 384189
[parent_id]: 384180
[tags]: 
TL;DR It depends on the algorithm. In some cases it won't make a difference, in other it will be recommended, yet in other cases it may be harmful. If you use algorithms that are based on linear combinations of features ( $\beta_1 f_1 + \beta_2 f_2 + \dots$ ), like generalized linear models (including linear regression, logistic regression), or densely connected neural networks, then it doesn't make sense to include variables like $f_4 = f_1 + f_2$ , since this is already a build-in feature of the model. Moreover, you would get into problems with multicollinearity (by definition, such feature is linearly dependent on others). Including features such as $f_5 = f_1 f_2$ means using interaction terms and is commonly used in statistics and $f_3 = f_1 / f_2$ is just an interaction in disguise . On another hand, with algorithms like random forest you could use exactly the same variable multiple times and it wouldn't matter, since what decision trees do is they look at one variable at a time to make a split. Adding such features when using random forest may make sense and simplify the task for the algorithm .
