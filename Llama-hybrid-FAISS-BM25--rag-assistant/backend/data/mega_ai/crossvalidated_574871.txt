[site]: crossvalidated
[post_id]: 574871
[parent_id]: 
[tags]: 
How to obtain Key, Value and Query in Attention and Multi-Head-Attention

I am currently trying to get the hang of BERT and Transformers, so I worked through the Paper "Attention Is All You Need" . Now I have a hard time understanding how the Key-, Value-, and Query-Matrices for the attention mechanism are obtained. The paper itself states that: all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. In addition, this tutorial taught me that the inputs (i.e. the outputs from the previous layer) are transformed into $K$ , $V$ , and $Q$ through three separate matrix multiplications - I found that nowhere in the paper, but it makes sense. With Multi-Head-Attention, I understand that the inputs are each mapped into several low-dimensional representations. My question now is: In a Multi-Head-Attention-Layer, are there first three weight matrices that map the entire input into $K$ , $V$ and $Q$ , and then subsequently the transformations into the lower dimensions for each head? Or can the first step be omitted and the inputs be directly transformed into the multi-head-representations? Edit to specify my question: In the paper the multi-head-attention is defined with the following formula: $ head_i = Attention(QW_i^Q, KW_i^K, VW_i^V) $ . In this context, are $K$ , $V$ and $Q$ identical matrices (the previous layer's output), or are they the result of three separate linear transformations on the previous layer's output?
