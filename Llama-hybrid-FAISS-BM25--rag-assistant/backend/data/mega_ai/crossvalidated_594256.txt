[site]: crossvalidated
[post_id]: 594256
[parent_id]: 
[tags]: 
Confusion about joint probability distribution in Bayesian Inference setup

I am confused by a simple fact but I can't solve my head around it! It's known that: $P(y,\theta) = P(y|\theta)*P(\theta)$ and $P(y,\theta) = P(\theta|y)*P(y)$ Giving us: $P(\theta|y)*P(y) = P(y|\theta)*P(\theta)$ (both valid probability density functions) But in bayesian inference literature we learn that $P(y|\theta)*P(\theta)$ is not a probability density function because $P(y|\theta)*P(\theta)$ when $P(y|\theta)$ as a likelihood does not sum to 1, needing the $P(y)$ as a denominator to normalize it and give a valid posterior probability density function. So, my confusion is: is the joint density function NOT a joint PROBABILITY density function in $P(y,\theta) = P(y|\theta)*P(\theta)$ since the right hand side does not sum to 1?
