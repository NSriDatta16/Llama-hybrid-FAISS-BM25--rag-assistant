{\displaystyle X_{i}} and the i th {\displaystyle i^{\text{th}}} output label is y i ∈ R {\displaystyle y_{i}\in \mathbb {R} } , one can tackle the distribution regression task by taking the embeddings of the distributions, and learning the regressor from the embeddings to the outputs. In other words, one can consider the following kernel ridge regression problem ( λ > 0 ) {\displaystyle (\lambda >0)} J ( f ) = 1 ℓ ∑ i = 1 ℓ [ f ( μ X i ^ ) − y i ] 2 + λ ‖ f ‖ H ( K ) 2 → min f ∈ H ( K ) , {\displaystyle J(f)={\frac {1}{\ell }}\sum _{i=1}^{\ell }\left[f\left(\mu _{\hat {X_{i}}}\right)-y_{i}\right]^{2}+\lambda \|f\|_{{\mathcal {H}}(K)}^{2}\to \min _{f\in {\mathcal {H}}(K)},} where μ X ^ i = ∫ Ω k ( ⋅ , u ) d X ^ i ( u ) = 1 N i ∑ n = 1 N i k ( ⋅ , X i , n ) {\displaystyle \mu _{{\hat {X}}_{i}}=\int _{\Omega }k(\cdot ,u)\,\mathrm {d} {\hat {X}}_{i}(u)={\frac {1}{N_{i}}}\sum _{n=1}^{N_{i}}k(\cdot ,X_{i,n})} with a k {\displaystyle k} kernel on the domain of X i {\displaystyle X_{i}} -s ( k : Ω × Ω → R ) {\displaystyle (k:\Omega \times \Omega \to \mathbb {R} )} , K {\displaystyle K} is a kernel on the embedded distributions, and H ( K ) {\displaystyle {\mathcal {H}}(K)} is the RKHS determined by K {\displaystyle K} . Examples for K {\displaystyle K} include the linear kernel [ K ( μ P , μ Q ) = ⟨ μ P , μ Q ⟩ H ( k ) ] {\displaystyle \left[K(\mu _{P},\mu _{Q})=\langle \mu _{P},\mu _{Q}\rangle _{{\mathcal {H}}(k)}\right]} , the Gaussian kernel [ K ( μ P , μ Q ) = e − ‖ μ P − μ Q ‖ H ( k ) 2 / ( 2 σ 2 ) ] {\displaystyle \left[K(\mu _{P},\mu _{Q})=e^{-\left\|\mu _{P}-\mu _{Q}\right\|_{H(k)}^{2}/(2\sigma ^{2})}\right]} , the exponential kernel [ K ( μ P , μ Q ) = e − ‖ μ P − μ Q ‖ H ( k ) / ( 2 σ 2 ) ] {\displaystyle \left[K(\mu _{P},\mu _{Q})=e^{-\left\|\mu _{P}-\mu _{Q}\right\|_{H(k)}/(2\sigma ^{2})}\right]} , the Cauchy kernel [ K ( μ P , μ Q ) = ( 1 + ‖ μ P − μ Q ‖ H ( k ) 2 / σ 2 ) − 1 ] {\displaystyle \left[K(\mu _{P},\mu _{Q})=\left(1+\left\|\mu _{P}-\mu _{Q}\right\|_{H(k)}^{2}/\sigma ^{2}\right)^{-1}\right]} , the generalized t-student kernel [ K ( μ P , μ Q ) = ( 1 + ‖ μ P − μ Q ‖ H ( k ) σ ) − 1 , ( σ ≤ 2 ) ] {\displaystyle \left[K(\mu _{P},\mu _{Q})=\left(1+\left\|\mu _{P}-\mu _{Q}\right\|_{H(k)}^{\sigma }\right)^{-1},(\sigma \leq 2)\right]} , or the inverse multiquadrics kernel [ K ( μ P , μ Q ) = ( ‖ μ P − μ Q ‖ H ( k ) 2 + σ 2 ) − 1 2 ] {\displaystyle \left[K(\mu _{P},\mu _{Q})=\left(\left\|\mu _{P}-\mu _{Q}\right\|_{H(k)}^{2}+\sigma ^{2}\right)^{-{\frac {1}{2}}}\right]} . The prediction on a new distribution ( X ^ ) {\displaystyle ({\hat {X}})} takes the simple, analytical form y ^ ( X ^ ) = k [ G + λ ℓ ] − 1 y , {\displaystyle {\hat {y}}{\big (}{\hat {X}}{\big )}=\mathbf {k} [\mathbf {G} +\lambda \ell ]^{-1}\mathbf {y} ,} where k = [ K ( μ X ^ i , μ X ^ ) ] ∈ R 1 × ℓ {\displaystyle \mathbf {k} ={\big [}K{\big (}\mu _{{\hat {X}}_{i}},\mu _{\hat {X}}{\big )}{\big ]}\in \mathbb {R} ^{1\times \ell }} , G = [ G i j ] ∈ R ℓ × ℓ {\displaystyle \mathbf {G} =[G_{ij}]\in \mathbb {R} ^{\ell \times \ell }} , G i j = K ( μ X ^ i , μ X ^ j ) ∈ R {\displaystyle G_{ij}=K{\big (}\mu _{{\hat {X}}_{i}},\mu _{{\hat {X}}_{j}}{\big )}\in \mathbb {R} } , y = [ y 1 ; … ; y ℓ ] ∈ R ℓ {\displaystyle \mathbf {y} =[y_{1};\ldots ;y_{\ell }]\in \mathbb {R} ^{\ell }} . Under mild regularity conditions this estimator can be shown to be consistent and it can achieve the one-stage sampled (as if one had access to the true X i {\displaystyle X_{i}} -s) minimax optimal rate. In the J {\displaystyle J} objective function y i {\displaystyle y_{i}} -s are real numbers; the results can also be extended to the case when y i {\displaystyle y_{i}} -s are d {\displaystyle d} -dimensional vectors, or more generally elements of a separable Hilbert space using operator-valued K {\displaystyle K} kernels. Example In this simple example, which is taken from Song et al., X , Y {\displaystyle X,Y} are assumed to be discrete random variables which take values in the set 