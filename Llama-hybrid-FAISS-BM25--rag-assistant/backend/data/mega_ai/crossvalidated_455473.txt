[site]: crossvalidated
[post_id]: 455473
[parent_id]: 
[tags]: 
Tune, validate a model and perform feature selection

I am developing a classification model on some time related dataset. I would like to ask if my procedure for validating my classifier is correct and unbiased. In the single validation split, what I would do is 1a. split my data in training, validation and test set. For example, if I have 24 months of data, I would take the last 2 months as test set and the previous 2 months as validation set. 2a. perform feature selection on the training set + validation set: fit a given model on the training set and construct predictions on the validation set. Given such predictions, we obtain a validation score and the top, say, 50 features (by using feature importance provided that the classifier is tree based). 3a. Repeat 2a for several models, pick the one giving the best validation score and use the corresponding top 50 features. We train the best model with top 50 features on the training + validation set and report its score on the Test set. In the cross validation approach, 1b. I perform a training - test set split of the dataset. Again suppose the test set contains the last 2 months of the data. 2b. Perform a time based cross validation split (see https://robjhyndman.com/hyndsight/tscv ) 3b. Iterate 2a. for every cv fold, and average out the Validation scores of the folds and the feature ranking of every fold (for instance via voting). 4b. Repeat 3b for several models, pick the one giving the best CV score and use the corresponding top 50 features. We train the best model with top 50 features on the full training and report its score on the Test set. Is this correct and unbiased? I am aware that in 2a one could perform a further validation split to select features, but I am trying to understand what the simplest approach could be. Edit: I am aware this question has been posted several times in different forms. Any input is welcome. Thank you for your help.
