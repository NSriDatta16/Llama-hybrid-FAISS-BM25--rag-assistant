[site]: datascience
[post_id]: 55267
[parent_id]: 
[tags]: 
Why does my cost either increase or go down very slowly

def hypothesis(W, B, X): Z = np.dot(W.T, X) + B return sigmoid(Z) def sigmoid(Z): return(1/(1+np.exp(-Z))) def cost(A, Y): return (-Y * np.log(A) - (1 - Y) * np.log(1 - A)).mean() def derivatives(W, B, X, Y): A = hypothesis(W, B, X) return np.dot(X.T, (A - Y)) / Y.shape[0] , (A - Y) / Y.shape[0] def updateParameters(W, B, X, Y, learning_rate): # print(W.shape, X.shape, Y.shape ) dW, dB = derivatives(W, B, X, Y) W -= learning_rate * dW B -= learning_rate * dB return W, B def LogisticRegression(X, Y): W = np.zeros((X.shape[0], 1)) B = np.zeros((X.shape[0], 1)) for i in range(0, 1000000): print(cost(hypothesis(W, B, X), Y)) W, B = updateParameters(W, B, X, Y, 0.0000003) LogisticRegression(X, Y) I'm a beginner trying to implement logistic regression from scratch. However, my cost always goes up, and when I reduced my learning rate in case that was too high, it started going down but only extremely slowly. Could someone point out if there is any issue in the implementation of the algorithm? Any help would be greatly appreciated
