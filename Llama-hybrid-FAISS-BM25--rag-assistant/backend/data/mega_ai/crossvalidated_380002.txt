[site]: crossvalidated
[post_id]: 380002
[parent_id]: 379963
[tags]: 
Often it is assumed that $Y_i$ is a random variable with expected value $a+bx_i$ and variance $\sigma^2>0$ and covariances $\operatorname{cov}(Y_i,Y_j)=0$ for $i\ne j.$ The parameters $a,b,\sigma$ are to be estimated based on the data and $x_i$ are observed rather than to be estimated. Notice that the above attributes no randomness to $x_i.$ Sometimes $x_i$ are fixed by design, so they have no randomness. In that case, if a new random sample of $n$ observations $Y_i,\,i=1,\ldots,n$ is taken, independently of the first sample of $n$ observations, the $Y_i$ change but the $x_i$ do not. However, sometimes in practice a new sample would alter both $x_i$ and $Y_i$ . In that case, often the same assumptions as in the first paragraph above are made. This may be justified by the fact that what is of interest is the conditional distribution of the $Y_i$ given the $x_i.$
