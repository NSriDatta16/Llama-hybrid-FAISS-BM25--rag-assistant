[site]: crossvalidated
[post_id]: 589944
[parent_id]: 
[tags]: 
What is the information plane theorem for an autoencoder neural network?

Slide 8 (about 19 minutes into the video) of the Stanford Seminar - Information Theory of Deep Learning, Naftali Tishby has the following (rather informally stated) theorem. Theorem (Information Plane) For large typical $\mathbf{X}$ , the sample complexity of a DNN is completely determined by the encoder mutual information $\mathbf{I(X;T)}$ , of the last hidden layer; the accuracy (generalization error) is determined by the decoder information, $\mathbf{I(T;Y)}$ , of the last hidden layers. I am having difficulty following what is meant by "the sample complexity is completely determined". What is the precise statement of this theorem?
