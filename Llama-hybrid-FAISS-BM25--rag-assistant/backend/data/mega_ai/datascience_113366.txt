[site]: datascience
[post_id]: 113366
[parent_id]: 113359
[tags]: 
Although a definitive answer can only be obtained by actually trying it and it would depend on the specific task where we evaluate the resulting model, I would say that, in general, no , it would not improve the results to remove stopwords and punctuation. We have to take into account that the benefit of BERT over more traditional approaches is that it learns to compute text representations in context. This means that the representations computed for a word in a specific sentence would be different from the representations for the same word in a different sentence. This context also comprises stopwords, which can very much change the meaning of a sentence. The same goes for punctuation: a question mark can certainly change the overall meaning of a sentence. Therefore, removing stopwords and punctuation would just imply removing context which BERT could have used to get better results. That is why you will not see stopwords being removed for deep learning approaches applied to tasks where language understanding is key for success. Furthermore, while blindly doing stopword removal has been a "tradition" in tasks like topic modeling, its usefulness is beginning to be questioned even for those tasks in recent research . regarding tokenizer BERT has a word-piece vocabulary that was learned over the training corpus. I don't think removing tokens manually is an option here, given that they are word pieces and you wouldn't know in which words they may be used. It would be possible, however, to identify low frequency tokens by encoding and large vocabulary and then remove the lowest frequency ones. Nevertheless, BERT's vocabulary has almost 1000 (purposefully) unused tokens, so there is room to remove unused tokens if that's what you want. I don't think it would make a difference, though.
