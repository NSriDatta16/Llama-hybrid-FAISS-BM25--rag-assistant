[site]: crossvalidated
[post_id]: 587096
[parent_id]: 587074
[tags]: 
If you just want to have a scale in $[0,1]$ to allow for a straight interpretation that, say, 55% of the maximum was achieved, the absolute min and max should be used rather than the observed. This may be helpful for data presentation. This would not be achieved by using min and max in the data as then future data could lead to values outside the $[0,1]$ range. This however does not mean that different scales constructed in different ways that are normalised in this way can be compared, as one may have a maximum that corresponds to an ideal state that in reality can be hardly ever achieved, and existing observations may therefore end up with a max of 0.8 or so, whereas another scale may be constructued so that real existing democracies end up with a value of 1. If you have different indexes you can plot them against each other (after having transformed them both to $[0,1]$ ) to see this. In such a case I don't think straight comparability can be achieved by any kind of mathematical normalisation, as different scales by construction measure at least slightly different things. One can study and try to explain the differences, but this requires subject matter knowledge and not only numerical comparison (although it could be of interest if there is a very strong connection between two indexes; they may turn out to be approximately equal, or may show a regression slope other than 1 between them, or even nonlinearity). Standardisation/normalisation is also often used when aggregating information from different scales, like constructing a single new scale out of several old ones. In that case it can be argued that data dependent normalisation could be more appropriate, because if you want to define a formal rule without a tedious discussion of the specific indexes (that may not imply how to normalise them anyway), the variation in the observed data is informative. In this case the min/max observed in the data may be used, arguing for example that if the realistically achievable maximum of one index is 0.8 (even though it theoretically goes up to 1) whereas the other one in reality reaches values up to 1, it may be appropriate to reflect the variation of the data better if 0.8 of the first index is transformed to 1 (correspondingly if the index range is 2-12 or whatever). In that case however one may want to use more information about the distribution than just the minimum and the maximum, and standardisation to mean 0 standard deviation 1 (or robust alternatives) may be better, see the answer of @user2974951 (whether it actually is better is hard to say and depends on a number of considerations that partly again refer to the subject matter). Also keep in mind that many statistical methods are invariant or equivariant against linear transformations, meaning that the results, if interpreted correctly, do not depend on whether and how you transform your variables. Invariance means that results do not change, equivariance means that they change in mathematically appropriate ways and can be transferred from one transformation to another without actually repeating the analysis. This holds for example for correlations and PCA computed on correlations (invariant against linear transformations of the individual variables), and linear regression (equivariant) if you know how (and how not) to interpret the coefficients. Certain other techniques (such as $k$ -means clustering) are not equivariant and require comparable scales and therefore normalisation/standardisation at least as long as the different amounts of variation in different variables do not indicate their importance for the task of interest.
