[site]: datascience
[post_id]: 57982
[parent_id]: 
[tags]: 
Autoencoder features very different after each training

I trained a convolutional (3D) autoencoder on about 350 volumes. The reconstruction of the volumes by the decoder is really nice and there is nearly no difference between the input volumes (images) and the output. My intention was to use the autoencoder for feature learning, extract the features after the encoder and use local sensitivity hashing (LSH) for a nearest neighbors search. I now trained the autoencoder five times in a row using the same dataset split to gain information about the model sensitivity. Iam aware of that there is always some randomness when training a model, nevertheless when applying the nearest neighbors search, each model finds totally different neighbors and it seems that each autoencoder learns totally different features. The loss during training (MSE) varies between 2.5e-05 to 3.5e-05, which seems ok. I would have expect the features of the models to be aproximately the same and I also would have expect to find the same neighbors. What am I doing wrong and why are the features so different? Thanks in advance for any advice, cheers, Michael
