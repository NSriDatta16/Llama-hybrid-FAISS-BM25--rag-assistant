[site]: crossvalidated
[post_id]: 592747
[parent_id]: 592535
[tags]: 
The problem often lies not with the models themselves, but with how they are applied and with how people make decisions. Too often I see the same pattern in companies starting with ML/Data Science: They face a hard technical problem. Someone high-ish up the management chain figures out it might be solvable by Data Science because everything is these days... Alternatively, they just catch on the hype and do not even have a problem formulation, just "more Data Science more better". They hire someone, very rarely a full team (or, if you are lucky, they actually outsource the problem). The new hire is able to come up with a proof-of-concept which performs better than all the old models on a small subset of test data. They are eager to deploy it. They want deliverables and start using "common sense" to rationalize how the model works. The added bias tends to downplay failures and emphasize successes - even if the model itself is terrible, it may be hailed as one of the greatest achievements ever by someone on C suite just because it is "AI". And then the issue is that with many statistical models the engineer can reason about the limits of their applicability fairly well. We have a decently developed framework for examining statistical models by now, and while the similar level of rigor is achievable in theory with ML/AI, it is seldom done so in practice. Also, crucially, the cost analysis is often performed poorly or even omitted entirely. That is how you have your tale of two bridges: one could factor in the cost of it failing spectacularly and so on but take a guess what ends up being done in practice? Execs taking over the "undecisive" engineer who is "too dumb" to take advantage of their competence, and the cost-saving measure gets rolled out. And if we are talking technical execs, many of them are engineers who will try to bridge the explanatory gap using their experience and intuition, neither of which is necessarily applicable to ML. Very basically, their reasoning would go like This model is smarter than all the old and dull models we have used before. Old models were well-regularized and you could extrapolate their behavior well. Ergo, this new and smart model will not freak out when encountering something it has never seen before. So there you have it - of course, predictive models can predict individual outcomes and even be fairly efficient at that. The problem is that the resulting models are opaque, hard to reason about, and people making decisions often will be incompetent about statistics and ML. A LOT of work data scientists/statisticians/engineers do ends up being communication and ensuring their models are applied properly - and it is just a lot easier to communicate with "classical" models. When something gets done more efficiently, it is not necessarily good, either - if this something was erroneous to begin with (remember Knight Capital?..).
