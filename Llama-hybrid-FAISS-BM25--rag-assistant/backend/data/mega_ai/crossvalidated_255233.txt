[site]: crossvalidated
[post_id]: 255233
[parent_id]: 255230
[tags]: 
It's not guaranteed. As you say, the ensemble could be worse than the individual models. For example, taking the average of the true model and a bad model would give a fairly bad model. The average of $k$ models is only going to be an improvement if the models are (somewhat) independent of one another. For example, in bagging, each model is built from a random subset of the data, so some independence is built in. Or models could be built using different combinations of features, and then combined by averaging. Also, model averaging only works well when the individual models have high variance. That's why a random forest is built using very large trees. On the other hand, averaging a bunch of linear regression models still gives you a linear model, which isn't likely to be better than the models you started with (try it!) Other ensemble methods, such as boosting and blending, work by taking the outputs from individual models, together with the training data, as inputs to a bigger model. In this case, it's not surprising that they often work better than the individual models, since they are in fact more complicated, and they still use the training data.
