[site]: crossvalidated
[post_id]: 613469
[parent_id]: 
[tags]: 
What the context window size in pre-train gpt-2

I have questions about GPT-2. What is the context window in the pre-train gpt-2 train? As I know language modeling is based on gpt-2 use prefix language modeling that is demonstrated in picture form T5 model below: And the formula of gpt model indicate below : So I would like to know what the number of K(context size)? that the initial first stage for training in pre-train GPT-2? If GPT is not prefix language modeling, then what is it?
