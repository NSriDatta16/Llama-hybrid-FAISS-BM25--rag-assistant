[site]: crossvalidated
[post_id]: 466105
[parent_id]: 
[tags]: 
Calculating three average precisions and a single value for ROC from raw predicted class outputs

I'm not a statistician or mathematician so I apologize if I use any terms incorrectly. Please do point out any errors in my use of terminology. The four values I need are the equivalent of Weka's ROC and PRC outputs. Those are technically 6 values in Weka: positive class ROC positive class PRC negative class ROC negative class PRC weighted average class ROC weighted average class PRC This is an unrelated Weka example for illustration purposes: === Detailed Accuracy By Class === TP Rate FP Rate Precision Recall F-Measure MCC ROC Area PRC Area Class 0.965 0.087 0.961 0.965 0.963 0.881 0.989 0.995 no 0.913 0.035 0.922 0.913 0.918 0.881 0.989 0.979 yes Weighted Avg. 0.949 0.070 0.949 0.949 0.949 0.881 0.989 0.990 However the 3 ROC values always seem to be identical. All the values are single values not curves. I have worked out that the PRC value is the average precision. I am making an educated guess that the ROC value is AUROC. I have looked hard at many webpages on this site and others but I can't find what I need: simple step-by-step instructions for what to do for each value in turn. I have also looked at the Weka source code for the Evaluation class but I can't understand it. I'll put the questions here to make them more prominent. Question 1 Does the approximate 2:1 ratio of classes make this an imbalanced problem? Question 2 I have seen that python's scikit-learn can calculate average precision and AUROC. However those python methods require an input vector of probabilities. What steps must I perform to calculate the three vectors of probabilities from the algorithm's single output vector? I am assuming it's three vectors of probabilities: one each for the positive class, the negative class, and the weighted average. Background I am working on a binary classification problem. One class is approximately twice the size of the other. I have 302 instances. Each instance has 40 input attributes and they have all been converted to a 0 or 1. Separately I know the truth about which class each instance actually is. I have a simple classification algorithm that predicts the class entirely on whether the sum of the input attributes is below a fixed arbitrary threshold. The threshold is fixed throughout one run of the algorithm. However I want to generate all the average precisions and AUROCs for all 40 possible thresholds separately. This is because in reality the truth is more complex than just a threshold. As an example, abbreviated to 9 instances: truth = [1 0 0 0 0 1 1 0 0] threshold_13_predictions = [1 0 1 0 0 1 1 0 1] threshold_14 predictions = [0 0 1 0 0 0 1 0 1] I need a step-by-step guide on how to turn each threshold prediction output vector into the vector of probabilities to pass to Scikit-Learn's sklearn.metrics.roc_auc_score and sklearn.metrics.average_precision_score methods.
