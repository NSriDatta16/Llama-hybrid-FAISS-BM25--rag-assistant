[site]: datascience
[post_id]: 82875
[parent_id]: 12532
[tags]: 
A couple of papers have been published showing – and conventional wisdom in 2020 seems to still be persuaded— that, as Yann LeCun put it, large batches are bad for your health. Two relevant papers are Revisiting Small Batch Training For Deep Neural Networks, Dominic Masters and Carlo Luschi which implies that anything over 32 may degrade training in SGD. and On Large-batch Training For Deep Learning: Generalization Gap And Sharp Minima which offers possible reasons. To paraphrase badly, big batches are likely to get stuck in local (“sharp”) minima, small batches not. There is some interplay with choice of learning rate.
