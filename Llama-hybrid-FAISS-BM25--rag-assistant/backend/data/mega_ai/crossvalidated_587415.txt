[site]: crossvalidated
[post_id]: 587415
[parent_id]: 587411
[tags]: 
R Code So I will explain what the code is first. For a simple regression, the lm function just runs a regression with the left-most part of the formula as the y or outcome variable, and the right part of the formula as your predictors. The data argument in the function is self-explanatory. So if we wanted to predict how much age influences anger, we could model that as: lm(formula = anger ~ age, data = data) For a multiple linear regression, you simply add predictors. If we added "tolerance for angry situations" or something fuzzy like that, our model would look like this in R: lm(formula = anger ~ age + tolerance_anger, data = data) Coefficients Now to your point estimates in the screenshot you shared. He ran two multiple linear regressions, one for CNN and one for Fox. I will focus on just the Fox model. The "intercept" estimate here is the y-intercept, which is where your regression line is fixed to make a line. More than that, it is a conditional mean value for whatever the Q8 variable is. So when all other factors are set to zero, your mean value of Q8 is 5.16. This of course isn't the actual mean of the y variable here, it is only the conditional mean that is adjusted per coefficient. Now all the other coefficients are simply adjustments to this mean. For Party ID Lean, when all other variables are set to zero, a 1 unit increase in Party ID Lean will lead to a -1.67 decrease in Q8. 2 unit increase will lead to -3.34. 3 units lead to -5.01. And so on. So if all your other variables are set to zero, and somebody's Party ID Lean is 2, the conditional mean of Q8 will become 1.82. Obviously that is a simplification. The variables will all have different combinations. The coefficients simply dictate what Party ID Lean should be when different values of each predictor are included into the model. The importance of these coefficients can be better understood in terms of how they can help us predict data. Lets say I ran this regression using R's iris dataset, which measures flower petals and sepals: iris.model And then I got new batch of flowers with a sepal width of 2 and a sepal length of 4: new.data I can then predict what the y, or petal length should be with my coefficients: predict(iris.model, newdata = new.data) Running it, my model says that this flower should have a petal length close to 1.90: 1 1.900362 R-Square You will notice there is also a printout for multiple R square. This is how much variance is explained by the model. However, including more predictors into the model almost necessarily inflates the R2 value, so most multiple linear regression models also include adjusted R2 , which penalizes the model for being too complex. In your case the multiple R2 and adjusted multiple R2 is nearly the same. So we can say that your model explains about 54% of the variance in Q8. Though this is for a simple linear regression, this plot shows what r-square looks like: how closely your data matches the regression model: Residuals The residual print out on the bottom tells you how much your actual data deviates from the "best fit" regression line to your data. Residual error is basically how badly your regression guesses the actual raw data looks like. The median error is about .11, which is saying on average your model badly guesses by .11 points (which isn't bad). The minimum residual error here says your model sometimes overs predicts as far as -3.38 units of Q8 (a data point falls -3.38 points below the regression line), and the max residual error says the regression line under predicts by 2.44 units (a data point falls 2.44 points above the regression line). To visualize what this actually looks like in practice, you can inspect the plot below, which shows a regression line and the raw data points, with lines traced from the regression line showing how badly the data matches with your line of best fit. Here, you can clearly see what the max residual is, as its the point that is stretching super far away from the line. Other Metrics The F statistic and p value of the model tell you if your model is significant, however, this doesn't speak to the individual predictors at all. If you look at the p values listed at the far right of the table for the Fox model, you can see that all the predictors are significant except for Political Knowledge, which is obvious given it's coefficient is extremely small (the e negative signs for p values btw are just scientific notation in R). Your standard errors for each coefficient is also how it sounds...it is on average how badly Q8 is guessed by each predictor in the model. If the standard error is obnoxiously high, this will tell you if you have poor predictors or there is something wrong with how you formulated the model. Something missing from this model that would help elucidate how strong your predictors are would be beta weights. They tell you which predictor is the best at explaining variation in Q8. We can use the previous iris regression I saved to calculate beta weights, but first you will need to install and load the lm.beta library to do so. library(lm.beta) lm.beta(iris.model) Running lm.beta will give us this: Call: lm(formula = Petal.Length ~ Sepal.Width + Sepal.Length, data = iris) Standardized Coefficients:: (Intercept) Sepal.Width Sepal.Length NA -0.3305168 0.8328950 We can see here that sepal length is a stronger predictor. 1 standard deviation increase in sepal length will lead to a .83 standard deviation increase in petal length. Equation I think your comments about the "signs" of the coefficients probably have to do with the model equation. The equation with the coefficients should look something like this: $Q8_1Rec = 5.16 - 1.67_\text{Party ID Lean} + .23_\text{College} + .00_\text{Political Knowledge} - .01_\text{Demo Age}$ As a final note, as noted already in the comments, the two models seem to be predicting separate outcomes. So depending on what your outcome variable means for each model, the equivalency between models may vary.
