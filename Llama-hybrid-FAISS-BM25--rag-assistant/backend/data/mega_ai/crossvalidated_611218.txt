[site]: crossvalidated
[post_id]: 611218
[parent_id]: 611057
[tags]: 
Doing what you propose, i.e. adding a regularization term other than the KLD in the loss, is totally feasible. You can find many classical autoencoder architectures which incorporate a diversity of regularization term along with the regularization term, see the wiki entry for autoencoder for example. Now the critical point is that you will not be correct referring to your new model as a VAE because a VAE is strictly derived in the variational Bayes setting by constructing a lower bound on the loglikelihood . If you replace the regularizing KLD term without proper care, you bindly modify the loss and cannot assert that you still have an ELBO whose maximization is at the core of VAE training.
