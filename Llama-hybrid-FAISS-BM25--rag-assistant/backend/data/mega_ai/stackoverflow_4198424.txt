[site]: stackoverflow
[post_id]: 4198424
[parent_id]: 4151815
[tags]: 
Previews are generated in one of two ways: Via normal Googlebot crawls On the fly using the user-agent Mozilla/5.0 (en-us) AppleWebKit/525.13 (KHTML, like Gecko; Google Web Preview) Version/3.1 Safari/525.13 If the content has been crawled recently, it's likely to reuse the normal crawled content. If not, then it'll try to generate it on the fly. Internally, it uses a similar Safari/Webkit-based browser, so that's what you'd probably want to use to check your pages. I believe there's a way to modify the user-agent that Safari uses, which would be one way to do a more specific check. If for some reason the AJAX content can't be fetched (say it's disallowed by robots.txt or the server actively blocks Googlebot from accessing it), then the site should fail gracefully (as it should when users run into the same problem). Don't show an error popup -- just log it internally and fall back to some default content. One way to double-check if it works after you have made changes to your site's template is to do a site:-query to find similar, obscure pages, and to check their preview images; if nobody else has checked the preview images, it will be rendered on the fly so you will see the results right away. Once you're happy with how it renders, it will take a bit of time to catch up and re-render the existing pages; it doesn't update with the same frequency as cached pages or normal snippets do.
