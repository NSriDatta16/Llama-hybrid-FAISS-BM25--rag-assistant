[site]: crossvalidated
[post_id]: 626255
[parent_id]: 626012
[tags]: 
There are many methods in the literature for uncertainty estimation. Ensembles are one standard category of methods. So, yes, training an ensemble of models and then using the distribution of predictions to estimate uncertainty is a very reasonable approach; it is one of the standard methods that are close to the state of the art. Is it better to use bagging or not, when training this ensemble, if each model is a complex neural network? I don't know. I haven't seen papers that have studied that. In the study of the literature that I've done on uncertainty estimation for neural networks, roughly speaking, the standard methods train an ensemble of models without bagging. But then, that is computationally expensive, because you have to train dozens of models and at inference time you have to run a forward pass through all of them. So, instead, a standard heuristic/approximation to that is instead we train only a single network, but with dropout . We obtain an ensemble by using different random values for the dropout, but the same weights. The benefit is that then we only have to train a single network, which is more tractable. At inference time, we run that model forward multiple times, with different independent random choices for the dropout in each run. This approach of obtaining an ensemble of models using dropout is often called the MC Dropout (Monte Carlo dropout) method for uncertainty estimation. Because of the use of dropout, it isn't possible to use bagging. For these reasons, the most standard, widely used method for uncertainty estimation with ensembles, based on the research literature I've read on uncertainty estimation for neural networks, does not use bagging. Does that mean bagging is worse than not using bagging? Not necessarily. I can't recall seeing any paper that studied that. I don't think it's a priori obvious whether bagging would be better. It could be better, if it captures a more accurate estimate of the variability in the prediction. It also could be worse, for the reasons you mention, of having less training data for each model. But it's also possible that those reasons might not apply, if there is sufficient training data that reducing the size of the training data by 2x does not harm the quality of the model too much. It's hard to know. As with most questions about neural networks, the only way to find out whether bagging is better or worse would be to conduct experiments. I suspect those experiments might well depend on the particular datasets you are using. So if you really care about it, implement both and benchmark them to see which does better in your experiments. But if you want to be pragmatic, don't bother. You can skip bagging and know that what you're doing is consistent with a standard scheme in the research literature, thus gaining some assurance that you are doing something reasonable.
