[site]: crossvalidated
[post_id]: 155177
[parent_id]: 
[tags]: 
What is the intuition of the role of multiple layers in a sequence modelling RNN?

If I build a deep sequence modelling recurrent neural-network, for instance a character-level model of text, is there an intuition as to what kinds of features are extracted by the different hidden layers of neurons? For instance, if I'm using a deep convolutional network for vision, the usual intuition is that the first layer extracts things like edges and corners, the next layer combinations of those, and so on with more and more abstracted concepts as you move up the network. Is there a similar intuition for RNNs doing text-modelling? The recurrent architecture makes it seem to me that it's not similar to the vision case (i.e. the first layer detecting common letter pairs, the next layer words, the next layer phrase snippets, etc.)
