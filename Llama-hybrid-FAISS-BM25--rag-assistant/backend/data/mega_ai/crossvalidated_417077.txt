[site]: crossvalidated
[post_id]: 417077
[parent_id]: 417073
[tags]: 
Since my point of view KNN is more less complex in time execution that SVM model. thanks so much. Empirical evaluation can't really determine which of two algorithms has lower asymptotic complexity. In fact i'm pretty sure that would violate Rice's theorem. What is more complex? O(n^3) or O(nd) and Why? Well these aren't comparable, because one is a function of the number of datapoints, and the other is a function of both # of datapoints AND also dimension. Furthermore, I really doubt the complexity of SVM is independent of dimension, so it is probably the case that $O(n^3)$ was derived assuming some fixed dimension, which makes it even more incomparable with a bound derived assuming $d$ -dimension data points. Since the large O notation only gives a higher asymptotic dimension, and not an asymptotically adjusted upper bound This is a bit of a mathematical nuance, but to abuse some notation you can think of " $O$ " as the " $\leq$ " inequality. So it is valid to say that a constant time algorithm is in $O(e^n)$ , because $1 \leq e^n$ . Of course such looseness is rarely useful so people use $\Theta$ to denote a tight bound. (And in most cases when people say $O$ they really mean $\Theta$ ). According to [2] the complexity of K-NN is O(nd) There are two tasks at hand here: training and inference. For SVM, training takes $O(n^3)$ according to you, but inference takes $O(d)$ , since you only need to determine which side of a hyperplane a given point lies on. For KNN, no training is needed, but inference is substantially more expensive (that is where the $O(nd)$ bound comes from). So really, it doesn't make much sense to compare the training time of one classifier with the inference time of another.
