[site]: crossvalidated
[post_id]: 436132
[parent_id]: 436124
[tags]: 
Usually, for most of the neural networks architectures, there is no constrains on the weights, the weights can take any values on the real line. There is no reason to constrain the weights. If you want neural network layer to return certain kind of values (e.g. between zero and one), then you can always use activation function for it.
