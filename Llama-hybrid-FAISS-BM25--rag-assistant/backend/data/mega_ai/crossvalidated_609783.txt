[site]: crossvalidated
[post_id]: 609783
[parent_id]: 583735
[tags]: 
I met the same issue as you. I tried to fine-tune a large language model with more than millions of parameters but it outputs exactly the same for each batch. Finally, I figured out that I used a too-large learning rate with 0.0001 for the Adam optimizer. Regularly, we leverage a tiny learning rate during the fine-tuning stage but I forgot to adjust it. In your case, though you are not using a pretrained model, you still set a too-large learning rate. As you stated, you change sigmoid to relu so that you get different outputs. I agree that Sigmoid is not the best choice for the activation function. However, if you take a look at how Sigmoid and Relu are different from each other. You will find that ReLu or LeakyRelu nearly lost their gradients if the input value is smaller than zero. That is why even if you use a big learning rate with a small batch size, the training process is still stable. In other words, the design of ReLu plays the role of preventing training vulnerability. But there may be a case when you cannot alter and need to fix to a certain kind of deep learning model architecture, the optimal way is to turn down the learning rate.
