[site]: crossvalidated
[post_id]: 461313
[parent_id]: 
[tags]: 
Does the irreducible error in a model include "unknown" variables?

My informal understanding of the irreducible error for a set of data that no algorithm can avoid is essentially the "noise" i.e the purely random effects. As part of refining/honing that understanding I am reading A Gentle Introduction to Bias Variance Tradeoff https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/ that makes the following statement: The irreducible error cannot be reduced regardless of what algorithm is used. It is the error introduced from the chosen framing of the problem and may be caused by factors like unknown variables that influence the mapping of the input variables to the output variable. This statement seems to diverge from my prior understanding - in effect saying that systemic disparities between the observed and predicted results may be considered irreducible . I would contend that any such non-random differences constitute bias . Would a seasoned statistician be able to comment on either the wording of that paragraph above were either misleading or flat-out incorrect or the paragraph is phrased accurately . If (2) then a further clarification of that statement in the context of random vs systemic errors would be appreciated.
