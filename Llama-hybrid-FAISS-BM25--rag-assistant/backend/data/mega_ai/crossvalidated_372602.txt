[site]: crossvalidated
[post_id]: 372602
[parent_id]: 371801
[tags]: 
To turn a wee bit pedantic, this derivation of the Bayes estimator is the construction of the Bayes rule when the observable is $X=x$ . The definition of a Bayes rule in game theory and Bayesian decision theory is rather the procedure $\hat{\theta}:\mathcal{X}\to\Theta$ that optimises the Bayes risk $$\int_\Theta\int_\mathcal{X} \text{L}(\theta,\hat{\theta}(x))\,f(x|\theta)\,\text{d}x\,\pi(\theta)\,\text{d}\theta$$ Since, by Fubini, $$\int_\mathcal{X} \int_\Theta \text{L}(\theta,\hat{\theta}(x))\,\pi(\theta|x)\,\text{d}\theta\,m(x)\,\text{d}x=\int_\Theta\int_\mathcal{X} \text{L}(\theta,\hat{\theta}(x))\,f(x|\theta)\,\text{d}x\,\pi(\theta)\,\text{d}\theta$$ this is equivalent to minimising the posterior expected loss $$\int_\Theta \text{L}(\theta,\hat{\theta}(x))\,\pi(\theta|x)\,\text{d}\theta$$ for almost every $x$ , as noted by Cowboy Trader in a comment. At least this is how it is defined in Jim Berger's book (and in mine ).
