d product offering to intelligence-based government agencies (SAIC) Mathematics of LSI LSI uses common linear algebra techniques to learn the conceptual correlations in a collection of text. In general, the process involves constructing a weighted term-document matrix, performing a Singular Value Decomposition on the matrix, and using the matrix to identify the concepts contained in the text. Term-document matrix LSI begins by constructing a term-document matrix, A {\displaystyle A} , to identify the occurrences of the m {\displaystyle m} unique terms within a collection of n {\displaystyle n} documents. In a term-document matrix, each term is represented by a row, and each document is represented by a column, with each matrix cell, a i j {\displaystyle a_{ij}} , initially representing the number of times the associated term appears in the indicated document, t f i j {\displaystyle \mathrm {tf_{ij}} } . This matrix is usually very large and very sparse. Once a term-document matrix is constructed, local and global weighting functions can be applied to it to condition the data. The weighting functions transform each cell, a i j {\displaystyle a_{ij}} of A {\displaystyle A} , to be the product of a local term weight, l i j {\displaystyle l_{ij}} , which describes the relative frequency of a term in a document, and a global weight, g i {\displaystyle g_{i}} , which describes the relative frequency of the term within the entire collection of documents. Some common local weighting functions are defined in the following table. Some common global weighting functions are defined in the following table. Empirical studies with LSI report that the Log and Entropy weighting functions work well, in practice, with many data sets. In other words, each entry a i j {\displaystyle a_{ij}} of A {\displaystyle A} is computed as: g i = 1 + ∑ j p i j log ⁡ p i j log ⁡ n {\displaystyle g_{i}=1+\sum _{j}{\frac {p_{ij}\log p_{ij}}{\log n}}} a i j = g i log ⁡ ( t f i j + 1 ) {\displaystyle a_{ij}=g_{i}\ \log(\mathrm {tf} _{ij}+1)} Rank-reduced singular value decomposition A rank-reduced, singular value decomposition is performed on the matrix to determine patterns in the relationships between the terms and concepts contained in the text. The SVD forms the foundation for LSI. It computes the term and document vector spaces by approximating the single term-frequency matrix, A {\displaystyle A} , into three other matrices— an m by r term-concept vector matrix T {\displaystyle T} , an r by r singular values matrix S {\displaystyle S} , and a n by r concept-document vector matrix, D {\displaystyle D} , which satisfy the following relations: A ≈ T S D T {\displaystyle A\approx TSD^{T}} T T T = I r D T D = I r {\displaystyle T^{T}T=I_{r}\quad D^{T}D=I_{r}} S 1 , 1 ≥ S 2 , 2 ≥ … ≥ S r , r > 0 S i , j = 0 where i ≠ j {\displaystyle S_{1,1}\geq S_{2,2}\geq \ldots \geq S_{r,r}>0\quad S_{i,j}=0\;{\text{where}}\;i\neq j} In the formula, A is the supplied m by n weighted matrix of term frequencies in a collection of text where m is the number of unique terms, and n is the number of documents. T is a computed m by r matrix of term vectors where r is the rank of A—a measure of its unique dimensions ≤ min(m,n). S is a computed r by r diagonal matrix of decreasing singular values, and D is a computed n by r matrix of document vectors. The SVD is then truncated to reduce the rank by keeping only the largest k « r diagonal entries in the singular value matrix S, where k is typically on the order 100 to 300 dimensions. This effectively reduces the term and document vector matrix sizes to m by k and n by k respectively. The SVD operation, along with this reduction, has the effect of preserving the most important semantic information in the text while reducing noise and other undesirable artifacts of the original space of A. This reduced set of matrices is often denoted with a modified formula such as: A ≈ Ak = Tk Sk DkT Efficient LSI algorithms only compute the first k singu