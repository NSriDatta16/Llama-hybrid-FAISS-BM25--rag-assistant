[site]: crossvalidated
[post_id]: 249192
[parent_id]: 
[tags]: 
Understand XGboost learning

I'm an experienced SWE and trying to learn some ML. I'm reading the XGBoost tutorial and I'd like to further understand one point in the learning process. It appears from the following picture that when spliting a leave, it first tries to sort the samples by their G(i) and H(i) scores and then do a linear scan to find the best split that maximize the gain. My question is: 1. Based on what does the algorithm sort (we have G(i) and H(i), not just one metric?) 2. Why does the split have to be continous? Shouldn't it be entirely possible that distinct chunk of the sorted sequence form one leave and the rest another leave? Explanations are appreciated. I'm sure I'm missing something.
