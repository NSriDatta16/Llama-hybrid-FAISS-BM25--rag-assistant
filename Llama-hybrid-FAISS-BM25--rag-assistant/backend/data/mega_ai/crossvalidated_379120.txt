[site]: crossvalidated
[post_id]: 379120
[parent_id]: 379119
[tags]: 
For a variety of reasons, this is very hard to do for random forests, or most blackbox methods with complex interactions. Remember that a random forest is basically taking the average over a large number of decisions trees that are all trained to perfectly fit their subset of the data. Understanding even a single one of these saturated trees is nearly impossible, and now to "understand" the average of a large number of them is not a reasonable human task. With that in mind, the simplest approach I would advise is to use a model in which the feature effects are much easier to understand, like an elastic net model. If the predictive ability is close to that of the random forest, then one might pick the elastic net model just due to interpretability over a random forest, even though the random forest might have slightly higher accuracy. Finally, if one does find that the interpretable models are unacceptably worse in predictions than the non-interpretable model, then one way to answer your question is to simply plug feature values into your fitted model and see how the outcome changes. However, this is actually slightly more challenging than it looks; remember that in a random forest, the model allows for interactions between variables. So it is possible that if $x_1 = 0$ , high values of $x_2$ lead to high probabilities of success in the outcome, but if $x_1 = 1$ , low values of $x_2$ lead to high probabilities of success in the outcome. One way to summarize this might be to plug in all rows of your data, except that you change your feature of interest into something like "low", "medium" and "high". Then compare the estimated probabilities given the different levels for your feature given several representative values of all the other covariates. Finally, just a word of caution in general when interpreting feature effects from a predictive model. Note that you are interpreting how the model makes a decision , which is different than the actual relation between a feature and the outcome. For example, if there's very little evidence for an effect from a given feature, then the elastic net model will likely set this estimated coefficient to 0 due to the $L_1$ penalty term. However, lack of evidence is not the same as evidence of lack; if $x_1 = 0$ for 99.9% of your data, then you have very little evidence about the effect of $x_1$ , and so the elastic net model is likely to set this coefficient to 0, even though the effect may be large.
