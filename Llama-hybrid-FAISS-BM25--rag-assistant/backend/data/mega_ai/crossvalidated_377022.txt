[site]: crossvalidated
[post_id]: 377022
[parent_id]: 
[tags]: 
How to handle measurement inaccuracy? [improved]

This is based on my old question but the question was too confusing. I've tried to make it clearer this time. This is the kind of data we're dealing with: Group A +--------+----------+----------+----------+------------------+ | Object | Prior #1 | Prior #2 | Prior #3 | Post-Measurement | +--------+----------+----------+----------+------------------+ | LUMN | 22 | 22 | 23 | 24 | +--------+----------+----------+----------+------------------+ | BQX9 | 112 | 108 | 115 | 115 | +--------+----------+----------+----------+------------------+ | LST1 | 33 | 30 | 32 | 35 | +--------+----------+----------+----------+------------------+ | FWWD | 291 | 288 | 287 | 293 | +--------+----------+----------+----------+------------------+ | LBQ0 | 173 | 277 | 165 | 270 | +--------+----------+----------+----------+------------------+ Object is the identifier of the Object we're measuring and Prior #n are the measurements that are done prior to treating the object. The Post-measurement is a single measurement we can do after having treated the object in some fashion. We can not measure the same object twice after it was treated. Since the physical property doesn't change between prior measurements all the variance in the measured values is measurement uncertainty. The measurement uncertainty is different for each object meaning the accuracy of our measurement depends on the specific object we're measuring. We don't know the exact value a prior measurement should measure but we know that it doesn't change during measurements because it's a physical property that doesn't just change randomly. The post measurement is of course also affected by uncertainty. In reality we have hundreds of objects. How do we establish a useful overall (over every object, not per object) metric for Did our treatment do anything and how much did it? We could of course just compute the averages over the prior measurement and then do calculate the difference between the averages and the post-measurements but that's not convincing enough for us because our average could be 105 and a post-measurement of 107 would indicate a change of two but we know from prior measurements that we've also measured a 108 and thus the 107 is probably not a significant change at all. We could of course assume a per-object normal distribution and then do significance testing to see whether the post-measurement is a significant change with p=0.95 and then repeat this for every object but how do we summarize this to an overall metric? I think we're kinda looking for the average amount of change between pre- and post-treatment with 95% confidence as in "We can be 95% confident that our treatment makes a difference on average of at least +3lm"
