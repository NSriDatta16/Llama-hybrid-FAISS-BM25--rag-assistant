[site]: datascience
[post_id]: 58310
[parent_id]: 58301
[tags]: 
I used a simple Neural Network with 3 Inputs - 1 hidden layer with 8 neurons - 3 Outputs (Labels) with k-fold crossvalidation in keras for your data set. import pandas as pd from keras.models import Sequential from keras.layers import Dense from keras.wrappers.scikit_learn import KerasClassifier from keras.utils import np_utils from sklearn.model_selection import cross_val_score from sklearn.model_selection import KFold from sklearn.preprocessing import LabelEncoder, scale from sklearn.pipeline import Pipeline path = r"User\train.csv" data = pd.read_csv(path) data.location.value_counts() dataset = data.values x = dataset[:,0:3] x_scaled = scale(x) y = dataset[:,3] encoder = LabelEncoder() encoder.fit(y) encoded_Y = encoder.transform(y) dummy_y = np_utils.to_categorical(encoded_Y) def model(): model = Sequential() model.add(Dense(8, input_dim=3, activation='relu')) model.add(Dense(3, activation='softmax')) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) return model estimator = KerasClassifier(build_fn=model, epochs=200, batch_size=5, verbose=0) kfold = KFold(n_splits=10, shuffle=True) results = cross_val_score(estimator, x_scaled, dummy_y, cv=kfold) print("Model Prediction: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100)) The used Model can categorize around 94% of the data correctly. Maybe this helps for the start. Out: Prediction: 93.95% (3.13%) In data science and statistics in general you have to try and train several models to see which suits your dataset best (model benchmarking). In the end it is often a trade-off between computational costs and accuracy.
