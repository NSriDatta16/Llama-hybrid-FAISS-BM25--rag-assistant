[site]: datascience
[post_id]: 126526
[parent_id]: 
[tags]: 
Gradient function in LogisitcLoss class

I am going through a code for XGBoost from scratch and I am referring to this repository here The log-loss function is given by On differentiating the above function with respect to y_pred (referring to the exact variable under the LogisitcLoss() in the repository above), I am getting my gradient as (y - p). Can anyone help me out, what is the reason behind gradient to be -(y - p)?
