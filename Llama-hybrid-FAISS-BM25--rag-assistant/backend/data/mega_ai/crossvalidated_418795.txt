[site]: crossvalidated
[post_id]: 418795
[parent_id]: 418766
[tags]: 
Usual neural networks used for $k$ -class classification use $k$ units in the output layer. These units output some numbers which relate to how much the network thinks the sample belongs to each of the $k$ classes: we assign the sample to the class with the highest output. Since these numbers can be quite arbitrary, we normalize them to sum up to one: that way we can use simple cross-entropy loss function to train that network. There are various ways to normalize the inputs, but the most popular one is exponential normalization, also known as softmax . It has the benefit over linear normalization that it can readily handle negative inputs too. Softmax-normalized outputs from the network may look like: [0.1, 0.7, 0.2] . Just like probability distribution, they sum up to one. It might suggest that the network is 70% certain that the given sample belongs to Class 2. In fact, we would be often very happy to get this uncertainty information: in some critical scenarios, uncertain samples could be given to a human expert to make the decision instead of the neural network. Unfortunately, neural networks do not output uncertainties by default: These numbers are just arbitrary and don't really reflect the strength of the network belief in its prediction. They look like probabilities because we normalized them. Probability calibration refers to methods that attempt to tackle this issue and force the networks to output actual probabilities.
