[site]: datascience
[post_id]: 100070
[parent_id]: 
[tags]: 
What should be the ratio between training time and accuracy?

What should be the ratio between time and accuracy? I mean when should you drop the accuracy a little bit but it will take less time for the Classifier/Regressor to run? Edit: As part of my studies I have to build a classifier for the Fashion-Mnist data set without using neural networks, I am using XGBoost as the classifier, and I see that with one set of hyperparameters I get 0.898 accuracy score, and with another set, I get 0.904. The difference is that the first second classifier takes more than double the time to train. My question is when should you take the classifier that takes less time to train but drops the accuracy as well?
