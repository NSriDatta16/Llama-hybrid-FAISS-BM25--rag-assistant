[site]: crossvalidated
[post_id]: 624156
[parent_id]: 
[tags]: 
Error between average of averages and true average

It is known that the average of averages can yield an incorrect result as compared to the true average. The classroom example comes to mind: if the test results of class A is [50, 60, 100], class B is [20, 30] and class C is [40], what is the average test result of the whole school? The true average should yield $$\mu_{\mathrm{true}} = \frac{50 + 60 + 100 + 20 + 30 + 40}{6} = 50;$$ whereas taking each class average and then average over the number of classes yield $$\mu_{\text{average of averages}} = \frac{1}{3} \left( \frac{50 + 60 + 100}{3} + \frac{20 + 30}{2} + \frac{40}{1}\right) = \frac{70 + 25 + 40}{3} = 45.$$ What I'm interested in is the absolute difference $D = | \mu_{\mathrm{true}} - \mu_{\text{average of averages}} |$ , where in the classroom example above, $$D = |50 - 45| = 5.$$ That is, in the classroom example, the average of averages underestimates the true average by 5 points. Is there any literature that tries to understand this error? Maybe there is a known bound on the error assuming a certain distribution on the samples? Can't seem to find any on the internet, maybe I'm searching the wrong keywords? edit: The reason why I'm interested in this quantity is because there are cases where I only get class averages (as in the example) rather than each individual values. I want to know how far off I am from the true average if I use average of averages in generality.
