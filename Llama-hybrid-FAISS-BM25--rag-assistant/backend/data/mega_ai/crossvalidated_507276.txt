[site]: crossvalidated
[post_id]: 507276
[parent_id]: 507267
[tags]: 
So I believe it's wrong, and I am not sure if you misunderstood what it's claiming. Ie the claim is that the features extracted are orthogonal and therefore unlikely to be relevant...( There is no guarantee that the features your model requires are orthogonal) PCA is used as a dimensionality reduction method. It finds the direction that maximises correlation, then finds the next direction orthogonal to the previous directions maximising correlation etc.and repeat. If you are are using a linear method or neural network ( with inner product) the orthogonality of the resulting features doesn't matter, since the model can create any linear combination anyway. The point though, of PCA, is that many signal and image processing tasks can be characterized as a Signal, correlated across inputs(pixels) and independent noise. Selecting the directions of maximal correlation therefore is filtering for the signal and throwing away noise. In terms of inputs, you should use PCA, precisely when your data is correlated and non orthogonal. I would agree with the comment that the actual directions are unlikely to be meaningful, but I would say that is irrelevant: what's important is the sub space covered by the directions. in terms of the Reddit thread where it mentioned finding clusters in the 3d set of pcas, it's the subspace that matters not the specifics of the axes chosen
