[site]: crossvalidated
[post_id]: 440513
[parent_id]: 
[tags]: 
Reparametrization trick in VAE at Evaluation time

So I've been trying to implement the Variational Auto-Encoder model of Kigma et.al, but something has been bugging me. While I understand the need for reparametrization trick at training time, the rational behind the usage of the same trick at evaluation time is escaping me. At training time we turn the random (latent) variable $z$ into a deterministic function in order to allow gradients to flow through it: $$= +⋅$$ where $∼N(0,1)$ . However, at evaluation time such trick is not required, since we can directly sample the latent variable $$ from the parametric model (encoder network) $(∣,)$ of the true posterior. Yet, most implementations of VAE I have come across still perform the reparametrization trick at evaluation time. So my question is "What is the rational behind using the reparametrization trick at testing time?".
