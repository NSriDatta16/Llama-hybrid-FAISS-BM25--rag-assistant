[site]: crossvalidated
[post_id]: 504779
[parent_id]: 504761
[tags]: 
Consider a typical GP regression model with zero mean function, covariance function $k$ , and i.i.d. Gaussian noise (with variance $\sigma_n^2$ ). The posterior predictive distribution $p(y_* \mid X_*, X, y)$ describes the outputs $y_*$ at new test points $X_*$ , having seen training data $(X,y)$ . It's expectation can be written as: $$\begin{array}{c} E[y_* \mid X_*, X, y] = H y \\ H = K(X_*, X) \big( K(X, X) + \sigma_n^2 I \big)^{-1} \\ \end{array}$$ where $K(A,B)$ denotes a matrix containing the covariance function evaluated between each pair of points in $A$ and $B$ . This says that each expected test output is a linear combination of the training outputs. Each test point has a corresponding row in $H$ , containing a weight for each training point. The predicted test output is then obtained as a weighted sum of the training outputs. If you use a 'local' covariance function that decays with distance in input space (e.g. squared exponential/RBF), then the weights in $H$ will also decay with distance to the test point. For more information, check out section 2.6 in Gaussian Processes for Machine Learning (Rasmussen & Williams 2006). Note that the predictions above are not strictly weighted averages of the training outputs, because $H$ may contain negative values, and the rows may not sum to one. However, you might be interested in comparing GP regression to Nadaraya-Watson kernel regression, which does indeed use weighted averages. Here, the test predictions are also obtained as $\hat{y}_* = H y$ . However, $H$ is computed by taking $K(X_*, X)$ , then normalizing each row to sum to one. This seems to be similar to what you've done in your plots.
