[site]: datascience
[post_id]: 89809
[parent_id]: 89799
[tags]: 
[EDIT] In case you have very long sequences , you can also try attention-based models to prevent the vanishing gradient issue. If I got it correctly, you might have the following cases: labeled samples with tag A or B per date-time index , having as informative attributes the present + some lag values of interest --> (this would be a standard classification approach without time ordering bein necessary) sliding window of samples (what you mean by subsequences) --> here you should respect the time ordering at least for the validation set, so you make sure you evaluate your LSTM with a real scenario with future never seen sequences . With this second approach, you can indeed shuffle the training batches (via the keras shuffle(BUFFER_SIZE).batch(BATCH_SIZE) functionality for instance ( info here ), but leaving the validation set without shuffling, as follows: BATCH_SIZE = 256 BUFFER_SIZE = 10000 train_data_multi = tf.data.Dataset.from_tensor_slices((x_train_multi,y_train_multi)) train_data_multi = train_data_multi.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat() val_data_multi = tf.data.Dataset.from_tensor_slices((x_val_multi, y_val_multi)) val_data_multi = val_data_multi.batch(BATCH_SIZE).repeat() You can find a complete worked-out example here You can also make use of the time series data preprocessing helper where you can decide whether to shuffle or not.
