[site]: crossvalidated
[post_id]: 637994
[parent_id]: 
[tags]: 
Fitting regression where data is concentrated at the origin

I'm doing an exploratory data analysis which is looking at a number of character-level features of Chinese writing. The association I am looking at currently is that between the complexity of the character (discrete) and estimated age of acquisition (continuous). The data in its raw form looks like this: Based on what I see, it appears the data is first generated closely around the origin, becomes a large ellipsoid that is weakly correlated, then tampers off towards the right. Given this information, I considered the following options: Regular OLS : This just doesn't work. It overpredicts where the data should be by a lot. No-intercept linear model : Because the data originates close to the origin $[0,0]$ I considered a suppressed intercept, but this seems implausible given its impossible for someone to have an age of zero when acquiring words, and it is likely also impossible to have a complexity score of zero, so I abandoned that idea. GAM : I used a variety of GAM fittings and the best fit seemed to be an adaptive smooth, but it still seems to overpredict data near the origin. This still generally did the job though in terms of capturing the rest of the distribution. Polynomial regression : I usually don't like these for nonlinear regression, but this seems to do the best job of capturing the bottom left part of the distribution that I am concerned with. The residual diagnostics seem to be fairly good too. The only issue I see here is that it interpolates somewhat poorly at the right-most side of the regression line. Poisson or beta regression : The DV can only take on values of 1 or greater. However, because the data is continuous, I don't think it works here. I tried beta regression too (by transforming the data into proportions) and the model just looked wrong. Below are the four candidate fits which appear to at least get close to reality. The residuals from the polynomial fit are shown below. The residuals are obviously banded and cluster more in certain areas, but I think generally they don't look terrible: Does the polynomial regression work best here or is another approach better? I will note that this dataset has thousands of values, so it clusters a lot in the middle of the ellipsoid in the scatterplot. Edit Filtering for the values closer to the origin (AoA Edit 2 As for the context: AoA is in single digits and is supposed to be an estimation of when the Chinese originally learned a character (it is a subjective self-recall measure). The rating is based off an averaging across subjects for a single character, hence the decimal units. For example, 3 people may recall that they learned æˆ‘ when they were 6 years old, 7 years old, and 7 years old respectively, which averages to 6.67ish years. So here it is highly unlikely the age will be higher than typical early childhood years. As for perimetric complexity, it is normally continuous and is formulated as: $$ \frac{P^2}{A 4 \pi} $$ where $P$ is the perimeter of the ink space of a character, $A$ is the area, and $\pi$ is just literally $\pi$ . But in this dataset it is unfortunately rounded and I can't get back those decimal values. As for the unusualness in the distribution, my major concern was that the center of the distribution is much heavier (there are 3600+ obs here in case that's not clear) so the outsides of the distribution have some fairly uneven weighting of the regression line. I also want to balance model parsimony here, so getting away from an overly complex polynomial would be ideal.
