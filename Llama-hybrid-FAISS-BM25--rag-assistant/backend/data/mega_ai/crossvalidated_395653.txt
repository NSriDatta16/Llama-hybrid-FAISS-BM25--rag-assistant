[site]: crossvalidated
[post_id]: 395653
[parent_id]: 
[tags]: 
Do there exist adaptive step size methods for Newton-Raphson optimization?

Stochastic/Mini-batch gradient descent, caused by interest in deep learning, has made lots of advances in adaptive step sizes. For example, Adam, Nadam, Adamax, ..., are all improvements to the standard SGD which uses a fixed step size. (Improvements in the sense of finding smaller minimums or more stable convergence.) However, I haven't seen much research on adaptive step sizes for Newton-Raphson algorithms. For example, would a momentum based algorithm, like Adam, make sense? From what I've seen, most step sizes in Newton-Raphson are fixed What about if we assume the function to minimize is convex? Can we do any better?
