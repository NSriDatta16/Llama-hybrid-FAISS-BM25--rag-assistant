[site]: crossvalidated
[post_id]: 106118
[parent_id]: 106111
[tags]: 
The problem here seems to be not the occurence of $(0,0)$-values (since any other points $(x,y)$ will behave similarly), but rather a problem of model selection. If understand you right, you want to assume a linear model and then let the fitting procedure state doubts about either the fitted parameters or about the linear model itself. Some points into three different directions: First, within the linear model, you could use Bayesian regression and then find that the slope has a much larger variance than the intercept (of course this depends also on the prior). Adding more (0,0) values will mainly narrow the distribtion of the intercept and not the distribution of the slope. Second, also assuming a linear model, you can use weighted least squares regression. If you have doubts about the (2,2) values, you can assume them to have a large variance and thus a small weight. Similar to the Bayesian setup, this will widen the error bars and draw higher doubt on the result for the intercept. Third, and more generally, if you want a procedure which doubts the model based on the training data, you can use model comparison schemes which will show that other models beside a straight line are possible and probably equally likely (e.g., a parabola will also fit the data well). However, in the end you are stuck with the model you assume. Hence, you should choose it carefully and with regard to the data.
