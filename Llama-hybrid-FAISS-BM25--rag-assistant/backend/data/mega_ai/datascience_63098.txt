[site]: datascience
[post_id]: 63098
[parent_id]: 63095
[tags]: 
Let me try with an explanation for at least 1+2+4: usually when talking about activation functions in your output layer you want to achieve on of two things: either have a binary output (0 or 1) for classification tasks (sometimes a soft output for probabilities like softmax) or a linear output for regression tasks. ReLU while being used "like" a linear function isn't really one so that you would only get 0 to -inf for negative values. Since your output hardly ever is something with 0 to -inf and linear for positive values, ReLU doesn't really make sense as output activation Sigmoid is not used in a Multi_class classification because it just gives you any value between 0 an 1. Usually you want something that either tells you binary 0 or 1 per class or gives you real probabilities per class like with Softmax. I'm not totally clear what you're asking Micro Averages are computed by taking all contributions of all classes into consideration and calculating their average. This is preferable if you suspect an imbalance between your classes (since it aggregates all class-contributions rather than looking at them individually; this btw. would be 'macro')
