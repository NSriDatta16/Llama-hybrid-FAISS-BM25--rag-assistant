[site]: datascience
[post_id]: 8377
[parent_id]: 5893
[tags]: 
Stopwords may be part of the solution at some point, but not the key. In any case for any major languages good lists of stop words exist, it should not be domain specific. I also don't think that using TD-IDF alone is really correct. There could be very rare (potentially garbage) words in poor quality strings. Instead of trying to guess which exact features are useful: I would start by creating a data set by randomly selecting some of the data and labeling them by hand (as good or bad , or on a scale from 0.0 to 1.0). Then code something up that pulls out many features (length, number of words (tokens), spam score, whether it contains URLs or botish chars, detected language, whether it has a question mark, whether it has proper capitalisation). Also don't forget to include non-linguistic features that you may have, like country of the geoIP of the user that made the query, whether the user was logged in, how old the user's account is. So at this point you will have a massive table/CSV, and a smaller one with one extra column for the label you've added. Then train some machine learning package with those labeled examples to build a model that is accurate enough for you. Then let that model run on the rest of the data. If you wish not to code too much, you could even just get those features into CSV form, and give them to the Google Prediction API's spreadsheet interface .
