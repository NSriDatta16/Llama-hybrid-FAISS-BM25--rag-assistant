[site]: crossvalidated
[post_id]: 320661
[parent_id]: 
[tags]: 
Unstable logistic regression when data not well separated

There are some good answers discussing convergence issues of logistic regression when the data are well separated here and here . I am wondering what can cause convergence issues when the data are not well separated . As an example, I have the following data, df y x1 x2 1 0 66.06402 -1.0264739 2 1 58.40813 0.2887934 3 1 58.58011 0.2626232 4 0 59.05929 -0.5286438 5 0 55.81817 -1.3184894 6 0 58.00018 -0.8445602 7 1 69.53926 -1.1018149 8 0 55.73621 -0.9000901 9 1 79.80170 0.6690657 10 0 55.40042 0.6600415 11 0 57.42124 -0.7237973 12 1 78.22012 -0.8121816 13 0 53.54296 0.2265636 14 1 56.14096 0.4216436 15 1 66.90146 0.6189839 16 0 50.40008 0.4311339 Fitting a logistic regression in R , I am getting a glm.fit: fitted probabilities numerically 0 or 1 occurred warning message even though the data are non-separable > attach(df) > safeBinaryRegression::glm(y ~ x1 + x2, family=binomial) Call: safeBinaryRegression::glm(formula = y ~ x1 + x2, family = binomial) Coefficients: (Intercept) x1 x2 -82.930 1.395 10.255 Degrees of Freedom: 15 Total (i.e. Null); 13 Residual NullDeviance: 21.93 Residual Deviance: 5.927 AIC: 11.93 Warning message: glm.fit: fitted probabilities numerically 0 or 1 occurred A visual confirmation that the data are in fact non-separable is also included Removing the red point seems resolve the convergence issues, however I am at a bit of a loss for why this is. > df2 detach(df) > attach(df2) > safeBinaryRegression::glm(y ~ x1 + x2, family=binomial) Call: safeBinaryRegression::glm(formula = y ~ x1 + x2, family = binomial) Coefficients: (Intercept) x1 x2 -82.930 1.395 10.255 Degrees of Freedom: 14 Total (i.e. Null); 12 Residual Null Deviance: 20.19 Residual Deviance: 5.927 AIC: 11.93
