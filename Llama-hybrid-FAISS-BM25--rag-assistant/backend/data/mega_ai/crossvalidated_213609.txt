[site]: crossvalidated
[post_id]: 213609
[parent_id]: 
[tags]: 
Why does Bayesian p-value involve the parameters in addition to the data?

On page 146 of Gelman's Bayesian Data Analysis, Gelman discusses Bayesian p-value as a way to check the fit of the model. The idea is to compare the observed data ($y$) with data that could have been generated by the model if we replicate the experiment ($y^{rep}$). He defines Bayesian p-value as $$ p_B = Pr(T(y^{rep}, \theta) \geq T(y, \theta) | y) $$ I don't quite understand why it makes sense to have the test statistic be a function of the parameters, $\theta$. Indeed, if the goal is to "compare the observed data with data that could have been generated by the model ", shouldn't the comparison be strictly between $y$ and $y^{rep}$? For example, on the same page, Gelman provides an example where he checks the fit of a normal model. The test statistic is: $$ T(y, \theta) = | y_{(61)} - \theta | - |y_{(6)} - \theta | $$ where $\theta$ is the mean of the normal model. This test statistic is designed to ignore the model fit at the extreme tail, beyond the 6th and 61th order statistics. Why don't we use the following test statistic instead, relying purely on the data? $$ T(y, \theta) = | y_{(61)} - \bar y | - |y_{(6)} - \bar y | $$
