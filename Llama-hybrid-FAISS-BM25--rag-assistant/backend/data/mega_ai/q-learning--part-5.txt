her than the expected return of each action. It has been observed to facilitate estimate by deep neural networks and can enable alternative control methods, such as risk-sensitive control. Multi-agent learning Q-learning has been proposed in the multi-agent setting (see Section 4.1.2 in ). One approach consists in pretending the environment is passive. Littman proposes the minimax Q learning algorithm. Limitations The standard Q-learning algorithm (using a Q {\displaystyle Q} table) applies only to discrete action and state spaces. Discretization of these values leads to inefficient learning, largely due to the curse of dimensionality. However, there are adaptations of Q-learning that attempt to solve this problem such as Wire-fitted Neural Network Q-Learning. See also Reinforcement learning Temporal difference learning SARSA Iterated prisoner's dilemma Game theory References External links Watkins, C.J.C.H. (1989). Learning from Delayed Rewards. PhD thesis, Cambridge University, Cambridge, England. Strehl, Li, Wiewiora, Langford, Littman (2006). PAC model-free reinforcement learning Reinforcement Learning: An Introduction by Richard Sutton and Andrew S. Barto, an online textbook. See "6.5 Q-Learning: Off-Policy TD Control". Piqle: a Generic Java Platform for Reinforcement Learning Reinforcement Learning Maze, a demonstration of guiding an ant through a maze using Q-learning Q-learning work by Gerald Tesauro