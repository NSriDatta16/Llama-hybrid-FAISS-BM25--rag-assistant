[site]: crossvalidated
[post_id]: 317689
[parent_id]: 315129
[tags]: 
When talking about the kernel trick the mapping is referred to as implicit because we are not mapping the actual instances in the data to the kernel-induced feature space. For training a support vector machine, it does not matter to us how the new feature space looks and where our data lies within it - we only need to know the inner product of our instance's images in that space. Given a kernel function $K$ satisfies Mercer's condition , it can be expressed as the inner product in the kernel-induced feature space $$K(x_1,x_2) = \langle\phi(x_1),\phi(x_2)\rangle$$ where $\phi$ is a mapping into a hilbert space $\mathcal F $ $$\phi:\mathcal{X}\rightarrow \mathcal{F}.$$ A kernel function like the Gaussian RBF kernel acts as a similarity measure of our instances in the original feature space and also fulfills Mercer's condition.
