[site]: crossvalidated
[post_id]: 223344
[parent_id]: 223310
[tags]: 
Because the second model has more variables, you're more likely to get missing values. So I think there are two/three things you could do: If there aren't that many missing values, you can look at just removing them and be done with them. But then when you are using your model to make a prediction, you can't predict on those customers who have missing data. You can look into the amelia package in R which gives a solution to replace the missing value by using multiple imputation. This again might pose a problem when predicting for people with missing values. Both the options above take away some information value from the data, because we no longer have the effect of 'missing' data in our model. In fact, this already is happening with your models since logistic regression just removes any row that has a NA value in one of its factors. So then, instead of trying to get rid of these missing values, you could replace them with a certain value such as $-9999$ if it is a numerical or simply just 'missing' if categorical. In terms of categorical, it will just become a new level so it works out great. But when it comes to numerical, you may have to look into binning your variable so that all missing values are in the same bin. This goes into WoE and information value, which I think this link would be useful for: http://ucanalytics.com/blogs/information-value-and-weight-of-evidencebanking-case/ In the end you have to make the decision of if you're ok with making certain trade-offs, for example maybe your current data set has missing values but for your future data that you'll collect you can make sure you won't have any missing values so you can then go ahead and remove missing values all together. It all depends on your situation.
