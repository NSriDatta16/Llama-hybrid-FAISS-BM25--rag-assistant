\displaystyle y_{0},y_{1},\dots } , autoregressively. That is, it always takes as input both the hidden vectors produced by the encoder, and what the decoder itself has produced before, to produce the next output word: ( h 0 , h 1 , … {\displaystyle h_{0},h_{1},\dots } , "<start>") → "la" ( h 0 , h 1 , … {\displaystyle h_{0},h_{1},\dots } , "<start> la") → "la zone" ( h 0 , h 1 , … {\displaystyle h_{0},h_{1},\dots } , "<start> la zone") → "la zone de" ... ( h 0 , h 1 , … {\displaystyle h_{0},h_{1},\dots } , "<start> la zone de contrôle international") → "la zone de contrôle international <end>" Here, we use the special <start> token as a control character to delimit the start of input for the decoder. The decoding terminates as soon as "<end>" appears in the decoder output. Attention weights As hand-crafting weights defeats the purpose of machine learning, the model must compute the attention weights on its own. Taking analogy from the language of database queries, we make the model construct a triple of vectors: key, query, and value. The rough idea is that we have a "database" in the form of a list of key-value pairs. The decoder sends in a query, and obtains a reply in the form of a weighted sum of the values, where the weight is proportional to how closely the query resembles each key. The decoder first processes the "<start>" input partially, to obtain an intermediate vector h 0 d {\displaystyle h_{0}^{d}} , the 0th hidden vector of decoder. Then, the intermediate vector is transformed by a linear map W Q {\displaystyle W^{Q}} into a query vector q 0 = h 0 d W Q {\displaystyle q_{0}=h_{0}^{d}W^{Q}} . Meanwhile, the hidden vectors outputted by the encoder are transformed by another linear map W K {\displaystyle W^{K}} into key vectors k 0 = h 0 W K , k 1 = h 1 W K , … {\displaystyle k_{0}=h_{0}W^{K},k_{1}=h_{1}W^{K},\dots } . The linear maps are useful for providing the model with enough freedom to find the best way to represent the data. Now, the query and keys are compared by taking dot products: q 0 k 0 T , q 0 k 1 T , … {\displaystyle q_{0}k_{0}^{T},q_{0}k_{1}^{T},\dots } . Ideally, the model should have learned to compute the keys and values, such that q 0 k 0 T {\displaystyle q_{0}k_{0}^{T}} is large, q 0 k 1 T {\displaystyle q_{0}k_{1}^{T}} is small, and the rest are very small. This can be interpreted as saying that the attention weight should be mostly applied to the 0th hidden vector of the encoder, a little to the 1st, and essentially none to the rest. In order to make a properly weighted sum, we need to transform this list of dot products into a probability distribution over 0 , 1 , … {\displaystyle 0,1,\dots } . This can be accomplished by the softmax function, thus giving us the attention weights: ( w 00 , w 01 , … ) = s o f t m a x ( q 0 k 0 T , q 0 k 1 T , … ) {\displaystyle (w_{00},w_{01},\dots )=\mathrm {softmax} (q_{0}k_{0}^{T},q_{0}k_{1}^{T},\dots )} This is then used to compute the context vector: c 0 = w 00 v 0 + w 01 v 1 + ⋯ {\displaystyle c_{0}=w_{00}v_{0}+w_{01}v_{1}+\cdots } where v 0 = h 0 W V , v 1 = h 1 W V , … {\displaystyle v_{0}=h_{0}W^{V},v_{1}=h_{1}W^{V},\dots } are the value vectors, linearly transformed by another matrix to provide the model with freedom to find the best way to represent values. Without the matrices W Q , W K , W V {\displaystyle W^{Q},W^{K},W^{V}} , the model would be forced to use the same hidden vector for both key and value, which might not be appropriate, as these two tasks are not the same. This is the dot-attention mechanism. The particular version described in this section is "decoder cross-attention", as the output context vector is used by the decoder, and the input keys and values come from the encoder, but the query comes from the decoder, thus "cross-attention". More succinctly, we can write it as c 0 = A t t e n t i o n ( h 0 d W Q , H W K , H W V ) = s o f t m a x ( ( h 0 d W Q ) ( H W K ) T ) ( H W V ) {\displaystyle c_{0}=\mathrm {Attention} (h_{0}^{d}W^