[site]: crossvalidated
[post_id]: 462100
[parent_id]: 461401
[tags]: 
Answer to my own question: After further reading, I couldn't find another application for $\Sigma_2$ than arriving quicker to the same results as $\Sigma_1$ , but only for the cases where $m . So, it was a mistake of mine to think $\Sigma_2$ represents the "correlations of the samples", it is just a possible short-cut to the "correlations of the features". If you know of another application of $\Sigma_2$ let me know! Below I will show how to use $\Sigma_2$ : so, for PCA one wants to find the representation of $X$ in a new coordinate space spanned by $u_{1i}$ vectors, the column vectors of the matrix $U_1$ , obtained from singular value decomposition: $U_1, S_1, V_1 = svd(\Sigma_1)$ If for the data Matrix $X$ , with $dim(X)=(n,m)$ , it holds $m than it might be quicker for you to obtain $U_1$ by doing the following: $U_2, S_2, V_2 = svd(\Sigma_2)$ $U_{2}^{*} = X U_2$ You need then to divide each of the columns $u^{*}_i$ of $U_{2}^{*}$ by the square root of the corresponding element $s_i$ in $S_2$ (in pythons numpy you can just do for that U_hat2 = U_star2/np.sqrt(S2) ): $\hat{U_{2}} = [\frac{1}{\sqrt{s_{1}}} \cdot u_1 , \frac{1}{\sqrt{s_{2}}} \cdot u_2, ..., \frac{1}{\sqrt{s_{m}}} \cdot u_m] $ $\hat{U_{2}}$ has dimensions $(n,m)$ , but if you cut it up to the n-th column it is equal to $U_1$ , in python this can be expressed as U_1 == U_hat2[:,:n] . And this is how you can arrive in two ways to $U_1$ , the second way offering to save time when $m
