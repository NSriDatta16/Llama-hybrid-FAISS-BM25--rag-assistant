[site]: crossvalidated
[post_id]: 609186
[parent_id]: 609166
[tags]: 
There have been numerous responses including some to your very posts earlier and the present one too. It should be reiterated that $\mathcal L(\theta\mid \mathbf x) $ or $\ell_\mathbf x(\theta)$ (to emphasize what the argument here is) even though has the same functional form as the corresponding density function of the distribution, in the former, what varies is the value of $\theta$ over the parameter space given the observed sample value $\mathbf x. $ As has been noted earlier too, $\ell_\mathbf x(\theta) $ , as a function of $\theta$ , doesn't have to be a legit density function. It returns likelihood as codified in the Likelihood Principle which basically says that two likelihood functions have same information about $\theta$ if they are proportional to one another or stating in more formal terms, if $E:=(\mathbf X, \theta,\{f_\theta(\mathbf x) \}),$ is the experiment, then any conclusion about $\theta$ (measured by the evidence function $\textrm{EV}(E,\mathbf x) $ ) should depend on $E,~\mathbf x$ only via $\ell_\theta(\mathbf x).$ So, if $\ell_\mathbf x(\theta)=C(\mathbf x, \mathbf y) \ell_\mathbf y, ~\forall\theta\in\Theta,$ ( $C(\mathbf x, \mathbf y) $ would be independent of $\theta$ ) for two sample values $\mathbf x, \mathbf y, $ then the inference on $\theta$ based on either of the sample observation is equivalent. Thus likelihood functions enable us to assess the "plausibility" of $\theta:$ if $\ell_\mathbf x(\theta_2) =c\ell_\mathbf x(\theta_1),$ then it is likely that $\theta_2$ is $c ~(c>0) $ (say) times as plausible as $\theta_1.$ By the likelihood principle, for the sample value $\mathbf y, ~\ell_\mathbf y(\theta_2) =c\ell_\mathbf y(\theta_1)$ and likely $\theta_2$ is $c$ times as plausible as $\theta_1$ irrespective of whether $\mathbf x$ or $\mathbf y$ is the realized observation of the sample. Since the confusion still lingers in light of the likelihoods and priors, let me quote verbatim from $\rm [II]$ (to articulate the relationship of Bayes' theorem and likelihood function; emphasis mine): [...] given the data $\mathbf y, ~p(\mathbf y\mid\boldsymbol\theta) $ in $$p(\boldsymbol\theta\mid\mathbf y) =cp(\mathbf y\mid\boldsymbol\theta) p(\boldsymbol\theta)$$ may be regarded as a function not of $\bf y$ but of $\boldsymbol\theta.$ When so regarded, following Fisher ( $1922$ ), it is called the likelihood function of $\boldsymbol\theta$ for given $\mathbf y$ and can be written $l(\boldsymbol\theta\mid\mathbf y). $ We can thus write Bayes' formula as $$ p(\boldsymbol\theta\mid\mathbf y) =l(\boldsymbol\theta\mid\mathbf y)p(\boldsymbol\theta).$$ In other words, Bayes' theorem tells us that the probability distribution for $\boldsymbol \theta$ posterior to the data $\bf y$ is proportional to the product of the distribution for $\boldsymbol\theta$ prior to the data and the likelihood for $\boldsymbol\theta$ given $\mathbf y. $ Reference: $\rm [I]$ Statistical Inference, George Casella, Roger L. Berger, Wadsworth, $2002, $ sec. $6.3.1, $ pp. $290-291, ~293-294.$ $\rm [II]$ Bayesian Inference in Statistical Analysis, George E. P. Box, George C. Tiao, Wiley Classics, $1992, $ sec. $1.2.1, $ pp. $10-11.$
