[site]: datascience
[post_id]: 13096
[parent_id]: 13010
[tags]: 
This is an interesting question. I also see people mentioning Spark as de-facto. Here are my two cents on the same Big Data: The size of the data and the goal is the key here. Goal could be defined as Decrease computation time for data wrangling Efficient storage Handling multiple file formats coming from different sources Ability to provide data summarization so one can keep track of the data being wrangled. Its very easy to make mistakes and handling missing data the right way is more difficult than it seems. R for BigData: A lot of wrangling can be done using R itself, if one has been able to resolve the problem of storage. May NAS or our good friend HDFS. The number of aggregators R provide is very detailed and mature(less buggy). But, one problem with R is that the syntax is not consistent and it becomes difficult to mange the code over a peroid of time. Also, there is a SparkR interface if you decide to go down the route of Spark. python for BigData: Pandas and Numpy provide a pretty good package for handling data in an efficient way. Types can be guaranteed through type-casting and not just depending on duck-typing. There is a binding with Spark called pyspark which could be useful if one decides to move from python based data-structure to spark dataframes/RDDs. data wrangling in python Throwing in another one scala for BigData: Spark is natively written in Scala and is a good candidate for data wrangling and data modeling. Unlike python and R it support functional paradigm as well OOP as a first class citizens hence allowing the ease to write manageable code with static typing. Twitter guys have a bunch of libraries to help one in data-wrangling e.g. algebird , scala collection api , shapeless , slick And then as other have mentioned there are other offering such as Pig, Hive(is batch oriented). Checkout Apache Drill( May be that suits your need better ). And if you land up using Spark then Spark SQL is an option too. To sum it up, would suggest to start with python to be safe and then move to pyspark if you feel that the size is not manageable anymore and you need more sophisticated data-structure. And once you realize your needs may be use Scala for production code. Let me know if I could be of help or you need more information or guidance in anyway. Thanks
