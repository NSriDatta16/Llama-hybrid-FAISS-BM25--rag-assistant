[site]: crossvalidated
[post_id]: 330025
[parent_id]: 
[tags]: 
Estimating standard error of Monte Carlo integration, non-MCMC version

Let us suppose that we're to evaluate the expectation of a random variable $h$ with respect to some distribution $\pi$, $\text{E}_{\pi}[h]$. The standard Monte Carlo estimate, using a sample of $X_1, X_2, \dots, X_N$ generated from $\pi$, is the familiar empirical average $$ \bar{h}_N = \frac{1}{N}\sum_{i = 1}^{N} h(X_i). $$ And to assess the goodness of convergence, the MC standard variance $$ \sigma_{\text{MC}}^2 = \frac{1}{N^2} \sum_{i = 1}^{N} \left[ h(X_i) - \bar{h}_N\right]^2 $$ is almost always the one quoted in the textbooks. The conundrum, as it appears to me, is that the derivation of $\sigma_{\text{MC}}$ assumes too much "goodness" on the part of the sampler, and the value found by na√Øve evaluation of the above formula might be "too good to be true." Specifically, it seems the usefulness of $\sigma_{\text{MC}}$ as a convergence diagnostic hinges on the central limit theorem, but what about establishing the sufficient condition for the CLT to hold in practice? It seems to me that this is the reason why significant literature has been dedicated to the science + art of monitoring convergence, for establishing the conditions of trusting a particular standard error estimate a priori seems too uncertain. However, most of the literature I can find appears in the context of Markov chain Monte Carlo. My question is about this: Are there some in-depth discussions about the estimation of the MC standard error in the context of MC methods other than Markov chains, or generic MC methods? Any help is heartily appreciated. Edit: To be specific, I'd like to know more about the estimation of MC standard error for the following sampling methods: Sampling from $\pi$ by transformation on pseudo-random numbers $\xi$'s as generated by a PRNG such as the Mersenne Twister. For example, the Box--Muller method of sampling from the normal distribution, in theory, produces iid samples, provided the underlying pseudo-random number $\xi$'s are iid uniform. Should this assumption be a concern with current widely-used samplers, such as those from numpy and R implementations? Those based on rejection sampling, which is also affected by the quality of sampling a basic random variable with PRNG. Even if the MC sample of $\pi$ is "close" to iid, are there cases of an "ill-conditioned" $h(X)$ function that pathologically amplifies the correlations in the sample, thereby making the convergence of $\bar{h}_N \to \text{E}_{\pi}[h]$ problematic? Some that I can think of would be those $h$'s without expectation or variance, but are there more subtle examples that I need to be aware of?
