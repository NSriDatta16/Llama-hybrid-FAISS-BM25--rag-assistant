[site]: crossvalidated
[post_id]: 471574
[parent_id]: 218542
[tags]: 
The choice of the activation function for the output layer depends on the constraints of the problem. I will give my answer based on different examples: Fitting in Supervised Learning: any activation function can be used in this problem. In some cases, the target data would have to be mapped within the image of the activation function. Binary decisions: sigmoid or softmax . Examples: Supervised Learning: classification of images in two classes A/B (cats/dogs, number/letter, art/non-art): sigmoid : the output could correspond to the confidence $c$ (valued between 0 and 1) that the image belongs to the first class. The value $1-c$ could be interpreted as the confidence that the image belongs to the second class. softmax : the outputs could be interpreted as the confidences $c_1$ and $c_2$ that the image belongs to each class. Reinforcement Learning: button actions in policy gradient: sigmoid : the output could correspond to the probability $p$ of pressing the button; the probability of not pressing it would be $1-p$ . softmax : the outputs would correspond to the probabilities $p_1, p_2$ of pressing/not pressing the button. Multiple decisions: softmax . Examples: Supervised Learning: classification of images in multiple classes A/B/C/... (for example, classification of digits 0/1/2/3/4/5/6/7/8/9; see MNIST ) softmax : in these cases the softmax function is usually chosen so that the sum of all the confidences $\{c_i\}_i$ adds up to 1. The most "reliable" class would have an output closer to $1$ . maxout : however, it is also possible to choose the class according to the maximum value of the layer (see this paper ). Reinforcement Learning: joestick-type actions in policy gradient; actions that exclude other actions (see openai gym environments; for example LunarLander-v2 where possible actions are "do nothing, fire left orientation engine, fire main engine OR fire right orientation engine."): softmax : the outputs of this function would represent the probabilities $\{p_i\}_i$ of choosing each action. others: they are not usually used since one needs to parameterize the probability of each action. There could be some other way to parameterize the probabilities, but that would probably result in a complicated expression for the computation of the gradient: $$ \nabla_{\boldsymbol{\theta}} \log \pi_{\boldsymbol{\theta}}( a_k | s_k ) $$ Continuous actions in policy gradient (reinforcement learning); actions that do not take discrete values ​​(see openai gym environments; for example BipedalWalker-v2 where the actions are the amount of torque applied to each joint of the robot): in these cases a probability distribution is usually defined, and used to choose the actions; each action has an associated probability density. The output layers would parameterize the probability distribution. A couple of examples of distributions would be: Normal distribution parametrized by the mean $\mu$ and variance $\sigma^2$ : in this case an output layer would provide the mean of the distribution, and another one would provide the variance: if $\mu$ can take values ​​on all $\mathbb{R}$ , activation functions like identity , arcsinh , or even lrelu could be used. if $\mu$ can take values ​​in a range $(a, b)$ , activation functions such as sigmoid , tanh , or any other whose range is bounded could be used. for $\sigma^2$ it is convenient to use activation functions that produce strictly positive values ​​such as sigmoid , softplus , or relu . Beta distribution parameterized by $a$ and $b$ : in this case, $a, b> 0$ , so any activation function that produces values ​​greater than $0$ could be convenient. However, this distribution usually presents problems when $a, b , which is why activation functions are often designed to produce values ​​greater than or equal to $1$ . A toy example would be softplus+1 . ***As a side note on the choice of activation functions in the HIDDEN layers. Only if you are interested in how different activation functions perform, please check the following video: https://www.youtube.com/watch?v=Hb3vIYUQ_I8
