[site]: crossvalidated
[post_id]: 33280
[parent_id]: 33269
[tags]: 
I find it surprising that a flat likelihood produces convergence issues: it is usually the opposite case that causes problems! The usual first check for such situations is to make sure that your posterior is proper : if not it would explain for endless excursions in the "tails". If the posterior is indeed proper, you could use fatter tail proposals like a Cauchy distribution... And an adaptive algorithm Ã  la Roberts and Rosenthal. If this still "does not work", I suggest considering a reparameterisation of the model, using for instance (i.e. if there is no other natural parametrisation) a logistic transform, $$ \varphi(x) = \exp(x)/\{1+\exp(x)\} $$ (with a possible scale parameter), which brings the parameter into the unit square. Regarding the earlier answers, Gibbs sampling sounds like a more likely solution than accept-reject, which requires finding a bound and scaling the t distribution towards the posterior, which did not seem feasible for the more robust Metropolis-Hastings sampler...
