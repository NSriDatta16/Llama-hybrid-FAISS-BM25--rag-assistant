[site]: crossvalidated
[post_id]: 460598
[parent_id]: 
[tags]: 
Mixture or Convolution

tl;dr is final paragraph at the bottom. I have read the posts explaining the differences between mixture distributions and convolutions of distributions, but am having a hard time understanding which applies to my real world problem. I am measuring the particle size distribution of a sample using a sedimentation technique. Long story short(ish) it measures the amount of particles that pass the sensor with time. And the sensor moves! Time and sensor position (sedimentation velocity) are related to the size of the particles, so by this method the PSD is determined. Actually the cumulative distribution is measured because of the nature of the settling - not sure if this makes a difference but, currently I have been converting this to the corresponding density distribution for the below. However, there is a school of thought that says the particles that are detected latest (smaller ones sediment slowest) are influenced by Brownian motion. If you imagine a completely monosized distribution (like a Dirac function of particles) you'd actually measure a slight spread of sizes due to some of these particles are buffeted upwards (so detected later - measured in a smaller particle-size bin) and some are buffeted downwards (so detected earlier - measured in a larger particle-size bin). Of course, in reality this is happening throughout your sample size ranges - and the amount of spreading is increasing with time (i.e. it gets larger for smaller particles). Therefore, the Brownian motion has the effect of adding a Gaussian noise to my measurement that gets larger as the measurement goes on (i.e. as I'm measuring smaller and smaller particles). Now, there's two ways I can see to solve this problem and try to eliminate the influence of the Brownian motion (the first of which was thanks to asking half of this question already). Analogy with Image Processing This could be thought of as your true size distribution being convolved with a point spread function that increases in width to the smaller end of my particle size distribution. I've looked into various image processing techniques but they didn't seem to quite work (or I couldn't bend my mind to understand how to get them to work) with my situation. I did manage to find the decon R package, which allows to deconvolve a noise factor from distribution data - including heteregeneous noise, which I need. I've got it working but it seems to make huge changes to my measured distribution that seem unphysical. I think. Thinking more about this I came across the idea of mixture distributions, and the explanations here that these are not the same as deconvolutions. And now I'm confused because I can seem to apply both to my situation. So my other approach is to: Analogy with Mixture Distributions Consider my particle size distribution (again, I'm converting my measured cumulative to an effective density - if that matters) as a mixture of Gaussian distributions. If I can determine those (effectively infinite) Gaussians - presumably with something like mixtools - the standard deviations of which will be related to the amount of Brownian motion experienced - then I can start to tend their standard deviations to zero and come up with my "true" size distribution. Summary So there's those two approaches - the latter of which seems much more challenging to implement - and I can't understand which is more correct for my situation. To be honest, while I can appreciate the differences between the two, in the case of an effectively infinite number of distributions being mixed, it seems to me the two approaches are the same for my case? tl;dr I guess then, my question is..... if I have some measurement of a distribution that I know is affected by normal errors (that have varying std dev throughout the process) should my measured distribution be thought of as a mixture of an (effectively infinite) number of Gaussian distributions (with appropriate weights), or as a convolution of a single noise generating Gaussian (that annoyingly has a varying std dev) with the true distribution, or are those cases degenerate when the mixture has an effectively infinite number of Gaussians?
