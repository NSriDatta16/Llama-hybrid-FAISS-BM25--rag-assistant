[site]: crossvalidated
[post_id]: 121077
[parent_id]: 
[tags]: 
cross-validation to predict distribution of errors on finite test sets

In one use of k-fold cross-validation for evaluating classifiers, one trains k models, each on n(k-1)/k examples, and tests each on n/k examples. The average accuracy on those k test sets of size n/k is used as an estimate of the accuracy of a classifier trained on n examples. There's different ways to define the true accuracy that we're estimating. One is the expected value of a 0/1 (incorrect/correct) random variable that would result by selecting a random instance from the population and applying the classifier. This expectation is the same as the expectation of accuracy on randomly selected test sets of any size m, not just 1. (Note while true for accuracy, this would not be true for other effectiveness measures, e.g. Van Rijsbergen's F-measure.) However, suppose one wants to estimate the distribution of observed accuracies that one would see from applying the classifier across lots of randomly selected test sets of size m. One application would be to estimate the probability of having a "failing grade" on a held out test set of size m. The true distribution of test set accuracy on a test set of size m is binomial with parameters m and p, where p is the true error rate of the classifier. But it's not obvious to me that simply plugging in m and the k-fold CV estimate of p into a binomial is the best thing to do. It certainly ignores the fact that we have uncertainty about our estimate of p. Beyond that, my real interest is in measures like the F-measure where the distribution would not be binomial even if we did know the true p. Does anyone know of literature on this question? The question is clearly related to the notion of prediction intervals ( http://en.wikipedia.org/wiki/Prediction_interval ) but with the added complexity of cross-validation.
