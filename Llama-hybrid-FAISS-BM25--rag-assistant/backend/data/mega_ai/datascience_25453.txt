[site]: datascience
[post_id]: 25453
[parent_id]: 25443
[tags]: 
Try the following code. It must work fine. It might take a lot of time (more than 100 features). Change max_depth to 6 if you want more accuracy. (because of 100 features.) We can change learning_rate between 0 and 1, to improve the efficiency. import xgboost as xgb model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, learning_rate=0.05, max_depth=3, min_child_weight=1.7817, n_estimators=4200, reg_alpha=0.4640, reg_lambda=0.8571, subsample=0.5213, silent=1, nthread=-1) X_train, X_test, Y_train, Y_test= train_test_split(X, Y, random_state= 0) def model_score_error(model): prepared_model=model.fit(X_train, Y_train) x=prepared_model.score(X_test,Y_test) print('Score: ',x) Target_predicted=prepared_model.predict(X_test) MSE=mean_squared_error(Y_test,Target_predicted) print('mean square error', MSE) model_score_error(model_xgb)
