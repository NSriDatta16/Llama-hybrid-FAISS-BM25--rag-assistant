[site]: crossvalidated
[post_id]: 396653
[parent_id]: 
[tags]: 
Why it is hard for `xgboost` to learn periodic functions?

In this simple example, I try to train a xgboost regressor to learn a periodic function: import numpy as np from xgboost import XGBRegressor as r size = 1000 sine_x = np.linspace(1, size, size).reshape(size, 1) sine_y = np.sin(sine_x) + 0.3 * np.random.uniform(0, 1) sine_model = r(max_depth = 5, n_estimators = 1) sine_model.fit(sine_x, sine_y) sine_model.get_booster().dump_model('sine_model.txt', with_stats=True) square_x = np.linspace(1, size, size).reshape(size, 1) square_y = np.square(square_x) + 0.3 * np.random.uniform(0, 1) square_model = r(max_depth = 5, n_estimators = 1) square_model.fit(square_x, square_y) square_model.get_booster().dump_model('square_model.txt', with_stats=True) From the result, when the training dataset is sampled from $y = x^2$ , xgboost does well in the sense that it divides the instances of a parent node evenly to each of its child nodes. However, when faced with the dataset sampled from $y = \text{sin}x$ , the binary trees grow in each step are totally unbalanced, almost all the instances of a parent node will go into one of the child nodes. How to intuitively understand this?
