[site]: datascience
[post_id]: 114913
[parent_id]: 
[tags]: 
Training XGBoost on time series features of varying sample length

I have some time series data that contain features that that go back anywhere from 5 to 50 years. I've considered imputation (e.g. taking the mean), but I'm not sure it's feasible to impute such large chunks of missing data. My initial thought was to partition the into three groups (e.g. I'm not sure how redundant this is since XGBoost is already an ensembled model. Moreover, I'm expecting each model to perform poorly since it's missing features, and I'm not sure if the relationship across groups of features are learned during ensembling. Are there more tried-and-true approaches to the problem of dealing with feature of varying sample lengths?
