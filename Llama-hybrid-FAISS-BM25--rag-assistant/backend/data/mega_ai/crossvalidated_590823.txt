[site]: crossvalidated
[post_id]: 590823
[parent_id]: 590805
[tags]: 
To add to @frank's answer, the reason using dropout is not the same as training a smaller network is that the neurons that are dropped out are randomly selected each time the weights are updated. So while on each iteration only some of the neurons are used and updated, over the entire training cycle all the neurons are trained. According to Jason Brownlee's A Gentle Introduction to Dropout for Regularizing Deep Neural Networks , dropout can be thought of as training an ensemble of models in parallel.
