[site]: crossvalidated
[post_id]: 479887
[parent_id]: 470956
[tags]: 
First, you need to think about what you mean by accuracy . Do you want to minimise false alarms (false positives)? Minimise the number of drops that you miss (false negatives)? Some combination of the two? Many textbooks provide good discussions of this , but Wikipedia is also a good place to start. Now, to the question. To control the false positive rate, you can calculate the one-tailed p-value : the probability of observing a conversion rate as low as that in the previous hour or lower, under the null hypothesis that there's nothing wrong with the app. In your case, this p-value can be calculated using a binomial test , where the null hypothesis is that $P(\text{Convert}) = 0.4$ . Importantly, this test deals with with the fact that the app open count fluctuates from hour to hour, and that the conversion rate is more variable at times when fewer people open the app. If you set the alert to fire when $p , or $\frac{1}{20}$ , and run the test every hour, you will on average have one false positive every 20 hours when the app is working fine. A lower threshold (e.g. $p ) will lead to fewer false positives (one every 100 hours), but also makes it harder to detect real problems. How likely are you to detect problems when they do arise? As your tags suggest, this is called the power of the test. This depends on a) your p-value threshold (call this $\alpha$ ), b) how low the conversion rate goes when problems occur, and c) how many people open the app each hour. This calculator gives you power for a binomial test given all of these values. The screenshot below shows a calculation assuming that a) we fire an alert if $p , b) the conversion rate goes down to 30% when there is an issue, and c) 200 people opened the app in that hour. Finally, you can always reduce both your false positive and your false negative rates by simply using a longer window. If you check every two hours, rather than every hour, keeping $\alpha = 0.05$ , you will have a false positive on average once every 40 hours (20 tests), and since you're including more data in each test, the power of the test (the ability to avoid false negatives) will be greater.
