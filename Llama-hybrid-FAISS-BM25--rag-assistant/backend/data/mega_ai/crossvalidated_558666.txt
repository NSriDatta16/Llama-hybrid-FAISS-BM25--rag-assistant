[site]: crossvalidated
[post_id]: 558666
[parent_id]: 558644
[tags]: 
First, excluding $X$ s that have low or zero correlation to $y$ from the model may not be smart, as it is possible that they are important in a multiple regression even if they are not in simple regressions. There are examples where $y$ is only weakly correlated with each of $x_1$ and $x_2$ , but $y$ is almost completely determined by a linear combination of $x_1$ and $x_2$ . Second, how many variables get nonzero coefficients in lasso depends on penalty intensity. As Dave mentions in a comment, you can get all of the variables to have nonzero coefficients with a sufficiently weak penalty and none of them have nonzero coefficients with a sufficiently strong penalty. Cross validation is a popular way of choosing penalty intensity. If your tuning target is minimization of prediction loss on the left-out folds, you will only keep these regressors which help achieve the target. It should produce a model that does well in out-of-sample forecasting as measured by the loss function used in tuning. Or maybe there is a collinearity issue between variables? If yes, do I need to check the correlation of all variables in a paired way (too much calculation). Having multicollinear or highly correlated regressors will make it likely that some of them get kicked out, as they are not contributing to reduction of prediction loss to a noteworthy degree, but their estimation variance is contributing to increased prediction loss. You do not need to check the correlation of all pairs of variables, however. First, lasso does the job for you by kicking out whatever variables are unhelpful in minimizing prediction loss. Second, pairwise correlations do not tell the whole story, with larger groups of variables (triples and larger) potentially being near or fully collinear, making one or more variables in the group more or less redundant in prediction. (This is analogous to the argument made in the first paragraph.)
