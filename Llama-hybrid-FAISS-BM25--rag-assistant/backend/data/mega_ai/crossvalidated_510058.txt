[site]: crossvalidated
[post_id]: 510058
[parent_id]: 364917
[tags]: 
In general, learning a discontinuous function is hard as the error around the point where the value jumps are usually large. However, in practice, there are ways to get around the problem by making the data less abrupt. For example, in your case, the function jumps when x = 100. We can change it to function: g(x) = 50, x >= 100 g(x) = 25, x We left the value for the function undefined when 0 g(x) = f(x), x >= 100 g(x) = f(x+100), x Learning g(x) is much easier as there are no abrupt jumps. The neural network is free to choose any value for g(x) when x is between 0 and 100. It is easy to see, one sigmoid function will be enough to describe the sampled data from g(x). After g(x) is learned, it is trivial to convert it back to f(x). This approach works when there are a limited number of know discontinuous points.
