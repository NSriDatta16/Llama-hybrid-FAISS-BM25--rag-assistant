[site]: datascience
[post_id]: 507
[parent_id]: 497
[tags]: 
Back in my data analyst days this type of problem was pretty typical. Basically, everyone in marketing would come up with a crazy idea that the sold to higher ups as the single event that would boost KPI's by 2000%. The higher ups would approve them and then they would begin their "test". Results would come back, and management would dump it on the data analysts to determine what worked and who did it. The short answer is you cant really know if it wasn't run as a random A/B style test on like time periods. But I am very aware of how deficient that answer is, especially if the fact that a pure answer doesn't exist is irrelevant to the urgency of future business decisions. Here are some of the techniques I would use to salvage the analysis in this situation, bear in mind this is more of an art then a science. Handles A handle is something that exists in the data that you can hold onto. From what you are telling me in your situation you have a lot of info on who the marketing agency is, when they tried a tactic, and to which site they applied it to. These are your starting point and information like this going to be the corner stone of your analysis. Methodology The methodology is going to probably hold the strongest impact on which agencies are given credit for any and all gains so you are going to need to make sure that it is clearly outlines and all stake holders agree that it makes sense. If you cant do that it is going to be difficult for people to trust your analysis. An example of this are conversions. Say the marketing department purchases some leads and they arrive at our landing page, we would track them for 3 days, if they made a purchase within that time we would count them as having been converted. Why 3 days, why not 5 or 1? Thats not important as long as everyone agrees, you now have a definition you can build off of. Comparisons In an ideal would you would have a nice A/B test to prove a definitive relationship, I am going to assume that you are running short on those, still, you can learn something from a simple comparison of like data. When companies are trying to determine the efficacy of radio advertising they will often run ads on offset months in the same market, or for several months in one market and compare that with the results in a separate but similar market. Its doesn't pass for science, but even with all that noise a strong results will almost always be noticeable. I would combine these in your case to determine how long an event is given to register an effect. Once you have the data from that time period run it against your modeled out traffic prediction, week over week growth, month over month etc. Which, can then allow a meaningful comparison between agencies, and across time periods. Pragmatism The aspiration is to be able to provide a deep understanding of cause and effect, but it is probably not realistic. Because of how messy outside factors make your analysis, you are constantly going to run up against the question over and over again: Did this event raise volume/sales/click throughs, or would doing anything at all have had the same effect? The best advise I can give for this is set very realistic goals for what you are looking to measure. A good starting point is, within the methodology you have, which event had the largest impact. Once you have those open your aperture from there. Summary Once you have reasoned out all of these aspects you can go about building a general solution which can then be automated. The advantage to designing your solution in this manner is that the business logic is already built in. This will make your results much more approachable and intuitive to non-technical business leaders.
