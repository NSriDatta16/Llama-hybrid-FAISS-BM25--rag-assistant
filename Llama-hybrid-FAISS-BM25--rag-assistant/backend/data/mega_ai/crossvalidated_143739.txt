[site]: crossvalidated
[post_id]: 143739
[parent_id]: 143690
[tags]: 
Don't remove the misclassified examples, as they give information about the correct location of the decision boundary. The decision surface for an SVM is defined by the support vectors, i.e. those patterns that lie on or within the margins, so if you selectively delete them, none of the theory underpinning the SVM will be valid. This can be a useful thing to do in order to reduce the run time expense of the classifier (or alternatively flipping the labels of misclassified patterns, see this paper ), but I wouldn't expect it to be a good idea from the perspective of generalisation. Also don't pay too much attention to the training set accuracy, it isn't much of a useful indicator of the quality of the classifier as classifiers that have over-fitted the training data will have an unrealistically optimistic training set error. For semi-supervised learning (using self-training), it may be better to only use those unlabelled patterns where the output of the SVM has an absolute magnitude greater than one (i.e. the SVM is confident of the classification, rather than being a bit uncertain).
