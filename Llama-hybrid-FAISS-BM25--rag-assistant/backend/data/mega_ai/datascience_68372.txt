[site]: datascience
[post_id]: 68372
[parent_id]: 68357
[tags]: 
Your linked paper appears to be wrong about feature subsetting. I couldn't find it in the documentation for randomForest, but the source for randomForest uses randomTree for the base models, and in that documentation it says a tree that considers K randomly chosen attributes at each node. So the selection seems to happen at each split. (Note that xgboost has feature subsetting at each of the tree, the level (depth), and the node. I don't see any obvious reason that one or more of these options should always be preferable...) For default number of features, sqrt(m) is the most common, but it looks like Weka uses lg(m). See option -K at https://weka.sourceforge.io/doc.dev/weka/classifiers/trees/RandomForest.html Yes, Weka uses the Quinlan family of decision trees, which split using information gain (as opposed to CART, which uses gini).
