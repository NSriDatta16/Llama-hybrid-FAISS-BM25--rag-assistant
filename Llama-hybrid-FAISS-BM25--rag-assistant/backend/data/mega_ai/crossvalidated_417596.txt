[site]: crossvalidated
[post_id]: 417596
[parent_id]: 348154
[tags]: 
I'll give a perspective on this question in your post: Why do we perform inference to create a generative model when a generative model is defined by the likelihood and prior? The generative model for particular observation $x$ $$ p(x) = \int p(x|z) p(z) dz \approx \sum_k p(x|z_k) p(z_k) $$ which can be considered as a mixture model over all possible values of $z$ . In training however it would be inefficient to consider all possible values of $z$ because most of them correspond to $p(z)$ that is near zero. Instead, a VAE simultaneously trains an encoder (approximate posterior) that maps an observed datup $x$ to a distribution $z$ that could have produced that $x$ . In implementations almost always a single sample from the encoder distribution is taken, so it seems weird to talk about a mixture, but in a few papers the use of multiple samples is discussed (and I think at least one argues that the quality improves slightly when this is done). Regardless, I think the original thinking for VAEs considered the implementation (even with one sample) as an approximation of the continuous generative model.
