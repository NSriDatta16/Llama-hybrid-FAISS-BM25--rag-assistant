[site]: crossvalidated
[post_id]: 572496
[parent_id]: 
[tags]: 
Weighting the R-squared as a measure of goodness-of-fit in Linear Regression

I have two observed time series $x_i$ and $y_i$ and I want to test if $x_i$ is a good predictor of of $y_i$ . So I run a simple linear regression Y ~ X and use $R^2$ as a measure of goodness of fit. However $x_i$ and $y_i$ are both noisy observations and don't have constant variance: for each entry i, the uncertainty in $x_i$ is $w_i = var(x_i)$ and uncertainty in $y_i$ is $v_i = var(y_i)$ . So when I look at $R^2$ , I want to make sure that observations with the smallest variance $var(x_i)$ and $var(y_i)$ have a higher weight . Intuitively, I want the samples where there is less variance in the observation to have a bigger weight in the regression. How can I achieve this? What should be the right metric / $R^2$ for this problem? I tried to look into Weighted Least Squares but it seems this is more about interpreting the error residuals and make the regressor BLUE. I also tried dividing both sides by the variance Y/var(Y) ~ X/var(X) but I am not sure of the statistical soundness of this approach.
