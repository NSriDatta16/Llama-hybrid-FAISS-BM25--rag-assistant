[site]: crossvalidated
[post_id]: 511477
[parent_id]: 
[tags]: 
Why identity mapping is so hard for deeper neural network as suggested by Resnet paper?

In resnet paper they said that a deeper network should not produce more error than its shallow counterpart since it can learn the identity map for the extra added layer. But empirical result shown that deep neural networks have a hard time finding the identity map. But the solver can easily push all the weights towards zero and get an identity map in case of residual function( $\mathcal{H}(x) = \mathcal{F}(x)+x$ ). My question is why it is harder for the solver to learn identity maps in the case of deep nets? Generally, people say that neural nets are good at pushing the weights towards zero. So it is easy for the solver to find identity maps for residual function. But for ordinary function ( $\mathcal{H}(x) = \mathcal{F}(x)$ ) it have to learn the identity like any other function. But I do not understand the reason behind this logic. Why neural nets are good to learn zero weights ?
