[site]: crossvalidated
[post_id]: 471755
[parent_id]: 471056
[tags]: 
The answer is that the way I call PCA from sklearn results in the covariates being centered to have $0$ mean (but not unit variance). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD. (The emphasis is mine.) I also found another mistake in the PCA code: I was calling PCA on all four predictors, not just the three covariates. This explains why $\hat{\beta_1}$ was changing between the two models... $x_1$ wasn't the same in both! When I center the covariates and only do PCA on them, I get the same intercept and $\hat{\beta_1}$ (code and output below). What I have found is that, while the standard errors inflate when I look at a predictor that is correlated with another predictor, the standard error on $\hat{\beta_1}$ remains about the same whether the covariates are correlated or not, so running PCA on covariates in an ANCOVA-style regression problem with multiple correlated predictors does not help. import numpy as np import statsmodels.api as sm from sklearn.decomposition import PCA import scipy.stats import sys np.random.seed(2020) # Define sample size # N = 50 # Define the parameter 4-vector WITHOUT an intercept # beta_1 = np.array([0.2, 1, -1, -1]) # Define categorical predictor # g = np.random.binomial(1, 0.5, N) # Define covariance matrix of covariates # S = np.array([[1, -0.8, 0.7], [-0.8, 1, -0.8], [0.7, -0.8, 1]]) # Define matrix of covariates # covs = np.random.multivariate_normal(np.array([0, 0, 0]), S, N) # Center the covariates # cov0 = covs[:,0] - np.mean(covs[:,0]) cov1 = covs[:,1] - np.mean(covs[:,1]) cov2 = covs[:,2] - np.mean(covs[:,2]) covs = np.c_[cov0, cov1, cov2] # Combine all predictors into one matrix # X = np.c_[g, covs] # Make three PCs and add them to g to give the PCAed model matrix # pca = PCA(n_components=3) pca.fit(covs) diag = pca.transform(covs) X_pca = np.c_[g, diag] # Simulate the expected value of the response variable # y_hat = np.matmul(X, beta_1) # Simulate error term, using the mean as the intercept, beta_0 # err = np.random.normal(3, 1, N) # Simulate response variable # y = y_hat + err # Fit full model on original data # orig = sm.OLS(y, sm.tools.add_constant(X)).fit() # Fit full model on PCAed data # pca_ed = sm.OLS(y, sm.tools.add_constant(X_pca)).fit() print(orig.summary()) print(pca_ed.summary()) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.677 Model: OLS Adj. R-squared: 0.648 Method: Least Squares F-statistic: 23.56 Date: Fri, 12 Jun 2020 Prob (F-statistic): 1.49e-10 Time: 07:53:13 Log-Likelihood: -65.894 No. Observations: 50 AIC: 141.8 Df Residuals: 45 BIC: 151.3 Df Model: 4 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 2.2753 0.188 12.111 0.000 1.897 2.654 x1 1.0204 0.273 3.741 0.001 0.471 1.570 x2 0.8992 0.256 3.511 0.001 0.383 1.415 x3 -1.0757 0.251 -4.286 0.000 -1.581 -0.570 x4 -0.9662 0.313 -3.091 0.003 -1.596 -0.337 ============================================================================== Omnibus: 0.231 Durbin-Watson: 2.074 Prob(Omnibus): 0.891 Jarque-Bera (JB): 0.429 Skew: 0.033 Prob(JB): 0.807 Kurtosis: 2.551 Cond. No. 4.32 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.677 Model: OLS Adj. R-squared: 0.648 Method: Least Squares F-statistic: 23.56 Date: Fri, 12 Jun 2020 Prob (F-statistic): 1.49e-10 Time: 07:53:13 Log-Likelihood: -65.894 No. Observations: 50 AIC: 141.8 Df Residuals: 45 BIC: 151.3 Df Model: 4 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 2.2753 0.188 12.111 0.000 1.897 2.654 x1 1.0204 0.273 3.741 0.001 0.471 1.570 x2 -0.6313 0.087 -7.233 0.000 -0.807 -0.455 x3 -0.3441 0.285 -1.207 0.234 -0.918 0.230 x4 -1.5435 0.371 -4.164 0.000 -2.290 -0.797 ============================================================================== Omnibus: 0.231 Durbin-Watson: 2.074 Prob(Omnibus): 0.891 Jarque-Bera (JB): 0.429 Skew: 0.033 Prob(JB): 0.807 Kurtosis: 2.551 Cond. No. 4.32 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. ```
