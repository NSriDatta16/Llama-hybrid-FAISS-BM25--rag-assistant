[site]: crossvalidated
[post_id]: 271490
[parent_id]: 271480
[tags]: 
The learnability of functions is a very deep subject. Indeed, the whole field of Computational learning theory is based around answering when this is possible and how. Typically, you need to assume the true function comes from some parameterised model class, such as fixed structure neural networks, then consider how much data is needed so that with high probability, the generalisation error is small. As you note, you need to restrict the flexibility of the class of functions you consider. It's not possible to fit an arbitrary real function from data, since it can always vary unpredictably anywhere you don't exactly have data points. In learning theory, the standard notion of the flexibility of a class is the VC dimension . Bounds on the error in the predictions from a fitted function depend directly on the VC constant. The case of neural networks has been heavily studied .
