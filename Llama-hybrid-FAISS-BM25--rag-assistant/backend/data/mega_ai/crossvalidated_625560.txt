[site]: crossvalidated
[post_id]: 625560
[parent_id]: 
[tags]: 
Gradient Clipping of Vanilla RNNs vs LSTMs

I am doing an online course that states that the reason we use LSTMs and similar variations of vanilla RNNs is because of the vanishing/exploding gradients problems with vanilla RNNs. However, an earlier part of the course said that gradient clipping can be used to combat issues with exploding and vanishing gradients by scaling the norm of the gradient to a certain value. If this is the case, then why do we use LSTM models instead of just applying gradient clipping on a vanilla RNN?
