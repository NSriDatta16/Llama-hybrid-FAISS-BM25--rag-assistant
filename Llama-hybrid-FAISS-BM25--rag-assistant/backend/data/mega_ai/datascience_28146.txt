[site]: datascience
[post_id]: 28146
[parent_id]: 20274
[tags]: 
As there are important questions to the scenario and data, I'm sharing some thoughts together with assumed answers to some questions rather than a complete solution. First of all, in sample data there is not clear how cases when user has not responded are modeled and/or whether each campaign has been sent to all users (which in real live is never the case as user base is changing over the time). But we can safely assume that there is this information given (either all campaign has been sent to all users - for simplicity or we now which users received with campaign). Then it can be easily imagined that some campaigns can be better suited to be sent earlier in the day, e.g. B2B services - during working hours, and some - rather in the evening, e.g. trip last minute offers. This can heavily influence the response rate for many users and we don't have any information about the campaigns themselves - no features to try to model the content of the campaign and it's impact on responsiveness. If we now that the same type of campaign has been sent always but at various time to various users, that could create a global (audience-wide) trend in responsive rate in different times of the day. Second aspect that influence responsiveness globally is the quality of the campaign - how it is appealing to the users. That could be partially inferred observing responsiveness of users when they receive various campaigns at the same times of the day. That poses a question what is the distribution of the times of the day each campaign separately and in comparison between them - whether there is enough data to try to infer that information. Assuming for starters that these are similar campaigns regarding the quality and they are time-of-the-day agnostic, we can focus on each user separately. If you look to a distribution of number of responses over the time of the day (e.g. averaged by the number of campaigns sent in the given point of time), you can spot the maximum there, which is the best one-shot candidate for the next campaign. This approach however has following caveats. For the longer term, it will create a "time point bubble" for the user trying to always send him on this time point. That would not reflect the change of the user preferences. Here you could apply moving average, e.g. over fixed time window (like "last 3 months") and/or trying other data points from time to time and probably other techniques to gain more diverse and future-proof strategy. - The maximum time point might have been an exceptional behaviour while there is much more evidence for a bit smaller number of responses but supported by more time points in other time range. To take into account various qualities of campaigns, their rating could be created by comparing responsiveness of users to which more than one campaign has been sent in the same time point. Then weigh number of responses by inverted rating of the campaign on the distribution of number of responses for the given user.
