[site]: crossvalidated
[post_id]: 409563
[parent_id]: 
[tags]: 
Proof for asymptotic error in logistic regression

Ng, A.Y., and Jordan, M.I. (2001). On Discriminative vs. Generative classifiers: A comparison of logistic regression and naive Bayes . Advances in Neural Information Processing Systems, 14, pp. 841-8, MIT Press. In the paper above, the author lists two propositions (1 & 2) saying that the asymptotical error of discriminative classifiers (e.g. logistic regression) is smaller than the one of generative classifiers (e.g. naive bayes). To prove it, the auther says, that "since the asymptotical error of h-dis converges to the one of the infimum of all linear classifiers, it must therefore asymptotically no worse than naive bayes". - How do we know, that logistic regression converges to that infimum? The second proposition gives an estimate about the error of logistic regression. - Here I can't find out the theory about convergence bounds and the VC Dimensions. I checked out the book "Statistical Learning Threory" -Vapnik , but without success Is anybody able to help me to find/prove those two propositions?
