[site]: datascience
[post_id]: 76816
[parent_id]: 
[tags]: 
How to handle Overfitting

I am working on machine learning classification problem with two classes (0/1). I would like to build a prediction model. The problem is that I have a small dataSet of shape=(89, 21) which may caused over-fitting. problem ( 20 independent variables ). I notice that the results are highly affected by the train data and test data sizes( i.e. how the split was done ). the best results with LR was 0.90 and the worst I got 0.74 . Algo I Split data using this instruction : X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.37, random_state=2) then I did an upsimpling of training set since it was unbalanced {0}=62 {1}=27 the table below contain the best results on the test-set I got (test error). $$\begin{array}{c|c} & \text{Accuracy} & \text{ROC } \\ \hline \text{LogisticRegression } & 0.909091 & 0.928571 & \\ \hline \text{DecisionTree } & 0.939394 & 0.934524 & \\ \hline \text{adaBoost } & 0.848485 & 0.845238 & \\ \hline \text{RandomForest } & 0.878788 & 0.928571 & \\ \hline \text{GradientBoosting } & 0.848485 & 0.827381 & \\ \hline \end{array}$$ the next table contain the training error $$\begin{array}{c|c} & \text{Accuracy} & \text{ROC } \\ \hline \text{LogisticRegression } & 0.963415 & 0.963415 & \\ \hline \text{DecisionTree } & 1.00 & 1.00 & \\ \hline \text{adaBoost } & 1.00 & 1.00 & \\ \hline \text{RandomForest } & 1.00 & 1.00 & \\ \hline \text{GradientBoosting } & 1.00 & 1.00 & \\ \hline \end{array}$$ I need some guideline (ideas, tutorials,...) about how to manage the over-fitting problem . Thanks
