[site]: stackoverflow
[post_id]: 3350424
[parent_id]: 431594
[tags]: 
If you are I/O bound saving human-readable text to the hard drive, I expect compression to reduce your total runtime. If you have an idle 2 GHz core, and a relatively fast 100 MB/s streaming hard drive, halving the net logging time requires at least 2:1 compression and no more than roughly 10 CPU cycles per uncompressed byte for the compressor to ponder the data. With a dual-pipe processor, that's (very roughly) 20 instructions per byte. I see that LZRW1-A (one of the fastest compression algorithms) uses 10 to 20 instructions per byte, and compresses typical English text about 2:1. At the upper end (20 instructions per byte), you're right on the edge between IO bound and CPU bound. At the middle and lower end, you're still IO bound, so there is a a few cycles available (not much) for a slightly more sophisticated compressor to ponder the data a little longer. If you have a more typical non-top-of-the-line hard drive, or the hard drive is slower for some other reason (fragmentation, other multitasking processes using the disk, etc.) then you have even more time for a more sophisticated compressor to ponder the data. You might consider setting up a compressed partition, saving the data to that partition (letting the device driver compress it), and comparing the speed to your original speed. That may take less time and be less likely to introduce new bugs than changing your program and linking in a compression algorithm. I see a list of compressed file systems based on FUSE , and I hear that NTFS also supports compressed partitions.
