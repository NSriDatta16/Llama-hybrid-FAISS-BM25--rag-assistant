[site]: crossvalidated
[post_id]: 505700
[parent_id]: 505698
[tags]: 
EDIT: This answer is attempting to explain that there is "no difference" in the residuals... The comment of jkpate above is nevertheless valid and I have the feeling that the question might also go into another direction... To be clarified... I think that you are on the path to discover the following: Time Series Analysis is nothing else than good old plain regression just with the "lag" of the target variable as additional feature. When you google for time series analysis you often find models like AR, AM, ARMA, ARIMA and all those derivatives. What I want to say is two things: Thea are nothing special and they even lack the property of including features of the 'now'. They are nothing special: Let's look at AR (Autoregressive) Model ( https://en.wikipedia.org/wiki/Autoregressive_model ): The basic equation is $$X_t = c + \sum_{i=1}^p \phi_i X_{t-i} + \epsilon_t$$ Notice that here, the $\phi_i$ do not depend on $t$ , they are always the same. What does that mean for concrete data in the case of $p=2$ for example? Let us look at the following data table: t | target 1 | 10 2 | 20 3 | 30 4 | 20 5 | 10 ... So the modelling equations would look like $$X_3 = \phi_1 X_2 + \phi_2 X_1 + c$$ $$X_4 = \phi_1 X_3 + \phi_2 X_2 + c$$ and so forth. Now let us look at this data table from a different perspective: The special thing in TSA (Time Series Analysis) is that at time $t$ you may only access anything that lies in the past, not the data from the future (that would be unrealistic and is therefore forbidden!). However, when we want to predict the target at time $3$ for example, we might include the same variable, just at past points in time. This is called 'lagging' (we lag the target variable). I will demonstrate that with the data table above: t | target | target before | target 2 steps before | 1 | 10 | NA | NA | 2 | 20 | 10 | NA | 3 | 30 | 20 | 10 | 4 | 20 | 30 | 20 | 5 | 10 | 20 | 30 | ... Let say that we do regular linear regression with 'target' as a target variable and with using the columns 'target before' and 'target 2 steps before' as features, what modelling equations do we get? It's exactly the same as above. Hence, AR is nothing special, its just like this: AR = good old linear regression with the lagged target variable as input features That is why the residuals are basically the same, e.g. residuals of AR = residuals of good old linear regression with the lagged target variable as input features How to include 'now' features? What happens if we have two features in the original dataset: time, temperature at that time (of something of interest) and the target variable like this t | temp | target 1 | 23 | 10 2 | 27 | 20 3 | 21 | 30 4 | 16 | 20 5 | 14 | 10 so now, target at time $t$ might depend on two things: the target at earlier points in time and on the temperature at time $t$ ... However, the classical AR method does not tell us how to include the temperature (or in fact, any feature that is valid for time $t$ )... So what do we do? Well, we create the same table as above (potentially also lagging temperature) and apply any good old regression tecnique like linear regression. In that way you can easily come up with your own time series model that incooperates both: the target at earlier points in time plus features in the 'now'. For example: Create the lagged table and apply gradient boosting or neural networks to it. Then you have a completely new time series model :-)
