[site]: datascience
[post_id]: 26112
[parent_id]: 
[tags]: 
Decay Parameter in Keras Optimizers

I'm currently training a CNN with Keras and I'm using the Adam optimizer. My plan is to gradually reduce the learning rate after each epoch. That's what I thought the decay parameter was for. For me, the documentation does not clearly explain how it works: decay: float >= 0. Learning rate decay over each update. However, when looking at the used learning rate in tensorboard, it stays the same as the initial learning rate. So, how does this decay parameter actually work?
