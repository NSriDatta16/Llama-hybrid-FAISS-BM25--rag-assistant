[site]: crossvalidated
[post_id]: 97397
[parent_id]: 97381
[tags]: 
TL;DR: Mixtures of normal distributions may look uniform when bin sizes are large. This answer borrows from @whuber's sample code (which I thought first was an error, but in retrospect was probably a hint). The underlying proportions in the population are equal: a = b = 0.5 . Each group, A and B has 10000 members: N = 10000 . We are going to conduct 5000 replicates of a simulation: for i in range(5000): . Actually, what we are doing is a $\rm simulation_\rm{prime}$ of a $\rm simulation_\rm{underlying}$. In each of the 5000 iterations $\rm simulation_\rm{prime}$ we will do $\rm simulation_\rm{underlying}$. In each iteration of $\rm simulation_\rm{prime}$ we will simulate a random number of A and B that are 'successes' (AKA converted) given the equal underlying proportions defined earlier: A = np.random.binomial(N, a); B = np.random.binomial(N, b) . Nominally this will yield A = 5000 and B = 5000, but A and B vary from sim run to sim run and are distributed across the 5000 simulation runs independently and (approximately) normally (we'll be coming back to that). Let's now step through $\rm simulation_\rm {underlying}$ for a single iteration of $\rm simulation_\rm{prime}$ in which A and B have taken on an equal number of successes (as will be the average the case). In each iteration of $\rm simulation_\rm{underlying}$ we will, given A and B, create random variates of the beta distribution for each group. Then we will compare them and find out if ${\rm Beta}_A > {\rm Beta}_B$, yielding a TRUE or FALSE (1 or 0). At the end of a run of $\rm simulation_\rm {underlying}$, we have completed 15000 iterations and have 15000 TRUE/FALSE values. The average of these will yield a single value from the (approximately normal) sampling distribution of the proportion of ${\rm Beta}_A > {\rm Beta}_B$. Except now $\rm simulation_\rm{prime}$ is going to select 5000 A and B values. A and B will rarely be exactly equal, but the typical differences in the number of A and B successes are dwarfed by the total sample size of A and B. Typical As and Bs will yield more pulls from their sampling distribution of proportions of ${\rm Beta}_A > {\rm Beta}_B$, but those on the edges of the A/B distribution will also get pulled. So, what in essence we pull over many sim runs is a combination of sampling distributions of ${\rm Beta}_A > {\rm Beta}_B$ for combinations of A and B (with more pulls from the sampling distributions made from the common values of A and B than the uncommon values of A and B). This results in mixtures of normal-ish distributions. When you combine them over a small bin size (as is the default for the histogram function you used and was specified directly in your original code), you end up with something that looks like a uniform distribution. Consider: a = b = 0.5 N = 10 samples = [] #collects the values of S for i in range(5000): assert a==b A = np.random.binomial(N, a); B = np.random.binomial(N, b) S = (beta.rvs(A+1, N-A+1, size=15000) > beta.rvs(B+1, N-B+1, size=15000)).mean() samples.append(S) P.hist(samples,1000) P.show()
