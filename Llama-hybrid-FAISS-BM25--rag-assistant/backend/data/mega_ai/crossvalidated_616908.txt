[site]: crossvalidated
[post_id]: 616908
[parent_id]: 616887
[tags]: 
From an ML perspective, there's not really a precise definition of whether you've overfit or not. It's also not binary, all models have picked up some signal at the expense of some noise, and are thus somewhat overfit. Being a little overfit is the cost of learning. The delta between training and test performance is a good guide, but a nice (admittedly pathological) example of where this will give you incorrect results are Random Forests which intentionally will get very good training loss knowing that this loss won't be achievable on a test set. Nonetheless, a random forest might allow you to achieve better test loss than another model which has a smaller gap between train and test loss, and in this case the Random Forest is the better model. Looking at the output from your training process, your validation loss has gone down for the entirety of the training process... so you've probably got the best you could have got out of that particular model setup. I guess I would categorically say you'd overfit if you had overshot the optimal point and your validation loss had started to go up. But you could have another model which you'd fit optimally (i.e. stopped training when your validation loss stopped going down), which had a bigger/smaller gap between training and validation loss, but the all-important metric is which of the two models performs better on the test set. So I personally think the question of "have I overfit" isn't quite the right one. You should always use the model that performs best on the test set, whilst taking great care to make sure your test is valid and you've not leaked information from the training into the test set or used information you wouldn't have available to you in prod (e.g. data that theoretically exists but only comes back from the data provider in a batch ever 24hrs and thus not available at the time you want to make the prediction) Also, seeing as you mention some of the metrics you've used, some thoughts on this: Fundamentally, the problem you're trying to solve is "should I deploy this model" so you want to understand whether it's sufficiently better than your current baseline (be that an ML model or just a business process, which could be "do nothing") so as to warrant putting into production. To that end, metrics like F1-score, or AUC aren't really what you need. When it comes to fraud, it would be typical to have an estimate of what a false positive costs you (presumably every predicted positive leads to an extra level of verification being introduced which leads to some dropoff, and you can make some assumptions on the typical range / maybe you have some data on this), as well as what a false negative costs you (essentially the cost of having to reimburse the transaction I would imagine) Then, at different thresholds, you can quantify how much money you're losing to the combination of fraud + dropouts and compare that to your current procedure. That will lead you to a number of how much money deploying this model could make/lose you. [Disclaimer: it's a little more complicated than this, there are secondary metrics, reputational concerns etc, this was more meant to be indicative]
