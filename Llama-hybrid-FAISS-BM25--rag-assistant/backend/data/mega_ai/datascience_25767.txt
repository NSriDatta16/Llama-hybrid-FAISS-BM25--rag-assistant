[site]: datascience
[post_id]: 25767
[parent_id]: 25350
[tags]: 
However, in the overall scheme, sliding this network can be represented by two 3 x 3 convolutional layers which reuses the activation between adjacent tiles. Since the replacement (two 3x3 instead of one 5x5) share weights, we don't have to calculate them twice. That is where the gain comes from. Edit: The gain comes from the sliding: using the following formula (n-filtersize+1) x (n-filtersize+1) that calculates the output of a filter on an n x n input a more detailed answer can be found here: Reducing Filter Size in Convolutional Neural Network Thank you @Thomas W.
