[site]: crossvalidated
[post_id]: 215648
[parent_id]: 
[tags]: 
Bayesian approach to estimate expected run time of an algorithm

I have a task to estimate expected run time (in seconds) of a tool. The tool is essentially a black box which appears to run to completion in amount of time which seems to be non-deterministic. In other words, run time given the same input varies slightly from run to run. The tool in addition to input accepts a positive integer as a parameter: 1, 2, 3, and so on. I would like to plot the distribution of the run times for a range of parameters. One parameter - one distribution plot. I would like to get the answers to the following questions: What is the mean run time for a fixed input parameter for the tool? What is the distribution or uncertainty of run times for each fixed input parameter? What is the best model to address the questions 1. and 2. The most straightforward way would be to create a histogram and compute mean for each. I am curious whether this problem can be cast into a Bayesian model. I know that some processes like time, rates usually are naturally modeled with distributions like Poisson (for integer) or Negative Binomial (for real) distributions. My prior knowledge is that individual measurements, or tool run times, are iid. I also know that run time should increase with increased value of parameter for the tool. For each parameter value for the tool (1,2,3,...,10) I have recorded 100 run time measurements. Essentially, I have ten files with 100 records in each. I am writing the code in Python using PyMC. I am curious and willing to learn how to specify the model that would best fit my problem.
