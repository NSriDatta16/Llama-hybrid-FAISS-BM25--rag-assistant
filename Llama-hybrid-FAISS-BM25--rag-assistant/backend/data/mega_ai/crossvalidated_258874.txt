[site]: crossvalidated
[post_id]: 258874
[parent_id]: 258865
[tags]: 
I'm a chess player, so I'll use chess in my answer. (a) and (b) are not identical. In (a), you have two agents playing against each other. Their underlying models might not be comparable. Even if they had the same model, their parameters highly likely wouldn't converge simultaneously. This is like matching two different chess engines. In (b), you have an agent playing against itself for as many times as possible. This is the typical definition for self-learning reinforcement learning, and is commonly seen in chess programming. (b) is very common in chess, so it does make sense. In chess, we can map the definitions like this: agent -> chess engine both sides learning -> update piece-square-table (PST) and evaluation parameters . The PST table defines where White pieces and Black pieces should go. Evaluation parameters define how a position should be evaluated statically. For example, rook on the seventh rook, two-bishop advantage, isolated pawns, protected passed-pawn etc. left side play -> white colour right side play -> black colour flips the current state - Minimax changes all X to O - get the alpha-beta evaluation from the last ply and change it's sign updates the values - update parameters based on whether the game is won/draw/loss, and choose appropriate learning rate I have only experience with self-play defined in (b). To my knowledge, nobody has done (a) for chess engine programming. NeuroChess is a chess engine by reinforcement learning. The engine learns by the final outcome of the game and it does that by playing itself .
