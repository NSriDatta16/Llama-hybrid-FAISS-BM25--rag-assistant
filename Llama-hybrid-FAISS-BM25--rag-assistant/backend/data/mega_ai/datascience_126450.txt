[site]: datascience
[post_id]: 126450
[parent_id]: 
[tags]: 
Does it make sense to do hp tuning for a Random Forest for top k precision or recall?

I've trained an RF with a binary classification task that achieves mediocre performance. However, they way it is intended to be used would have end-users look only at predictions with high scores (class 1, ranked highest by prediction score). This means I would want the ML to perform best only for say the top 20 highest scores. Does it make sense to optimize the hyperparam tuning during crossvalidation for recall or precision at top k when using RF? How come there is only top_k_accuracy_score and not a top_k_precision_score in sklearn ?
