[site]: crossvalidated
[post_id]: 288167
[parent_id]: 287556
[tags]: 
I'll try to brief and get some intuition across. In answer to your three questions: How does MCMC work? There are lots of packages which implement MCMC algorithms, which for the most part means you don't really need a deep understanding of their inner workings. The basic principle is: I try a bunch (thousands to hundreds of thousands) of different parameter values and pick the 'best' ones -- I pick a possible combination of parameters somehow (simplest case is randomly, as in Metropolis-Hastings). Then I compute the value of the posterior at those parameter values and compare it to the value of the posterior at the previous step. If the new value is higher than the previous, i.e. Ive found a better set of parameters, I accept the new values. If not, I reject it with a certain probability. So basically I iteratively find better and better parameters, but I allow some bad ones to sneak in too because I can't be completely sure that my data are any good. The rate at which I allow bad parameters to sneak in depends on how bad they are . Choice of priors. There are a few basic principles for choosing priors when you don't have substantive prior information. Andrew Gelman is probably the main guy for this, he has a great blog and a bunch of really good publications on what he calls 'weakly informative priors'. Basically something broad (large standard deviation) with a zero mean is generally OK. The idea is that, while I don't necessarily have any information about the precise distribution of my parameter a priori , I do know that it shouldn't be excessively large, so I can just use a prior that doesn't preclude any reasonable values but does preclude values which are unreasonably large, say in the thousands for a standardised predictor. So in short, broad zero-mean normal priors are generally good as a default. Bear in mind that 'broad' is relative to the scale of your predictors. What does the posterior mean? The posterior is just a distribution over plausible parameter values, just like you might have a normal distribution over the heights of females or whatever. Conceptually the posterior is slightly different in that it reflects a 'degree of belief' rather than the frequency at which we can expect to observe a given outcome -- the y axis is like a measure of credibility; given the data and my prior beliefs, how confident am I that the parameter lies in a given range -- this is what the posterior tells you. EDIT: I realise I didn't really address the mixed-effects part of the question. Essentially in a Bayesian framework, you assume that the mixed effects are the sum of two components, $\theta_{ij} = \tilde{\theta_i} + \eta_{ij}$. The priors for the fixed effects $\tilde{\theta_i}$ are chosen according to the problem, either informative or weakly informative will do. The priors for the random effects $\eta_{ij}$ are the distribution you're assuming for the mixed effects, which will usually be zero-mean Gaussian, but can be anything. Because the variance of the random effect is unknown, we also need a prior on that -- a uniform prior might be OK, but Gelman advocates for half-cauchy priors, and that's what I'd use. This tutorial is pretty practically-based and includes code etc.
