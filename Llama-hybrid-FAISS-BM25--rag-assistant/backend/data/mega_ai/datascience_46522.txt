[site]: datascience
[post_id]: 46522
[parent_id]: 40955
[tags]: 
Yes, you have overlooked something. Let me explain this with an example: When training a neural network, you use the loss function as a signal to backpropagation. Now let us say that you have a network which outputs three categories: Cat, Dog, and Toad. Let's say the prediction of the network in an iteration is [0.7, 0.6, 0.3], although the data is, in fact, an image of a dog, meaning the truth is: [0 1 0]. Without a softmax layer, you cannot really tell much the network got the prediction wrong. In this example you might think the difference is 0.7 - 0.6 = 0.1, however after running a softmax, you realize that it is in fact 0.03, since the network was very strong in differentiating between the Toad category vs. Dog and Cat, so the loss is not as big as it seems. Now as an experiment, run a neural network without the softmax layer and see for yourself, how severe it can affect the training. Not only that, but normalization of the input batches also makes a huge difference in training a network.
