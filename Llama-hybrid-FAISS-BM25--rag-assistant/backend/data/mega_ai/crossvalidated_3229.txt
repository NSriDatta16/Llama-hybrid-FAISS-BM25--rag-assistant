[site]: crossvalidated
[post_id]: 3229
[parent_id]: 3171
[tags]: 
You should adjust your standard errors (and p-values, confidence intervals, etc) to account for the observations not being independent. You can do this under some reasonable assumptions even though you don't know which observations are of the same person. For example, suppose you're estimating the mean of some variable, x. Let x_{it} be the observations in your sample with the understanding that x_{it} and x_{it+1} are not necessarily the same person. Let e_{it} = x_{it} - mean(x). A central limit theorem for dependent data (e.g. this one ) tells us that the sample mean is asymptotically normal with variance V = lim E[1/NT (sum e_{it})^2 ] = lim 1/NT sum E[e_{it}e_{js}] where the limit is as NT -> infinity, the first sum is over i,t and the second is over i,t,j,s. Assume that e_{it} are uncorrelated for different individuals. Suppose for a fixed individual, that e_{it} is weakly stationary with v_s = E[e_{t} e_{t+s} ]. Also, let's suppose that probability of the same person being sampled at time period s conditional on being in at time t is p. If the pool and sample sizes is fixed, then p = (sample size)/(pool size). Given that, we know E[e_{it}e_{js}] = v_0 if t=s else p v_{t-s} Now assuming that T -> infinity, we get V = v_0 + p 2 sum_{t=1}^infinity v_t This is the asymptotic variance of the sample mean. To use this result, you need to be able to estimate V consistently. V is called the long run variance. If N is fixed, it can be estimated by kernel methods. If N -> infinity seems like a more appropriate asymptotic approximation, then you need not resort to kernels. All this is just meant as example to get you started. You can modify many of the assumptions if they're not appropriate for your setting. For example, p could vary with t and s instead of being constant. If you're not just interested in sample averages, you can do similar calculations for any statistic you want.
