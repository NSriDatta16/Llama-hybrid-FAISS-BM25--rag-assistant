[site]: crossvalidated
[post_id]: 328071
[parent_id]: 328059
[tags]: 
I am assuming you are trying to see if the mean of all data points in one sample over the time series is different from 0. The choice of whether to use the t-test or the Mann-Whitney U test depends on how the data is distributed. The t-test is for normally-distributed data. The Mann-Whitney test is more general in that it can be applied to both normally and non-normally distributed data under more relaxed conditions (see https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test ). From the description but having not seen the data, having a series of small increases and then large drops would suggest to me it is non-normal since I can't picture the data scattered around one mean with the bulk of the other data points distributed around the mean in a certain standard deviation. But without seeing the data in question or any other visualizations (e.g., a QQ-plot) it is hard to say definitively. So you just need to decide whether the data is normally distributed or not when evaluating which test appears more appropriate. From what I remember, the Mann-Whitney U test result is fairly consistent with the t-test result when the data is normally distributed, so any deviations for large sample sizes between the two may imply the data is not normally distributed. But overall, if "most of then [values are] +2 and some of them -200", that would suggest to me that the mean isn't at 0.
