[site]: crossvalidated
[post_id]: 449247
[parent_id]: 339656
[tags]: 
I think there is a more satisfying solution than what has been suggested already, one that creates a single model that properly deals with the two kinds of input data and their relationship to the output class. Use a sequence model like an RNN to convert text into a kind of embedding. That embedding output is used directly as input to a dense layer that also takes the non-text data as input. The benefit of putting this into one model is you can merely rely on backpropagation to learn the right level of dependency of the output class on the two kinds of inputs, as well as let it train the RNN jointly with the final classifier. No need to add the complexity of an ensemble. For details, here is a good tutorial: http://digital-thinking.de/deep-learning-combining-numerical-and-text-features-in-deep-neural-networks/
