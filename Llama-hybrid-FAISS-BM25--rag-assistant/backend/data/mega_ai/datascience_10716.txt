[site]: datascience
[post_id]: 10716
[parent_id]: 10642
[tags]: 
Yes, you can take benefit of pre-trained models. Most famous one being the GoogleNewsData trained model which you can find here. Pre-trained word and phrase vectors https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing You can then load the vectors in binary format in your model using gensim as shown below. >>> model = Word2Vec.load_word2vec_format('/tmp/vectors.txt', binary=False) # C text format >>> model = Word2Vec.load_word2vec_format('/tmp/vectors.bin', binary=True) # C binary format Here is a different pre-built model for English Wikipedia: https://github.com/idio/wiki2vec/raw/master/torrents/enwiki-gensim-word2vec-1000-nostem-10cbow.torrent Source: https://github.com/idio/wiki2vec/ Using a prebuilt model Get python 2.7 Install gensim: pip install gensim uncompress downloaded model: tar -xvf model.tar.gz Load model in gensim: from gensim.models import Word2Vec model = Word2Vec.load("path/to/word2vec/en.model") model.similarity('woman', 'man') You can also use Stanford NLP Glove http://nlp.stanford.edu/projects/glove/ https://github.com/stanfordnlp/GloVe Here is a great compilation of pre-trained word2vec models. https://github.com/3Top/word2vec-api#where-to-get-a-pretrained-models Some additional pre-trained models: http://www.zuccon.net/ntlm.html https://code.google.com/archive/p/word2vec/ More on gensim and code here: https://radimrehurek.com/gensim/models/word2vec.html Quora forum with a similar questions https://www.quora.com/Where-can-I-find-pre-trained-models-of-word2vec-and-sentence2vec-trained-on-Wikipedia-or-other-large-text-corpus https://www.quora.com/Where-can-I-find-some-pre-trained-word-vectors-for-natural-language-processing-understanding
