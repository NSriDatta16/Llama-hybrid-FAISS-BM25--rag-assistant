[site]: datascience
[post_id]: 47903
[parent_id]: 
[tags]: 
LSTM cell input dimensionality

I am trying to wrap my head around LSTM networks, specifically about the input dimensionality. Suppose we have the following dataset Timestamp Value 1 32 2 44 3 52 4 33 5 23 6 11 And we derive a new dataset from this one that will have time lag of 3 timestamps for training our LSTM network: Value (t-3) Value (t-2) Value (t-1) Value (t) 32 44 52 33 44 52 33 23 52 33 23 11 Value(t) being the target value. Suppose that we want to construct an LSTM network with a single hidden layer, containing 5 LSTM cells, followed by a Dense layer (not really relevant for this question) and a single target output. This is what we would normally prepare for Keras, and it would do the rest. I am interested in how this architecture would be "drawn out" on a piece of paper. Specifically, what is the shape of the input vector for each LSTM cell? I understand the shape of X is (# rows, # features, # timestamps), but I don't really understand how it is fed into the LSTM cells. Because of all the Back Propagation Through Time, and the connections between each LSTM cell with its previous timestamp, is each of the values fed one by one? For example, for each of the 5 hidden LSTM cells, we first feed it Value (t-3), then using memory from what it learnt there we feed it Value (t-2) and so on? Since we have time lag of 3, there should be 3 time-steps for each LSTM cells. But is the shape of the input vector for each LSTM cell 1? Also, what about text processing? Suppose each sentence is treated as a sample in the dataset. Sure, we'd encode each word with word2vec, but then how would we feed it to the neural network (each sentence can have different number of words, so the overall input vector can vary)? I am a novice learner, please excuse my stupidity. Have a nice day.
