[site]: crossvalidated
[post_id]: 275876
[parent_id]: 
[tags]: 
Classifier Optimized for False Positive Rate

I’m working on an application where I have data points that can assigned to one of two classes, say positive and negative, where positive occurs very infrequently. I need to build a classifier that takes a point and returns 1 if it is extremely confident that it is a positive instance, and 0 otherwise. The goal here is to keep a 0 false positive rate while maximizing recall. This is essentially an anomaly detection problem where I have training data but cannot afford false positives. Given this constraint, I'd like to maximize the accuracy/recall. Are there algorithms that are best-suited for this? I’ve been experimenting with logistic regression as it provides a probability of classification, but am not completely satisfied with the results. I believe a somewhat ideal solution would be a decision tree-like algorithm whose optimization goal was to create as many completely homogenous leaves of a size above a configurable threshold. However I’m not aware of any such algorithm, and suspect the problem is likely NP-hard anyway (to determine the optimal tree at least)…
