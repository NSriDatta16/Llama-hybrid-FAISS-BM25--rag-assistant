[site]: crossvalidated
[post_id]: 552385
[parent_id]: 552356
[tags]: 
It is said that bagging reduces variance and boosting reduces bias. Indeed, as opposed to the base learners both ensembling methods employ. For bagging and random forests, deep/large trees are generally employed as base learners. Large trees have high variance, but low bias. Ensembling many large trees reduces the variance. Boosting is most effective with 'weak learners': base learners that perform slightly better than chance. Small trees generally work best, often stumps (i.e., single-split trees) are even used with boosting. Small trees have low variance, but high bias. Averaging over many trees (combined with updating the response variable after fitting each tree, which puts more weight on training observations not well predicted thus far) thus reduces the bias.
