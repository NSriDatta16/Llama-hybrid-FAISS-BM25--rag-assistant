[site]: crossvalidated
[post_id]: 439276
[parent_id]: 439274
[tags]: 
Unfortunately, it doesn't work that way. Hyperparameters cooperate in hard-to-predict ways. For example, a bit extreme to make the point. You have no hidden layers, in other words, you are fitting a logistic regression. A logistic regression will usually not really overfit. So you use a relatively big learning rate and a lot of epochs, and find that that works fine, at least, not worse than other hyperparameter configurations. Then you increase the number of layers. You get a complex model, that is now suddenly prone to overfitting. Then the big learning rate and the many epochs that worked fine earlier are no longer optimal. Small thing, I would say the number of hidden nodes, or more generally, the whole architecture of the neural network, is also part of the hyperparameters. So your question I read more like, will the same learning rate be optimal if I increase the complexity of the network.
