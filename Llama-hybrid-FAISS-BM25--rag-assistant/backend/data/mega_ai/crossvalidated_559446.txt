[site]: crossvalidated
[post_id]: 559446
[parent_id]: 196670
[tags]: 
I suspect the reasons they suggested this approach was that the magnitudes of the gradient descent steps in back-propagation are proportional to the derivative of the activation function. If you use a logistic activation function then the derivative is numerically zero before the output reaches 1 or 0. As a result the optimization can never absolutely converge as the loss function has a very long trough in weight-space with a very shallow slope along the bottom. Using these modified targets backpropagation is able to converge to a more definite minimum. I used this technique a bit back in the very early 90s‡, and it doesn't really give much of a benefit in practice, especially if the modification to the targets is as large as that! A more recent usage of this trick can be found in Platt Scaling , which is used to get estimates of the probability of class membership from support vector machines†. In Platt Scaling, a logistic regression model is fitted to the (leave-one-out) scores outputted by the SVM, but with modified targets (as @Sycorax explains +1) as a regularisation method to avoid over-fitting. However Platt adopts a much less heuristic (and less drastic) approach, and sets the targets to be $y_+ = \frac{N_+ + 1}{N + 2} \qquad \mathrm{and} \qquad y_- = \frac{N_- + 1}{N + 2}$ which is effectively a Bayesian regularisation based on the Laplace correction. † Really - don't do this, if you want probabilities (and for most practical applications, you will), use a proper probabilistic classifier, such as kernel logistic regression or Gaussian process classifiers (if you like to be Bayesian). ‡ The PDP books were where I first learned about neural networks! (my copy is from 1989 ;o)
