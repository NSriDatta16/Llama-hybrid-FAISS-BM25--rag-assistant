[site]: crossvalidated
[post_id]: 424884
[parent_id]: 424874
[tags]: 
In itself, this is kind of an odd question since we do not know what kind of relationship you are looking for. Epochs are the number iterations over the whole training set. By the typical definition, a neural network sees each training sample one time per epoch. Some people like to speak of steps instead, which are the number of training batches that the system needs to see in total. Learning rate indicates how big or small the changes in weights are after each optimisation step. If you choose a large learning rate, the weights in the neural network will change drastically (see below). Hidden units are the neurons in your network, typically those between the input and output layer. They are, of course, in their own layer(s). In other words, you can have an input layer with 120 features, one hidden layer with 256 hidden units, and a probabilistic output of one unit. Now, to discuss the relationship between the three, I'll limit myself to performance and time consumption. When you specify the number of epochs, your system will run for n epochs and you have to wait until it is finished. This of course is not ideal, since it might be that the system has already reached its best performance and it keeps training for n-m epochs. To combat this, you can look into early stopping which stops training if the neural network has not improved on the validation set for a given amount of epochs. How quickly a network is optimised (and thus, how many epochs are needed) also depends on the learning rate. Usually a high learning rate will get to improved results rather quickly, but then it overshoots the optimal solution. When the learning rate is too small, it will just take too much computation time (and too many epochs) to find a good solution. It is important to find a good learning rate. Hidden units, then are not specifically related to the other two. They are not specifically influenced by them.
