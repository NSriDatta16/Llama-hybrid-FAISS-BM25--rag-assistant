[site]: crossvalidated
[post_id]: 69986
[parent_id]: 69960
[tags]: 
Simple answer: Why don't you just simply drop the features which have a large amount of missing data from your analysis? They give you little information and are unlikely to be useful predictors. However if you start dropping the rows with missing data that might start introducing bias in your results. This highlights the more fundamental problem of dealing with missing data. More involved answer: Missing data is a well established problem in statistics and there are techniques which deal with this issue. One way you could go about addressing the missing data problem is to attempt to impute i.e guess the missing data. Once you've done that you would then be able to run a logistic regression. To factor in the uncertainty you introduce when guessing the missing data you could generate multiple imputed data sets and use the R package mitools when doing the logistic regression. I have used this approach in my data. However how you go about doing the actual imputation/guessing depends on the properties of your dataset. Are certain features correlated for example? Regarding your question about the performance of the classification regression tree method, I have not used this technique extensively but I imagine it is going to struggle with a large number of features since from my understanding it would construct a classification tree of at least 200 nodes. No idea how it deals with missing data either. It is a bit alarming if it doesn't complain! I think logistic regression is your best bet but you need to figure out how to deal with the missing data. Take home message: be wary about missing data don't run methods without knowing how they deal with missing data and what assumptions they make.
