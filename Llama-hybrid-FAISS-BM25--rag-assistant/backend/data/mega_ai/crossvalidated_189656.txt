[site]: crossvalidated
[post_id]: 189656
[parent_id]: 189652
[tags]: 
Well, I believe a more geometric point of view will help better decide whether normalization helps or not. Imagine your problem of interest has only two features and they range differently. Then geometrically, the data points are spread around and form an ellipsoid. However, if the features are normalized they will be more concentrated and hopefully, form a unit circle and make the covariance diagonal or at least close to diagonal. This is what the idea is behind methods such as batch-normalizing the intermediate representations of data in neural networks. Using BN the convergence speed increases amazingly (maybe 5-10 times) since the gradient can easily help the gradients do what they are supposed to do in order to reduce the error. In the unnormalized case, gradient-based optimization algorithms will have a very hard time to move the weight vectors towards a good solution. However, the cost surface for the normalized case is less elongated and gradient-based optimization methods will do much better and diverge less. This is certainly the case for linear models and especially the ones whose cost function is a measure of divergence of the model's output and the target (e.g. linear regression with MSE cost function), but might not be necessarily the case in the non-linear ones. Normalization does not hurt for the nonlinear models; not doing it for linear models will hurt. The picture below could be [roughly] viewed as the example of an elongated error surface in which the gradient-based methods could have a hard time to help the weight vectors move towards the local optima.
