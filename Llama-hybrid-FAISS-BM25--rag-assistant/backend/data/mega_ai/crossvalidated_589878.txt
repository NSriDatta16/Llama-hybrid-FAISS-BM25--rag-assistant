[site]: crossvalidated
[post_id]: 589878
[parent_id]: 
[tags]: 
Is the insertion of a flattening layer degrading my results, theoretically?

I have designed a couple of neural networks, one MLP dense-layers based, and one CNN, which are designed for speech separation in a 3D space. Either network can be used for the same task. Their inputs is a 4-dimensional tensor, in the form (N, F, T, C) , where N is the sample image, F the number of frequency bins, T the number of time frames, C the number of features. The output is a few dense layers, each representing a time-frequency mask, with of size (F, T) each. They both contain one flattening layer: the MLP has it right after the input (to adapt the input tensor to the first hidden layer), the CNN at the end, right before the output layer (to adapt the hidden convolutional layer shape to the dense layer outputs). I was advised not to use any flattening, since I may lose the information contained in the spatial feature. Woud you agree with this? Would you replace it? Instead, I was suggested to use time-distributed layers with a shared parameter setting for each input feature channel. Why would this specific layer make any difference?
