[site]: crossvalidated
[post_id]: 203253
[parent_id]: 203246
[tags]: 
MSE or mean squared error literally measures the average squared error for an estimator when estimating a parameter. The different interpretations rise from the estimation of different parameters. This is when the parameter being estimated is a future value (unobserved response) from current model. This is when the parameter being estimated is an attribute of a distribution, like the mean etc from statistic This is when the parameter being estimated is the mean regression function from the OLS line. In general MSE = $\dfrac{1}{n}\sum$ (parameter - estimate)^2. So the underlying formula is always the same, just the parameters and estimates change from problem to problem. In regression, sometime MSE refers to Residual sum of squares where then $$MSE = \dfrac{1}{n-p} \sum(y - y_i)^2 $$ The denominator changes in order for MSE to be an unbiased estimated of the error variance. This is one example of a slight deviation from the general understanding of mean squared error.
