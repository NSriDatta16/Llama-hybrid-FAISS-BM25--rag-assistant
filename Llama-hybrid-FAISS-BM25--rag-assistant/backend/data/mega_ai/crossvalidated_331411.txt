[site]: crossvalidated
[post_id]: 331411
[parent_id]: 
[tags]: 
Why don't we use non-constant learning rates for gradient decent for things other then neural networks?

Deep learning literature is full of clever tricks with using non-constant learning rates in gradient descent. Things like exponential decay, RMSprop, Adagrad etc. are easy to implement and are available in every deep learning package, yet they seem to be nonexistent outside of neural networks. Is there any reason for this? If it is that people simply don't care, is there a reason why don't we have to care outside of neural networks?
