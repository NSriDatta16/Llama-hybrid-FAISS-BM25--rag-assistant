[site]: crossvalidated
[post_id]: 641258
[parent_id]: 
[tags]: 
Can I skip test set and train on 100% of data?

Is it a viable solution to train on the whole dataset without splitting the data into 'train' and 'test' sets? In other words, is it okay to skip offline evaluation and only perform online evaluation (with the actual data that will occur in the future)? Wouldn't this provide my algorithm with more data, enabling it to derive conclusions better? After all, more data means better predictions, right? I am using XGBoost with predefined hyperparameters, so there's no need for a 'validation' set. I have been using an 80%:20% split, and my model never overfits. I don't see how offline evaluation provides me value. EDIT: I am training on hundreds of millions of records on a lot of CPUs. I don't have the resources to retrain multiple times. So that is why my hyperparameters are predefined and that is why I skip cross-validation. Do you think that I can do something differently?
