[site]: crossvalidated
[post_id]: 597851
[parent_id]: 597800
[tags]: 
The definitions of the terms parametric, non-parametric and semi-parametric are not as well agreed upon as we might think. The definition I learned in grad school was: 1.) Parametric: model is completely specified with a fixed number of parameters, regardless of how many observations we have. AFT model with Weibull baseline is an example: we have regression parameters and baseline parameters. 2.) Non-parametric: model is specified by an unbounded number of parameters. Kaplan Meier is an example: as your sample size grows, the number of steps in the curve grows without bound. So far so good. 3.) Semi-parametric: model is specified by a fixed set of parameters and an unbounded number of parameters. In this case, the Cox-PH model when we define the baseline with a non-parametric model would be semi-parametric, since we have a fixed number of regression coefficients and an unbounded number of baseline parameters. You can see this isn't really a formal definition: any non-parametric model could be also defined as semi-parametric by saying "and there's this one useless parameter $\Xi$ that is independent of the data". But the heuristic idea is that we're imposing structure on some part of the model for interpretability and/or statistical efficiency while allowing infinite flexibility to other parts of the model to minimize assumptions. Interesting results of this definition: models such as Gaussian Mixture Models or Neural Networks are considered parametric if the parameter set is fixed in advanced, but non-parametric (or semi-parametric) if we allow the data to infer the structure without some sort of bound.
