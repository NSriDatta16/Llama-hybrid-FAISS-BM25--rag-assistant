[site]: crossvalidated
[post_id]: 313451
[parent_id]: 201359
[tags]: 
It is very difficult to write an answer to the question What were the main statistical contributions of Ronald Fisher? since there are already numerous excellent works on this topic, created by excellent writers, including great statisticians e.g.: Hotelling, 1951, The Impact of R. A. Fisher on Statistics Savage, 1976, On Rereading R. A. Fisher Yates, 1964, Sir Ronald Fisher and the Design of Experiments Yates, 1962, Sir Ronald Aylmer Fisher (1890 - 1962) Pearce, 1979, Experimental Design: R. A. Fisher and Some Modern Rivals Efron, 1998, R. A. Fisher in the 21st Century These works are very difficult to match in a few simple lines on an internet Q&A board. On top of that it is not quite easy to grasp the entirety of ideas from Fisher, as Efron wrote in his work on Fisher: One difficulty in assessing the importance of Fisherian statistics is that it's hard to just say what it is. Fisher had an amazing number of important ideas and some of them, like randomization inference and conditionality, are contradictory. It's a little as if in economics Marx, Adam Smith and Keynes turned out to be the same person. Fisher was a pioneer Already a simple, but very good, source to Fisher's contribution is Wikipedia. Just reading the article on the history of statistics (or you can use any other text) will give you some insight in the amount and importance of Fisher's contributions. You will also see that it is partly time, location and luck that made Fisher a great contributor. Fisher was an important and influential statistician in the early 20th century when the basic foundations of applied statistics were created and the field was relatively small (comparable to the period of the 18th and 19th century in mathematics). The first journal of statistics and the first statistics department at a university had just been started when Fisher entered the stage. Before the beginning of the 20th century, there were mostly methods to do regression and several ideas about distributions of residual terms and errors, used in such fields as astronomy. Concepts of measurement errors and probability of results. This type of mathematics and logic (more close to pure mathematics, and... seen as more noble, and less condemned by serious mathematicians of that time), became applied more widely to fields of Fisher's choice: genetics, evolution, biology, agriculture. Since Fisher, an excellent mathematician, provided major contributions to these early developments (or may even be considered as the major driver for these developments), his work has been placed at an important position in the history of statistics. Basic concepts and tools If you look at the topics in an introduction book on statistics (specifically the mathematical concepts, or inference) you might consider Fisher as the dominating contributor. It is also Fisher who wrote the first, and most influential, introduction to statistics books : Statistical Methods for Research Workers (1925) The Design of Experiments (1935) (using the tea cup experiment to explain among others, randomization, the use of latin squares, null hypothesis, significance, sensitivity/power, and basically everything; Yates provides a historic background to this work) Note that online versions of these books exist SMRW and partially DE (see readings October 29 b) . From 1912 to 1925, Fisher: helped to improve the chi-square test (where Pearson and others were wrong about the number of degrees of freedom for many years), provided an exact test to calculate the p-value for goodness of fit with low number of observations (which was named after him as the Fisher's exact test ), wrote a proof (as an undergraduate) for Gosset's 'student's distribution' (and developed it further during his work on small observation numbers , such as ideas of using $N-1$ degrees of freedom instead of the sample size $N$ when using sample statistics) ( see historic description by Fisher's daughter Joan Fisher Box ), developed analysis of variance and the F-distribution (also named after him), and (another "little" thing that he did as an undergraduate) was developing the basics and concepts for maximum likelihood ( Aldrich's R. A. Fisher and the Making of Maximum Likelihood ). So roughly this covers most of the basic inferential tools that current introduction texts use. While doing this work on statistics Fisher tackled major problems in genetics that make people like Richard Dawkins admire him so much. Terminology Fisher introduced many concepts and terms and improved statistical language. Two recent questions on this Q&A site relate to Fisher. The question why so many variables are squared in statistics and why we so often the $L_2$ norm instead of the $L_1$ . It is Fisher who "proved" that the $L_2$ norm is a better (more efficient) estimator than the $L_1$ norm (assuming a perfect Gaussian distribution, which Fisher agreed later is not always true for 'real' errors), and introduced the terms deriving it as an 'efficient statistic' and a 'sufficient statistic' while doing so, as well as introducing the term 'variance' (in his 1920 paper A mathematical observation of the methods of determining the accuracy of observation by the mean error and the mean square error ). Foundations In the 1922 paper On the Mathematical foundations of theoretical statistics Fisher provides a short and simple overview of the main concepts, just to name the list of definitions: 'centre of location', 'consistency', 'distribution', 'efficiency', 'estimation', 'intrinsic accuracy', 'isostatistical regions', 'likelihood', 'location', 'optimum', 'scaling', 'specification', 'sufficiency', 'validity' . It requires a historian to see what Fisher contributed here in the sense of being the originator of concepts, and this also relates to Efron's statement. It is difficult to grasp what exactly is contributed by whom. But certainly Fisher helped to improve the statistical language and thinking. In that article Fisher starts mentioning the problem of applying terms like 'mean' and 'variance' to both the true distribution value as well as the estimated value. (I will try to avoid to put Fisher somewhere in a 'school' such as frequentist or Bayesian. I'd say he was just 'sufficiently' practical to whatever question was at hand). Advanced concepts In his further work Fisher developed early concepts of linear discriminant analysis: what linear function of the four measurements $X=\lambda_1 x_1 + \lambda_2 x_2 + \lambda_3 x_3 + \lambda_4 x_4$ will maximize the ratio of the difference between the specific means to the standard deviations within the species? The Use of Multiple Measurements in Taxonomic Problems, 1936 and the concept of estimation by likelihood that Fisher explored further, and has two concepts named after him, Fisher information and Fisher score . See Theory of statistical estimation, 1925 , Two new properties of mathematical likelihood, 1934 , and The logic of inductive inference, 1935 . More links: R.A. Fisher Guide , by John Aldrich. An enormous source, if not the largest, with information on Fisher, with many further references. Michael Hardy's answer on Mathoverflow on a question about the greatest Mathematicians: https://mathoverflow.net/a/173374 Written by StackExchangeStrike
