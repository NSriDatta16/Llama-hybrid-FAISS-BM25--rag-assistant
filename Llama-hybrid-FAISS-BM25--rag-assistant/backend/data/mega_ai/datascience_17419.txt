[site]: datascience
[post_id]: 17419
[parent_id]: 
[tags]: 
Model params tuning

I have a pipeline of models: Training pipeline (on training data): Tokenizer -> HashingTF -> IDF -> KMeans Test pipeline (prediction on query data): trained pipeline -> selecting features from training data which are in the same cluster for query data -> BucketedRandomProjectionLSH What it does explained a bit here . I was curious if that is possible to select params of BucketedRandomProjectionLSH , KMeans and HashingTF wisely enough, so they have rationale. Disclaimer: I'm new to all of that, so below are my assumptions/thoughts only. For instance, next params: hashingTF .numFeatures - "number of features". Underneath, it uses hashing in order not to face collision and false matches of different words during frequency calculations, so rationale beneath of that - #of words I likely will face. In my case I selected ~16k as it seems far beyond the dictionary of paper language + some performance considerations taken into account (e.g. default 2^18 led to long calculation and as the results - timeout has to be tuned due to connection timeout exception) kmeans .k - "the number of clusters to create". In unsupervised learning I have no insight on how to select this one param - just to guess. In future, it might be the #of topics my words belong to. Probably LDH + some manual consideration may help to set it correctly. bucketedRandomProjectionLSH .bucketLength - "the length of each hash bucket". As I understand it - it represents the param of reduction of initial word vector (represented by frequency of words occurrences), squeezing it to "length", with further distance calculating and assigning the vector (based on the "distance" accordingly) to the next param - bucketedRandomProjectionLSH.numHashTables . So the rationale on its value I see as the amount of word I more likely want to take into account to make a prediction on distance between vectors. E.g. if I have an article which is decomposed further into word tokens - how many words I want to take into account? e.g. half of the average #. bucketedRandomProjectionLSH.numHashTables - "number of hash tables" - as I see it - how many "distances" I want to consider. In other words - I have #of clusters (i.e. 5): it answers how further I want to split the cluster to get narrowed results when comparing what it related to my query . Will definately depends on the amount of features in the cluster and #of results I expect. So if to think about that backwards - if given X is: #results I expect out of prediction: [ bucketedRandomProjectionLSH.numHashTables ] = [Average #of articles / X] [ bucketedRandomProjectionLSH.bucketLength ] = [Average #of words per article / 2] [ kmeans.k ] = [X * some magic number >= 2] [ hashingTF.numFeatures ] = [number of words in dictionary] Kindly ask you to evaluate my thoughts process and correct.
