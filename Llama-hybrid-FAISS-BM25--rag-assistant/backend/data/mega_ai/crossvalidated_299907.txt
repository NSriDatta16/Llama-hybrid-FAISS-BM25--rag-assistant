[site]: crossvalidated
[post_id]: 299907
[parent_id]: 299204
[tags]: 
If I understood your question correctly, you want to test for dependencies among predicting variables. Such tests are commonly known as looking for multicollinearity. A simple way to look for multicollinearity is calculating variance inflation factors; basically regressing each predicting variable on all the other predicting variables. In R, in case you use this as software package, that may be done with the package "car" and works well for logistic regression. If Variance inflation is small, say smaller than 3, there is nothing you need to worry about. If it is much larger, in particular above 10, it may be a problem. Large correlation among confounding variables (i.e. predictors you control for but for which you are not interested in their coefficients) can suffer from multicolinearity without causing troubles for your analysis. However, if your predictive variable of interest suffers from multicollinearity, its predicted coefficient may be inflated (i.e. too large, too small or even with the wrong sign) - and that could be a problem for your findings. As an advanced tool in those situations, ridge regression may help. Finally, if you are only interested in the prediction (i.e. the independent variable; the classic situation in "machine learning" compared to "traditional statistics"), multicollinearity is not such a big issue - it "only" increases the confidence interval of your predicted independent variables. Please note that testing pairs of predictors using classic correlation tests is not the preferable approach to detect multicollinearity. That is because it is conceivable that for instance three predicting variables may jointly be much stronger correlated with a fourth variable than each of them is individually with this fourth variable.
