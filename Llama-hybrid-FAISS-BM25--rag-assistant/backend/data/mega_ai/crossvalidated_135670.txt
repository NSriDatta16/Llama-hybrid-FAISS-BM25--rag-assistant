[site]: crossvalidated
[post_id]: 135670
[parent_id]: 135591
[tags]: 
As a very simplistic illustration, let's consider OLS linear regression with 0 and 1 independent variables, both with offsets (i.e. $\beta_0$). Note I choose these dimensions as 2D graphs are easier to view / understand than higher dimensional ones. So we can generate two different models namely: $\hat{y}=\beta_0$ $\hat{y}=\beta_0 + \beta_1x$ Now let's look at a plot of what betas would minimize the square errors for each model. Note that model 1 is shown in red, model 2 in blue: You can draw a direct analogy to your question of why $\beta_1$ changes in logistic regression when adding more independent variables, and why $\beta_0$ changes in this toy example when we add a new independent variable (i.e. $x$ in equation 2). So in the case of both lines, $\beta_0$ is just where they cut the vertical axis. It is clear that adding an extra independent variable has considerably changed $\beta_0$. This is because your model now has an extra dimension on which it can try to minimize the errors. (Note that to be strictly fair, please think of the red line as just a single point on the vertical axis and when regressing for it, the black dots should also just be points on the y axis as this would be a 1D case, but that doesn't change anything). So adding the extra independent variable allows your model an extra parameter it can tune (i.e. $\beta_1$) which "takes the pressure off" of $\beta_0$ as it no longer has to model the variation actually due to another dimension. When that dimension is not there, the model still does its best (i.e. minimizes square errors) but in order to do that $\beta_0$ needs to model the variation due to both dimensions alone. Your question is in higher dimensions and uses logistic regression, but conceptually the reason that the $\beta$ changes is the same.
