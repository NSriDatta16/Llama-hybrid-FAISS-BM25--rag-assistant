[site]: crossvalidated
[post_id]: 478334
[parent_id]: 
[tags]: 
In a parametric model, if I do not have enough data, can I estimate the parameter, and simulate data from the estimated model and estimate again?

Suppose I have a logistic regression model $Y_i=\mathbf{1}(X_i\beta>\epsilon_i)$ to estimate, where the distribution of $\epsilon_i$ is known, $X_i$ follows distribution $F_{\theta}$ with an unknown scalar parameter $\theta$ . Suppose I only have 40 observations: $\{Y_i,X_i\}_{i=1}^{40}$ . I'm wondering if there are any formal studies on the properties of the following estimator: Step1. I estimate $\beta$ and $\theta$ with maximum likelihood and get : $\widehat{\beta},\widehat{\theta}$ . Step2. I simulate 160 new data points $\{Y^*_i,X^*_i\}_{i=1}^{160}$ from $Y_i=\mathbf{1}(X_i\widehat{\beta}>\epsilon_i)$ and $F_{\widehat{\theta}}$ . Step3. I reestimate $\beta$ and $\theta$ using the 200 observations $\{Y_i,X_i\}_{i=1}^{40}\cup \{Y^*_i,X^*_i\}_{i=1}^{160}$ , and obtain new estimate $\widetilde{\beta},\widetilde{\theta}$ . Intuitively, this procedure seems consistent. In finite samples, it might have smaller variance(because we used more data), but larger bias(because we are not generating data from the true parameter value). However, I would like to see more rigorous theoretical justification for using $\widetilde{\beta},\widetilde{\theta}$ . My questions are: 1.Suppose the simulation sample size is $B$ and the original sample size is $n$ , how to formally prove that $\widetilde{\beta},\widetilde{\theta}$ is consistent in the sense that it converges in probability to $\beta,\theta$ as $n$ (or $n$ together with $B$ ) goes to infinity? 2. Is there any criterion (such as MSE) under which $\widetilde{\beta},\widetilde{\theta}$ is better than $\widehat{\beta},\widehat{\theta}$ ï¼Ÿ Thanks!
