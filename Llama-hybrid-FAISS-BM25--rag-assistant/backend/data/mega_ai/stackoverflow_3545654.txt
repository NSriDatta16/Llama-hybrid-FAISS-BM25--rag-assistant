[site]: stackoverflow
[post_id]: 3545654
[parent_id]: 3495157
[tags]: 
The problems that dough has raised are all valid. Let me add another one. You didn't say how you would like to measure the agreement between the classification and the "gold standard". You have to formulate the answer to that question as soon as possible, as this will have a huge impact on your next step. In my experience, the most problematic part of any (ok, not any, most) optimization task is the score function. Try asking yourself whether all errors equal? Does miss-classifying the "3" as being "4" has the same impact as classifying "4" as "3"? What about "1" vs "5". Can mistakenly missing one case have disastrous consequences (miss HIV diagnosis, activate pilot ejection in a plane) The simplest way to measure the agreement between categorical classifiers is Cohen's Kappa . More complicated methods are described in the following links here , here , here , and here Having said that, sometimes picking a solution that "just works", instead of "the right one" is faster and easier. If I were you I would pick a machine learning library (R, Weka, I personally love Orange ) and see what I get. Only if you don't have reasonably good results with that, look for more complex solutions
