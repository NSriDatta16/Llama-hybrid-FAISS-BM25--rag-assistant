[site]: crossvalidated
[post_id]: 310190
[parent_id]: 277395
[tags]: 
I would honestly pick the package default values, and see what the performance was like. Selecting sum of these parameters is known to be a bit fiddly, so there is a whole field of research devoted to finding betters ways to avoid having to do this hyper-parameter tuning of stochastic gradient descent. I would suggest using an algorithm that takes care of some of this work such as Adam or Adadelta. I found this blog post very helpful for providing a comparison of the different methods. If you want to optimise the hyperparameters, this can be quite expense in terms of compute time. Instead of grid search, it is more efficient to use random search for the reasons outlined here (1). If you want to do this in a more complicated (but more efficient) way you can try Bayesian optimisation in package like Spearmint or BayesOpt . (1) Bergstra, James, and Yoshua Bengio. "Random search for hyper-parameter optimization." Journal of Machine Learning Research 13.Feb (2012): 281-305.
