[site]: datascience
[post_id]: 66591
[parent_id]: 
[tags]: 
Can I completely cancel the effects of using a smaller batch size by reducing the learning rate?

I'm having the problem that the data from a regular sized batch (e.g., 32, 64) doesn't fit in my GPU. Among other solutions, I'm considering reducing the batch size, as is normally suggested. Of course, this will make the neural network more unstable, so other things need to be considered. For instance, I'm thinking about reducing the learning rate in order to offset the instability brought by the smaller batch size. Would the higher instability be completely canceled by using a smaller learning rate? Or, does this bring other problems of its own, and does not solve the problem completely?
