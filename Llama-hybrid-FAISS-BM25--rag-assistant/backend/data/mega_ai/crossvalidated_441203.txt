[site]: crossvalidated
[post_id]: 441203
[parent_id]: 
[tags]: 
Tree-based machine learning algorithms and qualitative predictors with a lot of unordered categories

As far as I understood, tree-based methods are based on yes/no questions that refer to single predictors. E.g. is predictor A larger than 3000 or is predictor B = "TRUE". In case you have a qualitative predictor that has more than two categories, trees have to generate dummy-variables or rather dummy logical questions. The number of dummy variables / questions being the number of categories - 1. E.g. when you have a predictor C that is composed of the categories "red", "blue", "yellow", you might find the following logical questions in your tree: "is C = red? yes or no" and "is C = blue? yes or no". In order to correctly identify the color of an object, you would have to ask a maximum of two binary questions. Now, I often have data where you have 35 or even more categories. Therefore, the tree can ask 34 (or even more) logical questions that only relate to a particular predictor. When using a random forest, you randomly select a subset of available predictor variables per decision tree. Let's say, you have 1 qualitative predictor with 35 categories and 5 quantitative predictors. The tree will consider 34 dummy variables plus the 5 quantitative ones. If you randomy choose 10 variables per tree, there is a super high chance that you will only end up with the dummy variables and the tree will overrepresent the qualitative predictor. Are tree-based methods therefore unfit for handling data that is composed of qualitative predictors exhibiting a lot of categories? Or did I miss something? I would really like to read your input regarding to this question.
