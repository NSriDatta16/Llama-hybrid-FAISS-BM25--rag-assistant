[site]: stackoverflow
[post_id]: 527563
[parent_id]: 527525
[tags]: 
I do this whenever it makes sense. Compare with How do you make wrong code look wrong? What patterns do you use to avoid semantic errors? . In the example of Joel's article he discusses two cases: The case of a web application that needs to sanitize strings before output. He proposes type prefixes to distinguish the types. However, I argue that it's much better to simply define two different string types. The easiest way to do this is just to introduce a SafeString type. Only this type can be directly sent to the client. The normal string type has to be converted (either explicitly or implicitly; it doesn't matter in this case, because the implicit conversion can still sanitize the string). The example of Microsoft Word where the programmers needed to distinguish between screen coordinates and document coordinates. This is the classical case where two distinct types offer a lot of advantages, even though these types are nearly identical (they both describe 2D points). To address your disadvantages: Disadvantage 1, verbose and unexpected syntax Most languages actually allow to overload functions/operators in a way that make classes use expected syntax. In C++, this is even best practice: “make your own types behave like int s.” Usually, this doesn't necessitate implicit casts in the presence of appropriate overloads. Disadvantage 2, too many classes for every single thing, where do you draw the line? Make as many custom types as you need (but not more). Some languages, such as Haskell, actually encourage the proliferation of types. The only thing that makes us hesitate in languages like C++ is lazyness because we've got to write a lot of boilerplate code. C++0x makes this slightly easier by introducing type-safe type aliases. Of course, there's always a trade-off involved. However, I believe that as soon as any one of the advantages apply for you, this disadvantage is simply balanced.
