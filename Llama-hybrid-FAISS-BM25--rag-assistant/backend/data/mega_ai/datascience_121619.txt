[site]: datascience
[post_id]: 121619
[parent_id]: 
[tags]: 
Pytorch fails to detect workstation installed nvidia GPUs

We are trying to execute a deep learning model on a Linux workstation that from all counts has 2 NVIDIA GPUs installed. The model runs fine on our HPC cluster, but when we try to run it locally on the Linux workstation, we're getting an error initializing the model (see attached tl-error.png), which I think is caused because of some incompatibility with the GPUs/pytorch. We're wondering if anyone has run into this before and can help. This is what we've done: Confirmed that GPUs are installed and got the driver / cuda version (command: nvidia-smi ): NVIDIA-SMI 510.60.02 Driver Version: 510.60.02 CUDA Version: 11.6 Based on the above, created a new conda environment (torch_env) with the recommended installation instructions from the pytorch website. conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.6 -c pytorch -c nvidia However, when we activate the environment and run torch.cuda.is_available() and torch.cuda.device_count() we get False and 0 , respectively.
