[site]: crossvalidated
[post_id]: 630623
[parent_id]: 
[tags]: 
Is reinforcement learning conceptually equivalent to time-series with a latent dependent variable?

In reinforcement learning, there is a state $s_t$ , an action $a_t$ , and a policy $\pi(a|s)$ that maps states to the Probability Distribution Function (PDF) of actions. The goal is to choose the optimal action given the state; we can denote the optimal action $a_t^*$ , and the corresponding optimal policy that selects this action is $\pi^*(a|s)$ . Note that $a_t^*$ is typically unobservable, even after time $t$ . In time-series we have a dependent variable $y_t$ , explanatory variables $x_t$ , and a model (typically parameterized by $\theta$ ) linking them, typically including a residual term, denoted $y_t = g(x_t, u_t | \theta)$ . If the distribution of $u_t$ is known, and $\theta$ is observable, then the model yields the conditional PDF $f(y_t|x_t;\theta)$ . This is particularly straightforward when $g$ is linear. Since $u_t$ is latent, our model provides predictions of $y_t$ , which we denote $\hat{y}_t$ . In practice, we don't know $\theta$ , and may not know $f$ , so at any given time we can only observe $\hat{f}(\hat{y}_t|x_t;\hat{\theta})$ , which is an estimate of the true Conditional PDF of $y_t$ . Let $x_t = s_t$ , $y_t = a_t^*$ , $\hat{y}_t = a_t$ , $f(y_t|x_t;\theta) = \pi^*(a|s)$ , and $\hat{f}(\hat{y}_t|x_t;\hat{\theta}) = \pi(a|s)$ . Given these equivalencies, it appears that the time-series and reinforcement learning frameworks are equivalent, with the key difference being that in time-series, typically $y_t$ is observable, but in this situation it would need to be treated as latent. My Question: Given latent $y_t$ , are the above two frameworks equivalent? If so, why does the standard introductions to reinforcement learning (e.g. Sutton and Barto's textbook), not discuss topics like the Kalman Filter and EM algorithm? (given these topics are suited to time-series with latent variables) Background: My background is in time-series, and I'm currently reading Sutton and Barto's textbook and trying to get my head around Reinforcement Learning. Perhaps trying to cram it into a time-series framework is a bad idea, but given the whole thing seems to be based on Markov transition matrices, I can't escape the feeling that it fits there rather neatly. So I thought I'd ask here and see if anyone else has thought about this.
