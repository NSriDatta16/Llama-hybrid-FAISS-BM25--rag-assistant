[site]: crossvalidated
[post_id]: 246725
[parent_id]: 
[tags]: 
Time series feature normalization and clustering - Hyndman paper

I am working on clustering some time series and have decided to try to cluster based on features rather than the raw series (for now). I found this paper by Hyndman that chooses a series of features (serial correlation, non-linearity, skewness, kurtosis, periodicity) and discusses some of the algorithms used for clustering. At one point he mentions the need to bring the features to [0,1] and he proposes 3 different normalization methods. The first two: $f1$ mapping Q $\in$ [0,$\infty$) to q $\in$ [0,1]: $q = \frac{e^{aQ}-1}{b+e^{aQ}}$ $f2$ mapping $\in$ [0,1] to q $\in$ [0,1]: $q = \frac{(e^{aQ}-1)(b+e^{a})}{(b+e^{aQ})(e^a -1)}$ For $f1$ and $f2$ the constants a and b are chosen "such that $q$ satisfies the conditions: $q$ has 90th percentile of 0.10 when $Y_t$ (the time series) is standard normal white noise, and $q$ has value of 0.9 for a well-known benchmark dataset with the required feature." For each feature, and depending on whether the original values lie in the [0,$\infty$) or [0,1] range, Hyndman gives the values of $a$ and $b$. I am having trouble understanding this normalization technique. Why would we want a 90th percentile of 0.1 fo $q$? Are there any reasons why we can't simply use z-normalization? Is this a common and standard procedure? Any hints, tips, insights, and links for further reading are greatly appreciated. Thanks!
