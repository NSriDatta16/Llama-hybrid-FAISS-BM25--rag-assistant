[site]: crossvalidated
[post_id]: 119869
[parent_id]: 
[tags]: 
Recursive Bayesian Estimation, $p(C_k|\mathbf{x})$ as (discrete) likelihood

I''ve been struggeling with this problem for the last couple of days. The main goal is to use the probabilistic classification output $p(C_k|\mathbf{x})$, from for example a logistic regression, to enhance and make a overall classification performance more robust, over a given set of measurements. $C_k$ indicating the $i$-th class, $\mathbf{x}$ the feature vector In other words: I want to make a recursive bayesian estimation. Now, I'm wondering if there is any change to do this. Assuming I would just count $p(C_k|\mathbf{x})>0.5$ as a success, $y=1$ (interpreting as a Bernoulli distribution). I could easily set up a $Beta(α_0,β_0)$ and start updating, as the Beta is a conjugate prior, with \begin{align*} α_1 &= \alpha_0 + y \\ β_1 &= β_0 + 1 -y \end{align*} So this works fine and is mathematically correct, but I would really like to incorporate the probabilistic output, as it provides valuable additional information. When directly assuming \begin{equation*} y=p(Ck|x), \end{equation*} and then update $α$ and $β$, I get exactly the kind of behavior I'm trying to achieve. Unfortunately, I guess this violates the assumption of being Bernoulli distributed and hence $Beta$ would not be the right conjugate prior, or is ist? In other words, I would like to incorporate that I'm uncertain about the result $y$ of a $Bernoulli$ experiment. I could set up a Normal prior and interpret the output of the logistic regression also as a Normal distribution (somehow) but the $Beta$ fits perfectly my needs as it is bounded to 0...1
