[site]: crossvalidated
[post_id]: 555850
[parent_id]: 490853
[tags]: 
The data either come from the specified model or they do not. There is no probability regarding this concern. The candidate model serves as a convenient yet imperfect representation of the data generative process. To compare how competing models fit your data you can use a criterion such as AIC based on the likelihood. You can base your choice of probability model at least in part on this criterion. Regarding the K-S test, rather than defining an "acceptance/rejection" region you could view the p-value as the weight of the evidence regarding a particular model. For various competing models you could compare K-S p-values analogous to comparing AIC values. Models with larger p-values do a better job representing your data. You may instead be interested in a Bayesian belief probability regarding whether a candidate model is the "right" model. This would be the realm of Bayes factor or other type of model selection and would require a prior distribution on the candidate models. According to the adage, "All models are wrong but some are useful," you could end up assigning a 0% probability to every candidate model. You are also free to assign any other prior distribution. This would allow you to arrive at any conclusion for the belief probability that the sample comes from a certain distribution.
