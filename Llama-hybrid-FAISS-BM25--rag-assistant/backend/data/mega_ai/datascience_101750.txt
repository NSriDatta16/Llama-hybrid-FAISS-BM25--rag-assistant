[site]: datascience
[post_id]: 101750
[parent_id]: 66094
[tags]: 
I agree with ‘Carlos Mougan,’ and add the comparison with these models. Basically, GB(Breiman 1996, 1999)、SGB(Friedman 2002) and XGB(Chen and Guestrin 2016) do sampling w/o replacement. GB uses the full sample set for each iteration. Thus, GB performs like sampling w/o replacement. So, the three use sampling w/o replacement by tradition. However, bagging and RF(Breiman 2001) do sampling w/ replacement. So, the two use sampling w/ replacement by tradition. There are some discussion between sampling w/ or w/o replacement. Binder and Schumacher (2008) run an experiment, model sampling w/o replacement coverge faster. LGB(using GOSS, play a role of ‘hard example mining’)(Ke et al. 2017) and CatBoost(using Minimal Variance Sampling)(Prokhorenkova et al. 2018; Ibragimov and Gusev 2019) perform sampling by weights, thus both build the subset over iteration w/ replacement. Binder, Harald, and Martin Schumacher. 2008. “Adapting Prediction Error Estimates for Biased Complexity Selection in High-Dimensional Bootstrap Samples.” Statistical Applications in Genetics and Molecular Biology 7 (1). Breiman, Leo. 1996. “Bagging Predictors.” Machine Learning 24 (2): 123–40. ———. 1999. “Using Adaptive Bagging to Debias Regressions.” Technical Report 547, Statistics Dept. UCB. ———. 2001. “Random Forests.” Machine Learning 45 (1): 5–32. Chen, Tianqi, and Carlos Guestrin. 2016. “Xgboost: A Scalable Tree Boosting System.” In Proceedings of the 22nd Acm Sigkdd International Conference on Knowledge Discovery and Data Mining , 785–94. Friedman, Jerome H. 2002. “Stochastic Gradient Boosting.” Computational Statistics & Data Analysis 38 (4): 367–78. Ibragimov, Bulat, and Gleb Gusev. 2019. “Minimal Variance Sampling in Stochastic Gradient Boosting.” Advances in Neural Information Processing Systems 32: 15087–97. Ke, Guolin, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. 2017. “Lightgbm: A Highly Efficient Gradient Boosting Decision Tree.” Advances in Neural Information Processing Systems 30: 3146–54. Prokhorenkova, Liudmila Ostroumova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, and Andrey Gulin. 2018. “CatBoost: Unbiased Boosting with Categorical Features.” In NeurIPS .
