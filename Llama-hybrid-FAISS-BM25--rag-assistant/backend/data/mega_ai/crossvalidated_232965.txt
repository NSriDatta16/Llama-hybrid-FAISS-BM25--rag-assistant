[site]: crossvalidated
[post_id]: 232965
[parent_id]: 219521
[tags]: 
I agree with you (percentage estimate included). The reason is that the historically most prominent NLP tasks (tokenization, part-of-speech tagging) use relatively big dataset. Many times those are big enough to make a 0.1% difference in F1 measure statistically significant. The rest of the literature, many times, follows.
