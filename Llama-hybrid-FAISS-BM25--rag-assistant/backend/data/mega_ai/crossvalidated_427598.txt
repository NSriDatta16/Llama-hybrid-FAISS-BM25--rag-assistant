[site]: crossvalidated
[post_id]: 427598
[parent_id]: 248771
[tags]: 
you might find this article interesting: https://www.ncbi.nlm.nih.gov/pubmed/15572470 . It is about: Optimal number of features as a function of sample size for various classification rules . It includes some 3D-plots of the error rate, sample size and feature size, for a number of algorithms e.g. SVM. Their research concluded that: First, the behavior of the optimal-feature-size relative to the sample size depends strongly on the classifier and the feature-label distribution. An immediate corollary is that one should be wary of rules-ofthumb generalized from specific cases. Second, the performance of a designed classifier can be greatly influenced by the number of features and therefore one should attempt to use a number close to the optimal number.
