[site]: crossvalidated
[post_id]: 20434
[parent_id]: 20401
[tags]: 
It is known that the best strategy, that is the one that minimizes the probability of uncorrect guess, is to choose the hypothesis $H \in \{A,B\}$ that maximizes the $p(H|x)$, where $x$ is the observation. As a personal peeve, I dislike this particular statement because in my mind it hides what is really going on, and makes what should be obvious rather obscure. Unfortunately, such Olympian pronouncements are what lots of people take away from a course on Bayesian methods.... Suppose that the observation $X$ is a discrete random variable. We observe that the event $\{X = x\}$ has occurred and need to decide choose between $A$ or $B$. But the observed event can be partitioned into $\{X = x, A\}$ and $\{X = x, B\}$ and we have that $$p(x) = p(X=x, A) + p(X=x, B).$$ Now, one of the two events $\{X = x, A\}$ and $\{X = x, B\}$ has occurred. Clearly if we choose $A$ upon observing $\{X = x\}$, we are correct with probability $p(X=x, A)$ and incorrect with probability $p(X=x, B)$ while if we choose $B$ upon observing $\{X = x\}$, we are correct with probability $p(X=x, B)$ and incorrect with probability $p(X=x, A)$. So, the probability of being incorrect in this instance is minimized if we choose $A$ if $p(X=x, B) p(X=x, A)$. So, if $E$ and $C$ denote the events of the decision being in error and being correct respectively, we have that if the optimal (minimum-error probability) decision is made upon observing $\{X=x\}$, such occurrences contribute $\max\{p(X=x, A), p(X=x, B)\}$ to $P(C)$, and $\min\{p(X=x, A), p(X=x, B)\}$ to $P(E)$. We have $$\begin{align*} P(E) &= \sum_x \min\{p(X=x, A), p(X=x, B)\}\\ &= 1 - P(C)\\ &= 1 - \sum_x \max\{p(X=x, A), p(X=x, B)\} \end{align*}$$ Since $p(X=x, A) = p(x|A)P(A)$ and $p(X=x, B) = p(x|B)P(B)$, we can write $$\begin{align*} P(E) &= 1 - \sum_x \max\{p(x|A)P(A), p(x|B)P(B)\}\\ &= 1 - \sum_x \max\left\{\frac{p(x|A)P(A)}{p(x)}, \frac{p(x|B)P(B)}{p(x)}\right\}p(x)\\ &= 1 - E_x\left[\max\{P(A|x),P(B|x)\}\right]\\ &= E_x\left[\min\{P(A|x),P(B|x)\}\right] \end{align*}$$ where $E_x$ denotes expectation with respect to $x$. Alternatively, since $$ \max\{p,q\} = \frac{p+q+|p-q|}{2}, $$ we have $$\begin{align*} P(E) &= 1 - \sum_x \max\{p(x,A), p(x,B)\}\\ &= 1 - \sum_x \frac{p(x,A) + p(x,B)+ |p(x,A) - p(x,B)|}{2}\\ &= 1 -\frac{P(A) + P(B)}{2} - \sum_x \frac{|p(x,A) - p(x,B)|}{2}\\ &= 1 - \frac{1 + \sum_x |p(x|A)P(A) - p(x|B)P(B)|}{2}\\ &= 1 - \frac{||p(\cdot|A)p(A) - p(\cdot|B)p(B)||_1 + 1}2 \end{align*}$$ as OP Michele writes it. Finally, for $p, q \in [0,1]$, $\min\{p, q\} \geq pq$ and so $$\begin{align*} P(E) &= E_x\left[\min\{P(A|x),P(B|x)\}\right]\\ &\geq E_x\left[P(A|x)P(B|x)\right]. \end{align*}$$ In summary, the three expressions for $P(E)$ appear different but are based on the same fundamental idea. All three require computation of an expectation with respect to $x$, and differ very little in computational complexity. Simplifications can be made if something specific is know about the variables, e.g. $X$ is a geometric random variable with different parameters under the two hypotheses in which case one could compute some sums analytically instead of numerically, but they would appear to apply equally in all three cases. I see very little difference in terms of computational tractability in (1)-(3) as exhibited in the OP's question.
