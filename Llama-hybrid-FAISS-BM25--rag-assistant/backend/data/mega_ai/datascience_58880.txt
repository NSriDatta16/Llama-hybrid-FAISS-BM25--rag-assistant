[site]: datascience
[post_id]: 58880
[parent_id]: 58830
[tags]: 
Traditionally, spectrograms have been used for most audio ML tasks. Depending on your task, a different kind of spectrogram might be best suited (different frequency resolutions, scaling etc.). In your case, I'd probably try mel magnitude spectrograms that have unit variance and zero mean. To feed this spectrogram into a deep learning network, like a convolutional neural network (CNN) , you'd use fixed length overlapping windows (you do the same at prediction time). Alternatively, other approaches like RNN or most recently TCNs have been used. They all use spectrograms as input. To extract a mel spectrogram (or other formats), you may use a library like librosa and specifically melspectrogram . Examples for audio classification using spectrograms are Genre-Agnostic Key Classification With Convolutional Neural Networks or A Single-Step Approach to Musical Tempo Estimation Using a Convolutional Neural Network . You can find out more about animal sound classification, specifically for birds, at BirdCLEF2019 or at DCASE . For music, Dieleman showed in End-to-end learning for music audio (IEEE paywalled) that you can also use the waveform directly to classify an audio signal. Later work by Pons et al., End-to-end learning for music audio tagging at scale ( not paywalled), explains in more detail whether and why classifying the waveform instead of spectrograms is useful. I strongly recommend to sticking to spectrograms to get started, simply because IMHO the setup is much easier.
