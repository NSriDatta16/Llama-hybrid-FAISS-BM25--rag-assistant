[site]: crossvalidated
[post_id]: 555181
[parent_id]: 553986
[tags]: 
So a few things here: Firstly, it is worth mentioning for the sake of transparency that torch.nn.functional.conv1d is more strictly cross-correlation rather than convolution , which involves flipping the filter, in a more broad usage. However, for CNN applications, the distinction is not important, and so the term convolution is overwhelmingly overloaded to mean cross-correlation (no flip). The second thing that is worth mentioning is that ``mathematical operations'' can mean a few things as well. A relatively straightforward approach would be using $$(\vec{a}\star\vec{b})[i] = \vec{a}\left[i:i+\text{len}(\vec{b})\right]\cdot \vec{b}$$ where $\vec{a}\star\vec{b}$ is the desired result of cross-correlating $\vec{a}$ and $\vec{b}$ (given as a vector-describing function $\vec{a}\star\vec{b}:\mathbb{Z} \rightarrow\mathbb{R}$ that maps the vector indices in $\mathbb{Z}$ to results in $\mathbb{R}$ ). $:$ and $\cdot$ are range-indexing and dot product operations, which can be broken down into summation notation as desired. Incorporating stride, this is $$\text{corr}(\vec{a}, \vec{b}, \text{stride})[i] = \vec{a}\left[i\cdot\text{stride}:i\cdot\text{stride}+\text{len}(\vec{b})\right]\cdot \vec{b}$$ After breaking this down into summation notation, you should be able to extend the dot product operation to consider padded zeros (or more precisely, only sum/multiply existing elements). I will circle back to address this and your question involving matrix multiplication and reindexing when I'm free from work.
