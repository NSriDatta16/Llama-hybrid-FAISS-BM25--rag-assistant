[site]: crossvalidated
[post_id]: 242671
[parent_id]: 138229
[tags]: 
Using gradient descent, we optimize (minimize) the cost function $$J(\mathbf{w}) = \sum_{i} \frac{1}{2}(y_i - \hat{y_i})^2 \quad \quad y_i,\hat{y_i} \in \mathbb{R}$$ If you minimize the mean squared error, then it's different from logistic regression. Logistic regression is normally associated with the cross entropy loss, here is an introduction page from the scikit-learn library . (I'll assume multilayer perceptrons are the same thing called neural networks.) If you used the cross entropy loss (with regularization) for a single-layer neural network, then it's going to be the same model (log-linear model) as logistic regression. If you use a multi-layer network instead, it can be thought of as logistic regression with parametric nonlinear basis functions. However, in multilayer perceptrons, the sigmoid activation function is used to return a probability, not an on off signal in contrast to logistic regression and a single-layer perceptron. The output of both logistic regression and neural networks with sigmoid activation function can be interpreted as probabilities. As the cross entropy loss is actually the negative log likelihood defined through the Bernoulli distribution.
