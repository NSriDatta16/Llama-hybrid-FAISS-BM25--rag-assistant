[site]: datascience
[post_id]: 94859
[parent_id]: 
[tags]: 
Error while using pre-trained model

I'm working on NLP task using RoBERTa model. As training last very long I saved my model, but now for some reason, part of my code doesn't work with this pre-trained model (getting an error), and before saving and uploading newly it, the code worked fine with the trained model. My code: model = model.from_pretrained("/content/drive/MyDrive/model") model = model.cuda() model.eval() def to_list(tensor): return tensor.detach().cpu().tolist() def run_prediction(question_texts, context_text): """Setup function to compute predictions""" examples = [] for i, question_text in enumerate(question_texts): example = SquadExample( qas_id=str(i), question_text=question_text, context_text=context_text, answer_text=None, start_position_character=None, title="Predict", is_impossible=False, answers=None, ) examples.append(example) features, dataset = squad_convert_examples_to_features( examples=examples, tokenizer=tokenizer, max_seq_length=384, doc_stride=128, max_query_length=64, is_training=False, return_dataset="pt", # threads=1, ) eval_sampler = SequentialSampler(dev_dataset) eval_dataloader = DataLoader(dev_dataset, sampler=eval_sampler, batch_size=10) all_results = [] for batch in eval_dataloader: model.eval() batch = tuple(t.to(device) for t in batch) with torch.no_grad(): inputs = { "input_ids": batch[0], "attention_mask": batch[1], "token_type_ids": batch[2], } example_indices = batch[3] outputs = model(**inputs, return_dict=False) for i, example_index in enumerate(example_indices): eval_feature = features[example_index.item()] unique_id = int(eval_feature.unique_id) output = [to_list(output[i]) for output in outputs] start_logits, end_logits = output result = SquadResult(unique_id, start_logits, end_logits) all_results.append(result) output_prediction_file = "predictions.json" output_nbest_file = "nbest_predictions.json" output_null_log_odds_file = "null_predictions.json" predictions = compute_predictions_logits( examples, features, all_results, n_best_size, max_answer_length, do_lower_case, output_prediction_file, output_nbest_file, output_null_log_odds_file, False, # verbose_logging True, # version_2_with_negative null_score_diff_threshold, tokenizer, ) return predictions context = "New Zealand (MƒÅori: Aotearoa) is a sovereign island country in the southwestern Pacific Ocean. It has a total land area of 268,000 square kilometres (103,500 sq mi), and a population of 4.9 million. New Zealand's capital city is Wellington, and its most populous city is Auckland." questions = ["How many people live in New Zealand?", "What's the largest city?"] predictions = run_prediction(questions, context) # Print results for key in predictions.keys(): print(predictions[key]) The received error: Any ideas how to solve this issue? (this code part mostly is based on this notebook: https://colab.research.google.com/github/spark-ming/albert-qa-demo/blob/master/Question_Answering_with_ALBERT.ipynb )
