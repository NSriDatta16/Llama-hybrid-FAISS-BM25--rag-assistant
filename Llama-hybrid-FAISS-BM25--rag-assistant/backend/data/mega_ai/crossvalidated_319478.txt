[site]: crossvalidated
[post_id]: 319478
[parent_id]: 309642
[tags]: 
This question can be answered more precisely than the current answers. Fixing the deviation between the predicted probabilities (the output of the softmax layer of a neural network) and their true probabilities (which represent a notion of confidence), is known as calibration or reliability curves. The issue with many deep neural networks is that, although they tend to perform well for prediction, their estimated predicted probabilities produced by the output of a softmax layer can not reliably be used as the true probabilities (as a confidence for each label). In practice, they tend to be too high - neural networks are 'too confident' in their predictions. Chuan Guo et. al., working with Kilian Weinberger, developed an effective solution for calibrating the predicted probabilities of neural networks in this paper: On Calibration of Modern Neural Networks [1] This paper also explains how predicted probabilities can be interpreted as confidence measures when the predicted probabilities are correctly calibrated. [1] On Calibration of Modern Neural Networks, https://arxiv.org/abs/1706.04599
