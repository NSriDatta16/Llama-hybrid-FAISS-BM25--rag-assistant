ted by values outside the diagonal. In abstract terms, the confusion matrix is as follows: where P = positive; N = negative; TP = truepositive; FP = false positive; TN = true negative; FN = false negative. Plugging the numbers from the formula: MCC = 6 × 3 − 1 × 2 ( 6 + 1 ) × ( 6 + 2 ) × ( 3 + 1 ) × ( 3 + 2 ) = 16 1120 ≈ 0.478 {\displaystyle {\text{MCC}}={\frac {6\times 3-1\times 2}{\sqrt {(6+1)\times (6+2)\times (3+1)\times (3+2)}}}={\frac {16}{\sqrt {1120}}}\approx 0.478} Confusion matrix Let us define an experiment from P positive instances and N negative instances for some condition. The four outcomes can be formulated in a 2×2 contingency table or confusion matrix, as follows: Multiclass case The Matthews correlation coefficient has been generalized to the multiclass case. The generalization called the R K {\displaystyle R_{K}} statistic (for K different classes) was defined in terms of a K × K {\displaystyle K\times K} confusion matrix C {\displaystyle C} . MCC = ∑ k ∑ l ∑ m C k k C l m − C k l C m k ∑ k ( ∑ l C k l ) ( ∑ k ′ | k ′ ≠ k ∑ l ′ C k ′ l ′ ) ∑ k ( ∑ l C l k ) ( ∑ k ′ | k ′ ≠ k ∑ l ′ C l ′ k ′ ) {\displaystyle {\text{MCC}}={\frac {\sum _{k}\sum _{l}\sum _{m}C_{kk}C_{lm}-C_{kl}C_{mk}}{{\sqrt {\sum _{k}\left(\sum _{l}C_{kl}\right)\left(\sum _{k'|k'\neq k}\sum _{l'}C_{k'l'}\right)}}{\sqrt {\sum _{k}\left(\sum _{l}C_{lk}\right)\left(\sum _{k'|k'\neq k}\sum _{l'}C_{l'k'}\right)}}}}} When there are more than two labels the MCC will no longer range between −1 and +1. Instead the minimum value will be between −1 and 0 depending on the true distribution. The maximum value is always +1. This formula can be more easily understood by defining intermediate variables: t k = ∑ i C i k {\displaystyle t_{k}=\sum _{i}C_{ik}} the number of times class k truly occurred, p k = ∑ i C k i {\displaystyle p_{k}=\sum _{i}C_{ki}} the number of times class k was predicted, c = ∑ k C k k {\displaystyle c=\sum _{k}C_{kk}} the total number of samples correctly predicted, s = ∑ i ∑ j C i j {\displaystyle s=\sum _{i}\sum _{j}C_{ij}} the total number of samples. This allows the formula to be expressed as: MCC = c s − t → ⋅ p → s 2 − p → ⋅ p → s 2 − t → ⋅ t → {\displaystyle {\text{MCC}}={\frac {cs-{\vec {t}}\cdot {\vec {p}}}{{\sqrt {s^{2}-{\vec {p}}\cdot {\vec {p}}}}{\sqrt {s^{2}-{\vec {t}}\cdot {\vec {t}}}}}}} Using above formula to compute MCC measure for the dog and cat example discussed above, where the confusion matrix is treated as a 2 × Multiclass example: MCC = ( 6 + 3 ) × 12 − 5 × 4 − 7 × 8 12 2 − 5 2 − 7 2 12 2 − 4 2 − 8 2 = 32 4480 ≈ 0.478 {\displaystyle {\text{MCC}}={\frac {(6+3)\times {\color {green}12}\;-\;{\color {blue}5}\times {\color {brown}4}\;-\;{\color {purple}7}\times {\color {maroon}8}}{{\sqrt {{\color {green}12}^{2}-{\color {blue}5}^{2}-{\color {purple}7}^{2}}}{\sqrt {{\color {green}12}^{2}-{\color {brown}4}^{2}-{\color {maroon}8}^{2}}}}}={\frac {32}{\sqrt {4480}}}\approx 0.478} An alternative generalization of the Matthews Correlation Coefficient to more than two classes was given by Powers by the definition of Correlation as the geometric mean of Informedness and Markedness. Several generalizations of the Matthews Correlation Coefficient to more than two classes along with new Multivariate Correlation Metrics for multinary classification have been presented by P Stoica and P Babu. Advantages over accuracy and F1 score As explained by Davide Chicco in his paper "Ten quick tips for machine learning in computational biology" (BioData Mining, 2017) and "The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation" (BMC Genomics, 2020), the Matthews correlation coefficient is more informative than F1 score and accuracy in evaluating binary classification problems, because it takes into account the balance ratios of the four confusion matrix categories (true positives, true negatives, false positives, false negatives). The former article explains, for Tip 8: In or