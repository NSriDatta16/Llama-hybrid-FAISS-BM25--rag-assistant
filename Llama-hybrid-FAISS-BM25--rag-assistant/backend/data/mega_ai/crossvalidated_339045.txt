[site]: crossvalidated
[post_id]: 339045
[parent_id]: 339016
[tags]: 
Running Monte Carlo on undefined value is a common trap when working stable distributions or distributions with fat tails. For instance, suppose you are dealing with Cauchy distribution . You get a bunch of realizations and calculate the mean. The problem is that Cauchy doesn't have a mean, so your result is going to be nonsense. Here's why. Your Cauchy random numbers are $x_i$, and you calculate the average $\bar x=\frac 1 n\sum_{i=1}^nx_i$. When you have good behaving random numbers, due to CLT the variance of $\bar x$, which is itself a random number is going to decline with sample size increase as $\sigma^2/n$ where $\sigma^2=Var[x_i]$. With Cauchy distribution and with stable distributions this doesn't happen. The variance will keep growing with sample size. So, the same thing happens in Monte Carlo simulations: you get the finite result for the estimator, but its variance increases as you run more simulations, so your mean becomes less and less certain. Actually, with Cauchy distribution the variance itself is undefined: $\sigma^2=\infty$ My example with Cauchy distribution may feel artificial and forced if you're in social sciences. However, this distribution is known as Lorentzian distribution in physics and is sometimes written in a different parameterization. You run into this thing when dealing with certain spectra. Although it's bell shaped like Gaussian, you fit not the mean and standard deviation, but the center and half-width at half-height. These two are properly defined and can be robustly estimated.
