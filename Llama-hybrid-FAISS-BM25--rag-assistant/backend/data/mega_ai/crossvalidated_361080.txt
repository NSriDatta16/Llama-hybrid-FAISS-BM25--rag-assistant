[site]: crossvalidated
[post_id]: 361080
[parent_id]: 361079
[tags]: 
If the data is generated as $y_i = x_i+ \epsilon_i,\ \epsilon_i \sim\mathcal{N}(0, 1)$ then the best line of fit would not have zero error. In fact, no linear model could have zero error. This indicates that we shouldn't attempt to make the loss exactly zero (of course we would still like to make it small). You can always make the loss zero by the following machine learning algorithm: $\text{fit}(D) = f$ where $f(x) = y$ iff $(x,y) \in D$ else $f(x) = 0$. Of course if you constrain yourself to more limited hypothesis spaces such as linear models or neural networks, the finite capacity of the model or optimization difficulties will prevent you from reaching exactly 0 error.
