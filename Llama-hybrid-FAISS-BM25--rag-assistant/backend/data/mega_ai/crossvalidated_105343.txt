[site]: crossvalidated
[post_id]: 105343
[parent_id]: 105286
[tags]: 
It is important to understand what you are going to do with the data. If you are focusing on predictive modeling, as you hint at, then it depends on the algorithm. I would personally suggest using decision tree based algorithms (such as the venerable Random Forest) since they would be able to capture most of the intrinsic information in the raw data form. They will naturally segregate the zeroes if they behave distinctly from the non-zeroes for a given feature. If you use an algorithm more sensitive to scaling (such as Neural Nets or SVMs) then you will want to be more careful about the skewing of the data. It might be worth a simple square-root transformation to reduce the chances of a feature having too much influence. I would definitely not do a standard scaling (i.e. subtract mean, divide by stdev) since that would cause some influential values. For the most part, you want the resulting coefficients to plausibly come from the same distribution.
