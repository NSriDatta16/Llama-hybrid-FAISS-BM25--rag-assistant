[site]: crossvalidated
[post_id]: 103024
[parent_id]: 
[tags]: 
Inconsistency between RMSE and 95% CI Coverage

I am running simulations to compare different weighting methods to estimate the mean of y (with missing values). I use bias, RMSE and 95% CI Coverage as my performance metrics. However, looking at RMSE and coverage, I arrive at different conclusions. For example, for method A and B, bias(x10^3) are 75 and -134 respectively, the corresponding RMSEs(x10^3) are 158 and 159, while coverages are 85% and 34%. Given that the variance of method A is higher than method B, and the weighting model is non-linear, is it normal to have such RMSE-coverage inconsistency? I have been searching for references which point out to this inconsistency. But all I could find was this statement: "In selecting a best single model or model ensemble, the three types of assessments may have to be balanced. The model with the lowest root mean squared error (RMSE) may do worse on trend or have coverage that is too high or too low..." http://www.pophealthmetrics.com/content/pdf/1478-7954-10-1.pdf And if this is the case, can we think of Coverage as a more important performance metric?
