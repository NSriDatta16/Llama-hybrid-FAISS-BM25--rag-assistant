[site]: crossvalidated
[post_id]: 283697
[parent_id]: 274952
[tags]: 
The data in your training and test sets can be modeled as h(x) + noise. In this context, the noise is the variability in your training and test data that is not explained by some common (theoretically optimal) model h(x). The important thing here is that, for example, if your training and test sets are sampled from entirely different distributions, then ALL of your data is noise, even if on their own, both training and test set data are very well structured. In this case, even a model with 1 or 2 parameters will be overfitting right away - regardless of how many data points you have in your training set! In other words - the greater the amount of noise in your data, the easier it is to overfit and the simpler model you are restricted to using. With, say, gaussian noise, increasing the amount of data in your training set increases the data-to-noise ratio, reducing overfitting. If your training and test data are from (slightly) different distributions, increasing the amount of data will do nothing to reduce this source of noise! The data-to-noise ratio will stay the same. Only other sources of noise will be eliminated (e.g. measurement noise, if applicable). I highly suspect this is your problem, given that you are dealing with time series. With 30 million rows, I would attempt to reduce the amount of noise from your data as it is fairly unlikely that your data does not cover the feature space relatively well.
