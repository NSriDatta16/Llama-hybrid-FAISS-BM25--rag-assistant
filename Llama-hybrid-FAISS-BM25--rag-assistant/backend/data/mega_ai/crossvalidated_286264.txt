[site]: crossvalidated
[post_id]: 286264
[parent_id]: 286259
[tags]: 
First, we need to incorporate the impact of getting a wrong classification by using a loss function $L(\hat{y}_i,y)$, where $\hat y_i$ is the predicted classification and $y_i$ is the actual classification. In your case, the worst potential loss will be for $L(\hat y_i, \mathrm{A})$ if $\hat y_i \neq \mathrm{A}$ One approach is bagging : Pick some aggregation approach (consensus or more complicated approach). Examples of more a complicated aggregation approach could be one where you set a "bias fraction" $f$ for A, so that the consensus for a "non-A" class must be greater than $f$ (maybe require 60% supermajority not just 51%) or it must be $1+f$ times the conensus fraction for A. Split data into K folds, train classifiers on the K-1 training folds, and then test your aggregation methods by feeding test data to trained classifiers and calculating the loss of each prediction. Take the average loss as your metric. Now, you'll need to adjust your method and see how well it does under cross validation. If you're going to be doing a lot of comparisons, I'd suggest holding out a validation set for testing the final, chosen model (trained on the entire dataset (minus validation points)).
