[site]: crossvalidated
[post_id]: 489592
[parent_id]: 
[tags]: 
not understanding hidden layers and what weight*node really means

I know how to write up a neural network in python and I understand the "math" behind it. i,e. the value of hidden node is the linear combination of all the prior nodes and weights. This is then squashed with some sort of function (sigmoid, Relu, etc). What i'm trying to understand is, why this works out the way it is. For example, given a dataset, the intuition is that hidden layer 1 is used to detect a squiggle, hidden layer 2 detects a circle, etc etc....but I'm unable to see how this logic makes sense. How does doing a linear combination of the prior nodes and weights conclude to detecting squiggles, circles, eyes etc? I'm trying to get a very layman terms of what goes on in the hidden layer, but at the same time, I want it to make sense with the algebra that is being used.
