[site]: crossvalidated
[post_id]: 503079
[parent_id]: 31867
[tags]: 
The following is taken from my manuscript on p-value functions - Johnson, Geoffrey S. "Decision Making in Drug Development via Inference on Power" Researchgate.net (2021) . In any quantitative field it is not enough to simply apply a set of mathematical operations. One must also provide an interpretation. The field of statistics concerns itself with a special branch of mathematics regarding probability. When interpreting probability there are primarily two competing paradigms: Bayesian and frequentist. These paradigms differ on what it means for something to be considered random and what probability itself measures. Both frequentists and Bayesians would agree that once a test statistic is observed it is fixed, there is nothing random about it. Additionally, frequentists and most Bayesians would agree that the parameter under investigation, say $\theta$ , is an unknown fixed quantity and it is simply treated as random in the Bayesian paradigm as a matter of practice. The question then becomes, "How do we interpret probability statements about a fixed quantity?" Without delving into the mathematical details of how a posterior or a p-value is calculated, I explore various interpretations below and what makes them untenable. One interpretation of a Bayesian prior is that "random'' is synonymous with "unknown'' and probability measures the experimenter's belief so that the posterior measures belief about the unknown fixed true $\theta$ given the observed data. This interpretation is untenable because belief is unfalsifiable $-$ it is not a verifiable statement about the actual parameter, the hypothesis, nor the experiment. Another interpretation is that "random'' is short for "random sampling'' and probability measures the emergent pattern of many samples so that a Bayesian prior is merely a modeling assumption regarding $\theta$ , i.e. the unknown fixed true $\theta$ was randomly selected from a known collection or prevalence of $\theta$ 's (prior distribution) and the observed data is used to subset this collection, forming the posterior distribution. The unknown fixed true $\theta$ is now imagined to have instead been randomly selected from the posterior. This interpretation is untenable because of the contradiction caused by claiming two sampling frames. The second sampling frame is correct only if the first sampling frame is correct, yet there can only be a single sampling frame from which we obtained the unknown fixed true $\theta$ under investigation. A third interpretation of a Bayesian prior is that "random'' is synonymous with "unrealized'' or "undetermined'' and probability measures a simultaneity of existance so that $\theta$ is not fixed and all values of $\theta$ are true simultaneously; the truth exists in a superposition depending on the data observed according to the posterior distribution (think Schr√∂dinger's cat). This interpretation is untenable because it reverses cause and effect $-$ the population-level parameter depends on the data observed, but the observed data depended on the parameter. Ascribing any of these interpretations to the posterior allows one to make philosophical probability statements about hypotheses given the data. While the p-value is typically not interpreted in the same manner, it does show us the plausibility of a hypothesis given the data $-$ the ex-post sampling probability of the observed result or something more extreme if the hypothesis for the unknown fixed $\theta$ is true. One might notice the similarity between a p-value and a posterior probability (or a confidence interval and a credible interval) and wonder under what circumstances is each one preferable. At its essence this is a matter of scientific objectivity. To the Bayesian, probability is axiomatic and measures the experimenter. To the frequentist, probability measures the experiment and must be verifiable. The Bayesian interpretation of probability as a measure of belief is unfalsifiable. Only if there exists a real-life mechanism by which we can sample values of $\theta$ can a probability distribution for $\theta$ be verified. In such settings probability statements about $\theta$ would have a purely frequentist interpretation. This may be a reason why frequentist inference is ubiquitous in the scientific literature. If the prior distribution is chosen in such a way that the posterior is dominated by the likelihood or is proportional to the likelihood, Bayesian belief is more objectively viewed as confidence based on frequency probability of the experiment. In short, for those who subscribe to the frequentist interpretation of probability the p-value function summarizes all the probability statements about the experiment one can make as a function of the hypothesis for $\theta$ . It is a matter of correct interpretation given the definition of probability and what constitutes a random variable. The posterior remains an incredibly useful tool and can be interpreted as an approximate p-value function.
