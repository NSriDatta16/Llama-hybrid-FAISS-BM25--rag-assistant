[site]: datascience
[post_id]: 116473
[parent_id]: 
[tags]: 
Why does Adam outperform SGD in logistic regression?

I am training a logistic regression model. In case it matters, the features are 1376-dimensional embeddings output from a neural network. I tried both SGD and Adam with a learning rate of $10^{-3}$ for 100 epochs, and the final AUC is 0.875 for SGD and 0.973 for Adam. Why is Ada so much better for a convex optimization problem such as logistic regression?
