[site]: crossvalidated
[post_id]: 453686
[parent_id]: 159070
[tags]: 
I think for kNN distance plays a bigger role. As Bernhard Sch√∂lkopf put it, "a high-dimensional space is a lonely place". What happens to an (hyper) cube is analogous to what happens to the distance between points. As you increase the number of dimensions, the ratio between the closest distance to the average distance grows - this means that the nearest point is almost as far away as the average point, then it has only slightly more predictive power than the average point. This article explains it nicely Joel Grus does a good job of describing this issue in Data Science from Scratch. In that book he calculates the average and minimum distances between two points in a dimension space as the number of dimensions increases. He calculated 10,000 distances between points, with the number of dimensions ranging from 0 to 100. He then proceeds to plot the average and minimum distance between two points, as well as the ratio of the closest distance to the average distance (Distance_Closest / Distance_Average). In those plots, Joel showed that the ratio of the closest distance to the average distance increased from 0 at 0 dimensions, up to ~0.8 at 100 dimensions. And this shows the fundamental challenge of dimensionality when using the k-nearest neighbors algorithm; as the number of dimensions increases and the ratio of closest distance to average distance approaches 1 the predictive power of the algorithm decreases. If the nearest point is almost as far away as the average point, then it has only slightly more predictive power than the average point.
