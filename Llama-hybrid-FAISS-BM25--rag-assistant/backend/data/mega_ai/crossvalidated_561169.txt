[site]: crossvalidated
[post_id]: 561169
[parent_id]: 
[tags]: 
Benefits of Expectation Maximization for Mixture Models

What are the benefits of using expectation maximization for mixture models vs. direct maximization of the marginal likelihoods? Analytic maximization step In case of Gaussian mixtures the benefit is obvious: the maximization step can be done analytically, which allows to replace complex optimization by a simple iterative algorithms. This answer makes this point more generally: Expectation maximization is a technique to solve statistical problems that consist of an "easy" maximization (if some latent variables were known), and an "easy" expectation calculation on the log-likelihood (if the parameters were known). However, the "how" and "why" the expectation and maximization steps require ingenuity, and are model-specific. So while it's possible that some models from deep learning could be posed in fashion that might leverage EM, EM is not a generic optimization technique, not even to classical statistical models. Numerical maximization step This advantage is however less evident in cases where the maximization step has to be done numerically. This answer argues that EM is still advantageous in the sense that it eases the use of optimization algorithms and easier to interpret: One benefit of EM is that it naturally produces valid parameters for the mixture distribution on every iteration. In contrast, standard optimization algorithms would need constraints to be imposed. For example, say you're fitting a Gaussian mixture model. A standard nonlinear programming approach would require constraining covariance matrices to be positive semidefinite, and constraining mixture component weights to be nonnegative and sum to one. Another aspect of numerical maximization step is that it is not guaranteed to find the absolute maximum of the llikelihood. Therefore one may have to rerun the algorithm multiple times with different starting parameters . Moreover, it is not certain that monotonuous increase of the likelihood is guaranteed in this case (see my related question ). I am looking therefore for the discussion of the advantages of EM technique in comparison to direct maximization of the marginal likelihood (the comparison with Markov Chain Monte Carlo is perhaps more obvious). Related questions: Why should one use EM vs. say, Gradient Descent with MLE? What is the difference between EM and Gradient Ascent? Update @Xi'an point out in their comment that the likelihood for a mixture model is highly multimodal (I also encountered a similar point in these notes ). However this raises a few questions: Is multimodality of mixture likelihood an empirical observation or a mathematical fact? This problem is not exactly bypassed by EM: in order to find teh global minimum, the algorithm has to be launched several times, just as we would have done with direct gradient descent minimization.
