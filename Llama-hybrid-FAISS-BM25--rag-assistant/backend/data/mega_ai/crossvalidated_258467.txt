[site]: crossvalidated
[post_id]: 258467
[parent_id]: 192187
[tags]: 
The differences between complex-valued and real-valued networks have been thoroughly explained in an ESANN 2011 paper by Zimmermann, Minin and Kusherbaeva (Comparison of the Complex Valued and Real Valued Neural Networks Trained with Gradient Descent and Random Search Algorithms) which reading I recommend. Main points : The feed-forward function may have singularity points ; you can avoid that by using a sigmoid complex function that has a singularity at infinite values only, and use a bounded range for your inputs to stay safe ; The back-propagation algorithm can be recovered by basing the weight adaptation procedure on the Taylor expansion for the error $E(W+\Delta W)= E(W) + G^T\Delta W + 1/2 \Delta W^T G \Delta W $ then the rule for weight adaptation can be written as $\Delta w = -\eta . \delta E/\delta W $ with $\eta$ as learning rate. The sensitivity to initial conditions led the authors of the article to recommend the use of RSA (Random Search Algorithm) rather than direct random initialization of the weights. Once RSA is used the complex-valued neural network will always converge, and is reported to be on par with real-valued network (slightly better but not significantly in the experiments). In conclusion : you can use complex weights in neural networks if your domain requires it. However you should devote some time to build a sound specific library, as there are subtle pitfalls.
