[site]: crossvalidated
[post_id]: 86041
[parent_id]: 85583
[tags]: 
I found "Machine Learning with R" by Brett Lantz (Packt Publishing) a good intro into how to code many representitive ML algos in R. There's a chapter on Naive Bayes. http://www.packtpub.com/machine-learning-with-r/book Using that book for a reference to solve your model, Import the data into R (assuming it's in a CSV file. There are several ways data can be read into R) # set stringsAsFactors = TRUE as all your inputs are categorical A_raw This will create a data frame object which you can see the properties using str(A_raw). You should have 5 columns, a,b,c,d and T and all should be of type Factor. create trainging and test datasets Say you have 1000 rows in your data set, you can use 80% for training and 20% for testing. Before dividing the data first you need to make sure that the rows are in a random order set.seed(12345) # Randomize the data A_rand You can confirm you have a similar percentage of each type of T in both the training set and testing set prop.table(table(A_train$T)) prop.table(table(A_test$T)) Use the Naive Bayes function from the e1071 package A_classifier Evaluate the model : A_predicted You can then use CrossTable from the gmodels library to compare the predicited values to the actual values CrossTable(A_predicted, A_test$T, prop.chisq = FALSE, prop.t = FALSE, dnn = c('predicted', 'actual')) Improve the model performance One way (which the book mentions) is to set a value for the Laplace estimator when training the model but this really only is applicable when there is a large number of input variables and there is a chance that there could be zero instances of 1 of the inputs associated with one of the outcomes. In your case when there is only 4, then it is probably unlikely. In case you wanted to set the Laplace variable you can do this: A_classifier2 Of course you can then apply this method for dataset B too.
