[site]: datascience
[post_id]: 67245
[parent_id]: 
[tags]: 
Visualizing F-score differences in information extraction

I have several corpora and NLP systems (including a few merge ensembles of output of these systems combined in unions and intersections) with which I have extracted the annotation span sets {(begin, end)} for each corpus across all documents within the corpus and compared the span sets to each corpus's respective gold standard and thus obtained standard measures of F-score, precision and recall. I am trying to qualitatively assess why certain systems don't perform as well as a particular ensemble combination on F-score, so I figured the easiest way would be to generate precision-recall or ROC curves. The task is just a simple binary classification: either a span of text is annotated (labeled as 1) or it is not (labeled as 0). I have numpy vectors of the same length for each document in the corpus for both the system predictions and the gold standard, so I plan on using these for y_true and y_predict when trying to generate my ROC curve. Is this a good approach to observe the behavior of my F-scores, assuming I plot them all on the same graph? If not, any recommendations for a better approach would be most appreciated.
