[site]: crossvalidated
[post_id]: 410532
[parent_id]: 236797
[tags]: 
As far as I understand your question, you have five images, that should, but don't have to show the same thing, and given the pictures you want to predict something. For processing each of the pictures you probably should stick with stat of the art for the pictures, that is, some kind of convolutional neural network. Since each of the pictures is assumed to show the same thing, as said by others, you should use the Siamese architecture in here, so the same convolutional network should be used for processing each of the images. As I understand it, the problem is with combining the information from each of the pictures. Obviously, recurrent network would not work in here, because it explicitly assumes that the order does matter. Notice however that if you used convolutional network, then order would matter as well (pictures appearing next to each other would be processed by a single convolutional kernel). Same with dense network, since it would have separate parameters for each of the positions of an image, so it would make a difference if it appeared on the first position. If you really want to ignore the position, then if I were you, I would try an architecture that aggregates the information from all the pictures weighting them equally. For this purpose you can use single aggregating function (e.g. sum, product), or multiple such functions in parallel . The architecture, that I am thinking of is something like $$\begin{align} \boldsymbol{z}_i &= \operatorname{CNN}(\mathbf{X_i}) \\ \boldsymbol{a} &= g(\boldsymbol{z}_1, \boldsymbol{z}_2, \dots, \boldsymbol{z}_k) \\ y &= f(\boldsymbol{a}) \end{align}$$ where $g$ is something like $$ \boldsymbol{a}_i = g(\boldsymbol{z}_{1i}, \boldsymbol{z}_{2i}, \dots, \boldsymbol{z}_{ki}) = \sum_{j=1}^k \boldsymbol{z}_{ji} $$ So each of the $\mathbf{X_i}$ for $i \in 1,2,\dots,k$ pictures you use the same CNN network to output some latent representation $\boldsymbol{z}_i$ . Next, you aggregate it using aggregation function $g$ , and pass this through an "output" network $f$ (e.g. dense layer on top of it followed by an adequate activation function).
