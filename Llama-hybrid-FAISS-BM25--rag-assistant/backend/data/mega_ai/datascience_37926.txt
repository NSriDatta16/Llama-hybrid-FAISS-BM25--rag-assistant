[site]: datascience
[post_id]: 37926
[parent_id]: 
[tags]: 
Dueling DQN - why should we decompose and then combine them back into?

could anyone who can help me if we decompose them and combine back them into a single Q, what the network can learn? from my perspective，the V means the total reward when the agent follow the current policy;the Q means if we give a specific action then follow the current policy what the total reward; and if we get the optimal policy，the V will equal to Q;so we should learn to make the A reach zero;just like the answer ： Dueling DQN - can't understand its mechanism but in that paper，i cannot understand what is the matter if we cannot identify the sense that given Q we cannot recover V and A uniqueluy. and it will get this blow finally： and as well as this： refer to this blog: https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df
