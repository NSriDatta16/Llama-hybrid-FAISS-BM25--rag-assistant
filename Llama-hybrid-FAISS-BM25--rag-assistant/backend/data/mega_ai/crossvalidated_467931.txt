[site]: crossvalidated
[post_id]: 467931
[parent_id]: 467888
[tags]: 
As Pohoua commented, your understanding is correct (but I would say not entirely*). Concepts like confidence intervals, p-values, and hypothesis tests are not calculated from the likelihood $f(\theta|x)$ with $x$ fixed, but instead with the pdf $f(x|\theta)$ , where $\theta$ is fixed, which is a different slice of the joint distribution $f(x,\theta)$ .Confidence intervals, p-value, and hypothesis tests, are different things than just the information from likelihood ratios . So in that sense frequentist statistics 'needs'/'uses' the sampling distribution of the entire sample $f(x\vert \theta)$ (and as Tim Maks answer argues it does not need the sample distribution in many other ways). But in your example you speak about the sampling distribution of a statistic** as in a sample distribution of values like the sample mean and sample variance (an interpretation that you repeat in a question about the CLT ). This more narrow sense of sampling distribution is not necessary/needed for frequentists statistics. The sampling distribution (of a statistic) is not being used by frequentist statistics but it is the subject of many frequentist statistics. Frequentist statistics is a lot about sampling distributions of an estimate/statistic, and in Bayesian statistics the sampling distribution hardly occurs. But, for several reasons, it would be wrong to say that Bayesian statistics is ' bypassing the use of the sampling distribution'. A 'bypass' is not really the right word. Bayesian statistics is answering a different question than frequentist statistics (or at least takes a different point of view), and Bayesian statistics is no more bypassing the use of the sampling distribution than frequentist statistics is bypassing the use of the prior distribution. In a similar way a soccer/football player is not bypassing the use of a backhand and a tennis player is not bypassing the use of slidings, or a carpenter is not bypassing the use of paint and a painter is not bypassing the use of wood. *Your understanding is incorrect in the sense that it relates to the role of the difference between population distribution and sample distribution of a statistic. This misunderstanding relates to something that you expressed in an earlier question , where you end up concluding that in a Bayesian analysis one can not use the CLT because we are not supposed to think of sample distributions when using a Bayesian analysis. The likelihood function is not always so easy to compute and in that case one needs to use approximations instead of a direct analytical solution, like computational approximations by sampling. One may also use more analytical approximations, for instance like employing the CLT and a synthetic likelihood . The bypass occurs at a different level. A difference between Bayesian/frequentists statistics is that with a frequentist method you analyse the joint distribution $f(\boldsymbol{\theta},\mathbf{x})$ by considering the whole space of possible observations $x_1, x_2, \dots, x_n$ , whereas with Bayesian methods you condition on the observation and only consider the values of the function $f(\boldsymbol{\theta},\mathbf{x})$ for a fixed single particular observation. This difference makes that something like using a statistic (and the related sample distribution) is useful for a frequentist method because it greatly simplifies the computations and visualisation of the whole sample space for $\mathbf{x}$ , by replacing it with the sample space for a statistic. The Bayesian method does not bypass this sampling distribution. By this I do not mean that the Bayesian method needs the sampling distribution (it doesn't), but I mean that it is not a bypass. What the Bayesian method is 'bypassing' is a need to make calculations with the joint distribution of parameters and observations $f(\boldsymbol{\theta},\mathbf{x})$ for values other than the actual observation, since the method conditions on the observation. And maybe the question is indirectly about that (but it is not so clear). The sampling distribution is in fact a shortcut (and not something cumbersome that is to be bypassed). With a frequentist method you can just as well work with the likelihood function and for instance do maximum likelihood estimation or confidence intervals. But the sample distribution of an estimate/statistic is the best language to do this. Frequentist/Bayesian is not a dichotomy There is no clear border what frequentist and Bayesian statistics means. One can do empirical Bayesian analysis or use Jeffreys prior in which case one is loosening the conditioning on the observation. And one can make an analysis that is frequentist-like but is not using an estimate/statistic and it's sample distribution. Many people are just fitting curves with models by using some linear or non-linear fitting package and use something like an estimate of the inverse of the Fisher information matrix to express the variance/error of the estimate and there is no direct computation of the sample distribution. Or one can do something else like using AIC/BIC to express goodness of fit, or use a Bayes factor or fiduciary or likelihood intervals. When a sample distribution is used, then it is not really a tool that is something that can be 'bypassed'. The sample distribution is the goal itselve. And if you like you could apply it to a Bayesian estimate (although it makes less sense in such a setting). In frequentist statistics you don't have to compute a sampling distribution of an estimate. In frequentist statistics, or whatever it is, you don't have to calculate these statistics and their sample distribution. You can also work only with the likelihood function in order to make point or interval estimates. The method in the example of the question, with the sampling distribution of the mean is derived from maximum likelihood estimation and effectively equivalent. You do not need a sample distribution of a statistic or estimate (but it does make the analysis simpler) to compute it. For instance to make a maximum likelihood estimate for a population mean $\mu$ of a normal distributed population we use the likelihood function: $$\mathcal{L}(\mu \vert x_1,x_2,\dots,x_n ,\sigma) = \prod_{1\leq i \leq n} \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2}\left(\frac{x_i-\mu}{\sigma}\right)^2} $$ and the $\mu$ that maximizes this function is the MLE estimate. This is very similar to Bayesian maximum a posteriori estimate , which is just maximizing $$ f_{posterior}(\mu \vert x_1,x_2,\dots,x_n ) \propto \mathcal{L}(\mu \vert x_1,x_2,\dots,x_n ,\sigma) \cdot f_{prior}(\mu)$$ The only difference is that the likelihood function is multiplied with the prior probability. Similarly for confidence intervals, one could use z or t-statistics, but effectively those statistics are shortcuts for the more difficult geometrical shape of the density distribution in all coordinates of the observation $\mathbf{x}$ . We can derive p-values, statistical tests (and related confidence intervals) by only considering whether an observation is 'extreme' or not. And this can be defined by the likelihood function without considering a statistic/estimate and it's sample distribution (e.g. likelihood ratio test, if the likelihood is below a certain value than the value is not inside the confidence region). This view is also illustrated here where a test is not viewed by considering the sampling distribution of a statistic, but by considering the PDF of the whole data (in that case the data is two variables X and Y). The sample distribution occurs particularly in the method of moments. We can use the moments of a sample to estimate the moments of a distribution and in that case we may wish to express the sample distribution of the moments of a sample. But the method of moments is different from maximum likelihood estimation (but maybe this is already not frequentist?), and we do not use this sample distribution in every type of analysis. **This question is not entirely clear about what is meant with 'sampling distribution' (an ambiguity which causes two diverging type of answers). For this answer I interpret sampling distribution as the distribution of a statistic or the distribution of an estimate. And I interpret a statistic in the sense of R.A. Fisher "statistic may be defined as a function of the observations designed as an estimate of the parameters". In this answer I argue that you do not need such sampling distributions (e.g. you do not need to work as you describe, compute sample mean and sample variance. Instead, you can use the likelihood/probability function directly. But the sampling distribution, and related sufficient statistics, does make it easier.). I do not interpret sample distribution more generally as the distribution of observations/samples.
