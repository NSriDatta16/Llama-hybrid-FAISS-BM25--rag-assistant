[site]: datascience
[post_id]: 57208
[parent_id]: 57191
[tags]: 
Big problem and very good question! I used spacy in the past, which has a German module . I guess stemming is not supported, but lemmatization . Looking at the output below, I don't think that spacy will solve your problem to be honest. However, I just wanted to let you know about this option. Spacy Lemmatization: #pip install spacy #python -m spacy download de import spacy nlp = spacy.load('de_core_news_sm') mywords = "Das ist schon sehr schön mit den Expertinnen und Experten" for t in nlp.tokenizer(mywords): print("Tokenized: %s | Lemma: %s" %(t, t.lemma_)) Result: Tokenized: Das | Lemma: der Tokenized: ist | Lemma: sein Tokenized: schon | Lemma: schon Tokenized: sehr | Lemma: sehr Tokenized: schön | Lemma: schön Tokenized: mit | Lemma: mit Tokenized: den | Lemma: der Tokenized: Expertinnen | Lemma: Expertinnen Tokenized: und | Lemma: und Tokenized: Experten | Lemma: Experte
