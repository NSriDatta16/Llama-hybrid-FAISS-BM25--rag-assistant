[site]: crossvalidated
[post_id]: 387076
[parent_id]: 
[tags]: 
Which model should I try first?

I think about appropriate modelling technique in the following task: I have news texts (around 50K), and I have news topics made from the texts (250) which have various number of texts that made them up. I have already made topic modelling based on my data. I wan't to carry hypothesis testing on whether frequency of certain tokens inside texts is signficantly linked to the topic size. For example, texts with frequent word 'blast' tend to form topics where there are 500 news texts involved on average, while frequent word 'politics' makes lesser topics. Example of my data is on the picture. I have experience with linear models, so first that comes to mind is inclusion of the frequence of the tokens of interest as regressors, whether observation is a news text: > summary(lm(topic_size ~ freq_word1 + freq_word2, data = example_dat)) Call: lm(formula = topic_size ~ freq_word1 + freq_word2, data = example_dat) Residuals: Min 1Q Median 3Q Max -250.02 -183.07 -11.38 162.57 371.67 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 256.02 45.61 5.613 5.9e-06 *** freq_word1 -1182.85 3669.49 -0.322 0.750 freq_word2 -5199.39 3804.66 -1.367 0.183 --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 191.9 on 27 degrees of freedom Multiple R-squared: 0.0767, Adjusted R-squared: 0.008307 F-statistic: 1.121 on 2 and 27 DF, p-value: 0.3405 However, I am not sure how to deal with the dependent variable "topic_size" which stays the same of hundreds of texts (which belong to one topic), and sometimes different topics have the same "topic_size". Is it related to mixed modelling, or a OLS linear model suffices my task? UPDATE (one word frequency VS. topic_size): > mod summary(mod) Call: lm(formula = topic_size ~ word_freq, data = dat_train) Residuals: Min 1Q Median 3Q Max -232.95 -219.95 7.05 165.05 466.28 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 236.9471 0.9124 259.692
