[site]: crossvalidated
[post_id]: 500190
[parent_id]: 500167
[tags]: 
Yes, in some circumstances, no in others. For example, if a bias results from a self-inconsistent assumption, then no. Examples of this latter include omitted variable bias and AIC in the case of censored data, which violates the maximum likelihood assumption. Examples of when it pertains would be AIC in the case of complete support (i.e., without censoring such that the maximum likelihood assumption pertains), and ordinary least squares for equidistant independent axis data. In still others, for example, variance is generally unbiased, but standard deviation is not, see this . Standard deviation would still be asymptotically correct because the small number bias would reduce to zero for $n\to\infty$ . Nevertheless, one should not rely on just any asymptotic convergence, if a rather better estimator is available, and see how this was done in this example . Briefly, if you small number correct standard deviations from a large number of 2 sample SD's and then average them, you will obtain a more variable estimate than if you root mean square combine all the variances and then use a much lesser small number correction for the total number of trials. Some people are surprised at how ineffectual AIC can be for small samples. Thus, how fast asymptotic convergence occurs can be critical to interpretation of statistical results, and sometimes, for example for AIC, when we do not have measures that inform us of how precise or accurate statistical results are, it can be problematic. Thus, the question of whether or not a procedure is asymptotically convergent is not by itself a sufficient criterion of validity of statistical results. We also need confidence intervals for those results.
