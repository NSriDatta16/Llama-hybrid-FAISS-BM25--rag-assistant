[site]: crossvalidated
[post_id]: 320389
[parent_id]: 320375
[tags]: 
Probability distrubution over datasets: What are the datasets? How is the probability distribution generated? Once we can estimate the underlying distributions of the input data, we essentially know how they are picked and can do good predictions. (generative model). Normally, we can assume an underlying distribution according to what we believe (inductive bias). For example, if we believe that there is a high probability that values are close to zero, we can take a Gaussian distribution with mean $0$ and tune the parameters like variance when we train. Datasets are, for example, set of all coin tosses and the distribution assumed will be binomial. When we do say maximizing log-likelihood for the actual data points, we will get those parameters which make the dataset fit into the distribution assumed. The examples are independent of each other. Can you give me an example of where the examples are dependent? For example, we toss a coin and if we have a head we toss another otherwise we do not. Here there is a dependence between subsequent tosses Drawn from the same probability distribution as each other. Suppose the probability distribution is Gaussian. Does the term "same probability distribution" mean that all the examples are drawn from a Gaussian distribution with the same mean and variance? "This assumption enables us". What does this mean? Yes. That is why (4) is said. Once you have a probability distribution from one example, you do not need other examples to describe the data generating process. Finally, for the last paragraph of page 122, it is given that the samples follow Bernoulli distribution. What does this mean intuitively? It means that each example can be thought of as a coin toss. If the experiment was multiple coin tosses, you would have each coin toss independent with a probability of head to be $\frac{1}{2}$ . Similarly, if you choose any other experiment, the result of each example can be thought of as a coin toss or an n-dimensional dice. Generating examples means getting a distribution closest to what we see in the dataset for training. That is got by assuming a distribution and maximizing the likelihood of the given dataset and outputting the optimum parameters.
