[site]: crossvalidated
[post_id]: 573223
[parent_id]: 409496
[tags]: 
Other answerers mentioned EM, which is the most traditional approach. There are many others, however. you can do direct likelihood optimization, as various others have also remarked: run filter/smoother, obtain likelihood, iterate/optimize. Gradient expressions are available, see, e.g., Bayesian filtering and smoothing by Simo Särkkä , Chapter 12. (I have implemented this in my personal little Matlab Kalman filter/smoother toolbox , I'm sure it's also available elsewhere.) Notice that in this approach, you can use arbitrary combinations of unknown parameters, exploit known structure elements if you know part of or the functional form of one of the unknown matrices, impose constraints on the optimization scheme, etc. you can also exploit other gradient-based optimization schemes; see, e.g., Fitting a Kalman smoother to data (Barratt, Boyd; 2020) for an approach using a proximal gradient method; python toolbox available here . The optimization problem can, in general, be non-convex. You can ignore that and hope for the best and/or use some kind of global optimization strategy, use multiple starting points, use some stochastic optimization scheme, etc. If you do have some reasonable prior guess about possible parameter values, exploiting that in the initialization and/or providing appropriate constraints can greatly help with convergence.
