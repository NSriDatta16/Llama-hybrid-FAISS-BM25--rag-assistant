 game at two points: The discriminator's strategy set is the set of measurable functions of type D : Ω → R {\displaystyle D:\Omega \to \mathbb {R} } with bounded Lipschitz norm: ‖ D ‖ L ≤ K {\displaystyle \|D\|_{L}\leq K} , where K {\displaystyle K} is a fixed positive constant. The objective is L W G A N ( μ G , D ) := E x ∼ μ G ⁡ [ D ( x ) ] − E x ∼ μ ref [ D ( x ) ] {\displaystyle L_{WGAN}(\mu _{G},D):=\operatorname {E} _{x\sim \mu _{G}}[D(x)]-\mathbb {E} _{x\sim \mu _{\text{ref}}}[D(x)]} One of its purposes is to solve the problem of mode collapse (see above). The authors claim "In no experiment did we see evidence of mode collapse for the WGAN algorithm". GANs with more than two players Adversarial autoencoder An adversarial autoencoder (AAE) is more autoencoder than GAN. The idea is to start with a plain autoencoder, but train a discriminator to discriminate the latent vectors from a reference distribution (often the normal distribution). InfoGAN In conditional GAN, the generator receives both a noise vector z {\displaystyle z} and a label c {\displaystyle c} , and produces an image G ( z , c ) {\displaystyle G(z,c)} . The discriminator receives image-label pairs ( x , c ) {\displaystyle (x,c)} , and computes D ( x , c ) {\displaystyle D(x,c)} . When the training dataset is unlabeled, conditional GAN does not work directly. The idea of InfoGAN is to decree that every latent vector in the latent space can be decomposed as ( z , c ) {\displaystyle (z,c)} : an incompressible noise part z {\displaystyle z} , and an informative label part c {\displaystyle c} , and encourage the generator to comply with the decree, by encouraging it to maximize I ( c , G ( z , c ) ) {\displaystyle I(c,G(z,c))} , the mutual information between c {\displaystyle c} and G ( z , c ) {\displaystyle G(z,c)} , while making no demands on the mutual information z {\displaystyle z} between G ( z , c ) {\displaystyle G(z,c)} . Unfortunately, I ( c , G ( z , c ) ) {\displaystyle I(c,G(z,c))} is intractable in general, The key idea of InfoGAN is Variational Mutual Information Maximization: indirectly maximize it by maximizing a lower bound I ^ ( G , Q ) = E z ∼ μ Z , c ∼ μ C [ ln ⁡ Q ( c ∣ G ( z , c ) ) ] ; I ( c , G ( z , c ) ) ≥ sup Q I ^ ( G , Q ) {\displaystyle {\hat {I}}(G,Q)=\mathbb {E} _{z\sim \mu _{Z},c\sim \mu _{C}}[\ln Q(c\mid G(z,c))];\quad I(c,G(z,c))\geq \sup _{Q}{\hat {I}}(G,Q)} where Q {\displaystyle Q} ranges over all Markov kernels of type Q : Ω Y → P ( Ω C ) {\displaystyle Q:\Omega _{Y}\to {\mathcal {P}}(\Omega _{C})} . The InfoGAN game is defined as follows:Three probability spaces define an InfoGAN game: ( Ω X , μ ref ) {\displaystyle (\Omega _{X},\mu _{\text{ref}})} , the space of reference images. ( Ω Z , μ Z ) {\displaystyle (\Omega _{Z},\mu _{Z})} , the fixed random noise generator. ( Ω C , μ C ) {\displaystyle (\Omega _{C},\mu _{C})} , the fixed random information generator. There are 3 players in 2 teams: generator, Q, and discriminator. The generator and Q are on one team, and the discriminator on the other team. The objective function is L ( G , Q , D ) = L G A N ( G , D ) − λ I ^ ( G , Q ) {\displaystyle L(G,Q,D)=L_{GAN}(G,D)-\lambda {\hat {I}}(G,Q)} where L G A N ( G , D ) = E x ∼ μ ref , ⁡ [ ln ⁡ D ( x ) ] + E z ∼ μ Z ⁡ [ ln ⁡ ( 1 − D ( G ( z , c ) ) ) ] {\displaystyle L_{GAN}(G,D)=\operatorname {E} _{x\sim \mu _{\text{ref}},}[\ln D(x)]+\operatorname {E} _{z\sim \mu _{Z}}[\ln(1-D(G(z,c)))]} is the original GAN game objective, and I ^ ( G , Q ) = E z ∼ μ Z , c ∼ μ C [ ln ⁡ Q ( c ∣ G ( z , c ) ) ] {\displaystyle {\hat {I}}(G,Q)=\mathbb {E} _{z\sim \mu _{Z},c\sim \mu _{C}}[\ln Q(c\mid G(z,c))]} Generator-Q team aims to minimize the objective, and discriminator aims to maximize it: min G , Q max D L ( G , Q , D ) {\displaystyle \min _{G,Q}\max _{D}L(G,Q,D)} Bidirectional GAN (BiGAN) The standard GAN generator is a function of type G : Ω Z → Ω X {\displaystyle G:\Omega _{Z}\to \Omega _{X}} , that is, it is a mapping from a latent s