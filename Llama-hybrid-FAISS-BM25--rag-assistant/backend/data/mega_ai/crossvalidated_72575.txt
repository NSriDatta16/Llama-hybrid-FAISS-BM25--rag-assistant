[site]: crossvalidated
[post_id]: 72575
[parent_id]: 
[tags]: 
How high must logistic covariates' predictive accuracy be for a reversal effect to show up?

I am modeling an outcome for hospital patients, 'RA' (whether readmitted; 0=No, 1=Yes). My predictor of interest is 'HHS' (whether referred to Home Health Services such as from a visiting nurse; 0=No, 1=Yes). Those referred readmit at a 15.2% rate; others, 9.2%, but the former are needier, sicker patients. Conventional thinking is that if we controlled for severity of illness this difference would not only be washed out but would reverse itself. In other words, holding constant the severity of illness, having HHS should mean a lower RA rate. With HHS as the sole predictor, its coefficient (B) in a logistic regression = 0.6 (N ~ 25k). B is reduced to 0.2 with a group of covariates controlled, each accounting for some aspect of severity of illness, but B doesn't fall below zero. HHS alone explains only about 1% of the variance in RA; with the other predictors, this becomes 4%.* Perhaps this is the problem--that these covariates are not explaining enough variance to "succeed" in reversing the sign of the coefficient of interest. If this is true, is there a way to estimate how high their explained variance needs to be for such a reversal to show up? EDIT: Alecos Papadopoulos has come up with an impressive solution that answers this question, soon to be published in The American Statistician . See https://www.tandfonline.com/doi/full/10.1080/00031305.2019.1704873 *Using either of 2 pseudo-R-squared formulas; Cox & Snell's or Menard's [-2LL0 - (-2LL1)] / [-2LL0.]
