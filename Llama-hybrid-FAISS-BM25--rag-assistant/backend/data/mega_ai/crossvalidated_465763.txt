[site]: crossvalidated
[post_id]: 465763
[parent_id]: 
[tags]: 
The connection between the expectation in expectation maximization and the importance sampling?

The log-likelihood of the EM algorithm can be expressed as \begin{align} \ell(\theta, x) &= \log p(x|\theta) \\ &= \log \sum_z p(x, z|\theta) \\ &= \log \sum_z \frac{q(z|x)}{q(z|x)}p(x,z|\theta)\\ &= \log \sum_z q(z|x)\frac{p(x,z|\theta)}{q(z|x)}\\ &\ge \sum_z q(z|x)\log\frac{p(x,z|\theta)}{q(z|x)} \end{align} It seems that the third equality is very like the importance sampling, and $q(z|x)$ is like the proposal distribution and the $\frac{p(x,z|\theta)}{q(z|x)}$ is like the sampling ratio or sampling weight. Since the $q(z|x)$ is known after the expectation step can we utilize the MCMC to do the maximization? I know expectation maximization is simpler and computing cheaper than importance sampling, but I wonder if they have a connection in that way?
