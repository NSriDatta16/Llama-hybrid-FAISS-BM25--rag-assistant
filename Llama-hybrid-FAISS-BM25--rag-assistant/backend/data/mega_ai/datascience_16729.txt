[site]: datascience
[post_id]: 16729
[parent_id]: 
[tags]: 
Why neural networks models do not allow for multiplication of inputs?

In a neural network, each neuron value is multiplied by the weight of the connection. Then, each neuron's input is the sum of all those values, on which we apply the activation function (sigmoid, relu, tanh, etc.). Why can't the input be a vector of all those values, so that I can decide to multiply them if I want, or apply another function from the vector space to the reals, and then apply the activation function on that real value? Any implementation I have seen does not allow this, and if I wanted to do it, it seem I would have to implement my whole neural network architecture by myself without the possibility of using Theano, Keras or other libraries. Is there a reason that escapes me of why this would not work? And if there isn't, is there any library out there I could use that already does this without having to create it from scratch?
