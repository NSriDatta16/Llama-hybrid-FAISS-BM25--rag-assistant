[site]: crossvalidated
[post_id]: 27450
[parent_id]: 
[tags]: 
Best method for transforming low discrepancy sequence into normal distribution?

I've been using low discrepancy sequences for a while for Uniform Distributions, as I've found their properties useful (mainly in computer graphics for their random appearance and their ability to densely cover [0,1] in an incremental fashion). For example, random values above, Halton sequence values below: I was considering using them for some financial analysis planning, but I need different distributions than just uniform. I started off trying to generate a normal distribution from my uniform distributions via the Marsaglia polar algorithm, but the results don't seem as good as with the uniform distribution. Another example, again random above, Halton below: My question is: What is the best method for getting a normal distribution with the properties I get from a uniform low discrepancy sequence - coverage, incremental fill-in, non-correlation across multiple dimensions? Am I on the right track, or should I be taking a completely different approach? (Python code for uniform and normal distributions I use above: Gist 2566569 )
