[site]: datascience
[post_id]: 49226
[parent_id]: 
[tags]: 
Generate predictions that are orthogonal (uncorrelated) to a given variable

I have an X matrix, a y variable, and another variable ORTHO_VAR . I need to predict the y variable using X , however, the predictions from that model need to be orthogonal to ORTHO_VAR while being as correlated with y as possible. I would prefer that the predictions are generated with a non-parametric method such as xgboost.XGBRegressor but I could use a linear method if absolutely necessary. This code: import numpy as np import pandas as pd from sklearn.datasets import make_regression from xgboost import XGBRegressor ORTHO_VAR = 'ortho_var' TARGET = 'target' PRED = 'yhat' # Create regression dataset with two correlated targets X, y = make_regression(n_features=20, random_state=245, n_targets=2) indep_vars = ['var{}'.format(i) for i in range(X.shape[1])] # Pull into dataframe df = pd.DataFrame(X, columns=indep_vars) df[TARGET] = y[:, 0] df[ORTHO_VAR] = y[:, 1] # Fit a model to predict TARGET xgb = XGBRegressor(n_estimators=10) xgb.fit(df[indep_vars], df[TARGET]) df[PRED] = xgb.predict(df[indep_vars]) # Correlation should be low or preferably zero pred_corr_w_ortho = df.corr().abs()[PRED][ORTHO_VAR] assert pred_corr_w_ortho Returns this: --------------------------------------------------------------------------- AssertionError 1 pred_corr_w_ortho = df.corr().abs()[PRED][ORTHO_VAR] ----> 2 assert pred_corr_w_ortho ...and I would like something that maintains as much predictive accuracy as possible while remaining orthogonal to ORTHO_VAR
