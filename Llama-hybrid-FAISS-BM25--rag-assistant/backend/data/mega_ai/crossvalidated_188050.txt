[site]: crossvalidated
[post_id]: 188050
[parent_id]: 188046
[tags]: 
In neural network terminology we usually say a network is recurrent rather than recursive . Assuming you mean the same as I do (at least one layer in which nodes feed back into themselves). That list actually links right to this tutorial on recurrent neural networks and word embeddings . Also LSTM and GRU are variants on the RNN topology that have both been proven to work very well in language processing. It's worth learning abotu vanilla RNNs first but also checking out LSTM and GRU if you are in to NLP use cases.
