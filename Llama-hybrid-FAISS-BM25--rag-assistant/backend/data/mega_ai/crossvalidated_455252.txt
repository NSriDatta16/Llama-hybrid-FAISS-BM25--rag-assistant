[site]: crossvalidated
[post_id]: 455252
[parent_id]: 454814
[tags]: 
Following the notation from Wikipedia , the $L$ dimensional PCA compression is performed as following: $ T_L = X W_L $ where $W_L$ is a matrix whose columns are the $L$ eigenvectors with the highest eigenvalues of the matrix $X^TX$ . In order to reverse this compression you can right multiply both sides of the equation by of $W_L^T$ , giving: $T_L W_L^T \approx X $ You can see this implemented in sklearn in the implementation of inverse_transform . I can only give a hand-wavy version of the math, essentially because $X^T X$ is symmetric and positive definite we have that the eigenvectors of $X^T X$ are orthogonal (and can be made to have unit norm). Thus $W$ is orthonormal and $W^{-1} = W^T$ . So yes, in order to "decompress" you need to store $W_L$ , the matrix containing the eigenvectors derived from the original dataset.
