[site]: crossvalidated
[post_id]: 562340
[parent_id]: 562058
[tags]: 
The way how the OP is written and the results are evaluated, this question appears to be confusing a prediction interval and a confidence interval. The prediction interval gives an interval estimate of a random variable , while a confidence interval provides an interval estimate of a parameter. The difference between the two is conceptually as large as the difference between the standard deviation of a variable and the standard error of its mean. EDIT (closing logical gap): The confusion arises often in relation to model predictions, because they can be viewed both as guess for a single random variable as well as an estimate of a conditional mean. END EDIT. Jackknife estimates are providing uncertainties for an estimated parameter , so the method you try gives a confidence interval for the conditional mean, while your OP asks for a prediction interval. This would somewhat explain why more trees (-> more robust estimates of the conditional mean) leads to shorter intervals. Two approaches that I can think of: Quantile random forest I wanted to give you an example how to use quantile random forest to produce (conceptually slightly too narrow) prediction intervals, but instead of getting 80% coverage, I end up with 90% coverage, see also @Andy W's answer and @Zen's comment. Similar happens with different parametrizations. I cleaned up the code a little bit, so here is it, anyway. library(ranger) friedman $y) & (test$ y Generic prediction intervals There is a generic method that I use from time to time that works with any regression method. It is based on two models: the first is a model for the conditional mean. The second one models the absolute residuals of the model and, as such, provides a model for the conditional standard deviation of the conditional response distribution. The prediction interval is then constructed in the same way as you do in your example. Similar to the quantile approach above, it ignores model uncertainty. A second issue is that the loss function used in the second model is rather arbitrary. library(ranger) friedman $predictions - training$ y) ~ ., data = training) pred_mean $predictions pred_sd predictions Lower $y) & (test$ y Interestingly, the coverage of this method is very similar to the one of the quantile random forest, which might somewhat indicate that Friedman's data is easy to predict?
