[site]: crossvalidated
[post_id]: 341753
[parent_id]: 
[tags]: 
How do we know the value of the regularization parameter satisfies the gradient equations required by Lagrange Multipliers?

I've take multiple machine learning classes and I am always told been told say when we do regularization on the training error $\mathcal E(W) = \frac{1}{n} \sum^n_{i=1}Loss(f(x_i),y_i)$: $$ \text{ (1) } \min_W L(W,\lambda) = \mathcal E(W) + \lambda \|W\|^2 $$ and that the above is equivalent to the constraint problem: $$ \text{ (2) } \min_W \mathcal E(W) \text{ subject to } \|W\|^2 \leq R^2$$ and that some $\lambda$ corresponds to some radius $R$. This last claim is the one I don't fully understand why is true. The reason is the following. Usually I am told that the equivalence between the two comes from Lagrange multipliers. I think I do understand Lagrange multipliers (when constraints are equalities), in that case if we have an objective function $f(x)$ with constraint $h(x) = R$ then we just make a new constraint $g(x) = h(x) - R = 0$ $$ \min/\max f(x) \text{ subject to } g(x) = 0 $$ and solve wrt to $x$ and $\lambda$ the auxiliary equation: $$ \mathcal L(x,\lambda) = f(x) + \lambda g(x)$$ in the following way: $$ \nabla_{x,\lambda} L(x,\lambda) = 0 $$ and that works because that satisfies that the objective function and constraint have parallel paths (so intuitively you can't improve f if you keep walking in g so its a critical point): $$ \nabla f(x) = -\lambda g(x)$$ however usually in Lagrange multipliers we also take the derivative with respect to $\lambda$ because that implies the constraint is satisfied . However, because $\lambda$ is of degree 1 then it disappears we take the take the derivative: $$ \nabla_\lambda L(x, \lambda) = \nabla_\lambda (f(x,y) - \lambda g(x,y)) \iff g(x,y) = 0 $$ which satisfies the constraint but seems to be independent of any value of $\lambda$...this is the part where I get confused because usually in Tikhonov like Regularization we impose $\lambda > 0 $ which now I guess I see that the value of $\lambda$ doesn't matter because this last equation is always satisfied no matter what value it takes... but I've always been told that each different $\lambda$ corresponds to some different radius and I've never actually mathematically clearly seen why thats true. Can someone show me this? I guess intuitively its always sort of made sense because the larger $\lambda$ the more the constraint is enforced so the smaller $R$? Why can't we see an explicit relationship between the two $R$ and $\lambda$ even if its just theoretically or in paper? Also usually in machine learning we solve (2) and find some minimum (or critical point, even in neural nets things are empirically known to be minimums because the training loss is usually extremely close to zero or zero, as low as it can go by design) so because of this we know we are in a critical point of $L$ with respect to $W$, but are we in a critical point with respect to $\lambda$? It seems that we always are at a critical point with respect to $\lambda$ no matter its value so we only need to worry about minimizing with respect to $\lambda$? What is up with this $\lambda$ constraint and how do we know in machine learning that we are choosing it so that it actually satisfies the constraint and how does it relate to the constraint?
