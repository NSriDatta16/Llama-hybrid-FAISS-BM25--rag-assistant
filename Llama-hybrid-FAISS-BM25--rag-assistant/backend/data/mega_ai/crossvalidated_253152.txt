[site]: crossvalidated
[post_id]: 253152
[parent_id]: 
[tags]: 
Reversing "mean-centered" parameters in a multiple linear regression

I tried searching for this question on stats stack exchange and found Implementing linear regression with standardization but the answer was a little difficult to follow. I'm reading "Bayesian Analysis with Python" by Osvaldo Martin (great read btw) and in his hierarchical linear models section he often mean-centers the data and the reverses it. Can somebody please explain this process to me and how to rearrange the values to visualize the reversal after mean-centering? The line that is confusing me is alpha = pm.Deterministic("alpha", alpha_tmp - pm.math.dot(beta, X_mean)) why does subtracting the dot product of the betas and the mean from the alphas reverse the mean centering? I feel like I'm missing something very simple. The author implements it in Python 3.5 using a module that is up and coming called pymc3 . Here is the code excerpt below: alpha_tmp is the alpha when X is mean centered. The formula that is being used is: $$\mu = \alpha + \beta_1*x_1 + \beta_2*x_2$$ import pymc3 as pm import numpy as np # Multiple Linear Regression # pg. 132 np.random.seed(314) N = 100 alpha_real = 2.5 beta_real = [0.9, 1.5] eps_real = np.random.normal(loc=0, scale=0.5, size=N) X = np.array([np.random.normal(i,j, N) for i,j in zip([10,2],[1,1.5])]) X_mean = X.mean(axis=1, keepdims=True) X_centered = X - X_mean y = alpha_real + np.dot(beta_real, X) + eps_real with pm.Model() as model_mlr: alpha_tmp = pm.Normal("alpha_tmp", mu=0, sd=10) beta = pm.Normal("beta", mu=0, sd=1, shape=2) epsilon = pm.HalfCauchy("epsilon", 5) mu = alpha_tmp + pm.math.dot(beta, X_centered) alpha = pm.Deterministic("alpha", alpha_tmp - pm.math.dot(beta, X_mean)) y_pred = pm.Normal("y_pred", mu=mu, sd=epsilon, observed=y) start = pm.find_MAP() step = pm.NUTS(scaling-start) trace_mlr = pm.sample(5000, step=step, start=start) varnames = ["alpha", "beta", "epsilon"] pm.traceplot(trace_mlr, varnames) # Below is output of stderr Optimization terminated successfully. Current function value: 74.986175 Iterations: 23 Function evaluations: 31 Gradient evaluations: 31 100%|██████████| 5000/5000 [00:13
