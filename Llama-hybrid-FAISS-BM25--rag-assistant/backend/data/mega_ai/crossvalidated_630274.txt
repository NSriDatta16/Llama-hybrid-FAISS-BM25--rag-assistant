[site]: crossvalidated
[post_id]: 630274
[parent_id]: 630261
[tags]: 
Here is an image that I made with the R-code below. The computation is with n-bins = 50. There is in this simulation also an offset but smaller and with more bins I get a smaller offset. This might be because you approach the values of a discretized normal distribution and not the values of a continuous normal distribution. So one should not expect the same. For small sample sizes there is lower entropy. This might be counter intuitive because one would imagine a small sample to be more variable and more chaotic. But, the state described by the histogram of a small sample is a very particular state and has more order than the average of many different samples. The increase of the sample size can be seen as blending multiple states and that creates an average that blends out to a smooth histogram, a histogram that looks like a more homogeneous sample. set.seed(1) n_range = 10^seq(2,6,0.2) entropy_range = c() n_bins = 50 for (n in n_range) { x = rnorm(n, mean = 0, sd = 2) h = hist(x, breaks = n_bins, freq = 0) p = h $density[h$ density > 0] bin_size = h $mids[2]-h$ mids[1] entropy = -1*sum(log(p)*p*bin_size) entropy_range = c(entropy_range, entropy) } plot(x= n_range, y= entropy_range, log = "x") lines(x= c(1,10^6), y= c(1,1)*(0.5*log(8*pi)+0.5), lty = 2) A tricky part is that the computation of entropy is different for continuous distributions $$\int f(x) \log \left( f(x) \right) \text{d} x \approx \sum_{k = 1}^{n_{bins}} f_k \log \left( f_k \right) \cdot \text{bin_size}$$ When you just apply the bin probabilities $p_k = f_k \cdot \text{bin_size}$ and use the formula for discrete distributions then you get a different values. $$\sum_{k = 1}^{n_{bins}} f_k \cdot \log \left( f_k \right) \cdot \text{bin_size} \neq \sum_{k = 1}^{n_{bins}} f_k \cdot \text{bin_size} \cdot \log \left( f_k \cdot \text{bin_size} \right) = \sum_{k = 1}^{n_{bins}} p_k \cdot \log \left( p_k \right)$$ You will be wrong with a factor $\log(\text{bin_size})$ and that explains the different curve displacements shown in your second edit. In short: entropy is the expectation of the logarithm of the probability mass function or the probability density function $E[\log p(X)]$ and $E[\log f(X)]$ . Problems arrise when we mix those formulas and functions.
