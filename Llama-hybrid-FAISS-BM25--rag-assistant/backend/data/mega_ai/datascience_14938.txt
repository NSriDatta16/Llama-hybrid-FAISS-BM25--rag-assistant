[site]: datascience
[post_id]: 14938
[parent_id]: 14932
[tags]: 
Available compute power does not directly affect the accuracy of a neural network. If your different runs of the network have: identical architecture and meta-params identical code (including library code) all training data is identical all stochastic parts of training use the same random seed and generator all data types are identical precision (e.g. all vectors and matrices are 32-bit or 64-bit floats) then the behaviour of neural network training in each run is fully deterministic and repeatable. Having a faster processor will just get you to the result faster*. The most likely difference between your tests is due to not seeding the random number generators used in the training the process. For you this includes weight initialisation, possibly train/test split and possibly shuffling training data in each epoch. As you did not use any regularisation, then accuracy of the trained network can vary quite a bit due to over-fitting. To verify this, you can train a second or third time on each CPU. I expect you will see a lot of variation in final accuracy, regardless of which machine you run it on. * This does mean that having a faster machine can result in you having a more accurate final network in practice when you are tuning the parameters, because you can try more variations of meta-params with multiple training sessions.
