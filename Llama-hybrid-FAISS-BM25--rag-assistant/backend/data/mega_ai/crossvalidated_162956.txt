[site]: crossvalidated
[post_id]: 162956
[parent_id]: 162951
[tags]: 
What you are looking for is a recurrent neural network, and in particular the current favorite flavor, the LSTM. The amount of data you have is miniscule, but since the pattern is simple you may get it to work with a tiny network that has a handful of neurons. There are a number of state-of-the-art neural network toolboxes, although they are still maturing and only partially plug-and-play. Worse, they may not offer out-of-the-box support for LSTMs. There is Caffe, Torch, Theano, Nervana Systems Neon, etc. They work mostly on Python and Linux. However, I can recommend the perfect starting point. Andrej Karpathy did a fun piece of research that garnered a lot of press in which he trained an LSTM network on text corpuses (such as Shakespeare and the Linux source code) and used it to generate more text. The results, to put it objectively, were not as awful as one might expect. Anyway, his code, which uses Torch, is simple, well-commented, and available on GitHub . You should read his detailed blog post as well. I think you can repurpose his code for your problem quite easily. You can quantize and encode the numerical values as ascii characters, and use the char-rnn scripts to train on the data as well as produce a prediction. Hint: when using sample.lua for prediction, set the argument -primetext to the last data in the time series as the starting point for prediction, and set the argument -temperature low to generate the most expected progression. Have fun!
