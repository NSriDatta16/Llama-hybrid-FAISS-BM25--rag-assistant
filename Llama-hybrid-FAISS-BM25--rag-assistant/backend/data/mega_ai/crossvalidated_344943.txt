[site]: crossvalidated
[post_id]: 344943
[parent_id]: 344937
[tags]: 
From Bishop, a Markov chain is ergodic when you can run it starting from any initial distribution and end up converging to its invariant distribution (steady state, or equilibrium). A sufficient condition for ergodicity is that you can move from any state to any other with nonzero probability in a finite number of steps.
