[site]: crossvalidated
[post_id]: 555826
[parent_id]: 
[tags]: 
Choosing best hyperparameters for multiple regression when number of features is higher than number of samples

I am a chemist mostly, and I do not have much experience in statistical learning. However, I am currently starting work on a problem that requires multiple regression. I have a set of molecules, for each of which, there is a fingerprint (an array of 1024 binary digits). Each of those 1024 bits have something to do with the presence or absence of a certain feature in the molecule. I have ~130 molecules in my training dataset, each of which have a known target property (a floating point number between -100 and +100). I need to fit a model to the target property using the fingerprint array as input. I have used scikit-learn in python for basic linear regression before, but it seems that for multiple regression, there is a whole load of options, and for each of these options, there is a number of hyperparameters that need to be tuned. I am starting with support vector machine regression ( sklearn.svm.SVR ). The scikit-learn manual says that if the number of features is higher than the number of samples, then careful choosing of the kernel function and regularization term is necessary. However, I cannot find much information in how to do this, or at least a rule of thumb regarding what values to choose. I also wish to use ridge regression, lasso and possibly a CNN to test which algorithm gives the best prediction. However, as I have a much larger number of features (1024) than the number of samples (130), I am worried that the learning methods might fail because I chose the wrong hyperparameters. Any advice on where to start and if there are any rules of thumb for choosing the hyperparameters? Due to time constraints, it is not possible for me to do a proper optimization of the hyperparameters as a lot of people seem to have done (with scikit-optimize).
