[site]: crossvalidated
[post_id]: 129852
[parent_id]: 129843
[tags]: 
I don't think this is a clustering problem at all. You may have been using the wrong tool for the problem. See: clustering algorithms like k-means will try to force all your data objects into k clusters, minimizing the sum of squared deviations. But does it make sense in your case to minimize the sum of squared deviations? IMHO, it doesn't. Instead, you need to choose an approach that uses all your knowledge of the data (which k-means doesn't). In particular, you may want to break your data into three groups first: players who didn't play, players who won, and players who lost. By doing so, you will effectively eliminate the last two columns, but that is okay. Then you may want to figure out a derived measure for your score. For example, compute the relative change in score, i.e. $$ f(x) = \frac{\text{score}_t(x) - \text{score}_{t-1}(x)}{\text{score}_{t-1}(x)}$$ which hopefully makes players much more similar; since I assume the score change depends on the average score of the opponents. Then, plug in your team size. If you know the teams are always of size 2, find the partitioning of the data into pairs of two player, such that the errors are the smallest. Most likely, you can just sort your data and pair even and odd.
