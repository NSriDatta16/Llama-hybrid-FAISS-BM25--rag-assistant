[site]: crossvalidated
[post_id]: 235398
[parent_id]: 92903
[tags]: 
I suggest to look at it a different way ... In logistic regression we predict some binary class {0 or 1} by calculating the probability of likelihood, which is the actual output of $\text{logit}(p)$ . This, of course, is assuming that the log-odds can reasonably be described by a linear function -- e.g., $\beta_0 + \beta_1x_1 + \beta_2x_2+ \dotsm $ ... This is a big assumption, and only sometimes holds true. If those $x_i$ components don't have independent, proportional influence on the log-odds, then best to choose another statistical framework. I.e., the log-odds is made up of some fixed component $\beta_0$ , and increased incrementally by each successive term, $\beta_i x_i$ . In short, the $\beta_0$ value is the "fixed component" of that component-wise method to describe the log-odds of whatever event/condition you are trying to predict. Also remember that a regression is ultimately describing some conditional average, given a set of $x_i$ values. None of those things require that $x_i$ -values be 0 in your data or even possible in reality. The $\beta_0$ simply shifts that linear expression up or down so that the variable components are most accurate. Maybe I said the same thing in slightly different mindset, but I hope this helps ...
