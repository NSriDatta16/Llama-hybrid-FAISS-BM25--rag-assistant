[site]: datascience
[post_id]: 25752
[parent_id]: 
[tags]: 
How does keras calculate accuracy for multi label classification?

I am using this code for a multilabel problem classification. from __future__ import print_function from keras.preprocessing import sequence from keras.models import Sequential from keras.layers import Dense, Embedding from keras.layers import LSTM from keras.models import Model from keras.datasets import imdb max_features = len(vocabDic) maxlen = 500 # cut texts after this number of words (among top max_features most common words) batch_size = 32 train_set = sequence.pad_sequences(train_set, maxlen=maxlen) test_set = sequence.pad_sequences(test_set, maxlen=maxlen) print('train_set shape:', train_set.shape) print('test_set shape:', test_set.shape) print('Build model...') model = Sequential() model.add(Embedding(max_features, 128)) model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2)) model.add(Dense(90, activation='sigmoid')) # try using different optimizers and different optimizer configs model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) print('Train...') model.fit(train_set, train_labels, batch_size=batch_size, epochs=15, validation_data=(test_set, test_labels)) score, acc = model.evaluate(test_set, test_labels, batch_size=batch_size) print('Test score:', score) print('Test accuracy:', acc) My problem is there are 90 classes and the accuracy is too high from the second epoch. I suspect keras is computing something incorrectly. Any clues? Edit: I calculated the total recall of the model. It is barely 5%( 5 times better than random). Is it a normal behavior for such a problem?
