[site]: crossvalidated
[post_id]: 395860
[parent_id]: 391480
[tags]: 
Function approximation is used for its ability to generalize. Your proposal to parameterize $\hat{q}$ with a matrix $\mathbf{W}$ prevents any sort of generalization (or "cross-talk") across different actions. Depending on your set of actions, you might not need this generalization, and your approach might work just fine. I think this is probably true for the mountain car problem. However, imagine a 2d analogue to this problem in which the car had to choose a an angle of velocity $0 \leq \theta \leq 360$ that was discretized into 36 actions that are increments of $10^o$ . (We're imagining a car that could accelerate in any one of these directions instantaneously.) I wouldn't be surprised if "cross-talk" enabled the agent to learn more quickly that $\hat{q}(s,20^o)$ is often similar to $\hat{q}(s,30^o)$ , for example. As far as feature selection goes, I'd suggest tile coding , described on page 217 of the book you mentioned. By selecting the shape, spacing, and size of your tiles, you can control the nature of the resulting generalization across the state and action space.
