[site]: datascience
[post_id]: 116998
[parent_id]: 116800
[tags]: 
Here is how the fairseq implementation works. First it turns the BH x T x S matrix into B x T x S , where B is batch size, H is number of heads, T is target length, and S is source length. You are not using multihead so can skip this. (Also when doing self-attention T == S , what you have called L ). As can final step to split the data back into BH x T x S after applying the mask. Their key_padding_mask is what you are calling src_key_padding_mask , and is B x L in size. So the next step is to turn that into B x 1 x 1 x L . Again, as you are not using multihead, you only need to use unsqueeze(1) once to give you B x 1 x L . They then use masked_fill. So it becomes a one-liner: out = out.masked_fill( src_key_padding_mask.unsqueeze(1), float("-inf") ) It goes after your current mask code and just before the call to softmask. BTW, I wrote the above referencing my own local copy which I've hacked on and simplified a lot. Looking at the github version (linked above) I see they use view() to add a 5th dimension, and then use unsqueeze() three times. I can't tell you why; something to do with when kv_bsz is different to bsz . It is only in the not is_tpu block, so just some clever optimization?
