[site]: stackoverflow
[post_id]: 2609575
[parent_id]: 2609542
[tags]: 
100% code coverage is not a desirable goal. See this blog for some reasons. My best practice is to derive test cases from use cases. Create concrete traceability (I use a UML tool but a spreadsheet will do as well) between the use cases your system is supposed to implement and test cases that proves that it works. Explicitly identify the most critical use cases. Now look at the test cases they trace to. Do you have many test cases for the critical use cases? Do they cover all aspects of the use case? Do they cover negative and exception cases? I have found that to be the best formula (and best use of the team's time) for ensuring good coverage. EDIT: Simple, contrived example of why 100% code coverage does not guarantee you test 100% of cases. Say CriticalProcess() is supposed to call AppendFile() to append text but instead calls WriteFile() to overwrite text. [UnitTest] Cover100Percent() { CriticalProcess(true, false); Assert(FileContents("TestFile.txt") == "A is true"); CriticalProcess(false, true); Assert(FileContents("TestFile.txt") == "B is true"); // You could leave out this test, have 100% code coverage, and not know // the app is broken. CriticalProcess(true, true); Assert(FileContents("TestFile.txt") == "A is trueB is true"); } void CriticalProcess(bool a, bool b) { if (a) { WriteFile("TestFile.txt", "A is true"); } if (b) { WriteFile("TestFile.txt", "B is true"); } }
