[site]: datascience
[post_id]: 101901
[parent_id]: 101899
[tags]: 
Backpropagation algorithm is the way the neural network weights are optimized (learned), i.e., what the optimizer uses for this purpose, so yes it can be considered the training algorithm. In backpropagation, you do not need to explicitely calculate the Jacobian matrix (see this source of info for more detail ), but you calculate the first derivatives of your loss function with respect to its weights via the chain rule (from calculus). This chain rule lets us find the gradient of more complex functions by splitting it into simpler derivatives. Then, by applying the gradient descent strategy (following the steepest descent, i.e., adding the negative derivative at hand for each trainable weight), the weights are adjusted in each iteration until certain values generate a loss which can be accepted or until a certain number of rounds is reached. As a simple example of how each derivative would be used to update each weight, we can think of linear regression applying gradient descent: As clearly pointed out in the fantastic book Deep learning with python by Fran√ßois Chollet : "Backpropagation starts with the final loss value and works backward from the top layers to the bottom layers, computing the contribution that each parameter had in the loss value" .
