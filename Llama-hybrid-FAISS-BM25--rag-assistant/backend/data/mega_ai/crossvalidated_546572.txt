[site]: crossvalidated
[post_id]: 546572
[parent_id]: 
[tags]: 
How to prove that the posterior of the regression coefficients $\mathbf{w}$ is roughly gaussian in MAP regularized logistic regression?

The logistic regression model is $$ p(y=\pm 1 \mid \mathbf{x}, \mathbf{w})=\sigma\left(y \mathbf{w}^{\mathrm{T}} \mathbf{x}\right)=\frac{1}{1+\exp \left(-y \mathbf{w}^{\mathrm{T}} \mathbf{x}\right)} $$ It can be used for binary classification or for predicting the certainty of a binary outcome. See Cox & Snell (1970) for the use of this model in statistics. This note focuses only on computational issues related to maximum-likelihood or more generally maximum a-posteriori (MAP) estimation. A common prior to use with MAP is: $$ p(\mathbf{w}) \sim \mathcal{N}\left(\mathbf{0}, \lambda^{-1} \mathbf{I}\right) $$ Using $\lambda>0$ gives a "regularized" estimate of $\mathrm{w}$ which often has superior generalization performance, especially when the dimensionality is high (Nigam et al., 1999). Given a data set $(\mathbf{X}, \mathbf{y})=\left[\left(\mathbf{x}_{1}, y_{1}\right), \ldots,\left(\mathbf{x}_{N}, y_{N}\right)\right]$ , we want to find the parameter vector $\mathbf{w}$ which maximizes: $$ l(\mathbf{w})=-\sum_{i=1}^{n} \log \left(1+\exp \left(-y_{i} \mathbf{w}^{\mathrm{T}} \mathbf{x}_{i}\right)\right)-\frac{\lambda}{2} \mathbf{w}^{\mathrm{T}} \mathbf{w} $$ The gradient of this objective is $$ \mathbf{g}=\nabla_{\mathbf{w}} l(\mathbf{w})=\sum_{\boldsymbol{i}}\left(1-\sigma\left(y_{i} \mathbf{w}^{\mathrm{T}} \mathbf{x}_{i}\right)\right) y_{i} \mathbf{x}_{i}-\lambda \mathbf{w} $$ The Hessian of the objective is $$ \mathbf{H}=\frac{\mathrm{d}^{2} l(\mathbf{w})}{\mathrm{d} \mathbf{w} \mathrm{d} \mathbf{w}^{\mathrm{T}}}=-\sum_{i} \sigma\left(\mathbf{w}^{\mathrm{T}} \mathbf{x}_{i}\right)\left(1-\sigma\left(\mathbf{w}^{\mathrm{T}} \mathbf{x}_{i}\right)\right) \mathbf{x}_{i} \mathbf{x}_{i}^{\mathrm{T}}-\lambda \mathbf{I} $$ which in matrix form can be written $$ \begin{aligned} a_{i i} &=\sigma\left(\mathbf{w}^{\mathrm{T}} \mathbf{x}_{i}\right)\left(1-\sigma\left(\mathbf{w}^{\mathrm{T}} \mathbf{x}_{i}\right)\right) \\ \mathbf{H} &=-\mathbf{X} \mathbf{A} \mathbf{X}^{\mathrm{T}}-\lambda \mathbf{I} \end{aligned} $$ Note that the Hessian does not depend on how the x's are labeled. It is nonpositive definite, which means $l(\mathbf{w})$ is convex [sic]. For sufficiently large $\lambda$ , the posterior for $\mathrm{w}$ will be approximately Gaussian: $$ \begin{aligned} p(\mathbf{w} \mid \mathbf{X}, \mathbf{y}) & \approx N\left(\mathbf{w} ; \hat{\mathbf{w}},-\mathbf{H}^{-1}\right) \\ \hat{\mathbf{w}} &=\operatorname{argmax} p(\mathbf{w}) \prod_{i} p\left(y_{i} \mid \mathbf{x}_{i}, \mathbf{w}\right) \end{aligned} $$ Is the gaussian distribution for weights a trivial result that the prior is normally distributed? I think this proof might involve bayes rule, and they stated the likelihood function is can be approximated by $$ p(\mathbf{y} \mid \mathbf{X}) \approx p(\mathbf{y} \mid \mathbf{X}, \hat{\mathbf{w}}) p(\hat{\mathbf{w}})(2 \pi)^{d / 2}|-\mathbf{H}|^{-1 / 2} $$ where $d$ is the dimensionality of $\mathrm{w}$ . https://tminka.github.io/papers/logreg/minka-logreg.pdf
