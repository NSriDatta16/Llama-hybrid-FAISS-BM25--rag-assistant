[site]: crossvalidated
[post_id]: 513135
[parent_id]: 513131
[tags]: 
The predictors, let's call them var1 and var2, will be turned into dummy variables by the software performing your logistic regression. For example, if var1 and var2 were binary (i.e. values of 0 or 1), the model would look like: $logit(p)=log(\frac{p}{1-p})=\beta0+\beta1\times var1 +\beta2\times var2$ However, if var2 had 3 categories -- low, medium, high for example -- then the model would look like: $logit(p)=log(\frac{p}{1-p})=\beta0+\beta1\times var1 +\beta2\times var2Medium + \beta3\times var2High$ In this case, low is the 'reference category' for variable var2. When var2 is low, the dummy variable var2Medium is 0 and the dummy variable var2High is 0. If var2 is medium, then var2Medium is 1 and var2High is 0. If var2 is high, then var2Medium is 0 and var2High is 1. If there are X categories in your categorical predictor variable, you will need X-1 dummy variables to model it. The use of dummy variables to model categorical predictors in a regression model is crucial to all regression models, not just logistic regression. It is generally covered in beginner regression classes. https://www.youtube.com/watch?v=fTfMdCQJz4s To check for multicollinearity, you can check the variance inflation factor of each variable. If the VIF is > 2.5, you probably have some collinearity to sort out. However, for dummy variables (i.e. categorical variables with 3 or more categories, as demonstrated above), the VIF will be high and is nothing to worry about. Read this for more information: https://statisticalhorizons.com/multicollinearity
