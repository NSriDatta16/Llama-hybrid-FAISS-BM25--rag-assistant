[site]: crossvalidated
[post_id]: 512391
[parent_id]: 512374
[tags]: 
I'll abstract from the misspecification (the true relationship between the predictor and the observation is exponential, but you are fitting a polynomial) and assume our models are correctly specified. the SSE seems to be performing well on average, but with MAPE, the model is performing better than the one calculated by SSE, except at the larger values of x Key question: what does "performing well" mean ? Whether something performs well or not presupposes a metric. Whether car A performs "better" than car B depends on whether you are more interested in its fuel efficiency, or its handling, or its safety record. Here, whether a model "performs well" can only be answered by recourse to some error metric. We have two such metrics in play here: the MAPE and the SSE. I very much assume that the model that was fitted to minimize (in-sample) MAPE performs better than the model that was fitted to minimize SSE. And vice versa. To repeat, there is no absolute sense in which a model performs well or not - this can only be said relative to an error metric. the SSE seems to be performing well on average, but with MAPE, the model is performing better than the one calculated by SSE, except at the larger values of x Actually, the larger values of your predictor $x$ are a red herring. What is relevant are the larger values of the observations $y$ . Suppose we are looking at one particular observation $y_i$ , and our model fit is $\hat{y}_i$ . The error contribution is $(y_i-\hat{y}_i)^2$ to the SSE error metric, and $\frac{|y_i-\hat{y}_i|}{y_i}$ for the MAPE error metric. So for instance, if $y_i=1$ , then a $\hat{y}_i$ that is off by $1$ , or $\hat{y}_i=2$ , will contribute $1$ to the SSE, and also $1$ to the MAPE. However, if the true observation is $y_i=2$ , then a fit that is off by the same amount, $\hat{y}_i=3$ , will still contribute $1$ to the SSE, but only $\frac{1}{2}$ to the MAPE . The key observation here is that the percentage in the MAPE means that errors corresponding to larger actuals $y$ will get a lower weight in the error calculation. Consequently, our model fitting process will make less of an effort to fit the larger $y$ values, and rather concentrate on getting the smaller $y$ values right. Which is what we see in your plot. Here is a little simulation. Assume we sample some data, where the predictor is uniform on [0,1], and the outcome follows a quadratic relationship with the predictor, with some noise. Below, I'm plotting the simulated observations, together with the true relationship (the dashed black line), the fit that minimizes the SSE (the red line) and the fit that minimizes the MAPE (the green line). Note that because the MAPE prefers under-fitting , the entire green line is below the red line. As our model is correctly specified, the red line is unbiased, i.e., if we repeat the experiment many times, it will vary around the true relationship, the dashed line. The green line will, in contrast, always be biased low. And the bias will be stronger the smaller our observations are. R code: true_relationship
