[site]: crossvalidated
[post_id]: 535898
[parent_id]: 535853
[tags]: 
Your caution is warranted. Once you have used the outcome data to adjust how you formulate your model, the assumptions underlying p-values and confidence intervals have been violated.* The principles of how to proceed are explained and illustrated in Frank Harrell's course notes and book . Use a strategy that avoids those problems from the start. The best way to proceed is (1) to use a type of model that respects the expected error distribution in the outcome once the predictors have been taken into account, and (2) to model the predictors as flexibly as is consistent with the size of your data sample. Point 1. A standard ordinary least-squares (OLS) regression model is fine if, based on your understanding of the subject matter, the errors around the predictions will be homoskedastic. But if you already know that measurement errors in your strictly-positive outcome value tend to be proportional to the true value (often the case in practice), then modeling the log of the outcome or using a generalized linear model with a log link would be better. If you are modeling counts, then a Poisson generalized linear model will work better than OLS. Yes/no outcomes can be better handled with logistic regression. For continuous outcomes where you can't make assumptions about error distributions, ordinal regression provides a robust approach that doesn't depend on such assumptions; see Harrell's notes and book. Choosing the type of model appropriately first will tend to avoid surprises that come from starting blindly with OLS without regard to the nature of the outcome and its measurement. Point 2. The section of on Multivariable Modeling Strategies in Harrell's notes nicely addresses how to handle predictors. In outline: (a) Estimate how many degrees of freedom (df) you have to spend on your data without overfitting the size of your data set. For example, with a continuous outcome you might be able to spend 1 df for every 10 or so cases. (b) Choose which predictors to spend those df on. For categorical predictors, you spend 1 df per level beyond the reference. A continuous variable included as a linear predictor only spends 1 df. Linearity isn't a safe assumption, however, so spend more df for flexible fitting, for example with restricted cubic splines. That way the data themselves will tell you the shape of the association and you don't have to guess the form (and later correct it) on your own. (c) Run the model. You haven't cheated by looking at the outcomes first, so you have avoided the problems you fear. Harrell's course notes and book contain many examples of how to apply these principles in practice. *The types of erroneous modeling that you describe in the question don't necessarily lead to bias in the technical sense of the expected value of model predictions being systematically different from the value in the population. They do, however, wreak havoc with inference based on p-values and confidence intervals. Erroneous modeling is likely to greatly overstate the "statistical significance" of the findings.
