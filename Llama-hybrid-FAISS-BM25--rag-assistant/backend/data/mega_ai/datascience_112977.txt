[site]: datascience
[post_id]: 112977
[parent_id]: 
[tags]: 
Training a straight "copy the input to the output" autoencoder for audio is strangely slow

As a learning exercise, I'm training a "perfect" audio autoencoder. It has a hidden layer just as wide as the input layer, with linear activation. The expectation is that the network should learn to just copy the first input to the first output. I trained this network for 100 epochs with 3088 mini-batches of 128 examples each (consisting of 480 floating point inputs), using root mean squared error as the loss functions. However, learning seems to stop after the first few epochs and ultimately the accuracy doesn't go above ~0.4. When predicting the output for a full sample (by considering each 480 tick segment in series), the output sounds noisy and hissy. I tried different size data sets, different batch sizes, rmsprop vs adam and didn't see any significant difference. What's causing the ineffective training? Core model and training bit: def build_model(batch_size): input_shape = (batch_size, 480) input_layer = tf.keras.layers.Input(shape=input_shape) x = input_layer x = tf.keras.layers.Dense(480, activation='linear')(x) return keras.Model(inputs=input_layer, outputs=x) def root_mean_squared_error(y_true, y_pred): return K.sqrt(K.mean(K.square(y_pred - y_true))) model = build_model(train_sequence.batch_size) model.compile(optimizer="adam", loss=root_mean_squared_error, metrics=["accuracy"]) model.summary() model.fit(train_sequence, epochs=100, validation_data=validation_sequence) The training sequence is generated by segmenting the wav files from VCTK-Corpus into vectors of 480 float64 values. When segmenting, I use a stride of 240 to get a little overlap with different starting points. I normalise the audio to [-1.0, 1.0].
