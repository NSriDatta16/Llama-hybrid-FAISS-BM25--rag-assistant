[site]: crossvalidated
[post_id]: 91210
[parent_id]: 90458
[tags]: 
Measuring the quality of any predictive model is heavily dependent on the scope of the problem you are working on. Therefore it is very hard to generally say that one measure of goodness is better than another. However, I think the two questions you are working on would benefit from some cross validation. In regards to your first question, you can use the fact that random forests automatically give you an estimate of their out-of-bag error . This is essentially an unbiased estimate of how the random forest will perform on unseen data. Thus, instead of trying to pick between your two models based on their splitting quality (an indirect measure of prediction quality), you can pick the model that has the lowest out of bag prediction error. If you are not satisfied with the bootstrapped out of sample data that a random forest uses, you can hold out a portion of totally unseen data and run an additional validation test to choose between the two models. As a side note, once you have chosen the “best” model of the two, you do need a totally fresh testing set so can estimate the out-of-sample prediction error for the best member. For your second question, identifying which variable combination is optimal, again I would recommend using cross validation. Simply train all of your candidate models (different combos of variables) on the training data set. Then pick the model that performs the best on the out of bag error estimate or on your validation data set. That’s the best guess of which variable set is optimal. Then test the optimal combo on an untouched testing set to get an unbiased estimate of it’s out of sample error.
