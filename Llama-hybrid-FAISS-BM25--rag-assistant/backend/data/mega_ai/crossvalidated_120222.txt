[site]: crossvalidated
[post_id]: 120222
[parent_id]: 113370
[tags]: 
Being Markov is a property of the distribution , not the graph (although it is only defined relative to a given graph). A graph can't be Markov or fail to be Markov, but a distribution can fail to be Markov relative to a given graph. Here is an example in terms of causal networks. Say we know that $X_1$ influences $X_2$ and $X_3$, but $X_2$ and $X_3$ don't influence each other. Then we can represent the true causal relationships with the graph $G$: X_3"> The distribution $P(X_1, X_2, X_3)$ should represent that $X_2$ and $X_3$ become independent conditional on $X_1$, so the $P(X_1, X_2, X_3)$ is Markov relative to $G$. Now let's say we did not observe $X_1$, but only $X_2$ and $X_3$. Remember that neither of $X_2$ or $X_3$ influences the other. So we could represent the direct causal relationships between $X_2$ and $X_3$ using $G'$, the subgraph over those nodes: In this case, the marginal distribution $P(X_2, X_3)$ is not Markov with respect to $G'$. $X_2$ is dependent on its non-descendent $X_3$, conditional on its parents in $G'$ (i.e. the empty set). This is because $G'$ omits the common cause $X_1$. This example shows that the Markov condition is only reasonable when your graph includes all common causes of any pair of nodes in the graph (an assumption called "causal sufficiency"). Note that you can include a variable in the graph even if you did not observe it - $P(X_1, X_2, X_3)$ is Markov relative to $G$ even if I am missing data for $X_1$. Markov chains are similar, and often used to represent time series. The Markov assumption represents that the next state of the time series depends only on the current state, and not on any previous states. Here's an example representation:
