[site]: datascience
[post_id]: 128282
[parent_id]: 
[tags]: 
Why is my 3D CNN stuck at constant accuracy?

I am writing a CNN for binary classification MedMNIST data: https://medmnist.com/ , specifically the Lung Nodule 3D dataset (N=1633 and 7:1:2). Currently, my model is not training at all; it is either fixed at constant accuracy or randomly fluctuating from around 0.2 to 0.8. What I have attempted so far: Tweaking the learning rate and other hyperparameters. Changing the activation function between layers Changing the number of filters Changing the optimiser and loss function (currently I am using Adam and CrossEntropyLoss respectively) Introducing weight into the loss function, since the data is imbalanced by around 3:1. Various other small tweaks to the CNN architecture, like padding. I have stuck with the general architecture shown below: class Net(nn.Module): def __init__(self): super(Net,self).__init__() self.conv1 = nn.Conv3d(num_channels, 28, kernel_size=(3,3,3), padding=1) self.conv2 = nn.Conv3d(28, 56, kernel_size=(3,3,3), padding=1) self.dropout1 = nn.Dropout(0.5) self.fc1 = nn.Linear(153664, 64) self.fc2 = nn.Linear(64, num_classes) def forward(self, x): # Pass data through conv1 x = self.conv1(x) # Use the rectified-linear activation function over x x = F.relu(x) x = self.conv2(x) x = F.relu(x) x = F.max_pool3d(x, (2,2,2)) x = self.dropout1(x) x = torch.flatten(x, 1) x = self.fc1(x) x = F.relu(x) x = self.fc2(x) # Apply softmax to x output = F.log_softmax(x, dim=1) return output Any suggestions on how I could adapt this would be greatly appreciated. I'm at a loss because I'd kind of expect the model to at least learn something, with the aforementioned tweaks being used to squeeze out a final bit of optimisation.
