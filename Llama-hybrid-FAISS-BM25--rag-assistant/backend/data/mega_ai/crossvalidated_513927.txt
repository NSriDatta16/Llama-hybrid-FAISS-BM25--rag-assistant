[site]: crossvalidated
[post_id]: 513927
[parent_id]: 513911
[tags]: 
Significant eigenvalues You could use some sort of simulation to compute the probability for an eigenvalue exceeding a certain limit and base the selection on that. In the R-package psych , there is a function that does this (demonstrated below). If you assume samples from a Gaussian distributed population without correlation, then you should expect a screeplot with all points/eigenvalues on a weak slope. (I do not know a formula for this slope but you could compute it by simulations, either based on a distribution as done below or by some sort of bootstrapping) Applied to your plot If we apply this loosely your plot (I do not have the data but we can imagine how the line would be): Your 4-th eigenvalue is on this weak slope/line, and there is a clear distinction between the rubble and the steep hill. So probably the 4-th eigenvalue is not very meaningful (or the variance is just not significant/noticeable or properly scaled; a prerequisite for interpreting PCA eigenvalues is that any potential sources of variance will be of the same order of scale). The 4-th component is indeed part of the rubble. The 4-th eigenvalue is barely different from the 5-th eigenvalue. So any argument to include the 4-th should count as strong as including the 5-th. However, the significance is often not the issue The m vs. m-1 issue is more the situation when the scree plot is not so obvious. But, in that less clear case the question is not about whether or not a component has a (statistical) significantly high eigenvalue (more about that in the example below). But instead, about whether the effect size is large enough or dominant. The 'trick' in this case is to look for the most remarkable eigenvalues and then add 1 (Just to be sure. I guess. I do not understand this rule so well. But it isn't a very strict rule anyway.). Example from psych package and manual plot The R-code below generates the scree plots below for the example 7.4 from Harman . The plot on the left is created with a single function fa.Parallel from the psych package. The plot gives the scree plot along with a line that relates to eigenvalues of a simulation with Gaussian distributed data that has the identity matrix as covariance. The plot on the right is created manually. You can use the code (which I hope is intuitive enough) to figure out how it works. In this plot, I have used a slightly different measure for the eigenvalues. I have scaled the eigenvalues based on the average of all the lower eigenvalues. The reason for this is because due to the presence of higher eigenvalues the eigenvalue that is being considered will be relatively lower than the random Gaussian data, which does not have these higher eigenvalues, that are used for the comparison. The result is that many more points are above the line and seem to be significant. Isn't that a lot? Well, maybe not. The comparison is made with a model for data that is entirely spherical and variances are equal in all directions. But in practice is it not strange that data has some variations in variance/eigenvalues. Even if there is no structure, clustering, or other variances in-between-groups that cause an increase in variance, then one may still have that the noise is not the same for all directions. set.seed(1) psych::fa.parallel(Harman74.cor$cov, n.obs = 145, fa = "pc", main = "plot using psych package") ### compute eigenvalues for Harman74 data ### A correlation matrix of 24 psychological tests given ### to 145 seventh and eight-grade children in a Chicago ### suburb by Holzinger and Swineford ev $cov)$ values ### simulate normal distributed data ### and compute the eigen values sim_eigen Below is a simulation where we generate the data ourselves. Now we generate the data according to a Gaussian distribution. x The result is that you see the eigenvalues more closely within the simulated bandwidth. The vectors with eigenvalue 8, 2, 2 are picked out. The vector with eigenvalue 1.3 is too difficult. Summary The example above shows that you might get a nice clear scree plot like the last plot. But, this is only the case when all eigenvalues are the same except a few vectors/components (which you wish to detect and explore). In many practical situations, the assumption of equal eigenvalues/variance will be invalid in any case. The scree plot will not look perfect and there is no distinct border between rubble and a steep hill. The analysis of the scree plot in such cases is not about finding statistically significant eigenvalues. But instead, the scree plot is to see the distribution of importance/variance for the different components. Technically, all of the components can be important. The point of PCA is not to determine which ones are important, but it is to find some pragmatic cut-off value for the purpose of data reduction. The scree plot, if it is clear, can help you to categorize the different components, and determine a distinct group of large values.
