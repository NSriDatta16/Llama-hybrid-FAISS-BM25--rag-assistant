[site]: datascience
[post_id]: 10306
[parent_id]: 9880
[tags]: 
To understand the coefficients you just need to understand how the logistic regression model that you fit uses the coefficients to make predictions. No, it does not work like a decision tree. It's a linear model. Really, predictions are based on the dot product of the coefficients and the values from some new instance to predict. This is just the sum of their products. The higher the dot product, the more positive the prediction. So you can understand it as computing something like -2.40477246e-10 * MATCH_HOME + -5.57611571e-02 * MATCH_AWAY + ... (I don't know what coefficients go with what feature in your model.) That generally means that inputs with bigger coefficients matter more, and inputs with positive coefficients correlate positively with a positive prediction. That's most of what you can interpret here. The first of those conclusions is only really valid if inputs have been normalized to be on the same scale though. I'm not clear that you've done that here. You should also in general use L1 regularization if you intend to interpret the coefficients this way.
