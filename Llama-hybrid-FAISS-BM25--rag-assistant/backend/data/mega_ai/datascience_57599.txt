[site]: datascience
[post_id]: 57599
[parent_id]: 57586
[tags]: 
It is hard to say without trying it. But you should add features as much as you can . When you have so many features, according to your success metrics, you can do future selection / regularization for both increasing the accuracy and speeding up the convergence of gradient descent. Also you can also try (after adding more features) other classification algorithms such as RandomForest, GBM, Xgboost etc - Tree based models are more accurate in recent years. Hope this is helpful!
