[site]: crossvalidated
[post_id]: 557324
[parent_id]: 557323
[tags]: 
Since you have seen previous threads on thresholds, you may already have seen this thread on accuracy: Why is accuracy not the best measure for assessing classification models? Note that the exact same problems also apply to sensitivity and specificity. So I am already skeptical about using these inherently flawed measures in your step 1. Your component classifiers are not just calibrated differently , they are miscalibrated . If one classifier outputs a predicted probability of 0.8 for an instance to be of the target class, and the next one gives 0.6, and the third one gives 0.2, then something is off, because at least two of them have a very wrong idea of the true probability - no matter whether these numbers exceed the thresholds. What these classifiers give may be some KPI that is correlated with the true probability of target class membership, but it is quite obviously not a good estimate of it. Thus, I would say the first order of business would be to recalibrate your classifiers. Happily, this is quite simple. For each classifier, run a logistic regression for the target class on the classifier's predicted probabilities . (Use splines if necessary to account for nonlinearities.) The fitted outputs from these logistic regressions should be far better calibrated. I recommend you take a look at proper scoring rules for these recalibrated classifiers (and the miscalibrated ones, too), just to get an idea of how they perform. Once you have recalibrated classifiers, you can simply use the arithmetical average of the recalibrated predicted probabilities. And then you or your user can apply one or multiple thresholds based on costs as they see fit.
