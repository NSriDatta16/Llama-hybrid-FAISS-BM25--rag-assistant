[site]: datascience
[post_id]: 123442
[parent_id]: 
[tags]: 
Using auxiliary softmaxes to measure impact of each submodule on the final softmax classifier

I am attempting to assess the impact of various submodules (CNN 1D, CNN 2D, CNN 3D, FFNN) on the final classifier of the neural network that i am currently building. The neural network itself is complex, utilizing BERT word embeddings as inputs for four distinct submodules. Subsequently, I combine and channel the submodule outputs into an FFNN to reduce the latent space to a dense vector, where later a softmax is applied as classifier. (Maybe the late-stage FFN is not needed, i am currently training it but takes days) I am aware that there are more sophisticated and agnostic methods, such as Shapley Values in the realm of Explainable AI, to gauge feature influence or impact. However, in my specific problem, I believe it is impractical to compute these values, as the process would take an inordinate amount of time. Has anyone experimented with this technique before? Do you find it feasible or not? From a mathematical standpoint, comparing these values seems reasonable. Nonetheless, it's important to note that the overall loss function depends on the final softmax, rather than the auxiliary softmaxes. Consequently, there might be outliers that don't correlate with the final softmax's performance. The issue is that I haven't come across any research on this topic. Perhaps some of you have experience with this use case :) Best regards, Image: Neural Network
