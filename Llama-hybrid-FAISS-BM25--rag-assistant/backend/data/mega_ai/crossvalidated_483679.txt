[site]: crossvalidated
[post_id]: 483679
[parent_id]: 483533
[tags]: 
It's not . If it was 100% always necessary, it would be implemented as default in statistical packages or in the algorithm descriptions. In some cases scaling won't make much of a difference, in other may significantly affect the end results. E.g., for regularization (say lasso) we'd want the penalty to be consistent throughout different variables and penalize them equally in order to better gauge their explanatory information; without scaling we could under/over penalize certain variables, hence in most implementations the scaling is always done under the hood, with the option to disable it. Further reading for more detailed answers: Is standardization needed before fitting logistic regression? When conducting multiple regression, when should you center your predictor variables & when should you standardize them? Is it a good practice to always scale/normalize data for machine learning?
