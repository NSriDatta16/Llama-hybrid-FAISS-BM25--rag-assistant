[site]: crossvalidated
[post_id]: 532858
[parent_id]: 532796
[tags]: 
TL;DR: Yes, it does return biased coefficients. I believe this is incorrect, if we are using the word "bias" in the way that people talk about it with regard to statistical and machine learning. (EDIT: Actually it IS correct, just a little confusing, explained below in the edit. Leaving this here so others can track my thought process, since it is a little tricky—for me at least!) It gives biased regression coefficients that need shrinkage (the coefficients for remaining variables are too large; see Tibshirani [1996]). I think they are using "biased" in the colloquial sense of "not good." Doing stepwise regression for variable selection is going to harm your inference, as p-values are going to be a mess since you "cherry-picked" the best variables. (There's a whole bunch of literature on this on "post-selection inference" specifically with regards to variable selection methods like stepwise or the lasso). The problem with stepwise is that the coefficients are not biased—they will have high variance and will be sensitive to changing from different testing sets. In the long-run, like you find in your simulations, they will converge to the "true" coefficient and are thus unbiased. But in the real world, we don't get to do many many studies, we don't know the "true" coefficient, and our model will always be an oversimplification, never fully explaining the true data generating procedure. So we actually introduce bias. It's weird, because shrinkage methods are a way of introducing bias in an attempt to decrease variance. I wouldn't read too much into it—I really think they're just using the word "biased" in a colloquial sense instead of a statistical or machine learning sense. I re-read Tibshirani (1996), and stepwise methods aren't included in their simulations. (Apparently this is attributed to Frank Harrell originally, who is on this site, and, uh, exponentially smarter than I am, so I'll see if I can find anything there.) EDIT: OK, so I think I know where this argument comes from, after reading DOI: 10.1016/j.jclinepi.2011.06.016 and DOI: 10.1016/s0895-4356(99)00103-1. The problem here is that the p-values are used for selection, and multiple regression controls for other variables, but only the variables in the model. So, if we use these invalid p-values and standard errors to estimate the model, we are letting these define what variables are included in the model. So, we end up overestimating the coefficients (biased away from zero), because the estimation of that coefficient is dependent on it's p-value and standard error. That first reference I pointed to above (10.1016/j.jclinepi.2011.06.016) makes a pretty good analogy: An intuitive example includes publication bias, where trials with statistically significant results have a higher chance of being published, leading to too extreme effect estimates in summaries of published reports. And they do show simulation studies where stepwise biases the coefficients by estimating them to be too large. Think of it this way: Stepwise methods are only going to include the variables on the occasions when they suck up enough variance to be significant. This is going to bias the coefficients upwards, because, in the long-run, we aren't even considering the non-zero coefficients that do not happen to be significant. It is truncating the possible distribution of coefficients we could get, so it moves the mean away from zero. That's bias. I was confused, because often the goal is not to estimate unbiased coefficients. It's for good predictions. Indeed, we want to bias our coefficients toward zero (lasso) or toward the mean (mixed effects models) to decrease variance. ANOTHER EDIT: Since I wasn't happy with my flip-flopping, I did my own simulation: library(tidyverse) set.seed(1839) # set seed for randomization n % unlist() %>% summary() That should show you that the mean coefficient is .28, which is larger than .25. Min. 1st Qu. Median Mean 3rd Qu. Max. NA's -0.1947 0.2133 0.2699 0.2793 0.3357 0.7255 1435 The main thing you might be running into with your simulation: If the effect size is big enough (the power is high enough) that you just so happen to estimate the coefficient on every single iteration , then it will be unbiased—because there is no "selection bias," as they were all selected. What you need to do is make the sample size low enough or the effect weak enough so that the model does NOT estimate it every time, meaning there is a selection bias toward coefficients that are sufficiently away from zero so as to be significant. (Note that I have 1435 NA values where it was not estimated—this is where the selection bias comes from: We should be including those in our calculation of the mean coefficient, but we're not because forward stepwise introduced selection bias.) Note that my simulation is one coefficient of .25 and the rest of 0—all of them uncorrelated. Things might get funkier when there are correlated variables.
