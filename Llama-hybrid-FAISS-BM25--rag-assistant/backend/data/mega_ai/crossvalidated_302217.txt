[site]: crossvalidated
[post_id]: 302217
[parent_id]: 301842
[tags]: 
Thank you very much for your effort, @fnl ! Although I couldn't follow all of your points (probably because I'm quite a beginner), your answer gave me some clarity. Could you point out the problem again in terms of energy? I came across the following equation several times and one drawback of the ML method is that the denominator requires to sum over all possible combinations of visible and hidden states, which is exponentially complex, right? $$ \arg \max_\theta \log p(v | \theta) = \log \frac{\sum_{h} e^{-E(v,h)}}{\sum_{v', h'}e^{-E(v',h')}} = \log \sum_{h} e^{-E(v,h)} - \log {\sum_{v', h'}}e^{-E(v',h')} $$ However, I haven't completely understood, how the energy-related formulation fits into the whole problem. Also, I remember a quote (I believe by Hinton in one of his Coursera videos) which was like (in reference to the above equation) The gradient of the first sum is easy to comute, while the gradient of the second is not. It could be approximated when having samples, but even sampling is hard, so we also need to approximate them. So does the "first" approximation mean to repeatedly run the Markov chain and the "second" approximaton to perform Gibbs sampling?
