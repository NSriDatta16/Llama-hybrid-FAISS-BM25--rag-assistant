[site]: datascience
[post_id]: 103474
[parent_id]: 103471
[tags]: 
I suggest you to automate the optimal hyperparameters search with a standard hyperparametrization strategy, like bayesian search for instance; Keras offers you this option with Keras tuner as follows (example): from tensorflow import keras from kerastuner.tuners import BayesianOptimization n_input = 6 def build_model(hp): model = Sequential() model.add(LSTM(units=hp.Int('units',min_value=32, max_value=512, step=32), activation='relu', input_shape=(n_input, 1))) model.add(Dense(units=hp.Int('units',min_value=32, max_value=512, step=32), activation='relu')) model.add(Dense(1)) model.compile(loss='mse', metrics=['mse'], optimizer=keras.optimizers.Adam( hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4]))) return model bayesian_opt_tuner = BayesianOptimization( build_model, objective='mse', max_trials=3, executions_per_trial=1, directory=os.path.normpath('C:/keras_tuning'), project_name='kerastuner_bayesian_poc', overwrite=True) bayesian_opt_tuner.search(train_x, train_y,epochs=n_epochs, #validation_data=(X_test, y_test) validation_split=0.2,verbose=1) bayes_opt_model_best_model = bayesian_opt_tuner.get_best_models(num_models=1) model = bayes_opt_model_best_model[0] You can find a similar topic answered and validated here
