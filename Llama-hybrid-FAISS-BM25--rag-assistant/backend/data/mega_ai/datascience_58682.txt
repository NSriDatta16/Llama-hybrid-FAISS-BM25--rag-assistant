[site]: datascience
[post_id]: 58682
[parent_id]: 
[tags]: 
Max/Min of neurons at the hidden layer of a neural network

We have a neural network with inputs $x_1,...,x_n$ , a single hidden layer with neurons $y_1,\ldots,y_m$ , and outputs $z_1,...,z_v$ . There are no activation functions (i.e., the activation functions are $f(x)=x$ ). We know that input is between -1 and 1, and we need the outputs to be between -1 and 1 as well. The network has been trained so the weights are all available. Given the weights, find the maximum and minimum values that each $y_i$ can take. My effort: I thought of a simpler version of it. Let's say we have 2 inputs, 1 node at hidden layer, and one output. Then: $$y_1=w_1 x_1 + w_2 x_2 $$ $$z_1=w_1' y_1$$ The maximum for $y_1$ occurs when $x_1=\mathrm{sign}(w_1)$ and $x_2=\mathrm{sign}(w_2)$ . So, $y_1 \leq |w_1| + |w_2|$ . On the other hand, $-1 , so $y_1 . Thus, $y_1\leq \min\{|w_1|+|w_2|,1/|w_1'|\}$ . Similary, we can show that $y_1\geq - \min\{|w_1|+|w_2|,1/|w_1'|\}$ . Any idea for the general problem? Has this already been addressed in the literature?
