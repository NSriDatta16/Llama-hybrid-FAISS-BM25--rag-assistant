[site]: crossvalidated
[post_id]: 466099
[parent_id]: 464367
[tags]: 
In general there is no consensus about minimum / maximum variability. It is also discussed in this question . It depends on what you are trying to predict or regression against, and where the important predictor lies. As commented in Principal Component Analysis by Joliffe, section 8.2 : Deletion based solely on variance is an attractive and simple strategy, and Property A7 of Section 3.1 gives it, at first sight, an added respectability. However, low variance for a component does not necessarily imply that the corresponding component is unimportant in the regression model. For example, Kung and Sharif (1980) give an example from meteorology where, in a regression of monsoon onset dates on all of the (ten) PCs, the most important PCs for prediction are, in decreasing order of importance, the eighth, second and tenth If you take too many PCs, you might end up including what are basically noise variables and your model might not be so good. One thing you can do (also suggested for regression in Joliffe) is to use cross-validation to see how increasing or decreasing the PCs affect the model. You can work with some k value for kNN and see whether it does get less accurate with more PCs. There's also a recent review on applications of PCA and might be useful for you
