[site]: crossvalidated
[post_id]: 631865
[parent_id]: 631367
[tags]: 
For a start, you these packages (whether we're talking xgboost or lightgbm or whatever) are almost certainly working with non-softmaxed logits. You can see that by e.g. getting them for a trained multi-class model (here for the python lightgbm package) via model.predict(mydata, raw_score=True) . That also illustrates for you what happens under the hood: there's one tree per category that outputs a number that acts as the non-softmaxed logit for a category. You should be able to access those and apply whatever loss function you want to these, it's just a matter of finding the right bit of documentation. However, if you want to work with an ordinal loss-function, you also have another option you work with a single continuous output $\hat{\theta}$ (i.e. just one tree per iteration instead of one-per-category, which is the default for unordered classes). You'd assume an underlying $N(\hat{\theta}, 1)$ variable (for which you predict the mean) and define cut-points $c_\text{Low-Medium}$ and $c_\text{Medium-High}$ that indicate the boundaries between the categories. You then define that $P(\text{Low}|\hat{\theta}) = P(N(\hat{\theta}, 1) , $P(\text{Medium}|\hat{\theta}) = P(N(\hat{\theta}, 1) \in [c_\text{Low-Medium}, c_\text{Medium-High})$ , and $P(\text{High}|\hat{\theta}) = P(N(\hat{\theta}, 1)\geq c_\text{Medium-High})$ . You can even fix one of the boundaries, e.g. $c_\text{Low-Medium}:=0$ , while the other one would be a hyperparameter to tune. You may or may not find this easier to implement (perhaps at the cost of having additional hyperparameters to tune).
