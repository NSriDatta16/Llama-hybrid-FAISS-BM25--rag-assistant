[site]: crossvalidated
[post_id]: 360224
[parent_id]: 360148
[tags]: 
This question might well be closed as "opinion-based". So I'll give you my opinion. In my opinion, simulated data are always problematic, because you always get out what you put in. If you can't imagine a data generating process having a certain peculiarity, then the data you simulate won't exhibit it - because you won't even think of simulating it. This is a good case of an "unknown unknown" . When you do start seeing such unthought-of features, they will be a Black Swan to you. GANs can in principle simulate data extremely well, so this is certainly better than simulating your own data. Of course, they are still limited to what they saw when they were trained, or small extrapolations. No European GAN pre-18th century would have generated a black swan. Plus, if you don't have enough data for your actual use case, one could ask whether the amount you have was enough to train a GAN. Bottom line: it's always better to use actual data. You could ask at OpenData.SE , or consider using similar data to the kind you are actually interested in. I see that the paper you are looking at actually forecasts electricity loads, although you want to apply the models to CPU loads - if you are good with using electricity data, there should be tons of datasets out there, e.g. at Kaggle or in the GEFCom competitions organized by Tao Hong . If there simply is not enough data and you are pressed for time, then yes, by all means, do simulate your own. GANs are certainly better than simulating your own data. But if so, include a nice, big section on limitations of your study.
