[site]: crossvalidated
[post_id]: 495389
[parent_id]: 456038
[tags]: 
It is basically OK to use the criteria from PCA as part of a guide to selecting the number of factors. Most of the times, FA and PCA results will be in agreement. Parallel analysis and Velicer's minimum average partial (MAP) are the most reliable and accurate techniques to assess the number of components or factors to retain, according to Zwick & Velicer. 1 The fact that we use PCA instead of FA is motivated by historical reasons, and was more or less disputed in the last 20 years. Most research has focused on PCA but Velicer and coll. discussed some of the adaptations that were developed for principal axes factor analysis, which is widely used in factor analysis, and they concluded that "no satisfactory alternatives are available within factor analysis." 2 Sidenote: Horn's parallel analysis aims to provide an idea of the distribution of eigenvalues on randomly perturbed observations, from the same dataset, hence it gives an idea of the "residual" variance in your data, i.e. what would be expected if there were no signal in your dataset. Despite being rarely used in practice, this resampling-based approach is certainly a better criterion than Kayser's rule (eigenvalues > 1). Other aspects of parallel analysis are discussed in this related thread: How to correctly interpret a parallel analysis in exploratory factor analysis? . Some statistical packages, like paran available in R and Stata , even offer a way to correct for bias arising from colinearity due to sampling error (which, of course, is not accounted for when using Kayser's rule), whereby eigenvalues of PCA components might be greater than and less than 1, and other subtleties, like how to generate random samples (e.g., standardized values, using same rank for the simulated dataset as the observed response matrix, or drawing random variates from a distribution close to the empirical one). 3 Many of Velicer's papers and recent publications on parallel analysis contrast PCA and FA approaches in determining the number of factors to retain, so you will likely find additional information on how un-rotated PCA and FA models are able to recover the proper factor structure of a given dataset. Replying to @RichardDiSalvo, I believe the direction (marginal increase or decrease of explained variance, using your words) is not that relevant since in both cases you started with one factor (or component), then add more, and see how the residual variance is accounted for by any of those models. This applies both from a theoretical (start with the minimalistic model, then add complexity) and a computational (this is an iterative procedure where we start with the null model, and add parameters one after the other) point of view. The uniqueness (1 - communality) is specific to EFA, and ideally should be identical in all factor solutions --- that is, all items should have close loadings on their respective factors. That's the basis of most of classical test theory (tau-equivalent and congeneric measures are examples of models where we relax this assumption). References 1 Zwick, W.R. and Velicer, W.F. (1986). Comparison of five rules for determining the number of components to retain. Psychological Bulletin , 99 , 432–442. 2 Velicer, W. F., Eaton, C. A., & Fava, J. L. Construct explication through factor or component analysis: A review and evaluation of alternative procedures for determining the number of factors or components. In R. D. Goffen & E. Helms (Eds.), Problems and Solutions in Human Assessment--Honoring Douglas N. Jackson at Seventy (pp. 41–71) (Springer, 2000). 3 Liu, O.L. and Rijmen, F. (2008). A modified procedure for parallel analysis of ordered categorical data. Behavior Research Methods , 40 (2) , 556-562.
