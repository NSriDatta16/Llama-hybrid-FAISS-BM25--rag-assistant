[site]: crossvalidated
[post_id]: 614244
[parent_id]: 
[tags]: 
XGBoost: universal approximator?

There are various "universal approximation theorems" for neural networks, perhaps the most famous of which is the 1989 variant by George Cybenko. Setting aside technical conditions, the universal approximation theorems say that any "decent" function can be approximated as close as is desired by a sufficiently large neural network. $^{\dagger}$ Similar results exist for other classes of functions. For instance, the Stone-Weierstrass theorem says that decent functions can be approximated by polynomials as well as is desired (again, setting aside the technical details of what constitutes a decent function). Carleson's theorem has this same flavor for approximation by Fourier series. Does XGBoost also have a sense in which it can be a universal approximator? $^{\dagger}$ What constitutes a "decent" function is left vague as a technical detail of the specific assumptions of the various theorems.
