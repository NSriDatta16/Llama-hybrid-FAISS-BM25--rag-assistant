[site]: datascience
[post_id]: 94920
[parent_id]: 94910
[tags]: 
In case you want some state-of-the-art technique, you can implement a neural machine translation model with attention , as fully described in this course with Google members libraries like Trax. In case of short texts, you can still use sequence-to-sequence models without attention, but the attention mechanism prevents you from suffering the vanishing gradient problem on longer sentences. As a very brief schema of the type of model, find the image below (from the same given source):
