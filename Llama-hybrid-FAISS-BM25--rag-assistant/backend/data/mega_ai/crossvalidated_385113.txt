[site]: crossvalidated
[post_id]: 385113
[parent_id]: 
[tags]: 
Applying boosting to predictions from a Random Forest

I have a class of datasets for a binary classification problem where it is known that Random Forest performs poorly compared with GBM or FFNN, rarely adding anything to an ensemble. I've had an idea to use a best performing RF model as the first stage in a GBM. So the initial probability of 'TRUE' would be given by the RF, but subsequent incremental boosting models would be produced according to the standard GBM approach. A priori, does this seem sensible? I am working in R, using H2O to produce both models. My instinct is to get my first stage RF model from the H2O function, and build the GBM myself, running it in base R. Is there a better way to do this?
