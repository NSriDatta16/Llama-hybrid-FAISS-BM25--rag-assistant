[site]: crossvalidated
[post_id]: 336755
[parent_id]: 336743
[tags]: 
The L2 normalization takes an input vector and converts it to a unit vector. So each MNIST image is interpreted as a 784 dimensional vector, and is projected onto a point on the 784-dimensional sphere. L2 normalization can be useful when you want to force learned embeddings to lie on a sphere or something like that, but I'm not sure this function is intended for use in a data preprocessing scenario like you describe. The function, using the default axis, normalizes each data-point separately, in contrast to most scenarios where you use the max/min of the entire training set to perform data normalization. It's prone to losing information about the scale of inputs -- you can no longer tell the datapoint (3,5) apart from the datapoint (6,10). If used on the 0th axis, the normalization would change depending on how many datapoints you had, which doesn't make much sense to me.
