[site]: datascience
[post_id]: 107466
[parent_id]: 107462
[tags]: 
This sentence representation works well for some tasks (e.g. document classification, topic modeling) and poorly for others (e.g. sequence modeling). The reason it works well for, say, document classification is because distance is semantically meaningful in the embedding space. If the embeddings are trained well, then a word vector's location in the embedding space (roughly) corresponds to that word's meaning. If you take the average of all word vectors in a sentence, the result is a vector which represents the average meaning of the sentence. For example, the sentence "I have dogs because I like dogs" will probably have an average embedding near the word "dog". And the sentence "I have cats because I like cats" will have an average embedding closer to "cat". The average embedding gives you a decent idea of the sentence's topic.
