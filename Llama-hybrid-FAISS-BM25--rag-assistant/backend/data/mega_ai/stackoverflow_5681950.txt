[site]: stackoverflow
[post_id]: 5681950
[parent_id]: 5664987
[tags]: 
I just want to reiterate talonmies and say that there is no real limit to the "size" of a kernel in number of operations. As long as the computation is parallel, CUDA will be effective! As far a practical considerations, I would just add a few small notes long running kernels can timeout, depending on os (or when profiling with cudaProf). You might have to change a setting somewhere to increase maximum kernel execution time. long running kernels on systems without a dedicated gpu can freeze the display (interrupting ui). warps are executed asynchronously - one warp can access memory while another performs arithmetic in order to use clock cycles effectively. long running kernels might benefit more from attention to this kind of optimization. i'm not really sure about this last one.
