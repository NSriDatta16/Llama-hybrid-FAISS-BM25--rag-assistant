[site]: crossvalidated
[post_id]: 333680
[parent_id]: 
[tags]: 
Logistic regression and classification: Adjusting or removing decision boundaries

I'm taking Andrew Ng's Machine Learning Course. In the lesson on classification algorithms, he presents the logit function ($\frac{1}{1+e^{-x}}$) and the way it converts parameterized functions to probabilities. He marks the "decision boundary" at .5, noting that the logit function cross .5 at $x=0$. If the value of the logistic regression function is .5 or greater, the result is given as 1. If it's less than .5, it's given as 0. This sounds arbitrary. In real-world applications, is it common to adjust the decision boundary, or disregard the boundary and list only the probabilities? Scenario 1 Adjusting the decision boundary: In the logit function, when x is -.3, y is .4256. Because y is less than .5, the predicted value is 0. If the lump in my patient's breast has a 42.56% probability of being malignant, I don't want that rounded down to 0%. I would adjust the decision boundary from .5 to maybe .01. 1% or less probability is the threshold where I'd tell my patient their lump is nothing to worry about. Scenario 2 Removing the decision boundary: My team of social workers helps teens in my city. My classification parameters predict that 15% of the teen population is at high risk of committing a crime. That is, it predicts a positive result for 15% of P. My team is small, however, and can only reach 3% of the teen population. Knowing that the 15% prediction includes teens at 51% probability along with teens at 99% probability, I disregard the binary predictions entirely and order teens based on their probability, thus reaching the most at-risk teens first. Are these scenarios—where you adjust decision boundaries and/or list probabilities alone—common in real-world usage?
