[site]: crossvalidated
[post_id]: 72949
[parent_id]: 72935
[tags]: 
If you have two estimators, call them $\hat{\beta}_1$ and $\hat{\beta}_2$, of the same parameter $\beta$, then you can combine them in a variety of ways. Let's suppose you know that the two estimators are consistent and asymptotically normal --- this is generally true of estimators you get from maximum likelihood methods, method of moments methods, and some other methods as well. Furthermore, suppose you know the (asymptotic) variances of the two estimators, $V(\hat{\beta}_1)$ and $V(\hat{\beta}_2)$ and the covariance of the two estimators $Cov(\hat{\beta}_1,\hat{\beta}_2)$. You propose a combined estimator of $\beta$ given by $\frac{1}{2}\hat{\beta}_1+\frac{1}{2}\hat{\beta}_2$. This is consistent and asymptotically normal if $\hat{\beta}_1$ and $\hat{\beta}_2$ are. What is its variance? \begin{align} V(\frac{1}{2}\hat{\beta}_1+\frac{1}{2}\hat{\beta}_2) &= \frac{1}{4}V(\hat{\beta}_1) +\frac{1}{4}V(\hat{\beta}_2) +\frac{1}{2}Cov(\hat{\beta}_1,\hat{\beta}_2) \end{align} This estimator might be better (lower variance) or worse than either $\hat{\beta}_1$ or $\hat{\beta}_2$. If, say, $\hat{\beta}_1$ has a crazy-high variance, then the variance of $\frac{1}{2}\hat{\beta}_1+\frac{1}{2}\hat{\beta}_2$ might be higher than the variance of $\hat{\beta}_2$. We would like to avoid this. Also, we would like to find the best (i.e. lowest variance) way of combining the two estimators while preserving consistency. That is, we want to solve: \begin{align} &min_{\lambda}V(\lambda\hat{\beta}_1+(1-\lambda)\hat{\beta}_2)\\ &min_{\lambda}\lambda^2V(\hat{\beta}_1)+(1-\lambda)^2V(\hat{\beta}_2) +2\lambda(1-\lambda)Cov(\hat{\beta}_1,\hat{\beta}_2) \end{align} The first order condition is: \begin{align} 2\lambda V(\hat{\beta}_1)-2(1-\lambda)V(\hat{\beta}_2) +2(1-2\lambda)Cov(\hat{\beta}_1,\hat{\beta}_2)&=0 \\ \frac{V(\hat{\beta}_2)-Cov(\hat{\beta}_1,\hat{\beta}_2)}{V(\hat{\beta}_1)+V(\hat{\beta}_2)-2Cov(\hat{\beta}_1,\hat{\beta}_2)} &= \lambda \end{align} The combined estimator is then: \begin{align} \hat{\beta}_* = \frac{V(\hat{\beta}_2)-Cov(\hat{\beta}_1,\hat{\beta}_2)}{V(\hat{\beta}_1)+V(\hat{\beta}_2)-2Cov(\hat{\beta}_1,\hat{\beta}_2)} \hat{\beta}_1 + \frac{V(\hat{\beta}_1)-Cov(\hat{\beta}_1,\hat{\beta}_2)}{V(\hat{\beta}_1)+V(\hat{\beta}_2)-2Cov(\hat{\beta}_1,\hat{\beta}_2)} \hat{\beta}_2 \end{align} Because of the way we set up $\lambda$ in the minimization, you are assured that this new estimator is consistent. Notice, if the covariance between the two estimators is zero, then this new estimator is just the weighted sum of the two original estimators where the original estimators are weighted inversely to their variance---the new estimator "pays attention" to the old estimators inversely according to their variances. Finally, the variance of the new estimator is: \begin{align} V(\hat{\beta}_*) = \frac{V(\hat{\beta}_1)V(\hat{\beta}_2)-2Cov^2(\hat{\beta}_1,\hat{\beta}_2)}{V(\hat{\beta}_1)+V(\hat{\beta}_2)-2Cov(\hat{\beta}_1,\hat{\beta}_2)} \end{align} This process of optimally combining multiple estimators is called "minimum distance estimation" in econometrics. A cite is chapter 13 in Greene, Econometric Analysis, seventh ed .
