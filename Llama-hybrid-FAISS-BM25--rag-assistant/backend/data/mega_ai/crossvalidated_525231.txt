[site]: crossvalidated
[post_id]: 525231
[parent_id]: 524781
[tags]: 
The first part of your conclusion is correct. Sklearn computes probabilities in the RFC by classifying subsamples and then averaging the proportion of observations in each class. This will bias probabilities that should be near 0 and 1 inward. This is simply because for a class to have a probability near 0 or 1 we need that many of the subsample classifiers also tend to classify all or none of the observations within that class. You should read this paper that goes into some more details and compares different methods for retrieving probabilities: https://www.cs.cornell.edu/~alexn/papers/calibration.icml05.crc.rev3.pdf For the second part of your question, how much you should care about this all depends on what you are doing next, why you need these probabilities, and what the whole model looks like. In general, it's not a great idea to used biased estimates of a conditional probability in a second stage. If you are considering trimming along a cutoff this can also be problematic as this shifts the observed distribution of the probabilities.
