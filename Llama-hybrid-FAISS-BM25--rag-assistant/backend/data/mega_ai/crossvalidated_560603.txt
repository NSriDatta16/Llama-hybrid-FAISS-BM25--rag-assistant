[site]: crossvalidated
[post_id]: 560603
[parent_id]: 
[tags]: 
SVM from Scratch in R

I am attempting to build a linear SVM from scratch in R using gradient descent. I am nearly there, but I am running into an issue. I know that my true outcome contains 50 of 1 class and 50 of the second class. However, after making the calculations, my code is resulting in 46 in one class and 54 in the other class. For comparison of results, I am using the package e1071 . I am not sure why this is happening and it may be an issue with one of my formulas or code. If anyone can assist me, that would be very helpful. So, the cost function I am using is: $$ \lambda\|w\|^{2}+\left[\frac{1}{n} \sum_{i=1}^{n} \max \left(0,1-y_{i}\left(w^{T} x_{i}-b\right)\right)\right] $$ And the gradient with respect to the parameters are: if $y_{i} \cdot f(x) \geq 1:$ $$\frac{d J_{i}}{d w_{k}}=2 \lambda w_{k}$$ $$\frac{d J_{i}}{d b}=0$$ else: $$ \frac{d J_{i}}{d w_{k}}=2 \lambda w_{k}-y_{i} \cdot x_{i} $$ $$ \frac{d J_{i}}{d b}=y_{i} $$ Where: $$ f(x) = (w^{T} x_{i}-b)$$ Lastly, my update rules are as follows: $$ w = w - \alpha \cdot dw $$ $$ b = b - \alpha \cdot db $$ The code I am using to make the data set: #removing one of the classes df The code for the SVM algorithm using gradient descent: #specifying the learning rate Learning_Rate = 1){ w[k] Then, the prediction function: predSVM This ultimately results in 4 misclassifications with group -1 having 46 and group +1 having 54. One last note (an update). When I change the update rule to $$w = w + \alpha \cdot dw$$ In the event of a misclassification, rather than $$w = w - \alpha \cdot dw$$ I get the correct answer after 5000 iterations. I am not sure if this is correct. Does the update rule change in the event of a misclassification? else{ w[k] Update: I implemented the PEGASOS algorithm found in: https://home.ttic.edu/~nati/Publications/PegasosMPB.pdf . I also get the same result as my first attempt, with group -1 having 46 and group +1 having 54. This is from a published source and I believe I have implemented it correctly. The code is below: #specifying the learning rate Learning_Rate
