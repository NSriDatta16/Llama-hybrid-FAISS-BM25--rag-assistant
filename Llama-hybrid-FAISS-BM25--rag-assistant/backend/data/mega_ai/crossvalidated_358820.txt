[site]: crossvalidated
[post_id]: 358820
[parent_id]: 349654
[tags]: 
The comments produce a good exchange, which goes most of the way to an answer. The core idea is that standard tree-based models like random forest don't do any interpolation. The predictions of a random forest regression are piecewise linear because each leaf predicts a constant value. The predictions of a random forest classifier are "votes" for one class or another by each tree, for each sample. If you average the effect of all of these trees, you end up with some "gradations" of performance in the feature space. For example, a plot of random forest predictions to identify points inside the unit circle (including some noise features) looks like this : We can see that the model is a little bit less confident near the edges of the square, and the effect of having 2 signal features and several noise features is that the axis-aligned "boxes" lead to erroneous predictions. These are all related to the averaged effects of binary predictions on noisy data.
