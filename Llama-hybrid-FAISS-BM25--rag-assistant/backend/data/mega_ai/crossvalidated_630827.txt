[site]: crossvalidated
[post_id]: 630827
[parent_id]: 630812
[tags]: 
Orthogonalization of predictor variables can be used to handle multicollinearity in linear regression models, but it should be done with an understanding of its implications on interpretation. Orthogonalization transforms the predictor variables such that they become uncorrelated with each other. However, this process does not necessarily remove all correlations with the dependent variable y, as seen in your example. Here's a closer look at your questions: Should I orthogonalize highly correlated variables before including them in the model? Orthogonalization is not always necessary or desirable. While it can help with multicollinearity, it also makes the interpretation of individual coefficients more challenging because the transformed predictors do not have the same intuitive meaning as the original variables. If yes, what is the correct method to do it? If not, what is the alternative if I wish to include all of the correlated variables? To properly orthogonalize your variables, you might consider using Principal Component Analysis (PCA), which is a common technique for this purpose. PCA finds the orthogonal axes (principal components) that maximize the variance of the data. Each principal component is a linear combination of the original predictor variables and is uncorrelated with the other components. You can then use these components as predictors in your regression model. However, orthogonalization of the variables is a byproduct of this process. PCA can indeed help with the issue of high parameter variances that arise due to multicollinearity. When predictor variables are highly correlated, small changes in the data can lead to large changes in the parameter estimates, which is indicative of high variance. Since PCA creates new uncorrelated variables, the coefficients of the principal components tend to have lower variances compared to the original correlated variables. The first few principal components capture the majority of the variation in the data, and typically, the estimates associated with these components will have lower variances. However, the later principal components explain less variation in the data and are often associated with noise. If these later components are included in the model, their estimates may have higher variance and could be less stable. When using PCA for regression without dimensionality reduction (i.e., keeping all PCs), you transfer the variance from the original correlated variables to the coefficients of the principal components. The total amount of variance explained by the model remains the same, but the distribution of variance across the parameters is more balanced. The first few PCs will usually have lower-variance estimates, while the later PCs might have higher-variance estimates due to them representing less information about the response variable. If you do PCA for dimensionality reduction (i.e., discarding some PCs), you would be trading off some of the variance in your model for a more parsimonious model. By keeping only the first few PCs, you're focusing on the variables with the most significant impact on your dependent variable and simplifying the model, which can lead to lower variance in the parameter estimates of those components. However, if you still want to include the original variables in your model despite their correlation, there are several options: Regularization: Techniques like ridge regression or lasso can handle multicollinearity by penalizing the size of coefficients. Partial Least Squares Regression (PLSR): This approach is similar to PCA but also considers the dependent variable, seeking to maximize the covariance between the predictors and the dependent variable. Use of hierarchical or mixed models: If your correlated predictors are nested or grouped (e.g., time within subjects), hierarchical models can account for this structure. Remember, the choice of method depends on your specific goals, especially whether interpretability or prediction accuracy is more important for your study.
