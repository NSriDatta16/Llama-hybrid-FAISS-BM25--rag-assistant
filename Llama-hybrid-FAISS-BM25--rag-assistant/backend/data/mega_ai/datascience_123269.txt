[site]: datascience
[post_id]: 123269
[parent_id]: 123268
[tags]: 
A good way to go here might be to use the auto-diff tools developed for neural networks. I tend to use JAX lately, so JAX+optax will do the optimization provided you can write the likelihood in a useable form. As it stands, I am not quite sure what your integral means. Your measure is $N$ , does it mean that the integral is $$ \int p^d \cdot\left(1-p\right)^{n-d} dN= \int_0^1 N^d \cdot\left(1-N\right)^{n-d}dN=B\left(d+1,\,n-d+1\right) $$ where $B$ is the Beta-function ? If this is the case, it is not clear how you would want to proceed, since $R^2$ is no longer affecting the likelihood. If it happens that integral is more complex then you can either approximate the integral with something like trapezium rule and still use JAX+optax, or you can use some Bayesian libraries, like numpyro, to sample the density under the integral, and then marginalize that. If you only have two/three params, e.g. $c$ , $\psi$ , $R^2$ you might be able to simply brute-force that (evaluate likelihood at all possible combinations of these params, and pick the largest one - numpy.meshgrid will then suffice). Addendum here is a snippet that shows how to package integral into a differentiable function in JAX import jax import jax.numpy as jnp import jax.scipy as jsp import jax.lax as jl psi_arr = jnp.linspace(-100, 100, 2000) f = lambda a, b, n, d, psi: jnp.exp(-psi**2/2)*(jsp.special.erf(a+b*psi)**d)*((1-(jsp.special.erf(a+b*psi))**(n-d))) int_f = lambda p, n, d: jnp.trapz(y=f(p['a'], p['b'], n, d, psi_arr)) params = {'a': 5.0, 'b': 1.0} jax.value_and_grad(lambda p: int_f(p, n=5, d=1))(params) Output is: (Array(0.00360544, dtype=float64), {'a': Array(-0.01209422, dtype=float64, weak_type=True), 'b': Array(0.03792493, dtype=float64, weak_type=True)}) Because it is differentiable you can then use optimisers such as optax.adam to optimize for a and b
