[site]: crossvalidated
[post_id]: 630011
[parent_id]: 630007
[tags]: 
It sounds like you are asking specifically about conducting a two-sample test with a continuous endpoint - what would traditionally be taught as a "T-test". So can we test the same hypothesis as a T-test with linear regression? Even better, the T-test is linear regression. The standard OLS model is written: $$ Y = \alpha + \beta X + \epsilon $$ Where $\alpha$ is an intercept, $X$ is a covariate of interest, $\beta$ is a slope and $\epsilon$ \is a random error, $Y$ is the response. If $X$ is taken to be a 0/1 indicator, or a "dummy" variable for group membership for the $Y$ vector. Then the "slope" $\beta$ is a difference of means between the two groups. In other words, the hypothesis $\mathcal{H}_0 : \mu_1 - \mu_0$ is the same as $\mathcal{H}_0: \beta = 0 $ . The below visual aid helps to understand. Using R, with(sleep, abline(extra ~ as.numeric(group))) with(sleep, abline(lm(extra ~ as.numeric(group)))) The rise of the line of best fit over a unit length estimates the difference of means. Under an equal variance assumption identical inference is obtained (a classical OLS assumption) > t.test(extra ~ group, data=sleep, var.equal=TRUE) Two Sample t-test data: extra by group t = -1.8608, df = 18, p-value = 0.07919 alternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0 95 percent confidence interval: -3.363874 0.203874 sample estimates: mean in group 1 mean in group 2 0.75 2.33 compared to the linear model result > summary(lm(extra ~ group, data=sleep)) Call: lm(formula = extra ~ group, data = sleep) Residuals: Min 1Q Median 3Q Max -2.430 -1.305 -0.580 1.455 3.170 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 0.7500 0.6004 1.249 0.2276 group2 1.5800 0.8491 1.861 0.0792 . --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 1.899 on 18 degrees of freedom Multiple R-squared: 0.1613, Adjusted R-squared: 0.1147 F-statistic: 3.463 on 1 and 18 DF, p-value: 0.07919 There are a remarkable number of generalizations here where they are approximately (meaning very closely) the same as other regression findings. For instance, the unequal variance assumption typically uses Welch's Degrees of Freedom with Satterthwaite correction - this is merely a solution (not the solution ) to the Fisher Behren's problem. You can also use Robust Sandwich Error estimation in the linear model to account for unequal variance (yet another solution). For a categorical covariate, the Chi-square test of heterogeneity is the score test of a logistic regression with a bivariate regressor. For time to event, the log rank test is the score test of the Cox proportional hazards model. Using regression for these simple problems is entirely cogent.
