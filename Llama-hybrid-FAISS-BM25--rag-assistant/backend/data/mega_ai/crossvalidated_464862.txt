[site]: crossvalidated
[post_id]: 464862
[parent_id]: 464836
[tags]: 
As a recap, what $k$ -NN algorithm does when making prediction for the $\mathbf{x}_i, y_i$ sample is: for all the $\mathbf{x}_j$ training samples, calculate the distance between them and the target sample $d(\mathbf{x}_j, \mathbf{x}_i)$ , take $k$ training samples within the closest distance to $\mathbf{x}_i$ , let's call this subset $\mathcal{K}$ , calculate the prediction by using some aggregate statistic, e.g. mean $\hat y_i = \tfrac{1}{k} \sum_{j \in \mathcal{K}} y_j$ . If this is a regression problem, you'd take $\hat y_i$ as is, for classification if the labels are zeros and ones, this mean would be a rough approximation of the probability of belonging to target class, so for hard classifications you'd take some cut-off value to make the classification decision. Notice that this algorithm can make the decisions for each of the samples based on different variables. During preprocessing data for $k$ -NN we usually normalize the data, so that they all have similar scales, so that any of the variables would not dominate the distance metric, but they would vary on case-by-case basis. So by itself, this algorithm would not tell you anything about the "overall" influence of the individual variables. It simply does not learn any "general patterns" from the data. What $k$ -NN would give you is the "customers who bought this item also bought" kind of answer, by pointing to other samples that were similar to the $\mathbf{x}_i$ sample. To do this, you do not need to make classifications, just sort the samples by distance to $\mathbf{x}_i$ . Of course, you could use one of the algorithms for machine learning interpretability (see e.g. the Interpretable Machine Learning book by Christoph Molnar ), as for any other algorithm, but I'm not sure if this would make much sense in case of $k$ -NN for the reasons stated above.
