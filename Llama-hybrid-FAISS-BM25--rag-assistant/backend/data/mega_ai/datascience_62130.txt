[site]: datascience
[post_id]: 62130
[parent_id]: 62127
[tags]: 
The fact that some variables may be correlated among each other cannot be the source of your unsactisfactory results. Think about it: let's say that I'm predicting the wheather, having as variables v1 the number of fingers in my hand v2 in my foot (highly correlated variables, right?). Removing v2 from the model may help a little bit because my model won't consider a noisy variable to predict the wheather. If you are asking about colinearity, I would say that it's better to remove correlated variables, but it's not mandatory . If you are looking for better results in your time series predictor, you may want to understand how forecasting models, such as ARIMA, works in-depth. Often, this kind of models uses the past (with time series "lags", i.e. previous observations) to predict the future. Try to model lag variables (also referred to autoregressive components), moving averages (trends) and seasonality components. Although they can be very correlated between each other (look for ACF plots), they are also suitable variables for forecasting models.
