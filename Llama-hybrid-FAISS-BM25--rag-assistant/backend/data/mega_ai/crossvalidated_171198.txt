[site]: crossvalidated
[post_id]: 171198
[parent_id]: 171179
[tags]: 
I suggest a boosted multinomial logit modeling strategy as implemented in the mboost package in R, and possibly in other libraries or packages in R and other platforms. This method iteratively fits a set of base-learner generalized linear models that the user specified by the set of predictors to be included. Base learners are usually specified as very simple models with one or two effects, but can include as many effects as the user desires. The base learner that leads to the smallest negative loss is chosen in each iteration to be incorporated into the averaged model. At the end, you are awarded both the average effects of each parameter as well as the probability that each base learners was selected in a given iteration, which you could consider a sort of base-learner importance score. The mboost package allows you to specify smoothed effects in a base learner. Another benefit of this package is that the boosted GLM method is resistant to multicollinearity or separation issues, as well as the curse of dimensionality. Moreover, there is only one parameter to tune: the number of iterations. You can do so using bootstrap or k-fold cross-validation methods for estimating the generalization error. I believe that mboost and the packages that depend on it are among the most useful R packages that exist.
