y signed out of concern about risks such as AI-generated propaganda. The authors of one of the papers cited in FLI's letter, "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?" including Emily M. Bender, Timnit Gebru, and Margaret Mitchell, criticised the letter. Mitchell said that “by treating a lot of questionable ideas as a given, the letter asserts a set of priorities and a narrative on AI that benefits the supporters of FLI. Ignoring active harms right now is a privilege that some of us don’t have.” Operations Advocacy FLI has actively contributed to policymaking on AI. In October 2023, for example, U.S. Senate majority leader Chuck Schumer invited FLI to share its perspective on AI regulation with selected senators. In Europe, FLI successfully advocated for the inclusion of more general AI systems, such as GPT-4, in the EU's Artificial Intelligence Act. In military policy, FLI coordinated the support of the scientific community for the Treaty on the Prohibition of Nuclear Weapons. At the UN and elsewhere, the institute has also advocated for a treaty on autonomous weapons. Research grants The FLI research program started in 2015 with an initial donation of $10 million from Elon Musk. In this initial round, a total of $7 million was awarded to 37 research projects. In July 2021, FLI announced that it would launch a new $25 million grant program with funding from the Russian–Canadian programmer Vitalik Buterin. Conferences In 2014, the Future of Life Institute held its opening event at MIT: a panel discussion on "The Future of Technology: Benefits and Risks", moderated by Alan Alda. The panelists were synthetic biologist George Church, geneticist Ting Wu, economist Andrew McAfee, physicist and Nobel laureate Frank Wilczek and Skype co-founder Jaan Tallinn. Since 2015, FLI has organised biannual conferences with the stated purpose of bringing together AI researchers from academia and industry. As of April 2023, the following conferences have taken place: "The Future of AI: Opportunities and Challenges" conference in Puerto Rico (2015). The stated goal was to identify promising research directions that could help maximize the future benefits of AI. At the conference, FLI circulated an open letter on AI safety which was subsequently signed by Stephen Hawking, Elon Musk, and many artificial intelligence researchers. The Beneficial AI conference in Asilomar, California (2017), a private gathering of what The New York Times called "heavy hitters of A.I." (including Yann LeCun, Elon Musk, and Nick Bostrom). The institute released a set of principles for responsible AI development that came out of the discussion at the conference, signed by Yoshua Bengio, Yann LeCun, and many other AI researchers. These principles may have influenced the regulation of artificial intelligence and subsequent initiatives, such as the OECD Principles on Artificial Intelligence. The beneficial AGI conference in Puerto Rico (2019). The stated focus of the meeting was answering long-term questions with the goal of ensuring that artificial general intelligence is beneficial to humanity. In the media "The Fight to Define When AI is 'High-Risk'" in Wired. "Lethal Autonomous Weapons exist; They Must Be Banned" in IEEE Spectrum. "United States and Allies Protest U.N. Talks to Ban Nuclear Weapons" in The New York Times. "Is Artificial Intelligence a Threat?" in The Chronicle of Higher Education, including interviews with FLI founders Max Tegmark, Jaan Tallinn and Viktoriya Krakovna. "But What Would the End of Humanity Mean for Me?", an interview with Max Tegmark on the ideas behind FLI in The Atlantic. See also Future of Humanity Institute Centre for the Study of Existential Risk Global catastrophic risk Leverhulme Centre for the Future of Intelligence Machine Intelligence Research Institute The Precipice: Existential Risk and the Future of Humanity References External links Official website