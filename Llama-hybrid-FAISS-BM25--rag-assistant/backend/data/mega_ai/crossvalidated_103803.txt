[site]: crossvalidated
[post_id]: 103803
[parent_id]: 
[tags]: 
How to compare and validate imputation models?

I've seen a lot of interesting questions here about multiple imputation and also great answers that helped me a lot to impute my data. I've used Predictive Mean Matching, EMB and I would like to use Random Forest but I'm not being able to run the model (so sad). I also have results from the complete dataset. Overall, the dataset is composed by 8364 observations and only 2860 are complete observations without any missing data. The imputed models are somewhat similar results, the imputed models have narrow confidence intervals and the model obtained by PMM through Mice does not select one variable due to the use of predictors (Explanation: This function creates a predictor matrix using the variable selection procedure described in Van Buuren et al.~(1999, p.~687--688). The function is designed to aid in setting up a good imputation model for data with many variables.) I know I can validate the model with the function validate from the package rms (I plan to do that but I need to understand it better). But who can I compare imputed models and draw some conclusions. Is the output of validate() enough? For instance, if I wanted to repeat another analysis with the same data, could I choose the "best" method or it's better to repeat the analyses for all methods? I understand this question can be quite basic but I'm having some issues figuring out the best way to proceed.
