[site]: datascience
[post_id]: 19874
[parent_id]: 
[tags]: 
Why doesn't overfitting devastate neural networks for MNIST classification?

I have a simple neural network(NN) for MNIST classification. It includes 2 hidden layers, each with 500 neurons. Hence the dimensions of the NN are: 784-500-500-10. ReLU is used in all neurons, softmax is used at the output, and cross-entropy is the loss function. What puzzles me is why overfitting doesn't appear to devastate the NN? Consider the number of parameters (weights) of the NN. It's approximately $$784\times500+500\times 500+500\times 10=647000.$$ However, in my experiment, I used only $6000$ examples (a tenth of the MNIST training set) to train the NN. (This is just to keep the run time short. The training & test error would both go down considerably if I used more training examples.) I repeated the experiment 10 times. Plain stochastic gradient descent is used (no RMS prop or momentum); no regularization/drop-out/early-stopping was used. The training error and test error reported were: $$\begin{array}{|l|c|c|c|c|c|c|c|c|c|c|} \hline \textrm{No.} & 1 & 2 & 3 &4 &5&6&7&8&9&10\\ \hline E_{train}(\%) & 7.8 & 10.3 & 9.1 & 11.0 & 8.7 & 9.2 & 9.3 & 8.3 &10.3& 8.6\\ \hline E_{test}(\%) & 11.7 & 13.9 & 13.2 & 14.1 &12.1 &13.2 &13.3 &11.9 &13.4&12.7\\ \hline \end{array}$$ Note that in all 10 experiments (each with independent random parameter initialization), the test error differed from the training error only by approx. 4%, even though I used 6K examples to training 647K parameters. The VC dimension of the neural network is on the order of $O(|E|log(|E|))$ at least, where $|E|$ is the number of edges (weights). So why wasn't the test error miserably higher (e.g. 30% or 50%) than the training error? I'd greatly appreciate it if someone can point out where I missed. Thanks a lot! [ EDITS 2017/6/30] To clarify the effects of early stopping, I did the 10 experiments again, each now with 20 epochs of training. The error rates are shown in the figure below: $\qquad\qquad\qquad$ The gap between test and training error did increase as more epochs are used in the training. However, the tail of the test error stayed nearly flat after the training error is driven to zero. Moreover, I saw similar trends for other sizes of the training set. The average error rate at the end of 20 epochs of training is plotted against the size of the training set below: $\qquad\qquad\qquad$ So overfitting does occur, but it doesn't appear to devastate the NN. Considering the number of parameters (647K) we need to the train and the number of training examples we have (
