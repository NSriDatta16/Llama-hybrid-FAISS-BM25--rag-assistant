[site]: crossvalidated
[post_id]: 47738
[parent_id]: 
[tags]: 
Median Averaging and Error Analysis

I am not sure if i should be asking this on the Mathematics of Physics Stack Exchange site. I am taking data for a physics research application and have a question about using median vs. mean averaging. I am concerned with the amount it takes to acquire each data point (location). For each point I currently take multiple measurements, average them, and use the standard deviation of the mean as my error calculation for the point. My problem is that it is only reasonable for me to take a relatively small number of measurement(under 10) for each data point. It seems to me; if any of the measurement vary drastically from the actual value, the median of the measurements will converge to a value close to the actual value in fewer data points than the mean. What do you think of this assumption? If I use the median of my measurements what would be the correct way to calculate the error for each data point?
