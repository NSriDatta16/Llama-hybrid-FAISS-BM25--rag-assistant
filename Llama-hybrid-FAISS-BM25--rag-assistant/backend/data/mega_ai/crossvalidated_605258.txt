[site]: crossvalidated
[post_id]: 605258
[parent_id]: 
[tags]: 
"Prediction interval" for a slope of a model fitted to new data

Problem statement: I am using a simple linear regression with outcome $y$ and single predictor $x$ of the form: $$ y_i \sim N(\mu_i, \sigma) \\ \mu_i = \beta_0 + \beta_1x_i $$ I assume fitting $\beta_0, \beta_1$ with maximum likelihoood (although if other method works better for the purpose of this question, I can use that). Then, I observe a new set of outcomes $\bar{y}$ for a (known) new set of predictors $\bar{x}$ which I assume come from the same process. I then assume a regression is fitted independently to $\bar{x}, \bar{y}$ obtaining $\bar{\beta}_0, \bar{\beta}_1$ . Is there an analytical way to use the original fit to get something like a "prediction interval" for $\bar\beta_1$ ? I am looking for an interval with the expected coverage properties for $\bar\beta_1$ - the estimate of the slope from $\bar{x}, \bar{y}$ . I assume this is going to be noticeably wider than the confidence interval as there's the additional uncertainty arising from sampling $\bar{y}$ and constructing the estimate based on those samples. I could obviously use simulations to obtain that interval, but I am curious if there is a more elegant way. Background: I am attempting to do something akin to cross validation for testing the extent that each of a set of possible summaries is "reproducible" - having $N$ replicates, intuitively, a summary is good, if summarising $N - K$ replicates of the experiment lets me predict the summary the remaining $K$ replicates well. One of the summaries I am experimenting with is a slope from a regression model, so I'd like to develop the expected prediction interval if the model was actually capturing the data well. Code example: The following code shows the problem and solution with simulations in R. In each simulation, I am trying to use the linear fit to $x,y$ to construct an interval that would contain $\bar{\beta}_1$ in 95% of cases. I use the confidence interval and then I use simulations to construct a "prediction" interval. x = b1ci[,1] & b1bar = b1pred_sim[,1] & b1bar Due to the simulations this is quite slow, but a typical result is something like: Coverage CI: 0.81 Coverage prediction - sims: 0.96 so CI is too narrow while the simulations appear (at least approxiamtely) well calibrated. So the question is if I can get something like b1pred_sim analytically. Note that b1pred_sim is just appropriately widened CI:
