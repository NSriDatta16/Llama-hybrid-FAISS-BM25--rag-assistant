[site]: crossvalidated
[post_id]: 19318
[parent_id]: 4707
[tags]: 
My answer to this question provides one example. Two data sets, both containing two variables, with the same sign correlation (positive or negative or zero), will get the same PCs if you base your PCA on the correlation matrix (i.e. if you standardise your variables). The two PCs are always $\pm$45 degree rotations, with the first PC being the rotation with same sign as correlation, and second PC being the rotation with opposite sign. The size of the correlation is informative about the strength of the decomposition (via % variance explained), but totally uninformative about the direction. Another example, in any dimension is if the correlations are all equal. The correlation matrix then has two distinct eigenvalues $\lambda_1=1+(d-1)\rho$ (multiplicity $1$) and $\lambda_2=(1-\rho)$ (multiplicity $d-1$), with normalised eigenvector $e_1= \frac{1}{\sqrt{d}}(1,\dots,1)^T$. The remaining $e_2,\dots,e_d$ are not unique, but span the subspace orthogonal to $e_1$ (they are still pairwise orthogonal because the matrix is symmetric and real). One way to do this is for $j=2,\dots,d$ $$e_{j1}=\frac{1}{\sqrt{2}},e_{jj}=-\frac{1}{\sqrt{2}},e_{jk}=0\forall k\notin\{1,j\}$$ You can then create an infinite number of solutions by arbitrarily rotating this one (in $d-1$ dimesnions). $(\tilde{e}_2,\dots,\tilde{e}_d)=(e_2,\dots,e_d)R$.
