[site]: crossvalidated
[post_id]: 310905
[parent_id]: 
[tags]: 
What is the intuition behind assuming a variable satisfies a distribution?

I'm a new to the machine learning field and please forgive me if this question is trival. In a famous paper: M. Hoffman, D. Blei, and P. Cook, Bayesian nonparametric matrix factorization for recorded music. ICML 2010 (PDF: link , page 2, left column ), the authors assume that the elements in the latent submatrices $ W $ and $H$ satisfy a gamma distributions: $W_{ml} \sim Gamma(a, a), $ $H_{ln} \sim Gamma(b, b) $ I was wondering why we should assume these latent variables are drawn from a gamma distribution? What is the reason the authors choose the gamma distribution instead of another?
