[site]: datascience
[post_id]: 18439
[parent_id]: 18437
[tags]: 
15K features is a lot of features. I'd bet your 0.01 F1 score is a direct result of having so many features. Without knowing more details about your data it is hard to suggest a more sensible alternative. I'd want it to be closer to 100 features rather than 15,000 though. Are these features extracted from text? N-grams? Regarding your feature selection problem, the first step is to eliminate any features with no variance (all samples the same), then one can utilis various statistical methods for selecting features to retain those with high variance (sklearn has great libraries for this: Feature selection ). One could also utilize Ranked Guide Iterative Feature Elimination (RGIFE) for this purpose, which uses random forests: RGIFE - Ranked Guided Iterative Feature Elimination P.S. Your class ratio is very skewed as well. Once you're all settled with a more reasonable dataset in feature space I'd consider either undersampling the larger class, oversampling the smaller class or a combination of both, and keep an eye on the resulting confusion matrices when performing your evaluation.
