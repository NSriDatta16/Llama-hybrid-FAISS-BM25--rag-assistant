[site]: crossvalidated
[post_id]: 428325
[parent_id]: 428317
[tags]: 
My first question is: Why is it starting from this? Should not it start from $P(\mathbf{t|w,X)}?$ The statement is Bayes theorem . It is used to derive the posterior distribution from prior distribution $P(\mathbf{w)}$ and likelihood $P(\mathbf{t|w,X)}$ . I don't know what do you mean by "starting" from the likelihood, but in Bayesian setting, you need both elements: priors and likelihood. Why it is not possible to perform the integration? It is not that the integral is "not possible to compute". There is no closed-form mathematical solution for this integral. In plain English, this means that there is no simple mathematical formula for it, where you could just plug-in the data and get the result. That is the reason, why we use methods like MCMC to approximate the integral. Also assuming that when talking about proportionality it is rexpressing the posterior as: $P(\mathbf{w|t,X)}= \frac{P(\mathbf{t|w,X)}P(\mathbf{w)}}{\int P(\mathbf{t|w,X}) P(\mathbf{w}) d\mathbf{w}}$ , which is not explained, it is not saying which type of proportionality is that (inversely/directly) is it? if so how? Hard to say what you mean, but my best guess is that what was meant in the handbook is $$ P(\mathbf{w|t,X)} \propto P(\mathbf{t|w,X)}P(\mathbf{w)} $$ so it is proportional if we drop the normalizing constant $Z^{-1}$ (using the notation mentioned by you later). It says the first is not very Bayesian: Why? [...] Because in Bayesian setting $\mathbf{w}$ is a random variable, so it has a distribution and you are interested in finding this distribution, rather then single "best" candidate. [...] why a maximum in $g$ should correspond to a maximum in the posterior ? Because maximum of the posterior distribution is the maximum of the posterior distribution. As said earlier, $g$ is proportional to the posterior. In the optimization dividing $g$ by $Z$ would not make a difference in the result, so they are the same for this purpose. See also Normalizing constant in Bayes theorem , Normalizing constant irrelevant in Bayes theorem? , and Why Normalizing Factor is Required in Bayes Theorem? . And does logistic regression use a particular method or is it independent from that? The discussed model is a Bayesian logistic regression model . There is also the "classical" logistic regression that does not use priors and Bayes theorem at all, if this is what you are asking. Why is the expectation taken over $P(\mathbf{w|t,X)}$ ? Because that is what we are interested in, aren't we? We want to estimate the distribution of the parameters $\mathbf{w}$ given the data we observed $\mathbf{t,X}$ . The procedure is about estimating the parameters of the model. What is the first motivation of logistic regression? Does it start from the desire to keep the linearity hence using $w^Tx$ but forcing the model to output probabilities so passing the linear function to a sigmoid? Yes, that's it. You can think of logistic regression as of linear regression that outputs probabilities , where the target variable is binary (or is a probability of binary event ).
