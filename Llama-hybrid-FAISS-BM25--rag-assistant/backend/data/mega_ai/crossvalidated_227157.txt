[site]: crossvalidated
[post_id]: 227157
[parent_id]: 
[tags]: 
Several questions about using PCA on large data

I am using PCA on a square matrix of pairwise distances between 6000 elements, where the columns can be viewed as variables and rows as observations. Here are some of my questions and concerns: 1) What are the general assumptions in using PCA on large data, and what tests should I run beforehand? (for instance, is it necessary to check Bartlett's and KMO, and is it sufficient to just look at skewness values to confirm normality?) 2) Is it reasonable to keep all components with eigenvalues greater than 1? (my analysis yielded about 200 such components that explain 90% of the variance, without any definite breaking point in the scree plot) 3) Once the factors are extracted, what is a reasonable cutoff point for assigning a variable to a component? (I want to discard variables with loadings below 0.5, but this has meant keeping only about a third of them, leaving a few empty components. I don't mind this, since it means lower dimensionality, but I'm wondering whether it would be a terrible crime to discard so many variables) 4) After discarding variables that load poorly or cross-load, should the PCA be repeated again with only the retained variables? I would be grateful for any guidance you can offer
