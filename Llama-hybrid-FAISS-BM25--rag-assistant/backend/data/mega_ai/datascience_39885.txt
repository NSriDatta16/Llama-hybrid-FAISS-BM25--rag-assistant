[site]: datascience
[post_id]: 39885
[parent_id]: 8860
[tags]: 
Dropout is a regularization technique used to avoid overfitting in large neural networks specifically by leaving out some of the neurons in hidden layers(hence the name dropout for the left out neurons) after training. Basically if the network really learnt anything during training then dropping out some of the neurons shouldn't affect the precision of the predictions negatively. Bagging is an effective regularization technique also, used to reduce variance from the training data and improving the accuracy of your model by using multiple copies of it trained on different subsets of data from the initial/larger training dataset. see this question
