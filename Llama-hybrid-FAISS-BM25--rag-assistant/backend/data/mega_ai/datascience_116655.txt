[site]: datascience
[post_id]: 116655
[parent_id]: 
[tags]: 
Is there bias in matrix multiplications for self attention

When the query matrix Q is computed as $XW_Q$ , ( $W_Q$ is the weight matrix for the queries), is it implemented as a linear layer without bias? I see some blogs saying there is are bias terms as well. To paraphrase the question in a different way, are there bias terms when Q , K and V are computed using a linear layer in pytorch? If yes, why does it make sense since adding bias makes the transformation a non linear one
