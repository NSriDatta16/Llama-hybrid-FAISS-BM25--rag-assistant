[site]: crossvalidated
[post_id]: 547934
[parent_id]: 
[tags]: 
Optimizing over a subset of input variables

Consider the unconstraint optimization problem: $$\arg \min_{_0} f(X_0, X_\eta)$$ where $X_0$ is the input vector I need to optimize for $_\eta$ is a nuisance vector which I do not care about. This is a contextual bandit problem where the $X_0$ are actions and $X_\eta$ are the contexts. What I am actually doing is learning the function from data (e.g. random forest or neural net). After learning the function, I run an optimizer to minimize it. Due to the nature of my problem, I don't have data incoming in real time, and I have to precompute the optimal actions, across all possible contexts. So I need to somehow integrate or marginalize them. The reason I want to keep as input to the predictive model is because it really helps with prediction accuracy. I am seeking some guidance about how to deal with this nuisance vector. In particular: Is this an indication that I am formulating this optimization problem the wrong way? Is running the optimization over the full input space { $X_0$ , $X_\eta$ } and then just selecting only the $X_0$ part a good idea ? Is it better to not include $X_\eta$ when training the function $f$ at all, and just deal with the lower accuracy? or any other feedback or thoughts are appreciated!
