[site]: crossvalidated
[post_id]: 231495
[parent_id]: 64725
[tags]: 
Let me provide a response for the first situation that you analysed because the second situation essentially parallels it, except that you have two more items in the second situation and you chose a different model (more about that below). In providing this response, in some places I have a different interpretation from the extended explanation that has been provided elsewhere in these posts. As I understand it, you had 17 raters (participants), each of whom provided a rating on 5-point scales to seven different items AND you are wanting to see whether there is much agreement between the 17 raters in how they rated those 7 items. I think that, in order to do this (which is surely a pretty unusual situation; usually there are not as many as 17 raters involved in assessing something), you should have selected ABSOLUTE (not consistent) measures in SPSS, and, if your participants are the only raters of interest in this situation (I assume they are, and that you are not wanting to generalize your results to other participants / raters) you should indeed have chosen Model 3 (i.e., 2-way mixed, NOT Model 2 as you did in your second setup), which is the FIRST model offered in SPSS. So, in essence, you have made a basic mistake in selecting the kind of ICC that provides a consistency solution SPSS. (Sorry to give you the bad news.) Next, when you choose an ICC from the output you should choose the ICC from the row titled "Single measures" (i.e., .133) because each of your participants made a single rating for each of the 7 items (and I assume you entered 17 scores into the ICC analysis for each item). If you had averaged all of your 17 participants' ratings on each item BEFORE entering the data into the ICC analysis, it would be appropriate for you to report the ICC that pertains to the Averaged measures (.519). But, from your description, you didn't average the ratings that were made by your participants. If you had chosen Absolute rather than Consistency for your first analysis, an ICC as low as .133 would indicate that your 17 participants / raters exhibited EXTREMELY little agreement among themselves in terms of how they rated the 7 items. An article in 2016 by Trevethan in the journal Health Services and Outcomes Research Methodology provides the background for this answer as well as a lot of other information concerning the selection and interpretation of ICCs. Finally, the small number of items (7 in the first situation) might create some problems statistically. I am sorry, but I am not able to provide advice about that. Maybe it's OK in your situation, but it might be advisable to consult a friendly statistician. References Trevethan, R. (2016). "Intraclass correlation coefficients: Clearing the air, extending some cautions, and making some requests." Health Services and Outcomes Research Methodology. DOI 10.1007/s10742-016-0156-6. (Online publication available until volume, issue, and page numbers have been assigned.)
