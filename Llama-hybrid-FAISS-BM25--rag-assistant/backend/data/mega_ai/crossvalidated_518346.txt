[site]: crossvalidated
[post_id]: 518346
[parent_id]: 
[tags]: 
How can RNNs handle inputs of dynamic length?

Here is the structure of RNN (one layer): We know that Batch Normalization does not work for RNN . Suppose two samples $x^1,x^2,$ in each hidden layer, different sample may have different time depth (for $h^1_{T_1},\ h^2_{T_2},$ $T_1$ and $T_2$ may different). Thus for some large $T$ (deep in time dimension), there may be only one sample, which makes the statistical mean and variance unreasonable. However I don't understand why some samples will stop at some time levels? Could you give an example? I understand as that two sample sequence inputs: $(x^1_1,\cdots,x^1_T),(x^2_1,\cdots,x^2_T)$ have same length ( $=T$ ), then the lengths of their hidden layers $\Big((h^1_1)^{(l)},\cdots, (h^1_T)^{(l)}\Big),\Big((h^2_1)^{(l)},\cdots, (h^2_T)^{(l)}\Big)$ should be same for each layer ( $=T$ ). How could dynamic on length happens?
