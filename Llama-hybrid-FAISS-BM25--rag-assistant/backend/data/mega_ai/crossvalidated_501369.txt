[site]: crossvalidated
[post_id]: 501369
[parent_id]: 
[tags]: 
In a predefined (but realistic, and explaining) example, what $p(x)$ should be in a variational autoencoder?

I'm learning about variational autoencoders for a month now. The more I'm reading, and digging deep about it, the more confused I am. Thinking about it an a neural network perspective probably doesn't help this, and I can't find the kind of questions I'm asking, so I'm definitely missing something. In the tutorials/explanations, they say the initial goal is to calculate $p(z|x)$ . The reason why we have to set up the whole network is to approximate this. And the reason why don't have it directly is because of from $\frac{p(x|z)p(z)}{p(x)}$ , $p(x)$ is intractable. This makes the impression that we know $p(x|z)$ . We defined $p(z)$ as a standard normal distribution, that's clear. And even though we know that $p(x|z)$ is the output of decoder, if we look aside, or forget that we know this, it is not obvious at all. At least for me. Are we looking at $x$ , the input fed into the network as $p(x|z)$ , and basically telling it "this is the probability of $x$ given $z$ , figure out what's the probability of $z$ given $x$ , if this is the probability of $z$ "? But because it's missing $p(x)$ , that's why we have to just approximate it? And this is the reason why we think about $p(x|z)$ as we know it? How would it be possible to reproduce a "perfect" autoencoder, where we know $p(x)$ , even if it's a very simple example? How $p(x)$ should be imagined in the context of variational autoencoders?
