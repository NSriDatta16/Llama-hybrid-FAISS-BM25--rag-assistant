[site]: datascience
[post_id]: 126660
[parent_id]: 126658
[tags]: 
The hidden states in RNNs and Transformers share their purpose, but they work in different ways. In RNNs, the hidden state is a way for the network to maintain a memory of previous time steps. The hidden state at a given time step is calculated based on the input at the current time step and the hidden state from the previous time step. Transformers use a self-attention mechanism to capture dependencies between different positions in the input sequence. They process the entire input sequence in parallel, considering all positions simultaneously. "Hidden states" in transformers is normally used to refer to the output of each attention layer. Both RNNs and Transformers work on numeric vectors. Text is discrete, so we normally use embedding layers to represent discrete text tokens (e.g. words) as numeric vectors. The size of these vectors is defined a prior, when designing the network. The numbers you mentioned are typical values of the embedding dimensionality (e.g. BERT's embedding dimensionality is 768), but any value can be used. Of course, the chosen value has an effect on the capacity of the network and it's ability to learn.
