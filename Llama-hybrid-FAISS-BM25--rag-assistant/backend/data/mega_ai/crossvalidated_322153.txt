[site]: crossvalidated
[post_id]: 322153
[parent_id]: 
[tags]: 
Is it posible to obtain the next point without bruteforcing in Bayesian Optimization?

The implementations I have found till now of Bayesian optimization tipically find the optimal input point X for next step, which minimizes the output Y of the acquisition function which is being modeled, by testing a lot of samples (by random search, gridsearch, etc) and then selecting the best one. For example, this happens in this implementation , exactly in the acq_max() function located in this file Aren't there currently implementations where these new points are obtained directly by finding the local minima of the modeled function, maybe with the derivatives of dXi/dY of the corresponding acquisition function (either it is a Gaussian Process, a multi-layer perceptron...)? Is this even posible? I found some tracks that suggest me that this is posible, like the inversion of MLP , but I can not find this implemented in a bayesian optimizer
