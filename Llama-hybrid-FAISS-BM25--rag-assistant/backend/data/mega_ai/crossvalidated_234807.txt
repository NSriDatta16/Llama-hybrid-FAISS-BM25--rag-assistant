[site]: crossvalidated
[post_id]: 234807
[parent_id]: 234789
[tags]: 
Based on your confusion matrix, there does still seem to be a problem to be fixed as your errors are not symmetric. The first thing to do is look at your learning curves to be sure you aren't over fitting. Over sampling has a strong possibility to over fit with many machine learning algorithms. Depending on the ML algorithm, over sampling can teach the method "class b looks exactly like this but class a can be lots of things". Imaging as an extreme example, a Radial Basis Function SVM classifier with over sampled data. That function could make gamma really, really high making it so that the features of your new data point have to be practically identical to be classified correctly. Still, since there is over sampling and the same data may be in both your training and your test set for validation, the machine could still get perfect accuracy in validation, even though it won't generalize at all. One possible alternative (depending on your classification technique) is to use class weights instead using sampling techniques. Adding a greater penalty to misclassifying your under represented class can reduce bias without "over training" on the under-represented class samples. Another option could be leaving the samples alone changing your optimization metric to something not strongly effected by class bias like AUC or the Kappa statistic .
