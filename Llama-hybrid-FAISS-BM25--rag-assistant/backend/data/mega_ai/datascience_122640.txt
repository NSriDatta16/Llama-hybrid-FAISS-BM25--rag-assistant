[site]: datascience
[post_id]: 122640
[parent_id]: 
[tags]: 
How many features is too many when using feature selection methods?

Now obviously there is no such thing as an ideal number as every problem is different, but I've been Googling, ChatGPTing, & Youtubing this question for a few days now and I am constantly getting conflicting feedback. Some sources say you should throw as many features as you can engineer that are within reason to the problem at feature selection methods, and other sources say you should use domain knowledge and have a much more refined scope for the feature selection methods. I know this is a general question, but I'm hoping someone has a general "rule of thumb" response for this question. But if we are talking about specifics, I am working on a binary classification model using time series / balanced data. I could be at roughly 600 features for 6 months worth of data or 200 depending on the route I take.
