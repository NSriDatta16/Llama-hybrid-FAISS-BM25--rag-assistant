[site]: crossvalidated
[post_id]: 79317
[parent_id]: 79277
[tags]: 
You need to recalculate the PCA for each of the CV surrogate models. Acutally the same already applies to possible previous pre-processing steps that use more than just one case for their calculation (e.g. centering, variance scaling). But I just want to decide the optimal dimension k via PCA. Is it more convenient to do it once? More convenient, maybe, but the result will be overfit as @aplassard explains. This means that you cannot get the correct $k$ without recalculating the PCA : doing the PCA on the whole data will yield too high $k$. This will be particularly serious with small $n$. When optimizing the model complexity, in theory a constant optimistic bias does not hurt the decision. However, in practice, the optimistically biased "shortcuts" I've encountered so far are not even close to constant but point towareds too high model complexity. You say that you are in a small $n$ situation. In that case, is there any chance that you could fix $k$ for the PCA by your knowledge about data and application? In small sample size situations, two things happen You likely do not have enough samples to do the model comparisons necessary for the optimization (fixing of $k$). But nevertheless attempting to do this means that you need a second, outer (independent) validation, thus splitting your data again. Unless, of course, you'll validate against a data set that you don't have before. For what it is worth, for the data sets I usually work with (spectroscopic classification, p ca. 10² - 10³, n underestimates the error typically by a factor of at least 2 - 3 , and I've seen "perfect" classification internally being only 80 % or even 65% with proper separation of test and training data in the crossvalidation.
