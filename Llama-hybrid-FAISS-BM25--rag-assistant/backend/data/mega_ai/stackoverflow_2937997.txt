[site]: stackoverflow
[post_id]: 2937997
[parent_id]: 2937818
[tags]: 
Your final result is an just a weighted average of precises, so presumably you don't need to follow the rules used when calculating account balances etc. If I am correct about the above, then you don't need to use BigDecimal , double will suffice. The problem of overflow can be solved by storing a "running average" and updating it with each new entry. Namely, let a_n = (sum_{i=1}^n x_i * w_i) / (sum_{i=1}^n w_i) for n = 1, ..., N. You start with a_n = x_n and then add d_n := a_{n+1} - a_n to it. The formula for d_n is d_n = (x_{n+1} - w_{n+1}*a_n) / W_{n+1} where W_n := sum_{i=1}^n w_n. You need to keep track of W_n, but this problem can be solved by storing it as double (it will be OK as we're only interested in the average). You can also normalize the weights, if you know that all your weights are multiples of 1000, just divide them by 1000. To get additional accuracy, you can use compensated summation . Preemptive explanation: it is OK to use floating point arithmetic here. double has relative precision of 2E-16. The OP is averaging positive numbers, so there will be no cancellation error. What the proponents of arbitrary precision arithmetic don't tell you is that, leaving aside rounding rules, in the cases when it does give you lots of additional precision over IEEE754 floating point arithmetic, this will come at significant memory and performance cost. Floating point arithmetic was designed by very smart people (Prof. Kahan, among others), and if there was a way of cheaply increasing arithmetic precision over what is offered by floating point, they'd do it. Disclaimer: if your weights are completely crazy (one is 1, another is 10000000), then I am not 100% sure if you will get satisfying accuracy, but you can test it on some example when you know what the answer should be.
