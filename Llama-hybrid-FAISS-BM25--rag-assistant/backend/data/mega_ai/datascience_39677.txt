[site]: datascience
[post_id]: 39677
[parent_id]: 39661
[tags]: 
It is certainly correct in the sense that it is a legitimate neural network. The dropout layer introduces noise that is not injected during the test period. The goal is to combat overfitting so that the error in your test set will be lower due to better generalization. Applying the dropout layer on top of the input layer however throws away a lot of information, meaning it will be difficult to learn a good strategy. Usually dropout layers are applied after the first transformations, which means the neural network has to rely on multiple different transformations that will mean something similar, reducing complexity. However, if you put it over your input layer you are throwing away information that is not accessible in any other way,
