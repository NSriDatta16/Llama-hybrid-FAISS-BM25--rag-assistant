[site]: crossvalidated
[post_id]: 217336
[parent_id]: 
[tags]: 
How was "derivative of the error function with respect to the activation" (that looks like y -t) derived?

In Chapter 5 (Neural Networks) of Bishop Pattern Recognition and Machine Learning he mentions several times that the derivative of the error function with respect to the activation for a particular output unit takes the form $$ \frac{\delta E}{\delta a_k} = y_k - t_k $$ where $E$ is the error function, $a_k$ is the activation, $y_k$ is the output, and $t_k$ is the target value. This is equation 5.18 in the book. Could someone explain how this was derived? Thank you!
