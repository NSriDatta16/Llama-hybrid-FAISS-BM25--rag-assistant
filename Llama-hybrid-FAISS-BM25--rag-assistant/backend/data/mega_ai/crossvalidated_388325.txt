[site]: crossvalidated
[post_id]: 388325
[parent_id]: 
[tags]: 
How can I understand the concept of a noise in machine learning?

In Bishop's book, one of the first examples is shown here Essentially, the data $x$ are randomly generated, and $t$ are generated by running $x$ through a function $\sin(2\pi x)$ , then Gaussian noise is added. I have trouble understanding how this theoretical toy example Gaussian noise translates into real world application. For example, I am given a data set containing a bunch feature vectors representing spam emails, and a target $\{-1, +1\}$ corresponding to whether or not the user has indicated it is a spam $(+1)$ or not $(-1)$ . I have difficulty seeing where the Gaussian noise in the above example translates into this practical scenario. Is the noise added when the person who is creating the data set separates the emails into a spam or non-spam (and how)? I don't quite understanding since, I can imagine going through my mailbox right now and separating spam versus non-spam, and I could do so with 100% accuracy - no noise. Just knowing that there is some "noise" isn't very helpful (there is pretty much noise in everything in our physical realm), there any actual method of modeling this noise, mathematically speaking?
