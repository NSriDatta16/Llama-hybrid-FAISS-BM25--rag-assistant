[site]: datascience
[post_id]: 96468
[parent_id]: 96463
[tags]: 
Train with relevant/non-relevant approach using sentence-transformers . When you train the model you can encode all documents and get their BERT embedding vectors. Elastic search lets you put these vectors in properties of your corpus, so each document is saved along with its embedding vector. For each query get the first 1000 candidates and their vectors using elastic search and rerank them in python using cosine similarity and return the results. Don't be surprised if didnt perform well. PS: the calculation of cosine similarity can be also done on the fly using Learning-2-Rank plugin of elastic search but I didnt use it myself.
