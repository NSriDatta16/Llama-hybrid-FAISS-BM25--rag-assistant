[site]: crossvalidated
[post_id]: 626163
[parent_id]: 626160
[tags]: 
Let me sketch a brief picture of the underlying theme of linear regression: When $\mathbf y=\mathbf X\boldsymbol\beta+\boldsymbol\varepsilon,~\mathbf y\ne \mathbf X\boldsymbol\beta^\star$ for some $\boldsymbol\beta^\star\in\mathbb R^p$ that is, $\mathbf y\notin \mathcal C(\mathbf X). $ This means the system $\mathbf y= \mathbf X\boldsymbol\beta^\star$ is not solvable. The problem of linear regression ultimately boils down to the more intrinsic question: how to solve a non-solvable linear system? One can resort to a useful result in functional analysis, known as Closest Point Principle, which states that Let $U$ be a non-empty closed convex subset of a Hilbert space $\mathcal H;$ let $y\in U^\complement.$ Then there exists a unique $\hat a\in A$ such that $$\Vert y-\hat a\Vert=\inf\{\Vert y-a\Vert:a\in A\}.$$ Simplifying in our perspective, the result assures us that we can find $\hat{\mathbf y}\in\mathcal C(\mathbf X) $ such that it has the closest square distance to $\mathbf y\notin \mathcal C(\mathbf X)$ i.e. $\Vert \mathbf y-\hat{\mathbf y}\Vert^2\leq \Vert\mathbf y-\mathbf y^\star\Vert^2~\forall ~\mathbf y^\star\in \mathcal C(\mathbf X). \tag 1\label 1$ And one can find that $\hat{\mathbf y}=\mathbf P_\mathbf X\mathbf y, $ where $\mathbf P_\mathbf X:=\mathbf X(\mathbf X^\top\mathbf X) ^-\mathbf X^\top$ is the projection operator onto $\mathcal C(\mathbf X)$ and thus $\hat{\mathbf y}=\mathbf X\underbrace{(\mathbf X^\top\mathbf X) ^-\mathbf X^\top\mathbf y}_{:=\hat{\boldsymbol\beta}}.$ When the norm is $\Vert \cdot\Vert_2, ~\eqref 1$ becomes $$\sum_i (\mathbf y-\mathbf X\hat{\boldsymbol\beta})^2_i\leq \sum_i(\mathbf y-\mathbf X\boldsymbol\beta^\star)^2_i~\forall~\boldsymbol\beta^\star\in\mathbb R^p.\tag 2$$ So far, so good. But what is the working principle? The same old calculus to the rescue: $$\frac{\partial}{\partial\boldsymbol\beta^\star}\Vert \mathbf y-\mathbf X\boldsymbol\beta^\star\Vert^2=2\mathbf X^\top\mathbf X\boldsymbol\beta^\star-2\mathbf X^\top\mathbf y=\mathbf 0.\tag 3$$ Meanwhile there have been handful of discussions on or around this topic here, which you can look into. One of the more general and expository posts to start with would be this CV post: Why do we usually choose to minimize the sum of square errors (SSE) when fitting a model? and links therein. References: $\rm [I]$ Linear Regression, Jürgen Groß, Springer-Varleg, $2003, $ sec. $2.2.1, $ pp. $37-39.$ $\rm [II]$ An Introduction to Functional Analysis, James C. Robinson, Cambridge University Press, $2020, $ sec. $10.1, $ pp. $126-127.$
