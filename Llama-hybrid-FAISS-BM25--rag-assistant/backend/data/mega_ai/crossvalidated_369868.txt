[site]: crossvalidated
[post_id]: 369868
[parent_id]: 369843
[tags]: 
If you are not doing any sort of hyper parameter tuning, but are doing any sort of other phases of model building that could lead to data leakage/are part of the model building pipeline/process (some examples: filter based/wrapper based feature selection, feature engineering, pre-processing of the data) then in my experience it is still useful to have a validation and test set. Consider this example: I have two partitions of my data set only, a training and test set. Say I am running an ordinary least squares and therefore have no hyper parameters to tune. I have many different predictors in my dataset and I suspect many are collinear, and I know this problem can lead to singular matrices/inflated coefficients increasing the variance in my predictions, so I run a filter that removes a variable if it is found to have a pairwise correlation in excess of 0.9 with another (and has the highest average correlation with all of the other predictors as well) on my training set only. I then fit my model, etc. and then remove the variables removed in the training set from the test set. I then calculate R-squared on the test set. But, I notice that my R-squared is only 0.55 on the test set, which most would say is quite poor. So, I go back and I try to change the pairwise correlation to 0.95. I then repeat the process, and say my score is now 0.56. Very little difference, perhaps my filter is removing too much information? I then decide to forego the filter entirely, rerun the entire process, and my R-squared is now 0.70. That's quite the improvement, my model must have improved, right? Unfortunately, we don't really know. The problem in this scenario is that the more I keep going back and changing things, the more likely I am to over fit to the test set (as an aside, you see this a lot in Kaggle competitions so much in which people over fit to the public leaderboard and end up dropping off badly after the competition closes). Essentially, I am inadvertently leaking information from the test set and making choices from what I am seeing based on the test set back to the training set. Remember that the test set is to give a final, unbiased performance estimate of the entire model building process. It is not to fine tune the process, whether that be hyper parameters or a simple model building decision; this is what the validation set is for. If I had a validation set in the above process, I would be able to say with much more confidence whether my decisions actually helped in the end as I would fine tune my process to the validation set and then, see the final performance in the test set (and if you use, say nested cross validation, you could do this multiple times and take an average). I've heard in other cases people go as far as to keep the test set under lock and key so that they are not tempted to look back and do what I just described above, but in some cases it is tough not to do. If your test set score is poor, for instance, of course you probably would want to go back and look at maybe how you validated the model. Either way, just make sure to use the test set very sparingly. Basically, in my opinion, sacrificing some data in the training step for more flexibility and reliability is worth it. Even if you are not making model building choices like what I described above or tuning hyper parameters, you may have accidentally leaked data (very easy to do, perhaps you are accidentally using a variable that is from the future that you wouldn't have at run time, imputing data on an entire set, etc.) which a test set would help with, and also, comparing the validation and test set scores for reasonableness is useful (example: we would in most cases expect the test set scores to be lower (which suggests over fitting, in contrast, if the test set score is higher it might suggest under fitting) the two scores should be relatively similar, maybe you have many runs of scores and can form confidence intervals to check this, etc.).
