[site]: datascience
[post_id]: 104437
[parent_id]: 104435
[tags]: 
Suppose you have two models which you can choose $m_1$ , $m_2$ . For a given problem, there is a best set of hyperparameters for each of the two models (where they perform as good as possible), say $m_1^*$ , $m_2^*$ . Now say $Acc(m_1^*) > Acc(m_2^*)$ , i.e. model 1 is better than model 2. Now suppose you have tuned model 2 (or you have "okay" hayperparameter by coincidence) but you use inferior hyperparameter for model 1. You could end up finding $Acc(m_1^s) (i.e. "choose model 2"), while the true best choice would be: "use tuned model 1". Thus in order to make an informed decision, you would need to "tune" both models and compare the performance of the tuned models with "best hyperparameter". What I often do is to define test and train data, tune possible models using cross validation ( train data only!), and assess the performance of the tuned models based on the test set. In addition you may want to do feature engineering / feature generation. This should be done before tuning the models, since different data may lead to different optimal hyperparameter, e.g. in case of a random forest, where the number of split candidates per split can be contingent on the number and the quality of features.
