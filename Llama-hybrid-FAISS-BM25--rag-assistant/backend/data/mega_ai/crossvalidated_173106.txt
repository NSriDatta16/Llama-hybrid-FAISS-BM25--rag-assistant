[site]: crossvalidated
[post_id]: 173106
[parent_id]: 
[tags]: 
Learning Curves: Why do people use the holdout set method for Decision Trees and K-NN, but RMSE for Neural Networks?

Based on this question and this link , it seems like people generally use RMSE to understand error and generate learning curves when analyzing a neural network model. But it seems like people generally just use error rate (or fraction of false predictions) to understand error and generate learning curves when analyzing a decision tree or k-NN model. Is this correct? Or are RMSE and error rate equally applicable when analyzing decision trees, k-NN, and neural network models? Or does RMSE lend itself better to the analyzation of neural network models than it does for decision trees and k-NN models?
