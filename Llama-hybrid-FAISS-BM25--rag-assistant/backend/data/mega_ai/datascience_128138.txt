[site]: datascience
[post_id]: 128138
[parent_id]: 
[tags]: 
Effect of Sequential Data Quality Variation on Transformer Model Training: Seeking Insights and Experiences

I'm exploring the training efficiency of transformer models against the backdrop of data quality sequencing. Specifically, I ponder whether arranging unlabeled data by presumed quality affects training outcomes, akin to the structured progression observed in human learning paradigms. This inquiry stems from the hypothesis that a gradual escalation in data complexity or a focused emphasis on high-quality data during training (e.g., more epochs for data subsets) might optimize model performance. I seek insights or empirical evidence from this communityâ€™s experiences or relevant literature that might illuminate the impact of data sequencing on transformer training. Any shared knowledge or references to studies exploring this facet would be invaluable.
