[site]: crossvalidated
[post_id]: 581656
[parent_id]: 427068
[tags]: 
I would make a comment, but unfortunately, I do not have enough reputation to comment. Disclaimer, I am the author of some references. If you have a function that is non-differentiable, let's say in a machine learning pipeline, we need to think about if our function differentiable at least somewhere and whether the derivative is meaningful. In the case of a sorting function, it is differentiable almost everywhere, i.e., at most places it is differentiable but not everywhere. In this case, depending on the application, the derivative may not be meaningful. Do I manually compute the Jacobian ie. for a change in each x, find the change in each of the y's? Yes, you could do that, which results in a locally differentiable function. That way, I could then use this formula dL / dx = (dL / dy) * (dy / dx) --> where dL / dy was given and the Jacobian dy / dx was manually computed across all combinations of x and y? It then boils into a matrix multiplication for the last step to calculate dL / dx. Is this correct? Yes and yes. To obtain more meaningful gradients for sorting and ranking through a relaxation of the sorting function, the following recent line of work has covered differentiable sorting: https://arxiv.org/abs/2203.09630 https://arxiv.org/abs/2105.04019 https://arxiv.org/abs/2002.08871 https://arxiv.org/abs/1905.11885 https://arxiv.org/abs/1903.08850 To obtain more meaningful gradients for general algorithms, the following works might be interesting for you: https://arxiv.org/abs/2110.05651 https://arxiv.org/abs/2002.08676
