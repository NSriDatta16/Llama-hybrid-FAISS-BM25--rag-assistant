[site]: crossvalidated
[post_id]: 347525
[parent_id]: 347511
[tags]: 
I am not sure if I understand your question correctly, but I will try paraphrasing it and then answering: Why cannot we minimize a differentiable cost function by computing its derivative and setting it to zero? The problem is, even if a cost function is differentiable (necessary for gradient descent anyways), it does not mean that the equation $\frac{\partial \mathcal L(w)}{dw}=0$ has an analytical solution. A typical examples are neural networks. Note that this has already been asked before: Why use gradient descent with neural networks?
