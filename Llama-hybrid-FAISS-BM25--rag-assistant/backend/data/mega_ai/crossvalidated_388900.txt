[site]: crossvalidated
[post_id]: 388900
[parent_id]: 388891
[tags]: 
$k$ in $k$ -NN is a hyperparameter to tune. You can try rules of thumb , like $k=\sqrt{n}$ , where $n$ is a sample size, but better plot $k$ vs error metric as computed on validation set. The choice of the metric will depend on your needs, it can be any metric, or combination of metrics, that is appropriate for your problem (e.g. precision, recall, AUC). Since it is quite primitive algorithm, that simply does majority votes (or averages in $k$ -NN regression) given the nearest neighbors, it will overfit with smaller values of $k$ , so this is something you should consider. On another hand, plotting training set error in here would show you exactly this relation and would lead to pretty obvious conclusions, so while people usually plot it, it isn't that helpful as with more advanced algorithms.
