[site]: crossvalidated
[post_id]: 472431
[parent_id]: 
[tags]: 
Training of a deep Artificial Neural Network

I have few doubts related to training a neural network with more parameters (weights and biases) than number of data points. I know there exists discussion (on this platform) related to training such a neural network as in: Fitting a neural network with more parameters than observations , Can one (theoretically) train a neural network with fewer training samples than weights? etc. Broadly I understand that NN use different regularization criteria (such as early stopping in MATLAB) to prevent overfitting. However, I still do not understand: How is it possible to 'estimate' (not 'uniquely determine' because that is not possible) the parameters when no. of parameters (P) > no. of data points (K). I think I am missing some basic understanding of determination of weights by minimizing the error using backpropagation (making partial derivatives with respect to all parameters 0). How does all the weights get tuned from its initial guess if P>K? Is it not wise to only use a NN configuration (no. of hidden layers and nodes in each hidden layer) that has P One of the main reasons for Q2 is that certain metrics such as AIC penalize a complex model with larger number of parameters than an equally good model with fewer parameters. So if we want to apply AIC to compare different ML models, a deep NN would always be penalized heavily and wont be reommended by AIC?
