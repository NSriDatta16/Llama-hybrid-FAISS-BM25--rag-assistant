[site]: datascience
[post_id]: 85552
[parent_id]: 
[tags]: 
Logistic Regression performs better on longer texts

I trained the LogisticRegression model with TF-IDF (both birgams and unigrams) and while predicting class it revealed that in longer texts (up to 3000 symbols)it works better that if I use short (+-100 symbols) texts. I assume that the reason is that bigram weights "work better" on longer texts, but it doesn't help me to understand why. So any advice what to read to clarify this situation is welcome
