[site]: crossvalidated
[post_id]: 179135
[parent_id]: 179113
[tags]: 
I think that the process outlined in your upper diagram more accurately summarizes the subset called supervised learning. It's commonly presented in terms of function inference, the task of determining a (usually noisy) function from data. (See e.g. wikipedia .) Though, it may not fully reflect all supervised methods. Take for example k nearest neighbors . When asked to classify a new instance, it queries the data for those nearest to the new one, and classifies or predicts, often using something like a vote or distance-weighted average. Importantly, it doesn't do any upfront learning to output a model. More, PCA doesn't really fit this definition. It's a dimensionality-reduction technique that doesn't predict anything unknown about the data; it just projects it into a space of smaller dimension. If you use all of the principal components, you've simply expressed the data differently, and know nothing more or less about it than you would prior to using PCA. (Dimensionality reduction is many times categorized as unsupervised learning.) For an idea of how one might define a common architecture, I'll share the definition that really helped me unify many topics in machine learning under a single umbrella. From Tom Mitchell in Machine Learning : A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P , if its performance at tasks in T , as measured by P , improves with experience E . For me, it's very intuitive to specify machine learning applications using this definition. For example: A program learns to predict housing prices (task) if the sum of squared error of its predictions (performance measure) decreases as its given more data (experience). A drone learns to efficiently order package deliveries and select routes if its speed improves as it delivers more of them. The second example illustrates how this definition is broad enough to cover problems where training data doesn't come in the form of input-output tuples. I find it immensely useful for building a conceptual space that includes supervised learning, optimization and reinforcement learning. (I confess that it doesn't help me unify unsupervised learning, which I'm content to think of as either a precursor to some other learning task, or as an extension of descriptive statistics.)
