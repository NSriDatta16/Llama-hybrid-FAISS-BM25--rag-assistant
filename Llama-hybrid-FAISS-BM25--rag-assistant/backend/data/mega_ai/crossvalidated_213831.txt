[site]: crossvalidated
[post_id]: 213831
[parent_id]: 210835
[tags]: 
@Wolfgang already gave a great answer. I want to expand on it a little to show that you can also arrive at the estimated ICC of 0.75 in his example dataset by literally implementing the intuitive algorithm of randomly selecting many pairs of $y$ values -- where the members of each pair come from the same group -- and then simply computing their correlation. And then this same procedure can easily be applied to datasets with groups of any size, as I'll also show. First we load @Wolfgang's dataset (not shown here). Now let's define a simple R function that takes a data.frame and returns a single randomly selected pair of observations from the same group: get_random_pair Here's an example of what we get if we call this function 10 times on @Wolfgang's dataset: test Now to estimate the ICC, we just call this function a large number of times and then compute the correlation between the two columns. random_pairs This same procedure can be applied, with no modifications at all, to datasets with groups of any size. For example, let's create a dataset consisting of 100 groups of 100 observations each, with the true ICC set to 0.75 as in @Wolfgang's example. set.seed(12345) group_effects Estimating the ICC based on the variance components from a mixed model, we get: library("lme4") mod And if we apply the random pairing procedure, we get random_pairs which closely agrees with the variance component estimate. Note that while the random pairing procedure is kind of intuitive, and didactically useful, the method illustrated by @Wolfgang is actually a lot smarter. For a dataset like this one of size 100*100, the number of unique within-group pairings (not including self-pairings) is 505,000 -- a big but not astronomical number -- so it is totally possible for us to compute the correlation of the fully exhausted set of all possible pairings, rather than needing to sample randomly from the dataset. Here's a function to retrieve all possible pairings for the general case with groups of any size: get_all_pairs Now if we apply this function to the 100*100 dataset and compute the correlation, we get: cor(get_all_pairs(dat)) # [,1] [,2] # [1,] 1.0000000 0.7504817 # [2,] 0.7504817 1.0000000 Which agrees well with the other two estimates, and compared to the random pairing procedure, is much faster to compute, and should also be a more efficient estimate in the sense of having less variance.
