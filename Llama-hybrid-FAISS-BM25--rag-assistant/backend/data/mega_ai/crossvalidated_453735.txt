[site]: crossvalidated
[post_id]: 453735
[parent_id]: 453150
[tags]: 
Is it impossible to train an agent on a changing environment if the observation space is only its current position (but not the whole level) No, it is possible to learn an optimal policy for the average environment: To answer this question, let's try to identify the optimal policy first analytically: According to the linked file: The start is always in the top left, the goal is always in the bottom right corner. The reward is 1 if we reach the goal, and 0 if we fall into a hole. The action space is the cardinal directions, lets assume that the ice is not slippery. The rest of the tiles are random, but it is guaranteed that we have a possible path from start to finish. So what is the optimal policy here? As we can not know the location of the holes, we might as well ignore them, and it becomes apparent that any policy that moves towards the bottom right, i.e. only moves to the bottom and to the right is optimal. This is quickly illustrated in the colab I wrote for the task , which shows that we can learn a policy that is far better than a random agent, even when switching the environment each time. The learned policy also looks almost as expected, the top right corner was however visited so infrequently, that the policy is not optimal there: Learned policy (Down, Right, Left, Up): ['D', 'D', 'D', 'U'] ['D', 'D', 'D', 'D'] ['D', 'D', 'D', 'D'] ['R', 'R', 'R', 'D'] Is it possible to create an agent that can somehow generalise and learn to navigate every random level he is confronted with? No, the best we can do given only the position as observation is to learn an agent that performs optimally on the average task, as above. If yes, is the solution to give the agent more information about the state (i.e. a bigger observation space) (if yes, what kind of information could be given additionally? the whole level would be too much because then other algorithms could be used, or not?) As long as the information is enough to infer the position of the holes, we can learn an optimal policy on every new map, as such the whole map provides sufficient information.
