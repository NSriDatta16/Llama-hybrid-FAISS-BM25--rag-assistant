[site]: crossvalidated
[post_id]: 1756
[parent_id]: 841
[tags]: 
The most naive approach I can think of is to regress $Y_i$ vs $X_i$ as $Y_i \sim \hat{m}X_i + \hat{b}$, then perform a $t$-test on the hypothesis $m = 1$. See t-test for regression slope . A less naive approach is the Morgan-Pitman test. Let $U_i = X_i - Y_i, V_i = X_i + Y_i,$ then perform a test of the Pearson Correlation coefficient of $U_i$ vs $V_i$. (One can do this simply using the Fisher R-Z transform , which gives the confidence intervals around the sample Pearson coefficient, or via a bootstrap.) If you are using R, and don't want to have to code everything yourself, I would use bootdpci from Wilcox' Robust Stats package, WRS. (see Wilcox' page .)
