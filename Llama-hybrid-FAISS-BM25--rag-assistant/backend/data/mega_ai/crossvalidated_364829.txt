[site]: crossvalidated
[post_id]: 364829
[parent_id]: 364827
[tags]: 
Trees really do not depend on scaling, and a linear boosting algorithm shouldn't either. However, we have to keep in mind XGBoost is a Gradient Boosting algorithm that tries to optimize a loss function based on the addition of models, through gradient descent. Why is this important? Gradient descent update rules operate based on the estimation of an optimal direction for updating plus a step length for a step in this direction. The length of this step isn't obvious at all, and in XGBoost it's estimated based on a second-order expansion of the objective function gradient . I have to investigate it further, but I guess the reason might lie on this optimization.
