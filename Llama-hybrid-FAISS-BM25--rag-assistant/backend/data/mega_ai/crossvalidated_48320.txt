[site]: crossvalidated
[post_id]: 48320
[parent_id]: 48306
[tags]: 
To give precise definitions, let $X_1, \ldots, X_n$ be real valued random variables. Stationarity is usually only defined if we think of the index of the variables as time . In this case the sequence of random variables is stationary of $X_1, \ldots, X_{n-1}$ has the same distribution as $X_2, \ldots, X_n$. This implies, in particular, that $X_i$ for $i = 1, \ldots, n$ all have the same marginal distribution and thus the same marginal mean and variance (given that they have finite second moment). The meaning of heteroscedasticity can depend on the context. If the marginal variances of the $X_i$'s change with $i$ (even if the mean is constant) the random variables are called heteroscedastic in the sense of not being homoscedastic. In regression analysis we usually consider the variance of the response conditionally on the regressors, and we define heteroscedasticity as a non-constant conditional variance. In time series analysis, where the terminology conditional heteroscedasticity is common, the interest is typically in the variance of $X_k$ conditionally on $X_{k-1}, \ldots, X_1$. If this conditional variance is non-constant we have conditional heteroscedasticity. The ARCH (autoregressive conditional heteroscedasticity) model is the most famous example of a stationary time series model with non-constant conditional variance. Heteroscedasticity (conditional heteroscedasticity in particular) does not imply non-stationarity in general. Stationarity is important for a number of reasons. One simple statistical consequence is that the average $$\frac{1}{n} \sum_{i=1}^n f(X_i)$$ is then an unbiased estimator of the expectation $E f(X_1)$ (and assuming ergodicity , which is slightly more than stationarity and often assumed implicitly, the average is a consistent estimator of the expectation for $n \to \infty$). The importance of heteroscedasticity (or homoscedasticity) is, from a statistical point of view, related to the assessment of statistical uncertainty e.g. the computation of confidence intervals. If computations are carried out under an assumption of homoscedasticity while the data actually shows heteroscedasticity, the resulting confidence intervals can be misleading.
