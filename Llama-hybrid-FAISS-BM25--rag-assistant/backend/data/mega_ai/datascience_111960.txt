[site]: datascience
[post_id]: 111960
[parent_id]: 111959
[tags]: 
It does not. Gradient descent is not immune to local minima in non-convex function optimization. Nevertheless, the noise introduced by stochastic gradient descent (SGD) helps escaping local minima. Other hyperparameters, like the learning rate, momentum, etc, also help. You can check sections 4.1 and 4.2 of Stochastic Gradient Learning in Neural Networks for detailed explanations and mathematical formulation of SGD and its convergence properties.
