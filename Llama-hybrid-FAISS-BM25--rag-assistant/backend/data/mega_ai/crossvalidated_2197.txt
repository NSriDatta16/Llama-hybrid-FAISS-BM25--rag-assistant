[site]: crossvalidated
[post_id]: 2197
[parent_id]: 
[tags]: 
Precedent for Bootstrap-like procedure with "invented" data?

Let's say I have a dataset with 1000 observations in 10 variables, "A" through "J." I have 1000 responses/measures for each of the first 8 variables, through "H," but only the first 500 observations for "I" are not missing, and only the last 500 observations for "J" are not missing -- there are no observations for which I have measures of both of the last two variables, I and J. Thus, if I calculate (pairwise) correlations, I have a full correlation matrix, with only the correlation between I and J missing. Let's say I want to run Principal Component Analysis, or some other such scaling procedure on this correlation matrix. What I think I would like to do is: Randomly generate (perhaps from some distribution on [-1, 1], or perhaps via sampling from existing values in the rest of the correlation matrix) an "invented" correlation between I and J. Put that in the correlation matrix. Run PCA on the correlation matrix with this invented value. Repeat steps 1 - 3 some large number of times. Assess the collective results of this large number of PCAs, looking at the mean and variance of the loadings, scores, eigenvalues, etc., based on the "pseudo-bootstrapped" iterations. Questions: Is there a better way to handle (a) missing value(s) in the correlation matrix? Is there any precedent for replacing such (a) missing value(s) with random invented values? If so, what is it called? Is this related to the bootstrap? Thanks a lot, in advance. Edit: Question 4. Is this a defensible approach to imputation?
