[site]: crossvalidated
[post_id]: 88882
[parent_id]: 88880
[tags]: 
PCA is a simple mathematical transformation. If you change the signs of the component(s), you do not change the variance that is contained in the first component. Moreover, when you change the signs, the weights ( prcomp( ... )$rotation ) also change the sign, so the interpretation stays exactly the same: set.seed( 999 ) a shows PC1 PC2 X1.10 0.9900908 0.1404287 rnorm.10. -0.1404287 0.9900908 and pca2$loadings show Loadings: Comp.1 Comp.2 X1.10 -0.99 -0.14 rnorm.10. 0.14 -0.99 Comp.1 Comp.2 SS loadings 1.0 1.0 Proportion Var 0.5 0.5 Cumulative Var 0.5 1.0 So, why does the interpretation stays the same? You do the PCA regression of y on component 1. In the first version ( prcomp ), say the coefficient is positive: the larger the component 1, the larger the y. What does it mean when it comes to the original variables? Since the weight of the variable 1 ( 1:10 in a) is positive, that shows that the larger the variable 1, the larger the y. Now use the second version ( princomp ). Since the component has the sign changed, the larger the y, the smaller the component 1 -- the coefficient of y Possibly, the easiest way to see that is to use a biplot. library( pca3d ) pca2d( pca1, biplot= TRUE, shape= 19, col= "black" ) shows The same biplot for the second variant shows pca2d( pca2$scores, biplot= pca2$loadings[,], shape= 19, col= "black" ) As you see, the images are rotated by 180Â°. However, the relation between the weights / loadings (the red arrows) and the data points (the black dots) is exactly the same; thus, the interpretation of the components is unchanged.
