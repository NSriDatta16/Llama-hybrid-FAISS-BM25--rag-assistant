[site]: crossvalidated
[post_id]: 185834
[parent_id]: 
[tags]: 
Does it makes sense to perturb images on-the-run while training CNNs?

I'm training a convolutional neural network by adding small perturbations (like rotation and shifting) to the images each time it gets a batch data. I think a better way of doing this could be to generate a bigger data set by oversampling with these transformations before training, however I don't have much RAM to store a bigger data set, so I decided to transform them on the run. But it means that now I'm supplying a different data set in each epoch. I think it should make sense, and the training, in fact, converges to a better performance, but it watches a zig-zaggy road, which is understandable I guess. So am I safe to assume that I'm not making a mistake? Also is there a better way for this, i.e. training a generalizable classifier under a small data set and limited computational resources? Thanks for any help !
