[site]: datascience
[post_id]: 9364
[parent_id]: 
[tags]: 
Hypertuning XGBoost parameters

XGBoost have been doing a great job, when it comes to dealing with both categorical and continuous dependant variables. But, how do I select the optimized parameters for an XGBoost problem? This is how I applied the parameters for a recent Kaggle problem: param All I do to experiment is randomly select (with intuition) another set of parameters for improving on the result. Is there anyway I automate the selection of optimized(best) set of parameters? (Answers can be in any language. I'm just looking for the technique)
