ave proposed a multi-step approach to protecting machine learning. Threat modeling – Formalize the attackers goals and capabilities with respect to the target system. Attack simulation – Formalize the optimization problem the attacker tries to solve according to possible attack strategies. Attack impact evaluation Countermeasure design Noise detection (For evasion based attack) Information laundering – Alter the information received by adversaries (for model stealing attacks) Mechanisms A number of defense mechanisms against evasion, poisoning, and privacy attacks have been proposed, including: Secure learning algorithms Byzantine-resilient algorithms Multiple classifier systems AI-written algorithms. AIs that explore the training environment; for example, in image recognition, actively navigating a 3D environment rather than passively scanning a fixed set of 2D images. Privacy-preserving learning Ladder algorithm for Kaggle-style competitions Game theoretic models Sanitizing training data Adversarial training Backdoor detection algorithms Gradient masking/obfuscation techniques: to prevent the adversary exploiting the gradient in white-box attacks. This family of defenses is deemed unreliable as these models are still vulnerable to black-box attacks or can be circumvented in other ways. Ensembles of models have been proposed in literature, which have shown to be ineffective against evasion attacks but effective against data poisoning attacks. See also Pattern recognition Fawkes (image cloaking software) Generative adversarial network References External links MITRE ATLAS: Adversarial Threat Landscape for Artificial-Intelligence Systems NIST 8269 Draft: A Taxonomy and Terminology of Adversarial Machine Learning NIPS 2007 Workshop on Machine Learning in Adversarial Environments for Computer Security AlfaSVMLib Archived 2020-09-24 at the Wayback Machine – Adversarial Label Flip Attacks against Support Vector Machines Laskov, Pavel; Lippmann, Richard (2010). "Machine learning in adversarial environments". Machine Learning. 81 (2): 115–119. doi:10.1007/s10994-010-5207-6. S2CID 12567278. Dagstuhl Perspectives Workshop on "Machine Learning Methods for Computer Security" Workshop on Artificial Intelligence and Security, (AISec) Series