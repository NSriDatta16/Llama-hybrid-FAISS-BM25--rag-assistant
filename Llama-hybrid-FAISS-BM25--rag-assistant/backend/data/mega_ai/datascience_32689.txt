[site]: datascience
[post_id]: 32689
[parent_id]: 32399
[tags]: 
What I found while digging down: As neural network continues to train, it gets more confident in predictions. And while it overfits at certain scenarios, it makes log_loss to increase exponentially, since it was more confident while mistaken. Meanwhile, if I look at mean average error, I would see that it continues to decrease, which indicates that network still learns something. Which at the end make f1_score to increase. One important thing I noticed: it will not always hold true, because there is a tradeoff between overfitting/learning new, and MAE is weaker metric in binary outcomes.
