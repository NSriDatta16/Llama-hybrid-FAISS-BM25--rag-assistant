[site]: crossvalidated
[post_id]: 187223
[parent_id]: 185858
[tags]: 
KL is widely used in machine learning. The two main ways I know compression: compressing a document is actually all about finding a good generative model for it. Given that the true model has probability distribution $p(x)$ while you use the approximate $q(x)$, you will have to use excess bits to encode a sequence of $X$ values. The extra cost you pay is KL(p,q) bayesian approximate inference: bayesian methods are great for ML, but it's also extremely computationally expensive to obtain the posterior. Two solutions: either you use sampling methods (MCMC, gibbs, etc) OR you use approximate inference methods which aim at finding a simple (for example Gaussian) approximation to the posterior. Most approximate inference methods refer to KL in some way: so called "variational" (this name sucks) methods minimize KL(q,p), etc. Approximate inference is present in a lot of machine learning research, so KL is too
