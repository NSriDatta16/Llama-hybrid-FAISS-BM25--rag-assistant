[site]: crossvalidated
[post_id]: 616453
[parent_id]: 
[tags]: 
An Alternative In-sample-goodness-of-fit Measure in Machine Learning Methods for Estimating Heterogeneous Causal Effects Paper

In 3.5.2 of Machine Learning Methods for Estimating Heterogeneous Causal Effects by Susan Athey and Guido Imbens it is stated: "If the models include an intercept, as they usually do, most estimation methods would ensure that the average of $$(Y_i^{tr,obs}-\hat{\mu}(X_i^{tr})).\hat{\mu}(X_i^{tr})$$ would be equal to zero," I have the following questions: The paper uses trees for regressions in which case I am not sure what is meant by an intercept. The paper does state "Although we focus in the current paper mostly on regression tree methods (Breiman, Friedman, Olshen, and Stone, 1984), the methods extend to other approaches such as Lasso (Tibshirani, 1996), and support vector machines (Vapnik, 1998, 2010)." Update: My answer to (1) is that the intercept for any regressor is just the mean response when all predictors are zero. In trying to attempt to prove the above expression is zero I make the assumption that the residuals are independent of the fitted values. However if I understand correctly this is another check applied to linear regression so I am not sure if this can be applied to trees as well. My version of the proof based on the assumption stated in this point is: $$E[(Y_i^{tr,obs}-\hat{\mu}(X_i^{tr})).\hat{\mu}(X_i^{tr})]=E[Y_i^{tr,obs}-\hat{\mu}(X_i^{tr})].E[\hat{\mu}(X_i^{tr})]$$ If the estimator is unbiased and samples are drawn i.i.d then using linearity of expectations: $$E[Y_i^{tr,obs}-\hat{\mu}(X_i^{tr})] = E[Y_i^{tr,obs}] - E[\hat{\mu}(X_i^{tr})]=E[Y^{tr,obs}] - E[Y^{tr,obs}] = 0$$ therefore $$E[Y_i^{tr,obs}-\hat{\mu}(X_i^{tr})]=0 \implies E[(Y_i^{tr,obs}-\hat{\mu}(X_i^{tr})).\hat{\mu}(X_i^{tr})]=0$$ Is this reasoning correct and can we assume fitted values are independent of the residuals? This part is also not clear to me: " $$-\frac{1}{N^{tr}}\sum^{N^{tr}}_{i=0}((Y_i^{tr,obs})^2-\hat{\mu}^2(X_i^{tr})).$$ To interpret this, because the first component does not depend on the estimator being used, a model fits better according to this criteria if it yields higher variance predictions." If the above expression in this point is used as goodness of fit and the aim is to maximize the goodness of fit then this translates to maximizing $$\sum^{N^{tr}}_{i=0}\hat{\mu}^2(X_i^{tr})$$ but is $$\frac{1}{N^{tr}}\sum^{N^{tr}}_{i=0}\hat{\mu}^2(X_i^{tr})$$ the same as $$var(\hat{\mu}(X_i^{tr}))$$
