[site]: crossvalidated
[post_id]: 221636
[parent_id]: 221621
[tags]: 
First: yes, your assumptions that combining such models might be beneficial are reasonable (will likely reduce the overall variance). Consider also combining different models types if this is possible in your setup. There would be different options to do so, like: Model averaging Bagging Boosting You could easily implement model averaging yourself, independently of any framework you use for training your models (regression: average the prediction of a new sample from all models you obtained during training; classification: derive class probabilities from the amount of classifications for each class instead). Bagging is essentially the same, but uses subsets of training data (=not all samples) to train each model, which makes all the models different. Again, you could easily do this yourself. After training the models, the prediction process is the same as for model averaging. Only for boosting you probably need to use a framework instead, as it is a bit more complicated - so there you would need to look into the details of the scikit.learn API if it allows for combining different models with different hyperparameters. One more thing: be aware that by doing something like this after model evaluation, you will likely still overfit your problem. You should think about incorporating the parametrization of your ensemble (nr. of models etc.) into your evaluation routine as well.
