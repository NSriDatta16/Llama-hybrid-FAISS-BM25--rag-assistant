[site]: stackoverflow
[post_id]: 2255921
[parent_id]: 2255833
[tags]: 
What do you do to the data ? My answer: nothing . SVMs are designed to handle high-dimensional data. I'm working on a research problem right now that involves supervised classification using SVMs. Along with finding sources on the Internet, I did my own experiments on the impact of dimensionality reduction prior to classification. Preprocessing the features using PCA/LDA did not significantly increase classification accuracy of the SVM. To me, this totally makes sense from the way SVMs work. Let x be an m-dimensional feature vector. Let y = Ax where y is in R^n and x is in R^m for n Here is one discussion that debates the use of PCA before SVM: link What you can do is change your SVM parameters. For example, with libsvm link , the parameters C and gamma are crucially important to classification success. The libsvm faq, particularly this entry link , contains more helpful tips. Among them: Scale your features before classification. Try to obtain balanced classes. If impossible, then penalize one class more than the other. See more references on SVM imbalance. Check the SVM parameters. Try many combinations to arrive at the best one. Use the RBF kernel first. It almost always works best (computationally speaking). Almost forgot... before testing, cross validate ! EDIT: Let me just add this "data point." I recently did another large-scale experiment using the SVM with PCA preprocessing on four exclusive data sets. PCA did not improve the classification results for any choice of reduced dimensionality. The original data with simple diagonal scaling (for each feature, subtract mean and divide by standard deviation) performed better. I'm not making any broad conclusion -- just sharing this one experiment. Maybe on different data, PCA can help.
