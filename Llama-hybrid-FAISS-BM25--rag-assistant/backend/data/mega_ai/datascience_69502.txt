[site]: datascience
[post_id]: 69502
[parent_id]: 69442
[tags]: 
If you are performing a binary classification task then the following code might help you. from sklearn.model_selection import GridSearchCV for hyper-parameter tuning. from sklearn.linear_model import SGDClassifier by default, it fits a linear support vector machine (SVM) from sklearn.metrics import roc_curve, auc The function roc_curve computes the receiver operating characteristic curve or ROC curve. model = SGDClassifier(loss='hinge',alpha = alpha_hyperparameter_bow,penalty=penalty_hyperparameter_bow,class_weight='balanced') model.fit(x_train, y_train) # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class, not the predicted outputs. y_train_pred = model.decision_function(x_train) y_test_pred = model.decision_function(x_test) The former, decision_function, finds the distance to the separating hyperplane. For example, a(n) SVM classifier finds hyperplanes separating the space into areas associated with classification outcomes. This function, given a point, finds the distance to the separators. https://stackoverflow.com/questions/36543137/whats-the-difference-between-predict-proba-and-decision-function-in-scikit-lear train_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred) test_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred) plt.grid() plt.plot(train_fpr, train_tpr, label=" AUC TRAIN ="+str(auc(train_fpr, train_tpr))) plt.plot(test_fpr, test_tpr, label=" AUC TEST ="+str(auc(test_fpr, test_tpr))) plt.plot([0,1],[0,1],'g--') plt.legend() plt.xlabel("False Positive Rate") plt.ylabel("True Positive Rate") plt.title("AUC(ROC curve)") plt.grid(color='black', linestyle='-', linewidth=0.5) plt.show()
