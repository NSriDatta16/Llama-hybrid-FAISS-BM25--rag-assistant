[site]: datascience
[post_id]: 112182
[parent_id]: 112119
[tags]: 
The values depend on the activation function, but I usually see very small positive values close to 0: this is probably due to the small probabilities between tokens. You can see the behavior of the activation functions here: https://mlfromscratch.com/activation-functions-explained/#/ Gelu is nowadays the most popular one. On the other hand, the optimizers are also crucial in the weight values. Stochastic Gradient Descent or Adam are the most popular ones, but AdamW seems to perform even better because it has a progressive learning rate decreasing with learning iterations: https://huggingface.co/docs/transformers/main_classes/optimizer_schedules
