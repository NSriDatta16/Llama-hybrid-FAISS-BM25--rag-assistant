[site]: crossvalidated
[post_id]: 225734
[parent_id]: 
[tags]: 
Why isn't variance defined as the difference between every value following each other?

This may be a simple question for many but here it is: Why isn't variance defined as the difference between every value following each other instead of the difference to the average of the values? This would be the more logical choice to me, I guess I'm obviously overseeing some disadvantages. Thanks EDIT: Let me rephrase as clearly as possible. This is what I mean: Assume you have a range of numbers, ordered: 1,2,3,4,5 Calculate and sum up (the absolute) differences (continuously, between every following value, not pairwise) between values (without using the average). Divide by number of differences (Follow-up: would the answer be different if the numbers were un-ordered) -> What are the disadvantages of this approach compared to the standard formula for variance?
