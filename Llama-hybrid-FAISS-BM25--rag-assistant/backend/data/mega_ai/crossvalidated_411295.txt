[site]: crossvalidated
[post_id]: 411295
[parent_id]: 411290
[tags]: 
tl;dr Yes, you could build a voting classifier using the array of your trained estimators. I don't recommend that, especially as I don't know much about the use case. My typical workflow There could be multiple candidates to build a decent estimator in this use case (Let's say RandomForest , xgboost , SVM ) Build an estimator using each of them with their optimal hyper parameters ( here is one way to do that ) Evaluate the above estimators for predictive power and how well they generalize across different test sets (I would use Cross-validation here) Some/all candidate estimators seem to fair well ( this would be helpful here ) Build an ensemble of multiple estimators for better predictive and/or generalizing power. (I could use VotingClassifier here) VotingClassifier is mainly used to vote among different techniques, you could of course also use it as you said though. Following is a quick take on usage of cross validation and also about voting. Thoughts about your use case Cross-validation is mainly used as a way to check for over-fit. Assuming you have determined the optimal hyper parameters of your classification technique (Let's assume random forest for now), you would then want to see if the model generalizes well across different test sets. Cross-validation in your case would build k estimators (assuming k-fold CV) and then you could check the predictive power and variance of the technique on your data as following: mean of the quality measure. Higher, the better standard_deviation of the quality measure. Lower, the better A high mean and low standard deviation of your quality measure would mean the modeling technique is doing well. Assuming the above measure looks good, you could then conclude that random forest with the hyper parameters used is a decent candidate model. If you think your classification technique does well enough, then you could also build a voting classifier using the k estimators from CV.
