[site]: crossvalidated
[post_id]: 445799
[parent_id]: 444959
[tags]: 
Although this software-specific question is technically off-topic here, I do note that NA values were handled differently in the two calls: na.omit for the glo_mo model, na.fail for the dredge function call. AIC and BIC are discussed in detail on this page . There is some dispute about whether these approaches are correct for comparing the non-nested models you would evaluate in approaches like dredge uses. I'm not an expert on that, see this page for an introduction. Purely automated model selection is generally to be avoided , particularly when there is subject-matter knowledge available to guide your model building. Note that in logistic regression there is a danger in omitting any predictor that is expected to be related to outcome . The problem you face, as noted in the answer from Dimitris Rizopoulos, is that you do not have enough cases to evaluate 5 predictors plus the random-effect term. The usual rule of thumb for logistic regression is 10-20 members of the minority class per predictor that you are evaluating unless you are using some type of penalization. If you have 67 total observations then you have at most 33 members of the minority class. You could consider ridge regression to include all predictors with penalization to avoid overfitting. If you insist on a model-selection strategy, then you should validate your model-building approach by repeating all steps including the predictor-selection steps on multiple bootstrap samples from your data, and testing performance of each bootstrap-derived model on the full data set.
