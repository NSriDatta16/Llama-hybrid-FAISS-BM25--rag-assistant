[site]: datascience
[post_id]: 114289
[parent_id]: 
[tags]: 
Understanding MDP variants and "model-free" RL algorithms

RL is based on MDPs. But MDPs have other useful variants such as Semi MDP (variable time), POMDP (partially observable states) etc. Some industrial problems seem to be better suited for SMDP and/or POMDPs. For example optimizing maintenance, the time between maintenance events is variable. And, some states of equipment may not available for measurement directly -- hence we have "partially observable" states. If I decide to use a model-free algorithm such as DDPG or PPO, the theory of which I think is based on MDP -- am I comprising on the state or model definition? Will it create a less efficient agent? Of course I may not have a choice as creating an accurate environment is out of question. But just trying to understand. Appreciate your thoughts.
