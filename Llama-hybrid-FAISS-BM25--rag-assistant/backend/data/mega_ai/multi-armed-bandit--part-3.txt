splaystyle \delta \in (0,1)} , the objective is to identify the arm with the highest expected reward a ⋆ ∈ arg ⁡ max k μ k {\displaystyle a^{\star }\in \arg \max _{k}\mu _{k}} with the least possible amount of trials and with probability of error P ( a ^ τ ≠ a ⋆ ) ≤ δ {\displaystyle \mathbb {P} ({\hat {a}}_{\tau }\neq a^{\star })\leq \delta } . For example using a decision rule, we could use m 1 {\displaystyle m_{1}} where m {\displaystyle m} is the machine no.1 (you can use a different variable respectively) and 1 {\displaystyle 1} is the amount for each time an attempt is made at pulling the lever, where ∫ ∑ m 1 , m 2 , ( . . . ) = M {\displaystyle \int \sum m_{1},m_{2},(...)=M} , identify M {\displaystyle M} as the sum of each attempts m 1 + m 2 {\displaystyle m_{1}+m_{2}} , (...) as needed, and from there you can get a ratio, sum or mean as quantitative probability and sample your formulation for each slots. You can also do ∫ ∑ k ∝ i N − ( n j ) {\displaystyle \int \sum _{k\propto _{i}}^{N}-(n_{j})} where m 1 + m 2 {\displaystyle m1+m2} equal to each a unique machine slot, x , y {\displaystyle x,y} is the amount each time the lever is triggered, N {\displaystyle N} is the sum of ( m 1 x , y ) + ( m 2 x , y ) ( . . . ) {\displaystyle (m1_{x},_{y})+(m2_{x},_{y})(...)} , k {\displaystyle k} would be the total available amount in your possession, k {\displaystyle k} is relative to N {\displaystyle N} where N = n ( n a , b ) , ( n 1 a , b ) , ( n 2 a , b ) {\displaystyle N=n(n_{a},b),(n1_{a},b),(n2_{a},b)} reduced n j {\displaystyle n_{j}} as the sum of each gain or loss from a , b {\displaystyle a,b} (for example, suppose you have 100$ that is defined as n {\displaystyle n} , and a {\displaystyle a} would be a gain, b {\displaystyle b} is equal to a loss. From there you get your results either positive or negative to add for N {\displaystyle N} with your own specific rule) and i {\displaystyle i} as the maximum you are willing to spend. It is possible to express this construction using a combination of multiple algebraic formulation, as mentioned above where you can limit with T {\displaystyle T} for, or in time and so on. Bandit strategies A major breakthrough was the construction of optimal population selection strategies, or policies (that possess uniformly maximum convergence rate to the population with highest mean) in the work described below. Optimal solutions In the paper "Asymptotically efficient adaptive allocation rules", Lai and Robbins (following papers of Robbins and his co-workers going back to Robbins in the year 1952) constructed convergent population selection policies that possess the fastest rate of convergence (to the population with highest mean) for the case that the population reward distributions are the one-parameter exponential family. Then, in Katehakis and Robbins simplifications of the policy and the main proof were given for the case of normal populations with known variances. The next notable progress was obtained by Burnetas and Katehakis in the paper "Optimal adaptive policies for sequential allocation problems", where index based policies with uniformly maximum convergence rate were constructed, under more general conditions that include the case in which the distributions of outcomes from each population depend on a vector of unknown parameters. Burnetas and Katehakis (1996) also provided an explicit solution for the important case in which the distributions of outcomes follow arbitrary (i.e., non-parametric) discrete, univariate distributions. Later in "Optimal adaptive policies for Markov decision processes" Burnetas and Katehakis studied the much larger model of Markov Decision Processes under partial information, where the transition law and/or the expected one period rewards may depend on unknown parameters. In this work, the authors constructed an explicit form for a class of adaptive policies with uniformly maximum convergence rate properties for the total expected finite horizon rewa