[site]: crossvalidated
[post_id]: 247333
[parent_id]: 
[tags]: 
MCMC Metropolis - confused about prior distributions

I am studying the Metropolis-Hasting algorithm (from the book Understanding Computational Bayesian Statistics- Chap.6-7 ) in its two different formulations: Random Walk Candidate Density; Independent Candidate Density. And I am fitting a 6-parameter model to experimental data. I am a bit confused about the role of the prior distribution but, I want to first state you the problem as it is stated in this book, clarifying the terminology. In order to sample from the posterior distribution, we need to define: The unscaled posterior density $g(\theta|y)$ where $y$ is my dataset and $\theta$ is my model parameter (or the vector of the model parameters); A candidate distribution $q(\theta,\theta')$ that generates the candidate $\theta'$ given the starting value $\theta$. These two distributions allow the calculation of the probability of moving from the actual value to the candidate: $P(\theta,\theta')=min[1,\frac{g(\theta'|y) q(\theta',\theta)}{g(\theta|y) q(\theta,\theta')}]$ In my case, the unscaled posterior is the Likelihood function. In the Metropolis-Hastings with Random Walk Candidate Density, the candidate is a symmetric distribution centred in the actual value $\theta$. Thus, $q(\theta,\theta') = q(\theta',\theta)$ and the candidate density can be omitted in the calculation of the probability of acceptance. In the Metropolis-Hastings with an Independent Candidate Density, the candidate distribution is fixed and should "dominate the target". In case of a non Informative Bayesian Inference, the Prior distributions of the parameter have no effect. But, in case of an Informative Inference, is it right to quantify the effect of the prior distribution $prior(\theta)$ of the parameter(s) in this way? $P(\theta,\theta')=min[1,\frac{g(\theta'|y) q(\theta',\theta) prior(\theta)}{g(\theta|y) q(\theta,\theta') prior(\theta)}]$
