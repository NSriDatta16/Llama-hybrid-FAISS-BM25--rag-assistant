[site]: crossvalidated
[post_id]: 325589
[parent_id]: 325585
[tags]: 
In the case of K-means and the elbow algorithm, you can make the error as low as you want by increasing K continuously, until the error reaches $0$ when $K = N$ (the number of data points in your data set). But that doesn't make sense, since the whole point of K-means (or any other clustering algorithm) is to group the data into groups to measure their similarity: A grouping where each data point is in its own cluster doesn't tell us anything at all about the data. Hence the elbow method, which tries to find when there is a "sudden" change in the level of variation in the error as K increases. The idea is that while there is a rapid decrease in the error from one value of K to the next, then significant additional information is gained by increasing K. But when the error starts to decrease slowly, then the level of information gain from one K to the next is no longer significant, so you stop increasing K. There is a problem with the elbow method however: We're not guaranteed to have a sudden change in our error curve - there is no "elbow" in the data, so how can you use the elbow method? There are several approaches to avoid this problem: One way is to use a different clustering algorithm that doesn't require a fixed number of clusters (such as DBSCAN or Affinity Propagation). Another way is to use X-means: This is a variation K-means, where instead of minimizing the error, you try to minimize the BIC (Bayesian Information Criterion): The BIC includes an error term, but it also includes a term which penalizes the number of parameters in the model. A lower error will lower the BIC, but a higher K will increase the BIC. So when trying to minimize the BIC, you end up striking a balance between lowest error possible and lowest value of K possible.
