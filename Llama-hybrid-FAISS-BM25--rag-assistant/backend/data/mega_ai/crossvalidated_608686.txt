[site]: crossvalidated
[post_id]: 608686
[parent_id]: 
[tags]: 
Why is chi-squared divided by degrees of freedom a measure of model goodness of fit?

I was recently at a seminar where a statistician used chi-squared divided by the degrees of freedom (DF) as an assessment of model performance in a logistic regression. They suggested that having a $\frac{\chi^2}{DF}$ close to 1 suggested good performance in the model. I am having some difficulty googling more information around this measure of goodness of fit and was hoping to get some guidance or resources. Once source I found stated the following: "A $\chi^2$ statistic with k degrees of freedom, d.f., is the sum of the squares of k random unit-normal deviates. Therefore its expected value is k, and its model variance is 2k. This provides the convenient feature that the expected value of a mean-square statistic, i.e., a $\chi^2$ statistic divided by its d.f. is 1." ~ https://www.rasch.org/rmt/rmt171n.htm And there is some mentioned of dividing test statistics by degrees of freedom in SAS documentation guidelines . But I am having difficulty finding a more thorough and satisfactory answer. Why does having $\frac{\chi^2}{DF}$ close to 1 suggest decent good model performance (goodness-of-fit)? Related but different questions on $\frac{\chi^2}{DF}$ are found here and here .
