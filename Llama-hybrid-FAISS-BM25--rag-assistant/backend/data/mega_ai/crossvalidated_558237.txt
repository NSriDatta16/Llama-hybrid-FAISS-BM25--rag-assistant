[site]: crossvalidated
[post_id]: 558237
[parent_id]: 558221
[tags]: 
The amount of potential information contained in a signal is what we call the entropy, usually denoted by $H$ and defined as follows: $$H(X) = − \sum_X{P(X) \log P(X)}$$ The "surprise" is (or you could say proportional to, but it doesn't matter) the inverse of the probability and you can see that the example you mentioned. So the entropy can be written as $$ H(X) = \sum_X{P(X) \log\frac{1}{P(X)}} $$ which is a weighted average of the "surprise" i.e. if the probability is low, then there is a lot of surprise, and consequently a lot of information is conveyed by telling you that X happened. Now to answer the question, YES the surprise is not at maximum when the probability is 0.5 and the event occurs or not (which makes sense since we would be equally surprised by the occurrence and non-occurrence) BUT the entropy/information is a weighted average over the two events Thus, entropy essentially measures the “average surprise” or “average uncertainty” of a random variable. If the distribution $P(X)$ is highly peaked around one value, then we will rarely be surprised by this variable; hence it would be incapable of conveying much information. If on the other hand $P(X)$ is uniformly distributed, then we will be most surprised on average by this variable; hence it could potentially convey a lot of information.
