[site]: datascience
[post_id]: 15183
[parent_id]: 14568
[tags]: 
@hbaderts described the whole workflow perfectly. However, it may not make any sense in case you are completely new to this idea. Therefore, I am going to explain it in layman's way (therefore, I will be omitting details): Think of the deep networks as a function to transform your data. Example of transformations include normalization, taking log of data etc. The deep networks you are training has multiple layers. Each of these layers are trained using some kind of learning algorithm. For the first layer, you pass the original data as the input and try to get a function that will give you back those "same original data" as the output. However, you don't get the perfect output. Therefore, you are getting a transformed version of your input as the output of the first layer. Now, for the second layer, you take those "transformed data" and pass them as input and repeat the whole learning process. You keep doing that for all the layers in your deep network. At the last layer, what you get is a "transformed version" of your original input data. This can be thought of higher level abstraction of your original input data. Note that, you have not used the labels/output in your deep network yet. Therefore, everything till this point is unsupervised learning. This is called layer-wise pre-training. Now, you want to train a classifier/regression model and this is a supervised learning problem. The way you achieve that goal is by taking the "final transformed version" of your original input from the last layer in your deep network and use them as the input to any classifier (e.g. knn classifier / softmax classifier / logistic regression etc). This is called stacking. When you are training this last-step classifier/learner, you propagate all your learning in the complete network. This ensures that your are able to learn from the labels/outputs and modify the learned layer-wise parameters accordingly. So, once you have your generative model trained, take the output of your generative model and use that as input to a classifier/learner. Let the error flow through the whole network as the learning continues so that you can modify the layer-wise parameter learned in earlier steps.
