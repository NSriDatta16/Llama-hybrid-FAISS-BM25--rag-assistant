[site]: crossvalidated
[post_id]: 519220
[parent_id]: 518834
[tags]: 
A practical approach to take while training your model might be: Set a maximum number of iterations before the episode ends, some value that's probably more than sufficient to achieve an optimal solution. Let the agent terminate the episode at any point in time. Add a small negative reward at every point in time. This should encourage the agent to terminate the episode as soon as it can't make sufficient progress to outweigh the small penalty, without picking any hard limit. At test time: You can go with a simulated annealing approach: Introduce a temperature variable $T$ which decays from some initial value down to 0. Whenever the agent picks a rewrite rule, apply it with probability 1 if it's an improvement, and $\exp(s/T)$ otherwise, where $s$ is how much worse the solution got. Otherwise ignore the rewrite and resample from the agent policy (if it's a deterministic policy, you could add noise somehow). If the agent decides to terminate the episode, ignore it (that was a helpful heuristic for training, but needn't be applied here).
