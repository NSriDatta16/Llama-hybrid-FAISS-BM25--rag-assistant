[site]: datascience
[post_id]: 98401
[parent_id]: 98002
[tags]: 
I guess there is an assumption that $A(q, K, V)$ represents similarity between q and the words in the sentence. Actually $A(q, K, V)$ is encoding the relationships from q to the words in the sentence. The information about which other words in particular are important to the word under consideration has been already encoded by $q \cdot k^{ }$ . Suppose there is a sentence i love sushi , $v^{ }$ is a positionally-encoded word-embedding vector for each word, and the word under consideration $q = v^{ }$ which corresponds with love . $v^{ } = i\\ v^{ } = love\\ v^{ } = sushi$ $scale(i)=\frac{exp(q.k^{ })}{\sum_{j} exp(q.k^{ })}$ quantifies how strong the connection is between love and $v^{ }$ . $\frac{exp(q.k^{ })}{\sum_{j} exp(q.k^{ })}v^{ }$ or $scale(i)v^{ }$ is scaling each vector $v^{ }$ proportionally based on how strongly related $v^{ }$ is to love . Aggregating all the scaled vectors $\sum_{i} scale(i)v^{ }$ creates a new vector $A$ , which is encoding the relationships from q to the words in the sentence. The vector $A$ goes through the neural network and the back-propagation on the scaling and aggregation operations are how the Transformer learns the relations between q and the sentence.
