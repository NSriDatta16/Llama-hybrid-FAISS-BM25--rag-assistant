[site]: crossvalidated
[post_id]: 72481
[parent_id]: 
[tags]: 
Convergence theorem for Gibbs sampling

The convergence theorem for Gibbs sampling states: Given a random vector $X$ with components $X_1,X_2,...X_K$ and the knowledge about the conditional distribution of $X_k$ we can find the actual distribution using Gibbs Sampling infinitly often. The exact theorem as stated by book ( Neural Networks and Learning Machines ): The random variable $X_k^{(n)}$ converges in distribution to the true probability distributions of $X_k$ for k=1,2,...,K as n approaches infinity $\lim_{n \rightarrow \infty}P(X^{(n)}_k \leq x | X(0)) = P_{X_k}(x) $ for $k > = 1,2,...,K$ where $P_{X_k}(x)$ is the marginal cumulative distribution function of $X_k$ While doing research on this, for a deeper understanding, I ran across this answer. Which explains quite well how to pick a single sample using the Method, but I am not able to extend/modify it to fit the convergence theorem, as the result of the given example is one sample (spell) and not a final/actual probability distribution. Therefore, how do I have to modify that example to fit the convergence theorem?
