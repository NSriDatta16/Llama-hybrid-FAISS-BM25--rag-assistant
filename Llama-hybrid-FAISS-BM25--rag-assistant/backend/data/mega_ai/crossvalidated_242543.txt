[site]: crossvalidated
[post_id]: 242543
[parent_id]: 240920
[tags]: 
This question is really broad. Depends on the data and model, it can be a good practice and can be bad. The overall idea is to think about the "complexity of data and model". We may need to review Bias and Variance trade-off, i.e., when under-fitting and over-fitting will happen and how to detect it. How to know if a learning curve from SVM model suffers from bias or variance? To your question about turning on samples: In general, the more complex the data is, with limited sample size, the harder to get "representative" samples. If the data is "really complex", and samples are "representative", using samples to tune parameters is a bad practice. The way to fix is trying larger amount of samples, and use complicated models (such as neural network). You can see my answer is really unclear in may parts, this is because it is hard to say the complexity of data and model, and how much samples are needed to to be "representative".
