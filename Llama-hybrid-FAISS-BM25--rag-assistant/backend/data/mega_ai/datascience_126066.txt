[site]: datascience
[post_id]: 126066
[parent_id]: 
[tags]: 
Averaging Weights of Identical LSTM Models for a Unified Global Model

I'm currently working on a project where I have several pre-trained LSTM models, all with the same architecture. My goal is to combine these models into a single global model by averaging their weights. However, I'm encountering an issue where the output of the global model does not make sense, even though the trend captured by the model seems accurate. Has anyone here dealt with a similar challenge, and if so, could you share how you managed to average the weights while maintaining real results? I will greatful for any advice. def average_weights(model_list): avg_weights = model_list[0].get_weights() for model in model_list[1:]: for i, layer_weights in enumerate(model.get_weights()): avg_weights[i] += layer_weights print(len(avg_weights)) for i in range(len(avg_weights)): avg_weights[i] /= len(model_list) return avg_weights
