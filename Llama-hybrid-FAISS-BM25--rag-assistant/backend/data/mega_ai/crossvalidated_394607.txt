[site]: crossvalidated
[post_id]: 394607
[parent_id]: 
[tags]: 
Feeding a set of words to neural network

I know that LSTM (or also CNN) is good choice to input a word sequence such as $(x_1, x_2, ... x_T)$ . But I could not find a good practice to input a set of vectors $\{x_i\}_{i=1,2,3,...,T}$ , where the order of vector does not have sense. Let's say, if you have a sentence like "The president gave up to build a wall.'' on document classification, you may feed the sentence to LSTM to know the category of the sentence such as "politics''. On the other hand, if you have a set of words like {wall, president}, how do you feed these words into a model? One naive way is calculating embeddings of those words and summing up the embedding vectors to get a feature to feed a model. But, I do not think this feature does not work well.
