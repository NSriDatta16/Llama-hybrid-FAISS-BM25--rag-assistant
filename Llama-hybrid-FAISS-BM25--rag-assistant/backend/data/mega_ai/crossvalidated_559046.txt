[site]: crossvalidated
[post_id]: 559046
[parent_id]: 559042
[tags]: 
The problem here is that your dataset is a bit of an outlier in the population of datasets from the stated data generating process, in that the feature is more correlated with the target than average. If the feature and target were randomly generated with equal probabilities, then the probability that the feature matches the target is a Bernoulli trial with probability 0.5. In this case, out of 2000 samples, there are 1033 = 515 + (1000 - 482) "successes", and I think the probability of there being 1033 or more "successes" from 2000 trials is only about 0.073. Now the statistical distribution for the "test set" in each fold of the leave-one-out data is the same as that for the "training set", so the test data is also an unlikely sample from the true data generating mechanism, so it can't be expected to give the "right" answer. There is nothing wrong with leave-one-out cross-validation, the problem lies with being unlucky in the sample of data you have obtained. One thing you might want to do is to compute the Bayes factor comparing the two hypotheses, which I suspect would tell you that the evidence is not strongly in favour of either hypothesis, which is reasonable as the difference is only in 33 of the 2000 observations. Another way of looking at this would be to use NHSTs and consider the power of the test, which would probably be rather low. If you took either model as H0 in the test, you would be unable to reject it, which indicates that there isn't enough data to be confident of a difference in performance between the models. Essentially you need a lot of data to be able to be "confident" that a very small effect (such as this one) is not a random artefact. The key point is that cross-validation can provide evidence of over-fitting, it does not itself provide a reliable indicator of over-fitting. You need to consider the uncertainties involved.
