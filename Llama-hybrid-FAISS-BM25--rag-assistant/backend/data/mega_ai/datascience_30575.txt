[site]: datascience
[post_id]: 30575
[parent_id]: 
[tags]: 
Which learning algorithms to use in what order - dimensionality reduction, bayesian network structure, regression?

The data is a huge set of observations of dozens of variables, all (potentially, somehow) related to a dichotomous outcome variable, and all (potentially) correlated to each other, or to unknown / unobserved things. I think that "merely" applying logistic regressions or any related extension or improvement (such as Classification And Regression Trees etc.) might not be sufficient, because there is a "basic" bayesian network structure that seems to exist (based on expert opinion) - this means that the observed variables are somehow describing distinct nodes in a bayesian network, but the exact formula HOW they describe these nodes is unclear, as is the eventual predictive power of these nodes towards the outcome. Here is this exact problem in a highly simplified example: We have two runners who are going to compete in a 400m sprint against each other. We have a dataset of observations of thousands of such competitions with two runners, each observation containing, for each of the two runners, (a) age, (b) size, (c) number of training runs in the past 6 weeks and (d) average mileage per training run in the past 6 weeks. Now, we believe that (a) and (b) will determine (e) "talent for sprinting" and (c) and (d) will determine (f) "current form", and that (e) and (f) together will give the probability of winning - but we do not know HOW (a) and (b) determine (e) or how (c) and (d) determine (f) (e.g., is it "age times size" or is it "age times 2 while age lower than 45" etc.). Further, we also do not know how (e) and (f) together determine the probability of winning... Is there any "known" approach to such a problem, i.e. a way to determine the formula that connects the initial variables to the later variables that determine the outcome, and then further determining the predictive power of the resulting network? Or is that unnecessary (i.e. using the original variables only in some algorithm - which one? - should give the same result)? Or is it impossible (too many unknown things)? What should I do / read about / learn to solve this problem? And in addition, is there a way to CHECK whether my original assumption (that (a) and (b) determine (e) etc.) was correct, e.g. via some sort of clustering algorithm, PCA, ...? What would you do to approach such a question? Thank you very much indeed in advance for helping a rather new data science learner by pointing me into the right direction!
