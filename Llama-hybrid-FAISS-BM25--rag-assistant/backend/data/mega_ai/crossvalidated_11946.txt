[site]: crossvalidated
[post_id]: 11946
[parent_id]: 2516
[tags]: 
I think its a problem of most significance tests having some general undefined class of implicit alternatives to the null, which we never know. Often these classes may contain some sort of "sure thing" hypothesis, in which the data fits perfectly (i.e. a hypothesis of the form $H_{ST}:d_{1}=1.23,d_{2}=1.11,\dots$ where $d_{i}$ is the ith data point). The value of the log likelihood is such an example of a significance test which has this property. But one is usually not interested in these sure thing hypothesis. If you think about what you actually want to do with the hypothesis test, you will soon recognise that you should only reject the null hypothesis if you have something better to replace it with. Even if your null does not explain the data, there is no use in throwing it out, unless you have a replacement. Now would you always replace the null with the "sure thing" hypothesis? Probably not, because you can't use these "sure thing" hypothesis to generalise beyond your data set. It's not much more than printing out your data. So, what you should do is specify the hypothesis that you would actually be interested in acting on if they were true. Then do the appropriate test for comparing those alternatives to each other - and not to some irrelevant class of hypothesis which you know to be false or unusable. Take the simple case of testing the normal mean. Now the true difference may be small, but adopting a position similar to that in @keith's answer, we simply test the mean at various discrete values that are of interest to us. So for example, we could have $H_{0}:\mu=0$ vs $H_{1}:\mu\in\{\pm 1,\pm 2,\pm 3,\pm 4,\pm 5,\pm 6\}$. The problem then transfers to looking at what level do we want to do these tests at. This has a relation to the idea of effect size: at what level of graininess would have an influence on your decision making? This may call for steps of size $0.5$ or $100$ or something else, depending on the meaning of the test and of the parameters. For instance if you were comparing the average wealth of two groups, would anyone care if there was a difference of two dollars, even if it was 10,000 standard errors away from zero? I know I wouldn't. The conclusion is basically that you need to specify your hypothesis space - those hypothesis that you are actually interested in. It seems that with big data, this becomes a very important thing to do, simply because your data has so much resolving power. It also seems like it is important to compare like hypothesis - point with point, compound with compound - to get well behaved results.
