[site]: crossvalidated
[post_id]: 575132
[parent_id]: 
[tags]: 
How to estimate the time and memory requirement for Sparse and variational Gaussian Process (SVGP) with minibatch approach

I'm implementing Sparse and variational Gaussian Process (SVGP) using GPflow library using minibatches. I've been trying to search for details regarding the time and memory complexities e.g. from this source , but it remains a bit unclear how to get these estimates. I basically have a data matrix consisting from about two million data points with 28 input features, that is a data set $\mathcal{D}_{2,000,000 \times 28}$ matrix consisting from Python float-type numbers (64 bit = 8 bytes), and so my data matrix size in memory is roughly about 500 MB. In my machine learning process, I perform model hyperparameter and feature selection using cross-validation and genetic algorithm. I also take advantage of multicore processing for speeding up the computations. What I would like have, is an estimate on how much computational time and memory my process takes (I'm using a supercomputer so I need to estimate the required resources). For estimating the complexity of my process, I need to estimate how much time and memory one minibatch -based gradient descent -like step in the SVGP takes, as I've understood that SVGP is an iterative process, desgined to tackle the scalability of Gaussian Process model by using a similar process as in deep neural networks, that is, batch learning (using subset of the data in each training iteration). So my questions is: Given that I have a subset of my data matrix of size $N\times 28$ ( $N$ data points, $28$ features), how can I estimate 1) how long one iteration of SVGP fitting takes and 2) how much memory I need in this iteration. Basically I'm asking the time and memory complexities here per minibatch iteration.
