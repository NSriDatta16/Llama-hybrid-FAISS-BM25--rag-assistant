[site]: crossvalidated
[post_id]: 267925
[parent_id]: 
[tags]: 
How to add self-defined regularization term in ridge regression?

In typical ridge regression or (lasso in a more general sense). We have a regularization term after the MSE error term $cost = \sum{(y_{i} - \beta x_{i})^{2}} + \lambda\sum{|\beta|^{2}}$ However, I believe in most engineering cases, there could be a-priori knowledge about the $\beta$. For instance, house price should be positively correlated with medium household income. This relationship should NOT go negative from a business logic perspective: people making too much money and house price drops, this doesn't make sense. I am sure people must have looked Ridge/Lasso from a Bayesian perspective: $cost = \sum{(y_{i} - \beta x_{i})^{2}} + \sum{\lambda_{i}|\beta_{i}-\hat{\beta}_{i}|^{2}}$ where $\hat{\beta}$ carries information about what $\beta$ should be. This looks nice on paper, but in practice, $\beta$ might not be Gaussian. Using the house-price example, I only know $\beta$ is positive. So the penalty term is not smooth/differentiable. Therefore, it will be hard to tackle the problem using a gradient approach I am wondering if there is a 'universal' approach that can solve above general-purpose regression. Can someone share any insights here?
