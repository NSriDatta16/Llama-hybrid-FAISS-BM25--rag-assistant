[site]: crossvalidated
[post_id]: 230287
[parent_id]: 230057
[tags]: 
As you note, your definition of data-dredging implicitly excludes RF there's no significance testing taking place during the RF model construction. Your implicit question here, especially in the second paragraph, is whether RF is "trustworthy" in some sense. Fortunately, we have several reasons to believe that they are. A regular old vanilla decision tree is constructed as a forward-wise greedy algorithm that looks for the best split at every node. This is very unstable because the greedy nature of it means that a slight perturbation of the data could cause a dramatically different tree to be built: measurement noise or the exclusion of a small number of data points can entirely change the tree. In that sense, it is a high-variance estimator. On the other hand, the RF algorithm is explicitly designed to counter both of these in three ways (1) bootstrapping the sample (2) randomly subsetting features for each split and (3) averaging over many trees. This, of course, is all theoretical. As a practical matter, we can approximate how well a RF model is doing by making the standard out-of-sample measurements you make for any other model. To your specific claims, though, note that the "data dredging" definition doesn't mention "bias" at all. It doesn't even state that the relationships uncovered are invalid. It just says that data dredging is characterized by not forming a hypothesis formed ahead of time. I would say that by fitting the RF model, you've implicitly specified that you think the features you've presented have some bearing on the outcome, and that the relationship might be well-approximated by several decision trees (vice the relationship being linear as in a regression model). The RF hypothesis is much more flexible than in the linear regression context (the linear regression model has the hypothesis that the response is a multilinear function of the features), and I think that's more or less the source of your concern. I might also point out that by using this specific definition of data-dredging, you've implicitly adopted the conventions of significance testing and p-values as being an important part of the enterprise. By contrast, in my work, which has involved a considerable number of RF models, doesn't care about significance testing (at least in the sense of regression coefficients). It's difficult for me to directly address your claim that RF is "biased" because I don't think you mean statistical bias -- I think you mean that RF is untrustworthy because it engages in data dredging, but in a way that I can't quantify. Maybe you could sharpen your language here? I don't think that RF engages in data dredging, so I don't think that RF is untrustworthy for that reason. (I don't grant your premise.) The question here is one of model calibration . You've outlined the problem of the RF predicted probabilities not matching the true probability of the event in some reference population which is external to your training data. The simplest way to correct this would be to collect samples from whatever reference population you desire (and disjoint from your training data) and construct a logistic regression model which takes the RF output as the independent variable. Under specific conditions which are outlined extensively elsewhere, logistic regression yields calibrated probability estimates of the outcome.
