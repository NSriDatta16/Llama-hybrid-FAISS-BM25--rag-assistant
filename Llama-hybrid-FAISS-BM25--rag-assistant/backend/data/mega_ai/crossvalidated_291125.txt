[site]: crossvalidated
[post_id]: 291125
[parent_id]: 291081
[tags]: 
Standardization (or normalization using Student's t statistics, to use a longer name) is a good practice if you don't want your machine learning algorithm to improperly assign a larger relevance to the variable that has the largest variability in absolute terms, and sometimes just for easier interpretation of the machine learning algorithm parameters. That being said, some algorithms strictly require standardization (es Principal Component Analysis) and some other do not at all (Random Forests). In order to reply to your problem, you should provide some summary of data to understand the numerical range. If you are using python+pandas, that would be df.describe() , in R, that would be summary(df) . You should standardize the variables that are continuous, or quasi continuous (integers over a wide range). In your case, from what I see you should standardize the duration, bytes, counts and error rates. You are free to normalize the same variables instead of standardizing them, but it would not make sense to standardize some and normalize others.
