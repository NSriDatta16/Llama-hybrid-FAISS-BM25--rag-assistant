[site]: datascience
[post_id]: 30015
[parent_id]: 
[tags]: 
RNN: why Wx + Uh instead of W[x,h]

Traditionally, a state for RNN is computed as $$h_t = \sigma(W\cdot \vec x + U\cdot \vec h_{t-1} + \vec b)$$ For a RNN, why to add-up the terms $(Wx + Uh_{t-1})$ instead of just having a single matrix times a concatenated vector: $$W_m[x, h_{t-1}]$$ where $[...]$ is concatenation. In other words, we would end up with a long vector like $\{x_1, x_2, x_3, h_{1,t-1}, h_{2,t-1}, h_{3,t-1} \}$ multiplied by $W_m$. It seems like the second approach would have a significantly larger matrix, which has more elements than $W$ and $U$ combined. Does that mean $W$ and $U$ are a simplification, what do we lose by using them, and adding up the results?
