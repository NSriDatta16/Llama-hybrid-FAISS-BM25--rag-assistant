[site]: datascience
[post_id]: 109043
[parent_id]: 109041
[tags]: 
Is one-hot encoding an option? It seems like no, due to the high cardinality of your feature, it might result in the course of dimensionality problems if your sample size is small and also if you are using mean decrease impurity as a measure of feature importance you have to consider the bias to high cardinality features. So to avoid having that many categories ~200, you could group them. You could for example check the distribution on the train set of this feature and group those whose representativeness is below x% as OTHERS category. If I just use the original values, 1001, 1002, etc., will random forest think that department 1002 is higher than department 1001? Yes, it will be treated as a continuous feature and then a nonsense order will be established. What options do I have? The simplest, yet most efficient way of encoding categorical features is Target encoding , in short: Target encoding is the process of replacing a categorical value with the mean of the target variable. Any non-categorical columns are automatically dropped by the target encoder model. You could remove the target value of the observation $i$ to avoid leakage. There is another alternative named WOE, which is a more sophisticated encoding in logarithmic scale that is highly used in credit scoring None of those encodings will increase the feature dimension. Finally, if you are using python, both aforementioned and many other encodings are available in CategoryEncoders package .
