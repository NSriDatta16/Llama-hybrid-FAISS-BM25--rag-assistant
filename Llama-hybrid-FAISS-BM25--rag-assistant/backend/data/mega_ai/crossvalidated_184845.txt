[site]: crossvalidated
[post_id]: 184845
[parent_id]: 184804
[tags]: 
This situation can occur in logistic regression with a large number of candidate predictors, even with thousands of cases as you have here. With 500 binary predictor variables you have $2^{500} = 3.3 \times 10^{150} $ possible combinations of predictors. This is approximately the square of the number of atoms in the universe . You have found a particular combination of predictors that completely distinguish the two groups in this particular data set . This result is unlikely to generalize to new cases, as you recognize. For ideas about how to proceed, follow the hauck-donner-effect tag on this site. This page and this page are good places to start. In response to comment: If you set aside separate training and test sets, either you got really lucky in finding a reproducible linear separation in your training set, or some predictor is acting as a proxy for readmission. You are unlikely, unfortunately, to have solved the vexing problem of predicting readmission , where an AUC of 0.8 might be considered spectacular. Make sure that the "normal features" you are examining are only those with data available at the time of prior discharge (before the readmission/no-readmission cutoff time). With 500 features (a lot for clinical data) it's possible that one slipped through that would only have a particular value for readmitted patients. Follow the suggestions by @spdrnl in the comments to examine the individual predictors and combinations in more detail.
