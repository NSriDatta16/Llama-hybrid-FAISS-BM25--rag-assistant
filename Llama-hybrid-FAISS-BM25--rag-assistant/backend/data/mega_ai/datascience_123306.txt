[site]: datascience
[post_id]: 123306
[parent_id]: 
[tags]: 
Replication of XGBoost's binary:logistic loss

I am trying to replicate XGBoost's logistic loss function as a first step before implementing my own custom loss functions. Following from here and looking at the original code in git repository , I can deduct that the logistic loss first and second derivatives are given by: def logloss(predt: np.ndarray, dtrain: xgb.DMatrix) -> Tuple[np.ndarray, np.ndarray]: def gradiant(p: np.ndarray, dtrain: xgb.DMatrix) -> np.ndarray: y = dtrain.get_label() return p - y def hessian(p: np.ndarray, dtrain: xgb.DMatrix) -> np.ndarray: y = dtrain.get_label() return p * (1 - p) predt[predt In order to have a comparable evaluation metric for the purpose of the exercise, I am also following from the post above and define the log-loss as an evaluation metric as well: def logloss_metric(predt, dmat): y=dmat.get_label() if isinstance(dmat,xgb.DMatrix) else dmat predt=np.clip(predt,10e-7,1-10e-7) return 'logloss_myerror', - np.mean(y * np.log(predt) + (1-y) * np.log(1-predt)) I can then train two different model configurations, one with the default binary:logistic and one with my custom defined function, however both with the same evaluation metric. As follows: import numpy as np import xgboost as xgb from typing import Tuple from sklearn.datasets import load_breast_cancer import pandas as pd data = load_breast_cancer() X = pd.DataFrame(data['data'], columns=data['feature_names']) y = pd.DataFrame(data['target']) dtrain = xgb.DMatrix(X, y, feature_names=X.columns) clf0 = xgb.train({'tree_method': 'hist', 'seed': 1994, 'objective': 'binary:logistic', 'disable_default_eval_metric': 0}, # any other tree method is fine. dtrain=dtrain, num_boost_round=20, maximize=False, feval=logloss_metric, evals=[(dtrain,'train')]) [0] train-error:0.02109 train-logloss_myerror:0.51087 [1] train-error:0.01055 train-logloss_myerror:0.07307 [2] train-error:0.00879 train-logloss_myerror:0.07185 [3] train-error:0.00527 train-logloss_myerror:0.06112 [4] train-error:0.00527 train-logloss_myerror:0.03483 [5] train-error:0.00527 train-logloss_myerror:0.03002 [6] train-error:0.00527 train-logloss_myerror:0.02753 [7] train-error:0.00527 train-logloss_myerror:0.02704 [8] train-error:0.00527 train-logloss_myerror:0.02697 [9] train-error:0.00527 train-logloss_myerror:0.02646 [10] train-error:0.00527 train-logloss_myerror:0.02530 [11] train-error:0.00527 train-logloss_myerror:0.00331 [12] train-error:0.00527 train-logloss_myerror:0.00160 [13] train-error:0.00176 train-logloss_myerror:0.00078 [14] train-error:0.00176 train-logloss_myerror:0.00024 [15] train-error:0.00176 train-logloss_myerror:0.00001 [16] train-error:0.00000 train-logloss_myerror:0.00000 [17] train-error:0.00000 train-logloss_myerror:0.00000 [18] train-error:0.00000 train-logloss_myerror:0.00000 [19] train-error:0.00000 train-logloss_myerror:0.00000 clf1 = xgb.train({'tree_method': 'hist', 'seed': 1994, 'disable_default_eval_metric': 0}, # any other tree method is fine. dtrain=dtrain, num_boost_round=20, obj=logloss, feval=logloss_metric, maximize=False, evals=[(dtrain,'train')]) [0] train-rmse:0.14949 train-logloss_myerror:0.10558 [1] train-rmse:0.13031 train-logloss_myerror:0.08458 [2] train-rmse:0.12824 train-logloss_myerror:0.08155 [3] train-rmse:0.12494 train-logloss_myerror:0.07799 [4] train-rmse:0.12487 train-logloss_myerror:0.07805 [5] train-rmse:0.12485 train-logloss_myerror:0.07800 [6] train-rmse:0.12485 train-logloss_myerror:0.07801 [7] train-rmse:0.12485 train-logloss_myerror:0.07801 [8] train-rmse:0.12485 train-logloss_myerror:0.07801 [9] train-rmse:0.12485 train-logloss_myerror:0.07801 [10] train-rmse:0.12485 train-logloss_myerror:0.07801 [11] train-rmse:0.12485 train-logloss_myerror:0.07801 [12] train-rmse:0.12485 train-logloss_myerror:0.07801 [13] train-rmse:0.12485 train-logloss_myerror:0.07801 [14] train-rmse:0.12485 train-logloss_myerror:0.07801 [15] train-rmse:0.12485 train-logloss_myerror:0.07801 [16] train-rmse:0.12485 train-logloss_myerror:0.07801 [17] train-rmse:0.12485 train-logloss_myerror:0.07801 [18] train-rmse:0.12485 train-logloss_myerror:0.07801 [19] train-rmse:0.12485 train-logloss_myerror:0.07801 At the first iterations, both of them have completely different evaluations, and although my model clf1 seems to converge only after 3-4 iterations, it never reaches 0. I am posting the original code, as you can see the derivatives are the same. My classifier also seems to default the eval metric to rmse, which could mean that it recognizes the problem as a regression problem. Hence, I tried to also specify booster:gbtree , just in case. I have also tried to make eval_metric:auc , same thing happens. What is going on here? struct LogisticRegression { XGBOOST_DEVICE static bst_float PredTransform(bst_float x) { return common::Sigmoid(x); } XGBOOST_DEVICE static bool CheckLabel(bst_float x) { return x >= 0.0f && x EDIT: I have modified the derivatives such that the prediction output is transformed with a sigmoid, as follows: def sigmoid(x): return 1 / (1 + np.exp(-x)) def logloss(predt: np.ndarray, dtrain: xgb.DMatrix) -> Tuple[np.ndarray, np.ndarray]: def gradiant(p: np.ndarray, dtrain: xgb.DMatrix) -> np.ndarray: p = sigmoid(p) y = dtrain.get_label() return p - y def hessian(p: np.ndarray, dtrain: xgb.DMatrix) -> np.ndarray: p = sigmoid(p) y = dtrain.get_label() eps = 1e-16 hess = p * (1 - p) hess[hess This now works. Moreover, I have set up a native eval_metric just to be sure by passing it as an argument to xgb.train , in this case eval_metric='logloss' . To make sure that the problem does not get recognizes as a regression problem, I have set also booster:gbtree . Although this is now way closer to the native implementation and it actually converges, I still cannot match it and the convergence is not as smooth.
