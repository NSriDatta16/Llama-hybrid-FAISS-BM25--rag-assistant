[site]: crossvalidated
[post_id]: 236624
[parent_id]: 132639
[tags]: 
Out of the box, tSNE has a few hyperparameters, the main one being perplexity. Remember that heuristically, perplexity defines a notion of similarity for tSNE and a universal perplexity is used for all data-points. You could try generating a labelled dataset where each cluster has wildly different perplexity. This can be accomplished by doing a mixture of gaussians, with a wide range of different variances. I'm guessing this will also cause issues in the Barnes-Hut implementation of tSNE, which relies on quartiling data and using only nearest neighbours. tSNE also has an initial relaxation period, which attempts to pass clusters through each other. During this period, there is no penalty or repulsion. So for example, if your data looks a matted clump of noodles (each noodle representing a given cluster), you're gonna have a hard time calibrating the initial pass through, and I doubt tSNE will work well. In some sense I think this hints that tSNE will not work well if your data is woven together and initially resides in a low dimensional space, say 5. In general tSNE is good because of the "t" part, which resolves an outstanding issue in SNE of how to space points in lower dimensional spaces, compared to higher dimensions. It turns out that on average, the spacing of data points in higher dimensions behaves completely differently from lower dimensions. In particular, tSNE strongly advocates against using Gaussians to measure distances in lower dimensions, opting instead for the one dimensional $t$ distribution (i.e. the Cauchy Distribution) which has heaver tails and allows for more spread in the lower dimensional representation. So conceivably the "t" in tSNE could also be a hyperparameter, where instead you can choose different distributions (albeit at a high computational cost). You should think of tSNE as an unsupervised method of clustering, and as such there's zero reason to think that it's the only tool for the job. I think overall it can be a fantastic tool if calibrated right. However it is quite slow on large datasets and you might be better off using some optimized form of $k$-means for example, or even PCA, depending on how sparse the data is.
