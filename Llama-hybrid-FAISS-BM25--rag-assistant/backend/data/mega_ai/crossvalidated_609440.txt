[site]: crossvalidated
[post_id]: 609440
[parent_id]: 558970
[tags]: 
Depending on the situation, minimizing square loss might not even be the best way to estimate the mean. For instance, if data are Laplace-distributed, minimizing absolute loss gives an estimator that is, in many regards, better than minimizing square loss. Both are unbiased, but the minimization of absolute loss results in lower variance. This can be seen in a simulation. library(ggplot2) library(VGAM) library(quantreg) set.seed(2023) N $coef[1, 1] mae_based[i] coef[1] if (i %% 100 == 0){ print(paste( i/R*100, "% complete", sep = "" )) } } d_mse Visually, both have means around zero, which is the true value, but the MSE minimization results in considerably more variability in the estimates. This is not just a fluke of this simulation but can be shown mathematically for a Laplace distribution. This is just one situation where an alternative to MSE minimization is theoretically justified. Additionally, I demonstrate here that minimizing MSE need not produce a better out-of-sample MSE than minimizing MAE. This could be another justification for minimizing MAE instead of MSE. Finally, in "classification" problems like logistic regression, making the (reasonable) assumption of a Bernoulli conditional distribution means that minimizing the log-loss, not the square loss, corresponds with maximum likelihood estimation, giving another theoretical justification for minimizing a loss function other than square loss.
