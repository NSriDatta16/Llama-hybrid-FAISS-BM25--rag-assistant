[site]: datascience
[post_id]: 48192
[parent_id]: 
[tags]: 
Implementing back translation as a data augmentation for text classification

Since back translation English->other language -> English seems like quite a useful data augmentation technique , I wanted to experiment with it. E.g. it occurred to me that languages from very different language families (but very well supported for economic reasons such as Chinese, Russian, Spanish, Korean, Arabic...) could make for a diverse set of effects occurring in the back translation. Commercial translation APIs would be a straightforward way of doing this, but without free API key or budget from my organization (would not qualify as academic) that's quickly quite expensive for a private thing. Pretrained translation models would seem like an obvious alternative (I have a GPU for inference, but clearly that's not enough to train all the models from scratch), but I could e.g. not find those for any OpenNMT variant. Are there any recommendations from others that have used this approach?
