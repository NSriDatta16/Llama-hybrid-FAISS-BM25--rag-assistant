[site]: datascience
[post_id]: 104009
[parent_id]: 104003
[tags]: 
It's hard to say without seeing the distribution of the data, it would be useful if you added that, and described what the variables actually represent. If your data is really skewed, then even if a median of 0 sounds like the wrong thing to do, it is better than using the mean since doing so will place more emphasis on the outliers in your data. In general though, any form of imputation will be adding biases to your dataset, and if the number of missing samples is small then it may be worth ignoring those samples in your analysis. When you impute missing values with the mean, median or mode you are assuming that the thing you're imputing has no correlation with anything else in the dataset, which is not always true. Consider this example: x1 = [1,2,3,4] x2 = [1,4,?,16] y = [3, 8, 15, 24] For this toy example, $y = 2x_1 + x_2$ . We also know that $x_2 = x_1^2$ . Now suppose we wanted to do regression on X = [x1, x2] to determine the coefficients that give us the best y_hat . If we imputed x2 with the mean or the mode, i.e. 7 or 4 we'd be adding a bias into our analysis, since that doesn't consider any interaction from x1 ! Better methods would seek to learn how to predict the missing data from the other features. I've added some links for further reading: https://towardsdatascience.com/a-comprehensive-guide-to-data-imputation-e82eadc22609 https://en.wikipedia.org/wiki/Imputation_(statistics) http://num.pyro.ai/en/stable/tutorials/bayesian_imputation.html https://stats.idre.ucla.edu/wp-content/uploads/2016/02/multipleimputation.pdf https://scikit-learn.org/stable/modules/impute.html#impute
