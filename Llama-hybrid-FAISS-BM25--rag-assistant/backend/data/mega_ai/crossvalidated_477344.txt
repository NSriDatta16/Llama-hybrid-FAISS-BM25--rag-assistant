[site]: crossvalidated
[post_id]: 477344
[parent_id]: 
[tags]: 
How many repetitions to establish difference

(Real-world Context: I often make changes to code, which I suspect will have small impacts on the performance. i.e. improvements that lie within the natural variation of the code's runtime. How many reps of my perf test code do I need to run of the before/after code, in order to establish whether I have increased (or worse decreased!) the performance of my code? I think that the following precise mathematical question, is a reasonable proxy for that question, but I have no idea how to calculate the answer. Suppose I have 2 random variables, $X \sim N(0,\sigma)$ and $Y \sim N(\delta,\sigma)$ Suppose that I am going to take $N$ samples of $X$ and $Y$ . How many samples ( $N$ ) must I take, in order to be able to assert, with $p = 0.95$ confidence, whether $\delta > 0$ or $\delta . How many samples ( $N$ ) must I take, in order to be able to assert, with $p = 0.95$ confidence, that $|\delta| ?. Does it change things, if my variables are $X' \sim N(\mu,\sigma)$ and $Y' \sim N(\mu \pm \delta,\sigma)$ ? If it makes it easier to have the confidence threshold be arbitrary, feel free to leave it as $p$ . I'm assuming that once I've established the correct expression for $N_{enough}$ , then my test will be: Take $N_{enough}$ samples of each. Average each sample set. Compare the averages. Report "yes" is the average satisfies the goal. i.e. once I take "enough" samples the process is trivial, I just need to work out what is "enough".
