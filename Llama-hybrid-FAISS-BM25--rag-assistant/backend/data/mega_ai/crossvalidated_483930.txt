[site]: crossvalidated
[post_id]: 483930
[parent_id]: 
[tags]: 
what's the purpose of trying to have small model weights via regularization?

In machine learning, it is often advised to use weight regularization so that the model parameters don't grow big while training. I am not convinced that having small weights improves the model's ability to generalize. Why should a model have small weights? What happens when a model has 'big' weights? I would really appreciate it if you could give a simple example to illustrate your answer!
