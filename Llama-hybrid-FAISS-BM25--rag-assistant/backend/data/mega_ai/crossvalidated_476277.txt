[site]: crossvalidated
[post_id]: 476277
[parent_id]: 470219
[tags]: 
One statistical application of denoising autoencoders is multiple imputation: the autoencoder tries to compress the data to a low-dimensional signal (that isn't missing) plus noise (that's sometimes missing). Compared to either Bayesian data augmentation or the popular 'mice' algorithms, the autoencoders seem to scale better to large numbers of variables, and may potentially handle nonlinearity and interaction better. (This is still a research area, but it's a serious application.) Andrew Gelman writes about an early attempt here , and the current version of that specific project is here
