[site]: crossvalidated
[post_id]: 546651
[parent_id]: 546649
[tags]: 
The point of having a held-out test set is that your machine learning algorithm learned the rules from the data, so you cannot validate them on the same dataset, because you’d risk overfitting. If you derive your rule using exploratory data analysis or some kind of algorithm then the proper way would be to derive the rule using only training data and validate it using a test set. On another hand, if you made the rule a priori of looking at the data, just test it on the full dataset. Moreover, your rule sounds like a hypothesis to be tested, if yes, we go into the realm of statistics and hypothesis-testing, rather than building predictive models. If that’s the case, I’d recommend you consult a statistics handbook beforehand, because statistics often use their methodologies, different from machine learning. For example, if you tested multiple rules like this (“let’s try 20%”, “ok, now 22%”, …), this is a multiple testing problem , and you risk making false discoveries. You would “overfit”, but cross-validation would not help here.
