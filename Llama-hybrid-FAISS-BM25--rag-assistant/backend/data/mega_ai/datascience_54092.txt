[site]: datascience
[post_id]: 54092
[parent_id]: 53995
[tags]: 
For me embedding is used to represent big sparse matrix into smaller dimensions, where each dimension(feature) represent a meaningful association with other elements in the embedding matrix. Consider an example of NLP. Where each sentence broken down into words(also called token). Such set of different words make a vocabulary for NLP. Generally vocabulary have millions of words. All such words can be uniquely represented as OneHotEncoding. Demerits of OneHotEncoding representation of words: In case of large vocabulary, OneHotEncoding representation needs a big chunk of memory and computationally become very expensive. OneHotEncoding is used to represent categorical values, where each entity is independent to other one, whereas words in vocabulary represent some association in terms of similar meanings or in some other way. OneHotEncoding not utilizing that capability for NLP. In order to overcome both the issues, we use word Embedding, where each word represented in lesser dimension, where each dimension represent some sort of features and hence each dimension will have some values.
