[site]: crossvalidated
[post_id]: 357176
[parent_id]: 356023
[tags]: 
Let's generalize the question so that a key idea can be revealed. Let $Y$ and $R$ be independent random variables. Define $$X=f(Y)+R$$ for a specified (measurable) function $f$. For any $a\lt b,$ what is the conditional expectation $$E[X\mid a\le Y \le b]?$$ (This states that $f$ is the regression of $X$ on $Y$ with i.i.d. additive errors. ) The independence assumption makes this an easy question to answer, because the random variables $f(Y)$ and $R$ will still be independent, whence (by "taking out what is known") $$E[X\mid a\le Y \le b] = E[f(Y)+R\mid a\le Y \le b] = E[f(Y)\mid a \le Y \le b] + E[R].\tag{*}$$ This is very general. Let's specialize to the case $E[R]=0$ and $f$ is a linear transformation $$f(y) = \alpha\, y$$ for some constant number $\alpha.$ In this case $(*)$ simplifies to $$E[X\mid a\le Y \le b] = \alpha\, E[Y\mid a \le Y \le b].\tag{**}$$ The right hand side has a direct expression when the distribution of $Y,$ $F_Y,$ has a density $f_y,$ for then (by the definitions of conditional probability and expectation) $$E[Y\mid a \le Y \le b] = \frac{1}{F(b)-F(a)}\int_a^b y f_y(y) \mathrm{d}y.$$ The result is now immediate and obvious to anyone who has studied bivariate regression. (A good non-mathematical reference with all the necessary details is Freedman, Pisani, and Purves, Statistics [any edition].) The next section of this post is a review for those who might not have encountered this theory. In the case of the question, where $(x,y)$ is bivariate Normal with mean $(0,0),$ standard deviations $\sigma_x$ and $\sigma_y,$ and correlation $\rho,$ we know that the distribution of $(x,y)$ is the same as the distribution of $(X,Y)$ constructed as above where $Y$ has a Normal$(0,\sigma_y^2)$ distribution and $R$ independently has a Normal$(0,\tau^2)$ distribution (with $\tau$ to be found). The proof of this lies in the observations that Because $Y$ and $R$ are independent, the variance of $X = \rho\,Y+R$ equals $$\rho^2 \operatorname{Var}(Y) +\operatorname{Var}(R) =\rho^2\sigma^2_y + \tau^2.$$ Consequently, if we set $$\tau^2 = \sigma_x^2 - \rho^2\sigma_y^2,$$ the variance of $X$ will equal the variance of $x.$ The covariance $(X,Y)$ is $$\operatorname{Cov}(X,Y) = \operatorname{Cov}(\alpha\,Y+R,Y) = \alpha\operatorname{Var}(Y) = \alpha\,\sigma_y^2.$$ If we set $$\alpha = \rho\frac{\sigma_x}{\sigma_y}$$ then the correlation of $X$ and $Y$ will be $$\rho(X,Y) = \frac{\operatorname{Cov}(X,Y)}{\sigma_x\sigma_y} = \frac{\alpha\,\sigma_y^2}{\sigma_x\sigma_y} = \frac{\rho\frac{\sigma_x}{\sigma_y} \sigma_y^2}{\sigma_x\sigma_y} = \rho.$$ Observations (1) and (2) establish that $(X,Y)$ and $(x,y)$ have the same moments through second order, which means they have identical bivariate Normal distributions. Applying the main result $(**)$ gives $$E[X\mid a\le Y \le b] = \alpha\, E[Y\mid a \le Y \le b] = \rho\frac{\sigma_x}{\sigma_y}E[Y\mid a \le Y \le b].$$ We can go a little further towards simplification by recognizing that $Y/\sigma_y$ is a standard Normal variate. Thus, $$E[X\mid a\le Y \le b] = \rho\,\sigma_x E\left[\frac{Y}{\sigma_y}\mid \frac{a}{\sigma_y} \le \frac{Y}{\sigma_y} \le \frac{b}{\sigma_y} \right].$$ This shows that for fixed $a,b,$ the answer is directly proportional to $\rho\, \sigma_x.$ Evaluating the constant is a matter of manipulating integrals of a standard Normal variable, but doing so is primarily an exercise in Calculus rather than of statistical interest. I will conclude with the expression that I actually began with when solving this problem. Notice that the answer can also be written $$E\left[\frac{X}{\sigma_x}\mid a\le Y \le b\right] = \rho\, E\left[\frac{Y}{\sigma_y}\mid \frac{a}{\sigma_y} \le \frac{Y}{\sigma_y} \le \frac{b}{\sigma_y} \right].$$ The left hand side is the conditional expectation of a standard Normal variate $X/\sigma_x$ while the right hand side is just $\rho$ times the expectation of a truncated standard Normal variate $Y/\sigma_y.$ If you get used to standardizing variables automatically--which amounts to choosing a particularly convenient unit of measurement for them--then simply by visualizing the regression of $X$ against $Y$ you will say to yourself oh yes, since after standardization $X$ is just a multiple $\rho$ of $Y$, then the conditional expectation of $X$ must be $\rho$ times the corresponding expectation of $Y.$ That is the heart of the matter.
