[site]: datascience
[post_id]: 33524
[parent_id]: 
[tags]: 
Prediction in Machine Learning

When we use a regression algorithm in out dataset it's because we assume that there is a relation between our input data and some quantitative value. This is expressed as : $y = f(x)+\varepsilon $, where $x$ is an input vector and $\varepsilon$ is the random error term . In the case of a Prediction algorithm, this is the equation : $\hat{y}=\hat{f}(x)$ where $\hat{f}$ represents our estimate for $f$, and $\hat{y}$ represents the resulting prediction for $y$. What I don't understand is : why n this setting, $\hat{f}$ is often treated as a black box, in the sense that one is not typically concerned with the exact form of $\hat{f}$, provided that it yields accurate predictions for Y.
