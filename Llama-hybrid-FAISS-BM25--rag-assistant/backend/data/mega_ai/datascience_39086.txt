[site]: datascience
[post_id]: 39086
[parent_id]: 39085
[tags]: 
For layer 1, After all 5 Xi's are entered into the network, the 10 outputs from X5 from the first layer are then used as the inputs for the second layer. The other outputs from X1 to X4 are not used. No, the outputs from all time steps 1,2,...,5 are used as the input of the second layer. For layer 2, After all these are done, only the final 10 outputs from the layer 2 will be used? It depends on you. Some people choose to use only the final size=10 output as an encoding of all the input. Some people choose to average or concat all the outputs of second layers. When using attention, all the outputs are used. Here is a illustration of a 3-layer LSTM. Notice that all the outputs from the lower layer, not only the final output, are fed into the higher layer. .
