[site]: crossvalidated
[post_id]: 383984
[parent_id]: 
[tags]: 
Observational Data and Bias - A real problem

I'm hoping you all can provide some guidance. I'm working a problem with the following objectives and data set. I would like to be able to predict, for each unit, at each sampled moment, the expected response to an intervention. The objective being that we identify treatment candidates in an on-line way, and that we intervene only on candidates that are most likely to have a positive response. The dataset and its collection process (as well as it is understood) are described below: We have ~700 treatments in our data set, though the data is a collection of time series for 1000+ units over a varying period of a unit's lifetime. Some units have never been treated, some have been treated multiple times. Treatments are not randomly assigned. Experts examine these time series and select the units they believe will experience the best response. Positive responses result in increased cash flows. However, the experts' rules that are applied are undocumented, and only somewhat understood. In other words, we have reason to believe the expert selected the candidate for a good reason, but we have no way of knowing what that exact reason was. We know from speaking to experts what are the general characteristics that an expert will look for when selecting a candidate. Budgetary constraints mean that not all candidates that meet the experts' criteria for treatment will actually receive it, and presumably only the best candidates will actually get treated. We do not know which candidates were selected but untreated. My thoughts on this problem: I believe the data set is irredeemably biased. I believe that selection and response are correlated due to the candidate selection process (as intended). I know that candidate selection interferes with other candidates' opportunity to treatment due to the budgetary constraint. My thoughts on how to approach the problem thus far: My first thought was if I could reconstruct the candidate selection process by trying to predict the time to selection based on features engineered to account for the experts' analysis. Then a combination of that model together with a predictive model for expected treatment response might work. Think of it as attempting to replicate the selection process, restricting my domain to the same domain as my treated units where my expected treatment response model is valid. I tried this and was unsuccessful. Some of my colleagues have suggested that perhaps the aggregation of all the different rules applied to selecting candidates is equivalent to random selection, and thus ignorability. I highly doubt it. Finally I thought maybe there was a more traditional approach where the selection process could be made ignorable if instead of developing a model that selects candidates (per above) I just calculate a propensity score and condition on that. I'm very new to the concepts of propensity score matching and all of this so I'm not thinking clearly about the solution. Any insights would be helpful. Thank you for your thoughts!
