[site]: crossvalidated
[post_id]: 464971
[parent_id]: 
[tags]: 
Markov chains derivation for absorbing states

I am given that the probability to reach a specific absorbing state $s$ , from states 1, ..., M as $a_1, \cdots, a_M$ , which are unique solutions to equations $a_s = 1$ , $a_0 = 0$ for all absorbing states such that $i \neq s$ , and $a_i = \sum_i^M a_j p_{ij}$ for all transient states $i$ . Can someone show me how this summation is derived. It looks like the law of total probability, by conditioning on the next states reachable from $i$ . But it looks like a different kind of law of total probability (LOTP) that what I'm used to seeing. Usually when I see LOTP, it's something like this: $$ P(A) = \sum_k P(A, B_k) = \sum_k P(A | B_k) P(B_k). $$ For the Markov chain probability $a_i$ , say $i=1, M=2$ , we have $$ a_1 = p_{11}a_1 + p_{12}a2 \\ $$ Note that $$ p_{11} = P(X_{n+1} = 1 | X_{n} = 1) \\ p_{12} = P(X_{n+1} = 2 | X_{n} = 1) \\ $$ Is this an application of LOTP?
