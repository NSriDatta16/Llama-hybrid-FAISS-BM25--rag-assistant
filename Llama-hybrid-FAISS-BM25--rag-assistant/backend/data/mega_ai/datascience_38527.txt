[site]: datascience
[post_id]: 38527
[parent_id]: 38526
[tags]: 
Here's the question: is rewarding the agent for going into the right direction a "correct approach" in Reinforcement Learning? It depends on what you are hoping the agent is capable of learning by itself. This is an issue for you here, because you have a "toy" problem where you can control a lot more of the environment and alter the meaning of what it means to win. In general, then yes this is "cheating", at least in terms of claiming to have written an RL agent that solves the game. The academically ideal basic RL agent is rewarded by the gain of something meaningful in the context of the problem being solved, and is not helped by interim rewards. In a game of snake, and any other arcade-style game, it should really be the official points scored in the game and nothing else. Is passing the coordinates of the food as state an other way of "cheating"? Again it depends on what you expect the agent to learn from. If, in your target production environment, this data was easy to obtain, and you intended to use it to write a game bot working from the trained policy, then this is fine. There is no requirement that you do one thing or another if you have a practical problem to solve. However, learning from a pixel-only state, as in the DQN original papers, is of academic interest, because that is a generic state representation that applies to many problems, whilst the distance from the snake to food is a specific feature that you have engineered that makes learning easier in a smaller set of games. The main issue here is again that your goal is not really to put a "snake bot" into a production system, but to learn how RL works. RL is tricky, and often doesn't work as well as you expect - or at all, for many combinations of algorithm and problem. It is worth reading this article: Deep Reinforcement Learning Doesn't Work Yet - it may put disappointing results from basic DQN into perspective. I would encourage you to strip back your Snake problem to remove "helpful" rewards and state, and instead look into extensions to the core DQN algorithm, or different learning agents such as A3C.
