[site]: crossvalidated
[post_id]: 408872
[parent_id]: 408755
[tags]: 
There is many ways in which the lower bound of 0 can be obtained, they all represent multivariate distributions of $X$ where $X_1+X_2+\dotsm+X_n$ is constant (with probability one.) So they will be singular distributions, with a support which is not all of $\mathbb{R}^n$ . Let the covariance matrix of the random ector $X$ be $\Sigma$ . Then we can compute the variance of the mean $\bar{X}_n$ as $(\frac1{n})^2 1^T \Sigma 1$ where $1$ represents the $n$ -vector of all ones. So the lower bound is obtained if $1$ is an eigenvector of $\Sigma$ with eigenvalue zero, so $\Sigma$ is only positive semidefinite. But in a comment the OP adds that he has a strongly autocorrelated time-series. In that case maybe more can be said. For an example, say the series is (second-order) stationary autoregressive of order 1. Then the covariance matrix $\Sigma$ will be a symmetric Toeplitz matrix , more spcifically tridiagonal with 1's along the diagonal and the autocorrelation (at lag 1) $\alpha$ along it, below and above. Then we can calculate the variance of the mean as $$ (\frac1{n})^2 1^T \Sigma 1=\frac1{n}\left(1+2\alpha \frac{n-1}{n}\right) $$ and the requirement that the variance be nonnegative means that $\alpha \ge -\frac12 \frac{n}{n-1}$ and equality will result in a variance of zero. But if this really are observations from a stationary time series, then a value for $\alpha$ is possible only if it is possible for all $n$ , which in this case leads to $\alpha \ge -\frac12$ . In that case the minimum value of the variance of $\bar{X}_n$ (for an AR of order 1) becomes $(\frac1{n})^2$ . A similar analysis should be possible for higher-order AR processes.
