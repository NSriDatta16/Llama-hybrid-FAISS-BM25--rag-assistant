[site]: crossvalidated
[post_id]: 540323
[parent_id]: 
[tags]: 
In deep Q-learning how is the error calculated for each output node?

In the Q-learning approach to reinforcement learning where a neural network learns to approximate the Q-function, I believe that the output layer has one node for each possible action ( $a_i$ ) and the value of each node corresponds to the Q-value of the associated action given some state ( $S_t$ ) as input. The target value that the output node seeks to learn is $Q(S_t, a_i)$ which is approximated by $r_t+\gamma \max_{a} Q(s_{t+1}, a)$ and as the agent interacts with the environment it generates experiences which are used for training. These experiences take the form of a tuple: $(S_t, a_t, r_t, S_{t+1})$ (where $r_t$ is the reward at timestep $t$ ). My question is this: Given that each experience contains only one action (and one reward and state transition associated with that action) doesn't the training example only have information to calculate the error on one output node (corresponding to the one action taken)? If so how is this resolved when training? Is the error gradient only back-propagated through the one output node? Or is there some other solution?
