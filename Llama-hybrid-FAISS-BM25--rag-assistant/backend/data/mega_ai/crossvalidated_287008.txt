[site]: crossvalidated
[post_id]: 287008
[parent_id]: 26323
[tags]: 
The "Amount of relationship" between two discrete variables $X$, $Y$ is formally measured by mutual information : $I(X,Y)$. While the covariance/correlation is somehow the amount of linear relationship, mutual information is somehow the amount of (any kind of) relationship. I'm pasting the picture form Wikipedia's page : For continuous variables, the information-theoretic concepts are often defined as well but less manageable, maybe less meaningful. I don't want to bother for the moment. Let's stick to discrete variables. Anyway it makes sense approximating continuous variables by discrete ones (using slices) especially in information theoretic approaches. The problem with information theoretic concepts is often their impracticability. Being able to approximate the mutual information between $X$ and $Y$ is the same as being able to find arbitrary non-linear relationship between them : you need a statistical power (quantity of data) most often far beyond what is reasonable : for any possible value for $x$, you need many (say 1000) samples to compute an estimation of each $P(Y=y|X=x)$. This is not possible in most machine learning or statistical analysis problems. It is kind of logical : if you allow a model to be able to express "any possibility", then it can only be trained by an amount of data covering any possibility several times. But maybe such an approach is possible, for low dimensional variables, if you enforce low precision : decompose the domains of $X$ and $Y$ into a number of slices small enough so that it is ok for your data. Anyway I think this requires some research.
