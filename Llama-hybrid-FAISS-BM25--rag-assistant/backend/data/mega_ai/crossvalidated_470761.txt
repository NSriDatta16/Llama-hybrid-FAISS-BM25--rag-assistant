[site]: crossvalidated
[post_id]: 470761
[parent_id]: 470626
[tags]: 
Suppose one rolls one die (numered 1-6), and wants to compute its average deviation from the average value of 3.5. Two rolls would differ by 0.5, two by 1.5, and two by 2.5, for an average deviation of 1.5. If one takes the average of the squares of the values, one would have one deviation of 0.25, one of 2.25, and one of 6.25, for an average of 2.916 (35/12). Now suppose instead of rolling one die, one rolls two. The average deviation would be 1.94 (35/18), and the average square of the deviation would be 5.833 (70/12). If instead of rolling two dice, one wanted to estimate the expected deviation based upon what it was with one die, doubling the linear average single-die deviation (i.e. 1.5) would yield a value of 3, which is much larger than the actual linear average deviation of 1.94. On the other hand, doubling the average square of the deviation when using a single die (2.916) would yield precisely the average square of the deviation when using two dice. In general, the square root of the average of the squares is a more useful number than the average of the squares itself, but if one wants to compute the square root of the average of a bunch of squares, it's easier to keep the values to be added as squares, than to take the square roots whenever reporting them and then have to square them before they can be added or averaged.
