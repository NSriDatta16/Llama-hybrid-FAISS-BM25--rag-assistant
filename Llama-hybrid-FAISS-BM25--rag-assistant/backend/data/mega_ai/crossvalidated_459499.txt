[site]: crossvalidated
[post_id]: 459499
[parent_id]: 
[tags]: 
Interpretation of Tensor Flow CNN results with big dips in accuracy while training

I am trying to classify images using a CNN in tensor flow. I am doing 10 fold cross validation. At each fold, the training set is 900+ images and the validation set is 100 images. It is only two classes. I am having a hard time interpreting the results graph when I plot by epoch. As you can see, accuracy seems to go up, but between 140-165 epochs it dips very much, and then again toward 300 epochs. This is happening on each fold. Here are two graphs (two folds) as an example. How should I interpret this? Thank you. Setting up, these are the layers (each function will then load them). def add_convolutional_layers(self): # Two sets of convolutional layers for idx in range(0, 2): if idx == 0: self.model.add(tf.keras.layers.Conv2D(10, (5, 5), input_shape=( self.size, self.size, 3))) # todo: only works for color images else: self.model.add(tf.keras.layers.Conv2D(40, (5, 5))) self.model.add(tf.keras.layers.BatchNormalization()) self.model.add(tf.keras.layers.Activation("relu")) self.model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))) # Output shape: 40 x 61 x 61 self.model.add(tf.keras.layers.Flatten()) def add_hidden_layers(self): # TODO: In Mathematica, Dropout[] has a rate of dropping 50% of elements then * by 2 -- ours does not. self.model.add(tf.keras.layers.Dropout(0.5, seed=self.seed)) self.model.add(tf.keras.layers.Dense(500, activation="linear", activity_regularizer=regularizers.l2(0.01), kernel_regularizer=regularizers.l2(0.05))) self.model.add(tf.keras.layers.Dense(500, activation="relu", activity_regularizer=regularizers.l2(0.01), kernel_regularizer=regularizers.l2(0.05))) self.model.add(tf.keras.layers.Dropout(0.25, seed=self.seed)) def add_output_layers(self): self.model.add(tf.keras.layers.Dense(10, activation="linear", activity_regularizer=regularizers.l2(0.01), kernel_regularizer=regularizers.l2(0.05))) self.model.add(tf.keras.layers.Dense(10, activation="softmax", activity_regularizer=regularizers.l2(0.01), kernel_regularizer=regularizers.l2(0.05))) Then, the parameters of the CNN are: opt = tf.keras.optimizers.Adam(learning_rate=self.lr, beta_1=0.9, beta_2=0.999, epsilon=0.00001, # decay=0.0, decay=0.00001, amsgrad=False) self.model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
