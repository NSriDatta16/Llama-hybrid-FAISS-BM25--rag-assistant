[site]: crossvalidated
[post_id]: 198399
[parent_id]: 198394
[tags]: 
"Attributes" – also "features", "predictors", "independent variables", and some other terms – refer to the different dimensions of your dataset. For example, if you have a database of people with ages and incomes, age is one attribute/feature/whatever, income is another. The most common way to represent this data for a machine learning algorithm is as a vector. So, a given person might correspond to a vector $\mathbf x = \begin{pmatrix}32 & 46000\end{pmatrix}$, where the first element is the person's age, and the second their income. If you have $M$ features, the vectors will be of length $M$. When you have a full dataset of these things, one way to represent it is as a set of these vectors: a set of length $N$, where each element is a vector of length $M$. Sometimes it's more convenient to think about this instead as a matrix stacking up all the vectors, e.g. a matrix of size $N \times M$, where each row is one of the feature vectors from your dataset. So this might look like, for example, $$\mathbf X = \begin{bmatrix}32 & 46000 \\ 51 & 74000 \\ 19 & 12000 \\ 53 & 21000 \end{bmatrix}.$$ (You might also see it in the other order, with each feature across the rows and each instance as a column; this is just a convention difference that doesn't really mean anything.) If we work with matrices, then we can talk about our models using linear algebra, which is sometimes easier and more efficient to implement than always doing everything as a sum over elements of a set.
