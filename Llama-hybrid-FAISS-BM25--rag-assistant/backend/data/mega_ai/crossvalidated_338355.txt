[site]: crossvalidated
[post_id]: 338355
[parent_id]: 52897
[tags]: 
Equivalence is never something we can test . Think about the hypothesis: $\mathcal{H}_0: f_x \ne f_y$ vs $\mathcal{H}_1: f_x = f_y$. NHST theory tells us that, under the null, we can choose anything under $\mathcal{H}_0$ that best fits the data. That means we can almost always get arbitrarily close to the distribution. For instance, if I want to test $f_x \sim \mathcal{N}(0, 1)$, the probability model that allows for separate distributions of $\hat{f}_x$ and $\hat{f}_y$ will always be more likely under the null, a violation of critical testing assumptions. Even if the sample $X=Y$ identically, I can get a likelihood ratio that is arbitrarily close to 1 with $f_y \approx f_x$. If you know a suitable probability model for the data, you can use a penalized information criterion to rank alternate models. One way is to use the BICs of the two probability models (the one estimated under $\mathcal{H}_0$ and $\mathcal{H}_1$. I've used a normal probability model, but you can easily get a BIC from any type of maximum likelihood procedure, either by hand or using the GLM. This Stackoverflow post gets in nitty-gritty for fitting distributions. An example of doing this is here: set.seed(123) p BIC(lm(x~g)) }) mean(p) gives > mean(p) [1] 0.034 $p$ here is the proportion of times that the BIC of the null model (separate models) is better (lower) than the alternative model (equivalent model). This is remarkably close to the nominal 0.05 level of statistical tests. On the other hand if we take: set.seed(123) p BIC(lm(x~g)) }) mean(p) Gives: > mean(p) [1] 0.437 As with NHST there are subtle issues of power and false positive error rates that should be explored with simulation before making definitive conclusions. I think a similar (perhaps more general method) is using Bayesian stats to compare the posterior estimated under either probability model.
