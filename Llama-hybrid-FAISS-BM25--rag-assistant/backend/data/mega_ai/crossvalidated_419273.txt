[site]: crossvalidated
[post_id]: 419273
[parent_id]: 
[tags]: 
does $w_1^{n-1}$ denote whole the sentence in the context of word prediction?

this NLP book gives When we use a bigram model to predict the conditional probability of the next word, we are thus making the following approximation: $P(w_n|w_1^{n-1}) \approx P(w_n|w_{n-1})$ does $w_1^{n-1}$ denote whole the sentence?
