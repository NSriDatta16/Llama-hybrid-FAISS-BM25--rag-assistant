[site]: datascience
[post_id]: 67652
[parent_id]: 67641
[tags]: 
Maybe it's a bit overkill and biased toward my own field (neural machine translation), but you could go with a neural network architecture with self-attention in a masked language model-ish (i.e. BERT ) configuration The input to the network would be a fixed-size (40) sequence of discrete symbols meaning whether the element at that position is either present in the set ( $P$ ), absent from it ( $A$ ) or its presence is unknown ( $U$ ). The "vocabulary" of the input tokens would then be ${P, A, U}$ . This way, the input would be a sequence of 40 symbols, e.g. $A, A, P, U, U,..., P$ . The input would be fed into an embedding layer, followed by $N$ layers of unmasked self-attention. Finally, the last attention layer output would be projected into a representation space of size 40, which would then be applied a sigmoid function to obtain the probabilities of each of the 40 elements in the set be part of the original input. The loss function would only be computed on the elements where the input was marked as unknown ( $U$ ) and ignored elsewhere. The expected output at those positions would be $1$ if the element was actually present or else $0$ . You could use binary cross-entropy as the loss function to optimize. You should prepare your training data so that you mark as unknown ( $U$ ) elements from the original set, with the unknown element ratio that you expect in your test data. At inference time, you simply set the info you know as either $P$ or $A$ and set the unknown elements to $U$ , and get the output of the network at those positions.
