[site]: datascience
[post_id]: 116698
[parent_id]: 
[tags]: 
NLP - F1 score for positive class drops to 0 after data augmentation

I'm working on a 3-class text classification problem where my initial class distribution looked like this: positive: 50% negative: 25% and neutral: 25% And training on a model on this slightly imbalanced data gave me F1 scores of (45%, 57% and 68%) respectively for the negative, neutral and positive classes in the validation set. Since the F1 scores for negative and neutral classes seemed to be lesser than the positive class, I decided to try some data augmentation approaches for the negative and neutral classes alone. I used ContextualWordEmbsAug from the NLPAug Library to augment those two classes after which I got a class distribution like this: positive: 35% negative: 33% neutral: 32% But the same model trained on this augmented data give me strange results. The F1 scores are now (57%, 62%, 0%) for the negative, neutral and positive classes resp. I don't understand why the F1 score for the positive class had to drop to 0%. It makes sense that the results for negative and neutral have improved due to augmenting them, but I fail to understand why that should affect the performance on the positive class which had given me good results prior to augmentation. Am I missing something here? Could someone explain to me the possible reasons for this?
