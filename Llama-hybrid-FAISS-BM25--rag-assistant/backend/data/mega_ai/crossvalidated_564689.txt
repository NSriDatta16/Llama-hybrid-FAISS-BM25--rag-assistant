[site]: crossvalidated
[post_id]: 564689
[parent_id]: 
[tags]: 
Is there an incremental dimensionality reduction algorithm that can handle batch size less than number of components to be reduced?

I have a large dataset of patient data by hour. For example, given the shape as (hours, features) , patient 1 data shape could be (30, 76) and patient 2 data shape could be (5, 76) . I want to do incremental learning of the dataset because all the data does not fit inside the memory. Before feeding the data to a classifier, I am using IncrementalPCA from Scikit as dimensionality reduction method. If I set IncrementalPCA’s n_components to lets say 20 , patient 1 data’s dimensions can be reduced. However, for patient 2, an exception n_components=20 must be less or equal to the batch number of samples 5 is raised. I did not set n_components to less than or equal 5 because I think eventhough it will make the PCA work, it would cause a lot of reduction for Patient 1 which is not ideal. Setting n_components to None sets n_components to min(n_samples, n_features) as per Scikit documentation which I think is not ideal too. Correct me if I’m wrong. A minimal example is as following: import numpy as np from sklearn.decomposition import IncrementalPCA patient_1 = np.random.randint(0, 5, (30, 76)) patient_2 = np.random.randint(0, 5, (5, 76)) pca = IncrementalPCA(n_components=20) #======Successfully PCA reduced======= pca.partial_fit(patient_1) red_data = pca.transform(patient_1) print(f"red_data.shape {red_data.shape}") #======Failed to PCA reduced======= # Raises exception "n_components=20 must be less or equal to the batch number of samples 5" try: pca.partial_fit(patient_2) red_data = pca.transform(patient_2) except ValueError as v: print(v) Question: Is there an incremental dimensionality reduction algorithm (possibly with Python implementation) that can reduce the feature dimensions and does not have the restriction like in PCA where n_components must be less or equal to batch number of samples?
