[site]: crossvalidated
[post_id]: 281470
[parent_id]: 208178
[tags]: 
If you cannot afford to learn $k$ objective-dependent policies for $k$ different objectives, or you really want something to generalize across goal inputs, then you could try a policy gradient method that takes the goal as input. You are correct that the MDP is different, because you're implicitly changing the reward structure when $s_\text{goal}$ is a parameter. Here's an example approach of exactly this (in a synthetic environment): Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning There is a body of work on learning to generalize across MDPs; your problem is one instances of this.
