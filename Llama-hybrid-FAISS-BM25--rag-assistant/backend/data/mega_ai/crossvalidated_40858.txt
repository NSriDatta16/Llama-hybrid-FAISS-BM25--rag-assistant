[site]: crossvalidated
[post_id]: 40858
[parent_id]: 38831
[tags]: 
From my understanding, you are looking for a measure of variable importance. These come in several flavours based on several different theoretical approaches, but all have strong links to the methods used to optimise the algorithm you're talking abut. Generally, each machine learning algorithm will have a canonical optimisation method; back propagation for neural networks, sequential minimum optimisation for SVMs, various information criterion and statistical significance tests for decision trees including chi squared significance or gini impurity. Of course, other more novel optimisation methods are frequently proposed for each of the algorithms. These optimisation methods for each algorithm essentially define the variable importance for the model at hand. Essentially, you're looking for an approximation or interpretable representation of the results of that optimisation step that the algorithm is undertaking. However, this is problematic for several reasons. The difficulty of determining the influence of a given variable on model form selection, given that selection is often a stochastic process itself.The variables influence model selection to some degree, so that even if a variable is not important for the final prediction in a model, it may have crucially shaped the model form itself. Given that the generation of the model itself is often stochastic (optimised using particle swarm optimisation or a bagging method etc.), it is hard to understand exactly how a given variable may have shaped its form. The difficulty of extracting the importance of a single variable given that it may be important only in conjunction or interaction with another variable. Some variables may only be important for some observations. Lack of importance on other observations may confound measurement of overall importance by averaging out a real difference. It is also hard to get an immediately interpretable metric for variable importance exactly as defined by the model, as it may not produce a single number (especially in the case of bagging). Instead, in these cases there is a distribution of importance for each variable. One way to overcome these issues might be to use perturbation. This is a way to analyse your final model by adding random noise to your variables, and then checking how this affects the results. The advantage is that it allows you to find which variables are most important empirically through simulation - answering the question of which variables would destroy the prediction most if removed. The disadvantage, is that there is a good chance that even if the variables were removed/perturbed, the model (if re-trained) could use the other variables reconstruct their effect, meaning that the "variable importance" measure you derive still only truly indicates the importance in your trained model, but not the overall importance across all possible models.
