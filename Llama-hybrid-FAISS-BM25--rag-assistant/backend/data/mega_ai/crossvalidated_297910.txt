[site]: crossvalidated
[post_id]: 297910
[parent_id]: 297779
[tags]: 
On top of the 1st order algos listed out by @VadimSnolyakov, it is perhaps worth mentioning 2nd order gradient methods. Second order methods often converge much more quickly, but it can be very expensive to calculate and store the Hessian matrix. In general, most people prefer clever first order methods which need only the value of the error function and its gradient with respect to the parameters ( source ) 2nd order gradient methods include the classical Newtons's method, Broyden-Fletcher-Goldfarb-Shanno (BFGS) and other quasi-Newton methods, where an approximation for the Hessian (or its inverse directly) is built up from changes in the gradient, as it is the case, for example, in Levenberg-Marquardt . Edit 1: It has been questioned whether the use of second order methods is "popular". Well, in order to settle the dispute we would need to agree on how to define and measure "popularity". However, we read on the notes of the Stanford CS class CS231n: Convolutional Neural Networks for Visual Recognition Second order methods A second, popular group of methods for optimization in context of deep learning is based on Newtonâ€™s method ( source ) We can not draw on conclusion on our dispute based on this quote but yes, second order methods may be an option, especially if you have shallow NNs (due to the computational burden the Hessian approximation brings). On top of that, some promising research has already been carried on comparing optimization methodologies when dealing with auto-encoders and in particular sparse auto-encoders: More significant speed improvements of L-BFGS and CG over SGDs are observed in our experiments with sparse autoencoders. This is because having a larger minibatch makes the optimization problem easier for sparse autoencoders: in this case, the cost of estimating the second-order and conjugate information is small compared to the cost of computing the gradient ( source ) In the end, we might regard the extra cost associated to the approximation of the Hessian as an alternative to: 1) the need of pre-training why does pre-training work and why is it necessary? Some researchers (e.g. Erhan et al., 2010) have investigated this question and proposed various explanations such as a higher prevalence of bad local optima in the learning objectives of deep models. Another explanation is that these objectives exhibit pathological curvature making them nearly impossible for curvature-blind methods like gradient-descent to successfully navigate ( source ) having to tune first-order methods - curvature helps! One key disadvantage of SGDs is that they require much manual tuning of optimization parameters such as learning rates and convergence criteria. If one does not know the task at hand well, it is very difficult to find a good learning rate or a good convergence criterion. ( source ) All this is far for being exhaustive but I hope interesting enough to encourage us to dig deeper. In research, we assist to waves of interest and popularity. We might in fact see a resurgence of interest in second order methods in the upcoming future. Of particular interest to the use of second-order methods is the relatively recent (or not so recent, depending on your type of work) discovery that saddle points are far more common in high-dimensional space than local minima (see [1406.2572] Identifying and attacking the saddle point problem in high-dimensional non-convex optimization ). This is a bit counterintuitive, since local minima are far more ubiquitous in lower-dimensional problems. Purely first-order methods cannot differentiate between saddle points and local minima, and thus for certain problems, SGD can become stuck along paths of slow convergence around saddle points ( source )
