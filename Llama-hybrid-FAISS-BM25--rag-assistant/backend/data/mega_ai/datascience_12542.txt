[site]: datascience
[post_id]: 12542
[parent_id]: 12538
[tags]: 
There is nothing special about building something across many machines, or Hadoop, so maybe we can take that out of the question. One general reason is that, even if the model building process were worse, building on more data could result in a better result. You're asking why building many models on subsets of data could be better, but this is precisely why ensemble models like random forests can outperform single instances. But lastly, it's not true that distributed algorithms always work by making an ensemble of small models, so the premise is not strictly true.
