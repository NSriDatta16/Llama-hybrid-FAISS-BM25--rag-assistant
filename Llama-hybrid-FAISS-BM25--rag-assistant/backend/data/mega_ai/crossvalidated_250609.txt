[site]: crossvalidated
[post_id]: 250609
[parent_id]: 185856
[tags]: 
$\newcommand{\E}{\mathbb{E}}$I just want to address four subtleties which I think might help you understand this better, although I am not entirely certain if they answer your question. If they do, they would only do so indirectly. In any case, it seems that making sure you are clear about how these concepts apply to your situation seems to me likely a prerequisite for being able to phrase the question in a way that conveys clearly to others where the precise/exact source of your confusion lies. 1. Correlation is "normalized covariance". (Normalized means re-scaled to a standard, i.e. "normal", size, thus making it easier to compare apples and oranges, by making them equally-sized fruits.) Wikipedia has a nice page about how the two quantities differ and how they are similar. 2. There is an important distinction between a random variable $X$ and its expected value, $\E X$. The latter, $\E X$, is an attempt to take the "average" of all possible values of the former, $X$. This document (page 4) has a nice section about the geometric interpretation of expectation -- it is extremely similar (arguably in some way the same) as the notion of integral in calculus, which one often thinks of in terms of the area underneath a graph. 3. The difference between perfect, theoretical population parameters and the values we estimate for those quantities based on a limited, finite amount of (possibly slightly incorrect) sample data. The Wikipedia article about this seems readable to me and less technical than most similar articles, so you might find it a good place to start reading about the topic. In particular, as mentioned in the comments on this answer, it appears that you are working with estimators for the variance/covariance. This is different from the actual "perfect, theoretical" quantities themselves -- it can be convenient sometimes to blur the distinction between a statistic and its estimator, but it is also important to always remember the difference and keep it in mind. 4. The difference between centered and uncentered statistics. It looks like you are using estimators for uncentered quantities (unless we know a priori that $\E X = 0 \E Y$ -- this is not true in general), which is problematic in particular since the variance and covariance are centered statistics. In particular, how you might interpret the variance and/or covariance geometrically (for example, as "spread" of the distribution) depends crucially on the fact that they are centered about the expectation/mean of the random variables involved. This page gives a nice example interpreting the variance in a specific case as "the moment of inertia of the mass distributed about the center of mass" -- such a quantity makes much less sense without already specifying in advance where the "center of mass" lies.
