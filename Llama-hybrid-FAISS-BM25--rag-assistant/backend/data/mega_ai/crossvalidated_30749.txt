[site]: crossvalidated
[post_id]: 30749
[parent_id]: 30728
[tags]: 
Chemical concentration data often have zeros, but these do not represent zero values : they are codes that variously (and confusingly) represent both nondetects (the measurement indicated, with a high degree of likelihood, that the analyte was not present) and "unquantified" values (the measurement detected the analyte but could not produce a reliable numeric value). Let's just vaguely call these "NDs" here. Typically, there is a limit associated with an ND variously known as a "detection limit," "quantitation limit," or (much more honestly) a "reporting limit," because the laboratory chooses not to provide a numerical value (often for legal reasons). About all we really know of an ND is that the true value is likely less than the associated limit: it's almost (but not quite) a form of left censoring . (Well, that's not really true either: it's a convenient fiction. These limits are determined via calibrations which, in most cases, have poor to terrible statistical properties. They may be grossly over- or under-estimated. This is important to know when you're looking at a set of concentration data which appear to have a lognormal right tail which is cut off (say) at $1.33$, plus a "spike" at $0$ representing all the NDs. That would strongly suggest the reporting limit is just a little less than $1.33$, but the lab data might try to tell you it is $0.5$ or $0.1$ or something like that.) Extensive research has been done over the last 30 years or so concerning how best to summarize and evaluate such datasets. Dennis Helsel published a book on this, Nondetects and Data Analysis (Wiley, 2005), teaches a course, and released an R package based on some of the techniques he favors. His website is comprehensive. This field is fraught with error and misconception. Helsel is frank about this: on the first page of chapter 1 of his book he writes, ...the most commonly used method in environmental studies today, substitution of one-half the detection limit, is NOT a reasonable method for interpreting censored data. So, what to do? Options include ignoring this good advice, applying some of the methods in Helsel's book, and using some alternative methods. That's right, the book is not comprehensive and valid alternatives do exist. Adding a constant to all values in the dataset ("starting" them) is one. But consider: Adding $1$ is not a good place to start, because this recipe depends on the units of measurement. Adding $1$ microgram per deciliter will not have the same result as adding $1$ millimole per liter. After starting all the values, you will still have a spike at the smallest value, representing that collection of NDs. Your hope is that this spike is consistent with the quantified data in the sense that its total mass is approximately equal to the mass of a lognormal distribution between $0$ and the start value. An excellent tool for determining the start value is a lognormal probability plot: apart from the NDs, the data should be approximately linear. The collection of NDs can also be described with a so-called "delta lognormal" distribution. This is a mixture of a point mass and a lognormal. As is evident in the following histograms of simulated values, the censored and delta distributions are not the same. The delta approach is most useful for explanatory variables in regression: you can create a "dummy" variable to indicate the NDs, take logarithms of the detected values (or otherwise transform them as needed), and not worry about the replacement values for the NDs. In these histograms, approximately 20% of the lowest values have been replaced by zeros. For comparability, they are all based on the same 1000 simulated underlying lognormal values (upper left). The delta distribution was created by replacing 200 of the values by zeros at random . The censored distribution was created by replacing the 200 smallest values by zeros. The "realistic" distribution conforms to my experience, which is that the reporting limits actually vary in practice (even when that is not indicated by the laboratory!): I made them vary randomly (by just a little bit, rarely more than 30 in either direction) and replaced all simulated values less than their reporting limits by zeros. To show the utility of the probability plot and to explain its interpretation , the next figure displays normal probability plots related to the logarithms of the preceding data. The upper left shows all the data (before any censoring or replacement). It's a good fit to the ideal diagonal line (we expect some deviations in the extreme tails). This is what we are aiming to achieve in all the subsequent plots (but, due to the NDs, we will inevitably fall short of this ideal.) The upper right is a probability plot for the censored dataset, using a start value of 1. It's a terrible fit, because all the NDs (plotted at 0, because $\log(1+0)=0$) are plotted much too low. The lower left is a probability plot for the censored dataset with a start value of 120, which is close to a typical reporting limit. The fit in the bottom left is now decent--we only hope that all these values come somewhere near to, but to the right of, the fitted line--but the curvature in the upper tail shows that adding 120 is starting to alter the shape of the distribution. The bottom right shows what happens to the delta-lognormal data: there's a good fit to the upper tail, but some pronounced curvature near the reporting limit (at the middle of the plot). Finally, let's explore some of the more realistic scenarios: The upper left shows the censored dataset with the zeros set to one-half the reporting limit. It's a pretty good fit. On the upper right is the more realistic dataset (with randomly varying reporting limits). A start value of 1 does not help, but--on the lower left--for a start value of 120 (near the upper range of the reporting limits) the fit is quite good. Interestingly, the curvature near the middle as the points rise up from the NDs to the quantified values is reminiscent of the delta lognormal distribution (even though these data were not generated from such a mixture). On the lower right is the probability plot you get when the realistic data have their NDs replaced by one-half the (typical) reporting limit. This is the best fit, even though it shows some delta-lognormal-like behavior in the middle. What you ought to do, then, is to use probability plots to explore the distributions as various constants are used in place of the NDs. Start the search with one-half the nominal, average, reporting limit, then vary it up and down from there. Choose a plot that looks like the bottom right: roughly a diagonal straight line for the quantified values, a quick drop-off to a low plateau, and a plateau of values that (just barely) meets the extension of the diagonal. However, following Helsel's advice (which is strongly supported in the literature), for actual statistical summaries, avoid any method that replaces the NDs by any constant. For regression, consider adding in a dummy variable to indicate the NDs. For some graphical displays, the constant replacement of NDs by the value found with the probability plot exercise will work well. For other graphical displays it may be important to depict the actual reporting limits, so replace the NDs by their reporting limits instead. You need to be flexible!
