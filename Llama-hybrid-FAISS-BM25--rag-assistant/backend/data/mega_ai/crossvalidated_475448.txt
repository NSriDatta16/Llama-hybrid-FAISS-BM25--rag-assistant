[site]: crossvalidated
[post_id]: 475448
[parent_id]: 
[tags]: 
test of equal predictive accuracy of forecasts for multiple nested models in R

an out-of-sample forecast experiment (time series cross validation with a fixed rolling window size) using various models are applied. Suppose I have multiple forecasts from different models (LASSO,RR,elastic net) and in total 11 models. Since some models are restricted versions of one another (elastic net – LASSO), we´re dealing with nested models. In addition there are 6 forecasting horizons (h=1,3,6,9,12,24) and 3 samples [1x full sample, 2x subsamples]. Applying a test of equal forecast accuracy of each forecast against a benchmark, means that 198 tests have to be applied. In general I want to compare the accuracy of forecasts produced from distinct models. Therefore I wish to test equal predictive accuracy of forecasts, which means that I´m more interested how well a model approximates the data. Nullhypothesis: e.g. testing whether forecasts produced by RW are not better than forecasts from the distinct models or equivalently test the null hypothesis that “the benchmark is not inferior to any alternative forecast” Which test can you recommend me? How can it be implemented in R for a simple example (2 forecasts from 2 models and a benchmark) as the following?: library(forecast) #time series y1 = 2+ 0.15*(1:20) + rnorm(20,2) y2 = y1[20]+ 0.3*(1:30) + rnorm(30,2) y = as.ts(c(y1,y2)) #10obs in out-of-sample; 40 obs in-sample ntest
