[site]: crossvalidated
[post_id]: 132647
[parent_id]: 34903
[tags]: 
In plain English: Statistical distribution is a mathematical function $f$ that tells you what is the probability of different values of your random variable $X$ that has the distribution $f$, i.e. $f(x)$ outputs a probability of $x$. There are different such a functions , but for now let consider $f$ as some kind of "general" function. However, for $f$ to be universal , that is, one that is possible to apply to different data (that share similar properties), it needs parameters that change its shape so that it fits different data. A simple example of such a parameter is $\mu$ in normal distribution that tells where is the center (mean) of this distribution and so it can describe random variables with different mean values. Normal distribution has another parameter $\sigma$ and other distributions also have at least one such a parameters. The parameters are often called $\theta$, where for normal distribution $\theta$ is a shorthand for both $\mu$ and $\sigma$ (i.e. is a vector of the two values). Why is $\theta$ important? Statistical distributions are used to approximate the empirical distributions of data. Say you have dataset of ages of a group of people and on average they are 50 years old and you want to approximate the distribution of their ages using a normal distribution. If normal distribution didn't allow for different values of $\mu$ (e.g. had a fixed value of this parameter, say $\mu=0$), then it would be useless for this data. However, since $\mu$ is not fixed, normal distribution could use different values of $\mu$, with $\mu=50$ being one of them. This is a simple example, but there are more complicated cases where the values of $\theta$ parameters are not so clear and so you have to use statistical tools for estimating (finding the most appropriate) $\theta$ values. So you could say that statistics is about finding the best $\theta$ values given the data (Bayesians would say: given the data and priors).
