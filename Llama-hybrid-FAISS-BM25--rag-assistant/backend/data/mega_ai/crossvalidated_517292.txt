[site]: crossvalidated
[post_id]: 517292
[parent_id]: 
[tags]: 
Bias and variance of an estimator of a model mean

I have a binary classification model and I need to use its output to estimate the means of groups of observations. I have two questions: A. Can I compute the the bias and variance of the estimator of the group mean? B. Can I run any hypothesis test to compare the means of two groups? More details I may be running with assumptions so let me add more context: The data consists of thousands of observations that are expensive to label The classifier model is your typical black box optimised with maximum likelihood using cross-validation Softmax outputs are not calibrated, hence I will call them scores rather than probabilities Trained model meets the precision-recall requirements on the test set (using a 0.5 cut-off point as it turns out) A group is a collection of individual scores that share some characteristics e.g. if predicting Titanic passenger survival rate, a group could be a user class (1st, 2nd) or a family, or any groups for which we are interested in their average success (positive class) rate Group membership is available as metadata but the labelling set does not take it into account The downstream task will take the predicted scores for different groups, average them and compare groups against that estimate. As explained at the top, I see the sample mean as an estimator for the group's average success rate, and I am looking for a way to estimate its bias and variance. I am also worried that end users will come up with bold claims when comparing groups, e.g. "group A survival rate is waaaay bigger than group B (50.4% vs 51.5%)" so I would like to assess such claims by means of hypothesis tests for example.
