[site]: crossvalidated
[post_id]: 319538
[parent_id]: 319535
[tags]: 
If I'm not mistaken, this is just a linear combination/transformation. You just multiply every boundary by a coefficient, then you add another, without thresholds. This is entirely possible, but the "boundary" of decisions is still just a line. By adding, multiplying and subtracting nonlinear boundaries, you can get useful, "smart" combinations. By combining lines (a*input + b) you will only ever get a line. No matter how many layers you have, a combination of lines will only ever be a line, and neural networks are based on sophisticated decision boundaries: By adding and subtracting lines, you will never get "bends". A useful demonstration can be found at convnetjs . Try changing the 'activation' from tanh to relu back and forth to see what the difference is. Relu produces straight corners while tanh produces blobby boundaries.
