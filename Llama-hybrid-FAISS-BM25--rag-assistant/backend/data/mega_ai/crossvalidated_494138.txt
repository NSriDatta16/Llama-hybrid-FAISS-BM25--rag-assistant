[site]: crossvalidated
[post_id]: 494138
[parent_id]: 
[tags]: 
Bayesian optimization experiment to confirm learning rate schedules in DNNs

Common learning rate schedules usually decrease the learning based on some criteria or some predefined schedule which intuitively makes sense. Has it been confirmed with bayesian hyper parameter tuning though? For example fixing the number of epochs to something like 20 and a range for the learning rate of each epoch and finding out what schedule does bayesian optimization come up with?
