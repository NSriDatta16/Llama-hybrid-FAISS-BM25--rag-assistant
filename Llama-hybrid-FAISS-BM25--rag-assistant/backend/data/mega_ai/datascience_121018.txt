[site]: datascience
[post_id]: 121018
[parent_id]: 121015
[tags]: 
Does higher blocks represent longer phrases and learns what longer phrases attend to? No, each layer can attend to arbitrarily long sequences. While bottommost block represent single word and its attention; something like how first layer of CNN represent pixel and deeper layers represent edges and further deeper layers represents shapes (like nose, hand etc.)? Something like this. The layers that are closer to the token embeddings represent lower-level token relations, while deeper layers learn to represent higher-level information present in the input sequences. You can check the studies that probe Transformer models to understand what kind of language information is best represented in each layer, e.g.: What Does BERT Learn about the Structure of Language? Linguistic Knowledge and Transferability of Contextual Representations
