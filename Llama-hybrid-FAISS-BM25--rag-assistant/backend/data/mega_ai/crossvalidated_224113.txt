[site]: crossvalidated
[post_id]: 224113
[parent_id]: 224077
[tags]: 
Scaling prevents one feature from swamping the information learned from other features. Mean-zero standardization and normalization in the range [0,1] are the most common. Yes, artificial neural networks (ANNs) waste time learning the correlation between input features, if it exists, so PCA before use of ANNs is highly recommended. For all of the classifiers listed, feature transformation should be performed prior to using any of the them. You can also transform feature values into ranks, but there will be some bias due to ranks having a rectangular distribution. Use of ranks and log-transforms removes outlier effects.
