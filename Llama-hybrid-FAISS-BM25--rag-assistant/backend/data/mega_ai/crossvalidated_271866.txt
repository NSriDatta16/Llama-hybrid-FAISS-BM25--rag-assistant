[site]: crossvalidated
[post_id]: 271866
[parent_id]: 172666
[tags]: 
Random Forrest and other techniques that incorporate bagging are using the fact that the bootstrap sample that is drawn for the current tree excludes some data points, the so-called Out-Of-Bag samples (OOB). Since these samples are not used to build the current tree, they can be used to evaluate it without the risk of overfitting. With other supervised learning techniques that usually do not suffer from instability as much as decision trees (e.g. SVM), you usually do not draw bootstrap samples and thus you can not estimate variable importance in this way. However, the approach of training a model with different subsets of variables and evaluate their performance using k-fold cross-validation is also perfectly valid and called Wrapper approach in the literature. For instance, a popular feature selection technique with SVM is recursive feature elimination (see https://pdfs.semanticscholar.org/fb6b/4b57f431a0cfbb83bb2af8beab4ee694e94c.pdf )
