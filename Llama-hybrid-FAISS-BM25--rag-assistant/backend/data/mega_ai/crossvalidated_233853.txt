[site]: crossvalidated
[post_id]: 233853
[parent_id]: 
[tags]: 
Machine learning | Classifier method that uses distributions instead of scalar point values?

I usually use Random Forest or Logistic Regression classifiers. My data looks something like this in the toy example below where each i,j value is an int or float : A I have been experimenting with some feature reduction techniques and realized some of the attributes I can actually just group together. After grouping them, I end up with a distribution. In the toy example below, they are normally distributed but they can be anything. In Python 3 with SciPy , I could populate with i_j = scipy.stats.norm(loc=0, scale=1) for each of the cells . B My question is if there are any algorithms that can take in distributions as the training data? For example, if I trained on something like B and predicted on something like A . Sorry if this is a dumb question but I have been thinking about it and wanted to ask the community if it is something familiar to anyone. I tried looking it up but I'm not entirely sure what to search. I saw Linear regression problem with multi-dimensional vectors instead of scalar values as predictions but it was a little different.
