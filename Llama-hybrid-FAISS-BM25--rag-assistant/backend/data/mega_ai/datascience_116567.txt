[site]: datascience
[post_id]: 116567
[parent_id]: 
[tags]: 
Is it ok to use MC-dropout technique to estimate uncertainty without putting dropout after every weight layer?

In the paper by Kendall and Gal (What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?), dropout is being set after every convolutional layer. However, is it still legit to estimate epistemic and aleatoric uncertainty on the same way but using different architecture that doesn't have dropout after every weight layer but rather putting it only on FC layers in the network?
