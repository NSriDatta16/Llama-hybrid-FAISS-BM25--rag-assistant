[site]: crossvalidated
[post_id]: 113541
[parent_id]: 105578
[tags]: 
If you know the minimum and maximum values for the reward (let us denote them $R_{min}$ and $R_{max}$), you can easily derive the minimum and maximum values for the $Q$-value, by a simple application of power series: $$ Q_{max} = R_{max} + \gamma R_{max} + \gamma^2 R_{max} + \ldots = \frac{R_{max}}{1-\gamma} $$ Similarly, $Q_{min} = R_{min} + \gamma R_{min} + \gamma^2 R_{min} + \ldots = \frac{R_{min}}{1-\gamma}$. As an example, in the case where $R_{max} = 5, R_{min}=-5 \text{ and } \gamma = 0.9$, we know that the $Q$-values will be lying in the interval $[-50, 50]$. Given these bounds, you can easily divide the interval into $n$ buckets of equal size, where $n$ is the number of distinct discrete values you want to use for the $Q$-value. The boundaries for these intervals will be $$ Q_{min} + i \frac{Q_{max}-Q_{min}}{n} \text{ for } i = 1, 2 \ldots (n-1) $$ When doing the Q-learning update, you can then simply check in which of the $n$ buckets the new Q value will fall into, and map your initial, real-valued Q estimate to this discretised value. To make things easier, the discretised values for each bucket might correspond to the mean value for the sub-interval. In the example above, one could use the two values -25 and 25 as representative of the two intervals $[-50,0]$ and $[0,50]$.
