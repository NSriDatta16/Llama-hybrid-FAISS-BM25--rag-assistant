[site]: datascience
[post_id]: 54235
[parent_id]: 54208
[tags]: 
As others before me pointed out you should have exactly the same variables in your test data as in your training data. In case of one-hot encoding if you have unseen categories in your test data your model doesn't know how to handle them it was not trained on those variables. In that case during data preparation you shall create all the variables that you had during training with the value of 0 and you don't create new variable for the unseen category. I think your confusion and the differing number of variables come from the function that you use to do the one-hot encoding for you. Probably you run them on the two datasets separately and it will only create the variables that it founds in the specific datasets. You can overcome on it by using label encoder or onehotencoder transformer from scikit-learn that will save inside its obeject the original state and in every transformation it will recreate exactly the same structure. UPDATE to use sklearn onehotencoder: from sklearn.preprocessing import OneHotEncoder encoder = OneHotEncoder(handle_unknown='ignore') encoder.fit(train_categorical_data) encoded_train=encoder.transform(train_categorical_data) encoded_test=encoder.transform(test_categorical_data) You can save the encoder to use it later. See more about it in the official documentation .
