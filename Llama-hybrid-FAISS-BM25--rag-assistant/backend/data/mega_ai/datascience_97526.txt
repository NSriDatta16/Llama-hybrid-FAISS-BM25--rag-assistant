[site]: datascience
[post_id]: 97526
[parent_id]: 97521
[tags]: 
The number of features can be used to handle two situations: High bias (the common one): Adding features is one way to approach models with high bias because additional features can increase the predictive power of your data. This is commonly done as part of feature engineering. High variance (the uncommon one): However, a large number of features can also cause overfitting and in cases with high variance reducing the number of features may reduce the extend your models overfits. This is not very common because usually eliminating features is done to get rid of irrelevant or redundant features but usually the goal is not to reduce variance (rather other techniques, such as reducing model complexity through regularization, are applied). Therefore, it is not totally unexpected that your model performance improves with more features but it is important to check if this performance increase generalizes too (e.g. by applying cross validation) and you do not just overfit.
