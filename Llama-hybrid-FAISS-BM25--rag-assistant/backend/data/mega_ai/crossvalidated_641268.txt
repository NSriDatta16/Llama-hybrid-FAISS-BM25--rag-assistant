[site]: crossvalidated
[post_id]: 641268
[parent_id]: 641258
[tags]: 
If you train on the entire dataset, you lose the ability to objectively assess your model's performance on new, unseen data. This could lead to a model that performs exceptionally well on your current data (since it has seen all of it) but fails to perform on future data because it has not been properly validated for generalization. Even though you mentioned that your model has never overfitted with an $80\%:20\%$ split, the absence of a test set means there's no safeguard to check for overfitting in future changes to your model. Overfitting happens when the model learns the noise in the training data to the extent that it negatively impacts the performance on new data. Without a test set, you might not realize when your model starts to overfit. Even if you're using predefined hyperparameters for XGBoost , there's always a possibility that these are not optimal for your specific dataset. Typically, hyperparameters are tuned on a validation set (or through cross-validation) to improve model performance. Without this step, you might be missing out on improved performance that could be achieved with better-tuned hyperparameters. Solely relying on online evaluation can be risky, especially in a production environment. If the model performs poorly, it can lead to loss of revenue, customers, or credibility. Online evaluation should complement offline evaluation, not replace it.
