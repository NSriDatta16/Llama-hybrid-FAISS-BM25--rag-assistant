[site]: crossvalidated
[post_id]: 631060
[parent_id]: 
[tags]: 
Is $\ell_1$ regularization not compatible with SVM?

In the notes of Andrew Ng's CS229 Machine Learning course, it is mentioned: The $\ell_2$ norm regularization is much more commonly used with kernel methods because $\ell_1$ regularization is typically not compatible with the kernel trick. Knowing that Lasso regression can lead to sparse models where many parameters are updated to 0, and knowing that SVM (which uses the kernel trick for linearly non-separable cases) benefits from sparsity of non-zero $\alpha$ parameters, I wonder if the above statement implies that $\ell_1$ regularization is considered incompatibile with SVM as well. Or is it that the regularization used in SVM's soft margin is neither Lasso, nor Ridge, because it involves a different "slack variable", not the actual $\alpha$ parameters? My intuition tells me that the sparsity of support vectors is a consequence of the SMO algorithm, not of the regularization per se, and the regularization itself is neither Ridge, nor Lasso. If my intuition is not correct, can someone please point me in the right direction? Thank you!
