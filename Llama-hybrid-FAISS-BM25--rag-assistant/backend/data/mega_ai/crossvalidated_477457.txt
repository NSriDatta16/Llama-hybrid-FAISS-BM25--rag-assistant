[site]: crossvalidated
[post_id]: 477457
[parent_id]: 113177
[tags]: 
I will try to give a more thorough answer building on Donbeo's answer and Itachi's comment. Can Random Forests overfit? In short, yes, they can. Why is there a common misconception that Random Forests cannot overfit? The reason is that, from the outside, the training of Random Forests looks similar to the ones of other iterative methods such as Gradient Boosted Machines, or Neural Networks. Most of these other iterative methods, however, reduce the model's bias over the iterations, as they make the model more complex (GBM) or more suited to the training data (NN). It is therefore common knowledge that these methods suffer from overtraining, and will overfit the training data if trained for too long since bias reduction involves an increase in variance. Random Forests, on the other hand, simply average trees over the iterations, reducing the model's variance instead, while leaving the bias unchanged. This means that they do not suffer from overtraining, and indeed adding more trees (therefore training longer) cannot be source of overfitting. This is where they get their non-overfitting reputation from! Then how can they overfit? Random Forests are usually built of high-variance, low-bias fully grown decision trees, and their strength comes from the variance reduction that comes from the averaging of these trees. However, if the predictions of the trees are too close to each other then the variance reduction effect is limited, and they might end up overfitting. This can happen for example if the dataset is relatively simple, and therefore the fully grown trees perfectly learn its patterns and predict very similarly. Also having a high value for mtry , the number of features considered at every split, causes the trees to be more correlated, and therefore limits the variance reduction and might cause some overfitting (it is important to know that a high value of mtry can still be very useful in many situations, as it makes the model more robust to noisy features) Can I fix this overfitting? Like always, more data helps. Limiting the depth of the trees has also been shown to help in this situation, and reducing the number of selected features to make the trees as uncorrelated as possible. For reference, I really suggest reading the relative chapter of Elements of Statistical Learning, which I think gives a very detailed analysis, and dives deeper into the math behind it.
