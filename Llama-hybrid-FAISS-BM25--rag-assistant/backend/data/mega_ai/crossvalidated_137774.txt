[site]: crossvalidated
[post_id]: 137774
[parent_id]: 137768
[tags]: 
The issue of having too many variables in your model is called over-fitting. In predictive modeling, the idea is to use the data at hand to discover the trends that exist and that can be generalized to future data. By including variables in your model that have some minor, non-significant effect you are abandoning this idea. What you are doing is considering the specific trends in your specific sample that are only there because of random noise instead of a true, underlying trend. In other words, a model with too many variables fits the noise rather than discovering the signal. As an exaggerated example, consider a model where $\hat{Y}_i=Y_i$. In other words, the predicted value equals the observed value. In this example we have an extreme case of over-fitting (in fact, we have a perfect fit). However, we are not discovering the general trends that the data is generated from. When you have more variables in the model, $SSE=\sum_i^n(Y_i-\hat{Y}_i)^2$ is necessarily decreasing which means, on average, you are getting closer to the exaggerated $\hat{Y}_i=Y_i$ example. It might seem weird that we want our predicted value to be close to the observed value, but not too close . This is why many people like cross-validation techniques to help with model selection and it's one of the reasons why some people will say "statistics is an art not a science".
