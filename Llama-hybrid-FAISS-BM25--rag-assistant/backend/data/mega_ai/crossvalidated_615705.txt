[site]: crossvalidated
[post_id]: 615705
[parent_id]: 
[tags]: 
With ReLU activation, are we adding if conditions to a neural network's toolset?

I was reading Reverse Engineering a Neural Network's Clever Solution to Binary Addition . Without repeating the whole article, it seems the network figured out addition was already a part of its toolset (maybe the addition in x*W + B?), so instead of coming up with an logic gate-based adder like a human might it "cheated". For some reason, I immediately thought of ReLU activation when reading this. We know that ReLU and its cousins (SwiGLU, ELU, GELU) work better in practice than sigmoid/tanh. The vanishing gradient problem in sigmoid/tanh is often said to be reason and I'm mostly satisfied with that answer, but it's still not completely intuitive to me why ReLU works better. Is it right to think of ReLU as adding conditional statements to the network's toolset? With ReLU, the network can do boolean operations. For example, it's easy to come up with weights such that C = ReLU(Linear(A, B)) will reliably activate iff A and B both activate. We've effectively given the network the tools to build an AND gate when it needs one. Whereas with sigmoid/tanh something like this would be difficult due to the overall smoothness. I know this is a bit handwavy, but I find it much easier to work with ideas when they "fit" in my head perfectly, so I'd love to get a intuitive explanation for ReLU superiority.
