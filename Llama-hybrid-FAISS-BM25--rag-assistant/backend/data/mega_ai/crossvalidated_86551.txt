[site]: crossvalidated
[post_id]: 86551
[parent_id]: 86207
[tags]: 
In your clf , coef_ are the weights assigned to the features; (Note it only works for linear SVM) support_vectors_ and support_ are the support vectors and the corresponding index; dual_coef_ is the coefficients of the support vector in the decision function; and intercept_ is the bias in decision function. In linear SVM, $w^Tx+b=0$ is the decision boundary, and $w$ is the coefficients of the support vectors, $b$ is the bias, all defined above. The document reference: http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html For an RBF SVM case, original data space is transformed into another high-dimensional space. So the weights coefficients are not directly related to the input space. I think that's why coef_ cannot be viewed as the weights in your original input space. By the way, I'm not sure whether coef_ is the weight in the transformed space feature, I guess it's not, as the RBF space is actually infinite dimensional. I suggest you preprocess the data before SVM implementation since SVM is heavily influenced by feature scale variances. Standard normalization of the data; Decorrelation sigma^(-1/2)*X where sigma = cov(X) . Yet you may need to do some calculation by yourself to get the decision function. Compute the feature vector v from your data point under test. The length of v is supposed to be the same as the rows of support_vectors_. For each row i in support_vectors_, compute the Euclidean distance d[i] = numpy.linalg.norm(support_vectors_[i,] - v) . t[i] = exp{-gamma *d[i].^2} where gamma is the RBF parameter. Sum up dual_coef_[i] * t[i] over all i , then plus intercept_ . This will be the decision function.
