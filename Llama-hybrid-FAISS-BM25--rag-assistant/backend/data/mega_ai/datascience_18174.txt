[site]: datascience
[post_id]: 18174
[parent_id]: 
[tags]: 
What might explain a dramatic vertical jump in sensitivity

Context: I'm training an RNN with LSTM layers using the keras api. I have sequences of 20 timesteps, but just a binary response. For example: X=[[[1,2,3], [2,3,1]], [[4,1,2], [2,1,3]]], and y = [[1.0], [0.0]]. The binary variable y explains the action of the next timestep, because that is all I'm interested in, even though each vector in the sequence has an associated response to it. When training the model with one timestep ahead, I get specifications that look like the following. Everything looks normal there. However, since I also want to predict two timesteps ahead, I train the same model, but cutoff the last vector in the sequence, but keeping the same response, thereby forcing to look ahead by two timesteps. After plotting the same curve, something strange happens: A sharp vertical jump occurs around the cutoff of .43, in such a way that I've never seen before. When I test it with 3 timesteps ahead, the exact same thing happens, just at a different cutoff (around .2). What I curious about, is this common, and is it indicative of something greater? Why would such a drastic change in just the sensitivity occur? I'm not sure if it's relevant, but this is the layout of the model: model = Sequential() model.add(Masking(mask_value=0, input_shape=(seq_length, 66))) model.add(LSTM(64, return_sequences=False, W_regularizer=regularizers.l2(.001))) model.add(Dropout(.2)) model.add(Dense(1, activation='sigmoid')) rms = RMSprop(lr=.001) model.compile(loss='binary_crossentropy', optimizer=rms, metrics=['acc', 'fmeasure'])
