[site]: datascience
[post_id]: 108792
[parent_id]: 
[tags]: 
Why is the k-fold cross-validation needed?

I am using k-fold cross-validation but do not understand it's aim. Before splitting the data set in training and test data set, one usually randomizes the entries of the data set. Given the training data set, the k-fold cross validation is done with the purpose of estimating beforehand how well the model would perform. Given the randomization, it is unlikely that there will be a dramatic change from one run into the next one in the loop of the cross-validation. At most one observes some fluctuation which is normal. Given the performances inside the cross-validation, what question does the cross validation answer ? Assuming that the performance is low, does it mean that we should abandon training the model ? I could understand the aim of the k-fold cross validation, if training the whole model is expensive in time or other resources. In that scenario splitting the training set in k subsets on which the validation is done, could potentially provide a forecast for the performance of the model. Given that even on big data, fitting the model on the entire training set is done with acceptable time complexity, I thus do not understand the role of cross-validation. In case we use many estimators on the same data set aiming to pick the best one, eg. SVM, logistic regression, deep learning, do one has to perform the k-fold cross-validation on each one of them ? Can one expect that the performance of the k-fold cross-validation will be similar along the different estimators ? What is the best strategy ? Thanks for any answer.
