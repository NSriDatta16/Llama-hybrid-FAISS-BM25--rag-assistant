[site]: datascience
[post_id]: 92756
[parent_id]: 92670
[tags]: 
In practice, we never use any of the algorithms you list as base classifiers for Adaboost except for decision trees . Adaboost (and similar ensemble methods) were conceived using decision trees (DTs) as base classifiers (more specifically, decision stumps , i.e. DTs with a depth of only 1); there is a good reason why still today if you don't specify explicitly the base_classifier argument in scikit-learn's AdaBoost implementation, it assumes a value of DecisionTreeClassifier(max_depth=1) ( docs ). DTs are suitable for such ensembling because they are essentially unstable classifiers (this is also the reason they succeed as base classifiers in Random Forests, while you have never heard of "Random kNNs" or "Random SVMs"); this is not the case with SVM, kNN, or linear models, let alone models that they are themselves ensembles , like Random Forests and Boosted Trees (XGboost). Notice the following remark in the seminal paper by legendary statistician (and RF inventor) Leo Breiman on Bagging Predictors : Unstability was studied in Breiman [1994] where it was pointed out that neural nets, classification and regression trees, and subset selection in linear regression were unstable, while k-nearest neighbor methods were stable. None of these algorithms (except Decision Trees) is expected to offer much when used as base classifiers for Adaboost (something you seem to have already discovered yourself, judging from the comments in the other answer). Attempting to use them simply because the framework (here scikit-learn) superficially allows us to do so is not a reason to do it. See also the related Stack Overflow threads: Execution time of AdaBoost with SVM base classifier using random forest as base classifier with adaboost
