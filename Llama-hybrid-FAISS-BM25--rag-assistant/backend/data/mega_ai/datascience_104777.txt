[site]: datascience
[post_id]: 104777
[parent_id]: 104768
[tags]: 
Boosting is just a special way to fit some model by trying to successively/repeatedly "explain" the residual. See a minimal example for a linear booster here . So essentially the xgboost model with gblinear will be a "normal" linear model . From your question I would not expect that a linear booster delivers good results against the backdrop of your problem. I think if you want to use other models than NN, you have several options. Use boosting with "tree based" ( gbtree ). This will fit a model which is essentially "non-parametric". However, the success of this strategy will depend on the explanatory power of your "x" variables (which you did not mention in the question). Use linear-style models with more general structure ( i.e. generalised additiove models, GAM ). These model family is extremely well suited to fit highly non-linear functions. Find a minimal example here . There are GAM for Python and R. My minimal example would yield the following result (see figure). The blue line is a "normal" linear model, the black line is a fitted GAM model (red is the ground truth). If you know the parameterization of your model (more or less), you could also define a linear model (with proper parameterization) to solve your model. However, this seems to be a less attractive solution. It can be daunting to find a proper representation for the data. Introduction to Statistical Learning (ISL) provides a good overview of GAM models if you want to have a further look. There are also Python examples .
