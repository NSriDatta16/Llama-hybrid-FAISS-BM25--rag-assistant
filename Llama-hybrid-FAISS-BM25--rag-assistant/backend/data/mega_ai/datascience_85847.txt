[site]: datascience
[post_id]: 85847
[parent_id]: 
[tags]: 
Role of decoder in Transformer?

I understand the mechanics of Encoder-Decoder architecture used in the Attention Is All You Need paper. My question is more high level about the role of the decoder. Say we have a sentence translation task: Je suis ètudiant -> I am a student The encoder receives Je suis ètudiant as the input and generates encoder output which ideally should embed the context/meaning of the sentence. The decoder receives this encoder output and an input query ( I , am , a , student ) as its inputs and outputs the next word ( am , a , student , EOS ). This is done step by step for every word. Now, do I understand this correctly that the decoder is doing two things? Figuring out relationship between the input query and encoder embedding i.e how is the query related to the input sentence Je suis ètudiant Figuring out how is the current query related to previous queries through the masked attention mechanism. So when the query is student , the decoder would attend to relevant words which have already occurred ( I am a ). If this is not the right way to think about it, can someone give a better explanation? Also, if I have a task of classification or regression for a time series, do I need the decoder? I would think just the encoder would suffice as there is no context in the output of the model.
