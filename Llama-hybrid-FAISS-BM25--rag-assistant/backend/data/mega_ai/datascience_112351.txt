[site]: datascience
[post_id]: 112351
[parent_id]: 
[tags]: 
How to understand to what maximum size you can reduce the dimension of data and avoid the curse of the dimensionality?

i have a question, maybe someone could help me. I use t-sne (also tried umap) to reduce the dimensionality of the text embeddings dataset (size of embedding 300). after that I will cluster using dbscan. the question is, how to understand to what maximum size you can reduce the dimension of embbedings and avoid the curse of the dimensionality?
