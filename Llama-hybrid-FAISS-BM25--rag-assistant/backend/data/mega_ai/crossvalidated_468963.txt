[site]: crossvalidated
[post_id]: 468963
[parent_id]: 468821
[tags]: 
Bayesian inference is as coherent way to include prior knowledge. To my knowledge, only the mcp package allows for setting priors on change points. There are several approaches to your model comparison problem ( read more about them here ): Set a prior for the change point with greater density at x = 14 (e.g., prior = list(cp_1 = "dnorm(14, 2)") ) and then compare the posterior densities at 13 and 14. Same as (1) but with a flat prior (the mcp default for 1-change-point models) and then express your prior knowledge by multiplying the Bayes Factor for x = 14 with your relative prior credence in that model. Fit models with the change point fixed at 13 and 14 and compare them ( mcp uses leave-one-out cross validation). I will show approach (2) here. You can read a general introduction to mcp here . Set up the data, the model, and the prior. Think hard about which distribution best expresses your prior knowledge. Is a normal distribution fitting and what is it's mean and dispersion? Read more about mcp priors : df = data.frame(x = x, y = y) model = list( y ~ 1 + x, # Standard regression ~ 0 + x # change in slope but not intercept ) Then sample both the prior and the posterior with extra iterations and do a visual of the result (also check out summary(fit) and plot_pars(fit) ). library(mcp) fit = mcp(model, df, sample = "both", iter = 10000) plot(fit, lines = 40, q_fit = TRUE) For this data set there is greater density around 10, which seems to capture the data well. The red lines are the posterior 95% interval and the gray lines are draws from the posterior, showing some models that the sampler was "considering". The change point posterior is quite wide which is simply an expression of the fact that there is little data. Try adding more data and you will see narrower priors. This should not make you feel bad about going Bayesian - it should make you feel bad about the lacking quantification of uncertainty in many other change point approaches :-) Now for the model comparison. We can compute the change in credence (density) from the prior to the posterior which is the Savage-Dickey Bayes Factor. We do this for both 13 and 14, and I throw in an extra directional test too: > hypothesis(fit, c("cp_1 = 13", "cp_1 = 14", "cp_1 Notice that all the Bayes Factors are quite close to 1 (undecided), again because of the small dataset. The evidence for 13 over 14 (with this prior and data and model) is now 1.3 / 0.90 = 1.44 , i.e. 1.44 times more likely than before you observed the data. If you have prior knowledge that cp_1 = 14 is double as likely, simply do 1.3 / (0.90 * 2) = 0.72 , i.e., the change point at 14 is a bit more likely, but not twice as likely anymore. This is not cheating - this multiplying is coherent within a Bayesian Framework. Disclosure: I am the developer of mcp .
