[site]: crossvalidated
[post_id]: 48741
[parent_id]: 48739
[tags]: 
I would suggest that the massive coefficients, and the correspondingly massive standard errors, would almost definitely be caused by quasi-complete or complete separation. That is, for some combination of parameters, either everyone had the outcome or nobody had the outcome, and so the coefficient heads towards infinity (or negative infinity.) This tends to happen especially when one specifies a lot of interaction terms, as the chances of having a combination of factors which results in some "empty" (no outcomes in cell, or everyone has outcomes) cells will increase. See the following page for some further details and suggested strategies (link updated March 2021): https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faqwhat-is-complete-or-quasi-complete-separation-in-logisticprobit-regression-and-how-do-we-deal-with-them/ More generally, it means that you're probably trying to do "too much" with your model for the size of your dataset (particularly the number of outcomes observed). EDIT: A couple of pragmatic suggestions You might try (1) quick and simple: drop the interaction terms from your model, to see if that helps (whether this makes sense from a research question perspective is an entirely different issue); or (2) get R to make you a bi-i-i-i-g contingency table for (e.g. rows) the combinations described in the interactions by (e.g. columns) the outcome variable. You might be able to see some evidence of separation here.
