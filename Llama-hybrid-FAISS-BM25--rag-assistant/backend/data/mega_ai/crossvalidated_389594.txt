[site]: crossvalidated
[post_id]: 389594
[parent_id]: 389189
[tags]: 
It all comes down to what you're able to assume for a particular problem. The more assumptions that can be reasonably made, the easier it will be to solve the problem. In the very worst case, it can only be assumed that the current output $y_t$ is conditionally independent of future inputs/outputs, given the current input $x_t$ and the entire history of inputs and outputs. In this case, the likelihood function factorizes as: $$p(y_1, \dots, y_n \mid x_1, \dots, x_n, \theta) = \prod_{t=1}^n p(y_t \mid x_1, \dots, x_t, y_1, \dots, y_{t-1}, \theta)$$ where $\theta$ is a parameter vector. This hardly constrains the problem at all, as the conditional distribution of $y_t$ could be different at each moment in time. To have any hope of learning, further structure is necessary. For example, a common assumption might be that the conditional distribution of $y_t$ is the same for all timepoints. We might additionally assume that all outputs are conditionally independent, given the history of inputs (i.e. there's no correlated noise). Then, the likelihood function factorizes as: $$p(y_1, \dots, y_n \mid x_1, \dots, x_n, \theta) = \prod_{t=1}^n p(y_t \mid x_1, \dots, x_t, \theta)$$ To learn in this setting, we'd have to specify a proper form for $p(y_t \mid x_1, \dots, x_t, \theta)$ --a function that outputs a distribution over classes at each timepoint, given the entire history of inputs. For example, a recurrent neural net (RNN) with softmax outputs could work. Then, the parameters $\theta$ are learned by maximizing the likelihood, performing Bayesian inference, etc. Keep in mind that, even though the outputs of functions like RNNs technically depend on the entire input history, learning long-range temporal dependencies is very difficult. This is an active area of research, and we don't have a great solution yet. In many problems, it can also be reasonable to assume that the data-generating process has finite memory, so the output at each timepoint only depends on the current input and the previous $d$ inputs. In this case, the likelihood function factorizes as: $$p(y_1, \dots, y_n \mid x_1, \dots, x_n, \theta) = \prod_{t=1}^n p(y_t \mid x_{t-d}, \dots, x_t, \theta)$$ Note that, if we ignore the first $d$ timepoints, we can construct a set of 'augmented feature vectors' containing time-lagged inputs. That is, $z_t = [x_{t-d}, \dots, x_t]$ . In this case, each output $y_t$ depends only on the augmented input $z_t$ . The problem is then to predict $y_t$ as a function of $z_t$ , which can be solved using any off-the-shelf classifier designed for i.i.d. data. Finally, note that any cross-validation procedures used for model selection and/or performance evaluation must properly take temporal dependence into account. The usual procedures designed for i.i.d. data are not valid in this setting.
