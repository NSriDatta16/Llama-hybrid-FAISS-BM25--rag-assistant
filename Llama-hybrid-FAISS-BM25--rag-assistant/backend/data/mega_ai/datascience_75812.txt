[site]: datascience
[post_id]: 75812
[parent_id]: 39010
[tags]: 
You can try this method: from keras.preprocessing.text import Tokenizer from gensim.models import KeyedVectors # X is the corpus # GLOVE_DIR is the glove model # EMBEDDING_DIM is the embedding demension of glove model VOVAB_SIZE = 10000 tokenizer = Tokenizer() tokenizer.fit_on_texts(X) word_index = tokenizer.word_index glove_model = KeyedVectors.load_word2vec_format(GLOVE_DIR, binary=True) num_words = min(VOCAB_SIZE, len(word_index) + 1) embedding_matrix = np.zeros((len(num_words) + 1, EMBEDDING_DIM)) for word, i in word_index.items(): if i the embedding_matrix is the most frequent 10000 words in your corpus
