[site]: crossvalidated
[post_id]: 532265
[parent_id]: 532256
[tags]: 
RNN is an abbreviation for Recurrent Neural Networks, a family of models to which models such as LSTM (Long Short Term Memory) and GRU (Gated Recurrent Unit) belong. The main feature of recurrent networks is taking as input an output produced by itself at previous step . What follows, RNNs share weights used for different data points and can take as inputs sequences of any length. There are vanilla RNNs that do just that, below you can find a figure illustrating it. In comparison, LSTM has additional components, the “forget gate” and “input gate”, shown on the diagram below. The role of those gates is to cancel and amplify signals, instead of just passing them forward. LSTMs are one of the solutions for vanishing gradient problem observed in RNNs. Both images are taken from this blog , that goes into more details.
