[site]: crossvalidated
[post_id]: 359433
[parent_id]: 359420
[tags]: 
Assuming you had many more levels of the object variable, this might be a simpler problem. A standard approach would be to fit the linear model using generalized least squares (GLS), given the lack of balance. By lack of balance, I mean each object was measured a different number of times. Using GLS, you can estimate a group difference based on the response variable, but adjust this group difference for the correlations between observations obtained from the same object. We can also assume that the observations had different variances depending on the group to which they belonged. Your plot appears to suggest that objects in group A had more variance than objects in group B. Given your data, here's how we would proceed: library(nlme) summary(mod We can break down the results in how they are printed. The first thing we pay attention to is the Correlation Structure. We assumed it to be compound symmetry by using the syntax, corCompSym(form = ~ 1 | object) . This means we believe that the observations within an object are correlated at the same value. Based on the results, the software believes this value to be .97. This means the correlation between response values within each object are very highly correlated, almost identical as the repeated measures of each object do not show much variability, relative to the variability across objects. We move on to the next set of results under the Variance function. This suggests that observations in group A have a residual standard deviation of $1 \times 32.72$, 32.72 is the residual standard deviation printed at the bottom of the results. The observations in group B have a residual standard deviation of $.597 \times 32.72$, much smaller as your plot suggests. We have these different variances because we specified the weights = varIdent(form = ~ 1 | group) option. We next proceed to the coefficients which appear to be of topmost interest to you. The group difference adjusted for the correlation is 38.3 points, with a p-value of .037. The p -value would be much smaller had we ignored the correlation between the observations (I found it be near 0). Having walked through these steps, there are reasons to be skeptical of these results. Most importantly, there is good reason to doubt the .97 correlation we estimated given the small sample size we have (10 objects). And if that estimate is wrong, our group difference alongside statistical test may be adversely affected. You could actually simplify the analysis by averaging the response within each object to obtain a single value per object. This is a conservative approach as you lose sample size. Though given that high .97 correlation or more importantly, your plot above, the repeated measures about each object do not contain much additional information about the object. One can proceed thus: new_dat We now have 10 rows of data, one per object. First, what is the group mean difference? coef(lm(value ~ group, new_dat)) (Intercept) groupB 60.83597 -38.27042 The group mean difference of 38.3 is very close to the value we obtained earlier. Next, we can conduct an independent samples t -test using Welch's adjustment to account for heteroskedasticity: t.test(value ~ group, new_dat) Welch Two Sample t-test data: value by group t = 2.246, df = 4.84, p-value = 0.07641 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -5.969281 82.510118 sample estimates: mean in group A mean in group B 60.83597 22.56556 We now have a new statistical test. Assuming there is no group difference, we have an 8% chance of observing the mean difference we have observed. That's a pretty low number, however it does not account for all sources of uncertainty, such as having run a number of models to reach this point. Edit in relation to comments GLS does assume normality. To examine the complete set of normality assumptions, we can refit this model as a multilevel or mixed-effects model. When we assume that the observations within objects are correlated at the same value using GLS, it is equivalent to fitting a mixed effects model permitting the intercept of the model to be different depending on the object. This kind of varying intercept is known as a random-intercepts model. These two approaches are equivalent under normal circumstances, but on trying just now, I just realized that they differ slightly when you permit heteroskedasticity, i.e. by using weights = varIdent(...) . I will proceed thus: mod.me The formula reads, predict value using the group variable (value ~ group) , but permit the intercept to be different depending on the object (random = ~ 1 | object) . In this model, we can test that the residuals using the syntax, resid(mod.me) . However means by which you test the residuals for normality is up to you. The additional check is we assumed the individual object means arise from a normal distribution. We can retrieve them using ranef(mod.me)[, 1] and test them however we test normality. Note that at larger sample sizes, especially of the number of objects, central limit theorem kicks in, such that we may trust the inference. Parameterizing the model this way, as a mixed effects model, shows the need for a larger number of objects in your toy example. It is impossible to verify the normality of 10 data points. If there are more than two groups, there is the emmeans package. I do not use post-hoc tests in my own work, so I just you read the documentation. Normally, I might read up to give you a better response but I do not value such corrections. The simplest application of pairwise comparisons would be: pairs(emmeans(mod.me, "group")) . There is a contrasts() function in there too.
