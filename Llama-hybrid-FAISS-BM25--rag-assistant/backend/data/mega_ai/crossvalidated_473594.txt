[site]: crossvalidated
[post_id]: 473594
[parent_id]: 473588
[tags]: 
Ridge regression is defined in terms of applying $L_2$ penalty to the weights of the model $$ \operatorname{arg\,min}_\boldsymbol{\beta} \| y - \mathbf{X}\boldsymbol{\beta} \|^2_2 + \lambda\| \boldsymbol{\beta} \|^2_2 $$ LASSO uses $L_1$ penalty instead, dropout is applied to neural network weights as well, etc. Regularization is about making models less complex, not the data. Weights are part of the model and make sense only together with other weights, for example, you couldn't "copy and paste" weights between different regression models. So the fact that regularization has zeroed-out some weights for one model, doesn't need to mean that those features would not be important for other model. On another hand, there are model agnostic feature-selection algorithms that can be applied to the data to select the "best" set of features that can be used with different algorithms. The purpose of such algorithms it to find such "best" features, but different algorithms may select different features and you never have guarantee that the selected features are "ultimately best".
