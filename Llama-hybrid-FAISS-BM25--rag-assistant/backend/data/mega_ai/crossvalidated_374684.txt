[site]: crossvalidated
[post_id]: 374684
[parent_id]: 
[tags]: 
Clarification regarding Markov Decision Process (MDP) formulation

Most of the reinforcement learning problems are dealt with using an MDP framework. Iâ€™m a bit confused about the formulation after reading the paper: https://arxiv.org/abs/1503.02244 In an continuous MDP, state and action spaces are subsets of Euclidean space. In a Borel MDP, the spaces are Borel subsets of Euclidean space. Question: What does this mean?What are the differences between the two? When do results from one carry over to the other? Thanks!
