[site]: crossvalidated
[post_id]: 641729
[parent_id]: 615725
[tags]: 
I had the same question when I read "Attention is all you need" paper. The reason, in my opinion, is optimization-related. Suppose we are computing the attention between 2 sentences x_1, ..., x_n and y_1, ..., y_m. We will create a nxm attention matrix that we need to optimize. When using softmax, we need to optimize n values only, because for each row, we need to push one of the m value up which will automatically push down the remainings. With sigmoid, we need to optimize nxm values, because they are independant. So even if we use h heads with the softmax attention, we will have an easier optimization problem, in comparison to using a single head with sigmoid. This being said, I did not do the test, and I think it is very interesting to do, because if we can find a way to help the optimization with sigmoid, we can reduce drastically the number of parameters.
