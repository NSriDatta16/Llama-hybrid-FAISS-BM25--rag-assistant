[site]: crossvalidated
[post_id]: 625644
[parent_id]: 625594
[tags]: 
You are correct. This strongly depends on the amount of data and the nature of the data too. The functional form of $y = a/c + \epsilon$ can actually be quite tricky and in the case of a booster, we might severely overfit our training set. That division is a killer in cases where $a$ and $c$ are of different signs, if the sign is the same then the situation is much easier. Ultimately, boosting is performing a recursive partitioning of our sample space (through piece-wise flat functions), which means that even a seemingly trivial $y = x$ relation will need quite a few leaves to give us the perfect result a simple linear regression could do "immediately". That said, gradient boosters can be seen as "universal function approximators" as their building blocks (binary trees) are universal function approximators themselves; please see my answer to the question: XGBoost: universal approximator? for more details. With that in mind, it is good practice to use a baseline model to check if we are getting genuine improvements or not. I made a little toy simulation myself below and we can see that the impact of the easy indicator is massive. In the easy=True case where our covariates come from $N(10,1)$ so sign reversals are rare our booster generalises fine, if our covariates come from $N(0,1)$ (i.e. easy=False ) the test performance is horrible, despite potentially a strong in-sample performance. On the other hand, a simple ridge regression performs much more consistently (bad or good). import numpy as np from xgboost import XGBRegressor from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score from sklearn.linear_model import RidgeCV from sklearn.model_selection import train_test_split N, p = 2000, 3 seed = 2023 rng = np.random.default_rng(seed) easy = False X = rng.normal(10 if easy else 0, 1, [N, p]) y = X[:,0] / X[:,1] + X[:,2] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed) booster = XGBRegressor(n_estimators=200, learning_rate=0.02) booster = booster.fit(X_train, y_train) ridge = RidgeCV() ridge.fit(X_train, y_train) def results(desc, y_true, y_pred): print(f"{desc} has " + f"RMSE of {np.round(mean_squared_error(y_true, y_pred, squared=False),3)}, " + f"MAE of {np.round(mean_absolute_error(y_true, y_pred),3)}, " + f"R2 of {np.round(r2_score(y_true, y_pred),3)}.") results("Ridge Regression on test-set", y_test, ridge.predict(X_test)) results("Ridge Regression on train-set", y_train, ridge.predict(X_train)) results("XGBoost Regression on test-set", y_test, booster.predict(X_test)) results("XGBoost Regression on train-set", y_train, booster.predict(X_train)) from watermark import watermark print(watermark(packages="numpy,xgboost,sklearn")) # numpy : 1.23.5 # xgboost: 1.7.6 # sklearn: 1.2.2
