[site]: crossvalidated
[post_id]: 387469
[parent_id]: 260233
[tags]: 
The algorithm recommended in the question is not guaranteed to be the most efficient in terms of comparisons (even ignoring the number of comparisons done during the sorting phase.) Consider the following algorithm: (Initialization). Calculate the cumulative probabilities $p_k, \space k = 0,\dots, K$ , Sample $u$ from the Uniform distribution on $(0,1)$ , Find the smallest $k$ such that $p_k > u$ using binary search ( https://en.wikipedia.org/wiki/Binary_search_algorithm ). This works because the cumulative probabilities are ordered, and it takes on average $\log_2 K-1$ comparisons to find the answer no matter what the distribution of the probabilities is . The worst-case performance is $\log_2 K + 1$ for the worst possible choice of $u$ , regardless of distribution. The algorithm presented in the OP has a worst-case expected performance of $K/2$ , achieved when all the $p_k$ are equal, but a best-case performance arbitrarily close to $1$ (when one of the $p_k$ has a probability very near $1$ .) It can take as many as $K-1$ comparisons for any given $u$ no matter what the distribution is. Note that for distributions with nonincreasing probabilities, such as the Geometric, the actual order of the elements of the vector being searched is the same for the two algorithms, and is the same as the order of the values themselves. The recommended algorithm becomes a linear search on the vector of cumulative probabilities, which makes for easy comparisons between the two. Interestingly enough, if the lower bound on the values is 0, the expected number of comparisons is equal to the expected value of the variate + 1! We can see that by observing that, in order to return a given value $x$ , $x+1$ comparisons must have been made to reach that position in the list (the first position corresponding to $x=0$ ); and, given that the generator works, it must be that the probability of returning $x$ is equal to $p_x$ . Therefore the probability of making $x+1$ comparisons is $p_x$ , so the expected value of the number of comparisons is $\text{E}x+1$ . Obvious alterations can be made for other lower bounds. Clearly (in this case) if $\text{E}x+1 \geq \log_2K-1$ we would prefer the binary search algorithm. This comparison can obviously be extended to the general case by calculating the expected value of a variate on $[1, \dots, K]$ with probabilities $q_k$ corresponding to the sorted probabilities $p_j$ from the original distribution on whatever the range of $j$ was. It would seem that the more heavily the distribution is concentrated in a small fraction of values, the better the recommended algorithm will be, but if you want something with guaranteed good performance both in terms of expected comparisons across distributions and worst-case comparisons, the binary search approach may be better. (Of course, if $K$ is small enough, a linear search on the CDF will outperform a binary search in runtime if not comparison count, as it avoids calculating and tracking the upper and lower bounds of the interval in which the answer is known to lie.)
