[site]: crossvalidated
[post_id]: 157533
[parent_id]: 157368
[tags]: 
I see you are following a greedy layer-wise training with autoencoders. An autoencoder learns kernels or filters, in the case of images, which are group of units incoming to the same hidden unit. Hence, activation of each hidden unit indicates presence of a certain pattern. If size of input patches are $ mxm$ and number of hidden units are k, there are k learnt filters. You can visualize each kernel by reshaping weight vector incoming to a certain hidden unit. If weight vector is denoted by $ W_{kj}$, than each row of $ W_{kj}$ corresponds to an unrolled kernel which has size $ 1x(m^2)$. If you reshape it by $ mxm$, it turns into interpretable patterns. Function for visualization, 'display_network' is already provided in the site you mentioned. It couldn't really understand your second question. I think it is becacause of what you mean by pacthes. Patches are randomly cropped $ mxm$ parts of images. You are not using these patches during convolution layer, but rather using learnt kernels(filters) that I mentioned previously. After convolving each k kernels with an input image, k feature maps with size $ (n-m+1)x(n-m+1)$ are produced with a 'valid' convolution if the input size is $ nxn$. Than each feature map is passed from an activation function. You can't use the same features in the second layer. Second layer feature should be learned similar to first layer by using activations from the first layer. After convolution/pooling layers, the last feature maps are unrolled into a vector to feed into next fully connected layer. If the last feeature maps are 32@5x5, then the size of the vector is $ 800x1$ for each image. If there are m input images the overall vector is $ 800xm$. EDIT: The first part of the autoencoder is called encoder and the second part is decoder. The second part is used to recostruct the input image, so it is not used after feature learning. If you stack learnt autoencoders only encoder part remains. So, what you are looking for is the first part. The weights you want to visualize is between L1 and L2 layers.
