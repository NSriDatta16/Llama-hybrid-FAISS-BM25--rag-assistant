[site]: stackoverflow
[post_id]: 805170
[parent_id]: 804777
[tags]: 
It has nothing to do with counting up or down . What can be faster is counting toward zero . Michael's answer shows why — x86 gives you a comparison with zero as an implicit side effect of many instructions, so after you adjust your counter, you just branch based on the result instead of doing an explicit comparison. (Maybe other architectures do that, too; I don't know.) Borland's Pascal compilers are notorious for performing that optimization. The compiler transforms this code: for i := x to y do foo(i); into an internal representation more akin to this: tmp := Succ(y - x); i := x; while tmp > 0 do begin foo(i); Inc(i); Dec(tmp); end; (I say notorious not because the optimization affects the outcome of the loop, but because the debugger displays the counter variable incorrectly. When the programmer inspects i , the debugger may display the value of tmp instead, causing no end of confusion and panic for programmers who think their loops are running backward.) The idea is that even with the extra Inc or Dec instruction, it's still a net win, in terms of running time, over doing an explicit comparison. Whether you can actually notice that difference is up for debate. But note that the conversion is something the compiler would do automatically , based on whether it deemed the transformation worthwhile. The compiler is usually better at optimizing code than you are, so don't spend too much effort competing with it. Anyway, you asked about C++, not Pascal. C++ "for" loops aren't quite as easy to apply that optimization to as Pascal "for" loops are because the bounds of Pascal's loops are always fully calculated before the loop runs, whereas C++ loops sometimes depend on the stopping condition and the loop contents. C++ compilers need to do some amount of static analysis to determine whether any given loop could fit the requirements for the kind of transformation Pascal loops qualify for unconditionally. If the C++ compiler does the analysis, then it could do a similar transformation. There's nothing stopping you from writing your loops that way on your own: for (unsigned i = 0, tmp = domain; tmp > 0; ++i, --tmp) array[i] = do stuff Doing that might make your code run faster. Like I said before, though, you probably won't notice. The bigger cost you pay by manually arranging your loops like that is that your code no longer follows established idioms. Your loop is a perfectly ordinary "for" loop, but it no longer looks like one — it has two variables, they're counting in opposite directions, and one of them isn't even used in the loop body — so anyone reading your code (including you, a week, a month, or a year from now when you've forgotten the "optimization" you were hoping to achieve) will need to spend extra effort proving to himself or herself that the loop is indeed an ordinary loop in disguise. (Did you notice that my code above used unsigned variables with no danger of wrapping around at zero? Using two separate variables allows that.) Three things to take away from all this: Let the optimizer do its job; on the whole it's better at it than you are. Make ordinary code look ordinary so that the special code doesn't have to compete to get attention from people reviewing, debugging, or maintaining it. Don't do anything fancy in the name of performance until testing and profiling show it to be necessary.
