[site]: crossvalidated
[post_id]: 312070
[parent_id]: 
[tags]: 
Likelihood of an algorithm-based function

Forgive me if this is a dumb question, but it is way outside of my area of expertise. Suppose I have an algorithm that produces some sort of a fit function, but that fit is not expressed in closed form (e.g., computational models in biology or cognitive psychology). In other words, to find the optimal parameter, I have to simulate data based on the parameters, do a bunch of steps, simulate some sort of psychological or biological process, do a bunch of mathematical gymnastics, then finally I have a simulated behavior based on the $\theta$ values, which I then compare to the actual behavior. I then compute some sort of fit function that compares the simulated behavior to the actual behavior (e.g., sum of squared errors or something), then modify the $\theta$ values until the discrepancy is very small. What I want to do is incorporate some bayesian statistics into this computational model (so I can include prior information). I know that a poster $\propto$ likelihood $\times$ prior. I can easily get the priors (based on the literature), but how do I convert the results of the algorithm into a likelihood? My best guess is to do the following: Run the computational model algorithm with a bunch of different values for the parameter of interest (e.g., $\theta_1$), holding other parameters constant. For each proposed $\theta_1$ value, compute the $sse$. Invert $sse$ (to come up with a crude likelihood measure that increases as fit increases). Scale all inverted $sse$'s such that they sum to one to compute my pdf. Repeat for all $\theta_2:\theta_k$. Will this work to convert the results of an algorithm into a likelihood? If not, what else can I do? Edit Here's a bit more details. I'm trying to make an existing model of facial recognition under a criminal lineup (witness) bayesian. The basic idea is this: Simulate a vector of "features" from a uniform distribution for the criminal Simulate encoding of said features into one's memory by creating a new vector that is identical to the criminal vector for $\theta_1$ of features. We will call this vector the memory vector. Simulate "foils" that are used in the same lineup by creating 5 additional vectors that share $\theta_2$ of the features from the criminal vector. Simulate picking the criminal out of a lineup by computing the dot product between the memory vector and each of the foil vectors and the criminal vector. If the dot product exceeds some value ($\theta_3$), a decision is made as to who the witness thinks committed the crime. Otherwise, the person makes no decision. Record the proportion of times the simulated individuals picked the right person/wrong person/no person and compare it to the actual number of times experimental subjects picked the right/wrong/no person. This is typically done using $sse$. We adjust $\theta_1-\theta_3$ until the simulated data matches closely the experimental data (typically done through genetic algorithm). So, how do I convert $sse|\theta_1, \theta_2, \theta_3$ into a likelihood so I can allow bayesian estimation?
