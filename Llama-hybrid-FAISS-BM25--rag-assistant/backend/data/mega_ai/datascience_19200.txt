[site]: datascience
[post_id]: 19200
[parent_id]: 19191
[tags]: 
I could probably further expand them into individual columns by considering all the combinations like part_X-repaired-..-price, part_X-replaced-..-price, ... part_Z-replaced-..-price but this seems to get out of hand. That is the way I would go if the approach you are using doesn't like high cardinality categorical variables. There might be some interesting relationships that fall out, like part A tends to need a repair when part B is replaced with a cheap version. If you end up with too many variables, you could try something like PCA to reduce them . [Edit] Thought of another thing to look into: Vowpal Wabbit . It is an algorithm targeted almost exactly at your problem - lots of sparse variables. They claim it can handle 1 billion.
