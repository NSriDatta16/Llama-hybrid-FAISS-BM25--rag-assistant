[site]: datascience
[post_id]: 96678
[parent_id]: 73671
[tags]: 
So there are a few things you seem to be confused about. Short answer is no. In Reinforcement Learning (RL), the goal is to learn a policy for taking actions in a Markov Decision Process (MDP) to maximize a reward. If your problem can be described as a Markov Decision Process, then RL may be a good solution. Theoretical results show that with proper annealing, a linear policy, continuous state space, finite actions, the "Q-Learning" RL algorithm will converge to an optimal linear policy, where Q-Learning learns a function that maps from (state, action) to expected discounted sum of reward. A Markov Decision Process is easiest to think of as a graph. In an "episode", we have an initial state (node of the graph), then at each step we transition (along an edge) to another state (node) until we reach (or possibly never reach) a terminal state ending the episode. During each step we also choose an action and receive a reward. What state you transition to after each step is random, but the "transition probability" is a function of your current state and chosen action [ $P(s')=f(s,a)$ ], and our reward is random but the probability is a function of our current state, action, and resulting state [ $P(r)=f(s,a,s')$ ]. Our goal is to maximize the expected sum of this reward (discounted sum technically). In effect, we're randomly bouncing around this graph from node to node, taking actions that influence our destination node, and collecting rewards. In your case, the graph isn't a necessary abstraction and instead our state is a continuous vector. Q-Learning (and RL algorithms in general) learn by playing repeated episodes in our MDP, learning to optimize the discounted sum of rewards. After each episode Q-Learning updates a learned function that maps from (state, action) to expected discounted sum of reward. Algorithms trades off "exploiting" patterns they've learned for reward, and "exploring" new (state, action) pairs. So the algorithm is not necessarily maximizing the reward while training. So to answer your question. Q-learning does not learn within the episode. It updates the learned function after each episode, eventually converging to your final policy. That final policy is what you use in your application. That policy is a function that maps from observed state and action to expected reward. This works as long as there is no "hidden" or "unobserved" information that changes within the episode or from episode to episode. If there is "hidden" information, then a RL/MDP may be a poor fit. It may instead be a "POMDP", which require other tools than RL to solve. Further, if you cannot reset the environment and run multiple episodes, then RL/POMDP/MDP will be a poor fit. That being said, plenty of people have applied RL successfully to problems that don't fit these rules (e.g. multi-agent RL). So if your problem doesn't fit, this is more of a warning than a rule.
