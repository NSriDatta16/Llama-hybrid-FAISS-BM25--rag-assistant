[site]: datascience
[post_id]: 30662
[parent_id]: 
[tags]: 
Neural network does not converge with negative symbols

I've created a simple 2-2-1 feedforward ANN to predict an XOR using Keras. The activation function I'm using on all layers is a tanh , so in order to make use of the entire range of the function, i.e. [-1, 1], I've decided to use -1 instead of 0 as the symbol. My input data is, thus, [[-1, -1], [-1, 1], [1, -1], [1, 1]] , for an output of [[-1], [1], [1], [-1]] . I thought this would give better results since I'm using the entire range of the function and it supposedly would converge better because of that. Also, since I'm just using a different symbol, it should be the same as with using 0 and 1. However, my network can not converge (giving a 0.5 accuracy), and what baffles me the most is that using 0 and 1 as symbols converges, and at a much faster rate. Is there a reason for such a counter-intuitive (at least in my conception) thing to be happening?
