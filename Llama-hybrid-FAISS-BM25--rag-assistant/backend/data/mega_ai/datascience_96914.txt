[site]: datascience
[post_id]: 96914
[parent_id]: 96817
[tags]: 
Yes you are mostly correct. A feedforward neural network with a single layer and a sigmoid activation is a logistic regression which belongs to GLM type of models. Your second statement is unclear (weights interact with outputs) so I will try to break this down below: Non-linear transformations (e.g. polynomial regression, logistic unit etc.) is often misread for non-linearity in model parameters (non-linear models). As an example let's look at a feedforward neural network architecture. For $f(x)$ activation function and $w,b$ weights and biases, the output of a neuron from the first layer of a feed forward network would look like: $$a_{11}=f(w_{11}x_1 + w_{12}x_2 + .. +b_1)$$ whereas the output of a neuron from the second layer neural of that neural network would look like: $a_{21}=f(w_{21}a_{11}+ w_{22}a_{22} + .. + b_2)$ , and given $a_1$ above $ \Rightarrow a_2=f(w_2w_1x+w_2b1+b2)$ $$a_{21}=w_{21}w_{11}x_1 + w_{21}w_{12}x_2+ .. + b_2$$ The multiplication between parameters (here $w_1w_2$ ) is what makes a model non-linear. In order to acquire that you need: Either multiple layers OR A non-linearity from the activation function e.g. if $f(x)=x^2$ even the output from the first layer of the neural network would be $a_{11}=w_{11}^2x^2 + b_2^2+w_{11}b_1x$ that qualifies as parameter multiplication in the $w_{11}^2$ factor and a non-linear model of a non-linear relationship with additive errors. Hope it helps. Sauces: https://jamesmccaffrey.wordpress.com/2019/04/27/the-difference-between-linearly-separable-data-and-a-linear-classifier-in-machine-learning/ https://stats.stackexchange.com/q/344658/110383 https://stats.stackexchange.com/a/71444/110383
