[site]: crossvalidated
[post_id]: 78117
[parent_id]: 77999
[tags]: 
You confusion shows that you are a very exact person! ;-) Assumptions on the notation: - $\theta $: Parameters - $ \mathbf{x} , \mathbf{z} $: Variables Among Bayesian people, when someone talks about estimation , they refer to estimation of almost anything. See this: ftp://ftp.cs.utoronto.ca/pub/radford/bayes-tut.pdf In Neal's tutorial, page 4 he estimates the parameters of the model, given input data, using the probability of the posterior. Finding the maximizer will give you the most-probable estimates (your 1st definition). \n See, your 2nd and 3rd definition are basically the same. In some applications you might have some input $z$, which is usually known. Given the parameter $\theta$, the probability of seeing $X = x$ is: $$ p(X = x | Z = z , \Theta = \theta) $$ The most probable observation is (you second defintion) : $$ \arg\max_{x} p(X = x | Z = z , \Theta = \theta) $$ \n In a model, assume you don't have any input (like mixture of gaussians model). Given the parameter $\theta$, the probability of seeing $X = x$ is: $$ p(X = x | \Theta = \theta) $$ The most probable observation is (your third defintion) : $$ \arg\max_{x} p(X = x | \Theta = \theta) $$
