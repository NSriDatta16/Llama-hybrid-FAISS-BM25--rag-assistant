[site]: crossvalidated
[post_id]: 431995
[parent_id]: 428666
[tags]: 
F1 score is the harmonic mean of precision and recall. Precision is defined as $$\text{precision} = P(Y = 1 | \hat Y = 1)$$ while recall is defined as $$ \text{recall} = P(\hat Y = 1 | Y = 1)$$ Recall does not care about the distribution of true class labels since it is being conditioned upon in the definition. Say you trained your model and the recall is 90%. Putting more positive samples in the test set will not change that. This is not true for precision. You could in principle throw in tons of negative samples in your test set, and the precision will be pushed towards zero. In short, your model might be doing ok. Comparing class distribution sensitive metrics to detect over/under fitting when you up/down sample training set will give you misleading result. To detect over/under fitting in your case, I would suggest look at the ROC curve first. ROC is not sensitive to class distribution. If ROC AUC differs a lot between training and test set, then your are likely to over/under fit. Then you can look at things like average precision or others on test set .
