[site]: crossvalidated
[post_id]: 329908
[parent_id]: 
[tags]: 
Under the 0-1 loss function, the Bayesian estimator is the mode of the posterior distribution

My notes are rather light when it comes to this topic. I understand that the bayesian estimator, defined as (for sample space $\hat{x}$): $E[\Theta | \hat{x}] = \int_{ \forall \Theta}yf_{\theta|\hat{x}}(y|\hat{x})dy $ (ie. the mean of the posterior distribution). You can then look at loss functions where the form of the loss function determines that of $\theta^{'}$ (the estimator of $\theta$). By setting the loss function as a quadratic, absolute error or zero-one you get $\theta^{'}$ as the mean of the posterior, the median of the posterior and the mode of the posterior respectively. The first two proofs make sense to me but it is the 3rd that I am not sure about. This is my understanding: $E[l] = \int l(\theta^{'}, \theta)f(\theta|\hat{x})d\theta = \int (\theta^{'} - \theta)f(\theta|\hat{x})d\theta $ If $\theta^{'} = \theta \Rightarrow E[l] = 0$ If $\theta^{'} \neq \theta \Rightarrow \int (\theta^{'} - \theta)f(\theta|\hat{x})d\theta = 1 $ (as integrating over whole domain of $\theta$). This proof obviously isn't rigorous, (and it could be incorrect too). What I can't see regardless is how this can be regarded as the 'mode' of the posterior distribution. Thanks in advance!
