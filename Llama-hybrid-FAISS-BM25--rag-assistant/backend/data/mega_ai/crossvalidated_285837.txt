[site]: crossvalidated
[post_id]: 285837
[parent_id]: 285575
[tags]: 
Typically a multi-armed bandit approach is not supposed to order your images by popularity, but rather to trade off exploration and exploitation to maximize your cumulative rewards. Your deep learning model gives you a good initial guess for the mean value, and (loosely speaking) the "correlation coefficient" is useful in that it gives you an estimate of the confidence of the model. The next step is to figure out how measurements change your confidence. I think a reasonable approach would be to use posterior sampling (aka Thompson sampling). In this case, you would set up your network so that you feed in on-line data and it produces estimates of the distribution of the popularity of each image. Then at each iteration you sample from the posterior distribution and select the action with the highest sampled value. If your network is actually able to learn the model, then this approach should work pretty well (and you don't have to try and argue that everything is Gaussian!), and I would argue is a step above 'heuristic'. I think if you are trying to incorporate $P(I_k)$ into a sample mean based approach, you should initialize $\mu_i(0) = P(I_k)$, and then update as $\mu_i(t+1) \gets (y_i(t) + (t+b_k) \mu_i(t))/(t+1+b_k)$, where a large value of $b_k$ means that you think $P(I_k)$ is close to the true value. This is essentially like saying that you started by measuring $b_k$ values of $P(I_k)$, before starting all of your experiments.
