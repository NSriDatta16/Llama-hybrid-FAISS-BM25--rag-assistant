[site]: datascience
[post_id]: 126431
[parent_id]: 
[tags]: 
Falcon-7B llm giving random output

I am using a falcon 7B model for a chatbot without any finetuning with the following code model_name = "ybelkada/falcon-7b-sharded-bf16" bnb_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.float16, ) model = AutoModelForCausalLM.from_pretrained( model_name, quantization_config=bnb_config, trust_remote_code=True ) model.config.use_cache = False from transformers import pipeline generator = pipeline( "text-generation", model=model, tokenizer="gpt2", ) result = generator("Hi") print(result) the result isnt as expected and it outputs [{'generated_text': 'Hi8\x10=:AHi8\x10>Hi8\x10>:AHi8\x10?'}]. How can i fix this and make it output a proper response
