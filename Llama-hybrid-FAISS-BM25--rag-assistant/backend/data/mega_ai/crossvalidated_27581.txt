[site]: crossvalidated
[post_id]: 27581
[parent_id]: 3893
[tags]: 
To respond to the follow-up @Andy posted as an answer here ... Although I could not say which estimate is correct and which is false, doesn't the inconsistency in the Assault Conviction and the Gun conviction estimates between the two models cast doubt that either has a true causal effect on sentence length? I think what you mean is the discrepancy in the parameter estimates gives us reason to believe that neither parameter estimate represents the true causal effect. I agree with that, though we already had plenty of reason to be skeptical that such a model would render the true causal effect. Here's my take: Over-fitting data is a source of biased parameter estimates, and with no reason to believe that this bias offsets other sources of bias in estimating a particular causal effect, it must then be better, on average, to estimate causal effects without over-fitting the data. Cross-validation prevents over-fitting, thus it should, on average, improve estimates of causal effects. But if someone is trying to convince me to believe their estimate of a causal effect from observational data, proving that they haven't over-fit their data is a low-priority unless I have strong reason to suspect their modelling strategy is likely to have over-fit. In the social science applications I work with, I'm much more concerned with substantive issues, measurement issues, and sensitivity checks. By sensitivity checks I mean estimating variations on the model where terms are added or removed, and estimating models with interactions allowing the effect of interest to vary across sub-groups. How much do these changes to the statistical model affect the parameter estimate we want to interpret causally? Are the discrepancies in this parameter estimate across model specifications or sub-groups understandable in terms of the causal story you are trying to tell, or do they hint at an effect driven by, e.g. selection. In fact, before you run these alternate specifications. Write down how you think your parameter estimate will change. Its great if your parameter estimate of interest doesn't vary much across sub-groups, or specifications - in the context of my work, that is more important than cross-validation. But other substantive issues affecting my interpretation are more important still.
