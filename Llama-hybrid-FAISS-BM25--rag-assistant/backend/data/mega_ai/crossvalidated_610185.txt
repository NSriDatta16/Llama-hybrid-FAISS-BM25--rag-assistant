[site]: crossvalidated
[post_id]: 610185
[parent_id]: 
[tags]: 
Why is the square of the average of a set of positive numbers not always bigger than the average of the squares of the same set of positive numbers?

Why is the square of the average of a set of positive numbers not always bigger than the average of the squares of the same set of positive numbers? I am not talking about in an asymptotic case.
