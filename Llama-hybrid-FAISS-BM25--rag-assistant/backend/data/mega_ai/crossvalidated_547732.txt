[site]: crossvalidated
[post_id]: 547732
[parent_id]: 
[tags]: 
In attention models, why is dropout used for positional encodings?

In the "Attention is all you need" paper, they write: we apply dropout to the sums of the embeddings and the positional encodings I can understand why you might use dropout on the embeddings of the inputs (randomly mask out some of the words, the network should still be able to understand the text). But why is it used on the positional encodings as well? Assume I want to predict the next word, GPT style. If I understand it correctly, the query used to predicting word N is generated from the embedding of word (N-1) . If we perform dropout on the entire input at location (N-1) then the network will have to "waste" an entire attention layer just to try to figure out for which location it has to predict?
