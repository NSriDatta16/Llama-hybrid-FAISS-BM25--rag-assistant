[site]: crossvalidated
[post_id]: 503689
[parent_id]: 503526
[tags]: 
I note from the comments that you are primarily interested in changes in state. One way to look at these discrete time-series is to model then as a Markov chain, in which the probability distribution for the outcome depends only on the previous state. Transition probabilities for a Markov chain can be estimated empirically from their sample proportions, so the probability of transitioning into a different state than the present state can also be estimated in this manner. Thus, in a discrete-state time-series $\mathbf{x} = (x_1,...,x_n)$ the estimator for the probability of a change-of-state is: $$\hat{p} = \frac{1}{n-1} \sum_{t=2}^n \mathbb{I}(x_t \neq x_{t-1}).$$ For the time-series vectors in your question you have: $$\begin{matrix} \text{A, A, A, A, A, A, A, A, A} & & & \hat{p} = 1, \\[6pt] \text{A, B, C, A, B, C, C, C, A} & & & \hat{p} = \tfrac{3}{4}, \\[6pt] \text{A, Z, D, X, Y, T, A, A, F} & & & \hat{p} = \tfrac{7}{8}, \\[6pt] \text{C, D, C, D, C, D, D, C, C} & & & \hat{p} = \tfrac{3}{4}, \\[6pt] \text{A, A, A, A, B, A, A, A, A} & & & \hat{p} = \tfrac{1}{4}, \\[6pt] \text{A, A, A, A, A, B, B, B, B} & & & \hat{p} = \tfrac{1}{8}. \\[6pt] \end{matrix}$$
