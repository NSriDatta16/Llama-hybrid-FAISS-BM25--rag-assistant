[site]: datascience
[post_id]: 93801
[parent_id]: 
[tags]: 
Training a model purely on weak labels

I have read a couple of papers now use rules-based system to create weak labels and then train a BERT-based model only using these weak labels. Both studies have reported better performances on manually labelled gold-standard test data. However, I just don't follow the logic here. I understand distant supervision and all that. It's been around for a while now. I just don't understand if your model (BERT or not) is only trained on these weak labels then you are treating them as "ground-truth", and more importantly, don't you already know how to create "ground-truth" (by the rules-based system) ??? What's the point of the 2nd step? Even though the performances on test data are better. It doesn't convince me your ML model (from the 2nd step) have learnt something beyond the weak labels you fed to it. The only argument I find has some merits is that you basically treat the BERT-based model already as a zero-shot classifier and you are fine-tuning it with weak labels. I am just confused. Can someone pls enlighten me? Am I missing something obvious here?
