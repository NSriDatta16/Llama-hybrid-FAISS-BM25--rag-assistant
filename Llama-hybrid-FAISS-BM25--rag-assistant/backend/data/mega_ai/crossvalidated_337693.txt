[site]: crossvalidated
[post_id]: 337693
[parent_id]: 337492
[tags]: 
I did more research and found answers: 1. Why is downsampling used in the skip-gram case? Quote from its paper, We found that increasing the range improves quality of the resulting word vectors, but it also increases the computational complexity. Since the more distant words are usually less related to the current word than those close to it, we give less weight to the distant words by sampling less from those words in our training examples. So by choosing a random window size within the specified one, it effectively gave less weight to more distant words. 2. The way CBOW treats input and output. Suppose the training example is [(I, like, very, much), soup] , the input one hot vectors are summed instead of averaged. See the diagram from the CBOW paper. Also, it doesn't matter to sum the one hot vectors first and then multiply with embedding matrix, or multiple first and then sum, they are equivalent mathematically. 3. Can we just do the exact opposite to the Skip-gram way of generating word pairs No, because this is not the way how CBOW worked as described above. However, it might be interesting to try this way and see how the results would differ from Skip-gram. In a way, it appears to be more similar to the Skip-gram model than CBOW. For CBOW, see more details in the answer to another thread, Tensorflow: Word2vec CBOW model .
