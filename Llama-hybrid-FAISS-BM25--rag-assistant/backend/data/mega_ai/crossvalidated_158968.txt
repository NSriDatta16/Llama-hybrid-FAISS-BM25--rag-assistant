[site]: crossvalidated
[post_id]: 158968
[parent_id]: 158965
[tags]: 
It may first make sense to think about how PCA can be used to classify observations in this setting. PCA is not in itself a classification method. It is a method for fitting a particular type of model to a data set. With a PCA model, it is possible to talk about distance with respect to that model in two ways: Hotelling's $T^2$ and the $Q$-statistic. With a concept of distance to a model, the basis for a classification method is there, but in itself, PCA lacks a classification rule, which is crucial. Furthermore, a single PCA model fitted to a data set with multiple groups is unlikely to result in a meaningful model because multiple groups violate the assumption of ellipticity made by PCA-based inference. The Fisher Iris example you give has a number of groups, so in this context a typical PCA-based approach would be SIMCA ( Soft independent modelling of class analogies ). SIMCA uses a PCA model fitted to each group to determine the distance of the new observation that must be classified to each of the group's respective PCA models. Then, the new observation is classified to the group that it has the lowest distance to. Based on such an approach, you can count up how many new observations are accurately classified, just as you can with random forests. You may also be interested in whether the different methods you try are better at classifying certain types of observations. There are some nuances to this, but perhaps a good way to dig in is to look at an example with SIMCA on the Fisher Iris data. One such example can be found in On the Type II error in SIMCA method .
