[site]: crossvalidated
[post_id]: 577608
[parent_id]: 565533
[tags]: 
I was wondering the same thing. What you have to realize though, is that the bias is defined for the expectation of the prediction for a given x over all possible training sets (D): \begin{equation} D=\left\{\left(x_{1}, y_{1}\right) \ldots,\left(x_{n}, y_{n}\right)\right\} \end{equation} \begin{equation} \operatorname{Bias}_{D}[\hat{f}(x ; D)]=\mathrm{E}_{D}[\hat{f}(x ; D)]-f(x) \end{equation} This means that the expectation ranges over different choices of the training set. In your example (figure 2.10), a specific training set is given. Indeed, the prediction of the flexible model (green line) is way off the real function. Now imagine that you would create multiple training sets, and create multiple of these green lines. The average of these green lines would almost perfectly allign the black line (real model), even better than the yellow line would. The expectation of the prediction of all possible training sets for the more flexibel model is closer to the real model and the bias is therefore lower.
