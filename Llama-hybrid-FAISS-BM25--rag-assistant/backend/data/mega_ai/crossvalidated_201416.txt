[site]: crossvalidated
[post_id]: 201416
[parent_id]: 201408
[tags]: 
1 kNN suffers a lot from the curse of dimensionality, and with 90 features they are very likely to perform badly, the underlying reason is that in a high-dimensional space a "neighbourhood" of a point is not anymore local and so you will consider points quite far from your target. (Refer to page 22 of Elements of Statistical Learning for detailed informations) 2 Being very brief, in regression context and with just one hidden layers (that usually is already enough), if M is the number of your hidden units, neural networks take M linear combinations of your features, apply a non-linear function to this combination to obtain the M hidden units, after this step they apply another linear combination of the hidden layers to get your output. M is determined by cross-validation. They are known to be a black-box, but if your only goal is prediction they behave quite well as they are able to capture non-linear relationships. However gradient boosting are probably equally not interpretable. 3 Gradient boosting are one of the state-of-art techniques and they generally require little pre-processing of the features as they are able to handle missing-values as well. The only effort to do is to cross-validate in order to obtain the best value for the shrinkage parameter and the number of trees, potentially the depth of a tree as well. There is no ready recipe in order to determine which technique will perform better, the usual way to go is to try different models and cross-validate them in order to determine the optimal parameters and then choose the one with the best MSE. Regarding the other techniques, you can try a GAM (generalized additive model). Explaining GAM here is probably hard, so refer to page 295 of Elements of Statistical Learning for more informations. They are known to have a good predictive power and still retain interpretability as you can see the single effect of every features. If interpretation is not your goal, you can try Random Forests as well as they generally give good performance (probably they are more suited to a classification context, but it is worth trying). Regarding the final question, even I'm far from being an expert in rainfall, I would expect some features to be highly correlated and so applying a principal components to reduce the dimension of the problem will probably help.
