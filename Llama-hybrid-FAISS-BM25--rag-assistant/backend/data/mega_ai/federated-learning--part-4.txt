that stochastic gradient descent can reduce overfitting. Technical limitations Federated learning requires frequent communication between nodes during the learning process. Thus, it requires not only enough local computing power and memory, but also high bandwidth connections to be able to exchange parameters of the machine learning model. However, the technology also avoids data communication, which can require significant resources before starting centralized machine learning. Nevertheless, the devices typically employed in federated learning are communication-constrained, for example IoT devices or smartphones are generally connected to Wi-Fi networks, thus, even if the models are commonly less expensive to be transmitted compared to raw data, federated learning mechanisms may not be suitable in their general form. Federated learning raises several statistical challenges: Heterogeneity between the different local datasets: each node may have some bias with respect to the general population, and the size of the datasets may vary significantly; Temporal heterogeneity: each local dataset's distribution may vary with time; Interoperability of each node's dataset is a prerequisite; Each node's dataset may require regular curations; Hiding training data might allow attackers to inject backdoors into the global model; Lack of access to global training data makes it harder to identify unwanted biases entering the training e.g. age, gender, sexual orientation; Partial or total loss of model updates due to node failures affecting the global model; Lack of annotations or labels on the client side. Heterogeneity between processing platforms Variations A number of different algorithms for federated optimization have been proposed. Federated stochastic gradient descent (FedSGD) Stochastic gradient descent is an approach used in deep learning, where gradients are computed on a random subset of the total dataset and then used to make one step of the gradient descent.. Federated stochastic gradient descent is the analog of this algorithm to the federated setting, but uses a random subset of the nodes, each node using all its data. The server averages the gradients in proportion to the number of training data on each node, and uses the average to make a gradient descent step. Federated averaging (FedAvg) Federated averaging (FedAvg) is a generalization of FedSGD which allows nodes to do more than one batch update on local data and exchange updated weights rather than gradients. This reduces communication and is equivalent to averaging the weights if all nodes start with the same weights. It does not seem to hurt the resulting averaged model's performance compared to FedSGD. FedAvg variations have been proposed based on adaptive optimizers such as ADAM and AdaGrad, and tend to outperform FedAvg. Federated learning with dynamic regularization (FedDyn) Federated learning methods suffer when node datasets are distributed heterogeneously, because then minimizing the node losses is not the same as minimizing the global loss. In 2021, Acar et al. introduced a solution called FedDyn, which dynamically regularizes each node loss function so that they converge to the global loss. Since the local losses are aligned, FedDyn is robust to the different heterogeneity levels and so it can safely perform full minimization in each device. In theory, FedDyn converges to the optimal (a stationary point for nonconvex losses) by being agnostic to the heterogeneity levels. These claims are verified with extensive experiments on various datasets. Besides reducing communication, it is also beneficial to reduce local computation. To do this, FedDynOneGD modifies FedDyn to calculate only one gradient per node per round, regularizes it and updates the global model with it. Hence, the computational complexity is linear in local dataset size. Moreover, gradient computation can be parallelized on each node, unlike successive SGD steps. In theory, FedDynOneGD achieves the 