[site]: crossvalidated
[post_id]: 392086
[parent_id]: 
[tags]: 
Subsampling as a method for time series train/validation splits

I have a question concerning train-test splits for time series data: Background I have a dataset of sensor data points for 13 month with datapoints measured every 5 minutes which I downsample to 30min/60min intervals. The data shows signs of strong weekly and yearly seasonality and is strongly influenced by external factors such as holidays, weather,... My goal is to build a model that is able to forecast the next 3-6 hours at a 30-60 minute sampling interval as accurately as possible, taking all these factors into account. This should hold for the whole year. Question In an attempt to create a train-validation split that captures relevant features of a whole year in the test set and allows for validation of these predictions for a whole year, my colleagues and I developed the following idea: We put every second 5-minute sample into the test set and the other samples into the train set. Such that the results look like this (subscripts indicating time in minutes from start): train = [x_0, x_10, x_20, ...] validation = [x_5, x_15, x_25, ...] We would then downsample both datasets to 30min/60min-intervals such that every interval aggregates 3/6 measurements. In addition, we want to put the complete 13th month into the validation set, in order to get a more conservative estimate of the performance on out-of-sample data. I tried to google this idea, but I did not find any relevant information. Visual EDA, Wilcoxon signed-rank test as well as standard deviation, percentiles, min- and max-values indicate that the splits are reasonably close to the original distribution, though not identical. But I might be missing something. Does anyone have an idea weather this splitting is reasonable from a theoretical point of view and/or what other statistics might be reasonable to check? Thanks as lot.
