[site]: crossvalidated
[post_id]: 633663
[parent_id]: 
[tags]: 
Variable Selection for Longitudinal Data with a Binary Outomce

I have a large longitudinal dataset ( 100,000 observations) with firm IDs and Years with about 1000 features (most numeric and some factor). I have been running some classification tasks (random forest, xgboost, svm) on this data using a small subset of the available features. I want to be able to use the models to do some feature selection/ variable importance, however, running these models using all or most of the available predictors does not seem feasible. What are the possible methods I can employ to select pertinent features, within the context of a longitudinal dataset, where features vary with by ID's but also with time. Do I bite the bullet and allow these models to run on all features and then check variable importance and other metrics? Or are there some exploratory data analysis steps I can take prior to subset the features? EDIT: Would it perhaps make sense to run random forests on subsets of features, for example 100 features at a time, thus, 10 RF models and use variable importance on each of them to get a sense of what predictors I can finally employ?
