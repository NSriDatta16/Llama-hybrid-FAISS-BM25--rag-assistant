[site]: crossvalidated
[post_id]: 579327
[parent_id]: 578231
[tags]: 
NO It is important to keep in mind that an estimator of a parameter can take on many forms. In fact, constants can be estimators ! Consequently, we might find that calculating something other than the empirical mean might have desirable properties for estimating the mean. In this post, I give an example where estimating the conditional median, by minimizing MAE, gives better estimates of the regression parameters than the OLS estimates, in the sense that the estimator is unbiased (as is the case with the OLS estimator) but has lower variance than the OLS estimator. Another answer here mentions the Gauss-Markov theorem. As Richard Hardy explains in the answer to my linked question, the MAE minimizer is nonlinear. Thus, Gauss-Markov does not apply. It is fine for minimization of MAE to result in an unbiased estimator that has lower-variance than OLS. EDIT Another answer of mine shows when minimizing in-sample MAE results in lower out-of-sample MSE than minimizing in-sample MSE. EDIT 2 Let's check out a simulation and visualization. In the code, I simulate a heavy-tailed $t_{1.1}$ error term. At each iteration, I calculate the OLS coefficients (with lm ) and the MAE-minimizing coefficients (with rq ). Then I plot those regression lines, along with the true regression line. library(quantreg) set.seed(2022) N $coef[1, 1] L1[i] coef[2, 1] Q $coef[1, 1] Q1[i] coef[2, 1] } par(mfrow = c(2, 1)) plot( x, yhat, type = 'l', lty = 2, ylim = c(min(L0 + L1*x, Q0 + Q1*x), max(L0 + L1*x, Q0 + Q1*x)), main = "OLS" ) for (i in 1:B){ lines(x, L0[i] + L1[i] * x, col = 'red') } lines(x, yhat, type = 'l', lty = 2) # plot( x, yhat, type = 'l', lty = 2, ylim = c(min(L0 + L1*x, Q0 + Q1*x), max(L0 + L1*x, Q0 + Q1*x)), main = "MAE" ) for (i in 1:B){ lines(x, Q0[i] + Q1[i] * x, col = 'red') } lines(x, yhat, type = 'l', lty = 2) par(mfrow = c(1, 1)) The red estimates of the conditional means are much more reasonable when absolute loss is minimized. EDIT 4 Another example is in "classification" problems with discrete outcomes (say binary for now). The typical loss function minimized is log loss ("crossentropy" in some circles), which corresponds to maximum likelihood estimation in logistic regression. Our Frank Harrell has a strong opinion about minimizing this loss function as opposed to minimizing square loss. $$ \text{Log Loss}\\ L(y, p) = -\dfrac{1}{N} \sum_{i = 1}^N \bigg[ y_i\log(p_i) + (1 - y_i)\log(1 - p_i) \bigg] $$ EDIT 5 Finally, there is the James-Stein estimator , which shows that the OLS solution to linear regression is inadmissible for any reasonable sample size for doing regression, despite the Gaussian conditional distribution. That is, even the maximum likelihood estimator is inadmissible due to being dominated by James-Stein.
