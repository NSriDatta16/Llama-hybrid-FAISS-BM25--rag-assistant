[site]: crossvalidated
[post_id]: 184284
[parent_id]: 179915
[tags]: 
I don't think so. There's a good description of Nesterov Momentum (aka Nesterov Accelerated Gradient) properties in, for example, Sutskever, Martens et al."On the importance of initialization and momentum in deep learning" 2013 . The main difference is in classical momentum you first correct your velocity and then make a big step according to that velocity (and then repeat), but in Nesterov momentum you first making a step into velocity direction and then make a correction to a velocity vector based on new location (then repeat). i.e. Classical momentum: vW(t+1) = momentum.*Vw(t) - scaling .* gradient_F( W(t) ) W(t+1) = W(t) + vW(t+1) While Nesterov momentum is this: vW(t+1) = momentum.*Vw(t) - scaling .* gradient_F( W(t) + momentum.*vW(t) ) W(t+1) = W(t) + vW(t+1) Actually, this makes a huge difference in practice...
