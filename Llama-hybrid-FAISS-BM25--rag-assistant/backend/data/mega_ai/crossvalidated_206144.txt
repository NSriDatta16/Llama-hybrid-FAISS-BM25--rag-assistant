[site]: crossvalidated
[post_id]: 206144
[parent_id]: 
[tags]: 
Do word vectors obtained via word embedding techniques really form a vector space?

Word embedding refers to feature learning techniques in natural language processing where words are mapped to vectors of real numbers in a low-dimensional space, the embedding space . Similar to other feature learning (representation learning) techniques, the word vectors are learned within a neural network classifier (e.g. the weights a hidden layer) or by using dimensionality reduction methods (e.g. LSA / PCA) etc. The collection of learned word vectors is basically a matrix, where each row is the learned representation of a word, and each column corresponds to a learned latent feature. Now, the literature sometimes refers to the embedding space as to a vector space . And I've been recently challenged about the correctness of using the term vector space in this context. Is the usage of the term vector space acceptable e.g. in machine learning and math communities? Do word vectors obtained via word embedding techniques really form a vector space? Is it even practical to concern about this?
