[site]: crossvalidated
[post_id]: 591086
[parent_id]: 569615
[tags]: 
Yes, but how difficult it is depends on how your neural network is Bayesian. For example this is really easy in the way it was done for the BatchBALD paper . There it was done via the use of drop-out at inference time and the key thing was just to use the same dropout mask for all records for each Monte-Carlo sample. I.e. do inference repeatedly for all records, say, 1000 times and for each of the 1000 runs you use the same dropout mask for all examples. In practice, you do may end up doing that by setting the random number seed each time before running inference.
