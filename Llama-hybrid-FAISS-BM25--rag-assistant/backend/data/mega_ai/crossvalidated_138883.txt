[site]: crossvalidated
[post_id]: 138883
[parent_id]: 
[tags]: 
Does centering or mean normalizaiton alone every help in feature scaling?

In feature scaling, one way is to subtract the mean (centering) and then divide by the standard deviation for all data points. Suppose we just centered the data and didn't divide by the standard deviation. All the data points have just been shifted by a constant distance. Is this ever beneficial to any type of learning algorithm classifier (Naive bayes, kNN, linear classifier, perceptron, SVM, etc.)?
