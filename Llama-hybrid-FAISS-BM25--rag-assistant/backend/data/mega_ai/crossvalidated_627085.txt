[site]: crossvalidated
[post_id]: 627085
[parent_id]: 
[tags]: 
Unexpected distribution of scores after using class-weighted loss, when data is highly imbalanced (2%), low N and high p

I won't go into the way the data is built because I want to keep the discussion general. Relative to balancing, I couldn't find a lot of materials online about the results of cost-sensitive learning. Basically, I have data $X,y$ of relatively high dimension. About $ of $y$ is positive. More than that, I have relatively low number of $N$ . When I say "relatively" I actually mean that I have too little data to build a model that predicts $y$ adequately, using $X$ . We're talking about ~ $0.05$ area under the precision-recall curve. I ran a lot of experiments. Maybe the first thing to show is the the most naive ones. Edit: Comment about the plots below : All the plots present the results after the same amount of epochs. Naive versions In this iteration I ran a set of models without any change to the balance nor to the lost function. I used a Binary Cross Entropy. As mentioned above, $98\%$ of my data is labeled 0 . I followed the comment from here for debugging a neural network. The results I got is that starting from epoch 3 the network predicted 0 all the time. Then, following the suggestion in the post I mentioned, I tried to flip the labels, to see if the network has some intrinsic problem that is preventing it from training. After some epoch, the network predicted 1 all the time. Then, I tried to train the network on each class independently, to see if it can train. The loss went down, and the network was able to learn each of the classes separately. comment: What I learn from the above experiments is that the network can learn the general structures of my data, but it can not distinguish between the classes given such a small number of positive observations. Random-Oversampling The first modification that I made. Using a poor choice of architecture, I got that starting from some epoch, the network predicted 0.5 all the time. After some tweaks to the network, I was able to get the following distribution of scores (on the validation set): In the above distribution, each class has the same area under the curve (otherwise we wouldn't have seen anything). We can definitely see some discrimination between classes. There is a mass of negative classes that got scores very close to 0, but there is some tail of negative examples that got higher scores, that the model could not tell if they were positive or negative. Also it is sticking out that the model has failed to push observations towards 1, expressing it's inability to generalize with such small number of positive observations. Weighted Cost Function Then, I modified the cost function. As a first trial, I chose the positive weight to be 1/0.02 and got the following scores (on the validation set): I guess shouldn't be surprised by the span of the scores. The model's scores are highly skewed towards 0 due to low predictive power (and low quality/size of data), combined with the imbalance nature of the data. Increasing the Positive Class Weight What strikes me as a surprise is that when I keep increasing the positive class weight, while the expected effect of increasing false positive ratio happens, the scores aren't getting sticked to 1 , and the span is actually getting smaller and closer to 0 ! The following plot shows the results on the validation set. Scores Distribution Span As the above images show, using oversampling "corrected" the predictions' span. Using cost-sensitive learning was expected to yield similar span (if not results), but increasing the positive weight eventually made the span smaller.
