[site]: crossvalidated
[post_id]: 74922
[parent_id]: 74902
[tags]: 
As far as I know, the Iris data set should be (almost) linearly separable. Multinomial logistic regression (MLR) is a linear classifier. Neural networks (NN) are nonlinear classifiers. The problem with NNs is that they could overfit your training data and might not generalize as good as MLR. You can avoid that by adding a regularization term to the cost function (error function) of the NN, so that your error function will consist of a term that penalizes errors on the training set (e.g. cross entropy, sum of squared errors $\sum_n ||y^{(n)}-t^{(n)}||^2_2$) and a term that penalizes the model complexity (e.g. norm of the weight vector $\gamma||w||^2_2$, ...). Adding a penalty for large weights to your error function is like using a prior $p(w)$ for your hypothesis. Which means some $w$ become more likely than others.
