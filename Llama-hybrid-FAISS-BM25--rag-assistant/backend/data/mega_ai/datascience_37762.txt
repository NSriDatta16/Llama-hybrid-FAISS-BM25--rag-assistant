[site]: datascience
[post_id]: 37762
[parent_id]: 35804
[tags]: 
It highly depends on the "solver" you are using. Calling $n$ the number of observations and $p$ the number of weights, the overall complexity should be $n^2p+p^3$. Indeed, when performing a linear regression you are doing matrices multiplication whose complexity is $n^2p$ (when evaluating $X'X$) and inverting the resulting matrix. It is now a square matrix with $p$ rows, the complexity for matrix inversion usually is $p^3$ (though it can be lowered). Hence a theoretical complexity : $n^2p+p^3$. Side notes However, numerical simulations (using python's scikit library) seem to have a time complexity close to $n^{0.72} p^{1.3}$ This may be due to the fact that no implementation actually perform the full inversion (instead, the system can be solved using gradient descents) or due to the fact that there are other ways to calibrate the weights of a linear regression. Source An article from my blog : computational complexity of machine learning algorithms
