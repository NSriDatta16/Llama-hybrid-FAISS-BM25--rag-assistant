[site]: datascience
[post_id]: 61203
[parent_id]: 
[tags]: 
random forest classifier - impact of small n_estimator and repeated training

trying to have a better understanding of random forest algorithm here. With the same training and holdout datasets, I tried two things here: Set a small n_estimator (10), train on my training dataset and apply to my holdout dataset. If I repeat this several times, the result (e.g. correctly predicted target class) varies somewhat from run to run. My understanding is that since the # of trees is small in my model, there are variations in my model after training thus leading to different results. Set a high n_estimator (300) and do the same. Then the results don't vary. My take is that impact of high n_estimator reduces variation in the model and thus i get the same prediction every time. So if I run my scenario 1 a bunch of times and consolidate the results (i.e. run 1 predicts A B in class 1, run 2 predicts A C in class 1, run 3 predicts D in class 1), my final results would be A B C D are in class 1. My question is: 1. Is this essentially the same as running it once with a large n_estimator? 2. Is this approach problematic because I am relying more on "guessing" (e.g. small n_estimator leads to larger variation in outcomes)? Thanks!
