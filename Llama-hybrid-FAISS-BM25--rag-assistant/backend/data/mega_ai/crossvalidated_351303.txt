[site]: crossvalidated
[post_id]: 351303
[parent_id]: 207049
[tags]: 
Machine learning algorithms such as classifiers statistically model the input data, here, by determining the probabilities of the input belonging to different categories. For an arbitrary number of classes, normally a softmax layer is appended to the model so the outputs would have probabilistic properties by design: $$\vec{y} = \text{softmax}(\vec{a}) \equiv \frac{1}{\sum_i{ e^{-a_i} }} \times [e^{-a_1}, e^{-a_2}, ...,e^{-a_n}] $$ $$ 0 \le y_i \le 1 \text{ for all i}$$ $$ y_1 + y_2 + ... + y_n = 1$$ Here, $a$ is the activation of the layer before the softmax layer. This is perfectly valid for two classes, however, one can also use one neuron (instead of two) given that its output satisfies: $$ 0 \le y \le 1 \text{ for all inputs.}$$ This can be assured if a transformation (differentiable/smooth for backpropagation purposes) is applied which maps $a$ to $y$ such that the above condition is met. The sigmoid function meets our criteria. There is nothing special about it, other than a simple mathematical representation, $$ \text{sigmoid}(a) \equiv \sigma(a) \equiv \frac{1}{1+e^{-a}}$$ useful mathematical properties (differentiation, being bounded between 0 and 1, etc.), computational efficiency, and having the right slope such that updating network's weights would have a small but measurable change in the output for optimization purposes. Conclusion I am not sure if @itdxer's reasoning that shows softmax and sigmoid are equivalent if valid, but he is right about choosing 1 neuron in contrast to 2 neurons for binary classifiers since fewer parameters and computation are needed. I have also been critized for using two neurons for a binary classifier since "it is superfluous".
