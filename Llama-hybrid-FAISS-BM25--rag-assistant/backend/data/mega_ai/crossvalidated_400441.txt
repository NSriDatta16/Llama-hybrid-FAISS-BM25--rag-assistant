[site]: crossvalidated
[post_id]: 400441
[parent_id]: 400407
[tags]: 
Use a model that can learn incrementally if you want to try using the whole dataset. All deep learning frameworks support mini-batch processing, and you can formulate a Multilayer Perceptron, or use a single layer for Logistic Regression or SVM. Gradient Boosted Trees also support incremental learning, and using GPU for training. 50M samples with 200 features is possibly doable overnight on standard hardware. At approximately 40TB of data (assuming 4 bytes of single precision per feature), I/O to the database is the likely bottleneck. So sampling and chunking should be performed by the DB. Compute a learning curve over the model to estimate the number of observations actually needed to obtain desired performance. Can try 1/1000, 1/100,1/10 etc and see effect on validation scores.
