[site]: crossvalidated
[post_id]: 469636
[parent_id]: 468358
[tags]: 
I think it is either ill-defined or unhelpful as a concept, for basically the reasons you give. Another way to think about it is that regret is important in multiarmed bandit problems because the randomness in the return for any strategy is what makes the problem difficult. This means it makes sense to ask how well you could do without that randomness, and it's interesting that there are eventually zero-regret strategies. In your problem, there isn't any randomness. Now, for a Bayesian there is always randomness, so you could probably define a Bayesian analogue of the regret: given a prior distribution for the function you're trying to optimise, how well did you do compared to the strategy that minimises the posterior expected loss. The problem is, that's going to be really hard to evaluate (or even define). You can't just go for a flat prior, because the 'no free lunch' theorems say a flat prior over all possible functions on a discrete space means all strategies have the same expected loss.
