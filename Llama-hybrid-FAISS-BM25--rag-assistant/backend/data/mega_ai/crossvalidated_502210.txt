[site]: crossvalidated
[post_id]: 502210
[parent_id]: 502201
[tags]: 
The Confidence Interval (CI) definition the OP has provided is more or less the working definition - perhaps not the most rigourous one if we were to involve samples, but this form is used in general. However, one should note that The CI is random, not the parameter The population parameter $\theta$ is a fixed, unknown constant. It is by definition the true value, which is technically stronger than OP's understanding that "it is equal to its true value with probability 1 and equal to any other value with probability 0." What is random is the interval bounds $A$ and $B$ . This question contains a lengthy discussion on this topic, but I don't think it is a duplicate due to the point below, which I believe is where the confusion arises in this question. The definition is/should be a statement on the overall procedure, not a probability result on one estimation For one estimation procedure, once we calculated the interval the OP is more or less in the right thinking that either theta lies in this interval or it does not which is supported in Neyman's quote in the question [1]. However, this is not the intended level of view where we make such probability statement [2] - it should be viewed from a meta level (i.e. across many estimation procedures) instead. In other words, the definition is (or should be) saying that if one repeats the estimation procedure (not necessarily based on the same data or the same $\theta$ ) many times, around $1-\alpha$ of the times the calculated CI $(A, B)$ will cover $\theta$ . It says nothing about one particular estimation procedure. Neyman has written the following after what the OP quoted (pp. 349-350 of [3], emphasis mine, variables renamed to match what's in the question): The theoretical statistician constructing the functions [ $A$ ] and [ $B$ ], [...] may be compared with the organizer of a game of chance in which the gambler has a certain range of possibilities to choose from while, whatever he actualy chooses, the probability of his winning and thus the probability of bank losing has permanetly the same value, $1-\alpha$ . The choice of the gambler on what to bet, which is beyond the control of the bank, corresponds to the uncontrolled possibilities of [ $\theta$ ] having this or that value. The case in which the bank wins the game corresponds to the correct statement of the actual value of [ $\theta$ ]. In both cases the frequency of "successes" in a long series of future "games" is approximately known. On the other hand, if the owner of the bank, say, in the case of roulette, knows that in a particular game the ball has stopped at the sector No. 1, this information does not help him in any way to guess how the gamblers have betted. Similarly, once the sample [...] is drawn and the values of [ $A$ ] and [ $B$ ] determined, the calculus of probability adopted here is helpless to provide answer to the question of what is the true value of [ $\theta$ ]. [1] More rigourously it should be "the interval either contains theta or it does not", which hopefully implies the interval is the moving part, not theta. Ah, the nuances in English. [2] In fact, no such probability statement can really be made from a frequentist perspective. [3] J. Neyman (1937) Outline of a Theory of Statistical Estimation Based on the Classical Theory of Probability. Philosophical Transactions of the Royal Society of London. Series A, Mathematical and Physical Sciences , 236 :333â€“380. http://doi.org/10.1098/rsta.1937.0005
