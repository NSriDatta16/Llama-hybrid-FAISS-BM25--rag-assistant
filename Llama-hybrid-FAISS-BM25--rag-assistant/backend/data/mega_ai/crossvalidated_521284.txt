[site]: crossvalidated
[post_id]: 521284
[parent_id]: 
[tags]: 
Choosing the dimension in PCA

Suppose that a dataset $(x_1, \cdots x_N) =X \in \mathbb R^{d \times N}$ gives rise to the PCA (principal components analysis) result $\tilde X^\top \tilde X = U^\top DU$ where $\tilde X = (x_1 - \bar x, \cdots x_N - \bar x)$ is the centered matrix with $\bar x = \frac1N (x_1 + \cdots + x_N)$ , $U$ is a symmetric matrix, and $D = \text{diag}(\lambda_1, \cdots \lambda_N)$ is a diagonal matrix of the principal values. Here, we may assume that $\lambda_1 \ge \lambda_2 \ge \cdots$ . Question. How does one choose a number $k$ representing the intrinsic dimension of $X$ so that we only use the top $k$ principal components (the column vectors from $U$ )? It seems that there are a few ways to do this, for example by taking the maximum $k$ such that $\lambda_k \ge \epsilon$ , where $\epsilon \ge0$ is an error threshold. Analogously, one can bound the tail sum instead: $\lambda_{k+1} + \cdots + \lambda_N , or the $L^2$ analogue: $\sqrt{\lambda_{k+1}^2 + \cdots + \lambda_N^2} . There are more involved methods too, such as Minka's NeurIPS paper . However, I am curious of what is the most canonical method, either in practice or in theory.
