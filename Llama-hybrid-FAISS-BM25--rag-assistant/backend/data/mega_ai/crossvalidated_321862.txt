[site]: crossvalidated
[post_id]: 321862
[parent_id]: 
[tags]: 
Split challenge in multiple classifiers

I am trying to solve a challenge in which I have a number of cases (in the magnitude of 100-200) and for each case I want to solve a binary classification task. In more detail, for each case I have a number of documents by one author, and another document of unknown origin. My task is to decide if the document was written by the same author ( authorship verification ). My system takes pairs of text samples and outputs their respective distance based on writing style. I would like to use area under the ROC curve to evaluate how well the distance can separate the pairs that were written by the same author, and the samples that were written by different authors. Now my system performs bad when I train it on all cases at the same time. When I train several models for a small number of cases, however, it performs very well. Is it ok to split the task into multiple models and evaluate them separately? Am I missing something? Edit: To be more concise: is it ok to measure the performance (i.e. AUROC) on all subsystems and average it?
