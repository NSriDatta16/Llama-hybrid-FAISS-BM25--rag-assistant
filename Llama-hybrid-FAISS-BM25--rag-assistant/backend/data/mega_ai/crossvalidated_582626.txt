[site]: crossvalidated
[post_id]: 582626
[parent_id]: 582574
[tags]: 
This is an awesome question. So the example of PCA that you have shown projects a set of points to a vector which is along the direction of maximum covariance of the data. What is does is that, It reduces one dimension from the data, but wait what just happened due to that ??...You lost an essential component of your representation and that may just look like you got rid of noise !! Is it ? The principal component direction along which you are projecting only reduces the error, which is the distance of the old point and the new projected point. (Check the derivation of PCA if you did not get this) What it does not do is that, Hey that was not only noise that you got rid from, in the real world dataset if you want want to reduce dimensions and hope that you are getting rid of noise, you are, only if you consider the definition of noise in a mathematical setting. If you think in a realistic sense, the loss of representation (as I explained in the previous point) also takes place and that combined with loss of noise together cancels each other out in terms of actual benefit. The only time you benefit from that is when you do not need so many dimensions to represent the data.
