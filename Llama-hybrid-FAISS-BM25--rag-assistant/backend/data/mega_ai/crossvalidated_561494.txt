[site]: crossvalidated
[post_id]: 561494
[parent_id]: 561164
[tags]: 
The only reason that I can think of, is that if the ranked distribution of the optimization values are "heavily negative skewed" Sort of. There is a compounding that occurs when you add dimensions that is similar to what you get when you add more randomly sampled models, except that it works against you rather than for you. As you add more dimensions, the models become more and more likely to be "average", and the probability of them being exceptionally good decreases. It's not so much they're skewed towards "bad", so much as they're skewed towards "average", and the "average" model is really bad (remember, if you look at the space of all models, not just the ones created through a rational generation process, most of them are actually worse than just "Use $\bar y$ as your estimator regardless of the $x$ values"). There are many ways of thinking of this: According to the CLT, adding more features decreases the spread of the distribution of models in terms of loss function per feature. So the standard deviations needed to get to a model quality increases. If you look at how much increasing your percentile increases your standard deviations, this is the reciprocal of the probability density. As your percentile increases, the impact of increasing it further increases. Increasing your percentile from $0.999$ to $0.9999$ increases your z-score much, much more that increasing your percentile from $0.99$ to $0.991$ . The length of a vector increases as you add dimensions, if you keep the individual component lengths fixed. For instance, if you have an $n$ dimensional vector with components equal to $0.7$ , the length is $0.7\sqrt n$ . So if you want vectors that are within some fixed distance of a "good" solution, the percentage of actual solutions that are within that distance decreases as you add dimensions. Suppose we give each feature a percentile rank (i.e. "This model is in the 70th percentile as far as how well it incorporates this feature", however that is defined). If there are $k$ features, the probability that all them will be in the top $p$ percentile is $(1-p)^k$ . Even if $p$ is a relatively low number like $0.7$ , this probability quickly becomes tiny; with ten features, it's about $3$ %.
