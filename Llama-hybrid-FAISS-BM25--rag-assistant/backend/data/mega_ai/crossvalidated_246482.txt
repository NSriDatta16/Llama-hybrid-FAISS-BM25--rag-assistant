[site]: crossvalidated
[post_id]: 246482
[parent_id]: 246404
[tags]: 
Note that generally, the loss$(w,x_i,y_i)$ term you have written would actually be the $c_i$ value from the constraint, in both versions. The dual-formulation constraint $\alpha^T y = 0$ arises from the bias term $b$ which offsets the solution plane from the origin. The most common alternative to using the bias term is extending all the inputs $x_i$ by adding on a dimension with constant value $1$. A less commonly used approach is to use a slightly modified kernel function formulation to replicate $b$ (i.e., so that calculating $ _{k'}$ with the modified kernel $k'$ is similar to evalutating $ _k + b$ with the original kernel). From a practical perspective, there is little difference in the classifier performance between biased and unbiased SVM. Specific algorithms differ of course, and the time it takes to learn a classifier can be different. From a theoretical perspective, it is often easier to determine bounds for the unbiased version.
