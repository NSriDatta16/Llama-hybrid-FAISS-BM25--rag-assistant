[site]: crossvalidated
[post_id]: 296492
[parent_id]: 296488
[tags]: 
Interesting problem. If I understand correctly you essentially have a dataset where each input is a pair of sentence ID's and the output is the corresponding label, which here is the probability that those two sentences 'belonging together' constructed as you described. You want to learn some representation of the sentences that reflects the pairing labels. One way you could do this is through a siamese neural network . Essentially you have a neural network whose input is some vector representation of sentence (perhaps a N-hot representation of the sentence, i.e. a vector of 0's of the length of your vocabulary, with 1's for the particular words in the sentence, or perhaps you could first convert the sentences to some other representation using the methods suggested in this answer ). Then the output of this neural network is another vector - this will be your new your representation. Now for a pair of sentences, we put both through the same neural network. Then we take the cosine similarity of the two outputs (or some other similarity network). Then we can consider this as the output probability that the two sentences are the same. We can compare this to the 'true' probability from your labels, and backpropagate the cross-entropy loss (or whatever loss term is appropriate/works best) to the neural network parameters (note we have done two forward passes, one for each sentence, but we only update the parameters once, averaging over both of these forward passes). I'm sure there are other ways to do this (maybe you could draw a probabilistic graphical model) but this is what first came to my mind.
