[site]: crossvalidated
[post_id]: 507785
[parent_id]: 507779
[tags]: 
If you're not trying to generalize on new data, you don't need to split your data into train and test. If you are trying to generalize to a new set of data and hyperparameters to tune, you need to. A linear regression model can overfit your training data by overweighting unimportant variables. So, you split data into training and test sets to obtain a realistic evaluation of your learned model. If you evaluate your learned model with the training data , you get a cheerful measure of your model's goodness. So, you should use a separate set (a set that is not used during training i.e test set) to obtain a sensible evaluation of your model. Splitting the data can be used with models that include hyperparameters. For example, like in many machine learning algorithms, we have a penalty term in Ridge regression (that needs to be set before the learning process begins), which penalizes "big" values of the coefficients, and the degree of this penalization is proportional to the penalty term, i.e., $\lambda$ . To pick the best value for $\lambda$ , you need to split your data into train and test (you have to work further with your train set to estimate $\lambda$ ). Apart from Ridge regression, possible hyperparameters for linear regression could be including/not including an intercept using another error metric i.e mean absolute error instead of mean squared error.
