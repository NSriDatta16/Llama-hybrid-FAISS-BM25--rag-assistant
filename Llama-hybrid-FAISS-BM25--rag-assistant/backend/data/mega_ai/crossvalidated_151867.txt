[site]: crossvalidated
[post_id]: 151867
[parent_id]: 
[tags]: 
Stopping Criteria for Pre-Training using Stacked Autoencoders

In stacked autoencoders during greed layer-wise training of individual autoencoders using gradient descent and backpropagation to minimize the reconstruction error(squared error or cross entropy) what is the ideal stopping criteria for the pre-training ? Is it when a minimum reconstruction error is achieved or some other criteria ?
