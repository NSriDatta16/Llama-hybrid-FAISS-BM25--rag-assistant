[site]: crossvalidated
[post_id]: 351744
[parent_id]: 351741
[tags]: 
Yeah, thatâ€™s overfitting because the test error is much larger than the training error. Three stacked LSTMs is hard to train. Try a simpler network and work up to a more complex one. Keep in mind that the tendency of adding LSTM layers is to grow the magnitude of the memory cells. Linked memory-forget cells enforce memory convexity and make it easier to train deeper LSTM networks. Learning rate tweaking or even scheduling might also help. In general, fitting a neural network involves a lot of experimentation and refinement. Finding the best network involves tuning a lot of dials together.
