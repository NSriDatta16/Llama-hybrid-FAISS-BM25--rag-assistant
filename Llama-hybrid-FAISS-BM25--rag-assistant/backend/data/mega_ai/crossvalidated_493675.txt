[site]: crossvalidated
[post_id]: 493675
[parent_id]: 
[tags]: 
Alternating between Hamiltonian Monte Carlo and sampling from conjugate posterior in large models

For Bayesian models with a large number of parameters and pieces that have conditionally closed form solutions for drawing samples, when is it worth it to use these closed form updates instead of a Markov chain Monte Carlo method that samples all values simultaneously such as Hamiltonian Monte Carlo (HMC) or the No-U-Turn sampler? For the sake of discussion, suppose we have a model which is entirely differentiable in the parameters. For example suppose we have two vector-valued random variables $\alpha, \beta$ and observed data $x$ such that $\beta \vert \alpha, x$ has a conjugate posterior distribution while $\alpha \vert \beta, x$ has no such form and thus we must resort to using variants of the Metropolis algorithm. Is there any folk wisdom or established research which helps provide guidance on whether or not it makes sense to replace HMC with HMC-within-Gibbs? My understanding is that using the Gibbs sampler can provide much cheaper samples in computational cost, but because doing so precludes making joint moves across all parameters in parameter space as HMC does, there could be slow mixing and inefficient sampling in the end.
