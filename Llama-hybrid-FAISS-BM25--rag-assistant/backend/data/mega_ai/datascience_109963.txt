[site]: datascience
[post_id]: 109963
[parent_id]: 
[tags]: 
XGBoost results changing when one row is removed

I have a training dataset of 2,600 rows and 26 columns. I trained an XGBoost (1.3.1) Classification model using the data and evaluated it using a test set of c. 800 rows. Whilst experimenting I found that although the model has stable results when being re-run, if I remove just 1 row (chosen at random) from the dataset the individual row prediction probabilities change by up to 15%. I can't understand why removing 1 of 2,600 rows results in such considerable changes in the model! Parameters: {'learning_rate': 0.05, \ 'n_estimators': 66, \ 'max_depth': 6, \ 'subsample': 1, \ 'min_child_weight': 1, \ 'colsample_bytree': 1, \ 'gamma': 0.5, \ 'reg_alpha': 0.001, \ 'nthread': 1, \ 'seed': 11}
