[site]: crossvalidated
[post_id]: 452379
[parent_id]: 451868
[tags]: 
For SVMs the decision boundaries are given by $\omega^{*T}x^{(i)} + b = \pm 1$ , and $\frac{-b}{||\omega||}$ is the distance from the origin to the hyperplane. The closest positive and negative examples to the separating hyperplane are, $\arg\max_{i:y^{(i)} = -1} \omega^{*T}x^{(i)}$ , resp. $\arg \min_{i:y^{(i)} = 1} \omega^{*T}x^{(i)}$ These verify (because the must be support vectors) the equations for the decision boundaries, that is, $\max_{i:y^{(i)} = -1} \omega^{*T}x^{(i)} + b = -1$ , resp. $\min_{i:y^{(i)} = 1} \omega^{*T}x^{(i)} + b = 1$ Add the two and solve for $b$ . In the soft margin case (see for example Buerge's A Tutorial on Support Vector Machines for Pattern Recognition ), you have the following quadratic problem, $$ \begin{array}{c} \min \frac{1}{2} ||w||^2 + \frac{C}{n}\sum_i \xi_i + \sum_i \alpha_i (1-y_i w^T x_i - \xi_i) - \sum_i \lambda_i \xi_i \\ \alpha_i \geq 0, \xi_i \geq 0, \lambda_i \geq 0 \end{array} $$ where $\xi_i$ are the slack variables which allow for some samples to lie on the wrong side of the margin. When you calculate the dual, the problem becomes $$ \begin{array}{c} \max \sum_i \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i\alpha_jy_iy_jx_ix_j \\ \alpha_i \geq 0 \\ \lambda_i \geq 0 \\ \alpha_i + \lambda_i = \frac{C}{n} \end{array} $$ which implies that $0 \leq \alpha_i \leq C$ . It means that allowing for some errors, limits how much weight we put on each sample. Now, once you solve for it, you still only need to consider those cases for which $\alpha_i > 0$ and for which $\xi_i = 0$ . That is where the marging exactly lies. More specifically, if $\alpha_i > 0$ , then $y_i w^T x_i = 1-\xi_i \leq 1$ . It either lies exactly on the margin, or on the wrong side of it. In other words, the condition for support vectors lying exactly on the separating hyperplane is the same in both the hard and soft cases, namely $$ 1 = y_i (w^T x_i + b) = y_i(\sum_{j \in SV} \alpha_j y_jx_j^T x_i + b) $$ where $SV$ stands for the set of indices corresponding to the support vectors. Since $y_i \in \{1, -1\}$ then $y_i^2 = 1$ , and we can multiply both sides by $y_i$ to get $$ b = y_i - \sum_{j \in SV} \alpha_j y_jx_j^T x_i $$
