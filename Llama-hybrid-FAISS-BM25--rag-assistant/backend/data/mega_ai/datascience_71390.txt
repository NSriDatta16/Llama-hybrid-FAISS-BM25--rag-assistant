[site]: datascience
[post_id]: 71390
[parent_id]: 71375
[tags]: 
Upon further research I found that during training forward language models try to predict the next word in a sequence. Backwards language models on the other hand start at the end of a sequence and attempt to predict the proceeding word. It seems that by stacking both forward and backwards models produced from the same data-set you get better results than using a forward or backwards model alone.
