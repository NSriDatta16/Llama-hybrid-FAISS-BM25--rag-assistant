[site]: crossvalidated
[post_id]: 34041
[parent_id]: 33970
[tags]: 
If you are going to do model selection then I think you're better doing an exhaustive search and weighting each model rather than cherry picking. You only have 14 variables, which is certainly feasible - 16384 different models is not prohibitively large, especially because the sample size is small. I would also look at normalised weights, defined by: $$w_m=\left[\sum_{l}\exp\left(-\frac{1}{2}[AIC_l - AIC_m]\right)\right]^{-1}$$ These weights assume AIC is negative twice the log likelihood plus two times the number of betas. If the best model has a weight close to $1$ then just use that. otherwise you should average your results aross models with total weight close to $1$. What usually happens is that a "core" group of variables should be always included, with uncertainty over a "non-core" set, and a third set of unimportant variables which never appear in models with high weight. You could also replace AIC with BIC or some other penalty based IC to see how much the weights depend on the specific complexity penalty used.
