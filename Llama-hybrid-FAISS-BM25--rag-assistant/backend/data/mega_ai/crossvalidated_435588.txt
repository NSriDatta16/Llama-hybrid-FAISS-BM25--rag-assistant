[site]: crossvalidated
[post_id]: 435588
[parent_id]: 435323
[tags]: 
Thus I do not understand why logistic regression with a logarithmic cost function also converges to the optimum for the classification problem. Logistic regression isn't concerned with optimizing accuracy, but it's definitely optimizing the cross-entropy loss. Another way to think about it is that accuracy is discarding all the fine-grained detail in the predicted probabilities. Accuracy treats $0.5+10^{-10}$ and $1.0-10^{-10}$ as exactly the same, because both are larger than 0.5, even though the latter describes the probability of the event with much more confidence than the former. By contrast, the cross-entropy loss $J$ will penalize predictions which are far from the label, and the size of the penalty increases the "more incorrect" that prediction is. Because of this property, it's possible to have situations where a logistic regression will produce predicted probabilities that are all greater than 0.5 (or if we reverse the coding of the outcomes, all predicted probabilities are less than 0.5). This commonly arises when one class is much more prevalent than the other, and the features are, at best, weak predictors of the outcome. This clearly flies in the face of an expectation that the positive and negative examples should be on opposite sides of the 0.5 cutoff. See the discussion at Logistic regression is predicting all 1, and no 0 for more detail. See also: Why is accuracy not the best measure for assessing classification models?
