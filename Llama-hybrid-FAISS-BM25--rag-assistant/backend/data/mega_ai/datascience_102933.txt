[site]: datascience
[post_id]: 102933
[parent_id]: 102931
[tags]: 
Check this paper . Its introduction gives a very good definition of both: The classic approach towards the assessment of any machine learning model revolves around the evaluation of its generalizability i.e. its performance on unseen test scenarios. Evaluating such models on an available non-overlapping test set is popular, yet significantly limited in its ability to explore the modelâ€™s resilience to outliers and noisy data / labels (i.e. robustness). For generalizability, unseen data does not have to be noisy or contain more outliers compared to original data. You can simply split your original data set into 3: training, validation and test; use training and validation for the model development and keep test data unseen for a final check after cross validation. This will check your model's generalizability. Test set created in this way won't be more noise or have more outliers compared to the other two.
