[site]: crossvalidated
[post_id]: 400211
[parent_id]: 400058
[tags]: 
As Lucas already said (+1), yes, your intuition is correct; that is the sum of the weighted error terms. This steps is indeed awkward for Adaboost , because it is not the Adaboost commonly used but actually the early version of what we call Adaboost algorithm, namely Adaboost.M1 ; it is presented in Freund & Schapire (1996) Experiments with a New Boosting Algorithm . Adaboost.M1 explicitly encoded what the authors describe as: " (...) Adaboost (...), theoretically can be used to significantly reduce the error of any learning algorithm that consistently generates classifiers whose performance is a little better than random guessing ". Therefore as soon as $\epsilon_t > 0.5$ , the iteration stopped. Some further commentary on Point 2: As more people studied the algorithm it became clear that the break condition was unnecessary. The reproduction of Adaboost.M1 , simply as Adaboost , in Friedman et al. (2000) Additive logistic regression: a statistical view of boosting does not include this "hot-fix"/break condition. If anything it was superfluous for Friedman et al.'s core notion that the: " AdaBoost algorithm (population version) builds an additive logistic regression model via Newton-like updates for minimizing $E(e^{-yF(x)})$ ". To that extent, the break condition in Adaboost.M1 became uninterpretable when we moved to LogitBoost , BrownBoost and other boosting variances. F&S obviously knew this; it is evident even in their 1996 paper; the algorithm Adaboost.M2 does not include any break conditions. As a final note, I think that the CV.SE thread Binary classifiers with accuracy will also help one's understanding of the issue. It discusses why this break condition is indeed redundant; in short, we can naturally "flip" the signs of a "bad" classification result to get a "good" one.
