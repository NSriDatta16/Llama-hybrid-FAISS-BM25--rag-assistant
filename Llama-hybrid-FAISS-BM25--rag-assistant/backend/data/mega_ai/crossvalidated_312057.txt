[site]: crossvalidated
[post_id]: 312057
[parent_id]: 311791
[tags]: 
As I said, I kind of see where they are coming from in the way they are trying to teach this from a data-driven approach, but I find regression makes more sense when you start with theory then later figure out how to apply data. I think they are saying something like this... Let us develop an 'agnostic' framework - one that deals with arbitrary data and approaches. (Linear regression will be one of these special cases.) Now (per the prior question I answered) let's build a predictor function for $Y_i$ from the data at period (or whatever) $i$, $X_i$. That predictor is $\hat Y_i=E[Y|X_i]$, which is (sort of) $E[Y_i]$. Now, since it was designed that $Y_i$ is the mean of the data $X_i$, then any data point $x_i$ in $X_i$ can be seen as a random variable: $x_i=\hat Y+e_{x_i}$. This is by design. Now, since $\hat Y_i$ is the mean, of course $E[e|X]$ is zero. This may be easier if you think just of a set of data, not a 'regression'. Suppose we measure the heights of the 30 students in a classroom. If they average 150 cm, then they are all 150 cm +/- something. But since 150 cm is the average, the total of all those plus and minuses has to add to zero: that is what the average is. Put another way, if each student differs from the mean by $\delta_i$, then $E[\delta]=0$. (which is the same as saying $E[\delta|X]=0$, if $X$ is your height variable...since we got the $\delta$'s by measuring the average, $X$). I am trying to guess where they are going, but I think this is what they are trying to get ready for. This all assumes everything is linear. For example, there could be a lot of students just short of 150 cm, and a large number just above 150 cm, but a few that are over 190 cm. Everything they said about $E[e|X]$ is still true. But suppose you aren't in a linear place. In fact, in 'classic' regression (Ordinary Least Squares), you try to minimize the variance of that error term, which is, essentially, $e^2$ (or, more accurately, the sum of all the $e_i^2$'s). If things aren't symmetrical (as they aren't), you are going to end up choosing a larger value for $\hat X$ so those tall people contribute less from their being outliers: if a whole bunch of people are 150 cm +/- 2 cm, their errors add up to 4 each. But someone who is 20 cm above average contributes 400 to the squared error. What I don't like about this approach is that, rather than laying out the 200 year-old logic, they are trying to fashion it from the ground-up in a 'new' way. But they already know the answer. SO, even as I read the material and your questions, I am guessing, a bit, about what they are trying to get across, or what is up next. Indeed, you will get biased estimators if the model itself is non-linear and you try to model it as linear (you try to make $Y=aX+c+\epsilon$, when it is really $Y=aX^2+c+\epsilon$), or there is autocorrelation among the error terms, etc. But, yes, by construction they have made their assumption true by making other tacit assumptions...then said, "this won't be true if the tacit assumptions don't hold, but we haven't told you what those were". My advice to you would be to get a standard text on eonometrics and see if that approach helps. A classic, if your math is up to it, is Judge : https://www.amazon.com/Theory-Practice-Econometrics-George-Judge/dp/047189530X
