[site]: datascience
[post_id]: 32796
[parent_id]: 
[tags]: 
Can gradient boosted trees fit any function?

For neural networks we have the universal approximation theorem which states that neural networks can approximate any continuous function on a compact subset of $R^n$. Is there a similar result for gradient boosted trees? It seems reasonable since you can keep adding more branches, but I cannot find any formal discussion of the subject. EDIT: My question seems very similar to Can regression trees predict continuously? , though maybe not asking exactly the same thing. But see that question for relevant discussion.
