[site]: crossvalidated
[post_id]: 511049
[parent_id]: 511033
[tags]: 
The short answer: "It Depends". The longer answer: Whether to adjust for multiple comparisons is a very good question without a good/standard answer. It really does depend on things other than the data and basic questions. I like the approach of thinking through how it would change my interpretation of the results if I were to add an additional 20 (or 100 or other large number) of categories/tests/intervals based on random data to my process and either adjust for the multiple comparisons or not adjust. Think about for your case if you added a bunch more categories and filled in the pre and post measurements with purely random data. Scenario 1: You plan to declare "Success", publish, change policy, etc. if any of the tests are significant. So adding 20 or more tests on random data without adjusting for multiple comparisons greatly increases your chance of seeing at least one significant test (but adjusting for all the tests does not). Scenario 2: You are really interested in the "Math" category and whether "Physics" shows a difference or not will not affect your interest in the math class. But adding 20 or more additional tests on random data and adjusting for multiple comparisons will increase the C.I. interval width on math giving less precision. In scenario 1, definitely adjust. In scenario 2, definitely do not adjust. But most real scenarios fall somewhere in between, so you really need to think about what effect adjusting or not will have on your decisions. Note that there are other options that Bonferroni. If you are really interested in this then I would look at Bayesian Hierarchical models. Instead of just increasing the standard error because some effect sizes may be too extreme by chance, they actually shrink the extreme effect sizes using information from the other groups giving better effect size estimates (and intervals) as well as allowing for "Partial Correction" instead of just a yes/no.
