[site]: crossvalidated
[post_id]: 277573
[parent_id]: 
[tags]: 
Neural Network and accuracy

I created a NN system with TensorFlow to classify 40 thousands of training point data that are expressed through 16 attributes. Data are extracted randomly into batches with a size of 100. When I train the model, the average accuracy reaches sometime 85%, or 70%, or even stabilizes at 20%, at different running sessions. My questions are: Is it normal that the accuracy sometimes changes between 70-85% instead of retrieving the same predictions after each session? (my guess is this is due to randomized batched data). Same question when sometimes it does not change and stays at 20%. How to accurately evaluate when to stop a training process (through a fixed number of epochs)? In general, can the accuracy be improved by changing some parameters; e.g. changing batch size, or reducing number of attribute data. I understand that the quality and interpretability of the features matter, but my question is more about general practices.
