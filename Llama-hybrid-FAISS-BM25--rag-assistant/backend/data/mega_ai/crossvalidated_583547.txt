[site]: crossvalidated
[post_id]: 583547
[parent_id]: 
[tags]: 
Estimating probabilities based on non-deterministic models

Problem I have $N$ binary classification models $M_{i}$ that try to predict there own random variable $Y_{i}$ based on the same input data $X$ . Each $M_{i}$ is non-deterministic meaning every estimate of $Y_{i}$ given the same $X$ is not the same. As a note, the estimates are probabilities. The ultimate goal is then to rank these estimates with a quantifiable degree of confidence that they are in the correct order. Current approach I iteratively estimate each $Y_{i}$ until the combination of medians across all $Y_{i}$ are statistically different using Mood's Median Test or until I hit a maximum number of iterations. As an example, here $N=4$ . The dotted lines are each estimate's distribution median. (There is a kernel density estimation just to better see the distributions.) $Y_{i}$ estimates" /> Questions Repeating the process described above seems to rank the estimates in a different order more often than it should. Not only that, but the p-values generated from the median test vary wildly between trials (each repeat of the process). Is this because of the median test's low power? Is there a more robust way to rank estimates or collection of estimates given some input $X$ ? I feel like using some sort of Bayesian inference and/or beta distributions is applicable here but I don't know how something like that would work. Given a distribution of probability estimates from a single $M_{i}$ , which measure of central tendency makes the most sense to use?
