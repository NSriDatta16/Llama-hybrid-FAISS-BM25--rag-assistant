[site]: datascience
[post_id]: 54079
[parent_id]: 53995
[tags]: 
Embeddings are vector representations of a particular word. In Machine learning, textual content has to be converted to numerical data to feed it into Algorithm. One method is one hot encoding but it breaks down when we have large no of vocabulary. The size of word representation grows as the vocabulary grows. Also, it is sparse. With embedding (fixed size vectors with lower dimension), the size of word representation can be controlled. Also, the vector representation stores the semantic relationship b/w words. There are pretrained embeddings Word2Vec, Glove etc available which can be used just as a lookup. Embeddings improve the performance of ML model significantly.
