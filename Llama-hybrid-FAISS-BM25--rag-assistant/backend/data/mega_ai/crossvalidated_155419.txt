[site]: crossvalidated
[post_id]: 155419
[parent_id]: 
[tags]: 
Hull's GARCH vs. Definition in Time Series Literature

I have been reading up on volatility estimation and I encountered GARCH in Hull's "Options, Futures and Other Derivatives" (8e). He defines $u_n = \log{S_n/S_{n-1}}$ where $S_n$ is the price of some financial security, and then mentions how to calculate sample mean and sample variance, then assumes sample mean is zero so as to make calculations easier. I'm not sure he's still assuming zero mean when he mentions GARCH, but here's his rendition of GARCH(1,1): $\sigma_n^2 = \omega + \alpha u_{n-1}^2+\beta \sigma_{n-1}^2$ All other sources say it's: $\sigma_n^2 = \omega + \alpha a_{n-1}^2+\beta \sigma_{n-1}^2$, where $a_{n-1}=\sigma_{n-1} \epsilon_{n-1}$ with $\epsilon_i$ zero mean iid, i.e. $\sigma_n^2 = \omega + \alpha (\sigma_{n-1} \epsilon_{n-1})^2+\beta \sigma_{n-1}^2$ So my question is, how are these two formulations equal? By now I've read Tsay's "An Introduction to Analysis of Financial Data with R" and Ruppert's "Statistics and Data Analysis for Financial Engineering" and I still don't see how they could be similar. Thanks in advance for hints, /not a statistician
