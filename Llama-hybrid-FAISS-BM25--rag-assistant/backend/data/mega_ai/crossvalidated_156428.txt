[site]: crossvalidated
[post_id]: 156428
[parent_id]: 107610
[tags]: 
All of these answers are sales pitches for the natural log transformation. There are caveats to its use, caveats that are generalizable to any and all transformations. As a general rule, all mathematical transformations reshape the PDF of the underlying raw variables whether acting to compress, expand, invert, rescale, whatever. The biggest challenge this presents from a purely practical point of view is that, when used in regression models where predictions are a key model output, transformations of the dependent variable, Y-hat , are subject to potentially significant retransformation bias. Note that natural log transformations are not immune to this bias, they're just not as impacted by it as some other, similar acting transformations. There are papers offering solutions for this bias but they really don't work very well. In my opinion, you're on much safer ground not messing with trying to transform Y at all and finding robust functional forms that allow you to retain the original metric. For instance, besides the natural log, there are other transformations that compress the tail of skewed and kurtotic variables such as the inverse hyperbolic sine or Lambert's W . Both of these transformations work very well in generating symmetric PDFs and, therefore, Gaussian-like errors, from heavy-tailed information, but watch out for the bias when you try to bring the predictions back into the original scale for the DV, Y . It can be ugly.
