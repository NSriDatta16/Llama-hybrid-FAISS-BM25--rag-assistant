[site]: datascience
[post_id]: 103535
[parent_id]: 
[tags]: 
Understanding Max Pooling

I understand max pooling in CNNs can help decrease the computational load due to downsampling. Another thing mentioned is that max pooling can help provide a sort of "spatial invariance" for learned features. So essentially if say the upper left corner of an image has some feature let's say a circle, then max pooling might help in case that same circle is say shifted to the right slightly? Whereas had we not used max pooling this feature might seem like something new to the CNN despite it only being shifted by a small amount. I have a hard time sort of understanding why. I understand that (assuming no padding is used) that the next layer holds a smaller size image. In deep learning for python this passage on why a CNN without small pooling isn't good states: "It isn’t conducive to learning a spatial hierarchy of features. The 3 × 3 windows in the third layer will only contain information coming from 7 × 7 windows in the initial input. The high-level patterns learned by the convnet will still be very small with regard to the initial input, which may not be enough to learn to classify digits (try recognizing a digit by only looking at it through windows that are 7 × 7 pixels!). We need the features from the last convolution layer to contain information about the totality of the input." What the model looks like: Layer (type) Output Shape Param # ================================================================ conv2d_4 (Conv2D) (None, 26, 26, 32) 320 ________________________________________________________________ conv2d_5 (Conv2D) (None, 24, 24, 64) 18496 ________________________________________________________________ conv2d_6 (Conv2D) (None, 22, 22, 64) 36928 ================================================================ Total params: 55,744 Trainable params: 55,744 Non-trainable params: 0 I get that the third layer contains info about a smaller chunk of the main input image which just happens due to convolution. So can max pooling almost be thought of as normalizing? And this makes sure the "totality" is preserved? If so that makes sense to me, but I just wanted to make sure I understood the concept right. So I suppose it would be wrong to feed a 22*22 output from the third layer considering that the third layer really only knows about 7*7 windows of the input image. Is this what the author is saying here? That seems to make more sense now that I typed it out.
