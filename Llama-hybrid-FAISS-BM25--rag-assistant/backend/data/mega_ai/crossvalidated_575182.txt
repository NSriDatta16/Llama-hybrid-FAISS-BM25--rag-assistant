[site]: crossvalidated
[post_id]: 575182
[parent_id]: 
[tags]: 
Risk score uncertainty quantification

I am working on various risk score estimation problems . I assume individual subjects are associated with a true risk $$ r_i = f(x_i, \varepsilon)$$ where $x_i$ is some available information about the subject and $\varepsilon_i$ describes aleatoric uncertainty about the risk, which I assume to be heteroskedastic. (I.e., the distribution of $\varepsilon_i$ varies as a function of $x_i$ .) Furthermore, we are given observations $y_i$ of the (binary) outcome: $$ y_i \sim Ber(r_i).$$ The canonical way of estimating such risk scores is, of course, to use a probability model (say, a logistic regression or xgboost model), perform (log loss) regression, and interpret the model output (which lies in $[0, 1]$ ) as the risk estimate $\hat{r}_i(x_i)$ of that subject (possibly applying calibration to ensure that the risk scores coincide with the observed incidence, i.e., $\hat{r}_i(x_i) \approx E[y_i \mid X=x_i]$ ). What I am now interested in is quantifying the uncertainty of individual risk estimates $\hat{r}_i(x_i)$ . This should account for both aleatoric uncertainty (resulting from the action of $\epsilon_i$ ) and epistemic uncertainty (resulting from uncertainty about the learned model). However, I am having trouble finding canonical (ideally model-independent) methods to do so. What I have learned so far : (Regression) prediction intervals seem closest to what I am interested in (since risk estimation is a regression problem ). In my case, I would be interested in prediction intervals concerning the risk estimates $\hat{r}_i$ , not the actual outcome $y_i$ . Various comments and answers here on stats.SE suggest that this "does not make sense", but I do not see why that would be the case? Why exactly would it not make sense to apply any canonical PI method to the risk model and interpret the resulting interval as one within which a given individual's true risk is contained with likelihood X? (Notice that I am not interested in confidence intervals , because these only cover epistemic uncertainty.) I tried adapting standard bootstrapping-based approaches such as the one described here , but the problem is that that approach is based on resampling the residuals - and that does not seem feasible in my setting, because the "risk residuals" $r_i - \hat{r}_i$ are unobservable. There is a branch of work on quantile regression that can also be used to estimate prediction intervals. Again, I am having trouble adapting this line of work to the classification / binary outcome setting. The model I wrote down above differs from the standard logistic regression model due to the addition of the aleatoric risk uncertainty $\varepsilon_i$ . My model has two independent sources of aleatoric uncertainty: 1) uncertainty in the risk score, and 2) uncertainty in the outcome $y_i$ . Are there canonical methods that treat this setting? Or does my framing not make sense for any reason? For instance, I could imagine writing down a generative probabilistic model in the above form and then using MCMC or variational inference. This is something I am currently trying to implement, but I would prefer a model-agnostic approach. To summarize: what would be a canonical (model-independent) method for quantifying (heteroscedastic) aleatoric and epistemic uncertainty of binary risk scores? (Bonus points given for ways to actually do this in python.)
