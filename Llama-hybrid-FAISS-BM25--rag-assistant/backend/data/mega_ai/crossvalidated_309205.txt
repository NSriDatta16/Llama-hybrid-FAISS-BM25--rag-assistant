[site]: crossvalidated
[post_id]: 309205
[parent_id]: 309197
[tags]: 
This is called posterior predictive distribution in Bayesian statistics. The assumption for this to hold is that $y$ and $\bar{y}$ are independent conditioned on $\theta$ i.e. $$y \perp \bar{y} | \theta$$ Thus $p(\bar{y}|\theta, y) = p(\bar{y}|\theta)$. Then we can write $$p(\bar{y} | y) = \int p(\bar{y}|\theta, y)p(\theta| y)d\theta = \int p(\bar{y}|\theta)p(\theta|y)d\theta$$ We simply marginalize out $\theta$ from joint distribution conditioned on $y$: $p(\bar{y}, \theta | y)$
