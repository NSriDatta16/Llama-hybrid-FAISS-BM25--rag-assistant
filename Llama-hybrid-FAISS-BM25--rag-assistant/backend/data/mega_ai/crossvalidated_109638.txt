[site]: crossvalidated
[post_id]: 109638
[parent_id]: 109625
[tags]: 
If your strong belief is (as you say) that the probability is constant across samples and there's independence within-sample, you simply combine all of them into one big sample of $N_1+N_2+...$ and proceed as before. This is very easy. If on the other hand there is likely to be an amount of variation (not simply due to random variation, I mean variation in the underlying proportion itself) from a variety of subpopulations about an overall population proportion, you could consider trying a mixed model (and because it's binomial data, it would be a generalized linear mixed model), where the random factor is the group (the different times). In response to questions in comments: With very small samples (n=3 or 4 say) there's probably little to do but assume. With slightly larger samples you have a couple of possibilities for measuring the extra-variation that this would produce: i) One possibility is to look at the homogeneity across samples. You could do a formal chi-squared test of heterogeneity, perhaps with an exact test or even just simulated p-values if too many of the expected values are small for the chi-square approximation to work. However, note that formal hypothesis testing of assumptions isn't answering the right question. The question of interest with regard to assumptions isn't 'is the assumption false by enough that we can reject it?' -- with large enough sample size you always will reject, so given the assumption is probably always strictly false, that's merely telling you about your sample size. The real question is 'is it so false that it badly impacts the original inference?', which is an effect size question). This leads us to: ii) You could simply consider the differences visually by looking at/plotting the proportions and deciding if they're reasonably homogenous. You could make some choice about what you considered a big difference (in terms of 'does it matter' rather than 'is it statistically significant'). If you wished, you could go further by calculating the standard error of the proportions for each group. For example if the two standard errors of the proportions are $s$ and $l$ respectively (where $s important in size, you might find the assumption of homogeneity neither tenable nor a useful approximation. * you could come closer if you did them at 1.96 and 0.82 standard errors for the outer and inner intervals, but there's already several levels of approximation here anyway. Discussion of the derivation of (and more details of the use of) the standard error bars and how to visually judge that two proportions would be different at the 5% level. Refer to the Wikipedia page on binomial proportion confidence intervals for various intervals and references, but I give some additional ones here. In this case I'll write as if the asymptotic standard errors apply, but just as for single samples you're probably better off using something like Agresti-Coull type intervals -- say the bounds derived by using the "add two successes and two failures" rule $\tilde{p}=\frac{X+2}{\tilde{n}}=\frac{X+2}{n+4}$ estimate with the associated standard errors $s(\tilde{p})=\sqrt{\tilde{p}(1-\tilde{p})/\tilde{n}}$ (i.e. generate the usual simple symmetric Wald interval but about a biased estimate, which has reasonably good coverage properties). However, we're doing a comparison of two proportions. In that case (see Agresti and Caffo 1 ) we also add 4 observations, one success and one failure to each of the two samples, so now $\tilde{p}_i=\frac{X_i+1}{\tilde{n}_i}=\frac{X_i+1}{n_i+2}$ with standard error $s(\tilde{p})=\sqrt{\tilde{p}(1-\tilde{p})/\tilde{n}}$. The advantage of this over some of the other possible intervals (one it shares with the Wald) is that the Agresti-Caffo approach translates relatively easily to a visual display (and isn't so hard to explain). We're not so concerned about perfect coverage, so the Agresti-Caffo should be sufficient. Indeed, if you wanted to just use the asymptotic $\hat{p}=X/n$ with standard error $s(\hat{p})=\sqrt{\hat{p}(1-\hat{p})/n}$ that might do okay for what is largely intended to be a visual sense of whether the differences are important. In any case, assume for group $i$, we have some estimate, $\tilde{p}_i$ and some standard error $s_i(\tilde{p})$. Then when comparing two such estimates, using a Wald type interval for the difference, we could compute the standard error of the difference as $s_d = \sqrt{s_1^2+s_2^2}$. This is convenient but not so useful when looking for a visual difference. Now come some approximations. Let $s=\min(s_1,s_2)$ and $l=\max(s_1,s_2)$. Then $s_d = \sqrt{s^2+l^2}=l\sqrt{r^2+1}$ where $r =s/l$ lies between $0$ and $1$. We can approximate $\sqrt{r^2+1}$ by the upper bound $1+\frac{r}{1+\sqrt{2}}$. Hence $s_d = l\sqrt{r^2+1} \leq l\ (1+\frac{r}{1+\sqrt{2}}) =l+\frac{s}{1+\sqrt{2}}$ Now $1.96 s_d \leq 1.96\ l\ +\ s\frac{1.96}{1+\sqrt{2}}=1.96\ l\ +\ 0.82\ s \leq 2\ l + s$. So if the proportions are separated by at least twice the larger of the standard errors plus the smaller, the pairwise $(1-\alpha)$ interval for the difference would not include 0. Hence if you put intervals around the estimates at $\pm 1 s_i$ and $\pm 2 s_i$, and if neither of the wider intervals overlaps the narrower one in the other group, an interval for the difference in the original proportions on which the calculations are based would not include 0. In the above diagram, neither of the wider intervals overlaps the opposite narrower interval, which guarantees that $\tilde{p}_1-\tilde{p}_2$ is larger than $2l+s$ (note that we don't need to be able to tell which is $l$ and which is $s$, so 0 is not included in the 95% CI for the difference. A less conservative interval for the difference would use the inner bars at $\pm 0.82 s_i$ and the outer ones at $\pm 1.96 s_i$. But, just to repeat my earlier discussion, practical importance needs to be taken into account. In large samples this will tell you about trivial differences. While it can make sense to use it in very small samples (since it may tell you that differences that seem large may still not be from different populations), it's basically then just a test of homogeneity done in a way that doesn't control overall type I error, so you you should keep that in mind. Since I derived the approach for this question, there's no reference I can offer for it, but I wouldn't be at all surprised if it - or something quite similar - hadn't already been done. 1 : Agresti, A., and Caffo, B.(2000), "Simple and effective confidence intervals for proportions and difference of proportions result from adding two successes and two failures." The American Statistician , 54 (4), 280â€“288.
