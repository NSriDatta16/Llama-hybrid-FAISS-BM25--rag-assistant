[site]: datascience
[post_id]: 82210
[parent_id]: 82180
[tags]: 
First, let's clarify the issue with one-hot vectors: most NLP neural models nowadays don't use one-hot encodings for the model input; instead, they use (non contextual) embedding layers. While theoretically you get the same result multiplying a one-hot vector with a matrix, it is more practical just to index the position in the table directly, which is what embedding layers do. The Pytorch model you linked makes use of an embedding layer for the input, not one-hot vectors. Now, the specific questions: BERT representations have actually been used for chatbots. A quick google search reveals multiple relevant blog posts, github repos, etc. BERT representations are generic subword representations, meant to be reused in any context where text is received as input. The scenarios where it makes most sense to apply BERT's kind of transfer learning are those where training data is scarce. In the cases where there is abundant good-quality training data, the potential gains of using BERT representations are not so high. Summing up, it is perfectly fine to use BERT representations to create chatbots and many people have done it before.
