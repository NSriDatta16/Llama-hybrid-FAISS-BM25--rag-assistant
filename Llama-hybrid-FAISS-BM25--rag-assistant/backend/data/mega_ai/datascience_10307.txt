[site]: datascience
[post_id]: 10307
[parent_id]: 
[tags]: 
2 stage ensemble -- CV MSE valid in 1st stage but not in 2nd

I'm trying out a Kaggle competition, which puts me in the unusual position of being able to get feedback on my models' "true" performance (you can submit several predictions per day and they give you your score on a performance metric -- in this case MSE -- each time). As such, I've been observing a situation I can't explain -- my cross-validation results for my first stage Random Forest model, constructed with sklearn in Python , are consistently accurate within 1 percentage point of the true MSE. For example, my first stage model will give a CV MSE of about .22 and the "ground truth" (actual) MSE will turn out to be about .23. OTOH, my 2nd stage Random Forest, which is a different type of RF constructed in R, gives way "better" (smaller) training and cross validation MSE errors, like 0.05, but consistently turns out to have a much higher true MSE, like .23. In the 2nd stage Random Forest I have the predictions from the first stage and a few additional engineered features as regressors. Is there a statistical explanation for this? For example is it the result of some sort of recursion from stacking the models? If so, given that most winners of these Machine Learning contests are stacking many models, what's the appropriate way to do so that would avoid this statistical problem?
