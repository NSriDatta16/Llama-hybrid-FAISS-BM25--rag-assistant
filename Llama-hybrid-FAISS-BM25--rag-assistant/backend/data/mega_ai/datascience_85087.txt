[site]: datascience
[post_id]: 85087
[parent_id]: 85049
[tags]: 
Is smoothing in NLP ngram done on test data or train data? In short: both. Smoothing consists in slightly modifying the estimated probability of an n-gram, so the calculation (for instance add-one smoothing ) must be done at the training stage since that's when the probabilities of the model are estimated. But smoothing usually also involves differences at the testing stage, in particular for assigning a probability to unknown n-grams instead of 0.
