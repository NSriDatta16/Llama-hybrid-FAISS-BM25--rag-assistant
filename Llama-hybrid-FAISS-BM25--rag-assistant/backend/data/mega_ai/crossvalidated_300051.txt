[site]: crossvalidated
[post_id]: 300051
[parent_id]: 
[tags]: 
How to build a roc curve and do statistical analysis for discrete classifiers?

I have 5 supervised databases containing S similar documents and N not similar. Within each base, I separated 10 samples with bootstrapping. These samples contain the identifier of each document. For each document in the database, I retrieve the S documents that contain the largest similarity values ​​according to some measure of similarity between strings (so the similarity measure is my binary classifier). Since I have several measures of similarity between strings, I would like to test which one is best. Dataset 1 Sample 1 Document 1 - TP: 2125, TN: 3, FN: 2, FP: 1 ... Document X - ... Sample 2 ... Dataset 2 Sample 1 Document 1 - TP: 658, TN: 48, FN: 1, FP: 1 ... Within the S documents, if document X is really similar to the document I am testing, I increment the value of "true positives", otherwise I increase the value of "false positives". In the end, I have a confusion matrix for each element of the sample. And with that, I get a single confusion matrix, averaging all samples. I would also like to make the average confusion matrix of all bases. Can I do this since the size of each base is different? My question is whether I can make the ROC curve having only a single final confusion matrix, starting from the point (0,0) and going to the point (1,1). And if I may, what is the best statistical test for AUC since I am averaging the matrices of confusion.
