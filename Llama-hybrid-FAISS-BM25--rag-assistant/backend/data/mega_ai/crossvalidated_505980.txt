[site]: crossvalidated
[post_id]: 505980
[parent_id]: 505866
[tags]: 
The answer by gung-Reinstate Monica is fine. I add the following. In reality there is no such thing as a true model. There is no true parameter either (as such a parameter is only defined within a model). What we do is we use models to think about a reality that is different, however we don't have better tools than artificial formal models to make quantitative statements. So let's imagine that what we observe in reality behaves like data generating process, potentially infinitely repeatable, modelled by some distribution with a parameter $\mu$ , say, that we identify, in our brains, with some real quantity we are interested in. What we want to use the model for is to quantify the uncertainty, because we think of the real process as having some random variation, i.e., we will get other numbers when we do the same thing next time, the precise explanation of which is either unobservable or not of interest or not worth the effort finding out. What we want is some indication how far away from reality we might be with our best guess (the parameter estimate), because we are convinced, normally from experience, that the data that we have will not tell us precisely what is going on, however they hint at it, with some possible variation. The confidence interval is a way to use model-thinking to quantify this. It asks: If the model is true, which parameter values could have given rise to the data that we have observed? The confidence interval gives us a set of parameter values that, were the model true, are all compatible with what was observed, i.e., what was observed is a realistic, at least fairly typical thing if any of the values in the confidence interval were true, and pretty atypical if other values were true. Therefore it gives a set of "realistic" parameter values. That said, as I wrote before, none of these is really true , however as long as we think of the real situation in terms of the model, it makes sense to think of the model taking one of these parameter values. That may look disappointingly far away from reality, but it is hard to do better really. That's the nature of models. (Epistemic Bayesian logic would be an alternative, but it comes with problems that turn out to be fairly similar if you look at them in the right way.) The positive side of this modest way of interpreting things is that it doesn't rely on the model setup being literally fulfilled. Particularly there is no need to indeed repeat the experiment to give the result a meaning. This is an imagination anyway, a tool for thinking about the situation, the possibility of which can be more or less close to reality. (Obviously the advantage of indeed being able to repeat this several times is that we have better ways to assess if the model world is reasonably in line with the real world.) Issues: (1) As I wrote, the model is in fact not true. This is not generally a problem (it's the nature of models actually), but it is a problem if it is violated in ways that make thinking in terms of the specific model strongly misleading. A typical issue is if data are strongly positively correlated if in fact the model assumes them independent - you'll end up with a far too narrow confidence interval. (2) Confidence intervals oversimplify things in the sense that you specify a confidence level, and then parameters are either in or out. However, the difference in compatibility with the data of parameters that are borderline in or borderline out, respectively, is not that big. If your confidence interval is, say, $[5,10]$ , it is not really appropriate to think that 5.1 is a perfectly realistic value whereas 4.9 is totally not. Rather 4.9 is just slightly more unrealistic than 5.1, which is conceivable as true but may (depending on the exact model, statistic used etc.) be substantially less realistic than, say, 7.
