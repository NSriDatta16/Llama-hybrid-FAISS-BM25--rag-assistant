[site]: crossvalidated
[post_id]: 126646
[parent_id]: 122409
[tags]: 
Answering Jessica's question directly - one reason for downsampling is when you're working with a large dataset and facing memory limits on your computer or simply want to reduce processing time. Downsampling (i.e., taking a random sample without replacement) from the negative cases reduces the dataset to a more manageable size. You mentioned using a "classifier" in your question but didn't specify which one. One classifier you may want to avoid are decision trees. When running a simple decision tree on rare event data, I often find the tree builds only a single root given it has difficulty splitting so few positive cases into categories. There may be more sophisticated methods to improve the performance of trees for rare events - I don't know of any off the top of my head. Therefore using a logistic regression which returns a continuous predicted probability value, as suggested by Marc Claesen, is a better approach. If you're performing a logistic regression on the data, the coefficients remain unbiased despite there being fewer records. You will have to adjust the intercept, $\beta_0$, from your downsampled regression according to the formula from Hosmer and Lemeshow, 2000: $$\beta_c=\beta_0 - \log\left(\frac{p_+}{1-p_+}\right)$$ where $p_+$ is the fraction of positive cases in your pre-downsampling population. Finding your preferred spam ID threshold with the ROC can be done by first scoring the complete dataset with the model coefficients traned on the downsampled dataset, and then ranking the records from highest to lowest predicted probability of being spam. Next, take the top $n$ scored records, where $n$ is whatever threshold you want to set (100, 500, 1000, etc.) and then calculate the percentage of false positive cases in the top $n$ cases and the percentage of false negative cases in the remaining lower tier of $N$-$n$ cases in order to find the right balance of sensitivity/specificity that serves your needs.
