[site]: datascience
[post_id]: 12241
[parent_id]: 12222
[tags]: 
I've been using tree-based enesembling methods such as random forests and gradient boosting for several years now, and I have to say that I've never seen that behavior. Some packages measure variable importance solely based on trees' final splits rather than candidate/surrogate splits, so if you have two important inputs that are correlated, but one is consistently a tiny bit better than the other, the less important input might never get selected as a final split and thus look as important as some of the less valuable inputs. However, this phenomenon is independent of the number of trees/ensembling, so I don't think it fully explains the behavior you're seeing.
