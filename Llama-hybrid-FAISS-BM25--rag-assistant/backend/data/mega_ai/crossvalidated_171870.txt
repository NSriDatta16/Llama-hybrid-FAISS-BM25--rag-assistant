[site]: crossvalidated
[post_id]: 171870
[parent_id]: 120350
[tags]: 
Euclidean distance is not suitable for comparing documents or clusters of documents. When comparing documents, one key issue is normalization by document length. Cosine similarity achieves this kind of normalization, but euclidean distance does not. More over, documents are often modeled as multinomial probability distributions (so called bag of words). Cosine similarity is an approximation to the JS-divergence which is a statistically justified method for similarity. One key issue with documents and cosine is that one should apply proper tf-idf normalization to the counts. If you are using gensim to derive the LSA representation, gensim already does that. Another useful observation for your use case of 2 clusters is that you can get a good non-random initialization because LSA is just SVD. You do it in the following way: Take just the first component of each document (assuming the first component is the top singular vector). Sort those values by keeping track of the document ids for each value. cluster 1 = document ids corresponding to top e.g. 1000 (or more) values cluster 2 = document ids corresponding to bottom e.g. 1000 (or more) values just average the vectors for each cluster and normalize by vector length. Now apply k-means to this initialization. This means just iterate (1) assigning documents to the current closest centroid and (2) averaging and normalizing new centroids after reassignment
