[site]: stackoverflow
[post_id]: 2432344
[parent_id]: 2425549
[tags]: 
I don't agree with steve that an SVM is a bad choice here, although I also don't think there's much reason to think it will do any better than any other discriminative learning algorithm. You are going to need to at least think about designing features. This is one of the most important parts of making a machine learning algorithms work well on a certain problem. It's hard to know what to suggest without more idea of the problem. I guess you could start with counts character n-grams present in the URL as features. Nobody really knows how much data you need for any specific problem. The general approach is to get some data, learn a model, see if more training data helps, repeat until you don't get any more significant improvement. Kernels are a tricky business. Some SVM libraries has string kernels which allow you to train on strings without any feature extraction (I'm thinking of SVMsequel , there may be others). Otherwise, you need to compute numerical or binary features from your data and use the linear, polynomial or RBF kernel. There's no harm in trying them all and it's worth spending some time finding the best settings for the kernel parameters. Your data is also obviously structured and there's no point in letting the learning algorithm try and figure the structure of URLs (unless you care about invalid URLs). You should at least split the URL up according to the separators '/', '?', '.', '='. I don't know what you mean by 'keep it up to date'. Retrain the model with whatever new data you have. This depends on the library you use, in svmlight there is a program called svm_classify that takes a model and an example and gives you a class label (good or bad). I'm sure it's going to be straightforward to do in any library.
