[site]: crossvalidated
[post_id]: 229379
[parent_id]: 229378
[tags]: 
I believe this has been discussed in the literature. In regression context, the measures of model complexity utilize the linear regression relation of the rank of the projection matrix being equal to the number of (non-collinear) regressors. So Ye (1998) generalized this by perturbing the data $\tilde y_i^{(k)} \leftarrow y_i + \delta_i^{(k)}$, running your favorite machine learning model (these were called data mining back then) on these perturbed data, getting predictions $\hat y_i^{(k)}$, and measuring how much the predictions changed in response to perturbations. Repeating this many times, you collect a big data set with such predictions as outcomes, and the magnitude of perturbations as regressors, $\hat y_i^{(k)} \sim \mbox{const} + h_i \delta_i^{(k)}$. Then the sum of regression coefficients $h_i$ gives you the degrees of freedom. The HTF ESL book repeats this argument in Section 7.6, although without attribution to Ye. They further discuss MDL and VC dimension, which are probably relevant to your case, too.
