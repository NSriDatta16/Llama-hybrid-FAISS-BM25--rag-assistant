[site]: crossvalidated
[post_id]: 148250
[parent_id]: 148243
[tags]: 
Backpropagation, essentially, is just an algorithm to compute the gradient of a neural network using chain rule. Please refer to this question for details. If your loss (error) function is differentiable with respect to your model's parameters, there's no reason not to use the gradient, since it tells at each point in the parameter space where to go in order to minimize the loss.
