[site]: crossvalidated
[post_id]: 281291
[parent_id]: 281288
[tags]: 
Let's assume you are talking about mis-classification on training data, i.e., difficult to minimize the loss on training data set, no testing data over-fitting problem involved. You are correct that, in most cases, the mis-classification can coming from "model is too simple" or "the data is too noisy". I would like to give two examples to further illustrate. The model is "too simple" to capture the "patterns in data". The example is shown in the left figure. Suppose we want to use a logistic regression / a line to separate two classes, but the two classes are not linear separable. In this case, there still are "notable patterns in the data", and if we change the model, we may getting better. For example, if we use KNN classifier, instead of logistic regression, we can have very good performance. The data has too much noise, that it is very hard to do the classification task. The example is shown in the right figure, where, if you check the code, you will see two classes are very similar (two classes are 2D Gaussian, the mean is $0.01\times 2$ apart, but the standard deviation for each class is $1.0$ ). It is essentially a very challenging task. Note that the two examples are trivial, since we can visualize the data and the classifier. In the real world, it is not the case, when we have millions of data points and super complicated classifiers. Code: library(mlbench) set.seed(0) par(mfrow=c(1,2)) d=mlbench.spirals(500) plot(d) lg_fit=glm(d$classes~d$x[,1]+d$x[,2]-1,family=binomial()) abline(0,-lg_fit$coefficients[1]/lg_fit$coefficients[2]) d2=mlbench.2dnormals(500,r=0.01) plot(d2)
