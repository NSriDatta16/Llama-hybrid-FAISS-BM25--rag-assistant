[site]: crossvalidated
[post_id]: 106039
[parent_id]: 106028
[tags]: 
Drop the hypothesis test and the p value altogether. I'm not confident that 95% of "stats people" really understand p values; with "non-stats people", I'd bet it's less than half. Don't go there unless you must. In your example, it sounds like you're trying to reject the null that the slope parameter for the predictor in question is zero. Is that really what you want to do? Might you prefer to estimate what the slope parameter actually is, and express your degree of confidence in that estimate? My guess is that most non-stats people would prefer the latter. It doesn't just take statistical education to understand a p value; it also takes a fair amount of epistemology to appreciate the value of falsification, and why a big scientific study could somehow not produce good enough results to conclude anything trustworthy enough to deserve printing. Conversely, positively framed information appeals to basic intuition. Consider presenting your results as effect size estimates with confidence intervals. This won't entail falsificationism, will focus on the information gained from your study, and will communicate what needs to be said about the population from whence you sampled. The slope coefficient itself can be interpreted in simple terms of size (e.g., a standardized $\beta=.15$ is probably a fairly weak relationship, but if you have 3Ã— larger standard error, maybe your $\beta=.45$; that's a fairly strong relationship in many contexts), or in literal terms of the regression equation (e.g., for one unit increase in your predictor, your model estimates that the outcome increases by b ). Either way, it's much more intuitive than the correct understanding of a p value. It's somewhat different information, but again, it's probably the information a non-academic audience would want first. They might even have a healthily skeptical attitude toward the results of your study with regard to conclusions about the population based on intuition alone, so I don't think you cause any harm necessarily and inherently by focusing attention on your study's estimates instead of on its implications for the population. For academic, statistics, and all other kinds of audiences, confidence intervals should suffice as the vehicle of information regarding the population. In your case, while presenting your effect size estimate and suggesting its implications, it may be worthwhile to draw attention to the marginally >5% chance that the real relationship between predictor and outcome could actually operate very (probably even negligibly) weakly in the opposite direction from what your study indicates in the overall population...or it may not. Non-academic audiences may be just as content with a 90% confidence interval that leaves only a 10% doubt that your study has enumerated the correct range of possibilities for the strength of the relationship in question, and if you modeled your data correctly, you can be 90% confident that there is a relationship in the direction your model indicates. Furthermore, a confidence interval draws attention to the possibility that the relationship is larger than your model estimates. IMO, focusing on null hypotheses tends to draw attention toward the more dismissive half of the confidence interval, but that is only half of the story.
