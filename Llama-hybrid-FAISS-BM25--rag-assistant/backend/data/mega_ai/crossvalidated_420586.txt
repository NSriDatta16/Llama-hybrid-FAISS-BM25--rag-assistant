[site]: crossvalidated
[post_id]: 420586
[parent_id]: 420583
[tags]: 
With PCA, you rely on the magnitude of the coefficient in the principal component vector because each of the coefficients of the PC is explicitly multiplied with features while calculating the projection. However, in Kernel PCA, the transformed vectors, $\phi(x)$ , are never explicitly calculated; we directly get the projections. Especially, in some cases like Gaussian kernel, these vectors are infinite dimensional, and can't be stored. For finite mappings, you could try to rewrite the exact mapping and perform PCA directly on the transformed data to get which of the new features are important (based on this variance based criterion).
