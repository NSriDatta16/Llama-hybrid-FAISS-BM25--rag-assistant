[site]: crossvalidated
[post_id]: 641290
[parent_id]: 
[tags]: 
How does Cross Validation work in decision trees (or tree ensembles)

I've been working with tree-based models for a long time and I never really asked myself how cross-validation would work when building a tree. For the sake of this question, suppose I've split my training dataset into four folds. Wouldn't each set of folds result in a different tree? If so, wouldn't you be averaging the testing error of K different models instead of estimating K instances of the test error of the same model? For example, let's say I have 10,000 observations in my training dataset and I'm using four-fold cross-validation. If I train with folds 1, 2 and 3, I'll get a different model than if I train with folds 2, 3 and 4 because: The features selected at each split will be different; The cutoff points will be different even if the same features are selected; and The trees will have different depths. How do you interpret a cross-validated accuracy metric? The only thing that would make sense to me is to keep the same tree structure (features, order of features and cutoff values) and evaluate the model on the four folds. However, which of the four possible datasets is used to create the model's structure?
