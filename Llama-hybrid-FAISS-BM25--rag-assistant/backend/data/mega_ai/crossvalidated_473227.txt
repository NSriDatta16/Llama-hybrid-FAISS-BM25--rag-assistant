[site]: crossvalidated
[post_id]: 473227
[parent_id]: 473219
[tags]: 
I believe you're missing an essential point in both objectives in that we are trying to learn a distribute drepresentation $\in \Re^d$ as opposed to using the one hot encodings. Consider the example of having a vocabulary of size $|V|$ , then we can represent each word using a one hot encoding $\in [0, 1]^{|V|}$ . The skip-gram model will work as follows given a set of parameters $E \in \Re^{d \times |V|}$ , and $W \in \Re^{d \times |V|}$ : Generate the one hot input vector $x \in [0, 1]^{|V|}$ for the center word Obtain the embedding for the word vector $e = Ex \in \Re^d$ We then generate scores $z = W^Te \in \Re^{|V|}$ for the surrounding words We can then normalize the scores using a softmax function $y = \text{softmax}(z)$ , where $y_i$ are the probabilities of observing each context word. Here, the objective is to learn the parameters $E$ & $W$ so that our probabilities $y$ match the true one hot encodings of the context words given the center word. I gave the example of skip-gram above, but CBOW is essentially the same but reversed. For more details on the objective function you could refer to [1], or for a visual explanation of the whole process [2]. [1] https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf [2] http://jalammar.github.io/illustrated-word2vec/ [2]
