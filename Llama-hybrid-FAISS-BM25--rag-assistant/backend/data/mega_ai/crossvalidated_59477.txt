[site]: crossvalidated
[post_id]: 59477
[parent_id]: 59301
[tags]: 
Support vector machine classifiers use the following decision function to determine the label for a test instance $\mathbf{z}$: $f(\mathbf{z})=\mathtt{sign}\big(\sum_{i=1}^{totalSV} y_i \alpha_i \kappa(\mathbf{x}_i,\mathbf{z})-\rho\big)=\mathtt{sign}\big(\langle\mathbf{w},\Phi(\mathbf{z})\rangle-\rho\big)$, where $\kappa(\cdot,\cdot)$ is the kernel function, $\alpha$ contains the support values, $\mathbf{y}$ is the training label vector, $\rho$ is a bias term and $\mathbf{w}$ is the separating hyperplane in feature space. In a LIBSVM model, sv_coef contains $\alpha_i y_i$ and SVs contains the support vectors ($\mathbf{x}_i$). To predict you need to perform kernel evaluations between the test point and all support vectors. For the linear kernel ($\kappa(\mathbf{x},\mathbf{z})=\mathbf{x}^T\mathbf{z}$) you can compute $\mathbf{w}$ explicitly: $\mathbf{w}=\sum_{i=1}^{totalSV} \alpha_i y_i \mathbf{x}_i=\mathtt{sv\_coef}^T \times \mathtt{SVs}$. Subsequently, predictions are simply based on the sign of $\mathbf{w}^T\mathbf{z}-\rho$.
