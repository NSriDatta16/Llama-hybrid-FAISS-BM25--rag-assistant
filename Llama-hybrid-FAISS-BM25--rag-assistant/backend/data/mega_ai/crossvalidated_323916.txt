[site]: crossvalidated
[post_id]: 323916
[parent_id]: 
[tags]: 
Similarity metric for sets of sets of nominal data

I felt it important to include the full motivation for my question, since part of my trouble I think is simply not knowing the terms to search for. Similarity of set of sets and Similarity metric for 2 sets of vectors come close to what I'm looking for, but can't quite get me all the way. Suppose I have a collection of datasets D = { d 1 , d 2 , ... d n }. I evaluate |C| classifiers on these datasets, and have the results (accuracy, error, AUROC, whatever) in a |C|x|D| matrix, M. I want to find relationships between the performance of each of the classifiers and some characteristics of different datasets known prior to the experiments (num cases, num features, domain of origin, etcetc). I have pre-defined groupings of the datasets G, defined as e.g G i = { { d 1 , 3 ... }, { d 15 , d 32 ... }, ... { d 18 , d 41 ...}} So that maybe e.g G 1 is the datasets grouped into 2-class and multiclass datasets, G 2 is datsets grouped into 'small', 'medium' and 'large' number of attributes, etc. All the datasets in D can be found in each G. Within G i , all subsets are disjoint. However, different Gs may have different numbers of sets within them, of different sizes. I perform some form of unsupervised clustering (Xmeans currently) on the results matrix M, resulting in essentially new data grouping, G r , that is data driven. The actual question: is there some similarity metric I can use to define how 'close' the grouping G r is to each of the predefined groupings in G? In other words, similarity between sets of sets of categorical data? Or the similarity of different clustering results (in the context of nominal data)? If that could be defined, then I could as a result state something to the effect of: these dataset characteristics discriminate the most between the performance of these different classifiers (if a grouping is highly similar), or if the dataset groupings in G are roughly equally distant from G r then we could state that classifier A has similar performance regardless of data characteristics, etc. It occurs to me this may be an active field of research, and that there is no 'best' metric or anything like that, but will appreciate any links or search terms for further reading The best I could imagine myself is to say d(G i , G j ) would be something like: Calc the pairwise Jaccard similarities between each subset of i to each subset of j Find some mapping of group to group, e.g group G i,a is closest to G j,b , with potentially left-over subggroups if |G i | != |G j | Then the overall grouping distance is perhaps the average Jaccard distance of the 'analogous' subgroups of G i and G j , + some penalty for any groups left over.
