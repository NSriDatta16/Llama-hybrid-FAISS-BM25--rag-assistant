[site]: crossvalidated
[post_id]: 473403
[parent_id]: 
[tags]: 
How low does the cross entropy loss need to be for me to be confident in my model?

I am training a neural network for a multi-class classification task. The loss function I am using is the CrossEntropyLoss implemented in pytorch , which is, according to the documents, a combination of logsoftmax and negative log likelihood loss (forgive me for not knowing much about them, all I know is that cross entropy is frequently used for classification). I started out with a loss of 2.4 and then it descends to and fluctuates in the range 0.65~0.7. Can I infer from this value how accurate my model is now? I remember reading in an article that a cross entropy loss of 0.69 is equal to random guess in the binary classification case, is it different somehow in the multi-class classification? Or does it mean my model is basically guessing randomly? More details, if they matter: I am running on CIFAR10, so there are 10 classes. The loss is calculated per sample, so the batch size is not reflected here.
