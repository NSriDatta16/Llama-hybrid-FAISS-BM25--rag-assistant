[site]: crossvalidated
[post_id]: 275547
[parent_id]: 275527
[tags]: 
Quid est veritas? I can accept @amoeba's answer as readily as the original poster. I caution, however, that in all my work, I've not encountered a Bayesian analysis which calculated "the probability that the null hypothesis is true". And such a conclusion would attract a whole host of arguments from those reviewing your work! Philosophically, it does bring us back to the question: "what is truth?" Perhaps "truth" is irrefutable, even to evidence itself. Statistics is a tool of science to quantify uncertainty. I still maintain that, while evidence can strongly point to a truth, there is always a risk of a false positive finding, and the Good Statistician should report this risk. Even in Bayesian decision theoretic testing, a decision rule is given so that we can accept or reject hypotheses based on Bayes factors which are roughly proportional to $Pr(H_0 | X)$, but our belief is never $1$ or $0$ even when our decision is. Decision theory gives us a means of "going forward" with partial knowledge and accepting these risks. Part of the rationale for null hypothesis statistical testing (NHST) and the $p$-value is Karl Popper's philosophy of falsification . In this: a critical assumption is that the "truth" is never known, we can only whittle down other hypotheses. An interesting and a valid criticism of NHST is that you are forced to make ridiculous assumptions, like that smoking does not cause cancer when you're really interested in a descriptive (not inferential) study: and you are merely describing how much cancer smoking causes. The converse criticism has been applied to Bayesian studies where you can liberally apply priors: Dennis Lindley has said, "With prior probability 0 that the moon is made of cheese, astronauts returning with arms full of cheese still could not convince." The missing information to determine whether the null hypothesis is true is, trivially, the knowledge as to whether the null hypothesis is true. Ironically, when focused on descriptive statistics, we can accept tolerable ranges of possible effects and conclude somewhat strongly that a trend is probably true: but statistical testing does not lead us to such findings. Even in Bayesian inference, no data will lead to a singular posterior without having some methodologic issues, so incorporation of a prior does not fix this problem.
