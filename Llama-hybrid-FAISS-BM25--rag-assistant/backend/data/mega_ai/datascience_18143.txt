[site]: datascience
[post_id]: 18143
[parent_id]: 
[tags]: 
Correct order of operations involved into Dropout

Suppose we have CNN with any hidden layer with activation followed by dropout layer. What is the correct precedence of activation and dropout operation if dropout implementation is inverted dropout and CNN mode is training mode ? Do I need to compute activation in the first layer and then apply dropout with division by retain probability p, or I need to apply activation to the result of the division? Say we have the following keras code model.add(Dense(128, activation='relu')) model.add(Dropout(0.5)) If I understand correctly, dropout with division by p will be applied to the activated result: result = [survive_mask] * relu(output)/p Is this correct? Wouldn't it be more natural to have result = [survive_mask] * relu(output/p) because otherwise dropout operation breaks activation value normalization (i.e. to [0, 1]) ?
