[site]: crossvalidated
[post_id]: 427607
[parent_id]: 427227
[tags]: 
If I understand correctly what you are doing, each of the red dots is the estimated performance by the inner cross moving window validation. I.e., it is an RMSE based on 5 cases, each of the blue dots is the estimated performance by the outer cross moving window validation, i.e. the "RMSE" based on 1 case (the next one after the 52 training cases). In this case, this RMSE over 1 case is also the absolute error for that case, which is why I put the RMSE in quotes, and this is done 102 times, testing cases 53 - 154: t t t ... t t t r r r r r b x x x ... x x x x t t t ... t t t r r r r r b x x x ... x x x x t t t ... t t t r r r r r b x x x ... x . : x x x ... x x x t t t ... t t t r r r r r b with x = unused, t = inner training, r = red = inner testing = hyperparameter tuning, b = outer testing Then: The random uncertainty of the RMSE depends on the number of cases that enter the calculation of the RMSE: the more cases, the lower the variance on the RMSE estimate, thus the less noisy we expect the estimate to look. So the blue estimates being based on 1 case each are expected to be more noisy than the red estimates which are based on 5 cases each. Unless you retrain after fixing hyperparameters on t t t ... t t t r r r r r (do you?), the blue predictions have more lag, i.e. are further into future. This may lead to higher variance error in the predictions, thus also of the blue error. Also, if the inner RMSE estimate is based on 5 consecutive time points and shifted one at a time, it must have considerable auto-correlation (which it does). Autocorrelation will make it look ness noisy. Due to the complex interaction (the inner RMSE estimte being actually part of the training) I find it difficult to say whether the outer RMSE estimate is surprisingly noisy. A simulation of the behaviour of the two RMSEs may be helpful. As RMSE is the square root of the average squared error (as opposed to averaging RMSEs) the variance uncertainty of our RMSE estimate does not quite follow the simple variance of the mean $s^2_\bar x = \frac{1}{n} s^2_x$ dependency. See variance of variance estimate for more information. As a side note, I had only a very quick glance at the blog post you linked, so I cannot quite comment on its total virtue. However, already the first glance did raise some red flags: RMSE over 1 case is just absolute error of that one case. Of course, one can use this - but a reminder of this property to the unsuspecting reader would have been good. I wouldn't have mentioned this, though, if there weren't the nex concern: Taking the average of $i$ 1-case-"RMSE"s is not the same as calculating the RMSE over $i$ cases, it's the MAE of those $i$ cases. Again, it is not forbidden to do this - but it should IMHO be clearly labeled as "mean RMSE as opposed to RMSE of the pooled predictions", and again the unsuspecting reader should be made aware of what is done. Also a justification would be in place, IMHO. Together, these two points make me suspicious whether the author actually calculates what they think they are calculating (to me this has a distinct "smell" of possible error with operator precedence). Looking at RMSE of models with a varying length of the training window makes a lot of sense. But pooling these results across widely varying training window sizes IMHO does require at the very least a justification. I don't have access to the papers the blog references, but I'd have expected methods with rolling origin in their name to actually have a moving origin, i.e. a moving training window of fixed size. But the day-chaining described as being the same does use expanding windows with fixed origin (and moving end). "cross validation" seems to me a rather unlucky choice of name as it's explicitly about not doing cross validation due to the time dependency - though to be fair that may be the result of the blog post author trading off unlucky choice of name vs. term established by the Bergmeir & Ben√≠tez paper. I'd have used expanding window validation (or rolling window valiation in your case) instead.
