[site]: crossvalidated
[post_id]: 349778
[parent_id]: 349772
[tags]: 
The answer to the first question is yes, it can be overfitted. Your data might be very correlated and easy to predict. Also, yes, there are ways to manage overfitting of a neural network. Among these you have regularisation (where you suppress coefficients with a penalty) and drop-out (randomly leaving out some neurons during training) and data augmentation (generating more data that looks similar to the actual data). You can obviously also avoid overfitting by getting more data, but I'm guessing you're using all the data available. As you say "after first fold" I'm assuming that you've trained your network once on the other k-1 parts. In this case you can be "lucky enough" to have a combination of data in the folds making it fit very well to that specific fold. Btw, you could add some jitter to your plot or simply print a confusion matrix to get a better understanding of what's going on. I'd say the best way to ensure that your model is doing good is to leave out some data to test in the end. Remember to only use a test set once to avoid information leakage.
