[site]: crossvalidated
[post_id]: 381284
[parent_id]: 
[tags]: 
Mutual Independence in a Multivariate Normal with Identity Covariance

Consider a random vector $X$ which follows a multivariate nomal with zero means and Identity Covariance. $X\sim \mathcal{N}_n(\mathbf 0, \mathbf I)$ We can say that the individual variables $X_1, X_2, \cdots, X_n$ are pair-wisely independent, since in the case of variable pairs in a multivariate normal, zero correlation implies independence (see [1]). However, I am struggling to prove mutual independence of the individual variables. Note that pair-wise independence does not generally imply mutual independence , as the latter is a stronger condition (see these two links). In other words, I want to prove that not only any pair from $X_1, X_2, \cdots, X_n$ are independent but that any individual $X_i$ is independent of any intersection of the remaining variables. I also suspect that mutual independence holds for any diagonal covariance matrix... $$ $$ [1] Robert V. Hogg, Joseph W. McKean, and Allen T. Craig. "Introduction to Mathematical Statistics, 7th Edition". In: Pearson, 2013, pp. 182-183. ISBN: 978-0-321-84943-4.
