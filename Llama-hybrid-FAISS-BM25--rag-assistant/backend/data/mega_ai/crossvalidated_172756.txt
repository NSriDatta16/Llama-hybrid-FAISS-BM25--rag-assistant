[site]: crossvalidated
[post_id]: 172756
[parent_id]: 
[tags]: 
possible cause for in- and out-of-sample error go same direction when controlling regularization factor

I'm fitting a classification model, say with random forest (same issue happens if I use logistic regression or neural net). I can control overfitting by playing with the depth of trees (or L1/L2 regularization for logistic regression) What can cause in- and out-of-sample classification error go up or down together? In my case error(in) error(out) 0.45 0.48 0.35 0.46 0.0 0.43 So methodologically bias goes down quicker than variance goes up. Does that mean there is some structure in the data and the model needs more data to fit it better? If more data is not possible, what is the best to try next?
