[site]: crossvalidated
[post_id]: 242041
[parent_id]: 77018
[tags]: 
Random forest is a bagging technique and not a boosting technique. In boosting as the name suggests, one is learning from other which in turn boosts the learning. The trees in random forests are run in parallel. There is no interaction between these trees while building the trees. Once all the trees are built, then a voting or average is taken across all the trees' prediction depending on whether the problem is a classification or regression problem. The trees in boosting algorithms like GBM-Gradient Boosting machine are trained sequentially. Let's say the first tree got trained and it did some predictions on the training data. Not all of these predictions would be correct. Let's say out of a total of 100 predictions, the first tree made mistake for 10 observations. Now these 10 observations would be given more weightage when building the second tree. Notice that the learning of the second tree got boosted from the learning of the first tree. Hence, the term boosting. This way, each of the trees are built sequentially over the learnings from the past trees.
