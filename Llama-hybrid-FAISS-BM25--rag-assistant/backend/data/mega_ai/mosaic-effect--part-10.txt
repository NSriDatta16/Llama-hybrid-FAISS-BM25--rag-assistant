fication by compilation has been to ignore aggregation risks in the absence of explicit Security Classification Guides governing specific datasets. This strategy has proven inadequate, as it leaves certain sensitive combinations unprotected despite their aggregate risk profile. In counterterrorism contexts, public expectations often reflect a conflict between wanting stronger intelligence and opposing the data practices that enable it. The United States Supreme Court has not formally endorsed the mosaic theory, but has applied its logic in decisions on digital surveillance. It has been proposed that the mosaic theory apply to any surveillance method capable of long-term data collection, including persistent drone or camera tracking over extended periods. The mosaic theory holds that although isolated data points may not constitute a search, their aggregation can reveal patterns so revealing that they implicate constitutional protections. Agencies and mosaic The United States Department of Defense (DOD) utilizes shared, unclassified data repositories to consolidate relevant information for analysis and operational use. Immigration enforcement agencies draw from multiple databasesâ€”including bio-metrics, utility records, and driver's license data. These are used to piece together profiles and track individuals, often circumventing attempts to limit access to specific data sources. Although security agencies raised alarms, only one dataset out of over 90,000 on Data.gov was ever pulled back due to mosaic effect concerns. The mosaic effect was not anticipated during the original 2009 launch of Data.gov, despite its foundational role in the open government agenda. The DOD explicitly warns that modern data aggregation and correlation tools can combine unclassified data into compilations that require classification and special handling. Elevating the classification level of an entire environment to match its most sensitive component is another method of preventing aggregation-based disclosures. This method reduces the risk of spillage but simultaneously restricts access to data, limiting usability across agencies and users. An alternative relies on individual users to maintain compliance with classification by compilation rules during data retrieval and fusion. This method burdens users with the need to internalize all relevant classification policies, limiting effective access and discouraging broad data usage. A technical expert panel convened by HHS evaluated whether existing disclosure-limitation methods remain adequate in the face of mosaic-effect concerns or require enhancement. The phenomenon is considered a second-order effect stemming from the very design of shared data environments intended to support machine learning and related analytics. The U.S. Department of Health and Human Services has noted a lack of empirical studies quantifying mosaic-effect risks or prescribing best practices to mitigate them. At DHS, internal privacy leadership observed that agencies default to collecting all accessible data, regardless of operational necessity. Agency privacy plans sometimes outline overbroad or questionable practices that go unchallenged due to lack of public scrutiny. Merging behavioral and identity-linked government data, even without names or unique individual data like Social Security numbers, can produce composite profiles specific enough to identify individuals or small groups, a practice already enabled by formal data-sharing agreements among agencies including the Justice Department, HUD, the IRS, the Social Security Administration, HHS, and the Defense Department. Data held by U.S. agencies can create a granular, life-spanning profile of individuals, including financial, bio-metric, familial, and even posthumous records. Facial photographs, such as those from passports, are routinely converted into algorithmic templates usable across biometric systems. Formal inter-agency agreements permit record matching across depart