[site]: crossvalidated
[post_id]: 587941
[parent_id]: 
[tags]: 
Do positional encodings result in permutation invariance?

In Attention, Learn to Solve Routing Problems , it was mentioned that: but we do not use positional encoding such that the resulting node embeddings are invariant to the input order I'm confused about this sentence. Positional encodings in the original Transformer paper are to incorporate the idea of the position of a token in the sequence. Being invariant to the input order means that there's permutation invariance - in other words, an arbitrary labeling of the nodes (you can also view this an arbitrary sequence of nodes) should not change the resulting embedding. Does anyone know why positional encodings result in permutation invariance?
