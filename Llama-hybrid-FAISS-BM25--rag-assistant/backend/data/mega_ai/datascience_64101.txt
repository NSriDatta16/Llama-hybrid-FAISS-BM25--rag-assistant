[site]: datascience
[post_id]: 64101
[parent_id]: 64044
[tags]: 
I don't see much point in doing sliding windows with LSTM. The whole idea of this layer is to capture long-term dependencies and you're just throwing it away with the 5 step window limit. The easiest for this case would be using a Conv1D with kernel_size=5 instead. Then the windows would go automatically with the kernel size. (But only one layer, if you add more, you will capture more steps). Other possibilities are 2 layers with kernel size = 3, or maybe 3 layers with sizes 3, 2, 2. If you do want to use windows with LSTM, you will have to organize the data manually. This means you will loop your data and get segments of length 5 and treat each segment as an individual sequence. In this case your input shape will be (5,1) and you will have far more than 82 samples. On the other hand, if all your sets are longer than length 5, you will need no padding at all. Example loop: originalData = load_a_list_of_samples() windowData = [] for sample in originalData: L = len(sample) #number of time steps for segment in range(L - 5 + 1): windowData.append(sample[segment:segment+5]) windowData = np.array(windowData) My suggestion though, unless you have this requirement for some special reason, is don't use windows at all. Let the LSTM do their job and capture long term dependencies. Pad the data as intended, use the Masking layer and place the LSTM with return_sequences=True . The results will be like (showing only the length dimension) inputs = [step1 , step2 , step3 , step4 , step5 , step6 , step7 ] outpus = [class1, class2, class3, class4, class5, class6, class7] Make sure you mask your loss function as well, as I'm not sure Keras will do the masking job correctly there.
