[site]: crossvalidated
[post_id]: 314214
[parent_id]: 
[tags]: 
KL divergence term in VAEs

I was reading the textvae paper (A Hybrid Convolutional Variational Autoencoder for Text Generation). There are some things in the paper that are counter-intuitive to me. First, "In most cases the model converges to a solution with a vanishingly small KL term, thus effectively falling back to a conventional language model." Which means a model converging to a solution with small KL learns bad latent vectors and essentially is a pure autoencoder. My doubt is, shouldn't small KL term means the model learns good latent vectors, since a solution with small KL term means the latent space is close to the guassian which we want, while keeping the reconstruction error small. Second dobut is, "the KL term value indicates how much information the network stores in the latent vector" Why?
