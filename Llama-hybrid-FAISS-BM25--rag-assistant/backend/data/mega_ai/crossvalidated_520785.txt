[site]: crossvalidated
[post_id]: 520785
[parent_id]: 520667
[tags]: 
If you think about the gradient-boosted CART (aka an atom of gbm), the model is "boxes" (1) . This (data): Is represented by a (CART fit) as this: Each leaf-tip is a mean. Each split is perpendicular to an axis. It is trying to adjust the box bounds and the mean height to minimize error in representation. From the above, you see the motivation for oblique forests . If you have a deeper tree, then there are many splits and means per level of boosting. We know there is weakness in an individual tree, it is literally a "weak learner". If you have a stump, then every line is boosted as it is made. The "boostings" to "split-and-mean" ratio is at its upper extreme value. If you have a deep tree, or even one whose depth is limited by leaf-count or other control metrics, then that ratio is arguably at its opposite extreme. $$ \text {root to leaf ratio} = \frac {\text {number of tree roots}}{\text{mean splits per tree}}$$ So why the stump? A wonderful young woman over at Microsoft described the gbm like a tiger-mom, where the student is force to study over and over. For a student that is poor or content that is very difficult, the learning rate is lower and the iterations are higher. This makes a model that is larger, takes more to train, and takes more to execute. If the student is good then they can move faster through the content, and they can take bigger steps; similarly if the content is easier then progress is easier to make. In that case a deeper tree, with a larger learning rate, and with a smaller ensemble-count is possible. It makes a smaller file, with fewer parameters, that trains quickly and runs quickly. All of these are elements of the "art" where the practitioner must make decisions using some limited design of experiments. Exhaustive grid searching can be expensive (in terms of time and money) here. I like the " philosophy " section: RF is an example of a tool that is useful in doing analyses of scientific data. But the cleverest algorithms are no substitute for human intelligence and knowledge of the data in the problem. Take the output of random forests not as absolute truth, but as smart computer generated guesses that may be helpful in leading to a deeper understanding of the problem.
