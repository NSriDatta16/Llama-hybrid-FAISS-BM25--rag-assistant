[site]: crossvalidated
[post_id]: 156940
[parent_id]: 
[tags]: 
What does the notation $t_{nk}$ mean for neural networks in Bishop's Pattern Recognition book?

I was reading Bishop's Pattern Recognition book, specifically, I was reading his notation for expressing the error just before back propagation. The particular equation I am a little confused about is the following: In that section he discussed how there is an error measure for each data point specifically. I think that is what the subscript means for $E_n$. In that same section he clearly says what $y_{nk}$ means: $$ y_{nk} = y_{k}(\bf{x}_n , \bf{w} )$$ With that information I tried to deduce/infer what $t_{nk}$ meant. I believe he usually uses $t$ to mean the target value, so I think thats the supervised label we are trying to learn. However, what confuses me is why it requires two subscripts, both an $n$ and $k$. I would assume that $n$ refers to the data point we want to learn from, but $k$ is a bit unclear to me. Also, something that bugs me about this notation that seems unclear is that every output unit $y_k$ have the same $w$, but that doesn't seem correct to me at all for a neural network. Shouldn't each output be a combination of the inputs? As in: $$ y_{nk} = y_{k}(\bf{x}_n , \bf{w}_k )$$ where $\bf{w}_k$ is for each output? He seems to fix this in equation 5.45: so he did mean to give a subscript to each weight, right? As in $ y_{nk} = y_{k}(\bf{x}_n , \bf{w} )$? Does it mean that the data set that we are trying to learn is $x_n \in \mathbb{R}^{d_x}$ $t_n, \in \mathbb{R}^{d_t}$ and the output of the network is a vector rather than a single number? I am not sure what I am confused about but I am guessing thats it. Also, as a reference and add context I will add a bit more form the exact section that I am reading:
