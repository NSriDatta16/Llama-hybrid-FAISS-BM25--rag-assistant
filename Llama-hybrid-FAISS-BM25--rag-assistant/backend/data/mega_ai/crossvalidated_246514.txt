[site]: crossvalidated
[post_id]: 246514
[parent_id]: 246504
[tags]: 
Perhaps you're unfamiliar with the Bayesian method. In the Bayesian view the model doesn't make predictions about the evidence, instead the evidence (data) is certain and it leads us to make predictions about the model. This is like flipping a biased coin, if you flipped a coin 5 times and got 5 heads you would doubt that it's a fair coin but you wouldn't jump to the conclusion that it lands on head 100% of the time. If you flipped it 100 times and got 100 heads then you would think it's biased but you don't have proof that the chance of heads is $1$, it could be $0.999$ as in your example. You would have to flip an infinite number of coins and never get tails to be 100% sure that a tails cannot occur in the future. You only have two pieces of data, if we could only make inference from two pieces of data we wouldn't be able to conclude much (like only flipping a coin twice). But in the naive bayes training method the algorithm "observes" these data again and again like flipping a coin repeatedly and observing the same results. This allows its predictions to converge. In your case, once the probability reached 0.999000999 it decided that the algorithm had converged sufficiently so it stopped learning. If it was allowed to continue it would get closer and closer to a probability of 1.
