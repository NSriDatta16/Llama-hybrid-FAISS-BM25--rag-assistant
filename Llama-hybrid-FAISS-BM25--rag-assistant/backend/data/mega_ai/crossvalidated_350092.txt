[site]: crossvalidated
[post_id]: 350092
[parent_id]: 350084
[tags]: 
The direct problem is that your data matrix has a ridiculously high condition number, and we're probably not even finding the solution to the least squares problem. Up to numerical precision, the data matrix is rank deficient, you're not successfully solving the least squares problems, and the properties of a least squares solution don't apply. As @Josef in the comment suggests, you have an ill-posed problem because of massively unequal scaling. Standardizing the variables improves matters quite a lot. Even with better scaling, it's quite problematic to run a kitchen sink regression with neither clear motivation for the right hand side variables nor regularlization to reduce overfitting. I'd have serious concerns as to model performance on new, independent test data. Discussion Let $X$ be an $n$ by $k$ matrix representing your data. A classic assumption to run OLS is no collinearity, that $X$ is rank $k$. If your data lies in a $s What does this mean once we move to computers and actual, numerical computing? A quick diagnostic is to examine the condition number . Wolfram's Mathworld says The base-b logarithm of C is an estimate of how many base-b digits are lost in solving a linear system with that matrix. In other words, it estimates worst-case loss of precision. With X2 = np.matrix([np.log(DF.X + 1), DF.X, np.log(DF.X), np.exp(-DF.X), np.sqrt(DF.X), np.power(DF.X, 4), np.power(DF.X, -1), np.power(DF.X, -2), np.power(DF.X, 2), np.power(DF.X, 3), np.exp(DF.X), np.power(DF.X, -3)]) X2_condition_number = np.linalg.cond(X2) You find that the condition number is about 7.7e+15, which means you can lose 15-16 digits of precision when you solve a linear system with X2. Double precision floating point only has 15-17 digits of precision! The first digit of the solution returned by numpy could be wrong. The solution isn't to use more advanced numerical techniques, it's that numpy is being asked to solve a ridiculous, collinear least squares problem. Such systems are ridiculously sensitive to small changes. Example of a system with a ridiculous condition number The matrix on the left has a condition number of 2e+15. $$ \begin{bmatrix} 1 & 10000000.0 \\ 1 & 10000000.1 \end{bmatrix} \begin{bmatrix}b_1 \\ b_2 \end{bmatrix} = \begin{bmatrix} 2 \\ 1\end{bmatrix}$$ The first column and the 2nd column are almost collinear. Ways forward Standardize variables. As @Josef described in the comments, you can have ill-conditioned problems because of widely unequal scaling. X3 = (X2 - X2.mean(axis=1)) / X2.std(axis=1) X3 now has a condition number of 2.9e+3. Another common scaling in machine learning is max-min scaling. Other ways forward include: Drop regressors that are collinear with existing regressors (i.e. stop doing a kitchen sink regression). Add regularlization , eg. do ridge regression or LASSO. If you're trying to fit a flexible curve, there are better things to do such as low order polynomials with splines .
