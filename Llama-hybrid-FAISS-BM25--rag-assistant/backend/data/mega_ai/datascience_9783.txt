[site]: datascience
[post_id]: 9783
[parent_id]: 
[tags]: 
Using machine learning specifically for feature analysis, not predictions

I'm new to machine learning and have spent the last couple months having a blast using Sci-Kit Learn to try to understand the basics of building feature sets and predictive models. Now I'm trying to use ML on a data set not to predict future values but to understand the importance and direction (positive or negative) of each feature. My features (X) are boolean and integer values that describe a product. My target (y) is the sales of the product. I have ~15,000 observations with 16 features a piece. With my limited ML knowledge to this point, I'm confident that I can predict (with some level of accuracy) a new y based on a new set of features X. However I'm struggling to coherently identify, report on and present the importance and direction of each feature that makes up X . Thus far, I've taken a two-step approach: Use a linear regression to observe coefficients Use a random forest to observe feature importance The code First, I try to get the directional impact of each feature: from sklearn import linear_model linreg = linear_model.LinearRegression() linreg.fit(X, y) coef = linreg.coef_ ... Second, I try to get the importance of each feature: from sklearn import ensemble forest = ensemble.RandomForestRegressor() forest.fit(X, y) importance = forest.feature_importances_ ... Then I multiply the two derived values together for each feature and end up with some value that maybe perhaps could be the information I'm looking for! I'd love to know if I'm on the right track with any of this. Is this a common use case for ML? Are there tools, ideas, packages I should focus on to help guide me? Thank you very much.
