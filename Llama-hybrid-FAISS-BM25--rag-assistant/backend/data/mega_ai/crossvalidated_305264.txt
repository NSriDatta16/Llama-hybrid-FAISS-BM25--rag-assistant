[site]: crossvalidated
[post_id]: 305264
[parent_id]: 285931
[tags]: 
This is a riff on the first answer from Djib2011. The short answer has to be no. Longer - Firstly photos are always encoded as a tensor as follows. An image is a number of pixels. If the photo is considered to have m rows and n columns, each pixel is specified by it's row and column location, that is by the pair (m,n). In particular there are m*n pixels which is very large even for 'small' photos. Each pixel of the photo is encoded by a number between zero and one (blackness intensity) if the photo is black and white. It is encoded by three numbers (RGB intensities) if the photo is color. So one winds up with a tensor that is either a 1xmxn or a 3xmxn. Image recognition is done through CNN's which, taking advantage of the fact that photos don't change that much from pixel to pixel, compress the data via filters and pooling. So the point is that CNN's work by compressing the incredibly large numbers of data points (or features) of a photo into a smaller number of values. So whatever format you start with, CNN's start off by further compressing the data of the photo. Hence the per se independence from the size of the representation of the photo. However, a CNN will demand that all images being run through it are all of the same size. So there is that dependency that will change depending on how the image is saved. In addition, to the extent that different file formats of the same size produce different values for their tensors, one cannot use the same CNN model to identify photos stored by different methods.
