[site]: crossvalidated
[post_id]: 635180
[parent_id]: 
[tags]: 
Neural ODE and Adjoint method

I am trying to understand the paper NeuralODE which is very interesting. I get the general idea and the proof they give about the dynamics of the adjoint are fairly simple. They define a network $f$ , parametrized by $\theta$ : $$ \frac{dz(t)}{dt} = f(z(t), t, \theta) $$ Of course, to train the network, it is important to compute the gradient of the loss with respect to the parameters: $$ \frac{\partial L}{\partial \theta} $$ However, I do not understand why it is needed to compute the gradient of the loss with respect to $z(t)$ and to $t$ also: $$ \frac{\partial L}{\partial z(t)}, \frac{\partial L}{\partial t} $$ I would say once you have the first gradient, it is possible to update the weights of the neural network and train it right? Is this needed only because torch.autograd.Function (that they are using in original implementation) needs it to build the computation graph correctly? Or is there any other reason behind it? Thank you for you help!
