[site]: crossvalidated
[post_id]: 458623
[parent_id]: 458579
[tags]: 
The mention of Tensorflow in the question is a red herring -- the reasons that scaling is beneficial are not unique to Tensorflow, but instead are common to all methods that update model parameters using gradient descent. The reason we scale data is to improve gradient descent dynamics, and backpropagation using Tensorflow is just one application of gradient descent among many. Optimization via gradient descent proceeds more easily when the optimization surface is more "circular" and less "elliptical." The optimization proceeds more quickly in directions corresponding to the largest eigenvalue and more slowly in directions corresponding to the smallest eigenvalue. In other words, the optimization procedure is easier when the eigenvalues of the Hessian are on the same scale. None of this depends on the input data conforming to any particular parametric distribution; rescaling the inputs to have a common variance has the effect of preconditioning the Hessian matrix. More information: In Machine learning, how does normalization help in convergence of gradient descent? In general, real-valued inputs can be rescaled. There are some corner cases where rescaling inputs doesn't make any sense. For example, embeddings use a lookup table to transform integer-coded inputs to a specific vector. Rescaling this to have 0 mean and variance 1 (or vary between 0 and 1) is meaningless and not helpful, because it breaks the lookup table property.
