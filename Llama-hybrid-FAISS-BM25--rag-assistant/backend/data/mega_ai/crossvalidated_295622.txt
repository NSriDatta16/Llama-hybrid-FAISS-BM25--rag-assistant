[site]: crossvalidated
[post_id]: 295622
[parent_id]: 270310
[tags]: 
I think you got it quite right but not exactly. Here my suggestion: Separate the dataset into a test and a train+validation set. Perform a grid search using cross validation on the train set to find optimal hyperparameters optimized on the validation set (for random forest, this would be defining your mtry). Use the entire train+validation with the optimal hyperparameters and report the error using the test set. Only this way, you ensure that the performance is measured on a part of the data the model has never seen. I recommend splitting Nr. 1 a few times, for example with the 10-fold CV, to make the performance measure less prone to variance. The mlr package has good explanations on this nested cross-validation https://mlr-org.github.io/mlr-tutorial/devel/html/nested_resampling/index.html
