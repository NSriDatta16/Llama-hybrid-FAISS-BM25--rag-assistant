[site]: crossvalidated
[post_id]: 276618
[parent_id]: 276497
[tags]: 
It might help to give slightly more of an overview of MMD. $\DeclareMathOperator{\E}{\mathbb E}\newcommand{\R}{\mathbb R}\newcommand{\X}{\mathcal X}\newcommand{\h}{\mathcal H}\newcommand{\F}{\mathcal F}\DeclareMathOperator{\MMD}{MMD}$ In general, MMD is defined by the idea of representing distances between distributions as distances between mean embeddings of features. That is, say we have distributions $P$ and $Q$ over a set $\X$ . The MMD is defined based on a feature map $\varphi : \X \to \h$ , where $\h$ is some Hilbert space; this corresponds to a kernel (as in SVMs, not KDE) by $k(x, y) = \langle \varphi(x), \varphi(y) \rangle_\h$ . In general, the MMD is $$ \MMD(P, Q) = \big\lVert \E_{X \sim P}[ \varphi(X) ] - \E_{Y \sim Q}[ \varphi(Y) ] \big\rVert_\h .$$ As one example, we might have $\X = \h = \R^d$ and $\varphi(x) = x$ , corresponding to a linear kernel. In that case: \begin{align} \MMD(P, Q) &= \bigl\lVert \E_{X \sim P}[ \varphi(X) ] - \E_{Y \sim Q}[ \varphi(Y) ] \bigr\rVert_\h \\&= \bigl\lVert \E_{X \sim P}[ X ] - \E_{Y \sim Q}[ Y ] \bigr\rVert_{\R^d} \\&= \bigl\lVert \mu_P - \mu_Q \bigr\rVert_{\R^d} ,\end{align} so this MMD is just the distance between the means of the two distributions. Matching distributions like this will match their means, though they might differ in their variance or in other ways. Your case is slightly different: we have $\mathcal X = \mathbb R^d$ and $\mathcal H = \mathbb R^p$ , with $\varphi(x) = A' x$ , where $A$ is a $d \times p$ matrix. So we have \begin{align} \MMD(P, Q) &= \bigl\lVert \E_{X \sim P}[ \varphi(X) ] - \E_{Y \sim Q}[ \varphi(Y) ] \bigr\rVert_\h \\&= \bigl\lVert \E_{X \sim P}[ A' X ] - \E_{Y \sim Q}[ A' Y ] \bigr\rVert_{\R^p} \\&= \bigl\lVert A' \E_{X \sim P}[ X ] - A' \E_{Y \sim Q}[ Y ] \bigr\rVert_{\R^p} \\&= \bigl\lVert A'( \mu_P - \mu_Q ) \bigr\rVert_{\R^p} .\end{align} This MMD is the difference between two different projections of the mean. If $p or the mapping $A'$ otherwise isn't invertible, then this MMD is weaker than the previous one: it doesn't distinguish between some distributions that the previous one does. You can also construct stronger distances. For example, if $\X = \R$ and you use $\varphi(x) = (x, x^2)$ (giving a particular quadratic kernel), then the MMD becomes $\sqrt{(\E X - \E Y)^2 + (\E X^2 - \E Y^2)^2}$ , and can distinguish not only distributions with different means but with different variances as well. And you can get much stronger than that: for general choices of kernel, you can use the kernel trick to compute the MMD: \begin{align} \MMD^2(P, Q) &= \bigl\lVert \E_{X \sim P} \varphi(X) - \E_{Y \sim Q} \varphi(Y) \bigr\rVert_\h^2 \\&= \langle \E_{X \sim P} \varphi(X), \E_{X' \sim P} \varphi(X') \rangle_\h + \langle \E_{Y \sim Q} \varphi(Y), \E_{Y' \sim Q} \varphi(Y') \rangle_\h - 2 \langle \E_{X \sim P} \varphi(X), \E_{Y \sim Q} \varphi(Y) \rangle_\h \\&= \E_{X, X' \sim P} k(X, X') + \E_{Y, Y' \sim Q} k(Y, Y') - 2 \E_{X \sim P, Y \sim Q} k(X, Y) .\end{align} It's then straightforward to estimate this with samples, for any kernel function $k$ -- even ones where $\varphi$ is infinite-dimensional , like the Gaussian kernel (also called "squared exponential" or "exponentiated quadratic") $k(x, y) = \exp\left( -\frac{1}{2\sigma^2} \lVert x - y \rVert^2 \right)$ . If your choice of $k$ is "characteristic," then the MMD becomes a proper metric on distributions: it's zero if and only if the two distributions are the same. (This is unlike when you use, say, a linear kernel, where two distributions with the same mean have zero linear-kernel MMD.) If you've heard of a "universal" kernel, those are characteristic, but there are a few kernels that are characteristic but not universal. Here's an explanation of the name, which is also useful for understanding the MMD. For any kernel $k : \X \times \X \to \R$ , there exists a feature map $\varphi : \X \to \mathcal F$ , where $\mathcal F$ is a special Hilbert space called the reproducing kernel Hilbert space (RKHS) corresponding to $k$ . This is a space of functions , $f : \X \to \R$ . These spaces satisfy a special key condition, called the reproducing property : $\langle f, \varphi(x) \rangle_\F = f(x)$ for any $f \in \F$ . The simplest example is the linear kernel $k(x, y) = x \cdot y$ . This can be "implemented" with $\h = \R^d$ and $\varphi(x) = x$ . But the RKHS is instead the space of linear functions $f_x(t) = x \cdot t$ , and $\varphi(x) = f_x$ . The reproducing property is $\langle f_w, \varphi(x) \rangle_\h = \langle w, x \rangle_{\R^d}$ . In more complex settings, like a Gaussian kernel, $f$ is a much more complicated function, but the reproducing property still holds. Now, we can give an alternative characterization of the MMD: \begin{align} \MMD(P, Q) &= \lVert \E_{X \sim P}[\varphi(X)] - \E_{Y \sim Q}[\varphi(Y)] \rVert_\h \\&= \sup_{f \in \h : \lVert f \rVert_\h \le 1} \langle f, \E_{X \sim P}[\varphi(X)] - \E_{Y \sim Q}[\varphi(Y)] \rangle_\h \\&= \sup_{f \in \h : \lVert f \rVert_\h \le 1} \langle f, \E_{X \sim P}[\varphi(X)] \rangle_\h - \langle f, \E_{Y \sim Q}[\varphi(Y)] \rangle_\h \\&= \sup_{f \in \h : \lVert f \rVert_\h \le 1} \E_{X \sim P}[\langle f, \varphi(X)\rangle_\h] - \E_{Y \sim Q}[\langle f, \varphi(Y) \rangle_\h] \\&= \sup_{f \in \h : \lVert f \rVert_\h \le 1} \E_{X \sim P}[f(X)] - \E_{Y \sim Q}[f(Y)] .\end{align} The second line is a general fact about norms in Hilbert spaces that follows immediately from Cauchy-Schwarz: $\sup_{f : \lVert f \rVert \le 1} \langle f, g \rangle_\h = \lVert g \rVert$ is achieved by $f = g / \lVert g \rVert$ . The fourth line depends on a technical condition known as Bochner integrability, but is true e.g. for bounded kernels or distributions with bounded support. Then, at the end we use the reproducing property. This last line is why it's called the "maximum mean discrepancy" â€“ it's the maximum, over test functions $f$ in the unit ball of $\h$ , of the mean difference between the two distributions. This is also a special case of an integral probability metric .
