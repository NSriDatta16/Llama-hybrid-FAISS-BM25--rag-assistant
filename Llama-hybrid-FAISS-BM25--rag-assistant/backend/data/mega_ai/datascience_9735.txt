[site]: datascience
[post_id]: 9735
[parent_id]: 
[tags]: 
what is the difference between "fully developed decision trees" and "shallow decision trees"?

As reading Ensemble methods on scikit-learn docs, it says that bagging methods work best with strong and complex models (e.g., fully developed decision trees), in contrast with boosting methods which usually work best with weak models (e.g., shallow decision trees). But search on google it always return information about Decision Tree . I'd like to know the detail of the two trees mentioned in the doc, what's the fully developed and shallow meanings. Update: About why bagging work best with fully developed and why boosting work best with shallow . First, I think complex models (e.g., fully developed decision trees) means such a data set has a complex format be called as fully developed decision trees . After I read above quote over 20 times and rapaio's answer, I think my poor English lead me to the wrong road(misunderstand). I also mistake shallow as shadow , which make me confusing a long time....Now I understand the meanings of fully developed and shallow . I think the quote is saying bagging work best with a models(already trained) which algorithm is complex. And boosting only need simple model. Both bagging and boosting need many estimators , as n_estimators=100 in scikit-learn examples. If n_estimators=100 : bagging need 100 fully developed decision trees estimators(models) boosting need 100 shallow decision trees estimators(models) Does my thoughts is right? Hope my update can help non-native speakers. e.g. means for example, so there are other models can use for bagging and boosting. How about changing the model to svm or something else? Or both of they need a tree base model?
