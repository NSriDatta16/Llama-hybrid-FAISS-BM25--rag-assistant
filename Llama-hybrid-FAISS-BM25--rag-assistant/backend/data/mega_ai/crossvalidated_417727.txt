[site]: crossvalidated
[post_id]: 417727
[parent_id]: 417725
[tags]: 
Why use GEE (instead of Maximum Likelihood, ML)? Because the likelihood could be wrong. In fact, since we never know if it's right or not, GEEs safeguard against biases. We want results that require few assumptions. GEE's assumptions are some of the most general, hence the Generalized "G" of GEE. ... the problem with GEEs is I can't calculate the likelihood and therefore all models I make with it aren't comparable and I don't know which model is right. You can absolutely compare nested models with the robust score or Wald tests. These results usually mostly agree with ML. Also, you never know if a model is right. Tests don't tell you that. For all I know what I am fitting with GEEs could be the noise rather than the true value. True of ML as well. If models are specified differently the entire conclusions can be different. True of ML as well. Someone told me that GEEs are interpreted in the case of drug trials as if I were to give the entire population (all the people) many of whom are not sick, the drug, what kind of effect we would see. This is a misunderstanding of what an "average treatment effect" is. In fact, GEE and ML agree on point estimates for independent data. Only with a repeated measures design is there a difference between individual-level and population-averaged effects. You need some critical imbalance in the design to get those different estimates. In that case, GEE does not actually estimate the desired quantity. The main strength of GEE from a basic modeling perspective is that: The probability model for the response is a "working model" The data do not have to be independent or identically distributed to obtain valid inference. The mean model does not have to be correctly specified.
