[site]: crossvalidated
[post_id]: 108986
[parent_id]: 108982
[tags]: 
Basically, yes.* That formula will enter both main effects and the product term as predictors in your model. So will your first version BTW: compare lm(y~x1*x2);lm(y~x1+x2+x1*x2) . For example, using set.seed(8);x1=rnorm(99);x2=rnorm(99);y=rnorm(99) , either way the equation is: $$\hat Y=-.16-.02x_1-.08x_2+.09(x_1\times x_2)$$ If for some (probably improper) reason you wanted only the interaction term, you'd use x1:x2 – note this is how the interaction term is labeled in the output, not as x1*x2 . * You might want to scale your predictors to remove nonessential multicollinearity if you're interested in the standard errors of your regression coefficents or other associated statistics (including $t$s and $p$s for your predictors). This is unnecessary for the interaction term though; it only changes standard errors for the main effects. If you use y~scale(x1,T,F)*scale(x2,T,F) , this will mean-center x1 and x2 but not divide by the standard deviation, thus preserving your units of measurement. If you use y~scale(x1)*scale(x2) , this will standardize x1 and x2 to the scale of $Z$ . Either works for controlling nonessential multicollinearity, but neither removes essential multicollinearity. For more on that, see the following reference: Dalal, D. K., & Zickar, M. J. (2012). Some common myths about centering predictor variables in moderated multiple regression and polynomial regression. Organizational Research Methods, 15 (3), 339–362. Retrieved from https://umdrive.memphis.edu/dsherrll/public/SCMS8540/Dalal%20%26%20Zickar-2012.pdf .
