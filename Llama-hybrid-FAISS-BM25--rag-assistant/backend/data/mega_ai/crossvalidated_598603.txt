[site]: crossvalidated
[post_id]: 598603
[parent_id]: 
[tags]: 
How to evaluate Natural Question-Answer Generation pairs?

I am trying to generate Natural Question-Answer for a specific domain. I am using a Large Language Model (LLM). I have only context to generate question-answers but don't have any ground truth. How to measure the accuracy or how good the generation is? I am repeating the experiment 2-3 times, How to compare which question-answers pairs are good? Because each time the generated question-answers are different. For example,e : Context : """This section describes our proposed method. The detailed setup for our experiments is described in Sections 4.1 and 4.2.""" Iteration1 (generated question-answer) Q: This section describes what? A: This section describes the paper's proposed model. Iteration2 (generated question-answer) Q: Which section describes the detailed experiments? A: Sections 4.1 and 4.2 describes the detailed setup and experiments. Iteration2 (generated question-answer) Q: Sections 4.1 and 4.2 describes what? A: Detailed setup and experiments are described in Sections 4.1 and 4.2 Now I want to measure how good this model is in generating questions and answers based on the given paragraph. What matrices I can use? Please guide me on this, and if possible share the papers too.
