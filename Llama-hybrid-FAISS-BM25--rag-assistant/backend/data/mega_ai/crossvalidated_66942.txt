[site]: crossvalidated
[post_id]: 66942
[parent_id]: 65292
[tags]: 
I'm not an expert in neural nets but I think the following points might be helpful to you. There are also some nice posts, e.g. this one on hidden units , that you can search for on this site about what neural nets do that you might find useful. 1 Large errors: why didn't your example work at all why errors are so big and why all predicted values are almost constant? This is because the neural network was unable to compute the multiplication function you gave it and outputting a constant number in the middle of the range of y , regardless of x , was the best way to minimize errors during training. (Notice how 58749 is pretty close to the mean of multiplying two numbers between 1 and 500 together.) It's very hard to see how a neural network could compute a multiplication function in a sensible way. Think about how each node in the network combines previously computed results: you take a weighted sum of the outputs from previous nodes (and then apply a sigmoidal function to it, see, e.g. an Introduction to Neural Networks , to scrunch the output inbetween $-1$ and $1$). How are you going to get a weighted sum to give you multiplication of two inputs? (I suppose, however, that it might be possible to take a large number of hidden layers to get multiplication working in a very contrived way.) 2 Local minima: why a theoretically reasonable example might not work However, even trying to do addition you run into problems in your example: the network doesn't train successfully. I believe that this is because of a second problem: getting local minima during the training. In fact, for addition, using two layers of 5 hidden units is much too complicated to compute addition. A network with no hidden units trains perfectly well: x Of course, you could transform your original problem into an addition problem by taking logs, but I don't think this is what you want, so onwards... 3 Number of training examples compared to number of parameters to estimate So what would be a reasonable way to test your neural net with two layers of 5 hidden units as you originally had? Neural nets are often used for classification, so deciding whether $\mathbf{x}\cdot\mathbf{k} > c$ seemed a reasonable choice of problem. I used $\mathbf{k} = (1, 2, 3, 4, 5)$ and $c = 3750$. Notice that there are several parameters to be learnt. In the code below I take a very similar approach to yours except that I train two neural nets, one with 50 examples from the training set, and one with 500. library(neuralnet) set.seed(1) # make results reproducible N=500 x 3750, 1, 0) trainSMALL It is apparent that the netALL does a lot better! Why is this? Take a look at what you get with a plot(netALL) command: I make it 66 parameters that are estimated during training (5 inputs and 1 bias input to each of 11 nodes). You can't reliably estimate 66 parameters with 50 training examples. I suspect in this case you might be able to cut down on the number of parameters to estimate by cutting down on the number of units. And you can see from constructing a neural network to do addition that a simpler neural network may be less likely to run into problems during training. But as a general rule in any machine learning (including linear regression) you want to have a lot more training examples than parameters to estimate.
