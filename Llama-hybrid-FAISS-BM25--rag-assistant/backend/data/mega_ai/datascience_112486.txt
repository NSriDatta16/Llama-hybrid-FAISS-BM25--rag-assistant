[site]: datascience
[post_id]: 112486
[parent_id]: 112484
[tags]: 
In general, the software is probably expecting an $N$ examples (or records) by $M$ (features) array as the input matrix $X$ , plus a 1D array of length $N$ as the expected output labels (with 12 unique values, corresponding to the 12 classes). So if you're saying that your entire dataset is 700 × 2000, just use it as-is. On the other hand, if you're saying that one single example of your data is 700 × 2000, then you would need to flatten each example into a 1,400,000-feature vector... but this is a very high-dimensional vector, and although SVMs are good at high-dimensional data, it can lead to problems (see the curse of dimensionality ). So you could either reduce the size of the original examples, or take some steps to otherwise reduce the dimensionality of your data.
