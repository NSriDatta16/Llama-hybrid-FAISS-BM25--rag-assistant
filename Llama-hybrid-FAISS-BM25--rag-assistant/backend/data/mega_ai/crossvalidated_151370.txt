[site]: crossvalidated
[post_id]: 151370
[parent_id]: 113301
[tags]: 
As you rightly pointed out, a pure classifier (with probability 1) will have log loss of 0, which is the preferred case. Consider a classifier that assigns labels in a completely random manner. Probability of assigning to the correct class will be 1/M . Therefore, the log loss for each observation will be -log(1/M) = log(M) . This is label independent. Log loss for an individual observation can be compared with this value to check how well the classifier is performing with respect to random classification. However, this may not make much sense. Let us take an example. Consider a powerful classifier which misclassified an observation. Let us assume that the observation actually belongs to class 'x' and the predicted probability of belonging to class is 0 (nearly). Therefore, the individual and overall value of log loss will be Inf. This is very common and mostly ignored - it is an observation, but it does not comment on the overall accuracy of the classifier. However, we can make sense of this in 2 ways: Method 1: The observation could be an outlier. Remove it and run the classification again Method 2: Smooth the probability density function for class belongingness of all observations (not just the current observation) Note: If you are concerned with the predicted probability of class belongingness and not just the predicted class, I strongly recommend you to look at method 2. It is generally studied in text retrieval (Language model); it may be relevant to your case. Addition: e^(-loss) is the average probability of correct prediction. This value can be compared to that of random classification.
