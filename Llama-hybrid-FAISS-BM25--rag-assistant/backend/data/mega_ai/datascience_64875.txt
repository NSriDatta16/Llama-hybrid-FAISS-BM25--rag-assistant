[site]: datascience
[post_id]: 64875
[parent_id]: 
[tags]: 
early-stopping changes final epoch in training each time

I am training a CNN built using transfer learning with a VGG16 network as pre-trained model, and in the training I am using early-stopping as regularization technique. I have run several time the training, and what I see is that early stopping stops the training process everytime at a different epoch, so for example if I train the CNN the first time I arrive until the epoch 17, then another day I train again the network and during training it arrives at the epoch 8, and the epoch at which arrives it is different everytime. Is this normal? I don't understand why everytime it changes the epoch at which it arrives. Can someone explain this to me? Thanks in advance. [EDIT] My code for the training is the following: from keras import callbacks # fit the transferNet on the training data stopping = callbacks.EarlyStopping(monitor='val_acc', patience=3) steps_per_epoch = train_generator.n//train_generator.batch_size val_steps = test_generator.n//test_generator.batch_size+1 try: history_transfer = transfer_model.fit_generator(train_generator, epochs=100, verbose=1, callbacks=[stopping],\ steps_per_epoch=steps_per_epoch,\ validation_data=test_generator,\ validation_steps=val_steps) except KeyboardInterrupt: pass
