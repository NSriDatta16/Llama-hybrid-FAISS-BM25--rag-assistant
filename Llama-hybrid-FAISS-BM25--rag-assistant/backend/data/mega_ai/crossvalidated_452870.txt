[site]: crossvalidated
[post_id]: 452870
[parent_id]: 452260
[tags]: 
I'm not sure this is a big deal. For one thing, the agent isn't maximizing one-step reward but future return (sum of future reward). So in that sense, the agent in your example will attribute a higher weight to the earlier decision of moving right a lot. If you consider the state-action-reward sequence $\{s_1, \text{right}, 1, s_2, \text{right}, 1, s_3, \text{left}, 1, s_4, \text{right}, 0, s_\text{terminal}\}$ and (for simplicity) the REINFORCE objective then we have that $$\nabla_\theta J(\theta) = 3 \cdot \nabla_\theta \log \pi_\theta (\text{right} | s_1) + 2 \cdot \nabla_\theta \log \pi_\theta (\text{right} | s_2) + 1 \cdot \nabla_\theta \log \pi_\theta (\text{left} | s_3) + 0 \cdot \nabla_\theta \log \pi_\theta (\text{right} | s_\text{terminal}).$$ Now imagine if the last action was left, ending in termination. Then that term would be multiplied by $0$ and rather inconsequential. Likewise, if it was able to recover and extend the episode by another 10 time steps, the return would be higher and that sequence of actions would be good overall. In this set-up, I do not see the point of delaying reward since the return (or value) is what is being optimized. For something more complicated like DQN, it is still true that return is being optimized - not immediate reward. Furthermore, it's not correct to say that the agent will memorize certain actions in certain states. That is only true in the tabular setting. This is also why we require that the policy is suitably exploratory. That way, the agent won't get "stuck" in this behavior loop.
