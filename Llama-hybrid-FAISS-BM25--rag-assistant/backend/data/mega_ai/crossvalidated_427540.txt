[site]: crossvalidated
[post_id]: 427540
[parent_id]: 411456
[tags]: 
If you assume some gaussian prior on weights, and are willing to accept some small quantization error, then you can do the following: Quantize your weights/prior to some very fine tolerance Encode them with the optimal code corresponding to your prior. This should take $-\log P(\theta)$ bits where $P$ is your prior and $\theta$ are your parameters Of course negative log density of the standard gaussian is proportional to $||\theta||_2^2$ , so the number of bits it takes to express your model is proportional to the L2 regularization term . Regarding the quantization part: while this may at first seem like an ugly workaround to the impossibility of encoding real numbers in a finite amount of bits, you can take a more bayesian approach and say that you have a posterior distribution over the model weights. In that case, a wider posterior corresponds to a lower coding cost and a narrower posterior corresponds to a higher cost. Hinton and Camp show via the "bits back" argument that you can actually quantize as finely as you want in this case without spending any bits at all (technically you still have to spend them, you just get a refund)!
