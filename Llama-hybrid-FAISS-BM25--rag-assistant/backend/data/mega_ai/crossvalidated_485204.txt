[site]: crossvalidated
[post_id]: 485204
[parent_id]: 485201
[tags]: 
The best way to picture how this works is that automatic differentiation can be implemented by augmenting your existing computational graph with a few extra nodes / edges (which connect to your "forward comptuation") and compute the gradients. There's nothing which distinguishes these new nodes from your original forward computation (the graph itself is not aware of notions of "forward" versus "backward" pass), so it's completely possible to apply AD again, to get higher order derivatives. Generally speaking, the backward "augmentation" of the graph is usually abstracted away from the end-user, but most machine learning frameworks such as tensorflow and pytorch do expose the relevant utilities, for example tf.gradients and torch.autograd.grad .
