[site]: crossvalidated
[post_id]: 539113
[parent_id]: 538743
[tags]: 
You should let your agent know in some way which environment it's currently playing. For example, pass in an additional one-hot vector e.g. [0,0,1] when it's playing the "FB" environment, [0,1,0] when it's playing "TSLA", etc. Furthermore, it's good practice to store your rollouts in a replay buffer, and instead of training on one game at a time, to sample batches of (state, action, reward, next-state) tuples from the replay buffer to train on. This way, you can train on multiple environments simultaneously.
