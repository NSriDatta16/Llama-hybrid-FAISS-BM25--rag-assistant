[site]: crossvalidated
[post_id]: 182461
[parent_id]: 
[tags]: 
Machine Learning: Trying to understand an example of Expectation Maximization

I'm trying to understand this example in EM: I have a set of data points $x_1,...,x_n$ that are generated in the following way: First, a number $i$ in the range (0,31) is picked with a probability $p_i$. We then look at $i$'s representation and randomly (uniformly) pick 2 bits to be replaced by 2's. So for example, if $i = 31 = 11111$, I can end up with $x= 22111, x = 11122, x = 12121$ all with equal probability. What I'm trying to understand exactly how this fits the EM model. Looking at the definition on Wikipedia, I see this: obviously $x_1,...,x_n$ is our data. Our parameter $\theta$ is $p_1,...p_n$. I can't quite see what my latent variable is supposed to be. Also, I can't really see a way to construct a likelihood function from this data. I'm a little bit confused and would appreciate any help!
