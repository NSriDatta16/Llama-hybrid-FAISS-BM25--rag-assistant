[site]: crossvalidated
[post_id]: 242721
[parent_id]: 
[tags]: 
How to compute the likelihood when uncertain about underlying distribution

In a Bayesian approach, when I have no previous idea about the likelihood of a given event, can I simply gather a lot of data and use that distribution as my likelihood (1) and as I gather more and more data I keep updating that likelihood? (2) Example: Let's say I want to estimate if it is going to rain in the afternoon based on how cloudy it is in the morning. I have no idea about the likelihood. So in this case, I would gather a lot of data points (let's say when it was cloudy, 56 times it rained afterwards and 17 times it didn't rain and when it was not cloudy 5 times it rained and 89 times it didn't rain afterwards). Since it is a binary event I would map it with a Bernoulli distribution (one for when it's cloudy and one for when it's not cloudy). Now, let's say a new data point comes in and it says it is cloudy in the morning. Then I'd simply use the likelihood that it will rain $Likelihood(cloudy|rain) = \frac{56}{56+17} = 0.767$ and multiply by the prior distribution $P(H = cloudy)$ to obtain the posterior distribution. Then I would observe what happened. Let's say it indeed rained. Then I'd "update" the likelihood and now when a new "cloudy" data point comes in I would use the updated (57, 17) instead of (56, 17) to compute its likelihood. Are (1) and (2) the correct approach to tackle those kinds of problems?
