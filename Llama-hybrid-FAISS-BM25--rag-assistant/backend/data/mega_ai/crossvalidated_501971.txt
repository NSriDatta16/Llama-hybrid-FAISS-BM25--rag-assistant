[site]: crossvalidated
[post_id]: 501971
[parent_id]: 501767
[tags]: 
It sounds like you're asking two questions here: How can the decision boundary be computed / represented explicitly How can such a high-dimensional object be plotted in 2D To answer the first question for neural networks with relu activation: The first layer of a relu network (which consists of a affine transform followed by a relu activation) divides up $R^d$ with $n$ hyperplanes, $n$ is the number of units in that first layer. Within each region, it's easy to see that the output of the first layer is a linear function. Furthermore, you can compute all the vertices of each region, as they're just formed by the intersection of hyperplanes. All subsequent layers of a relu network simply subdivide each region into more and more convex polyhedrons, with the same property that within each (sub)region, the network is a linear function. The decision boundary can be written as $w^Tx \geq b$ for some $w$ and $b$ , so now you can compute for each polyhedral region, whether it intersects with the decision hyperplane, and if it does, you know have part of the decision boundary. So the full decision boundary is simply a large number of "connected" $(n-1)$ -dim regions.
