[site]: datascience
[post_id]: 68002
[parent_id]: 67989
[tags]: 
You can try with this: import pandas as pd import nltk df = pd.DataFrame({'frases': ['Do not let the day end without having grown a little,', 'without having been happy, without having increased your dreams', 'Do not let yourself be overcomed by discouragement.','We are passion-full beings.']}) df['tokenized'] = df.apply(lambda row: nltk.word_tokenize(row['frases']), axis=1)
