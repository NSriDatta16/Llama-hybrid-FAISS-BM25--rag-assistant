[site]: crossvalidated
[post_id]: 345519
[parent_id]: 345215
[tags]: 
I don't know if you should consider just a lag, like a simple constant deviation between real values and predicted values of the model AFTER a fit. Fitting data means that you obtain the better fit searching for something. For exemple, the OLS (ordinary least squares) model finds the better fit searching for the coefficients that minimizes the sum of the squared residuals (including the intercept). Your model searches for something too, and push or pull data within a period after a fit does not mean that you will predict future values with precision. If you manually adjusts the intercept, the other coefficients will lose what the modeling wanted to find. So you need to find a better model! You could search for correlation (residuals and dependent variables). And verify econometric assumptions (prediction depends on that, while fitting a model is just math with no intrinsic meaning) (1). I think this offers a direction: Applying 2SLS in a time series context When there are concerns of included endogenous variables in a model t to time series data, we have a natural source of instruments in terms of predetermined variables. For instance, if y2t is an explanatory variable, its own lagged values, y2t.1 or y2t.2, might be used as instruments: they are likely to be correlated with y2t, and they will not be correlated with the error term at time t; since they were generated at an earlier point in time. The one caveat that must be raised in this context relates to autocorrelated errors: if the errors are themselves autocorrelated, then the presumed exogeneity of predetermined variables will be in doubt. Tests for autocorrelated errors should be conducted; in the presence of autocorrelation, more distant lags might be used to mitigate this concern. [Wooldridge, Introductory Econometrics, 4th ed., Chapter 15: Instrumental variables and two stage least squares] - You can find it here . (1) For OLS: Gauss-Markov
