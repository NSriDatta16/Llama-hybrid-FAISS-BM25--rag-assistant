[site]: crossvalidated
[post_id]: 595987
[parent_id]: 595939
[tags]: 
The $d$ figure is Cohen's $d$ [1] , which is the number of population standard deviations, $\sigma$ (common to both populations), that the population means are apart ( $|\mu_2-\mu_1|$ ), that is, $d = \frac{|\mu_2-\mu_1|}{\sigma}$ (See footnote $\dagger$ ). When the units of the raw variable are not especially meaningful, this is often a sensible measure of effect size in a comparison of means. This is routine in psychology and very common in the behavioural sciences more generally [Conventionally, being a population parameter, it should be $\delta$ rather than $d$ but psychologists writing for psychologists don't bother much with statistical convention (no disparagement intended; they have other concerns). It wouldn't matter too much if it was merely convention, but this has the unfortunate side effect of leading psychologists to completely confound the distinct concepts of the parameter and a sample estimate of it and that in turn has further consequences, including pretty directly leading to a conflation of power and sample-based estimates of it -- and so the problems that relying on post hoc power can entail. But this is perhaps not the place to pursue it further.] In Cohen's discussion $d=0.8$ is taken as a 'large' effect size and $0.5$ as 'medium' (with $0.2$ as small), so $0.7$ is close to large on that scale. I don't think these conventional sizes are particularly well justified (the labels would have to be application-dependent, for one thing), but they have become a seemingly ironclad convention in psychology, to the extent that an actually externally justified effect size (were that to turn out to be possible in some circumstance) that differed from these conventions might well have trouble being accepted. This is not a particular criticism of Cohen; any attempt at a scale for variables where the units are essentially arbitrary would encounter difficulties; it's very difficult even when the units have a direct meaning. If you do follow the Cohen approach, I suggest reading what he has to say on it (including in the prefaces; the second edition that I was able to take a look at includes all the earlier prefaces and there's at least some discussion of those effect sizes in the prefaces). In chapter 1 he calls his suggestions for small , medium and large for each circumstance arbitrary , and limits his scope to behavioral and biological sciences. Each section that discusses new tests / new effect size measures (largely arranged into chapters) addresses the specific values, so beside the discussion in preface and chapter 1 you would consult the relevant chapter on the procedure (at least if it's discussed in the book at all; the t-test is in chapter 2). I do think the justification for calling that $0.8$ value large outside of the areas it was intended for are perhaps more likely to be dubious. As far as the input to the R function goes, $d$ is the (scaled) effect size you want to attain the specified power at. With an unbiased, consistent test (such as the t-test under fairly broad assumptions), the power will increase as you move away from $H_0$ through increasing effect sizes, forming a power curve / power function . Here we're looking at the way power changes with effect size at a fixed sample size: (image taken from my answer here ; in this diagram the effect size is signed, $(\mu_2-\mu_1)/\sigma$ and the power value indicated is the pure rejection rate, so here the rejection rate as you approach effect size $0$ is actually $\alpha$ . The power values for the plot were generated by supplying a vector of $\delta$ values to the in-built R function power.t.test .) Since - as we can see - the value of power depends on the effect size and the sample size (and the significance level but I'll assume that you have chosen that already), to identify a sample size you must choose a specific effect size at which you want the given power. In the above diagram, we can see that at $35$ observations per sample, a power of $0.8$ is attained at $d=0.68$ ; if you only needed to guarantee that power when $d$ was $0.7$ you could get away with a slightly smaller $n$ - as you can see from the output in your question, where it's very nearly $33$ . Since your variables have interpretable raw effect sizes, you might well prefer to work with the units of the original variable in specifying effect sizes -- e.g. to specifying a temperature effect size as a change in Celsius rather than in numbers of standard deviations. You would choose a population effect size that would be meaningful to pick up; e.g. a reasonable possibility for a 'small' effect size would be the smallest effect that would be of practical interest, while a 'large' one might be one that would be expected to have a substantive environmental effect. Be aware that I (along with most statisticians) will be an ignoramus in relation to what effect sizes will make sense in your application area -- that's a consideration for someone with some subject-matter expertise, which presumably you will have. On the "add 15%" rule of thumb, it (eventually) occurs to me where that might come from -- under the assumption of a shift alternative, and restricted to symmetric distributions, the worst-case ARE of the Wilcoxon-Mann-Whitney test to the ordinary two-sample t-test is 108/125 = 0.864 (which occurs for the location-scale family of the beta(2,2) density) This would suggest for that very specific situation to multiply the sample size by 1/0.8640 (i.e. add 15.74%), but that's a pretty limited circumstance (population symmetry + shift alternative + large n) â€“ and a worst case (whereas for many distributions heavier tailed than normal you'd be able to use smaller , not larger, sample sizes). You'll also want to assume finite population variance (and indeed perhaps even something a bit stronger than that); it will not be be useful with very heavy tails. A suitable reference for it is [2] . (There's similar justification for the signed rank test compared to the one-sample/paired t-test. Outside those two cases I don't think that it applies.) I think it would be better, as far as possible, to put your effect sizes in terms of the population change that the Wilcoxon-Mann-Whitney looks at ( $P(A>B)$ in the continuous case, where $A$ is a random member from one population and $B$ from the other), suitably scaled to an effect size. That should work across a wider variety of circumstances. In the case of a sample estimate I believe some people use $r=|Z|/\sqrt{n}$ or $r=|Z|/\sqrt{n-1}$ (which up to a typically negligible factor of $\sqrt{1-1/n}\approx 1-\frac{1}{2n}$ ) seem to be sample versions of the same sort of thing as I was describing, while SPSS uses $\hat{\eta}^2=\frac{Z^2}{n-1}$ which it appears would correspond in the population to the square of the quantity I was referring to, so it seems that's also essentially the same idea. It looks like several other suggested measures of effect size are also monotonically related to that same quantity, or nearly so. Sal Mangiafico discusses various measures of effect size for the test here in the context of an example involving Likert scales. While you're not dealing with Likert scales, there's some interesting comparisons there. You mention comparing medians, but you're using Wilcoxon-Mann-Whitney, which does not compare medians (in spite of quite a few books claiming otherwise). We sometimes get confused questions from readers of such books when they have identical sample medians and yet get a rejection with the Wilcoxon-Mann-Whitney, or cases where the test's estimate of the effect is in the opposite direction to the difference in medians. $\dagger$ This is a response to the points in Jeremy Miles' comments below which grew to be many comments long (a) Cohen's quite clearly talking about population values in his book (page refs are 2nd ed, and hereafter, for convenience, writing $m_A$ for Cohen's $\bf{\textsf{m}}_\bf{\textsf{A}}$ and so forth): (i) "the investigator wishes to test the hypothesis that their respective population means are equal, $H_0: m_A-m_B=0$ " (Sec 2.1 p19) this unambiguously establishes that his $m_A$ and $m_B$ are intended as population values; and (ii) he then defines $d=\frac{|m_A-m_B|}{\sigma}$ (eq 2.2.2 p20) so that it is purely in terms of population quantities (as indeed he must because power is a long-run property of the test at a given alternative, not of the sample; Cohen did understand what power was). (b) If he did somehow mean sample values in spite of explicitly saying otherwise, then all his tables are wrong , since the values he gives are definitely based off population quantities. If they were based off sample estimates the sample sizes would need to be larger to account for the uncertainties involved. (c) I agree completely with your statement about Cohen recognizing the problem and expressing some degree of regret over events. I did not seek to lay more blame at his feet than he did $-$ indeed rather less $-$ but the issue did occur, and continues even in spite of his pointing it out, and so needs at least a passing mention here I think. I could perhaps make it more explicit than I did that this is not really his fault, and he did indeed address many of my points either right from the start or later but still quite a long time ago. [1]: Cohen, J, Statistical Power Analysis for the Behavioral Sciences [2]: J. L. Hodges Jr. E. L. Lehmann. "The Efficiency of Some Nonparametric Competitors of the t-Test." Ann. Math. Statist. 27 (2) 324 - 335, June, 1956. https://doi.org/10.1214/aoms/1177728261 (there's a pdf at the AOMS link)
