[site]: crossvalidated
[post_id]: 314664
[parent_id]: 314658
[tags]: 
Freezing a layer of a neural network during training is equivalent to having a random nonlinear projection in your algorithm. Surprisingly, perhaps, this can work quite well (though contentious, look up the ELM [1]) in practice. So the most likely answer is that it really isn't doing anything to your convergence rate since the transform happens to preserve sufficient information that it makes no difference. This may also be interesting for you to test. Have you tried it with multiple frozen layers, ideally right after each other? Or with different initializing distributions? I would imagine that would make a difference -- if your transform throws away sufficient information then it wouldn't be possible to get the same accuracy. With respect to the idea of loss of modelling power, that depends somewhat on how large the network you're training is. However, generally speaking, modern architectures are far more than expressive enough to classify MNIST. [ https://en.wikipedia.org/wiki/Extreme_learning_machine]
