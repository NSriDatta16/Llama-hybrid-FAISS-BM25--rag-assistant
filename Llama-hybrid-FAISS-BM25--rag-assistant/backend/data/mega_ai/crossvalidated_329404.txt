[site]: crossvalidated
[post_id]: 329404
[parent_id]: 291524
[tags]: 
I'm working on a similar case. I have a program that gives the user the same questions over and over again, and they slowly learn the answers. It takes them 3 or 4 attempts on average to 'learn' an answer. I can easily calculate their probability based on data from the start of time. The problem is that assumes the user never improves. In reality their old answers need to be thrown out. It is possible to add a weighting to newer events and still use this Rule of Succession method: probability = (success + 1) / (total + 2) . But it means you have a trade off between punishing users for past mistakes or having a long and detailed memory about the user. What i've found best so far is the same as you put originally in your question: Just use a history size of 10 questions and forget ones that are older. The historic weighting / damping doesn't matter that much unless you want to avoid having sharp jumps in your results when the history buffer empties.
