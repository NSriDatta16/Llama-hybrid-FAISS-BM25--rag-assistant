[site]: crossvalidated
[post_id]: 354936
[parent_id]: 354927
[tags]: 
There is good news and bad news for you here. The good news is that yes, you can avoid calculating the action value function $Q(s,a)$. In fact you don't need to calculate any value function, although it may be helpful to calculate the state value function $V(s)$ - and clearly if that is your goal you should use a method that helps with this. The class of algorithms that you should look into are called policy gradient methods . The simplest algorithm in this class is REINFORCE, and it works directly with a stochastic policy function $\pi(a|s)$ that can be optimised based on sampled episodes. There is no value function estimate in REINFORCE. If you want to have an estimate for $V(s)$ using policy gradient, you can assess it whilst learning separately. However, it can also be used to augment the gradient estimates in policy gradient. This is how Actor-Critic algorithms work. Recent popular Actor-Critic implementations are A3C and A2C . Using a policy gradient method like Actor-Critic, you avoid the need to maximise over $a$ for $Q(s,a)$ in order to select a greedy action. The bad news is that using a policy gradient method with a state value estimate does not remove the need to explore the state/action space in order to learn it. If the action space is large and the environment response to different action values is complex, then finding near-optimal solutions can still be a hard problem. Whether it is better to use value-based methods like Q-learning, or policy-gradient methods like Actor-Critic depends on features of the problem. In your case, the continuous action space more or less forces you to abandon action value methods that learn $Q(s,a)$, so you don't really have the choice.
