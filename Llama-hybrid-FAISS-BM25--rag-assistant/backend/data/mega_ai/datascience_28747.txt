[site]: datascience
[post_id]: 28747
[parent_id]: 
[tags]: 
Purpose of weights in neural networks

I'm beginner at Neural Networks. After reading multiple articles on wikipedia, i've seen the term "weight" being used a lot, although it is a little confusing. I know, that before the inputs are summed and passed to activation functions, they are separately weighted, after some research, i found out that the purpose of the weight function was to: ensure orthonormality avoid data loss I know that two inputs are only orthonormal if they are orthogonal unit vectors, if they are orthogonal then their dot product is always 0. Dot product: where theta is angle between two unit vectors and absolute value of a is norm. For example, if two unit vectors are perpendicular to each other, then they are orthonormal and their dot product is 0. But what does this have to do with data loss? I've also heard that sometimes the value of input might be zero, and we know that multiplication by zero outputs zero. So what is the real purpose of the weight in neural networks? and what does it have to do with orthonormality? For example, what would be the purpose of weights in linear regression?
