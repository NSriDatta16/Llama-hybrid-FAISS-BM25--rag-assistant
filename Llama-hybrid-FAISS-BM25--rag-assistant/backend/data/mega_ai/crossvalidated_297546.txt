[site]: crossvalidated
[post_id]: 297546
[parent_id]: 290701
[tags]: 
What you're doing here is what I refer to as "Holdout Stacking" (sometimes also called Blending but that term is also used for regular Stacking), where you use a holdout set to generate the training data for the metalearning algorithm (i.e. predDF ). I use the term Holdout Stacking to differentiate from regular Stacking (or "Super Learning") where you generate cross-validated predicted values from the base learners to generate the training data for the metalearner algorithm (in your case, a Random Forest) rather than a holdout set (your testing frame). The problem here is not how you're doing the stacking, but how you're evaluating the results. Once you've used the testing frame to generate the predDF frame, you have to throw that data away and not use it for model evaluation. In your example, you are also using the testing frame to evaluate the performance of the base models and the ensemble learner. To fix this, just partition off another chunk of your data. You should have three datasets: training , validation and testing . Use the validation set to create predDF (also known as the "level one" dataset in stacking terminology). # Generate level-one dataset for training the ensemble metalearner predRF Then evaluate your base learners and your ensemble on the testing set to get a better idea of how the ensemble compares to the individual learners. # Generate predictions on the test set testPredRF Lastly, as a suggestion, I'd recommend trying a GLM for the metalearning algorithm because they seem to perform better than tree-based models in my experience, though that is not always the case. If you're specifically looking for multiclass support in Stacking, it will be available soon in the h2o R package. If you don't need multiclass, then you can check out either the SuperLearner or h2o packages to do stacking more easily than writing it all out by hand. See the SuperLearner() or the h2o.stackedEnsemble() functions to do Stacking with one line of code.
