[site]: crossvalidated
[post_id]: 398503
[parent_id]: 398434
[tags]: 
This opens up a big topic that is addressed in the maximum likelihood estimation chapter in my book Regression Modeling Strategies . And the links provided by others above do not address penalization. To put this into perspective, note that you will not know the true regression coefficients in practice. But if you knew that one coefficient is likely to be larger than the others, you could elect not to penalize that coefficient at all, or to put a milder penalty on that one. In the Bayesian world this would be called putting a less skeptical prior on the one with the suspected large coefficient. Directly to your question, standardizing the data mainly helps if the predictors have different scales. It can also help, even if their scales were the same, in choosing the ridge, lasso, elastic net, etc. penalty parameter(s). So you need to distinguish between different scales and different effects on $Y$ . Synonymous with the need to specify a variance in a prior in Bayesian modeling, penalized maximum likelihood estimation has a scaling problem when scales of the $x$ 's differ, and it is traditional to scale by the $x$ standard deviations. This is arbitrary but usually better than not scaling. In my book I make the model a little more interpretable by putting the scaling constants in the penalty function instead of pre-processing $x$ 's. This is also the way my R rms package works for penalizing logistic and ordinary regression models.
