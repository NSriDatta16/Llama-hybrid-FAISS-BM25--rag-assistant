[site]: crossvalidated
[post_id]: 434860
[parent_id]: 
[tags]: 
A problem about the off-policy prediction via importance weight sampling

Recently, I have read the “Reinforcement Learning: An Introduction” 2nd Edition, and I found that the pseudo-code relevant to the incremental implementation of off-policy learning is weird, the weight updating statement seems like defined in an error position. Why does this piece of code assigning the weight value after the value estimation? The final result is slightly different from the different positions of that statement. Therefore, is that pseudo-code wrong?
