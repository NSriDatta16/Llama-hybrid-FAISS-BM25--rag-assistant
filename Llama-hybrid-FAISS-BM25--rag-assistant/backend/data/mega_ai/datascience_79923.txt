[site]: datascience
[post_id]: 79923
[parent_id]: 
[tags]: 
How can be proved that the softmax output forms a probability distribution and the sigmoid output does not?

I was reading Nielsen's book and in this part of chapter 3 about the softmax function , he says, just before the following Excercise, that the output of a neural network with a output softmax layers forms a probability distribution and the sigmoid output does not always forms it. Now I've been wondering about the output of a neural network, if I have a sigmoid output layer, say for one observation the output is 0.7 for class 0, should the probability for class 1 be 0.3? Or, in this binary classification example, using a softmax output, the first output neuron would be 0.7 for class 0 and 0.3 for class 1 in that particular observation?
