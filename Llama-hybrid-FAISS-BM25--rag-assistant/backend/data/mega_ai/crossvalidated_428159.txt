[site]: crossvalidated
[post_id]: 428159
[parent_id]: 428156
[tags]: 
In PCA, you find new axes that explains the most variance. Your old axes were $x_1,x_2,x_3$ ; and your new axes (i.e. eigenvectors of covariance matrix) are linear combinations of these axes. For example, the principal component that can explain $99 \%$ of the variance is of the form $e_1=\alpha x_1+\beta x_2+\theta x_3$ . If you drop one of the features, say $x_3$ , you'll try to explain the data (i.e. $x_1,x_2$ ) using axes of the form $ax_1+bx_2$ , which of course does not guarantee keeping $99 \%$ explainability.
