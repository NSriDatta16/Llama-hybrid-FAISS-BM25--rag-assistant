[site]: crossvalidated
[post_id]: 237794
[parent_id]: 
[tags]: 
Ridge regression degrees of freedom: limit according to sample size?

I'm working on a high-ish dimensional logistic regression problem. I have 40 variables ($p=40$) and 900 samples ($n=900$), but only 30 of those samples are in the class I'm trying to predict. I'm fitting a predictive model using penalized maximum likelihood (ridge regression, via glmnet ). This requires setting a value for the penalty term $\lambda$. Typically, $\lambda$ is determined through cross-validation. I've also been reading Frank Harrell , who suggests keeping a model's degrees of freedom below a fraction of the "limiting sample size", $m$. As a rule of thumb, he suggests $df The effective degrees of freedom in ridge regression can be calculated as a function of $\lambda$ , $$ df(\lambda) = \sum^{p}_{j=1} d_j^2 / (d_j^2 + \lambda) $$ where $d_j$'s are the singular values of the sample matrix ( ESL , eq 3.50). In ridge regression (or other penalized maximum likelihood techniques), is it ever advisable to choose $\lambda$ using the limiting sample size $m$, instead of choosing it through cross-validation? When might this be wise?
