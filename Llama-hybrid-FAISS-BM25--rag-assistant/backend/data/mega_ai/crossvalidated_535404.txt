[site]: crossvalidated
[post_id]: 535404
[parent_id]: 
[tags]: 
R vs spss: ttest in python with different result after bootstrapping

Running paired t-test after bootstrapping results in very different results when using SPSS versus "ttest_rel" function in python. Why is that and which one is correct? I first had 5-fold cross-validation. To compare pair-wise, each fold was run with two models. That is, for instance, fold k=0 was run with model 1 and model 2. I then measured the mean average precision (MAP). Now I want to compare which model is more accurate. Please note that I am comparing the values for each fold separately in order to prevent the violation of the independence condition for t-test. To summarize, I have now paired scores of ap_x and ap_y, on which I ran t-test with bootstrapping. Given a sample with pairs of ap_x and ap_y with size n=30, after bootstrapping 10,000 times for pair-wise t-test, the SPSS gives a t-value of about 1.717 and a p-value of .157. SPSS code: BOOTSTRAP /SAMPLING METHOD=SIMPLE /VARIABLES INPUT=ap_x ap_y /CRITERIA CILEVEL=95 CITYPE=BCA NSAMPLES=10000 /MISSING USERMISSING=EXCLUDE. T-TEST PAIRS=ap_x WITH ap_y (PAIRED) /CRITERIA=CI(.9500) /MISSING=ANALYSIS. However, doing the same in python using "scipy.stats.ttest_rel" returns t=81.378 with p def func_bootstrapping(n_samples, x, y): diff_list = [] sample_means_overall_x = [] sample_means_overall_y = [] for _ in range(n_samples): sample_mean_x = np.random.choice(x,size=len(x)).mean() sample_means_overall_x.append(sample_mean_x) sample_mean_y = np.random.choice(y,size=len(y)).mean() sample_means_overall_y.append(sample_mean_y) diff_list += [sample_mean_x - sample_mean_y] return sample_means_overall_x, sample_means_overall_y, diff_list The mean difference between two vairables= 0.033. I first thought it is the sampling effect; however, repeating it several times, returns the same values. ap_x and ap_y, the repeated measurement of ap variable over different conditions, are for the most part similar, with mean values of 0.968 and 0.934. The two variables have mostly values of 1.0 except a few cases where the value of ap_x is larger than ap_y. I cannot interpret the results. Can anyone please help with explaining why this happens and which one is the correct response? Are the difference significant or not? A sample dataset pasted below. However, please note that this issue occures for lots of the conditions (>80) that I tested pairwise. The scores are the results of Mean Average Precision in an offline evaluation. Each row is a user: index,ap_x,ap_y 0,1.0,1.0 1,0.8875,0.679166666666667 2,1.0,1.0 3,1.0,1.0 4,1.0,1.0 5,1.0,1.0 6,1.0,1.0 7,1.0,1.0 8,1.0,1.0 9,1.0,1.0 10,1.0,1.0 11,1.0,1.0 12,1.0,1.0 13,1.0,1.0 14,1.0,1.0 15,0.5,0.0 16,0.833333333333333,0.804166666666667 17,1.0,1.0 18,1.0,1.0 19,1.0,1.0 20,1.0,1.0 21,1.0,1.0 22,1.0,1.0 23,1.0,1.0 24,1.0,1.0 25,1.0,1.0 26,1.0,1.0 27,0.805555555555555,0.5333333333333329 28,1.0,1.0 29,1.0,1.0
