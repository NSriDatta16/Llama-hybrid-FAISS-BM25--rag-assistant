[site]: crossvalidated
[post_id]: 603535
[parent_id]: 
[tags]: 
In Transformers, for the maximum length of Encoder's input sequences and Decoder's input sequences - should they be two different numbers?

I noticed that there's usually a limit for the input length of transformers. But considering there are actually two input layers - one for the Encoders and one for the Decoders, can we and should we have two separate MAX_TOKENS for them? If we have different MAX_TOKENS for Encoder input and Decoder input, then the cross attention matrix would be not a square matrix anymore. However in my opnion there are practical reasons to do so, for example, in machine translation we have a source language sentence and target language sentence, which highly likely have different lengths. English: I like weightlifting . ------------------ 3 tokens Chinese: 我 喜 欢 举 重 。----------- 5 tokens But from TensorFlow's Tutorial on Transformers ( Neural machine translation with a Transformer and Keras ), the source sequence and target sequence are trimmed to the same length MAX_TOKENS : MAX_TOKENS=128 def prepare_batch(pt, en): pt = tokenizers.pt.tokenize(pt) # Output is ragged. pt = pt[:, :MAX_TOKENS] # Trim to MAX_TOKENS. pt = pt.to_tensor() # Convert to 0-padded dense Tensor en = tokenizers.en.tokenize(en) en = en[:, :(MAX_TOKENS+1)] en_inputs = en[:, :-1].to_tensor() # Drop the [END] tokens en_labels = en[:, 1:].to_tensor() # Drop the [START] tokens return (pt, en_inputs), en_labels Is this just for the sake of simplicity, or it is a norm to use a unified MAX_TOKENS for both Encoder input and Decoder input? In a practical application, what is the best way to determine the value of MAX_TOKENS ?
