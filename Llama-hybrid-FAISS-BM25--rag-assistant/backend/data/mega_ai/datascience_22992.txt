[site]: datascience
[post_id]: 22992
[parent_id]: 22731
[tags]: 
As far as the paper "Sample-Level deep convolutional neural networks for music auto-tagging using raw waveforms", I can give you some of my intuitions about the question since I and my colleague proceeded the experiments. To summarize, I suggest you to use spectrogram based approaches in your situations. There are two reasons I would like to point out, First, training raw waveform based architecture takes about 4 times longer than spectrogram based model when the sampling rate is ranging from 16kHz to 22kHz. In your case, sampling rate is even 22Mhz. I think it will take a lot more time than spectrogram based model with similar performances. Second, to obtain well trained raw waveform based model, we need more than 50 hours audio since the model has more parameters and deeper layers. In my opinion, the benefit of using a raw waveform-based model is not the performance improvement, but on the generative model. If we use well performing raw waveform based model, we would not need to reconstruct audio signal from spectrogram when the case is generative model. This is the main reason why we performed reported experiments. If computing power and memory improve with current trends, we expect that the raw waveform-based model will be the mainstream in the near future. But now I think the spectrogram-based model is more convenient, especially for industrial applications.
