[site]: datascience
[post_id]: 31653
[parent_id]: 31652
[tags]: 
Once converted to numerical form, models don't respond differently to columns of one-hot-encoded than they do to any other numerical data. So there is a clear precedent to normalise the {0,1} values if you are doing it for any reason to prepare other columns. The effect of doing so will depend on the model class, and type of normalisation you apply, but I have noticed some (small) improvements when scaling to mean 0, std 1 for one-hot-encoded categorical data, when training neural networks. It may make a difference too for model classes based on distance metrics. Unfortunately, like most of these kind of choices, often you have to try both approaches and take the one with the best metric.
