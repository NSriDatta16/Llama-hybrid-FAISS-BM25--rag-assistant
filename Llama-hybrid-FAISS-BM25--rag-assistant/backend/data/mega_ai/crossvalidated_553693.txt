[site]: crossvalidated
[post_id]: 553693
[parent_id]: 352916
[tags]: 
Let’s do a really extreme example where neither of the two variables give much predictive ability, but the combination does. library(ggplot2) set.seed(2021) N Look at the plots of the two marginal predictor variables. Given an x0 or x1 value, can you predict the color? ggplot(d, aes(x = x0, fill = y)) + geom_density(alpha = 0.3) + theme_bw() ggplot(d, aes(x = x1, fill = y)) + geom_density(alpha = 0.3) + theme_bw() By construction, each color has the same distribution for x0 and x1. If you know that x0 = 3, you have no idea what color the point is, and if you know that x1 = -3, you have no idea what color the point is. However, let’s look at both predictors simultaneously. ggplot(d, aes(x = x0, y = x1, col = y)) + geom_point() + theme_bw() It’s easy to see how the colors are separated in two dimensions! If the features are (x0, x1) = (3, -3) , you can be pretty much certain about the color (red, not blue). Depending on the model you run, this particular example might require an interaction (e.g., a classical logistic regression would require the interaction to be specified explicitly, while a large neural network would be able to figure it out).
