[site]: crossvalidated
[post_id]: 310460
[parent_id]: 
[tags]: 
How to use residuals to choose the best model parameters and avoid over-fitting?

A common way to avoid an over-fitting is to train a model on one set and then to check the error on the validation set. If the out-of-sample error is much larger than the in-sample error than we have an over-fitting and therefore we need to suppress the expressive power of the model (either by using a model with a smaller number of parameters or by implying penalties proportional to values of the parameters on the model). In fact the procedure is based on comparison of two errors (in-sample and out-of-sample). Is it possible to generalize this procedure? For example, for any value of the model parameters we can calculate the residuals (differences between the target and the predicted values). Then we somehow compare the residuals for two alternative models and in this way (some how) we decide what set of model parameters is better. We want to do it in such a way that avoid over-fitting. Obviously the simplest (naive) way to do it is to calculate some average of residuals and then choose the model providing smaller residuals on average, but this obviously do not prevent us from an over-fitting. It looks to me that it should go in the direction of variation of residuals. We want to know not only what are their values on average but also what is there variance. For example if average is small but variance is large we can assume that such a small average is observed just by chance. Maybe it should go in the direction of statistical significance. We can say that model B is better than model A not only if the mean of the residuals of the model B is smaller, this decrease should also be statistically significant.
