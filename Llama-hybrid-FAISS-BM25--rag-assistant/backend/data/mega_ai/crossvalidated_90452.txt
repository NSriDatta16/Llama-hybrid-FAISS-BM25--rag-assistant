[site]: crossvalidated
[post_id]: 90452
[parent_id]: 
[tags]: 
Why is the state-action value function required when the model is unknown?

At page 117 of the book "Reinforcement Learning: An Introduction" by (Andrew Barto and Richard S. Sutton), it is stated With a model (e.g. a full DP or MDP), state values (e.g. $V(s)$ are sufficient to determine a policy - just look ahead to the next step and choose the best combination of reward and state. Without a model, however, state values alone are not sufficient. One must explicitly estimate the value of each action for the values - e.g $Q(s, a)$ to be useful in suggesting a policy. What is an example that makes this obvious? Why can't one just use a form of Bellman's equation in $V(s)$ to determine the best policy via e.g. value iteration, without a state model?
