[site]: crossvalidated
[post_id]: 598487
[parent_id]: 521332
[tags]: 
...how we can use $\theta_{k+1}$ before even defining it? It's actually defined by the update rule: $$ \theta_{k+1} = \theta_k - \gamma_k \nabla f(\theta_{k+1}) $$ What you do is you search for a $\theta_{k+1}$ that satisfies this equation. You could also rewrite this as follows: $$ \begin{aligned} &\text{Find }\theta_{k+1}\text{ such that:}\\ &h(\theta_{k+1}) = \theta_{k+1} - \left[ \theta_k - \gamma_k \nabla f(\theta_{k+1}) \right] = 0 \end{aligned} $$ To solve this, one could use any root finding method, including fixed-point iteration, as suggested in the comments: Start from a guess $\theta_{k+1}^{(0)}$ . Update the guess: $\theta_{k+1}^{(1)} = h(\theta_{k+1}^{(0)})$ . Loop step (2) until $\theta_{k+1}^{(n)}$ stops changing. You found $\theta_{k+1} \equiv \theta_{k+1}^{(n)}$ . Each step of such a fixed-point iteration requires evaluation of the gradient $\nabla f(\theta_{k+1}^{(n)})$ , which could be really computationally intensive. Why do we need implicit SGD? The paper below shows that: Implicit and explicit SGD are asymptotically unbiased (Theorem 4.1). Both algorithms provide the same asymptotic efficiency (Thm. 4.2). Similarly to implicit Euler method for solving differential equations, implicit SGD is more stable (section 4.3): ...in the standard SGD procedure, the effect from the initial conditions can be amplified in an exponentially large way before fading out, if the learning rate is misspecified... ...the effects of the initial conditions monotonically decrease in the implicit method... Seems like the main advantage of implicit SGD compared to the regular explicit one is increased stability of the algorithm. References Toulis, Panagiotis, Jason Rennie, and Edoardo M Airoldi. 2014. "Statistical Analysis of Stochastic Gradient Methods for Generalized Linear Models." In Proceedings of the 31st International Conference on Machine Learning, edited by Eric P. Xing and Tony Jebara, 32:667â€“75. Bejing, China: PMLR. https://proceedings.mlr.press/v32/toulis14.html .
