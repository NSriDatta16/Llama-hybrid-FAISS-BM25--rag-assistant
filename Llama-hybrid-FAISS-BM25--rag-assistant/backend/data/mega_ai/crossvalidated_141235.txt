[site]: crossvalidated
[post_id]: 141235
[parent_id]: 
[tags]: 
How is Gram-Schmidt procedure used in the following time series context?

I was reading the innovation algorithm in Brickel's Time Series Theory and Methods (page 171-172). Let $H$ denotes a Hilbert space, $P$ denotes the projection operator and $\bar{sp}$ denotes closed span . It mentioned that The innovation algorithm depends on the decomposition of $H_{n}$ into $n$ orthogonal subspaces by means of the Gram-Schmidt procedure . Then it says As before, we define $H_n = \bar{sp}\{X_1, ..., X_n\}$ and and the one-step predictors $$\hat{X}_{n+1} = 0 \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; \text{if } n = 0$$ $$\hat{X}_{n+1} = P_{H_n} X_{n+1}\;\;\;\;\;\;\; \text{if } n \geq 1$$ and we also define $\hat{X_1}:= 0$, $$H_{n} = \bar{sp}\{X_1 - \hat{X}_1, X_2 - \hat{X}_2,..., X_n - \hat{X}_n\} \;\;\;\;\;\;\;\;\;\; n \geq 1 \;\;\;\;\;\;\;\;\;\;(1)$$ so that $$\hat{X}_{n+1} = \sum^n_{j=1}\theta_{nj}(X_{n+1-j} - \hat{X}_{n+1-j})\;\;\;\;\;\;\;\;\;\;\;(2)$$ The recursive scheme for computing ${\theta_{nj}, j = 1,...,n = 1,2,...}$, is derived later on in the book. My first question is: how does $(1)$ come about from the Gram-Schmidt procedure ? Because from my understanding, according to the Gram-Schmidt procedure, the orthogonal vectors of $H_n$ that is also a closed span should be the following: $$u_1 = X_1 = X_1 - \hat{X}_1$$ $$u_2 = X_2 - P_{H1}X_2 = X_2 - \hat{X}_2$$ $$u_3 = X_3 - P_{H1}X_3 - P_{H_2}X_3= X_3 - \hat{X}_{1+2} - \hat{X}_{2+1}$$ $$....$$ and I thought it should be $H_n = \bar{sp}\{u_1, u_2, u_3,...\}$ which is clearly different from $(1)$ above. My second question is how does $(1)$ lead to $(2)$ , suppose that we can ignore the what exactly $\theta_{nj}$ is for now?
