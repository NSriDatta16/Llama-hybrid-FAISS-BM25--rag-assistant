[site]: crossvalidated
[post_id]: 602858
[parent_id]: 601441
[tags]: 
It depends on your output domain It can be a Multivariate Gaussian $$p(x|z) \sim \mathcal{N}(z, \mu_\phi(z),\Sigma_\phi(z))$$ For images, you can also take a categorical distribution over the pixel values. $$p(x|z) \sim \text{Categorical}(x|\phi(z))$$ In practice, this translates to a neural network where you take the softmax over the pixel values. You can then learn the parameterization $\phi$ of $p_\phi(x|z)$ by MLE, i.e. maximizing $$\mathbb{E}_{z \sim q_\theta(z)}[\log p_\phi(x|z)]$$ Indeed, Wikipedia seems to say that p(x|z) is usually defined as a Normal distribution I think Wikipedia is talking about the parameterization of $q(z|x)$ in the passage you cited. $f_1(z)$ computes $\mu_\theta$ and $f_2(z)$ computes $\log \Sigma_\theta$ . This is the distribution from where you sample $z \sim \mathcal{N}(z,\mu_\phi,\Sigma^2_\phi)$ , which you then feed into the decoder. You can find a detailed description of variational autoencoders here
