[site]: crossvalidated
[post_id]: 262566
[parent_id]: 262554
[tags]: 
A practical reason is as follows. Typically, you will try out some hyper parameters when optimising losses that are sums over data samples. E.g. the step rate of the optimiser (be it plain SGD or something more sophisticated such as Adam) and the number of samples to estimate the gradient, i.e. the batch size . Now, if you do not average, the step rate and the batch size get coupled. E.g. if your update is $$ \theta_t = \theta_{t-1} - \alpha \sum_{i=1}^{N} \frac{\partial \ell_i}{\partial \theta} $$ then the step length you do depends on your batch size $N$. Now, if you increase $N$ from say, 16 to 128, you will have to decrease your step rate $\alpha$ by 128/16 in order to keep the magnitude of the update roughly the same.
