[site]: crossvalidated
[post_id]: 380306
[parent_id]: 
[tags]: 
How do we know we're maximizing the Lagrangian objective function in PCA?

In Principal Component Analysis, we start with $m$ observations $x_1,\dots,x_m$ , each of which is an $n$ -dimensional vector. Assume we have centered the data; that is, we have subtracted the variable means from each observation. To project each observation into a $1$ -dimensional subspace while maximizing sample variance, we want to compute a unit length basis vector, call it $v$ . So we get the constrained problem $$ \mathop{\arg\,\max}\limits_{v}\,\frac{1}{m}\sum_{i=1}^{m}\left\lVert\langle x_i,v\rangle v\right\rVert $$ $$ \text{subject to }v^Tv=1 $$ After some computations, this becomes $$ \mathop{\arg\,\max}\limits_{v}\,\frac{1}{m}\sum_{i=1}^{m}v^TCv $$ $$ \text{subject to }v^Tv=1 $$ where $C$ is the covariance matrix of the data. Then the Lagrangian objective function is $$ \mathcal{L}(v,\lambda)=v^TCv-\lambda(v^Tv-1) $$ If we start taking partial derivatives of $\mathcal{L}$ , setting these to zero, etc, how do we know that we are maximizing $\mathcal{L}$ ? Do we need to check that the Hessian is negative definite? Even then, I think that only guarantees a local maximum?
