[site]: datascience
[post_id]: 84810
[parent_id]: 
[tags]: 
When to use GAN over conventional sampling methods?

Let's say I have a dataset from a diabetes hospital which has 30000 Type 2 diabetes and 300 Type 1 diabetes patients. So this dataset has millions and millions of other data points like lab measurements, drugs prescribed and diagnosis data. Now my objective is to build a model which can classify Type 2 and Type 1 diabetes patients. As you can see that the dataset is highly imbalanced and I don't have enough T1DM patients to understand their patterns/behavior/feature which can help me differentiate them from T2DM. So, my question now is when should I use sampling approaches like oversampling and when should I use GANs ? Should I select features of my interest and then apply oversampling or should I apply GAN? update (addition to the above scenario) Let's consider another scenario. I have a dataset which has only 300 T1DM patients (there are no T2DM patients). Now, I would like to just increase my dataset size. Let's also think that I don't have any ML task in my mind (meaing classification/ Regression etc) but I know that 300 samples are very less and can never be used for any meaningful analysis. So, now I would like to increase the dataset size and the use it for analysis. Here, GAN is the only solution for the synthetic data? Since no model is involved, I can't apply oversampling etc. can help me with this?
