[site]: crossvalidated
[post_id]: 399333
[parent_id]: 309441
[tags]: 
According to the chapter-2 from the book of Michael Nielsen, two assumptions are required regarding the form of the cost function for backpropagation to work properly. One of the assumptions is to define the cost as a function of the output activations. Now, whatever activation function (i.e., sigmoid, softmax, linear) you are using in your neural network, whenever you take the partial derivative of the cost function with respect to weight, the derivative of the weighted input will always produce an additional $x_{j}$ due to the application of the chain rule. It is not possible to get rid of this additional $x_{j}$ due to the constraint that cost has to be a function of the output activations. N.B.: I don't think there is a very formal argument required to approach this question.
