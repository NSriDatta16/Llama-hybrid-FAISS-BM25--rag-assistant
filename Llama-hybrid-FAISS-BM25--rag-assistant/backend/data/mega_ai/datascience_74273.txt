[site]: datascience
[post_id]: 74273
[parent_id]: 74258
[tags]: 
After trying to recreate the issue with random numbers (and failing initially), I figured out that the problem comes from the fact that the x_train data that I'm using contains columns that have a very small, near-zero values. To recreate, the first section is only run once: scale = 0.0001 # making this larger eliminates the issue x_train = np.random.uniform(0,scale,size=(1000,10)) y_train = np.random.uniform(0,1,size=(x_train.shape[0])) Then using the same values for x_train and y_train , run the section below but with use_weights set to True and then False . use_weights = True model = RandomForestRegressor(n_estimators=10, random_state=0) if use_weights: weights = np.arange(1,x_train.shape[1]+1)[None,:] model.fit(x_train * weights, y_train) prediction = model.predict(x_train * weights) else: model.fit(x_train, y_train) prediction = model.predict(x_train) print(prediction[0]) # changes based on use_weights value assuming scale is very small As a side note, the y_train values for the real dataset are also very small and I have to multiply them by 100 or more to get the model to even run. That is, it will not create any leaves at all without expanding the scale of the y_train values (confirmed by running the get_depth method on each tree). I'm wondering, is this purely a numerical imprecision issue or is it something unique to the random forest calculations that are happening under the hood?
