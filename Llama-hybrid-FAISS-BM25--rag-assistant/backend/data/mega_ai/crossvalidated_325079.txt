[site]: crossvalidated
[post_id]: 325079
[parent_id]: 264533
[tags]: 
No one mentioned approaches that make hyper-parameter tuning and feature selection the same so I will talk about it. For this case you should engineer all the features you want at the beginning and include them all. Research now in the statistics community have tried to make feature selection a tuning criterion. Basically you penalize a model in such a way that it is incentivized to choose only a few features that help it make the best prediction. But you add a tuning parameter to determine how big of a penalty you should incur. In other words you allow the model to pick the features for you and you more or less have control of the number of features. This actually reduces computation because you no longer have to decide which features but just how many features and the model does the rest. So then when you do cross-validation on the parameter then you are effectively doing cross-validation on feature selection as well. Already there are many ML models that incorporate this feature selection in some way or another. Doubly-regularized support vector machines which is like normal SVM but with feature selection Elastic net which deals with linear regression Drop-out regularization in neural networks (don't have reference for this one) Random forest normally does random subsets of the features so kind of handles feature selection for you In short, people have tried to incorporate parameter tuning and feature selection at the same time in order reduce complexity and be able to do cross-validation
