[site]: crossvalidated
[post_id]: 478067
[parent_id]: 
[tags]: 
How to verify that one ML-classifier is better then the other using the same train and test data set without cross-validation?

I have compared 5 methods (ML classifiers) on the same data set. These methods are 5 different types of neural networks. Each is trained on the training set and evaluated with precision, recall and f1-score on a test set. Fourtanetly, there is one model that have some descent better precision, recall and f1-score which also makes logical sense because of the architecture of the model. Additionally, I want to attribute these improvements to the method / model architecture and exclude any statistical chance. I can not use any cross-validation methods, since my data set is manually divided to avoid any relation between groups of examples of each class. So are there any statistical methods or approaches that strength my assumption that the better performance is contributed by the special architecture of the model? Thanks in advance!
