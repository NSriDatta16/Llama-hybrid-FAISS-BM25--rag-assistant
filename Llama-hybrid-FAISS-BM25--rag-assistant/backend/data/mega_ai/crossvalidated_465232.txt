[site]: crossvalidated
[post_id]: 465232
[parent_id]: 
[tags]: 
Is this statement about linear-activation neural network in the PRML in error?

I found the following statement in "Pattern Recognition and Machine Learning" (C. M. Bishop, 2016) p.229. If the activation functions of all the hidden units in a network are taken to be linear, then for any such network we can always find an equivalent network without hidden units. This follows from the fact that the composition of successive linear transformations is itself a linear transformation. However, if the number of hidden units is smaller than either the number of input or output units, then the transformations that the network can generate are not the most general possible linear transformations from inputs to outputs because information is lost in the dimensionality reduction at the hidden units. However, I think that there are two exceptions: D M > K, where D, M, and K are the numbers of input, hidden, and output units respectively. For example, in the former case, when the input and hidden units are connected by identity matrix (only D of M nodes are used, and others are zero), the neural network (without activation) can represent all possible linear transformation using linear transformation between M and K. Is my understanding correct?
