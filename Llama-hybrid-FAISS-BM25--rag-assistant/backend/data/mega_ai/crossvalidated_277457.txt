[site]: crossvalidated
[post_id]: 277457
[parent_id]: 277442
[tags]: 
I am not very familiar with reinforcement learning, but the very next line in the Wikipedia article you cite (currently) refers to the paper Double Q-learning (NIPS 2010) . The abstract to that paper says These overestimations result from a positive bias that is introduced because Q-learning uses the maximum action value as an approximation for the maximum expected action value. Together, these seem to be saying that when the $Q$ function is in reality stochastic, observed rewards $\hat{r}$ resulting from a state-action pair $(s,a)$ will have some (0-mean) noise associated with them, e.g. $\hat{r}=r+\epsilon$. Then, because $Q$ is updated based on $\max_aQ_\text{old}$, the maximum value will tend to be a combination of high reward $r$ and/or large positive noise realizations $\epsilon$. By assuming $r_\max\approx\hat{r}_\max$ and ignoring $\epsilon$, the value of $Q$ will tend to be an over-estimate. (As noted I am unfamiliar with this area, and only glanced at Wikipedia and the above abstract, so this interpretation could be wrong.)
