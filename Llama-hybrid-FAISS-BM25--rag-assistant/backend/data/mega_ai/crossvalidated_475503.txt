[site]: crossvalidated
[post_id]: 475503
[parent_id]: 469799
[tags]: 
Logistic regression is a convex optimization problem (the likelihood function is concave), and it's known to not have a finite solution when it can fully separate the data , so the loss function can only reach its lowest value asymptomatically as the weights tend to Â± infinity. This has the effect of tightening decision boundaries around each data point when the data is separable, asymptotically overfitting on the training set. On a more practical note, logistic regression is often trained with gradient descent. This is a shallow model with a smooth non-zero loss everywhere, so the gradient doesn't vanish easily numerically. Since the optimizer cannot reach an optimal solution via gradient steps with finite step sizes, it can iterate "forever", pushing the weights to increasingly extreme values, in an attempt to reach asymptotically zero loss. In high dimensions this problem is exacerbated because the model will have even more ways to separate the data, so gradient descent is more likely to overfit asymptotically, i.e. if you let it run for long . Note that early stopping is a form of regularization in itself, and that it can take a relatively long time for these models with vanilla gradient descent to overfit.
