[site]: crossvalidated
[post_id]: 400603
[parent_id]: 399757
[tags]: 
Briefly, although decision trees have a low bias / are non-parametric, they suffer from a high variance which makes them less useful for most practical applications. By aggregating multiple decision trees, one can reduce the variance of the model output significantly, thus improving performance. While this could be archived by simple tree bagging, the fact that each tree is build on a bootstrap sample of the same data gives a lower bound on the variance reduction, due to correlation between the individual trees. Random Forest addresses this problem by sub-sampling features, thus de-correlating the trees to a certain extend and therefore allowing for a greater variance reduction / increase in performance. Details on this can be found in chapter 15 of The Elements of Statistical Learning as well as chapter 8 of An Introduction to Statistical Learning by Hastie and Tibshirani.
