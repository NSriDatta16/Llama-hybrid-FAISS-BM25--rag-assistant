[site]: crossvalidated
[post_id]: 583827
[parent_id]: 265037
[tags]: 
A hypothesis on why the cosine similarity loss could be more efficient for training neural networks on parallel sentences comprised of Word2Vec vectors: The mean squared error loss optimizes the different dimensions of the output w2v vector independently. It treats the vector as an array of independent numbers, which is actually not the case since they all contribute to/are a function of the higher-order geometric/semantic properties of the word vector. The cosine similarity loss on the other hand, takes into account the geometric relationships that exist between the different components of the Word2Vec vector. And so gradient updates should systematically modify these vector components in a non-greedy manner. At any given time step, the most effective gradient update might not be one that modifies every vector component equally (as the mse is likely to do), but one that modifies them in a way that moves the overall output vector closer to the target. This perspective makes sense to me- it'll be interesting to see what experiments have to say.
