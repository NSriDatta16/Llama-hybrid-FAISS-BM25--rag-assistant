[site]: datascience
[post_id]: 123740
[parent_id]: 
[tags]: 
XGBoost tweedie regression objetive from scratch

I'm trying to gain a deeper understanding of the tweedie loss function and how it is used in XGBoost. So, I tried to implement it from scratch. I started by examining the original implementation . I think the important part is bst_float grad = -y * expf((1 - rho) * p) + expf((2 - rho) * p); bst_float hess = -y * (1 - rho) * std::exp((1 - rho) * p) + (2 - rho) * expf((2 - rho) * p); I used these formulas to create a custom objective in Python following the documentation on custom objective functions . def custom_tweedie(predt: np.ndarray, dtrain: xgb.DMatrix, rho: float = 1.5): """Custom squared error objective function for XGBoost""" y = dtrain.get_label() scores = np.log(predt) exp_1_score = np.exp((1 - rho) * scores) exp_2_score = np.exp((2 - rho) * scores) grad = -y * exp_1_score + exp_2_score hess = -y * (1 - rho) * exp_1_score + (2 - rho) * exp_2_score return grad, hess scores = np.log(predt) is necessary, because XGBoost doesn't know the link function for custom objectives (see: Reverse Link Function for reference). Based on this implementation, I made minimal example to compare my custom loss function with the vanilla implementation: from functools import partial from matplotlib import pyplot as plt import numpy as np from sklearn.datasets import fetch_california_housing import xgboost as xgb # I know this isn't a tweedie distributed target variable, # but both implementations should be equally good/bad. X, y = fetch_california_housing(return_X_y=True, as_frame=True) n_samples, n_features = X.shape def custom_tweedie(predt: np.ndarray, dtrain: xgb.DMatrix, rho: float = 1.5): """Custom squared error objective function for XGBoost""" y = dtrain.get_label() scores = np.log(predt) exp_1_score = np.exp((1 - rho) * scores) exp_2_score = np.exp((2 - rho) * scores) grad = -y * exp_1_score + exp_2_score hess = -y * (1 - rho) * exp_1_score + (2 - rho) * exp_2_score return grad, hess # vanilla xgboost regression params = { "seed": 42, "objective": "reg:tweedie", "tweedie_variance_power": 1.5, "tree_method": "gpu_hist", "max_depth": 3, "learning_rate": 0.01, } results_vanilla = {} xgb.train( params=params, dtrain=xgb.DMatrix(X, y), num_boost_round=500, evals=[(xgb.DMatrix(X, y), "train")], evals_result=results_vanilla, ) # custom xgboost regression params = { "seed": 42, "tree_method": "gpu_hist", "eval_metric": "tweedie-nloglik@1.5", "max_depth": 3, "learning_rate": 0.01, } results_custom = {} xgb.train( params=params, dtrain=xgb.DMatrix(X, y), num_boost_round=500, evals=[(xgb.DMatrix(X, y), "train")], evals_result=results_custom, obj=partial(custom_tweedie, rho=1.5), ) x = np.arange(500) y_vanilla = results_vanilla["train"]["tweedie-nloglik@1.5"] y_custom = results_custom["train"]["tweedie-nloglik@1.5"] fix, axs = plt.subplots(figsize=(12,8)) axs.plot(x, y_vanilla, label="vanilla") axs.plot(x, y_custom, label="custom") axs.legend() axs.set_xlabel("Number of Trees") axs.set_ylabel("Tweedie-nloglik") axs.set_title("Comparison of the learinging curves of vanilla and custom objective") Unfortunately, the custom and vanilla objective behave differently and I don't know why. Can some check my custom implementation and tell me what I am missing?
