[site]: crossvalidated
[post_id]: 143541
[parent_id]: 
[tags]: 
Calculating Diversity using normalized species counts

I am trying to calculate the change in diversity of species encountered throughout time. My dataset is comprised to detections of tagged fish for 48 consecutive weeks. My thought was to bin the detections by time intervals (weeks, two weeks, 4 weeks, etc), to look at the change in diversity of species detected over time. In addition, I think I need to normalize the species counts by the number of tagged individuals. For example, if I observed 10 individuals of Species 1 during week 1, I would divide 10 by the total number of individuals of that species that are tagged to account for issues in the detectability of certain species. I have two questions. 1. Is it appropriate to treat time intervals like sites when computing diversity indices (Simpson's, Shannon's, etc)? 2. Is it appropriate to use normalized species counts in this diversity indices? It is my impression that these indices expect count data (integers), or proportion of site (coverage) data, which is not what I would be inputting. I have attempted to look for similar situations in other studies to see if there is a precedent for handling a dataset like this in this way, but I have not been able to find the answers I am looking for. If anyone has any articles or books they would recommend please let me know. I hope I explained my situation adequately. I can provide an example dataset if needed, but I figured these questions could be answered without. Please let me know if it does not make sense or if this question would be more appropriate on a different board. I don't know if this matters but I am working with this data in R and have been using the vegan package.
