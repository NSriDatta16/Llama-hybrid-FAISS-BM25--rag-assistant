[site]: crossvalidated
[post_id]: 363852
[parent_id]: 363809
[tags]: 
You need to take a time out and think about the $M$ features you already have, since your OP basically starts with shoving a lot of stuff in an oven and expecting a pizza to come out when done. For example, when performing micro-surgery on your features -- before you do anything with them -- you need to determine whether: the features were already selected from a class or function approximation prediction analysis the features represent everything known about a system, i.e., all possible measurements including signals and noise, and PCA would be used to learn about groups of correlated features because I don't know anything about the data. Firstly, what's the scale and range of all the features, and what's the correlation between the features? If the data are merely all noise, then the correlation would be degenerate (but not zero, orthogonality for noise data only occurs as $n \rightarrow \infty$). PCA is not intended for all possible subsets or random sets of features to understand the spectral information of a set of data. You could always develop something that would analyze the data ad nauseam via random resampling, but the result would be like hyperemesis $\rightarrow$ projectile vomiting. The beauty of PCA stems from eigendecomposition, and there have been many theoretical aspects of eigendecomposition that are still unexplored in applied numerical analysis.
