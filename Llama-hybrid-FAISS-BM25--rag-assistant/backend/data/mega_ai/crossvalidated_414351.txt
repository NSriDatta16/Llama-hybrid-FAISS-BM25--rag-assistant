[site]: crossvalidated
[post_id]: 414351
[parent_id]: 414347
[tags]: 
CNNs are very sparse compared to dense networks, meaning that there are fewer parameters to estimate. In addition to this being nice for computing times, it reduces the potential to overfit to training data. CNNs don't just drop parameters the way that you can specify in Keras with a dropout command. CNNs are smart about which parameters are forced to be zero, exploiting structure present in the data. Which parameters get dropped by a dropout command in Keras will depend on the particular training data. CNNs will drop the same parameters every time. Further, CNNs force some connections to share parameters, again by being smart and exploiting structure present in the data. Together, these reduce the variability of parameter estimates (some can't vary from 0) to combat overfitting. Remember that performance on training data isn't interesting. It's all about the ability to generalize to unseen data. If you've never drawn out what a couple of 2x2 filters do to a 3x3 image, I suggest drawing out what happens. CNNs are typically thought of in terms of putting squares on top of bigger squares. However, they're absolutely neural networks with predictable numbers of parameters and nodes. As an exercise, if you have two 2x2 filters and a 3x3 image, how many nodes are in the next layer, and how many parameters are there? You can check your answer in Keras.
