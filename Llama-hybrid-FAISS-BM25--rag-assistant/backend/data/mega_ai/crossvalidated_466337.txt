[site]: crossvalidated
[post_id]: 466337
[parent_id]: 466336
[tags]: 
This is not a rigorous answer, but just to try to give some intuition behind the concept of early stopping. We know that neural networks have a very high flexibility, which means that they can model extremely complex functions. The more complex a function gets, the more likely is that it will overfit the data. Searching for that perfect minimum increases the complexity of the function. You start with a relatively smooth function and start tweaking every point of it until it gets you the best performance on the training data. Which means you have overfitted it. Instead, you can stop early, to a simpler function. Think of it as something in between a smooth function and a highly complex function. It still gets the general shape you want, but didn't have time to fine tune all the small variations needed for a perfect error.
