[site]: crossvalidated
[post_id]: 591820
[parent_id]: 
[tags]: 
Distributions as Features in Machine Learning

The Problem Let's assume I have a problem that seems perfect for supervised learning. However, some of the measurements I would like to use as features are not point estimates but are instead distributions representing my uncertainty about the measurement e.g. the output of a Bayesian model. How do I use these distributions when training a model and later when I want to make predictions? Is it possible to propagate the uncertainty of the features to the uncertainty of the prediction? What I've Considered I've considered a few possibilities but none of them feel ideal: Use the Expected Value . Create a point estimate from each distribution and use that scalar in the feature vector. This feels like I am throwing away valuable information e.g. it will treat N(3, 1) the same as N(3, 10) Use Percentiles . Choose a set of percentiles (50, 97.5, etc.) and create a feature for each. Use Monte Carlo . Build the feature vector by sampling from the distributions, creating multiple feature vectors for each observation. Not performant and I'm not certain this will produce meaningful results Use Distribution Parameters Create a feature for each parameter in the distribution e.g. mean and variance for Normal.
