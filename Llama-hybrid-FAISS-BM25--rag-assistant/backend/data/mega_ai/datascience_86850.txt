[site]: datascience
[post_id]: 86850
[parent_id]: 86822
[tags]: 
Neural Networks are notoriously good at performance and bad at interpretability, i.e. it's very difficult (almost impossible) to explain why a particular prediction was made. It's even more difficult to link the prediction with the features, since the NN does a lot of intermediate calculations where all the features play a role. The notoriously good models for interpretability are the simple ones: a decision tree is very intuitive to interpret, assuming the tree is not too large. A human can actually follow the tree based on the conditions on the features.
