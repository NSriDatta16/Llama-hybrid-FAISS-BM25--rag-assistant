[site]: datascience
[post_id]: 43526
[parent_id]: 43517
[tags]: 
The output for every sentence is some floating-point number, representing some value that has been calculated beforehand (...) the important part is that every input is a sentence mapped to a number. In essense this is a regression problem, i.e. given some input predict the (most likely) numeric output. However you could also see this as a classification problem if you are not interested in real numbers but in some range (=class), or a probability for each class. RNNs are popular in NLP so I am leaning towards them, RNNs are well suited for sequence processing. The way you present the problem does not seem like there is much sequence processing involved - while you can get this to work with RNNs, I propose to try other less computationally involved approaches first. I have looked for RNNs and regression but I can only find use cases involving time series, not NLP or one-hot encoded features. Unless there is some specific requirement to use RNNs or in extension LSTMs, I would try random forests, e.g. the RandomForestRegression regressor from scikit-learn. This should work reasonably well using the one-hot encoded sentences (as outlined in the question) as X and the pre-calculated number as Y. Should you decide to instead make this a classificastion problem, you can easily switch to the RandomForestClassification classifier from the same package. an unseen given sentence can be pre-processed and one-hot encoded and given as input, and that an output (floating-point number) is predicted I suggest to use a Pipeline for both training and prediction, then serve this Pipeline to your application.
