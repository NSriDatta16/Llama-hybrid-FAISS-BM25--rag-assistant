[site]: crossvalidated
[post_id]: 462950
[parent_id]: 
[tags]: 
How is MCMC relevant for Bayesian inference?

I understand how MCMC can be useful for approximating the shape of a density function if you're able to generate random samples from that density function. However, I don't understand how this is applied towards Bayesian inference because any specific selection of model parameters using MCMC would yield a number, not a distribution. For example, if I have a binomial likelihood function and a beta prior, I might use Metropolis-Hastings to select a random starting parameter, $\theta=.3$ . When that starting parameter and some data are input into my likelihood function and prior, I get a deterministic number as the output. No randomness involved. Framing this issue another way: Why does the MH algorithm generate the posterior distribution from the count of visits to a parameter value rather than using the posterior values generated directly?
