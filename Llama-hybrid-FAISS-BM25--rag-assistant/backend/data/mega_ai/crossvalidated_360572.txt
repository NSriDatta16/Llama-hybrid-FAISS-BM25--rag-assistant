[site]: crossvalidated
[post_id]: 360572
[parent_id]: 360326
[tags]: 
Assuming you have no ground truth labels about how similar the voice sequences are, you would probably want to map the variable length voice sequences to a vector representation of dimension d where similar sequences are located near eachother. One would then simply check how similar sequences are by measuring, for example, the cosine similarity. Given the fact you have a variable sequence as input but want to output a fixed length, you might want to use LSTM's to achieve this. I'm not sure this is the best approach but one could imagine using an LSTM autoencoder to create a fixed length vector representation (the hidden state of the LSTM encoder after feeding an input through it). The only problem is that autoencoders don't tend to always map similar sequences to similar locations in such a space. Therefore you might think about using models such as Adversarially Regularized Autoencoders, which have the benefit of mapping the input to a smooth representation space, where similar inputs are near eachother.
