[site]: crossvalidated
[post_id]: 580681
[parent_id]: 580647
[tags]: 
Relationship with bayesian inference. To make it complicated: regression towards the mean can be related to Bayesian inference. If we estimate an $\alpha \%$ confidence interval for some parameter then this confidence interval will not contain the true parameter with $\alpha \%$ probability when we condition on the observation (see Why does a 95% Confidence Interval (CI) not imply a 95% chance of containing the mean? ). On the other hand, an $\alpha \%$ credible interval will contain the parameter with $\alpha \%$ probability (if the prior is correct/reasonable) and it does this by making estimations that are closer to the population mean. Some critical notes to your dice example If we observed an outcome that had a very low probability of happening, then the next time we do the same thing again, the outcome will not be as extreme. To me this is self-evident, since by definition, the first event was unlikely. If a bunch of people throw a "gaussian die" with values 1-10, and some throw 1 or 10, you don't expect them to repeat it the next time for the same reason you didn't expect them to do it the first time. In your example you are not speaking about parameter estimates or predictions based on parameter estimates. You have a random dice roll. It is obvious that it will be different every time. Instead, you should have this Gaussian dice roll be depending on some parameter, let the rolls from a dice be distributed as $\mathcal{N}(\mu,1)$ and this parameter $\mu$ is unknown . If somebody rolls 10, what will be your guess for the next roll by the person? Probably it should be close to $10$ again, but if you know the distribution of $\mu$ for the population then you can make a much better estimate by using a Bayesian estimate (which will be something that is closer to the population mean of $\mu$ ). The dice roll example that you used, that would make students understand that if we got an observation above $\mu$ then the next time the observations will be likely lower, and if we got an observation below $\mu$ then the next time the observations will be likely higher. But what sort observation did we have. Did we observe above or below $\mu$ ? With the dice example it makes sense that the observations shift towards the parameter $\mu$ associated with the individual . But why should it be such that our estimates regress toward the population mean? If the parameter $\mu$ happens to be above the population mean, then why should we get more often an observation like $\mu+10$ and overestimate instead of an observation like $\mu-10$ and have a mean that is underestimated? The dice example explains why extreme observation will move towards the mean of an individual $\mu$ . We can have observations above $\mu$ and observations below $\mu$ like with your dice. But the dice example does not explain why these extreme observations are more often away from the population mean, such that we observe regression towards the population mean. Regression away from the mean To show how it can be more subtle, it is possible to create an example where there is regression away from the mean conditional on the measurement. Let $\mu_i$ be a mixture distribution with fifty fifty two Gaussian distributions, one with mean -5 and the other with mean 5 and deviation 1. Let a first observation $x_i$ and a second observation $y_i$ be distributed as Gaussian with mean $\mu$ and deviation 1. Then for a first observation between -2 and 2, there will be mostly regression away from the mean.
