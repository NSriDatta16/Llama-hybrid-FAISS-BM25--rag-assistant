[site]: crossvalidated
[post_id]: 152054
[parent_id]: 
[tags]: 
estimates for least squares vs estimates for ridge

The coefficient estimates for RSS is given $\hat{\beta} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}$, while for ridge regression, $\hat{\beta}^\text{ridge} = (\mathbf{X}^T\mathbf{X} + \lambda \mathbf{I})^{-1}\mathbf{X}^T\mathbf{Y}$. In Elements of Statistical Learning, p64, there is a claim at the end of the paragraph that for the case of orthogonal inputs, $\hat{\beta}^\text{ridge} = \dfrac{\hat{\beta}}{1+\lambda}$. I am not sure how this result is proved, and what is the advantage of using this form?
