[site]: crossvalidated
[post_id]: 488426
[parent_id]: 488421
[tags]: 
I would use multiple imputation here, on the whole dataset, to produce several complete datasets. A rule of thumb is to use the average % of missing data as the number of imputed datasets to create - so if you have 30% missing data on average, then impute 30 complete datasets. Then run each of your models on all off the datasets, pool the results, and then use the usual methods for comparing models. Many packages that perform multiple imputation can handle the process including fitting the final analysis models and the pooling automatically once you specify the imputation model (that is, the way that the variables are related, and even this can be automated to a large extent). If the data are missing completely at random (MCAR), or just missing at random (MAR) then a suitable imputation model can result in unbiased and more precise estimates. If you just delete missing cases then if the data are MAR you will likely produce biased as well as less precise estimates. If they are MCAR then you will produce less precise estimates.
