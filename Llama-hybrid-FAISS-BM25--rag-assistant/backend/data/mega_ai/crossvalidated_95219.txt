[site]: crossvalidated
[post_id]: 95219
[parent_id]: 95212
[tags]: 
Random forests should be able to handle categorical values natively so look for a different implementation so you don't have to encode all of those features and use up all your memory. The problem with high cardinality categorical features is that it is easy to over fit with them. You may have enough data that this isn't an issue but watch out for it. I suggest looking into random forest based feature selection using either the method Breiman proposed or artificial contrasts . The artificial contrasts method (ACE) is interesting because it compares the importance of the feature to the importance of a shuffled version of itself which fights some of the high cardinality issues. There is a new paper "Module Guided Random Forests" which might be interesting if you had many more features as it uses a feature selection method that is aware of groups of highly correlated features. Another sometime used option is to tweak the algorithm so it uses the out of bag cases to do the final feature selection after fitting the splits on the in bag cases which sometimes helps fight overfitting. There is an almost complete ace implementation here and I have a more memory efficient/fast RF implementation that handles categorical variables natively here ...the -evaloob option supports option 4 I'm working on adding support for ACE and a couple of other RF based feature selection methods but it isn't done yet.
