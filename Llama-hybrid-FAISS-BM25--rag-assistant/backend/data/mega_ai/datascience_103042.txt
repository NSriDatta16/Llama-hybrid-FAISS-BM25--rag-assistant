[site]: datascience
[post_id]: 103042
[parent_id]: 103011
[tags]: 
I believe that your general problem is a sequential problem and it could be tackled with RL or other optimization methods. As you suggested the simulator (environment) is real life therefore you should look at offline RL methods (recent review here ). The main objective of offline RL is to tackle the problem of applying RL in real life without having a simulator and use collected data. In other words, we want to avoid the risk of deploying a RL algorithm that hasn't interacted with the real MDP. Additionally, I would suggest you as a starting point to do some analysis. For example, detect which actions immediately have impact to the CLV and which are not. Which features are important? You could formulate the problem as a Supervised Learning problem (features --> improve/don't improve/same). You could do some clustering of your customers depending on the features. You might be able to come up with a custom CLV to optimize and run small scale experiments without the need of learning algorithms and you could potentially use as baselines later on. Then you could also run small experiments in Bandit settings (no sequential aspect of your problem here as credit assignment is tough!). You can do all these on parallel with your exploration in offpolicy RL methods. Please note that eventually careful feature selection, problem formulation and data analysis are keys to the success of your implementation.
