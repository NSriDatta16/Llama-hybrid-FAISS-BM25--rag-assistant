[site]: crossvalidated
[post_id]: 627006
[parent_id]: 532073
[tags]: 
I'm more an expert on the gradient descent side, but here's a couple thoughts. Tim mentions that BO is approximating the function, but gradient descent does this as well, by framing it as an optimization problem: We approximate the function by minimizing the error in our approximation. We now also have a host of sampling tricks to allow us to optimize the parameters of various distributions directly, a trend which IIUC started with variational autoencoders (the 'reparamaterization trick'). For example, we can model an arbitrary normal distribution as mu + sigma * N(0, 1) , which allows us to keep the gradient for mu and sigma, where it's lost if we simply write N(mu, sigma) . These sorts of tricks allow us to apply gradient descent in a lot of contexts where we couldn't ten years ago. This /should/ let us abstract away the method for finding the parameters of the model and focus on the model itself. In theory, I shouldn't have to care whether the model was fit with GD or MCMC, though in practice I find MCMC to be fickle, difficult to reason about, and difficult to debug. (but I imagine there's a bunch of folks who say the same about GD.)
