[site]: crossvalidated
[post_id]: 626203
[parent_id]: 626109
[tags]: 
First off, I would be very careful about interpreting the RMSE as a "weighted average error". Yes, larger errors add more to the RMSE than smaller ones... But it's simply not the case that the RMSE is a weighted sum of some quantities that are errors in themselves. (The wMAPE, in contrast, can be so interpreted, Kolassa & Sch√ºtz, 2007 , but it has its own issues.) Second, you can certainly scale the RMSE by some quantity and express the result as a percentage. It would then be a percentage of or with respect to that scaling denominator. As long as your denominator is positive, this is at least well-defined, but it may not give you the kind of information you or your audience needs. I see such "scaled" RMSEs more often referred to as, well, a "scaled RMSE". As identified in the comments, all your proposed candidates can result in errors that exceed 1, or 100% - just like the standard Mean Absolute Percentage Error can exceed 100%. This is often disconcerting to a non-technical audience. For the standard MAPE, this can be avoided by using a flat zero forecast (which is usually no use to anybody). Forecast accuracy measurement is unfortunately harder than it looks , especially if the "accuracy" is used as a target to be "improved", which can lead to all kinds of gaming - like setting forecasts to a hard zero per above. Forecasters have to educate their audience not to over-interpret errors, and to focus more on the implications of forecast errors (e.g., Kolassa, 2023 ). Yes, this is often hard, and very unintuitive. You may already have seen this: What are the shortcomings of the Mean Absolute Percentage Error (MAPE)?
