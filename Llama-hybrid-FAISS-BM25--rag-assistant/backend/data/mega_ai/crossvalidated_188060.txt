[site]: crossvalidated
[post_id]: 188060
[parent_id]: 188040
[tags]: 
Lately there has been some work on why deep learning is successful. Loss Surfaces of Multilayer Networks Identifying and attacking the saddle point problem in high-dimensional non-convex optimization are some good references, and of course the references therein. Roughly it seems that high dimensional optimization problems, which frequently occur in deep learning, are hypothesized to have better optimization landscapes. For example, consult Figure 3 in the first link. Notice that the local minima obtained by wider MLPs (more parameters, higher dimensional optimization problem) have their local minimas banded much closely together than with the thinner nets. Further note that they did an experiment similar to yours, i.e. n = 100 hidden units and they have quite a wide band in local minima values. That's from a theory perspective. From a computational perspective, can you try bumping up the number of hidden units, and perhaps even the number of hidden layers? You might start to see that training the network is less and less sensitive to your step size (within reason of course, choosing step size 1e+10 is probably a bad idea). Geometrically, perhaps shallower/thinner networks have much more "treacherous" optimization landscapes and can get caught in these sorts of local minima.
