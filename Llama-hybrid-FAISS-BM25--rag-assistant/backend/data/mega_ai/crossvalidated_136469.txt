[site]: crossvalidated
[post_id]: 136469
[parent_id]: 136373
[tags]: 
Depending on which neural net (NN) you are using and what you are doing, NN tries to minimize some cost function $f(W)$ where $W$ are weights. For example, for autoencoders the cost function involves reconstruction error. If you are doing logistic regression, the cost function would be related to classification error. NN tries to find a $W$ that minimizes the cost. Generally in optimization two quantities are monitored for convergence: $f(W)$ and $W$. If you observe either your cost $f$ or your parameter $W$ does not vary much from one iteration ($iter-1$) to the next ($iter$), the convergence has happened and the loop stops: $|f(W_{iter}) - f(W_{iter-1})| OR $||W_{iter} - W_{iter-1}||
