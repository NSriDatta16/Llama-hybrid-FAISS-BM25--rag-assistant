[site]: datascience
[post_id]: 121836
[parent_id]: 23384
[tags]: 
In the context of stochastic gradient descent (SGD) in neural networks, the term "stochastic" refers to the randomness introduced during the weight updates. Unlike traditional gradient descent, which computes the gradient using the entire dataset, SGD updates the weights based on a random subset of the training data, known as a mini-batch. This introduces stochasticity or randomness into the optimization process. The randomness arises from the random selection of mini-batches from the training data. Each mini-batch represents a random sample of the dataset, and the gradients computed on these mini-batches are used to update the weights. By randomly sampling mini-batches, SGD introduces variations in the estimated gradients, which can help the optimization process escape local minima and explore different regions of the weight space.
