[site]: datascience
[post_id]: 63099
[parent_id]: 
[tags]: 
SGDClassifier partial_fit() for online learning - is one step of gradient descent enough?

I'm interested in incremental (online) learning for my logistic regression model trained with SGDClassifier . Basically updating the model as more labeled data comes in. I know I can use partial_fit() for this. The following seems to be a common example on StackOverflow: clf = linear_model.SGDClassifier() x1 = some_new_data y1 = the_labels clf.partial_fit(x1,y1) x2 = some_newer_data y2 = the_labels clf.partial_fit(x2,y2) The problem is, partial_fit() performs just one step of gradient descent : Internally, this method uses max_iter = 1 . Therefore, it is not guaranteed that a minimum of the cost function is reached after calling it once. Matters such as objective convergence and early stopping should be handled by the user. Would one step be enough? How do I know if I don't need to make more steps than just one per each new labeled datapoint / minibatch that comes in?
