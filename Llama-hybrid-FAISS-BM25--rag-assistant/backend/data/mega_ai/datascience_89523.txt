[site]: datascience
[post_id]: 89523
[parent_id]: 89522
[tags]: 
PCA reduces dimensionality by generating orthogonal (i.e. uncorrelated) "new" features from the original features. This can be very useful, e.g. when you do linear regression. So principle components "transform" the original features to orthogonal ones. You can see this as some kind of feature engineering. Usually PCA will reduce the number of features which can be very helpful. See Ch. 10.2 in ISL . Feature selection basically means to choose features which perform well in some model (e.g. have high explanatory power) or exclude features which are noisy. This can be any kind of features (transformed or original features). So feature selection is a very broad term and may be linked to various different methods.
