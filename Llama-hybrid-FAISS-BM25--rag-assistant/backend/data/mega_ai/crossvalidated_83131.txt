[site]: crossvalidated
[post_id]: 83131
[parent_id]: 
[tags]: 
How much smaller can $p$ values from ANOVA's $F$-test be vs. those from multiple $t$-tests on the same data?

Intro: Having noted the attention received today by this question, " Can ANOVA be significant when none of the pairwise t-tests is? ," I thought I might be able to reframe it in an interesting way that would deserve its own set of answers. A variety of incongruous results (at face value) can occur when statistical significance is understood as a simple dichotomy and judged on the mere basis of which is higher, the $p$ or the $\alpha$. @Glen_b's answer to the above question presents a useful example of a case where: An ANOVA $F$-test produces a $p_F $p_t>.08$ for all two-sample $t$-tests that compare differences in the same dependent variable (DV) among observations corresponding to each pair of the IV's four levels. A similar case arose despite Bonferroni corrections for post-hoc pairwise comparisons via this question: Anova repeated measures is significant, but all the multiple comparisons with Bonferroni correction are not? Previously mentioned cases with a slightly different test in multiple regression also exist: Why is it possible to get significant F statistic (p : $p_F .09$ How can a regression be significant yet all predictors be non-significant? In @whuber's answer , $p_F=.0003,p_{\beta t}>.09$ I bet that in cases like these, some (but not all) pairwise comparisons' (or regression coefficients' significance tests') $p$ values must be fairly close to $\alpha$ if a corresponding omnibus test can achieve a $p . I see this is the case in @Glen_b's first example, where $F_{(3,20)}=3.19$, $p_F=.046$, and the largest pairwise difference gives the smallest $p_t=.054$. Must this be the case in general? More specifically : Question: If an ANOVA $F$-test produces a $p_F=.05$ for one polytomous IV's effect on a continuous DV, how high could the lowest $p$ value be among all two-sample $t$-tests that compare each pair of the IV's levels? Could the minimum pairwise significance be as high as $p_t=.50$? I welcome answers that address only this specific question . However, to further motivate this question, I'll elaborate and throw in some potentially rhetorical questions. Feel welcome to address these concerns as well, and even to ignore the specific question if you like, especially if the specific question gets a definitive answer. Significance: Consider how much less important the difference between a $p_F=.04$ and a $p_t=.06$ would be if statistical significance were judged in continuous terms of the strength of evidence against the null hypothesis (Ron Fisher's approach, I think?), rather than in dichotomous terms as above or below an $\alpha=.05$ threshold for acceptable probability of error in choosing whether to reject the null wholesale. " $p$-hacking " is a known problem that partly owes its notoriety to an unnecessary vulnerability introduced by interpretation of $p$ values according to the common practice of dichotomizing significance into the equivalents of "good enough" and "not good enough." If one were to dispose this practice and focus instead on interpreting $p$ values as strength of evidence against the null on a continuous interval, might omnibus testing be somewhat less important when one really cares about multiple pairwise comparisons? Not useless necessarily, as any reasonably efficient improvement in statistical accuracy is of course desirable, but...if, for instance, the lowest pairwise comparison's $p$ value is necessarily within $.10$ of the ANOVA (or other omnibus test) $p$ value, doesn't this make the omnibus test somewhat more trivial, less compulsory, and even more misleading (in conjunction with preexisting misunderstandings), especially if one doesn't particularly want to control $\alpha$ across multiple tests? Conversely, if data may exist such that an omnibus $p=.05$, but all pairwise $p>.50$, shouldn't this further motivate omnibus and contrast testing throughout practice and pedagogy? It seems to me that this issue should also inform the relative merits of judging statistical significance according to a dichotomy vs. a continuum, in that the dichotomous interpretive system should be more sensitive to small adjustments when differences are "marginally significant", whereas neither system is safe from a failure to perform an omnibus test or adjust for multiple comparisons if this difference / adjustment can be very large (e.g., $p_t-p_F>.40)$ in theory. Other optional complexities to consider or ignoreâ€”whatever makes answering easier and more worthwhile : How high $p$s for $t$s could be if, for $F$, $p Sensitivity to number of levels in a polytomous IV Sensitivity to unevenness in significance of pairwise differences (while all $p_t>p_F$) whuber's answer indicates that including small differences can mask big differences. Differences among various omnibus tests' corrections for multiple comparisons See also: Correcting for multiple comparisons in a within subjects / repeated measures ANOVA; excessively conservative? With multiple IVs, it seems multicollinearity can exacerbate this issue . Restricted cases where data meet all assumptions of classic parametric tests optimally This restriction may be important to prevent this question from being somewhat moot.
