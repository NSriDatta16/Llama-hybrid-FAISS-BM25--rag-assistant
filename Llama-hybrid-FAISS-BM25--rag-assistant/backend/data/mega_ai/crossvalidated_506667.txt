[site]: crossvalidated
[post_id]: 506667
[parent_id]: 
[tags]: 
How to make the best final prediction of the optimum value in a Bayesian optimization process?

I'm trying to understand the process of Bayesian optimization of a black box function and the bit I'm confused about is how to make the very last prediction of the true maximum after you have made all the function evaluations you can afford to make. I reviewed the code for two Python implementations: Bayesian Optimization: Open source constrained global optimization tool for Python How to Implement Bayesian Optimization from Scratch in Python by Jason Brownlee and in both, the final estimate is simply whichever parameter values resulted in the highest previous actual function value. My question is, would it not be better to find the maximum of the final gaussian process model (the maximum mean prediction) and use that as the best estimate of the peak of the true function? To illustrate the question, here is the result of Bayesian optimization of a simple 1-D function after 7 steps using the bayes_opt package noted above. Full code is here if you need to see it. The best estimate based on the highest actual function evaluation so far is: sample_max_x = optimizer.max['params']['x'] sample_max_x, function_to_maximize(sample_max_x) # (2.203, 5.574) The best estimate based on the peak of the current GP model is: mu, sigma = posterior(optimizer, x_obs, y_obs, grid=X.reshape(-1, 1)) posterior_max_x = X[mu.argmax()] posterior_max_x, function_to_maximize(posterior_max_x) # (2.475, 5.713) The true maximum is (2.509, 5.714), so obviously, in this case taking the maximum of the GP model is the best estimate but is it always a good idea? Or is there too much risk given the uncertainty that the maximum of the GP model might be far worse than the best sample so far?
