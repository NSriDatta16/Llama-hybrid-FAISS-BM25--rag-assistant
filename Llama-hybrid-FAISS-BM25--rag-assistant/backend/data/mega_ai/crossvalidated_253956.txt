[site]: crossvalidated
[post_id]: 253956
[parent_id]: 253937
[tags]: 
For another view of linear separability (assuming that's what linear features means) imagine you have a simple dataset with sex as a binary, categorical covariate and results as a binary outcome of some experiment. sex outcome m 1 m 1 m 0 f 0 f 0 f 0 For binary response data (i.e. like this), it's common to use a logistic regression to model, among other things, the probability of outcome = 1 given sex . However, what if your input was sex = f . Then: $$P(\text{outcome} = 1 | \text{sex} = f) = 0$$ because we have no training examples of this ever happening. In this way, we say that the data is linearly separable per @hxd1011's image above. In fact, if you tried to fit a logistic regression you'd likely get an error because the MLE estimates tend to infinity (unless your computer software has a parameter which stops the algorithm, usually IRLS , from continuing to looks for a minimum). If you get data like this and want to use a logistic regression, you can look into penalized logistic regression . Here is nice write up about it.
