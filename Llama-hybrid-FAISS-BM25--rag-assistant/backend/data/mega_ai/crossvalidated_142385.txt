[site]: crossvalidated
[post_id]: 142385
[parent_id]: 
[tags]: 
Machine Learning

I have been working on some self study "machine learning". Based on a few posts here, I wanted to make a program that "learned" via Bayes Law. I test it with some simple truth tables. It recalls the past training data well. I note that some machines are able to make inferances in new situations, to different degrees. My particular version cannot do so, which raises a question. Perhaps it doesn't learn, perhaps it only regurgitates. My question is: In a broader, philosophical sense, does a program (any program) still qualify as "learning" if it cannot infer about things that it has not seen historically? What are the bounds on such things?
