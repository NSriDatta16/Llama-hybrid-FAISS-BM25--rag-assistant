[site]: crossvalidated
[post_id]: 202202
[parent_id]: 201919
[tags]: 
Here is another, more indirect, but I believe interesting one, namely the connection between different approaches to computing the partial autocorrelation coefficient of a stationary time series. Definition 1 Consider the projection \begin{equation} \hat{Y}_{t}-\mu=\alpha^{(m)}_1(Y_{t-1}-\mu)+\alpha^{(m)}_2(Y_{t-2}-\mu)+\ldots+\alpha^{(m)}_m(Y_{t-m}-\mu) \end{equation} The $m$th partial autocorrelation equals $\alpha^{(m)}_m$. It thus gives the influence of the $m$th lag on $Y_t$ \emph{after controlling for} $Y_{t-1},\ldots,Y_{t-m+1}$. Contrast this with $\rho_m$, that gives the `raw' correlation of $Y_t$ and $Y_{t-m}$. How do we find the $\alpha^{(m)}_j$? Recall that a fundamental property of a regression of $Z_t$ on regressors $X_t$ is that the coefficients are such that regressors and residuals are uncorrelated. In a population regression this condition is then stated in terms of population correlations. Then: \begin{equation} E[X_t(Z_t-X_t^\top\mathbf{\alpha}^{(m)})]=0 \end{equation} Solving for $\mathbf{\alpha}^{(m)}$ we find the linear projection coefficients \begin{equation} \mathbf{\alpha}^{(m)}=[E(X_tX_t^\top)]^{-1}E[X_tZ_t] \end{equation} Applying this formula to $Z_t=Y_t-\mu$ and $$X_t=[(Y_{t-1}-\mu),(Y_{t-2}-\mu),\ldots,(Y_{t-m}-\mu)]^\top$$ we have $$ E(X_tX_t^\top)=\left(\begin{array}{cccc} \gamma_{0} & \gamma_{1}&\cdots& \gamma_{m-1}\\ \gamma_{1}& \gamma_{0} & \cdots &\gamma_{m-2}\\ \vdots & \vdots & \ddots &\vdots\\ \gamma_{m-1}&\gamma_{m-2} & \cdots & \gamma_{0}\\ \end{array} \right) $$ Also, $$ E(X_tZ_t)=\left( \begin{array}{c} \gamma_1 \\ \vdots \\ \gamma_m \\ \end{array} \right) $$ Hence, \begin{equation} \mathbf{\alpha}^{(m)}=\left(\begin{array}{cccc} \gamma_{0} & \gamma_{1}&\cdots& \gamma_{m-1}\\ \gamma_{1}& \gamma_{0} & \cdots &\gamma_{m-2}\\ \vdots & \vdots & \ddots &\vdots\\ \gamma_{m-1}&\gamma_{m-2} & \cdots & \gamma_{0}\\ \end{array} \right)^{-1}\left( \begin{array}{c} \gamma_1 \\ \vdots \\ \gamma_m \\ \end{array} \right)\end{equation} The $m$th partial correlation then is the last element of the vector $\mathbf{\alpha}^{(m)}$. So, we sort of run a multiple regression and find one coefficient of interest while controlling for the others. Definition 2 The $m$th partial correlation is the correlation of the prediction error of $Y_{t+m}$ predicted with $Y_{t-1},\ldots,Y_{t-m+1}$ with the prediction error of $Y_{t}$ predicted with $Y_{t-1},\ldots,Y_{t-m+1}$. So, we sort of first control for the intermediate lags and then compute the correlation of the residuals.
