[site]: crossvalidated
[post_id]: 168999
[parent_id]: 168996
[tags]: 
One possible approach which is statistically sound is to build an ensemble of models using only subset of features (random) in each simple one. For example consider building something like random forest, but each tree can only use 80% of features (classical RF removes features in each node, and I am talking here about removing them globally). Then, once you train your ensemble, and you get a testing point which does not have some features - you simply use only these models from the ensemble, which are consistent with observation (were trained on features which are available). Given enough weak learners in ensemble this should work well.
