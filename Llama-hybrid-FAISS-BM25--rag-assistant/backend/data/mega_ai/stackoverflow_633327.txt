[site]: stackoverflow
[post_id]: 633327
[parent_id]: 633211
[tags]: 
In general, I consider a number of points when talking about scale: How often does it get executed? For less frequently accessed queries you may be able to accept some degradation in performance. What's the rate of growth / change? If the records are relatively static in some of those tables, you may want to consider caching the contents externally in a dbm-type of file (or whatever the windows equivalent is). There's also things like memcache that might be worth looking at. This may or may not be possible, though. This is predicated on performing "joins" in application code. Profile. If you're joining on indexed columns (and you are, aren't you?), you're not necessarily going to degrade as the number of rows grow. This is going to depend heavily on whether you're dealing with 1:1 or 1:N relationships, what the average size of N is, how much available memory you have available on the database server, how often your table statistics are computed, and the type of columns and indexes. If you're dealing with a 1:1 relationship and it's unique, the database is going to be able to do a simple hash and look up. Make sure you limit the columns fetched to absolutely no more than what you need, especially when joining many tables, because if all that is required to join two tables are the columns that are indexed, the database may not even consider the table at all; the join can be performed using just the indexes. This reduces contention and improves the performance of less optimal queries that need to deal with the actual contents of the table because there's fewer queries pulling on the table. All relational databases have a tool or feature to view the query execution plan for the given query. Use it. If the output doesn't make sense to you, learn it. This is your primary window into understanding what the database will do with a given query, what indexes will be used, what the estimated (or actual) number of rows that will be encountered in each step of execution, and other fun stuff. Once you have information about what the query optimizer is actually doing with the query, and you have all your indexes / statistics / column selection straight, you'll have a better idea of where to go from there. If you do all you can in the database, you'll have to look at using caches of the data and doing things like going at fewer tables with a more specific / better where clause. Disclaimer: I have no direct experience with SQL Server, but I have a lot of experience on other RDBMSs (Oracle, MySQL, PostgreSQL, etc.) and architecture in general.
