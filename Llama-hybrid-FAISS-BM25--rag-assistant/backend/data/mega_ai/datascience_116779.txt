[site]: datascience
[post_id]: 116779
[parent_id]: 116761
[tags]: 
Ideally, I would address the extreme imbalance first to get a more reliable score, try oversampling or under (I prefer over). Then try with conservative parameters, like max_depth only until 10. I prefer to use XGBoost over Random Forest though, it has an early stopping parameter to help prevent overfitting, although one still has to be conservative especially in the max_depth. Then I use Optuna for hyperparameter tuning, it can also be used for Random Forest and is so much faster than GridSearchCV (especially with a huge dataset).
