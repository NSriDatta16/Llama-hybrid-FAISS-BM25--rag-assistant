[site]: crossvalidated
[post_id]: 361441
[parent_id]: 361435
[tags]: 
I cannot say I fully understand your pseudocode, however, the usual procedure is following: Test set : Take a part of your data (if needed, use stratified sampling or similar technique to ensure this test set is a good representative of your data) and put it aside . Do not use this data for learning nor model selection. Parameter tuning : Use k-fold cross validation to find the best parameters. Training : Train the model with parameters selected in step 2 on the whole dataset. Testing : See how your model performs on the testing data put aside in step 1. I hope this should answer both of your questions. But, more specifically and using some fancy-like math notation: Split your training data $D$ into $K$ splits, $\{D_k; k\in[1;K]\}$. Repeat $K$-times: (kinda for k in range(K) ) a. Train the network using splits $\bigcup D_{j; j\neq k}$ as training data. b. Evaluate the trained network on the remaining part, $D_k$. Do not use this part for evaluation after each epoch. If you want to do early stopping, define yet another validation set for each $\bigcup D_{j; j\neq k}$. I think your second code sample reflects this. Regarding your second question, always keep a separate test set. Also, this is a very common topic on this site. See some related answers: Neural network training without early stopping Choosing best training-validation split? Is epoch optimization in CV with constant mini-batch size even possible? Should I use the same weight initialization for each fold in cross validation?
