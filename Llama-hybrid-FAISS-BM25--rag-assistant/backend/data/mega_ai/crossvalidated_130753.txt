[site]: crossvalidated
[post_id]: 130753
[parent_id]: 
[tags]: 
Fast Algorithm for Bayesian Measurement Model

I want to estimate a Bayesian Measurement model. That is I am concerned with the rating of each judge $j$ of the value of some trait $z$ for each observation $i$. Not all raters will have rated each observation and some raters' ratings will be continuous and others will be ordinal. My intention is to set this up as a multivariate ordered probit model (c.f. Johnson and Albert (1999), http://goo.gl/Ty3G2d ). At least initially, I will discretise the continuous variables so that they too can be analysed using an ordered probit. Thus, I want to analyse a model in which each judge perceives the true value plus an idiosyncratic error, and the variance of these errors varies by judge: $$X_{ij} = z_{i}+e_{ij}\quad e_{ij}\stackrel{\text{iid}}{\sim} N(0; \sigma^{2}_{j})$$ where $x_{ij}$ is a latent variable and judges instead report the ordinal variable $y_{ij}$ where the vector $\gamma$ defines the cutpoints that maps $x_{ij}$ on to $y_{ij}$. This gives the following Likelihood Function: $$L(z,\gamma,\sigma)=\prod_{i=1}^{n}\prod_{j}[\Phi(\frac{\gamma_{j}-z_{i}}{\sigma_{j}})^{y_{ij}}\Phi(\frac{-\gamma_{j}+z_{i}}{\sigma_{j}})^{1-y_{ij}}]$$ Johnson and Albert suggest priors such that the joint prior density of $(z,\gamma,\sigma)$ is: $$g(Z,\gamma,\sigma)=\prod_{i=1}^{n}\phi(z_{i};0,1)\prod_{j}g(\sigma^{2}_{j};\lambda,\alpha)$$ where $\phi(x;0,1)$ denotes the standard normal density. And $g(z;\alpha,\lambda)$ denotes the density of an inverse gamma distribution with parameters $\alpha$ and $\lambda$. My Question I would be interested in any existing packages that are known to be reliable and can handle this problem. My preference would be R, but Matlab or Python would be fine. Other languages were they faster would also be fine. Others (e.g. Albert and Johnson (1999) and Jackman (2004)) have used MCMC to approximate the resulting distribution. My particular problem is that $j$ is going to be relatively large, in the range 30-40, and whilst it could potentially be reduced I want to maximise the value that can be handled. Thus, I am keen to learn of algorithms/implementations fast enough that I can estimate a problem of this size I have access to HPC facilities and GPUs if parallelisation were an option. I would also be pleased to learn of any computational issues that I should be aware of.
