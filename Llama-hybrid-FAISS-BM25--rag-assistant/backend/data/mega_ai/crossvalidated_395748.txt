[site]: crossvalidated
[post_id]: 395748
[parent_id]: 395008
[tags]: 
In addition to Neil's answer : Your observation would be valid for any deterministic target policy (where all actions but one have a 0 probability of occurrence), not just the greedy policy. For such target policies, the only cases where the importance-sampled return will be non-zero is when the behavior policy follows a trajectory that exactly matches one that the target policy would follow. The probability of following such a trajectory is non-zero if all actions possible under target policy have non-zero probability of occurrence in behavior policy. However for longer trajectories the probabilities of choosing the correct action at each step will multiply to result in an extremely small probability of following the target policy trajectory completely. Hence most of the returns observed will be 0. Say your behavior policy is $\epsilon$ -greedy. Now when you do follow the "correct" trajectory, the importance sampling weight will be $1/(1-\epsilon + \epsilon/n)$ when the greedy action as per your current value estimates is the correct action (same as target policy's choice), or $n/\epsilon$ when its not. These weights are quite large, and will be multiplied for each step. So we have 0 returns when you don't follow the same trajectory as target policy, and very large returns when you do. And these values are such that the average is guaranteed to converge to the true value. This is a good example of a low bias, high variance estimate. Refer to this lecture snippet on off-policy MC by Hado van Hasselt of DeepMind. This lecture series is a very good resource for RL in general.
