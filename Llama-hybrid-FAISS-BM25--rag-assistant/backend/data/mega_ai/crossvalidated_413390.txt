[site]: crossvalidated
[post_id]: 413390
[parent_id]: 408748
[tags]: 
This is not really an answer but rather a comment (don't have the rep yet) - in general, LDA is perfect for dimensionality reduction under the bag-of-words assumption. But you wouldn't really do that the way you suggested (or at least I haven't seen it done like this). Using LDA you would estimate two things - topics (that is distributions over the vocabulary) and the document specific mixing proportions (how much of each topic is there in each of the documents?). You can now use those mixing proportions as a lower dimensional representation of your documents (instead of using full vector of term counts for example). Another solution would be to look into word embeddings such as word2vec.
