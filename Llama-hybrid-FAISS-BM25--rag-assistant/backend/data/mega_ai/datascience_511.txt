[site]: datascience
[post_id]: 511
[parent_id]: 
[tags]: 
Cross-validation: K-fold vs Repeated random sub-sampling

I wonder which type of model cross-validation to choose for classification problem: K-fold or random sub-sampling (bootstrap sampling)? My best guess is to use 2/3 of the data set (which is ~1000 items) for training and 1/3 for validation. In this case K-fold gives only three iterations(folds), which is not enough to see stable average error. On the other hand I don't like random sub-sampling feature: that some items won't be ever selected for training/validation, and some will be used more than once. Classification algorithms used: random forest & logistic regression.
