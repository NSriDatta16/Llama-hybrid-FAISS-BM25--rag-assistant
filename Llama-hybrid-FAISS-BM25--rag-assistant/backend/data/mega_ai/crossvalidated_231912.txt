[site]: crossvalidated
[post_id]: 231912
[parent_id]: 
[tags]: 
Why is the output weights matrix initialized in a different way as the word embedding matrix in word2vec?

I was plagued by this problem while reading the tensorflow tutorial . There the word embedding is intialized as follows: embeddings = tf.Variable( tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)) while the output words matrix in a differenct way: nce_weights = tf.Variable( tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size))) nce_biases = tf.Variable(tf.zeros([vocabulary_size])) Why should the variance of the output weights distribution be limited to be within a range(and the range be 1.0 / math.sqrt(embedding_size)? Is it set to make the backpropagation tractable as here described ? In order to make the learning process tractable, it is a common practice to truncate the gradients for backpropagation to a fixed number (num_steps) of unrolled steps.
