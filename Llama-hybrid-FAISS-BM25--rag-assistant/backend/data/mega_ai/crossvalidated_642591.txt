[site]: crossvalidated
[post_id]: 642591
[parent_id]: 
[tags]: 
Closed Form Bayesian Updating with single observations of unknown variance to estimate mean

So here is another question regarding bayesian updating, which I try to understand. In my scenario, I sequentially process single pieces of information from an unknown distribution. I am interested in estimating the mean of this distribution . According to my readings , best would be, to know the variance of the observed distribution. Well, it would be better, to know the mean, but I don't know either, and (appart from maybe the need for the process of bayesian updating) I am also not interested in knowing the variance. So my questions: Do I need to estimate this variance, if it is not in my desired output? What is the downside on largly over/underestimating this variance, by just stating e.g. I assume it is something 50ish? If so, how would I do that, given that I process a single draw at a time. Would it help, if I introduced a short-term memory holding a few (maybe 1 or 2 of the past values) to estimate the variance of the samples? Now, what I read was , that I would need to estimate the variance using the inverse gamma distribution with two parameters $\alpha, \beta$ - which I assume I somehow need to have? What is the intuitive undestanding of those parameters in my scenario?
