[site]: crossvalidated
[post_id]: 363464
[parent_id]: 363418
[tags]: 
You've posted loss values from the second epoch of each network. I wouldn't read too much into the difference, since the loss of the 2-layer network at the second epoch is clearly not converged (it's 3.2, which is enormous for this flavor of language models). With loss that high, it's absolutely typical that the text it generates is nonsense. I recommend training the 2-layer network until its loss equals or beats the loss of the 1-layer network. It's generally true that it's harder to train a model with more parameters, so this may require some effort to achieve: tweaking the optimizer and learning rate, batch size, sequence lengths and so on. The primary benefit to a 2-layer LSTM network in this context is that it has more capacity to learn the language; in other words, the minimum loss that it obtains may be substantially lower than the minimum loss of the 1-layer network. On the other hand, this increase in complexity comes at the cost of increased computation time, and a harder optimization problem.
