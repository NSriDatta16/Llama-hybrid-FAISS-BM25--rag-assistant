[site]: crossvalidated
[post_id]: 552091
[parent_id]: 
[tags]: 
How to scale multidimensional time series data per group

I am dealing with panel data and want to scale it in order to use it for some ML models: id year A B C 1 2000 3,539,101 265.152 .0683649 1 2001 3,539.101 2,485.833 .0683649 1 2002 3,539.101 2,939.903 .0688288 1 2003 3,733.545 3,021.591 -.0257413 2 2000 3,960.184 9,418.228 .9781774 2 2001 3,960.184 9,418.228 .4855057 2 2002 3,960.184 9,880.249 .049056 2 2003 3,960.184 1,287.206 .2310434 3 2000 4,724.285 1,287.206 -.0373083 3 2001 4,724.285 1,582.817 .1202868 3 2002 4,724.285 1,279.348 -.1824576 3 2003 4,724.285 1,213.678 -.0513311 However, I'm not sure if I should scale per group (ID) or not. I'm using the following code: features = df.columns[2:5].tolist() df[features]=sklearn.preprocessing.minmax_scale(df[features], feature_range=(0, 1), axis=0, copy=False) Or should I do something like this instead: from sklearn.preprocessing import minmax_scale() df[features]=df.groupby("id")[features].transform(lambda x: minmax_scale(x.astype(float))) Or should I scale by year? df[features]=df.groupby("year")[features].transform(lambda x: minmax_scale(x.astype(float)))
