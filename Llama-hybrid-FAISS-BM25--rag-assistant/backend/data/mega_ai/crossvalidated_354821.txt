[site]: crossvalidated
[post_id]: 354821
[parent_id]: 354780
[tags]: 
We know that $$q_\pi(s,a) = E_{r|s,a}[r] + E_{s'|s,a}[ \max_{a'} q_\pi(s',a')]$$ This is where the Q-learning loss comes from: $$\left( q_\pi(s,a) - r - \max_{a'} q_\pi(s',a') \right)^2$$ Note that we approximate the expectation by sampling. Crucially, the expectation does not depend on the policy $\pi$! It only depends on the dynamics of the environment $P(r|s,a)$ and $P(s'|s,a)$. Therefore it is perfectly fine to sample $s,a,r,s'$ from any policy rollout, including an "old" version of a policy. This is why Q-learning is called "off-policy". Contrast this to the on-policy method REINFORCE: $$\nabla_\theta J = E_{\tau \sim \pi_\theta}[\nabla_\theta \log p_\theta(\tau) r(\tau)]$$ where $J$ is the expected reward of the policy and $\tau$ are "trajectories" sampled from that policy. Since the expectation draws $\tau$ from $\pi_\theta$, a replay memory could not be used in this case, since we would be incorrectly sampling $\tau$ from $\pi_{\theta_\text{old}}$.
