[site]: datascience
[post_id]: 117851
[parent_id]: 117850
[tags]: 
Neural networks in general are not bijective. This means that inputs and outputs do not map onto each other in a 1-to-1 fashion and you cannot simply reverse an output to obtain the input it was generated from. If you want to convince yourself this is true, consider a neural network with one input, one output and ReLU as activation function. The positive ReLU outputs will map, but all zero outputs could come from different inputs. You can constrain your neural network such that it becomes bijective and in that case it will be possible to reverse the operation. You should have a look at autoencoders . These are neural networks that perform a task that is similar in what you are probably thinking about. These networks consist of an encoder that decreases the dimensionality of the input and a decoder that takes the encoded input and reproduces the original data.
