[site]: crossvalidated
[post_id]: 579927
[parent_id]: 
[tags]: 
If I need to reduce my sample size due to computation time, should I standardize using my original or post-reduction means and standard deviations?

I have data such that each data point, xi, is a multidimensional time series, with each feature, p, having 10000 time stamps. xi = [pi0(0), pi0(1),… pi0(9999)], [pi1(0), pi1(1),… pi1(9999)], …. I want to use dynamic time warping to calculate the distance between each pair of points, but I need to standardize the values first. For each p I do this by exploding all of the values and finding the mean and standard deviation. Unfortunately, the computation time for dynamic time warping was too much. I then artificially reduced the fidelity by taking every 100th time stamp and now the computation time is much more reasonable. However, I don’t know if it’s more appropriate to standardize with the mean and standard deviation of the original values or with my reduced fidelity values. Which should I do?
