[site]: crossvalidated
[post_id]: 349464
[parent_id]: 
[tags]: 
How to use regularization to avoid overfitting in neural network using penalty?

Suppose I have a error function $R(\theta) + \lambda J(\theta )$ . $\lambda >=0 $ is a tuning parameter . In book Elements of statistical learning ,it is said , Larger values of Î» will tend to shrink the weights toward zero How are weights shrinking to zero?
