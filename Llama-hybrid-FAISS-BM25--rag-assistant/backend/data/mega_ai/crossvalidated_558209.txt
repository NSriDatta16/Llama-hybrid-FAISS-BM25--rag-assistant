[site]: crossvalidated
[post_id]: 558209
[parent_id]: 394582
[tags]: 
Under the Bernoulli distribution parameterized by say $p = 0.3$ by the output of the autoencoder, the probability of drawing $x = 0.2$ is zero (and is zero for all $0 ). This indeed makes the Bernoulli distribution a bad choice for non-binary data. However, a slightly different view of the input can resurrect the Bernoulli distribution. Let's assume instead that $x = 0.2$ is a sample from some measuring device, and this $x = 0.2$ reading might be best described as itself being a parameter of a probability distribution, such as a normal or Bernoulli distribution. Let's go with the latter and say that $x = 0.2$ represents a Bernoulli process with parameter $p' = x = 0.2$ . Thus, there is some underlying binary sensor or event which is $0$ with probability $0.2$ and $1$ with probability $0.8$ . The output of our autoencoder is a Bernoulli distribution with say $p = 0.3$ . It does make sense to ask: what is the expected result of drawing $0$ or $1$ readings from the real Bernoulli process (with parameter $p'=0.2$ ) and then calculating its likelihood value according to the autoencoder's Bernoulli distribution (with parameter $p = 0.3$ ). This expected likelihood is $p'p + (1-p')(1-p) = (0.2)(0.3) + (0.8)(0.7)$ . We can also ask what the expected log-likelihood is, and that is $p'\log(p) + (1-p')\log(1-p)$ . When we replace the symbol $p'$ with the usual symbol, $y$ , we get the usual expression $y\log(p) + (1-y)\log(1-p)$ . By interpreting the input differently (as a distribution parameter), the cross-entropy loss does make sense as the negative of the expected log-likelihood, where the expectation is over the "input" distribution, and likelihood is calculated against our "output" distribution.
