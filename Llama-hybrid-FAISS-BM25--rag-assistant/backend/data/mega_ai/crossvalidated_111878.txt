[site]: crossvalidated
[post_id]: 111878
[parent_id]: 111779
[tags]: 
Why the expected value âˆ«h(x)f(x) is desired for inference in Dirichlet Process Mixture? I assume the author's $h(\cdot)$ is an arbitrary function of $x$. Monte Carlo estimation allows us to take expected values of arbitrary functions, where the expected value is over the posterior distribution $f(\cdot)$ in the above. So slide 40 is saying something very general about Bayesian stats, not particular information about DP models. What is the intuition for MCMC in Dirichlet Process Mixture? The DP mixture model is over the space of all partitions of the data. That's a pretty big/difficult thing to work with. MCMC for DP-mixtures works by moving around between plausible partitions of the dataset. Inference for DP mixtures is still an active research topic because on big datasets the number of partitions is vast meaning finding 'good' ones is hard. --EDIT: There are multiple Gibbs and other MCMC algorithms for the DP-MM. Neal 2000 gives a review as they stood then. For example, Algo 3 from that paper updates the latent class memberships one at a time, using conditional probability given all the other class assignments.-- Checking out Teh's Dirichlet Process entry for the encyclopedia of machine learning might help.
