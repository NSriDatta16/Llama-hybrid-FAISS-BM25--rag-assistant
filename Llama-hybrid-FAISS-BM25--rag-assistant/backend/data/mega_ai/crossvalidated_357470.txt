[site]: crossvalidated
[post_id]: 357470
[parent_id]: 357464
[tags]: 
Does zero centering your data make it more linearly separable or does that not affect the separability? No, it does not. Linear separability is about being able to "separate" your classes with a line (or hyperplane on multiple dimensions). Imagine a line on 2D coordinate system. If you subtract some constant from x axis, the line would move by the same amount, same with y axis. Moving the line in any direction does not make it a non-line. Same with curves: moving the curve in some direction does not straighten it. I read that by zero-centering the data, it allows algorithms to train faster because small changes in the features shift the decision to one side or the other. If you have to walk 4 kilometres from point A to B, then if you needed to move from the point C at -2 km to point D at +2 km, you would have to walk the same distance. Subtracting constant does not make any distance smaller. For example, with zero-centered data using logistic regression, a small change would shift the point to one of the side of the sigmoid. Does it make a difference if the point is shifted to one of the sides of 0.5, or of 0? There's nothing magical in 0. Yes, some algorithms work better with centered or normalized data, but not because of linearizing anything, or making anything bigger, but because making the predictors "similar" in terms of their mean (and scale when normalizing), so the differences between them do not have to be accounted for. ( This is simplified explanation, for more details you'd need to read/aks about particular algorithms. )
