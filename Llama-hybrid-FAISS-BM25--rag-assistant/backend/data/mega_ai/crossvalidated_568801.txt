[site]: crossvalidated
[post_id]: 568801
[parent_id]: 423543
[tags]: 
The answer to this question is already in the text of the question: "Assume $\mu$ has a density", which means the statement is only valid when the distribution $\mu(X,Y)$ is absolutely continuous. Hence NFL does not apply here. To expand on this point, NFL theorem says that (see eg. 7.2 in Devroye book): " given any classification rule, there exists a distribution such that the excess risk is large. " Intuitively, NFL implies that inductive bias is unavoidable and any consistency result regarding the performance of a classifier must be accompanied with certain assumptions on the distribution of data. These assumptions are required to exclude those "bad" distributions that result in large excess risk. All consistency results for local averaging methods (histogram, k-NN, kernel) require absolute continuity of distribution of $(X,Y)$ . Sometimes this assumption is stated in terms of continuity of the posterior probability function $\eta(x) = \mathbb{Pr}(Y=1\mid X)$ . Such restrictions are very much necessary, since a local averaging method relies on labels varying "smoothly" within the feature space. In other words, two data points that are close with respect to some distance metric should have labels that are also close with high probability. Local averaging methods, such as nearest-neighbor, utilize the neighborhood of a test point to make a decision about its label. Therefore, a bad distribution for k-NN would be one where the conditional distribution function $\eta(X)$ is very rough and the labels of the neighbors are no longer useful. The NFL theorem is about the existence of such bad distributions, while consistency results are specialized to avoid such bad distributions.
