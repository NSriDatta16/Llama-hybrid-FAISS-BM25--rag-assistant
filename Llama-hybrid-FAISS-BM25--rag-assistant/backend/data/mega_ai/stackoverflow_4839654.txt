[site]: stackoverflow
[post_id]: 4839654
[parent_id]: 881894
[tags]: 
Let's see exactly what the standard says : 5.2.4.2.1 Sizes of integer types ... Their implementation-deﬁned values shall be equal or greater in magnitude (absolute value) to those shown, with the same sign. number of bits for smallest object that is not a bit-field (byte) CHAR_BIT 8 This tells us that a byte is at least 8 bits (the paragraph just aboves If the value of an object of type char is treated as a signed integer when used in an expression, the value of CHAR_MIN shall be the same as that of SCHAR_MIN and the value of CHAR_MAX shall be the same as that of SCHAR_MAX. Otherwise, the value of CHAR_MIN shall be 0 and the value of CHAR_MAX shall be the same as that of UCHAR_MAX. The value UCHAR_MAX shall equal 2^CHAR_BIT - 1 For each of the signed integer types, there is a corresponding (but different) unsigned integer type (designated with the keyword unsigned) that uses the same amount of storage (including sign information) and has the same alignment requirements. For unsigned integer types other than unsigned char,the bits of the object representation shall be divided into two groups: value bits and padding bits (there need not be any of the latter). These passages tell us that : an unsigned char needs to represent 2^CHAR_BIT-1 values, which can be encoded on minimum CHAR_BIT bits (according to the conventional bit representation, which is prescribed by the standard) an unsigned char does not contain any additional (padding) bits a signed char takes exactly the same space as an unsigned char a char is implemented in the same way as either as signed or unsigned char Conclusion : a char and it's variants unsigned char and signed char are guaranteed to be exactly a byte in size, and a byte is guaranteed to be at least 8 bits wide. Now they are other indications (but not formal proof as above) that a char is indeed one byte : Except for bit-ﬁelds, objects are composed of contiguous sequences of one or more bytes, the number,order,and encoding of which are either explicitly specified or implementation-defined. Values stored in non-bit-ﬁeld objects of any other object type consist of n × CHAR_BIT bits, where n is the size of an object of that type, in bytes. The value may be copied into an object of type unsigned char [n] The sizeof operator yields the size (in bytes) of its operand, which may be an expression or the parenthesized name of a type. The size is determined from the type of the operand. The result is an integer.If the type of the operand is a variable length array type, the operand is evaluated; otherwise, the operand is not evaluated and the result is an integer constant. When applied to an operand that has type char, unsigned char,or signed char, (or a qualified version thereof) the result is 1. When applied to an operand that has array type, the result is the total number of bytes in the array. 88) When applied to an operand that has structure or union type, the result is the total number of bytes in such an object, including internal and trailing padding. (Note that there is an ambiguity here. Does the sizeof(char) here override the sizeof(type) rule or does it merely gives an example ?) Still, there's a problem left to tackle. What exactly is a byte ? According to the standard it is "the smallest object that is not a bit-field". Note that this theoretically may not correspond to a machine byte , and that there is also ambiguity as to what is referred as an "machine byte" : it could whatever the constructors refers to as "byte", knowing that each constructor may have a different definition of "byte"; or a general definition like "a sequence of bits that a computer processes in individual units" or "the smallest addressable chunk of data". For instance, a machine that have 7-bits bytes, would have to implement a "C byte" as two machine bytes. Source of all citations : Committee Draft — September 7, 2007 ISO/IEC 9899:TC3 .
