[site]: crossvalidated
[post_id]: 267924
[parent_id]: 
[tags]: 
Explanation of the 'free bits' technique for variational autoencoders

I have been reading through a couple of papers on the variational autoencoder model: 'Variational Lossy Autoencoder' and 'Improving Variational Inference With Inverse Autoregressive Flow'. There is one (perhaps very obvious) thing that is confusing me - the former paper mentions the 'free bits' technique for training, and references the latter paper. My question is - what is the 'free bits' technique?! As I understand, the IAF paper is concerned with allowing a more complex posterior distribution so as to better fit the true posterior, which in turn will improve the coding length of the VAE model, as detailed in the first paper. It is not clear to me what 'free bits' is referring to. Any help in understanding these papers better is appreciated! Edit: references for the above papers: Xi Chen, Diederik P. Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya Sutskever, Pieter Abbeel "Variational Lossy Encoder", https://arxiv.org/abs/1611.02731 . Diederik P. Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, Max Welling "Improving Variational Inference with Inverse Autoregressive Flow", https://arxiv.org/abs/1606.04934 .
