[site]: datascience
[post_id]: 116087
[parent_id]: 115886
[tags]: 
The problem with using clustering like this is that there is no guarantee the clusters found have any relation to the target variable. If the clusters are based on common features that are not relevant for the classification task, then the target class of the "B"'s in your "good" clusters would probably be random. If you want a better classification for your "B"'s I'd suggest trying boosting, or if you have enough training samples which get classified as "B"'s, you could try training a second classifier on just these training samples. Then use this classifier to reclassify any cases that the first model classifies as "B". Edit to explain boosting, as requested by the OP: Boosting is training a series of classifiers with the aim of improving accuracy over that of the initial classifier. Each time a new classifier is trained, the training samples are weighted so that ones previously classified incorrectly have a higher weight, and those classified correctly have a lower weight, so the classifier puts more emphasis on those samples classified incorrectly so far. For inference, the series of classifiers is used as an ensemble, so the final prediction is a combination of the predictions made by each classifier. One of the most well-known boosting algorithm is Adaboost. A couple of blogs discussing boosting are Quick Introduction to Boosting Algorithms in Machine Learning by Sunil Ray or A Quick Guide to Boosting in ML by Jocelyn D'Souza.
