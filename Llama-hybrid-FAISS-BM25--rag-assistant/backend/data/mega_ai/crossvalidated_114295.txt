[site]: crossvalidated
[post_id]: 114295
[parent_id]: 
[tags]: 
Chi-squared calculation for (nonlinear) model goodness of fit for data with trials

I would like to be certain of how to calculate the $\chi^2$ value for this scenario: We desire a measure of the goodness of fit of a nonlinear model to a data set. The data consists of many data points $x_i$, where $i$ is an index, and $x_i$ is specific to the experiment (e.g., measured in seconds, meters, etc). At each data point $x_i$, a set of trials $\{y_{j}\}_x$ is collected. I frequently take data of this form. Usually, I simply average the trials and fit the results to an appropriate function. Until now, I've only needed to judge goodness of fit by eye, but now I need a more quantitative measure. Take for example the following data: There are 16 data points. At each point, 20 trials were "run", and the results are plotted above with black dots. The trials at each point are distributed normally, with a mean that follows a Voigt distribution across the entire data set, and with variances that are slightly different for each data point. The means are shown as red dots. What if we didn't know that the parent distribution of the data is a Voigt distribution? We might suspect that it is normally distributed. We could find the mean at each data point (red dots), and then fit them to a Gaussian and examine the goodness of fit. The goodness of fit may be measured by $\chi^2$, which is defined by Bevington & Robinson (2003) as $\chi^2 = \sum \frac{\left(y_i - y(x_i)\right)^2}{\sigma_i^2}$ where $y_i$ are the deviations of the data from the fitting function $y(x_i)$, and $\sigma_i^2$ are the variances at each point. My question is how to interpret this. Three different interpretations are: (a) At each data point, we sum the squared residuals $(y(x_i) - y_i)^2$, and divide the result by the variance for that point, $\sigma_i = \sum (y_j - \mu_i)^2 / (N_i - 1)$. This results in a list of $\chi^2$ values for each data point: I might then take the mean of these values and report the result as the $\chi^2$ statistic for the entire data set. For the above data, the resulting $\chi^2$ using this method would be 1.082. (b) I use the mean at each data point for $y_i$ in the above definition. I again use the set of trials at each point to calculate a variance at each point (as in (a)). A pseudo-formula for $\chi^2$ would then be: $\chi^2 = \sum \frac{\left( \text{fit result at data point} - \text{mean of trials at data point} \right)^2}{\text{variance of trials at data point}}$ There is a question of whether the denominator should be the variance of the trials w.r.t the mean at that data point, or w.r.t the fit value. The following plot shows the residuals (red) and variances (gray). Assuming the former, the resulting $\chi^2$ for the above data is 1.243. (The latter gives 0.933.) (c) I consider the data as a set of scans where in each scan I have only one trial per data point. In other words, I have 20 independent sets of measurements of the data set distribution. (This is not unrealistic - data is often collected in this manner!) I fit each scan, and then calculate the $\chi^2$ value for each one, using the method of (b). The variance for each data point is calculated the same way as in (a) and (b): by considering all the trials at each data point. The result is 20 separate $\chi^2$ values, one for each "scan". I might then average these. Once $\chi^2$ is known, it must be interpreted. One possibility would be to convert it to the reduced chi-squared statistic by dividing by the degrees of freedom, and then comparing to one. This is problematic for nonlinear curves (see for example this paper ). Another possibility is to use the probability distribution for $\chi^2$ to check if $P_{\chi}\left(\chi^2, \nu \right)$ is reasonably close to 0.5. Before I seriously consider either, though, I want to be sure that I'm calculating $\chi^2$ correctly.
