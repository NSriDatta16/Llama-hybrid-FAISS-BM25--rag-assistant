[site]: crossvalidated
[post_id]: 399868
[parent_id]: 
[tags]: 
Why does larger perplexity tend to produce clearer clusters in t-SNE?

Why does larger perplexity tend to produce clearer clusters in t-SNE? By reading the original paper , I learned that the perplexity in t-SNE is $2$ to the power of Shannon entropy of the conditional distribution induced by a data point. And it is mentioned in the paper that it can be interpreted as a smooth measure of the effective number of neighbors. If the conditional distribution of a data point is constructed by Gaussian distribution (SNE), then the larger the variance $\sigma^2$ , the larger the Shannon entropy, and thus the larger the perplexity. So, the intuition that I built is: The larger the perplexity, the more non-local information will be retained in the dimensionality reduction result. When I use t-SNE on two of mine test datasets for dimensionality reduction, I observe that the clusters found by t-SNE will become consistently more well-defined with the increase of perplexity. Although this is a desirable outcome, I just cannot explain this with the intuition that I just built. Here is the result that I have, the data and the script used for generating the figure have been uploaded here : Besides the confusion that I just mentioned, I also don't know how to interpret the result. What could be a possible explanation for the fact that it is easier for t-SNE to find well-defined clusters in Random dataset then Benchmark dataset ?
