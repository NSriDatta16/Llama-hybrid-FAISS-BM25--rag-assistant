[site]: crossvalidated
[post_id]: 336939
[parent_id]: 
[tags]: 
Frequentest and Bayesian analyses contradicting each other

I have the following set of data which I'm trying to analyse: | Treatment A | Treatment B | did the thing | 284 | 333 | didn't do the thing | 2554 | 2509 | n | 2838 | 2842 | Normally I'd use a fisher or chi square test to see if there is a difference between the treatments. In this case a two tests give a p values of 0.04065 and 0.04254 respectively, so I should reject the null hypothesis and say that there is a difference between the two treatments. Lately though I've been dabbling with the Bayesian approach to analysis. When I calculate a Bayes Factor for this set of data though I get a BF = 0.21, implying a moderate level of evidence for the null hypothesis. I guessing this difference in inference from the data is down to the differences in philosophy between the frequentest and Bayesian approaches to statistics but don't have enough experience to know how to continue from here as I kinda thought they were really just two different ways of skinning the same cat. Has anyone had any experience in this situation? Code I used in R below: # Input Data line1
