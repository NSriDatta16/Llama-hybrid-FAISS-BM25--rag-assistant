[site]: crossvalidated
[post_id]: 10235
[parent_id]: 10234
[tags]: 
I believe the origin of this is the likelihood paradigm (though I have not checked the actual historical correctness of the below, it is a reasonable way of understanding how it came to be). Let's say in a regression setting, you would have a distribution: $$ p(Y | x, \beta) $$ Which means: the distribution of $Y$ if you know (conditional on) the $x$ and $\beta$ values. If you want to estimate the betas, you want to maximize the likelihood: $$ L(\beta; y,x) = p(Y | x, \beta) $$ Essentially, you are now looking at the expression $p(Y | x, \beta)$ as a function of the beta's, but apart from that, there is no difference (for mathematical correct expressions that you can properly derive, this is a necessity --- although in practice no one bothers). Then, in bayesian settings, the difference between parameters and other variables soon fades, so one started to you use both notations intermixedly. So, in essence: there is no actual difference: they both indicate the conditional distribution of the thing on the left, conditional on the thing(s) on the right.
