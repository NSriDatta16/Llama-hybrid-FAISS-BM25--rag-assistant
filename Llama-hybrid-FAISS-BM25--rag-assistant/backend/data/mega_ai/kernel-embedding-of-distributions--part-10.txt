h^{t})} are computed by applying Bayes' rule to condition on a new observation). The RKHS embedding of the belief state at time t+1 can be recursively expressed as μ S t + 1 ∣ h t + 1 = C S t + 1 O t + 1 π ( C O t + 1 O t + 1 π ) − 1 φ ( o t + 1 ) {\displaystyle \mu _{S^{t+1}\mid h^{t+1}}={\mathcal {C}}_{S^{t+1}O^{t+1}}^{\pi }\left({\mathcal {C}}_{O^{t+1}O^{t+1}}^{\pi }\right)^{-1}\varphi (o^{t+1})} by computing the embeddings of the prediction step via the kernel sum rule and the embedding of the conditioning step via kernel Bayes' rule. Assuming a training sample ( s ~ 1 , … , s ~ T , o ~ 1 , … , o ~ T ) {\displaystyle ({\widetilde {s}}^{1},\dots ,{\widetilde {s}}^{T},{\widetilde {o}}^{1},\dots ,{\widetilde {o}}^{T})} is given, one can in practice estimate μ ^ S t + 1 ∣ h t + 1 = ∑ i = 1 T α i t φ ( s ~ t ) {\displaystyle {\widehat {\mu }}_{S^{t+1}\mid h^{t+1}}=\sum _{i=1}^{T}\alpha _{i}^{t}\varphi ({\widetilde {s}}^{t})} and filtering with kernel embeddings is thus implemented recursively using the following updates for the weights α = ( α 1 , … , α T ) {\displaystyle {\boldsymbol {\alpha }}=(\alpha _{1},\dots ,\alpha _{T})} D t + 1 = diag ⁡ ( ( G + λ I ) − 1 G ~ α t ) {\displaystyle \mathbf {D} ^{t+1}=\operatorname {diag} \left((G+\lambda \mathbf {I} )^{-1}{\widetilde {G}}{\boldsymbol {\alpha }}^{t}\right)} α t + 1 = D t + 1 K ( ( D t + 1 K ) 2 + λ ~ I ) − 1 D t + 1 K o t + 1 {\displaystyle {\boldsymbol {\alpha }}^{t+1}=\mathbf {D} ^{t+1}\mathbf {K} \left((\mathbf {D} ^{t+1}K)^{2}+{\widetilde {\lambda }}\mathbf {I} \right)^{-1}\mathbf {D} ^{t+1}\mathbf {K} _{o^{t+1}}} where G , K {\displaystyle \mathbf {G} ,\mathbf {K} } denote the Gram matrices of s ~ 1 , … , s ~ T {\displaystyle {\widetilde {s}}^{1},\dots ,{\widetilde {s}}^{T}} and o ~ 1 , … , o ~ T {\displaystyle {\widetilde {o}}^{1},\dots ,{\widetilde {o}}^{T}} respectively, G ~ {\displaystyle {\widetilde {\mathbf {G} }}} is a transfer Gram matrix defined as G ~ i j = k ( s ~ i , s ~ j + 1 ) , {\displaystyle {\widetilde {\mathbf {G} }}_{ij}=k({\widetilde {s}}_{i},{\widetilde {s}}_{j+1}),} and K o t + 1 = ( k ( o ~ 1 , o t + 1 ) , … , k ( o ~ T , o t + 1 ) ) T . {\displaystyle \mathbf {K} _{o^{t+1}}=(k({\widetilde {o}}^{1},o^{t+1}),\dots ,k({\widetilde {o}}^{T},o^{t+1}))^{T}.} Support measure machines The support measure machine (SMM) is a generalization of the support vector machine (SVM) in which the training examples are probability distributions paired with labels { P i , y i } i = 1 n , y i ∈ { + 1 , − 1 } {\displaystyle \{P_{i},y_{i}\}_{i=1}^{n},\ y_{i}\in \{+1,-1\}} . SMMs solve the standard SVM dual optimization problem using the following expected kernel K ( P ( X ) , Q ( Z ) ) = ⟨ μ X , μ Z ⟩ H = E [ k ( x , z ) ] {\displaystyle K\left(P(X),Q(Z)\right)=\langle \mu _{X},\mu _{Z}\rangle _{\mathcal {H}}=\mathbb {E} [k(x,z)]} which is computable in closed form for many common specific distributions P i {\displaystyle P_{i}} (such as the Gaussian distribution) combined with popular embedding kernels k {\displaystyle k} (e.g. the Gaussian kernel or polynomial kernel), or can be accurately empirically estimated from i.i.d. samples { x i } i = 1 n ∼ P ( X ) , { z j } j = 1 m ∼ Q ( Z ) {\displaystyle \{x_{i}\}_{i=1}^{n}\sim P(X),\{z_{j}\}_{j=1}^{m}\sim Q(Z)} via K ^ ( X , Z ) = 1 n m ∑ i = 1 n ∑ j = 1 m k ( x i , z j ) {\displaystyle {\widehat {K}}(X,Z)={\frac {1}{nm}}\sum _{i=1}^{n}\sum _{j=1}^{m}k(x_{i},z_{j})} Under certain choices of the embedding kernel k {\displaystyle k} , the SMM applied to training examples { P i , y i } i = 1 n {\displaystyle \{P_{i},y_{i}\}_{i=1}^{n}} is equivalent to a SVM trained on samples { x i , y i } i = 1 n {\displaystyle \{x_{i},y_{i}\}_{i=1}^{n}} , and thus the SMM can be viewed as a flexible SVM in which a different data-dependent kernel (specified by the assumed form of the distribution P i {\displaystyle P_{i}} ) may be placed on each training point. Domain adaptation under covariate, target, and conditional shift The goal of dom