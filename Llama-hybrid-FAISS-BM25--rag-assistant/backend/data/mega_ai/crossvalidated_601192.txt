[site]: crossvalidated
[post_id]: 601192
[parent_id]: 145602
[tags]: 
As I understand it, a zero variance variable is one whose values are all the same constant variable and a near-zero variance (NZV) variable is one where almost all values are constant and only a few have values that differ from that constant. I cannot give a definitive answer, but I think that there is a reasonable logic to the recommendation of removing such variables. (Although I can appreciate the logic, I think there might be a better solution that I give at the end of my answer.) Let us take for granted that we agree that zero-variance variables are bad, whether or not an algorithm automatically drops them. The question is whether NZV variables are also so bad that they ought to be dropped. Let's leave aside for the moment the nontrivial question of how near-zero is serious to be dropped--I will return to this issue shortly. Suppose we leave an NZV variable in the analysis and suppose this variable is very near zero. (Again, let's hold off on quantifying how much "very near zero" actually means--just stay with me for the concept.) Many typical machine learning workflows involve splitting an original dataset into several subsets. (Machine learning workflows are relevant here because that is Max Kuhn's context in the book you cited.) For example, we might start with a dataset of 10000 lines. Then we split it into a 70% training set (7000 lines) and a 30% test set (3000 lines). Then to compare and select among multiple models that are being trained, we might employ 10-fold cross-validation on the training set which involves splitting the training set such that there are ten overlapping subsets of 6300 lines each. With all these splits, it is quite plausible that a variable that has near-zero variance ends up having zero variance in one or more of the 6300-line cross-validation samples. That would be a problem. Some algorithms might crash on such cross-validation subsets. Other algorithms might automatically drop the problematic variable, but that would also be a problem because then the ten cross-validation folds would no longer have identical sets of variables, so the models created would not be comparable. Because of these potential problems, it makes sense to identify in advance which variables are so near zero variance that they could end up becoming zero variance with various data splits. That said, the question of "how near zero is too near" is challenging as it varies widely across datasets. As several comments have indicated, systematically removing NZV variables based on any criteria risks eliminating some variables that might have a strong relationship with the target outcome. I would suggest to rather run the data anyways even if there are some NZV variables and then dealing with any problems as they might come up. If the scenario I raised above never comes up (that is, no subset of the data ends up with zero variance), then there is no problem. However, if that situation comes up, then one possible solution would be to stratify the cross-validation folds by the NZV variable to ensure that every fold has some of the few minority cases. In short, whereas NZV variables are a legitimate concern, there might be better ways for dealing with the problem that do not risk eliminating these potential valuable estimators.
