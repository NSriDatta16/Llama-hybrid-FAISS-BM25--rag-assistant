[site]: crossvalidated
[post_id]: 446857
[parent_id]: 
[tags]: 
Understanding the meaning of different standard deviations from different values

Disclaimer: Before I stated the question, I should say that I'm very new at the area of statistics and the understanding of its application. So I ask you people to please be patient with me because I'm trying to learn about this wonderful world. Question : I have a table of test grades from some year where more than 10,000 students took the exam. It goes as follows: | Test Grade | Standard Deviation | |---------------|-----------------------| | 272.891526 | 29.494996 | | 240.767634 | 42.557313 | | 375.347864 | 15.259388 | Since each student receives different tests from a pile of content, each grade is calculated through some scores which reflect the probability that he/she has some set of skills. For example : If student A gets $250$ +, he/she probably understand how to calculate the average term of a sequence. What I'm trying to understand is: How can I analyse the groups of students in this whole set of grades taking into account the standard deviation? Are one properly way to doing so or it depends of "where I'm going with this data?" If all these results on Test Grade had $100\%$ probability of being right, I would only need then to separate the students into proper categories. But in this case, student A and student B may have the same grade, but different standard deviations . I honestly don't expect a full detailed answer, but something to guide me through this problem.
