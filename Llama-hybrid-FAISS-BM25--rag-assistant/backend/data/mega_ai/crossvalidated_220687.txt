[site]: crossvalidated
[post_id]: 220687
[parent_id]: 
[tags]: 
Thomson/Bayesian Bandit Algorithm

I am looking to use the Bayesian Bandits Strategy to find the best arm of a Multi armed bandit. As outlined in the link, the Bayesian algorithm is Sample a random variable $X_b$ from the prior of bandit $b$, for all $b$. Select the bandit with the largest sample, i.e. select bandit $B$ = argmax $X_b$. Observe the result of pulling bandit $B$, and update the prior of bandit $B$. Return to 1. My questions are: Does each bandit have a different prior? i.e. does each bandit have an individual beta distribution, with different initial $\alpha, \beta$? If each bandit does have a different prior, how do we incorporate prior knowledge about the bandits? i.e. if I suspect that bandit $1$ has a higher chance of probability than bandit $2$, how do I include this?
