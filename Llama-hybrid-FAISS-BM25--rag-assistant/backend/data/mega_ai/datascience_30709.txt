[site]: datascience
[post_id]: 30709
[parent_id]: 
[tags]: 
Understanding logistic regression loss function equation

I am writing a scientific paper that - among other things - deals with logistic regression in the context of machine learning. I read this article where the author states that, given a set of instance-label pairs $(\boldsymbol{x_i},y_i),\ i=1,...,l\ \ \boldsymbol(x_i) \in R^n,\ y_i \in \{-1,+1\}$, Logistic Regression solves the following optimization problem: $$ \begin{equation}\label{eq:log} \min_{w} \frac{1}{2}\boldsymbol{w}^T\boldsymbol{w} + C\sum\limits_{i=1}^l log(1+e^{-y_i\boldsymbol{w}^T \boldsymbol{x_i}}) \end{equation}\ \ \ \ \ \ \ \ \ \ (Eq.1) $$ Is it correct if I say that: " The first summand of Equation 1 is a regularization term while the second represents the negative log-likelihood loss function "? Thank you for your kind assistance.
