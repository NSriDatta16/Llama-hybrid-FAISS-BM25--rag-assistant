[site]: stackoverflow
[post_id]: 5147392
[parent_id]: 5144164
[tags]: 
Note: this is a revised answer after the question was updated with a traceback and version info Your pack fails during garbage collection because you have a corrupted pickle in your ZODB database. You can verify this by running the fsrefs.py script on your Data.fs. If you have a zopepy or similar part in your buildout (a script that just runs the arguments as a python script with the sys.path extended with all your eggs), you can do this: bin/zopepy /usr/local/www/cstools/buildout-cache/eggs/ZODB3-3.8.1-py2.4-linux-i686.egg/ZODB/scripts/fsrefs.py var/filestorage/Data.fs That's the full path to the fsrefs.py script. It'll only confirm if there is a pickle error. Most likely this is problem with an old transaction you don't care about anymore, and you can fix this problem by editing the serialize.py module to ignore this particular pickle error. You'll find the path to it at the end of your traceback. Edit the referencesf function (line 608 and on) to catch the exception in question. Originally the top of that function looks like this: refs = [] u = cPickle.Unpickler(cStringIO.StringIO(p)) u.persistent_load = refs u.noload() u.noload() Change it to: refs = [] u = cPickle.Unpickler(cStringIO.StringIO(p)) u.persistent_load = refs u.noload() try: u.noload() except AttributeError: print "Ignoring AttributeError during garbage collection unpickling." pass The risk is that the garbage collection algorithm will not detect a set of objects that are still in use and thus pack away too much, but that risk diminishes with the age of the affected transaction. The risk appears minimal, after all that pickle is already broken and any application code won't be able to reach any object referenced through whatever that pickle held anyway.
