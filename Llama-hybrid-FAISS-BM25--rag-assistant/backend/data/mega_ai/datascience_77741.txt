[site]: datascience
[post_id]: 77741
[parent_id]: 30181
[tags]: 
So, here the question is asking how you use text in a CNN architecture. Just like when we process images with a CNN, the text data will also be a 2D matrix, where the rows will represent the text features and the columns the sequence of characters (which make up a name) for example. Now, of course, if we simply use one-hot encoding, we get data sparsity, which is not not particularly computational efficient, especially when using CNNs. So, preferably, we would use word embeddings, which you have described in your post. Here, word embeddings aim to collapse the high dimensionality of the input so that reduces the chance of data sparsity. However, there is a potential problem with using word embeddings in your problem. The principal behind word embeddings is that it will assign similar word embeddings to words which can be easily replaced in similar contexts (e.g. Sam is kind & Simon is kind) and that don't affect the coherence of a sentence. As you can see, when it comes to proper nouns, we might end up with similar vectors for all names since they are very interchangeable in any context and do not affect the overall coherence of the sentence. Therefore, although this is contradictory, I would advise using one-hot encoded sequence of characters as a start point for this problem and then when you implement this, you might come up with a more effective way to represent the names.
