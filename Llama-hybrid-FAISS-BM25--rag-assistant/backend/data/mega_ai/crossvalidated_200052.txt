[site]: crossvalidated
[post_id]: 200052
[parent_id]: 
[tags]: 
Projecting to lower/higher-dimensional space for classification: dimensionality reduction vs kernel trick

Whilst learning about classification, I have seen two different arguments. One is that projecting the data to a lower-dimensional space, such as with PCA, makes the data more easily separable. The other is that projecting to a higher-dimensional space, such as with kernel SVM, makes the separation easier. Which is correct? Or does it depend on the data? What about projecting down with PCA, and afterwards projecting that space up with kernel SVM (or vica versa)?
