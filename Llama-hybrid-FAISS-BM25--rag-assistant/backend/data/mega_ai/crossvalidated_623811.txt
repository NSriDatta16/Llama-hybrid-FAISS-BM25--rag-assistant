[site]: crossvalidated
[post_id]: 623811
[parent_id]: 623677
[tags]: 
I'm sorry for breaking the news for you, but it works absolutely like that (this is mostly anecdotal based off my experience and what I see in the field, so I'm willing to delete this answer if someone brings up something else). Obviously, researchers have operationalized such choices, but the design of deep neural networks is most often based on applying heuristics, starting from previous successful network architectures, and is quite handwavy. AutoML takes this to the extreme, performing meta-learning (i.e., optimizing the architecture to the task at hand). It's also possible to 'elicit' some choices based on group equivariances (i.e., convolutions give you translation equivariance). This is an active area of research, although it does not answer the question of how many layers and how many units per layer are necessary. Neural network architecture design is the topic of many books in the literature as well, you might want to check them. Geometric deep learning can be a good gateway, providing an unification of most current neural network layers. On this topic, you can find this book Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges and the accompanying portal https://geometricdeeplearning.com/ , which contains a lot of material.
