[site]: datascience
[post_id]: 74097
[parent_id]: 
[tags]: 
The gradient of expectation of an objective function

Denoting $x$ : 2D image, $\delta $ ~ $ N(0,I)$ , $\theta=x+\sigma\delta$ , $F(x)$ : objective function, $\pi(\theta|x)$ : search distribution, we have $\nabla_x E_{\pi(\theta|x)}[F(\theta)]= E_{\pi(\theta|x)}[F(\theta)\nabla_xlog(\pi(\theta|x))]$ . But suddenly they say that $\nabla_x E_{\pi(\theta|x)}[F(\theta)] \approx \frac{1}{\sigma n}\sum_{i=1}^{n}\delta _i F(\theta+\sigma\delta_i)$ . How is the approximation formed? We can approximate the expectation using $n$ samples, but why $\delta_iF(\theta+\sigma\delta_i)$ ? I'm reading a machine learning paper and want to know the missing links as stated above in section 2.1.1. The paper is : A. Ilyas, L. Engstrom, A. Athalye, and J. Lin, “Black-box Adversarial Attacks with Limited Queries and Information,” Available: http://arxiv.org/abs/1804.08598 . Thank you in advance.
