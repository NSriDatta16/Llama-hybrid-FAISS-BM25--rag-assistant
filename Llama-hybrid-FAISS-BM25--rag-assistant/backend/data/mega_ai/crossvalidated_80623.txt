[site]: crossvalidated
[post_id]: 80623
[parent_id]: 80611
[tags]: 
We can rationalize this as follows: Underlying logistic regression is a latent (unobservable) linear regression model: $$y^* = X\beta + u$$ where $y^*$ is a continuous unobservable variable (and $X$ is the regressor matrix). The error term is assumed, conditional on the regressors, to follow the logistic distribution, $u\mid X\sim \Lambda(0, \frac {\pi^2}{3})$. We assume that what we observe, i.e. the binary variable $y$, is an Indicator function of the unobservable $y^*$: $$ y = 1 \;\;\text{if} \;\;y^*>0,\qquad y = 0 \;\;\text{if}\;\; y^*\le 0$$ Then we ask "what is the probability that $y$ will take the value $1$ given the regressors (i.e. we are looking at a conditional probability). This is $$P(y =1\mid X ) = P(y^*>0\mid X) = P(X\beta + u>0\mid X) = P(u> - X\beta\mid X) \\= 1- \Lambda (-Î§\beta) = \Lambda (X\beta) $$ the last equality due to the symmetry property of the logistic cumulative distribution function. So we have obtained the basic logistic regression model $$p=P(y =1 \mid X) = \Lambda (X\beta) = \frac 1 {1+e^{-X\beta}}$$ After that, the other answers give you how we manipulate this expression algebraically to arrive at $$\log \frac {p}{1 - p} = X\beta $$ It is therefore the initial linear assumption/specification related to the Latent variable $y^*$, that leads to this last relation to hold. Note that $\log \frac {p}{1 - p}$ is not equal to the latent variable $y^*$ but rather $y^* = \log \frac {p}{1 - p} + u$
