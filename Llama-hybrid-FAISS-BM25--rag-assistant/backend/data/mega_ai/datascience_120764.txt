[site]: datascience
[post_id]: 120764
[parent_id]: 
[tags]: 
How does an LLM "parameter" relate to a "weight" in a neural network?

I keep reading about how the latest and greatest LLMs have billions of parameters. As someone who is more familiar with standard neural nets but is trying to better understand LLMs, I'm curious if a LLM parameter is the same as a NN weight i.e. is it basically a number that starts as a random coefficient and is adjusted in a way that reduces loss as the model learns? If so, why do so many researches working in the LLM space refer to these as parameters instead of just calling them weights?
