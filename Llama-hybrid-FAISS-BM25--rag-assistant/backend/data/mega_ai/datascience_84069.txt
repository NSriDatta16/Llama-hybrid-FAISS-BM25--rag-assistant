[site]: datascience
[post_id]: 84069
[parent_id]: 
[tags]: 
Is there any advantage of limiting the value of a feature in neural networks

In a machine learning algorithm, I have a feature that has a value in the range 0-20 it is very rarely value goes over 20 and if does I clamp it 20. Does it help the neural network model somehow using reducing the infinite floating number set to integers between 0-20? Or even further if I categorize between the floating numbers between like; 0-5 than 0, 5-10 than 1, 10-15 than 2, and 15-20 than 3 does it helps my model to converge better and be more accurate? Does it reduce the effect of "Curse of dimensionality" because the possible inputs are reduced from an infinite set to few possibilities?
