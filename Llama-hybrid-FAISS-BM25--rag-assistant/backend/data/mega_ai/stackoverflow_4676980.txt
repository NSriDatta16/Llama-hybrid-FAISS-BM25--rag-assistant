[site]: stackoverflow
[post_id]: 4676980
[parent_id]: 1578062
[tags]: 
The Stanford CoreNLP Java library contains a lemmatizer that is a little resource intensive but I have run it on my laptop with To use it: Download the jar files ; Create a new project in your editor of choice/make an ant script that includes all of the jar files contained in the archive you just downloaded; Create a new Java as shown below (based upon the snippet from Stanford's site); import java.util.Properties; public class StanfordLemmatizer { protected StanfordCoreNLP pipeline; public StanfordLemmatizer() { // Create StanfordCoreNLP object properties, with POS tagging // (required for lemmatization), and lemmatization Properties props; props = new Properties(); props.put("annotators", "tokenize, ssplit, pos, lemma"); // StanfordCoreNLP loads a lot of models, so you probably // only want to do this once per execution this.pipeline = new StanfordCoreNLP(props); } public List lemmatize(String documentText) { List lemmas = new LinkedList (); // create an empty Annotation just with the given text Annotation document = new Annotation(documentText); // run all Annotators on this text this.pipeline.annotate(document); // Iterate over all of the sentences found List sentences = document.get(SentencesAnnotation.class); for(CoreMap sentence: sentences) { // Iterate over all tokens in a sentence for (CoreLabel token: sentence.get(TokensAnnotation.class)) { // Retrieve and add the lemma for each word into the list of lemmas lemmas.add(token.get(LemmaAnnotation.class)); } } return lemmas; } }
