[site]: stackoverflow
[post_id]: 2903470
[parent_id]: 2785382
[tags]: 
You should choose the right loss function to minimize. The squared error does not lead to the maximum likelihood hypothesis here. The squared error is derived from a model with Gaussian noise: P(y|x,h) = k1 * e**-(k2 * (y - h(x))**2) You estimate the probabilities directly. Your model is: P(Y=1|x,h) = h(x) P(Y=0|x,h) = 1 - h(x) P(Y=1|x,h) is the probability that event Y=1 will happen after seeing x. The maximum likelihood hypothesis for your model is: h_max_likelihood = argmax_h product( h(x)**y * (1-h(x))**(1-y) for x, y in examples) This leads to the "cross entropy" loss function. See chapter 6 in Mitchell's Machine Learning for the loss function and its derivation.
