[site]: crossvalidated
[post_id]: 425555
[parent_id]: 425545
[tags]: 
Yes. In k-fold cross-validation, you would eventually train your selected model on all of the "training and validation" data. And this is where your understanding of (2) has to change. If you weren't interested in comparing across different values of $k$ or across models or other hyperparameters, you'd just take your entire data and run k-fold cross-validation with your chosen (not changing) set of parameters. However, if your intention was to compare across any variables that have an effect on your output model, you would split your data into: training and validation, and test I am doing a course in data-science and we're learning that as long as "training and validation" data is 50% or more of the dataset, this is good. The test dataset needs to just be large enough to test. This split is key to answer your next question. You would train with different values of $k$ and other hyperparameters - anything and everything you wanted to test the change of - by performing k-fold cross-validation on the training and validation sample of the dataset. What you change is then key to reading your results as meaningful or not. Say, you wanted to try different kernels for SVM model, then you would keep the $k$ in k-fold constant, $\lambda$ constant and change just the kernels. This would allow you to compare across kernels. Say you had picked a kernel and a value of $\lambda$ , you could then vary the $k$ in k-fold cross-validation and compare the effect of different $k$ values. Eventually, you'll come up with one set of hyperparameters that performs better than the others. This you would then train on all of training and validation sample of the dataset. To work out the resulting model's performance, you would test it on the test sample of the dataset. I hope this helps.
