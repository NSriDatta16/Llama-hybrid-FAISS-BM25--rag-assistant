[site]: datascience
[post_id]: 38514
[parent_id]: 38507
[tags]: 
It is not uncommon to use dropout on the inputs. In the original paper the authors usually use dropout with a retention rate of 50% for hidden units and 80% for (real-valued) inputs. For inputs that represent categorical values (e.g. one-hot encoded) a simple dropout procedure might not be appropriate. They also argue that dropout applied to the inputs of Linear Regression yield a model that is similar to Ridge Regression where the dropout rate is related to the regularization strength [dropout adding variability/noise to the inputs leading to squeezing of the weights]. For deeper networks this is not quite as clear. but, in general, dropout adds noise to the data and is more useful for bigger datasets. Approaches similar to dropout of inputs are also not uncommon in other algorithms, say Random Forests, where not all features need to be considered at every step using the same ideas. The question is if adding dropout to the input layer adds a lot of benefit when you already use dropout for the hidden layers. In my experience, it doesn't for most problems. For some problems it makes more sense to inject noise earlier in the network to avoid overfitting from the beginning and sometimes only at later layers after some more complex features have already been built.
