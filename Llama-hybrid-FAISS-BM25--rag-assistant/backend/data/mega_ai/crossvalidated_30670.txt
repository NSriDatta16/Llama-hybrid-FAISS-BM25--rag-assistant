[site]: crossvalidated
[post_id]: 30670
[parent_id]: 
[tags]: 
Posterior derivation with prior as Normal-Gamma distribution

I am studying Gary Koop's Bayesian Econometrics , it is bit confusing in the beginning. I will reproduce some results from the textbook here in order to smoothly move to my question. For a simple linear regression model with normal distribution assumption for the error term, its likelihood can be rearranged into $$p(y|\beta, h)=\frac{1}{(2\pi)^{N/2}}\bigg\{h^{\frac{1}{2}}\mathrm{exp}\bigg[-\frac{h}{2}(\beta-\hat{\beta})^2\sum_{i=1}^Nx_i^2\bigg]\bigg\}\bigg\{h^{\frac{\nu}{2}}\mathrm{exp}\bigg[-\frac{h\nu}{2s^{-2}}\bigg]\bigg\}$$ the first curly brace on the right hand side resembles Normal distribution, the second curly brace resembels Gamma distribution, where $h=\frac{1}{\sigma^2}$, $\nu$ the degree of freedom, $s^2=\frac{\sum (y_i-\hat{\beta x_i})^2}{\nu}$, $N$ the sample size. Then we have a natural conjugate prior , with a Normal-Gamma density $$\beta,h \sim NG(\underline{\beta},\underline{V},\underline{s}^{-2},\underline{\nu})$$, underline indicates the prior parameters. $\underline{V}$ is the variance of $\beta|h$. After this, the author provide the posterior directly $$\beta,h|y \sim NG(\bar{\beta},\bar{V},\bar{s}^{-2},\bar{\nu})$$ where $$\bar{V}=\frac{1}{\underline{V}^{-1}+\sum x_i^2}$$ $$\bar{\beta}=\bar{V}(\underline{V}^{-1}\underline{\beta}+\hat{\beta}\sum x_i^2)$$ $$\bar{\nu}=\underline{\nu}+N$$ and $\hat{\beta}$ is the OLS estimate. I have no idea how author get these results, I checked the reference he mentioned, but I still could not figure out the deriviation. Would anyone show me how do get posterior as author did under these model settings? Thank you!!
