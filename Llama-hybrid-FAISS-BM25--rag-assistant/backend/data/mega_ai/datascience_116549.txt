[site]: datascience
[post_id]: 116549
[parent_id]: 
[tags]: 
Decision tree vs logistic regression feature importances

I have trained Logistic regression and decision tree in skearn on the same standardized dataset (binary classification). Top important coefficients for the decision tree are (sorted by tree.feature_importances_ ): And for logistic regression (sorted by abs(logreg.coef_) ): 'Total day charge' is the most important coefficient for decision tree, but for logistic regression it is only the 5-th. Why? 'Total day charge' s approximately 35% more important than the next coefficient for decision tree, but for logreg it is almost 50% less important than the most important coefficient. I can't imagine such optimal separating hyperplane, at least in 3 dimensions. As I can imagine, the biggest coeff in logreg must correspond to dimension, which is the best separating dimension itself.
