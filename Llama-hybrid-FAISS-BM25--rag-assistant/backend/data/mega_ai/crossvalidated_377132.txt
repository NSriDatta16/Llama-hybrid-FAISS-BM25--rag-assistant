[site]: crossvalidated
[post_id]: 377132
[parent_id]: 376619
[tags]: 
The point I am making in my book and in the previous question is not original but worth repeating. For a dominating measure $\text{d}\mu$ , the maximum entropy prior is defined as maximising $$\int_\mathcal{X}\log p(x) \text{d}\mu(x)$$ in the density $p$ under a set of constraints $$\mathbb{E}_p[g_k(X)]=\int_\mathcal{X} g_k(x) p(x) \text{d}\mu(x)=\omega_k\qquad k=1,\ldots,K$$ including $$\int_\mathcal{X} p(x) \text{d}\mu(x)=1$$ If there is such a probability distribution, then it is given by $$p(x) = \exp\{\lambda_1 g_1(x)+\cdots+\lambda_K g_K(x)\}\Big/ \int_\mathcal{X} \exp\{\lambda_1 g_1(x)+\cdots+\lambda_K g_K(x)\} \text{d}\mu(x)$$ whose notation is simplified into $$p(x) \propto \exp\{\lambda_1 g_1(x)+\cdots+\lambda_K g_K(x)\}$$ to avoid dragging the denominator into every equation. This being a density against $\text{d}\mu$ , changing the measure changes the resulting density and the resulting distribution. Except in your example where you unfortunately chose to move from the measure $\text{d}\mu$ to the measure $e^{-x^2/2}\text{d}\mu$ and a constraint involving $g_2(x)=x^2$ : in this case the maximum entropy prior distribution is the same for both measures . Indeed, they both have the same dominating measure $\text{d}\mu$ and the same defining equations: \begin{align*} p_1(x)\text{d}\mu &\propto\exp\{\lambda_1 x+\lambda_2 x^2\}\text{d}\mu\\ p_2(x)e^{-x²/2}\text{d}\mu &\propto\exp\{\lambda_1^\prime x+\lambda_2^\prime x^2\}e^{-x²/2}\text{d}\mu\\ &=\exp\{\lambda_1^\prime x+(\lambda_2^\prime -1/2)x^2\}\text{d}\mu\\ \end{align*} Hence $$\lambda_1=\lambda_1^\prime\qquad\lambda_2=\lambda_2^\prime -1/2$$ and the probability distributions are the same distributions even though $p_1(\cdot)\ne p_2(\cdot)$ . If, for this example of the first two moments [meaning $g_1(x)=x$ and $g_2(x)=x^2$ ], I choose instead to replace the Lebesgue measure $\text{d}x$ with the new measure $|x|^{-1/2}\text{d}x$ , which is absolutely continuous wrt the Lebesgue measure, the corresponding maximum entropy prior distributions will differ since their density wrt the Lebesgue measure will be \begin{align*} p_1(x)\text{d}\mu &\propto\exp\{\lambda_1 x+\lambda_2 x^2\}\text{d}\mu\\ p_2(x)|x|^{-1/2}\text{d}\mu &\propto\exp\{\lambda_1^\prime x+\lambda_2^\prime x^2\}|x|^{-1/2}\text{d}\mu\\ \end{align*} For $\omega_1=0$ and $\omega_2=1$ , the first solution $p_1(\cdot)$ is the density of the standard Normal distribution, while the second one satisfies $$\int_{-\infty}^\infty \frac{x}{\sqrt{|x|}} \exp\{\lambda_1'x+\lambda_2'x^2\}\text{d}x=0$$ which is achieved when taking $\lambda_1'=0$ and implies that $$p_2(x)\propto \exp\{ \lambda_2^\prime x^2\}|x|^{-1/2}$$ The normalising constant is thus given by $$\int_{-\infty}^\infty \exp\{ \lambda_2^\prime x^2\}|x|^{-1/2}\text{d}x \overbrace{=}^{y=x^2}2\int_0^\infty \exp\{ \lambda_2^\prime y\}|y|^{-1/4}\underbrace{y^{-1/2}\text{d}y/2}_\text{Jacobian}=\Gamma(1/4)(-\lambda_2^\prime)^{-1/4}$$ The second parameter is determined by the constraint $$\int_{-\infty}^\infty x^2 \exp\{ \lambda_2^\prime x^2\}|x|^{-1/2}\text{d}x =1 \times \Gamma(1/4)(-\lambda_2^\prime)^{-1/4}$$ or $$2\int_0^\infty \exp\{ \lambda_2^\prime y\}|y|^{1+1/4-1}\text{d}y/2= \Gamma(5/4)(-\lambda_2^\prime)^{-5/4}= \Gamma(1/4)(-\lambda_2^\prime)^{-1/4}$$ which simplifies as $$\frac{1}{4}(-\lambda_2^\prime)^{-1}=1\quad\text{i.e.}\quad\lambda_2=-1/4$$ These two distributions share the same first two moments but are clearly not identical distributions $$p_1(x)=\dfrac{\exp\{-x^2/2\}}{\sqrt{2\pi}}\qquad p_2(x)=\dfrac{\exp\{-x^2/4\}}{\Gamma(1/4)(1/4)^{-1/4}}$$
