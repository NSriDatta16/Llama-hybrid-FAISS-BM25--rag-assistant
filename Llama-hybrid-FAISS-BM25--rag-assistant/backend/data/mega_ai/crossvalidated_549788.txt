[site]: crossvalidated
[post_id]: 549788
[parent_id]: 485981
[tags]: 
The reason they use a low rank plus diagonal approximation is to be parameter efficient. In the case of a deep neural network (the paper is about training Bayesian Neural Networks), the number of parameters in the networks (which becomes latent variables in the Bayesian model) is very large. Considering all the covariances across all these model parameters would require a number of variational parameters roughly proportional to the square of the number of model parameters, which is probably too many for a computer to handle. By using a diagonal plus low rank approximation, you need only a number of variational parameters roughly proportional to the number of model parameters directly. The diagonal plus low rank approximation is more efficient than just a diagonal approximation, or a block diagonal approximation.
