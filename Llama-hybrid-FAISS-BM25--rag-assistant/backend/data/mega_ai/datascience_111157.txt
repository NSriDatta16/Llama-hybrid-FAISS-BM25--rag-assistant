[site]: datascience
[post_id]: 111157
[parent_id]: 
[tags]: 
Best gradient free methos to converge a 2 layers Neural Network on MNIST?

We've developed a C++ Neural Network that work on the MNIST dataset. We don't want to use back-propagation. Are there optimal methods to avoid it and have the net to converge to high accuracies?
