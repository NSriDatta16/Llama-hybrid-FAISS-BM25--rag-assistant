[site]: datascience
[post_id]: 9227
[parent_id]: 5224
[tags]: 
The idea with Neural Networks is that they need little pre-processing since the heavy lifting is done by the algorithm which is the one in charge of learning the features. The winners of the Data Science Bowl 2015 have a great write-up regarding their approach, so most of this answer's content was taken from: Classifying plankton with deep neural networks . I suggest you read it, specially the part about Pre-processing and data augmentation . - Resize Images As for different sizes, resolutions or distances you can do the following. You can simply rescale the largest side of each image to a fixed length. Another option is to use openCV or scipy. and this will resize the image to have 100 cols (width) and 50 rows (height): resized_image = cv2.resize(image, (100, 50)) Yet another option is to use scipy module, by using: small = scipy.misc.imresize(image, 0.5) - Data Augmentation Data Augmentation always improves performance though the amount depends on the dataset. If you want to augmented the data to artificially increase the size of the dataset you can do the following if the case applies (it wouldn't apply if for example were images of houses or people where if you rotate them 180degrees they would lose all information but not if you flip them like a mirror does): rotation: random with angle between 0째 and 360째 (uniform) translation: random with shift between -10 and 10 pixels (uniform) rescaling: random with scale factor between 1/1.6 and 1.6 (log-uniform) flipping: yes or no (bernoulli) shearing: random with angle between -20째 and 20째 (uniform) stretching: random with stretch factor between 1/1.3 and 1.3 (log-uniform) You can see the results on the Data Science bowl images. Pre-processed images augmented versions of the same images -Other techniques These will deal with other image properties like lighting and are already related to the main algorithm more like a simple pre-processing step. Check the full list on: UFLDL Tutorial
