[site]: datascience
[post_id]: 77093
[parent_id]: 
[tags]: 
what is the difference between positional vector and attention vector used in transformer model?

what is the difference between positional vector and attention vector used in transformer model ? , i saw a video in youtue and the defintion for positional vector was give as :* "vector that gives context based on postion of word in sentence "* defintion for attention vector was give as "For ever word we can have attention vector generated which captures contextual relationship between words in sentence" Capturing context information based on distance(postional vector) and attention (attention vector ) sounds same right? or is it different ?
