[site]: crossvalidated
[post_id]: 550367
[parent_id]: 550308
[tags]: 
This is generally because the use-case, at least historically, for hypothesis testing in statistics is often about simply making a generalization. The use-case in machine learning is to build a useful model, usually under the assumption of the corresponding generalization. Take, for example, Fisher's Iris Flower Dataset . One question someone might ask is "Do setosa , virginica and versicolor have, on average, different sepal lengths?" We can tackle this question with the scientific method: Hypothesize that they have the same sepal length. (bc this is falsifiable) Attempt to gather evidence to the contrary. Is evidence strong? If so, discard same-sepal-length hypothesis. The $p$ -value in hypothesis testing tries to help answer the "Is evidence strong?" question in this procedure. In an ML setting, it is generally assumed that these species differ by, for example, sepal length. A question to ask might be "Can we predict { setosa , virginica , versicolor } from the sepal length?" A model is built and its ability to predict the species from the sepal length is measured in precision, recall, accuracy, etc etc etc. Note that since there are many possible models, the precision, recall and accuracy may or may not give you information about whether or not there is a relationship there in the first place. So, for example, we build a decision tree to distinguish these species based on sepal length, and it reports a 33% accuracy (i.e. no better than guessing) -- Does this mean that there is not a relationship, or just that you chose the wrong model? Of course, in a sense, the hypothesis testing procedure also involves building and evaluating a model. However, the model usually isn't even used explicitly: it merely informs the particular equations that we use to get at "Do the species differ by sepal length?"
