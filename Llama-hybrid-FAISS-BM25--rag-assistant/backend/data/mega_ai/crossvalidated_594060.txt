[site]: crossvalidated
[post_id]: 594060
[parent_id]: 
[tags]: 
Using 1-Layer Fully-Connected Neural Network to Appoximate Exponential Functions

Consider a 1-layer fully-connected neural network (FCNN) given by $$ f(x) = \sum_{i=1}^n v_i\sigma\!\left({w_i}^T x\right) $$ where $x,w_i\in\mathbb{R}^d$ , $v_i\in\mathbb{R}$ , and $\sigma(y)=\max(y,0)$ is the ReLU activation function. According to my professor, he was telling us that it is not possible for this $f(x)$ to approximate an exponential function $g(x)=e^x$ but I don't understand why though. Can someone please explain to me why is this the case and what are the ways to get around this issue?
