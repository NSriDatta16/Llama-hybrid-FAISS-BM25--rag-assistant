[site]: crossvalidated
[post_id]: 314730
[parent_id]: 314714
[tags]: 
Not worthy of an answer, but too long for a comment. I am also interested in this question. The theory seems quite difficult as you are layering a non-linear algorithm over top of another one. TL;DR I'm surprised it works very well, I would look into the hyperparameters of your random forest. But since the theory is quite complex there may be something happening that I don't understand. First, an intuitive understanding. t-SNE attempts to map data from a high dimensional space down to a lower dimensional space while keeping the distance between points which are 'close' in high-dimensions but without caring about points which are not close. In effect, it creates clusters of the points which are near one another in predictor space and maps them to two (or three) dimensions. This, along with a classification scheme can provide reasonable accuracy. Consider the example from the original paper of t-SNEs application to the MNIST dataset -- in two dimensions you can easily pick out the digits by eye with ~90% accuracy, a k-means classifier layered on top will do about that or better. A well-specified random forest ought to pick out that structure in the high-dimensional space without the need to first map it down. As you are not adding any information, and you accept some loss in t-SNE you are (in some sense) just placing your data on a 3-dimensional manifold and appending this lossy new dataset to your existing set. As random forests already have a means of regularizing, it is unclear why your result is improved. It makes me skeptical of how well your hyperparameters are set, i.e. it may be the case that your Random Forest is not regularizing very well so that you have t-SNE do it for you your accuracy improves. Setting the hyperparameters correctly in the first place and removing the t-SNE ought to further improve on this performance.
