[site]: datascience
[post_id]: 117988
[parent_id]: 117983
[tags]: 
A function $f: A \to \mathbb{R}$ , with $A \subset \mathbb{R}$ , where $\mathbb{R}$ is the set of real numbers, is differentiable at $a \in A$ if its derivative: $$f'(a) = \lim_{h \to 0}\frac{f(a+h) - f(a)}{f(h)}$$ exists, which implies that $f$ is continuous at $a$ . The function $f$ is said to be differentiable on $A$ if it is differentiable at every point of $A$ . Differentiable loss functions are the bread and butter in training neural networks. Historically, artificial intelligence (AI), and thus machine learning, started in 1956 with the proposal of McCharty et al. for a two-month, 10-person summer conference at Dartmouth College. Without going too deep with the chronological milestones, it is important to stress that many promises by AI researchers of future successes failed. The latter started what is called an AI winter. The return of neural networks came in 1986 thanks to the influential work on the backpropagation algorithm by Rumelhart, Hinton, and Williams. To understand backpropagation, you must comprehend what partial derivatives are and what the gradient of a function is. Let $A \subset \mathbb{R}^n$ be an open set on $\mathbb{R}^n$ , $f: A \to \mathbb{R}$ a function of $n$ variables (i.e., $f(x_1,\ldots,x_n)$ ), and $\mathbf{a} = (a_1,\ldots,a_n)$ a point. Then, the partial derivative of $f$ at $\mathbf{a}$ in the $i$ th variable $x_i$ is: $$\frac{\partial}{\partial x_i}f(\mathbf{a}) = \lim_{h \to 0}\frac{f(a_1,\ldots,a_{i-1},a_i+h,a_{i+1},\ldots,a_n) - f(a_1,\ldots,a_{i-1},a_i,a_{i+1},\ldots,a_n)}{f(h)},$$ and the gradient of $f$ at $\mathbf{a}$ is: $$ \nabla f = \begin{bmatrix} \frac{\partial}{\partial x_1} f(\mathbf{a}) \\ \ldots \\ \frac{\partial}{\partial x_n} f(\mathbf{a}) \end{bmatrix}. $$ Fundamentally, backpropagation is a technique for computing partial derivatives quickly. For example, when training neural networks, we think of the cost as a loss function $\ell$ of the parameters $W$ of the network (i.e., the numbers describing how the network behaves). Therefore, the goal is to calculate derivatives of the cost concerning all the network parameters.
