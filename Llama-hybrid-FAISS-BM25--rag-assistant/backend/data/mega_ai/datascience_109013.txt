[site]: datascience
[post_id]: 109013
[parent_id]: 
[tags]: 
How to do feature selection or feature engineering in datasets with a lot of features?

To make a good ML model, we have to select features that increase model accuracy and, if needed, to "engineer" features (e.g. apply some function like logarithm or square to linear regression predictor if it doesn't linearly correlate with predicted variable). But how can I do that to a large data set with a lot of features? Shall I test every single variable for different conditions, or there are some simpler ways to do feature selection or feature engineering.
