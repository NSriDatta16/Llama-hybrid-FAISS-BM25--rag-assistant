[site]: crossvalidated
[post_id]: 540492
[parent_id]: 539436
[tags]: 
Whuber has already done a better job of that than I could of answering the question, but I have some figures I generated for other work may help illustrate some of the issues. A similar version is in the supporting information associated with this paper on visual exploration of PCA for spectroscopists (open access) Whuber indicated that SVD is good for accurate calculation of eigenvalues and vectors, and from my reading that is a widely held opinion, this arises from the use of matrix decomposition on the covariance matrix, which has the consequence that all eigenvectors are solved in conjunction and computation errors are spread over them all. Sequential/iterative methods such as the power method (sequential fitting of covariance matrix) or NIPALs (sequential fitting of residual) in contrast accrue errors in each eigenvector calculation and these can build up to the point they over power very low variance eigenvectors. In that paper I had implemented PCA using NIPALs and as a sanity check I published data in the supporting information comparing the results to an established python function that used SVD. The first PC was very similar for both algorithms: The difference between them was miniscule ( $10^{-15}$ ), on the order of computational accuracy: The difference was dependent on the specified tolerance for the NIPALs algorithm. Next we see a plot of the SVD rank against the NIPALs rank for the SVD-eigenvector that the NIPALs eigenvector most closely correlated with: At first the rank is perfectly correlated, until we hit NIPALS PC24 where things get wobbly. NIPALS-24 is suddenly best correlated with SVD-1! Above NIPALS-24 the correlations get really messed up. It is not limited to rank, but actual eigenvector shape becomes much less consistent (even if we match by optimum correlation), as seen if we plot the correlation between SVD/NIPALs vs NIPALs rank: This shows that information in the lower rank PCs is getting muddled up in the NIPALS wrt to SVD. If we look at SVD-1 (blue) and NIPALS-24 (orange) and the difference between them (green) we can seen that the two are indeed very similar and the difference is very noisy. As you mention in the OP and Whuber mentions, this is usually handled by projection which also goes by the name of renormalisation or re-orthogonalisation . The process starts by applying the $PC_{1-i-1}$ eigenvectors to the residual and reconstructing (inner product of scores and eigenvectors), then recalculating the residual. If we done this after NIPALS-23 then the residual would have popped up as mostly NIPALS-1. Reconstructing the data at this point would have created a residual without the NIPALS-1 ghost and therefore would not have caused the same level of problems. As Whuber indicates it is not possible to achieve infinitesimal accuracy, so it is a matter of what limits of precision do you need.
