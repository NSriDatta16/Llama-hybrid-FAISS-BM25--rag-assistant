[site]: crossvalidated
[post_id]: 510818
[parent_id]: 226923
[tags]: 
Why do we use ReLUs? We use ReLUs for the same reason we use any other non-linear activation function: To achieve a non-linear transformation of the data. Why do we need non-linear transformations? We apply non-linear transformations in the hope that the transformed data will be (close to) linear (for regression) or (close to) linearly separable (for classification). Drawing a linear function through non-linearly transformed data is equivalent to drawing a non-linear function through original data. Why are ReLUs better than other activation functions? They are simple, fast to compute, and don't suffer from vanishing gradients, like sigmoid functions (logistic, tanh, erf, and similar). The simplicity of implementation makes them suitable for use on GPUs, which are very common today due to being optimised for matrix operations (which are also needed for 3D graphics). Why do we need matrix operations in neural networks? : It's a compact and computationally efficient way of propagating the signals between the layers (multiplying the output of the previous layer with the weight matrix). Isn't softmax activation function for neural networks? Softmax is not really an activation function of a single neuron, but a way of normalising outputs of multiple neurons. It is usually used in the output layer, to enforce the sum of outputs to be one, so that they can be interpreted as probabilities. You could also use it in hidden layers, to enforce the outputs to be in a limited range, but other approaches, like batch normalisation , are better suited for that purpose. P.S. (1) ReLU stands for " rectified linear unit ", so, strictly speaking, it is a neuron with a (half-wave) rectified-linear activation function. But people usually mean the activation function when they talk about ReLUs. P.S. (2) Passing the output of softmax to a ReLU doesn't have any effect because softmax produces only non-negative values, in range $[0, 1]$ , where ReLU acts as identity function, i.e. doesn't change them.
