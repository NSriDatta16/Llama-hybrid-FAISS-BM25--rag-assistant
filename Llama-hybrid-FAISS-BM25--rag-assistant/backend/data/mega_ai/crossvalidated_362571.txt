[site]: crossvalidated
[post_id]: 362571
[parent_id]: 362425
[tags]: 
Perhaps, a more accurate name for ANNs is "differentiable networks", i.e. complex parametrized functions that can be optimized using gradient descent or its variant. This is a very general definition that emphasizes differentiability, but doesn't tell anything about principal ideas, tasks that it's suited for, underlying mathematical framework, etc. Note that differentiability is a trait, not necessary the main. For example, SVM can be trained using gradient descent and thus exhibits properties of a neural/differentiable network, but the main idea is in data separation using hyperplanes. Variational autoencoder uses MLPs for encoder and decoder, but the function you optimize comes from Bayesian statistics, and so on. There's also a few models that are often referred to as neural networks but don't use GD for learning. A good example is RBM. My guess is that the label "neural network" was attached to it mostly for historical reasons - eventually, RBM's creator is Geoffrey Hinton, and Hinton is a neural network guy, right? However, if you analyze the model you'll see that RBM's structure is a Markov net, energy-based cost function comes from statistical physics of the beginning of 20th centenary and MCMC/Gibbs sampling have been developing in parallel and totally independently from neural networks.
