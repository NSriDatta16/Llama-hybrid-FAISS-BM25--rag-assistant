[site]: crossvalidated
[post_id]: 147926
[parent_id]: 147921
[tags]: 
Generally, in Bayesian model you do predictions on new data the same way as you do with non-Bayesian models. As your example is complicated I will provide a simplified one to make things easier to illustrate. Say you want to estimate linear regression model $$ y_i = \beta_0 + \beta_1 x_i + \varepsilon_i $$ and based on the model you want to predict $y_\text{new}$ values given $x_\text{new}$ data. In this case you plug in the $x_\text{new}$ into estimated model and JAGS samples $y_\text{new}$ values based on your model. The code would look similar to this: beta0 ~ dnorm(0, 10) beta1 ~ dnorm(0, 10) sigma ~ dunif(0, 50) for (i in 1:N) { y[i] ~ dnorm(beta0 + beta1 * x[i], sigma) } for (j in 1:Nnew) { ynew[j] ~ dnorm(beta0 + beta1 * xnew[j], sigma) } where y , x and xnew are data vectors and ynew is a variable for storing predictions. What you get is a distribution of values that are plausible given your estimated model. Since the model is probabilistic, the prediction is also probabilistic, i.e. we get the whole distribution of possible ynew values. For point-values take the average of ynew , you can also make prediction intervals taking the highest density intervals from ynew values.
