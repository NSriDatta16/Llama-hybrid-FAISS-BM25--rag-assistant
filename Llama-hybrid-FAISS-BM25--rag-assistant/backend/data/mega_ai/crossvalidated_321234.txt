[site]: crossvalidated
[post_id]: 321234
[parent_id]: 
[tags]: 
Actor-critic loss function in reinforcement learning

In actor-critic learning for reinforcement learning, I understand you have an "actor" which is deciding the action to take, and a "critic" that then evaluates those actions, however, I'm confused on what the loss function is actually telling me. In Sutton and Barton's book page 274 (292 of the pdf) found here http://ufal.mff.cuni.cz/~straka/courses/npfl114/2016/sutton-bookdraft2016sep.pdf they describe the algorithm. I can understand that you want to update the actor by incorporating information about the state-value (determined by the critic). This is done through the value of $\delta$ which incorporates said information, but I don't quite understand why it's looking at the gradient of the state-value function? Shouldn't I be looking at the gradient of some objective function that I'm looking to minimize? Earlier in the chapter he states that we can regard performance of the policy simply as its value function, in which case is all we are doing just adjusting the parameters in the direction which maximizes the value of each state? I thought that that was supposed to be done by adjusting the policy, not by changing how we evaluate a state. Thanks
