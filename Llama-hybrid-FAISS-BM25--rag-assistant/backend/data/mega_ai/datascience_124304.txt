[site]: datascience
[post_id]: 124304
[parent_id]: 
[tags]: 
Is there an online Vicuana model demo that supports plugging in embeddings directly?

I'm doing some work with using multi-modal data with LLMs like LLaMA and Vicuana. For example, something similar to Video-LLaMA, which converts images/videos into embeddings, concatenates it with the text embeddings, and feeds the embeddings into LLaMA. It's difficult for me to run some of the models myself because of computational constraints (and I just want to do a few inferences anyways). I'm wondering if there is an online demo or API that supports inputting embeddings directly (as opposed to text tokens).
