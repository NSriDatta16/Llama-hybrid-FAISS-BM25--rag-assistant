[site]: crossvalidated
[post_id]: 312154
[parent_id]: 312150
[tags]: 
It depends on the data. Let's assume the data is valid, because you labeled it yourself. I try to think of it as a new problem. Imagine this simple example: Let's say this is your data and the line (your DNN) that separates them: Everything is fine, right? So now you get new data and it looks like this: The new black circle is classified wrongly by your old line (blue) and you need to find a new one (green). Hence adaptation is needed. In more technical terms: It seems that the new data you introduce changes the feature space which causes the decision boundary to change also. I would try the following, since you are using a Neural Network: Adapt the batch size. As you may know batch updating introduces noise to the gradient. It could simply be that the samples you take for a batch is too large/too small for 3.3k data points. Change the learning rate decay. I'll just assume you are using this technique. You can try to change the rate and make it slower such that it can adapt to more data. Train your net on the original 2.8k data points and use the weights as a initialization and train the net on the remaining 500 data points. Change the architecture of the DNN. These are the standard ways to approach this problem.
