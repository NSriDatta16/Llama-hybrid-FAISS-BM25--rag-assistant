[site]: crossvalidated
[post_id]: 167201
[parent_id]: 167171
[tags]: 
It's not really clear what is going on in this paper as the authors seem to have breezed over some important information. From what it appears, they seem to want to estimate all the parameters in IRT models through a single call to R's optim function, but unless further constrains are supplied to the optimization process this simply will not work (a similar approach could be used for linear factor analysis, and will suffer the same problem). In the typical approach to using logistic regression in IRT, the latent trait values ($\theta$) are realized by simply using the (potentially rescaled) total/sum score as a proxy for each participants ability, $\theta_i$. Admittedly this is a bit of a hack, but for bias detection methods in unidimensional tests where a grouping variable is included it has seen some success. Here the authors are trying at a joint maximum-likelihood approach but do not seem to be imposing any additional constraints on the distribution of the ability parameters. Hence, the optimizer really won't really know where to go and essentially just returns the starting values (i.e., the mean of the abilities could arbitrarily increase to explain successful item endorsement, or alternatively the difficulty of the items could decrease to indicate that the items were just really easy. Both are valid because the model is not sufficiently identified). Here's some R code to try and reproduce what the authors are doing. Note that the cost function only seems to change the value of the objective function. library(mirt) dat 1 - tol, 1 - tol, ret) ret } L ncol(X)) X
