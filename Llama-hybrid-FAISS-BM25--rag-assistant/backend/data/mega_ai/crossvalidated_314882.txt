[site]: crossvalidated
[post_id]: 314882
[parent_id]: 314677
[tags]: 
I don't think it's possible to provide meaningful confidence intervals in that case in a natural way. Call $\theta$ the parameter and $\hat\theta$ the penalized likelihood estimator. Confidence intervals at 95% say that $P(|\theta-\hat\theta|\leq d|\theta)\geq0.95$ for all $\theta$ . If you reframe the Bayesian idea in a back and white fashion: the reason why you need penalization is that not all $\theta$ are expected to really be "possible": you focus on the ones that you consider to be "possible". Outside of the "possible" zone, $\hat\theta$ is a very poor estimate. If the real $\theta$ is far from the possible zone $\hat\theta$ can be far from the real $\theta$ with high probability. For such $\theta$, $P(|\theta-\hat\theta|\leq d|\theta)\geq0.95$ is only true if $d$ is big. Since $d$ is required to not depend on $\theta$, you would just use the worst case scenario $d$ yielding useless confidence intervals. Actually, I think $d=+\infty$ if you consider $\theta$ going to infinity. A way to fall back on a frequentist analysis in this case is to define a "possible" region $\Theta$ inspired from the penalization. One possibility is a region containing 99% of the weight of the prior. With $L^2$ regularized MLE for example, if this region is a ball whose radius is exactly the norm of the penalized estimate, then the frequentist MLE raw estimate is the same as the penalized one and lies on the border (sphere). With this method, you can say: if $\theta\in \Theta$ then $P(|\theta-\hat\theta|\leq d|\theta)\geq0.95$ with a meaningful $d$. It is a confidence interval with condition $\theta\in \Theta$.
