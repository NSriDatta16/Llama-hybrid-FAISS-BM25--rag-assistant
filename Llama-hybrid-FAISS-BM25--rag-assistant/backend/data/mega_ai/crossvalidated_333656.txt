[site]: crossvalidated
[post_id]: 333656
[parent_id]: 333648
[tags]: 
These regularization techniques are often encountered with variable selection. Assume you have a regression model $$ Y=\beta_0+\beta_1X_1+\ldots+\beta_NX_N+\epsilon. $$ Your intention is select a few predictors and hence use a prediction model with those predictors. You will often go for the $X_i$ with the largest $\beta_i$. It turns out that if you first select and try to estimate $\beta_i$, your estimator $\hat \beta_i$ is positively biased (i.e., larger on average than $\beta_i$). Regularization is used to reduce this positive bias. Say you were to estimate $\mathbf{\beta}=(\beta_0,\beta_1,\ldots,\beta_N)^T$ by least squares, then you will do something like $$ \hat \beta=\text{argmin}_\mathbf{\beta}\left\{(Y-\mathbf{X}\mathbf{\beta})^2\right\} $$ and return the largest $\hat \beta_i$s. Like I said these $\hat \beta_i$ s are positively biased. To counteract this bias you could regularize your least square action. Instead find $$ \hat \beta^\text{Ridge}=\text{argmin}_\mathbf{\beta}\left\{(Y-\mathbf{X}\mathbf{\beta})^2+\lambda \sum_{i=1}^{N}\beta_i^2\right\} $$ or $$ \hat \beta^\text{Lasso}=\text{argmin}_\mathbf{\beta}\left\{(Y-\mathbf{X}\mathbf{\beta})^2+ \lambda\sum_{i=1}^{N}|\beta_i|\right\} $$ The ridge uses the $L_2$ regularization and the lasso uses the $L_1$ regularisation. In general we could refer to the type of regularizations as $L_p$ where an $L_p$ regularazation takes the form $$ \hat \beta^\text{general}=\text{argmin}_\mathbf{\beta}\left\{(Y-\mathbf{X}\mathbf{\beta})^2+\lambda\sum_{i=1}^{N}|\beta_i|^p.\right\} $$ So you see that when $p=1$ we get the lasso and $p=2$ the ridge and so on. Mathematicians general refer to the function say $L_p(\mathbf x)=\left(\sum_{i=1}^{N}|x_i|^p\right)^{1/p}$ as a norm placed on the vector $\mathbf x=(x_1,\ldots,x_N)^T$.
