[site]: crossvalidated
[post_id]: 177584
[parent_id]: 177227
[tags]: 
Let's look at the formula: I(X,Y) measures the shared information between X and Y. p(x,y) is the joint probability density function. The marginal p(x) describes the relative likelihood for the random variable X to take on a particular value x (also, a probability density function). The question is actually whether the random sample is representative of the population marginal distribution. Each time you take a random sample for one vector you have a different p(x) for vector x. If your sample is large enough and you're lucky, then this should be a good approximation and you're done. Now, let's say you want to be more confident about this approximation. You repeat several times and you get multiple estimations of this distribution. Average these and you have a good approximation of the population distribution, which usually turns out to be a gaussian. This is called the Central limit theorem which states that if a sufficiently large number of samples is taken from a population (given some conditions), then the distribution will be normal. TL/DR : You need to randomly sample without replacement multiple times. In your case, this means you randomly select rows from your dataset, let's say S=1000 rows / observations as in your example and repeat this N=10 times. Average the results and you should get an accurate approximation. Adjust S and N according to your computational constraints. " Dice sum central limit theorem " by Cmglee shows how the approximation gets better as n increases.
