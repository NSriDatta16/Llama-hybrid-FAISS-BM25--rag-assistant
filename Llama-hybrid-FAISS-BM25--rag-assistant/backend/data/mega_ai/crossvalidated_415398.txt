[site]: crossvalidated
[post_id]: 415398
[parent_id]: 415386
[tags]: 
Key difference is in the estimation of the return. For policy gradient method (disregarding the baseline term which is only used to reduce variance) you have: $\sum_{t^{\prime} = t}^T \gamma^{t^{\prime} - t}r(s_{i,t^{\prime}}, a_{i,t^{\prime}})$ and for actor-critic method you have: $r ( s _ { i , t } , a _ { i , t } ) + \gamma \hat { V } _ { \phi } ^ { \pi } ( s _ { i , t + 1 })$ Policy gradient return estimate is unbiased because all rewards at all timesteps are sampled from the environment, so that gives us unbiased estimate. Actor-critic return estimate is biased because $\hat { V } _ { \phi } ^ { \pi } ( s _ { i , t + 1 })$ term is biased. It is biased because it is an approximation of the expected return at state $s _ { i , t + 1 }$ . This term is represented by an approximator, for example a neural network or a linear regression model. That approximator will usually be randomly initialized so it will not give a true estimation of the return, it will be biased towards some random value that was initialized with.
