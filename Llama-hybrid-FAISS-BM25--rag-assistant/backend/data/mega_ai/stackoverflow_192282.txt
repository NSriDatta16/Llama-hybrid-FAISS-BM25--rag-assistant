[site]: stackoverflow
[post_id]: 192282
[parent_id]: 170452
[tags]: 
This is an interesting little exercise, but I would urge you to consider it nothing more than an introduction to the concept of the difference in natural language between types and tokens. A type is a single instance of a word which represents all instances. A token is a single count for each instance of the word. Let me explain this with the following example: "John went to the bread store. He bought the bread." Here are some frequency counts for this example, with the counts meaning the number of tokens: John: 1 went: 1 to: 1 the: 2 store: 1 he: 1 bought: 1 bread: 2 Note that "the" is counted twice--there are two tokens of "the". However, note that while there are ten words, there are only eight of these word-to-frequency pairs. Words being broken down to types and paired with their token count. Types and tokens are useful in statistical NLP. "Lexical encoding" on the other hand, I would watch out for. This is a segue into much more old-fashioned approaches to NLP, with preprogramming and rationalism abound. I don't even know about any statistical MT that actually assigns a specific "address" to a word. There are too many relationships between words, for one thing, to build any kind of well thought out numerical ontology, and if we're just throwing numbers at words to categorize them, we should be thinking about things like memory management and allocation for speed. I would suggest checking out NLTK, the Natural Language Toolkit, written in Python, for a more extensive introduction to NLP and its practical uses.
