[site]: datascience
[post_id]: 13232
[parent_id]: 13185
[tags]: 
Yours is not an example of nested cross-validation. Nested cross-validation is useful to figure out whether, say, a random forest or a SVM is better suited for your problem. Nested CV only outputs a score, it does not output a model like in your code. This would be an example of nested cross validation: from sklearn.datasets import load_boston from sklearn.cross_validation import KFold from sklearn.metrics import mean_squared_error from sklearn.grid_search import GridSearchCV from sklearn.ensemble import RandomForestRegressor from sklearn.svm import SVR import numpy as np params = [{'C': [0.01, 0.05, 0.1, 1]}, {'n_estimators': [10, 100, 1000]}] models = [SVR(), RandomForestRegressor()] df = load_boston() X = df['data'] y = df['target'] cv = [[] for _ in range(len(models))] for tr, ts in KFold(len(X)): for i, (model, param) in enumerate(zip(models, params)): best_m = GridSearchCV(model, param) best_m.fit(X[tr], y[tr]) s = mean_squared_error(y[ts], best_m.predict(X[ts])) cv[i].append(s) print(np.mean(cv, 1)) By the way, a couple of thoughts: I see no purpose to grid search for n_estimators for your random forest. Obviously, the more, the merrier. Things like max_depth is the kind of regularization that you want to optimize. The error for the nested CV of RandomForest was much higher because you did not optimize for the right hyperparameters, not necessarily because it is a worse model. You might also want to try gradient boosting trees.
