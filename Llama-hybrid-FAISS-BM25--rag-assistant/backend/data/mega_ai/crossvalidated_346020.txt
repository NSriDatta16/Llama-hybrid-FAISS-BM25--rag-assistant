[site]: crossvalidated
[post_id]: 346020
[parent_id]: 345737
[tags]: 
First of all there are plenty of regularization methods both in use and in active research for deep learning. So your premise isn't entirely certain. As for methods in use, weight decay is a direct implementation of an L2 penalty on the weights via gradient descent. Take the gradient of the squared norm of your weights and add a small step in this direction to them at each iteration. Dropout is also considered a form of regularization, which imposes a kind of averaged structure. This would seem to imply something like an L2 penalty over an ensemble of networks with shared parameters. You could presumably crank up the level of these or other techniques to address small samples. But note that regularization implies imposition of prior knowledge. The L2 penalty on the weights implies a Gaussian prior for the weights, for example. Increasing the amount of regularization essentially states that your prior knowledge is increasingly certain and biases your result towards that prior. So you can do it and it will overfit less but the biased output may suck. Obviously the solution is better prior knowledge. For image recognition this would mean much more structured priors regarding the statistics of your problem. The problem with this direction is you are imposing lots of domain expertise, and avoiding having to impose human expertise was one of the reasons you used deep learning.
