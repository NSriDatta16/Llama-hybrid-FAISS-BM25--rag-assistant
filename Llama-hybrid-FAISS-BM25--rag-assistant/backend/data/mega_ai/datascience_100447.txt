[site]: datascience
[post_id]: 100447
[parent_id]: 100392
[tags]: 
It might depend on what embedding algorithm you are referring to, but you often decide your vocabulary size in advance, and any words are given the OOV (out of vocab) or UNK (unknown) token. Sorting your vocabulary by frequency (in a representative sample of data) is the most sensible way to decide which words make the cut, and which get discarded. Does it have any effects, or is it arbitrary? As described above, it is arbitrary, and you should get identical results by randomly shuffling your word list, and then using that. As a possible aside, embeddings now often use the output of SentencePiece or a similar algorithm like BPE. These work by being given a target vocabulary size, and breaking words up into tokens, to avoid having any UNK tokens. This looks like a good article, or searching for BPE or SentencePiece. but the basic idea is that the most frequent strings of characters become tokens in their own right. So (for English) the letters 'a' to 'z' are tokens, but then things like 'it', 'and', 'the' and so on will become tokens. But if the corpus is all about American cities, you might get multiple word strings such as 'New York' become a token too. But, again, the actual number assigned to each token is arbitrary.
