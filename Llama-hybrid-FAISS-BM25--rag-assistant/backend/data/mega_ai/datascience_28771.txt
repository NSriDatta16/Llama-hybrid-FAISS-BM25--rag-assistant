[site]: datascience
[post_id]: 28771
[parent_id]: 28770
[tags]: 
Models with direct regularization on their weights benefit from this. These regularizations add a prior to the model and punishes high weights. If your variables are not in the same range then the regularization is not the same for each input. Search for weight regularization. Another form is where in neural networks the weights are initialized in a way that they expect the inputs to be normally distributed. If they are far from that, the impact in the later layers can be fairly big, greatly impacting convergence and numerical stability. On the other hand, tree based methods don't care at all usually.
