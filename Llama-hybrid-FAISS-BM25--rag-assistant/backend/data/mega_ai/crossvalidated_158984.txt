[site]: crossvalidated
[post_id]: 158984
[parent_id]: 158982
[tags]: 
1) When you train your classifier or your regressor, the goal if to maximize $\rho$. It actually corresponds to the geometrical margin for the case of SVM classification!. So there is one value, the one you which is optimized. Check slide 30. Here $\rho$, the margin, is introduced in an abstract way, using the properties of the Rademacher complexity to bound the empirical error, so you can have an estimate of how well your algorithm generalizes. Concretely, he introduces the Lipschitz function depicted in slide 28 (the hinge loss) to bind the 0-1 loss. It is just to get a bound on the Rademacher complexity. Notice that, in principle, that value is arbitrary. But, in practice, you want the bound to be as tight as possible. And that corresponds to the maximum margin you are used to think of in geometrical terms. What this bound is telling you is: given test data sampled from the same distribution as the training data (which is supposed to be stationary), how likely is that you perform as well on the new data as on the training data?. Is about analysing the stability of your classifier. 2) The hypothesis class is given by the choice you make of your kernel. Given the kernel, by the Mercer theorem , the $\Phi$'s are also determined.
