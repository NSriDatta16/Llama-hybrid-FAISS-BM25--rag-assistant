[site]: crossvalidated
[post_id]: 490383
[parent_id]: 490379
[tags]: 
There is a balance between the margin width and the misclassification error (depending on the slack variables). Apparently $C=1$ favours the margin width more. If you increase it, forcing it to penalise misclassification more severely, it'll find a better split point. from sklearn import svm import numpy as np clf = svm.SVC(C=1, kernel='linear') X = [[0.], [0.5]] y = [0, 1] clf.fit(X, y) print('coefs: ', clf.coef_) print('svs: ', clf.support_vectors_) if np.all(y == clf.predict(X)): print('classification worked') else: print('classification failed:') print('X=', X, ',y=', y, ' ,prediction=', clf.predict(X)) print('\n\n') X.append([0.4]) y.append(1) clf = svm.SVC(C=10, kernel='linear') clf.fit(X, y) print('coefs: ', clf.coef_) print('svs: ', clf.support_vectors_) if np.all(y == clf.predict(X)): print('classification worked') else: print('classification failed:') print('X=', X, ',y=', y, ' ,prediction=', clf.predict(X)) which yields the following coefs: [[0.5]] svs: [[0. ] [0.5]] classification worked X= [[0.0], [0.5]] ,y= [0, 1] ,prediction= [0 1] coefs: [[4.]] svs: [[0. ] [0.4]] classification worked X= [[0.0], [0.5], [0.4]] ,y= [0, 1, 1] ,prediction= [0 1 1]
