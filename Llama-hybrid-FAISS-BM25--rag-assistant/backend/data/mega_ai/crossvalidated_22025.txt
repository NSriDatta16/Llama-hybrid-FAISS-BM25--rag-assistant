[site]: crossvalidated
[post_id]: 22025
[parent_id]: 22021
[tags]: 
There is no universally followed convention on what a "margin of error" is but I think (as you have observed) it is most often used as meaning the radius of a confidence interval , either in the original scale of the estimate or as a percentage of an estimate. Sometimes it is used as synonymous with the "standard error", so you need to be careful that others understand what you mean when you use it. A "confidence interval" does have universal convention on its meaning. It basically is the range of possible estimates generated by an estimating process that would, X% of the time (95% being the most commonly used) contain the true value of the parameter being estimated. This concept of a "process" that would produce the true value X% of the time is a bit counter-intuitive and not to be mixed up with a "credibility interval" from Bayesian inference, which has a much more intuitive definition, but is not the same thing as the widely used confidence interval. Your actual quote is a little messy and needs some minor fixing as described. I would avoid this additional use of the word "margin" and favour "error bars". So: "Confidence intervals are estimated as 1.96 multiplied by the relevant standard errors and shown on the graphs as error bars." (This puts aside the question whether this is a good way to calculate confidence intervals, which depends on your model etc and isn't relevant). Final comment on terminology - I don't like "standard error", which just means "the standard deviation of the estimate"; or "sampling error" in general - I prefer to think in terms of randomness, and the variance of statistics, rather than "errors". But I slipped into using the term "standard error" above because it is so widely used I guess.
