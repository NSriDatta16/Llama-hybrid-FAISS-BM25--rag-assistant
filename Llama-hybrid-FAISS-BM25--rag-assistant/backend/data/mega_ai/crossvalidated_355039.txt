[site]: crossvalidated
[post_id]: 355039
[parent_id]: 
[tags]: 
Why don't neural networks get stuck in loops when they overshoot a backprop step?

In a normal feedforward network I wrote with linear activations I've noticed that after a while when the network has found a pretty viable solution to a problem it sometimes takes a step in the wrong direction. At this point, the learning rate is very small but the steps still causes a noticeable change in the error. What appears to follow is quite puzzling, the error, weights and biasses all grow very until eventually, they become too big and the network breaks. My hypothesis is that when it takes a step in the wrong direction the error goes up and so does the learning rate. These two factors combine so that when the weights and biasses are adjusted to take a step in the opposite direction they are the derivatives are larger and so are the adjustments. For this reason on the way back in the other direction, it overshoots even further than the time before, ultimately creating an infinite loop causing the error, weights, biasses and learning rate to grow exponentially. (I've got a cap on the learning rate so it maxes out at like 0.05). This, however, is just a hypothesis. If this is possible how is it normally handled... If this isn't it and I've just made a mistake why doesn't this happen? Thanks for your help in advance.
