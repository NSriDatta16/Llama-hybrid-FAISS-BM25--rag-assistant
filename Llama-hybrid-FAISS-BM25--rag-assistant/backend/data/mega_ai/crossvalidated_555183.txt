[site]: crossvalidated
[post_id]: 555183
[parent_id]: 
[tags]: 
Gaussian Processes: multi-class Laplace approximation

In Chapter 3.5 of Gaussian Process for Machine Learning book by Rasmussen and Williams (R&W 2006), authors present a Laplace approximation for a multi-class Gaussian Process (GP) classifier . After going through their proposed inference (Algorithms 3.3 and 3.4), I noticed a crucial step in achieving their claimed time complexity (linear in the number of classes) that authors do not explain and that I could not prove myself. In the following, I will try to present the problem in a succinct way. Let's first introduce a vector of latent function values at all N training points and for all C classes $$ \textbf{f} = (f_{1}^1, \dots,f_{N}^1,f_{1}^2, \dots,f_{N}^2,\dots,f_{1}^C, \dots,f_{N}^C)^T \in \mathbb{R}^{CN} $$ (so with $f_{i}^c$ we denote a latent variable for the $c$ -th channel of the $i$ -th training point). The GP prior has the form $\textbf{f} \sim \mathcal{N}(0, K)$ , where $K \in \mathbb{R}^{CN \times CN}$ is the kernel matrix. Latent channels are additionally assumed to be independent, which gives rise to the block-diagonal kernel matrix $K = diag(K_1,\dots,K_C), \: K_c \in \mathbb{R}^{N \times N}$ . With $\mathbf{\pi}$ we denote the softmax of latent function values vector $\mathbf{f}$ , i.e. $$\pi_i^c = \frac{exp(f_i^c)}{\sum_{c'}exp(f_i^{c'})}.$$ In Algorithm 3.3, the authors then present an approach to compute GP marginal likelihood with $O(CN^3)$ time complexity. To achieve linear scaling in the number of classes, authors rely on the following result $$\log |I_{CN} + W^{1/2} K W^{1/2}| = \sum_{c=1}^C \log|I_N + D_c^{1/2} K_c D_c^{1/2}|$$ where $D_c := diag(\pi^c)$ and $\pi^c \in \mathbb{R}^N$ is the subvector of $\mathbf{\pi}$ pertaining to class $c$ . Matrix $W \in \mathbb{R}^{CN \times CN}$ corresponds to the Hessian of classification log-likelihood w.r.t. $\mathbf{f}$ and has the form $W := diag(\mathbf{\pi}) - \Pi \Pi^T$ , where $\Pi \in \mathbb{R}^{CN \times N}$ is a matrix obtained by vertically stacking $D_c$ matrices. Note in particular, that the matrix $W$ is not diagonal. Could anyone point out if the above log-determinant equality holds (preferably with proof)? I was not able to prove it myself using matrix determinant lemma . I also observed that none of the "mainstream" implementations of multi-class GP classifiers relies on the algorithm from R&W 2006, e.g. sklearn implementation uses one-vs-rest or one-vs-one approaches to tackle multi-class GP problems.
