[site]: datascience
[post_id]: 85326
[parent_id]: 81248
[tags]: 
By default, BERT fine-tuning involves learning a task-specific layer (For classification task, a neural network on top of the CLS token), as well as update the existing parameters of the model to adapt for the task. Thus, it's both, new layer + BERT model weights. However, you still have a choice of using just the emebdding of CLS token and train only the layer on top of it to reduce the training complexity. However, its a matter of trade-off between performance and the compute cost.
