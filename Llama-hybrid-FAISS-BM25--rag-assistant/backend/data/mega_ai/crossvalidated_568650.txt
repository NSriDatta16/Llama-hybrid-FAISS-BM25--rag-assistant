[site]: crossvalidated
[post_id]: 568650
[parent_id]: 567085
[tags]: 
Taking random subsets of the data, training a model on each, and averaging their predictions is not a new idea . This is what random forest does, it is a very popular approach in machine learning in general. This however is usually about averaging the predictions of the models rather than the parameters. You can also average the parameters, but this would not work in all the cases. It doesn't sound like a great idea for neural networks. Take as an example a convolutional neural network for visual images, because they are great for illustrating things. Early layers of such network (as for other networks) work as feature detectors. In the example below from Krizhevsky et al, each neuron (kernel) in the layer detects a different edge or texture. Say that you trained this network multiple times, on different random subsets of the data. Let's say that all the networks have learned exactly the same features (unlikely to happen, they would rather be similar but not the same), but by chance, they are just randomly shuffled for each model (due to data shuffling, weights initialization). If you averaged those kernels, all the boxes on the image below would be uniformly gray, they wouldn't do anything. This is because neurons of the neural networks are redundant . Averaging the parameters could work if you averaged the parameters that have the same function in the model. For example, if you have a linear regression model $$ y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \varepsilon $$ you could average $\beta_0,\beta_1,\beta_2$ parameters from different models using the same $X_1,X_2$ features.
