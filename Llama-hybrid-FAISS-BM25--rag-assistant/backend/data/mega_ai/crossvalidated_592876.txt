[site]: crossvalidated
[post_id]: 592876
[parent_id]: 592868
[tags]: 
The backshift operator is a mapping (an "operator") between vector spaces, namely spaces of time series or sequences, $$ B\colon \mathbb{R}^\mathbb{N} \to \mathbb{R}^\mathbb{N}, (y_i)\mapsto (y_{i-1}). $$ Here, $\mathbb{R}^\mathbb{N}$ is the space of mappings from $\mathbb{N}$ to $\mathbb{R}$ , i.e., of real-valued sequences indexed by natural numbers. Time series that are not infinite can be accommodated by having only finitely many observations nonzero. The space $\mathbb{R}^\mathbb{N}$ is naturally a real vector space: we can add time series and multiply them by real scalars. In functional analysis, there is a notion of differential of an operator between normed vector spaces. If an operator $f$ admits a "local linear approximation" $\varphi$ near a point $x$ , then we say that $f$ is differentiable and that $\varphi$ is its differential at $x$ (see, e.g., Coleman, 2012 , section 2.2). Note that this generalizes the familiar notion of differentiability of mappings between finite dimensional spaces: a function $f\colon\mathbb{R}^n\to\mathbb{R}^m$ is differentiable at a point $x$ if and only if it admits a well-defined tangential subspace (tangent line in the most familiar case of $n=m=1$ ) near $x$ . Now, we observe that $B$ is already linear: $$ B\big(\lambda(y_i)\big) = \lambda B\big((y_i)\big)\quad\text{and}\quad B\big((y_i)+(z_i)\big) = B\big((y_i)\big)+B\big((z_i)\big). $$ Thus, a (unique!) "best approximation" to $B$ is $B$ itself ( see here at Math.SE ). Therefore, the differential of $B$ at all "points" (i.e., sequences) is $B$ itself. Note that we don't even need to think about what norm we put on our space $\mathbb{R}^\mathbb{N}$ .
