[site]: crossvalidated
[post_id]: 439191
[parent_id]: 438875
[tags]: 
It's true that you're not missing information when you use only $k-1$ categories. In linear models, we are all familiar with the dummy variable trap and the relationship between a model with $k-1$ levels and an intercept and a model with $k$ levels and no intercept. However , you're using a tree-based model, so the mechanics of how recursive binary splits work are important! In the case of a factor with 2 levels, e.g. "red" and "blue", it's obvious that using the $k-1$ 1hot method is equivalent to choosing the $k$ 1-hot method. This is because NOT blue implies red . In this case, there is no difference. But for $k>2$ categories, you'll need $k-1$ binary splits to isolate the the omitted level (the $k$ th level). So if you have 3 levels, e.g. "red", "green", "blue", but you only include 1-hot features for "red" and "green", it will take 2 successive splits to isolate the "blue" samples. This is because if you split on "red", the children are nodes for red and NOT red = green OR blue . To isolate "blue" when the category "blue" is omitted from the coding scheme, you'll have to split again on "green" because then the children nodes of green OR blue will be blue and green . As $k$ increases, this problem becomes more pronounced, as you'll require more splits. This may interact with your other hyperparameters in strange ways, because specifying a maximum tree depth is a common strategy to avoid overfitting with boosted trees/ xgboost . If isolating category $k$ isn't important, then this effect may not matter at all for your problem. But if category $k$ is important, you'll tend to grow very deep trees to try and isolate it, either via the categorical variables or else by identifying latent interactions of other variables.
