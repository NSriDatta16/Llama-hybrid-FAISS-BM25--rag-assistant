[site]: datascience
[post_id]: 102660
[parent_id]: 102650
[tags]: 
I wonder what you mean by "Everything after (and including) the 1st LSTM layer outputs the same value"? It's not technically possible for that to be true and for the loss to be changing. Knowing nothing else about your model, it looks like it's probably over-parameterized relative to your dataset and/or training resources. The layers that sticks out most to me are Processing = layers.Reshape((12,9472))(encoder) Processing = layers.Dense(128, activation='relu')(Processing) This first dense layer will have ~1.2M parameters. This then leads into your (bidirectional) LSTM layers which each will have much smaller parameter sizes of 200k or less. What is probably happening is that CTC loss is guiding the network to getting marginally closer to correct answers but because CTC looks at probabilities rather than the categorical output, these changes aren't apparent. And the very large parameter space is causing this to go quite slowly. If I'm right about the overparameterization, the following should help a lot: Cut the top dense layer. Consider reducing your input vector size (9.5k is pretty huge IMO). Either reduce the size of the last two dense layers. I'm assuming l is less than 358, in which case you are going from 64 -> 358 -> less, creating a bottleneck. There's unlikely to be a benefit from making those last dense layers larger than the previous layer. Or, cut one or both of the last two dense layers. As long as your loss is improving, you're actually decent shape, you'll just have to design a network that make those improvements useful. One last note: Are you using CNNs for part of your network? I'm guessing you're referring to this demo . I would rebuild that demo almost exactly to start, and then tweak the network from there. For OCR having a CNN to do the initial image processing is usually extremely helpful.
