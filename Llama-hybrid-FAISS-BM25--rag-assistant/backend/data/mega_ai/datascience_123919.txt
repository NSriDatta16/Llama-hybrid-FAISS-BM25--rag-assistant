[site]: datascience
[post_id]: 123919
[parent_id]: 
[tags]: 
Help understanding working of KeyBERT for keyphrase extraction

I'm fairly new to reading and understanding research papers, so I wanted to get a second opinion on whether my understanding of KeyBERT was correct. Here is a high level overview of my understanding with an example - example sentence - “I like reading. I enjoy swimming. Reading is fun.” let stop_words = None, ngram_range = (2, 2) that means bigrams will be considered, let number of candidate words we want to return ( top_n ) = 2 steps - sentences will be split into unique bigrams ( 2 words ) - [ ‘i like’, ‘like reading’, ‘i enjoy’, ‘enjoy swimming’, ‘reading is’, ‘is fun’ ] this list is passed to BERT for embedding. BERT will encode each element of the list and return a list of vectors v1,...,v6 The document is also passed to BERT for embedding, and a vector v7 representing the document is returned. if seed_keywords, i.e. words that should be given more importance are given as a parameter, they are also embedded into vectors, and mean, say c1, of those vectors is taken. This vector c1 is then added to document vector v7. it computes a similarity score between each of the candidate embedding vectors v1,v2,...,v6 with the document embedding vector v7. it uses cosine similarity for this. it sorts the candidate vectors in descending order by similarity score, and returns the top 2 ( top_n ) vectors Let me know if I'm missing any details. Also, are there any better alternatives for the task of keyphrase extraction for specific domain?
