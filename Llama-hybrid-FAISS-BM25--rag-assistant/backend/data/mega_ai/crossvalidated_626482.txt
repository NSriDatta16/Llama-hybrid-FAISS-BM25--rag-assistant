[site]: crossvalidated
[post_id]: 626482
[parent_id]: 
[tags]: 
Hypothesis testing on a model's inference

I have the following situation, which I've mapped to an example below. At the conclusion of an A/B test done on a website I've a metric I care about. Let's assume its the number of users clicking on a signup button with the control set of users seeing a blue colored button while the treatment sees a red button. We observe a statistically significant lift in proportion of users clicking on the signup button in treatment vs control. So far so good. Separately a machine learned model M which has been trained on a set of user features F , forecasts whether the customer will stay in the service and not cancel the service in future weeks conditional on them signing up for the service. The model has been trained independently of the bucket test of course. The forecasts the model makes is for (say) 1 week after a customer signs up. My question is Is there a way to do a statistical hypothesis test on model's forecast for the 1 week window? My attempt is the following. For each of the weeks have the model M emit the average probability of customers in the control group staying in the service $p_1$ its pooled uncertainty of its forecasts $\sigma_1^2$ and compare that to the treatment group's metrics $p_2$ and $\sigma_2^2$ alongwith the standard error of the proportions, eventually yielding $ z = \frac{p_2 - p_1}{\sqrt{\frac{p_1(1-p_1)}{n} + \sigma_1^2 + \sigma_2^2} }$ Am I thinking about this right? Or is the act of using model forecasts in hypothesis testing fundamentally flawed? Any guidance / pointers appreciated.
