[site]: datascience
[post_id]: 80402
[parent_id]: 80389
[tags]: 
An easy way to think about autoencoders is: how well a prticlar pice of infrmaton can be reconstrcted frm its reducd or otherwse comprssed reprsentaton. If you made it this far it means that you sucessfully reconstructed the previous sentence by using only 92 of its original 103 characters. More specifically, autoencoders are neural networks that are trained to learn efficient data codings in an unsupervised manner. The aim is to learn a representation of a given dataset, by training the network to ignore "not important" signals like noise. Typically AE are considered for dimensionality reduction. Practically, an AE initially compresses the input data into a latent-space representation reconstructs the output from this latent-space representation calculates the difference between the input and output which is defined as reconstruction loss. In this training loop, the AE minimises this reconstruction loss so that the output is as similar to the input as possible.
