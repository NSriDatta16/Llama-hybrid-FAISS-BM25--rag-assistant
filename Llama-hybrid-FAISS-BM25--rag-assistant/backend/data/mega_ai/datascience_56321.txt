[site]: datascience
[post_id]: 56321
[parent_id]: 
[tags]: 
Interpretability of RMSE and R squared scores on cross validation

I'm working on a regression problem with 30k rows in my dataset, decided to use XGBoost mainly to avoid processing data for a quick primitive model. And i noticed upon doing cross-validation that there's a noticeable difference between R² for train and R² for CV => clear signs of overfitting. Here's my code for CV : oof_train = np.zeros((len(train_maisons))) ind = 0 cv_scores = [] train_scores=[] for ind,(ind_train,ind_val) in (enumerate (kfolds.split(X,y))): #ind=+1 X_train,X_val = X.iloc[ind_train],X.iloc[ind_val] y_train,y_val = y.iloc[ind_train],y.iloc[ind_val] xgb = XGBRegressor(colsample_bytree=0.6,gamma=0.3,learning_rate=0.1,max_depth=8,min_child_weight=3,subsample=0.9,n_estimators=1000,objective='reg:squarederror',eval_metric='rmse') xgb.fit(X_train,y_train) val_pred = xgb.predict(X_val) train_pred = xgb.predict(X_train) oof_train[ind_val] += val_pred score_fold_validation=np.sqrt(mean_squared_error(y_val, val_pred)) score_fold_train=np.sqrt(mean_squared_error(y_train, train_pred)) train_scores.append(score_fold_train) cv_scores.append(score_fold_validation) #r2_score(y_val, grid.best_estimator_.predict(X_val)) print('Iteration : {} - CV Score : {} - R² Score CV : {} - Train Score : {} - R² Score train : {}'.format(str(ind+1),score_fold_validation,r2_score(y_val, val_pred),score_fold_train,r2_score(y_train,train_pred))) end_train_score=np.mean(train_scores) train_scores.append(end_train_score) end_cv_score=np.mean(cv_scores) Using SquaredError as objective ( loss function ) , evaluating with RMSE and R², here are the metrics' outputs : CV Score : 96416.84137549331 - R² Score CV : 0.6545903695464426 - Train Score : 30605.655815355676 - R² Score train : 0.9730563148067477 My question : is this considered an overwhelming overfitting problem? or is it mild? and should I do more feature engineering or tune hyperparameters more? ( used GridSearchCV for the current hyperparameters ). And one last thing, is my result on X_train indicative that my features are informative enough to learn the target? or is the R² train score somehow biased ? Note : In this code i'm using 10 folds for CV. Used 3 folds gave me a better result on CV, if someone can also explain that , it would be great.
