[site]: crossvalidated
[post_id]: 532184
[parent_id]: 532157
[tags]: 
It may be worth a quick review of interactions. To keep things simple we will look at a linear model with an interaction between a continuous and a categorical (binary in this case) variable. Suppose we have the following data: Here both lines are parallel, so we don't expect to find an interaction: > lm(y ~ x * g, data = dt) %>% summary() Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 2.66783 0.29409 9.072 1.47e-14 *** x 1.03295 0.04740 21.794 And indeed we have no interaction. And from this output we can also say: The intercept for group A is 2.7 The intercept for group B is 2.7 + 2.5 = 5.2 The slope of x for both groups is 1 and this of course makes sense in terms of the plot. Now, suppose we have this situation: Here it is clear that the slopes of the 2 lines are not the same, hence we expect to find an interaction: > lm(y ~ x * g, data = dt) %>% summary() Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 2.97603 0.27725 10.73 And indeed we do. We can interpret this as follows: The intercept for group A is 3 The intercept for group B is 3 + 2.3 = 5.3 The slope of x for group A is 1 The slope of x for group B is 1 + 1.4 = 2.4 Now, suppose we have: Again it is abundantly clear that there is a difference in slope so we expect to find an interaction: > lm(y ~ x * g, data = dt) %>% summary() Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 2.97603 0.27725 10.734 which indeed we do, but this time we also see that the main effect for x is zero, which of course makes sense because the line in the plot for A is flat, since the estimate for x tells us the association of y with x when g is at it's reference level ( A ). But now suppose that we remove the main effect for x from the model, since it was "not significant": > lm(y ~ g + x:g , data = dt) %>% summary() Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 2.97603 0.27725 10.734 So we can see that removing the main effect of x from the model is basically a reparamterisation of the previous model. Instead of getting a main effect of zero for x , which make sense because of the flat line for the group A , we have an additional interaction term gA:x , along with the interaction gB:x which is equivalent to the interaction estimate from the previous model. For this reason alone I think that removing a non-significant main effect from a model is a bad idea because it makes interpretation less intuitive. Also see here for other thoughts on why it is generally not a good idea to remove main effects when a variable is included in an interaction: Logistic Regression Models Without Main Effects? Including the interaction but not the main effects in a model Do all interactions terms need their individual terms in regression model?
