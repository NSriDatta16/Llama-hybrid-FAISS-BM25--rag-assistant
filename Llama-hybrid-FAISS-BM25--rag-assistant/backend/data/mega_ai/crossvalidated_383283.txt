[site]: crossvalidated
[post_id]: 383283
[parent_id]: 
[tags]: 
Optimal scaling of the Random Walk Metroplis-Hastings algorithm and the speed measure of the limiting diffusion

Let $d\in\mathbb N$ with $d>1$ $\ell>0$ $\sigma_d^2:=\frac{\ell^2}{d-1}$ $f\in C^2(\mathbb R)$ be positive with $$\int f(x)\:{\rm d}x=1$$ and $g:=\ln f$ $Q_d$ be a Markov kernel on $(\mathbb R^d,\mathcal B(\mathbb R^d))$ with $$Q_d(x,\;\cdot\;)=\mathcal N_d(x,\sigma_dI_d)\;\;\;\text{for all }x\in\mathbb R^d,$$ where $I_d$ denotes the $d$ -dimensional unit matrix Now, let $$\pi_d(x):=\prod_{i=1}^df(x_i)\;\;\;\text{for }x\in\mathbb R^d$$ and $\left(X^{(d)}_n\right)_{n\in\mathbb N_0}$ denote the Markov chain generated by the Metropolis-Hastings algorithm with proposal kernel $Q_d$ and target density $\pi_d$ (with respect to the $d$ -dimensional Lebesuge measure $\lambda^d$ ). Moreover, let $$U^{(d)}_t:=\left(X^{(d)}_{\lfloor dt\rfloor}\right)_1\;\;\;\text{for }t\ge0.$$ In the paper Weak convergence and optimal scaling of random walk Metropolis algorithms , the authors show (assuming that $g$ is Lipschitz continuous and satisfies some moment conditions) that $U^{(d)}$ converges (in the Skorohod topology) as $d\to\infty$ to the solution $U$ of $${\rm d}U_t=\frac{h(\ell)}2g'(U_t){\rm d}t+\sqrt{h(\ell)}{\rm d}W_t,$$ where $W$ is a standard Brownian motion, with $U_0\sim f\lambda^1$ . Now, they conclude that the "optimal choice" for $\ell$ is obtained by maximizing $$h(\ell):=2\ell^2\Phi\left(-\frac{\ell\sqrt I}2\right),$$ where $\Phi$ denotes the cumulative distribution function of the standard normal distribution and $$I:=\int\left|g'\right|^2\:{\rm d}(f\lambda^1) Why ? In which sense (e.g. total variation distance or variance) does this optimize the Metropolis-Hastings algorithm? I've read that $h(\ell)$ is called the "speed function/measure" of the diffusion $U$ ... Would be very happy about a reference for that topic.
