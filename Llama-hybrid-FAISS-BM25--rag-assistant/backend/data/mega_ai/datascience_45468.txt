[site]: datascience
[post_id]: 45468
[parent_id]: 45464
[tags]: 
With batch you feed the entire data through each EM iteration. In the online implementation you feed only some of the data through each EM iteration (a "mini-batch"). From the sklearn user guide : While the batch method updates variational variables after each full pass through the data, the online method updates variational variables from mini-batch data points. From the paper about online LDA : We then update $\lambda$ using a weighted average of its previous value and $\tilde\lambda$ . The weight given to $\tilde\lambda$ is given by $\rho_{t} = (\tau_{0} + \tau)^{-\kappa}$ , where $\kappa$ ∈ (0.5, 1] controls the rate at which old values of $\tilde\lambda$ are forgotten and $τ_{0} \ge 0$ slows down the early iterations of the algorithm. In the sklearn implementation : weight = np.power(self.learning_offset + self.n_batch_iter_, -self.learning_decay) So learning_offset is $\tau_{0}$ which slows down early iterations, and learning_decay is $\kappa$ which controls rate at which old weights are forgotten.
