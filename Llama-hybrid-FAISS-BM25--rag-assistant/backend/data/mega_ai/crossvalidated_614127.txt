[site]: crossvalidated
[post_id]: 614127
[parent_id]: 613879
[tags]: 
Frank Harrell explains here that measures based on ROC curves aren't very sensitive ways to compare different models. Also, unless you have a few tens of thousands of events, you don't have enough cases to use train/validation/test splits reliably, as Harrell explains here in general. (The power of survival models depends on the number of events, not the number of cases as does linear regression.) Furthermore, in your generation of df2 you seem to have ignored an important warning in the answer to which you link : Transformation of continous data to categorical means an information loss... From those perspectives, you are not "doing it the right way." One approach would be to take a bootstrap sample (of the same size, but with replacement) from from the full data set, perform the entire modeling process (cross validation to get penalty, etc.) with each of the modules on the bootstrap sample, and compare the 2 models' abilities to predict results on the entire data set. Repeat for a few hundred bootstrap samples, and combine the results.* Finally, if your model doesn't also include outcome-associated clinical variables, it probably won't be very useful. See this recent report on better ways to use LASSO and other approaches for combining genomic data with clinical data for survival analysis: David Wissel, Daniel Rowson, and Valentina Boeva, "Systematic comparison of multi-omics survival models reveals a widespread lack of noise resistance," Cell Reports Methods 3, 100461 (2023). In response to comment The document you cite includes a different bootstrap-resampling approach for comparing two "modules." If you have precalculated continuous scores for all samples based on each of your 2 modules, then you can use that approach very simply. Identify the interquartile range for each of your module's scores. Call them something like IQR1 and IQR2 . IQR1 Set up a null vector to store the results of the bootstrapping. resultStore Set a random seed for reproducibility. set.seed(97675) Then do the bootstrapping. Repeat the following a large number of times, say 999. Take a bootstrap sample from the full data set. That can be done from your dataSet simply via: bootSample Do your Cox fits on bootSample for each of the modules. bootFit1 Get the log-hazard-ratio across the IQR for each module's scores, and their difference: logHR1 Append that difference to the result-storage vector resultStore After your multiple bootstrap samples, model fits, and calculations, your resultStore has an estimate of the distribution of the differences between log-hazard ratios for the two modules if you had applied the modules to new data samples. Put them in order: resultStore and if you did 999 bootstrap samples then the 25th and 975th in order give you 95% confidence limits for the difference between the modules. If a difference of 0 is outside those confidence limits, then you have evidence that one of the modules is superior. That seems to be the approach used in the link you provided. With continuous scores, you might be advised to use a regression spline rather than a linear fit in the Cox model, so you'd have to adapt the logHR calculations to get the actual logHR between the 25th and 75th percentiles of the corresponding module's scores. There also are potentially better ways to evaluate the bootstrapping results . And this doesn't deal with potential randomness in how you developed the score to start, which the approach I suggested earlier could address. But it might be good enough for your purpose. *You also might try to to compare Akaike Information Criterion (AIC) values on the 2 models as built on the full data set, but that requires some extra care as explained on this page .
