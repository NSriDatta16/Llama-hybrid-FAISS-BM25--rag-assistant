can be rising, falling or level. Final lowering – the amount by which the frequency falls at the end of an utterance. Pitch range – measures the spread between the maximum and minimum frequency of an utterance. Time-related features: Speech rate – describes the rate of words or syllables uttered over a unit of time Stress frequency – measures the rate of occurrences of pitch accented utterances Voice quality parameters and energy descriptors: Breathiness – measures the aspiration noise in speech Brilliance – describes the dominance of high or low frequencies In the speech Loudness – measures the amplitude of the speech waveform, translates to the energy of an utterance Pause Discontinuity – describes the transitions between sound and silence Pitch Discontinuity – describes the transitions of the fundamental frequency. Facial affect detection The detection and processing of facial expression are achieved through various methods such as optical flow, hidden Markov models, neural network processing or active appearance models. More than one modality can be combined or fused (multimodal recognition, e.g. facial expressions and speech prosody, facial expressions and hand gestures, or facial expressions with speech and text for multimodal data and metadata analysis) to provide a more robust estimation of the subject's emotional state. Facial expression databases Creation of an emotion database is a difficult and time-consuming task. However, database creation is an essential step in the creation of a system that will recognize human emotions. Most of the publicly available emotion databases include posed facial expressions only. In posed expression databases, the participants are asked to display different basic emotional expressions, while in spontaneous expression database, the expressions are natural. Spontaneous emotion elicitation requires significant effort in the selection of proper stimuli which can lead to a rich display of intended emotions. Secondly, the process involves tagging of emotions by trained individuals manually which makes the databases highly reliable. Since perception of expressions and their intensity is subjective in nature, the annotation by experts is essential for the purpose of validation. Researchers work with three types of databases, such as a database of peak expression images only, a database of image sequences portraying an emotion from neutral to its peak, and video clips with emotional annotations. Many facial expression databases have been created and made public for expression recognition purpose. Two of the widely used databases are CK+ and JAFFE. Emotion classification By doing cross-cultural research in Papua, New Guinea, on the Fore Tribesmen, at the end of the 1960s, Paul Ekman proposed the idea that facial expressions of emotion are not culturally determined, but universal. Thus, he suggested that they are biological in origin and can, therefore, be safely and correctly categorized. He therefore officially put forth six basic emotions, in 1972: Anger Disgust Fear Happiness Sadness Surprise However, in the 1990s Ekman expanded his list of basic emotions, including a range of positive and negative emotions not all of which are encoded in facial muscles. The newly included emotions are: Amusement Contempt Contentment Embarrassment Excitement Guilt Pride in achievement Relief Satisfaction Sensory pleasure Shame Facial Action Coding System A system has been conceived by psychologists in order to formally categorize the physical expression of emotions on faces. The central concept of the Facial Action Coding System, or FACS, as created by Paul Ekman and Wallace V. Friesen in 1978 based on earlier work by Carl-Herman Hjortsjö are action units (AU). They are, basically, a contraction or a relaxation of one or more muscles. Psychologists have proposed the following classification of six basic emotions, according to their action units ("+" here mean "and"): Challenges in facial detection As with e