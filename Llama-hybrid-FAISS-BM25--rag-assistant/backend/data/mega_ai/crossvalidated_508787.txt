[site]: crossvalidated
[post_id]: 508787
[parent_id]: 
[tags]: 
Accuracy estimator for scan sample of categorical time series

Suppose we are tracking the behavior of some agent over time, and this agent can only exist in two states (+, -). This agent is only active for 20 timesteps, but we are only able to observe the agent for 5 sequential timesteps. The scan sample we take of the agent's behavior is denoted by | | in the following time series: ++++---|++-+-|-++----+ Drawing from time series in this way results in the following count data: + - Full Dataset 10 10 Sample Dataset 3 2 To measure the degree of association between the sample and full time series, I was going to perform a chi-squared test and then derive Cram√©r's V. However, this procedure seems to violate at least two related assumptions of the chi-square test. 1) A subject can only contribute to only one cell of the contingency table. 2) Samples must be random, however sequential timesteps for the agent are autocorrelated. Is there a correction to the chi-square test I can implement to overcome these violations, or is there another unbiased estimator I can use to measure the similarity between the sample and the full dataset? Citation for assumptions: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3900058/
