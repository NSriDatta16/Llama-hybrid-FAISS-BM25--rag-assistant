[site]: crossvalidated
[post_id]: 465721
[parent_id]: 465516
[tags]: 
Extracting keywords as binary features were the state of the art for a very long time. Most implementations of decision trees/forests can deal with a pretty large set of features. You should also consider weighting the features with TF-IDF scores. If speed is really critical, there are toolkits for linear models that can easily deal with a large number of sparse features (such Vowpal Wabbit ). Dense representation from neural networks will work also with traditional ML algorithms. From your question, it seems to me that you don't want to run heavy-weighted neural representations models such as BERT (which would certainly work here too). In that case, a reasonable thing would be representing the paragraphs using an average word embedding. In depends on the length of the paragraphs, but it may pay off to extract keywords first and then do the averaging.
