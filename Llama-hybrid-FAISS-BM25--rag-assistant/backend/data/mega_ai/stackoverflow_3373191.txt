[site]: stackoverflow
[post_id]: 3373191
[parent_id]: 3372006
[tags]: 
If you have discrete values and lots of repetition you could store the values and counts, which would save a bit of space. Possibly at stages through the computation you could discard the top 'n' and bottom 'n' values, as long as you are sure that the median is not in that top or bottom range. e.g. Let's say you are expecting 100,000 values. Every time your stored number gets to (say) 12,000 you could discard the highest 1000 and lowest 1000, dropping storage back to 10,000. If the distribution of values is fairly consistent, this would work well. However if there is a possibility that you will receive a large number of very high or very low values near the end, that might distort your computation. Basically if you discard a "high" value that is less than the (eventual) median or a "low" value that is equal or greater than the (eventual) median then your calculation is off. Update Bit of an example Let's say that the data set is the numbers 1,2,3,4,5,6,7,8,9. By inspection the median is 5. Let's say that the first 5 numbers you get are 1,3,5,7,9. To save space we discard the highest and lowest, leaving 3,5,7 Now get two more, 2,6 so our storage is 2,3,5,6,7 Discard the highest and lowest, leaving 3,5,6 Get the last two 4,8 and we have 3,4,5,6,8 Median is still 5 and the world is a good place. However, lets say that the first five numbers we get are 1,2,3,4,5 Discard top and bottom leaving 2,3,4 Get two more 6,7 and we have 2,3,4,6,7 Discard top and bottom leaving 3,4,6 Get last two 8,9 and we have 3,4,6,8,9 With a median of 6 which is incorrect. If our numbers are well distributed, we can keep trimming the extremities. If they might be bunched in lots of large or lots of small numbers, then discarding is risky.
