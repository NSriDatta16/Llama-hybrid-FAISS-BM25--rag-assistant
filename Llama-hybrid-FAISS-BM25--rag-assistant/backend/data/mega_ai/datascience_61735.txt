[site]: datascience
[post_id]: 61735
[parent_id]: 61733
[tags]: 
XGBoost has several parameters to tune for imbalance datasets. You wouldn't mess with the objective function from my knowledge. You can find them below: scale_pos_weight max_delta_step min_child_weight Another thing to consider is to resample the dataset. We talk about Undersampling, Oversampling and Ensemble sampling. I think I was using the imbalanced-learn Python library for that. Things can get even more creative there. For instance, create many XGBoost trees on the same undersampled class instances but each XGBoost tree holds another copy (or randomly sampled copy) of your oversampled class instances. Then you can average your results. In the end, you will have to try and see for yourself.
