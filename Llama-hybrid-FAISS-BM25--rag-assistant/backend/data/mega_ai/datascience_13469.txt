[site]: datascience
[post_id]: 13469
[parent_id]: 
[tags]: 
Are there more layer types like convolution layers and fully connected layers?

I've been learning about neural networks and I'm curious: Are there any other layer types like convolution and fully connected layers? I know there are things like max pooling or various regularization layers, but I'm mainly interested in more variations on fully connected layers (like how convolution layers do the same thing as fully connected layers but take the shape of data into account). It just seems strange that we wouldn't need anything except straight matrix multiplication or matrix multiplication on smaller chunks. Do these exist? If not, why not?
