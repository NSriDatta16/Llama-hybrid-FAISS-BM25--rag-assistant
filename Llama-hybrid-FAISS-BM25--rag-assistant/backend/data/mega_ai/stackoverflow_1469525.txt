[site]: stackoverflow
[post_id]: 1469525
[parent_id]: 1467898
[tags]: 
Of all the programs in this thread that I've tested so far, the OCaml version is the fastest and also among the shortest . (Line-of-code-based measurements are a little fuzzy, but it's not clearly longer than the Python version or the C or C++ versions, and it is clearly faster.) Note: I figured out why my earlier runtimes were so nondeterministic! My CPU heatsink was clogged with dust and my CPU was overheating as a result. Now I am getting nice deterministic benchmark times. I think I've now redone all the timing measurements in this thread now that I have a reliable way to time things. Here are the timings for the different versions so far, running on a 27-million-row 630-megabyte input data file. I'm on Ubuntu Intrepid Ibex on a dual-core 1.6GHz Celeron, running a 32-bit version of the OS (the Ethernet driver was broken in the 64-bit version). I ran each program five times and report the range of times those five tries took. I'm using Python 2.5.2, OpenJDK 1.6.0.0, OCaml 3.10.2, GCC 4.3.2, SBCL 1.0.8.debian, and Octave 3.0.1. SquareCog's Pig version: not yet tested (because I can't just apt-get install pig ), 7 lines of code. mjv's pure SQL version: not yet tested, but I predict a runtime of several days; 7 lines of code. ygrek's OCaml version: 68.7 seconds ±0.9 in 15 lines of code. My Python version: 169 seconds ±4 or 86 seconds ±2 with Psyco, in 16 lines of code. abbot's heap-based Python version: 177 seconds ±5 in 18 lines of code, or 83 seconds ±5 with Psyco. My C version below, composed with GNU sort -n : 90 + 5.5 seconds (±3, ±0.1), but gives the wrong answer because of a deficiency in GNU sort , in 22 lines of code (including one line of shell.) hrnt's C++ version: 217 seconds ±3 in 25 lines of code. mjv's alternative SQL-based procedural approach: not yet tested, 26 lines of code. mjv's first SQL-based procedural approach: not yet tested, 29 lines of code. peufeu's Python version with Psyco : 181 seconds ±4, somewhere around 30 lines of code. Rainer Joswig's Common Lisp version: 478 seconds (only run once) in 42 lines of code. abbot's noop.py , which intentionally gives incorrect results to establish a lower bound: not yet tested, 15 lines of code. Will Hartung's Java version: 96 seconds ±10 in, according to David A. Wheeler’s SLOCCount, 74 lines of code. Greg's Matlab version: doesn't work. Schuyler Erle's suggestion of using Pyrex on one of the Python versions: not yet tried. I supect abbot's version comes out relatively worse for me than for them because the real dataset has a highly nonuniform distribution: as I said, some aa values (“players”) have thousands of lines, while others only have one. About Psyco: I applied Psyco to my original code (and abbot's version) by putting it in a main function, which by itself cut the time down to about 140 seconds, and calling psyco.full() before calling main() . This added about four lines of code. I can almost solve the problem using GNU sort , as follows: kragen@inexorable:~/devel$ time LANG=C sort -nr infile -o sorted real 1m27.476s user 0m59.472s sys 0m8.549s kragen@inexorable:~/devel$ time ./top5_sorted_c outfile real 0m5.515s user 0m4.868s sys 0m0.452s Here top5_sorted_c is this short C program: #include #include #include #include enum { linesize = 1024 }; char buf[linesize]; char key[linesize]; /* last key seen */ int main() { int n = 0; char *p; while (fgets(buf, linesize, stdin)) { for (p = buf; *p && !isspace(*p); p++) /* find end of key on this line */ ; if (p - buf != strlen(key) || 0 != memcmp(buf, key, p - buf)) n = 0; /* this is a new key */ n++; if (n I first tried writing that program in C++ as follows, and I got runtimes which were substantially slower, at 33.6±2.3 seconds instead of 5.5±0.1 seconds: #include #include #include int main() { using namespace std; int n = 0; string prev, aa, bb, cc; while (cin >> aa >> bb >> cc) { if (aa != prev) n = 0; ++n; if (n I did say almost . The problem is that sort -n does okay for most of the data, but it fails when it's trying to compare 0.33 with 3.78168e-05 . So to get this kind of performance and actually solve the problem, I need a better sort. Anyway, I kind of feel like I'm whining, but the sort-and-filter approach is about 5× faster than the Python program, while the elegant STL program from hrnt is actually a little slower — there seems to be some kind of gross inefficiency in . I don't know where the other 83% of the runtime is going in that little C++ version of the filter, but it isn't going anywhere useful, which makes me suspect I don't know where it's going in hrnt's std::map version either. Could that version be sped up 5× too? Because that would be pretty cool. Its working set might be bigger than my L2 cache, but as it happens it probably isn't. Some investigation with callgrind says my filter program in C++ is executing 97% of its instructions inside of operator >> . I can identify at least 10 function calls per input byte, and cin.sync_with_stdio(false); doesn’t help. This probably means I could get hrnt’s C program to run substantially faster by parsing input lines more efficiently. Edit: kcachegrind claims that hrnt’s program executes 62% of its instructions (on a small 157000 line input file) extracting double s from an istream . A substantial part of this is because the istreams library apparently executes about 13 function calls per input byte when trying to parse a double . Insane. Could I be misunderstanding kcachegrind's output? Anyway, any other suggestions?
