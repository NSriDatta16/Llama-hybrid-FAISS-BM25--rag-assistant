[site]: crossvalidated
[post_id]: 428111
[parent_id]: 428097
[tags]: 
Actually, $\pi(\theta | x ) = \frac{f(x|\theta)\pi(\theta)}{\int_{\Theta}f(x|\theta)\pi(\theta)d\theta} = \frac{f(x|\theta)1}{\int_{\Theta}f(x|\theta)1d\theta}\propto f(x|\theta)$ . I suspect you are uncertain with the idea of a likelihood function. The likelihood function $f(x|\theta)$ (or $f(\theta;x)$ ), is a function of the parameter $\theta$ . Taking the Binomial example, suppose you have $f(\theta; x=4, n=6)$ , then for any $\theta \in [0,1]$ , say $\theta=0.3$ , $f(\theta)=f(0.3)$ returns you the probability of observing four heads given six flips if your coin was biased with 0.3 probability of heads .Try to plot this for $\theta=0.01, 0.02, \dots, 0.98, 0.99$ . To reiterate, the likelihood function of your parameter. It takes the form of your sampling distribution, except you hold your data constant and take parameter as input instead. To derive a posterior with improper priors, it is sufficient the integral $\int_\Theta f(\theta;x)\pi(\theta)d\theta (is finite). See Bayesian Choice - Section 1.5
