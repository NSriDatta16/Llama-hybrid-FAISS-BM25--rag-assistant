[site]: datascience
[post_id]: 22838
[parent_id]: 
[tags]: 
What is the relationship between hard-sigmoid function and vanishing gradient descent problem?

I think that the vanishing gradient problem occurs when the derivative of activation function is less than 1. The deeper the neural network is, the f' * f' * f'... operation will result in the gradient closer to zero. However, if we use hard-sigmoid as an activation function (its derivative is 0 or 1), the vanishing gradient problem can be solved. I think the dying problem (derivative = 0 @ ReLU) is different from the vanishing gradient problem. Is it right?
