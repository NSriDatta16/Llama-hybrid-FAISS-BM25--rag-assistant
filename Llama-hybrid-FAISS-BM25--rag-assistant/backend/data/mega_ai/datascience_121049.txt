[site]: datascience
[post_id]: 121049
[parent_id]: 44635
[tags]: 
Summing Embeddings can makes sense under certain conditions. If you want to just do a naive euclidian distance based k-NN to find close samples, summing the vectors doesn't work because the tip of the vector gets moved away from the origin. This is easy to visualize if you add the same vector to itself (or scale it with a factor of 2), there will be a new euclidian distance to it's un-summed version that is equal to its own magnitude. But there are other metrics where this does make sense like the cosine distance . As this only looks at the relative angle between the vectors, the magnitude of the vectors doesn't influence the result. So a scaled vector will still give you a cosine distance of 1 to it's unscaled variant.
