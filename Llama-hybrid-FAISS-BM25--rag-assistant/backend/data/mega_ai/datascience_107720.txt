[site]: datascience
[post_id]: 107720
[parent_id]: 107717
[tags]: 
Without being fully aware of what you are trying to solve, see below some explainability packages and/or methodologies that I have found very useful in the past especially for DL and NLP tasks. Packages: https://github.com/pair-code/lit https://github.com/mruberry/captum https://www.tensorflow.org/tensorboard/what_if_tool Writeups and papers: https://proceedings.neurips.cc/paper/2019/file/567b8f5f423af15818a068235807edc0-Paper.pdf https://arxiv.org/pdf/1512.02479.pdf Specific to tabular data, I have found quite insightful to use the masks that are produce per step in the TabNet model. Tabnet also produces aggregated importance in the same context that Shap does. Masks can inform how these important features are getting combined in order to boost model performance. You can easily experiment with this using the dreamquark-ai implementation in pure pytorch or fast.ai. TabNet paper: https://arxiv.org/abs/1908.07442 TabNet pytorch implementation: https://github.com/dreamquark-ai/tabnet TabNet fast ai: https://github.com/mgrankin/fast_tabnet Hope this helps. Edit: Tabnet
