[site]: crossvalidated
[post_id]: 402597
[parent_id]: 401853
[tags]: 
I've never heard of such a thing. In fact many fully convolutional autoencoder models do have a bottleneck layer which has spatial dimensions 1x1, which is equivalent to saying that the layer just before and just after are fully connected. I could imagine that such a layer could slow down learning if it has a massive number of parameters, since FC layers tend to have more parameters than convolutional ones. It could also hurt the translational equivariance of the network. But to the best of my knowledge there's nothing which affects the quality of the gradients.
