[site]: crossvalidated
[post_id]: 212899
[parent_id]: 
[tags]: 
Reference summarizing various machine learning algorithms' computational complexity

For example, suppose you train a linear regression model using the Normal Equation, on a training set $\mathbf{X}$ containing $m$ instances and $n$ features. The Normal Equation requires computing $(\mathbf{X}^t \cdot \mathbf{X})^{-1}$, which is the inverse of an $n \times n$ matrix. According to the Wikipedia , inverting a matrix has a computational complexity of about $O(n^{2.4})$ using the best algorithms. On the other hand, the algorithm is linear with regards to the number of training examples $m$. So overall, it should be possible to train a linear regression model in $O(m \cdot n^{2.4})$. Is there a table providing similar information for other Machine Learning algorithms, such as SVMs, Neural Networks (with $u$ layers and $v$ neurons per layer), Random Forests, KNN, Naive Bayes, etc.?
