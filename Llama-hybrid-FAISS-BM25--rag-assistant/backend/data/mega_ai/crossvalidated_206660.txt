[site]: crossvalidated
[post_id]: 206660
[parent_id]: 205858
[tags]: 
I will answer myself and let you know my findings in case anybody is interested. First the bias: I took the time to collect all the recent data and format it correclty and so on. I should have done this long before. The picture is the following: You see the data from the end of 2015 and then April 16. The price level is totally different. A model trained on 2015 data can in no way get this change. Second: The fit of xgboost. I really liked the following set-up. train and test error are much close now and still good: xgb_grid_1 Thus I use a lot of trees and all of them are at most 3 splits deep (as recommended here ). Doing this the calculation is quick (the tree size grows by a factor of 2 with each split) and the overfit seems to be reduced. My summary: use trees with a small number of leaves but a lot of them and look for recent data. For the competition this was bad luck for me...
