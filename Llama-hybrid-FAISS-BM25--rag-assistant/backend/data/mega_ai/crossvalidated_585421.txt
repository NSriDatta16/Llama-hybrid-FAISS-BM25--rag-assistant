[site]: crossvalidated
[post_id]: 585421
[parent_id]: 584756
[tags]: 
If you increase the regularisation parameter too much, the model will ignore the data completely and the norm of the weights will be forced arbitrarily close to zero". This ought to be the solution you would asymptotically get modelling fake targets, as there is no correlation between the attributes and the targets. Making generalisation worse by regularising too much (or by using a model that is too structurally simple, e.g. linear regression for a non-linear problem) is known as "under-fitting". The key to regularisation lies in tuning the regularisation parameter to optimise out-of-sample performance without over- or under-fitting. I tend to tune the regularisation to minimise the leave-one-out error (because it is cheap for many useful models) or use a Bayesian approach. The regularisation parameter is equivalent to placing a bound on the magnitude of the weights (see my answer to this question ). This creates a nested set of models of increasing complexity as the bound is made looser (by reducing the regularisation parameter). If you increase the bound slightly, the model can implement all of the functions it previously could, plus a few more. Thus the value of the regularisation parameter is an indication of the complexity of the model class that you are fitting to the data. In tuning the parameter, you are matching the complexity of the model to suit the complexity of the learning problem. You don't want it to be too complex, but you don't want it to be too simple either.
