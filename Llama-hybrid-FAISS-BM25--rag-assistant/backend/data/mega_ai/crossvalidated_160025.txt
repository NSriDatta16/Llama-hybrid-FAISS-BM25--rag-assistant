[site]: crossvalidated
[post_id]: 160025
[parent_id]: 160007
[tags]: 
The idea of the QR decomposition as a procedure to get OLS estimates is already explained in the post linked by @MatthewDrury. The source code of the function qr is written in Fortran and may be hard to follow. Here I show a minimal implementation that reproduces the main results for a model fitted by OLS. Hopefully the steps are easier to follow. Recap: The QR procedure is used to decompose the matrix of regressor variables $X$ into an orthonormal matrix $Q$ and a non-singular upper-triangular matrix $R$. Substituting $X = QR$ in the normal equations $X'X\hat\beta = X'y$ yields: $$ R'Q'QR\hat\beta = R'Q'y \,. $$ Premultipying by $R^{-1}$ and using the fact that $Q'Q$ is a diagonal matrix gives: $$ R\hat\beta = Q'y \,. \tag 1 $$ The point of this result is that, since $R$ is an upper-triangular matrix, this equation is easy to solve for $\hat\beta$ by backwards substitutions. Now, how to we get the matrices $Q$ and $R$? We can Householder transformation, Givens rotations or the Gram-Schmidt procedure. Below I use Householder transformations. See details for example here . The code below is based on the Pascal code described in the book Pollock (1999) Chapters 7 and 8. The matrix of regressors is used to store the matrix $R$ of the QR decomposition. The dependent variable $Y$ is overwritten with the results of $Q'y$ (right-hand-side of equation (1) above). Notice also that in the last step the residual sum of squares can be obtained from this vector. QR.regression We can check that the same estimates than lm are obtained. # benchmark results fit We can also get the matrix $Q$ and check that it is orthogonal: Q The residuals can be obtained as y - X %*% res$beta . References D.S.G. Pollock (1999) A handbook of time series analysis, signal processing and dynamics , Academic Press.
