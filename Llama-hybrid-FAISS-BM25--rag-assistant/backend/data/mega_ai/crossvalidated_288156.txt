[site]: crossvalidated
[post_id]: 288156
[parent_id]: 287785
[tags]: 
The word 'failure' is close to 'error'. To me the term error makes sense since you can calculate a probability for it to occur (provided you set a certain minimal effect size that would be desirable to detect). And you want to calculate this probability in situations where you want it to be small. In those situations the failure would be considered an error. To me it is very symmetric with type I errors. Like p-values, which relate to type I error, you can also calculate the probability for (falsely) not rejecting the null-hypothesis. For a given effect size and a given test (e.g. number of measurements) you can calculate with what probability this 'failure' might occur. These thoughts do require that you set a boundary for the null hypothesis. The tendency not to consider type II errors, or at least providing the bounds of the effect size that could have been detected with sufficient probability, is large in a scientific world that is obsessed by p-values, significance, and hypothesis testing (the inverse happens as well by putting large emphasis on minor effects that happened to be significant, only by a huge number of measurements). If $p$ is larger than some $\alpha$ then the effect is said/considered not to be present (or more elegantly not shown to be present). In any way it certainly influences our future actions as if we accept the $H_0$.
