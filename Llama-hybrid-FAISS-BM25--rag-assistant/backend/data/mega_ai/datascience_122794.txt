[site]: datascience
[post_id]: 122794
[parent_id]: 122789
[tags]: 
Contextual word embeddings are a type of word representation that captures not only the semantic meaning of individual words, but also the context in which they appear. This is completely opposite to traditional word embeddings like Word2Vec or GloVe, which generate a single, context-independent vector for each word in the vocabulary. For example, consider the word "bank" in the sentences "I sat on the bank of the river" and "I deposited money in the bank". Traditional word embeddings would generate the same vector for "bank" in both sentences, failing to capture the different meanings. Contextual embeddings, on the other hand, would generate different vectors for each instance of "bank", reflecting the different meanings implied by the context.
