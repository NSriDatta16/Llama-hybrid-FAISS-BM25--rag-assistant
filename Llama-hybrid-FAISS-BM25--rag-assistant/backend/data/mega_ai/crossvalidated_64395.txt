[site]: crossvalidated
[post_id]: 64395
[parent_id]: 64389
[tags]: 
Take exactly whatever values you used during learning to do your normalisation/rescaling/etc, and apply them directly to your online data. It's a little hacky in some sense, in that you're following a slightly different process preparing your test data than you did with your training data. However, unless you're doing something like transductive learning, you're often already doing that anyway. Say you were fitting a gaussian to some real valued data using maximum likelihood, by finding the arithmetic mean and variance of the training data. To evaluate a new sample, you typically wouldn't retrain the gaussian using your training set + the new sample. You'd leave the gaussian fixed where it was, and just evaluate. Same deal here - it's like an extra set of parameters you've learned during training, and now keep fixed. The only other thing to watch out for is situations where you've assumed something about the data as a result of the preprocessing. For example, you've rescaled your training data to lie in the range 0 to 1, and then made later assumptions in your algorithm based on that (e.g. 'no negative vales, so I can cancel a value from this inequality without changing its sign'). Your online data may break these assumptions after preprocessing. Of course, there's schools of thought that say you should re-evaluate to take into account your test data - bayesian marginalisation, say, or the aforementioned transductive learning. In those situations then maybe you do think about updating how everything is preprocessed. If you're not already going down that route, though, I don't think it's worth worrying about.
