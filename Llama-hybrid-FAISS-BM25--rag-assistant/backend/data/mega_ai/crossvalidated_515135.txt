[site]: crossvalidated
[post_id]: 515135
[parent_id]: 515109
[tags]: 
Like I said in my comment, I don't necessarily see how a change in the range of the response is an issue. In particular, in your case, I think it is a red herring. First, it's important to ask, should the test set have the same range of responses as the train set? In many cases it is not the case and I don't know why that would be problematic necessarily . If the goal was to predict daily high temperature, it's easy to imagine a dataset where a range of temperatures from 0-100F is observed in a multi-year train set while only temperatures in the range of 40-60F are observed in a test set which corresponds to a single month. If we created a model that acurately predicted temperature, we should still expect it to perform well on this test set assuming we have a good model in the first place. Some exceptions include If the relationship between the predictors and the response changes (concept drift). If the domain of the data shifts away from the domain of the training data (applicability domain). In this case, your model has to extrapolate to a region it has never seen before. (See Applied Predictive Modeling pg 535 ) In this example, we don't know anything about the range of the predictors, but the range of the response for the test set is clearly within the range of the response we see in the train set. If the goal is to estimate how well the model will perform when applied, it makes sense to split the train and test sets based on time and evaluate performance on future months. If there are minor performance losses related to shifts in the distribution of the response, then that is normal and part of what you want to estimate. That being said, it would be better to evaluate the model over a longer period, or over multiple one month periods using time series cross-validation (See Forecasting: Principles and Practice, Section 3.4 ). That is, unless you're only concerned with how it will perform in Februay, sepcifically. If I had to guess, the reason you are not getting acceptable performance has nothing to do with the range of the response. One, more likely, cuplrit is omitted variable bias. In particular, the effects of covid are very likely at play and should somehow be accounted for (see the comment by @Sycorax). Additionally, without knowing anything else, I might imagine the quanitity of 'applications' to have some seasonality. Both of these factors will complicate the analysis, especially when you do not have much data.
