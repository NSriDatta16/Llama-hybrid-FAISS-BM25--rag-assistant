[site]: crossvalidated
[post_id]: 643365
[parent_id]: 643361
[tags]: 
I originally voted to close this question because it contains a lot of different comments and questions, but coming back to this there are only two real questions here which are directly answerable, the problem of transformation and the problem of deleting missing values . First, there is absolutely no reason to care about this: My issue is as follows, all explanatory variables are extremely skewed, some only have positive values which lend themselves nicely to a log transform, while others include negative values, zero values or both. Because the regression modeling depends on the distribution of the residuals , which are already modeled explicitly with the logit link function in logistic regression, there is no reason to transform in this case. With respect to deleting missing values: I wanted to ask if there is ever a reason to delete observations (for MCAR), as opposed to using imputation. Generally speaking it is a bad idea, but if you can clearly identify that it was an erroneous entry, then it is okay to fix or delete the value. For example, we may accidentally create missingness by pivoting data which doesn't have unique values, or because a statistical software attributes an incorrect entry as missing based off some coding (e.g. NA values = 999, but somebody accidentally enters 999 when the value was supposed to be 99). In all other cases, there is plenty of literature that says it is an otherwise bad practice to simply employ listwise deletion.
