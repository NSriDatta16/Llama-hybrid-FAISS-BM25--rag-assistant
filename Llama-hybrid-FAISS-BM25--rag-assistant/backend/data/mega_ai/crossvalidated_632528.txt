[site]: crossvalidated
[post_id]: 632528
[parent_id]: 631675
[tags]: 
Methods get overlooked for lots of reasons, but technical difficulties is actually very often a big part of it. E.g. if you look at these examples: Bayesian methods: on the one hand people only realized MCMC methods let you fit really flexible models in the 1990s (and computing power started to get to the point where MCMC sampling became possible for more complex models), on the other hand you have a culture that was quite negative towards Bayesian approaches (typically attributed to Fisher and others being quite negative about them). Neural networks: on the one hand using GPUs made it so much easier to train deeper models in a reasonable time in the 2010s, on the other hand there were all sorts of perceptions about neural networks not being so promising (e.g. that a single-layer perceptron cannot implement every function and multiple layers with linear activations are actually mathematically equivalent to a single layer etc.) In both cases there's both a perception issue and a technical issue (with some people keeping on pursuing these approaches despite not being mainstream). Interestingly, after the technical issue got largely solved, the second case took off massively relatively quickly, while the uptake of Bayesian methods is slower. That may of course simply reflect massive performance improvements over previous approaches on tangible use cases (e.g. image classification, translation...). In other cases, it may really just be a case of university teaching/text books being out-of-date, peer-reviewers relying on outdated knowledge or people keeping on doing what others have done: e.g. why people use stepwise regression instead of the many more sensible methods, pre-tests for normality, why people do pairwise comparisons of doses to control instead of dose-response modeling (e.g. MCP-Mod) etc. I also know one example where the problem was outdated regulatory guidance: for drug approvals for diabetes treatments, the US Food and Drug Administration (FDA) required last-observation-carried-forward (LOCF) for imputing data for missing data/patients that stopped treatment. This guidance did not get changed for several years, even after it was basically clear to everyone that this has all sorts of problems that could be avoided by e.g. appropriate multiple imputation for most estimands (or implicit imputation through likelihood methods for some estimands).
