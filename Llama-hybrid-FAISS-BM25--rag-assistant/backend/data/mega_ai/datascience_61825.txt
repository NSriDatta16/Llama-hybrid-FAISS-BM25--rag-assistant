[site]: datascience
[post_id]: 61825
[parent_id]: 
[tags]: 
Why is word prediction an obsession in Natural Language Processing?

I have heard how great BERT is at masked word prediction, i.e. predicting a missing word from a sentence. In a Medium post about BERT , it says: The basic task of a language model is to predict words in a blank, or it predicts the probability that a word will occur in that particular context. Let’s take another example: “FC Barcelona is a _____ club” Indeed, I recently heard about SpanBERT , which is "designed to better represent and predict spans of text". What I do not understand is: why? I cannot think of any common reason that a human would need to do this task, let alone why it would need to be automated. This does not even seem to be a task where it is particularly easy to evaluate the success of a model. For example, My ___ is cold This could reasonably be a number of possible words. How can BERT be expected to get this right, and how can humans or another algorithm be expected to evaluate whether "soup" is a better answer than "coffee"? Clearly there are a lot of smart people who think that this is important, so I accept my lack of understanding is likely based on my own ignorance. Is it that this task itself is not important, but it's a proxy for ability at other tasks? What am I missing?
