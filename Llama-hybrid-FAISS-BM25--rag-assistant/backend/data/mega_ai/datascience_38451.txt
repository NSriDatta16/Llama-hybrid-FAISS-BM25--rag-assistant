[site]: datascience
[post_id]: 38451
[parent_id]: 38443
[tags]: 
In principal, they are exactly the same. A numpy array holds the RGB values of an image saved on disk in a memory container ( numpy.ndarray ). This container offers certain built-in functions, such as the ability to do some fancy slicing . An example would be to flip an image across the vertical axis, giving a mirror image: flipped =image[:, ::-1] # memory efficient and therefore fast Numpy arrays aren't able to do everything we need for modelling, especially on GPUs using Tensorflow or PyTorch, for example. So we pass the numpy arrays to these frameworks and they put another wrapper on them, making them tensor objects. These objects have special methods and properties that are tailored to our needs for deep learning. They can do things such as store gradient information or deduce the shapes of the tensor before/after operations - all things that make our lives easier. Loading images directly into a deep learning framework, using one of their own tools, would skip the numpy step and get straight to the tensors. That is fine when your pipeline is already set up and you don't need to perform any pre-processing. PyTorch has its basic tensor object; however, it allows you to do most operations you could do on a standard numpy array, which many see as a big advantage of that framework.
