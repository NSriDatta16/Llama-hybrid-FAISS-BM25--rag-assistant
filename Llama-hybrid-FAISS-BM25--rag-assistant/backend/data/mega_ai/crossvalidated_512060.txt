[site]: crossvalidated
[post_id]: 512060
[parent_id]: 
[tags]: 
Resiudals of linear regression between two correlated explanatory variables as a new explenatory variable

I'm starting a new data science project with few explanatory variables, so I'm trying to keep as much variability as possible. Also I have a lot of response variables, so don't really now what I want to do in the end (what response variable I want to predict for example). For the moment I'm just looking for insights and doing data visualisation. Doing such, I've noticed two really correlated explanatory variables : Their correlation coefficient is 0.8465772, so if I want to use a ML algorithm to predict one of the response variables, it would not be great to keep both. Would it be a good idea to perform a linear regression between these two variables, and to replace one of the variables by the residuals of the regression ? If so, does the "direction" of the regression (var1 ~ var2 or var2 ~ var1) is important ? Considering that the coefficient of determination is the same in both models? Here are the results of the two regressions performed with R's lm() function : And here are the plots of the residuals of both models, so what i would use as a new explanatory variable : Maybe it's important to mention that individuals are ranked by time, so Index is like a temporal variable. If you know anything about using residuals as an explanatory variable I'm listening. Thank you :)
