[site]: stackoverflow
[post_id]: 4766051
[parent_id]: 4764298
[tags]: 
It's hard to say without knowing more, but from the Bayesian perspective, you may be interested in the case of missing features . I'll discuss in general terms. For more, see [Duda and Hart, 2nd ed., pp. 54-55]. For any classifier, the Bayes decision rule is to choose class i which maximizes the probability of class i occurring given that the data x was observed, i.e., max P(i|x) . The vector x contains features, e.g., a student's grades, age, etc. Not all students take the same classes, so the feature vector x may have empty elements, i.e., "missing features". In that case, you must marginalize over the missing features, i.e., just sum over the missing features, and then make a decision upon the good, remaining features. Example. Suppose a student took biology, but not chemistry: P(student drops out | A+ in biology) = P(student drops out, A+ in biology)/P(A+ in biology) = P(student drops out, A+ in biology, A in chemistry) --------------------------------------------------- P(A+ in biology, A in chemistry) + P(student drops out, A+ in biology, B in chemistry) --------------------------------------------------- P(A+ in biology, B in chemistry) + ... + P(student drops out, A+ in biology, F in chemistry) --------------------------------------------------- P(A+ in biology, F in chemistry)
