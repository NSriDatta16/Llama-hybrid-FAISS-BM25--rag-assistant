[site]: crossvalidated
[post_id]: 599792
[parent_id]: 599789
[tags]: 
Clearly, the problem is how you technically implemented your pruning, not the pruning. If you did pruning of neurons, you need to really remap everything to smaller dimensions (i.e. you remove the removed neurons and remap all the indices, which is tedious but obvious how it would be done), then computation should be faster. When you prune weights (instead of neurons), your weight matrix becomes more sparse. As far as I am aware, sparse matrix operations should speed things up. By really setting the weights to zero in weight matrix (to make the matrix partially sparse) instead of matrix-multiplying on the mask every time, you should be able to realize these benefits (at the very least there should be no way this is meaningfully slower than without pruning). However, note that there are whole packages written to do pruning (e.g. this one ) so I would check whether they have implemented what you want/need. Pruning techniques that aim to optimize actual training/inference speed on real existing devices (or even co-optimizing hardware and neural network) as opposed to just reducing the number of model parameters are areas of ongoing research.
