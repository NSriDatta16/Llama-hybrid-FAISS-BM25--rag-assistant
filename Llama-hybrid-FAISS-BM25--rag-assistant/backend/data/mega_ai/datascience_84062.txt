[site]: datascience
[post_id]: 84062
[parent_id]: 
[tags]: 
Backpropagation and gradient descent

I just want to clear one doubt - we use gradient descent to optimize the weights and biases of the neural network, and we use backpropagation for the step that requires calculating partial derivatives of the loss function, or am I misinterpreting something?
