[site]: crossvalidated
[post_id]: 206546
[parent_id]: 206524
[tags]: 
Yes, you initialize the weights/positions randomly. The position update during learning consists in moving the position of the best matching unit (BMU) and its neighbors towards the input . From Wikipedia: The update formula for a neuron v with weight vector Wv(s) is Wv(s + 1) = Wv(s) + Θ(u, v, s) α(s)(D(t) - Wv(s)) , where s is the step index, t an index into the training sample, u is the index of the BMU for D(t), α(s) is a monotonically decreasing learning coefficient and D(t) is the input vector ; Θ(u, v, s) is the neighborhood function which gives the distance between the neuron u and the neuron v in step s. Regarding the 2D space, it is just the map coordinates of your neurons. You'll never explicitly convert your N-D space into a 2D space, the SOM neurons' weights will always be N-dimensional. You may be confused by images like this one: which represent your data as a 2D lattice, but that's because the input data is also 2D (the points in the figure)! If your input space is N-D (N > 2), you can't use this type of representation. For instance, if you're working with images, you may represent the learned SOM like this: note that the stored data is still N-D (the images), just their organization is 2D.
