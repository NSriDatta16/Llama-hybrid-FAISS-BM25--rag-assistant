[site]: datascience
[post_id]: 53357
[parent_id]: 53354
[tags]: 
L2 loss is based on the square of the weights of your Network. As a given weight increases in size, the loss will increase exponentially (quadratically, to be precise). Neural Networks are therefore "pushed" to spread the weight values more evenly across each layer, since for such a quadratic loss factor it's better to have a lot of smaller weights, rather than few very big ones. The exponential relationship between weight value and loss is the reason why large weights get penalized. L2 loss is typically useful when your model suffers from overfitting, since weights redistribution across layers is good to prevent few neurons from staling all the "explanatory power" and generating overfitting. (The main drawback of L2 loss is that the model becomes more sensitive to outliers, therefore less robust.) Hope this helps, otherwise let me know.
