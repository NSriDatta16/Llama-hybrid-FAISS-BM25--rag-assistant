[site]: crossvalidated
[post_id]: 571507
[parent_id]: 571500
[tags]: 
This is a judgment call. Treating ls_composite as continuous is a special case (nested model) of the model where you treat it as categorical (i.e., if the differences between subsequent pairs happen to be identical). If the pattern is approximately linear then you will be better off treating ls_composite as continuous; otherwise you will be better off with a categorical model. My guess is that you will lose power/predictive accuracy either way if you "guess wrong": if the pattern really is approximately linear but you allow the flexibility of a categorical model, then you will lose power/run the risk of overfitting/increase variance because you have a more complex model than necessary. if the pattern is not approximately linear then you will lose power/run the risk of underfitting/increase bias because you have a less complex model than necessary. There are a variety of ways of attempting to split the difference, but unless you're very careful and do something sophisticated, most of them will constitute "data snooping" and will mess up your statistical inference, e.g.: you could fit the linear/continuous-parameter version and see if there are worrying patterns/indications of non-linearity in the residuals; you could fit the model with polynomial contrasts (i.e., make your predictor into an ordered factor in R), which will fit parameters L (linear), Q (quadratic), C (cubic), .4 (quartic). If the parameters beyond L are small/noisy, that suggests that you could have gotten away with the linear model in the first place ... If you want to split the difference in an a priori way (i.e. decide in advance that you want to use somewhere between 1 and 4 degrees of freedom (Ã— all the interaction stuff), you could use a spline model with a specified number of degrees of freedom, or a polynomial (e.g. compute the polynomial contrasts as above and then throw away a few of the highest-order terms). Another approach is to use penalized regression splines (e.g. as implemented in the mgcv or gamm4 packages) to automatically choose the degree of complexity. (To be honest I'm not sure how this automatic selection fits in with "non-snoopy" inference ...) You will have to specifically tell mgcv to limit the maximum complexity of the spline (it is generally used for continuous variables with much finer sampling). While it doesn't cover mixed models specifically, Chapter 3 of Frank Harrell's Regression Modeling Strategies gives very specific advice about how to 'spend degrees of freedom' (i.e. decide on model complexity), with some fairly elegant and rigorous strategies. Well worth buying or borrowing from a library ... Unfortunately, because Harrell doesn't discuss mixed/multilevel models, he doesn't say much about how the "10-20 observations per parameter" rules of thumb generalize to the multilevel case. As a very rough guide, if a predictor varies within levels of the grouping variable then you can say $n \approx$ the number of individual observations; if it varies only between groups then $n \approx$ the number of groups. (If anyone has a good reference for this area I would love to hear about it ...)
