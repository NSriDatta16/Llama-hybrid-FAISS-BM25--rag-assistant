[site]: crossvalidated
[post_id]: 490457
[parent_id]: 490416
[tags]: 
I wouldn't advise making that constraint. Failure of the solver to converge just means it hasn't reached the global optimum* to within the specified tolerance . If you're getting better cross-validation scores, then you should probably be "close enough" to that optimum to not worry about it. Increase max_iter for the refit, if you'd like. *(In logistic regression the loss is convex, so there's just one global optimum, barring collinear features or perfect separation.) In a similar spirit, I wouldn't search over solvers (except maybe as a convenient way to deal with different solvers being capable of using different regularization penalties), or maximum number of iterations. After fixing the regularization type and strength, there's unique optimal coefficients (again, barring degenerate cases), and running different solvers should produce the same results unless (1) the solver goes off the rails somehow, or (2) difference in precision causes some difference. And the number of iterations should just be set high enough to reach convergence (note that it's "maximum" number of iterations; if a solver gets within its tolerance before that, it won't keep chugging).
