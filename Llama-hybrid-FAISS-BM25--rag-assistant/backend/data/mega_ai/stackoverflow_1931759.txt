[site]: stackoverflow
[post_id]: 1931759
[parent_id]: 1931359
[tags]: 
Some of the mathematical solutions here are very good. Here's a simple technical solution. Use a larger data type. This breaks down into two possibilities: Use a high-precision floating point library. One who encounters a need to average a billion numbers probably has the resources to purchase, or the brain power to write, a 128-bit (or longer) floating point library. I understand the drawbacks here. It would certainly be slower than using intrinsic types. You still might over/underflow if the number of values grows too high. Yada yada. If your values are integers or can be easily scaled to integers, keep your sum in a list of integers. When you overflow, simply add another integer. This is essentially a simplified implementation of the first option. A simple (untested) example in C# follows class BigMeanSet{ List list = new List (); public double GetAverage(IEnumerable values){ list.Clear(); list.Add(0); uint count = 0; foreach(uint value in values){ Add(0, value); count++; } return DivideBy(count); } void Add(int listIndex, uint value){ if((list[listIndex] += value) = 0; i--){ rtn *= shift; remainder Like I said, this is untested—I don't have a billion values I really want to average—so I've probably made a mistake or two, especially in the DivideBy function, but it should demonstrate the general idea. This should provide as much accuracy as a double can represent and should work for any number of 32-bit elements, up to 2 32 - 1. If more elements are needed, then the count variable will need be expanded and the DivideBy function will increase in complexity, but I'll leave that as an exercise for the reader. In terms of efficiency, it should be as fast or faster than any other technique here, as it only requires iterating through the list once, only performs one division operation (well, one set of them), and does most of its work with integers. I didn't optimize it, though, and I'm pretty certain it could be made slightly faster still if necessary. Ditching the recursive function call and list indexing would be a good start. Again, an exercise for the reader. The code is intended to be easy to understand. If anybody more motivated than I am at the moment feels like verifying the correctness of the code, and fixing whatever problems there might be, please be my guest. I've now tested this code, and made a couple of small corrections (a missing pair of parentheses in the List constructor call, and an incorrect divisor in the final division of the DivideBy function). I tested it by first running it through 1000 sets of random length (ranging between 1 and 1000) filled with random integers (ranging between 0 and 2 32 - 1). These were sets for which I could easily and quickly verify accuracy by also running a canonical mean on them. I then tested with 100 * large series, with random length between 10 5 and 10 9 . The lower and upper bounds of these series were also chosen at random, constrained so that the series would fit within the range of a 32-bit integer. For any series, the results are easily verifiable as (lowerbound + upperbound) / 2 . * Okay, that's a little white lie. I aborted the large-series test after about 20 or 30 successful runs. A series of length 10 9 takes just under a minute and a half to run on my machine, so half an hour or so of testing this routine was enough for my tastes. For those interested, my test code is below: static IEnumerable GetSeries(uint lowerbound, uint upperbound){ for(uint i = lowerbound; i
