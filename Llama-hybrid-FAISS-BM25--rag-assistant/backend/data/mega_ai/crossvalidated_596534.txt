[site]: crossvalidated
[post_id]: 596534
[parent_id]: 596524
[tags]: 
If you're dealing with linear regression - the answer is yes, $\epsilon$ is indeed subG. First and most obvious, $E[\epsilon]=E[Y]-E[E[Y|X]]=0$ by law of total expectation. Then, according to Vershynin , all you need to prove is the bound on the MGF of $\epsilon$ . Now, if you're modelling linear regression then $E[Y|X]=\hat{Y}\sim\mathcal{N}$ , which means this is a subG variable as well and by Hoeffding's lemma we get that their sum is also subG. This won't hold for Poisson regression, as the Poisson distribution is not subG. Edit: The thing with subG property is this: you need the distribution to fulfill that, from a point $\lambda>0$ , the probability tail $P(|Y|>\lambda)$ is lighter than the Gaussian tail. This is usually the case when $Y$ is some variant of Gaussian or when it is bounded. If $Y$ is subG with variance proxy $\sigma^2$ then by definition of regression (classic, not Bayesian) we get that $\hat{Y}$ has subG distribution with the same proxy, hence the residual is subG with this proxy as well. The only condition here is that $E[Y|X]=E[Y]$ - that is, $\hat{Y}$ should be an unbiased estimator.
