[site]: crossvalidated
[post_id]: 347980
[parent_id]: 
[tags]: 
How to compare standard deviation across models?

I am doing research in (multi-touch) attribution models. The goal is to assign credit to each channel. I have several models that assign normalized attribution to the channels. I have simulated some data to illustrate it. Model 1: Channel 1: Attribution = 0.80, SD = 0.030 Channel 2: Attribution = 0.10, SD = 0.001 Channel 3: Attribution = 0.10, SD = 0.003 Model 2: Channel 1: Attribution = 0.30, SD = 0.010 Channel 2: Attribution = 0.30, SD = 0.010 Channel 3: Attribution = 0.40, SD = 0.015 What is a "fair" way to say something about the stability of the model? Options: Calculate the average SD. Model 1 = 0.0113 ((0.030 + 0.001 + 0.003) / 3) Model 2 = 0.0116 Compute the average standard deviation as a percentage of the average attribution. Model 1 = 2.5833 ((0.8/0.03*100) + (0.1/0.001*100) + (0.1/0.003*100) / 3) Model 2 = 3.4722 ((0.3/0.01*100) + (0.3/0.01*100) + (0.4/0.015*100) / 3) Or are there other options to compare the SD between models? Does anyone know a how to compare the standard deviation between models?
