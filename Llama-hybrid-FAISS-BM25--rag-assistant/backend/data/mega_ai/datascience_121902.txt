[site]: datascience
[post_id]: 121902
[parent_id]: 
[tags]: 
I have created a classification model for predicting data, and the problem is that the two classes are highly imbalanced

I have a problem. I have created a classification model for predicting data, and the problem is that the two classes are highly imbalanced. So, I dealt with it using the SMOTE+ENN technique. I applied SMOTE+ENN before splitting the data into training and test sets. The reason is that SMOTE generates synthetic data to balance the classes. I thought that performing SMOTE+ENN before splitting the data would create a representative state for the data What I want to ask is, I performed SMOTE+ENN first and then split the data into training and test sets. I am asking if my approach was correct based on ChatGPT's advice? Currently, I am conducting research for a journal article, and I am unable to modify the model. The only thing I can do is to provide supporting research or reasoning as to why SMOTE+ENN is performed before splitting the training and test data. Can you please help me with some supporting arguments or rationales for this approach Can I provide the following rationale: "Performing SMOTE+ENN before splitting the data can still be effective because it aims to create a more balanced situation in the dataset by generating synthetic data through SMOTE that resembles the original data but with different statistical values. This means that there will be new data points introduced. At the same time, ENN helps reduce the redundancy of samples close to the minority class. I have also set the parameter to increase the data by only 10% and decrease it by 10%, which is a minimal change. Therefore, the model's performance remains relatively unchanged, and the interpretation of model evaluation only slightly varies."
