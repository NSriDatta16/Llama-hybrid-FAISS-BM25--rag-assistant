[site]: crossvalidated
[post_id]: 339859
[parent_id]: 337030
[tags]: 
As a rule for lme4 and other packages with a similar parameterization (at least at the level of the user interface), it does not make sense to have random slopes for terms not present in the fixed effects. The reason for this is straightforward: the random effects (or more precisely, the BLUPs / conditional modes) are computed as offsets from the population-level / fixed effects. So if a given fixed effect is missing, then this is equivalent to assuming that the population-level effect is zero. This is a rather strong assumption and not one we generally want. It will also mess up the estimation of the variance (the actual critical part for random effects, which are in other contexts called variance components ), because the variance is calculated as the mean squared distance to the mean and if your assumed mean doesn't match the actual one, then your variance will be wrong. (Note that this is part of the reason for calculating random effects as offsets from the population mean: it means that the random effects have mean equal to 0, so that part of the formula just cancels out.) As an example of the repercussions of this, consider the following two models: m (I'm using the intercept term here for simplicity instead of dealing with slopes, but the same ideas hold equally.) The random effects show that the models actually use the offsets : > ranef(m) $Subject (Intercept) 308 37.829172 309 -72.209815 310 -58.536726 330 4.087222 331 9.476087 ... > ranef(m.0) $Subject (Intercept) 308 341.3933 309 214.7671 310 230.5013 330 302.5651 331 308.7663 ... The first set include negative values because some subjects are faster than the population average, while the second set includes only positive values because all subjects had positive reaction times. We can also extract the individual predictions by combining the offset and the population mean, lme4 will helpfully do this for you: > coef(m) $Subject (Intercept) 308 336.3371 309 226.2981 310 239.9712 330 302.5951 331 307.9840 ... (For m.0 , this is of course identical to the random effects.) Note that these values do not match up with the random effects from m.0 . This is important -- the random effects for both models are shrunk towards 0, but for m this corresponds shrinking just the offsets, i.e shrinking the individual predictions towards the (grand) mean. For m.0 , this corresponds towards shrinking the individual predictions towards 0. This will of course yield different results -- all the individual predictions in m.0 become smaller, but the individual predictions in m can become bigger or smaller, depending on whether an individual subject was faster or slower than the (grand) mean reaction time. The variance estimates also differ: > VarCorr(m) Groups Name Std.Dev. Subject (Intercept) 35.754 Residual 44.259 > VarCorr(m.0) Groups Name Std.Dev. Subject (Intercept) 300.505 Residual 44.259 Clearly m.0 is wrong in some rather fundamental sense: the standard deviation between subjects is not 300.505! Now, overall m.0 does a decent job of fitting the data (with a similar log likelihood to m ), but it does so less efficiently (because the computational assumptions of the model is not met) and with parameter estimates that are incorrect/misleading. Now, it is possible to parameterize mixed models such that random effects aren't centered (or "spherical") in this way, and indeed I believe brms uses a non-centered parameterization for its Stan code (there's something about the way the centered parameterization creates weird chokepoints in the critical set for Hamiltonian MCMC), but the formula interface for the most popular packages -- nlme , lme4 , brms , rstanarm -- nonetheless requires a centered specification. Since you've recently discovered the different types of sums of squares, make sure to check out Venables' Exegeses on Linear Models , which is often mentioned in such discussions, especially with regards to whether Type-III SS even examine interesting hypothesis (instead of the usual rant about whether they make "sense"). John Fox's excellent book Applied Regression Analysis & Generalized Linear Models . The index coveniently has an entry "Marginality, principle of" with references to many different points in the text whether issues related to this (and thus Type II vs III SS) come to play. car::Anova() which can compute both Type II and III SS for lmer models, either using the $\chi^2$ distribution (i.e. treating the $F$ denominator degrees of freedom as infinite, analogous to treating $t$ values as $z$ values) or using $F$ distribution with Kenward-Roger approximated denominator degrees of freedom. ( car is an abbreviation for "Companion to Applied Regression".) lmerTest::anova() which will compute Type I, II and III sums of squares for lmer models using with options for using the Satterthwaite or Kenward-Roger approximations for the denominator degrees of freedom. Note that as of this writing, there is a major package rewrite in beta which generally improves computational efficiency (by caching the ddf approximations) compared to the current version on CRAN.
