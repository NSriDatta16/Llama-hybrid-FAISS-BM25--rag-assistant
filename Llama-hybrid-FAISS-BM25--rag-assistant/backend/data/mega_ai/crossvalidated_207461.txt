[site]: crossvalidated
[post_id]: 207461
[parent_id]: 99806
[tags]: 
I don't think there is anything published on this. You could always work with simulations: simulate normally distributed data with a given mean $\mu$ and variance $\sigma^2$ and assess the MAE of the perfect forecast $\mu$ (which you know in this case). Yes, for the normal distribution, you don't really need to simulate - you can easily calculate the expected MAE . But simulations are more easily adopted to other distributions. So now you can give the minimum possible expected MAE for a time series with a given residual variance $\sigma^2$. You would still need to argue that for a given time series, you actually know the residual variance - that is, you need to argue that you cannot explain any more variance. This is hard, because with more information about the data generating process, you may indeed be able to explain more variance - it may just become unrealistic. Note incidentally that minimizing the MAE may lead to biased forecasts for non-normal (specifically: asymmetric) distributions .
