[site]: crossvalidated
[post_id]: 622408
[parent_id]: 
[tags]: 
Simulating a Markov Chain by Hand

As a learning exercise, I am trying to learn how to fit and simulate from Continuous Time Markov Chains. Suppose I have a Stochastic Process that can assume 3 States: S1, S2 or S3. Lets say I have observe this process making 100 state transitions and I record the time that each transition was made at. I simulated this data using the R programming language: # Define the possible states states As I understand, there are 4 Steps required in fitting a Continuous Time Markov Chain: Step 1: First we need to calculate the probabilities within the Embedded Discrete Time Markov Chain. This contains the probabilities of transitioning from "state i to state j" and can be calculated as the number of times the process went from 'state i to state j' divided by the number of total exits out of 'state i', represented as $$p_{ij} = \frac{n_{ij}}{n_i}$$ Step 2: Next, we need to calculate the "lambda_i" - this is the "rate of leaving state i". The formula for "lambda_i" is based on the Maximum Likelihood Estimate of the "Rate Parameter" in an Exponential Distribution. This is equal to the number of transitions out of "state i" divided by the total time spent in "state i" prior to exiting: $$\lambda_i = \frac{n_i}{t_i}$$ Step 3: Next, we need to calculate the entries of the "Rate Matrix" (Q_ij). These entries represent the rates of transitioning between two states "i" and "j". This can be considered as the "rate of leaving "state i" multiplied by the probability of transitioning from "state i to state j". We can write this as follows: $$Q_{ij} = \begin{cases} \lambda_i p_{ij}, & \text{if } i \neq j \ \sum_{k\neq i} Q_{ik}, & \text{if } i = j \end{cases}$$ Step 4: Finally, we can calculate the "time dependent probabilities of transitioning from state i to state j" : $$ p_{ij}(t) = e^{Qt} $$ The above expression is said to be approximated using Numerical Methods such as Taylor Expansions or Pade Approximants. For the above data - I tried to write a computer program to first fit a Continuous Time Markov Chain to this data, and then to simulate trajectories of this Continuous Time Markov Chain at different time periods: # Step 1 # Define the possible states states $times[my_data$ transitions == "S1"]) / nrow(my_data[my_data$transitions == "S1", ]) # Calculate lambda_2 lambda_2 $times[my_data$ transitions == "S2"]) / nrow(my_data[my_data$transitions == "S2", ]) # Calculate lambda_3 lambda_3 $times[my_data$ transitions == "S3"]) / nrow(my_data[my_data$transitions == "S3", ]) # STEP 3 # Define the rate matrix rate_matrix Now, I would like to try and simulate trajectories from the Continuous Time Markov Chain at different times (e.g. suppose I start at State 1 with probability = 1): library(Matrix) library(expm) p0 We can also observe the long term probabilities of being in any state over a period of time: library(ggplot2) # define the time range t_range My Question: Have I understood and done this correctly? Note: Stationary Distribution Calculations for the Continuous Time Markov Chain As I understand, the Stationary Distribution of a Markov Process can be considered to be a "magical distribution" such that if this "magical distribution" happens to be the initial distribution of the Markov Process, then the transition dynamics of the Markov Process will reach an equilibrium (i.e. stabilize). In a Discrete Time Markov Process, the Stationary Distribution can be written as follows: $$\pi P = \pi$$ In a Continuous Time Markov Process, the Stationary Distribution has a similar implications: # Calculate the eigenvectors and eigenvalues of the rate matrix eig $vectors[, which.min(abs(eig$ values))] # Normalize the eigenvector to obtain the stationary distribution pi
