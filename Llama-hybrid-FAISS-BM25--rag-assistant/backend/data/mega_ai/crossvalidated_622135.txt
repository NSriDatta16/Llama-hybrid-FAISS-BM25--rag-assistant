[site]: crossvalidated
[post_id]: 622135
[parent_id]: 
[tags]: 
How can I accumulate R-squared over time intervals with a model that makes predictions for a single interval?

I have a model that makes a prediction of the change in a number during a time interval. The model depends on covariates that happen during the interval of prediction, as well as a weighted average of those same covariates during recent past intervals. There is a lot of noise in this process. $y_t = f(x_t) + g(x_{t-1},x_{t-2},...) + \epsilon_t$ I want to use the model to estimate the expected evolution of $y$ given some expected values of $x_t$ . There is a question about the appropriate size of the interval with which data is sampled. As I expected, using larger interval sizes almost always results in a larger $R^2$ . This is intuitive to me because noise scales with the square root of time. Taken to the extreme, I might naively conclude that I should make the interval size cover the entire process, which would effectively mean that $g()$ drops out of the model, since there are no previous periods anymore. But I have good theoretical and empirical reasons to believe that $g()$ is an important part of the process. How can I go about choosing an optimal interval size in this case? My hope is that there is some way to scale the $R^2$ based on the interval size, analogous to how the mean of a process scales linearly with $t$ but the standard deviation scales with $\sqrt{t}$ . To ask this question in another way: given the model specification above for $y_t$ and it's associated $R^2$ , you can derive a model for $y_t + y_{t+1}$ , but can you derive the $R^2$ for this new model? EXAMPLE : if you understand the above, no need to look at the below. This is some code that simulates a simple process with the features described above. It then fits a model directly on the output, and another on some downsampled output. The second model has a higher $R^2$ even though it is the wrong model. I know it is wrong because I simulated the data and fit the first model in a way that matches the true data generating process. The reason the send model has a higher $R^2$ is because the noise "cancels itself out a bit" over the longer interval length. rm(list = ls()) library(conflicted) library(tidyverse) library(foreach) conflict_prefer_all("purrr", "foreach") set.seed(123) n_runs x_prob_threshold) { x_val } else { 0 } } else { if (x_prob > x_prob_threshold) { 0 } else { x_val } } }, .init = 0) |> pad_result() } epsilon_sd print() downsample sapply(\(i){x[i*2-1] + x[i*2]}) } x_ print() Output: correct model results Call: lm(formula = y ~ x + cum_x - 1) Residuals: Min 1Q Median 3Q Max -159.64 -32.78 -0.76 31.54 172.30 Coefficients: Estimate Std. Error t value Pr(>|t|) x 21.637 1.561 13.87 |t|) x_ 25.510 2.217 11.51
