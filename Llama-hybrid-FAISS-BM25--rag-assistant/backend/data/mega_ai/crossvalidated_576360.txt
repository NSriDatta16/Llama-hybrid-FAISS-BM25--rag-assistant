[site]: crossvalidated
[post_id]: 576360
[parent_id]: 
[tags]: 
Does a CNN always learn a latent space?

In general, a latent space is a structure of reduced dimensionality than that of the input space where points on this space share resemblance the closer they are to each other. This article also refers to the layers of a convolutional neural network as a latent space (see the diagram). Some CNNs essentially squash an input image into a compressed representation too with appropriate use of convolutional and pooling layers. What I want to understand is: can we really look at the CNN's layers as the same kind of latent space representation as described in the former definition, i.e are the feature representations generated by these layers also like points on a latent space? I cannot seem to understand this. If so, where can I find some good literature where a latent space representation is explained like this on a more general level for CNNs?
