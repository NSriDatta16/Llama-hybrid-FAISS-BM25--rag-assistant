[site]: crossvalidated
[post_id]: 322534
[parent_id]: 322532
[tags]: 
Ridge regression is mathematically equivalent to a Bayesian regression with a gaussian prior with $\beta$ centered around 0 (see page 11 of the Gaussian Process book here ). I think that's helpful as a way to understand when to deploy ridge regression. If you are worried that the data may generate unnaturally large $\beta$ (say, many $x$ share a high linear correlation) you might think of the ridge regression as a way to limit that problem. It tends to produce smaller $\beta$ than the corresponding OLS. Lasso regression tends to set a lot of $\beta$ to zero. It is useful in studies where you have many potential $x$ but only a few observations (say, genetics) or where you don't have a strong theoretical reason to believe that all $x$ matter. I think the most common approach is actually to do both lasso and ridge at the same time (the elastic nets approach). R has the beautiful covnet package precisely for this purpose (including self-tuning by cross-validation). Of course each problem is different and what you should do is to withhold some data and test prediction error after the fits are done.
