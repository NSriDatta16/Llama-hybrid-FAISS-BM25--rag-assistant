[site]: datascience
[post_id]: 116610
[parent_id]: 
[tags]: 
How does dropout behave like model averaging?

It is claimed Srivastava, Hinton, et al. that "dropout can be effectively applied in the hidden layers as well and that it can be interpreted as a form of model averaging" and that "training a neural network with dropout can be seen as training a collection of $2^n$ thinned networks with extensive weight sharing". First: I can intuitively see how dropout is a form of model averaging. At every epoch during training, neuron values are stochastically set to 0. This is akin to having a different neural network (with different numbers of hidden neurons disabled) at every epoch. Is there a mathematical reason why this is interpreted as model averaging? Second: where does the $2^n$ come from in the second quote?
