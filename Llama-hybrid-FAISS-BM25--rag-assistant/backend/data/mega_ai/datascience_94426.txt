[site]: datascience
[post_id]: 94426
[parent_id]: 
[tags]: 
Should I repeat the testing/training split while repeating K-fold cross validation?

I'm fitting random forest regressions on my data, and using 10 K-fold cross-validation to evaluate model performance. While re-runing the cross-validation, I noticed that the results differed between each run, sometimes by a lot. So, I decided to repeat the cross-validation calculation 20 times, creating a for-loop, and then summarising the results afterwards. Just to illustrate, I'm doing something like this: for (i in 1:20) { trainIndex So, essentially, each re-run I am splitting my dataset into training and testing sets again. This results in a lot of variance, when ploting the resuls of each run together they vary from ~ 0.2 to 0.6 R.squared . If I don't do this within the loop (i.e. if I split into training/testing before the loop), then the results of the 20 runs are very similar. Which way is the right way to go about this?
