[site]: datascience
[post_id]: 74975
[parent_id]: 
[tags]: 
Approipriate use of rfe with xgboost classifier

I have an imbalanced data set with about 6000 features. I am using the XGBoost classifier to classify the labels. However, due to the large number of features, I am opting for rfe function to select an appropriate number of features (to prevent overfitting). Is there a less expensive feature selection alternative to rfe? I would like a demo maybe... I am pretty stuck right now and I am not sure how to do this properly. Here is my code: y=training['prediction'] x=training.drop(['prediction'],axis=1) imp_mean =SimpleImputer(missing_values=np.nan,strategy ='most_frequent') impx=pd.DataFrame(imp_mean.fit_transform(x)) imptest=pd.DataFrame(imp_mean.fit_transform(test)) x=impx test=imptest model =XGBClassifier() feat_selector = RFE(model,10) feat_selector = feat_selector.fit(x, y) f =feat_selector.get_support x = df[df.columns[f]] x_train,x_val,y_train,y_val = train_test_split(x,y,test_size =0.30,random_state=1) scaler = StandardScaler() x_train = scaler.fit_transform(x_train) x_val=scaler.fit_transform(x_val) abc =XGBClassifier(max_depth=2,n_estimators=50) abc.fit(x_train,y_train) y_pred =abc.predict(x_val) print(accuracy_score(y_val,y_pred)) print(confusion_matrix(y_val,y_pred)) Thanks.
