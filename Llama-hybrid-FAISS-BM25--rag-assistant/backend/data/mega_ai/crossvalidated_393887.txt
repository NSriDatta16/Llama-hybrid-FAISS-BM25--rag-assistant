[site]: crossvalidated
[post_id]: 393887
[parent_id]: 393776
[tags]: 
Why do I receive poor performance on the natural test distribution, when clearly the classifier is able to discriminate between the two classes SVM doesn't try to optimize the performance of any single outcome (i.e. positive or negative). It attempts to produce the 'best' classification overall. This is done by dividing the dataset into two regions with the region determining how to classify points. In order to do this SVM tries to finds a 'good' hyperplane (boundary) between the two regions. SVM defines 'good' as a balance between a split that accurately classifies data points (minimizes hinge loss) and a split that maximizes the size of the 'margin' (distance between closest correctly classified negative point and the boundary plus the closest correctly classified positive point and the boundary). Kent Munthe Caspersen gives a great explanation of this in the following thread. " What is the influence of C in SVMs with linear kernel? ". If you add additional training data (of a single class) you will potentially do two things to the costs being minimized by SVM. 1. you could increase the number of points misclassified in that class (increasing ridge cost) 2. you could decrease the margin (also increasing cost as SVM tries to maximize the margin) in order to compensate for the increased cost the optimal boundary would move to more accurately predict the class of data added. this would naturally come at the cost of the accuracy of the remaining class. What is the appropriate solution to solve this problem? Is it solvable? The most appropriate solution depends on your goals. Though my bias is normally to use a probabilistic classifier of some type if possible as there is normally the option to collect more data about an observation before classifying it. Assuming SVM is the best classifier for your purposes. You can weight the costs of the objective function SVM is minimizing. If your implementation of SVM supports adding weights to the cost function of the SVM then that can be used otherwise duplicating the positive class would produce an equivalent effect to weighting the cost function. You would want to duplicate 4 times in order to make positive errors 4 times as important as it currently is (and therefore balanced with negative). Is it a problem of poor features? No Is it a problem which SVM cannot solve? see above
