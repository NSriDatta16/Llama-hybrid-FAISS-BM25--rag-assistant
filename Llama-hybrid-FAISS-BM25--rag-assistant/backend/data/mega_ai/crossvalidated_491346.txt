[site]: crossvalidated
[post_id]: 491346
[parent_id]: 
[tags]: 
Do typical NNs generate new features by applying some function to the input?

The toy network on playground.tensorflow.org has the option to generate new features by applying some function based on the input, e.g. with the inputs $X_1, X_2$ it generates new inputs $X_1X_2, sin(X_1), sin(X_2), etc$ . This leads to faster convergence of the NN when used. I could not find however typical NNs using the same technique, e.g. image processing CNNs have 3 channels for RGB. However, they usually do not generate new channels based on applying some function to the input. Why is this the case? I did not find any papers about this.
