[site]: crossvalidated
[post_id]: 589684
[parent_id]: 589681
[tags]: 
If I understand you correctly, you are asking "what is the black box" . First of all, not only neural networks are "black boxes", but this would be the case also for simple machine learning algorithms like linear regression or decision trees if they have a lot of parameters. You are correct that there is nothing mysterious about piecewise linear functions, or even linear functions packed in the sigmoids. The problem is when you have a lot of such functions and they all interact with each other. Look once again at the popular graphic from Wikipedia: Each arrow means that the output of the previous function goes as input to the next one. Even for as simple a neural network as above, you wouldn't be able to give a simple answer to the question "ok, so how does it work?". It works by applying the mathematical functions to the inputs, but without doing any calculations by hand could you approximately guess what it predicts for a given input..? Likely not. If I asked you what should you change in the neural network so that it returns a specific prediction, you wouldn't have a simple answer to that question. Say that you have an engineering degree. It would mean that you "know" engineering and know how things work. However, this wouldn't mean that if someone gave you a space shuttle and some tools, but no manuals whatsoever, you could easily backward engineer how it works and "understand it". With your engineering skills, you wouldn't be able to fix it or fly it. It would be easy to learn how individual components work, but you wouldn't be able to produce a realistic mental model of a whole space shuttle. Even in the case of Nasa, you have whole teams dedicated to individual components rather than one engineer who "knows how it works".
