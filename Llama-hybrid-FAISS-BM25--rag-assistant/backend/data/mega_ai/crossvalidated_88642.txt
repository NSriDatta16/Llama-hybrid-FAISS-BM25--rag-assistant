[site]: crossvalidated
[post_id]: 88642
[parent_id]: 
[tags]: 
Model instability in data mining. When it is big enough to discredit a model and how to measure it?

Let's say I have two models. One has cumulative lift on test data 4.322578, second 2.84488. The only advantage of the second over the first consists in the quality of having the cumulative lift curve nearly identical for both train and validate sets. For the first model the curve of cumulative lift looks as you can see below. Sorry for non-english elements on the plot, but that's the only SAS Enterprise Miner version I have access to. Mean standard error for the model from the plot - the first one - for train, validate and test sets are: 0.038389, 0.04999 and 0.055314. For the second one: 0.070393, 0.071889 and 0.07727. First model seems to be obviously better in terms of accuracy. But at the same time it is less stable, more sensitive to data changes. My question is whether those stability issues are here big enough to discredit that model in favor of the second one? How to measure instability and at which point it renders a model unusable? If it is important, the models in question are neural networks.
