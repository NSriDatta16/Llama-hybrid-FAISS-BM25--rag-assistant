[site]: stackoverflow
[post_id]: 343968
[parent_id]: 
[tags]: 
Reading a large file into a Dictionary

I have a 1GB file containing pairs of string and long. What's the best way of reading it into a Dictionary, and how much memory would you say it requires? File has 62 million rows. I've managed to read it using 5.5GB of ram. Say 22 bytes overhead per Dictionary entry, that's 1.5GB. long is 8 bytes, that's 500MB. Average string length is 15 chars, each char 2 bytes, that's 2GB. Total is about 4GB, where does the extra 1.5 GB go to? The initial Dictionary allocation takes 256MB. I've noticed that each 10 million rows I read, consume about 580MB, which fits quite nicely with the above calculation, but somewhere around the 6000th line, memory usage grows from 260MB to 1.7GB, that's my missing 1.5GB, where does it go? Thanks.
