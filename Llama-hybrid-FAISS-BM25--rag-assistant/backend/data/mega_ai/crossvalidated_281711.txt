[site]: crossvalidated
[post_id]: 281711
[parent_id]: 281702
[tags]: 
Fourier basis functions are "global", extending over the entire signal/image domain. More typically the convolution filters used in image processing/computer vision will be local . For example moving average or derivative style filters. My understanding of ConvNets is that the filters are typically local. But rather than using a pre-defined set of filters, the filter coefficients are learned (only the window- sizes are pre-specified). To expand on the global vs. local distinction, for FFT, one basis function gives a single (complex-valued) output for a given image, since the basis function is global. For a local filter, as in CNN, one basis function (filter) gives an image output of local filter-response over the input image. (Possibly the output image is smaller, depending on padding and stride .) In each case the total output will be a set of filter-responses, one for each basis-function in the filter bank. For the FFT the "filters" will correspond to different frequencies for FFT. For CNNs the filters are more flexible, e.g. after training they could end up effectively being "oriented edge detectors". Beyond this, at a high level, a key component of successful CNNs is depth , which is enabled* by nonlinearity such as max-pooling and ReLU activations. (*Since composition of linear functions would just give a linear function.) I cannot really speak from experience on the details of how this plays out. But to speculate, both of those classic CNN nonlinearities would allow "attention focusing", by eliminating non-salient filter responses. Thus at the lower levels the CNN can implicitly accomplish feature detection and then description, while at the higher levels feature-arrangements can be used for object detection and then discrimination. So a single deep architecture can accomplish multiple tasks in the more classical end-to-end pipeline (e.g. SIFT ).
