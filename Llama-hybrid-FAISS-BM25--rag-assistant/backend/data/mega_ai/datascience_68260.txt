[site]: datascience
[post_id]: 68260
[parent_id]: 
[tags]: 
Papers presenting results that are worse than random chance

Is it me or has there been an increasingly large amount of object detection papers describing models that are performing worse than chance. Here is an example (an extract so not to name names):- AP represents the average precision. No mention of recall. The paper goes on to say that YOLOv3-Lite reaches state of the art performance in detecting the specified object. This table would suggest to me that the models are worse just flipping a coin. What am I missing here?
