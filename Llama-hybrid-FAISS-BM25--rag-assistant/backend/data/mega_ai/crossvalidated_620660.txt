[site]: crossvalidated
[post_id]: 620660
[parent_id]: 477026
[tags]: 
For your second question, I just found a related paper (2020 Granziol D, Wan X, Roberts S) Gadam: Combining Adaptivity with Iterate Averaging Gives Greater Generalisation that probably provides an answer: We introduce Gadam, which combines Adam and iterate averaging (IA) to significantly improve generalisation performance without sacrificing adaptivity. This paper provides an approach to combine Polyak averaging (the IA therein) and Adam, discusses its strengths in shrinking original Adam's generalization error, and lists possible reasons that this line of research has not been much investigated. For your first question, I would like to point out that Polyak averaging is developed solely for SGD to stabilizes the estimates. A simple average of historical estimates in a regular GD is meaningless. Meanwhile, the momentum can also be used in GD to alleviate the zig-zag behavior when the parameters are travelling in a deep valley of the optimization surface. From my perspective, there is not much to compare between momentum and Polyak averaging beyond that they are both used to accelerate the convergence. They are more of two independent techniques. To confirm this, note that Adam also includes a momentum term and it can be used in conjunction with Polyak averaging according the aforementioned paper.
