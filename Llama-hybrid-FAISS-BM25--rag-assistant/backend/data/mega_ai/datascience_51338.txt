[site]: datascience
[post_id]: 51338
[parent_id]: 51335
[tags]: 
For the correlation problem, this basically sums up why I dislike univariate feature selection. Just because a feature has low correlation with your response in isolation, does not mean that the variable itself is not informative. The variable could relate in a non linear way (so transforming it could be useful), it could work well with other variables in a multivariate setting, and also could be used to generate other variables through feature engineering. With respect to a "low p value", note that a lower p value suggests that the variable is significant, that is, the effect of the variable on the response is statistically different from 0. So if you were to use p values as a means of feature selection, you would select variables with smaller values. However, this is not at all a good way to select variables if your goal is to improve predictive performance. P values are meant to test very specific statistical hypotheses related to statistical inference, not prediction. Removing variables with high p values will likely lead to a loss in predictive performance because p values don't even answer the right question in this context. Also, realize that by conducting multiple hypothesis (looking at individual p values for coefficients) you further increase your chance of a false positive beyond what you set your significance level at (i.e. the cutoff that you use to deem a variable to be significant). If you want to do feature selection within the context of a linear model, look into the LASSO/Elastic Net. These models will shrink the coefficients of variables to zero if they do not improve the fit of the model. Otherwise, if you only have a handful of variables then go with model 2 since it is the highest scoring model (on this specific test set anyway). Possibly repeat model evaluation on different partitions of the dataset to see if model 2 is consistently better than model 1. This would allow you form confidence intervals which could be useful in deternining if the extra column gives a significant performance increase over a simpler model. Finally, I recommend not using AUC or accuracy for comparing the performance of models. These metrics are not proper scoring rules and may lead you to selecting models that don't discriminate between your classes as well as others.
