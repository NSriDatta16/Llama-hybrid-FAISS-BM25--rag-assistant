[site]: crossvalidated
[post_id]: 495402
[parent_id]: 
[tags]: 
Is cross-validation was a better way to show the variance of the slope of a line than a Bootstrap or a Bayesian approach?

Background I am trying to estimate the slope of a line for a univariate linear regression analysis as well as uncertainty about that slope (95% Confidence Interval). Originally I used two approaches to accomplish this: (1) Bootstrapping, where the data were repeatedly resampled with replacement and a regression line was fit for each resampling, and (2) a Bayesian regression, where 100,000 credible regression lines were generated based on the data and vague prior assumptions about the data. I felt good about these approaches to estimate the variance in the slope of the regression line for my data, but I was told that cross-validation was a better way to show variance. I was under the impression that cross-validation was a good way to assess how well a particular model will perform when given out of sample data, but wasn't aware of its usefulness in estimating the variance of a particular parameter within a model such as slope. My questions Is it true that cross-validation was a better way to show the variance of the slope of a line than a Bootstrap or a Bayesian approach? Assuming that #1 is true, if the database only consists of 21 sample points then would there be a certain type of cross-validation that is more appropriate than others (e.g., leave-one-out cross-validation, 10-fold cross-validation, repeated k-fold cross-validation)? Again assuming that #1 is true, what measures would be most important for quantifying the variance of slope using cross-validation? (e.g., some average of the parameter estimates of the various folds?) There is another post that sort of addresses the idea of variance determined via cross-validation but it doesn't seem to provide a clear answer to these particular questions. Appreciate any insights!
