[site]: crossvalidated
[post_id]: 606236
[parent_id]: 
[tags]: 
How to calculate the decay rate given an initial learning rate and final learning rate for schedulers when training neural networks?

I am training a neural network in TensorFlow and I would like to use firstly an exponential decay optimizer scheduler ( https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay ) and then also a cosine decay ( https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/CosineDecay ). I need to set the decay rate for exponential decay and alpha for cosine decay in such a way, that after x epochs, the initial learning rate will just pass the final learning rate value. I am having trouble formalizing this mathematically and calculating the necessary decay rates. Any ideas?
