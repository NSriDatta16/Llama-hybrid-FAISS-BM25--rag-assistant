[site]: crossvalidated
[post_id]: 510475
[parent_id]: 507827
[tags]: 
If you notice in the book the previous section says that the purpose of Hard Margin SVM is the following: $$argmax_{(w,b): ||w||=1} \min_{i \in [m]} | \langle w,x_i\rangle+b| \quad s.t\quad \forall i, \quad y_i(\langle w,x_i\rangle+b) \geq 0$$ . So theoretically lets say the optimal hyperplane for this is $w^*, b^*$ i.e it maximizes among all the minimum possible margins (calculated from all of the dataset). Thus clearly: $$\min_{i \in [m]} | \langle w^*,x_i\rangle+b| = \delta$$ . Then clearly, $(\frac{w^*}{\delta}, \frac{b^*}{\delta})$ is the hyperplane which gives a margin of $1$ for all the examples $i \in [m]$ . And that is the reason for the same. You are trying to find the hyperplane which provides correct classification, and you that by finding a classifier which classifies with a large margin i.e $(\frac{w^*}{\delta}, \frac{b^*}{\delta})$ and the associated problem is actually a convex optimization problem unlike the actual objective which is very difficult to optimize (I believe it is called min-max problems). And then so that the actual problem is solved i.e $||w|| = 1$ you scale it after solving the convex optimization problem.
