[site]: datascience
[post_id]: 28707
[parent_id]: 28584
[tags]: 
If I understand you correctly, you are wondering whether it is possible to have inputs where one element is a list or a matrix. It is not. First, this would require a complete rewrite of all activation and decision functions in Keras. Take a linear activation for instance. Keras expects that it gets a bunch of numbers $x_1, \dots, x_n$ and can calculate a weighted sum $w_1 x_1 + \dots + w_n x_n$. What then should it do if you pass it a couple of lists $(x_1^1, \dots, x_1^{m_1}), \dots, (x_n^1, \dots, x_n^{m_n})$? Edit 1: Let me be a bit clearer about this point. Every component of a neural network is built on the assumption that the input tensors contain real values. Mathematically: The input must be of the form $X \in \mathbb{R}^{n_1 \times \dots \times n_k}.$ What you would like instead are elements of the set containing all real vectors of arbitrary length, $\mathcal{R} := \{x \in \mathbb{R}^m \mid m \in \mathbb{N}\}$: You want to input $X \in \mathcal{R}^{n_1 \times \dots \times n_k}.$ This would require you to rethink and redefine absolutely everything: Activations, loss functions, forward propagation, back propagation... End of edit 1 . Second, it is unnecessary. Say your input consists of a list $(x_1^1, \dots, x_1^m)$ and two scalar inputs $x_2$ and $x_3$. Then you should simply pass Keras the $m+2$ scalar inputs $x_1^1, \dots, x_1^m, x_2, x_3$. See JahKnows' answer for a nice implementation. Things get a bit more tricky if your input lists have different lengths ... so your first observation contains a list with five items, your second observation a list with seven items, etc. In that case you need to use padding and masking . Edit 2: You say in the comments that you are unhappy with padding because some of your sequences are very long. Let me point out some alternatives: Use batch padding with variable length. This is common for networks that take sentences as input. The test set is ordered by sentence length and split into batches. Each batch is padded individually with as few pads as possible. Cut off sequences, either by dropping the oldest entries (for a time series), or the smallest, or the largest, or the most extreme, or the least extreme, or whatever makes sense for your data. Feed your network with derived properties of the sequence, not the sequence itself. The more I think about it, the more this seems like the best approach for your problem. A distribution is fully described by its moments . By feeding enough moments (and the sequence length) into your network, you retain almost all information about the it. Notice also that moments remain unchanged for permutations of a sequence. Since you say that your sequences are not time series, this might be a very important property (and one that padding does not have!)
