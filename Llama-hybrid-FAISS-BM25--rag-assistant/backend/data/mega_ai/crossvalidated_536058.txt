[site]: crossvalidated
[post_id]: 536058
[parent_id]: 
[tags]: 
Misconception about ReLu

I have already gone through the post and this post , but they didn't clear my doubt. Let us say if I have a deep neural network like (having more layers about 50): Now, my question is: If I'm using an activation function as the ReLU , the gradient will be 1 for all values of x>0 and 0 for for all values of x vanishing gradient problem .
