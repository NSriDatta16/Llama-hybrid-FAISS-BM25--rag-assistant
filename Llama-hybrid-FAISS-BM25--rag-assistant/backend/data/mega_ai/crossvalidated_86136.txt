[site]: crossvalidated
[post_id]: 86136
[parent_id]: 86024
[tags]: 
From what I understand, your question basically boils down to one about model selection and cross-validation. When it comes to getting a prediction of test error, you can't hand-pick the best training/test sets in order to minimize your error. Before starting any sort of parameter tuning or model selection, you must separate your labeled data into a training and test set. Usually we see something such as 80/20 or 90/10 (sometimes even 50/50 for huge datasets) for test/train respectively. In your case, you want to make as much use of your limited data as possible so I suggest reading up on cross-validation. You would want to use cross-validation to select the best model (and tune the parameters within the Naive Bayes, if that's what you are set on using) by calculating the error within each fold. Once you 'know' what model will work best on your data using your test/training splits, you would train your final production model on the full data. P.S. - I would suggest looking into a Random Forest model for this sort of data just because in my experience it tends to work better most of the time.
