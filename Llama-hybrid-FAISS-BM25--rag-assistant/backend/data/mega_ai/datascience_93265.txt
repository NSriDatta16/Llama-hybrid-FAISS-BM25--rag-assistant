[site]: datascience
[post_id]: 93265
[parent_id]: 93213
[tags]: 
In the context of that paper, pre-train then fine-tune on the same dataset does not really make sense, as the pre-training is unsupervised, and the fine-tuning is with labelled data. But, generally, if you have trained on dataset X for N epochs, and then you fine-tune one more epoch using the whole of X , it is just another way of saying you trained for N+1 epochs. Nothing wrong with it, except as you note in the question, if you were starting to overfit after N epochs, you are even more over-fitted now. What does make sense, and something we have used in production models, is to do the initial training on a large dataset that is the combination of X1, X2, X3, ... and so on. Then once the learning curve starts to level out, we take a copy of the model and then fine-tune on e.g. just X1. This is in the context of NLP Transformer models, so may not make sense for other domains, but we try to do initial training on as much data, from all domains, as possible, then we might fine-tune on just medical papers, or just economic reports, depending on what the model will be used for. (We've also done a final fine-tune on just a subset of the data to have it learn a specific style. It is amazing that even with just one epoch of this final fine-tune it can switch to using a different way of writing numbers, for instance.)
