[site]: crossvalidated
[post_id]: 67129
[parent_id]: 67128
[tags]: 
What about Kullbackâ€“Leibler divergence? This is a good measure (in terms of usability in the classification based machine learning) of differences between two probability distributions. This is an asymmetric measure, but as you are comparing many compositions with one golden standard it seems quite a good choice. $$D_{KL}(P||Q) = \sum_i \ln \left ( \frac{P(i)}{Q(i)} \right ) P(i) $$ It is the expectation of the logarithmic difference between the probabilities P and Q, where the expectation is taken using the probabilities P. It can be also treated as a difference in the entropy $$D_{KL}(P||Q) = H(P,Q) - H(P)$$ where $H(P,Q)$ is a cross entropy and $H(P)$ is entropy.
