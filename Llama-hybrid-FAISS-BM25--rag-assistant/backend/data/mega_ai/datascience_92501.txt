[site]: datascience
[post_id]: 92501
[parent_id]: 
[tags]: 
Why don't we set initial hidden states in RNN to random small numbers like we do to the weights?

I'm following a couple of tutorials on RNNs and the instructor said that we should always set the initial hidden state in our RNNs to a tensor of all zeros and I couldn't really understand why. Even when I looked online, it said that fixing the hidden state to zero is a common choice and that this has been shown to force the RNN to focus on the input sequence regardless of the initial choice of the hidden state . What do they mean by that exactly?
