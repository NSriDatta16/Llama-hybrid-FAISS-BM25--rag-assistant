[site]: crossvalidated
[post_id]: 464495
[parent_id]: 
[tags]: 
High cross validation score but low model performance on test set

I'm doing a machine learning project and need to predict a user's credit default probability. I tried some simple automated feature engineering and got a good AUC score on training set using stratified cross validation. However, my model performed poorly on unseen test set with AUC 0.66. See below: Here are some details about my data and model: In my training data, each row represents a user along with over 500 features I generated. My data set is imbalanced and that's why I used stratified cross validation There is no time series so there shouldn't be any trends in the data. I don't think there is any problem with the unseen test set as I tried manual feature engineering and another model which got a score of 0.71 I used LGBM Classifier. Here is my model: model=lgb.LGBMClassifier(n_estimators=1000,objective='binary',class_weight='balanced',learning_rate=0.05,reg_lambda=0.1,subsample=0.8) I tried to reduce the number of estimators but that only reduced 'train_auc' and auc on unseen test set became even worse. It would be much appreciated if someone could suggest any reason or remedy to this problem. Thanks in advance.
