[site]: crossvalidated
[post_id]: 204372
[parent_id]: 
[tags]: 
Learning Neural Networks with self similarity

Consider neural network $$f_\theta:\mathbb{R}^n \rightarrow \mathbb{R}^n$$ specified by some parameters $\theta$. We then build a larger neural network again only depending on $\theta$, $$g_\theta:\mathbb{R}^{n^2} \rightarrow \mathbb{R}^{n^2}$$ where the network $g$ is made up of blocks of $f$. Example (a 2-layer $g$) Define the projection that splits up $\mathbb{R}^{n^2}$ into $\mathbb{R}^{n}$ blocks $$P_i : \mathbb{R}^{n^2} \rightarrow \mathbb{R}^n$$ as $$P_i({\bf x}) = [x_{n.(i-1)+1}, \dots, x_{n.i}]$$ where ${\bf x} = [x_1, \dots, x_{n^2}]$ i.e. $P_1( {\bf x}) = [x_1, \dots,x_n]$. Now we can define a hidden layer $$ h_\theta : \mathbb{R}^{n^2} \rightarrow \mathbb{R}^{n^2} $$ as $$h_\theta({\bf x}) = M (\oplus_{i=1}^n f_\theta(P_i({\bf x})))$$ where $M: \mathbb{R}^{n^2} \rightarrow \mathbb{R}^{n^2}$ is some known permutation. This finally gives $$g_\theta({\bf x}) = \oplus_{i=1}^n f_\theta(P_i(h_\theta({\bf x})))$$ Question Is there an efficient way to learn networks of the form of $g$ i.e. made up of smaller networks $f$? (references to papers would be great)
