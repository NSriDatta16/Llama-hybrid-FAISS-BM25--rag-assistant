[site]: crossvalidated
[post_id]: 83620
[parent_id]: 83554
[tags]: 
Given the skew in the data with x, the obvious first thing to do is use a logisitic regression ( wiki link ). So I am with whuber on this. I will say that $x$ on its own will show strong significance but not explain away most of the deviance (the equivalent of total sum of squares in an OLS). So one might suggest that there is another covariate apart from $x$ that aids explanatory power (e.g. the people doing the classification or the method used), Your $y$ data is already [0,1] though: do you know if they represent probabilities or occurrence ratios ? If so, you should try a logistic regression using your non-transformed $y$ (before they are ratios/probabilities). Peter Flom's observation only makes sense if your y is not a probability. Check plot(density(y));rug(y) at different buckets of $x$ and see if you see a changing Beta distribution or simply run betareg . Note that the beta distribution is also an exponential family distribution and thus it should be possible to model it with glm in R. To give you an idea of what I meant by logistic regression: # the 'real' relationship where y is interpreted as the probability of success y = runif(400) x = -2*(log(y/(1-y)) - 2) + rnorm(400,sd=2) glm.logit=glm(y~x,family=binomial); summary(glm.logit) plot(y ~ x); require(faraway); grid() points(x,ilogit(coef(glm.logit) %*% rbind(1.0,x)),col="red") tt=runif(400) # an example of your untransformed regression newy = ifelse(tt EDIT: after reading the comments: Given that "The y values are probabilities of being of a certain class, obtained from averaging classifications done manually by people," I strongly recommend doing a logistic regression on your base data. Here is an example: Assume you are looking at the probability of someone agreeing to a proposal ($y=1$ agree, $y=0$ disagree) given an incentive $x$ between 0 and 10 (could be log transformed, e.g. remuneration). There are two people proposing the offer to candidates ("Jill and Jack"). The real model is that candidates have a base acceptance rate and that increases as the incentive increases. But it also depends on who is proposing the offer (in this case we say Jill has a better chance than Jack). Assume that combined they ask 1000 candidates and collect their accept (1) or reject (0) data. require(faraway) people = c("Jill","Jack") proposer = sample(people,1000,replace=T) incentive = runif(1000, min = 0, max =10) noise = rnorm(1000,sd=2) # base probability of agreeing is about 12% (ilogit(-2)) agrees = ilogit(-2 + 1*incentive + ifelse(proposer == "Jill", 0 , -0.75) + noise) tt = runif(1000) observedAgrees = ifelse(tt From the summary you can see that the model fits quite well. The deviance is $\chi^2_{n-3}$ (std of $\chi^2$ is $\sqrt{2.df}$). Which fits and it beats a model with a fixed probability (difference in deviances is several hundred with $\chi^2_{2}$). It is a bit harder to draw given that there are two covariates here but you get the idea. xs = coef(glm.logit) %*% rbind(1,incentive,as.factor(proposer)) ys = as.vector(unlist(ilogit(xs))) plot(ys~ incentive, type="n"); require(faraway); grid() points(incentive[proposer == "Jill"],ys[proposer == "Jill"],col="red") points(incentive[proposer == "Jack"],ys[proposer == "Jack"],col="blue") As you can see Jill has an easier time getting a good hit rate than Jack but that goes away as the incentive goes up. You should basically apply this type of model to your original data. If your output is binary, keep the 1/0 if it is multinomial you need a multinomial logistic regression. If you think the extra source of variance is not the data collector, add another factor (or continuous variable) whatever you think makes sense for your data. The data comes first, second and third, only then does the model come into play.
