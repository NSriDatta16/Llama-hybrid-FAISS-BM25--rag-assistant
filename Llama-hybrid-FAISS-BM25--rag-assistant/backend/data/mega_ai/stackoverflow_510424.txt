[site]: stackoverflow
[post_id]: 510424
[parent_id]: 510412
[tags]: 
I always encountered entropy in the sense of Shannon Entropy. From http://en.wikipedia.org/wiki/Information_entropy : In information theory, entropy is a measure of the uncertainty associated with a random variable. The term by itself in this context usually refers to the Shannon entropy, which quantifies, in the sense of an expected value, the information contained in a message, usually in units such as bits. Equivalently, the Shannon entropy is a measure of the average information content one is missing when one does not know the value of the random variable.
