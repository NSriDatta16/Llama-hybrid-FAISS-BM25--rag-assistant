[site]: crossvalidated
[post_id]: 474440
[parent_id]: 
[tags]: 
Why do transformers use layer norm instead of batch norm?

Both batch norm and layer norm are common normalization techniques for neural network training. I am wondering why transformers primarily use layer norm.
