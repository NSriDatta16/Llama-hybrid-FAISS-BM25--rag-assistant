[site]: crossvalidated
[post_id]: 320585
[parent_id]: 318947
[tags]: 
I am looking for feedback on the following potential solution to the second question. The idea is to use a combination of (1) the EM algorithm and (2) standard model selection techniques (e.g., BIC). Specifically, for each natural number $k$, let $M_k$ be the set of distributions of the form $\sum_{1 \leq j \leq k} \beta_j \cdot N(j \cdot s, \sigma^2)$ where $s \in \mathbb{R}^{>0}$, i.e., $M_k$ contains sums of exactly $k$ many normal distributions. At each sample size $m$, use the EM algorithm ( http://www.cmlab.csie.ntu.edu.tw/~cyy/learning/papers/EM_TomasiEM.pdf ) to find an approximation to the MLE $\hat{\theta}_{m, k}$ over $M_k$. Here, $\hat{\theta}_{m, k}$ outputs a $k+1$-dimensional vector, namely, the values of $\beta_1 \ldots \beta_k$ and $s$, which are estimates of $\alpha_1 \ldots \alpha_n$ and $r_1$ under the assumption that $k=n$. Now use a model selection technique (e.g., Bayesian Information criterion) to select the best model $M_k$.
