[site]: datascience
[post_id]: 120379
[parent_id]: 120374
[tags]: 
Any language model can generate text with different approaches: Greedy decoding : you get the highest probability token at each time step. Sampling : the generated token is sampled from the probability distribution at each time step. Temperature sampling : the generated token is sampled from the probability distribution after applying a temperature factor $\alpha$ , which can either flatten the distribution or sharpen it. Beam search : you keep the highest k most probable subsequences (i.e. the "beam"); when you finish decoding them, you output the most probable one. Top-k sampling : you sample from the probability distribution, but only considering the top k most probable tokens. Nucleus sampling : you sample from the probability distribution, but only consider the top probability tokens that add up to a specific cumulative probability p . The OpenAI's API allows selecting the following approaches for both the completions and the chat endpoints: Temperature sampling, with the temperature parameter. Nucleus sampling, with the top_p parameter. You can specify both of them, but OpenAI suggests you only use one of them at the same time. If you want to know the specific detail of the implementation of these approaches, you can check this post from the Huggingface blog.
