[site]: crossvalidated
[post_id]: 620783
[parent_id]: 
[tags]: 
Question regarding the assumption for Gaussian process regression error and Bayesian optimization

I am wondering in the research of Bayesian optimization and Gaussian process regression how the function error could be rigorously quantified. I am aware that in general either of the following assumptions would be made to quantify the regression error: (1) the true function is a sample from the GP prior (2) the true function has a bounded RKHS norm. Regarding assumption (1), I cannot understand why in [Lemma 5.1, 1] $f$ complies with the posterior GP $f(x) \sim N\left(\mu_{t-1}(\boldsymbol{x}), \sigma_{t-1}^2(\boldsymbol{x})\right)$ after having the assumption of $f$ being a sample path of the GP prior. I am aware that in [Section 7.3, 2], the target function is assumed to be a sample path of GP prior, and in Eq. (7.23) only the GP prior impacts the statistical analysis of the regression performance. Should the error bound in [1] only be dependent on the GP prior as it is not assumed that $f$ complies with the posterior GP? Or does the assumption of being a sample path of the GP prior implicitly imply $f$ complies with the posterior GP? Any hints would be really appreciated. Thanks! [1] Srinivas, Niranjan, et al. "Information-theoretic regret bounds for gaussian process optimization in the bandit setting." IEEE transactions on information theory 58.5 (2012): 3250-3265. [2] Williams, Christopher KI, and Carl Edward Rasmussen. Gaussian processes for machine learning. Vol. 2. No. 3. Cambridge, MA: MIT press, 2006.
