[site]: crossvalidated
[post_id]: 555285
[parent_id]: 555260
[tags]: 
Welcome to the Bayesian world! I think your confusion comes from not separating the likelihood and priors. When you develop a Bayesian model, it is always a good practice to develop it line by line. By this, I mean like: \begin{equation} \begin{split} y_{i} &\sim N(\mu, \sigma),\\ \mu &\sim U(1, 15),\\ \sigma &\sim Exp(1), \end{split} \end{equation} where the first line is likelihood, i.e., the distribution of the data you have, while the second and third lines are priors, i.e., the prior distribution of parameters, $\mu$ and $\sigma$ here. The above model means: "I think data come from the normal distribution. I don't know what the mean of this distribution is, but from my intuition, it should be any value between 1 and 15 with all values being equally probable. I don't know what the standard deviation of the normal distribution is either, but from my intuition, it should be any positive value but different probability following the exponential distribution with the rate = 1." Generally, priors express your uncertainty of the true parameter value (if such a thing exists). There is no hard-coded rule on how to specify a prior, but there are several versions such as noninformative priors, weakly informative priors, and informative priors, and the specification all depends on what you know about the parameters of interest beforehand. Sometimes, you can reason why certain values are impossible (e.g., the waiting hours cannot be a negative value, right?). Then, the data updates your priors to posteriors, so you obtain the posterior distribution of $\mu$ and $\sigma$ , $p(\mu | D)$ and $p(\sigma | D)$ . Then, if you want to calculate "the posterior distribution for the time of ship," you draw $y_{i}$ from $N(\mu, \sigma)$ , where $\mu$ is drawn from $p(\mu|D)$ and $\sigma$ from $p(\sigma|D)$ (the two posteriors could be correlated, in which case, you would draw values from the joint distribution). The results are the "posterior predictive distribution": $p(\hat{y}|\mu,\sigma)$ . It is usually wider than the distribution of predicted values in the Frequentist world, because there $\mu$ and $\sigma$ are fixed values.
