[site]: crossvalidated
[post_id]: 604931
[parent_id]: 604884
[tags]: 
Non-normal parametric tests occur often in practice and are widely used; e.g. generalized linear models, parametric survival models. If you need an example to give someone, I suggest (a) pointing to the definition of parametric statistics given in Wikipedia (to disabuse them of the terminological error) and then mentioning logistic regression as an explicit example of a model where parametric tests are used that do not make an assumption about normality of the data, which are 0/1 (again, logistic regression has a Wikipedia page). Both Wikipedia pages include references, if some are needed instead. Hypothesis tests for many given distributional assumptions may be constructed essentially at will , as the situation might require. You simply start with some specific distributional assumption, choose a suitable test statistic, and work out its distribution under $H_0$ , in order to compute critical regions. (Even if you can't do it algebraically, simulation can be used.) Let's start with an extremely simple example: Imagine our parametric assumption is that the waiting time to the next bus is exponential , with some unknown mean $\mu$ . My friend Casey has claimed that the average waiting time for a bus during morning peak hour at a certain bus stop is no more than 6 minutes. I doubt this claim. In this instance we have a one-tailed test. We will be measuring the wait time in seconds. The hypotheses are: $H_0: \mu \leq 360s$ $H_1: \mu > 360s$ If the hypothesis is not rejected I will agree that there's not sufficient evidence to dispute my friend's claim. We decide to collect data in the following fashion: each week for 10 weeks, choose a random weekday and then we choose an instant at random (uniformly distributed) during an agreed peak hour (the 'arrival time' of a theoretical traveller at the bus stop) and measure the time from that instant until the next bus passes, agreeing that this corresponds to the intent of the claim for our purposes. We agree that it is reasonable to treat this as if it were a random sample from the process of interest and (from experience) that the distribution is consistent enough across days that we'll treat it as homogeneous. (We appear not to have other demands on our time.) What statistic to use? Let us choose the sample mean. [It turns out that this is an excellent choice but let us not get distracted with considerations of most powerful tests just yet and content ourselves for the moment with statistics that work sufficiently well for our purposes.] In this case we can compute the null distribution (the distribution of the test statistic when $H_0$ is true) exactly; the sample mean of an i.i.d. sample of size $n$ drawn from an exponential distribution with mean $\mu_0$ is has a gamma distribution with mean $\mu_0$ and shape parameter $n$ (shape-mean parameterization). In shape-scale parameterization, this has shape $\alpha=n$ and scale $\beta=\mu_0/n$ . For our specific example, $\alpha=10$ and $\beta=36$ (note that $\beta$ is in units of seconds). In the shape-rate parameterization $\alpha=10$ and $\lambda=\frac{1}{36}$ ; we can use functions for whatever gamma-distribution parameterization we have available. Given the small chosen sample size, let's assume we have agreed to test at $\alpha=0.10$ . The $0.9$ quantile of a gamma with shape $10$ and scale $36$ is $511.4...$ (in R: qgamma(0.9,shape=10,scale=36) ). We agree to the rejection rule "Reject $H_0$ if the average waiting time across the sample of size 10 equals or exceeds 511 seconds." That's a parametric hypothesis test (in the Neyman-Pearson framework) with an exponential distributional assumption. If desired, a p-value could be obtained from that gamma distribution. e.g. imagine the average time in the sample was 498 seconds. The tail area beyond 498 seconds would be 0.1175, so our p-value is about 11.75% ( pgamma(498,shape=10,scale=36,lower.tail=FALSE) if we're working in R). This test could have been done in anything that allows us to compute gamma quantiles (which does not limit us to just stats packages), or indeed it could be done by hand via ordinary chi-squared tables. I'm skipping a number of important considerations when constructing tests in the hope of giving the general concepts and some signposts; e.g. I'm almost completely ignoring dealing with nuisance parameters here. Consult books on mathematical statistics for a more complete coverage. Simulating the null distribution of a test statistic, and simulation to obtain p-values In the above example, it was possible to obtain an explicit null distribution of the test statistic algebraically. This is not always possible, and sometimes possible but nevertheless inconvenient. We could obtain a critical value by simulating many samples where $H_0$ is true and finding the relevant quantiles to mark the boundary of a critical region. We can similarly obtain p-values by finding where the sample statistic occurs in a large simulation from the null distribution of the test statistic. Given this is extremely rapid to carry out, even with a modest laptop, I tend to use $10^5$ or $10^6$ (or indeed, sometimes more) samples for such purposes. I don't really need a critical value in a hundredth of a second -- what harm is there in waiting a few seconds? Or even a half a minute? It's also simple to estimate a standard error on such simulated p-values, and to bound it (using calculations used in margin-of-error calculations). If my simulated p-value is 0.038 and the standard error is 0.002, I have no qualms about rejecting at the 5% level or not-rejecting at the 2.5% level. A brief word on 'optimal' tests Clearly we have a fair bit of freedom in our choice of statistic. Given that high power is usually regarded as a nice property to have, an obvious criterion would be to choose a test with the 'best' power, were one available. This leads to the concept of uniformly most powerful tests. I won't labor the point by going into details, but this line of thought leads to useful ways to choose your test statistics via tests based off the likelihood ratio, including explicit likelihood ratio tests themselves. There's also a convenient result related to the asymptotic null distribution of the likelihood ratio, meaning that in sufficiently large samples we don't necessarily have to worry about obtaining the distribution of the test statistic, as the asymptotic distribution will eventually be quite accurate There's also the related score tests and Wald tests . For example, I have (including in questions here on site) fitted Weibull models to data on wind speed or rainfall using parametric survival regression models (the program doesn't know that the wind-speeds aren't survival times, nor does it object if none of the values are censored), and it's possible to test a variety of hypotheses in these frameworks with little effort. Powerful, small-sample, nonparametric 'exact' tests based on the same ideas: In simple cases, we can use some parametric statistic as the basis for a permutation test. This is sometimes convenient in several ways; generally offering very good power when the distributional assumption is true, but avoiding the risk of a potentially higher significance level if it's not. Indeed, the likelihood ratio itself might be used quite directly, if desired. [In more complicated cases there may often be tests that are approximately (/asymptotically) distribution free / nonparametric tests that attain the desired significance level in large samples, whether via the bootstrap or via a permutation test based on an asymptotically exchangeable quantity - like a residual in multiple regression, given some assumptions. These won't be small-sample exact, but their properties can be investigated, such as via simulation, so you're not left entirely in the dark as to how close you might be getting to a desired $\alpha$ in specific situations.]
