[site]: crossvalidated
[post_id]: 320734
[parent_id]: 
[tags]: 
How can't the Softmax layer never converge using hard targets

Here's a quote from the deep learning book by Ian Goodfellow ( page 236 ): Maximum likelihood learning with a softmax classifier and hard targets may actually never converge -- the softmax can never predict a probability of exactly 0 or 1... I have built many DNN models and I used Softmax layer as a classification layer but I actually never noticed that this is possible. Actually, I think it does make sense, but why don't we face that in practice? Do frameworks "terminate" the gradient descent algorithm earlier and handle this issue internally? I studied many books and articles about DNN but this is the first time that I read something about that. Or, is that valid only in some contexts?
