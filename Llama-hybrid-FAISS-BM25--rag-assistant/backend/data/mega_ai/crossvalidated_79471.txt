[site]: crossvalidated
[post_id]: 79471
[parent_id]: 
[tags]: 
Inferring dimension weight in a mapping from a triangle to a distribution over its vertices

I have a dataset $(y_i, \mathbf{X}_i)$, where $\mathbf{X}_i$ is a $3 \times n$ matrix of reals and $y_i$ takes a value in $\{1, 2, 3\}$. Essentially, $y_i$ represents a "selection" of the row vector $\mathbf{x}_{iy_i}$. (One way to think about this is as selecting a vertex of a triangle in $n$ dimensions.) I would like to find a general method for learning $f: \mathbb{N}^{m \times n} \rightarrow \{1, \dots, m\}$ (where in my case $m = 3$) that also allows me to calculate feature importance (dimensions weight) for each of the $n$ features represented in the columns of $\mathbf{X}_i$. This last constraint precludes me from, for instance, calculating a Minkowski $p$-distance between each vertex and using those distances as features. My initial stab at this problem was to map $\mathbf{X}_i$ to a matrix of naturals with some precision $p$, $\mathbf{X}^{(p)}_i$, s.t. $x^{(p)}_{ijk} = \lfloor(10^p)x_{ijk}\rfloor$, and then to a unique $n$-dimensional vector $\mathbf{z}_i$, e.g. by applying the Cantor pairing function down the columns of $\mathbf{X}^{(p)}_i$ such that $z_{ij} = \langle\langle x^{(p)}_{ij1}, x^{(p)}_{ij2}\rangle, x^{(p)}_{ij3}\rangle$. I then used an off-the-shelf supervised method, e.g. a random forest, to learn $g : \mathbb{N}^n \rightarrow \{1, 2, 3\}$. The essential idea here is to map each $\mathbf{X}^{(p)}_i \in \mathbb{N}^{3 \times n}$ to a unique point in $n$ dimensions, then map from those points to (probabilities distributions over) categories. EDIT: Another possible solution is to assume that there exists a fourth ($[m+1]^{th}$ for the general case) $n$ dimenional point $\mathbf{z}$ and that the parameter(s) $\phi$ of the distribution over the triangle's vertices are drawn from a distribution whose parameters are some function $d$ of the distance between each of the vertices $\mathbf{x}_{ij}$ and this fourth ($[m+1]^{th}$) point. The feature importances $\mathbf{w}$ can be incorporated into the distance parameter as weights applied to each dimension: for instance, if $d$ is euclidean distance $d(\mathbf{x}_{ij}, \mathbf{z}) = \sqrt{\sum_{k=1}^n(x_{ijk} - z_k)^2}$ then the $\mathbf{w}$-weighted distance is $d_{\mathbf{w}}(\mathbf{x}_{ij}, \mathbf{z}) = \sqrt{\sum_{k=1}^n w_k(x_{ijk} - z_k)^2}$ For concreteness, let $\phi_{k(i)} \sim \mathrm{Dirichlet}\left(d_{\mathbf{w}}(\mathbf{x}_{i1}, \mathbf{z}), \ldots, d_{\mathbf{w}}(\mathbf{x}_{im}, \mathbf{z})\right)$. Since each unique triangle triangle has some number of vertex selections associated with it, we draw a single $\phi_{k(i)}$ for all instances of triangle $k(i)$ (of which $\mathbf{X}_i$ is one). Let the vertex selections $\mathbf{y}_i \sim \mathrm{Categorical}(\phi_{k(i)})$. We place an exponential prior on each $w_l \sim \mathrm{Exponential}(\alpha)$ and a multivariate gaussian prior on $\mathbf{z}_{k(i)} \sim \mathcal{N}(\frac{1}{m}\sum_{j=1}^m \mathbf{x}_{ij}, \mathrm{Cov}(\mathbf{X}_i))$. We are interested in the posterior over the weights $p(\mathbf{w}\;|\;\mathbf{y}, \mathbf{X}; \alpha) \propto p(\mathbf{w};\alpha) \int \mathrm{d}\mathbf{z}\; p(\mathbf{z}\;|\;\mathbf{X}) \int \mathrm{d}\phi\; p(\mathbf{y}\;|\;\phi)p(\phi\;|\;\mathbf{z})$. Due to the Dirichlet-Multinomial conjugacy, the deepest integral can be analytically solved, but I suspect the integral over $\mathbf{z}$ will require brute force. Further clarity---and consequently, less bookkeeping---can be achieved by specifying this model in terms of a multinomial likelihood instead of a categorical likelihood (since we can remove the need for the indexing function $k()$).
