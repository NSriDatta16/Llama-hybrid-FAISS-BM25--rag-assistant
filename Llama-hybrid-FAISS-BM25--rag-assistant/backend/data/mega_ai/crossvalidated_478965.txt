[site]: crossvalidated
[post_id]: 478965
[parent_id]: 478963
[tags]: 
A one-classification SVM takes only the "normal" class of data and trains on that. It learns the boundaries of that class. Then, when fed new data, it attempts to classify the new data points as either "in" or "out" of the normal class. Because of this, the normal 80/20 split rule becomes a bit more complicated. You obviously do not train on any of the abnormal data by definition of a one-classification model. You also should withhold some normal data in order to make sure that it is properly classifying the normal data. I can't find a specific example of the appropriate split percentage, but testing on your train data is definitely not appropriate. Personally, I would withhold 20% of the normal data and all of the abnormal data to test the model.
