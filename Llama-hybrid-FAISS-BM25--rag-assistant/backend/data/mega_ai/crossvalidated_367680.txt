[site]: crossvalidated
[post_id]: 367680
[parent_id]: 365590
[tags]: 
I believe both these assumptions are not always necessary and often broken. Batch normalization normalizes values in hidden layers / feature maps across all inputs in a mini-batch when doing mini-batch SGD. This means the cost function can no longer be split up into individual costs for each input. Batch norm is a pretty ubiquitous component of modern neural networks. The second claim depends on what you would consider an "output" of the network. Weight decay / regularization adds on to the cost function the L2 norm of the weights, which I think can hardly be called an output. The only real requirement for backpropagation is that you can define gradients on all the computation steps between weights you want to backprop into, and the cost function you want to backprop from. These gradients don't necessarily need to be mathematically well defined, or even correct and unbiased (see straight-through gradient estimators).
