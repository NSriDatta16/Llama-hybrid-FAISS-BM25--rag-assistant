[site]: crossvalidated
[post_id]: 470608
[parent_id]: 99162
[tags]: 
I will assume you understand how a standard SVM works. To summarise, it separates two classes using a hyperplane with the largest possible margin. One-Class SVM is similar, but instead of using a hyperplane to separate two classes of instances, it uses a hypersphere to encompass all of the instances. Now think of the "margin" as referring to the outside of the hypersphere -- so by "the largest possible margin", we mean "the smallest possible hypersphere". That's about it. Note the following facts, true of SVM, still apply to One-Class SVM: If we insist that there are no margin violations, by seeking the smallest hypersphere, the margin will end up touching a small number of instances. These are the "support vectors", and they fully determine the model. As long as they are within the hypersphere, all of the other instances can be changed without affecting the model. We can allow for some margin violations if we don't want the model to be too sensitive to noise. We can do this in the original space, or in an enlarged feature space (implicitly, using the kernel trick), which can result in a boundary with a complex shape in the original space. Note: this is my account of the model as described here . I believe this is the version of One-Class SVM proposed by Tax and Duin. There are other approaches, such as that of Sch√∂lkopf et al, which is similar, but instead of using a small hypersphere, it uses a hyperplane which is far from the origin; this is the version implemented by LIBSVM and thus scikit-learn.
