[site]: crossvalidated
[post_id]: 167807
[parent_id]: 
[tags]: 
Backpropagation with Cross-entropy Cost Function

I'm using the cross-entropy cost function for backpropagation in a neutral network as it is discussed in neuralnetworksanddeeplearning.com . I got help on the cost function here: Cross-entropy cost function in neural network I'm confused on: $\frac{\partial C}{\partial w_j}= \frac1n \sum x_j(\sigma(z)âˆ’y)$ I'm not sure what $w_j$, $x_j$, $\sigma(z)$ are. Is $w_j$ the matrix for the final layer, $x_j$ is input vector, $\sigma(z)$ is output vector? That doesn't really make sense, but I'm not sure what it is.
