[site]: crossvalidated
[post_id]: 589871
[parent_id]: 
[tags]: 
Why do we scale features in PCA? Wouldn't that mean the variance in all dimensions is just $1$?

According to https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html , Feature scaling through standardization (or Z-score normalization) can be an important preprocessing step for many machine learning algorithms. Standardization involves rescaling the features such that they have the properties of a standard normal distribution with a mean of zero and a standard deviation of one. However, I thought PCA wanted to capture the principal components with the most variance. If we are rescaling the features so they all have standard deviation and variance 1, then wouldn't there be no point in running PCA - everyone has variance 1? In particular, I'm asking not why we need or want standardization, but why standardization doesn't fail (so it is not a duplicate of the linked question).
