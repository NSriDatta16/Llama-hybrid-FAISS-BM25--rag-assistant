[site]: crossvalidated
[post_id]: 449886
[parent_id]: 449884
[tags]: 
As a term of art, the usage of "stochastic gradient descent" is inconsistent. In some sources, SGD is exclusively the case of using 1 observation randomly-chosen without replacement per epoch to update a model. In other sources, stochastic gradient descent refers to using a randomly-selected sample of observations for updating the model, of any size, including a mini-batch of size 1 as a special case. I do not see any compelling reason to reserve the terminology SGD for the case of 1 observation, so the convention that I use here is that SGD can have mini-batches of any size $1 \le m . The special case of $m = n$ is batch gradient descent, and is not stochastic because it does not have a randomized component depending on random selection of data used for an update. More information: Stochastic gradient descent Vs Mini-batch size 1 Your understanding is not correct. An epoch is a pass through all $n$ data points. (See: Why do neural network researchers care about epochs? ) So if you have $n$ observations and $k$ epochs, then SGD with a mini-batch size of $m=1$ computes $n k$ updates, not $k$ updates. The data points are chosen randomly, without replacement at each of $n$ iterations during the $k$ epochs. SGD mini-batch gradient descent with $1 data points chosen randomly without replacement to update the model. This means that $\lceil \frac{n}{m} \rceil k$ total updates are applied during $k$ epochs.* Computation time is shorter for $m > 1$ because that involves $nk \ge \lceil \frac{n}{m} \rceil k$ updates, so fewer forward passes and backward passes in total. Vectorization and other computational tricks improve the scaling properties of the arithmetic, and therefore the time it takes to complete an epoch, even though all $n$ samples are moving through the model during an epoch in both cases. *In some unusual cases, a researcher might want to leave off the last few observations if the final mini-batch would have size less than $m$ . One example is training a model with online hard negative mining, because online hard negatives are order statistics of the mini-batch, and the distribution of order statistics changes with sample size.
