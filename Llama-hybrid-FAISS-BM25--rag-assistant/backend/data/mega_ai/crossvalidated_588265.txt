[site]: crossvalidated
[post_id]: 588265
[parent_id]: 586031
[tags]: 
I'd leave $R^2$ out since your nonlinear models under comparison may have a different number of parameters. I would, instead, rely on one of these three approaches: (a) use of information criteria such as BIC or AIC to select the best model or (b) goodness of fit (GOF) test (c) paired two-sample test. In (a) you compute the AIC (or BIC) for each model and, then you choose as the 'best model' that with the lowest AIC (or BIC). In (b) essentially you have to choose a statistic that measures either the fit of the model or the difference in fit between two models and compare it's observed value against a distribution. There is huge literature here, see Goodnessof-Fit Techniques , edited by Ralph B. D'Agostmo and Michael A, Stephens. (c) is more heuristic, still kind of GOF, and goes as this. Suppose you want to compare two models $M_1, M_2$ . Use your preferred estimation method and get the fitted values under both models, say $\hat y_{M_1}$ and $\hat y_{M_2}$ . Now, we expect $\hat y_{M_1},\hat y_{M_2}$ to be correlated, so take $$ \hat{d} = \hat y_{M_1}-\hat y_{M_2}, $$ and apply a $t$ -test or $z$ -test to $\hat d$ . If you reject the null then, the two fits have different population averages, i.e. the fits are different. Alternatively to $t$ and $z$ , you may use non-parametric tests for differences in location for two paired samples. If you want to test if the distributions of the fitted values under $M_1$ and $M_2$ are the same, you may try something like the Anderson-Darling test or the Kolmogorov-Smirnov test.
