[site]: crossvalidated
[post_id]: 249724
[parent_id]: 249714
[tags]: 
What heteroskedasticity describes is that the variation of the errors may depend on the values of the regressors. That is, that for certain values of $x$ we expect that, while we still expect zero errors on average, any given error tends to be further away from the true regression line in either direction . The situation you describe rather concerns the situation in which the errors systematically deviate from the regression line in one direction (or in one direction for some range of $x$, and in another for another range of $x$), so that errors would no longer have mean zero for such predictor values, for example due to omitted nonlinearities or omitted variables. Here is an example in which the error term $u$ of the model is generated such that it correlates with the regressor $X$ (see code below). This causes the scatter plot not to scatter around the true (red) regression line, such that, despite the huge sample size of $n=10,000$, the (blue) estimated OLS line is pretty far away from the true value $\beta_1=0.5$. library(mvtnorm) # truth beta0
