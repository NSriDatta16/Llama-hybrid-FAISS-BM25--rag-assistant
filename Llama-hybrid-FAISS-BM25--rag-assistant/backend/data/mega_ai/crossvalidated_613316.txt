[site]: crossvalidated
[post_id]: 613316
[parent_id]: 
[tags]: 
Posterior predictive distribution simulation: why not average over parameter values?

I'm very unfamiliar with Bayesian statistics, so forgive me if this question is obvious. The idea of a posterior predictive distribution seems to be incorporate uncertainty about the parameter $\theta$ (or, I guess, its estimate if you look at it in the classical way) when simulating and/or predicting the distribution of a new observation, and is based on the formula $$ p(y_+ \mid y_1, \dots, y_n) = \int p(y_+ \mid \theta) p(\theta \mid y_1, \dots, y_n) \, d\theta \,. $$ However, the sources I've found, (including answers on this site ) suggest that one should simulate realisations of $Y_+$ by sampling $\theta^{(i)}$ from $\Theta \mid Y_1= y_1, \dots, Y_n = y_n$ and $y_+^{(i)}$ from $Y_+ \mid \Theta = \theta$ . Why is it not necessary to average over $\theta$ here? Intuitively, the correct procedure seems to me rather to be: Draw $\theta_1, \dots, \theta_M$ from $\Theta \mid Y_1= y_1, \dots, Y_n = y_n$ For each $\theta_i$ , draw $y_+^{(1, \theta_i)}, \dots, y_+^{(N, \theta_i)}$ from $Y_+ \mid \Theta = \theta$ $y_+^{(j)} := \frac{1}{M}\sum_{i = 1}^M y_+^{(j, \theta_i)}$ for $i = 1, \dots, N$ is a sample from $Y_+ \mid Y_1 = y_1, \dots, Y_n = y_n$ What am I misunderstanding here?
