[site]: datascience
[post_id]: 11557
[parent_id]: 
[tags]: 
Equation for likelihood in logistic regression

I read the likelihood is defined in logistic regression as the probability $$ L(w) = P(y|x, w) = \prod P(y^i| x^i,w) = \prod (\sigma(z^i))^{y^i}(1-\sigma(z^i))^{(1-y^i)} $$ and the log of the last equation is: $$ log(L(w)) = \sum y^i log((\sigma(z^i)) + (1-y^i)log(1-\sigma(z^i)) $$ I understand (independent probabilities) $$ L(w) = P(y|x, w) = \prod P(y^i| x^i,w) $$ and I understand that from $$ L(w) = \prod (\sigma(z^i))^{y^i}(1-\sigma(z^i))^{(1-y^i)} $$ the log is (basic log properties) $$ log(L(w)) = \sum y^i log((\sigma(z^i)) + (1-y^i)log(1-\sigma(z^i)) $$ However, how do I get $$ \prod P(y^i| x^i,w) = \prod (\sigma(z^i))^{y^i}(1-\sigma(z^i))^{(1-y^i)} $$ This basically means that $$ P(y| x,w) = (\sigma(z))^{y}(1-\sigma(z))^{(1-y)} $$ and I just don't see that.
