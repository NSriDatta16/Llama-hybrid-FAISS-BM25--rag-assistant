[site]: crossvalidated
[post_id]: 105789
[parent_id]: 105782
[tags]: 
You should clearly separate between training error and prediction error . Being a universal approximator, a neural network is in principle able to reproduce any given set of data exactly, i.e. the training error can always be brought to 0%. In particular, it doesn't matter whether your data contains noise or not -- it simply reproduces all given training results when feeding in the corresponding predictors. (However, this is hardly desired as it leads to overfitting and thus usually weak prediction results. To avoid this, methods such as regularization/weight decay, early stopping, Bayesian ANNs, special network topologies, etc. are often used). On the other hand, you have prediction error. If you want to approximate an unknown function (again, let there be noise or not), in general you cannot expect to obtain 0% prediction error. This is because you make a special assumption about the underlying model (in the case of ANNs it is a stacked sum of sigmoids), which in general might behave arbitrarily badly between two training points. In reality, however, the underlying models are often smooth and ANNs will work well, but you can never be sure. (Yet, in special cases, it might work. A synthetic example: if you use an ANN to reproduce another ANN, you can get an exact match of the models). So, the answer to your question is you can have 100% accuracy for the training error, but in general never for the prediction error.
