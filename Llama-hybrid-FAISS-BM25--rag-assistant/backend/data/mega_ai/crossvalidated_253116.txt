[site]: crossvalidated
[post_id]: 253116
[parent_id]: 253106
[tags]: 
Breiman said in his original papers in Machine Learning journal, to not be "stingy" and use 5,000 to 50,000 trees -- but he had a lot of computational power. Recall, that for RF, the in-bag bootstrapped objects are only used for training the trees, not testing. Testing (i.e. error) is determined when the out-of-bag (OOB) objects are dropped down the trained trees, after which error is based on the percentage of class purity in the final nodes when node-splitting can no longer be performed. Importance scores are also based on permuting feature-based columns in the OOB datasets, which gives loss of predictive accuracy as a function of each feature. I never use less than 1,000 trees for any RF runs. Also, the default number of features randomly selected for node-splitting in a tree is typically srqt(total # of features). You probably don't need to use the 6-hour run approach since you are hunting down better accuracy. RF is one of the better classifiers, whose default parameter usage should do fine in terms of bias/variance and robustness across different data sets. Recall, RFs generalize better than other classifiers, so the increased generalization comes at a cost of lower accuracy -- I am always comfortable with 75%-80% accuracy from RF, while other classifiers can be 90-95% accurate. When you launch an ensemble of classifiers into the real world, you can always rest assured that the RF will probably do better than any of the other classifiers. It won't get faked out, and probably won't break down, etc.
