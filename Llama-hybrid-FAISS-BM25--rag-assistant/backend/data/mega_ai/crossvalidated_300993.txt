[site]: crossvalidated
[post_id]: 300993
[parent_id]: 300837
[tags]: 
In the pure optimization sense, this is true. The intuition for why this is true is trivial. Suppose that $x^*$ is a solution to the optimization problem $\max ~~ f(x)~~s.t. ~~ g(x) = b $. If you relax the problem, say, by dropping the condition $g(x) = b$, this means,you have more candidate options, including your original one. In other words, you can always pick $x^*$ again in your relaxed optimization problem. Hence, if you pick a different solution $x' \ne x^*$, by definition this means $f(x')\ge(x^*)$, otherwise you would not have picked $x'$. In this sense, having more options never hurts. However, in a statistical sense, this is not true , because in statistics usually our goal is not to optimize the fit in sample. That is, considering the appropriate metric (out of sample prediction, causal inference, etc), having more options can hurt. I'm bringing this up here because this is a statistics Q&A site, not a math Q&A site. If your question is purely about optimization, then it's better suited here . So, for statistical problems, having more options in your optimization for sure guarantees you will get a better (or the same) fit in sample. But it could hurt your fit out of sample. That is, allowing your search space to be every possible complex model might lead to overfitting. This is the very reason why we do regularization . So we have constrained regression methods like the ridge, or the lasso, and that's also why people regularize their estimates with prior information in Bayesian inference. To sum up: the claim that "more is better" is true when considering optimizing in sample fit. It's not necessarily true for optimizing out of sample fit or when you have other inferential goals.
