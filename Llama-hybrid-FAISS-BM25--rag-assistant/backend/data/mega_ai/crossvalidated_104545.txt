[site]: crossvalidated
[post_id]: 104545
[parent_id]: 104528
[tags]: 
Some cases where "centering the data on its mean" (hereafter just "de-meaning") is useful: 1) Visual detection of whether a distribution is "the same" as another distribution, only, it has been shifted on the real line. Making both distributions having zero-mean, makes this visual inspection much more easy. Sometimes, if the mean value differs by much, viewing them on the same chart is impractical. Think of two normal r.v.'s, say a $N(10,4)$ and a $N(100,4)$. The shapes of the density graphs are identical, only their position on the real line differs. Now imagine that you have the graphs of their density functions, but you don't know their variance. De-meaning them will superimpose the one graph over the other. 2) Simplify calculations of higher moments: although adding a constant to a random variable does not change its variance, or its covariance with another random variable, still, if you have a non-zero mean, and you must write out the detailed calculations, you have to write all the terms and show that they cancel out. If the variables are de-meaned, you save a lot of useless calculations. 3) Random variables centered on their mean are the subject matter of the Central Limit Theorem 4) Deviations from the "average value" are in many cases the issue of interest, and whether they tend to be "above or below average", rather than the actual values of the random variables. "Translating" (visually and/or computationally) deviations below the mean as negative values and deviations above the mean as positive values, makes the message clearer and stronger. For more in-depth discussions, see also When conducting multiple regression, when should you center your predictor variables & when should you standardize them? Centering data in multiple regression If you search "centered data" on CV, you will also find other interesting posts.
