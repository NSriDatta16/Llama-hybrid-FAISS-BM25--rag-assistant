[site]: crossvalidated
[post_id]: 295494
[parent_id]: 295457
[tags]: 
This is obviously a question for a detailed discussion, but here is the simplest answer I can come up with. The reason that neural networks are a good model for complicated tasks like modelling high-level tasks like image understanding is that they are universal approximators ; Put very blantly, you can approximate any function with a network with sufficient capacity. However, this is only true in theory : In practice, the search space of the opimization problem is so huge, that actually getting to a good solution is very difficult (this is essentially the expected "no free lunch" result). Most of the success of deep learning in the last few years have been around making the optimization easier to solve. Generally, there are two approaches to doing this: The first approach is just to "hand craft" optimization procedures that work well for the types of functions we usually want to estimate in DL (typically, high-dimensional and very non-convex). There are numerous examples, from dropout to different variants of gradient descent methods. The second approach is to incorproate some structure into the network's design. Essentially, translate some real-world knowledge you have about the problem you're trying to solve, in order to make the search space (hypothesis class) significantly smaller. I claim that while the first approach is undoubtedly useful and important, it's not enough. Hence, the reason that most of the "success stories" in Deep Learning are in the field of computer vision and speech recognition is that in this field scientists have been able to impose the "correct" type of structure - namely, convolutional architectures . By "correct", I mean that convolutional models are significantly simpler models (much fewer parameters), but we do not lose anything in the sense that we can still express the sort of functions we see in natural images and natural audio. I believe that the right way of using domain knowledge from other fields - economics, biology - to simplify the search space of neural networks is still an open question. Progress on that will definitely lead to some nice results like the ones we have in vision.
