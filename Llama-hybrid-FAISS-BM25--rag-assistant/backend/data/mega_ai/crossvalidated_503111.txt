[site]: crossvalidated
[post_id]: 503111
[parent_id]: 489429
[tags]: 
You are right, it would make a lot more sense to me too if they would use the inverse. There are two reasons why I think they do this: I think the calculation of the inverse is more costly than doing a transposed matrix multiplication. As I remember, the differences might not be in pure algorithmic time complexity, rather GPUs are simply better at doing matrix multiplications. ( https://scicomp.stackexchange.com/questions/5372/for-which-statistical-methods-are-gpus-faster-than-cpus ) Transposition is motivated by arguing that AEs are can be looked at as a dimensionality reduction method like PCA when considering linear layers, activation functions. In that case, transposition would be inverse. By training with weight tying, the desire is probably to learn a set of weights where this property is true.
