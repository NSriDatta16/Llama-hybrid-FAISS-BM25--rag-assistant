[site]: crossvalidated
[post_id]: 82903
[parent_id]: 82664
[tags]: 
Basically, what you're doing by increasing the degrees of your polynomials is increasing the number of parameters or degrees of freedom of your model space, ie. its dimension. The more parameters you add, the more the model can fit the training data easily. But this also depends heavily on the number of observations. Your models $H_1$ and $H_2$ might just as well overfit the training data if the number of observations is low, just as $H_3$ may not overfit at all if the number of training instances is large enough. For example, let's grossly exaggerate and suppose you are given only $2$ training examples, than even $H_1$ will always overfit your data. The advantage of imposing priors for instance through regularisation is that the parameters are either shrunk to zero or some other predefined value (you can even add parameters to "tie" the coefficients together if you like), and thus you are implicitly constraining the parameters and reducing the "freedom" of your model to overfit. For example, using the lasso (ie. $l^1$ regularisation or equivalently a Laplace prior) and tuning the corresponding parameter (using 10x cross validation for example) will automatically get rid of the surplus parameters. The Bayesian interpretation is similar : by imposing priors, you are constraining your parameters to some more probable value, inferred from the overall data.
