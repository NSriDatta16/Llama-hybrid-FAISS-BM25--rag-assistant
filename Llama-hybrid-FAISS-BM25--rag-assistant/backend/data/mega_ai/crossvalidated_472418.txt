[site]: crossvalidated
[post_id]: 472418
[parent_id]: 471811
[tags]: 
Character sequences are much longer than word sequences. This is even more critical with Transformer models that require quadratic memory with respect to the sequence length. Also, word embeddings are quite efficient in storing information about the words. You need memory to store the information, but the embedding lookup is very fast. With a character-level model, the hidden layers of the model need to be able to build up the same information from the characters. This needs computation time and extra parameters in the hidden layers. As a result, character-level models are much slower, both because of the longer input sequence and because they need bigger intermediate layers with more parameters.
