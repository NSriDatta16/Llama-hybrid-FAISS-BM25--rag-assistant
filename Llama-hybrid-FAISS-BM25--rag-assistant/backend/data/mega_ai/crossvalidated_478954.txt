[site]: crossvalidated
[post_id]: 478954
[parent_id]: 
[tags]: 
How to present ML model performance before deployment?

I have machine learning model performance metrics on training set with cross validation and metrics from when I ran the model on the test set. The model performs similar on both sets, so I want to deploy model and make predictions. Which performance metrics are the ones that matter more and that others care about, training set or test set metrics? Which results get displayed more prominently? From research papers, itâ€™s not clear to me which dataset is being used to show, for example AUC with confidence intervals. My question: When most people talk about how their model performs, are they usually talking about training set performance or test set performance?
