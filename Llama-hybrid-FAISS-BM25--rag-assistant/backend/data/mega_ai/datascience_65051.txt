[site]: datascience
[post_id]: 65051
[parent_id]: 65023
[tags]: 
It is possible to use GANs to generate text (see this question ), but the results are worse than normal LMs in terms of quality and diversity. If you trained a textual GAN, it should be possible to reuse the trained Discriminator for transfer learning into a classification task, either directly reusing the features and projecting them into the new logit space or finetuning the whole model. However, I would not expect good results, as the features learned by the Discriminator are specifically aiming at uncovering the data created by the Discriminator instead of just the real data. If you have lots of unlabeled textual data you want to profit from in your classification problem, a better option would be to train a masked language model on the data, or even better, use BERT . If your domain is close to wikipedia data (which was used to train BERT), then you could use it as-is and simply fine-tune it on your classification task. If your domain is very different, then you can fine-tuning BERT on both the masked LM task with your textual data and your classification problem at the same time.
