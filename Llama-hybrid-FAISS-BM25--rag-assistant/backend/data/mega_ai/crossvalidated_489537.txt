[site]: crossvalidated
[post_id]: 489537
[parent_id]: 489459
[tags]: 
In short, $k$ -means can be viewed as the limiting case of Expectation-Maximization for spherical Gaussian Mixture Models as the trace of the covariance matrices goes to zero. What follows is a presentation of portions of sections 9.1 and 9.3 of Pattern Recognition and Machine Learning . $K$ -means $K$ -means seeks to find a binary assignment matrix $[r_{j,i}]$ , with exactly one non-zero value in each row, one row for each of $N$ observations, and one column for each of $K$ clusters. The algorithm itself amounts to picking initial mean vectors $\mu_i$ , and then alternating between the following two steps: E-step : For each observation $j$ , set $r_{j,k^*}=1$ and $r_{j, k} = 0$ for $k \neq k^*$ , where $k^*$ is the index of the closest cluster center: \begin{align} k^* = \underset{k}{\text{argmin}}~ ||x_j - \mu_k||^2 \end{align} M-step : For each cluster $j$ , re-estimate the cluster center as the mean of the points in that cluster: \begin{align} \mu_k^{\text{new}} = \frac{\sum_{j=1}^N r_{j,k}x_j}{\sum_{j=1}^N r_{j,k}} \end{align} Expectation-Maximization for Gaussian Mixture Models Next, consider the standard Expectation-Maximization steps for Gaussian Mixture models, after picking initial mean vectors $\mu_k$ , covariances $\Sigma_k$ , and mixing coefficients $\pi_k$ : E-step : For each observation $j$ , evaluate the "responsibility" of each cluster $k$ for that observation: \begin{align} r_{j,k} & = \frac{\pi_k \mathcal{N}(x_j | \mu_k, \sigma_k)}{\sum_{i=1}^K\pi_i \mathcal{N}(x_j | \mu_i, \sigma_i)} \end{align} M-step : For each cluster $k$ , re-estimate the parameters $\mu_k$ , $\Sigma_k$ , $\pi_k$ as a weighted average using the responsibilities as weights: \begin{align} \mu_k^{\text{new}} & = \frac{1}{\sum_{j=1}^N r_{j, k}} \sum_{j=1}^N r_{j,k} x_j \\ \Sigma_k^{\text{new}} & = \frac{1}{\sum_{j=1}^N r_{j, k}} \sum_{j=1}^N r_{j,k}( x_j - \mu_k^{\text{new}})(x_j - \mu_k^{\text{new}})^T \\ \pi_k^{\text{new}} & = \frac{\sum_{j=1}^N r_{j, k}}{N} \end{align} If we compare these update equations to the update equations for $K$ -means, we see that, in both, $r_{j,i}$ serves as a probability distribution over clusters for each observation. The primary difference is that in $K$ -means, the $r_{j,\cdot}$ is a probability distribution that gives zero probability to all but one cluster, while EM for GMMs gives non-zero probability to every cluster. Now consider EM for Gaussians in which we treat the covariance matrix as observed and of the form $\epsilon\textbf{I}$ . Because $\mathcal{N}(x | \mu, \epsilon\textbf{I}) \propto \exp\left(-\frac{1}{2\epsilon}||x - \mu||^2\right)$ , the M-step now computes the responsibilities as: \begin{align} r_{j,k} & = \frac{\pi_k \exp\left(-\frac{1}{2\epsilon}||x_j - \mu_k||^2\right)}{ \sum_{i=1}^K \pi_i \exp\left(-\frac{1}{2\epsilon}||x_j - \mu_i||^2\right) } \end{align} Due to the exponential in the numerator, $r_{j, k}$ here approaches the $K$ -means $r_{j, k}$ as $\epsilon$ goes to zero. Moreover, as we are now treating the covariances $\Sigma_k$ as observed, there is no need to re-estimate $\Sigma_k$ ; it's simply $\epsilon\text{I}$ .
