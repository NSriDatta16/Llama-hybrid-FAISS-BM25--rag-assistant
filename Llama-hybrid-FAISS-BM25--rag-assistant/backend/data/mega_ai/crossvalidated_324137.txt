[site]: crossvalidated
[post_id]: 324137
[parent_id]: 323925
[tags]: 
I know that if using PCA for preprocessing, then the input data to the machine learning algorithm will rotate. Not exactly. PCA rotates, and then projects on the features with highest variance. If you only did rotation, how could you call this dimensionality reduction? Does this rotation of input affect the classification methods such as KNN, SVM and Random Forest? hxd1011 already tackled trees. SVM and linear methods - the structure can change after the projection, for example check out this: example of linearly separable data that isn't linearly separable after PCA . Also I think it this needs to be added to hxd1011's part on logistic regression: it depends whether you use regularization . For example, if you use Lasso or elasticnet, then you'd get different answer: $l_1$ norm is not invariant to rotations. This might be useful to know, since sparsity-inducing norms for linear problems are useful for very high-dimensional problems, and they sort of do feature selection for themselves, so they might be useful in context where you'd need dimensionality reduction before running another model.
