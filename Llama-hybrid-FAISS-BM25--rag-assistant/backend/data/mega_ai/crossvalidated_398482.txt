[site]: crossvalidated
[post_id]: 398482
[parent_id]: 396167
[tags]: 
Linear models are great for extrapolation, since they don't make too complex assumptions which might not generalize outside of the training set. Unfortunately that also means that they are not able to use the full potential of other features which are not changed when extrapolating. Since extrapolation is done only in respect of input $x_e$ we can split all inputs $X$ into vector $x_e$ and matrix $X_{other}$ . One approach could be to add more features based on the values of $X_{other}$ . These could be interactions between these features, squared values, logarithms etc. This could work well especially if you have a good idea on how each feature should contribute to the output. One could also build a custom model, which fits only a linear relationship in respect of $x_e$ but uses more complex relations for $X_{other}$ . Here is an example model built with keras which fits a neural network with 4 hidden layers for $X_{other}$ , then concatenates the last hidden layer with $x_e$ and uses one final layer with softplus activation (to ensure that the predictions are positive). With keras from keras.layers import Input, Dense, Lambda, concatenate from keras.models import Model import keras.backend as K from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler def loss(y_true, y_pred): return -K.log(1/y_pred) + y_true/y_pred all_inputs = Input(shape=(n_inputs,)) # Only the first feature will be used for extrapolation x_e = Lambda(lambda x: x[:,:1], output_shape=(1,))(all_inputs) x_other = Lambda(lambda x: x[:,1:], output_shape=(n_inputs-1,))(all_inputs) hidden_1 = Dense(20, activation='relu')(x_other) hidden_2 = Dense(20, activation='relu')(hidden_1) hidden_3 = Dense(10, activation='relu')(hidden_2) hidden_4 = Dense(20, activation='relu')(hidden_3) merged = concatenate([hidden_3, x_e]) preds = Dense(1,activation='softplus')(merged) model = Model(inputs=all_inputs, outputs=preds) model.compile(optimizer='adam', loss=loss, metrics=['mse']) model = Pipeline([ ('scale', StandardScaler()), ('keras', model), ]) With a model similar to this (I added some customization to my specific problem) I was able to get a test loss of 0.653. This model also seems to be best so far for interpolation within the training set.
