[site]: crossvalidated
[post_id]: 545427
[parent_id]: 514013
[tags]: 
The linear layer processes each element in the sequence independently, there is no interaction between them. Therefore information about future tokens is not available, also during training. Interaction/information exchange between tokens only occurs in the attention layers (hence we need masking)* (* I have not really looked into the normalization layers but I guess that there is no interaction between elements there either. Note that transformers use layer normalization and not batch normalization where elements influence each other.)
