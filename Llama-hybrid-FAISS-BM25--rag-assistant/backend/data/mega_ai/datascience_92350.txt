[site]: datascience
[post_id]: 92350
[parent_id]: 91073
[tags]: 
For your first question, you can check if the tokenizer covers a certain string with the following: text = 'today is a good day ' ids2string = lambda ids: tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(ids)) ids2string(tokenizer(text)['input_ids']) > today is a good day If emoji is not included in the tokenizer creation, the tokenizer will replace it with the unknown special token. You can access that with tokenizer.special_tokens_map['unk_token']. You can drop or keep them, shouldn't make much of a difference. Alternatively, if you're going to fine-tune, you can add your own tokens to the existing tokenizer with tokenizer.add_special_tokens . However, in this case the embeddings for that token will be random. And you need to train them.
