[site]: crossvalidated
[post_id]: 372721
[parent_id]: 372696
[tags]: 
Actually the labels "generalization" and "overfitting" might be a bit misleading here. What you want in your example is a good prediction of the dropout status. So technically: In training you therefore need to have an unbiased sample of dropout and non-dropout-students. It is extremely important to prepare not only the model, but even more the data you are using to evaluate (train, validate, etc.). There are text-book examples of overfitting, where you can e.g. plot a performance indicator (e.g. mislabelling rate) of the your training data and compare it with validation data. The training performance will always become better, but at some point the validation will become worse. There it is probably very clear that you'd rather stop the learning process before it worsens the performance. What is meant by "generalization" is actually very specific. You want your trained model to be the best possible once it encounters previously unseen data. You use the validation data, because there you know "the truth". Unlike with real data. So still as above: you want your model to give a prediction of the students status. - if your model is overfitted, it will give higher valued indicators for the students in your training set; but will perform worse on non-trained data - if you model has good generalization power; it will perform equally well on training data, as well as non-training data If you talk about "specific data sets", then either these are the basis of your training and validation; or you simply do it wrong. And this has nothing to do with generalization or overfitting in machine learning.
