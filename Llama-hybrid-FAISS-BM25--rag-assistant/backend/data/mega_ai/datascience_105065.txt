[site]: datascience
[post_id]: 105065
[parent_id]: 
[tags]: 
Using SHAP values as features in a classification problem

I'm looking for feedback on a methodology I've tried that has yielded strange results. Problem background: supervised multi-class classification problem for which I've used a random forest to create a predictive model 80% of data split into a training subset (Set A); 20% of data split into a testing subset (Set B) resulting precision and recall are around 0.7, with "harder" classes seeing modestly lower class-specific metric values Proposed method: Create a SHAP explainer object that leverages Shapley values, using the above-mentioned predictive model Use the explainer to predict SHAP values for each of the features in the feature vector used by the above-mentioned random forest, for a new training set (Set C) as well as testing set B Use the SHAP values as features for a new classifier; Set C's SHAP values become the training data and set B's SHAP values are used for testing data The resulting precision and recall are both 1.0 for all classes. Something is clearly going wrong, and I'm assuming there's some information bleed-through due to which data sets I'm using for which tasks. When looking at the method, though, I'm having trouble seeing where it is. I suspect this is due to my very basic knowledge of SHAP; I'm probably making a beginner mistake that I just can't spot. I'm hoping someone here can spot it for me and in doing so improve my understanding of the underlying tools I'm using. The TreeExplainer object I'm creating is described here: https://shap-lrjball.readthedocs.io/en/docs_update/generated/shap.TreeExplainer.html Any help would be greatly appreciated. If the question has insufficient detail, please feel free to comment and I will happily edit to address deficiencies.
