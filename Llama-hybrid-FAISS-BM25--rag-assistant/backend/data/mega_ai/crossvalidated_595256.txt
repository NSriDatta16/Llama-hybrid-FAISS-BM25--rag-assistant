[site]: crossvalidated
[post_id]: 595256
[parent_id]: 420998
[tags]: 
This depends on whether you are talking about logistic regression or penalized (e.g. LASSO, ridge or elastic net) logistic regression. In standard logistic regression re-scaling features in any way doesn't really make any difference to the maximum likelihood estimate (obviously, coefficients will correspondingly end up being re-scaled), unless you multiply the feature by 0 (which just destroys all information). I.e. if before re-scaling the log-odds for an event increase by 0.5 for every year of age and you multiply age by 0.1, then the new coefficient will be 5 (i.e. for every 10 years of age the log-odds go up by 5). However, re-scaling coefficients can cause or fix numerical issues in fitting a logistic regression model. In penalized logistic regression, re-scaling features matters, because (as explained above) the absolute size of the maximum likelihood estimates changes and so features that were re-scaled with small factors get penalized more and those re-scaled with large factors get penalized less. Here, "penalized more" means that the regression coefficients will be shrunk from the maximum likelihood estimate relatively more towards zero than for the coefficients for other features. The second bullet is why, if one has no particular reason to favor some features over others, it is common to standardize (i.e. center on zero and fix SD to 1) features before applying penalized logistic regression. The kind of approach you describe can be applied thereafter to do something like expressing a prior belief about which coefficients are more likely to be truly further away from zero. If done without standardizing first, I'm not really sure why I would do it.
