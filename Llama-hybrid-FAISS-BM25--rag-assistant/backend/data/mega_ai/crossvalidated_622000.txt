[site]: crossvalidated
[post_id]: 622000
[parent_id]: 621930
[tags]: 
Dummy variable trap is a problem only in classical regression models. It is not a problem for most of the machine learning models, including things like regularized regression. Label encoding is a poor way of encoding categorical data. If you encode {red, green, blue} as {1, 2, 3} you are implicitly assuming that the categories have numerical sense, so for example red + 2 = blue, which doesn't make sense. Rarely it is a valid choice. You should use dummy encoding or one-hot encoding. For the difference between them check the One-hot vs dummy encoding in Scikit-learn thread. TL;DR dummy encoding is one-hot encoding with one of the categories dropped, in some cases, you use one, in other another. If your “gender” feature has only two categories you can use dummy coding, i.e. a column of zeros and ones. One-hot encoding would introduce a redundant column, so it is rarely used for two categories. On another hand, if your “gender” feature has more categories, one-hot encoding may make sense.
