[site]: crossvalidated
[post_id]: 510521
[parent_id]: 510324
[tags]: 
Nice question! Since I am a frequentist and not a Bayesian, I will state the linear model you are fitting below: error = beta0 + beta1*conditionB + beta2*roundPost + beta3*conditionB:roundPost + error_term where: a) conditionB is a dummy variable defined as 1 if condition = Group B and 0 if condition = Group A; b) roundPost is a dummy variable defined as 1 if round = Post and 0 if round = Pre; c) error_term is a random error term assumed to be normally distributed with mean 0 and unknown variance $\sigma^2$ . Note that the model formulation is the same, regardless of what statistical framework you will use to fit the model: frequentist or Bayesian. The above model is in effect a collection of 4 sub-models corresponding to all possible combinations of levels/categories of your condition and round variables represented in your data. Submodel 1 [condition = Group A; round = Pre] error = beta0 + error_term Submodel 2 [condition = Group A; round = Post] error = beta0 + beta2 + error_term Submodel 3 [condition = Group B; round = Pre] error = beta0 + beta1 + error_term Submodel 4 [condition = Group B; round = Post] error = beta0 + beta1 + beta2 + beta3 + beta4 + error_term These submodels suggest that the (true) mean value of the error response is: beta0 for condition = Group A and round = Pre; beta0 + beta2 for condition = Group A and round = Post; beta0 + beta1 for condition = Group B and round = Pre; beta0 + beta1 + beta2 + beta3 for condition = Group B and round = Post. So you can define the effect of round on error for condition A as the difference of the means (beta0 + beta2) - beta0, which equals beta2. Further, you can define the effect of round on error for condition B as the difference of the means (beta0 + beta1 + beta2 + beta4) - (beta0 + beta1), which equals beta2 + beta3. The quantities beta2 and beta2 + beta3 represent the simple effects of round for conditions A and B and can be estimated using the contrast() function, as suggested by Russ in his comment. Given a linear regression model, a contrast is a linear combination of (true) regression coefficients which allows you to compare mean values of your response variable across conditions of interest.
