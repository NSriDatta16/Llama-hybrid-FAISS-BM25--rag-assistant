[site]: datascience
[post_id]: 68146
[parent_id]: 
[tags]: 
ADAM algorithm for multilayer neural network

I’m trying to touch neural networks without using “in box” algorithms. And so I found out that nowhere is written how to calculate square of gradient for hidden layers in ADAM optimizer. I took the description from original article and the problem is that Vt is some kind of normalizing coefficient, so it should be scalar. But for hidden layers gradient is not really a vector byt a matrix. So how should we calculate gt^2 in this case?
