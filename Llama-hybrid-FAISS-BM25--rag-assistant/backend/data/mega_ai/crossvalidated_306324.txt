[site]: crossvalidated
[post_id]: 306324
[parent_id]: 
[tags]: 
Do models overfit simple predictors alongside complicated predictors?

Consider a multiple regression model $f$ (e.g. a random forest or neural network) where you try to predict a target $y$ given a two predictor variables $x_1$ and $x_2$: $$f(x_1, x_2) = \hat y$$ Let's assume the relationship between $x_1$ and $y$ is quite simple, but its relationship with $x_2$ is more complicated. For example, let's assume the truth would be (where $\epsilon$ is some added noise): $$y = x_1 + \sin({x_2}^2) + \epsilon$$ In that case, is the following statement correct: Choosing a high variance (complex) model could capture the complicated relationship of $x_2$ and $y$, but it would overfit the simple relationship between $x_2$ and $y$? And if that's true, what models allow different regularization for different inputs? I know for example that there are ridge regression implementations which allow to set a separate regularization parameter for every input. Are there others? How would it be done in a Random Forest or a Multi-Layer Perceptron?
