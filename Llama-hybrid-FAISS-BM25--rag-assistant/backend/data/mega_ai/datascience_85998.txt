[site]: datascience
[post_id]: 85998
[parent_id]: 67206
[tags]: 
The 'Attention' terminology varies before and after a landmark paper in 2018 - Attention is all you need. Before 2018 Here 'query' is the hidden state of the decoder of the previous timestep. 'Values' - All the hidden states of the encoder Remember - the 'query' attends to all the 'values' So far so good. Attention mechanisms were used widely between 2014 to 2017 to improve the performance of RNNs. Then in 2018 a revolutionary paper comes out - Attention is all you need. It means what its title says - Basically chuck out your RNNs and use just Attention to encode sequences. By using self-Attention the model is able to build relationships between timesteps within an input sequence and encode it. RNN is not needed. So post-2018, you can use self-attention on the input sequences or the output sequences independently. You dont need the traditional RNN as encoder or decoder. The attention mechanism itself does the job of the RNN. So you have an encoder and a decoder that use Attention models. In this scenario both the query and the values (and also the key) come from the previous layer output. The calculation goes something like this: Get the word embedding for each word, then create three vectors from the embedding vector - key, query, values. These are created from the weight matrices found during training Now take each word and calculate scores by taking the dot product of the query vector (of the current word) with the key vector of the respective word weâ€™re scoring. Divide the scores by square root of the dimension of the key vectors to normalize the value to get more stable gradients. Softmax this Multiply each value vector by the Softmax score to amplify relevant words and and drown-out irrelevant words. The final step is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word)
