[site]: crossvalidated
[post_id]: 1824
[parent_id]: 1813
[tags]: 
Maybe you could measure the average difference between two runs of the same algorithm to the average difference between two runs from different algorithms. Doesn't solve the problem of how to measure that difference, but might be a more tractable problem. And the individual values of the time series would feed into the difference calculation instead of having to be treated as individual datapoints to be evaluated against each other (I also don't think that the particular difference at the nth step is what you really want to make statements about). Update Concerning details - well which features of the time series are you interested in, beyond the final error? I guess you actually got three different questions to solve: What constitues similarity for you, ie what do you mean when you say you don't believe the two methods are different? How do you quantify it - can be answered after 1, and How can you test for significant differences between your two methods? All I was saying in the first post was that the answer to (1) probably doesn't consider the individual differences at each of the 1000 generations. And that I'd advise coming up with a scalar value for either each time series or at least similarity between time series. Only then you get to the actual statistics question (which I know least about of all three points, but I was advised to use a paired t-test in a similar question I just asked, when having a scalar value per element).
