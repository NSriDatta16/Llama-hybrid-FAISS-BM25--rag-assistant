[site]: crossvalidated
[post_id]: 380037
[parent_id]: 
[tags]: 
Universal approximation theorem for whole R^d

The well-known universal approximation theorem states that neural network with one hidden layer can approximate any continuous function on every compact subset of $\mathbb{R}^d$ . My question is whether there is any paper which considers the approximation on the whole $\mathbb{R}^d$ domain? In my opinion, since the main themes of neural network are image recognition and natural language processing, it is enough to consider functions on a compact subset. However along with its great success, its application field is now widely opened to problems based on whole $\mathbb{R}^d$ domain. Although I can find some papers (Chen and Chen, 1990; Ito, 1992), they cannot tackle with whole $\mathbb{R}^d$ domain, because the former introduces "extended real line" $\bar{\mathbb{R}^d}$ instead of $\mathbb{R}^d$ , and the latter considers only continuous function with compact support. If you know the related paper, could you please tell me?
