[site]: datascience
[post_id]: 102627
[parent_id]: 102625
[tags]: 
Of course, classic techniques, such as min-max scaler and z-score normalization, just change the range of the values, hence they are prone to outliers and do not solve the problem. However, what these papers probably suggest, makes sense, providing a few conditions are met. In this context, I will try to summarize everything I can think of regarding both normalization and standardization. Although not entirely accurate, providing your data follow a power law distribution, you can scale the data with the log function (log scaling). This would change your data distribution to a "narrower" scale, ultimately decreasing the potential effect of your outliers. Feature Clipping: If your dataset has extreme outliers, you can always clip your features to fixed numerical value (or fixed value +- 3 std (standard deviation)). This would result in an information loss but would effectively combat outliers' effect in your analysis. Robust Scaler: When there are many instances of outliers in your dataset, you can normalize the data with the median divided by the IQR = the difference between the 75th and 25th percentiles of your data. This would not negate the effect of outliers in your machine learning model but will instead make normalize your data correctly, despite the existence of these extreme points. You can always use tree-based algorithms or neural networks for your analysis, which are robust to outliers.
