[site]: crossvalidated
[post_id]: 78599
[parent_id]: 
[tags]: 
Differences in power in regression versus mean of a ratio

I've been futzing around with the following example, and feel that there must be a good explanation or reference that I am blindly missing. Supposer you have a linear relationship between two variables. Let's even assume that the intercept goes through 0. Why would there be more power in analysis by regression of y~x versus y/x ~ 1? And in particular, if x is biased, the decrease in power is even more dramatic. I can show this by simulation in the following R code, it strikes me that there is easy explanation here. simBiasPower This produces the following estimates of power for the four analyses: unbiasedSlopeP unbiasedRatioP biasedSlopeP biasedRatioP 1 1 0.66 1 0.16 Note, I've tried this also forcing the intercept through 0 and thus not estimating an intercept for y~x, and the answer doesn't change. I've also changed the distributional form of the unbiased x, and it doesn't seem to matter very much. Similar drop in power. Of of the top of my head, when one divides by x, you reduce the leverage of points at the extreme end of the relationship - along the lines of reducing your variability in x in a regression context. But it feels like there is something more forehead-smackingly basic going on here.
