[site]: crossvalidated
[post_id]: 637802
[parent_id]: 534618
[tags]: 
In deep neural network layers, we want to retain the signals having normal distributions, and avoid diminishing and exploding gradients. This is the primary goal of weight initialization such as Xavier or He. As demonstrated Building makemore Part 3: Activations & Gradients, BatchNorm , the activation functions are basically squashing (-inf to inf) range into (0, 1) or (-1, 1) or (0,) . Because of this, the variance of layer output signal tends to shrink causing difficulties to maintain normal distribution. To alleviate the this squashing effect, weight initializations are incorporating expanding gain such as $\sqrt{2}$ for ReLU or $5/3$ for tanh activation functions. Hence, what we need to look for is where such a squashing effect is occurring, which the expanding effect of the multiplying with $\sqrt(D)$ will alleviate it. The Transformer Architecture does not use Activation Functions except the ReLU at the Position Wise Feed Forward sub layer. Then, this is the place to look at. Or, look at how the embedding layer weights are initialized which may require the alleviation by $\sqrt(D)$ .
