[site]: datascience
[post_id]: 92885
[parent_id]: 92825
[tags]: 
Neural networks are black boxes, and figuring out its internal workings can be really hard. When you decide to use ML for your model, you are sacrificing accuracy for interpretability. However, that does not mean it is totally impossible to interpret networks. The most tedious way to determine feature importance would be to remove features and see how it impacts the model. If you're interested in research into the subject, I've encountered some research papers to use decision trees to understand networks. https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Interpreting_CNNs_via_Decision_Trees_CVPR_2019_paper.pdf https://arxiv.org/pdf/1802.02195.pdf
