[site]: crossvalidated
[post_id]: 369796
[parent_id]: 369794
[tags]: 
In your linked paper (you should provide the full citation to avoid link rot) we see the following Our neural network classifier, depicted in Figure 3 (and based on a one-layer model in Bow- man et al. 2015), is simply a stack of three 200d tanh layers Following the reference through Samuel R. Bowman, Christopher Potts, and Christopher D. Manning. 2015. Recursive neural networks can learn logical semantics. In Proc. of the 3rd Workshop on Continuous Vector Space Models and their Compositionality. it seems like the authors are using "tanh layer" to mean a fully connected layer with a tanh activation. This is not said outright but it seems heavily implied by the following section:
