[site]: datascience
[post_id]: 77537
[parent_id]: 
[tags]: 
Is it reasonable to train a neural network many times and cherry pick the best result based on test dataset accuracy?

My current advisor at Uni insists that I train 10 instances of the same network and pick the one with best test accuracy in order to escape the "local minima". In my opinion this does not work at all, and should lead to picking the model that best fits the test_set, but may not be generalizable enough for it to work with the actual distribution behind it. Is there any material or research on this? I really think this method is archaic and makes no sense, but i can't argue with my professor without actual scientific material.
