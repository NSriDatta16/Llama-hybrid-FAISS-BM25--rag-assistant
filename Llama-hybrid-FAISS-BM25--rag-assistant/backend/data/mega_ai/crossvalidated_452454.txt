[site]: crossvalidated
[post_id]: 452454
[parent_id]: 452148
[tags]: 
Eigenvectors & Eigenvalues The concept of eigenvectors and eigenvalues which are the basis for principal component analysis (PCA), as explained on wikipedia: In essence, an eigenvector $v$ of a linear transformation $T$ is a nonzero vector that, when $T$ is applied to it, does not change direction. Applying $T$ to the eigenvector only scales the eigenvector by the scalar value $\lambda$ , called an eigenvalue. This condition can be written as the equation: $T(v) = \lambda v$ . The above statement is very elegantly explained using this gif: Vectors denoted in blue $\begin{bmatrix}1 \\1 \\ \end{bmatrix}$ and magenta $\begin{bmatrix}1 \\-1 \\ \end{bmatrix}$ are eigenvectors for the linear transformation, $T = \begin{bmatrix}2 & 1 \\1 & 2 \\ \end{bmatrix}$ . The points that lie on the line through the origin, parallel to the eigenvectors, remain on the line after the transformation. The vectors in red are not eigenvectors, therefore their direction is altered by the transformation. Blue vectors are scaled by a factor of 3 -- which is the eigenvalue for the blue eigenvector, whereas the magenta vectors are not scaled, since their eigenvalue is 1. Link to Wikipedia article.
