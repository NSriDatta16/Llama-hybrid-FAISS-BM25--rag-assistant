[site]: crossvalidated
[post_id]: 250655
[parent_id]: 250638
[tags]: 
Neural networks are universal function approximatorsâ€“consequently, a single hidden layer with a tanh transfer function and many hidden neurons can separate the data, in theory. That does not tell us anything about generalization though. It might be that the function separating the training data does horribly on unseen data. If I had to do that ask, I'd use an MLP with elu transfer functions, 2 layers of 32 neurons each and residual blocks and maybe batch or weight normalisation. For optimisation, I'd use Adam. I suppose that should be able to solve it.
