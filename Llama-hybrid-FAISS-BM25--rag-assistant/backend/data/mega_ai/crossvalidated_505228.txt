[site]: crossvalidated
[post_id]: 505228
[parent_id]: 
[tags]: 
Policy gradient theorem for a finite number of steps

Consider a continuous state-action space $\mathcal{S} \times \mathcal{A}$ . For the timing convention, suppose that we start in period $t=0$ by drawing a state $s_0 \in \mathcal{S}$ from some probability distribution with density $p(s)$ . Then, our agent draws the action $a_0 \in \mathcal{A}$ from a policy density function $\pi(a|s_0)$ . He then receives a reward $r_0 := r(s_0, a_0)$ and, at time $t=1$ , the new state is drawn from the transition density $p(s_t|s_{t-1}, a_{t-1})$ . This sequence is repeated for $t=0,1,2,\dots,T$ producing a trajectory $\tau := \{ s_t, a_t \}_{t=0}^T$ for the episode. Our objective is to choose a policy so as to maximize the expected cumulative reward for a given episode, i.e. $$ J(\theta) = E_{\pi} \left[ \sum_{t=0}^T r_t \right]. $$ I understand that the probability for an entire trajectory can be obtained as $$ p(\tau) = p(s_0) \Pi_{t=1}^{T} p(s_t | s_{t-1}, a_{t-1}) \pi(a_{t-1}| s_{t-1}) \pi(a_T|s_T) $$ such that if we parametrize the policy by a vector $\theta \in \mathbb{R}^K$ , we get $$ \nabla_\theta ln p(\tau, \theta) = \sum_{t=0}^T \nabla_\theta ln \pi(a_t|s_t, \theta).$$ I'm not sure how exactly I'm supposed to proceed from here. Even in the book of Sutton and Barto, they present a proof which is presumably for the "episodic" case, yet they end up with a sum over an infinite number of steps (chapter 13, p.325). I see that they invoke a recursion, but the only way I see this making sense is if the $k$ step transition densities for $k > T$ are all zero. I'd be very satisfied with a reference to a proof elsewhere, to be honest.
