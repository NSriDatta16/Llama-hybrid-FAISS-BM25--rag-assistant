[site]: crossvalidated
[post_id]: 561354
[parent_id]: 561164
[tags]: 
regardless of how many dimensions your function has, there is a 95% probability that only 60 iterations are needed to obtain an answer in the top 5% of all possible solutions! Finding a 95th-percentile solution is no guarantee of finding a good solution. The nature of the curse of dimensionality is that your "optimization distribution" becomes very skewed. When you have a lot of dimensions, 99.99999999% of your parameter space is going to be far from optimum. If you ask random search to find you a 99.99999999th percentile result, it will take billions of times as many trials as finding a 95th-percentile result (and honestly, I probably haven't added nearly enough nines for most real-world scenarios). And from an information-theoretic perspective, random search is purely "stupid" â€” it doesn't use any information from the objective function to inform its next guess, and the millionth guess is no more likely to be near the optimum than the first guess is. In many cases, a gradient is no more expensive per-evaluation than the objective function itself, and its value is obvious: it's a signpost that says "go this way to get a local improvement". In other cases, no gradient is available (unless we want to estimate it by doing a bunch of function evaluations, which is of course expensive), but then, the answers to the 2016 question you linked cover other techniques we can use to incorporate information about "where we've been and what we found there", which will hopefully enable our later guesses to be more productive than our earlier guesses. Every optimization technique (whether gradient descent, the simplex method, Bayesian optimization, or whatever else) encodes some sort of assumption about the structure of the objective function. They perform well (and justify their overheads) when the objective function agrees well with that structure, and poorly when it doesn't. Random search incorporates no implicit structure, which means that it's the optimal optimizer when the objective function itself is unstructured and completely random (you're looking for a needle in a haystack, and finding a slightly more silvery stalk of hay doesn't indicate the presence of a needle nearby). Otherwise, you can probably win by doing something else.
