[site]: crossvalidated
[post_id]: 115211
[parent_id]: 115207
[tags]: 
Any finite set of points can fit on any manifold (theorem reference needed, I cant remember what the theorem is, I just remember this fact from uni). If one does not want all points to be identified, then the lowest possible dimension is 1. Take as a simple example, given N 2d points, there exists some N - 1 order polynomial where all N points lie on that polynomial. Therefore we have a 1d manifold for any 2d dataset. I think the logic for arbitrary dimensions is similar. So, that's not the issue, the real assumptions are on the structure/simplicity of the manifold, particularly when treating connected Riemannian manifolds as metric spaces. Ive read papers on this manifold hocus pocus, and found if you read carefully some pretty huge assumptions emerge! The assumptions made are when the induced definition of "closeness" is assumed to "preserve the information in our dataset", but since this not formally defined in Information Theoretic terms, the resulting definition is pretty ad hoc and quite a huge assumption indeed. In particlar the problem seems to be that "closeness" is preserved, i.e. two close points, stay close, but that "farness" is not, and so two "far" points do not stay far. In conclusion I would be very wary of such trickery in machine learning unless its known the dataset is indeed naturally euclidean, e.g. visual pattern recognition. I would not consider these approaches appropriate for more general problems.
