[site]: crossvalidated
[post_id]: 66651
[parent_id]: 66649
[tags]: 
In general, Bayesian estimation is not very concerned with unbiasedness of estimators since the model is always misspecified. There definitely exist proofs about conditions for unbiased estimation in Bayesian frameworks. I just don't think practitioners care very much about that and try to avoid using fitting procedures that would be susceptible to this kind of thing at all. And sometimes doing tricky things just to get an "unbiased" estimator can come at the expense of other exploitable problem structure (e.g. when pooling is used to get an unbiased estimator, you are trading usable category-level variance in exchange for guarantees of unbiasedness under implausible assumptions. Whether that is a useful trade-off or not should be considered at the level of specific applied inference problems, rather than as a generic thing to do with any model. Here is a post by Andrew Gelman about that. ) For the problem at hand, I believe Bayesian practitioners look more generally at model fit assessment and model misspecification. It's more about whether you are missing an appreciable or significant effect size for the omitted variable, and less about whether the omission has sprayed effect size onto other variables. One way to address this is to perform posterior predictive checks on your model. If you do this with a procedure like continuous model expansion ( section 5.2 of this paper ), then the posterior predictive checks should give you evidence about the best model specification (or better yet, the best distribution over some set of model specifications), rather than forcing you to make an unnatural choice like "The model with Variable Z is 'better' than the model without Variable Z" (which are almost always misunderstood or misinterpreted later by readers).
