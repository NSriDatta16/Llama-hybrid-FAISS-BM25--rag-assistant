[site]: crossvalidated
[post_id]: 367362
[parent_id]: 284211
[tags]: 
You can't "force" it, but you can surely add terms to your loss function that would make your network try to minimize the dissimilarity between the distribution of said activations and a probability distribution. Namely, you can employ the Kullbackâ€“Leibler divergence to this end, adding specific terms to your loss function. This is done in Variational Autoencoders (VAEs), for example.
