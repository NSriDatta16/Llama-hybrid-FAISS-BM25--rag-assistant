[site]: datascience
[post_id]: 23913
[parent_id]: 23789
[tags]: 
It's easier to start with your second question and then go to the first. Bagging Random Forest is a bagging algorithm. It reduces variance. Say that you have very unreliable models, such as Decision Trees. (Why unreliable? Because if you change your data a little bit, the decision tree created can be very different.) In such a case, you can build a robust model (reduce variance) through bagging -- bagging is when you create different models by resampling your data to make the resulting model more robust. Random forest is what we call to bagging applied to decision trees, but it's no different than other bagging algorithm. Why would you want to do this? It depends on the problem. But usually, it is highly desirable for the model to be stable. Boosting Boosting reduces variance, and also reduces bias. It reduces variance because you are using multiple models (bagging). It reduces bias by training the subsequent model by telling him what errors the previous models made (the boosting part). There are two main algorithms: Adaboost: this is the original algorithm; you tell subsequent models to punish more heavily observations mistaken by the previous models Gradient boosting: you train each subsequent model using the residuals (the difference between the predicted and true values) In these ensembles, your base learner must be weak. If it overfits the data, there won't be any residuals or errors for the subsequent models to build upon. Why are these good models? Well, most competitions in websites like Kaggle have been won using gradient boosting trees. Data science is an empirical science, "because it works" is good enough. Anyhow, do notice that boosting models can overfit (albeit empirically it's not very common). Another reason why gradient boosting, in particular, is also pretty cool: because it makes it very easy to use different loss functions, even when the derivative is not convex. For instance, when using probabilistic forecast, you can use stuff such as the pinball function as your loss function; something which is much harder with neural networks (because the derivative is always constant). [Interesting historical note: Boosting was originally a theoretical invention motivated by the question " can we build a stronger model using weaker models "] Notice: People sometimes confuse random forest and gradient boosting trees, just because both use decision trees, but they are two very different families of ensembles.
