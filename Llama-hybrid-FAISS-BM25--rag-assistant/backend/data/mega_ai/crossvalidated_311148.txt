[site]: crossvalidated
[post_id]: 311148
[parent_id]: 311136
[tags]: 
20'000 lines and around 100 columns is comfortably large to be outside the small sample settings (100 observations or so) where e.g. gradient boosting does not do well. A well tuned gradient boosting tree (a science for itself...) will outperform a linear model by quite some degree, even if most relationships are linear. The reason for this is that even if you invest ages, you cannot guarantee to capture that one key interaction between $\log(X_1)$, $X_2^3$ and $\sqrt X_3$... Since I don't have access to your data, I will drop some R code with the diamonds data set of ggplot2 . Maybe you can adapt it slightly to fit your data set. In your situation, with many strongly correlated predictors, I'd assume that elastic-net regression will beat a pure OLS. Since the code for optimizing the gradient boosting model via cross-validation is relatively long and uninteresting, I did not put it here. #====================================================================== # Regression Examples using Diamonds data #====================================================================== library(glmnet) library(ggplot2) # for data set "diamonds" library(xgboost) library(ranger) #====================================================================== # Data prep #====================================================================== diamonds
