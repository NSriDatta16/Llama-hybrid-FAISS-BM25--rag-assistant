[site]: crossvalidated
[post_id]: 83745
[parent_id]: 83588
[tags]: 
This answer implements an approach along the lines of whuber's comment, where $p$ is estimated naively from the 10 tosses made at the start, and then 'plugged-in' to $P(F_{HH}|p)$ to get the prediction interval. The approach does not explicitly account for uncertainty in $p$, which leads to poor performance in some cases, as shown below. If we had many more than 10 tosses available to estimate $p$, then this approach might work fine. It would be interesting to know of other approaches which can account for the uncertainty in $p$. All code in this answer is in R. Step 1: Code to compute $P(F_{HH}|p)$ Firstly, we need to be able to compute $P(F_{HH}|p)$. The following code does that analytically (since the simulation approach is very inefficient for small $p$): pmf_FHH To test the above function and for later use testing the prediction intervals, we write another function to simulate the coin toss process. sim_FHH_p Now I run a test to check that the simulated and analytical results are 'the same' (increase the 1e+07 to get better agreement). set.seed(1) p=0.3 # Simulate coin-toss qq=sim_FHH_p(p,n=1e+07, pattern='11') Nmax=round(10/p**2) # Convenient upper limit where we check pmf_FHH empirical_pmf=rep(NA,Nmax) for(i in 1:Nmax) empirical_pmf[i] = (sum(qq==i)/length(qq)) png('test_analytical_relation.png',width=6,height=5,res=200,units='in') plot(1:Nmax,empirical_pmf,main='Test of analytical relation',ylab='pmf') points(1:Nmax,pmf_FHH(p, 1:Nmax),col='red',t='l') legend('topright', c('Approximate empirical pmf', 'Analytical pmf'), pch=c(1,NA),lty=c(NA,1),col=c(1,2)) dev.off() It looks fine. Step 2: Code to compute the prediction interval, assuming $p$ is known. If p is known, then we can directly use $P(F_{HH}|p)$ to get a prediction interval for $F_{HH}$. For a one-sided (1-$\alpha$) prediction interval, we just need to get the (1-$\alpha$) quantile of $P(F_{HH}|p)$. The code is: ci_FHH (1-alpha/2))) }else{ lowerInd=2 upperInd=min(which(cdf_FHH>(1-alpha))) } return(c(lowerInd,upperInd, cdf_FHH[lowerInd-1],cdf_FHH[upperInd])) } Step 3: Test the prediction interval coverage Theoretically we expect the prediction intervals developed above to be very good if $p$ is estimated correctly, but perhaps very bad if it is not. To test the coverage, the following function assumes the true value of $p$ is known, and then repeatedly makes an estimate of $p$ based on 10 coin flips (using the fraction of observed heads), and computes a prediction interval with the estimated value of $p$. test_ci_with_estimated_p =myci[1] & simRuns A few tests confirm that the coverage is nearly correct when $p$ is estimated correctly, but can be very bad when it is not. The figures show tests with real $p$ =0.2 and 0.5 (vertical lines), and a theoretical coverage of 0.9 (horizontal lines). It is clear that if the estimated $p$ is too high, then the prediction intervals tend to undercover, whereas if the estimated $p$ is too low, they over-cover, except if the estimated $p$ is zero, in which case we cannot compute any prediction interval (since with the plug-in estimate, heads should never occur). With only 10 samples to estimate $p$, often the coverage is far from the theoretical level. t5=test_ci_with_estimated_p(0.5,theoretical_coverage=0.9) t2=test_ci_with_estimated_p(0.2,theoretical_coverage=0.9) png('test_CI.png',width=12,height=10,res=300,units='in') par(mfrow=c(2,2)) plot(t2$est_p,t2$coverage,xlab='Estimated value of p', ylab='Coverage',cex=2,pch=19,main='CI performance when p=0.2') abline(h=0.9) abline(v=0.2) plot(t5$est_p,t5$coverage,xlab='Estimated value of p', ylab='Coverage',cex=2,pch=19,main='CI performance when p=0.5') abline(h=0.9) abline(v=0.5) #dev.off() barplot(table(t2$est_p),main='Estimated p when p=0.2') barplot(table(t5$est_p),main='Estimated p when p=0.5') dev.off() In the above examples, the mean coverage was pretty close to the desired coverage when true $p$=0.5 (87% compared with the desired 90%), but not so good when true $p$=0.2 (71% vs 90%). # Compute mean coverage + other stats summary(t2$coverage) summary(t5$coverage)
