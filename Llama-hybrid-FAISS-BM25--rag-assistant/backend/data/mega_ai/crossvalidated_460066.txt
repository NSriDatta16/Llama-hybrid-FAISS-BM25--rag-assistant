[site]: crossvalidated
[post_id]: 460066
[parent_id]: 
[tags]: 
Why is the model performance better with more data, while it does not seem to be due to reduced model variance?

So according to most of the sources I saw, increasing training data size will only benefit high-variance ML models by exposing the model to less spurious patterns, which occur more frequently in smaller data sets. However, I observed that my NLP model had consistently low (which suggests that it is not due to high variance) prediction accuracy when the training data set is small, and its prediction accuracy improved dramatically with more training data. The image below is the learning curves for three runs of the same NLP algorithm. This seems intuitive as statistical methods need more data to be trained properly. However, I'm interested to know if there's any theory/rules that explains this observation?
