[site]: crossvalidated
[post_id]: 22391
[parent_id]: 
[tags]: 
Why should we discuss convergence behaviors of different estimators in different topologies?

In the first chapter of the book Algebraic Geometry and Statistical Learning Theory which talks about the convergence of estimations in different functional space, it mentions that the Bayesian estimation corresponds to the Schwartz distribution topology, whereas the maximum likelihood estimation corresponds to the sup-norm topology (in page 7): For example, sup-norm, $L^p$-norm, weak topology of Hilbert space $L^2$, Schwartz distribution topology, and so on. It strongly depends on the topology of the function space whether the convergence $K_n(w)\to K(w)$ holds or not. The Bayes estimation corresponds to the Schwartz distribution topology, whereas the maximum likelihood or a posteriori method corresponds to the sup-norm. This difference strongly affects the learning results in singular models. where $K_n(w)$ and $K(w)$ are respectively the empirical KL-divergence (summation over observations) and the true KL-divergence (integral w.r.t. the data distribution) between the true model and a parametric model (with parameter $w$). Can anyone give an explanation, or hint me which place in the book has the justification? Thank you. Update : copyright contents are removed.
