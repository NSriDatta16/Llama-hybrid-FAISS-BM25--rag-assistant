[site]: crossvalidated
[post_id]: 172453
[parent_id]: 172163
[tags]: 
It looks like you're looking to address two concerns - 1. interpretability and 2. efficiency of prediction. As already mentioned in the comments above, you can extract variable importance in Python, so that addresses point 1. To address point 2, if you are concerned with the efficiency down to microseconds, then you may want to explore other algorithms, such as logistic regression, and compare the out-of-sample performance to that generated by Random Forest; if the performance is nearly equivalent but logistic regression is much faster, then you can make the decision to go with logistic regression. If you are set on using Random Forest, the short answer is that you technically could build one random tree by setting ntree=1 and it might produce a decent prediction, but a collection of trees will be much better than a single tree. So it doesn't make sense to build just one tree from the subset of trees unless you are will to trade off out-of-sample performance for efficiency. Additionally, you could also speed up the predictions by a factor of 10 or more by only using a subset of the trees in the final prediction. If you train 1500 trees, then you could select the subset that best contributes to the final prediction. I'm thinking of something along the lines of Ensemble Selection from a Model of Libraries , where each tree in your forest would be the model in your ensemble.
