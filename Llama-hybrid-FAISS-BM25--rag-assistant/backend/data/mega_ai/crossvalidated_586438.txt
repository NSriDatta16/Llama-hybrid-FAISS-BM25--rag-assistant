[site]: crossvalidated
[post_id]: 586438
[parent_id]: 
[tags]: 
Peaks in error during training - Regression problem with DL model (LSTM)

During training I get unusual behavior of my model. Peaks show up in the both validation and training errors that I could not understand. I use MSE as a loss 1 and similar behavior appears in other metrics MAE 2 and MPAE 3 . I am using Deep LSTM model to do regression to approximate another function (time sequence -> real value). It is a determenistic function with no noise (thus no overfitting should appears). The function is costful to execute hence the usefulness of ML model to approximate it. Data set : Input = ~45000 Time Series (Sequence length = 60, features = 14) - Output = One real value for each sequence (~45000 real values). Similar behavior when changing model complexity (parameters number). Peaks are more significants when model is more complex (deeper and/or bigger). I used model with params number between 500k to 3M. How harmful is this to my model ? For example, i can not determine when to stop training it will always be after several epochs (=patience if early stopping is used) from the peak. What are possible interpretations ? Wrong loss function or extra layer What are the possible solutions to avoid this ? Smaller LSTM layers gives higher error on test set
