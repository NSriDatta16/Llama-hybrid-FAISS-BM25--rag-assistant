[site]: crossvalidated
[post_id]: 618467
[parent_id]: 618451
[tags]: 
I don't think "true" is a suitable category for models. Models are mathematical formalisms and live, if you will, in the world of mathematics, which is not (normally) the world we are interested in when doing statistics. I tend to agree with (and often cite myself) Box's "all models are wrong, but some are useful" as a good reminder that we shouldn't expect being "true/correct" from a model, however it may be misleading to think of a model as "wrong" in the sense that there is some cure; that reality could somehow show us a truth and we could verify (in the formal world of mathematics) that "all models" disagree with it. (We can show formally based on data that certain not well chosen models look very wrong, but this doesn't generalise to "all models"). To become more practical, I first give credit to existing answers that emphasise the pragmatic/performance aspect; we could try to formalise what we want from a model, and then measure to what extent the model delivers it. The easiest way to do this is obviously predictive power, as stated in the question. A number of remarks on this. Firstly, our ability to measure predictive power obviously relies on the availability of test/validation data. Not only are there situations in which there is not enough such data, also in some situations available test data don't allow to generalise predictive power as desired, because the model will later be applied in situations in which new data to be predicted may arise in (more or less) different ways and may turn out to be systematically different from the original test data. Furthermore, it is useful here to distinguish between models and methods . A model (with assumed unknown true parameter values) does not predict; in order to predict you need a prediction method, which may (or may not) be based on parameter estimation. There is often more than one method (or algorithm) that qualify to get a prediction (and/or estimation) based on the same model, which particularly also means that if prediction doesn't work well, the model may not necessarily be at fault. Also prediction may not work well because the information in the data is too weak for good prediction, regardless of the quality of the used model or method. My last aspect regarding predictive power is that this is not necessarily what is most important in a given project. Particularly, often we want to make statements about the importance of underlying factors and causality, and sometimes we want a simple message for the sake of communication. Also, it may be at least as important as prediction itself to have a reliable quantification of uncertainty. The relation of predictive power to these aims isn't always direct. Particularly, predictive power doesn't require model interpretation, but this may be the major thing we worry about. So when being pragmatic, I think that it is important to clarify the aims of analysis. Predictive power may be the most important one, or one of several aims, or not that important at all. I don't think it is appropriate to set up any quality measure (as for example predictive power) as a "dogma" (as said in the question) just because it can be measured. We should use quality measures to the extent that they measure what is important to us, in the given situation. Now the question is correct suggesting (implicitly) that it is often a problem to evaluate the quality of a model in ways other than predictive quality. I will touch upon some aspects of this. Firstly, there are direct measurements and tests of model fit, which in some sense measure the "distance" between what is implied by the model and the data (to be distinguished from the assumed truth underlying the data). An example for this is the Kolmogorov distance (and associated test) between the empirical distribution of 1-dimensional data and an assumed cdf. Also there are various test statistics to test independence of observations, linearity of regression etc. These can be used in principle, however, once more, it is important to think about whether and to what extent the outcomes of such tests/measurements are relevant to our aims. For example, for one-dimensional data, the sample mean and the standard distribution theory used for tests and confidence intervals are based on a normality assumption, however often, for estimating the underlying "true" mean and specifying its uncertainty, they work quite well also if the underlying distribution is not normal (and, e.g., using Kolmogorov distance and the KS-test, data are quite obviously not normal). This is backed up by the Central Limit Theorem, which itself relies on certain (more general) model assumptions. But in certain situations, particularly with gross outliers in the data, it may not work well. In fact, the claims I just made, namely what "works well" and what doesn't, need to be well defined as well of course, and are relative to the aim of analysis. What I'm basically saying is that if we imagine different models from the one we originally assumed, and we define what we are interested in relative to these different models, and we then apply a method to the data that is based on the original model, then we can explore, by theory or simulation, to what extent the results we get are still good/useful (according to a well defined criterion such as predictive power, test error rate, number of (in)correctly selected variables, or estimator MSE). So even though "all models are wrong", I recommend, when assuming one model, to ask, what would happen if another model would be true, that is, say, also compatible with our data, and background knowledge that we have. Actually, this leads to investigations of robustness and sensitivity analysis . In robust statistics , typically one looks at how the performance of methods could change if another model were true that is "in some sense" similar to the assumed model, and typically one considers "worst cases" in such classes of models. The implication is often to use different, more robust methods that are still calibrated for use with the same original model, but less easily affected if there are deviations as formalised in robustness theory (most prominently outliers). One could also generalise the model to something nonparametric in order to use methods that have "performance guarantees" over broader classes of models. (Once more always keep in mind which performance measurement is actually relevant for the situation in hand.) Often there is a trade-off here because methods based on more general models may lose quality for specific models that we may be particularly interested in, and theoretical "performance guarantees" may require quite large data sets to be actually useful. A good recommendation is to set up one or more realistic models for your data, to try out the method(s) that you want to use/compare, and see how they perform in such an artificial situation where you know the truth (because you actually set it yourself). That you can do such a thing is actually one of the major benefits of statistical models. You can set up an artificial world where you control the truth, and then you can see what works well! (Here you do not rely on enough available test data as you produce that yourself; note that most statistical theory actually does just that - which means you can learn from it without having to "believe" or "trust" the model.) Of course you need to address to what extent what happens in the specific artificial world you set up is relevant for the real world, but for sure if you are accepting a certain model in the first place, it also makes sense to look at other models that may produce similar data and/or share a similar interpretation. Here is a paper that is connected to the "philosophy" explained above, very recently accepted for publication by Journal of Data Science, Statistics, and Visualisation. I. Shamsudheen & C. Hennig: Should we test the model assumptions before running a model-based test? (In this work we particularly emphasise that finding model deviations that are problematic regarding the results of model based methods is not the same as finding problems regarding the model fit of the data.)
