[site]: crossvalidated
[post_id]: 321264
[parent_id]: 321258
[tags]: 
What the book mentions and what the author of the post meant are two different things. As the book mentions, 'unfolding' is dependent on the length of the input sequence. To understand this, suppose you want to lay down the exact computations that are happening in an RNN, in that case, you have to 'unfold' your network and the size of your 'unfolded' graph would depend on the size of the input sequence. For more information refer to this page . It says that "By unrolling we simply mean that we write out the network for the complete sequence. For example, if the sequence we care about is a sentence of 5 words, the network would be unrolled into a 5-layer neural network, one layer for each word." In case of the post , what the author meant is that during training you need 'unrolling' because you need to store the activations/ hidden states for backpropagation. During testing, you need not store the hidden states (as you don't need to do back-propagation), so no 'unrolling' is required.
