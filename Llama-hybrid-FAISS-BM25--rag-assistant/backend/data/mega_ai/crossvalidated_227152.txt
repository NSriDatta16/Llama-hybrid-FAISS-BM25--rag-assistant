[site]: crossvalidated
[post_id]: 227152
[parent_id]: 158712
[tags]: 
If you are interested in only the linear case then logistic regression (LR) is a better choice, as it is both convex and analytic(you may want to ridge it if you are interested in the regularization). But when you go for non-linear that's where the tricky part comes into picture. For non linear cases there is no reasonable way to keep things both convex and analytic you will need to sacrifice one of the two.In neural nets you sacrifice convexity and in svms you sacrifice holomorphism. strictly speaking there is no difference between LR and SVM ,svms just predict on which side of the line a point lies, LRs take also into consideration how far they lie from the boundary(on the boundary-margin line the sigmoid gives you the probability 0.5 in case of LR) . SVMs are forced to make this compromise because for non linear kernels the intuition of distance from a curved-hyperplane(algebraic variety is a better term) is not the same as in linear case , in fact the problem of solving shortest distance from a hyper surface to a specific point is very hard(harder than the SVM itself), but on the other hand Vapnik realized to just predict on which side of boundary a point lies is very easy as in O(1) time. This is the true insight behind SVM, making it the only available convex optimization alternative in statistical learning theory.But my feeling is you sacrifice a little too much, both holomorphism and probabilistic nature are lost. But for specific cases like ground-truthing SVMs are very reliable and are also fully falsifiable scientific models unlike its non convex alternatives. Tldr: yes, the mean value theorem comes to rescue for non analytic functions.In convex-non analytic cases the mean value thorem turns into an inequality setting some boundary conditions on the sub-gradients use that to do a sub gradient decent
