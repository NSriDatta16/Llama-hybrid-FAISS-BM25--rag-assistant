[site]: crossvalidated
[post_id]: 375385
[parent_id]: 
[tags]: 
Using step function as activation function in the final layer

I am using variational autoencoders as machine learning algorithm. My input data are images/matrices that represent user interface layouts or how the HTML page will be divided. I am thinking to implement step function as an activation function in the final layer of my model. However, according to the documentation that I read, back-propagation requires differentiable activation function. The derivative of step function is 0. My question is whether it is a good idea to use step or staircase as activation in the final layer (in the decoder) ? If I do, will it effect the weights of the other previous layers ? The motivation behind using step function as activation function is the fact that I want to discretize my output data.
