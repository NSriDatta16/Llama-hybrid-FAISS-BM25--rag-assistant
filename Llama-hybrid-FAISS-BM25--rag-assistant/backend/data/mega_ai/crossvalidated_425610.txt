[site]: crossvalidated
[post_id]: 425610
[parent_id]: 
[tags]: 
Why Massive Random Spikes of Validation Loss?

My problem is to estimate the length of a straight line in an image, in pixel. My training size is 6000 images, validation is 1000 images. Each image has 200 x 200 pixels. My data is generated using python code, and distance is calculated using Pythagoras of 2 points. Here is how my data looks like: a) length: 153.2220610747682201 b) length: 190.7668734345667190 c) length: 98.38699100999075142 My training code is here: img_size = 200 def preprocess_image(image): image = tf.image.decode_jpeg(image, channels=3) image = tf.image.resize(image, [img_size, img_size]) image /= 255.0 # normalize to [0,1] range return image def load_and_preprocess_image(path): image = tf.read_file(path) return preprocess_image(image) AUTOTUNE = tf.data.experimental.AUTOTUNE BATCH_SIZE = 16 train_labels = np.loadtxt("train_labels.txt") val_labels = np.loadtxt("val_labels.txt") train_images = sorted(glob.glob("train_img/img_*.jpg")) val_images = sorted(glob.glob("val_img/img_*.jpg")) train_path_ds = tf.data.Dataset.from_tensor_slices(train_images) val_path_ds = tf.data.Dataset.from_tensor_slices(val_images) train_image_ds = train_path_ds.map(load_and_preprocess_image, num_parallel_calls = AUTOTUNE) train_label_ds = tf.data.Dataset.from_tensor_slices(tf.cast(train_labels, tf.float32)) train_image_label_ds = tf.data.Dataset.zip((train_image_ds, train_label_ds)) val_image_ds = val_path_ds.map(load_and_preprocess_image, num_parallel_calls = AUTOTUNE) val_label_ds = tf.data.Dataset.from_tensor_slices(tf.cast(val_labels, tf.float32)) val_image_label_ds = tf.data.Dataset.zip((val_image_ds, val_label_ds)) model = tf.keras.models.Sequential([ tf.keras.layers.Convolution2D(16,3,3, input_shape=(img_size, img_size, 3), activation = 'relu'), tf.keras.layers.BatchNormalization(), tf.keras.layers.MaxPooling2D(pool_size=(2,2)), tf.keras.layers.Convolution2D(32,3,3, activation = 'relu'), tf.keras.layers.BatchNormalization(), tf.keras.layers.MaxPooling2D(pool_size=(2,2)), # tf.keras.layers.Convolution2D(64,3,2, activation = 'relu'), # tf.keras.layers.BatchNormalization(), # tf.keras.layers.MaxPooling2D(pool_size=(2,2)), tf.keras.layers.Flatten(), # tf.keras.layers.Dense(500, activation=tf.nn.relu), # tf.keras.layers.Dropout(0.2), # tf.keras.layers.Dense(200, activation=tf.nn.relu), # tf.keras.layers.Dropout(0.2), # tf.keras.layers.Dense(64, activation=tf.nn.relu), # tf.keras.layers.Dropout(0.3), tf.keras.layers.Dense(5, activation=tf.nn.relu, kernel_regularizer = tf.keras.regularizers.l2(0.001)), tf.keras.layers.Dropout(0.3), tf.keras.layers.Dense(5, activation=tf.nn.relu, kernel_regularizer = tf.keras.regularizers.l2(0.001)), tf.keras.layers.Dropout(0.3), # tf.keras.layers.Dense(100, activation=tf.nn.relu, kernel_regularizer = tf.keras.regularizers.l2(0.001)), # tf.keras.layers.Dropout(0.05), tf.keras.layers.Dense(1, activation="linear") ]) model.compile(optimizer=tf.keras.optimizers.RMSprop(0.01), loss = "mean_squared_error", metrics = ["mean_absolute_error", "mean_squared_error"] ) train_ds = train_image_label_ds.cache(filename='./cache.train-data') train_ds = train_ds.apply( tf.data.experimental.shuffle_and_repeat(buffer_size=len(train_images))) train_ds = train_ds.batch(BATCH_SIZE) train_ds = train_ds.prefetch(buffer_size=AUTOTUNE) val_ds = val_image_label_ds.apply( tf.data.experimental.shuffle_and_repeat(buffer_size=len(val_images))) val_ds = val_ds.batch(BATCH_SIZE) val_ds = val_ds.prefetch(buffer_size=AUTOTUNE) print(model.summary()) history = model.fit( train_ds, epochs = 160, validation_data = val_ds, steps_per_epoch = math.ceil(len(train_images)/BATCH_SIZE), validation_steps = math.ceil(len(val_images)/BATCH_SIZE) ) However, no matter what hyperparameters I tuned, like batch size, learning rate, number of layers, number of nodes, even regenerated smaller sample dimension, all comes out bad. The train_loss (mse) has no problem to reduce nicely down to about 300 (still pretty high IMO) after 100 epochs, but the loss is flucturing crazily between 300 to 4M, even near the 160th epochs. My question, is CNN a right approach for this kind of problem? Or the way I feed the input data is wrong? Since CNN is good for extract feature, but mine is not classification or segmentation problem. Thanks. Edit: Simplified the model, with model = tf.keras.models.Sequential([ tf.keras.layers.Convolution2D(16,3,3, input_shape=(img_size, img_size, 3), activation = 'relu'), tf.keras.layers.MaxPooling2D(pool_size=(2,2)), tf.keras.layers.Convolution2D(32,3,3, activation = 'relu'), tf.keras.layers.MaxPooling2D(pool_size=(2,2)), tf.keras.layers.Flatten(), tf.keras.layers.Dense(2, activation=tf.nn.relu), tf.keras.layers.Dropout(0.5), tf.keras.layers.Dense(2, activation=tf.nn.relu), #, kernel_regularizer = tf.keras.regularizers.l2(0.001) tf.keras.layers.Dropout(0.5), tf.keras.layers.Dense(2, activation=tf.nn.relu), #, kernel_regularizer = tf.keras.regularizers.l2(0.001) tf.keras.layers.Dropout(0.5), tf.keras.layers.Dense(2, activation=tf.nn.relu), #, kernel_regularizer = tf.keras.regularizers.l2(0.001) tf.keras.layers.Dense(1, activation="linear") ]) And the training result: I got almost identical train and validation loss. Is this normal? Thanks.
