[site]: crossvalidated
[post_id]: 519652
[parent_id]: 519640
[tags]: 
Gradient boosting in R (xgboost) is widely used to instead of running and combining many random forest models. Your question is not only about how to run models in parallel but also about how to combine different models. One easy way of doing the combination is just majority vote, i.e., treat each forest as a tree and do another level of random forest. This approach is not ideal because the basic idea of the random forest is each tree is overfitting (low bias and high variance), the whole forest will reduce the variance and increase the bias. But if you are doing another level of majority vote, you may further reduce the variance and increase the bias. Which means the final aggregated model may be under-fitting comparing with one random forest in whole data.
