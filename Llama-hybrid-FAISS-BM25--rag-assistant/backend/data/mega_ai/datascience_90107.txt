[site]: datascience
[post_id]: 90107
[parent_id]: 
[tags]: 
Results Random Forest Regression Optimization

I'm going to optimize the hyperparameters of a Random Forest Regressor using scikit-learn and GridSearchCV, with the following code: from sklearn.model_selection import GridSearchCV param_grid = { 'bootstrap': [True], 'max_depth': [30,40,50,60,79], 'min_samples_leaf': [5,10,15,20], 'min_samples_split': [10,12,14,16,20], 'n_estimators': [80,100,110,120] } rf = RandomForestRegressor() grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 10, n_jobs = -1, verbose = 2) grid_search.fit(X_train, y_train) def evaluate(model, test_features, test_labels): predictions = model.predict(test_features) errors = abs(predictions - test_labels) mape = 100 * np.mean(errors / test_labels) accuracy = 100 - mape print('Model Performance') print('Average Error: {:0.4f} degrees.'.format(np.mean(errors))) print('Accuracy = {:0.2f}%.'.format(accuracy)) return accuracy print (grid_search.best_params_) best_grid = grid_search.best_estimator_ grid_accuracy = evaluate(best_grid, X_test, y_test) print (grid_accuracy) The performances after the optimization decrease in terms of RMSE. Basically, just using the standard library call the RMSE is lower than the RMSE with hyperparameters optimization. is it possible?
