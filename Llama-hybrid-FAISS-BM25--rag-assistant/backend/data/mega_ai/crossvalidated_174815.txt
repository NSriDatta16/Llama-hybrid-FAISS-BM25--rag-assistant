[site]: crossvalidated
[post_id]: 174815
[parent_id]: 
[tags]: 
Average duration of a double outage in a system with exponentially-distributed failure and repair times

Assume that we have a system with two units. For each unit, its failure frequency follows an exponential distribution with mean $\lambda_1$ and its repair time follows an exponential distribution with mean $\lambda_2$. In addition, we assume that these units fail indendently. My question is: how do i calculate the average (and ideally the distribution of) time spent in double outage conditions (i.e. both units have failed)? Thank you in advance for your replies.
