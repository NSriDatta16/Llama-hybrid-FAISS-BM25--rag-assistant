[site]: crossvalidated
[post_id]: 588663
[parent_id]: 
[tags]: 
Which metric(s) can I use to evaluate how well a group of binary models agree in their predictions?

My request is best explained with an example. Suppose the upcoming week of (American) football matches are Bills vs Jets Saints vs Falcons Rams vs Lions Packers vs Bears Chiefs vs Chargers Giants vs Cowboys I ask three guys and three girls "who will win?" Their responses are guy 1 : Bills and Rams guy 2 : Jets, Rams, and Giants guy 3 : Rams, Chiefs and Cowboys girl 1 : Jets, Rams, and Giatns girl 2 : Jets, Rams, and Bears girl 3 : Packers and Chiefs I'm looking for a metric that can help me answer the following: How well do the guys agree? How well do the girls agree? Which gender has a higher level of agreement? Notes I do not know wins each game (nor do I care). Each person doesn't predict on the exact same set of games Current Approach My current idea is to implement the following algorithm. Let $P_i$ be the set of predictions for model $i$ . Given $P_1$ , $P_2$ , ..., $P_N$ , For $i$ from 0 to N: Assume $P_i$ is the "source of truth". Now I can calculate $F_i$ as the average F score amongst the other predictors $\{P_k\}_{k \neq i}$ . This tells me how well the other predictors agree with $P_i$ . Finally, I just can just average the F scores (perhaps weighted by sample size). I suspect there's a more elegant approach to this..
