[site]: crossvalidated
[post_id]: 179349
[parent_id]: 
[tags]: 
What are the pros and cons of applying pointwise mutual information on a word cooccurrence matrix before SVD?

One way to generate word embeddings is as follows ( mirror ): Get a corpora, e.g. "I enjoy flying. I like NLP. I like deep learning." Build the word cooccurrence matrix from it: Perform SVD on $X$ , and keep the first $k$ columns of U. Each row of the submatrix $U_{1:|V|,1:k}$ will be the word embedding of the word that the row represents (row 1 = "I", row 2 = "like", â€¦). Between steps 2 and 3, pointwise mutual information is sometimes applied (e.g. A. Herbelot and E.M. Vecchi. 2015. Building a shared world: Mapping distributional to model-theoretic semantic spaces . In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Lisbon, Portugal .). What are the pros and cons of applying pointwise mutual information on a word cooccurrence matrix before SVD?
