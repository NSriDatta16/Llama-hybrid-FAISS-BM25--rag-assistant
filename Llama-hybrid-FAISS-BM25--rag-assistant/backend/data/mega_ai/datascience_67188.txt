[site]: datascience
[post_id]: 67188
[parent_id]: 66913
[tags]: 
Attention weights are learned through backpropagation , just like canonical layer weights. The hard part about attention models is to learn how the math underlying alignment works. Different formulations of attention compute alignment scores in different ways. The main is Bahdanau attention , formulated here . The other is Luong 's, provided in several variants in the original paper . Transformers have several self-attention layers instead (I just found a great exaplanation here ). However, backprop lies at the basis of all them. I know it's amazing how attention alignment scores can improve the performance of our models while using the canonical learning technique intact.
