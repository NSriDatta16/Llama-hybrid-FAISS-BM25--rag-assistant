[site]: datascience
[post_id]: 118096
[parent_id]: 40930
[tags]: 
The token embeddings are not fixed, they are learned. Therefore, during training, the value learned for the token embeddings is intrinsically one that is useful after adding it up with the positional embeddings. Token embeddings are trained precisely based on such a situation and therefore nothing gets "erased", just that the learned vectors are appropriate to be combined with positional embeddings.
