[site]: crossvalidated
[post_id]: 374176
[parent_id]: 
[tags]: 
Precomputed Kernels for Support Vector Machines (SVM)

To calculate the linear kernel matrix for some training matrix X with dimensions n x d where d is the number of features and n is the number of data points, we can simply do: $X * X^T$ . The result is a n x n matrix. However, for some test matrix Y with dimensions n' x d where d is the number of features and n' is the number of data points, we calculate the linear kernel matrix by: $Y * X^T$ . This results in a n' x n matrix. My question is what is the intuition/explanation behind why we calculate the kernel matrix for test data this way. How does this differ from calculating $ Y * Y^T$ ?
