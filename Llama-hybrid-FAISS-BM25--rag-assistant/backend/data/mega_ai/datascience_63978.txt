[site]: datascience
[post_id]: 63978
[parent_id]: 
[tags]: 
considerations on growing number of parameters in a neural network

I have built a convolutional neural network, in particular an AlexNet, and I have noticed that the number of parameters grow a lot as we go forward in the network. Now, I know that the parameters in the network is what we want to learn, and also know that its number grows as we go forward in the network because a summation wth other neurons is performed. To me it seems like a crucial point to learn to unerstand truly how neural networks work, so my question is: What are the implications of this growing number of parameters (if any)? And what considerations is it possible to do on this fact? Thanks in advance. [EDIT]For exampple, if I use an AlexNet, I have the following table: which goes on until the last layer. The parameters in this table should be the weights.
