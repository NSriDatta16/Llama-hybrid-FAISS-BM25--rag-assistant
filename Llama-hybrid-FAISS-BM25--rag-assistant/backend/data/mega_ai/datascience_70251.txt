[site]: datascience
[post_id]: 70251
[parent_id]: 70249
[tags]: 
In short, AdaBoost works in that way that it trains in subsequent iterations and then measures the error of all available weak classifiers. In each subsequent iteration, the "validity" of incorrectly qualified observations is increased, so that classifiers pay more attention to them. So confusion matrix could be shown after each iteration(after 13). In case n_estimators=19 its look like you have the perfect fit so for bigger values of n_estimators model starts to overfit , which gives worse performance. In your case please read about early stopping . That technique help you find you best value of n_estimators . https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_early_stopping.html
