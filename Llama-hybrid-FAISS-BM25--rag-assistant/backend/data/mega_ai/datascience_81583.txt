[site]: datascience
[post_id]: 81583
[parent_id]: 
[tags]: 
Automating the training and deployment of the model as more data becomes available

My question is about automating the training of the model as more data becomes available. In this scenario, I have 1MM items that I split into training and test datasets to train, validate and ultimately deploy the model. As more data becomes available on a daily basis, is it common in real-life projects or even advisable to automate the process above, as to leverage the new data and potentially make the model more accurate?
