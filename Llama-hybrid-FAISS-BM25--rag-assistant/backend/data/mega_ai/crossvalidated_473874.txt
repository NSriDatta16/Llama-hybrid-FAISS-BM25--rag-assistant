[site]: crossvalidated
[post_id]: 473874
[parent_id]: 
[tags]: 
Why is the inverse chi-squared distribution a natural prior and posterior for an unknown variance of a normal distribution?

Wikipedia says [the inverse-chi-squared distribution] arises in Bayesian inference, where it can be used as the prior and posterior distribution for an unknown variance of the normal distribution. Why is this distribution the one to use? EDIT : I know about the computational convenience of conjugate priors. I don't know why inverse chi-squared is a natural one to pick for the unknown variance of the normal distribution. EDIT 2 : Let me give you an example of the type of answer I'm interested in, but with something I understand. We often assume means are normally distributed because the central limit theorem tells us that adding together many independent, identically distributed random variables with finite non-zero variance converges to a normal distribution, and a mean is just a sum of variables (divided by a constant). That gives me some intuition about why people assume means are normally distributed. I have no similar intuition about why the inverse chi-squared distribution would be a natural choice to model the unknown variance of a normal distribution. EDIT 3 : For context, I saw Gelman making this assumption in this paper .
