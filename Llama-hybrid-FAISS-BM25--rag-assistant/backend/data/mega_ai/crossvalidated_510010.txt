[site]: crossvalidated
[post_id]: 510010
[parent_id]: 509633
[tags]: 
Since I have not received any answers to my question, I decided to try some numerical experiments in which I generated a large set (20 million realizations) of simulated measurements, with $f$ ranging from 0 to 1 in increments of 0.01, M ranging from 1 to 200, and 1000 randomly generated samples based on the Python/Numpy statement N = np.sum(np.random.random(M) The values of the true $f$ were averaged to an $M\times N$ array, and the standard deviation was computed in another array. What I found is that, for $N > 3$ and $N \ll M$ , a reasonably unbiased estimate of the true $f$ is given by $$ \hat f \approx \frac{N+1}{M} $$ and the error is $$ \sigma_f = \frac{\sqrt{N+1}}{M} $$ The above formulas appear to be correct to within better than 10% of the true value for $N$ as small as 3-4. Barring a computational error on my part, these results seem good enough to serve my purposes, but I would still welcome a theoretical justification if one exists. EDIT: Subsequent to posting the above, I discovered that there is extensive discussion of this problem here and here . The short version is that there is no single right answer to how to estimate either the fraction $f$ or the confidence interval on $f$ (and none of the formally derived formulas are the same as my empirical expressions above), but the method/formula chosen is subject to various considerations (e.g., the range of $M$ and $N$ ) and tradeoffs in simplicity and accuracy. There is also a python program that calculates the confidence interval here .
