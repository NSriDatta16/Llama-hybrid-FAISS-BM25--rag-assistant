[site]: crossvalidated
[post_id]: 222548
[parent_id]: 222515
[tags]: 
The first $k$ principal components minimize the squared reconstruction error. That is, we project the data onto the first $k$ principal components, then back into the original space to obtain a 'reconstruction' of the data. The first $k$ principal components are the vectors that minimize the sum of squared distances between each point and its reconstruction (the paper below mentions this point, among many other sources). Among all sets of $k$ vectors, the first principal components do not maximize the sum of the variance of the data projected onto each vector. For example, in many cases we could increase the variance by making all vectors point near the direction of the first principal component. But, if we constrain the vectors to be orthogonal (as PCA does), then the first principal components do indeed have this property (e.g. see here ). Another interpretation is that the first $k$ principal components maximize the likelihood of a particular Gaussian latent variable model. See the following paper: Tipping & Bishop (1999). Probabilistic principal component analysis.
