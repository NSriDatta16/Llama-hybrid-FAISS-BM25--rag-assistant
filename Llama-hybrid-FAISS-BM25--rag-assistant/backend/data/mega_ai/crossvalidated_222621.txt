[site]: crossvalidated
[post_id]: 222621
[parent_id]: 222603
[tags]: 
There are a number of reasons training might be taking a long time, but the first reason that comes to mind is that you haven't set an appropriate learning rate. If your learning rate is too high, instead of descending towards a minimum, your gradient path will bounce around uncontrollably/erratically, and this could happen indefinitely. If your learning rate is too low (I've seen this happen less frequently in practice), your model will take a longer time to reach the minimum, but it will eventually arrive. I'm not familiar with the particular libraries you've referenced, so unfortunately I can't offer library-specific details. There is a pretty good general answer (with references) on Stack Overflow about setting a good learning rate in neural networks. See the link below. https://stackoverflow.com/questions/11414374/neural-network-learning-rate-and-batch-weight-update
