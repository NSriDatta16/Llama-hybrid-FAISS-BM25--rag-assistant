[site]: datascience
[post_id]: 82929
[parent_id]: 82922
[tags]: 
Here is a example for what @Christos Karatsalos is describing : If you take a statement like "The cat jumped over the puddle", In CBOW 's case, the neural network's word embeddings are trained by passing a input set of words to the neural network and making the network to predict an output word as shown below: $\{ cat, jumped, over, the, puddle\} \rightarrow The $ $\{ The, jumped, over, the, puddle\} \rightarrow cat $ $\{ The, cat, over, the, puddle\} \rightarrow jumped $ $\{ cat, jumped, the, puddle\} \rightarrow over $ $\{ The, cat, jumped, over, puddle\} \rightarrow the $ $\{ The, cat, jumped, over, the\} \rightarrow puddle $ In skip-gram 's case, the neural network's word embeddings are trained by passing an input word to the neural network and teaching the network to predict a set of words as shown below: $The \rightarrow \{ cat, jumped, over, the, puddle\} $ $cat \rightarrow \{ The, jumped, over, the, puddle\}$ $jumped \rightarrow \{ The, cat, over, the, puddle\}$ $over \rightarrow \{ cat, jumped, the, puddle\}$ $the \rightarrow \{ The, cat, jumped, over, puddle\}$ $puddle \rightarrow \{ The, cat, jumped, over, the\}$ For usage, if you are thinking of a fill-in-the-blanks kind of problem CBOW Word2Vec would be a suitable vector to use, on the other hand if you have a word and you are trying to come up with a new sentence with it then skip-gram Word2Vec will be useful. Take a look this lecture notes for more information.
