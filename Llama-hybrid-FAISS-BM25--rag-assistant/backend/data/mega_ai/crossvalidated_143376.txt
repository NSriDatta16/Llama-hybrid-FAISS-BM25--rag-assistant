[site]: crossvalidated
[post_id]: 143376
[parent_id]: 143246
[tags]: 
It is instructive to consider a simpler case of PCA first. Given data matrix $\mathbf X$, PCA finds a direction $\mathbf w$ such that the variance of the projection is maximized: $$\mathbf w = \mathrm{argmax}\; \mathrm{Var}(\mathbf X \mathbf w).$$ To be able to search for the optimal direction, one needs to fix the length of $\mathbf w$, otherwise the variance can be made arbitrarily large by increasing its length. Constraining the length of $\mathbf w$ to any number $\alpha$ would work fine, but it is particularly convenient to choose $\|\mathbf w\|=1$, because only then is $\mathbf X \mathbf w$ a projection on the direction of $\mathbf w$. See this thread for more details: Why is the eigenvector in PCA taken to be unit norm? Now turning to PLS, we will see that the situation is exactly the same. Given data matrix $\mathbf X$ and a response variable $\mathbf y$, PLS looks for a direction $\mathbf w$ such that the covariance of $\mathbf y$ and the projection of $\mathbf X$ onto $\mathbf w$ is maximized: $$\mathbf w = \mathrm{argmax}\; \mathrm{Cov}(\mathbf X \mathbf w, \mathbf y).$$ Again, for this formulation to make sense, one needs to fix the length of $\mathbf w$ and it is convenient to fix it to $1$.
