[site]: crossvalidated
[post_id]: 611635
[parent_id]: 588998
[tags]: 
This is an important and surprisingly difficult question. Two of the reasons why it's hard: One way that classifiers work is by constructing directions in feature space that separate the classes you are interested in. If you train it on dogs and horses, it might learn that horses are (a) bigger and (b) tend to be less hairy. Along that direction in feature space you find buses. A bus is even bigger and less hairy than a horse, so it is super-horse-like and your classifier will be extremely confident that it's a horse. Another way that classifiers work is by taking the test data and finding 'nearby' points in the training set. You might say that a test point should be classified as 'other' if there are no nearby points. This works when you have low-dimensional data. For high-dimensional data, though, there are no points that are 'nearby' on all variables. If you have a 'nearby' decision rule, it has to work by ignoring/downweighting many of the variables, so a new test point can appear 'nearby' even if it's very different on those ignored/downweighted variables. As further evidence that it's a hard problem, people overgeneralise learned decision rules. They don't mistake a skyscraper for a dog, but they do (for example) learn the features that distinguish edible and inedible mushrooms in their home region and then move somewhere else and tragically fail to recognise that the distinguishing features are different in their new home.
