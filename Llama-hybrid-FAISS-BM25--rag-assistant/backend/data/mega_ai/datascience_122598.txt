[site]: datascience
[post_id]: 122598
[parent_id]: 
[tags]: 
Overfitting on implemented Dense-Net architecture

I have been playing with different architectures and see how they would perform on the quick draw dataset. Even though the accuracy is significantly higher, I can't reduce overfitting no matter what I do. Even with data augmentations, the model is still overfitting. I suspect it might be because of the data itself, but maybe I am wrong. Here is my code to get the data and load the data: import quickdraw as qd import tensorflow as tf import tensorflow.keras.layers as layers import tensorflow.keras.activations as activations from tensorflow.keras.regularizers import l2 import os import datetime import numpy as np import matplotlib.pyplot as plt import seaborn as sns import pandas as pd import pathlib import math %load_ext tensorboard Some parameters n_classes = 100 input_shape = (128, 128, 1) img_size = (128, 128) max_drawings = 1200 batch_size = 256 dataset_dir = "/content/dataset" Load data code def load_image_classes(name, max_dws=1200, recognized=True): directory = pathlib.Path("dataset/" + name) if not directory.exists(): directory.mkdir(parents=True) images = qd.QuickDrawDataGroup(name, max_drawings=max_dws, recognized=True) for img in images.drawings: filepath = directory.as_posix() + "/" + str(img.key_id) + ".png" img.get_image(stroke_width=3).resize(img_size).save(filepath) for name in qd.QuickDrawData().drawing_names[:n_classes]: load_image_classes(name=name, max_dws=max_drawings, recognized=True) Load data train_data = tf.keras.utils.image_dataset_from_directory( dataset_dir, validation_split=0.2, subset="training", seed=42, color_mode="grayscale", image_size=img_size, batch_size=batch_size ) test_data = tf.keras.utils.image_dataset_from_directory( dataset_dir, validation_split=0.2, subset="validation", seed=42, color_mode="grayscale", image_size=img_size, batch_size=batch_size ) Dense net implementation def dense_conv(x, filter, kernel=1, s=1, p='same'): x = layers.BatchNormalization()(x) x = layers.Activation('relu')(x) x = layers.Conv2D(filter, kernel, strides=s, padding=p)(x) return x def dense_block(x, filter, repetition): for i in range(repetition): y = dense_conv(x, filter=filter*4, kernel=1) y = dense_conv(y, filter=filter, kernel=3) x = layers.concatenate([y, x]) return x def transition_layer(x): x = dense_conv(x, tf.keras.backend.int_shape(x)[-1] //2, kernel=1) x = layers.AveragePooling2D(2, strides=2, padding='same')(x) return x def construct_densenet121(filter): input_im = layers.Input(shape=input_shape) x = layers.RandomRotation(0.2)(input_im) x = layers.RandomFlip('horizontal_and_vertical')(x) # Stage 1 x = layers.Conv2D(64, 7, strides=2, padding='same')(x) x = layers.MaxPool2D(3, strides=2, padding='same')(x) # Stage 2 for repetition in [6, 12, 24, 16]: x = dense_block(x=x, filter=filter, repetition=repetition) x = transition_layer(x) # Stage 3: Classification x = layers.GlobalAveragePooling2D()(x) x = layers.Dense(n_classes, activation='softmax')(x) model = tf.keras.models.Model(inputs=input_im, outputs=x, name='DenseNet121') return model densenet121 = construct_densenet121(32) densenet121.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy']) epochs = 14 tf.random.set_seed(42) logdir = os.path.join("logs", datetime.datetime.now().strftime("%Y%m%d-%H%M%S")) tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1) densenet121.fit(train_data, validation_data=test_data, epochs=epochs, callbacks=[tensorboard_callback]) This is the training process: Epoch 1/14 625/625 [==============================] - 687s 908ms/step - loss: 2.4456 - accuracy: 0.3724 - val_loss: 15.6159 - val_accuracy: 0.0206 Epoch 2/14 625/625 [==============================] - 571s 912ms/step - loss: 1.3466 - accuracy: 0.6168 - val_loss: 29.3167 - val_accuracy: 0.0134 Epoch 3/14 625/625 [==============================] - 563s 900ms/step - loss: 1.0751 - accuracy: 0.6876 - val_loss: 15.2280 - val_accuracy: 0.0292 Epoch 4/14 625/625 [==============================] - 563s 900ms/step - loss: 0.9426 - accuracy: 0.7224 - val_loss: 17.8689 - val_accuracy: 0.0376 ```
