[site]: datascience
[post_id]: 62185
[parent_id]: 33737
[tags]: 
The main problem with oversampling before the split is that the reported score is optimistically biased (in your experiment, quite heavily!). The resulting model isn't necessarily bad for future purposes, just probably not nearly as good as you think. (N.B. the scores used to pick best hyperparameters are no longer unbiased estimators of performance either.) Now with hyperparameter tuning in the mix, which is looking at those scores, you might end up with a set of hyperparameters that helps the model overfit on the duplicated rows, so your resulting model might suffer in future performance for this reason. However, in your experiment you only look for number of trees in a random forest, which has little effect on the final performance (just reducing the variance due to the random row/column sampling). So similar test set performances are not unexpected here.
