[site]: crossvalidated
[post_id]: 519957
[parent_id]: 519944
[tags]: 
I'm going to take the opportunity to put my foot in my mouth. What I'm about to say might be wrong, but the easiest way to learn the right answer is to be wrong on the internet. The Gauss-Markov theorem states that the errors in OLS need not be normal -- hell, they need not even be iid -- in order for OLS to have minimum variance among the class of linear unbiased estimators. That does not mean we do not make the Gaussian assumption , it only means that the Gaussian assumption is not critical for our estimates of the conditional mean. We make the Gaussian assumption implicitly by minimizing the squared errors. Indeed, maximizing the Gaussian log likelihood is the same as minimizing the sum of squares, so in my mind whether or not you intend to make the Gaussian assumption is irrelevant; you end up making it anyway and enjoy the results of Gauss-Markov nonetheless. Now, as for your question, I believe that you require a likelihood for Bayesian analyses. The bayesian perspective puts heavy emphasis on the data generating processes, and that means the likelihood plays a fairly large role (never mind that most sampling techniques require it as well). That being said, you are free to change that likelihood to relax the assumption of normality. One way to do this would be to assume a student t likelihood and estimate the degrees of freedom. You can very easily change the likelihood in libraries like brms, pymc3, and Stan. Here is a small example I wrote in Stan which uses the student t likelihood. library(cmdstanr) library(tidybayes) n = 1000 x = rnorm(n) y = 2*x + 1 + rt(n, 20) model_data = list(n=n, x=x, y=y) model_code = ' data{ int n; vector[n] y; vector[n] x; } parameters{ real b1; real b0; real dof; real sigma; } model{ b1 ~ normal(0, 1); b0 ~ normal(0,1); dof ~ cauchy(0,1); sigma ~ gamma(2,2); y ~ student_t(dof, b0 + b1*x, sigma); } generated quantities{ real y_ppc[n] = student_t_rng(dof, b0 + b1*x, sigma); } ' model_file = write_stan_file(model_code) model = cmdstan_model(model_file) fit = model$sample(model_data, adapt_delta = 0.99, parallel_chains=4) In summation, Gauss Markov requires that the errors be homoskedastic and have mean 0. That means that any distribution which satisfies those should work (in principle). The difficulty is then picking one and determining if that is a sufficiently good likelihood for your problem.
