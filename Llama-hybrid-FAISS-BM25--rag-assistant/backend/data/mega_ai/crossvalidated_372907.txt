[site]: crossvalidated
[post_id]: 372907
[parent_id]: 
[tags]: 
Trouble understanding value iteration

I have trouble understanding how the value iteration algorithm for MDP:s work. I'm trying to follow the canonical grid world example (slide 17), but I don't get the correct results. Here's my work: Initially I set the value function to 0 everywhere. So the matrix representation of each state's utility becomes: $$ V_0 = \begin{pmatrix} 0 & 0 & 0 & 0\\ 0 & -999 & 0 & 0\\ 0 & 0 & 0 & 0 \end{pmatrix} $$ $-999$ is arbitrarily choosen as a dummy value for the square that can't be entered. Then I use the Bellman update to calculate values for the next iteration, $$ V_{t+1}(s) = R(s) + \max_a\sum_{s'}P(s'|a, s)V_t(s'), $$ where $P(s'|a, s)$ is the probability of reaching state $s'$ given current state $s$ and action $a$ . Using the values found in the PDF, the utility for square at row 1 column 4 at time step 1 is calculated to $$ V_1((1, 4)) = 1 + \max_a\sum_{s'}P(s'|a, (1,4))V_0(s') = 1. $$ $V_1((2, 4)) = -1$ is calculated similarily, yielding the matrix $$ V_1 = \begin{pmatrix} 0 & 0 & 0 & 1\\ 0 & -999 & 0 & -1\\ 0 & 0 & 0 & 0 \end{pmatrix}. $$ I again use the Bellman update to calculate $V_2((1,4))$ $$ V_2((1,4)) = R((1,4)) + \max_a\sum_{s'}P(s'|a, (1,4))V_1(s')\\ = 1 + \max_a\sum_{s'}P(s'|a, (1,4))V_1(s'). $$ $a = \mathrm{n}$ (as in go "north") is obviously the action that maximizes the expression. Then $P((1,4)|\mathrm{n},(1,4)) = 0.9$ and $P((1,3)|\mathrm{n},(1,4)) = 0.1$ and I get: $$ V_2((1,4)) = 1 + P((1,4)|\mathrm{n}, (1,4))V_1((1,4)) + P((1,3)|\mathrm{n}, (1,4))V_1((1,3))\\ = 1 + 0.9\cdot1 + 0.1\cdot0 = 1.9 $$ But this result is not correct the value should remain at 1. What am I doing wrong?
