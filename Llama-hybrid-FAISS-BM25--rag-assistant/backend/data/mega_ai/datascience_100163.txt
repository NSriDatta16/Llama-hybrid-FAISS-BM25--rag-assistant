[site]: datascience
[post_id]: 100163
[parent_id]: 
[tags]: 
Detecting a Piecewise, Noisy, Linear Signal, with Constant Slope and Changing Y-Intercepts

I am trying to algorithmically detect a 2D linear signal under some noisy data. It is almost a textbook candidate for Robust Linear Regression, except for the fact that, while the slope remains constant, the y-intercept of the underlying signal changes on average every 1 million samples, although has a non-zero probability to change twice in two sample. Here is some python code that generates and plots mock data that satisfies my constraints: from matplotlib import pyplot as plt import numpy as np import random # Seed for repeatable code random.seed(420) def generate_mock_data(): num_values = 10 ** 5 slope = 0.314 y_intercept = 100 background_noise_amplitude = 2.5 min_b, max_b = -300, 300 y_min, y_max = 0, int(num_values * slope) probability_noise = 0.25 chance_of_large_shift = 10 ** -3 x = np.arange(num_values) y = np.zeros(num_values, dtype=np.float64) for i in range(num_values): if random.random() Here's a zoomed in view of the above code's plot where you can visually see the signal in question: What I'm looking for is an algorithm (or at least advice on how to tackle creating one), ML or otherwise, that can identify the break points in the signal, calculate the underlying slope of the entire graph, and the y-intercepts of each segment. I've tried detecting the borders of the segments with a rolling moving average and then applying a Robust Linear Regression algorithm to each segment, however, the boarders are not precise due to the noise, and small segments sometimes fail to find a signal under the noise. But perhaps the problem can simplified to finding the boarders of the change in y-intercept, calculating the slope from a large segment (or a few large segments), and then applying a type of Robust Linear Regression over each segment, with the slope already constrained.
