[site]: datascience
[post_id]: 60290
[parent_id]: 
[tags]: 
How to justify the usage of 200 dimensions in word vectors instead of the 300 dimensions?

When employing machine learning methods in NLP, most of studies use 200 or 300 dimensional vectors. 300 dimensional embeddings carry more information and this, therefore, is considered to produce better performance results in general. If you have unlimited computational resources and the training time is not a problem for you, when does it make sense to use 200 dimensional embeddings instead of 300 dimensional vectors in a classification problem (e.g. in sentiment analysis) and why? I am assuming that you are using off-the-shelf word2vec, GloVe, or other pretrained vectors. That is, the vectors are not being learnt from scratch in your classification task.
