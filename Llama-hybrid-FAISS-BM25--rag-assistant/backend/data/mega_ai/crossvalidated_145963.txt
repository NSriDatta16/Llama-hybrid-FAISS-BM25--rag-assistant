[site]: crossvalidated
[post_id]: 145963
[parent_id]: 144770
[tags]: 
Generally the dimensionality of the problem is, as you suspected, equal to the number of inputs ( also known as, features, measurement variables ). So in the NN model, that would be the number of nodes in the input layer. There may be unmeasured features from the problem, but normally dimensionality only refers to the measurements you have. You may synthesise extra features from the ones you have, perhaps choosing to square one of the existing features, to make a new one, if you think this might be helpful. That would add one extra feature to the dimensionality. On a separate point, if you are looking at SVMs you may encounter something called the Vapnikâ€“Chervonenkis dimension ( VC dimension ). This is generalised concept which refers to the 'power', or expressiveness, of a learning algorithm. It is based on the number of points that an algorithm, with a certain set of parameters, can 'shatter" ( separate ). It is not directly related to the dimensionality of the learning problem.
