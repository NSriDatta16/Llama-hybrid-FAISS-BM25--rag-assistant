[site]: datascience
[post_id]: 25699
[parent_id]: 25693
[tags]: 
We use freezing to employ transfer learning . Deep learning has a great hunger for data. In some tasks you may not have so much data, but there may already be a pre-trained network that can be helpful. In such cases you use the model and its weights and by replacing the soft-max layer, in situations where you have small amount of data, you try to customize the network for you specific task. If you have more data, the more number of layers can be trainable. Take a look at here
