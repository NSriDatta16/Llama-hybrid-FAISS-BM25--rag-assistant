[site]: datascience
[post_id]: 73741
[parent_id]: 
[tags]: 
Scaling monotonically increasing features between 0 and 1

To keep the test set blind to the neural network algorithm it is generally better to build a scaler based on the training set and then scale the test set on this scaler. I am building an LSTM for predicting a somewhat monotonically increasing price chart. I am not shuffling the data before splitting into training and testing since the sequence is important. This will mean that the test set can have values higher than 1 (say 1.2, 1.01 etc...) Would this be a problem to the neural network? Is it important that every input to the neural network be less than zero if that is what it was trained with?
