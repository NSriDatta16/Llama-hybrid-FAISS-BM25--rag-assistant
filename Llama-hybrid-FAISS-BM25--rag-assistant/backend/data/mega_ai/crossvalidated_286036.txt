[site]: crossvalidated
[post_id]: 286036
[parent_id]: 286014
[tags]: 
When the machine learning problem is prediction, then there can be an automated accuracy/precision measurement. However, in many other problems, you need to compare the machine's choice $m$ against human choice $h$. A basic accuracy score on a test set of $N$ elements is naturally percentage of match : $$score = \frac{1}{N}\sum_{i=1}^N 1_{m(i)=h(i)}$$ Measuring accuracy of non-supervized learning is a bit tricky and there is no definition or solution out of the box: the machine invents its own categories, you invent yours. They may not be the same, it does not mean the machine's choice is not as meaningful as yours. My team did a certain work : categorization of texts into "topics". First we created a small dataset with 5 very different topics chosen by us. We said the algo was perfectly accurate if it divided the texts into the same topics as we did, and agreed for each text. Actually, since the topics were clearly different, accuracy was close to 98% But in real life datasets, when you don't have clearly non-overlaping categories, things are not so simple. Deciding to divide according to politics/economics/people may be as "good" as dividing into employment/elections/cinema for example. We found a way to overcome this problem with a sort of semi-supervision. But it's not a universal solution. We decided of a predefined set of 24 categories : "art&entertainment", "sports"... and ran a k-means with 100 clusters : more than the categories. We had actually many texts : > 100 000 so that 100 clusters was still a small number. Then we decided to relate each cluster to categories (thanks to "important" words in the cluster for example) manually: cluster 1 (car, race, schumacher...) : "automotive" cluster 2 (audi, bmv, nissan...) : "automotive" cluster 3 (funk, jazz, concert...) : "art&entertainment" cluster 4 (dali, picasso, louvres...) : "art&entertainment" ... So the predicted category was obtained in two steps : text ---(automated)---> cluster ----(manual)---> category Then we took a test set of say 1000 texts and chose the category of each text by reading the text. We compared the predicted category (closest center -> category) versus our own human choice and calculated a score : match percentage.
