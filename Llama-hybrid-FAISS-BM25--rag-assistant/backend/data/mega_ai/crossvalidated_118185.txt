[site]: crossvalidated
[post_id]: 118185
[parent_id]: 118078
[tags]: 
You will need to look at the results in lmProfile$variables . That has the variable importance values per resample and subset size. For your example, using random forest (not linear models), I used rfFuncs which is faster since it doesn't tune over mtry : set.seed(10) ctrl The data look like: > head(rfProfile$variables) Overall var Variables Resample selectedVars.real4 18.721549 real4 50 Fold01.Rep1 selectedVars.real2 10.917085 real2 50 Fold01.Rep1 selectedVars.real5 10.305418 real5 50 Fold01.Rep1 selectedVars.real1 8.813745 real1 50 Fold01.Rep1 selectedVars.bogus17 3.932762 bogus17 50 Fold01.Rep1 selectedVars.bogus44 2.384077 bogus44 50 Fold01.Rep1 You can get the rate that each predictor was selected at each iteration: library(plyr) selected For example: > subset(selected, Variables == 5) Variables Var1 Freq Prob 14 5 bogus14 4 0.08 15 5 bogus17 35 0.70 16 5 bogus2 1 0.02 17 5 bogus26 1 0.02 18 5 bogus44 7 0.14 19 5 real1 50 1.00 20 5 real2 50 1.00 21 5 real3 2 0.04 22 5 real4 50 1.00 23 5 real5 50 1.00 These are the variables that were selected when 5 predictors are retained over the 50 resamples. real1 , real2 , real4 and real5 were all selected each time. You can also get summary statistics for the Overall variable importance score for each predictor also. Max
