[site]: crossvalidated
[post_id]: 527010
[parent_id]: 287172
[tags]: 
People tend not to take an ML approach to stemming, because it’s meant to be a cheap and quick hack for text normalization. Why throw a complex model at something that can be handled by a few well-chosen string operations? In other words, you probably want to stick to the Porter or Snowball stemmers. Not everything requires (or benefits) from ML. But for completeness, here are other options. Lemmatization By contrast, lemmatization is well defined and principled. The goal is to convert words into their citation form (“lemma”). This is better defined cross-lingually than stemming is. Both are forms of text normalization. But (in part because this also has value to computational linguists) lemmatization garners more attention. Modeling Either one is easy to operationalize, if you have a training set of (word, normalized) pairs. You can use any off-the-shelf model of $p(y \mid x)$ where $x$ and $y$ are string-valued random variables. You then seek the argmax of that expression over all $y$ s for your word $x$ . Two major directions for this string-to-string modeling are statistical modeling using Bayes’s rule (e.g. EGYPT, GIZA++) and neural network “sequence-to-sequence” models that model the direct probability. These approaches both stemmed from the machine translation literature, but they can apply to stemming and lemmatization if you treat the input and output “sentences” as the sequence of letters in your input and output words.
