[site]: datascience
[post_id]: 97171
[parent_id]: 96904
[tags]: 
I was able to write a generator but had to mention the output shape def padded_tokenizer(text, max_len=80): result = dict(tokenizer(text, truncation=True, padding=True, return_tensors='tf', pad_to_multiple_of=2)) for key in result.keys(): result[key] = tf.keras.preprocessing.sequence.pad_sequences(result[key], padding="post", maxlen=max_len) return result def _generator(arg=0, batch_size=1): if arg==0: text=train_texts label=Y_oh_train if arg==1: text=val_texts label=Y_oh_val else: text=test_texts label=Y_oh_test # batch_size=1 # label = tf.ragged.constant(label) while True: for i in range(0,len(text),batch_size): yield padded_tokenizer(text[i:i+batch_size]), label[i:i+batch_size] train_dataset = tf.data.Dataset.from_generator(_generator, output_signature=({'input_ids':tf.TensorSpec(shape=(None,80), dtype=tf.int32), 'attention_mask':tf.TensorSpec(shape=(None,80), dtype=tf.int32)}, tf.TensorSpec(shape=(None,49), dtype=tf.int32)), args=([0,16]) ) It seems the signature has to be mentioned if output is generated from a generator, but the speed is too slow, compared to loading everything in memory(for obv reasons) Still would appreciate a faster solution
