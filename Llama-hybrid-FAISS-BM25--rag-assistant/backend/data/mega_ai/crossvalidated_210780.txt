[site]: crossvalidated
[post_id]: 210780
[parent_id]: 210741
[tags]: 
The example by Glen_b makes the point that yes you can indeed implement a Gibbs sampler where one component is discrete and the other is continuous. I present an example where you need to resort to Gibbs sampling to draw from the posterior. Consider the Bayesian variable selection model of George and McCulloch . They present the following Bayesian model \begin{align*} Y|X,\beta, \sigma^2 &\sim N(X\beta, \sigma^2I)\\ \beta_i|\gamma_i = 1 &\sim N(0, c_i^2\tau_i^2)\\ \beta_i| \gamma_i = 0 &\sim N(0, \tau^2_l)\\ \sigma^2 &\sim \text{Inverse Gamma}(v/2, v \lambda/2)\\ P(\gamma_i = 1) &= 1 - P(\gamma_i = 0) = p_i. \end{align*} Here, $\tau_i^2, c_i^2, \tau_l^2 v, \lambda$ are fixed. The parameters to be estimated are $\gamma$ which are discrete and $\beta$ and $\sigma^2$. The posterior distribution is intractable, so we resort to MCMC techniques such as Gibbs sampling. As you mentioned, for that we need the full conditionals. The paper provides the following conditionals. Let $A = \left(\sigma^{-2}X^TX + D^{-2} \right)^{-1}$, where $D$ is a diagonal matrix dependent on $\gamma$. \begin{align*} \beta|\gamma, \sigma^2 & \sim N_p(\sigma^{-2}AX^Ty, A)\\ \sigma^2 |\beta, \gamma & \sim \text{Inverse Gamma}\left(\dfrac{n+v}{2}, \dfrac{(Y - X\beta)^T(y - X\beta) + v\lambda}{2} \right)\\ P(\gamma_i = 1|\beta, \sigma^2) & = \dfrac{p_i f(\beta|\sigma^2, \gamma_i = 1)f(\sigma^2|\gamma_i = 1)}{p_i f(\beta|\sigma^2, \gamma_i = 1)f(\sigma^2|\gamma_i = 1) + (1-p_i)f(\beta|\sigma^2, \gamma_i = 0)f(\sigma^2|\gamma_i = 0)} \end{align*} Notice that the conditional of $\gamma$ looks complicated but is not too bad if you plug in the distributions. One run through this Gibbs sampler, updates $\beta$ from a normal, $\sigma^2$ from an Inverse Gamma and $\gamma$ by tossing a coin with the probability indicated.
