[site]: crossvalidated
[post_id]: 230805
[parent_id]: 230676
[tags]: 
Here is a "first stab". Standard disclaimers apply. Here is a simulation: #make data #reproducibility set.seed(230676) #rows of data N My results for the "summary" are: > summary(est1) Call: lm(formula = GDP ~ chairs + honey + stock) Residuals: Min 1Q Median 3Q Max -8079 -1920 163 1799 6914 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 8574.1175 3353.9718 2.556 0.0126 * chairs -208.0676 95.3243 -2.183 0.0321 * honey -62.6255 3264.2498 -0.019 0.9847 stock 1.0186 0.0241 42.267 So the 'R' tool gives some measure of "importance" through "t-value", Pr(>|t|), Residual standard error, Multiple R^2, F-statistic, and p-value. Those are mouthfuls, but each has a meaning. Study them to get a sense of what a "101" sort of tool primarily developed by statistics grad students think is important. The R^2 around 96% suggests that when you use this linear model and compare the errors to a model comprised of only a single constant (the average), then this linear model has typical error that is about 25x smaller than the typical error for using only the average. Also - there was something hidden inside the "stock". That added constant became an "intercept". Try changing the stock so that it does not have 10,000 added to it, and see what that does to the coefficients. If you add "+0" to the linear model it changes how this is handled. One good lesson here is to be careful about how you specify your model because unexpected things can come out of tools if you aren't careful. Another way to look at the data is EDA ( NIST link ). One very basic approach is an array of scatter plots , one for each interaction. Here is the code: #load the library (you may have to install it) library(psych) #make the plot (using data above) pairs.panels(data.frame(GDP,chairs,honey,stock)) My result from this is: The left column of sub-plots shows the interaction of GDP with the various inputs. You can see there is a stronger linear relation with "stock" in the lower left corner. The red line is a line and the correlation coefficient (upper right corner) is also very high. One thing you see here is that when the coefficient of an effect, like chairs, is small then noises can wash it out. We know it is really in there somewhere, but the "epsilon", the real world random noise, washed it out. This is really a very early starting point for looking at the relationship between inputs and outputs. So here is use of AIC or sample size corrected AICc to pick the better model given constant input data. GDP2 For this my result is: > print(c(AIC1,AICc1)) [1] 1265.751 1266.067 > print(c(AIC2,AICc2)) [1] 1622.278 1622.330 Both AIC1 and AICC1 are lower than AIC2 and AICc2 respectively, therefore I would suggest that the model 1 (the least squares fit) is initially preferred. There are truckloads of disclaimers, but here is a decent starter reference for AIC or AICc use in this case. ( link1 , link2 ) There is a great point on page 25 of link1, you should check that out.
