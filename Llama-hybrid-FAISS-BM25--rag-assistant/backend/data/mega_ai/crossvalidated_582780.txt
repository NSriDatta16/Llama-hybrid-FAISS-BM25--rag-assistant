[site]: crossvalidated
[post_id]: 582780
[parent_id]: 582652
[tags]: 
The other answer here covers the specific transformation issues you have asked about, so I'll focus solely on the intuition of the LASSO regression. One useful way to look at LASSO regression is that it is equivalent to Bayesian maximum-posterior (MAP) estimation when you use an IID Laplace prior for the coefficient vector. This prior has density given by: $$\pi(\boldsymbol{\beta}|\lambda) = \prod_{i=1}^p \text{Laplace} \Big( \beta_i \Big| 0, \frac{2}{\lambda} \Big) = \prod_{i=1}^p \frac{\lambda}{4} \cdot \exp \bigg( -\frac{\lambda}{2} |\beta_i| \bigg),$$ Taking $\text{RSS}_{\mathbf{x}, \mathbf{y}}(\boldsymbol{\beta})$ as the residual-sum-of-squares (taken as a function of the coefficient vector), the log-likelihood for the regression model is: $$\ell_{\mathbf{x}, \mathbf{y}}(\boldsymbol{\beta}) = \text{const} - \frac{1}{2} \cdot \text{RSS}_{\mathbf{x}, \mathbf{y}}(\boldsymbol{\beta}).$$ Consequently, the MAP estimator is is the argument that maximises the log-posterior for the model, which is: $$\begin{align} \hat{\boldsymbol{\beta}}_\text{MAP} &= \underset{\boldsymbol{\beta}}{\text{arg max}} \log p(\boldsymbol{\beta}|\mathbf{x}, \mathbf{y}, \lambda) \\[6pt] &= \underset{\boldsymbol{\beta}}{\text{arg max}} \bigg[ \ell_{\mathbf{x}, \mathbf{y}}(\boldsymbol{\beta}) + \log \pi(\boldsymbol{\beta}|\lambda) \bigg] \\[6pt] &= \underset{\boldsymbol{\beta}}{\text{arg max}} \bigg[ \text{const} - \frac{1}{2} \cdot \text{RSS}_{\mathbf{x}, \mathbf{y}}(\boldsymbol{\beta}) - \frac{\lambda}{2} \sum_{i=1}^p |\beta_i| \bigg] \\[6pt] &= \underset{\boldsymbol{\beta}}{\text{arg max}} \bigg[ - \frac{1}{2} \cdot \text{RSS}_{\mathbf{x}, \mathbf{y}}(\boldsymbol{\beta}) - \frac{\lambda}{2} \sum_{i=1}^p |\beta_i| \bigg] \\[6pt] &= \underset{\boldsymbol{\beta}}{\text{arg min}} \bigg[ \text{RSS}_{\mathbf{x}, \mathbf{y}}(\boldsymbol{\beta}) + \lambda \sum_{i=1}^p |\beta_i| \bigg] \\[6pt] \end{align}$$ If you have a look at the Laplace distribution you will see that, in this case, it gives a density with a peak at $\beta_i=0$ and with exponential decay in prior density as we move away from this point in either direction. The estimator above is the one that maximises the posterior density under this prior. You can read more about this in this related question .
