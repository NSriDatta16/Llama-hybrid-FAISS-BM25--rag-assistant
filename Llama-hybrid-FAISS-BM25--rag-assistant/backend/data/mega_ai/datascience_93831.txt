[site]: datascience
[post_id]: 93831
[parent_id]: 93801
[tags]: 
Based on a quick read of the paper linked in the comment: I just don't understand if your model (BERT or not) is only trained on these weak labels then you are treating them as "ground-truth", Correct, but only for the training: the model is trained to recognize the labels obtained with a "quick and dirty" method. and more importantly, don't you already know how to create "ground-truth" (by the rules-based system) ??? What's the point of the 2nd step? No, because the real ground truth they are interested in is not those from the "quick and dirty" method. If they were, it would indeed be sufficient to run their rule-based system. The goal is to predict the labels obtained in what the authors call the "Gold Standard Corpus", which was manually annotated and never seen by the model. Typically the quick and dirty method will result in some classification errors. The point of tuning the model with these labels is to see if the model can extrapolate from these low-quality labels to high-quality labels. This ability to generalize beyond the training data is based on the underlying semantic information contained in the original BERT-like model. For example this model might be able to associate a specific sport like swimming with "physical activity", even though the weak supervision doesn't contain this association.
