[site]: crossvalidated
[post_id]: 323591
[parent_id]: 
[tags]: 
Variance of n discrete time series

I'm currently having problems understanding how to calculate the variance between n given time series, each containing data of exactly one day. So basically, I'm given one array containing double values (which have at most 4 decimal places) from a specified time interval (granularity of measurements is 30 minutes, which means there are 48 samples a day, defining a time series). Since the variance needs the probability for each value that may occur (the values reach from 0 to mostly 10, even though all values between 0 to infinity COULD appear, the probability is just very (very) low that the value is greater than 10). So now I want to ask how to calculate these probabilities for each value, or is there a approximative way (or is it even necessary?). I'm (obviously) not that advanced in statistics, it is just important in context of a software project. Thanks for any suggestions or answers in advance.
