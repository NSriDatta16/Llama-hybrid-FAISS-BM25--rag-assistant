[site]: stackoverflow
[post_id]: 1163483
[parent_id]: 736514
[tags]: 
For your immediate concern: higher values mean the variables are more important. This should be true for all the measures you mention. Random forests give you pretty complex models, so it can be tricky to interpret the importance measures. If you want to easily understand what your variables are doing, don't use RFs. Use linear models or a (non-ensemble) decision tree instead. You said: An explanation that uses the words 'error', 'summation', or 'permutated' would be less helpful then a simpler explanation that didn't involve any discussion of how random forests works. It's going to be awfully tough to explain much more than the above unless you dig in and learn what about random forests. I assume you're complaining about either the manual, or the section from Breiman's manual: http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#varimp To figure out how important a variable is, they fill it with random junk ("permute" it), then see how much predictive accuracy decreases. MeanDecreaseAccuracy and MeanDecreaseGini work this way. I'm not sure what the raw importance scores are.
