[site]: crossvalidated
[post_id]: 232765
[parent_id]: 232755
[tags]: 
Here's an example that should be in everyone's toolbox regarding regression/decision trees, as it succinctly makes two very important points. The vertical axis shows a response $y$. There are two predictor variables, $x_1$ ranges from $-1$ to $1$ continuously. The relationship between $y$ and $x_1$ is constructed so that no matter how you partition the $x_1$ axis, the response always averages out to zero. In particular: > cor(df$x_2, df$y) [1] -0.001792121 Or, for all intents and purposes, zero. The other variable $x_2$ allows you to distinguish the "arm" of the $X$ shaped data. This data has two very interesting features: The true or correct decision tree for this data splits on $x_2$ first. This allows it to distinguish the arms of the $X$ shape, and immediately breaks the zero correlation structure between $x_1$ and $y$ for all sub-splits. A greedy algorithm will generally not find the optimal fist split (you could get very lucky, but probably it will find some noisy split on $x_1$)! The first split on $x_2$ does not lead to a reduction in squared error, it is only important because it lets later splits focus on the association between $x_1$ and $y$. Here is the R code I used to make this plot, so you can experiment yourself df
