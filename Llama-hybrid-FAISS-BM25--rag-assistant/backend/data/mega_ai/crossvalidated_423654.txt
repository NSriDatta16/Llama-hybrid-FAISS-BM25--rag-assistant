[site]: crossvalidated
[post_id]: 423654
[parent_id]: 
[tags]: 
Tossing a bayesian coin 1000 times and get P(450 heads)

I have a problem with a coin toss. The exercise is following: You toss a coin 10 times and it gives 4 heads. What fee would you pay to play a game where you win 1$ if you flip at least 450 heads out of 1000 coin tosses? My approach: Update an informative prior with the data. Then calculate P(450H). P(450H) = Fee. x := obtaining a head Prior: assume the belief that pdf(x) follows Beta(20, 20) Why Beta(20,20)? I want to build a prior with the belief that a head should very probably come with a probability of 0.5... Hence a = b. However, I want the prior to be weak enough to fit the data. Hence relatively low values for a and b. Posterior: updating pdf(x) as Beta(20+4, 20+6) Why Beta(24,26)? We can easily update the prior to obtain the posterior adding the number of heads and tails to a and b. Posterior ‚àù Likelihood x Prior Getting the probability P(450 heads) Now that I have the pdf(x), I am looking for the probability of obtaining at least 450 heads from 1000 tosses. I am not sure how to proceed from here... If the coin was fair (P[x] = 0.5), we could answer the question easily by calculating the integral from 450 to 1000 of the binomial distribution. However, as I have a posterior pdf of P(X) and not a number, I am not sure what how to proceed... Could you please give me a hint what to do with the posterior to answer the question? Also, what would be the correct notation of the sought probability? P(450H | 4heads from 10 tosses)? Not sure how I write down that I used an informative prior... I suspect that P(450H) will highly depend on the parameters of my prior... Is there a more scientific way to justify the choice of my parameters? Any help or guidance will be highly appreciated!
