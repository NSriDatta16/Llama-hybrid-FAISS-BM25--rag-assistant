[site]: datascience
[post_id]: 32568
[parent_id]: 32531
[tags]: 
While doing this might simplify your analysis, it is not a recommended approach. Let’s use an example. Suppose that you are using a series of explanatory variables ( X1, X2, X3 ) to estimate car sales (in US$) . Now, suppose that our variable for car sales is interval, i.e. we have a series of car sales figures (25000, 37500, 3000, 71000…), etc. Let’s say you were to convert your dependent variable to a categorical one. e.g. >25000 = 1, By doing this, you would lose a lot of information from your dependent variable and your model would have significantly less capacity to quantify unit effects of each explanatory variable on the dependent one. This is why when binary logistic regressions are run, it is often recommended that a minimum of 500 observations should be used to induce a significant level of variation in the dependent variable to analyse the effects of such variation (Studenmund, 2010). Additionally, it is for this reason that traditional measures of fit in a regression model such as R-Squared become invalid when analysing a dataset with a categorical dependent variable. Categorical variables – or classes – are used in situations where it is not possible to use an interval variable to quantify a particular condition. e.g. suppose you are a medical researcher constructing a model to determine the presence of diabetes in a particular patient. Now, either a person has diabetes or he does not (diabetes = 1, no diabetes = 0). Therefore, your model is not “losing” information because the dependent variable is providing all the information necessary for processing the model. By discarding information, you risk increasing the possibility of a Type 1 error – where you reject a true null hypothesis. In summary, it depends on the data you are working with. In some instances, the dependent variable might make more sense if it were expressed categorically. However, there are many other instances where it is not a recommended approach.
