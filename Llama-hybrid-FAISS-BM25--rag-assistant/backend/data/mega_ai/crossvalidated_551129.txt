[site]: crossvalidated
[post_id]: 551129
[parent_id]: 
[tags]: 
mutual information and edge weights in a bayesian network

The mutual information between two random variables X and Y can be stated formally as follows: I(X ; Y) = H(X) â€“ H(X | Y) Where I(X ; Y) is the mutual information for X and Y, H(X) is the entropy for X and H(X | Y) is the conditional entropy for X given Y. The result has the units of bits. Is the above a realistic representation of the weights along the edge of a bayesian network? Or is a probabilistic representation more suitable? If so, what is the best representation? How should the edge weights be view from probabilistic perspective in a bayesian network context for directed edges; The probability of the nodes I understand to be posterior or marginal probability, but the edges are slightly more ambiguous. Update 2021/12/08:
