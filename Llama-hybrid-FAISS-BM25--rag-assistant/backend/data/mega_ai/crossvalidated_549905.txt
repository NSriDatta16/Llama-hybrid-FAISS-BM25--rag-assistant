[site]: crossvalidated
[post_id]: 549905
[parent_id]: 549896
[tags]: 
You don't recognize how "likelihood times prior maps to a particular distribution". The distribution obtained from applying Bayes theorem does not need to be any of the named distributions like normal, Poisson, binomial, etc. If you are using a simple model with conjugate prior there would be a simple solution like this. In other cases (most of the cases) there won't be a simple solution and because of that, we use MCMC sampling to approximate the resulting distribution. If we knew the distribution, we wouldn't need to approximate it. The only "mapping" like you describe is the case of the most simple models using conjugate priors. You don't need to "know" the distribution to sample from it, you need only to know it up to a constant , i.e. know the likelihood and a prior $$ p(\theta | X) \propto p(X|\theta) \, p(\theta) $$
