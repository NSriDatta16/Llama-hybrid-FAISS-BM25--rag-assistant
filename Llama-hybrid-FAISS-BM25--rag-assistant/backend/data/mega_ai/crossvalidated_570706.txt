[site]: crossvalidated
[post_id]: 570706
[parent_id]: 
[tags]: 
Using a Variational AutoEncoder with an inverse bottleneck

For a problem I'm dealing with, I'm trying to understand if my approach could make sense. I'm using a Variational AutoEncoder (VAE) having relatively low-dimensional inputs, say $x \in \mathbb{R}^n$ . Since my dimension $n$ is small, I'm trying to use the VAE by substantially inverting the role of encoder and decoder (inverse-bottleneck) to create high dimensional latent representation $z \in \mathbb{R}^m$ where $m > n$ and then reducing its dimension again obtaining the reconstruction $\hat{x}$ . So by the means of neural networks I'm substantially trying to approximate a decoder to the high-dimensional space $\phi : \mathbb{R}^ n \rightarrow \mathbb{R}^m$ and then an encoder back to the original space $\psi : \mathbb{R}^m \rightarrow \mathbb{R}^n$ . This usage of VAE is not common at all (it is used instead as dimensionality reduction approach). But I'm still trying to understand why my approach could eventually be wrong at the bottom from a mathematical perspective. I mean, in general it's reasonable to project features in higher dimensional space to enhance separation (kernel methods), so here we are just learning the best way to project these inputs to a such space..
