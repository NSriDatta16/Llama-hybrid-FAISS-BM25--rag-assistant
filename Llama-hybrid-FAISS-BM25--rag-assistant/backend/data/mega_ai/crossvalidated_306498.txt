[site]: crossvalidated
[post_id]: 306498
[parent_id]: 306489
[tags]: 
With 'oversampling', what you are doing is effectively changing the mutual weights of the samples. The mutual weight here is proportional to the number of times an observation is re-sampled. The regression model you fit will be biased towards the observations with higher mutual weight (given that you haven't modified the loss function definition as well). If you did sufficient cross-validation with purely randomized 'oversampling' you will find that model quality is in fact not improved. I suspect the reason you have arrived at the erroneous conclusion of improved model quality is that the cross-validation i.e. train-test split is done wrong. Actually, with 'oversampling' and proper cross-validation (training observations do not occur in test observations), your model quality should actually become worse . This is due to increased weight of some training samples and therefore increased bias in training data. In conclusion, you are correct in your intuition that 'oversampling' is causing over-fitting. However, improvement in model quality is exact opposite of over-fitting, so that part is wrong and you need to check your train-test split in cross-validation. See: http://scikit-learn.org/stable/modules/cross_validation.html -- Beyond the question asked, a tip would be to use feature selection. You can use a simple PCA step to find the principal components. It is unlikely that all 60 variables (or attributes) are equally important (typically not the case for real world data). You can retrain your regression model on just the principal components, or just the most important variables (columns) and see if that is more helpful to your overall objectives.
