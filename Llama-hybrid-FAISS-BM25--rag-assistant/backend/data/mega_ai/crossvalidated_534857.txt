[site]: crossvalidated
[post_id]: 534857
[parent_id]: 534851
[tags]: 
Maximum likelihood estimation does not really fit within an objective Bayes perspective for at least two reasons: the Bayesian paradigm is concerned with full inference rather than point estimation and producing the posterior distribution is its primary goal. Point estimates require a decision theoretic addendum, through a loss function, and maximum likelihood estimators are not universal Bayes estimators, as there is no loss function that always return the maximum likelihood estimator, even though they may coincide for some specific distributions (like the Normal mean under quadratic loss); the maximum likelihood estimator is invariant by reparameterisation, while the maximum a posteriori is not. Even though some parameterisation see MAP (which is arguably un-Bayesian ) and MLE coincide, assuming the associated prior is flat, most changes of parameterisation see the coincidence vanish. (In simpler terms, the flat or uniform prior does not remain flat or uniform by a change of variable. This was an early criticism of this choice of prior by Chrystal, Boole, Lhostes, &tc.) There exists an earlier X validated entry on the links between MaxEnt Ã  la Jaynes and maximum likelihood.
