[site]: crossvalidated
[post_id]: 608277
[parent_id]: 608254
[tags]: 
There's a simple option, if you are fine with maximum-a-posteriori estimation and prediction, but don't care so much about uncertainty. I guess you don't, because it's hard to see how you'd get that with the preceding "deterministic" neural network in the mix. In that case you can just penalize the parameters of your last linear layer as per the prior distributions you want (i.e. add that to the loss function). If you want a fuller posterior (but are happy to ignore that the inputs are really also model estimates from the deterministic NN, the uncertainty about which does not end up being propagated properly), then perhaps look at O'Hagan 2010 or the like, where you can see how a Bayesian linear regression is just solving some linear algebra, too. That is, as long as you are fine to work with conjugate priors (or mixtures of them, which can approximate most things). I wonder whether you can convert this into the final layer of your neural network, but that's much more involved (no idea how complex this gets). O'Hagan, A., 2010. Kendall's Advanced Theory of Statistic 2B. John Wiley & Sons.
