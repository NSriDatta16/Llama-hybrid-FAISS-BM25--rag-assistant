[site]: datascience
[post_id]: 126169
[parent_id]: 
[tags]: 
Using activations at a specific layer as an input for an LLM such as OPT-350m

I'm working with the OPT-350m model and aiming to utilize embeddings from different layers as inputs for generating data. I've encountered issues when trying to feed these embeddings back into the model using the provided methods. # Import necessary libraries (e.g., PyTorch, Hugging Face Transformers) import torch from transformers import AutoTokenizer, OPTForCausalLM model_name = "facebook/opt-350m" tokenizer = AutoTokenizer.from_pretrained(model_name) model = OPTForCausalLM.from_pretrained(model_name) # Encode the input text inputs = tokenizer("hello", return_tensors="pt") # Get model output including hidden states outputs = model(**inputs, output_hidden_states=True) # Extract embeddings from a specific layer (e.g., the second layer) embeddings = outputs.hidden_states[1] output = model(inputs_embeds=embeddings) However, the code above resulted in a shape error: RuntimeError: mat1 and mat2 shapes cannot be multiplied (2x1024 and 512x1024) Additionally, I attempted to use 'get_input_embeddings()' to extract embeddings, but it only provided embeddings from the initial stage, lacking the layer diversity I intended to explore. # Insert a dummy input sentence input_sentence = "London is the capital of" # Tokenize the input sentence and get embeddings of the first layer inputs = tokenizer(input_sentence, return_tensors="pt", padding=True, truncation=True) with torch.no_grad(): embeddings = model.get_input_embeddings()(inputs.input_ids) # Use the embeddings as input to the same model with torch.no_grad(): output = model.generate(inputs_embeds=embeddings, max_length=10) Is there a reliable method to effectively utilize embeddings from different layers as input to the OPT-350m model without encountering shape errors or limitations in layer selection?
