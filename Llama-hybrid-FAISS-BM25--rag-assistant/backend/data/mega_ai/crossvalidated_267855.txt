[site]: crossvalidated
[post_id]: 267855
[parent_id]: 267807
[tags]: 
Yes ,these kernels are generally updated via backpropagation when training (although ,you always have a choice of not updating some/all kernels ) . Also , it is possible to set some of (or all of) these kernels with predetermined 'patterns' to accelerate the learning. This is equivalent to saying that you are using a weight initialization method for your CNN . There could be a variety of ways to do this (this list is no way exhaustive but just to give you an idea). 1) Transfer learning :- For an image classification problem you could use pre-trained kernels from a 'VGG16 architecture', this blog in keras teaches you how to do this . 2) Unsupervised feature learning :- You could also try also learn autoencoders from image patches,use techniques like RICA . Here , you could stack various trained autoencoders (your learned patterns !) and choose not to fine tune (training the complete stacked structure on the training data) and in this case avoid updation through backpropagation. I do not see any reason not to fine tune though.. In case you do not understand this just read this blog (complete it !). 3) Intelligent weight initialisation :- use something like Xaviers initialization or some other heuristic. This is an open research area in machine learning (you should have a look at this and this ). Intuitively we know that humans can learn from single example's so there must be some kind of one shot learning going on. Hope you find this useful.
