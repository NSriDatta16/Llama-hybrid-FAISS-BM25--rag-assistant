[site]: stackoverflow
[post_id]: 5210290
[parent_id]: 3718269
[tags]: 
I'm not sure that I understand the issues completely such that I can offer you a solution but from what you've explained I may have some alternative view points which may be of help. I program in C so what works for me may not be applicable in your case. Your processors have 12MB of L3 and 6MB of L2 which is big but in my view they're seldom big enough! You're probably using rdtsc for timing individual sections. When I use it I have a statistics structure into which I send the measurement results from different parts of the executing code. Average, minimum, maximum and number of observations are obvious but also standard deviation has its place in that it can help you decide whether a large maximum value should be researched or not. Standard deviation only needs to be calculated when it needs to be read out: until then it can be stored in its components (n, sum x, sum x^2). Unless you're timing very short sequences you can omit the preceding synchronizing instruction. Make sure you quantifiy the timing overhead, if only to be able to rule it out as insignificant. When I program multi-threaded I try to make each core's/thread's task as "memory limited" as possible. By memory limited I mean not doing things which requires unnecessary memory access. Unnecessary memory access usually means as much inline code as possible and as litte OS access as possible. To me the OS is a great unknown in terms of how much memory work a call to it will generate so I try to keep calls to it to a minimum. In the same manner but usually to a lesser performance impacting extent I try to avoid calling application functions: if they must be called I'd rather they didn't call a lot of other stuff. In the same manner I minimize memory allocations: if I need several I add them together into one and then subdivide that one big allocation into smaller ones. This will help later allocations in that they will need to loop through fewer blocks before finding the block returned. I only block initialize when absolutely necessary. I also try to reduce code size by inlining. When moving/setting small blocks of memory I prefer using intrinsics based on rep movsb and rep stosb rather than calling memcopy/memset which are usually both optimized for larger blocks and not especially limited in size. I've only recently begun using spinlocks but I implement them such that they become inline (anything is better than calling the OS!). I guess the OS alternative is critical sections and though they are fast local spinlocks are faster. Since they perform additional processing it means that they prevent application processing from being performed during that time. This is the implementation: inline void spinlock_init (SPINLOCK *slp) { slp->lock_part=0; } inline char spinlock_failed (SPINLOCK *slp) { return (char) __xchg (&slp->lock_part,1); } Or more elaborate (but not overly so): inline char spinlock_failed (SPINLOCK *slp) { if (__xchg (&slp->lock_part,1)==1) return 1; slp->count_part=1; return 0; } And to release inline void spinlock_leave (SPINLOCK *slp) { slp->lock_part=0; } Or inline void spinlock_leave (SPINLOCK *slp) { if (slp->count_part==0) __breakpoint (); if (--slp->count_part==0) slp->lock_part=0; } The count part is something I've brought along from embedded (and other programming) where it is used for handling nested interrupts. I'm also a big fan of IOCPs for their efficiency in handling IO events and threads but your description does not indicate whether your application could use them. In any case you appear to economize on them, which is good.
