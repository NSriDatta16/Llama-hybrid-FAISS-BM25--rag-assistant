[site]: crossvalidated
[post_id]: 287057
[parent_id]: 286219
[tags]: 
I posted this question to the weka mailing list and got the following answer: For squared error as the loss function, WEKA has AdditiveRegression. You can run it with depth-limited and otherwise unpruned REPTree models to implement gradient-boosted regression trees. You can also combine it with Bagging, and use RandomTree as an alternative to REPTree to inject randomness into the learning process. For minimising negative loglikelihod in classification problems, you can take a look at LogitBoost as an alternative. Again, use it with depth-limited and otherwise unpruned REPTree models (or RandomTree). To automatically determine an appropriate number of iterations for these methods, use IterativeClassifierOptimizer. Google weka iterativeclassifieroptimizer etc. If you install the RPlugin package for WEKA, you can also access xgboost and well-known gradient boosting implementations in R through WEKAâ€™s MLRClassifier (which uses the mlr package in R to access learning algorithms in R).
