[site]: crossvalidated
[post_id]: 177393
[parent_id]: 
[tags]: 
Applying variational inference to this model

I am basically trying to do a weighted linear regression in a bayesian way. This is to ensure that the I can take care of the heretoscedastic noise. So, my model is like: $$ y_i \sim \mathcal{N}(\beta^Tx_i, \sigma^2/w_i) $$ Here, $y_i$ are each of the output for the input $x_i$. Now, I have the following distributional assumptions on $\beta$ and $w_i$. $$ \beta \sim \mathcal{N}(\beta_0, \Sigma_0) $$ $$ w_i \sim \textrm{Gamma}(a, b) $$ Now, I want to approximate the posterior $p(\beta, w|Y, X)$ using VB. So the full log joint model can be written as: $$ \ln p(Y, \beta, w|X) = \sum_{i=1}^N \ln p(y_i|w_i, \beta, x_i) + \sum_{i=1}^N \ln p(w_i) + \ln p(\beta) $$ Now, looking at variational inference, I need to minimise the KL-divergence between some appropriately chosen distribution $q(\beta, w)$ and the joint model i.e. $$ E_q\bigg[\ln \frac{p(Y, \beta, w|X)}{q(\beta, w)}\bigg] $$ We can expand this as: $$ E_q\bigg[\sum_{i=1}^N \ln P(y_i|w_i, \beta, x_i)\bigg] + E_q\bigg[\sum_{i=1}^N \ln P(w_i)\bigg] + E_q\bigg[\ln p(\beta)\bigg] - E_q\bigg[\ln q(\beta, w)\bigg] $$ I can now apply the mean field approximation i.e. $q(\beta, w) \approx q(\beta) q(w)$. $$ E_{q_{\beta,w}}\bigg[\sum_{i=1}^N \ln P(y_i|w_i, \beta, x_i)\bigg] + E_{q_w}\bigg[\sum_{i=1}^N \ln P(w_i)\bigg] + E_{q_{\beta}}\bigg[\ln p(\beta)\bigg] - E_{q_{\beta}}\bigg[\ln q(\beta)\bigg] - E_{q_{w}}\bigg[\ln q(w)\bigg] $$ I think the reasoning so far is correct but now I am quite lost as to how to proceed. I am not looking for a full solution here but an idea into how to proceed and what tools I would need to generate the update equations for estimating the parameters of the $\beta$ and $w$ variables.
