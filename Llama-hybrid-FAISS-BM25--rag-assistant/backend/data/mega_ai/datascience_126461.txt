[site]: datascience
[post_id]: 126461
[parent_id]: 110967
[tags]: 
A few years later after this post, some authors wrote a paper at NeurIPS about it. Why do tree-based models still outperform deep learning on typical tabular data? https://proceedings.neurips.cc/paper_files/paper/2022/file/0378c7692da36807bdec87ab043cdadc-Paper-Datasets_and_Benchmarks.pdf The twitter thread ( https://twitter.com/GaelVaroquaux/status/1549422403889106944 ) A chatGPT summary of the thread (unrevised) The gap in performance between deep architectures and tree models narrows on large datasets, but such datasets are exceptions for tabular data. Smoothing outcomes in feature space narrows the gap, as deep architectures struggle with irregular patterns, while tree models are not affected by smoothness. Uninformative features have a greater impact on MLP-like neural architectures. Removing the least informative features narrows the performance gap. Data non-invariance by rotation suggests that learning procedures should be rotationally invariant. After applying random rotations to the data, deep architectures outperform tree models, which excel in rotational invariance.
