[site]: crossvalidated
[post_id]: 586090
[parent_id]: 
[tags]: 
Train-test split within modeling function

I have this function I wrote: def singleModelCV(data, target, model): features = data.drop(columns=target) target = data[target] state = 42 X_train, X_test, Y_train, Y_test = train_test_split(features, target, test_size = 0.2, random_state = state) k_fold = KFold(n_splits=10) score = np.mean(cross_val_score(model, X_train, Y_train, scoring='r2', cv=k_fold)) return(score) My question is: is it a bad practice to include the splitting of the data within the function that also trains the model and returns the final scores? I'm doing this because throughout my project I will alter the dataset through feature transformation, feature engineering, etc, in an iterative manner, and I want to compare the scores to the previous ones to see how each technique influences the models. That is why I have one method that would return that both splits the data and trains the model as I would pass the new transformed datasets through this method. I am also aware of pipelines and will implement this to make the workflow cleaner and prevent any data leakage from standardization. If someone could clarify this for me it would be greatly appreaciated. Also any suggestions on how I could carry this task out in an efficient manner would also be fantastic. Thanks!
