[site]: crossvalidated
[post_id]: 451557
[parent_id]: 
[tags]: 
Metrics for avoiding overfitting in reinforcement learning

I wonder if there is useful metrics or methodologies for avoiding/monitoring overfitting in reinforcement learning. For example, in a typical supervised learning, we have the observation that as we adopt more features, the training error goes down while the validation error goes down the up, and we consider the model with lowest validation error as an optimal model with good generalization performance. On the other hand, I have never seen such studies for reinforcement learning, as people usually train and test on the same environment. However, for a stochastic environment the algorithm can still overfit to the noisy observed states and rewards. How should I avoid such overfitting in reinforcement learning?
