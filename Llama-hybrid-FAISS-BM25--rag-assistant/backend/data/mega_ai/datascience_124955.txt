[site]: datascience
[post_id]: 124955
[parent_id]: 
[tags]: 
Best model for regression in this case?

I am doing some modeling to predict a variable of interest given a big set of features (500) for which I expect a considerable amount of interactions happening at least among some of them. I first tried to do a multilinear regression and the predictions were not so good (R2 around 0.2) but then I switched to Random forests and my R2 got much better (0.5/0.7 test/train set). I thought by using more complex models I could get an even better performance, but in reality, this does not happen. I tried a vanilla FNN after standardizing all features, with different setups: If I set 2 hidden layers it gets overtrained very easily (R2= 0/0.8 test/train set) If I set one hidden layer and a reasonably high number of nodes it still gets overtrained but less badly (R2 = 0.2/0.8 test/train set) I also tried increasing a lot the number of epochs (up to 1000) and it didn't help at all. So for some reason, FNN cannot outperform the random forest in this case. Do you have any suggestions for a good choice of model for a case like this? Some relevant info are the following Most features are coordinates (define the position of the object in the space) They are all numeric features, some are continuous and some are discrete I expect that there is a lot of interactions between some features Some feature are probably useless (according to the RF model) In my case the best I can expect is probably R2 = 0.8 If you need more info comment below and i will add those in the questions. More Info I have 6400 observations not time related Each observation is basically a measure of a pair of 2 objects. That means that my response variable is not a trait of one simple object but can be measured only between pairs of objects
