[site]: crossvalidated
[post_id]: 620457
[parent_id]: 620445
[tags]: 
You noticed that the question can be made more precise if we use a precise definition of "information" from the information theory. Let's take that route. Recall that the entropy for multivariate normal distribution is $$ H(x) = \frac{n}{2} \ln(2\pi) + \frac{1}{2} \ln|\Sigma| + \frac{1}{2} n $$ It depends on the determinant of the covariance matrix $|\Sigma|$ , which squashes it to a single number. Hopefully, the determinant for a $2\times 2$ matrix, such as the covariance of the bivariate normal distribution is trivial to calculate $$ \begin{vmatrix} a & b\\c & d \end{vmatrix}=ad-bc $$ hence it is just $\sigma^2_1\sigma^2_2 - \rho^2$ . Now, if you keep everything else constant and treat it as a function of $\rho$ , the only moving part is $-\rho^2$ , hence the entropy decreases as $|\rho|$ increases. The more correlated the variables are, the less information they carry. Now, for an intuition about this, consider two cases. First, when the correlation is perfect $\rho=1$ , then $x_1 = x_2$ . Second, when $\rho=0$ , $x_1$ and $x_2$ can be any values on the real line. The average is a linear combination, so it will fall somewhere in between $x_1$ and $x_2$ . In the first case, imagine that $x_1$ is higher than $\mu$ , then the mean would be equally higher than $\mu$ . In the second case, if $x_1$ is higher than $\mu$ , $x_2$ can be any value, it can be even higher, slightly less high, less, etc, so $x_2$ has a chance to "correct" the $x_1$ being high. Even more practically, this is the reason why we prefer to draw our samples randomly. Non-random samples would be correlated, hence more biased, and less "informative".
