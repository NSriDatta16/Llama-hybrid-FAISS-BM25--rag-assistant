[site]: crossvalidated
[post_id]: 310058
[parent_id]: 309997
[tags]: 
In the positive phase, the gradient is $E[hx^T|x] = E[h|x]x^T$. Since $E[h|x]$ can be computed without sampling, there is no sampling involved in the first phase. In the negative phase, the gradient is $-E[hx^T]$. We must sample some $h', x'$ from the $\textit{joint}$ distribution and then approximate $-E[hx^T$] as $-h'x'^T$. The reason we need to sample is because computing this expectation is intractable. The sampling is done by block Gibbs sampling: first we start with the given $x$ which is the datapoint you are training on. Then sample some $h|x$, then sample some $x|h$, and so on. Stop after sampling $k$ times of this. Edit: For the general (unrestricted) Boltzmann machine, computing $E[h|x]$ requires sampling as well. The easiest way to do this is to fix all the values of the visible $x$, and then sample a new value $h_i' \leftarrow h_i|h_{-i},x$ for some random hidden node $i$. Repeat this until the process converges. Theoretically, starting the hidden nodes at any value will result in convergence, since the markov chain is stationary. I think in practice, starting from the last hidden state should lead to good behaviour. From what I know of it, simulated annealing is usually used for optimization, so it could find $\max_h P(h|x)$, which might be a good starting point for the gibbs sampler, but I haven't seen it mentioned as a sampler by its own.
