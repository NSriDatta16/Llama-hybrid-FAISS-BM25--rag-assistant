[site]: datascience
[post_id]: 43977
[parent_id]: 
[tags]: 
How much data to use for feature selection?

Working on my master's thesis, this is a problem I'm unable to find good resources about. I'm working with data of 18 participants, who are either active or passive. Each participant is then subjected to a 3 x 3 experiment and results in a total of around 676 trials per participant (around 12.168 trials in total). There are 100 data points in each trial but cannot be used separately from the trial (since its an EEG epoch). My data consists of 579 features, so I need some sort of feature selection as literature shows that most of them are irrelevant, but I want to use a bottom-up machine learning approach (to verify this). Is there a rule of thump/literature to use for the amount of data needed for feature selection?
