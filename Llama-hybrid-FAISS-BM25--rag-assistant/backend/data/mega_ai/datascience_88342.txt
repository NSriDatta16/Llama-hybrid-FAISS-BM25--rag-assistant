[site]: datascience
[post_id]: 88342
[parent_id]: 88337
[tags]: 
I will start with an advice - just google "n gram language model" and you will find a lot of good detailed explanations. With that being said I will give a short explanation about the "training phase" of n-gram language models (answer to question 2). The simplest way to build an N-gram language model strats with finding a big corpus - a set of many sentences. the words of the model will be the words that appear at least once in the corpus. The probability of the word xn given a past context of the words x1,x2,...,xn-1 will be the number of occurrences of the sequence x1,x2,...xn-1,xn in the corpus / the number of occurrences of the sequence x1,x2,...,xn-1 in the corpus. This is the simplest way and it has problems, especially What happens if the sequence x1,x2,...,xn does not appear in the corpus? It will always get propability zero. Therefore there are smoothing techniques to handle this problem (read about it). And now for question 1 - In the simplest case, without smoothing, the candidates are the words that appear in the corpus. In models with smoothing the candidates may be all of the words in the sense that every word might get a positive probability.
