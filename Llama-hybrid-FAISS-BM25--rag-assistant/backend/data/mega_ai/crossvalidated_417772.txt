[site]: crossvalidated
[post_id]: 417772
[parent_id]: 
[tags]: 
Bayesian random slope model with divergent transitions. Help in setting stronger priors

I have a model with which I have convergence problems. This are the model specifics ( brms package): Family: gamma Links: mu = log; shape = log Formula: Attesa ~ log(N.prest) + (log(N.prest) | Context) shape ~ log(N.prest) + (log(N.prest) | Context) Data: TdA (Number of observations: 778) Samples: 8 chains, each with iter = 8000; warmup = 2000; thin = 1; total post-warmup samples = 48000; max_treedepth = 15, adapt_delta = .90 The dataset has just two observation per Context, for around 400 Contexts. This is the results summary: Group-Level Effects: ~Context (Number of levels: 408) Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat sd(Intercept) 1.10 0.10 0.91 1.27 89 1.06 sd(logN.prest) 0.07 0.03 0.02 0.12 43 1.13 sd(shape_Intercept) 0.81 0.20 0.41 1.25 997 1.01 sd(shape_logN.prest) 0.06 0.05 0.00 0.18 1359 1.02 cor(Intercept,logN.prest) -0.75 0.16 -0.97 -0.42 3767 1.00 cor(shape_Intercept,shape_logN.prest) -0.16 0.55 -0.96 0.91 3552 1.00 Population-Level Effects: Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat Intercept 2.62 0.08 2.46 2.79 334 1.02 shape_Intercept 0.32 0.21 -0.08 0.74 3996 1.00 logN.prest 0.08 0.01 0.05 0.10 733 1.01 shape_logN.prest 0.54 0.05 0.44 0.64 677 1.02 Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1). Warning messages: 1: The model has not converged (some Rhats are > 1.1). Do not analyse the results! We recommend running more iterations and/or setting stronger priors. 2: There were 7996 divergent transitions after warmup. Increasing adapt_delta above 0.9 may help. See http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup that shows how sd(Intercept) , sd(logN.prest) and many Context level specific random slopes and intercepts are particularly problematic. These are the priors: prior class coef group resp dpar nlpar bound 1 cauchy(0, 2.5) b 2 b logN.prest 3 b shape 4 b logN.prest shape 5 normal(0, 2) Intercept 6 student_t(3, 0, 10) Intercept shape 7 lkj_corr_cholesky(1) L 8 L Context 9 normal(0, 1.5) sd 10 student_t(3, 0, 10) sd shape 11 sd Context 12 sd Intercept Context 13 sd logN.prest Context 14 sd Context shape 15 sd Intercept Context shape 16 sd logN.prest Context shape Keeping in mind that I'm more interested in prediction (and prediction distribution) than parameter estimation, where can I put stronger priors to improve the model? And a bonus, brms specific, question: are priors intended hierarchically? That is, is I set eg set_prior('normal(0, 2), class = 'sd') , is it applied to all sd parameters, even if in the prior_summary() output above they are blank?
