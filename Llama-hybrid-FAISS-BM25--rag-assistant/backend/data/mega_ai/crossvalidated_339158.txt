[site]: crossvalidated
[post_id]: 339158
[parent_id]: 330036
[tags]: 
You're conflating two different things with regard to LSTM models. The batch size refers to how many input-output pairs are used in a single back-propagation pass. This is not to be confused with the window size used as your time series predictors - these are independent hyper-parameters. The normal way to solve this would be to pick a window size (let's say 25 since that was what you proposed). Now say that we use an LSTM network to predict the 26th point using the previous 25 as predictors. You would then repeat that process for each of the remaining points (27-100) using the preceding 25 points as your inputs in each case. That will yield you exactly 75 training points. Batch size will dictate how many of these points are grouped together for backprop purposes. If you picked 5, for instance, you'd get 15 training batches (75 training points divided into 5 batches). Note that this is a very small amount of data, so unless you use a very small NNet or heavy regularization, you're going to be at great risk of overfitting. You'd normally want to do a train-test split to be able to perform out-of-sample validation on the model, but given how few data points you have to work with that's going to be a bit tough.
