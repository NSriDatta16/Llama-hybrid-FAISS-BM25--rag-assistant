[site]: crossvalidated
[post_id]: 147580
[parent_id]: 147551
[tags]: 
Bayes theorem is: $$ P(A|B) = \frac{P(B|A)P(A)}{P(B)} $$ In a case where you have some data and a parameter, it is common to use $\theta$ for the parameter (or parameter vector) and $x$ for the data. You might place a prior on $\theta$, $p(\theta)$, and you might have a model $p(x|\theta)$ which gives the probability of your data given the model. You can then use Bayes rule/theorem to "invert" this and get $p(\theta|x)$. Only in a relatively small set of examples is it possible to get closed form solutions for $p(\theta|x)$. For arbitrary cases, often you approximate the posterior distribution using some standard methods in Bayesian statistics -- for example, the two most common broad approaches are markov chain monte carlo or variational Bayes. Suppose you are interested in a simple case where a closed form posterior exists. An example of this would be if $p(\theta)$ is a standard normal (Gaussian with unit variance and zero mean) and $p(x|\theta)$ is a normal with mean value of $\theta$ and unit variance. I will omit normalization factors for convenience. Also note that the denominator in Bayes rule tends to simply renormalize things: $$ p(\theta|x) \propto e^{-(x-\theta)^2/2} e^{-\theta^2/2}\\ $$ Let's combine the exponents and complete the square $$ -(x-\theta)^2/2 - \theta^2/2 \propto - (x^2 - 2\theta x + \theta^2) - \theta^2 $$ Recall that x is fixed here because it has been observed and we want expect our answer will be in terms of it. Complete the square and see that the exponent is $\propto -(\theta - x/2)^2$ with other terms of which depend on x. So: $$ p(\theta|x) \propto e^{-a(\theta - x/2)^2} $$ where 'a' is a factor that can be obtained by book-keeping. Notice that the posterior is a normal distribution with mean value x/2. Attempt to compute the variance for yourself. Note that our answer makes intuitive sense...the prior said that $\theta$ is zero and we observe a sample $x$ that has expected value of $\theta$. Since the variance of the prior and the distribution $p(x|\theta)$ are equal magnitude, we trust them equally. Accordingly, our posterior is a distribution with a mean that is the average of $x$ and 0 and which ends up having smaller variance than the initial $p(x|\theta)$ or $p(x)$ (not shown here). For model comparison, you could look at a ratio: $$ \frac{p(x|\theta_1)}{p(x|\theta_2)} $$ This is called the likelihood ratio (see wikipedia or elsewhere). Here you don't need the posterior, you simply are looking at how (relatively) likely your data (or observations) are given either $\theta_1$ or $\theta_2$ being the parameter of the model that generated your observations. Hope this helps.
