[site]: crossvalidated
[post_id]: 493541
[parent_id]: 493536
[tags]: 
Like penalized maximum likelihood estimation (e.g., ridge regression), random effects result in shrinkage of parameter estimates towards a common value. For example, in a 10-group problem, the use of random slopes may effectively assume for small samples that the 10 slopes are more alike than they are different. Information is borrowed across groups, reducing the variance of the slopes. Fixed effects tailor the slope estimates to each group, without shrinkage. This is effectively allowing the variance of the slopes to be arbitrarily large. Shrinkage (discounting; penalization), by making parameter estimates smaller, reduces the effective degrees of freedom. Effective d.f. comes from something similar to the ratio of variance of a parameter estimate after and before shrinkage. If you were to impose a very small random effect variance (say with a Bayesian prior) the 10 slope estimates would be almost identical and you would effectively be estimating only one slope.
