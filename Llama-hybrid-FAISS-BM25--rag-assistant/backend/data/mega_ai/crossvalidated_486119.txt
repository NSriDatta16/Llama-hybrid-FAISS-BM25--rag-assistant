[site]: crossvalidated
[post_id]: 486119
[parent_id]: 
[tags]: 
Murphy: A probabilistic perspective - Completing the in Variational Inference

I don't understand how bishop derives the fact that $q_{\mu}(\mu)$ follows a gaussian distribution by completing the square. On page 774 of the book, a probabilistic perspective, he says the following: $\text { The optimal form for } q_{\mu}(\mu) \text { is obtained by averaging over } \lambda \text { : }$ $$ \begin{aligned} \log q_{\mu}(\mu) &=\mathbb{E}_{q_{\lambda}}[\log p(\mathcal{D} \mid \mu, \lambda)+\log p(\mu \mid \lambda)]+\text { const } \\ &=-\frac{\mathbb{E}_{q_{\lambda}}[\lambda]}{2}\left\{\kappa_{0}\left(\mu-\mu_{0}\right)^{2}+\sum_{i=1}^{N}\left(x_{i}-\mu\right)^{2}\right\}+\mathrm{const} \end{aligned} $$ $\text { By completing the square one can show that } q_{\mu}(\mu)=\mathcal{N}\left(\mu \mid \mu_{N}, \kappa_{N}^{-1}\right), \text {where }$ $$ \mu_{N}=\frac{\kappa_{0} \mu_{0}+N \bar{x}}{\kappa_{0}+N}, \kappa_{N}=\left(\kappa_{0}+N\right) \mathbb{E}_{q_{\lambda}}[\lambda] $$ Thanks
