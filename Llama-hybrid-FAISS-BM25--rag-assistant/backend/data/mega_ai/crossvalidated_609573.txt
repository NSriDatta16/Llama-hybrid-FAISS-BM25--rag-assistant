[site]: crossvalidated
[post_id]: 609573
[parent_id]: 608880
[tags]: 
Multivariate outputs tends to be something that's difficult for many traditional machine learning models other than neural networks. Neural networks of various forms could be good candidates for this sort of thing, but it's not clear to me why a transformer architecture would necessarily be more promising than other options. It's unclear to me whether it's necessary to output all categories at once. I assume your idea is to provide the whole history (sales across all categories) to predict the sales in the next time step (or a few ahead). One of the key questions is how would one represent the different categories of products sensibly. One of the key innovations in neural networks for tabular data with high cardinality data (i.e. categorical data with many categories such as different products, customers, store locations etc.) is embedding layers (as e.g. famously used in the Rossman Store Sales Kaggle competition). If I input some history / features and the category I want to predict for (one output at a time), it's easy to see how I do that. I guess, if you want a multi dimensional output, you'd have to somehow "tell the model" that a different embedding (or possibly more than one, if you e.g. also have a product category embedding) is pertinent to each input and to each output. Not 100% sure how one would do that. With a large number of products at some point you'll eventually run into problems with memory, I would assume (even worse if you want to attend at the same time over many time-steps). No idea whether a sufficient complexity model is feasible with the type of realistically sized portfolio of products that a shop might have (probably much more than 100 products). There's examples of similar forecasting settings in various machine learning competitions such as the M5 Forecasting competition on Kaggle (there is a accuracy and a uncertainty part to that competition) and of course Rossman Store Sales .
