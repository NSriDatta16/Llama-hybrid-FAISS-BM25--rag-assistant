[site]: crossvalidated
[post_id]: 299924
[parent_id]: 299917
[tags]: 
A loss function for a statistical or machine learning model almost always averages (or sums, as the difference between a sum and an average is just a multiplicative constant) over all the training data: $$ L(\beta) = \sum_i (y_i - \hat y_i)^2 $$ So when you take the gradient with respect to any parameter, the derivative can be pushed into the sum: $$ \nabla L(\beta) = - 2 \sum_i (y_i - \hat y_i) \nabla \hat y_i $$ I think what you have computed is the individual gradients $\nabla \hat y_i$, so you just need to sum to account for the fact that you are minimizing the total loss across all your training data.
