[site]: crossvalidated
[post_id]: 553308
[parent_id]: 552347
[tags]: 
1. Is non-max suppression for bounding boxes obtained from a Region Proposal Network performed during training? Yes, according to the Faster-RCNN paper it states, Some RPN proposals highly overlap with each other. To reduce redundancy, we adopt non-maximum suppression (NMS) on the proposal regions based on their cls scores. We fix the IoU threshold for NMS at 0.7, which leaves us about 2000 proposal regions per image. As we will show, NMS does not harm the ultimate detection accuracy, but substantially reduces the number of proposals. So, NMS on proposals is not essential, but it reduces computation without significant loss of performance. 2. NMS is not differentiable-- in which case, it can't be performed during training According to the paper, authors use Approximate joint training which is, Approximate joint training : In this solution, the RPN and Fast R-CNN networks are merged into one network during training as in Figure 2. In each SGD iteration, the forward pass generates region proposals which are treated just like fixed, pre-computed proposals when training a Fast R-CNN detector. The backward propagation takes place as usual, where for the shared layers the backward propagated signals from both the RPN loss and the Fast R-CNN loss are combined. This solution is easy to implement. But this solution ignores the derivative w.r.t. the proposal boxesâ€™ coordinates that are also network responses, so is approximate. So if Approximate joint training method is used (which is common use), each proposals are treated as pre-computed proposal. This method might not be mathematically legitimate, but hey, it does the job.
