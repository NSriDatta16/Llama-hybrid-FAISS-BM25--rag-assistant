[site]: datascience
[post_id]: 26716
[parent_id]: 26714
[tags]: 
It's not uncommon for someone to label it as an unsupervised technique. You can do some analysis on the eigenvectors and that help explain behavior of the data. Naturally if your transformation still has a lot of features, then this process can be pretty hard. Nevertheless it's possible thus I consider it machine learning. Edit: Since my answer was selected (no idea why) I figured i'll add more detals. PCA does two things which are equivalent. First, and what is commonly referred, it maximizes the variances. Secondly, it minimizes the reconstruction error by looking at pair-wised distances. By looking at the eigenvectors and eigenvalues, it becomes rather simple to deduce which variables and features are contributing to the variance and also how different variables move in conjunction with others. In the end, it really depends on how you define "learning". PCA learns a new feature space that captures the characteristics of the original space. I tend to think that can be meaningful. Is it complex? No, not really, but does that diminish it as an algorithm? No I don't think so.
