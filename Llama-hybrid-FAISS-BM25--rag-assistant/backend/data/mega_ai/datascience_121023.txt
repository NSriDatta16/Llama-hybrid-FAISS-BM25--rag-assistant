[site]: datascience
[post_id]: 121023
[parent_id]: 
[tags]: 
Transformer-based autoencoder generates same output and bad embeddings

I am trying to implement the transformer-based autoencoder presented in this paper: https://arxiv.org/abs/2210.08288 The paper seems rather vague to me and I do not fully understand how the model is built. My current implementation generates the same blurry output for every input, which looks like this: Note: The dataset is the MNIST dataset. Even if I intensely overfit the model by just training it on a single input, the generated output is still not great: The generated embeddings do not look pleasing either: My current implementation is the following: class TransformerBlock(layers.Layer): def __init__(self, embedding_size, num_heads, ff_dim, output_dim): super().__init__() key_dim = int(embedding_size/num_heads) if key_dim == 0: key_dim = 1 self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim) self.ffn = keras.Sequential( [layers.Dense(ff_dim, activation="gelu"), layers.Dense(output_dim)] ) self.layernorm = layers.LayerNormalization(epsilon=1e-6) def call(self, inputs): input_shape = tf.shape(inputs) att_output = self.att(inputs, inputs) norm_output = self.layernorm(att_output + inputs) ffn_output = self.ffn(norm_output) return ffn_output class PatchAndPositionEmbedding(layers.Layer): def __init__(self, maxlen, embed_dim): super().__init__() self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim) def call(self, x): maxlen = tf.shape(x)[-2] positions = tf.range(start=0, limit=maxlen, delta=1) positions = self.pos_emb(positions) return x + positions sequence_length = 2 embed_dim = 28*14 num_heads = 2 ff_dim = embed_dim ff_multiplier = 2 def create_model(): inputs = layers.Input(shape=(sequence_length, embed_dim), dtype=tf.float32) embedding_layer = PatchAndPositionEmbedding(sequence_length, embed_dim) x = embedding_layer(inputs) transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim*ff_multiplier, output_dim=128) x = transformer_block(x) transformer_block = TransformerBlock(128, num_heads, 128*ff_multiplier, output_dim=64) x = transformer_block(x) transformer_block = TransformerBlock(64, num_heads, 64*ff_multiplier, output_dim=32) x = transformer_block(x) transformer_block = TransformerBlock(32, num_heads, 32*ff_multiplier, output_dim=16) x = transformer_block(x) transformer_block = TransformerBlock(16, num_heads, 16*ff_multiplier, output_dim=1) z = transformer_block(x) decoder = keras.Sequential([ TransformerBlock(1, num_heads, 1*ff_multiplier, output_dim=16), TransformerBlock(16, num_heads, 16*ff_multiplier, output_dim=32), TransformerBlock(32, num_heads, 32*ff_multiplier, output_dim=64), TransformerBlock(64, num_heads, 64*ff_multiplier, output_dim=128), TransformerBlock(128, num_heads, 128*ff_multiplier, output_dim=embed_dim) ]) output = decoder(z) model = keras.Model(inputs=inputs, outputs=output) loss_fn = tf.keras.losses.MeanSquaredError() encoder = keras.Model(inputs, z) model.compile( "adam", loss=loss_fn ) return (model, encoder, decoder) I have also tried training an autoencoder set up the same way, except skipping the embedding layer and replacing all transformer blocks with dense layers. The outputs and embeddings of this model look much better. What could be the reason for my transformer-based autoencoder failing to generate satisfying outputs? Is the implementation of this type of autoencoder even worth pursuing or is the model, and possibly the paper, in itself flawed?
