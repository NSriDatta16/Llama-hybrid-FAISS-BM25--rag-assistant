[site]: crossvalidated
[post_id]: 529592
[parent_id]: 14197
[tags]: 
I'm actually working on this same topic of the "Cross-validation with Financial Predictive Modeling" type of problem. So here are some of my findings. Basically, I think that Cross-Validation by itself needs additional important considerations in order to produce valid and useful results, for the case of Financial Time Series Predictive Modeling. A short answer for your question is in two parts: Performance on the last fold is better than average , sounds like an example of the Simpson's paradox The results you are getting could be due to back-test overfitting, and/or an information leakage between explanatory variables in the fold 2 and the target variable in the fold 1. Particularly in FTS, out-of-sample generalization does not guarantee out-of-distribution generalization. More resources, reformulated questions, known effects in Financial Time Series. No unbiased estimator for the variance We normally perform CV in order to increase our confidence in the model performance whenever new data arrives. Such endeavor ultimately leads to a trade-off we have to make (it would be good to do that consciously and explicitly), and that would be the Bias-Variance trade-off, which happens when we are trying to simultaneously minimize these Biased results and variance in the results (two different sources of error), being those the main reasons of preventing supervised learning algorithms from generalizing (learning) beyond your training data. Theoretically, we can have certainty that Cross-Validation does help to reduce bias, but we do not have (yet) a way to prove that there exists an estimator to express correctly some properties of the variance, so it goes frequently underestimated, as stated in this work . And so, The case of Financial Time Series (FTS) In my current opinion, this work does provide various extra and special considerations when working with FTS. A quick list of important effects will be: Leakage of information, backtest-overfitting, Memory-loss. And the respective techniques, as proposed in the cited work, are: Purge&Embargo, Deflated performance metrics, fractional differentiation. I do mention those concepts because it could be the real causes of performance variation in your methods, a more "deep" reason since you are working with FTS, not just the cross-validation perspective by itself. Visual "classical" examples. I am working on a python library that will provide methods, visualizations, and tests for this particular question of "What type of Cross-Validation is useful for FTS". Here are some early examples I draw for that. Hope this comment helps new visitors, maybe not for answering the question directly (there can only be 1 accepted answer), but to provide more new questions, sources, and terms for further explorations.
