[site]: datascience
[post_id]: 57474
[parent_id]: 30659
[tags]: 
The reason it can save computation time is because your network would already be able to extract generic features from your dataset. The network will not have to learn extracting generic features from scratch. A neural network works by abstracting and transforming information in steps. In the initial layers, the features extracted are pretty generic, and independent of the particular task. It is the later layers which are much more tuned specific to the particular task. So by freezing the initial stages, you get a network which can already extract meaningful general features. You would unfreeze the last few stages(or just the new untrained layers), which would be tuned for your particular task. Also, I would not recommend unfreezing all layers if you have any new/untrained layers in your model. These untrained layers will have large gradients in the first few epocs, and your model will train as if initialized by random(and not pre-trained) weights.
