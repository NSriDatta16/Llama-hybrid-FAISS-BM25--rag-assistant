[site]: crossvalidated
[post_id]: 366839
[parent_id]: 
[tags]: 
Adding Denoiser to Existing Autoencoder Network

I currently have implemented a Machine Learning Model that is very similar to the model proposed by this paper. It works pretty well on the data I have, although sometimes the training goes awry and I get bad results. By "bad results", I mean that my sample entropy (described in detail in the paper) stays near the maximum value $\mathcal{B} \log{k}$ (where $\mathcal{B}$ is the mini-batch size and $k$ is the number of clusters) or my batch entropy (also described in detail in the paper) goes to zero (ideally it should stay relatively close to its maximum, that is, $\log{k}$). I have done some reading and have found that denoising autoencoders can be good for increasing the robustness of a model. I am thus interested in trying to implement this into my model, but I have some questions to which I am unsure. This is a pretty quick and easy-to-understand resource describing what the denoising autoencoders do. Would this model even benefit from adding this denoising layer? I have no evidence to suggest that adding a denoising layer to my autoencoder will help the robustness of my model in any way (besides the internet saying so). Also, there could be something else wrong with my model that is causing these errors (I would love to hear suggestions in comments/answers if you think the cause of the error lies in something else I am potentially doing wrong). Assuming that adding a denoising layer is a reasonable thing to try and improve the robustness of my model, where in the model would I add the layers to do the denoising? There are many candidate places that I could try putting it but it is not immediately obvious (to me, at least) that one of these places will yield better results than the rest. Any help with either of these questions would be much appreciated, leave a comment/question if you have any inquiries and I will do my best to answer them so you guys can hopefully give me a good answer.
