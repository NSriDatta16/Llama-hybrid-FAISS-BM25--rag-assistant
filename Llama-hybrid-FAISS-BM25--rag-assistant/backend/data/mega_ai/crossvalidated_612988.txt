[site]: crossvalidated
[post_id]: 612988
[parent_id]: 597969
[tags]: 
I would expect the issue to go away as you got an enormous amount of data, especially if you used a flexible model that can figure out patterns (e.g., deep learning). You can think of the probability prediction in terms of Bayes' theorem, where you have a prior probability (the class ratio) and get the posterior probability as your prediction. As the data size gets large, the prior probability ought to get overwhelmed by the data, allowing an unusual case to scream out as being particularly likely to belong to the minority category, regardless of prior probability. Thus, whether you do artificial balancing or not, the model should figure out the truth. While you do not have to go full-Bayesian to view the problem in these terms, it might help you to think of how even a rather strong prior distribution gets overwhelmed by a huge amount of contradictory data in maximum a posteriori estimation. (Now, it might be that such an event is unusual no matter what, but a good model trained on a huge data set should catch this.)
