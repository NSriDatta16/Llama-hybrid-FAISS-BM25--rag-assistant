[site]: datascience
[post_id]: 62142
[parent_id]: 62121
[tags]: 
This could be many things. Since you want to calssify the data into three classes, I would use one-hot-encoding, rather than the binary enumeration, because you kind of introduce the information to the model that [1,0] and [1,1] are "closer" than [0,0] and [1,1] for example. Unless this is actually the case, better encode the classes like [1,0,0], [0,1,0] and [0,0,1]. That the eigenvalues in your PCA are large can have two reasons: a) you need to divide XX^T by the number of data points, as the PCA is computed on an approximation of the covariance matrix, which in turn is based on an expectation value b) not sure if you did, but make sure to center your data first I think you discovered this yourself already, but if the PCA is useful on your problem depends actually on your data distribution and how you computet the Z. It sounds a bit as if there was no covariance between the features, so there are no primary components to discover. And in case you picked the features to compute Z randomly, you have no guarantee that the primary components tell you anything about Z. Maybe do a PCA on the data and append Z to X. This might help with dimensionality reduction or identifying relevent features. Since you have many features and use saturating activation functions, it is important that you normalize the data before passing it to the model and that you initialize the weights properly, otherwise the neurons will saturate and the gradient will be almost zero. Also training might be very slow anyway, as with all the "useless" features you introduce a lot of noise. I hope this helps. If not, feel free to post some more information on your training data and the neural network you wanted to use.
