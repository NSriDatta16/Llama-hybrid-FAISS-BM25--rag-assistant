[site]: crossvalidated
[post_id]: 522045
[parent_id]: 521786
[tags]: 
The terms refer to properties of the loss function. When we optimize the parameters of, say, a neural network, we can get caught in a local minimum that has some nice properties that a global minimum has (it is a valley of some sort), but it is not the minimum of the function. In other words, we can do better. In this drawing, the horizonal axis is some parameter value for a one-parameter model, and the vertical axis is the loss function. A loss function for a useful model might have many more parameters than just one, and you can wind up with a loss function that is twisted and crumpled in thousands or millions of dimensions.
