[site]: crossvalidated
[post_id]: 199004
[parent_id]: 199002
[tags]: 
This is a good question. Let me begin with a couple of clarifications: It doesn't really mean anything for a "[t]ype II error [to] be significant" (or for a type I error to be). Certainly, it might be very important that we missed a true effect, though. Also, we do not generally "[accept] the null hypothesis". (For more on that, it may help to read my answer here: Why do statisticians say a non-significant result means “you can't reject the null” as opposed to accepting the null hypothesis? ) I think you are (unfortunately) right that less attention is paid to power and type II errors. While I think the situation is improving in biomedical research (e.g., funding agencies and IRBs often reqire power analyses now), I think there are a couple of reasons for this: I think power is harder for people to understand than simple significance. (This is in part because it depends on a lot of unknowns—notably the effect size, but there are others as well). Most sciences (i.e., other than physics and chemistry) are not well mathematized. As a result, it is very hard for researchers to know what the effect size 'should' be given their theory (other than just $\ne0$). Scientists have traditionally assumed that type I errors are worse than type II errors.
