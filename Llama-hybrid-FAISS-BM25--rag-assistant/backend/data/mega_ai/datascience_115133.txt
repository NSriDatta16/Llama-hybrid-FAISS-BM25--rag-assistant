[site]: datascience
[post_id]: 115133
[parent_id]: 115052
[tags]: 
Two ideas: I understand that you calculate similarity between every pair of rows/documents, right? If so, the bottleneck is due to the quadratic processing of all the pairs First, you should compare only $(d_1,d_2)$ and not $(d_2,d_1)$ (using indexes: if $i ), this saves 50% time. I also assume that the goal is to capture pairs/groups of strongly similar documents. If yes, a method would be to first apply the BoW/TFIDF method (simpler and faster), then to apply the embeddings method only to the pairs which obtain at least some similarity threshold with the first method. A completely different approach: apply topic modelling (LDA, or HDP, or other recent method) on the set of document. This would be likely faster. It might also reveal a different kind of semantic similarity betweem documents.
