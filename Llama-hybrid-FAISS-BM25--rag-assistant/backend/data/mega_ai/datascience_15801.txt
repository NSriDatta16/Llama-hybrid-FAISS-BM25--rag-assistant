[site]: datascience
[post_id]: 15801
[parent_id]: 15795
[tags]: 
The problem with directly optimising the F1 score is not that it is non-convex, rather that it is non-differentiable. The surface for any loss function for typical neural networks is highly non-convex. What you can do instead, is optimise a surrogate function that is close to the F1 score, or when minimised produces a good F1 score. One way to do this is to simply optimise the cross entropy as usual with weights on the classes - see this answer on stackoverflow.
