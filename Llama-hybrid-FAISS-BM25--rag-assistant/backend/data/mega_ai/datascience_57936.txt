[site]: datascience
[post_id]: 57936
[parent_id]: 57928
[tags]: 
You can think about phenomenons close to the curse of dimensionality. Embedding words in a high dimension space requires more data to enforce density and significance of the representation. A good embedding space (when aiming unsupervised semantic learning) is characterized by orthogonal projections of unrelated words and near directions of related ones. For neural models like word2vec, the optimization problem (maximizing the log-likelihood of conditional probabilities of words) might become hard to compute and converge in high dimensional spaces. Youâ€™ll often have to find the right balance between data amount/variety and representation space size.
