[site]: crossvalidated
[post_id]: 530378
[parent_id]: 483988
[tags]: 
When you have tons of data, but there is just a lot of variability that nothing in the data can explain (aleatory uncertainty - vs. epistemic that can be reduced with more data), I've had the experience (mostly with large medical datasets) that a good logistic regression is often only marginally worse than a well-tuned tree-based model (such as random forrest or the usually best-performing xgboost/LightGBM boosted tree family). In some settings - especially when you have high cardinality categorical features (e.g. you want to predict whether a patient will die during at hospitalization and primary reason for admission will be one of hundreds of ICD-10 codes) or a time series of information (e.g. patients that record daily symptom diaries) - neural networks can be quite promising. However, even if you are reserving a final test set, trying these out on the training data using, say, some sensible cross-validation (e.g. it can be quite important what data you make sure to "group" so it always is either in the training or the validation data of a fold, e.g. for medical data you often want to make sure a single patient is never split across training and validation data, or perhaps you might want to always keep the data from each hospital together, or perhaps it's a time series problem and you always want to train on data from the past and predict the future etc.) is the main way to see what appears to work for the particular problem.
