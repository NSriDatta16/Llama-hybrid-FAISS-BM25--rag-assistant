[site]: crossvalidated
[post_id]: 146490
[parent_id]: 146483
[tags]: 
You can estimate the generalization performance of a given tuple of hyperparameters via cross-validation. The traditional way to find suitable values for these parameters is using grid search, that is test a predefined set of hyperparameter tuples and select the best one. Another common way is to use expert knowledge and somehow optimize it manually, which sometimes works but is certainly not reproducible. Both of these standard approaches are quite poorly suited. Grid search and manual search become infeasible when the number of hyperparameters grows. It is far better to use true optimization methods. Recently random search was proposed as a good baseline, but this search method does not focus on good regions. Another commonly used approach is the Nelder-Mead simplex, which I strongly advise against, because it cannot cope with the stochastic nature of hyperparameter search and is therefore prone to getting stuck in local minima. I wrote a brief article describing the main challenges of hyperparameter optimization. The best existing methods are all forms of black-box optimization approaches. This is currently heavily researched in machine learning. The current trend leans towards Bayesian optimization methods . A few good software libraries are available that can make tuning easy for you. I recommend Optunity which I developed ( paper ) and Hyperopt , because these two are the easiest to use and can tackle most problems easily. The figure below illustrates that the response surface in hyperparameter optimization has many local optima (and hence is a poor fit for Nelder-Mead). This figure shows cross-validation performance (higher is better) for an SVM with RBF kernel (with hyperparameters $C$ and $\gamma$) based on the trace of Optunity's particle swarm optimization.
