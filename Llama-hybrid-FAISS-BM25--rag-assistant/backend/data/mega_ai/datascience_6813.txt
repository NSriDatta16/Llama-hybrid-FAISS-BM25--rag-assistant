[site]: datascience
[post_id]: 6813
[parent_id]: 731
[tags]: 
While ffriend's answer gives some excellent pointers for learning more about how neural networks can be (extremely) difficult to tune properly, I thought it might be helpful to list a couple specific techniques that are currently used in top-performing classification architectures in the neural network literature. Rectified linear activations The first thing that might help in your case is to switch your model's activation function from the logistic sigmoid -- $f(z) = \left(1 + e^{-z}\right)^{-1}$ -- to a rectified linear (aka relu) -- $f(z) = \max(0, z)$. The relu activation has two big advantages: its output is a true zero (not just a small value close to zero) for $z \le 0$ and its derivative is constant, either 0 for $z \le 0$ or 1 for $z > 0$. A network of relu units basically acts like an ensemble of exponentially many linear networks, because units that receive input $z \le 0$ are essentially "off" (their output is 0), while units that receive input $z > 0$ collapse into a single linear model for that input. Also the constant derivatives are important because a deep network with relu activations tends to avoid the vanishing gradient problem and can be trained without layerwise pretraining. See "Deep Sparse Rectifier Neural Networks" by Glorot, Bordes, & Bengio ( http://jmlr.csail.mit.edu/proceedings/papers/v15/glorot11a/glorot11a.pdf ) for a good paper about these topics. Dropout Many research groups in the past few years have been advocating for the use of "dropout" in classifier networks to avoid overtraining. (See for example "Dropout: A simple way to prevent neural networks from overfitting" by Srivastava, Hinton, Krizhevsky, Sutskever, & Salakhutdinov http://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf ) In dropout, during training, some constant proportion of the units in a given layer are randomly set to 0 for each input that the network processes. This forces the units that aren't set to 0 to "make up" for the "missing" units. Dropout seems to be an extremely effective regularizer for neural network models in classification tasks. See a blog article about this at http://fastml.com/regularizing-neural-networks-with-dropout-and-with-dropconnect/ .
