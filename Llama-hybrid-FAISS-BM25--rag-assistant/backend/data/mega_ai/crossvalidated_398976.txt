[site]: crossvalidated
[post_id]: 398976
[parent_id]: 398966
[tags]: 
There's a lot here to break down. I hate to say it, but some of the advice in your course is quite misguided and wrong. What is that transformation actually doing? I don't mean the nitty gritty math, but what is it doing conceptually? The math here is pretty simple. You have a bunch of measurements of people's age that you would like to use as a feature in predicting some other measurement (looks like the probability of something happening). You're simply creating a new feature which is the logarithm of the original feature. I'll explain why you would want to do this below. For linear and logistic regression, for example, you ideally want to make sure that: the relationship between input variables and output variables is approximately linear – why? This is a structural assumption of the linear and logistic regression models. I'll focus on linear regression, because its a bit simpler, but the same thing holds for logistic regression. The linear regression model makes predictions by building a formula based on the data you feed into the algorithm. All prediction models work this way, but linear regression is distinguished by building the simplest possible formula. If $y$ is the thing you are trying to predict, and $x_1, x_2, \ldots$ are the features you are using to predict it, then the linear regression formula is: $$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_k x_k$$ Here, the $\beta_i$ 's are just numbers, and the job of the algorithm is to determine what numbers work best. Notice that if you vary one of the $x$ 's, and look at how the output $y$ changes as a result, you'll get a line. This is a direct consequence of the way the linear regression model works. If you want it to give you sensible results, then you need to make sure this drawing lines assumption is at least approximately true. the input variables are approximately normal in distribution- why? This is simply wrong. Linear regression works fine even if the distribution of the input variables is highly non-normal. What is important is the relationship between the inputs and outputs, not the distribution of the inputs themselves. This is what I meant by the advice the course being misguided. You don't transform input variables because their distribution is skew, you transform them so that the linear shape the model is trying to draw through your data is reasonable For example, here is a scatterplot I found online of a country's GDP vs. its average life expectancy (attribution is in the image): Clearly, drawing a line through the scatter plot is completely unreasonable, so the linear regression equation: $$ \text{Life Expectancy} = \beta_0 + \beta_1 \text{GDP} $$ is a bad choice for the data. On the other hand, it looks like a logarithmic relationship is reasonable, so something like: $$ \text{Life Expectancy} = \beta_0 + \beta_1 \log(\text{GDP}) $$ looks like it would work a lot better. This is the type of situation where transforming the GDP measurements with a logarithm is a good idea. But it has nothing to do with the distribution of GDP. You can't tell it's a good idea by drawing a histogram of GDP, it's about the relationship between GDP and life expectancy. the output variable is constant variance (that is, the variance of the output variable is independent of the input variables – why? This is a deeper issue of a different nature than the others. For prediction models, it doesn't really matter, so if you're focusing on learning to build good predictive models don't worry about it for now. As a summary, this assumption is intended to support the computation of the sampling distribution of parameter estimates. For example, if you want to say something like "the probability that I would collect data in which the relationship between log(GDP) and Life Expectancy is greater than what I actually observed, even when the there is truly no relationship, is very, very small" you need to be able to compute the sampling distribution of the parameter estimates. There are various assumptions that allow this to be done, and this constant variance assumption is one them. That said, if you're only trying to make predictions, this isn't really relevant. And in no case is the distribution of the input data assumed to be normal, that's just a misconception.
