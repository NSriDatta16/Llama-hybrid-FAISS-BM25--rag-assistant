[site]: crossvalidated
[post_id]: 129071
[parent_id]: 128957
[tags]: 
The prior predictive would have no dependence on $x,t$: $$p(t_{new}|x_{new}, \sigma, \alpha) = \int p(t_{new}|x_{new}, w, \sigma)p(w|\alpha)dw$$ The posterior predictive would have such dependence. In principle, to find a posterior predictive, we use the data to learn about the parameters, and then integrate over our updated beliefs about those parameters. It would typically be expressed: $$p(t_{new}|x_{new}, \sigma, x, t, \alpha) = \int p(t_{new}|x_{new}, w, \sigma)p(w|x,t,\alpha, \sigma)dw$$ (Assuming your example assumes $\sigma$ known, otherwise we would also integrate over the posterior for $\sigma$.) So it would seem that by including $x,t$ in the first term, your professor wishes to stress that the posterior predictive depends—via $w$—on $x,t$. But if the equation you give is a valid expression of the posterior predictive, it would seem this must somehow hold: $$ p(t_{new}∣x_{new},w,t,x,\sigma)p(w|\alpha) = p(t_{new}|x_{new}, w, \sigma)p(w|x,t,\alpha,\sigma)$$ $t_{new}$ is conditionally independent from $x,t | w, x_{new}$, and perhaps your professor's expression somehow leverages that fact, but it isn't immediately clear to me how. This short tutorial might help approach the posterior predictive of linear regression from another angle.
