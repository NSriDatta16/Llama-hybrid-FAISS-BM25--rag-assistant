[site]: datascience
[post_id]: 86055
[parent_id]: 54847
[tags]: 
Since most models are built using pre-trained embeddings the problem of outliers in textual data is not that prominent. This is because the training is done on millions of words/sentences and outliers if any do not have an effect. Coming to the specific problem statement, outliers in textual data could mean many things. For e.g assuming you are collating all news articles related to 'tech'. Now if there is a 'health' article in that corpus, then this is an outlier. Credit card fraud detection is another area where we train models to detect outliers in textual data. The typical way to identify these outliers is via clustering. The techniques vary mildly from paper to paper
