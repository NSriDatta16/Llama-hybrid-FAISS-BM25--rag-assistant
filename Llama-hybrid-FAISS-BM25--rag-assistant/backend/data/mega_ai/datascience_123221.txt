[site]: datascience
[post_id]: 123221
[parent_id]: 123216
[tags]: 
Yes, you are right, Transformer decoders are meant to only attend to previous tokens. From the original article : [...] We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i. And yes, Transformers are deterministic, in the sense that for a given prefix, you will always get the same next token probabilities. This is because they only attend to previous tokens and there is no source of stochasticity whatsoever in the Transformer itself. Note that the decoding process can be stochastic, but that is another matter.
