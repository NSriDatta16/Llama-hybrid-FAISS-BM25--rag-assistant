[site]: stackoverflow
[post_id]: 1249871
[parent_id]: 1249771
[tags]: 
The good news is this: if the DELETE statement will always delete about 3000 rows, the situation may not get any worse as the table grows larger. The structure of your table may have a big effect on how long your DELETE operation takes and on how directly it affects users due to locks. The index "helps" by making it easy to identify the row locator of the ~3000 doomed rows. However, these rows must then be located in the "whole" table (and in every index on the table) and then deleted. A likely reason for this to be slow is that these 3000 rows are spread across the table (and indexes) on separate data pages. There's no one-size-fits-all answer for you, but you should take a good look at the organization and indexing of your table. There may be a way to change the organization and indexing in such a way that the doomed rows will be on fewer data pages and that the query plan for the DELETE won't perform 3000 separate lookups to reach them. If you post the CREATE TABLE and CREATE INDEX statements for [cache], I might have specific suggestions instead of generalizations. Additional remarks: Here are some more thoughts. Do you have a PRIMARY KEY constraint? If not, you have no clustered index, and this means your table is stored as a heap. That's not good, especially for a table that undergoes a lot of activity. Though I don't have all the details, I also agree with Dems below. It should help to have the primary key (which should be clustered) on (cache_event_id,cache_id). Another bottleneck might be the cache data itself. You have INCLUDEd it in three indexes, so you are storing it in four places! I'm only guessing, but it seems very unlikely you have queries that return the cache_data column from many rows at once. As a result, you can get away with storing cache_data in the clustered index only (by default, a clustered index includes all columns). The database tuning advisor is good to give you ideas, but it's not always a good idea to do exactly what it says. How big is the typical cache_data column? If it is almost always large (over 8K in size), it's going to cause a lot of activity with LOB overflow pages. I'm not an expert on workload tuning when there is a lot of LOB activity, but there are probably some good resources with advice. One thing to consider (not until you try index improvements and actually look at memory use, cache hits, etc.) is to consider changes that will allow more table rows to fit on a page: Reconsider whether you need the type nchar(128) for cache_name. (You might, but think about it. Is it always nearly 128 bytes of data? Is the use of Unicode necessary and worth the extra space? If not, maybe nvarchar(128) or varchar(128) is ok.) Consider whether it might be useful to set the 'large value types out of row' option to ON. The default is OFF, and this could cause you to have only one table row per data page on average, but no reduction in the need for LOB overflow pages. Look at the result of sp_spaceused or sys.dm_db_partition_stats to try to assess this. If you have only 1 or 2 rows per page, it might help to change the setting.
