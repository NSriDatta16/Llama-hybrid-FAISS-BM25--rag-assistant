[site]: crossvalidated
[post_id]: 390105
[parent_id]: 
[tags]: 
What are the theoretical/practical reasons to use normal distribution to initialize the weights in Neural Networks?

I'm aware that there are many different practices of initializing the weights when training a neural network. It seems traditionally standard normal distribution is the first choice. Most articles I found argue there are better ways to initialize the weights other than using normal distrubtion, but they did not explain why normal distribution would at least work. (1) I think restricting the weights to have mean at 0 and std at 1 can make the weights as small as possible, which make it convenient for regularization. Am I understanding it correctly ? (2) On the other hand, what are the theoretical/practical reasons to use the normal distribution ? Why not sampling random weights from any other arbitrary distributions? Is it because normal distribution has the maximum entropy given the mean and variance? Having the maximum entropy means it's most possible chaotic and thus making least assumptions about the weights. Am I understanding it correctly?
