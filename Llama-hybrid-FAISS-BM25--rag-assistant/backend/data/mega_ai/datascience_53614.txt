[site]: datascience
[post_id]: 53614
[parent_id]: 42030
[tags]: 
So, there are a number of questions that needs to be answered in this question. First of all, I should say that there is no need to normalize the dataset before feeding it into the network as the embeddings are always normalized if the dataset model architecture is properly defined. Just before merging the values, there is certainly a need to bring the values to fixed range having appropriate dimensions. I believe that just to be double sure about the embedding representations, you must refer to Deep contextualized word representations and GloVe original papers.
