[site]: crossvalidated
[post_id]: 463338
[parent_id]: 236280
[tags]: 
Suppose 80 percent of your data was in group A and 20 percent in group B, and they are almost completely overlapping (according to the variables in your model). Then logistic regression will try to pick a rule (based on linear combinations of your variables, of course) that assigns items in group A the highest (group A probability) and items in group B the lowest probability. The best compromise might be to assign all items an 0.8 probability of being in group A. Then your classification rule (p>0.5) will assign everything to group A, which is probably the best you can do in these circumstances . A similar result might hold even if the classification does have some discriminating power, leaning to classify almost everything into one group could basically give the best fit. You can overcome this with the incorporation of misclassification cost, or else priors. Logistic regression has no idea which miss-classifications you care about. That is why you need to follow up with (using cross-validation or test/training sets, perhaps) ROC curves and lots of classification tables so that you can decide what misclassification rates you can tolerate. It is not necessarily a bad move for a classifier to "throw up its hands" and say, "Yo, just put them all in group A" if that works. For instance, if you have a diagnostic test and decide that false positives are no problem (there is an easy follow up) and false negatives are a disaster, the logical conclusion is to not use the test at all and assign everyone into the positive group.
