[site]: crossvalidated
[post_id]: 178599
[parent_id]: 178535
[tags]: 
From a definitional sense, there is no such thing as "mixing unsupervised learning and supervised learning" since any problem for which you have target variables is by definition supervised learning. When you don't have target variables it's called unsupervised learning. (See the introduction from C. M. Bishop, Pattern recognition and machine learning, 1st ed. 20. Springer, Oct. 2006.) This is complicated because any supervised learning algorithm can be adapted into an unsupervised learning algorithm by letting the targets be the inputs. E.g., autoencoders are unsupervised neural networks. And any unsupervised learning algorithm can be adapted into a supervised one by letting targets be inputs as was done for RBMs (figure 4 here: Hinton, G. E. (2007). To Recognize Shapes, First Learn to Generate images. Progress in Brain Research, 165, 535–547.) But in general, I think there is a clear difference between what typical unsupervised learning algorithms do well, and what typical supervised learning algorithms do well. Unsupervised learning algorithms create features from inputs: sometimes called discovery . Supervised learning algorithms learn mappings from features to targets: sometimes called learning (it's a bad name!) If you have targets, I think it's better to apply supervised learning techniques. Even Boltzmann machines have a final training phase using gradient descent. But if your input features are in an inconvenient space, or are highly correlated (in some space) then transforming them using unsupervised learning is going to help a lot. If both points apply — you have targets and your inputs are correlated — then you should combine both techniques.
