[site]: crossvalidated
[post_id]: 488787
[parent_id]: 488760
[tags]: 
The strength of convolutional layers over fully connected layers is precisely that they represent a narrower range of features than fully-connected layers. A neuron in a fully connected layer is connected to every neuron in the preceding layer, and so can change if any of the neurons from the preceding layer changes. A neuron in a convolutional layer, however, is only connected to "nearby" neurons from the preceding layer within the width of the convolutional kernel. As a result, the neurons from a convolutional layer can represent a narrower range of features in the sense that the activation of any one neuron is insensitive to the activations of most of the neurons from the previous layer. Restricting the range of features in this way can be useful in cases where we expect most of the information to be local . In image classification, for example, a bird will look like a bird based on the pixels in the location of the bird, regardless of its location in the image and regardless of whether there is also a car somewhere else in the image. The utility of this prior expectation is born out by the observation that even CNNs with totally random weights provide features that are nearly as useful for classification as fully-trained CNNs.
