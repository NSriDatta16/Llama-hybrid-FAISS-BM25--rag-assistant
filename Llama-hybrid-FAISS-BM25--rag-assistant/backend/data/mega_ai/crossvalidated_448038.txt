[site]: crossvalidated
[post_id]: 448038
[parent_id]: 266387
[tags]: 
I am sorry, but these answers are dangerously wrong. No, you cannot just flip AUC after you see the data. Imagine you are buying stocks, and you always bought the wrong one, but you said to yourself, then it's ok, because if you were purchasing the opposite of what your model was predicting, then you would make money. The thing is that there are many, often non-obvious reasons how you can bias your results and get consistently below-average performance. If you now flip your AUC, you might think you are the best modeler in the world, although there was never any signal in the data. Here is a simulation example. Notice that the predictor is just a random variable with no relationship to the target. Also, notice that the average AUC is around 0.3. library(MLmetrics) aucs Results Of course, there is no way a classifier could learn anything from the data since the data are random. The bellow chance AUC is there because LOOCV creates a biased, unbalanced training set. However, that doesn't mean that if you don't use LOOCV, you are safe. The point of this story is that there are ways, many ways how the results can have bellow average performance even if there is nothing in the data, and therefore you should not flip the predictions unless you know what you are doing. And since you've got bellow average performance, you don't see what you are doing :) Here is a couple of papers that touched this problem, but I am sure others did as well Classification based hypothesis testing in neuroscience: Below‚Äêchance level classification rates and overlooked statistical properties of linear parametric classifiers by Jamalabadi et al (2016). How to control for confounds in decoding analyses of neuroimaging data by Snoek et al (2019).
