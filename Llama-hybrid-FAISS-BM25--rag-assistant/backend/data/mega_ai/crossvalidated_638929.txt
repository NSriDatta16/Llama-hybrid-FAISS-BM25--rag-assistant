[site]: crossvalidated
[post_id]: 638929
[parent_id]: 
[tags]: 
Cautions or considerations when setting coefficients from linear model into the fixed-effects component of a mixed effects model

A straightforward (single level) regression model, showing the connection between the reading scores of Year 5 students and their corresponding scores when they were in Year 3. Might be formulated like so: $$y_{i,t} = \beta_0 + \beta_1*y_{i,t-1} +e_i \tag{1}$$ Where $y_{i,t}$ represents the reading test score of the $i^{th}$ student at time $t$ (e.g., in Year 5, 2021), $yi,t-1$ is the reading test score of the $i^{th}$ student at a previous time t - 1 (e.g., in Year 3, 2019). I am tasked with developing a multi-level model to estimate the 'value add' of individual schools that the students are nested within. The value add model is a common approach for quality improvement in education. In theory, allow a fair comparison of schools by controlling for student-level factors and school-level contextual factors, and estimate the random-effects component for each school to provide a ranking of positive/negative impact on the 'typical' students' performance. This is given by $$ y_{i,j} = \beta_{0} + \beta_{1}*X_{1ij} + ... + \beta_{n}*X_{nij} + \beta_{n+1}*W_{1j} + ... \beta_{n+k}*W_{kj} + u_{0j} + e_{ij} \tag{2} $$ where $X_{1ij}$ to $X_{nij}$ are $n$ student level controlling factors, which include the student's prior score. $W_{1j}$ to $W_{kj}$ are $k$ school level controlling factors and $\beta_{0}$ is the average performance of all students, conditioned on the student and school factors. In the context of school effect analysis, $u_{0j}$ is of primary interest as it signifies the contribution each school makes to its students' learning beyond what can be predicted from student (e.g., background, prior academic achievement) and school characteristics (e.g., demographic and academic composition). $u_{0j}$ is assumed to follow a normal distribution with a mean of zero and variance of $\sigma_{\mu0}^2$ across the population of schools. Typically, a value added model will estimate random effects for each school (usually random intercept) for the focal year. For example, with it being 2024 we would fit the model on this year's results to derive random effect estimates for this year to then rank order schools and differentiate between schools that benefit student learning growth from those that do not. But what if we want to know if the random effect for School X has improved over time? Or if the schools, within the school system, are improving in general, given by the VA values, stable or declining in general? Typically VA values are mean-centred or scaled to the focal year and therefore our 'average' reference school belongs to the focal year. I am now considering a 'benchmarking' approach by fitting the linear model in equation $(1)$ to historical student performance data (2017), and using the estimated fixed-effect coefficients (eg for prior scores) to 'set' the fixed-effects component in equation $(2)$ for the focal year (2024). I am able to do this using this approach: How to manually set coefficients for variables in linear model? [duplicate] but just updating the function like: setCoeffs First fitting the linear model ( $1$ ) to historical student performance data: mod Reserve the fixed-effect coefficients from $(1)$ : benchmark_weights Then fitting the mixed-effects model $(2)$ to the focal year: frml = And then update the fixed-effects component with the 'benchmark' fixed effects for the 'typical' student in 2017: benchmark_mod_mixed And this gives two mixed-effects models that yield different value-add values per school: va_values Taking the very first school as an example, the fitted 'value add' estimate for School A using 'mod_mixed' where the fixed effects are estimated by the focal year (2024), is -13. Whereas with the 'benchmark_mod_mixed' models, which borrow the fixed effect estimates for the 'average or typical student' from 2017 would be -27.4. Given the first value, and ignoring for a moment the CIs (both negative, under zero line), the takeaway would be that the school, with reference to all other schools in the focal year (2024) has had a negative impact on student growth. But given the second value, we would say that this school's value add for a typical student's growth, benchmarked to 2017 is worse. Furthermore, if we had had the value add for the same school in 2017, we would see whether or not the school has improved or declined in performance over time. But we wouldn't be able to know this with the original approach that uses the focal year's student performance data to estimate the 'typical student' performance. My main question about this is whether this is a reasonable approach to evaluate the quality of an individual school's value add over time. I would also like to know if there are some considerations regarding update.merMod and if the function is appropriate given that it fixes the intercept and fixed effects coefficients from the LM to the MLM. Also, suggestions for alternative approaches would be welcome. I am currently reviewing modernVA package which considers time series in the value add modelling approach as well as other interesting avenues to explore further. All cautions, critiques and considerations warmly welcomed. Use case The following results may help further illustrate the intended use case of a value add model that 'benchmarks' to 'typical' student performance set by a historical cohort period (in this case 2015-2017, year 3 to year 5 results). Here I set the MLM fixed effect coefficients to the Benchmark model or allowed the MLM to re-estimate the typical student performance each year: As expected the centre of the distribution of VA values for the Relative model are zero, whereas the centre of VA values for the Benchmark model varies above and below the zero line. I'm left wondering if the results validly indicate that there has been systemic improvement or decline in student performance over time (eg based on state-wide investment or effects of the pandemic) and if the Benchmark model results for VA values (2019,2021) are appropriately estimated. Does this approach address the critique provided in this paper (page 13): "while value-added measures are likely to be useful for individual schools aiming to improve their performance, all value is added relative to the average school. This means that it is generally not possible for all schools to achieve positive values, or to use the measures to track changes in systemic performance over time. Alternative formulations of the value-added measure (such as measuring value-added relative to a school in a baseline year, or measuring the change in schoolsâ€™ value-added scores) could be used to create a benchmark of performance, but further work is required to validate these models." Or is there a better way?
