[site]: crossvalidated
[post_id]: 624601
[parent_id]: 
[tags]: 
Is there anything like spaced repetition learning for machine learning models?

I would like to know if something like spaced repetition exists but for helping machine learning models learn. Spaced repetition is a flashcard learning method for humans where the algorithm tries to present the card just before the learner might forget it. For example, get a flashcard right and you'll see it again in a day. Get it right again and you'll see it in two days, then four days, and so on. If you get it wrong, you drop right back down to zero days, and you have to do it all over again. It's based on Hermann Ebbinghaus's discovery of the forgetting curve. From 1880-1885, he experimented on himself (because nobody else wanted to do it) by memorizing the words of a fake language. This prevented him from relating known words to the words he's trying to memorize, so he could get better data on raw memorization. The algorithm for determining the wait time is a little more sophisticated nowadays, but the basic idea still holds. (google "anki" if you're interested in trying it) Would applying this idea to machine learning yield any benefits, or does something like this already exist? Basically the same idea but using epochs instead of days to manage when the review should occur. I know catastrophic forgetting is a thing, but maybe there are architectures that will persist in memory if training is done in a similar manner to spaced repetition. The idea makes me think of batching, sometimes you can get out of local minimums if you don't focus on the gradient of all your examples at the same time. Also a few details: How do you gauge if the model "got it right?" For example, say you're training a classification model, is it correct when it has 90% certainty on the correct category? Maybe you could slide the percentage required to be correct higher as the training goes on for each card to coax it into the 99.99% territory? Maybe there's a better general method for determining when the review should occur based its history of certainties on that card? Is there danger for bias when using this method? (i.e.: it trains harder on the ones it has trouble with, so then it gets better at that but gets worse at the ones that it had deemed easy, and see-saws back and forth?) Do you have to do regular training for some interval before turning on spaced repetition in order to get this to work? Thanks for reading.
