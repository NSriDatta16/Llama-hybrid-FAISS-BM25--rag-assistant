[site]: crossvalidated
[post_id]: 582350
[parent_id]: 
[tags]: 
Improving a forest model by dropping features below a percent importance threshold?

I'm wondering if there is a term for this process, where I can find more reading/information about it, and if this is valid or hacky overall. I've only used this process for tree-based models which have easily accessible feature importances (random forest, xgboost, etc.) The process is as follows: I train a preliminary model with all the features, and track each feature's % importance. I then loop through different thresholds (e.g. 0%, 2%, 4%...) and drop all the features, from the original model, that have a percent importance below the threshold. I then choose the model, features, and threshold that yield the best accuracy, since that is what I am interested in for this particular case. Is this a known approach, and is there anything wrong with it? Where can I find more information about it?
