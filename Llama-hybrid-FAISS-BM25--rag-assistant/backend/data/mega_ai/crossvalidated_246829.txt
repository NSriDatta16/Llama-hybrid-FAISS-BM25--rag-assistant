[site]: crossvalidated
[post_id]: 246829
[parent_id]: 
[tags]: 
Are there better estimators of misclassification error than the fraction of misclassified test points?

Assume we train a binary classification model using the training set. Also assume that the model returns an estimate of the probability of success $\hat f(x)$ for every feature vector $x$ and was trained with "an intent of" minimizing out of sample cross-entropy error (maximizing likelihood). Moreover, assume we actually picked the algorithm based on the training data (but never looked at the test data), so we don't trust cross-validation (because training data can't be "unseen" by researcher who created an algorithm to train $\hat f$). We are interested in the estimation of the out of sample misclassification error $E_{out} = P((\hat f(x) \geq 0.5) \neq y)$. The commonly used approach is to interpret the number of misclassified points $n_{\text{test,misclassified}}$ in the test set as an observation of a binomial random variable with probability $p = E_{out}$ and number of trials $n = n_{\text{test}}$ (number of points in the test set). Then the classical estimate would be $\hat E_{out} = n_{\text{test,misclassified}} / n_{\text{test}}$. This gives an estimator with 0 bias but potentially high variance (if the test set is small). We may want to use additional knowlede we have to reduce the variance. We have e.g.: Test and training sets, Estimated probabilities $\hat f(x)$. Are there any commonly occurring situations where we can leverage any additional knowledge to provide a better estimate of misclassification error?
