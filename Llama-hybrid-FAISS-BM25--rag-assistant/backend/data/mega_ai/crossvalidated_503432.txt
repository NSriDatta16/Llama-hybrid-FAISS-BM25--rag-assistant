[site]: crossvalidated
[post_id]: 503432
[parent_id]: 503213
[tags]: 
Gaussian-process (GP) regression is almost always handled in a Bayesian manner. When people talk about "picking the right kernel" for their GP, they mean for the prior, and it has a dramatic effect on the predictions they make once updated via data to their posterior. The article you linked makes this idea pretty clear. The author's use of the term "confidence interval" is a misnomer. They are just plotting the spread as dictated directly by the covariance matrix of the assumed Gaussian distribution. These are actual probabilities over the thing being estimated. I.e. your picture of the author's prior could be described as " $f(x_i)$ under the prior has a 95% probability of being between -2 and 2 $\forall i$ ." The "marginal likelihood" maximization in Eq.11 is not the "training" (the "training" is the Bayesian updating of the prior GP to the posterior GP). Rather, it is sort of data-driven tuning of the prior by selecting kernel hyperparameters that maximize the "evidence" (denominator in Bayes rule), which the author calls "marginal likelihood."
