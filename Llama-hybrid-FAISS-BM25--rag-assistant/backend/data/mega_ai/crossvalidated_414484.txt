[site]: crossvalidated
[post_id]: 414484
[parent_id]: 414154
[tags]: 
Addressing your last paragraph first: Kathryn Masyn, who wrote [a chapter][1] about latent class and latent profile models in the Oxford Handbook of quant methods (2013), described entropy as a characteristic, not a model selection criterion. On page 570 of the PDF, she says (emphasis mine): Because even when $E_K$ is close to 1.00 there can be a high degree of latent class assignment error for particular individuals, and because posterior classification uncertainty may increase simply by chance for models with more latent classes, $E_K$ was never intended for, nor should it be used for, model selection during the class enumeration process. However, $E_K$ values near 0 may indicate that the latent classes are not sufficiently well separated for the $K$ classes that have been estimated (Ramaswamy et al., 1993). Masyn is talking about normalized entropy above, which ranges from 0 to 1. I'm not sure if poLCA gives normalized or non-normalized entropy. Regardless, entropy should not be able to equal infinity. Normalized entropy is defined in Masyn as: $E_K = 1 - \frac{\sum_{i = 1}^n\sum_{k = 1}^K[-\hat p_{ik}ln(\hat p_{ik})]}{n * ln(K)}$ Where $n$ indexes individual observations, $K$ indexes all the latent class you identified (e.g. say your final model is 4-classes, $K=4$ ), and $\hat p_{ik}$ is the model-estimated probability that the $i$ -th person belongs to the $k$ -th latent class (i.e. there are $K$ such probabilities for each person). If entropy is displaying as NaN , I'd suspect some sort of error, and it's not saying the entropy is actually infinity. Maybe it's a programming error (unlikely?). Maybe this is what happens if some observations have some missing data (also unlikely, I think?), or maybe some observations have all indicators missing and this is causing their probabilities to be missing? I'd email the author. No way to provide insight from this end, especially considering that I don't use this package. A later update: the R package referenced in the question defines entropy differently I've been using the R package poLCA , written by Drew Linzer. He calculates entropy quite differently. I haven't verified if this produces equivalent or comparable results to the formula I gave above. If I read the manual correctly, he sums up: $E_L = -\sum_c p_c * ln(p_c) $ Where the subscript L is for Drew Linzer, the package author, $p_c$ is the probability in each cell of the cross-classification table generated by the model, and $c$ indexes cells. By cross-classification table, he means the probability of each response pattern. With $i$ indicators, each of which has $j$ levels, there are $j^i$ total response patterns. This doesn't change the statement that entropy can't be infinity, even if it's not normalized. However, in this formulation, the maximum value of $E_L$ is the natural log of the total number of cells in the cross-classification table. Thus, if you normalized entropy using the denominator I provided above, that was wrong. I apologize if this has led anyone astray; I was only familiar with the formulation I originally quoted (which I believe MPlus uses). You can verify this yourself by reading the manual entry for the poLCA.entropy function itself. Linzer gives an example of using the carcinoma dataset provided by the package, and fitting a 2-class LCA model with 7 binary indicators (A thru G, coded as 1s and 2s) to the data. f $probs,ncol))) poLCA.entropy(lca2) ```` If you run that code, you will find that the stated max entropy is 4.85203, the exponent of which is 128. That is $ 2^7$. [1]: https://www.statmodel.com/download/Masyn_2013.pdf
