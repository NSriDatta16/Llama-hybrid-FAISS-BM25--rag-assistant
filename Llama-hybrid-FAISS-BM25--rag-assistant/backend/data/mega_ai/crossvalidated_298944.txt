[site]: crossvalidated
[post_id]: 298944
[parent_id]: 
[tags]: 
How do we define value functions for episodic reinforcement learning tasks?

IIUC, the value function of a state $s$ given a policy $\pi$, $v^{\pi}(s)$, is the expected (discounted) return starting from $s$. The confusing part for me is that if it is possible to revisit a state in an episodic task and each episode has at most $\tau$ time steps, the time that we see a state is relevant. Is there an underlying assumption that the MDP of episodic tasks is a DAG and the maximum number of time steps in an episode $\tau$ is just the length of longest path in the DAG (and not a artificially imposed limit)?
