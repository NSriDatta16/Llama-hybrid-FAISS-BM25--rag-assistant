[site]: datascience
[post_id]: 47430
[parent_id]: 45578
[tags]: 
Let us assume our model to be described by $y = f(x) +\epsilon$ , with $E[\epsilon]=0, \sigma_{\epsilon}\neq 0$ . Let furthermore $\hat{f}(x)$ be our regression function, i.e. the function whose parameters are the ones that minimise the loss (whatever this loss is). Given a new observation $x_0$ , the expected error of the model is $$ E[(y-\hat{f}(x))^2|x=x_0]. $$ This expression can be reduced (by means of more or less tedious algebra) to $$ E[(y-\hat{f}(x))^2|x=x_0] = \sigma_{\epsilon}^2 + (E[\hat{f}(x_0)]-f(x_0))^2 + E[\hat{f}(x_0)-E[\hat{f}(x_0)]]^2 $$ where the second term is the difference between the expected value of our estimator $\hat{f}$ and its true value (therefore the bias of the estimator) and the last term is the definition of variance. Now for the sake of the example consider a very complex model (say, a polynomial with many parameters or similar) which you are fitting against the training data. Because of the presence of these many parameters, they can be adapted very closely to the training data to even the average out (because there is many of them); as a consequence the bias term is reduced drastically. On the other hand, though, it is generally the case that whenever you have many parameters their least square estimations come with high variance: as already mentioned, since they have been deeply adapted to the training data, they might not generalise well on new unseen data. Since we have many parameters (complex model) a small error in each of them sums up to a big error in the overall prediction. The converse situation may happen when one has a model that is very static (imagine very few parameters): their variances do not sum up very much (because there is few of them) but the trade-off is that their estimation of the mean might not correspond closely to the true value of the regressor. In the literature one refers to the former behaviour as overfit , to the latter as underfit . In the description I have given you can see that they may be related to the complexity of the model but need not necessarily be, namely you may as well have particularly complex models that do not necessarily overfit (because of the way they are constructed, one above all is random forest) and simple model that do not necessarily underfit (for instance linear regressions when the data are actually linear).
