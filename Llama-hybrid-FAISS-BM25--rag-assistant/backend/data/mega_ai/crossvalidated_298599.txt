[site]: crossvalidated
[post_id]: 298599
[parent_id]: 298560
[tags]: 
The Forecasting: principles and practice book by Rob J. Hyndman and George Anthanasopoulos answers your question: STL has several advantages over the classical decomposition method and X-12-ARIMA: Unlike X-12-ARIMA, STL will handle any type of seasonality, not only monthly and quarterly data. The seasonal component is allowed to change over time, and the rate of change can be controlled by the user. The smoothness of the trend-cycle can also be controlled by the user. It can be robust to outliers (i.e., the user can specify a robust decomposition). So occasional unusual observations will not affect the estimates of the trend-cycle and seasonal components. They will, however, affect the remainder component. On the other hand, STL has some disadvantages. In particular, it does not automatically handle trading day or calendar variation, and it only provides facilities for additive decompositions. So STL can deal with phenomena such as multiple seasonalities, high-frequency seasonalities (e.g. 365 for daily data) and cycles. However if you have daily data you can go for a model which tackles the thematic of multiple seasonalities, e.g. TBATS instead of STL. If you have many multiple seasonalities and millions or billions of observations you can go for data-savvy complex models such as recurrent neural nets. STL might be a useful approach for modeling business cycles. In a business cycle not every cycle has exact the same length, but they are rather an irregularly recurring phenomenon. Sometimes recession might last 2 years and sometimes it might last 5 or 6. STL is more able to capture this kind of uncertainty than ARIMA. Also if you cannot make your data stationary STL will be more useful than ARIMA. ARIMA requires the data to be stationary or at least to be stationary in differences. You can also take a part of your data as test set and test whether on your particular dataset STL works better than ARIMA. STL is usually used for understanding and describing the data, but you can combine it with simple forecasting methods as described in chapter 6.6 of the book I mentioned above . These simply forecasting methods are not always "worse" than complex methods such as Auto-regressive models, neural nets or Bayesian models such as Kalman filter. You can compare them for instance by using scaled errors (e.g. MAE and RMSE) on the training and the test set of your data.
