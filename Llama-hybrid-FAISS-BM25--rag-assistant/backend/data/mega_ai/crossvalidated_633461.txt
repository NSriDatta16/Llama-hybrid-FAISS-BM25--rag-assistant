[site]: crossvalidated
[post_id]: 633461
[parent_id]: 621726
[tags]: 
I believe their method is to have a seperate encoder for user and item information. Each encoder outputs the mean and variance of a gaussian in an embedding space ("probability density function framework"). Then, they use wasserstein distance between users and items as a measure of similarity. Finally, they use some sort of representation learning loss formulation such as InfoNCE to fit the model ("self-learning"?) Sounds similar to Probabilistic Metric Learning with Adaptive Margin for Top-K Recommendation ( manuscript ).
