[site]: crossvalidated
[post_id]: 154896
[parent_id]: 154827
[tags]: 
Theano creates a symbolic graph. This graph allows it to compute derivatives based on the connected inputs, the Op implemented on the Variables, and the output(created by the Apply Node). import theano.tensor as T x = T.dmatrix('x') y = T.dmatrix('y') z = x + y The Apply nodes are blue, Variables are red, Op is green, and Types are purple. As given in the theano official documentation, Having the graph structure, computing automatic differentiation is simple. The only thing tensor.grad() has to do is to traverse the graph from the outputs back towards the inputs through all apply nodes (apply nodes are those that define which computations the graph does). For each such apply node, its op defines how to compute the gradient of the node’s outputs with respect to its inputs. Note that if an op does not provide this information, it is assumed that the gradient is not defined. Using the chain rule these gradients can be composed in order to obtain the expression of the gradient of the graph’s output with respect to the graph’s inputs . Comparing with the Python language, an Apply node is Theano’s version of a function call whereas an Op is Theano’s version of a function definition. While finding derivatives by hand is simple for feed forward neural networks, it becomes exceedingly complex in the case of Recurrent Neural Networks and Long Short Term Memory Cells, especially if the network is deep.
