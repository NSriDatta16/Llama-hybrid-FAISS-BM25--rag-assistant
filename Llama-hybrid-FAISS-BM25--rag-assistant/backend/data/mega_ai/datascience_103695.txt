[site]: datascience
[post_id]: 103695
[parent_id]: 66893
[tags]: 
The process of dividing a piece into small units is called tokenization . Most tokenization systems are combinations of hard-coded rules (e.g., string methods or regular expression) and learned rules (e.g., machine learning). Many tokenizations can be solved with hard-coded rules. Then the tokens can be one-hot encoded. Here is some code to get started: import re words = ["1-(aminoiminomethyl)-N'-[2,3,6-tri-O-benzoyl-4-O-(2,3,4,6-tetra-O-benzoyl-α-D-glucopyranosyl)-β-D-glucopyranosyl]-", "2,4,5-trideoxy-2-[(16-mercapto-1-oxohexadecyl)amino]-1,3-O-(1-methylethylidene)-6-O-undecyl-", "octahydro-7-hydroxy-1-[[2-O-(4-hydroxybenzoyl)-α-D-allopyranosyl]oxy]-7-methyl-",] tokens = [] for word in words: # Remove non-informative characters word = word.replace("[", "").replace("]", "").replace("(", "").replace(")", "") # Split tokens.append(re.split(r"-", word))
