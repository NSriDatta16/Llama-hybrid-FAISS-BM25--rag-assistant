[site]: crossvalidated
[post_id]: 242486
[parent_id]: 238090
[tags]: 
Two facts: When you regress one random walk on another random walk and incorrectly assume stationarity, your software will generally spit back statistically significant results, even if they are independent processes! For example, see these lecture notes. (Google for spurious random walk and numerous links will come up.) What's going wrong? The usual OLS estimate and standard-errors are based upon assumptions that aren't true in the case of random walks. Pretending the usual OLS assumptions apply and regressing two independent random walks on each other will generally lead to regressions with huge $R^2$ , highly significant coefficients, and it's all entirely bogus! When there's a random walk and you run a regression in levels the usual assumptions for OLS are violated, your estimate does not converge as $t \rightarrow \infty$ , the usual central limit theorem does not apply, and the t-stats and p-values your regression spits out are all wrong . If two variables are cointegrated , you can regress one on the other and your estimator will converge faster than usual regression, a result known as super-consistency. Eg. checkout John Cochrane's Time Series book online and search for "superconsistent."
