[site]: crossvalidated
[post_id]: 414481
[parent_id]: 
[tags]: 
why does random forest trees need to be deeper than gradient boosting trees

in Elements of Statistical Learning chapter 15. Random Forest, we see authors' note on RF v.s. GBT. One of them is that at 1000 terms, GBM depth 4 has smaller error than RF depth 6. Also we notice RF depth 6 has smaller error than RF depth 2. This led me to think that does random forest trees need to be set deeper than trees in GBT? If so, what would be the mathematical proof to it? One guess I have is in each RF tree we only use a subset of variables, hence to achieve same descent in error naturally we need more depth, but not sure if my direction is correct. Thanks for your inputs!
