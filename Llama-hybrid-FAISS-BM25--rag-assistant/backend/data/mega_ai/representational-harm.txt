Systems cause representational harm when they misrepresent a group of people in a negative manner. Representational harms include perpetuating harmful stereotypes about or minimizing the existence of a social group, such as a racial, ethnic, gender, or religious group. Machine learning algorithms often commit representational harm when they learn patterns from data that have algorithmic bias, and this has been shown to be the case with large language models. While preventing representational harm in models is essential to prevent harmful biases, researchers often lack precise definitions of representational harm and conflate it with allocative harm, an unequal distribution of resources among social groups, which is more widely studied and easier to measure. However, recognition of representational harms is growing and preventing them has become an active research area. Researchers have recently developed methods to effectively quantify representational harm in algorithms, making progress on preventing this harm in the future. Types Three prominent types of representational harm include stereotyping, denigration, and misrecognition. These subcategories present many dangers to individuals and groups. Stereotypes are oversimplified and usually undesirable representations of a specific group of people, usually by race and gender. This often leads to the denial of educational, employment, housing, and other opportunities. For example, the model minority stereotype of Asian Americans as highly intelligent and good at mathematics can be damaging professionally and academically. Representational harm happens when the representation of details teams improves damaging stereotypes, developing social exclusion and prejudice. This experience is particularly noticeable in the depiction of marginalised groups, containing people of color, women, LGBTQ+ people, and people with handicaps. Media depictions of these groups generally stop working to catch their array and intricacy. Instead, they are typically reduced to one-dimensional caricatures, which ultimately continue social prejudices. These organised depictions contribute to the help of hazardous stereotypes and the marginalisation of these locations. Denigration is the action of unfairly criticizing individuals. This frequently happens when the demeaning of social groups occurs. For example, when searching for "Black-sounding" names versus "white-sounding" ones, some retrieval systems bolster the false perception of criminality by displaying ads for bail-bonding businesses. A system may shift the representation of a group to be of lower social status, often resulting in a disregard from society. Research shows that hazardous depictions in the media can have substantial emotional and social impacts on both individuals and areas. Lawrence Bobo examined the issue of Ethnic stereotype in film, tv, and marketing. African Americans are commonly received duties specified by features such as "violent tendencies," "laziness," or being "merely for contentment features." While these representations might appear varied externally, they stay to boost underlying frameworks of white prominence and racial inequality. As a circumstances, Black individuals are frequently represented as law offenders or in secondary roles, which adds to the support of Ethnic stereotype and Institutional racism. Misrecognition, or incorrect recognition, can display in many forms, including, but not limited to, erasing and alienating social groups, and denying people the right to self-identify. Erasing and alienating social groups involves the unequal visibility of certain social groups; specifically, systematic ineligibility in algorithmic systems perpetuates inequality by contributing to the underrepresentation of social groups. Not allowing people to self-identify is closely related as people's identities can be 'erased' or 'alienated' in these algorithms. Misrecognition causes more than surface-level harm to individuals: 