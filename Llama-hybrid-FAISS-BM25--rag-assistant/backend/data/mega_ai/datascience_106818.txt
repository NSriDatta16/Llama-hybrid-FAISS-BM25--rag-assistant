[site]: datascience
[post_id]: 106818
[parent_id]: 
[tags]: 
CNN auto-encoder performs much worse than Fully connected auto-encoder

I am trying to develop a one-class classifier, that will learn regular examples, and, hopefully, will have hard times reconstructing anomaly observations. I have 1D signals which I tried to reconstruct using the following three methods: PCA A vanilla Auto Encoder - 3 fully connected layers with relu activations CNN Auto Encoder, 4 chunks of 1D CNN, followed by batch norm and max-pooling layers. The PCA reconstruction error was Where on the X axis is the number of PCs. The vanilla autoencoder loss (MAE) over epochs was Which was nice, but didn't learn as the PCA. The CNN's loss over time was the following As shown in the plot above, the validation loss was really spiky which may indicate that the model overfitted the training set. Nevertheless, even if it isn't clear from the scales, it did fit the training as the other two methods. I have about 10K signals in the training set and about 2K in the validation set. Does the loss indicate I should increase the batch size? Add L1/2 layers? Change the architecture? how? Thanks!
