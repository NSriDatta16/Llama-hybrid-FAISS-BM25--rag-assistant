[site]: datascience
[post_id]: 118211
[parent_id]: 118210
[tags]: 
This can only be answered with actual experiments with your data, model, and training setup. However, previous research (see Sequence Length is a Domain: Length-based Overfitting in Transformer Models , published at EMNLP'21) suggests that Transformers do not generalize well to unseen sequence lengths: We showed in our targeted experiment that vanilla Transformer sequence-to-sequence models have a strong tendency to overfit with regard to the target side length of the training sequences. On a simple algorithmic task, we documented that Transformer can generalize very well to unseen examples within the same length bucket but falls short if the same task is required for input of a different length, shorter or longer. The algorithm of the task, even if very simple, is not learned.
