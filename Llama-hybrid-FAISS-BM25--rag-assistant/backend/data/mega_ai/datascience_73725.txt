[site]: datascience
[post_id]: 73725
[parent_id]: 
[tags]: 
Implementing neural Network using caltech course

I am trying to implement neural network using numpy using this lecture https://www.youtube.com/watch?v=Ih5Mr93E-2c&list=PLD63A284B7615313A&index=10 at timestamp 59:40. In this lecture there is no separate bias vector used. As you can see top neural node output is constant 1 and multiplied with weight. I could implement forward propagation but I have problem in backward propagation while updating weights I'm stuck near at "Update the weights" equation. When I multiply x vector with delta vector dimension doesn't match with weight matrix. For example consider NN of 2,4,2 where 2 is input,4 is hidden and 2 is output layer. Now I want to update weight of hidden layer which is of 3x4 dimension(3 cuz of 1 extra constant node).Now when I multiply two vectors that is activation in layer 1(2x1) and transpose delta of hidden layer(4x1) I get 2x4 matrix which I can't use it to subtract with weights. Can somebody tell what I am doing wrong. As I can't afford to go university so this place is only hope for me. Any clue would be great help for me. Thank you
