[site]: crossvalidated
[post_id]: 616565
[parent_id]: 614383
[tags]: 
This is actually not an error - it is possible for the effective sample size to be larger than the actual sample size. This means that your MCMC samples provide more information about the parameter, than if you had obtained the same number of independent samples directly from the target (posterior) distribution. To get some intuition about what this means: Imagine that we are doing a Bayesian analysis of a problem where the true posterior distribution for some parameter, $\theta$ , (unbeknownst to us) is a normal distribution with $\mu=172$ and $\sigma=12$ . If an omniscient statistician were to estimate the posterior mean, $\mu$ , for that parameter by directly drawing $N=100$ samples from the posterior $N(172, 12)$ distribution and taking the average, then the standard error of the mean for that estimate would be: $$ \textrm{SE} = \frac{\sigma}{\sqrt{N}} = \frac{12}{\sqrt{100}} = 1.2 $$ This means that if the omniscient statistician were to repeat this experiment many times (drawing $100$ random samples from the $N(172,12)$ distribution and each time computing the sample mean), then the resulting values would be distributed around the true value, $\mu=172$ , with a standard deviation of approximately $1.2$ . If we instead use MCMC to estimate $\mu$ using $N=100$ iterations (post warmup), and learn that $\textrm{ESS}>N$ then this means that our uncertainty about $\mu$ is less than $1.2$ : if we were to run the MCMC for 100 iterations multiple times, then the posterior means from each run would be distributed around the true value with a standard deviation that was less than $1.2$ . This can happen when consecutive MCMC samples are negatively correlated: ESS is often defined as: $$ \textrm{ESS} = \frac{N}{1 + 2 \sum_{n=1}^{\infty}\rho_n} $$ where $\rho_n$ is the lag-n autocorrelation (i.e., the correlation between the samples and the samples shifted n steps). If there is negative autocorrelation for odd lags then the denominator can be less than 1 leading to the situation where $\textrm{ESS}>N$ . Stan reference manual: Effective Sample Size Andrew Gelman blog: Simple example of anticorrelated samples Wikipedia: Antithetic variates Aki Vehtari GitHub: Comparison of MCMC effective sample size estimators
