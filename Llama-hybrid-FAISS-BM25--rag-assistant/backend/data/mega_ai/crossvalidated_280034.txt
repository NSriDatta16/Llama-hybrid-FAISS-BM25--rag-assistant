[site]: crossvalidated
[post_id]: 280034
[parent_id]: 
[tags]: 
Simple expert weighting algorithm

I am ensembling neural networks for my research project. Each network is considered an expert. I am looking for a good algorithm to dynamically weight them, and I am finding all the prior algorithms to be to very mathematically complex. Is there anything wrong with just doing something like this: Percentage of total accuracy ($1- percent\; of \; total\; cost$) achieved by learner $j$: $$\gamma_j = 1 - \frac{C_j}{\sum_{j=1}^n C_j}$$ Normalized accuracy/weight so that all final weights sum to 1: $$w_j = \frac{\gamma_j}{\sum_{j=1}^n \gamma_j}$$ Final prediction: $F(x) = \sum_{j=1}^n w_j \cdot f_j(x)$ where $f_j(x)$ is the predicted output by the $j$th expert. This is really simple, so why are the rest have to be so complicated? I albeit, have not tried this one, but is there anything wrong with it in theory? P.S. I know this may be a little specific, but I feel quite stuck.
