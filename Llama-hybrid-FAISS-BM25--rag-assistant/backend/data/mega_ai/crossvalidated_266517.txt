[site]: crossvalidated
[post_id]: 266517
[parent_id]: 266504
[tags]: 
The normal family of distributions for a regression model $Y_i \sim \text{N}(\mu_i=x_i^T \beta,\sigma^2)$. This means that we assumes that our random variable $Y_i$ have some normal distribution with some unknown mean $\mu_i=x_i^T \beta$ and unknown variance $\sigma^2$. This says that, whatever is the value of the predictor variables $x_i$, the variance of the observation is the same. That is homoskedasticity! We could have written the model in some other way, such as (lets say with one predictor variable $z$) the distribution is $Y_i \sim \text{N}(\mu_i=\beta_0+\beta_1 z,\exp(\gamma z))$, where the variance is some function of the predictor $z$, depending on parameters to be estimated. Such a model would be heteroskedastic, but is not a part of the usual glm framework. In R such models can be estimated with the gamlss package (and certainly others). EDIT Answer to extra question in comment: Look at the binomial family. For the normal family, we have two independent parameters, so we can specify mean and variance separately. Not so for the binomial model! If $Y \sim \text{Bin}(n,p)$, where $n$ is not considered usually a parameter, since it is defined by the data, we have only one parameter $p$. Then we can calculate that the expectation is $\mu=n p$ and the variance is $\sigma^2= n p (1-p)$. Note that we can write the variance as a function of the expectation : $\sigma^2 = \mu (1-\frac{\mu}{n})$. So, in say, logistic regression, when we estimate in some way the expectation as a function of covariables, that will determine automatically the variance as a function of covariables. There is no freedom in estimating the variance as soon as we have estimated the mean, is it already determined. The same happens for the Poisson family.
