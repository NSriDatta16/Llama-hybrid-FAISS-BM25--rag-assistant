[site]: crossvalidated
[post_id]: 614205
[parent_id]: 
[tags]: 
What is it called when an outlier falls out of a rolling window statistical calculation?

I have a time series $X_t \sim N(0, 1)$ . There is a single outlier at index 347, at 8.5 standard deviations from the mean. If I now compute a rolling window standard deviation of $X_t$ with window size 100, we of course see a large spike at index 347. Once this observation has fallen out of the 100 period window, the rolling standard deviation falls back down to normal levels at index 447 (347+100=447). This is all as expected. My question is, what is this phenomenon formally called, because I wan't to research it more to be able to understand it and mitigate it. Specifically, once the outlier observation falls out of the rolling window, I do not want the standard deviation estimation to jump back down to normal levels. I know some ways to mitigate this may be to time weigh or exponential-decay weigh the observations prior to estimating the standard deviation. But I am wondering what this phenomenon is called, if it even has a name, and if there are potentially other ways to mitigate it. EDIT: Just for clarity, I am using an equal weighted rolling window, and at each point in time $t$ I can only use observations up to and including $t$ i.e. at time $t$ , all observations $x$ used to estimate the standard deviation must be in $x_l, l\leq t$ . This is to avoid lookahead bias. So any method that uses "future" information is not allowed.
