[site]: crossvalidated
[post_id]: 224171
[parent_id]: 223740
[tags]: 
Usually, performing feature selection inside the inner loop would be the safer option. Think about if you are able to tune your feature selection with certain parameters too - like the amount of correlation you allow, information you preserve, or similar. If you want to optimize those, not doing so in the inner loop would likely leave you with an overly optimistic error estimate (as you don't have a separate inner-loop performance estimation anymore). Therefore doing such things in the inner loop and using the outer loop for the final error estimation would usually be the way to go. Update: I tried to sketch a workflow that I think should be applicable for your problem, in as few steps as possible (see below, I hope I didn't mess anything up). If you want to check out more details, consider reading one of those papers: Varma & Simon (2006). "Bias in error estimation when using cross-validation for model selection." BMC Bioinformatics, 7: 91 Cawley & Talbot (2010). "On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation." Journal of Machine Learning Research, 11: 2079-2107 Do data partitioning (train subjects/test subjects) Do e.g. repeated CV (leave-subject-out-CV) on train data: For every subject in N: Leave out N1 Leave out N2 Fit all combinations of feature selection parametrization, hyperparameters, etc. on N-N1-N2 Evaluate and remember performance for all on the inner left out subject N2 Evaluate and remember performance for all on the outer left out subject N1 Select "best" parametrization from performance of leaving out N2 Report CV model performance from performance of leaving out N1 Train final model from all training data using chosen "best" parametrization Test final model - double check that model does what it should
