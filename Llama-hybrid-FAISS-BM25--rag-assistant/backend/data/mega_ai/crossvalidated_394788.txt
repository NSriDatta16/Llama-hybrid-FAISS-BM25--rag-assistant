[site]: crossvalidated
[post_id]: 394788
[parent_id]: 393572
[tags]: 
Your questions are aligned to the topic UNSUPERVISED PRETRAINING which is well explained to in this chapter of Deep Learning Book. The answer to your first two questions are well explained to you by @Wayne. I have more insights for you for the third question. I think for "deep autoencoder" you usually start from the random initialization and while training we try to optimize all the layers simultaneously given the objective function. Which makes the solution deep autoencoder more vulnerable to the random initialization. More deep is the network more randomness is there in the network and sometimes your model may not converge to the optimal minimum. On the other hand, the idea of Unsupervised pre-training is you train all the hidden layers greedily (and individually) and then you stack them up again. Hence you get "stacked deep autoencoder". Now the advantage is instead of random initialization, all the hidden layers already have a lot of information encoded about the training data. And empirically it has been shown that this method is reliable and usually converges to better local minimum. Hence, I think the main idea is to have a better initialization strategy with "stacked deep autoencoder".
