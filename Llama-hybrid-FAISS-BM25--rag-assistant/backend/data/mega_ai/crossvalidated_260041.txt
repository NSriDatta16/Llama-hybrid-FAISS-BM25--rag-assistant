[site]: crossvalidated
[post_id]: 260041
[parent_id]: 89036
[tags]: 
I think rapaio is conflating a couple key concepts and in doing so misinterpreted the OP's question. Yes, the bootstrap samples utilized within a bagging algorithm are IID. However, the bagging estimator is ID, NOT IID . The bagging algorithm will generate B trees and the corresponding prediction estimates, $\{\hat{f}^b(X)\}_{b=1}^B$. Since the tree estimator estimated each tree using draws from the same distribution, the identical distribution assumption will hold. However, the independence assumption will not hold!!! For example, imagine that there is one very strong predictor within the data. In each tree this strong predictor will likely be the first split. Therefore, the prediction of most trees will be similar. Said another way, the predictions will be correlated (i.e. not independent ). Think about it, the bagging algorithm is taking a sequence of IID random variables (i.e. the bootstrap samples) and turning them into a sequence of ID random variables (by generating tree estimates) The bagging algorithm is still helpful. The bagging estimator is unbiased; bias is unaffected the lack of independence. Therefore the average of $\hat{f}^b(X)$ will be the same as the expected value of any tree, i.e. $E(f^b(X)) = \frac{1}{B} \sum_{i=1}^B \hat{f}^b(X)$. The variance of the bagging estimator will, however, be affected by non-independence i.e. remember $Var(X+Y) = Var(X) + Var(Y) + 2Cov(X,Y)$. It turns out that bagging estimator will have a smaller variance then a tree estimator (see pg 518 Elements of Statistical Learning). However, we can further reduce estimator variance by attempting to decorrelate the trees. This is where the notion of Random Forest comes from. Again see pg 518 Elements of Statistical Learning or pg 319 Introduction to Statistical Learning for more.
