[site]: stackoverflow
[post_id]: 2790366
[parent_id]: 2789722
[tags]: 
The only answers I see are: Use a small sample to time the transfer rate. Time the actual data in chunks (say 1k) and report the average. Some of the issues complicating the matter: The processor bandwidth of the sending machine (i.e. other tasks running). Traffic density on the network. Tasks running on the client machine. Architecture of all machines. Since the client may be running other tasks, and the host (sending machine) will be running different tasks, the transfer rate will vary. I vote for sending a chunk of data timing it, sending another and timing it. Accumulate these durations and average over the number of chunks. This allows for a dynamic timing, which would be more accurate than any precalculated timing.
