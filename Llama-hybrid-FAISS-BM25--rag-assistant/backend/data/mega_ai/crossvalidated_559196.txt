[site]: crossvalidated
[post_id]: 559196
[parent_id]: 540800
[tags]: 
There's actually a way to pull this off using Pipeline base estimators with simple custom transformers. Create a custom transformer for each classifier. This will be a simple transformer simply slicing the data. It can be achieved by using a lambda function inside sklearn's FunctionTransformer . Create a two-step Pipeline for each base estimator with its corresponding transformer Create a StackingEstimator with its base estimators being the Pipelines from (2) from sklearn.datasets import make_classification from sklearn.ensemble import StackingClassifier from sklearn.preprocessing import FunctionTransformer from sklearn.linear_model import LogisticRegression from sklearn.svm import LinearSVC from sklearn.pipeline import Pipeline # Dummy data, `X` is the super set containing all features: X, y = make_classification(n_features=10) # Base estimator 1 uses the first 3 features with a logistic regression: clf_1 = LogisticRegression() clf_1_transformer = FunctionTransformer(lambda X: X[:, :3]) tclf_1 = Pipeline( [('transformer_1', clf_1_transformer), ('clf_1', clf_1)] ) # Base estimator 2 uses the last 7 features with a SVM: clf_2 = LinearSVC() clf_2_transformer = FunctionTransformer(lambda X: X[:, 3:]) tclf_2 = Pipeline( [('transformer_2', clf_2_transformer), ('clf_2', clf_2)] ) # The meta-learner uses the transformed-classifiers as base estimators: sclf = StackingClassifier([('tclf_1', tclf_1), ('tclf_2', tclf_2)]) sclf.fit(X, y) # Sanity check: check how many coefficients each estimator uses: print(sclf.estimators_[0].steps[-1][1].coef_.size) # 3 print(sclf.estimators_[1].steps[-1][1].coef_.size) # 7 Note that a more proper way to create the column-selection transformer might be to inherent from BaseEstimator and TransformerMixin instead of hacking the FunctionalTransformer .
