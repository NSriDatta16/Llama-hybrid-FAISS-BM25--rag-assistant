[site]: crossvalidated
[post_id]: 631547
[parent_id]: 
[tags]: 
Is it preferred to evaluate with a metric at a single decision threshold (eg Fbeta) vs averageing across thresholds (eg ROC-AUC)

Consider these two approaches to evaluating a classifiers performance: Choose a metric that summarizes the confusion matrix at a pre-determined decision threshold. Common suggestions seems to be Fbeta score for a model where True Negatives are not of great importance and Matthews correlation coefficient if True Negatives are of importance. There are of course many other metrics, but they all have in common that they are aggregating a confusion matrix derived from a pre-determined decision threshold (often p=0.5 by default in popular software such as sklearn). Choose a metric that aggregates the performance over multiple decision . Commons suggestions seems to be to optimize AUC-ROC or average precision, which are both measures that don't relate to the performance at a single decision threshold, but rather integrates/averages the performance over multiple decision thresholds. It seems to me that these two approaches are doing fundamentally different things, yet they are used interchangeably in many recommendations of metrics to choose when optimizing and/or comparing models. Approach 1 is finding the best possible model given that we have already decided on our decision threshold, and approach 2 is finding the model that has the most flexibility in adjusting the decision threshold while still getting good performance (ie it performs well at many thresholds). Q1. Is one of these two approaches theoretically more sound or preferable for other reasons (for example specific contexts when one is more helpful)? Intuitively, it seems to me that approach one is unnecessarily inflexible and that approach two is not guaranteed to find the best performing model, just the most flexible one. Naively, I would think that a third approach would be appropriate where a combination of decision thresholds and context-appropriate confusion matrix aggregation metrics are optimized. Then the top performing models could be visualized in a way so that it is easy for the client to pick the trade-off between the different outcomes in the confusion matrix that they think is appropriate for their specific problem (e.g. instead of viewing a single model's PR curve as a line, we would view multiple combinations of models and decision thresholds plotted as points in PR space and pick the one that aligns the closest with the business objective). Q2 Would this be a reasonable approach or what are the shortcomings here? (I have seen this , but that does not quite answer what I'm asking here). I'm have read some of the discussion around using proper scoring rules like Brier score and log loss to evaluate classifiers instead (such as this and this post), which seems helpful for both this and other issues, but it would still be helpful for me to understand the issues I asked about above.
