[site]: datascience
[post_id]: 98183
[parent_id]: 98178
[tags]: 
If your predictors have nothing to do with the outcome, you should not be able to build a model that works out-of-sample. This is a feature, not a bug, of machine learning. For instance, do you consider what time I set my alarm in the morning to be predictive whether or not you have cereal for breakfast? Features can, however, have just a small relationship with the outcome and combine to be quite predictive. Perhaps my alarm does not influence your breakfast choice, but there are a number of factors that do, each of which might be poor at predicting the outcome, but the combination of $3$ or $10$ might be very predictive. At an extreme, consider individual pixels of the MNIST digits. Does the middle pixel, on its own, have much ability to distinguish between the digits? What about some other pixel? Every individual pixel is a poor predictor of the digit, yet all $784$ combined result in strong performance.
