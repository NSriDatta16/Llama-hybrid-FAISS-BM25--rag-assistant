[site]: crossvalidated
[post_id]: 564787
[parent_id]: 564528
[tags]: 
Yes, high dimensional space is tough in general, but there a couple things that make Bayesian optimization with Gaussian processes particularly perplexing on those prickly problems. In my view, there isn't a single reason high dimension makes BO difficult, but the difficulty is rather due to a confluence of multiple factors. Firstly , Bayesian optimization is classically conducted with a Gaussian process using something like a squared exponential kernel. This is a kernel which gives great flexibility which is perfect in low dimension but can become a liability in high dimension, as it puts reasonable probability mass on too many explanations of the point cloud. So the first issue is that the model we're using is already struggling to understand what's going on. This is related to the volume argument from the other answer. Since GPs depend explicitly on distances, when all interpoint distances are equal and large, the GP has little discrimination power. See how as we go into higher dimension, the distances between random points vary less than in low dimension: [ "A survey on high-dimensional Gaussian process modeling with application to Bayesian optimization" by Mickaël Binois and Nathan Wycoff ] As you mention, putting structure into the kernel function so that we are learning a mapping into a low dimensional space onto which we put a "standard" gaussian process is a good way to go here. Another option is to assume a kernel function which combines information from input dimensions in a more frugal manner, such as additive (Additive Gaussian Processes Part of Advances in Neural Information Processing Systems 24 (NIPS 2011) by David K. Duvenaud, Hannes Nickisch, Carl Rasmussen) or ANOVA kernels ("ANOVA decomposition of conditional Gaussian processes for sensitivity analysis with dependent inputs" Gaëlle Chastaing, Loic Le Gratiet). Secondly , we can't forget about the fact that BO is a nested optimization procedure: every time a BO algorithm wants to suggest a next point to optimize, it has to solve an entire sub-optimization problem over the entire space! Unlike the original optimization problem, the acquisition function defined by our Gaussian process (whose optimizing point is our next candidate in our outer search) usually has a known gradient and Hessian, which is indeed helpful in finding a local solution. However, the acquisition function is notoriously nonconvex, which means that in high dimensional spaces, however quickly we can find a local optimum, we may have little chance of finding anything close to a global optimum without considerable effort. Though this doesn't impact the ability of the Gaussian process to model the unknown objective, it does impact our ability to exploit its knowledge for optimization. When combined with kernel shenanigans, sometimes the acquisition function can be optimized in a lower dimensional space, which can make things easier (or sometimes harder in practice; linear dimension reduction means we're doing linear programming rather than unconstrained optimization now and also doesn't vibe well with hyperbox constraints). And third is the hyperparameter estimation risk. Most popular these days are "separable", "ARD", "tensor product" or otherwise axis-aligned anisotropic kernels which look something like $k(\mathbf{x}_1,\mathbf{x}_2) = \sigma^2 e^{\sum_{p=1}^P \frac{(x_{1,p}-x_{2,p})^2}{2\ell_p}}$ , so we have one additional thing to estimate for each input dimension, and estimating gaussian process hyperparameters is tough, both from a statistical inference perspective and numerical analytic one. Using a parameterized mapping into low dimension only makes the estimation risk worse (but may be offset by a substantially lower variance class of functions a priori ). Fourth : Computation Time. GPs are known for their small data statistical efficiency (in terms of error/data) and large data computational inefficiency (in terms of inference / second). In high dimension, if we really want to find a global optimum, we are probably going to have to evaluate the objective function tons of times, and thus have a large dataset for our GP to consider. This is simply not been the GPs historical niche, as classical, decomposition-based GP (i.e. where you actually take a Cholesky of the kernel matrix) inference scales with $n^3$ so gets intractable quickly. Optimization of the acquisition function also only gets more expensive as $n$ goes up too. And keep in mind that you have to do this between every optimization iteration . So if we have an optimization budget of $10,000$ , naively we would need to do all this numerical optimization literally $10,000-n_{\textrm{init}}$ times (though of course in practice we would evaluate batches of points at a time as well as only optimize hyperparams every few iters). Large scale GP inference in high dimension has really matured over the last few decades, however, and have been the engine of some of the recent large scale GP-BO papers recently.
