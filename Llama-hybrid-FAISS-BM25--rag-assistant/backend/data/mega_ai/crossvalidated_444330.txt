[site]: crossvalidated
[post_id]: 444330
[parent_id]: 444328
[tags]: 
The optimizer state is the optimizer's momentum vector or similar history-tracking properties. For example, the Adam optimizer tracks moving averages of the gradient and squared gradient. If you start training a model without restoring these data, the optimizer will operate differently. The updates will be different, so the optimizer will proceed along a different trajectory. More details about adam : How does the Adam method of stochastic gradient descent work?
