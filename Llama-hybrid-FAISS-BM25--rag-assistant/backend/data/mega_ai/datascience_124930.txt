[site]: datascience
[post_id]: 124930
[parent_id]: 38392
[tags]: 
The paper "Why do tree-based models still outperform deep learning on tabular data?" by Grinsztajn, Oyallon, and Varoquaux ( arXiv ) attempts to tease this out. They have three empirical conclusions (my rephrasings): NNs tend to favor too-smooth solutions. NNs are more likely to overfit to uninformative features. NNs are invariant under rotations of the training data (but problems represented by tabular data have solutions that "shouldn't" be rotation-invariant).
