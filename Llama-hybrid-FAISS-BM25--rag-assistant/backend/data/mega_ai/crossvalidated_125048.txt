[site]: crossvalidated
[post_id]: 125048
[parent_id]: 124901
[tags]: 
Answer to the follow-up question Unfortunately, your intuition about the smoothing distribution $\mathbb{P}(x|y)$ is not correct as $\mathbb{P}(x|y)$ is not the product of the full conditionals: $$ \prod_{i=1}^9 \mathbb{P}(x_i|x_{[-i]},y) \ne \mathbb{P}(x|y) $$ even up to a proportionality constant. And even less on a log scale. Note also that the time step $t$ should not appear in the formula as time is only connected with Gibbs iterations in your problem, not with the initial statistical problem. Now, you know $f(x|y)$ up to a constant. This constant is actually $$ \sum_{x\in\{-1,1\}^9} \mathbb{P}(x|y) $$ which remains a manageable sum, hence can be calculated. My first solution for the marginal posterior density of $X_i$ (given $Y$) would be to use the decomposition \begin{align*} \mathbb{P}(x_i|y) &= \sum_{x_{[-i]}} \mathbb{P}((x_i,x_{[-i]})|y)\\ &= \sum_{x_{[-i]}} \mathbb{P}(x_i|x_{[-i]},y) \mathbb{P}(x_{[-i]}|y)\\ &= \mathbb{E}\left[ \mathbb{P}(x_i|X_{[-i]},y) | y \right] \end{align*} where the expectation is understood as the one of $X_{[-i]}$ given $y$. Since the Gibbs sampler returns simulations from all subsets of $X$ given $y$, a converging approximation to the above is $$ \hat{\mathbb{P}}(x_i|y) = \frac{1}{T} \sum_{t=1}^T \mathbb{P}(x_i|x_{[-i]}^t,y) $$ where $x^t$ denotes the value of the Markov chain at the $t$-th iteration. However, since your state space $\{,-1,1\}^9$ is small, you can also derive this marginal exactly : $$ \mathbb{P}(x_i|y) = \sum_{x_{[-i]}\in\{-1,1\}^8} \mathbb{P}((x_i,x_{[-i]})|y) . $$ Hence an exact derivation of the entropy as well.
