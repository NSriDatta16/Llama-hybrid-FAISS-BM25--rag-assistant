[site]: crossvalidated
[post_id]: 199513
[parent_id]: 199397
[tags]: 
For random forest, the default prior is the empirical class distribution of training set. You would like to adjust this prior, when you expect the training set class distribution is far from matching new test observations. The prior can be adjusted by stratification/downsampling or class_weights. Stratifictaion/downsampling does not mean, that some observations are being discarded, they'll just be bootstrapped into fewer root nodes. Besides adjusting the prior, it is also possible to obtain probabilistic predictions from the random forest model and choose a threshold of certainty. In practice, I find a mix of adjusting priors by stratification and choosing best threshold as the best performing solution. Use ROC plots to decide for thresholds. Adjusting class_weights will likely provide a similar performance, but it is less transparent, what the effective prior becomes. For stratification, the ratio of stratification is simply the new prior. See also this answer for more details
