[site]: crossvalidated
[post_id]: 455098
[parent_id]: 
[tags]: 
In most machine learning books, how are matrices of data organized?

I'm trying understand linear algebra but am still a bit confused. I think I am missing some of the conventions. Let's say there is a supervised learning problem. We have 100 observations, 7 predictors. How is the matrix of predictor values organized usually? Is it a 100x7 matrix or a 7x100 matrix? Normally in practice, (like in Excel or Python DataFrames, we would usually have a 100x7 matrix). Each column is a different feature. However, this doesn't make sense when I see stuff like, for linear regression: $ y = w^Tx$ Where w is a vector of parameters. If w is a 7x1 matrix (column vector), and we transpose it so that it becomes a 1x7 matrix - we are unable to multiply it by the 100x7 matrix of predictor values. That would suggest that in books, matrices of predictor values are actually 7x100 matrices where the row headers are actually the features and the columns are the observations. Is this usually the correct convention?
