[site]: crossvalidated
[post_id]: 110536
[parent_id]: 110513
[tags]: 
Here are some strategies that may help you: Try glmnet , a user-friendly R package that implements elastic-net-penalized regression. In tuning the hyperparameters, you might be best off using leave-one-out cross-validation because your small sample size is so small. I suggest glmnet because it can handle $p\gg n$ cases where you have more predictors than observations. You could try this procedure using as many orders of interaction as your RAM can handle. Variable reduction strategies like Principle Components Analysis (choosing the components that explain, say 85-95% of the variance) may help, as well. Then again, every single time I have tried variable reduction strategies in $p\gg n$ cases, it hasn't reduced the parameter space that much, and once I found out about regularized regression I've depended even less on this. When using variable reduction, maybe focus on subsets of the variables that show high multicollinearity. If you find yourself in the happy situation whether you've reduced your predictor set to something quite small. Still, you may have separation or quasi-separation in your data, so maybe still go with glmnet for regularization, or a Bayesian regularization method as implemented in R package bayesglm . Use subject matter experts to create a set of alternative models containing more reasonable subsets of the predictors, and which actually use scientific theory to arrive at the functional relationships between the smaller subset of predictors and the response. Use Bayesian model averaging to combine these models together into a single predictive model. If you don't end up using all 30 features, who cares so long as you have improved prediction substantially beyond a coin flip? Sometimes, the machine learning approach isn't the right one. When you have a very small data set, and you're trying to reduce the dimensionality as much as possible, well, that's pretty much the anti-machine-learning-big-data-type use case. Collect more data.
