[site]: crossvalidated
[post_id]: 567463
[parent_id]: 567458
[tags]: 
Nowadays, my first thought would actually be to fine-tune a pre-trained language (neural network) model (such as such LSTMs or BERT) via e.g. huggingface, fastai or similar. Instead of a classification head (i.e. outputting probabilities or logit probabilities for different classes with the number of outputs corresponding to the number of classes) one would simply instead use either a regression head (i.e. single continuous number, either restrained to be in 1 to 10, or subsequently taken to (1,10) via the expit transformation, the inverse of the logit, and multiplied by 9 and having 1 added), or a 10 class classification head (for the probability of being 1, 2, ..., 10), or a 9 output classification head with the categories coded as ( [0,0,0,0,0,0,0,0,0] for 1, [1,0,0,0,0,0,0,0,0] for 2, [1,1,0,0,0,0,0,0,0] for 3, ..., [1,1,1,1,1,1,1,1,1] for 10). Single keywords are tricky, because there's a difference between different texts that contain "luxurious" - e.g. "this modern house is very luxurious" vs. "this 1960s cabin is not luxurious by modern standards" vs. "the abandoned house is luxurious only for the rats that infest it" mean rather different things. The traditional way to go beyond single words would be n-grams. E.g. one set of possible 1- and 2-grams is "not luxurious", "very luxurious", "luxurious", "rats", "infested", "modern house". Additionally, different words have different frequency in language in general and its interesting when a word occurs more often "than usual", so term-frequency inverse-document-frequency (TF-IDF) is something that gets used a lot to encode such n-grams (or single words) for regression models (instead of just encoding them, say, as 1=word is present vs. 0=word is not present in sentence).
