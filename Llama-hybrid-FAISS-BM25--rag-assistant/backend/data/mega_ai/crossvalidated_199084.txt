[site]: crossvalidated
[post_id]: 199084
[parent_id]: 199074
[tags]: 
Starting from the Neural Network perspective: I would say that the base Neural Network has all neurons interconnected between layers. The convolutional version simplifies this model using two hypotheses: meaningful features have a given size in the image. features are shift equivariant (shifted input leads to similarly shifted output), and may occur anywhere in the image. The first asumption is expressed by setting to zero the weights leading to a hidden neuron, except for a region of interest/patch from the input. Shift invariance is obtained by sharing the same weights across all the patches. In order to capture features anywhere in the image, it is simpler to pave the input with patches only slided by one pixel. Those simplifications drastically reduce the number of parameters and lead to much simpler computations which 'happen' to take the form of a convolution, hence the C in CNN. Note 1: the fixed feature size hypothesis is alleviated by the use of multiresolution and/or by using separate networks with different patch sizes. Note 2: equivariance is usually not as useful as invariance, so the latter is often emulated with additional pooling layers. Alternative approach Before deep learning, a popular problem solving method was to extract features and feed them to a classifier. For images, the features were often extracted using expertly chosen filters such as Gabor filters/wavelets. On can view CNN as a parameterized filtering function, where parameters are trained using methods for Neural Networks
