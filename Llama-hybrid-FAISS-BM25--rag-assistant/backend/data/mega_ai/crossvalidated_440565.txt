[site]: crossvalidated
[post_id]: 440565
[parent_id]: 440558
[tags]: 
Gradient descent isn't a great optimizer. It's not unusual for GD to reach a plateau of very slow progress before finding a good descent direction again. See this post for a comparison of GD and Levenberg-Marquardt for a simple linear regression network with poor conditioning. Can we apply analyticity of a neural network to improve upon gradient descent? See this post for an explanation of why poor conditioning makes gradient descent challenging. For convex problems, does gradient in Stochastic Gradient Descent (SGD) always point at the global extreme value? In the case of this particular task, it's true that a small, single-layer network with sigmoid neurons can solve the XOR task. However, there's a difference between the theoretical possibility of solving a task and whether or not it's practical to train a network to do so. One reason that networks are wider and deeper and use tricks like residual layers, batch norm, sophisticated optimizers and ReLUs is to make these networks easier to train using gradient descent.
