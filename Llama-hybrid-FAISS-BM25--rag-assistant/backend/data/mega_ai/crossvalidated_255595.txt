[site]: crossvalidated
[post_id]: 255595
[parent_id]: 
[tags]: 
Calculating the probability of a point in a moving average series exceed a particular threshold

Let's say I have a uniform random sequence of 0s and 1s. I then calculate a 20-window moving average series of this sequence. Meaning the first data point of this MA-20 series is the average of all 0s and 1s between the first and 20th index of the sequence, and the second data point is the average between the 2nd and 21th index. I do this and get a moving average series that looks like the image below: I noticed that points rarely move above 0.7 and below 0.3, this is of course due to the fact that when flipping a coin 20 times, getting tails only 3 times is a rare event. However, each point in this series is dependent on the previous point due to the nature of how moving averages are calculated. So the points are not independent from each other. My question is: How do I properly calculate the probability that any particular point in this series exceeds this 0.7 or 0.3 threshold? To provide context, I want to use this knowledge to eventually determine whether or not a given moving average series is generated from a random sequence of 0s and 1s. My thought is to measure the distribution of points in a given series and compare it with a theoretical one. So I guess what I'm really looking for is the theoretical distribution of points in a moving average series with an N window.
