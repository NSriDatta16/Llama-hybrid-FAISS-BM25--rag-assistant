[site]: datascience
[post_id]: 30045
[parent_id]: 23194
[tags]: 
From my point of view, both answers are correct - it depends on what you want to achieve. One way to use an RBM is as a generative model . If you wanted to do this, you would train the network in order to obtain weights generating a distribution the network which is as close as possible to the real distribution of the input data (not exactly true as CD is known not to converge to the sample distribution, but this just as an aside). You could then, for instance, reconstruct images from partial or sample completely new images. In this context, the main purpose of the hidden units is to increase the power of the network - a Boltzmann machine without hidden units is not able to distinguish target distributions with the same second moments. However, the actual value of the hidden units during and after is less important, as your primary goal is to generate samples. If you want to use the model to detect features , the hidden units have an additional role. To explain this, suppose you use contrastive divergence as a learning algorithm. In each iteration, you would then sample the hidden units from the visible units and the visible units from the new values of the hidden units. If the result is close to the original value of the visible units, i.e. if your reconstruction error is small, then, apparently, the hidden units and the weights have learned enough about the training data to reconstruct the value of the visible units from the value of the hidden units. As you say, this is in fact similar to a dimensional reduction or an autoencoder. You could then build a second RBM on top the hidden units, i.e. you could start to stack RBMs to build deep networks, where each new layer of hidden units learns more and more abstract features from the training data.
