[site]: datascience
[post_id]: 58462
[parent_id]: 58422
[tags]: 
The reason why one uses any particular evaluation measure should be based on the semantics of the task. Corollary: there's no unique evaluation measure which is perfect for every task. Obviously there are also technical constraints to take into account, but imho considering only the technical aspect of performance optimization is the most common mistake made in ML applications. For instance OP mentions non-symetry of the F1-score: in some tasks it makes complete sense to use a non-symmetric measure, for example any task where the goal is to extract specific elements, e.g. named entity recognition. Moreover the question of symmetry makes sense only for binary classification, and many tasks involve more than 2 possible classes. In such cases it's common to use micro or macro F-score, but again it depends what one wants to measure. It's also important to keep in mind that a single metric is always a simplification of the performance , often there are many other relevant aspects. For instance, mistakes for a particular class might be more costly than for another, like in the case of medical prediction systems where a false negative means that a patient doesn't get a potentially life-saving treatment (note that generic F-score can be weighted to account for that). The interpretability of the performance measure is also very important. In many applications one needs to estimate some kind of average error rate that a non-expert can understand, otherwise the ML system is just a magic black box to its users and this can cause serious ethical and practical issues.
