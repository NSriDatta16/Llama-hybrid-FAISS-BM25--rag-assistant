[site]: crossvalidated
[post_id]: 638442
[parent_id]: 638394
[tags]: 
McNemar is only for binary outcomes (0,1), (1,2) or test results (+,-), and thus two classes, not multiclass. A multiclass test for two classifers can be performed using Cohen's kappa. However, Cohen's kappa suffers from imbalanced class sample sizes, called Cohen's kappa paradox. Margineantu and Dietterich [1] introduced a paradox-resilient kappa defined as \begin{equation} \kappa=\frac{\Theta_1 - \Theta_2}{1-\Theta_2}, \end{equation} where \begin{equation} \Theta_1= \frac{\sum_{i=1}^L C_{ii}}{n}, \end{equation} and \begin{equation} \Theta_2=\sum_{i=1}^L \left( \frac{\sum_{j=1}^L C_{ij}}{n} \right) \left(\frac{\sum_{j=1}^L C_{ji}}{n} \right). \end{equation} In the above formulation, $n$ is the number of test objects, $L$ is the number of class labels, $C_{ii}$ is the number of objects both classifiers voted for class $i$ , and $C_{ij}$ is the number of objects for which the first classifier voted for class $i$ and the second classifier voted for class $j$ . Smaller $\kappa$ values indicate a larger diversity and when $\kappa= 0$ , the agreement between two classifiers is based on chance, whereas when $\kappa=1$ , the two classifiers agree on every example. D. Margineantu, T. Dietterich. Pruning adaptive boosting. Proc. 14th Int. Conf. Machine Learning. San Francisco(CA), Morgan Kaufmann Press, 1997.
