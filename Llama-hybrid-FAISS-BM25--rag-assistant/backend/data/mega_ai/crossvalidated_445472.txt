[site]: crossvalidated
[post_id]: 445472
[parent_id]: 445453
[tags]: 
Yes, samples in the dataset may not be completely iid, but the assumption is present to ease the modelling. To maximize the data likelihood (in almost all models this is explicitly or implicitly part of the optimization), i.e. $P(\mathcal{D}|\theta)$ , without the iid assumption, we'd have to model the dependence between the data samples, i.e. the joint distribution and you won't be able to quickly write the following and maximize: $$P(\mathcal{D}|\theta)=\prod_{i=1}^nP(X_i|\theta)$$ Typically, with lots of samples (random variables), the slight dependencies between small set of samples will be negligible. And, you end up with similar performances (assuming the dependence is modelled correctly). For example, in Naive Bayes, not necessarily the samples but features/words are surely dependent. They're part of the same sentence/paragraph, written by the same person etc. However, we model as if they're independent and end up with pretty good models. The shuffling is an another consideration. Some algorithms are not affected by shuffling. But, algorithms using gradient descent are probably affected, specifically neural networks, because we don't train them indefinitely. For example, if you feed the network with all $1$ 's at first, then $2$ 's etc, you'll go all the way to the place where those $1$ 's lead you, then try to turn back to the direction where $2$ 's lead you and then $3$ 's etc. It might end up in plateaus and hard to go back to other directions etc. Shuffling enables you to go in every possible direction a little bit, without going deeper and deeper in some dedicated direction.
