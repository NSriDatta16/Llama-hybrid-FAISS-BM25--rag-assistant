[site]: stackoverflow
[post_id]: 5637156
[parent_id]: 5435913
[tags]: 
I've actually had a similar problem, but with somewhat different problem parameters. My application deals with 2 types of strings - relatively short ones measuring 60-100 chars and longer ones with 100-1000 bytes (averages around 300). My use case also has to support unicode text, but a relatively small percentage of the strings actually have non-english chars. In my use case i was exposing each String property as a native String, but the underlying data structure was a byte[] holding unicode bytes. My use case also requires searching and sorting through these strings, getting substrings and other common string operations. My dataset measures in the millions. The basic implementation looks something like this: byte[] _myProperty; public String MyProperty { get { if (_myProperty== null) return null; return Encoding.UTF8.GetString(value); } set { _myProperty = Encoding.UTF8.GetBytes(value); } } The performance hit for these conversions, even when you search and sort was relatively small (was about 10-15%). This was fine for a while, but i wanted to reduce the overhead further. The next step was to create a merged array for all the strings in a given object (an object would hold either 1 short and 1 long string, or 4 short and 1 long string). so there would be one byte[] for each object, and only require 1 byte for each of the strings (save their lengths which are always This reduced much of the byte[] overhead, and added a little complexity but no additional impact to performance (the encoding pass is relatively expensive compared with the array copy involved). this implementation looks something like this: byte _property1; byte _property2; byte _proeprty3; private byte[] _data; byte[] data; //i actually used an Enum to indicate which property, but i am sure you get the idea private int GetStartIndex(int propertyIndex) { int result = 0; switch(propertyIndex) { //the fallthrough is on purpose case 2: result+=property2; case 1: result+=property1; } return result; } private int GetLength(int propertyIndex) { switch (propertyIndex) { case 0: return _property1; case 1: return _property2; case 2: return _property3; } return -1; } private String GetString(int propertyIndex) { int startIndex = GetStartIndex(propertyIndex); int length = GetLength(propertyIndex); byte[] result = new byte[length]; Array.Copy(data,startIndex,result,0,length); return Encoding.UTF8.GetString(result); } so the getter looks like this: public String Property1 { get{ return GetString(0);} } The setter is in the same spirit - copy the original data into two arrays (between 0 start to startIndex, and between startIndex+length to length) , and create a new array with the 3 arrays (dataAtStart+NewData+EndData) and set the length of the array to the appropriate local variable. I was still not happy with the memory saved and the very hard labor of the manual implementation for each property, so i built an in-memory compress paging system that uses the amazingly fast QuickLZ to compress a full page. This gave me a lot of control over the time-memory tradeoff (which is essentially the size of the page). The compression rate for my use-case (compared with the more efficient byte[] store) approaches 50% (!). I used a page size of approx 10 strings per page and grouped similar properties together (which tend to have similar data). This added an additional overhead of 10-20% (on top of the encoding/decoding pass which is still required). The paging mechanism caches recently accessed pages up to a configurable size. Even without compression this implementation allows you to set a fixed factor on the overhead for each page. The major downside of my current implementation of the page cache is that with compression it is not thread-safe (without it there is no such problem). If you're interested in the compressed paging mechanism let me know (I've been looking for an excuse to open source it).
