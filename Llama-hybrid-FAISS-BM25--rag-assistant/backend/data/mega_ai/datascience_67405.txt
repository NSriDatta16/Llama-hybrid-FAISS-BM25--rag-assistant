[site]: datascience
[post_id]: 67405
[parent_id]: 16119
[tags]: 
Great answers but the answer also depends on the model usage. A small change in the training data row slice produces a large change in a validation set performance lowers my confidence that this is a good model. When using cross validation, I look at the variance of the performance to gauge if there was a lucky split or general instability. I do not ever blindly take the largest average of the cross validation performance. When I encounter a model with a significant variance measured how you described, I may need to declare a problem if the model will be used in certain business contexts. I may not be able to help the business understand the impact of the model, the best way to use the model in certain situations, the expected performance of the model over the long-term (some models are used once to score and some are scored millions of times over months or longer) and how to monitor the performance over the lifecycle. In these cases, I may need to start from scratch - get more data (rows or columns), feature engineering, redefine my target (broader, narrower or different to achieve a similar business result), look for a different algorithm, or change how we approach this business problem. Or accept the risk, watch it closely, and be prepared to act accordingly. Once again, dependent on the usage of the model if and how big a problem this may be. And how large a variance in model performance is also business context sensitive.
