[site]: crossvalidated
[post_id]: 591009
[parent_id]: 590999
[tags]: 
You say you use the lme4 package (designed for fitting mixed-effects models) but your model formula seems to have fixed effects only. How come? expertiseNOV to courseYES should be the main effects at the average level of the other predictors This is wrong. We know (from the naming convention used in the summary table) that the model is fitted with the treatment coding, which is the default in R. The intercept corresponds to: (expertise, condition, mood, course) = (expert, together, 0, NO) the interaction "expertiseNOV:condALO" is for example telling me that there is a significant difference between experts and novices for the condition "alone" Actually, it's telling you that there is a significant interaction between expertise and condition. Since expertise is interacted with mood and course as well, the interpretation of the interaction terms is a bit more involved. expertiseNOV:condALO is the expected difference in score between novices and experts when mood=0 and course="NO" . (Substitute either mood>0 and/or course="YES" , to see how the other two interactions contribute to the expected difference as well.) So what to do? Learn about contrasts and how to use them to make the post-hoc comparisons that you'd like to make. A popular package for post-hoc comparisons is emmeans . I generate fake data to illustrate how to use it (that code is attached at the end) but the best place to start is to read the vignettes. # Not the same summary table since you don't provide your data. #> Coefficients: #> Estimate Std. Error t value Pr(>|t|) #> (Intercept) 0.57980 0.41055 1.412 0.161 #> expertiseNOV -0.59440 0.60245 -0.987 0.326 #> mood -0.05071 0.06741 -0.752 0.454 #> condTOG -0.18903 0.26240 -0.720 0.473 #> courseYES -0.22131 0.26476 -0.836 0.405 #> expertiseNOV:mood 0.13485 0.10451 1.290 0.200 #> expertiseNOV:condTOG 0.24750 0.40291 0.614 0.541 #> expertiseNOV:courseYES -0.19014 0.40774 -0.466 0.642 Making post-hoc comparisons in a model with multiple interactions is not equivalent to looking at individual regression coefficients. library("emmeans") pairs(emmeans(model, ~ expertise | cond)) #> cond = ALO: #> contrast estimate SE df t.ratio p.value #> EXP - NOV 0.139 0.287 92 0.485 0.6288 #> #> cond = TOG: #> contrast estimate SE df t.ratio p.value #> EXP - NOV -0.108 0.284 92 -0.381 0.7037 #> #> Results are averaged over the levels of: course You can also use the contrast package to specify the comparisons you want to make as contrasts. (Be careful loading contrast and emmeans at the same time. They both define a contrast function.) Let's reproduce the emmeans output for the comparison between expert and novices when the condition is "together". We set (condition, moon, course) = ("TOG", average mood, either "YES" or "NO")`. library("contrast") contrast( model, list(expertise = "EXP", cond = "TOG", mood = mean(data $mood), course = c("YES", "NO")), list(expertise = "NOV", cond = "TOG", mood = mean(data$ mood), course = c("YES", "NO")), type = "average" ) #> contrast #> lm model parameter contrast #> #> Contrast S.E. Lower Upper t df Pr(>|t|) #> 1 -0.1082324 0.283703 -0.671691 0.4552262 -0.38 92 0.7037 If we choose a different mood setting, the contrast between the two expertise levels changes because of the expertise-mood interaction. contrast( model, list(expertise = "EXP", cond = "TOG", mood = 7, course = c("YES", "NO")), list(expertise = "NOV", cond = "TOG", mood = 7, course = c("YES", "NO")), type = "average" ) #> lm model parameter contrast #> #> Contrast S.E. Lower Upper t df Pr(>|t|) #> 1 -0.5019994 0.4301315 -1.356278 0.3522789 -1.17 92 0.2462 And if you are not interested in a particular mood, you may prefer to visualize the expected score differences between experts and novices for all combinations of course and condition as a function of mood. This can be done quickly with the ggeffects package. library("ggeffects") plot( ggemmeans(model, terms = c("mood [1:7]", "course", "expertise", "cond")) ) In short, multiple interactions make post-hoc comparisons more complex and more fun. The R code used to simulate data for illustration purposes. set.seed(1234) n
