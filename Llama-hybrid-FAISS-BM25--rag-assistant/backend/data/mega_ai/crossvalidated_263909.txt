[site]: crossvalidated
[post_id]: 263909
[parent_id]: 173636
[tags]: 
@ttnphns has provided a good answer. Doing clustering well is often about thinking very hard about your data, so let's do some of that. To my mind, the most fundamental aspect of your data is that they are compositional . On the other hand, your primary concern seems to be that you have a lot of 0s for green products and specifically wonder if you can transform only the green values to make it more similar to the rest. But because these are compositional data, you cannot think about one set of counts independently from the rest. Moreover, it appears that what you are really interested in are customers' probabilities of purchasing different colored products, but because many have not purchased any green ones, you worry that you cannot estimate those probabilities. One way to address this is to use a somewhat Bayesian approach in which we nudge customers' estimated proportions towards a mean proportion, with the amount of the shift influenced by how far they are from the mean and how much data you have to estimate their true probabilities. Below I use your example dataset to illustrate (in R) one way to approach your situation. I read in the data and convert them into rowwise proportions, and then compute mean proportions by column. I add the means back to each count to get adjusted counts and new rowwise proportions. This nudges each customer's estimated proportion towards the mean proportion for each product. If you wanted a stronger nudge, you could use a multiple of the means (such as, 15*mean.props ) instead. d = read.table(text="id red blue green ... c3 4 8 1", header=TRUE) tab = as.table(as.matrix(d[,-1])) rownames(tab) = paste0("c", 0:3) tab # red blue green # c0 12 5 0 # c1 3 4 0 # c2 2 21 0 # c3 4 8 1 props = prop.table(tab, 1) props # red blue green # c0 0.70588235 0.29411765 0.00000000 # c1 0.42857143 0.57142857 0.00000000 # c2 0.08695652 0.91304348 0.00000000 # c3 0.30769231 0.61538462 0.07692308 mean.props = apply(props, 2, FUN=function(x){ weighted.mean(x, rowSums(tab)) }) mean.props # red blue green # 0.35000000 0.63333333 0.01666667 adj.counts = sweep(tab, 2, mean.props, FUN="+"); adj.counts # red blue green # c0 12.35000000 5.63333333 0.01666667 # c1 3.35000000 4.63333333 0.01666667 # c2 2.35000000 21.63333333 0.01666667 # c3 4.35000000 8.63333333 1.01666667 adj.props = prop.table(adj.counts, 1); adj.props # red blue green # c0 0.6861111111 0.3129629630 0.0009259259 # c1 0.4187500000 0.5791666667 0.0020833333 # c2 0.0979166667 0.9013888889 0.0006944444 # c3 0.3107142857 0.6166666667 0.0726190476 There are several results of this. One of which is that you now have non-zero estimates of the underlying probabilities of purchasing green products, even when a customer doesn't actually have any record of having purchased any green products yet. Another consequence is that you now have somewhat continuous values, whereas the original proportions were more discrete; that is, the set of possible estimates is less constricted, so a distance measure like the squared Euclidean distance might make more sense now. We can visualize the data to see what happened. Because these are compositional data, we only actually have two pieces of information, and we can plot these in a single scatterplot. With most of the information in the red and blue categories, it makes sense to use those as the axes. You can see that the adjusted proportions (the red numbers) are shifted a little from their original positions. windows() plot(props[,1], props[,2], pch=as.character(0:3), xlab="Proportion Red", ylab="Proportion Blue", xlim=c(0,1), ylim=c(0,1)) points(adj.props[,1], adj.props[,2], pch=as.character(0:3), col="red") At this point, you have data and a lot of people would begin by standardizing them. Again, because these are compositional data, I would run cluster analyses without doing any standardizationâ€”these values are already commensurate and standardization would destroy some of the relational information. In fact, from looking at the plot I think you really have only one dimension of information here. (At least in the sample dataset; your real dataset may well be different.) Unless, from a business point of view, you think it's important to recognize people who have any substantial probability of purchasing green products as a distinct cluster of customers, I would extract scores on the first principal component (which accounts for 99.5% of the variance in this dataset) and just cluster that. pc.a.props = prcomp(adj.props[,1:2], center=T, scale=T) cumsum(pc.a.props$sdev^2)/sum(pc.a.props$sdev^2) # [1] 0.9946557 1.000000 pc.a.props$x # PC1 PC2 # c0 -1.7398975 -0.03897251 # c1 -0.1853614 -0.04803648 # c2 1.6882400 -0.06707115 # c3 0.2370189 0.15408015 library(mclust) mc = Mclust(pc.a.props$x[,1]) summary(mc) # ---------------------------------------------------- # Gaussian finite mixture model fitted by EM algorithm # ---------------------------------------------------- # # Mclust E (univariate, equal variance) model with 3 components: # # log.likelihood n df BIC ICL # -2.228357 4 6 -12.77448 -12.77448 # # Clustering table: # 1 2 3 # 1 2 1
