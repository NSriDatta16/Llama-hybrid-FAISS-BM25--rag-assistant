[site]: crossvalidated
[post_id]: 162099
[parent_id]: 161770
[tags]: 
This is kind of a boilerplate answer. Hopefully someone with experience in similar problems will come along with some more insight. Logistic regression would work fine as a first approximation, but keep in mind that linear regression can only capture linear effects. I would say to start there. Since you're looking for relative importance of the parameters, you could use regularization (eg ridge, lasso, or elastic net regression) to get a more principled idea of which parameters have the biggest effect on class. Otherwise you would have to run a Wald test on each parameter, or on groups of parameters, and control for multiple testing. If the results are understandable, you're done. However this might also be a good place to look into a nonlinear or nonparametric approach, like a generalized additive model, or another spline model like MARS. Since they are based on splines, they're best used for interpolation instead of prediction, but since your "features" here are parameters selected from a grid, that's just fine. Another alternative would be a decision tree grown by an algorithm like CART, or even an ensemble of trees like in the Random Forest algorithm. These are difficult to use for determining the relative importance of predictors, but usually you can efficiently fit each model leaving out one predictor at a time, and then determine importance according to how much worse the model is when that predictor is excluded. A lot of programs for fitting these models include built-in functions/routines for this kind of thing. Another issue with these nonparametric models is that they usually need to be tuned in some way (eg with cross validation). That's why, if the regression results are satisfying, it's not worth digging into these methods
