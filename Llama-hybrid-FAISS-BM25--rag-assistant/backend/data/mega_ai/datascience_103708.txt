[site]: datascience
[post_id]: 103708
[parent_id]: 103689
[tags]: 
Here's is my attempt to answer these questions: Avoid taking any insights from test_data. Do changes WRT the insights taken from the train_set only. However, every change I make should be replicated in test_set as well : The second part is mostly true. Only in some rare cases where you may be applying class balancing techniques to the train set which should not be applied to the test/evaluation set. Test set should represent the real world data as much possible, while, train set should do the same, some tweaks to this data set may be applied independently to make it easy/possible to train with the chosen ML algorithm. However, any scaling/normalization/transformation parameters must be replicated exactly to the test set as well. Use test_data too for insights and do preprocess accordingly. There should be no harm in conducting an EDA on both train and test datasets and using the combined knowledge for taking feature engineering decisions. Do something to change the test_data to make it more balanced and representative. NO. As noted in the #1 above, the test data should represent the real world data as much as possible. How else would you rely on the test set's evaluation metrics for the unseen real world data? You can read some more about it here Should I balance the test set when i have highly unbalanced data? Somethings else :S You should not be worried if a category/class of a certain feature is missing from the test set. However, the reverse is not true: there must not be any new category/class in the test or real world data set which was absent from the train set - the preprocessing routine and model will not know how to handle it. Also, if there are a very low number of unique values or near zero variance in a feature, you should consider dropping it (from both train and test sets). In the end, this should reflect in low feature importance and highlighted by any of the feature selection procedures like this Feature selection
