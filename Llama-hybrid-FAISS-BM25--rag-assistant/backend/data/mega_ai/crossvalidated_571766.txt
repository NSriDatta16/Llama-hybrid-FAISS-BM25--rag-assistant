[site]: crossvalidated
[post_id]: 571766
[parent_id]: 
[tags]: 
If all computed features ( or components ) in Neural Network nodes are positive numbers , does using Relu meaningful?

I am trying to understand the following issue. The reason we use activation functions such as sigmoid,tanh or relu in neural networks is to obtain a nonlinear combination of input features ( x's). My question is when all of the numbers in neural network nodes are positive , relu function becomes a linear function itself. In such a situation how does it create nonlinearity and help the neural network to actually learn a nonlinear combination of input features ?
