[site]: datascience
[post_id]: 103688
[parent_id]: 37378
[tags]: 
In the activation function, the initialization is important to train the neural network. I pasted the suggestion about kernel from DeeplearingAI webpage . Initializing kernel weights with inappropriate values will lead to divergence or a slow-down in the training of your neural network. The illustrated the exploding/vanishing gradient problem with simple symmetrical weight matrices, the observation generalizes to any initialization values that are too small or too large. Case 1: A too-large initialization leads to exploding gradients Consider the case where every kernel weight is initialized slightly larger than the identity matrix like: \begin{bmatrix}1.5 & 0 \\ 0 & 1.5\end{bmatrix} When activation is used in backward propagation, this leads to the exploding gradient problem. That is, the gradients of the cost with the respect to the parameters are too big. This leads the cost to oscillate around its minimum value. Case 2: A too-small initialization leads to vanishing gradients Similarly, consider the case where every kernel weight is initialized slightly smaller than the identity matrix. \begin{bmatrix}0.5 & 0 \\ 0 & 0.5\end{bmatrix} When these activation is used in backward propagation, this leads to the vanishing gradient problem. The gradients of the cost with respect to the parameters are too small, leading to convergence of the cost before it has reached the minimum value.
