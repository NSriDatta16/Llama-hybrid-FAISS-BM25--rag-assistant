[site]: datascience
[post_id]: 48797
[parent_id]: 
[tags]: 
Keras NN - Learning a simple formula

I'm struggling with a seemingly simple problem and could really use your help! I'm starting to learn about neural networks. I thought I would create a simple example to get started, using one dimensional inputs X and one dimensional outputs Y. When I first started with Y = 2X, the neural network was able to learn it very well over 1000 epochs. However, when I increased the complexity of the formula to below, it completely breaks apart (the MSE is very high). X = random numbers between 0 and 100 (one-dimensional) Y = (8(x^3)) + 5 I have tried adding more hidden layers and epochs without much success. Could anyone provide me with some guidance on where I'm going wrong in my method? Much appreciated! Code: from keras.models import Sequential from keras.layers import Dense import numpy as np # fix random seed for reproducibility np.random.seed(7) def generate_Y(X): return np.add(np.power(np.multiply(X, 2.0), 3.0), 5.0).T[0] X = np.random.rand(100,1) X = np.multiply(X, 100.0) Y = generate_Y(X) # create model model = Sequential() model.add(Dense(12, input_dim=1)) model.add(Dense(12)) model.add(Dense(12)) model.add(Dense(12)) model.add(Dense(1)) # Compile model model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse']) # Fit the model model.fit(X, Y, epochs=10000, batch_size=10) # evaluate the model scores = model.evaluate(X, Y) print("\n%s: %.2f%%" % (model.metrics_names[1], scores[1]*100)) test_X = np.random.rand(10,1) test_X = np.multiply(test_X, 500.0) test_Y = generate_Y(test_X) preds = model.predict(test_X) print preds diffs = np.subtract(preds.T[0], test_Y) print diffs MSE: mean_squared_error: 73360074604544.00% Prediction vs actual diffs: [ 4.97601229e+05 -1.60322447e+06 -7.85835700e+08 -7.00977203e+08 -7.52094155e+08 -1.88780991e+08 -1.64643340e+08 -9.38229901e+08 -2.02173897e+08 4.45327360e+05]
