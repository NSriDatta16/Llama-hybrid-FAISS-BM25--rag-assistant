[site]: crossvalidated
[post_id]: 611235
[parent_id]: 274123
[tags]: 
AutoEncoders are essentially regression, so you could calculate the $R^2 = 1-\frac{SSE}{SST}$ , where $SST=\sum_i (x_i-\bar x)^2$ and $SSE$ is the reconstruction loss (sum, not mean). This metric has an upper bound of 1 for perfect reconstruction but doesn't have a lower bound (as the network outputs can be worse than the mean, in case of bad "learning"). You could maybe (at your own risk) interpret a positive score as a percentage of variance explained by the model using $k$ latent variables. The difference in $R^2$ between the models with different dimensions could be associated with the % variance associated with this extra dimensions. But note that this depends on the network actually learning and reaching a good minima, which in NN with SGD is not guaranteed. Also, there's a question about the hidden dimensions before the final latent bottle-neck. So, a lot of assumptions and approximations, but could "kind-of" work. Here's a Colab notebook where I did some experiments on the MNIST data with some basic and shallow AE network. And here's the $R^2$ scores:
