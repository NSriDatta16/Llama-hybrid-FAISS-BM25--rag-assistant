[site]: crossvalidated
[post_id]: 225748
[parent_id]: 
[tags]: 
Does ReLU layer work well for a shallow network?

I am currently working on training a 5-layer neural network, and I got some problems with tanh layer and would like to try ReLU layer. But I found that it becomes even worse for ReLU layer. I am wondering if it is due to that I did not find the best parameters or simply because ReLU is only good for deep networks? Thanks!
