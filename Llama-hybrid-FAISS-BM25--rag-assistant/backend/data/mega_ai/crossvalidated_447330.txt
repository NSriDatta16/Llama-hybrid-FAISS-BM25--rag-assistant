[site]: crossvalidated
[post_id]: 447330
[parent_id]: 446472
[tags]: 
In the absence of enough data to perform stacking, just simple model averaging might be adequate for predictions. It will (typically) offer some improvement in terms of prediction errors. Dormann et al. (2018) " Model averaging in ecology: a review of Bayesian, information-theoretic and tactical approaches for predictive inference " offers a great and accessible overview of the subject. In terms of feature selection, I would push back and ask what is the reason behind it. Unless we have some particular constraints (e.g. cost of selecting data, problem of storing the model) then using a model that directly incorporates regularisation (like LightGBM or XGBoost do) is the proper thing to do. Avoid direct feature selection . Particular for Gradient Boosting Machine methods I would also note that there is no single accepted way to define feature importance (e.g. e can look into how many times a feature is used for splitting or what is the feature's permutation importance) It would therefore be prudent to ensure that a common methodology is used.
