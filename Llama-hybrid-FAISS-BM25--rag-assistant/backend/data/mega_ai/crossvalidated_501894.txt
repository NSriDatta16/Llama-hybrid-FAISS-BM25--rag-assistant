[site]: crossvalidated
[post_id]: 501894
[parent_id]: 501734
[tags]: 
First, as noted in a comment, you seem to have two different data sets with two different models, so comparisons between them are problematic at best. Second, you seem to be using a particular fixed penalization on all model coefficients, a ridge regression. You don't seem, however, to have tried to optimize the penalty factor, just choosing a value of 0.1 for all predictors. It's standard practice to choose that penalty factor by cross validation, as a particular choice might otherwise lead to substantial over- or under-fitting. And if you did that, then any p-values you get would be questionable, as they wouldn't take into account your use of the data to build the model. Third, as the lifelines author pointed out in a comment, those p-values don't really matter if your interest is in prediction. You could consider repeating your modeling process on multiple bootstrapped samples of your data to get some estimate of coefficient-estimate reproducibility, though. Fourth, Harrell's c-index isn't very sensitive for comparisons between models, as he states in the related context of logistic regression. It only tells you how good the relative risk rankings are among cases, not how precise the event-time estimates are. A measure more directly related to model calibration and prediction accuracy would be better.
