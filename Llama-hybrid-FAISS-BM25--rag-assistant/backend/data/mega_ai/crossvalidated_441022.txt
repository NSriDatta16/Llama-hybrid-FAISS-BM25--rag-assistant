[site]: crossvalidated
[post_id]: 441022
[parent_id]: 
[tags]: 
When re-fitting XGBoost on most important features only, their (relative) feature importances change

I am using 60 obseravation*90features data (all continuous variables) and the response variable is also continuous. These 90 features are highly correlated and some of them might be redundant. I am using gain feature importance in python( xgb.feature_importances_ ), that sums up 1. I run xgboost 100 times and select features based on the rank of mean variable importance in 100 runs. Let's say I choose the top 8 features and then, again run xgboost with the same hyperparameters on these 8 features, surprisingly the most important feature (when we first run xgboost using all 90 features) becomes least important when we run xgboost using top 8 variables. Any feasible explanation for this?
