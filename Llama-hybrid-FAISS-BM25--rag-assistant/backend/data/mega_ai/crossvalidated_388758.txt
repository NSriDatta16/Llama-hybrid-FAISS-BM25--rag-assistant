[site]: crossvalidated
[post_id]: 388758
[parent_id]: 388740
[tags]: 
Yes this is a common way of overfitting your model to the test data. In NLP a similar mistake is to do vocabulary selection and bag-of-words vectorization on the full train/test data. This is a bit insidious since doing model selection is a lot easier with most tools once you got your feature matrix. In addition the "boost" you get is not alarmingly big so it is tempting to just think your model is great and pat yourself on the back. On a positive note I think this was a lot more common 5-10 ten years ago and most practitioners are wise to this error today.
