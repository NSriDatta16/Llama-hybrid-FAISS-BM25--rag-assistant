[site]: crossvalidated
[post_id]: 118363
[parent_id]: 
[tags]: 
How to interpret variables that are excluded from or included in the lasso model?

I got from other posts that one cannot attribute 'importance' or 'significance' to predictor variables that enter a lasso model because calculating those variables' p-values or standard deviations is still a work in progress. Under that reasoning, is it correct to assert that one CANNOT say that variables that were EXCLUDED from the lasso model are 'irrelevant' or 'insignificant'? If so, what can I actually claim about the variables that are either excluded or included in a lasso model? In my specific case, I selected the tuning parameter lambda by repeating 10-fold cross-validation 100 times in order to reduce randonmess and to average the error curves. UPDATE1: I followed a suggestion below and re-ran lasso using bootstrap samples. I had it a go with 100 samples (that amount was what my computer power could manage overnight) and some patterns emerged. 2 of my 41 variables entered the model more then 95% of times, 3 variables more than 90% and 5 variables more than 85%. Those 5 variables are among the 9 that entered the model when I had run it with the original sample and were the ones with the highest coefficient values then. If I run lasso with say 1000 bootstrap samples and those patterns are maintained, what would be the best way to present my results? Does 1000 bootstrap samples sound enough? (My sample size is 116) Should I list all the variables and how frequently they enter the model, and then argue that those that enter more frequently are more likely to be significant? Is that as far as I can go with my claims? Because it is a work in progress (see above) I cannot use a cut-off value, right? UPDATE2: Following a suggestion below, I have calculated the following: on average, 78% of the variables in original model entered the models generated for the 100 bootstrap samples. On the other hand, only 41% for the other way around. This has to do in great part with the fact that the models generated for the bootstrap samples tended to include much more variables (17 on average) than the original model (9). UPDATE3: If you could help me in interpreting the results I got from bootstrapping and Monte Carlo simulation, please have a look at this other post.
