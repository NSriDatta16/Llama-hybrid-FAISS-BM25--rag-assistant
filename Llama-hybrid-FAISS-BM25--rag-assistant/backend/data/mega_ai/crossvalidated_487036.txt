[site]: crossvalidated
[post_id]: 487036
[parent_id]: 
[tags]: 
Can I see i.i.d. variables as just one?

I'm trying to understand the part of this book , page 276 which explains about sample mean: In statistical inference, a central problem is how to use data to estimate unknown parameters of a distribution, or functions of unknown parameters. It is especially common to want to estimate the mean and variance of a distribution. If the data are i.i.d. random variables $X_1,... ,X_n$ where the mean $E(X_j)$ is unknown, then the most obvious way to estimate the mean is simply to average the $X_j$ , taking the arithmetic mean. For example, if the observed data are $3, 1, 1, 5$ , then a simple, natural way to estimate the mean of the distribution that generated the data is to use $(3+1+1+5)/4 = 2.5$ . This is called the sample mean. My question is Instead of saying "... i.i.d. random variables $X_1,... ,X_n$ where the mean $E(X_j)$ is unknown" can I write "let X be a random variable and take the $n$ outputs of $X$ , where E(X) is unknown". In another words, can I see these $X_1,... ,X_n$ as only an one random variable? since the $X_1,... ,X_n$ are independent and identically distributed?
