[site]: crossvalidated
[post_id]: 156376
[parent_id]: 
[tags]: 
Dimensionality Reduction on a single Character Vector

I have a dataset I'm using to predict a binary outcome variable with 6 columns. Five of them are 10-30 level categorical variables with information about the user, e.g. job function, industry, country, but one is a raw, user entered text field (job title) with thousands of discrete values. In order to perform the prediction, I need to reduce Job Title to a usable number of categories (say, 100 or less). Do any standard or published approaches exist for single vector dimension reduction? I haven't been able to find anything online. My current approach is as follows. I've looked at PCA, but it doesn't seem to be applicable to this problem, as I'm only interested in reducing the single vector. Instead what I've done is group some job titles by generating a matrix containing the Levenshtein distance between each string and grouping the best matches. However, this mostly only serves to group mis-spellings, and I worry about losing important information (e.g. grouping SVP with VP). The second step is to simply select the 100 most frequent character strings and push the rest to "Uncommon Title", which I treat as it's own category in prediction. The problem is that after both of these steps, I'm left with roughly 50% of my observations as having the "Uncommon Title" category. Is there anything I can do to maintain at least some information on these observations?
