[site]: crossvalidated
[post_id]: 30474
[parent_id]: 
[tags]: 
Neural network training on unlimited theoretical data

I'm considering using a neural network on financial time series but rather than train the network on actual data I am going to train on a model of the data which is perturbed by random noise. This being the case, I will potentially have unlimited amounts of model training data. However I don't want to generate a huge amount of data and then train the model as it might take a very long time to reach a solution, and I have no idea how much data to actually generate. What I am thinking of doing is training on a small amount of model data (say 5000 examples) and get the values for the hidden nodes, record these values, and then repeat again, thereby building up a distribution of values for each node. These distributions could then be bootstrapped to get the mean value per node, and the whole process would stop once the change in the bootstrapped mean values falls below a given threshold. Edit - the purpose of the network will be to classify/label the time series over the recent past as being in one of a finite number of states, e.g. trending up/down, moving sideways, in congestion etc. These states can be modelled using synthetic data with known labels, and then on real data the network's job will be to identify which state the real data most closely resembles. This will be used as input to a separate decision making process. My question is - is there any reason why this would not be a valid approach to take?
