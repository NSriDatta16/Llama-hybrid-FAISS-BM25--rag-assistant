[site]: crossvalidated
[post_id]: 428125
[parent_id]: 
[tags]: 
Explain autoencoder anomalies

I developed an autoencoder model to detect anomalies in a set of signals coming from a machine. After the scoring, the most anomalous point (i.e. the ones with highest reconstruction error) are presented to domain experts. However, seeing the single anomalous point features, domain experts usually find it difficult to understand exactly why and what is anomalous. Sometime it's very easy (i.e. a single feature is out of a well known (by them) threshold), but sometime it's not clear at all. So I've been asked to implement some kind of explanation of the anomaly, which should point out which features are the most responsible of the anomaly and why. For instance, something like "if X3 has this value, then X10 should have this value and instead X10 has this other value which is very different" or "the relationship of X4 and X5 with the other features is very different". Clearly I'm trivializing here to explain to you what I mean. They generally expect that a single anomalous point is linked to a list of percentages where each percentage indicate the responsibility of the feature in the anomaly. For example, for the anomalous point (x1,x2,x3) may have the percentages list as (10%,20%,70%) so that x3 value is the most responsible of the anomaly. How would you proceed?
