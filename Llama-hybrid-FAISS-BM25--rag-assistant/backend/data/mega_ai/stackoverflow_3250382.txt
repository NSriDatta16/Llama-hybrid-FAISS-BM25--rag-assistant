[site]: stackoverflow
[post_id]: 3250382
[parent_id]: 3250192
[tags]: 
In typical case, loading speed will be limited by speed of storage you're loading data from--i.e. hard drive. If you want it to be faster, you'll need to use faster storage, f.e. multiple hard drives joined in a RAID scheme. If your data can be reasonably compressed, do that. Try to find algorithm which will use exactly as much CPU power as you have---less than that and your external storage speed will be limiting factor; more than that and your CPU speed will be limiting factor. If your compression algorithm can use multiple cores, then multithreading can be useful. If your data are somehow predictable, you might want to come up with custom compression scheme. F.e. if consecutive numbers are close to each other, you might want to store differences between numbers---this might help compression efficiency. Do you really need double precision? Maybe floats will do the job? Maybe you don't need full range of doubles? For example if you need full 53 bits of mantissa precision, but need only to store numbers between -1.0 and 1.0, you can try to chop few bits per number by not storing exponents in full range.
