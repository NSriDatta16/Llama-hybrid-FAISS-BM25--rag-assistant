[site]: stackoverflow
[post_id]: 3641480
[parent_id]: 3641152
[tags]: 
While I hate to discourage the use of something cool like a trie, have you thought about doing it in straight python? I wrote a simple benchmark using the corncob worlist and performance wasn't that bad. import time with open('corncob_lowercase.txt') as f: filetime = 0 starttime = time.time() words = f.read().split('\n') endtime = time.time() filetime = endtime - starttime print "file opened in {0} seconds".format(filetime) nonwords = ['234' + word for word in words] totaltime = 0 for word in nonwords: starttime = time.time() word in words endtime = time.time() totaltime += endtime - starttime wordcount = len(words) avgtime = totaltime / wordcount print "average time for word: {0}".format(avgtime) print "with {0} words".format(wordcount) runningtimes = (filetime + i * avgtime for i in xrange(10)) print "running times: {0}".format(list(runningtimes)) note that I'm testing the worst case where the word isn't in the file. I'm also including the time to load the file and process the file. If you were to memcache it, that would disappear. One further thing to note is that my machine is basically crap. C is fast but most of the code involved in searching a list is written in C anyways. Finally, this test is for pretty much every word in the english language . If you just want 10,000, I think that's cake. file opened in 0.0135519504547 seconds average time for word: 0.00249605141253 with 58113 words running times: [0.013551950454711914, 0.016048001867237236, 0.018544053279762558, 0.021040104692287877, 0.023536156104813199, 0.026032207517338521, 0.028528258929863839, 0.031024310342389162, 0.033520361754914484, 0.036016413167439809]
