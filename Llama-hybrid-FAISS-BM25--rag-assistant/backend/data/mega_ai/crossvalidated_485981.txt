[site]: crossvalidated
[post_id]: 485981
[parent_id]: 
[tags]: 
Low rank plus diagonal approximation of covariance of model parameters

In this paper regarding Bayesian Deep Learning, in Section 3.4, the authors want to approximate the covariance of a distribution over the model parameters $\theta$ . They first get an estimate of the diagonal covariance by approximating \begin{equation} \Sigma_{diag} = E[\theta^2] - E[\theta]^2 \end{equation} where the expected values are estimated from $T$ samples. They also compute a low-rank approximation $\Sigma_{low-rank}$ of the full covariance matrix using only the last $K$ samples. Then, they approximate the covariance matrix by $\Sigma = \frac{1}{2} (\Sigma_{diag} - \Sigma_{low-rank})$ What is the reason for doing this? Why do we need a low rank approximation? And why is the sum of the diagonal and the low rank a good estimate of the covariance?
