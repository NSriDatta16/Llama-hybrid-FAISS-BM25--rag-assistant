[site]: crossvalidated
[post_id]: 519774
[parent_id]: 
[tags]: 
estimate the divergence between two distributions by comparing the sufficient statistics

I'm interested in comparing two distributions $p(x,y)$ and $q(x,y)=pr(x)q(y|x)$ . I want to estimate the KL-divergence between $p(x,y)$ and $q(x,y)$ . It could be other divergence too, and I'm happy to know if there is any nice results on that. Now, suppose I only have many samples, $x_i$ draw from $pr(x)$ , and two sufficient statistics $f(x)$ and $g(x)$ for $p(y|x)$ and $q(y|x)$ respectively. By the definition of sufficient statistics, we have $$y\rightarrow f(x)\rightarrow x ~\mbox{ and }~ y\rightarrow g(x)\rightarrow x, $$ as two Markov Chains. Here $f(x)$ and $g(x)$ are both deterministic functions of $x$ . Would it be possible to come up with some approximation of $KL(p(x,y) || q(x,y))$ using samples $x_i$ , and the two functions $f(x)$ , $g(x)$ ? ----------(Updated)-------- Sorry I shouldn't have introduced $pr(x)$ . I hope the following description is better. Let $x$ be the data, $y$ the label. $y$ can be understood as some hidden variable that control the generation of $x$ , via $p(x|y)$ and $q(x|y)$ respectively. I'm interested in estimating $KL(p(x,y)||q(x,y))$ . Or perhaps, $KL(p(x|y)||q(x|y))$ , if we don't care the prior on $y$ . Let $f(x)$ be a function of $x$ , so this process is $y\rightarrow x \rightarrow f(x)$ , as a Markov chain. Further assume $f(x)$ is a sufficient statistics of $x$ for $y$ when the generative process is $p(x|y)$ (check Thomas Cover's elements of Information Theory). That is to say, $y\rightarrow f(x) \rightarrow x$ is also a Markov chain, when the generative process has conditional probability $p(x|y)$ . Likewise, let function g(x) be the sufficient statistics of $x$ for $y$ when the generative process is $q(x|y)$ . The question is, can we relate $KL(p(x,y)||q(x,y))$ (or perhaps $KL(p(x|y)||q(x|y))$ ) to the two sufficient statistics, $f(x)$ and $g(x)$ ?
