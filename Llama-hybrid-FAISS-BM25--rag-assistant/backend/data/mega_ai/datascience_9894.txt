[site]: datascience
[post_id]: 9894
[parent_id]: 
[tags]: 
How to decide the number of trees parameter for Random Forest algorithm in PySpark MLlib?

I am working on Random Forest algorithm in PySpark MLlib and have a doubt regarding the number of trees parameter that we pass to the model. The standard format of Random Forest modeling in PySpark MLlib is: model = RandomForest.trainRegressor(trainingData, categoricalFeaturesInfo={}, numTrees=3, featureSubsetStrategy="auto", impurity='variance', maxDepth=4, maxBins=32) The doubt that I have is how to decide the optimum value of trees to pass to numTrees parameter? I assume the more the number of trees better should be the performance but would it keep on improving with increase in number of trees? Is there a point after which it will start to have negative impact in performance? If yes then how can I find the optimum number of trees for my dataset?
