[site]: crossvalidated
[post_id]: 31744
[parent_id]: 22468
[tags]: 
Let us recall the geometry of the least squares: we have the basic equation $$ y_i = x_i'\beta + \epsilon_i $$ written in the matrix form as $$ \mathbf{y} = \mathbf{X}\beta + \mathbf{\epsilon} $$ from which we derive the residuals $$ \mathbf{e} = (I-H) \mathbf{y} $$ where $$ H = X(X'X)^{-1} X'$$ is the projection matrix , or hat-matrix. We see that each individual residual $e_i$ is a combination of potentially a large diagonal value $(1-h_{ii})$ times its own residual $\epsilon_i$ , and a bunch of small magnitude off-diagonal values $h_{ij}$ times their residuals $\epsilon_j, j\neq i$ . (The reason I am saying that the off-diagonal values are small is because $\sum_{j\neq i} h_{ij}^2 + h_{ii}^2 = h_{ii}$ , and in fact either the diagonal or off-diagonal entries are roughly of order $O(1/n)$ although this is not a very strict statement that is easily thrown off by the high leverage points; $X'X$ is a sum of $n$ cross-products and is roughly of order $O(n)$ ; it's inverse is $O(1/n)$ .) So what happens if you sum up a lot of i.i.d. pieces with small weights? Right, you get the normal distribution by the central limit theorem. So the contribution of the off-diagonal terms to the residual will produce an essentially normal component in large samples, smoothing out the non-normality that the original distribution of the errors $\epsilon_i$ may have featured. It is true of course that the major part of the residual $e_i$ still comes from the own error, $(1-h_{ii})\epsilon_i$ , but the interplay of all these terms may produce distributions that are much closer to the normal than the original distribution of errors.
