[site]: datascience
[post_id]: 93425
[parent_id]: 45070
[tags]: 
The last number 40 has nothing to do with the sequence length, its a hyper-parameter setting that is basically the length of the vector 'representation' of each token in the sequence. In this case , its 40 length. If you set it to 40 and use input embeddings of input 300-dimension (common if Glove), then the 300-dimensional word gets mapped to a 40-dimensional word that goes through the LSTM permutations. The idea is very similar to the number of 'kernel maps' in a CNN, if you're familiar with those. Its just a way to tell your network how many internal features you want your model to generate as your LSTM does its thing. More features means stronger representational power and better information flow, but at the possible cost of overfitting.
