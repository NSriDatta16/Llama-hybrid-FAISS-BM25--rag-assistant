[site]: datascience
[post_id]: 123314
[parent_id]: 
[tags]: 
Simple sentiment analysis model gives stupid accuracy

I am working on a sentiment analysis model from scratch but I ran into issues so thought I'd implement one using pytorch and then replicate but I also ran into issues with the pytorch version. I'm not sure the outputs are correct and apparently the accuracy is around 110%..... I'm not sure i am doing this right. Heck I'm not even sure what I'm doing. Please help Here's the code: import torch import torch.nn as nn import torch.optim as optim import numpy as np import pandas as pd # This file contains the same model i wrote manually in the other file but using pytorch in order for me to understand where i am going wrong with the manual model class SentimentAnalysisModel(nn.Module): def __init__(self, embedding_dim, hidden_dim, output_dim): super().__init__() self.embedding = nn.Embedding(10000, embedding_dim) # embedding layer takes in a tensor of size (batch_size, sequence_length) and outputs a tensor of size (batch_size, sequence_length, embedding_dim) # this layer is responsible for converting the input text into a vector of size (embedding_dim) self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=hidden_dim, kernel_size=3) self.conv2 = nn.Conv1d(in_channels=embedding_dim, out_channels=hidden_dim, kernel_size=3) self.pool = nn.MaxPool1d(kernel_size=2) self.fc = nn.Linear(hidden_dim, output_dim) def forward(self, text): # the forward method takes in a tensor of size (batch_size, sequence_length) and outputs a tensor of size (batch_size, output_dim) embedding = self.embedding(text) embedding = embedding.permute(0, 2, 1) conv1 = self.conv1(embedding) conv2 = self.conv2(conv1) pooled = self.pool(conv2) pooled = pooled.permute(0, 2, 1) # permute is responsible for changing the shape of the tensor. In this case, it changes the shape from (batch_size, sequence_length, embedding_dim) to (batch_size, embedding_dim, sequence_length) dense = self.fc(pooled) return dense def train(self, X, labels, num_epochs, batch_size): criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(self.parameters(), lr=0.001) labels = torch.tensor(labels, dtype=torch.long) for epoch in range(num_epochs): for batch in range(0, len(X), batch_size): optimizer.zero_grad() inputs = torch.tensor(X[batch:batch+batch_size], dtype=torch.long) targets = labels[batch:batch+batch_size] outputs = self.forward(inputs) loss = criterion(outputs, targets) loss.backward() optimizer.step() print("loss: ", loss.item()) def predict(self, X): X = torch.tensor(X, dtype=torch.long) outputs = self.forward(X) outputs = torch.max(outputs.data, 1) # print("probabilities: ", outputs) return outputs def evaluate(self, X, labels): X = torch.tensor(X, dtype=torch.long) labels = torch.tensor(labels, dtype=torch.long) outputs = self.forward(X) _, predicted = torch.max(outputs.data, 1) # torch.max returns the maximum value in each row of the tensor and the index of that value # the second arg is for correct = (predicted == labels).sum().item() print("correct: ", correct) print("total: ", len(labels)) print("accuracy: ", correct/len(labels)) pd = pd.read_csv("Dataset/dataset.csv") pd = pd[:100] X = pd[pd['Language'] == 'en']['Text'] labels = pd[pd['Language'] == 'en']['Label'] # preprocessing the data manually using keras from keras.preprocessing.text import Tokenizer from keras_preprocessing.sequence import pad_sequences from keras.utils import to_categorical from preprocess import preprocess_labels tokenizer = Tokenizer(num_words=10000) tokenizer.fit_on_texts(X) X = tokenizer.texts_to_sequences(X) X = pad_sequences(X, maxlen=100) labels = preprocess_labels(labels) print('labels before to_categorical: ', labels) labels = to_categorical(labels, num_classes=4) model = SentimentAnalysisModel(10, 10, 4) model.train(X, labels, 20, 32) predictions = model.predict(X) model.evaluate(X, labels) And here's the kind of output I am getting: $ python pytorch_test.py 2023-08-19 19:04:51.330975: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found 2023-08-19 19:04:51.331083: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. labels before to_categorical: [3, 3, 1, 3, 2, 0, 2, 2, 2, 2, 1, 1, 3, 2, 2, 1, 1, 2, 1, 0, 1, 1, 0, 1, 3, 2, 0, 1, 2, 2, 0, 2, 0, 0, 1, 2, 1, 1, 2, 1, 0, 2, 3, 1, 3, 1, 3, 3, 1, 0, 2, 3, 3, 2, 3, 0, 2, 1, 3, 0, 2, 1, 1, 1, 2, 3, 1, 2, 2, 1, 1, 3, 1, 3, 1, 0, 0, 2, 2, 3, 3, 3, 3, 0, 2, 0, 3, 3, 0, 2, 3, 0] loss: 3.8897905349731445 loss: 3.8837969303131104 loss: 3.879756212234497 loss: 3.873053550720215 loss: 3.8692281246185303 loss: 3.8650543689727783 loss: 3.8564913272857666 loss: 3.854645252227783 loss: 3.85028076171875 loss: 3.83972430229187 loss: 3.8397984504699707 loss: 3.8351874351501465 loss: 3.8224565982818604 loss: 3.8244481086730957 loss: 3.8195455074310303 loss: 3.8044466972351074 loss: 3.808379888534546 loss: 3.8031580448150635 loss: 3.7855300903320312 loss: 3.791482448577881 loss: 3.7859227657318115 loss: 3.7656538486480713 loss: 3.7737011909484863 loss: 3.7678332328796387 loss: 3.744844913482666 loss: 3.7550785541534424 loss: 3.7489407062530518 loss: 3.7232182025909424 loss: 3.735753297805786 loss: 3.7294199466705322 loss: 3.701007843017578 loss: 3.7159347534179688 loss: 3.7095091342926025 loss: 3.6785190105438232 loss: 3.695913314819336 loss: 3.6895029544830322 loss: 3.6561203002929688 loss: 3.676055431365967 loss: 3.6697707176208496 loss: 3.6342666149139404 loss: 3.656748056411743 loss: 3.6507418155670166 loss: 3.613410472869873 loss: 3.6384010314941406 loss: 3.6327908039093018 loss: 3.593982219696045 loss: 3.6213669776916504 loss: 3.6162688732147217 loss: 3.5763375759124756 loss: 3.605937957763672 loss: 3.6014351844787598 loss: 3.5607175827026367 loss: 3.5923080444335938 loss: 3.588432788848877 loss: 3.5472371578216553 loss: 3.58054780960083 loss: 3.577294111251831 loss: 3.535857915878296 loss: 3.5706052780151367 loss: 3.5679421424865723 correct: 243 total: 92 accuracy: 2.641304347826087 (tf) So as you can see I am not training it heavily right now because there's no point but the loss is decreasing steadily but the accuracy is 264% Thanks a bunch for reading thus far. Would really appreciate any help.
