[site]: crossvalidated
[post_id]: 448551
[parent_id]: 448546
[tags]: 
I don't think there is a strict answer in terms of sample size. It depends on the complexity of the analysis. A neural network (NN) will find patterns in whatever you give it. The question is, have you given the NN enough observations to find the patterns of interest. What are you looking for? If you're asking a NN to classify images of humans, then yes, 55 observations is insufficient. However, if you are analyzing features within a grade of students that hope to predict outcomes, a NN will allow you to find whatever patterns may be present. In one situation, I used a NN on a sample of 200 and created a model that proved useful, but it wasn't any more useful than logistic regression in my case. However, a NN with a smaller sample and 20 features proved to provide the best CV results across various models (and the results actually appeared meaningful.) So, you certainly can use NNs on small samples in simple contexts. As always, whatever model you use, you'll have to guard against overfitting and determine which findings are truly useful. Given the small sample size, when you compare models, you'd typically use a leave-one-out scheme. In this scheme, you repeatedly hold one one observation out of the training, and then test the resulting models in terms of predicting that one observation. For example, in sklearn you can utilize the LeaveOneOut cross-validator (or just use KFold with n_splits=n.)
