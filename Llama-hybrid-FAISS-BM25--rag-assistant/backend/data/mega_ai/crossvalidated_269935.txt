[site]: crossvalidated
[post_id]: 269935
[parent_id]: 253967
[tags]: 
Its fairly common to get repetitive sequences out of LSTMs. They tend to 'forget' the earlier outputs, and so enter a loop, like, from Karpathy's blog page, http://karpathy.github.io/2015/05/21/rnn-effectiveness/ : is that they were all the same thing that was a startup is that they were all the same thing that was a startup is that they were all the same thing that was a startup is that they were all the same A partial solution to this is to add some stochasticity to the prediction process, by using the output distribution as a probability distribution, and sampling from that, at each step, rather than simply taking the most likely prediction at each step. In Karpathy's char-rnn implementation, ie https://github.com/karpathy/char-rnn , this is implemented using an adjustable 'temperature' parameter, which smoothly adjusts between stochastic outputs, and choosing the most likely output at each step. Edit: if you suspect that your implementation may have a bug, one way to check is to set the random seed in your own implementation equal to that of Karpathy's, and check you get exactly the same outputs, eg see pull request https://github.com/karpathy/char-rnn/pull/68 for an example of such a comparison. (yes, its my own pull request :P )
