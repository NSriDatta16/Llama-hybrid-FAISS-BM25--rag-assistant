[site]: crossvalidated
[post_id]: 300259
[parent_id]: 300254
[tags]: 
It is reasonably widely recognised that feature engineering improves the outcome when using relatively advanced algorithms such as GBMs or Random Forests. There are a few reasons, relating both to overall accuracy and to useability. Firstly, if you actually want to use the model, features will require maintenance and implementation and will often require explanation to users. That is, each extra feature will create extra work. So for practical purposes, it's useful to eliminate features that don't contribute materially to improved accuracy. With respect to overall accuracy, additional features and/or poorly engineered features increase the likelihood that you're training your model on noise rather than signal. Hence using domain knowledge or inspection of the data to suggest alternative ways to engineer features will usually improve results. The kaggle blog - blog.kaggle.com - includes 'how they did it' write-ups from podium finishers in each competition. These usually include descriptions of feature engineering - arguably more frequently than descriptions of model tuning, emphasising the importance of feature engineering - and some of them are very creative, including leveraging off domain knowledge provided by competition organisers or otherwise discovered during the competition. This recent write-up is a good example of domain knowledge acquired during competition being used to select/ engineer features https://medium.com/kaggle-blog/2017-data-science-bowl-predicting-lung-cancer-2nd-place-solution-write-up-daniel-hammack-and-79dc345d4541 (the sections headed 'Pre-processing' and 'External Data' give good examples).
