[site]: crossvalidated
[post_id]: 557368
[parent_id]: 557365
[tags]: 
The big ideas are: Even if there's a few pixels omitted in one application of the filter, other applications of the same filter will include those pixels. This is how a CNN works: apply the same filter(s) to all parts of the image. If you include a border around the image, the convolution is invertible (and can be explicitly inverted with an inverse Fourier Transform, aka "deconvolution*). When you don't include a border, the only information lost concerns the details around the edges. It's also useful to think about it in terms of tradeoffs between these two options. Both of the convolutions you're comparing both cover 9 pixels, but they have different numbers of trainable parameters (9 for the ordinary convolution vs 4 for the dilated convolution). A convolutional layer applies the same filter to all patches of an image. In this case, the pixels that are omitted at one step does not omit them when the filter is shifted by one pixel. In other words, there is less overlap among pixels in the input. You're correct that the dilated convolution is not including 5 pixels' worth of information in a specific pixel patch, but this information might not be too terribly valuable. If you think about a 3x3 patch of pixels in a natural image, there's a high probability that those values are all very similar in your image, so a dilated convolution is not discarding "too much" information. (On the other hand, if your task has high variation among pixels, then a dilated convolution might not be a good choice). It's cheaper to do a smaller number of operations. In very large neural networks, finding places to economize the computations can be very valuable (cloud compute time can be expensive!)
