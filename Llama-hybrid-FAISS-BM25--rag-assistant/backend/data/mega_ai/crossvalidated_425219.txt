[site]: crossvalidated
[post_id]: 425219
[parent_id]: 324433
[tags]: 
Note enough reputation to comment I don't think correlation of differences necessarily implies correlation of levels. If you try to solve this you will see that you error terms sum over the series of defined error terms and you will struggle to define a correlation coefficient to solve this. Assume that: $\Delta y_t = a\Delta x_t + \epsilon_t$ for $ t \geq 1$ and $t \in \mathbb{Z}$ . Define: $y_0 := a x_0 + \epsilon_ 0$ Then: $y_t = ax_t + \sum_{n=0}^{t}\epsilon_n $ I think you can define a series whereby you will struggle to find a better coefficient, probably because of the converse which you stated in your question. This growing error term is your problem, as you can probably define errors such that the errors are sufficiently small, but the sum is sufficiently begin to negate your question. On the other hand if the time series is sufficiently small you could be OK in those specific cases. In the example provided by @mlofton the differences are perfectly correlated. Hence the error terms are zero. The sum of zeros is zero and hence the levels are also perfectly correlated. That is why the example provided does not work.
