[site]: crossvalidated
[post_id]: 203672
[parent_id]: 200699
[tags]: 
When dimensionality reduction techniques are part of larger pipeline, what really matters is how the reduction helps the end goal. If at all possible, I would try to see how different values of alpha affect the resulting predictions. Since that may not always be feasible, a common way to evaluate dimensionality reduction is through reconstruction accuracy. For a simple example, one often selects the number of principal components by looking at the variance explained, which is equivalent to using the squared error of the reconstruction. There is usually no clear cut way to select the number of principal components, however, because the accuracy always increases with increasing principal components. Similarly, decreasing alpha necessitates increases in reconstruction accuracy. A related option is to hold out a portion of the data (set to missing) and see how well the decomposition predicts the values of the held out data. It is then straight-forward to select the alpha that has the largest accuracy. For example, if the elements of your non-negative matrix are the counts of the number of times words are used in documents, you may randomly set a portion of the counts to missing and see how well the decomposition predicts the missing values for different values of alpha, choosing the alpha with the highest accuracy.
