[site]: crossvalidated
[post_id]: 568021
[parent_id]: 567039
[tags]: 
As in deep reinforcement learning, one does not need to literally hold the state space in memory explicitly (i.e. we don't need to "know all the states"). Rather, what is used is a function approximator that maps a description of a state to a prediction. In DRL that would be for example $Q_\theta(s,a)$ , or $\pi_\theta(a|s)$ . In this paper, the flow $F_\theta(s,a)$ is what is being approximated, with a deep neural network. All that is needed is thus a description of the current state $s$ that can be given to a neural network (e.g. an image). In the paper, a graph is used as an input to a graph neural network. These graphs are not to be confused with the state space DAG, which in this work is a graph of (molecular) graphs, where an edge represents a graph edit. Disclaimer, I am the first author of this paper.
