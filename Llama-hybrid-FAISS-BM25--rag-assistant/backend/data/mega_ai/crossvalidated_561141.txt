[site]: crossvalidated
[post_id]: 561141
[parent_id]: 261694
[tags]: 
It is impossible, except for when it is possible. It is impossible because minimizing the loss function literally finds the parameter values that are the best. If regularized parameter values were better (lower loss), then the minimization of the unpenalized loss function would have found those parameter values. However... ...neural networks have nasty loss functions where, unlike the closed-form solution for OLS regression, we have to look around the loss function until we find a place that is adequate so that we can declare, "This is the model," perhaps a global minimum but perhaps a local minimum. If you only get a local (but not global) minimum with the unregularized model, I find it completely plausible that the regularized model could wind up optimized by parameters that result in lower loss than the ones from the local minimum of the unregularized model, as you described.
