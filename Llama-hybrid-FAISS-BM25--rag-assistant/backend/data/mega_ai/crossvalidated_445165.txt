[site]: crossvalidated
[post_id]: 445165
[parent_id]: 445160
[tags]: 
Firstly, within the scope you've set, I think the EM algorithm does the right thing. A normal distribution cannot have lots of values that are exactly the same value - the only way to make that occur is to set the mean of one mixture component to that value and the variance to zero (the second mixture component you are presumably getting). If your zero values are truly exactly zero, then a point distribution at zero (basically a $N(0,\epsilon)$ as $\epsilon \to 0$ ) and a normal as a mixture may well be the best way to describe this. In that scenario, showing that a single component normal is "better" is futile, because it clearly is not (it just cannot have lots of values that are truly exactly zero). If the zero values are not truly zeros, but perhaps only zeros up to rounding, then including the interval censored nature of the data (i.e. use the likelihood for interval censored normal data) might well lead to a fit that has two mixture components with non-zero variance. If you want to force the variance of both components to be non-zero, there's also the option of going down a full Bayesian approach and to specify proper priors on the variances - but, again, that might be sensitive to the choice of the prior distributions.
