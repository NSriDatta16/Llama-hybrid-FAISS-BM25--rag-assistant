[site]: datascience
[post_id]: 28403
[parent_id]: 28398
[tags]: 
Some algorithms like boosting trees (XGBoost, for example) easy deals with almost anything "strange" in data: NaN values, outliers, different scalings. And such algorithms can find complex feature interactions in data by itself. But if you use some different or simple algorithms such as linear regression, you have to do some features preprocessing to get good results from it: fill or drop NaNs and outliers; make similar scales for all features; try to make polynomial features and so on. Otherwise you can get strange or very inaccurate results.
