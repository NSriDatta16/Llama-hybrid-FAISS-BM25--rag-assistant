[site]: crossvalidated
[post_id]: 437127
[parent_id]: 
[tags]: 
Improving the Naive Bayes classifier performance through decorrelation?

I was wondering if it is possible to improve the performance of the Naïve Bayes classifier by decorrelating the data. The Naïve Bayes assumes conditional independence of the features given some class $P(a_1, a_2 | c_1) = P(a_1 | c_1)P(a_2 | c_1)$ which is not necessarily true. What if we applied a transformation into the space using decorrelation which will result in $Cov(a_1, a_2) = 0$ for all features. We have that if $a_1$ , $a_2$ independent, then $Cov(a_1, a_2) = 0$ . Although the converse does not strictly follow (e.g. consider the two dependent random variables $X \sim Norm(0, 1)$ , and $X^2$ whose covariance is 0), I want to know if the new features will be closer to being independent. Another problem is, even if the features are closer to being independent, conditional independence will not so easily follow. I know this may theoretically not follow, but so does many other methods commonly used in machine learning. So, I also wonder if someone has gained an accuracy boost by doing such decorrelation in practice. Also, is there any other method for improving the performance of the Naive Bayes by moving the random variables closer to being conditionally independent? The problem with just doing regression is that the interpretability of the model is crucial in the setting I am working with, so I have to stick with Naive Bayes (unless you know a similarly interpretable alternative).
