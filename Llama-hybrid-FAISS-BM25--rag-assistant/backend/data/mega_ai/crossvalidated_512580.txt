[site]: crossvalidated
[post_id]: 512580
[parent_id]: 
[tags]: 
Does GridSearchCV actually fit the best model to the training data, or do you have to refit after hyperparameter optimisation?

I have this code, with the aim being to develop a neural network with cross validation and hyperparameter optimisation for a regression problem (continuous features, continuous label). from keras.layers import Dense, Activation from keras.models import Sequential from sklearn.model_selection import StratifiedKFold def create_model(activation='relu',learning_rate = 0.01): model = Sequential() model.add(Dense(32, activation = activation, input_dim = 101)) model.add(Dense(units = 64, activation = activation)) model.add(Dense(units = 64, activation = activation)) model.add(Dense(units = 1)) model.compile(optimizer = 'adam', loss = 'mean_squared_error') return model model = KerasRegressor(build_fn = create_model, verbose = 1) params = {'activation': ["relu", "tanh"], 'batch_size': [16, 32, 64], 'epochs': [50, 100], 'learning_rate': [0.01, 0.001, 0.0001]} } random_search = GridSearchCV(model,param_grid = params, cv = KFold(5)) random_search_results = random_search.fit(X_train, y_train) print("Best Score: ",random_search_results.best_score_, "and Best Params: ", random_search_results.best_params_) Can someone confirm that if my next line is: y_pred = random_search_results.predict(X_test) That that is fitting the most optimal model to X_test? I thought it was, but then I saw this post, where they say 'The next task is to refit the model with the best parameters', after the code above is run. Did I not already fit the best param model to the training data using this method? Can someone explain to me what extra code is needed to add the optimal model according to GridSearchCV to the training data?
