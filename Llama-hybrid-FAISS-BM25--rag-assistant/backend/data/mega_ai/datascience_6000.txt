[site]: datascience
[post_id]: 6000
[parent_id]: 5992
[tags]: 
So my question is: Would I need to create separate algorithms for every natural language to be interpreted? Yes, I believe so. But building a model for detecting the used language is not hard: usually taking n-grams (n-shingles) and then doing classification on them works quite well in practice. By the way, for the start you can use stop words to detect a language, e.g. like it's described here . Then once the language is detected, I'd do the NLP stuff for each language separately.
