[site]: crossvalidated
[post_id]: 585518
[parent_id]: 106334
[tags]: 
By definition, a function $f(x)$ is convex over a convex set $S$ if for all $x, y \in S$ and $t \in [0, 1]$ , $tf(x) + (1-t)f(y) \geq f(tx + (1-t)y)$ . Think of this as a straight line connecting two points of $y = x^2$ always being above the curve itself. In the general case, $f$ can be shown to be convex if its Hessian is positive definite. Therefore, most of the cost functions used for training neural networks are convex with respect to the net's final output and expected value. This includes MSE, CCE. There are cherry-picked non-convex loss functions that could be used as well, such as the Rosenbrock function, $f(x, y) = 100(x^2-y)^2 + (x-1)^2$ . However, I have not seen non-convex functions be used in literature or in practice unless the author is trying to show the goodness of their new update scheme. That an some $L \leq 1$ regularisation schemes. As for convexity with respect to the intermediary layer weights, unless the output of these intermediaries is non-convex, convexity is still found. Linear layers, convolutions, and activation functions like ReLU are convex, so the loss is also convex with respect to these layers. Generally you just check the convexity of activation functions. The argument about how to permute the weights and get the same loss shows that the loss isn't convex isn't true, and when it is, it's not useful. Consider again $f(x, y)=x^2$ as a loss function. This is convex. Say the net currently outputs $x=2$ yielding a loss of $4$ . But, if the weights are changed so that the net now outputs $x = -2$ , the loss is also $4$ . This is a convex function that (assuming suitable net expressivity) has a way to get the same loss. Technically the argument also hinged on the fact that the loss you currently have is a global one - but this is an odd assumption as there's no way of knowing you've attained a global loss unless the function is cherry-picked or convex. There is also no suitable permutation of nodes to use, so there's no proof that the argument can be carried out.
