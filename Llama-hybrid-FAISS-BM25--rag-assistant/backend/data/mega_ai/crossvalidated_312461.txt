[site]: crossvalidated
[post_id]: 312461
[parent_id]: 312405
[tags]: 
I cannot think of a way, to do this with lm . You can do it with optim . I did hardcode the train_data set in my function res.sum.squares . There will most certainly be a more elegant way to code this, but for simplicity and with this small a data set, this is my approach: res.sum.squares The result for this training data is: $par doc law dri mia bos chi 11.177929 13.156439 14.577809 2.279492 2.280387 2.168559 nyc 2.512093 $value [1] 9.467214e-25 $counts function gradient 76 26 $convergence [1] 0 Please note, that we searched for seven coefficients in only 6 observations. This will not yeald reliable results. It is probably a reason for the extremely low residual sum of squares as well. lm would have given us standard errors for coefficients, optim does not do that. At least, it is a way to solve your equations.
