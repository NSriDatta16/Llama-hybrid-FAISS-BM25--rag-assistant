[site]: crossvalidated
[post_id]: 287128
[parent_id]: 287046
[tags]: 
Based on this article by Lin and Tegmark (below), I think the answer is "it depends." In other words and as they state, most deep learning algorithms assume a lognormal distribution. As long as your data fits that assumption, then there is no problem. The problem is when the tails of the distribution are more extreme than lognormal, e.g., power lawed or super-exponential. Their Figure 1 and related discussion outline the issues with the lack of tail fit based on lognormality across several different data types and solutions are proposed, specifically in the context of deep learning NNs. Lin and Tegmark, Critical Behavior from Deep Dynamics: A Hidden Dimension in Natural Language arXiv:1606.06737
