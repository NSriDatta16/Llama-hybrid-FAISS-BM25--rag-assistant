[site]: datascience
[post_id]: 28795
[parent_id]: 28672
[tags]: 
According to Mueller in Introduction to machine learning with Python: a guide for data scientists a Random Forest consists in several Decision Trees whose input is a reorganization of the initial datasets. This reorganization is called bootstrap. Still according to him, each tree will be trained on a feature space subset. During the training each tree will try to fit the data according to its feature subset. In Gradient Boosting tree (a part of what is called Gradient boosting machine) the idea is to take shallows trees (shallow here means that each tree will have a very thin subset of the feature space) without "boostraping" the dataset. The different trees are set in a series circuit and each tree will correct the previous tree error. You may tune the learning rate between each tree for a better learning. PS: Gradient Boosting Machine, in general, combines several "simple" models. In Gradient Boosting Tree the simple model are shallow trees etc. You can have a look to this non mathematical article GB from Scratch It is not a complete answer but I hope it will be completed and still useful
