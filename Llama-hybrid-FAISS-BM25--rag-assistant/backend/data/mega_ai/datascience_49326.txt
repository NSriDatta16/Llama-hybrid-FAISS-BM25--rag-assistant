[site]: datascience
[post_id]: 49326
[parent_id]: 48432
[tags]: 
I got to say that I haven't found any literature regarding this topic. As far as I know, what you ask is impossible. You should be aware of this, and the product owner should be too. The reason is that any loss function relies on known labels, so there is no way you can predict a label which is not in the training data. Also, is science-fiction that a machine learning algorithm can predict something which It hasn't been trained for Having said so, I think there can be a workaround (let me point out that this is an opinion not based on formal literature). If the classifier is probabilistic, the output is the probability for each class to be true and the decission is the higher prob. Maybe you can set a threshold for that probability, such that the model predicts "unknown" if all probabilities are below that threshold. Let me give you an example. Let $M(x)$ be a model such that: given an $x$ , decides if $x$ belongs to one out of three categories $c_1, c_2, c_3$ . The output of $M$ is a vector of probabilities $p$ . The decision is made by taking the highest prob in $p$ . So an output of $M(x) = p(x) = (0.2,0.76,0.5)$ would correspond to the decision $x$ belongs to $c_2$ . You can modify this decision by setting a $\tau$ such if none of $p_i \geq \tau$ then the decision is $x$ belongs to unknown class What do you do with those unknown 's depends on bussines logic. If they are important, you can create a pool of them and re-train the model using available data. I think you can do sort of "transfer learning" from the trained model by changing the dimension of the output. But this is something I haven't faced, so I am just saying Take on count that SGDClassifier uses SVM underneath, which is not a probabilistic algorithm. Following SGDClassifier documentation you can modify the loss argument to modified_huber or log in order to get probabilistic outputs.
