[site]: crossvalidated
[post_id]: 61994
[parent_id]: 
[tags]: 
Neural Network Black Box Workarounds

I am dealing with a data set that includes rich textual data (e.g., blog entries, magazine articles, essays, book reviews, etc.) as well as a host of proprietary metrics, including numerous quantitative measures of consumer resonance. All of the textual data is in the same language and the data set sample size is approximately 400. There is some human pre-processing of the textual data designed to summarize roughly 10 key aspects of each textual work (these metrics do not change across the sample), and these key aspects can be either boolean, categorical, or discrete values. Additionally, I plan on utilizing a bag-of-words algorithm across the sample to supplement the human-generated analysis just described. I am considering utilizing a neural network to attempt to predict consumer resonance using the data described above, but it is important for me to understand relative importance of each factor. Additionally, I expect that of the roughly 10 textual metrics, some will play a much larger role in affecting consumer resonance than others, and the importance of these may change depending on the subject content of the particular article. For example, individuals reading an article on women's beauty or dating issues in large cities may value different things than those reading a book review in the New York Times. I have these questions: Is it possible to back out this relative importance using a neural network? Is this data set large enough to properly train a neural network? Would I be better off utilizing a Bayesian tree or SVM approach, or perhaps use these in conjunction with a neural network to supplement the analysis? I was considering a backpropagation network, but is there a potentially more valid type I should be looking at?
