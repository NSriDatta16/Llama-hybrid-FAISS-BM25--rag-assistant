[site]: crossvalidated
[post_id]: 535794
[parent_id]: 535789
[tags]: 
It seems that you are swapping two different concepts here. The concepts are unbiased and consistent , which are properties of an estimator. A sequence of estimators $(T_n)_{n=1}^\infty$ is said to be unbiased for a quantity $\theta$ if, for all $n\,\in\mathbb{N}$ , $$ E[T_n] = \theta \quad.$$ It is said to be consistent if it converges in probability to $\theta$ . These are different concepts: the first says that, for every finite sample size, the average of your estimator is $\theta$ . The other states that, as the sample sizes grow, the estimator getting arbitrarily close to $\theta$ with increasing probability. Let $I = \int_a^bf(x)dx$ be your quantity of interest (assume it exists). What the most basic Monte Carlo method does is to observe that $$I = \int_a^bf(x)dx = (b-a)\int_a^bf(x)\frac{1}{b-a}dx = (b-a)E[f(X)] \quad.$$ In the last line, we wrote the integral as being the expectation of $f(X)$ , where $X$ has a uniform distribution in $(a,b)$ . Hence, if we sample i.i.d. random variables $(X_i)_{i=1}^n$ with $X_1 \sim U((a,b))$ , then the estimator $$T_n = \frac{(b-a)}{n}\sum_{i=1}^nf(X_i) \quad,$$ is easily shown to be unbiased for $I$ . When you think of Riemann sums, it is usual to take a deterministic partition. If it is deterministic, then the expected value for any fixed sample size is the value of the summation it self, which in general is not the value of the integral.
