[site]: crossvalidated
[post_id]: 594667
[parent_id]: 594589
[tags]: 
The variance of the residuals is constant. At each residual (true value - predicted value), the difference is about the same. Variance is generally: $(x_{i}-\bar{x})^{2}$ which is the difference of value of the observation from the average of all observations, squared, showing you how much all the points differ from their average. With a high value meaning they differ a lot, and a low value meaning that they generally are about the same. You can kind of think of a graph of these residuals as one with dots representing the residual at each prediction (n predictions = n dots), and for each of those dots, another dot illustrating the mean value of the residuals. So you have 2 dots for each prediction: the residual for the prediction and the mean residual for all predictions. If the dots are all about uniformly the same distance apart from one another at each prediction, then you’ve got constant variance. If the dots are all kinds of different distances apart, they vary a lot, and so you’ve variation (potentially heteroskedasticty). If the variance of the residuals is constant( $var(true value - predicted value)$ ), or very small in applied circumstances, then the estimator predicts or fits a curve of the data that is constantly about the same amount off for each prediction. If that’s the case, then on average, your model predicts each data point with about the same performance. Your estimator performs about the same at each predicted point (for that data). So, homoskedastcity means the variance of the residuals is constant. If the variance of the residuals is constant, then your model may generalize the data you have well. The other assumptions of linear regression, or GLMs, are what make the homskedastic assumption also contribute to having a model that represents your data well.
