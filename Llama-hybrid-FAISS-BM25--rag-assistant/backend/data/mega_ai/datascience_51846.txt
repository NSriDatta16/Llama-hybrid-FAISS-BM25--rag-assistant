[site]: datascience
[post_id]: 51846
[parent_id]: 
[tags]: 
Linear Regression

I'm starting to learn machine learning and one of the first things that is mentioned is the usage of a linear regression method. Basically, we have a bunch of data points and we want to fit a line such that the errors we get from the line and the actual data points are minimized. I understand the theory and why we would use, for example, something like gradient search methods to find the global minimum point. What I don't understand is why to have to use something as complicated as gradient search or least mean squares (LMS) algorithm or whatever when, in introductory statistics, we were given a very simple -still lots of calculating, but thankfully no partial derivates- formula to find this line: $$y=mx+b$$ For $m$ , we have: $$m = \frac{S_{xy}}{S_{xx}}= \frac{\sum(x_i-\bar{x})(y_i-\bar{y})}{\sum(x_i-\bar{x})^2}$$ For $b$ , we have: $$ b= \frac{1}{n}\left(\sum y_i-m\sum x_i\right)$$ The $i$ subscripts are used to refer to each $x$ or $y$ in the data set. $X$ or $Y$ bar are averages. $n$ is the cardinality or # of data points we have in the set. Am I missing something? Or am I trying to use one method in statistics that isn't allowed in the machine learning world?
