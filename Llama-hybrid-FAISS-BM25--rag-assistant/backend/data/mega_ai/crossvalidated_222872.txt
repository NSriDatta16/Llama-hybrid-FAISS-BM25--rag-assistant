[site]: crossvalidated
[post_id]: 222872
[parent_id]: 222859
[tags]: 
As others have noted, you can of course use any real-valued (haven't seen a model with complex inputs, but I guess it could happen) features. It may help to even think of your binary values as special cases of general input features. Since it sounds like you're unsure about the appropriateness of your chosen method (neural networks), I'll lay out how I might structure the problem. Given the small number of features (and without knowing how much or how high quality the data), I'd recommend starting with a simple linear model. Something like Logistic Regression or a Binary Probit model would be a reasonable place to start. You can easily implement these in a language like R, or use libraries for Python or whatever your favorite language is. If the linear model gives you sufficient predictive power, great, you're done! But if your linear model isn't where you want it to be, and you can't get more features nor more data, maybe you start thinking about transforming the inputs. Instead of $X_i$, you try $X_i^2$ or $log(X_i + X_j)$. Given the modest number of features you have, this might be enough to get a decent model. If the model still isn't where you need it, now's a good time to look into neural networks. Unless your features have some inherent structure, like pixel values on a grid, you should probably start with a densely connected network. I advise starting small, with a few neurons on one layer, and adding to see if that improves your results. You're going to sacrifice easy understanding of the importance of your variables, but you can get more predictive power as the net effectively learns useful nonlinear combinations and transformations of your input data. If you're new to implementing neural networks, I'd suggest giving Keras or skflow a try. They're both high-level interfaces to well-known neural network libraries. For your problem of a binary outcome, just make sure you end with one dense neuron that has a sigmoid activation, and use binary cross entropy as your loss function (you don't need to one-hot encode your output labels in this case because it's only 1 bit of information). This is basically equivalent to binary logistic regression at the final layer. This is just one approach that may or may not make sense for your problem. And this may have been more than you wanted to read, but hopefully it's useful to someone starting out.
