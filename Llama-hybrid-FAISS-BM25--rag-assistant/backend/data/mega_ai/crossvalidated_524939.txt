[site]: crossvalidated
[post_id]: 524939
[parent_id]: 
[tags]: 
Word2vec/SkipGram: Why softmax?

In Word2Vec (SkipGram version), there is a softmax layer at the end of the neural net. As this is expansive to calculate, some approximations are used instead, such as negative sampling. But if in the end of the day we will only care about the weights in the first layer (the embeddings), why do we bother using a softmax at all? Couldn't we just use raw outputs instead?
