[site]: crossvalidated
[post_id]: 359813
[parent_id]: 
[tags]: 
What does $\mathcal{L}$ stands for in back propagation?

I am trying to learn how to do deep neural networks with this Ipython notebook . I'm puzzled about notations in linear backward learning section. For layer $l$, the linear part is: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (followed by an activation). I want to get $(dW^{[l]}, db^{[l]} dA^{[l-1]})$. The course suppose that I have already calculated the derivative $dZ^{[l]} = \frac{\partial \mathcal{L} }{\partial Z^{[l]}}$. Yet, What does $\mathcal{L}$ stands for in the derivative $dZ^{[l]} = \frac{\partial \mathcal{L} }{\partial Z^{[l]}}$ means ? I have never seen it defined here.
