[site]: crossvalidated
[post_id]: 610428
[parent_id]: 535624
[tags]: 
Summary Yes, SHAP values are potentially misleading when predictors are correlated -- they can be imprecise and even have the opposite sign. The correlation does not need to be incredibly high, around roughly 0.2, we already see large deviations from true SHAP values arising due to the independence assumption. Detailed answer This paper by Aas et al. (2021) answers your questions, so I will include quotes from it (italicized): The original Shapley values do not assume independence. However, their computational complexity grows exponentially and becomes intractable for more than, say, ten features. That's why Lundberg and Lee (2017) proposed using an approximation with the Kernel SHAP method, which is much faster, but assumes independence as you correctly mention. However, as the Mase et al. quote you mention shows, independence is rarely the case in real-world data. Assuming independence causes Shapley values to suffer from inclusion of predictions based on unrealistic data instances when features are correlated. To your question: Are SHAP (SHapley Additive exPlanations) values potentially misleading when predictors are highly correlated? How and why? If so, is there any guidance on when not to use SHAP? Are there any rules of thumb based on Var[X] telling us when features are too correlated for SHAP? Yes SHAP values assuming independence may be misleading. Aas et al. show using simulations that while the Kernel SHAP method is accurate for independent features, for correlations higher than about 0.05, SHAP values give results further and further from the true Shapley value. On the graph below, the black line shows the average difference between the true Shapley value and its approximation. The yellow, teal and purple lines represent errors from alternative SHAP calculations the authors propose (these do not assume independence). According to the simulations, even for small correlations (0.05), it is more precise to calculate SHAP values by not assuming independence. One might try to calculate "standard" SHAP values and also ones not assuming independence and comparing the results. This can be done via the shapr package written in R . I'm interested in a regression setting where X is a p-dimensional vector of predictors (aka features), and we are using SHAP to understand the behavior of a nonlinear regression model f(X), which allows interactions. Suppose f is a gradient boosted regression tree, for example. In an example using a real dataset, the authors use XGBoost to predict mortgage default using transactional data. The dataset has 28 features, some of which, such as br_mean, br_min, br_max (mean, minimum, and maximum balance on a checking account in 365 days) are correlated. They calculate SHAP values for 2 individuals using the Kernel SHAP method (assuming independence, see white bars) and using an alternative they propose which doesn't require independence (see black bars). The resulting SHAP values are quite different for the correlated values and sometimes even have opposite signs -- for example for Individual A br_min and br_max .
