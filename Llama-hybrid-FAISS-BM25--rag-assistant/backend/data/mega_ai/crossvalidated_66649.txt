[site]: crossvalidated
[post_id]: 66649
[parent_id]: 
[tags]: 
A Bayesian perspective on omitted-variable bias (and other covariate-selection bias problems)

As I know OVB, from a frequentist education, when you leave a variable $(z)$ out of your control set $(X)$ that is correlated with both your independent variable of interest (treatment $T$) and your dependent variable of interest ($Y$), your coefficient estimate will be biased because the explanatory power of the missing variable $z$ is distributed to the coefficients of included variables $(\hat\beta_i X_i$). How does a Bayesian perspective view OVB? For instance, if we use a data-based, rather than theory-based, variable-selection algorithm -- LR, AIC, BIC -- is it hard to conceive of OVB? Furthermore, how would one formally integrate the awareness of $z$ into our conditional probability statement? I mean, in Bayesian inference we want to estimate $P( \text{model} \mid \text{data}) = P(\theta \mid X)$. If we acknowledge some important but unobserved $z$, would we write $P(\theta \mid X, z)$? Furthermore, how would a Bayesian perspective interpret other classes of covariate-selection bias problems? I thinking about covariate-selection issues as elaborated by Pearl and others including, Pearl: Controling for an instrument can augment bias in linear models, and introduce and augment bias in non-linear models: http://arxiv.org/pdf/1203.3503.pdf Greenland: Conditioning on pre-treatment variables may induce bias (M-bias, aka Collider-Stratification bias): http://journals.lww.com/epidem/Abstract/2003/05000/Quantifying_Biases_in_Causal_Models__Classical.9.aspx (Sorry, paywalled)
