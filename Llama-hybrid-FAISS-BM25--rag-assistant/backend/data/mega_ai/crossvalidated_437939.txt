[site]: crossvalidated
[post_id]: 437939
[parent_id]: 437852
[tags]: 
The issue with the approach you described is that the breakpoint is a parameter, which appears to have been chosen by hand to fit the data. But, it's treated as a known value when calculating the AIC. This fails to account for the uncertainty in estimating the breakpoint from the data. As a result, the estimated AIC will be overoptimistically biased, and unfairly favor this model more than it should. If the breakpoint truly is a known parameter (i.e. constrained by theory), then disregard the above. Otherwise, the proper approach is to estimate the breakpoint as part of the fitting algorithm (e.g. see the comment from @alexis above). And, then account for this during model selection (whether AIC or otherwise). AIC and its cousin BIC are certainly established model selection criteria, but their validity rests on asymptotic assumptions. It's not particularly clear when these assumptions are or aren't justified for real world datasets (especially small ones). Their main advantage lies in their computational efficiency, but that's not really a limiting factor in your case. In general, I have greater trust in cross validation. The fact that you have time series data raises another important point. Standard model selection procedures are often designed for i.i.d. data, and can lose their validity when dependencies are present (as is often the case for time series, spatial data, etc.). You must be very careful to perform model selection properly in this setting. For example, there are specialized forms of cross validation designed for time series and other dependent data. This can also be true for statistical inference (e.g. p-values and confidence intervals on model parameters).
