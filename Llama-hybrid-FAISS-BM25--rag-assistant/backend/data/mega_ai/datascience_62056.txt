[site]: datascience
[post_id]: 62056
[parent_id]: 45033
[tags]: 
From the same blog, In this hybrid approach, we encode the subject and previous email by averaging the word embeddings in each field. We then join those averaged embeddings, and feed them to the target sequence RNN-LM at every decoding step. The BoW part of their hybrid approach is to get the general context of the email conversation by averaging the word embeddings in the subject and the previous email. Passing this vectorized representation of the general context at each step of the RNN-LM, which takes into account word order , helps get predictions that are more tailored to the subject of the conversation. Note that at each step, the RNN-LM is getting 3 types of input - Embedding for the previous word - For immediate context. State vector - For localized context Subject and previous email averaged embeddings - For a wider, conversation-level context Hopefully, it is clearer now, that the model incorporates order with the use of RNN-LM and the BoW part is to condition the output to the subject of the conversation.
