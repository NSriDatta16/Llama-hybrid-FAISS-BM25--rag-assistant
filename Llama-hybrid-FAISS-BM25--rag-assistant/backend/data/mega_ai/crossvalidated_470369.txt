[site]: crossvalidated
[post_id]: 470369
[parent_id]: 470322
[tags]: 
Let me address some preliminaries first, before addressing the main question. For reference, it will help to read these threads: Significant predictors become non-significant in multiple logistic regression How can adding a 2nd IV make the 1st IV significant? Is there a difference between 'controlling for' and 'ignoring' other variables in multiple regression? Estimating $b_1x_1+b_2x_2$ instead of $b_1x_1+b_2x_2+b_3x_3$ Intuition behind the names 'partial' and 'marginal' correlations Basic Simpson's paradox As those threads discuss, the reason a variable can move from non-significant to significant as more variables are included is that the additional variables are associated with the response and so reduce the error variance, increasing the power of the test of the focal variable. To come to understand how this phenomenon is playing out in your data, you can use the method @whuber demonstrates in his answer to the second linked thread above. Likewise, the reason the sign flips is because the variable is correlated with the other variables that are being added. I gather seeing the sign flip was perplexing, because you had checked for correlations and found they were small to moderate (you thought they "seem to be uncorrelated"). Nonetheless, the phenomenon is due to those correlations. It's important to recognize that those correlations, and the scatterplot of value x b , are marginal projections (see linked thread #5 above), and relationships can exist between more than two variables oriented in such a way within the full multidimensional space such that they cannot be seen in any of the 2D projections. The best way to see this is to use visualizations that move beyond 'flatland' via conditioning , pseudo-3D representations , motion , or interactivity (e.g., brushing ). These can all be done conveniently in R (e.g., via ?coplot , ?lattice , ?scatterplot3d , ?rgl , or ?rggobi ). I'm not sure how easy they are in other software, though. A simple hack is to examine a scatterplot matrix and use color, or different symbols, to distinguish different values of your focal variable. cols = ifelse(dat $b b), "red", "blue") # low b values are red, high blue windows() pairs(dat[,c(4,1:3)], col=cols, pch=16) windows() plot(value~c, dat, col=cols, pch=16) abline(a=1, b=.15, col="gray") Now it is possible to see if a value for b is low or high. The plots with b aren't the ones to look at, instead, look at how the colors play out in the plots that are collapsing over b . The key plot is the plot of value vs c in the top right corner (row 1, column 4). To get a better look, we can make a larger version of that scatterplot alone: Here you can see that there is a strong correlation between value and c , with a band of points moving from lower left to upper right. Running parallel within that band, we see stripes of mostly red points (lower values of b ) along the band's top and mostly blue points (higher values of b ) along the band's bottom (separated by a line I arbitrarily drew through the data). As a result, the relationship between b and value , after controlling for c , is downward sloping. That gets you your negative coefficient sign in the multiple regression model. Having now attempted to establish the phenomena at play here, and to understand how they are occurring in these data, let's move to the main question: What interpretations can be made, given what you've found? When we see conflicting results (e.g., non-significant vs. significant, or positive vs negative) when modeled different ways (alone or controlling for covariates), or having made different judgment calls (e.g., which measurement of a construct, transformations, outliers, etc.), it is common to ask, 'which is right?' There isn't necessarily an answer to that. In many cases, transparency is the best course of action: report the analysis planned a-priori, but then discuss other analyses / results as 'sensitivity analyses'. In this case, I gather the question is about causality ("the effect of b on value "). There are three things to bear in mind: This appears to have been an observational study. Causal inferences are valid primarily due to the design of the study. That is, you can infer causality because the study is a true experiment due to randomization and independent manipulation of the treatment, or due to exploiting naturally occurring exogeneity. These effects are due to the correlations amongst variables, and there will likewise be correlations with other variables that are not included in the study. (Variable b is confounded with a and c , which are 'measured confounders', and is certainly confounded with any number of other variables that are unmeasured in this study.) Each of these models / correlations is a marginal association, and each of the plots is a marginal projection. Point three is really important to understand. It may help to read linked threads #s 5 and 3 above. It is entirely possible that both models are correct: there is no (or just a very small positive) association with value when ignoring all other possible variables, and there is a clear negative association with value when ignoring all other variables except a and c (which are being controlled for). If you were to gather data on additional variables and assess other marginal associations (controlling for d and e , controlling for a and d , for c and e , for all four covariates, etc.), you could get completely different answers and they could also be right . I don't necessarily think other tests or diagnostics are needed. The exploratory data visualizations listed above and in @whuber's linked answer can help you with understanding the results of the tests you've already run, though. Ultimately, if you want to know if b causes value , you need to run a true experiment. Find plots, and divide them randomly into subplots. Independently manipulate the levels of the variables of interest ( b , but possibly all three) and treat the subplots. Then wait whatever period of time is appropriate in this context and assess the resulting values. Good experimental design will make a , b , and c orthogonal, and randomization will make all background variables uncorrelated at the population level. That will allow for valid causal inferences.
