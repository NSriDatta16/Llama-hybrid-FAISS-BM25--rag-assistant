[site]: crossvalidated
[post_id]: 523499
[parent_id]: 
[tags]: 
reweigh cost function in regression

In fraud detection, we can increase the cost of classification mistakes on the minority class, called cost-sensitive training. penalize-SVM is one of algorithms doing it. I am wondering if there are similar ways for regression problem? i.e. re-weigh cost by errors: $C = \sum_i (y_i - \hat{y}_i)^2 \to C^{\prime} = \sum_i \beta_i(y_i-\hat{y}_i)^2$ , where $\beta_i = f(y_i-\hat{y}_i)$ is a function of errors, and here I want to optimize $C^{\prime}$ . I feel adaboost regression could be an option, but didn't figure out how to implement it in sklearn.
