[site]: crossvalidated
[post_id]: 355655
[parent_id]: 355634
[tags]: 
I know I have am in the comments, but I actually found this picture pretty useful. It shows what the PCA is doing (drawing axes, projecting, separating the pictures), albeit in 2-dimensions (2 PCs being used). This 2-dimension process is extended for $n$-dimensions, based on the number $n$ of Principal Components you choose to use. Here you can see (in the second case) drawing a new axis based on the "features" allows you separate the images well. You ran PCA to get the 2-D plot, and now you can use the position of images on the "feature" graph to predict the image classification, based on its position on the corresponding line. The image also shows the downfall of this method's assumptions (in the top sequence). You are assuming most discriminant information can be captured by the largest variances (first two PCs, or "features," in this case). In some cases, like in the first case, the most discriminant information is not held in the direction of the largest variance. You do not need to worry about this case, most likely, for your application, but it is good to note for future cases or cases where discriminant information is held in the directions of smallest variance.
