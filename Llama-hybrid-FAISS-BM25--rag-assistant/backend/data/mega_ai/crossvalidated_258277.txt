[site]: crossvalidated
[post_id]: 258277
[parent_id]: 258272
[tags]: 
The backpropagated deltas are derived via the chain rule of calculus. Notice that, although they are valid over all inputs, in the weight update step they are multiplied with the actual activation of that layer. For the loss function we usually use MSE for linear layers or cross-entropy for softmax layers such that the backpropagated error becomes the difference of the prediction and the target. I suggest for a detailed understanding to study the topic in the deep learning book by Goodfellow et al.: http://www.deeplearningbook.org/ A more limited treatment of can be found here: Backpropagation with Softmax / Cross Entropy
