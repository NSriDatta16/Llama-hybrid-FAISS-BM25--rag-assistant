[site]: crossvalidated
[post_id]: 266394
[parent_id]: 266220
[tags]: 
Note that this has some overlap with an earlier, somewhat similar question (where I suggested to group the words in the TF-IDF matrix by their covariance, and selecting the most frequent word in each group as the best feature). Typical approaches are to just take some $n$ top most frequent words (or some top fraction $x$), which you can do, as you suggest, after various forms of TF-IDF scaling those word frequencies. While spectral analysis and clustering (e.g., of word embeddings, instead of TF-IDF values, and then choosing/selecting the most central word in each cluster) indeed have been suggested recently (2012-2016) to improve unsupervised word feature selection, they are not very common, however (and way more complex to set up than a quick TF-IDF-ranked frequency filter). As to measuring the "correct" choice of $n$ (or $x$), if all your work is unsupervised, you can only measure intrinsic correctness (c.f., model perplexity ); Or you need to evaluate your unsupervised results against some supervised task with a simple setup, e.g., as is common practice when evaluating word embeddings .
