[site]: crossvalidated
[post_id]: 329318
[parent_id]: 
[tags]: 
Is it correct to evaluate Neural Network after a fixed number of batch-updates, rather than at the end of epoch?

I'm training a neural network on a number of datasets of different size with a fixed batch size and an exponential learning decay. Normally, I would evaluate model performance, save checkpoint and reduce learning rate at the end of epoch. But in the case with multiple datasets, is correct to do these steps after a fixed number of batch-updates (say 2000)?
