[site]: crossvalidated
[post_id]: 626997
[parent_id]: 
[tags]: 
Policy Gradients Therom for Episodic Cases: How are different formulas related?

While I am generally aware of how the Policy Gradients algorithms work theoretically, I was recently a bit confused between two definitions of the Policy Gradient and later the derivation of the REINFORCE algorithm, with a focus on episodic cases. The gradient has been derived in Sutton's book as: $$\nabla_{\theta}J(\theta) \propto E_{\pi}\left[ q_{\pi}(S_t,A_t) \nabla_{\theta}\log\pi(A_t|S_t) \right]=E_{\pi}\left[ G_t\nabla_{\theta}\log\pi(A_t|S_t) \right]$$ Where $G_t$ are the returns from $t$ up to end of the episode: $G_t = \sum_{k=t+1}^{T}\gamma^{k-t-1}R_k$ . This definition of the Policy Gradients and REINFORCE algorithm was new to me. I was more used to the one where all steps from the whole episode are included in the definition. Assuming a single episode as $\tau=s_0,a_0,R_1,s_1,a_2,R_2, \dots, s_{T-1},a_{T-1},R_{T}$ , that is sampled from a joint distribution: $P(\tau;\theta)=p(s_0)\pi(a_0|s_0,\theta)p(s1|s_0,a_0)\pi(a_1|s_1,\theta) \dots p(s_{T-1}|s_{T-2},a_{T-2})\pi(a_{T-1}|s_{T-1},\theta)$ , we try to maximize the expected return $ J(\Theta)= E_{P(\tau;\theta) }[\sum_{k=1}^{T}\gamma^{k-1}R_k] $ by optimizing the policy, with the gradient: $$\nabla_{\theta} J(\Theta)= \nabla_{\theta}E_{P(\tau;\theta)}[ \sum_{k=1}^{T}\gamma^{k-1}R_k]$$ After applying the usual log trick, moving the derivative inside the expectation and rearrenging individual return terms for every step, this becomes: $$\nabla_{\theta} J(\Theta)=E_{\pi}\left[ \sum_{t=0}^{T-1} G_t\nabla_{\theta}\log\pi(a_t|s_t) \right]$$ A derivation similar to this can be found here: https://web.stanford.edu/class/cs234/CS234Win2019/slides/lnotes8.pdf So, the two gradients are actually looking very similar, however they are different, in the sense Sutton book's derivative just includes a single term for a single step. However, the second derivation is a summation over all episode steps. In general, if we were able to say that the expectation term is equal for every time step in the episode, like: $$E_{\pi}\left[ G_{t_1}\nabla_{\theta}\log\pi(A_{t_1}|S_{t_1}) \right] = E_{\pi}\left[ G_{t_2}\nabla_{\theta}\log\pi(A_{t_2}|S_{t_2}) \right] \tag{1}$$ Then, it would be the case that: $$T * E_{\pi}\left[ G_{t}\nabla_{\theta}\log\pi(A_{t}|S_{t}) \right] = E_{\pi}\left[ \sum_{t=0}^{T-1} G_t\nabla_{\theta}\log\pi(a_t|s_t) \right]$$ However I was not able to show that gradient terms from different steps are equal to each other. That was appealing to try, since Sutton's derivative is differing from the actual derivative by a constant factor (hence the $\propto$ sign). So, my question would be, how these two derivations are related? Could it be the case that equation (1) holds such that each time step is equal to each other, hence the first gradient is equal to the whole episode's gradient, when multiplied with the episode length?
