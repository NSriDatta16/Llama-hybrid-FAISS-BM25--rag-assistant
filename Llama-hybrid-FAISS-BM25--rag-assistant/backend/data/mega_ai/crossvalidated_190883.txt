[site]: crossvalidated
[post_id]: 190883
[parent_id]: 
[tags]: 
Single layer NeuralNetwork with ReLU activation equal to SVM?

Suppose I have a simple single layer neural network, with n inputs and a single output (binary classification task). If I set the activation function in the output node as a sigmoid function- then the result is a Logistic Regression classifier. In this same scenario, if I change the output activation to ReLU (rectified linear unit), then is the resulting structure same as or similar to an SVM? If not why?
