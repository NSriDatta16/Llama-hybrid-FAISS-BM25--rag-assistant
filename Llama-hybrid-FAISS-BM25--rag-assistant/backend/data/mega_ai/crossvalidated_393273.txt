[site]: crossvalidated
[post_id]: 393273
[parent_id]: 
[tags]: 
Normalizing Time Series Records of Different Time Lengths

I have several million records, some of which have data points for few time points (>=3 time points) and others have data for hundreds of time points. How do I normalize data of different time lengths such that I can analyze records in aggregate over time? Crucial to the analysis is to be able to analyze first datapoint to first datapoint, last to last, and having some fidelity around the intermediate data points. I was thinking about ranking their data-points such that the first event would be 0 and the last would be 100, and the intermediate would be some percentile between the two. This method by in large works, but i wanted to see if there are any other ideas out there.
