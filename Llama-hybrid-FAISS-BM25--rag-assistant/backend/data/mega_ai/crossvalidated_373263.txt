[site]: crossvalidated
[post_id]: 373263
[parent_id]: 95340
[tags]: 
LR gives calibrated probabilities that can be interpreted as confidence in a decision. LR gives us an unconstrained, smooth objective. LR can be (straightforwardly) used within Bayesian models. SVMs donâ€™t penalize examples for which the correct decision is made with sufficient confidence. This may be good for generalization. SVMs have a nice dual form, giving sparse solutions when using the kernel trick (better scalability) Check out Support Vector Machines vs Logistic Regression, University of Toronto CSC2515 by Kevin Swersky.
