[site]: crossvalidated
[post_id]: 469053
[parent_id]: 468913
[tags]: 
On the question: "Why if I project my points in a subspace that has no orthogonality constraints, my features end up to be not necessarily independent?, per the theorem (already cited): "Any orthogonal set of vectors is linearly independent", it follows orthogonality also implies linearly independent. However, a source cited above notes that "the new features we end up do not have to be independent (no orthogonality constraints in the neural networks)", so the new features data set is neither orthogonal or even [EDIT] necessarily [END EDIT] linearly independent. So, this topic's very title question: "PCA versus Linear Autoencoder: features independence", appears to be [EDIT] possibly [END EDIT] problematic as there is [EDIT] necessarily [END EDIT] no 'independence', at least, in a Linear Algebra sense. On Principal Component Analysis (PCA), per a source , to quote: Given a collection of points in two, three, or higher dimensional space, a "best fitting" line can be defined as one that minimizes the average squared distance from a point to the line. The next best-fitting line can be similarly chosen from directions perpendicular to the first. Repeating this process yields an orthogonal basis in which different individual dimensions of the data are uncorrelated. These basis vectors are called principal components, and several related procedures principal component analysis (PCA). And, importantly related to applications: PCA is mostly used as a tool in exploratory data analysis and for making predictive models. It is often used to visualize genetic distance and relatedness between populations. So the referred to data reduction construct referred to as 'features independence' may result in data consolidation, but it, I would argue, in a comparative sense to PCA, does not readily foster facile paths to either exploratory data analysis or statistical-based predictions. [EDIT] Further, with respect autoencoder, some background material provided by Wikipedia displaying its usefulness in various areas, clearly diverging from PCA, to quote: An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner.[1] The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise”. Along with the reduction side, a reconstructing side is learnt, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name. Several variants exist to the basic model, with the aim of forcing the learned representations of the input to assume useful properties.[2]...Autoencoders are effectively used for solving many applied problems, from face recognition[5] to acquiring the semantic meaning of words.[6][7]
