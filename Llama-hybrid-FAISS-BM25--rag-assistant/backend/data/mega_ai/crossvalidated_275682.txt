[site]: crossvalidated
[post_id]: 275682
[parent_id]: 275677
[tags]: 
NHST relies on p-values, which tell us: Given the null hypothesis is true, what is the probability that we observe our data (or more extreme data)? We assume that the null hypothesis is true—it is baked into NHST that the null hypothesis is 100% correct. Small p-values tell us that, if the null hypothesis is true, our data (or more extreme data) are not likely. But what does a large p-value tell us? It tells us that, given the null hypothesis, our data (or more extreme data) are likely. Generally speaking, P(A|B) ≠ P(B|A). Imagine you want to take a large p-value as evidence for the null hypothesis. You would rely on this logic: If the null is true, then a high p-value is likely. ( Update: Not true. See comments below. ) A high p-value is found. Therefore, the null is true. This takes on the more general form: If B is true, then A is likely. A occurs. Therefore, B is true. This is fallacious, though, as can be seen by an example: If it rained outside, then the ground being wet is likely. The ground is wet. Therefore, it rained outside. The ground could very well be wet because it rained. Or it could be due to a sprinkler, someone cleaning their gutters, a water main broke, etc. More extreme examples can be found in the link above. It is a very difficult concept to grasp. If we want evidence for the null, Bayesian inference is required. To me, the most accessible explanation of this logic is by Rouder et al. (2016). in paper Is There a Free Lunch in Inference? published in Topics in Cognitive Science, 8, pp. 520–547.
