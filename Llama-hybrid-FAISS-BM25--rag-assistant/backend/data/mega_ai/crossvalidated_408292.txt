[site]: crossvalidated
[post_id]: 408292
[parent_id]: 408274
[tags]: 
This is exactly one of the reasons why people use Monte Carlo integration rather than deterministic numerical integration: you avoid the normalizing constant, which can be rather large or rather small. Notice also that $\theta>0$ , as it is the parameter of a Poisson distribution. You can use a half-normal distribution as a prior, but you need to take into consideration that this is a positive parameter. If you have the extreme desire of using deterministic numerical integration, then you need to play with the integration range and other specifications. You are using the automatic choice -Inf, Inf , which works in simple problems (and actually it should be 0, Inf ), but you cannot always trust automatic command specifications (computers are not as smart as people assume sometimes). For instance, check the following example: # Simulated data set.seed(123) n The idea is to plot the unnormalized function in order to identify an appropriate integration range, and hope that the integral is still within the reach of the precision of your computer. So, for instance, if $n$ is very large and the sample values are also large, you may end up with a normalizing constant of the order $10^{-100000}$ , which cannot be calculated in this way unless you have access to NASA's supercomputing power. Otherwise, MCMC will do a good job: # Using Metropolis algorithm library(mcmc) log.g 0) return( n*mean(y)*log(theta) -n*theta -0.5*theta^2 ) else return(-Inf) }) out $batch, probability = T, breaks = 100, ylim = c(0,4)) curve(f,0,1.5,add=T, col = "red", n=1000, lwd = 2) # Mean mean(out$ batch) # Variance var(out$batch)
