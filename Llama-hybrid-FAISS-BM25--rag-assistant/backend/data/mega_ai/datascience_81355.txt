[site]: datascience
[post_id]: 81355
[parent_id]: 81233
[tags]: 
Your description is apt. There isn't anything especially "mathematical" happening here, aside from the AdaBoost algorithm itself. In psuedocode, something like this is happening: For n in 1 .. N_Estimators do Train classifier Tn on data X with weights W Compute weighted residuals E from Tn Update W based on E Renormalize W end In your case, Tn would be a Random Forest model, which is itself an ensemble based on bagging. So at each iteration of the "outer" AdaBoost model, an entire Random Forest model is being trained, i.e. several decision trees are fitted on random sub-samples of data points and features. Of course, this is an unusual setup for a boosting model. But there's no conceptual or computational reason why you couldn't run the algorithm this way. If you are curious about how exactly the weights are computed and updated, Scikit-learn uses the SAMME algorithm , which is based on but not exactly identical to the original AdaBoost. SAMME is described in "Multi-Class AdaBoost" by Zhu, Rhosset, Zhou, & Hastie (2006).
