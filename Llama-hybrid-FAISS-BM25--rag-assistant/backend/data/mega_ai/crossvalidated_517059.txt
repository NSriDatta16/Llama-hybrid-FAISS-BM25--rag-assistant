[site]: crossvalidated
[post_id]: 517059
[parent_id]: 517053
[tags]: 
There is no "best" way to encode your data before knowing what model you will use and its performances. You can optimize for memory or other constraint, but no definite answer can be given based like that. However there is an entire field of NLP dedicated to word embedding. It basically encodes words (or tokens) into fixed-length real vectors. There are many ways to create embeddings but what they all try to do is to account for the pattern that in present in Natural Language regardless of the task you will try to solve later. https://en.wikipedia.org/wiki/Word_embedding
