[site]: crossvalidated
[post_id]: 224383
[parent_id]: 
[tags]: 
Learning vector embeddings from distances

So... I have a set of entities $\mathcal{E} = \{e_i \mid i \in [1,n]\}$, and I have a proper distance metric defined over $\mathcal{E}\times\mathcal{E}$, call it $d$, so the distance between $e_i$ and $e_j$ is $d(e_i, e_j)$. I'd like to find some vectors $x_1, \dots, x_n$ s.t. $\mid\mid\! x_i - x_j \!\mid\mid = d(e_i, e_j)$. Unfortunately, the system of equations constraining the $x_i$ is not easily solvable by a machine, as far as I know... I'm considering some iterative methods to compute the $x_i$ exactly, but I'm wondering if anyone can suggest any embedding techniques out there that only require a distance metric? In other words, are there any embedding techniques that don't require there to exist an original, higher-dimensional space that you're reducing? Is it sufficient to have a distance metric? Thanks!
