[site]: crossvalidated
[post_id]: 99772
[parent_id]: 99725
[tags]: 
In short, because in time series, the observations depend on previous observations/errors. Consider just a simple AR(1), for example. The next observation depends on the previous one. When you're predicting time $t+1$ from time $t$, you know $y_t$, but you have an uncertain prediction because of variation in the process and the uncertainty in the parameters. Each time you go out another time step, you have not only the variation in the process and the uncertainty in the parameters, you compound the uncertainty because you don't know where you were on the previous time step - on average you were at the previous prediction (e.g. at $t+2$ you're on average going from $\hat y_{t+1}$), but you may have been higher or lower (because the prediction at $t+1$ had that parameter uncertainty and process variation). So it compounds as you go - each step your previous value was uncertain, and so the next value is less certain still.
