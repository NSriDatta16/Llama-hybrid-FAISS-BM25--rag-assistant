[site]: crossvalidated
[post_id]: 246418
[parent_id]: 
[tags]: 
Can anomaly detection work without the assumption of Normal Distribution of the underlying data?

I've been reading up on Gaussian-based Anomaly detection and it seems, the classic assumption is to assume the data fits the normal distribution. However, is this a hard requirement? What if one of my data points followed a Beta distribution, another one normal, yet another one Poisson, Gamma, etc.? Would the anomaly detection algorithm still work? It seems the principle that $$p(x) = p(x_1;\mu_1,\sigma_1^2)\times ...\times p(x_n;\mu_n,\sigma_n^2) I wasn't able to find any reference that says that we could use the above formula for features $x_1$ to $x_n$ each of which can follow a different distribution. Most sources and Andrew Ng's Machine Learning class on Coursera too says that you should play with transformations to "transform" the data to a normal distribution if it isn't normal to begin with. Questions: Why is the assumption of normality crucial to the algorithm? If we use different distributions for each of the features does the algorithm still work? How good/bad does it get?
