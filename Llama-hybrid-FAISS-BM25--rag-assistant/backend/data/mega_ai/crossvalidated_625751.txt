[site]: crossvalidated
[post_id]: 625751
[parent_id]: 
[tags]: 
Is there a way to know if a machine learning model is extrapolating at inference time?

I am working on a regression model, trained with a finite dataset. The algorithm that I use is a LightGBM, but I think the solution I am looking for would be algorithm-agnostic in essence. I suspect that at evaluation time, some of the predictions of my model are off because it is extrapolating too far away from the examples it has seen during training. Is there a general sound way to detect if a model is in the extrapolation domain during inference? Even better: is there a way to quantify "how far" a test example is from the training set? One could probably use a distance metric such as the Mahalanobis distance, but I am not sure if it would be the correct way to do it (moreover, to use this specific metric would require the features to be distributed according to a Gaussian distribution, which is rarely the case in practice). I found this interesting related article : To what extent should we trust AI models when they extrapolate? , which introduces the idea of the convex hull defined by the training set to define the extrapolation distance. But more generally, I am a bit surprised that there doesn't seem to be much more discussion, investigations or Python code developed about this issue. Any interesting view about this, or any source / code to share? Thank you!
