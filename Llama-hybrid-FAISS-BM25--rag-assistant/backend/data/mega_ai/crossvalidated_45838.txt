[site]: crossvalidated
[post_id]: 45838
[parent_id]: 43127
[tags]: 
You can always force the $\beta$s to be between 0 and 1 by rewriting the likelihood in terms of $\beta_j = f(\beta^*)$ for some function $f$, e.g. $1/(1+\exp(-\beta^*))$. Then optimise the $\beta^*$s but report the $\beta$s. However that won't automatically get you an interpretation in terms of the probability that $\beta$ is relevant to the regression. So I think the first thing to do is separate , conceptually speaking, the question of the probability that each $\beta_j$ is equal to zero i.e. not included/relevant from the value it should take if it is relevant. As for models, the keywords are probably Bayesian and automatic relevance determination . Assume a Bayesian model with a hierarchical prior such that $p(\beta \mid \alpha) = \prod_j N(0, \alpha_j^{-1})$ and maybe a gamma prior on the $\alpha$s if you're going to sample. As $\alpha_j$ goes to infinity, $\beta_j$ is more probably equal to zero. Then integrate out (or optimise) the (ir)relevance parameters $\alpha$ during posterior inference to get predictions for $y$ and separately examine the $\alpha$s to give you an idea which $\beta$s are irrelevant. For more details on these methods, the references I have to hand are MacKay (ms) or Tipping's work , and a tech report by Minka et al. discussing fitting stategies. I've no idea what is your $m$ parameter(s), so I may have missed something to do with that.
