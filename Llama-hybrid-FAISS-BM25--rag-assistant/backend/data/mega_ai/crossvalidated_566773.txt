[site]: crossvalidated
[post_id]: 566773
[parent_id]: 
[tags]: 
Why do we concatenate the condition vector two times in conditional variational autoencoder (CVAE)

I don't quite understand why, in conditional variational autoencoders (CVAEs) we condition on both the encoder and the decoder. In particular, in CVAE the objective function is defined to be: $$\mathcal{L}_{CVAE} = \mathbb{E}_\textbf{z}[log \, p_\theta(\textbf{x}|\textbf{z},\textbf{c})] - D_{KL}[q_\phi(\textbf{z}|\textbf{x},\textbf{c})||p(\textbf{z}|\textbf{c})]$$ Then I don't understand conceptually the sense of conditioning on inputs, then sampling from latent representation, then conditioning again on the sampled vector, to reconstruct the input. I mean, if we already concatenate the label vector $\textbf{c}$ at the beginning, why do we do that again on the latent representation?
