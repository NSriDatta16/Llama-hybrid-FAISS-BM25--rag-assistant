[site]: crossvalidated
[post_id]: 268830
[parent_id]: 
[tags]: 
Is it correct to do hyperparameter tuning based on validation error and not test error?

I am new in Statistics and Data Science. I would like to use "academic" data for training and testing for overfitting. However, I would like to get the classifier accuracy from "real-world" data and do hyperparameter optimization based on the score obtained with this data (instead of the usual grid search with cross validation). Would it be correct?
