[site]: crossvalidated
[post_id]: 244695
[parent_id]: 244681
[tags]: 
The model is new and therefor every new word is "predictable" I don't find this notion so intuitive. Perhaps better to say that given that you have no knowledge of which words are more likely to begin with, all words are equally probable. A word is very common and therefore always predictable In this case, probably the best descriptor is going to be an entropy measure . These have been used, for example, to detect fraud by identifying unexpected sequences of numbers, or to improve compression algorithms by identifying common components. A word arrives once an hour, and therefore predictable at specific times I agree with Gung on this, as I don't think you'll be able to find a satisfactory fusion of this point with the other one. What you could do is something like: The probability of a given word appearing is based on a count process (e.g. Poisson) with an unknown, varying rate parameter. You then seek to infer, based on your data for a particular word, what the most likely rate parameter might be at a given time. If you assume all the words are independent, then each might have its own Poisson process which governs the frequency with which it appears. If, as I would expect is the case in general, the latent rate parameters are in fact correlated, you might seek to identify the correlations between the latent parameters (corresponding to words of similiar topics arising at similiar times). If you took a Bayesian approach towards this, such that you had a principled way of estimating the uncertainty associated with your predictions, you could estimate the range of possible values that you believe that the word would take, and use something like the standard deviation of the estimates as a measure of predictability.
