[site]: crossvalidated
[post_id]: 586813
[parent_id]: 
[tags]: 
Intuitive explanation for summing the embedding and positional encoding in the Transformer's embedding

In the Transformer model, the embedding and positional encoding are summed together to represent a word in each location ('positional embedding' from now on). This way, each cell contains semantic and positional information. However, since two types of information are represented by one number only, it seems unclear to which extent semantic information and positional information are represented in each cell. For example: word1 = [0.2, 0.1, -0.3] Can be the sum of: embedding1 = [-0.34,0.52,0.69] positional_encoding1 = [0.54,-0.42,-0.99] Or from: embedding1 = [-0.64,-0.81,0.16] positional_encoding1 = [0.84,0.91,0.14] Hence, summing two cells makes no sense to me. Yet, Transformers are very effective algorithms, so I probably miss something. Is the problem that I describe a problem indeed? And can someone provide an intuitive explanation for summing the embedding and positional encoding?
