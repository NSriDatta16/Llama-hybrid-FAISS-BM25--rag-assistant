[site]: crossvalidated
[post_id]: 566834
[parent_id]: 566820
[tags]: 
It looks like the problem here is with how you're doing you're power analysis but please let me know if I've misunderstood something. With a sample of 93, the means (and thus also variance) of the samples will vary considerable from sample to sample. If you run this simulation thousands of times and average results (as you seem to have done with your graph), you will get very precise estimates, but presumable you're only going to run your experiment once so you won't have thousands of iterations to average. For the power analysis, you need to compare one control sample to one treatment sample. Here's a simple example: control The resulting power is 0.798. So if you randomly sampled from a distribution of 0.5 and a distribution of 0.7 with a sample size of 93 for each and you did this thousands of times (as we did above), ~80 percent of the time you could reject that null under the significance level of 0.05. Notes. As I understand it, in this case, the prop.test in R implements a Chi-squared test utilizing the Yates continuity correction. However, with this correction, the power is ~0.75. Utilizing the chisq.test directly allows one to calculate the p-value without the Yates continuity correction. Whether you should use the Yates continuity correction, some other corrections, or some other test for a case like this is debated, and there are several good threads on this here. I am a little surprised, though, that the prop.power.test function seems to calculate the power without the Yates continuity correction when the prop.test only implements the correction but perhaps someone else can explain this.
