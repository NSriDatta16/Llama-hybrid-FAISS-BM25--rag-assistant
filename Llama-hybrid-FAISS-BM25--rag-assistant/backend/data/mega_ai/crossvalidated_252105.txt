[site]: crossvalidated
[post_id]: 252105
[parent_id]: 252046
[tags]: 
This is not outlier detection, because you have a definite hypothesis concerning which value is an outlier. You simply want to see whether $Y_{n+1}$ is "consistent with" the $Y_1, \ldots, Y_n$. This is a job for a prediction interval. A general formula for prediction intervals in an (asymptotic) regression setting, which generalizes this one, is given at Relationships between two variables , but it is not explained. The following indicates how a prediction interval can be developed and exposes some of the assumptions. The correctness of this procedure is demonstrated with simulations, which also inform us about how well the procedure performs in various circumstances. When all $n+1$ values are independently drawn from the same Normal distribution, it is clear (from standard theoretical considerations) that the prediction interval must depend on the ratio $$X = \frac{Y_{n+1} - \bar Y}{s_Y}$$ where $$\bar Y = \frac{1}{n}\sum_{i=1}^n Y_i$$ is the mean of the first $n$ $Y_i$ and $$s_Y^2 = \frac{1}{n-1}\sum_{i=1}^n (Y_i - \bar Y)^2$$ is their sample variance. The three random variables $Y_{n+1}$, $\bar Y$, and $s_Y$ are independent (because, as is well known, $s_Y$ is independent of $\bar Y$ even though they are computed from a common set of variables). The numerator of $X$ has a Normal distribution because it's a linear combination of Normal variables. We may compute that its mean is zero and its variance is $$\operatorname{Var}(Y_{n+1}-\bar Y) = \operatorname{Var}(Y_{n+1}) + \operatorname{Var}(\bar Y) = \sigma^2 + \frac{1}{n}\sigma^2 = \left(1+\frac{1}{n}\right)\sigma^2.$$ As usual, $s_Y^2$ has a Chi-squared distribution with $n-1$ degrees of freedom (d.f.). Consequently $X$ is $\sqrt{1+1/n}$ times a variable having a Student $t$ distribution with $n-1$ d.f. Let $t_{\alpha/2,n-1}$ be the $\alpha/2$ quantile of that distribution and $t_{1-\alpha/2,n-1}$ be the $1-\alpha/2$ quantile, so that $\alpha/2 + 1 - (1-\alpha/2)=\alpha$ of its probability is omitted (symmetrically) at both tails. Then a symmetrical prediction interval is $$\mathcal{I}_{\alpha, n}(Y_1,\ldots,Y_n) = \big[\bar Y + t_{\alpha/2,n-1}\sqrt{1+\frac{1}{n}}s_y, \bar Y + t_{1-\alpha/2,n-1}\sqrt{1+\frac{1}{n}}s_y\big].$$ It has the following defining property: Before observing any of $Y_1, \ldots, Y_n, Y_{n+1}$, the chance that $Y_{n+1}$ lies within the interval $\mathcal{I}_{\alpha, n}(Y_1,\ldots,Y_n)$ is $1-\alpha$. You will therefore conclude that when $Y_{n+1}$ is outside the prediction interval, it likely was not drawn independently from a Normal distribution with the same mean as $Y_1, \ldots, Y_n$. By selecting your risk of a false conclusion (under these hypotheses) to be a sufficiently small value of $\alpha$, you can assure it will be unlikely to identify $Y_{n+1}$ as "different" when in fact all $n+1$ of the $Y_i$ are independently drawn from the same distribution. (This procedure sometimes is viewed as a Student $t$ test of $Y_1, \ldots, Y_n$ versus $Y_{n+1}$ using a "pooled variance" estimate.) To demonstrate this procedure and to show it really works, I ran a simulation study using various values of $n=2, 10, 50$. For each sample of size $n$, I computed prediction intervals of levels $\alpha=0.20, 0.05, 0.01$. I then independently generated $Y_{n+1}$ from another Normal distribution with the same variance but a mean increased by $\delta$. I recorded the proportion of times $Y_{n+1}$ fell in this interval. Ideally, when $\delta=0$, the prediction interval should include $Y_{n+1}$ in $100-100\alpha\%$ of all cases. Here are the results for $5000$ replications: 2 10 50 Target 0.2 0.799 0.792 0.805 0.80 0.05 0.952 0.947 0.950 0.95 0.01 0.989 0.992 0.989 0.99 Rows correspond to $\alpha$ while columns correspond to $n$. Evidently, the proportions are what we expect, regardless of the sample size: all values on the first row are very close to the target of $1-0.2=0.8$, all values on the second are close to $1-0.05=0.95$, and so on. I then re-ran this simulation (using the same random number sequence) with $\delta=2$: that is, $Y_{n+1}$ was drawn from a normal distribution averaging two standard deviations more than the other $Y_i$. This time, ideally, $Y_{n+1}$ would not lie within any of the prediction intervals--but that's too much to hope for, because two SDs isn't that much of a difference. Here are the results: 2 10 50 Target 0.2 0.593 0.313 0.246 0.80 0.05 0.903 0.598 0.502 0.95 0.01 0.982 0.838 0.740 0.99 The good news is that an interval with $\alpha=0.2$ and a largish sample of $n=50$ usually concluded $Y_{n+1}$ was different: $100 - 24.6 = 75.4\%$ of the time. The bad news is that intervals with smaller $\alpha$ didn't detect a difference most of the time. With a larger inherent difference of $\delta=4$ the results are 2 10 50 Target 0.2 0.308 0.0094 0.0054 0.80 0.05 0.796 0.0834 0.0272 0.95 0.01 0.959 0.3090 0.1090 0.99 We can't accomplish much with a sample of $n=2$ and low $\alpha$, but otherwise in the vast majority of cases $Y_{n+1}$ is outside the prediction interval. Studies like this give us a sense of the power of the prediction interval procedure to detect differences. Use the results to select an appropriate value of $\alpha$ and, if you have the option, to determine how large $n$ needs to be. set.seed(17) n.sim
