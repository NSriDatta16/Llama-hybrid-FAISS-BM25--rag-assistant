[site]: crossvalidated
[post_id]: 240971
[parent_id]: 
[tags]: 
Matrix form of partial derivatives of weights in a neural network

I'm implementing a (vanilla) neural network, from scratch in R, following Hastie & Tibshirani. As they suggest on page 397, I'm doing it via a conjugate gradient optimizer ( optim in R). Here's the code: y Now, optim should work faster if I supply gradient functions. Given that I'm working on a regression problem, these are as follows, following the notation in ESLI: $$ \frac{\partial R_i}{\partial \beta_m} = -2(y_i - f(x_i))g'(\beta_m^Tz_i)z_{mi} = -2(y_i - f(x_i))z_{mi} \text{ (given that $g(x) = x$)} $$ and $$ \frac{\partial R_i}{\partial \alpha_{ml}} = -2(y_i - f(x_i))g'(\beta_m^Tz_i)\beta_m\sigma'(\alpha^T_m x_i)x_{il} = -2(y_i - f(x_i))\beta_m\sigma'(\alpha^T_m x_i)x_{il} $$ Where $R$ is the squared error, $i$ is an observation, and $m$ is a hidden unit. In matrix form, the first one is simply: $$ \frac{\partial R(\theta)}{\partial \beta} = -2(\mathbf{y - \hat{y}})^T\mathbf{z} $$ where $R(\theta) = \displaystyle\sum_i R_i$. $$ \frac{\partial R(\theta)}{\partial \alpha} = ??? $$ What is the second? It has got to be a $p \times N_Z$ matrix, because that's the dimension of $\alpha$. But a straightforward translation of the second equation gives me matrices that are not conformable. Here is a brute-force method of computing it with for-loops -- it is painfully slow: drda Here is a slightly faster version, looping only over $i$. It is still slow: library(foreach) drda What am I missing?
