[site]: crossvalidated
[post_id]: 560543
[parent_id]: 560411
[tags]: 
Typically k-fold is not used in neural networks because it's more expensive. When dataset is large enough, one validation set is more economic, and you end up with a single final model. The test set (i.e. 10K samples) is not to be touched until your model is ready. So, you'll use your 60K samples for train+validation. A 15-20 % of the 60K samples can be chosen as validation for example. And, for the early stopping, you should look at validation loss, because that's more meaningful in terms of generalization performance. Once your model finishes the training, the performance is reported on the test set.
