[site]: datascience
[post_id]: 40471
[parent_id]: 40467
[tags]: 
I would argue and say that different machine learning algorithms use different optimization algorithms. Linear regression has a direct solution $(X'X)^{-1}X'Y$ . For logistic regression sklearn python library uses a Liblinear C++ library. The library uses "a trust region Newton method". SGD (Stochastic Gradient Descent) with Backpropagation is mainly used in neural networks and I did not hear about both of them before implementing my first neural network. LightGBM and XGBoost use their own invented heuristics to build decision trees. Generally speaking most of ML algorithms are fast. If the training time is in seconds there is no need to optimize the time of algorithm execution. If the training time is actually in years (most recent papers on neural networks by Google where they use hundreds of GPUs and CPUs) then you definitely start thinking about speed optimization. SGD is supposed to be inferior to other algorithms because it uses only first derivatives. The problem with the neural networks is that the gradient changes too fast and the calculation of the second derivative appears too costly (There are papers that benchmark it). SGD was specifically optimized for neural networks with momentum calculations and reducing learning rate after the networks stopped training. Constant attempts are made to improve it, for example ADAM. But ADAM is still considered to perform worse on validation sets (There was a big benchmark paper about it). There are some papers that try to use neural networks to choose the direction of the descent instead of using SGD. Papers show some marginal improvements in speed but the algorithms are still not used as the main method of optimization. Lots of other algorithms are tried for neural networks. For example, genetic algorithms. There are papers that use the idea of "stochastic" in SGD for genetic algorithms. According to their benchmarks they achieve 1/10th of the speed of SGD. They argue that the best neural networks are influenced a lot by the ability of SGD to find a minimum (like ResNet with residual links). But if you choose genetic algorithms then maybe it is possible to use more complicated neural networks where SGD actually fails. In term of how to call the algorithms. I prefere the name "optimization algorithm" or "algorithm used for training". But for some of the ML methods "optimization algorithm" sounds wrong. LightGBM and XGBoost have pretty complicated algorithms of building trees where they balance between optimization of a score (logloss or sum of squares) and using other algorithms to fight overfitting. Decision trees could be trained to zero error but then they are one of the worst predictors for the validation set. LightGBM and XGBoost do not reduce their errors to zero and thus allow to win most of machine learning competitions with structured data on Kaggle.
