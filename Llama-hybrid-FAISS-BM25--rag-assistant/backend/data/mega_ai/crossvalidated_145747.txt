[site]: crossvalidated
[post_id]: 145747
[parent_id]: 145746
[tags]: 
This is more from a linear model point of view, you can use these encodings for other machine learning algorithms. Some may be more appropriate than others. For a search term or keyword you could look into "categorical variable encoding", or "contrast coding". There are many possibilities. Here is a resource from UCLA that provides a nice overview for some of these. Another popular encoding that can't be used in regression (because it makes $X^T X$ not invertible) but is commonly used is what you described for neural networks. It's called one-hot encoding. Further down the rabbit hole, hierarchical linear models avoid the problem of having large numbers of independent variables, and so large degrees of freedom, (like the 365 NN nodes like you mention) by what amounts to partially pooling the regressions. The software packages STAN , JAGS , and BUGS are primarily for these types of models. Also, consider making features (call them $f_{i}$) that are products, etc. of other features. E.g. define $f' = f_1 \times f_2$, or maybe $f' = \frac{f_1 \times f_2}{log(f_3)}$. This can be very effective in some cases. Maybe google "feature engineering". This isn't a complete list, I'm sure there is more, but hopefully this can get you started.
