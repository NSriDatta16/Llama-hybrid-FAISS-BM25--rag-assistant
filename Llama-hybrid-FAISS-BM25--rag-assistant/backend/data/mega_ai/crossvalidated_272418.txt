[site]: crossvalidated
[post_id]: 272418
[parent_id]: 
[tags]: 
What sort of analysis method is most appropriate for computer monitoring data that includes samples at a given time?

I'm interested in what sort of modelling is best suited for data that includes samples collected at different times. My use case is to do with computer monitoring, so for the initial case assume that the data has 2 parameters: CPU load and the time of the sample. In the future I want to scale this to look at many features. The CPU parameter is expected to vary across a given day (and perhaps a week): for example, users will come in to work and log into their computers in the morning and perform some complicated queries first thing, which will result in a spike of CPU that will last a while. Thereafter, CPU might plateau or decay a bit and at some point, people will go home and CPU will decrease. I am interested in understanding both absolute anomalies (e.g. CPU > 90% irrespective of time, meaning perhaps you need a new machine, as well as time-based anomalies, e.g. 9am on Monday it's normal for CPU load to be high, but not 5pm on a Sunday and thus there's something odd that's going on). I am interested in the possibilities that a Bayesian approach might offer, as I have no fixed opinions on what the data will look like and what "normality" will look like or what probabilities are associated with seeing a particular event. I'd like to have my model give me a probability that expresses the likelihood of seeing a particular event and to update the model over time to reflect the new posterior based on prior events. Are there particular approaches that I should look at or ideas as to how to best solve this? Any particular papers that I could go to read? My maths is pretty reasonable but I'm not a statistician, I'm a software engineer, so I'd prefer to have resources that are not impenetrable to those without a PhD in maths please.
