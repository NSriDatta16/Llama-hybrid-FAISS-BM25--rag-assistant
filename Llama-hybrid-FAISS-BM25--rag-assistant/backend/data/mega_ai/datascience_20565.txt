[site]: datascience
[post_id]: 20565
[parent_id]: 
[tags]: 
Why we use transposed filter as the deconvolution operation instead of the pseudo inverse of filter?

I am trying to visualization CNN by the method in this paper " Visualizing and Understanding Convolutional Networks " According to this tutorial A guide to convolution arithmetic for deep learning (P.19) We can always rewrite convolution as a matrix multiplication For example, $Y$ is a feature map & $X$ is input & $W$ is a filter & $\otimes$ is convolution operation $$ \begin{bmatrix} y_{11} & y_{12} \\\\ y_{21} & y_{22} \end{bmatrix} = \begin{bmatrix} x_{11} & x_{12} & x_{13} & x_{14}\\\\ x_{21} & x_{22} & x_{23} & x_{24}\\\\ x_{31} & x_{32} & x_{33} & x_{34}\\\\ x_{41} & x_{42} & x_{43} & x_{44} \end{bmatrix} \otimes \begin{bmatrix} w_{11} & w_{12} & w_{13} \\\\ w_{21} & w_{22} & w_{23} \\\\ w_{31} & w_{32} & w_{33} \end{bmatrix} $$ In the matrix multiplication form $$ \begin{bmatrix} y_{11} \\\\ y_{12} \\\\ y_{21} \\\\ y_{22} \end{bmatrix} = C \begin{bmatrix} x_{11}\\\\ x_{12}\\\\ \vdots \\\\ x_{44} \end{bmatrix} $$ $$ C= \left[\begin{smallmatrix} w_{11} & w_{12} & w_{13} & 0 & w_{21} & w_{22} & w_{23} & 0 & w_{31} & w_{32} & w_{33} & 0 & 0 & 0 & 0 & 0\\\\ 0 & w_{11} & w_{12} & w_{13} & 0 & w_{21} & w_{22} & w_{23} & 0 & w_{31} & w_{32} & w_{33} & 0 & 0 & 0 & 0\\\\ 0 & 0 & 0 & 0 & w_{11} & w_{12} & w_{13} & 0 & w_{21} & w_{22} & w_{23} & 0 & w_{31} & w_{32} & w_{33} & 0\\\\ 0& 0 & 0 & 0 & 0 & w_{11} & w_{12} & w_{13} & 0 & w_{21} & w_{22} & w_{23} & 0 & w_{31} & w_{32} & w_{33}\\\\ \end{smallmatrix} \right] $$ So we can visualize filter by multiplying $C^{-1}$. OK, my question is why we use $C^{T}$ instead of the pseudo inverse $(C^{T}C)^{-1}C^{T}$?
