[site]: datascience
[post_id]: 109297
[parent_id]: 109276
[tags]: 
When reducing the dimensionality of data, you want to keep local characteristics of data such is nearest neighbours while preserving the holistic approach such as keep far data points still far from each others. This is a trade-off that most dimensionality reduction algorithms, specially non-linear ones, try to keep. It is a trade-off because usually and naturally going toward one of them will destroy the other one, so what we want is preserving local structure while preserving the global one. Example See the nonlinear curve bellow. It looks like an English S . Now let's discuss modelling of two different distances here. Those distances are the distance between dark blue region at the beginning of S and: yellow region immediately under it which is on the curvature of S very light blue region which is on the curvature of the opposite side How do you want to model it? The global structure says that distance (1) is smaller than (2) (right?), but you cognitively see and know that the continues form of S shape suggests that distance (1) is actually larger than (2). Simply because you see the global structure of S and you see that this structure is a continues form of many local structures shown by different colours. You intuitively know that if you walk on the data, you arrive to light blue region faster than yellow region! Here you are preserving local structure and if you don't, you will fall in the trap of seeing yellow region closer than blue one. This is what LLE showing you. It embeds data in a lower dimensional space in which light blue is more close to dark blue rather than yellow region, that means local structure was safely preserved, while the global structure of the shape is reduced from a S shape to a simple band (see it as opening or flattening S-shape). Hope I did not confuse you even more! Good luck
