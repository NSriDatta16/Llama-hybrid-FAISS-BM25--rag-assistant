[site]: crossvalidated
[post_id]: 463924
[parent_id]: 160439
[tags]: 
First, I recommend approaching this descriptively, not inferentially. You can show the difference you describe (the regression residual, assuming your predictions come from regression) in the context of all the other residuals. A straightforward way would be to plot residuals (Y-axis) vs. predicted values (X-axis). You and your readers can then judge the extent to which the residual in question stands out from the rest. If you prefer a univariate plot, you could create a histogram of the residuals for the same purpose; perhaps the point in question will show itself to be an outlier. And you could calculate its Z-score in the sense of how many standard deviations from the mean it falls. As to the sample size underlying each data point (reflecting size of health care organization): this is a good issue to recognize. If you were computing some statistic on all the residuals or all the observations, you might want to weight according to these underlying sample sizes. But in examining one point at a time for its unusualness, these weights would be less of a help. I advocate a descriptive, not an inferential, analysis because I can't think of a proper way to implement the latter. The fact that no one else has suggested an upvoted way after 250+ views of this post may say something. To obtain a p-value for a single prediction's difference from its corresponding observation, one would need to be able to specify the "rules" of the way chance (randomness) would operate, and then to show just how unusual the finding in question would be in light of those rules. But are there rules for what chance "normally" produces when it comes to residuals? One sees all sorts of patterns, and they differ by topic, scale of measurement, variable distributions, type of predictive approach, and level of predictive accuracy, if not more. Even if you standardize or studentize, some analyses produce residuals as large as +/-2.5; others, as large as +/-5. Moreover, residuals are the product of human modeling decisions and as such are perhaps by definition not a chance phenomenon. I see no good basis for saying, "By chance -- under a certain null hypothesis -- a difference as large as or larger than this particular difference would occur only __% of the time."
