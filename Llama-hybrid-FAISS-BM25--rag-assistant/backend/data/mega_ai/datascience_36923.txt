[site]: datascience
[post_id]: 36923
[parent_id]: 36861
[tags]: 
Assuming Gaussian normal distribution is usual but sometimes unrealistic, especially if your model is a least square of identical independent observations. Even if it is rarely the case, many will propose very convincing arguments to justify the distribution of the forecast is normal, mainly because you can compute confidence intervals easily. At side of these so called "exact test", you have resampling techniques, which are in facts the more practical way to answer your second question (discrete distribution). In two words, you create a simulated history with the same distribution as the observed history, make your forecast and repeat this a great number of time. Then you can make statistic on these simulated forecasts and measure (as opposed to compute) the confidence intervals. The various resampling methods differ on how to make the the simulated sample. The two more common techniques are: The Jackknife : you forget one point, which makes $n$ simulated sample ($n$ being the size of the original sample). The Bootstrap : you take randomly $n$ points of the observed sample, some point being taken once, some point taken twice, some point taken 3 time, some points no taken at all. This will produce as many simulated sample with a similar distribution, and whose "average" is the observed sample. I like the jackknife better because the bootstrap is problematic when the process evolves over time, but in your case, it seams you may use a bootstrap.
