[site]: crossvalidated
[post_id]: 346439
[parent_id]: 344987
[tags]: 
Power analysis for F-test The R functions calculate the power of a certain effect size for an F-test (in English it is is not 'size effect' like the French 'taille d'effet', but it is 'effect size' instead). The used method is explained with large detail in chapters 8 and 9 of Statistical power analysis for behavioral sciences by Jacob Cohen. When the F-test is used for comparison of nested linear models than the effect size can be expressed by the ratio: $$f = \frac{PV_S}{PV_E}$$ with $PV_S$ the model variance (source) and $PV_E$ the residual variance (error). There are several alternative different ways to express this $f$ for instance: in regression: by change in the correlation coefficient or $R^2$ value: $$f^2 = \frac{dR^2}{1-R_{\text{full model}}^2} $$ in anova: (comparison of means) by a term $\eta = \frac{\sigma_{means}}{\sigma_{means}+\sigma_{within}}$ for the proportion of the total variance made up by the variance of the means (partial $\eta$ is used in mixed designs when variances due to other interactions have been 'canceled') $$f^2 = \frac{\eta^2}{1-\eta^2}$$ in (oneway) anova: by relative spread of the means $d = \frac{m_{max}-m_{min}}{\sigma}$, which together with the way that the groups are spread allows to calculate $f$. The power is calculated by using the non-central F-distribution. under the null hypothesis the sampling distribution for the F-score would be described by the central-F-distribution, under the alternative hypothesis , with the specified effect size (and model design), the sampling distribution of the F-score is described by the non-central-F-distribution (with non-centrality parameter $f^2v$ where $v$ is the degrees of freedom of the error term). compare for instance with a more manual/backend calculation (both give power = 0.997) > # using the modelPower function > modelPower(pc=4, pa=5, N = 545, alpha=0.05, peta2=0.04) Results from Power Analysis pEta2 = 0.040 pa = 5 pc = 4 alpha = 0.050 N = 545.000 power = 0.997 > # using directly the F-distribution (central and non-central) > 1 - pf( qf(0.95, 1, 540) , 1, 540, ncp = 540*0.04/(1-0.04) ) [1] 0.9972399 (a small difference exist because the R function pwr.f2.test calculates the non-centrality parameter differently: not the F-test for regression models but the F-test for comparison of means. The wrapper function modelPower did not take this into account) How is this done in R lmSupport ? The R function modelPower which is a wrapper for pwr.f2.test allows you to specify: $f^2$ by the parameter f2 $\eta^2_{partial}$ by the parameter peta2 $dR^2$ and $R^2_{\text{full model}}$ by the parameters dR2 and R2 The parameters pc pa N (for the function modelPower ) specifying the model are calculated into the degrees of freedom that are used in the F-test. These are the parameters u and v for the function pwr.f2.test . This has been mixed up in your question. Related to your data I don't think that testing your Bernouilli/binomial model is doing well with an F-test. Your data has certain small numbers, with very few observations outside the group of 'Regular'+'Home' and very few observations of 'Yes'. You can not really express a deviation as Gaussian distributed error terms for such a model, and also a power calculation for such test would be wrong. You might be better of to compute the power manually by using simulations (it is the dumbest way, but maybe someone else has a smarter and more elegant method): Simulate thousands of outcomes with your covariates and a model with the desired effect size (difference in log odds) and see how often you fail to reject the null hypothesis. Below is a table of your data that may help you to get an intuitive impression of your data (the small amount of observations for several cells) and that a different way of power analysis (different than power for the f-test) would be more suitable. $$\tiny\begin{array}{lllllll}&\\ Gender \qquad & Times \qquad & Age \qquad & Type \qquad & Motif \qquad & Yes \qquad & No \qquad \\ Male & t 39 & Regular & Home & 3 & 33 \\ Male & t 39 & Regular & Other & 0 & 0 \\ Male & t 39 & Irregular & Home & 0 & 4 \\ Male & t 39 & Irregular & Other & 0 & 0 \\ \\ Male & 14 39 & Regular & Home & 0 & 42 \\ Male & 14 39 & Regular & Other & 0 & 0 \\ Male & 14 39 & Irregular & Home & 0 & 1 \\ Male & 14 39 & Irregular & Other & 0 & 2 \\ \\ Male & 26 39 & Regular & Home & 2 & 41 \\ Male & 26 39 & Regular & Other & 1 & 0 \\ Male & 26 39 & Irregular & Home & 1 & 1 \\ Male & 26 39 & Irregular & Other & 0 & 0 \\ \\ Female & t 39 & Regular & Home & 0 & 30 \\ Female & t 39 & Regular & Other & 0 & 0 \\ Female & t 39 & Irregular & Home & 0 & 3 \\ Female & t 39 & Irregular & Other & 0 & 3 \\ \\ Female & 14 39 & Regular & Home & 1 & 40 \\ Female & 14 39 & Regular & Other & 0 & 0 \\ Female & 14 39 & Irregular & Home & 0 & 3 \\ Female & 14 39 & Irregular & Other & 0 & 2 \\ \\ Female & 26 39 & Regular & Home & 1 & 51 \\ Female & 26 39 & Regular & Other & 0 & 0 \\ Female & 26 39 & Irregular & Home & 0 & 1 \\ Female & 26 39 & Irregular & Other & 0 & 1 \\ \\ \end{array}$$ a simpler table ignoring all the multiple levels and just looking at Regular vs Irregular: $$\begin{array}\\ &yes&no&total\\ Regular & 12 & 481 & 493\\ Irregular & 2 & 50 & 52\\ Total & 14 & 531 & 545 \\ \end{array}$$ Giving probabilites for 'yes' as $p_{\text{R}} = 0.0243$ and $p_{\text{I}} = 0.0385$, for regular and irregular. But these results (what one might consider as big enough difference/effect in probability or odds) are not significant. This is not due to a low number of $n_{total}=545$ but due to the small marginal group sizes of $n_{yes}=14$ and $n_{Irregular}=52$.
