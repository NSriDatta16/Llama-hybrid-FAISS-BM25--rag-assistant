[site]: crossvalidated
[post_id]: 196670
[parent_id]: 
[tags]: 
Targets of 0.1/0.9 instead of 0/1 in neural networks and other classification algorithms

Rumelhart, Hinton and Williams (PDF) wrote in 1986 in the context of training a neural network (page 12): One other feature of this activation function should be noted. The system can not actually reach its extreme values of 1 or 0 without infinitely large weights. Therefore, in a practical learning situation in which the desired outputs are binary {0, 1}, the system can never actually achieve these values. Therefore, we typically use the values of 0.1 and 0.9 as the targets, even though we will talk as if values of {0, 1} are sought. I haven't seen this advice in any more recent paper, nor in any piece of code implementing a neural network. My questions: Is this advice still valid, or was it disproved as ineffective at some point? Was this advice used (historically) in other algorithms, e.g. logistic regression?
