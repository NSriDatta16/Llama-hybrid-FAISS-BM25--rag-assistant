[site]: crossvalidated
[post_id]: 621460
[parent_id]: 
[tags]: 
Fluctuating validation accuracy with steady accuracy increase

I have four layers of CNN to predict Javanese script letter data . The training accuracy and loss monotonically increase and decrease respectively. But, my test accuracy starts to fluctuate wildly. I have tried changing the learning rate, reducing the number of layers, and introducing dropout regularization. But, it doesn't stop the fluctuations. Can anyone helps me figure out where I am going wrong? For reference, I used Adam for my optimizer and categorical crossentropy for my loss function. I also normalized my input like so: def extract_file(path): data = image_dataset_from_directory( directory=path, label_mode='categorical', image_size=(image_height, image_width), batch_size=32, color_mode='grayscale' ) return data normalization_layer = tf.keras.layers.Rescaling(1. / 255) validation_data = extract_file(val_dir) normalized_validation_data = validation_data.map(lambda x, y: (normalization_layer(x), y)) train_data = extract_file(train_dir) normalized_train_data = train_data.map(lambda x, y: (normalization_layer(x), y)) Model setup code: image_augmentation = tf.keras.models.Sequential([ tf.keras.layers.RandomRotation(factor=0.2, fill_mode='nearest', input_shape=(image_height, image_width, 1)), tf.keras.layers.RandomTranslation(height_factor=0.2, width_factor=0.2, fill_mode='nearest'), tf.keras.layers.RandomZoom(height_factor=0.2, fill_mode='nearest'), ]) model = tf.keras.models.Sequential([ image_augmentation, # 1st convolution tf.keras.layers.Conv2D(16, (3, 3), activation='relu'), tf.keras.layers.BatchNormalization(), tf.keras.layers.MaxPooling2D(2, 2), tf.keras.layers.Dropout(0.2), # The second convolution tf.keras.layers.Conv2D(32, (3, 3), activation='relu'), tf.keras.layers.BatchNormalization(), tf.keras.layers.MaxPooling2D(2, 2), tf.keras.layers.Dropout(0.2), # The third convolution tf.keras.layers.Conv2D(64, (3, 3), activation='relu'), tf.keras.layers.BatchNormalization(), tf.keras.layers.MaxPooling2D(2, 2), tf.keras.layers.Dropout(0.2), # The fourth convolution tf.keras.layers.Conv2D(128, (3, 3), activation='relu'), tf.keras.layers.BatchNormalization(), tf.keras.layers.MaxPooling2D(2, 2), tf.keras.layers.Dropout(0.2), tf.keras.layers.GlobalAveragePooling2D(), # Flatten the results to feed into a DNN tf.keras.layers.Flatten(), # 512 neuron hidden layer tf.keras.layers.Dense(512, activation='relu'), tf.keras.layers.Dropout(0.2), tf.keras.layers.Dense(128, activation='relu'), tf.keras.layers.Dense(20, activation='softmax') ]) Below is a plot of my model from epoch 1 - 50 x axis in # of Epochs
