[site]: datascience
[post_id]: 65860
[parent_id]: 65830
[tags]: 
From a theoretical perspective, I think this question is a variant of "given a problem, is there a way to determine the absolute best learning algorithm for it?". Why? Because as OP correctly suggests, if there was a way to represent all the possible models/priors, then assuming that an infinite stream of data is provided we would be able to eventually determine the absolute best learning algorithm. Unfortunately the No Free Lunch theorem is a theoretical result which states that there can be no such "absolute best" learning method. (to be honest personally I'm out of my depth with this kind of thing, but Wikipedia gives a nice short explanation of the implications of the theorem for machine learning .) From a practical perspective, this is a matter of design, i.e. how one chooses to represent the problem. It's easy to overlook how much simplification is made when one represents a problem. In the example proposed by OP, one could imagine an infinity of "alternative worlds": maybe the word "father" doesn't have its usual meaning; maybe it happens in a future where people can have any number of parents; etc. My point is: whenever a problem is formally stated, a vast number of assumptions are made anyway, most of them unconsciously. It's not only a matter of cost (it's important, of course), it's a matter of "targeting" the exact problem: if we tried to leave the space of possibilities completely open, then we cannot even start to solve the problem, in the same way that people have to agree on a common language in order to understand each other. Thus choosing a specific space of priors/models for a particular problem is akin to fixing the language of the problem, and when doing it it's important to take into account the limitations/assumptions associated with it.
