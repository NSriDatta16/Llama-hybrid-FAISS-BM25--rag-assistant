[site]: crossvalidated
[post_id]: 613013
[parent_id]: 610542
[tags]: 
How can I compare the semantic similarity of the answer it provides me with a reference question? With a text generation metric. See Evaluation of Text Generation: A Survey . Typical metrics: TF-IDF cosine , Rouge , Bleu, BertScore, Sentence-Bert, and more recently, GPTScore and G-Eval . Note that they have some serious limitations when evaluating GPT output {1,2}. References: {1} Goyal, Tanya, Junyi Jessy Li, and Greg Durrett. " News Summarization and Evaluation in the Era of GPT-3. " arXiv preprint arXiv:2209.12356 (2022). {2} Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, Tatsunori B. Hashimoto. Benchmarking Large Language Models for News Summarization . arXiv:2301.13848. Since comparing the semantic similarity of the predicted answer with a reference question has some limitations, one may look at complementing the analysis with a human evaluation. The following papers survey criteria used for human evaluation of generated texts (thanks Mengjiao Zhang for pointing me to refs {3,4}): {3} A Survey of Evaluation Metrics Used for NLG Systems {4} Perturbation CheckLists for Evaluating NLG Evaluation Metrics (unfortunately, they forgot about QA) From {3}:
