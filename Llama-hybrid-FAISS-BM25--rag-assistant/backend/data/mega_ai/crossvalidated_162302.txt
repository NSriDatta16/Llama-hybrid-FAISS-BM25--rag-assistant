[site]: crossvalidated
[post_id]: 162302
[parent_id]: 162250
[tags]: 
I would say that your understanding still needs some work, because your description is very vague, which tells us that you're unclear as to exactly what to say about GLMs. First, your statement "for machine learning problems we can base our models on different distributions" is somewhat ambigious. For linear regression models this makes more sense; you'll see later in the course that there are many nonlinear metheds that don't follow a pre-defined probability distribution. Remember that in regression problems, you're modeling the mean of the response variable as a function of the linear combination of predictors. When we perform ordinary least squares, we are restrained to several assumptions - like that the response variable is normally distributed around the mean and that variance of the response is independent of the predictors. This doesn't always hold in reality, so GLMs allow us to relax some of these assumptions by specifying the response variable distribution, a link function, etc. In other words, GLMs are a generalization and extension of least squares; they're still linear regression problems which are only a part of the overall machine learning theme. Andrew Ng's lecture notes may not be the best introductory source when it comes to this topic. I recommend reading up on some additional sources if you wish to get to know GLMs a bit better, like this chapter from Applied Regression Analysis and GLMs .
