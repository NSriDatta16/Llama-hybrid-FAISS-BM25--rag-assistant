[site]: crossvalidated
[post_id]: 368895
[parent_id]: 368685
[tags]: 
I'll try to address your questions in order, but I don't think this is the right approach, so you may also skip to the third quote. Should I ignore the variables that are non-significant in the coefficient table? Non-significant results are also results and you should definitely include them in the results. However, you should not focus too much on what the implications of their estimated coefficients might be. Namely, their large standard errors (or similarly: high $p$ -values) suggest that you might as well have observed an effect this large if the true effect were zero. Do we account for significance or non-signficance from the corresponding 1-tailed sig in Table 4 (correlations) for each variable or should we consider the 2-tailed sig in Table 1 (coefficients)? Table 1 shows the estimated coefficients of your explanatory variables. While bearing in mind that no causal relationship has been demonstrated, you can interpret significance here as: Does a unit change in this explanatory variable correspond to a significant change in the response variable? Table 4 appears to show the correlations of your fixed effects. Whether or not your explanatory variables strongly correlate to each other says more about whether you might have potential problems with estimating this model, rather then their effect on the outcome. Significance here could mean that you have collinearity issues, but there are better ways to diagnose collinearity . Now as to why I don't think you can best answer your research objective with multiple regression: I am planning to investigate how each variable in a framework are related to each other Unless you have a variable that can clearly be considered the outcome of the others, and you have some idea of which interactions to test for, I don't think multiple regression is the way to go here. Using multiple regression, you would have to regress all variables on all other variables and interpret a multitude of output tables. You are almost guaranteed to find spurious correlations and I doubt any $p$ -values would be significant after correcting for multiple testing . If you really want to use multiple regression, I suggest you forget about significance and instead construct a set of confidence intervals using the reported standard errors in table 1. You should clearly state that the goal is exploration and then you can propose which variables might correlate with which. A future study could then try to confirm/refute these findings. Instead, you might be interested in graphical models : In brief, you can find the partial correlations between variables by standardizing the precision matrix (the inverse of the covariance matrix). $^1$ Using a form of regularization (e.g. LASSO ), you can shrink the smallest partial correlations to zero, such that variables with zero partial correlation can be considered conditionally independent. The remaining non-zero partial correlations can then form an undirected graph, which gives you a single, intuitive representation of which variables 'interact' with one another. $^1$ This also has the interpretation of regressing all variables on each other, but with a single resulting network to interpret the results with. I don't know of any SPSS implementations, but you can download $\textsf{R}$ for free and use the glasso package (or try rags2ridges for a ridge regularization approach). Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert (2008): "Sparse inverse covariance estimation with the graphical lasso"
