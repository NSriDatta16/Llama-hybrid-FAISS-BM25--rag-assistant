[site]: datascience
[post_id]: 120672
[parent_id]: 
[tags]: 
How can I implement some data constraints on a neural network?

I want to implement a NN model to predict the cost of my car trips. My dataset is something as follows (this is just a small sample): distancia kilometraje precio_carburante coste 0 4.6 44676 1.3 0.39 1 5.2 44672 1.3 0.38 2 4.2 44667 1.1 0.29 3 3.5 44662 1.1 0.38 4 5.5 44659 1.1 0.38 That is, given the distance (km), mileage (km) and fuel cost (€), I want to predict the cost of the trip. So I've implemented this functions in order to achieve it: import pandas as pd import numpy as np from scikeras.wrappers import KerasRegressor from tensorflow.keras import layers from tensorflow import keras from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline from sklearn.model_selection import train_test_split def create_model(): """ Helper function to create the NN architecture """ # Create model model = keras.Sequential() model.add(layers.Dense(12, input_dim=3, activation="relu")) model.add(layers.Dense(8, activation="relu")) model.add(layers.Dense(1, activation="relu")) # Compile model optimizer = keras.optimizers.RMSprop() model.compile(loss="mse", optimizer=optimizer, metrics=["mse"]) return model def make_split( df: pd.DataFrame, random_state: int = 42, target: str = "coste", test_size: float = 0.3, ): X = df.drop(columns=target) y = df[target] X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=test_size, random_state=random_state ) print(f"Training data size: {X_train.shape}\nTest data size: {X_test.shape}") return X_train, X_test, y_train, y_test scaler = StandardScaler() # to normalize the data model = KerasRegressor(model=create_model, epochs=200, batch_size=10) pipe = Pipeline(steps=[("scaler", scaler), ("model", model)]) X_train, X_test, y_train, y_test = make_split(df=df, target="coste", test_size=0.2) pipe.fit(X_train, y_train) pipe.score(X_test, y_test) # this scoring gives about 0.99558, so good! def predict_single(distancia: float, kilometraje: int, precio_carburante: float): X = pd.DataFrame( dict( distancia=[distancia], kilometraje=[kilometraje], precio_carburante=[precio_carburante], ) ) return float(np.squeeze(pipe.predict(X))) The issue is that, although the predictions seem to be close to the real values, the network is not taking into account the real physical contribution that each of the features should have. That is, if for example I make the following prediction: predict_single(distancia=200, kilometraje=60000, precio_carburante=1.5) It returns me 12.525€. However, if I maintain the distance and fuel price, and decrease the vehicle mileage, what I get is a greater cost. predict_single(distancia=200, kilometraje=1000, precio_carburante=1.5) Which gives 13.142€. Obviously, this makes no sense at all (if the car has fewer kilometers it will be newer and therefore consume less), which leads me to wonder if it is possible to incorporate that knowledge to the network in the form of constraints. Something like "Increasing any one variable, while keeping all other variables constant, should always increase the cost."
