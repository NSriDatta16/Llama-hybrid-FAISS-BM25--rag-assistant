[site]: crossvalidated
[post_id]: 298331
[parent_id]: 298328
[tags]: 
Yes, classical regression analysis can be performed on big data sets - it's not having a large number of records that violates assumptions but other conditions such as nonlinearity, lack of independent errors, heteroscedasticity, and multicollinearity. Recall that a p-value estimates the probability of observing a given statistic, such as a z-score, having a magnitude the same or greater than the observed value given that the null hypothesis (and underlying statistical distribution) is actually true. p-values still retain the same meaning whether you're conducting hypothesis tests on thousands or billions of records. Usually it's very small datasets, for example less than 10 observations per variable in a multiple regression analysis, where normality assumptions are be violated, and even then a Student's t-distribution score may be used as a better approximation of the standard error distribution. With big datasets you may have the power to detect even slight associations between variables. For example, all of the coefficient estimates in a multiple regression may end up being statistically significant, with p-values well below the standard 0.05 threshold. In those situations, the effect sizes themselves become more useful for interpreting results rather than the p-values. Keep in mind even when analyzing the whole data set with billions of records, in a sense you're still sampling from a much larger (theoretically infinite) population since you're drawing records from a limited number of observations during a finite period of time. Machine learning and data mining techniques are still making inferences for this larger population, even when there are no p-values or standard errors.
