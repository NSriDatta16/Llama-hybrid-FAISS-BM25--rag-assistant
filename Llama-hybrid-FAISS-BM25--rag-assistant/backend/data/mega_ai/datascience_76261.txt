[site]: datascience
[post_id]: 76261
[parent_id]: 
[tags]: 
What's the input dimension for transformer decoder during TRAINING?

For example, translate English sentence A to French sentence B. During training with ith word in B, all previous words before B will be fed to decoder, whose length will change for different i. How this is handled so that it can fit into a fixed dimension in the final linear layer during TRAINING?
