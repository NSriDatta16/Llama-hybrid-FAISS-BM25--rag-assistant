[site]: crossvalidated
[post_id]: 138844
[parent_id]: 138841
[tags]: 
You can't compare information criteria between different fitting methods. AIC and friends involve a constant that different fitting algorithms set to different values. You can compare AICs for different models fitted by the same method . So no help there. Looking at rolling out-of-sample forecasts was already exactly the right thing to do. Now you know that each model is best one third of the time. You could now also look at the magnitude of the errors (MAD or MSE) - perhaps one model sometimes yields very low, sometimes very high forecasts. Failing that, it may well be that all three methods are equally good. One smart trick to improve forecast accuracy is: calculate forecasts from all three methods and average them within each future time bucket. Averaging forecasts, in particular from "very different" methods, almost always improves accuracy and also reduces error variance.
