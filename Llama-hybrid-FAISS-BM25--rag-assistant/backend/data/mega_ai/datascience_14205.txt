[site]: datascience
[post_id]: 14205
[parent_id]: 
[tags]: 
Hyper-parameters for character-level RNN models

I am try to recreate Karpathy's great tutorial on RNN character-level text generation using TensorFlow - Promise to contribute the code back to the community once it works. Meanwhile, my code runs, but the model does not converge. It stays stuck on the same level of error and emits rubbish. I suspect that the code is correct but my hyper-parameters are off. If someone previously trained a text-generating RNN successfully, I would be glad to get some hints on the parameters below. The input I use currently is NLTK Bible corpus, about 4MB of text, but I can use something else. number of steps, i.e. sequence length batch size number of hidden units number of layers Optimizer and optimization parameters - I'm using Adagrad with decaying learning rate, trying to mimick the original script, but it is not a must Any other tricks for the network architecture or optimization Most important, the number of epochs (full scans of the data) needed before the model start emitting something readable
