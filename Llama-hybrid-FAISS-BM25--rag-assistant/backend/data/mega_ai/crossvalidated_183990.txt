[site]: crossvalidated
[post_id]: 183990
[parent_id]: 183840
[tags]: 
Average. Examples: Notes to Andrew Ng's Machine Learning Course on Coursera compiled by Alex Holehouse. Summing the gradients due to individual samples you get a much smoother gradient. The larger the batch the smoother the resulting gradient used in updating the weight. Dividing the sum by the batch size and taking the average gradient has the effect of: The magnitude of the weight does not grow out of proportion. Adding L2 regularization to the weight update penalizes large weight values. This often leads to improved generalization performance. Taking the average, especially if the gradients happen to point in the same direction, keep the weights from getting too large. The magnitude of the gradient is independent of the batch size. This allows comparison of weights from other experiments using different batch sizes. Countering the effect of the batch size with the learning rate can be numerically equivalent but you end up with a learning rate that is implementation specific. It makes it difficult to communicate your results and experimental setup if people cannot relate to the scale of parameters you're using and they'll have trouble reproducing your experiment. Averaging enables clearer comparability and keeping gradient magnitudes independent of batch size. Choosing a batch size is sometimes constrained by the computational resources you have and you want to mitigate the effect of this when evaluating your model.
