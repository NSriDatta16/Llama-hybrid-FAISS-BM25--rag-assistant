[site]: crossvalidated
[post_id]: 479033
[parent_id]: 
[tags]: 
In Logistic Regression models, should classification cutoff always be approximately equal to the prior $p = P(Y = 1)$?

Mathematically speaking, when using a logistic regression model for binary classification, the output of the model $\hat{y}_i$ for any instance $x_i$ not only can be interpreted as, but is defined as the probability of that instance belonging to the positive class ( see this answer ). $$\hat{y}_i = P(y_i = 1 | x_i)$$ If $\hat{y}_i , a classification threshold, we assign $x_i$ 's class to 0. Otherwise, 1. There are many applications in which we set $t$ to a value $t^*$ that optimizes a certain metric. For example, when having false positives is much more costly than having false negatives (as in automated trading systems), we may want to optimize recall, therefore choosing an arbitrarily high $t^*$ . Question: knowing the training set's prior probability $p_{\text{prior}} = P(Y = 1)$ ; accounting for a difference $d$ in training / production sets variance; In theory, for probabilistic classification purposes , shouldn't we always assign $t \gets (p_{\text{prior}} \pm d)$ , leaving $t^*$ as a purely decision making threshold? Seems like assigning, for example, $t^* \gets 0.95$ because it bumps recall to a comfortable level has more to do with playing safe and has nothing to do with classifying an event based on probability.
