[site]: crossvalidated
[post_id]: 385583
[parent_id]: 
[tags]: 
Trouble replicating sklearn logistic regression outputs

I am trying to create my own logistic regression classifier using scipy.optimize but I am having trouble getting close to the output of sklearn's built in logistic regression function. I am testing it on a binary portion of the iris dataset shown below: from scipy import optimize from sklearn import datasets from sklearn import linear_model from sklearn.preprocessing import StandardScaler import numpy as np def costFunc(thetas, X, Y, penalty): # Log loss cost function with regularization (except for intercept term) sigmoid = 1/(1+np.exp(-(X @ thetas))) cost = -1/len(X)*(Y @ np.log(sigmoid+1e-15) \ +(1-Y) @ np.log(1-sigmoid+1e-15)) \ +sum(penalty/(2*len(X))*thetas[1:]**2) # 1e-15 added to log to prevent divide by 0 errors return cost iris = datasets.load_iris() # Load example dataset thetas = np.zeros(2) # initial coefficients penalty = 0 x1 = iris.data[:100, 0].reshape(-1, 1) x2 = iris.data[:100, 1].reshape(-1, 1) scalerx = StandardScaler().fit(x1) # fit the standard scaler to the training data scalery = StandardScaler().fit(x2) # fit the standard scaler to the training data X = np.hstack([scalerx.transform(x1), scalery.transform(x2)]) Y = iris.target[:100] # Minimize the cost function coefficients0 = optimize.minimize(costFunc, thetas, args=(X, Y, penalty), method='L-BFGS-B') The coefficient output of this is [127.44071673, -100.51413182] while the following sklearn code logreg = linear_model.LogisticRegression(C=1e15, solver='lbfgs') # regularization set to 0 logreg.fit(X, Y) coefficients1 = logreg.coef_ has an output of [46.14677281, -28.27345264]. I would like to attribute this to differences in the solver, or perhaps the initial coefficients chosen but I'm not certain. EDIT: I forgot to add in the intercept term into the code, once added in x0 = np.ones((len(x1), 1)) X = np.hstack([x0, scalerx.transform(x1), scalery.transform(x2)]) the output becomes [7.59366865, 35.25397633, -21.54576954] for my code vs the [4.8815577 , 42.64049218, -26.29487637] for sklearn which is much closer but still somewhat different.
