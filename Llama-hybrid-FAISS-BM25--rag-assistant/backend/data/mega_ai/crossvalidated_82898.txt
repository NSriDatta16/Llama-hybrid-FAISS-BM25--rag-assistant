[site]: crossvalidated
[post_id]: 82898
[parent_id]: 59715
[tags]: 
Basically, I think you ask intuitively how sample size affects machine learning techniques. So, the real factor that affects the required sample sizes is dimensionality of the space that data live in, and its sparseness. I will give you two examples, because I find it hard to summarise everything in one... Let's say you have some dense data and you try to fit a model using some regression. If the data follow a polynomial of degree $n$ then you need more that $n$ data so your algorithm can find the correct curve. Otherwise, it will make an over-simplistic model, different than reality. Of course in reality there will be noise, so you need even more data to make a better model. Let's say you have some sparse data, i.e., most dimensions are zeros. Such an example is text, like tweets or SMS (forget books for now), where the frequency of each word is a dimension and of course documents don't have the majority of the words in the dictionary (sparse space). You try to classify tweets based on their topic. Algorithms, like kNN, SVMs etc, work on similarities between samples, e.g. 1-NN will find the tweet in the training set closest to the one that you try to classify and it will assign the corresponding label. However, because of the sparseness... guess what... most similarities are zero! Simply because documents don't share enough words. To be able to make predictions you need enough data so that something in your training set resembles the unknown documents you try to classify. Of course since it is a continuous space you can never fill all the gaps between samples... but the more data you put in, the higher the chance that the unknown sample will find something similar in the training set.
