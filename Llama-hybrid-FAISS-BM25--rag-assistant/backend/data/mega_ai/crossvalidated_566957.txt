[site]: crossvalidated
[post_id]: 566957
[parent_id]: 
[tags]: 
Are there any theoretical guarantees about the log-likelihood's inverse Hessian when the observations are not i.i.d.?

Let $X=[X_1...X_n]$ be some random variables in which $X_i$ are not independent. For example, you may envision the observations came from some stochastic process. Let $\ell(\theta; X)$ be the log-likelihood, whose scores are $s=\nabla\ell$ . It is easy to show that even if $X_i$ are not independent, $\mathbb{E}s=0$ still holds, and the varance-covariance matrix of $s$ still satisfies \begin{equation} \mathbb{V}(s) = -\mathbb{E}\left(\textrm{J}s\right) = -\mathbb{E}(\textrm{H}\ell) \quad\quad\quad\quad(1) \end{equation} where $\textrm{J}$ and $\textrm{H}$ denotes the Jacobian and Hessian operator under some regularity conditions $^1$ . We call $\mathbb{V}(s)$ the Fisher's Information matrix . It is a standard result that if $X_i$ were in fact i.i.d. then the MLE $\hat{\theta}_n$ is asymptotically $\mathcal{N}(\theta,\mathbb{V}(s_n)^{-1})$ distributed. Furthermore, the observed negative Hessian asymptotically converges to $\mathbb{V}(s_n)$ for large $n$ if there were i.i.d., so we could invert it to estimate the variance of the MLE. But most text book does not discuss what will happen if we don't have i.i.d., say, if $X_i$ were a time series. So here are my (open) questions: Under what conditions can one use the observed negative Hessian to estimate the Fisher information if $X_i$ were not independent? Under what conditions will $\hat{\theta}_n$ be asymptotically $\mathcal{N}(\theta,\mathbb{V}(s_n)^{-1})$ distributed when we do not have independence? By "under what conditions" I mean, are there any conditions that are general enough to be applicable to a wide range of time series/stochastic process problems? $^1$ A main requirement is that the density or p.m.f. of $X$ is nice enough to allow the $\mathbb{E}$ and $\nabla_\theta$ operators to commute.
