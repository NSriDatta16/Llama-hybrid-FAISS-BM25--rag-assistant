[site]: datascience
[post_id]: 48528
[parent_id]: 48446
[tags]: 
Doc2Vec (and words vectors) need significant amount of data to learn useful vector representation. 50k sentences is not sufficient for this. To overcome this, you can feed word vectors as initial weights in Embedding Layer of network. For example, code from following question : How to implement LSTM using Doc2Vec vectors? model_doc2vec = Sequential() model_doc2vec.add(Embedding(voacabulary_dim, 100, input_length=longest_document, weights=[training_weights], trainable=False)) model_doc2vec.add(LSTM(units=10, dropout=0.25, recurrent_dropout=0.25, return_sequences=True)) model_doc2vec.add(Flatten()) model_doc2vec.add(Dense(3, activation='softmax')) model_doc2vec.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) Output of "Flatten" layer will be vector representation of a sentence / document. Article with example code .
