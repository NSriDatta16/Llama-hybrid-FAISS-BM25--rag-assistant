[site]: stackoverflow
[post_id]: 1418483
[parent_id]: 1418223
[tags]: 
In short. Sorting algorithm efficiency will vary on input data and task. sorting max speed, that can be archived is n*log(n) if data contains sorted sub data, max speed can be better then n*log(n) if data consists of duplicates, sorting can be done in near linear time most of sorting algorithms have their uses Most of quick sort variants have its average case also n*log(n), but thy are usually faster then other not heavily optimized algorithms. It is faster when it is not stable, but stable variants are only a fraction slower. Main problem is worst case. Best casual fix is Introsort. Most of merge sort variants have its best, average and worst case fixed to n*log(n). It is stable and relatively easy to scale up. BUT it needs a binary tree (or its emulation) with relative to the size of total items. Main problem is memory. Best casual fix is timsort. Sorting algorithms vary also by size of input. I can make a newbie claim, that over 10T size data input, there is no match for merge sort variants.
