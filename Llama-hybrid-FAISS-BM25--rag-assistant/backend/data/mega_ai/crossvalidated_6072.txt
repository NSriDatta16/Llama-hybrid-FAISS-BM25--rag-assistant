[site]: crossvalidated
[post_id]: 6072
[parent_id]: 6067
[tags]: 
The problem is not that the classes are imbalanced per se, it is that there may not be sufficient patterns belonging to the minority class to adequately represent its distribution. This means that the problem can arise for any classifier (even if you have a synthetic problem and you know you have the true model), not just logistic regression. The good thing is that as more data become available, the "class imbalance" problem usually goes away. Having said which, 4:1 is not all that imbalanced. If you use a balanced dataset, the important thing is to remember that the output of the model is now an estimate of the a-posteriori probability, assuming the classes are equally common, and so you may end up biasing the model too far. I would weight the patterns belonging to each class differently and choose the weights by minimising the cross-entropy on a test set with the correct operational class frequencies. Alternatively (see the comments) it might be better to weight the positive and negative classes so they contribute equally to the training criterion (so there isn't a class imbalance problem in the estimation of the model parameters), but afterwards to rescale the posterior probabilities estimated by the classifier in order to compensate for the difference in the (effective) training set class frequencies and those in operational conditions (see this answer to a related question)
