[site]: crossvalidated
[post_id]: 586479
[parent_id]: 573524
[tags]: 
The existing answers provide useful information and arguments, but I disagree with them on what I see as the core question: is it useful to do a post hoc power analysis? I would argue that it IS useful - if you are set on using a Null Hypothesis Statistical Testing framework. It is absolutely crucial that this analysis is not done with the effect size estimated from the study itself. Instead, I would use either (a) the best estimate based on previous studies that have examined this question, or if this isn't possible, (b) the smallest effect size that would be interesting. What would such an analysis achieve? It would indicate whether the study was appropriately powered to detect a meaningful effect size. If your study was unlikely to detect the expected effect size, it doesn't mean your finding was wrong - but it is reasonable to be sceptical of the robustness of the analysis and inferences. Why is it important to not use the effect size estimated in the same study? Because as @dipetkov's answer shows, significant effect sizes are inflated in underpowered studies (and are quite likely to be in the opposite direction of the true effect!). Plugging an inflated estimate into a power calculation would be circular and overestimate the study's power. That said, I agree with the answers pointing you towards thinking of this in a Bayesian framework. This avoids some of the messiness arising as a result of NHST, which can sometimes struggle for coherence as a result of the "p In support of these claims, I'll point you to the same Andrew Gelman letter that @dipetkov's answer does, and also to his blog post about it that makes the same point less formally: https://statmodeling.stat.columbia.edu/2018/09/24/dont-calculate-post-hoc-power-using-observed-estimate-effect-size/ Gelman, A. (2019). Donâ€™t calculate post-hoc power using observed estimate of effect size. Annals of Surgery , 269(1), e9-e10.
