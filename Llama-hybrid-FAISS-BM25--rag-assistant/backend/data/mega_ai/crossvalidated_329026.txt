[site]: crossvalidated
[post_id]: 329026
[parent_id]: 
[tags]: 
Why is every local minimum of a neural network with one layer and $H$ hidden units in a family of size $H!2^H$?

Every local minimum of a feedforward neural network with one layer and $H$ hidden units is in a family of size $H!2^H$ (MacKay 1992) (link to paper) I find it very intuitive that there are several equivalent minima. For example, by interchanging the order of each hidden unit, the output is unchanged, so from here we get a factor $H!$. I don't understand where the factor $2^H$ comes from.
