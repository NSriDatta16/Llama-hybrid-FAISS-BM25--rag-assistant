[site]: crossvalidated
[post_id]: 156912
[parent_id]: 71946
[tags]: 
You can overfit with any method, even if you fit the whole population (if the population is finite). There are two general solutions to the problem: penalized maximum likelihood estimation (ridge regression, elastic net, lasso, etc.) and the use of informative priors with a Bayesian model. When $Y$ has limited information (e.g. is binary or is categorical but unordered), overfitting is more severe, because whenever you have low information it is like having a smaller sample size. For example, a sample of size 100 from a continuous $Y$ may have the same information as a sample of size 250 from a binary $Y$ , for the purposes of statistical power, precision, and overfitting. Binary $Y$ assumes an all-or-nothing phenomenon and has 1 bit of information. Many continuous variables have at least 5 bits of information.
