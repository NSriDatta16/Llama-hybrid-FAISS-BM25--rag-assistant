[site]: stackoverflow
[post_id]: 4689070
[parent_id]: 4488023
[tags]: 
There are many useful bots besides search engine bots and there are a growing number of search engines. In any case, the bots you want to block are probably using incorrect user-agent strings and ignoring your robots.txt files so how are you going to stop them? You can block some at the IP level once you detect them but for others it's hard. The user agent string has nothing to do with crawl rate. Millions of browser users are all using the same user agent string. Web sites throttle access based on your IP address. If you want to crawl their site faster you'll need more agents, but really, you shouldn't be doing that - your crawler should be polite and should be crawling each individual site slowly whilst making progress on many other sites. Crawler should be polite per-domain. A single IP may server many different servers but that's no sweat for the router that's passing packets to and fro. Each individual server will likely limit your ability to maintain multiple connections and how much bandwidth you can consume. There's also the one-web-site-served-by-many-IP addresses scenario (e.g. round robin DNS or something smarter): sometimes bandwidth and connection limits on sites like these will happen at the router-level, so once again, be polite per domain.
