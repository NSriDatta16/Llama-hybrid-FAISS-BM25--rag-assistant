[site]: crossvalidated
[post_id]: 79717
[parent_id]: 79712
[tags]: 
This is a good question. Strictly speaking, using a mixed model does not make you Bayesian. Imagine estimating each random effect separately (treating it as a fixed effect) and then looking at the resulting distribution. This is "dirty," but conceptually you have a probability distribution over the random effects based on a relative frequency concept. But if, as a frequentist, you fit you model using full maximum likelihood and then wish to "estimate" the random effects, you've got a little complication. These quantities aren't fixed like your typical regression parameters, so a better word than "estimation" would probably be "prediction." If you want to predict a random effect for a given subject, you're going to want to use that subject's data. You'll need to resort to Bayes' rule, or at least the notion that $$f(\beta_i | \mathbf{y}_i) \propto f(\mathbf{y}_i | \beta_i) g(\beta_i).$$ Here the random effects distribution $g()$ works essentially like a prior. And I think by this point, many people would call this "empirical Bayes." To be a true Bayesian, you would not only need to specify a distribution for your random effects, but distributions (priors) for each parameter that defines that distribution, as well distributions for all fixed effects parameters and the model epsilon. It's pretty intense!
