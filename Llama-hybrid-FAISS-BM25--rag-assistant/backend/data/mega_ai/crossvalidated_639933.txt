[site]: crossvalidated
[post_id]: 639933
[parent_id]: 639873
[tags]: 
It's perfectly fine to pool the individual predictions across all surrogate models. the standard method is to evaluate the error on each test fold and take the arithmetic average of the statistic I don't think so. Some software implementations do this by default, others by default pool the predictions from all surrogate models. Key assumptions of cross validation are that the surrogate models are equivalent (sufficiently similar) to the model built on the whole data set, so that they can actually be used as a surrogate for estimating generalization error. A second, weaker assumption recognizes that frequently the surrogate models have on average somewhat worse performance since they are built on fewer training cases (explaining a small pessimistic bias of cross validation estimates of generalization error). Since the cases are anyways assumed to come from the same distribution, this allows to pool the cross validation results across the surrogate models. Note that this approach yields results that are rather independent on the choice of $k$ , which is IMHO a preferrable property since $k$ is in no way related to the predictive performance of the model built on the whole data set. For some figures of merit, the arithmetic mean is a "natural" way of averaging since it doesn't matter how you "pre-pool" your individual predictions, the result will be the same for equal fold sizes. That is e.g. the case for the mean squared error MSE. For other figures of merit, e.g. the root mean squared error RMSE, this is not true and I'd advise some careful thought which way of pooling the per-fold results is then appropriate. The same holds for your NSE calculations. I'd advise against leave-one-out cross validation in general since it leads to collinearity between surrogate model and tested case and thus does not allow to distinguish between model instability and case-to-case variance in the prediction error. Since the fold sizes in k-fold cross validation should differ by not more than 1 case, I'd expect the difference between both ways of averaging usually to be small compared to the total uncertainty (due to model training instability and due to case-to-case variance) on the estimates. For the particular case of the NSE, the variance in the denominator asks for sufficiently large folds if per-fold NSEs are to be meaningful.
