[site]: crossvalidated
[post_id]: 181488
[parent_id]: 
[tags]: 
Creating a Markov Decision Process

So I have an idea for an MDP for my Machine Learning class, but I'm just wanting to make sure I take the right approach for setting it up. The Key Underlying question here is How do I approach a problem in such a way that I can describe it in a MDP fashion in a logical/consistent manner (I am hoping that simply "filling out" the 4 things below, it'll be magically correct, but I suspect not). I was looking at this outstanding post: Real-life examples of Markov Decision Processes Copying the comments about the absolute necessary elements: States: these can refer to for example grid maps in robotics, or for example door open and door closed. Actions: a fixed set of actions, such as for example going north, south, east, etc for a robot, or opening and closing a door. Transition probabilities: the probability of going from one state to another given an action. For example what is the probability of an open door if the action is open. In a perfect world the later could be 1.0, but if it is a robot, it could have failed in handling the door knob correctly. Another example in the case of a moving robot, would be the action north, which in most cases would bring it in the grid cell north of it, but in some cases could have moved too much and reached the next cell for example. Rewards: these are used to guide the planning. In the case of the grid example we might want to go to a certain cell, and the reward will be higher if we get closer. In the case of the door example an open door might give a high reward. I'm not sure if this would work as an MDP, but I was thinking about using DVR scheduling as a state space. Assuming 2 tuners (so I can schedule and record 2 things at once) My thought was that I could try to maximize the number of minutes recorded, so if 2 shows are set to record at the same time, but one is longer, then pick the second one, alternately if one is longer and starts just after another, pick the longer one, and skip recording the first. (No partial records), there are a lot of considerations For States is it only States of the actual Tuners that I need to identify or states of the scheduling algorithm? Should I be drawing a State Machine Diagram and describing what to do if a new request comes in and is processed FOR DVRX? States {Not Recording, Recording DVR1, Recording DVR2, Recording DVR1 and DVR2} --> How do I account for pre-planning states, so assuming the window from 1-2pm is scheduled by 2 dvr's and a new request comes in, how do I factor that into a state? Actions {Start Recording DVR1, Start Recording DVR2, Stop Recording DVR1, Stop Recording DVR2} --> I think these make sense, agreed? Transition Probabilities {1.0} --> I think these are high, I don't expect the DVR to continue recording unless there is a bug, so I suppose I could put something like .99 for each of the transitions? Are there as many probabilities in here as there are actions, or is it the probability for ANY of the actions? Rewards {length of recording request in minutes} --> reward is the length of the recording, so if two recording requests came in (for DVR1) one is 30 minutes, the other 60 minutes, then the reward if picking "optimal" is 60 (minutes). I'm not sure how the input is factored into the above, so would we consider that a state change? (Incoming 30 minute show?), Any ideas how to account for time? Lets assume this is a 24 hour window. Does that even make sense, is this the right approach? I took a stab at making a state machine, should I have more things called out explicitly or is this sufficient detail for an MDP? (I assume Blue Bubbles are States, and would the lines between be considered "actions"?)
