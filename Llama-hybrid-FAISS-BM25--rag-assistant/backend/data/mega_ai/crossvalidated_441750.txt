[site]: crossvalidated
[post_id]: 441750
[parent_id]: 
[tags]: 
Why can't this autoencoder reach zero loss?

I'm building an autoencoder and was wondering why the loss didn't converge to zero after 500 iterations. So I created this "illustrative" autoencoder with encoding dimension equals to the input dimension. To make sure that there was nothing wrong with the data, I created a random array sample of shape (30000, 100) and fed it as input and output (x = y). The NN is just supposed to learn to keep the inputs as they are. So why doesn't it reach zero loss? x_rand = numpy.random.rand(30000, 100) # this is the size of our encoded representations encoding_dim = 100 inputs = Input(shape=x_rand.shape[1:]) encoded = Dense(100, activation='relu')(inputs) encoded = Dense(100, activation='relu')(encoded) encoded = Dense(encoding_dim, activation='relu')(encoded) decoded = Dense(100, activation='relu')(encoded) decoded = Dense(100, activation='relu')(decoded) decoded = Dense(x_rand.shape[-1], activation='sigmoid')(decoded) # this model maps an input to its reconstruction autoencoder = Model(inputs, decoded) autoencoder.compile(optimizer='adam', loss='binary_crossentropy') history = autoencoder.fit(x_rand, x_rand, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=2)
