[site]: crossvalidated
[post_id]: 623866
[parent_id]: 623860
[tags]: 
Your intuition is on the right track. The usage of kernels is what makes SVMs so nice/convenient, since (very informally) it gives us the ability to build a classifier in some higher-dimensional space implicitly without having to operate in that higher-dimensional space, so I can understand why you'd have this question. The short answer is, theoretically, sure, you could recover the coefficients defining your separating hyperplane if, given your feature map $\phi: \mathbb{R}^n \to \mathbb{R}^p$ , you know the explicit form of $\phi$ . As an illustrative example, consider the polynomial kernel of degree 2, i.e., $K(x^{(i)}, x^{(j)}) = (x^{(i)\top} x^{(j)})^2$ for some $x^{(i)}, x^{(j)} \in \mathbb{R}^n$ . Some algebra shows that $$(x^{(i)\top} x^{(j)})^2 = \sum_{k=1}^n \sum_{\ell=1}^n (x^{(i)}_kx^{(i)}_\ell)(x^{(j)}_k x^{(j)}_\ell),$$ which is an inner product between two feature maps of $x^{(i)}, x^{(j)}$ . That is, $K(x^{(i)}, x^{(j)})$ can be written in the form $\phi(x^{(i)})^\top \phi(x^{(j)})$ , where $d = n^2$ for this case. Further examples of such kernels can be found here , as well as criterion for what constitutes a "valid" kernel. In practice, I'm not completely sure what the use of seeing the hyperplane coefficients in that higher-dimensional space is. But if you really, really want the coefficients for your hyperplane in that higher dimensional space, then they can be computed in terms of the coefficients of the dual optimization problem. That is; for coefficients $w, x^{(i)} \in \mathbb{R}^n$ , class labels $y^{(i)} \in \{-1, +1\}$ , and dual variables $\alpha^{(i)} \in \mathbb{R}^+$ , and $i \in \{1, \dots, M\}$ : $$w = \sum_{i=1}^M \alpha^{(i)} y^{(i)} \phi(x^{(i)}) $$ The derivation follows from taking the dual of the primal SVM objective, constructing the Lagrangian, and minimizing in terms of the primal variables (i.e., the hyperplane coefficients) for some fixed $\alpha^{(i)}$ . However, some feature maps are such that $p = \infty$ (i.e., your kernel projects features into an "infinite dimensional" space, which is called a Hilbert space ), such as the RBF/Gaussian kernel, which makes it a bit hard to compute the above. To see how this is possible, consider the Gaussian kernel $K(x^{(i)}, x^{(j)}) \triangleq \exp(-\frac{1}{2}\lVert x^{(i)} - x^{(j)}\rVert_2^2).$ We can expand the squared norm term and, recalling that $\exp(z) = \sum_{k=0}^\infty z^k/k!$ , we end up with something like $$\sum_{k=0}^\infty \frac{(x^{(i)\top} x^{(j)})^k}{k!} \exp(-\lVert x^{(i)} \rVert_2^2 / 2) \exp(-\lVert x^{(j)} \rVert_2^2 / 2).$$ Using the multinomial theorem to expand the $(x^{(i)\top} x^{(j)})^k$ term, you can verify that we end up with a valid inner product, giving us a kernel. Alternately, you can analyze each $k$ separately, and, noticing that each term of the infinite summation for fixed $k$ is a rescaled polynomial kernel of degree $k$ , interpret the RBF kernel as an infinite sum of polynomial kernels. Thus, we can't really recover the true hyperplane parameters. If you haven't seen this before, it might be surprising that you can (informally) implicitly work in an infinite-dimensional feature space.
