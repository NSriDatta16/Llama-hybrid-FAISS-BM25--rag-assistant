[site]: crossvalidated
[post_id]: 87464
[parent_id]: 87447
[tags]: 
Let me first point out that you appear to have a common misunderstanding about the meaning of p-values . In conventional ( frequentist ) statistical analysis, the p-value is the probability of getting a sample statistic (say a sample mean) as far or further from the proposed null value as yours, if the null value is the true value. Importantly, there is no such thing as (e.g.) "bananas cure cancer with probability at least 99.99%". The fact that a p-value might be $ What is the meaning of p values and t values in statistical tests? That having been said, it is possible to assert a (subjective) probability associated with the null hypothesis within the Bayesian framework . Bayes' rule is: $$ Pr(H_0|D) = \frac{Pr(D|H_0)}{Pr(D)}Pr(H_0) $$ In words, the probability that the null hypothesis is true that you should believe after having seen some data is equal to the distinctiveness of the data with respect to the null hypothesis (indexed by the quotient on the RHS) multiplied by the probability that the null hypothesis is true that you believed before having seen the data in question. To make this easier, consider the following example 1 : MAMMOGRAPHY A reporter for a women's monthly magazine would like to write an article about breast cancer. As a part of her research, she focuses on mammography as an indicator of breast cancer. She wonders what it really means if a woman tests positive for breast cancer during her routine mammography examination. She has the following data: The probability that a woman who undergoes a mammography will have breast cancer is 1%. If a woman undergoing a mammography has breast cancer, the probability that she will test positive is 80%. If a woman undergoing a mammography does not have breast cancer, the probability that she will test positive is 10%. What is the probability that a woman who has undergone a mammography actually has breast cancer, if she tests positive? How can we figure out that probability? We must revise the a priori probability that a woman who undergoes a mammography has breast cancer, p(cancer) which according to the text is 1% or p=.01, in light of the new information that the test was positive. That is, we are looking for the conditional probability of p(cancer|positive). The probability of a positive result given breast cancer, p(positive|cancer), is 80% or p=.8, and the probability of a positive result given no breast cancer, p(positive|no cancer), is 10% or p=.1. Thus, we have: $$ Pr({\rm cancer|positive}) = \frac{0.80}{\underbrace{0.80\!\times\! 0.01}_{Pr(D)\text{ w/ cancer}}\;+\;\underbrace{0.10\!\times\! 0.99}_{\Pr(D)\text{ w/o cancer}}} 0.01 = 0.075 $$ (The denominator of the fraction in Bayes' rule is often hard for people to understand. In this case, it is possible to enumerate the possible probabilities of the data, and $Pr(D)$ is simply the sum of all the individual enumerated probabilities. For greater clarity, I annotated them here. Often, the set of possible probabilities is much harder to determine. In practice, people often ignore the denominator and replace the equals sign with $\propto$, 'proportional to'.) Now in this example, the cancer rate is known beforehand. To make this example more like your new research finding example, let's imagine that no one knows exactly what the cancer rate is, but two different doctors believe the cancer rate is 1%, and 5% respectively. If we use the latter value in the equation above, we get: $$ Pr({\rm cancer|positive}) = \frac{0.80}{0.80\!\times\! 0.05\;+\;0.10\!\times\! 0.95} 0.05 = 0.296 $$ The probability is now 29.6%, which is very different from the 7.5% above. So who is right? We don't really know, but the important part is that both doctors are rational in believing their (very different) probabilities that their patient has breast cancer. To put this a different way, what is rational isn't the probability that each believes, but rather the manner in which they change their belief in light of new evidence. Since both doctors changed their belief using a correct application of Bayes' rule, both are rational, even though they came to different conclusions. The reason they didn't end up with the same probability is because they didn't believe in the same probability beforehand; this is what @AlecosPapadopoulos meant by 'they do not have "same information"'. 1. This example is copied from: Sedlmeier, Improving Statistical Reasoning , pp. 8-9.
