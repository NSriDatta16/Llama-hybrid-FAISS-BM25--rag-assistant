[site]: datascience
[post_id]: 24347
[parent_id]: 14984
[tags]: 
The paper cited in the question "FaceNet: A Unified Embedding for Face Recognition and Clustering" is available at https://arxiv.org/pdf/1503.03832.pdf . Page no 5 of this paper lists the reference [16] for the Inception module architecture. The original paper introducing and describing this Inception architecture is - "Going Deeper With Convolutions", it is accessible at https://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf . An overview of inception modules is given in the diagram on page 4, its included here - The key idea for devising this architecture is to deploy multiple convolutions with multiple filters and pooling layers simultaneously in parallel within the same layer (inception layer). For example, in the architecture shown in this diagram (figure a) employs convolution with 1x1 filters as well as 3x3 and 5x5 filters and a max pooling layer. Figure b demonstrates how the use of 1x1 convolution filters can achieve dimensionality reduction (since no. of channels is reduced). The intention is to let the neural network learn the best weights when training the network and automatically select the more useful features. Additionally, it intends to reduce the no. of dimensions so that the no. of units and layers can be increased at later stages. The side-effect of this is to increase the computational cost for training this layer. To address this, a number of solutions have been suggested in the paper such as to deploy parallel computations for this architecture. As an illustration of these concepts, the table on page 5 shows an incarnation of this architecture termed as "GoogLeNet" which was used for the ILSVRC 2014 competition.
