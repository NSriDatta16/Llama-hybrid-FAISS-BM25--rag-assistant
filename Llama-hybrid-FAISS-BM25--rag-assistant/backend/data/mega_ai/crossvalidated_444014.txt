[site]: crossvalidated
[post_id]: 444014
[parent_id]: 4284
[tags]: 
The basic idea is that too simple a model will underfit (high bias) while too complex a model will overfit (high variance) and that bias and variance trade off as model complexity is varied. (Neal, 2019) However, while bias-variance tradeoff seems to hold for some simple algorithms like linear regression, or $k$ -NN, it's not that simple . I'll briefly summarize some of the points made in this blog entry , by Neal (2019), and Neal et al (2018). There's growing body of evidence that this is not generally true and in some machine learning algorithms we observe, so called, double descent phenomenon. There are some preliminary evidence that for random forests, gradient boosting algorithms, and neural networks this might not be the case. It was observed that wider networks (more neurons) generalize better. Moreover, as discussed by Belkin et al (2019), for overparametrized neural networks and random forests, the bias-variance curve hits certain threshold, where the model overfits, and then, as the number of parameters grows beyond the number of datapoints, the test error starts falling again with growing model complexity (see figure from the paper reproduced below). Nice example for this was given by Neal (2019), and Neal et al (2018), using simple, single layer, dense neural network, trained with stochastic gradient descent on the subset of 100 samples from MNIST. Nonetheless that the number of parameters starts exceeding the number of samples, we do not see tradeoff in terms of decrease of test set performance. Belkin et al (2019) give even more striking example using random forest. As discussed by Neal (2019), lack of bias-variance tradeoff for neural networks was even visible in the widely cited paper by Geman et al (1992) who did the first empirical study on this topic and popularized it. Moreover, when discussing bias-variance tradeoff, it is often shown how squared error can be decomposed into bias and variance, no matter that it does not directly apply to other error metrics, and the fact that you can decompose it does not prove anyhow that there is a tradeoff. All this shows that we do not yet have good understanding of how and why some of the modern machine learning algorithms work, and some of our commonly held intuitions may be misleading. Belkin, M., Hsub, D., Maa, S., & Mandala, S. (2019). Reconciling modern machine learning practice and the bias-variance trade-off. stat, 1050, 10. Neal, B. (2019). On the Bias-Variance Tradeoff: Textbooks Need an Update. arXiv preprint arXiv:1912.08286. Neal, B., Mittal, S., Baratin, A., Tantia, V., Scicluna, M., Lacoste-Julien, S., & Mitliagkas, I. (2018). A modern take on the bias-variance tradeoff in neural networks. arXiv preprint arXiv:1810.08591.
