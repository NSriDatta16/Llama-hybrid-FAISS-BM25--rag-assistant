[site]: crossvalidated
[post_id]: 580540
[parent_id]: 
[tags]: 
Doubt in rewriting of equation to approximate expectation w.r.t posterior

I'm reading Speagle's A conceptual introduction to Markov Chain Monte Carlo Methds to try to learn the contexts where MCMC sampling is needed (in this case the paper focuses on Bayesian inference). We have the goal of approximating the integral: \begin{equation} \mathbb{E}_{\mathcal{P}}[f(\Theta)]=\int f(\Theta)\mathcal{P}(\Theta)d\Theta \tag{1} \end{equation} where: $$ \mathcal{P}(\Theta)=\frac{\tilde{\mathcal{P}}(\Theta)}{\int \tilde{\mathcal{P}} \,d\Theta}. $$ with $\mathcal{P}(\Theta)$ being the posterior, $\tilde{\mathcal{P}}(\Theta)=\mathcal{L}(\Theta)\pi(\Theta)$ being the unnnormalized posterior, $\mathcal{L}(\Theta)$ the likelihood and $\pi(\Theta)$ the prior. We assume that we only know $\mathcal{P}(\Theta)$ up to multiplicative constant, $\propto \mathcal{P}(\Theta)$ . The author proposes to use numerical integration as a first approach, defining a discrete grid and approximating $(1)$ as: \begin{equation} \mathbb{E}_{\mathcal{P}}[f(\Theta)]= \frac{\int f(\Theta)\tilde{\mathcal{P}}(\Theta)d\Theta}{\int \tilde{\mathcal{P}}(\Theta)d\Theta}\approx\frac{\sum_{i=1}^n f(\Theta_i)\tilde{\mathcal{P}}(\Theta_i)\Delta\Theta_i}{\sum_{i=1}^n \tilde{\mathcal{P}}(\Theta_i)\Delta\Theta_i}\equiv \frac{\sum_{i=1}^n w_i f_i}{\sum_{i=1}^n w_i}, \tag{2} \end{equation} where $\Theta_i = (\Theta_{j+1}+\Theta_j)/2$ and $\Delta\Theta_i = \Theta_{j+1}-\Theta_j$ , for $j=1, \dots, n+1$ , and where: $$ f_i =f(\Theta_i), \qquad w_i = \tilde{\mathcal{P}}(\Theta_i)\Delta \Theta_i. $$ We thus can consider $(2)$ as a weighted mean. After pointing the flaws of numerical integration, the author introduces a Monte Carlo approach. In order to improve the approximation, we wante to change the grid to make the weights $w_i = \tilde{\mathcal{P}}(\Theta_i)\Delta \Theta_i$ as uniform as possible, so we take $\Delta \Theta_i \propto 1/\tilde{\mathcal{P}}(\Theta_i)$ . Taking $n\to \infty$ , we can see $\Delta\Theta$ as a function of $\Theta$ , $\Delta\Theta(\Theta)$ . Using this, we can now define the density of points $\mathcal{Q}(\Theta)$ as: $$ \mathcal{Q}(\Theta)\propto \frac{1}{\Delta\Theta(\Theta)}. $$ Quoting the author: Using $\mathcal{Q}(\Theta)$ , we can now rewrite our original expectation as: \begin{equation} \mathbb{E}_{\mathcal{P}}[f(\Theta)]=\frac{\int f(\Theta)\tilde{\mathcal{P}}(\Theta)d\Theta}{\int \tilde{\mathcal{P}}(\Theta)d\Theta} =\frac{\int f(\Theta)\frac{\tilde{\mathcal{P}}(\Theta)}{\mathcal{Q}(\Theta)}\mathcal{Q}(\Theta)d\Theta}{\int \frac{\tilde{\mathcal{P}}(\Theta)}{\mathcal{Q}(\Theta)}\mathcal{Q}(\Theta)d\Theta}=\frac{\mathbb{E}_{\mathcal{Q}} \big[f(\Theta)\tilde{\mathcal{P}}(\Theta)/\mathcal{Q}(\Theta) \big]}{\mathbb{E}_{\mathcal{Q}} \big[\tilde{\mathcal{P}}(\Theta)/\mathcal{Q}(\Theta) \big]} \tag{3} \end{equation} [...] Initially, we looked at trying to estimate $\mathbb{E}_{\mathcal{P}}[f(\Theta)]$ over a grid with n points. In the limit of infinite resolution, however, our grid becomes equivalent to some distribution $\mathcal{Q}(\Theta)$ . Using $\mathcal{Q}(\Theta)$ , we can then rewrite our original expression in terms of two expectations, ${\mathbb{E}_{\mathcal{Q}} \big[f(\Theta)\tilde{\mathcal{P}}(\Theta)/\mathcal{Q}(\Theta) \big]}$ and ${\mathbb{E}_{\mathcal{Q}} \big[\tilde{\mathcal{P}}(\Theta)/\mathcal{Q}(\Theta) \big]}$ , over $\mathcal{Q}(\Theta)$ instead of $\mathcal{P}(\Theta)$ . This helps us because we can in theory estimate these final expressions explicitly using a series of n randomly generated samples from $\mathcal{Q}(\Theta)$ . I have two questions. How exactly is the distribution $\mathcal{Q}$ derived and what does it represent? I can't get to see the reason for rewriting the expectation and then sampling from $\mathcal{Q}$ to approximate $(1)$ via the last term in $(3)$ . Can we not just sample from the distribution $\tilde{\mathcal{P}}$ (which I assume is readily available since it appears in $(3)$ anyway) to approximate $(1)$ using the second term in $(2)$ ?
