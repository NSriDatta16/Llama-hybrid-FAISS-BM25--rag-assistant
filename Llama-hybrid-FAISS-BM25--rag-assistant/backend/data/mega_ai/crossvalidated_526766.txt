[site]: crossvalidated
[post_id]: 526766
[parent_id]: 526762
[tags]: 
Extending my comment: yes, that’s what happens. The loss is either summed or averaged over the minibatch. You’d then compute the gradient of this reduced value with respect to the parameters. You perform one update per minibatch. The two extremes of this are true (“batch”) gradient descent, which uses the entire dataset for each update (i.e., minibatch size $N$ ); and a batch size of 1, which uses one training example for each update. The former has some nice convergence guarantees but can be demanding in terms of computer memory. For massive datasets, you also may get significant gains in your objective function without needing the whole dataset. Stochastic gradient descent is an approximation to batch gradient descent that behaves nicely. Even though an individual step might go in the wrong direction (in the parameter space), the average direction often tends toward a local optimum.
