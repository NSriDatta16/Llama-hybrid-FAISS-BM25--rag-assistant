[site]: datascience
[post_id]: 114876
[parent_id]: 
[tags]: 
Why is it an advantage "that Markov chains are never needed" to obtain gradients?

In the original GAN (Generative Adversarial Network) paper , Generative adversarial networks by I. Goodfellow, J. Pouget-Abadie, M. Mirza et. al. they state an advantage of the GAN is "that Markov chains are never needed, only backprop is used to obtain gradients, no inference is needed during learning" (Section 6 of paper). I don't understand why this is an advantage? If we look at this statement from the other way around, why would using Markov chains be a disadvantage?
