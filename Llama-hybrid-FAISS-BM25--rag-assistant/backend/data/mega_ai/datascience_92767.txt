[site]: datascience
[post_id]: 92767
[parent_id]: 12575
[tags]: 
To complement other answers: You can featurize both sentences and then look at cosine similarity between their feature representations. To featurize text, there are many methods you can use; from simple counting-based operators like TFIDF to word embeddings like word2vec or more complex language models like BERT. The TextWiser Library might come in handy if you want to experiment with several text featurization methods including their transformations for dimensionality reduction like SVD, LDA, UMAP etc. Here is a usage example: # Conceptually, TextWiser is composed of an Embedding, potentially with a pretrained model, # that can be chained into zero or more Transformations from textwiser import TextWiser, Embedding, Transformation, WordOptions, PoolOptions # Data documents = ["Some document", "More documents. Including multi-sentence documents."] # Model: TFIDF `min_df` parameter gets passed to sklearn automatically emb = TextWiser(Embedding.TfIdf(min_df=1)) # Model: TFIDF followed with an NMF + SVD emb = TextWiser(Embedding.TfIdf(min_df=1), [Transformation.NMF(n_components=30), Transformation.SVD(n_components=10)]) # Model: Word2Vec with no pretraining that learns from the input data emb = TextWiser(Embedding.Word(word_option=WordOptions.word2vec, pretrained=None), Transformation.Pool(pool_option=PoolOptions.min)) # Model: BERT with the pretrained bert-base-uncased embedding emb = TextWiser(Embedding.Word(word_option=WordOptions.bert), Transformation.Pool(pool_option=PoolOptions.first)) # Features vecs = emb.fit_transform(documents) You can easily switch between different Embedding and Transformation options and see how they impact your downstream tasks, in your case, the similarity between the sentences. Notice you can even chain the Transformations; e.g., NMF followed by SVD operation. Disclaimer: I am a member of the TextWiser team.
