[site]: crossvalidated
[post_id]: 539102
[parent_id]: 538855
[tags]: 
You could do almost anything to implement this -- you could set the predictions to 0, or 1, or any value for the unlabelled predictions, and the training would still work, because the autodiff engine of whatever neural network library you're using would see that those predictions are not a function of your paramters $\theta$ , and therefore the update wouldn't depend on them. In particular, setting the predicted values to the ground truth is convenient, because the resulting loss is "average cross-entropy (or other loss) of the labelled pixels", rather than "average cross-entropy of labelled pixels, plus some arbitrary constant". A common term for this is "masking". The loss of those pixels is "masked out", because they are set to 0 / ignored.
