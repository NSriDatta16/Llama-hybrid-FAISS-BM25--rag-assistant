[site]: crossvalidated
[post_id]: 546388
[parent_id]: 
[tags]: 
Making a row-wise convolutional layer in keras

I want to make a layer that is almost exactly the same as a Conv2D layer, but essentially has a kernel for each row of the image, so that each row of the output is generated by taking the dot product of the same row of the weights matrix with the same row of the input matrix, and then stepping one column to the right. Our put another way, it's like Conv2D, but once you've done the element-wise multiplication of the input with the filter, you only sum along the rows, not the column. (See image below) I can think of a couple of ways to do this, but I'm completely lost about what would be the best. And I've probably not even thought of the best. I could use tf.image.extract_patches to create the windows and then do an element wise multiplication with a stacked weights matrix, and then sum it. This seems straight forward but I think it will blow out memory. I could literally make 1000s of cropping layers and conv2D filters so the crop layers picks a row, and then a Conv2D layer runs along it. That sounds like it should work, but it doesn't seem very elegant. Any ideas appreciated. --EDIT-- I don't think the answer is conv1d, or if it is, I'm very confused. If I run x = tf.constant([[[0, 1],[0, 0],[1, 0], [0, 1]]], dtype='float32') y = tf.keras.layers.Conv1D(1, 1, activation="linear", input_shape=(3,2), use_bias=False) conv = y(x) print(x) print(y.weights[0]) print(conv) then I get tf.Tensor( [[[0. 1.] [0. 0.] [1. 0.] [0. 1.]]], shape=(1, 4, 2), dtype=float32) tf.Tensor( [[[ 0.43913543] [ 0. ] [-1.3466451 ] [ 0.43913543]]], shape=(1, 4, 1), dtype=float32) I have a weights vector that is being marched down the 1st dimension of the input (good), but the output is being dotted with the 2nd dimension of the input (bad). The expected behaviour is I get an output of [[[0, 0.4] [0, 0] [-1.3, 0] [0, 0.4]]] So with a 1D kernel, I expect just an elementwise multiplication. It is once the second dimension of the kernel is introduced that their should be any dot products.
