[site]: crossvalidated
[post_id]: 40882
[parent_id]: 40870
[tags]: 
To expand on a very good answer that Placidia gave: Unbiasedness is not necessarily the best possible property for an estimator. Shrinkage estimators applied in situations of multiple collinearity or with many possible regressors (lasso) are intentionally biased, and this is done to improve their other properties (easier interpretation of the results). Any Bayesian posterior mean or posterior mode estimator with an informative prior is biased; it does not mean that we want to dismiss this whole area of statistics. As far as other criteria of performance of a statistical estimator go, the mean squared error (MSE) is a popular criterion: by how far the estimator is off, on average, disregarding whether it is biased or not. The best estimator of the population $\sigma^2$ is then the one that has not $n-1$, not $n$, but $n+1$ in the denominator. So if your inferential target is the population variance, you might want to use the estimator that divides the sum of squared errors by $n+1$. The observation that the MLE $S^2$ does not appear to make much sense in regression context has been made earlier, of course, and corrections have been proposed to force it to use the "right" degrees of freedom. This is the idea of restricted maximum likelihood (REML), where the estimators are defined conditioning on the residual subspace that has the "right" dimension. Yet another useful property of the ML is transformation invariance. If $S^2$ is the MLE of $\sigma^2$, then automatically $S$ is the MLE of $\sigma$, and $\ln S$ is the estimator of $\ln(\sigma)$. This comes handy in software code: maximizing with respect to $S$ or $S^2$ is being complicated by the quantity being non-negative, while maximizing with respect to $\ln S$ does not involve any constraints. (You'd observe that by Jensen's inequality, unbiasedness of an estimator $$s^2 = \frac1{n-1} \sum (Y_i - \bar Y)^2$$ is easily destroyed by any transformation: $s$ is not an unbiased estimate of $\sigma$. In fact, the unbiased estimator of $\sigma$ is pretty difficult to construct, and I won't be too much ashamed to admit that I don't know one off the top of my head.)
