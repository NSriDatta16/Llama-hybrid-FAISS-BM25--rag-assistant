[site]: crossvalidated
[post_id]: 520799
[parent_id]: 270275
[tags]: 
I have a very straight forward answer to this from a machine learning perspective. So given $\theta$ is mean square consistent, a reformulation is (Bias - Variance tradeoff): \begin{equation} \mathbb{E}\left[(\theta-\mu)^2\right] \to 0 \Leftrightarrow Var\left[\theta\right] + Bias\left(\theta,\mu\right)^2 \to 0 \end{equation} as $n \to \infty$ . As both terms are nonnegative, it implies that $Var\left[\theta\right] \to 0$ and $Bias\left(\theta,\mu\right)^2 \to 0$ . Now, as mentioned above, with help of Chebyshev's inequality, convergence in probability is induced.
