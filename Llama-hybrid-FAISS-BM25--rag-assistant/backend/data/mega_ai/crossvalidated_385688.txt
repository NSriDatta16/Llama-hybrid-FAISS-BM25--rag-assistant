[site]: crossvalidated
[post_id]: 385688
[parent_id]: 
[tags]: 
all-relevant feature selection vs minimum optimum feature selection

There are many different ways to selection features in modeling process. One way is to first select all-relevant features (like Boruta algorithm). And then develop model upon those those selected features. Another way is minimum optimal feature selection methods. For example, recursive feature selection using random forest (or other algorithms). Build models on subsets of features and select the subset based on estimated model performance in cross-validation process. My question is in what situations/applications one has advantages over the other? Any guidance and advice? How we can choose one over the other? Thanks.
