[site]: datascience
[post_id]: 124100
[parent_id]: 124020
[tags]: 
This answer is based on Eduardo Munoz's blog "Attention is all you need: Discovering the Transformer paper" in Towards Data Science. To understand how transformer prediction works, the main thing to remember is that the entire input sequence is used, but the output is the next word or token. And the predict function is called repeatedly until a stopping condition is reached (i.e. the "End of Sequence" token). So, say you've used the model to predict $n$ tokens and now you want to predict the next token ( $n+1$ ). The input to the predict call is the input sequence, plus the sequence of $n$ output tokens that have been generated so far. These $n$ output tokens are used to generate the output embedding and positional encoding that are the input to the decoder. The transformer model uses a special token "SoS" (start of sequence) to indicate the start of the output sequence, so before the call to predict the first "real" output token, the output sequence is initialised to this "SoS" token. This works because of the the shifted outputs plus the masked attention layer used in the decoder. When the model is trained, the entire input and output sequences are used. However, the shifted outputs plus the masked attention layer ensure that only output information up to position $i-1$ is used to generate the predicted $i^{th}$ token.
