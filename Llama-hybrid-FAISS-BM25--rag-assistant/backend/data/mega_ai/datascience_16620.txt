[site]: datascience
[post_id]: 16620
[parent_id]: 
[tags]: 
Do convolutions "flatten images"?

I'm looking for a good explanation of how convolutions in deep learning work when applied to multi-channel images. For example, let's say I have a 100 x 100 pixel image with three channels, RGB. The input tensor would then have dimensions 100 x 100 x 3. If I apply a convolution with N filters and a stride of one, will the output dimension be: 100 x 100 x 3 x N ? or 100 x 100 x N ? In other words, does the convolution that is applied "flatten" the image, or is the convolution applied on a channel by channel basis?
