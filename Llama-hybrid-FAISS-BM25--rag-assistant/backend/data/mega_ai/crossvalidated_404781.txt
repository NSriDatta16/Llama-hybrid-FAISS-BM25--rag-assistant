[site]: crossvalidated
[post_id]: 404781
[parent_id]: 
[tags]: 
Second order approximation of the loss function (Deep learning book, 7.33)

In Goodfellow's (2016) book on deep learning, he talked about equivalence of early stopping to L2 regularisation ( https://www.deeplearningbook.org/contents/regularization.html page 247). Quadratic approximation of cost function $j$ is given by: $$\hat{J}(\theta)=J(w^*)+\frac{1}{2}(w-w^*)^TH(w-w^*)$$ where $H$ is the Hessian matrix (Eq. 7.33). Is this missing the middle term? Taylor expansion should be: $$f(w+\epsilon)=f(w)+f'(w)\cdot\epsilon+\frac{1}{2}f''(w)\cdot\epsilon^2$$
