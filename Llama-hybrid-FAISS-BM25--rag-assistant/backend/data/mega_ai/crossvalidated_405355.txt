[site]: crossvalidated
[post_id]: 405355
[parent_id]: 
[tags]: 
Does the Jensen-Shannon divergence maximise likelihood?

Minimising the KL divergence between your model distribution and the true data distribution is equivalent to maximising the (log-) likelihood. In machine learning, we often want to create a model with some parameter(s) $\theta$ that maximises the likelihood of some distribution. I have a couple of questions regarding how minimising other divergence measures optimise our model. In particular: Does the Jensen Shannon Divergence also maximise the likelihood? If not what does it maximise? Does the reverse KL divergence also maximise the likelihood? If not what does it maximise? Edit: As you can see from the figure below from this paper , the KL and JSD have different optimal solutions, so if minimising the KL is equivalent to optimising the likelihood, then the same cannot necessarily be the case for JSD.
