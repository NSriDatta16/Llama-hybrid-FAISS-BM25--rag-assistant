[site]: crossvalidated
[post_id]: 288159
[parent_id]: 277647
[tags]: 
I think it is how leave-one-out cross validation works. Let's assume we have ten data points {x1,x2,...,x10}. Every time we train the bandwidth without only one point which will be used for validation purpose. I agree with you that we do the training and aim to minimize the error, but we may face an over-fitting issue and that's the reason why we apply cross-validation. Actually, it is full sample average; otherwise, the second term in J(h)^ should be divided by n-1 instead of n. In the ten data points example, we will train ten times and each time we have 9 training points and 1 validation point. Here n=10.
