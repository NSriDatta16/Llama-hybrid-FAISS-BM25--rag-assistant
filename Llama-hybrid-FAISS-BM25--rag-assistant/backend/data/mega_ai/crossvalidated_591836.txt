[site]: crossvalidated
[post_id]: 591836
[parent_id]: 
[tags]: 
VAE mean and Standard Deviations are input dependent?

The original presentation of variational autoencoders, VAE assumes the mean $\mu$ and the sd $\sigma$ are functions of the input variable, say $x$ . I am studying " Learning Structured Output Representation using Deep Conditional Generative Models " by Sohn et al. and came across the lines The prior of the latent variables $z$ is modulated by the input $x$ in our formulation; however, the constraint can be easily relaxed to make the latent variables statistically independent of input variables, i.e., $p_\theta (z|x) = p_\theta (z)$ and the only justification to this statement is citation of " Semi-supervised Learning with Deep Generative Models " by Kingma et al., which I have not not yet found to be supportive of the claim. My questions: Could anyone justify or shed lights to this claim? Most VAE tutorials talk about the generative power of it using only the decoder and samples from the latent representation (aka, throw away the encoder), is this really the case since $\mu = \mu(x)$ and $\sigma = \sigma(x)$ ? An answer in How to generate new data given a trained VAE - sample from the learned latent space or from multivariate Gaussian? points to the fact that there is a misconception as people often take $\mu$ and $\sigma$ as constant learnable parameters. Any support, justification or reference to this statement?
