[site]: crossvalidated
[post_id]: 613448
[parent_id]: 
[tags]: 
Perplexity with log probabilities

I want to calculate the perplexity of my language model. To avoid underflow, I stored the log of the probabilities. Since the probabilities are between 0 and 1, the log of them is negative. So when I want to calculate the perplexity, when the number of words is even, the perplexity becomes an irrational number. On the other hand, according to its formula: $$PP(W) = (\frac{1}{p(w_1w_2\dots w_N)})^\frac{1}{N}$$ , since the sum of log probabilities are greater than 1, the perplexity becomes less than 1. But I also saw that perplexity is $2^{entropy}$ so it can't be less than one. Would you please help me with understanding these problems?
