[site]: crossvalidated
[post_id]: 460184
[parent_id]: 460169
[tags]: 
I think it will be reasonable to consider encoding township and ranges in a way other than simply treating them as integers. While it is safe to assume that RF are able to encapsulate non-linear relations, it will still be rather difficult to interpreter results from a integer encoding. In addition, spurious effects might be very prevalent. For example, if a relatively low-counts categorical level has an assigned numerical value that is close to the assigned numerical variable of high-counts categorical level, it will be probably pooled together with it even if they encode relatively different information; and that would be just due to the encoding, another (random) integer encoding might give something else. Similarly, as you have correctly deduced, because township and range have high cardinality (i.e. they have many distinct variables) as well as they have an inherit hierarchy, using them directly as factors quickly becomes unwieldy. The main problems are: 1. We might overfit out data when concerned about the influence of very unusual/rare factors and 2. with thousands of factor levels is difficult to estimate the overall importance of a factor as a whole. Bonus problem : (3.) We might not respect the hierarchy among two level (e.g. in the UK, where the post-codes (e.g. EC1Y 8LX ) have an outward code ( EC1Y ) and inward code ( 8LX ) part, it makes little sense to analyse inward post codes in absence of outward post codes). I would suggest looking into target encoding (Micci-Barreca (2001) A preprocessing scheme for high-cardinality categorical attributes in classification and prediction problems ) (or other target aware encodings like the James-Stein Encoder or the M-Estimator encoding). The basic idea is that a discrete factor variable is replace by a (regularised) average of the response variable. Here is a quick example: if we model a response variable y and we have three options of a variable Sex : Female / Male / Other , each of them represent 53%, 46% and 1% of our sample respectively. We will then create a new "numeric version" of the variable Sex where Female will be replaced with the mean of y for Female correspondents, $\mu_{y;Female}$ , Male will be replaced with the mean of y for Male correspondents $\mu_{y;Male}$ (or something a bit closer to the overall average of y , $\mu_y$ due to regularisation) and Other will be replaced with the mean of y for Other correspondents, $\mu_{y;Other}$ (or likely something even closer to $\mu_y$ , due to even stronger regularisation as the proportion is smaller). That and a bunch of other goodies can be found in the vtreat package that has a number of pretreatment step for data. They have a number of vignettes that touch upon a number of different variable pre-processing issues. Side-note: There are other encoding schema like Binary Encoding (i.e. turn everything into integers and then make $p$ distinct variable where they hold the $p$ digits 0/1 required to encode the integer) and Feature Hashing (Weinberger et al. 2009 Feature Hashing for Large Scale Multitask Learning ). I do not touch upon them here as variable importance and influence is even harder to interpreter in these use cases. They are mostly dimensionality reduction pre-processing steps.
