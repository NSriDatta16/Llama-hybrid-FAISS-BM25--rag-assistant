[site]: crossvalidated
[post_id]: 465364
[parent_id]: 465346
[tags]: 
The (complete information) likelihood in this model is $$\prod_i p(x_i|m_i,\theta)\tag{1}$$ which can be maximised or used in a Bayesian analysis, providing a prior $p(\theta)$ is chosen. (There is no "best way to estimate" a quantity, it all depends on the utility function for running the estimation.) In the event the $m_i$ 's are not observed , the (observed) likelihood becomes $$\prod_i \int p(x_i|m,\theta)\,p(m)\,\text{d}m\tag{2}$$ If there exists an unbiased estimator of $\theta$ in this setting, $T_n(X_1,\ldots,X_n|m_1,\ldots,m_n)$ then it satisfies the Cram√©r-Rao inequality $$\text{cov}(T_n(X_1,\ldots,X_n|m_1,\ldots,m_n)) \ge I_n^{-1}(\theta;m_1,\ldots,m_n)$$ where $I_n(\theta;m_1,\ldots,m_n)$ is the (complete) Fisher information matrix associated with (1): $$I_n(\theta;m_1,\ldots,m_n)=-\sum_{i=1}^n\mathbb{E}_\theta \left[ \dfrac{\partial ^2}{\partial \theta \, \partial \theta^\text{T}} \log p\left(X| m_i,{\theta}\right)\right]$$ Once again, if the $m_i$ 's are not observed , the observed Fisher information become $$I_n(\theta)=-n \mathbb{E}_\theta \left[ \dfrac{\partial ^2}{\partial \theta \, \partial \theta^\text{T}} \log p\left(X| {\theta}\right)\right]=-n \mathbb{E}_\theta \left[ \dfrac{\partial ^2}{\partial \theta \, \partial \theta^\text{T}} \log \int p\left(X| m,{\theta}\right)p(m)\,\text{d}m \right]$$
