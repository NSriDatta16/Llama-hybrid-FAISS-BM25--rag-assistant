[site]: datascience
[post_id]: 124804
[parent_id]: 
[tags]: 
How can I tell if my CNN tuning made a difference?

I'm working on a detection CNN, estimates pose for some classes of objects. I am able to compute a bunch of different metrics on performance, things like position error, rotation error, tracking performance etc etc. And each time I tweak data or training or architecture settings, I can output means, P5, P95 and a bunch of other stats. Now sometimes I'll change things and I'll get say 1mm better performance on position error. But I have no idea if this is due to random variation in my data and the training process, or if it's due to something I actually did. I basically do not know if my improvements are statistically significant. My instinct would be to re-train many times on different random seeds and then use standard tests for significance. But unfortunately my resources are very limited and I can't do this. Is there some other random resampling method that I can use when comparing one version of my network to another?
