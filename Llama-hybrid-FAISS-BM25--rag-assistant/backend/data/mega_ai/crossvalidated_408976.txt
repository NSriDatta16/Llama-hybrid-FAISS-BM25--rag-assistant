[site]: crossvalidated
[post_id]: 408976
[parent_id]: 
[tags]: 
What is metrics.roc_curve and metrics.auc measuring when I'm comparing binary data with probability estimates?

I was working on a challenge, and I was excited because the metric.auc for my predicted values compared to my test values was very high. This was for a binary selection process. However, when I looked at it, my predicted values outputted by logistic regression were actually probabilities, not binary values. So I rounded them, as the challenge requires binary predictions. When I rounded them, the auc score dropped drastically. My understanding of the auc score and roc curve is that it compares false positives/negatives etc., and I don't even know how it came up with an actual value for these probabilistic predictions. What was it computing before, and why was it so high?
