[site]: datascience
[post_id]: 42938
[parent_id]: 42785
[tags]: 
As you are talking about RNN, I'm going to change the parameters of your question a little bit. The multi sequence to one will be: $[x_1] , [y_1]$ $[x_1, x_2, y_1] , [y_2]$ ... $[x_1, x_2, ..., x_n, y_{(n-1)}], [y_n]$ as the previous state will always be used. This is typically a Markov process with a state containing a n input vector. It has known properties and that's what is used by RNN to be able to create a sequence one element by one element. On the case of one sequence to one sequence, there is no such state, and we don't have the concept of one word following the other. It's basically n values in the input and n value as the output. But there is no constraint about "one after the other" in such a pseudo sequence anymore, so the outputs could be scrambled and it would still be a sequence to sequence output. Now, for NLP, the fact that you don't have a state before time 0 is not a problem. You can discard the first n-1 elements even and keep the Markov property of the system. This is exactly what I'm doing in chapter 8 in Building Machine Learning with Python and it works well if you start retrieving words when the system is in a proper state.
