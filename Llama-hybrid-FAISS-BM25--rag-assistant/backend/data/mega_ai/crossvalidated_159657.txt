[site]: crossvalidated
[post_id]: 159657
[parent_id]: 
[tags]: 
Metrics for evaluating ranking algorithms

I'm interested in looking at several different metrics for ranking algorithms - there are a few listed on the Learning to Rank wikipedia page, including: • Mean average precision (MAP); • DCG and NDCG; • Precision@n, NDCG@n, where "@n" denotes that the metrics are evaluated only on top n documents; • Mean reciprocal rank; • Kendall's tau • Spearman's Rho • Expected reciprocal rank • Yandex's pfound but it isn't clear to me what are the advantages/disadvantages of each or when you may choose one over another (or what it would mean if one algorithm outperformed another on NDGC but was worse when evaluated with MAP). Is there anywhere I can go to learn more about these questions?
