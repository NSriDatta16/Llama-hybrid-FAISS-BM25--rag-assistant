[site]: crossvalidated
[post_id]: 632089
[parent_id]: 
[tags]: 
Pros and cons of different methods for comparing betas in regression

In my line of work, we often hypothesize that one continuous predictor will have a stronger relationship with some outcome than another closely related (i.e., collinear) continuous predictor. We fit a multiple regression $y = a + b_{1}X_{1} + b_{2}X_{2}$ and start by checking to see if $b_{1} > b_{2}$ . If it is, then yay, the relationship between those betas was in the predicted direction! Even better is when $b_{1}$ is statistically significant while $b_{2}$ is not, however we can't stop here lest we commit the "difference in significance = significantly different" fallacy. And in my field, you can't publish without p-values, meaning we need some sort of null hypothesis test that is consistent with the verbal hypothesis that "predictor 1 is more strongly related to y than predictor 2", beyond merely showing that $b_{1} > b_{2}$ and/or $b_{1}$ is "statistically significant" while $b_{2}$ is not. After scouring stack, I have identified at least four methods for doing this, all of which give me slightly different "stories" in my actual dataset. This leaves me with some paralysis about how to proceed given that I don't have a good sense of which method is generally considered to be the most persuasive. These are the methods I've found: A Wald test between the betas (as in this answer ). In other words, compare the two partialized betas. Pivot the dataset longer so that continuous predictors 1 and 2 are stacked into a single column C alongside a grouping column G, and fit $y = a + b_{1}C + b_{2}G + b_{3}(C \times G)$ , such as mentioned in this question . As far as I can tell, the interaction term in this model is equivalent to the difference between two un partialized betas from one univariate regression with predictor 1 as the predictor and another with predictor 2 as the predictor. Use a linear hypothesis test (such as from the car package in R and outlined in this answer ), which as I understand it does a model comparison of a model where $b_{1} = b_{2}$ vs the observed model. Use dominance analysis to determine whether/to what extent predictor 1 dominates predictor 2 (as nicely explained in this answer ). It seems like significance tests of difference in dominance (if that's what we would call it) are less common, but there is an R package that seems to give you bootstrapped standard errors if you need them. My questions are: 1. In your opinion, which of these methods conceptually most closely aligns with the verbal hypothesis that "predictor 1 relates more strongly to y than predictor 2"? 2. What exactly is the null hypothesis of each test, in a sentence? 3. In your opinion, which of these methods is most flexible to real-world issues one is likely to face, such as multicollinearity? 4. Is there another method not mentioned here that is worth mentioning? I would be open to a bayesian perspective as well. P.S. I'm not super proficient at the math, so I've tried to explain in words what I can't express mathematically. Verbal answers are preferred over heavily mathematical answers, but both are appreciated! Edit: For the sake of practicality, let's assume that a) both predictors are on the same scale and b) the sample size is large enough to reasonably address estimation issues (such as instability) caused by collinearity. The reason I mentioned the collinearity is that some methods may prefer to account for covariance between predictors.
