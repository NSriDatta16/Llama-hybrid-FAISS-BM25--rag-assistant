[site]: datascience
[post_id]: 69299
[parent_id]: 
[tags]: 
Constraining a neural net during training

I have simple "image-like" objects that contain non-linearly encoded information about real images. Each real image is zero except for two pixels, whose float values sum to unity. I created a simple neural net in pytorch and began training with 1000 "image-like" objects. It sort of works without employing any constraints (two float pixels summing to unity, rest zero), but not very well. Eventually the loss function and learning rate bottom out after about 200 epochs. So I thought I would try employing the constraints. I first tried doing this in the forward method of my neural net. I was not optimistic, but I tried it anyway because it was easy. This attempt didn't disappoint, it was truly awful. Clearly, I have to employ Lagrange multiplier constraints in the loss function. I've seen numerous web articles on how to create custom loss functions, but I have no idea how to implement such constraints in them. Any help would be greatly appreciated.
