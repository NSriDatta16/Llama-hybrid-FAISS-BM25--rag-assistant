[site]: datascience
[post_id]: 22739
[parent_id]: 21952
[tags]: 
I am not an expert but this is how I see things: use spark. Pyspark seems to be the flavor to use since it's python and that's the language that data science has settled on. Dataset sizes are expanding way faster than RAM is on laptops. What "they" don't tell you is that scikit learn, pandas, R, etc all require your dataset and all intermediate steps to fit inside RAM. If you have an 8gb dataset, and you pivot it, your intermediate step could easily be say 30gb. Even if it's temporary and your final cleaned dataset to be fed into. A neural network shrinks back down to say 6gb, too bad. All of your steps must fit inside RAM. In the real world, and even the limitation that datasets be limited to RAM sizes is quite limiting. This is what I wished I had known earlier. It's like saying here is a grocery store which is all yours (scikit learn has tons of features ) but you can only leave the store with a single shopping bag full of items). I would much rather have a grocery store with the standard 100 items and let me have a buffet on just those 100. Yes, there are some libraries here and there for scikit learn that read from files in chunks, and then you add the chunks together, etc. But you will quickly see that that is reinventing the wheel and distracts from doing data science. The fact remains that these frameworks were built for in RAM usage. It's not a bad thing at all per se, but quickly becomes limiting. What you want is to write the code once and not have it change whether the dataset is 1gb or 100gb. This is what pyspark is for. You can just learn their API and pyspark will handle chunking and distributing the workload such that it fits in the available RAM that you have. Of course if you are RAm limited computations take longer but this is ok. At least you will get an answer. The downside is that if you want the very much appreciated ability to not have to worry about "is this dataset too large for my ram" question then you need to stay in pyspark's API. If they don't have a feature that say scikit learn has, or keras has, then too bad. Write your own or work around it. Thankfully the API looks very complete. Where does it lack? Well for say neural networks you aren't going to have all kinds of special activation functions and customization that you can have with Keras. But in my experience and it really is very little, more data and better data beats the specific algorithm much of the time. I only mean to say that I would rather be able to process tons of data and use a vanilla classifier than be limited to in RAM dataset sizes and while being able to optimize 1000 hyperparameters. You also don't want to be switching between different aPIs all the time as a beginner. So if there is any chance that your dataset will be larger than RAM (I think that likelihood is a near certainty in the years ahead) I would most definitely learn pyspark. Even before scikit learn.
