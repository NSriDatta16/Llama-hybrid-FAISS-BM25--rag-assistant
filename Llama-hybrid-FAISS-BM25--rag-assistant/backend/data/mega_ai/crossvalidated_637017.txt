[site]: crossvalidated
[post_id]: 637017
[parent_id]: 521207
[tags]: 
Let's compare and contrast the two methods, VAE and normalizing flows. Similarly, both techniques optimize (by maximizing) the log-likelihood. However, it is not so apparent (from the image) how they construct the latent space $z$ that defines the joint posterior distribution $p(z,x)$ . The straightforward answer to your question is normalizing flows construct the latent space as a chain of series of simple, separate and independent mappings in the following form $x \: = \: f_{\phi} \: = \: f_{k} \cdot \ldots \cdot f_{2} \cdot f_{1} (z_{0}),$ where each $f_{\phi}$ is a differentiable and invertible flow layer function, also known as a bijection. The sequential and independent flow layers transform a random variable $z_{0}$ to a new random variable $x$ (during inference). By design, they leverage tractable density evaluation, which is in contrast to approximate inference of VAEs, therefore overcoming the deficiencies of learning a variational lower bound on the log-likelihood. The computation of the Jacobian log determinant for each basic bijection operator is analytically determinable using the change of variables formula: $P_X(x) \: = \: P_Z(z) \cdot \big\vert \: \det \: \big( \frac {dz} {dx} \: \big) \: \big\vert \: \: where \: \: x \: = \: f(z)$ This ultimately is the property that enables the final flow to produce expressive, flexible posteriors. It clarifies why the design, construction, and coupling of flow layers are core research problems and why restrictions on the types of neural network architectures are necessary (to impose invertibility and easy computation of the Jacobian of the determinant). This construction differs from VAEs, which typically model the latent space as a single Gaussian distribution. Accordingly, the composition of the latent space of the two generative methods also affects how sampling from the posterior is performed. For those curious to link the said techniques to more state-of-the-art generative algorithms, diffusion models can be transformed into continuous normalizing flows (CNFs) and interpreted as a specific form of a Markovian Hierarchical Variational Autoencoder. The following excerpts are taken from my book on variational inference. Learn more on the topic by visiting https://www.thevariationalbook.com/
