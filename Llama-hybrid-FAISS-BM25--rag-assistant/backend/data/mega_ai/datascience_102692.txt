[site]: datascience
[post_id]: 102692
[parent_id]: 102691
[tags]: 
Very generally speaking I don't see any major problems with your approach. You can make a few modifications though. For starters you might want to scale your data. You can use 0-1 scaling or -1,1, shouldn't matter much. Of course each column needs to be scaled separately. I am assuming there is no relation between your columns, if there is a specific structure, you might be better off using convolutions before fully connected layers: CNN are not only used for image but any type of data that has "features" to be extracted. Without knowing specifics of the columns, it is impossible to say anything though. I also suggest you include dropout layers in your model. You say you tried ReLU but you might want to check again after scaling and adding dropouts. And also give other variations of ReLU a chance: leakyReLU, ELU, SiLU etc.. And finally make sure your model does not have more trainable parameter compared to your data. Your model seems to be too large for your data to me. You can use model.summary() to get a count of your parameters.
