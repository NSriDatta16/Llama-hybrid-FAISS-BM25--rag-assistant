[site]: crossvalidated
[post_id]: 4109
[parent_id]: 4104
[tags]: 
Your first task is to find a reasonable model relating an outcome $Y$ to the sequence of workouts that preceded it. One might start by supposing that the outcome depends quite generally on a linear combination of time-weighted workout efforts $X$, but such a model would be unidentifiable (from having more parameters than data points). One popular simplification is to suppose that the "influence" of a workout at time $t$ on the outcome at time $s$ is a. proportional to the intensity of the workout, b. decays exponentially; that is, is reduced by a factor $\exp(\theta(t-s)))$ for some unknown decay rate $\theta$, and c. independently adds to the influences of all other workouts preceding time $t$. Of course we must be prepared to allow some deviation between the actual outcome and that predicted by the model; it is natural to model that deviation as a set of independent random variables of zero mean. This leads to a formal model which can serve as a useful point of departure for EDA. To write it down, let the times be $t_1 \lt t_2 \lt \ldots \lt t_n$ with corresponding workout intensities $x_1, x_2, \ldots, x_n$ and let the outcomes be measured at times $s_1 \lt s_2 \lt \ldots \lt s_m$ with values $y_1, \ldots, y_m$, respectively. The model is $$y_j =\alpha + \beta \exp(-\theta(s_j - t_{k})) \left( x_{k} + \exp(-\theta \Delta_{k,k-1}) x_{k-1} + \cdots + \exp(-\theta \Delta_{k, 1})x_1 \right) + \epsilon_j$$ where $\alpha$ and $\beta$ are coefficients in a linear relation, $k$ is the index of the most recent workout preceding time $s_j$, $\Delta_{i,j} = t_i - t_j$ is the time elapsed between the $i^\text{th}$ and $j^\text{th}$ workouts, and the $\epsilon_j$ are independent random variables with zero expectations. This can get messy when workouts and endpoint measurements are unevenly spaced. If to a good approximation the spacing between a workout and the next measurement is constant (say, a time difference of $s$) and--as an expository simplification--if each workout is followed by a measurement (so that $m = n$), then this model suggests some useful EDA procedures. As an abbreviation, let's write (somewhat loosely) $$f_k(x,t,\theta) = \left( x_{k} + \exp(-\theta \Delta_{k,k-1}) x_{k-1} + \cdots + \exp(-\theta \Delta_{k, 1})x_1 \right)$$ for the weighted sum of the workouts up to and including the $k^\text{th}$ one, whence $$y_k = \alpha + \gamma f_k(x,t,\theta) + \epsilon_k$$ where $\gamma = \beta \exp(-\theta s)$. Note that this formulation accommodates any irregular time sequence of workouts, so it's not grossly oversimplified. What you want to know is whether this makes sense: do the data at all behave like this? We're really asking about the possibility of a linear relationship between the $x$'s and the $y$'s. We need that to hold for at least one decay constant $\theta$ with a reasonable value. One way to check is to note there is a relatively simple relationship between successive terms $f_{k+1}$ and $f_k$; you let $f_k$ decay for additional time $t_{k+1} - t_k$ and add $x_{k+1}$ to it: $$f_{k+1}(x,t,\theta) = x_{k+1} + \exp(-\theta \Delta_{k+1,k}) f_k(x,t,\theta).$$ (This formula, by the way, provides an efficient way to compute all the $f_k$ by starting at $f_1 = x_1$ and continuing recursively for $k = 2, 3, \ldots, n$--a simple spreadsheet formula. It is a generalization of the weighted running averages used extensively in financial analysis.) Equivalently, we can isolate $x_k$ by subtracting the right hand term. This suggests exploring the relationship between the adjusted values $z_k = y_{k+1} - \exp(-\theta \Delta_{k+1,k})y_k$ and the workouts $x_k$, because $$z_k = (1 - \exp(-\theta \Delta_{k+1,k}))\alpha + \gamma x_k + (\epsilon_{k+1} - \exp(-\theta \Delta_{k+1,k}))\epsilon_k).$$ If the workouts are approximately regularly spaced, so that $\Delta_{k+1,k}$ is roughly constant, then for any fixed value of $\theta$ this expression is in the form $$z = \text{ constant + constant *} x + \text{ error}.$$ The error terms will be positively correlated (in pairs) but still unbiased. It is now clear how to check linearity: Pick a trial value for $\theta$, compute the $z$'s (which depend on it), make a scatterplot of the $z$'s versus the $x$'s, and look for linearity. Vary $\theta$ interactively to search for linear-looking scatterplots. If you can produce one, you already have a reasonable estimate of $\theta$. You can then estimate the other parameters ($\alpha$ and $\beta$) if you like. If you cannot produce a linear scatterplot, use standard EDA techniques to re-express the variables (the $x$'s and $y$'s) until you can. Look for a value of $\theta$ that minimizes the typical sizes of the residuals: that is a rough estimate of the decay rate. I don't expect this method to be highly stable: there is likely a wide range of values of $\theta$ that will induce linearity and relatively small residuals in the scatterplot. But that is something for you to find out. If you discover that only a narrow range of values accomplishes this, then you can have confidence that the decay effect is there and can be estimated. Use maximum likelihood; it will be convenient to suppose the $\epsilon$'s are normally distributed. (The profile likelihood, with $\theta$ fixed, is an ordinary least squares problem, so it will be easy to fit this model. Alternatively you could try fitting the relationship between $z$ and $x$ directly using generalized least squares, but I think that would be trickier to implement.) This all might sound complicated but it's actually quite simple. You could set up a spreadsheet in which $\theta$ is the value in a cell, add a $\theta$-varying control to a scatterplot of $x$ and $z$ (computed in the spreadsheet from a column of $t$ values and a column of $y$ values), and simply adjust the control to straighten out the plot. It will be harder to explore a dataset in which there are fewer (or more) measurements $y_j$ than there are workouts $x_k$ or where the temporal spacing between workouts and measurements varies a lot. You might have to settle for maximum likelihood solutions alone, without the benefit of supporting graphics to verify the reasonableness and the adequacy of this model a priori . Even if my assumptions do not agree with your situation in all details, I hope that this discussion at least suggests effective approaches for furthering your investigation.
