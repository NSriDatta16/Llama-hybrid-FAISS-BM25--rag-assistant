[site]: datascience
[post_id]: 73855
[parent_id]: 73845
[tags]: 
It seems to me you did not understand what polynomial regression is. Generally speaking, when you apply polynomial regression, you add a new feature for each power of x of the polynom. When you write : polynomial_features= PolynomialFeatures(degree=2) that means you have degree=2 , that means that you add to your training dataset a new feature filled with x^2. That means that if in your first example you had : Y' = theta0 + x * theta1 now you will have : Y' = theta0 + x * theta1 + x^2 * theta2 . This is a function of second degree represented by a curve. Polynomial regression is linear because you have in fact Y' = Theta * X , where Theta and X are vectors. In a non linear algorithm you will have for example sigmoid(Theta * X) (used in neural networks for example). One cannot say the coeficient will increase in any case, it depends on the data you have and on your model. If you want to avoid having huge differences between the values in Theta, you should apply normalization to your training and test dataset.
