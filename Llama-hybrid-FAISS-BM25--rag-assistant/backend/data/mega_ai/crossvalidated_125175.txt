[site]: crossvalidated
[post_id]: 125175
[parent_id]: 125152
[tags]: 
Here is one way to fit the model that you describe. # sample series x Indicator variables for the seasonal intercepts can be created in several ways, for example: # monthly intercepts S Some arrangements for non-square time series (this has no effect if the series starts in the first season and ends in the last season): monthly.means Regardless of the month the first observation belongs to, the first column is related to January, the second to February and so on. # column names if (S == 12) { colnames(monthly.means) First rows of the monthly.means object: monthly.means[1:15,] # Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec # [1,] 0 1 0 0 0 0 0 0 0 0 0 0 # [2,] 0 0 1 0 0 0 0 0 0 0 0 0 # [3,] 0 0 0 1 0 0 0 0 0 0 0 0 # [4,] 0 0 0 0 1 0 0 0 0 0 0 0 # [5,] 0 0 0 0 0 1 0 0 0 0 0 0 # [6,] 0 0 0 0 0 0 1 0 0 0 0 0 # [7,] 0 0 0 0 0 0 0 1 0 0 0 0 # [8,] 0 0 0 0 0 0 0 0 1 0 0 0 # [9,] 0 0 0 0 0 0 0 0 0 1 0 0 # [10,] 0 0 0 0 0 0 0 0 0 0 1 0 # [11,] 0 0 0 0 0 0 0 0 0 0 0 1 # [12,] 1 0 0 0 0 0 0 0 0 0 0 0 # [13,] 0 1 0 0 0 0 0 0 0 0 0 0 # [14,] 0 0 1 0 0 0 0 0 0 0 0 0 # [15,] 0 0 0 1 0 0 0 0 0 0 0 0 For the monthly trends we can reuse `monthly.means': monthly.trends Some arbitrary regressors: set.seed(123) xreg1 Edited As I did in my first edit, the model could now be fitted using the function lm . lm(x ~ 0 + monthly.means + monthly.trends + xreg1 + xreg2) But I overlooked the AR(1) structure for the error term that is mentioned in the question. As @RobHyndman says in the comment below, we can use the function arima specifying an AR(1) process for the error term, order = c(1,0,0) . An intercept should not be included in order to avoid exact multicollinearity with the dummies for the seasonal means. (The example is arbitrary, the output is printed just to show the coefficients that are estimated.) xreg We can do some simple check. In a model containing only the seasonal intercepts, the estimates of the coefficients match the values of the sample means. fit2 Edited (Answer to a comment posted by the OP.) If the disturbance term follows an AR(1) process, the Ordinary Least Squares estimator is unbiased but not efficient, i.e. on average it gives the true value but the standard errors of parameters estimates are higher than in the classical setting of independent errors (in other words, estimates are not efficient). As you say, extending the model with the AR(1) error term via arima will not change much the the estimates (standard OLS is still unbiased), but their standard errors will be smaller due to the gain in efficiency. When the disturbance term is autocorrelated, omitting the AR term will lead to larger standard errors for the estimates, which implies a larger denominator in the t-statistics and therefore larger t-statistics. Hence, the tests for the null of non-significant regressors will be biased towards non-rejection. Using arima to specify the AR error term will protect against this issue. The code below is a small exercise to check these ideas: 10,000 series are generated from a model with intercept and AR(1) errors. If xregcoef is set to a value other than zero, then a external variable is added to the data. A model that includes the intercept and the external regressor is fitted by means of arima with order=c(1,0,0) (stored in fit1 ) and by means of lm (stored in fit2 ). set.seed(123) xreg Setting for example xregcoef=3 , we can see that estimates from both regressions are very close to the true value (unbiased estimates) but the standard errors are slightly higher for lm , where when the AR structure is omitted. # results for xregcoef=3 t(apply(res, 2, summary)) # Min. 1st Qu. Median Mean 3rd Qu. Max. # coef 1 2.81600 2.96300 3.00100 3.00000 3.03800 3.20100 # s.e. 1 0.04313 0.05286 0.05491 0.05492 0.05693 0.06677 # coef 2 2.65800 2.93300 3.00100 2.99900 3.06400 3.37000 # s.e. 2 0.05928 0.08393 0.08879 0.08905 0.09400 0.12280 Let's now consider xregcoef=0 , that is, the regressor is not part of the data generating process but we fit a model including this regressor. On average the coefficient is estimated as zero in both cases, however, as the standard errors are slightly higher in the second case, the null of the t-test is rejected slightly more often than it should, given a 5% significance level chosen below. # results for xregcoef=0 t(apply(res, 2, summary)) # Min. 1st Qu. Median Mean 3rd Qu. Max. # coef 1 -0.18440 -0.03698 0.0007062 0.0003874 0.03763 0.20080 # s.e. 1 0.04313 0.05286 0.0549100 0.0549200 0.05693 0.06677 # t-stat. 1 -3.68100 -0.67410 0.0129200 0.0072410 0.68500 3.53400 # coef 2 -0.34200 -0.06694 0.0012510 -0.0006191 0.06422 0.36970 # s.e. 2 0.05928 0.08393 0.0887900 0.0890500 0.09400 0.12280 # t-stat. 2 -3.96600 -0.75560 0.0138800 -0.0070140 0.72210 4.44500 # # rejections of the null xregcoef=0 at the 5% significance level sum(abs(res[,3]) > 1.96) / nrow(res) # [1] 0.0516 sum(abs(res[,6]) > 1.96) / nrow(res) # [1] 0.0749
