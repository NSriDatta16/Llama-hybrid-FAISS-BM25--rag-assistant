[site]: crossvalidated
[post_id]: 499303
[parent_id]: 499290
[tags]: 
Big data has proven itself and may use methods that rely little on "statistics" (rely little on considerations about errors and noise) but It is still far from clear whether big data is gonna replace everything. Are we gonna be having so much abundance of resources (measurements and computation) to be able to put every space full of thousands of sensors to gather a universe of data in order to solve a problem that could be simply attacked with a minimalistic approach? It may be doubted how scalable big data is and whether it is able to deal (in a simplistic way, in a non-advanced way) with noise and random variations. Is big data really so different from small data? If you got bad data that can't solve a problem because too low signal to noise ratio... can it be solved by just gathering more of it? And maybe it can be done in some cases but will it be the most efficient? The analysis methods of big data might be very powerful but what about underlying assumptions of the model and data gathering process? We can have an extremely high precision by taking simple averages (or some neural network that does this in a more fluid way), but if there is some systematic error then the result can still be completely wrong (think about image recognition that can be tricked). These type of errors still need to be evaluated with "classical" statistical methods that handle small data. In addition maybe the question about 'classical statistics' being replaced by 'big data' is a loaded question , and it sketches a false dichotomy . It is wrong in the first place to think about these two as unrelated or different.
