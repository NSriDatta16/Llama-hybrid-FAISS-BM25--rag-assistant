[site]: crossvalidated
[post_id]: 617124
[parent_id]: 
[tags]: 
Necessity of keys and queries in a decoder-only transformer model

when we compute self-attention in a decoder model, we compute, for an embedding $x$ , the tensors $Q = W_Q x, K = W_K x, V = W_V x$ . However, in the next step, we compute a dot product on the embedding dimension: $(Q|K) = x^T W_Q^T W_K x$ . In this way, the matrices $W_Q, W_K$ only appear through the product $W_Q^T W_K$ . So why do we need to train two matrices if all that matters is this product ?
