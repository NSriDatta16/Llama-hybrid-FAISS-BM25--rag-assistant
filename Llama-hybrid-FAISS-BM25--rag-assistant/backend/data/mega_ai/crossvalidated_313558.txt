[site]: crossvalidated
[post_id]: 313558
[parent_id]: 311884
[tags]: 
Maybe stacking PCA layers doesn't make sense, but that certainly hasn't stopped people from doing it. One variation is called PCANet first proposed here : Chan, Tsung-Han, et al. "PCANet: A simple deep learning baseline for image classification?." IEEE Transactions on Image Processing 24.12 (2015): 5017-5032. These researchers also look at an LDA variant. This paper is pretty well-known and I have seen several researchers applying this work. The performance on MNIST is reasonable, see below for the benchmarking the error rate against several other contemporary image classification methods: In light of your remark about the utility of having the multiple linear layers, you might enjoy the following comment from the authors: At least one characteristic of the PCANet model seems to challenge common wisdom regarding building a deep learning network such as ConvNet [4], [5], [8] and ScatNet [6], [10]: no nonlinear operations in the early stages of the PCANet until the very last output layer, where binary hashing and histograms are utilized, to compute the output features. Nevertheless, as we will see through extensive experiments, such a drastic simplification does not appear to undermine the performance of the network on various typical datasets.
