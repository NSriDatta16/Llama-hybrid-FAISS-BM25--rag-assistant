[site]: datascience
[post_id]: 99612
[parent_id]: 99609
[tags]: 
BERT's self-attention will be computed for each pair of tokens. If the input sentence has $N$ tokens, then the attention weights will be computed over the $N^2$ pairs of tokens. The attention, nevertheless, will be computed in each one of the attention heads of each of the layers of BERT. If you want to understand the self-attention patterns that are normally found in BERT, you can check this article , where you will find analyses like this one:
