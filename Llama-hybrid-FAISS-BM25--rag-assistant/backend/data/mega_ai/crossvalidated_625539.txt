[site]: crossvalidated
[post_id]: 625539
[parent_id]: 
[tags]: 
Predictive Diagnostic, Comparison of simulated data with observed data

The question is quite abstract, so I display it with only the essential information. Suppose that we have three models $B_{1}, B_{2}$ and $F_{3}$ . The $B_{1}, B_{2}$ are Bayesian models and the $F_{3}$ is a Frequentist model. Suppose also that we are happy with all the three models in terms of diagnostics, i.e. they do not violate any diagnostics and we are more than happy to use them to make inference. Now I have information that $B_{1}$ is the best model compared to $B_{2}$ and $F_{3}$ , in terms that it fits the actual observation process, whereas the other two $B_{2}$ and $F_{3}$ use a part of the full likelihood that it is used for $B_{1}$ . What I would like to do is to make a quite quick comparison between those models. I would like to create an argument that easily can be applied to all of them. Let's be more concrete. Let $D$ the observed data. Also, $\hat{\theta_{B_{1}}}, \hat{\theta_{B_{2}}}$ and $\hat{\theta_{F_{3}}}$ are the parameter estimates for each model. Then we can easily generate simulated data (a couple of thousands replications) conditional on those parameter estimates, denote them as $D_{B_{1}}, D_{B_{2}}$ and $D_{F_{3}}$ . I've seen somewhere years ago (and sadly cannot find a reference at the moment) that we can compare the distances $D-D_{B_{1}}, D-D_{B_{2}}$ and $D-D_{F_{3}}$ , and make an argument if the estimate parameters are actually let say good in the sense that they can actually reproduce the observed data and they do not produce nonsense data. Is there a reference about that?
