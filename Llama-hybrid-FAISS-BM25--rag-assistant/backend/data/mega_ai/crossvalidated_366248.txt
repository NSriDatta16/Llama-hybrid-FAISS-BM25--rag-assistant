[site]: crossvalidated
[post_id]: 366248
[parent_id]: 
[tags]: 
How do you compute the P(x>y) for a joint density function in R?

I'm trying to understand the Bayesian AB testing process more thoroughly. If I have two tests such that the posteriors are: $$x\sim Beta(\alpha_1, \beta_1)$$ $$y\sim Beta(\alpha_2, \beta_2)$$ Where $x$ and $y$ are posterior distributions. Then the joint pdf is of the form: $f(x,y) = \frac{\gamma(\alpha_1+\beta_1)}{\gamma(\alpha_1)\gamma(\beta_1)}x^{\alpha_1-1}(1-x)^{\beta_1-1}\frac{\gamma(\alpha_2+\beta_2)}{\gamma(\alpha_2)\gamma(\beta_2)}y^{\alpha_2-1}(1-y)^{\beta_2-1}\quad 0\leq x \leq 1,\quad 0\leq y \leq 1$ I want to know $P(x>y)$, how do u solve the equation $$ P(x>y) = \int_{0}^{1}\int_{y}^{1}\frac{\gamma(\alpha_1+\beta_1)}{\gamma(\alpha_1)\gamma(\beta_1)}x^{\alpha_1-1}(1-x)^{\beta_1-1}\frac{\gamma(\alpha_2+\beta_2)}{\gamma(\alpha_2)\gamma(\beta_2)}y^{\alpha_2-1}(1-y)^{\beta_2-1}dxdy$$ in R? I have the joint function coded like so joint_dist How do I use the integrate function to integrate from y to 1 and then 0 to 1? Also I'm aware of Evan Miller's summation derivation, but I'm under the impression that he uses a $Beta(1,1)$ prior for both control and variation. I'd like to be able to choose a different prior for each. Maybe an uniformative prior for the variation, but a more informed prior for the control. Maybe I'm wrong here.
