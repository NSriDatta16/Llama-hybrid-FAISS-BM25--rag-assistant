[site]: crossvalidated
[post_id]: 349418
[parent_id]: 
[tags]: 
Definition of softmax function

This question follows up on stats.stackexchange.com/q/233658 The logistic regression model for classes {0, 1} is $$ \mathbb{P} (y = 1 \;|\; x) = \frac{\exp(w^T x)}{1 + \exp(w^T x)} \\ \mathbb{P} (y = 0 \;|\; x) = \frac{1}{1 + \exp(w^T x)} $$ Clearly those probabilities sum to 1. By setting $w = \beta_1 - \beta_0$ we could also define logistic regression as $$ \mathbb{P} (y = c \;|\; x) = \frac{\exp(\beta_c^T x)}{\exp(\beta_0^T x) + \exp(\beta_1^T x)} \quad \forall \; c \in \{0, 1\} $$ However, the second definition is rarely used because the coefficients $\beta_0$ and $\beta_1$ are not unique. In other words, the model is not identifiable, just like linear regression with two variables that are multiples of each other. Question In machine learning, why is the softmax regression model for classes {0, 1, ..., K â€“ 1} usually defined as follows? $$ \mathbb{P} (y = c \;|\; x) = \frac{\exp(\beta_c^T x)}{\exp(\beta_0^T x) + \dots + \exp(\beta_{K-1}^T x)} \quad \forall \; c \in \{0, \dots, K-1\} $$ Shouldn't it instead be $$ \begin{align*} \mathbb{P} (y = c \;|\; x) &= \frac{\exp(w_c^T x)}{1 + \exp(w_1^T x) + \dots + \exp(w_{K-1}^T x)} \quad \forall \; c \in \{1, \dots, K-1\} \\ \mathbb{P} (y = 0 \;|\; x) &= \frac{1}{1 + \exp(w_1^T x) + \dots + \exp(w_{K-1}^T x)} \end{align*} $$ Side note: In statistics, softmax regression is called multinomial logistic regression and the classes are {1, ..., K}. I find this a bit awkward because when K = 2, the classes are {1, 2} instead of {0, 1} so it's not exactly a generalization of logistic regression.
