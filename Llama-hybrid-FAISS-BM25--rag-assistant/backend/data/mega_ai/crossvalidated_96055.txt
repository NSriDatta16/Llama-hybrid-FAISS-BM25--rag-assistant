[site]: crossvalidated
[post_id]: 96055
[parent_id]: 
[tags]: 
Proving conditional independence from mutual independence

Suppose I have a Bayesian inference problem where $E$ is the result of a chemistry experiment. The chemistry experiment consists of a controlled sequential digestion of one end of the protein using a specific enzyme that has known statistical properties. The sequence of the protein is unknown at the beginning of the experiment. The data collected is the number of reactions and the times in which they occur. The goal is to infer sequence information using the enzyme. $P\left(x_{1},\, x_{2},\,\ldots,\, x_{n}|E\right)$ = $\dfrac{P\left(x_{1},\, x_{2},\,\ldots,\, x_{n}\right)P\left(E|x_{1},\, x_{2},\,\ldots,\, x_{n}\right)}{P\left(E\right)}$ Suppose also that the discrete random variables $X_{1},\, X_{2},\,\ldots,\, X_{n}$ of the posterior are known to be mutually independent a priori. These random variables of the posterior give sequence information (the 20 amino acids) for the first n positions in the protein. This Bayesian inference calculation is performed for all amino acid sequences of length n. The marginal distributions are then taken to infer the unknown sequence. How do I prove the conditional independence of the posterior random variables given the evidence: $P\left(x_{1},\, x_{2},\,\ldots,\, x_{n}|E\right)$ = $P\left(x_{1}|E\right)P\left(x_{2}|E\right)\cdots P\left(x_{n}|E\right)$ .
