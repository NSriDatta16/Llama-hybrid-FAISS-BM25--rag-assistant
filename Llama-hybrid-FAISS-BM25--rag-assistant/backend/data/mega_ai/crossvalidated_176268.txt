[site]: crossvalidated
[post_id]: 176268
[parent_id]: 176262
[tags]: 
@Cliff AB is right, but I would like to add something In a cross section we don't really (usually at least) think about the correlation within the error term itself - you do in time series and panal analysis. Uncorrelated errors, in regression, can a mean a couple of thing. But the one the comes to mind is the (perhaps) most important assumption in linear regression. Namely the relationship between the error term and X-variables. For consistent estimates of the parameters, you need the assumption of no correlation between the x and the error term, that is: $E(e'x)=0$. This is almost always violated in terms of omitted variables, unless you have actual experimental data (or using simulated data, i.e. a computer experiment). EDIT: To actually adress your question; consider that: $$ \text{Cov}(x,x) = \text{Var}(x) $$ Therefore we can write: $$ \rho_{(x,x)}= \frac{\text{Var}(x)}{\text{sd}(x) \cdot \text{sd}(x)} = 1 $$ Given that $\text{Var}(x) \neq 0$, which is not really an assumption. So to state that a single variable is uncorrelated, does not really make sense. In a regression context we think of uncorrelated errors like above, or across time (for time series and/or panel data applications). The are many exelent post on CV that descibes this.
