[site]: crossvalidated
[post_id]: 153539
[parent_id]: 
[tags]: 
Intuition behind sparsity in over-complete sparse auto-encoders

I am trying to get a grasp of the intuition behind the sparse representation used in over-complete auto-encoders. One piece of text that offers a somewhat intuitive explanation is from Yoshua Bengio 's Learning Deep Architectures for AI : Suppose that the representation learned by an auto-encoder is sparse, then the auto-encoder cannot reconstruct well every possible input pattern, because the number of sparse configurations is necessarily smaller than the number of dense configurations. I have also consulted Stanford's Unsupervised Feature Learning and Deep Learning, which wasn't quite helpful. I feel that I require more reading material in order to get a firmer grasp on the topic. Are there any recommendations or resources which might help?
