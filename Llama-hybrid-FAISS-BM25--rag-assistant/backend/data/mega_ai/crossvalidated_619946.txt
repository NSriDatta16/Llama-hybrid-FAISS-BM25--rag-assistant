[site]: crossvalidated
[post_id]: 619946
[parent_id]: 
[tags]: 
Low coverage of prediction intervals from quantile regression using LightGBM on heldout data

I fit three models using LightGBM with quantile objective (which uses pinball loss) using alpha values 0.10, 0.50, and 0.90. The following code is used to wrap the three models into a single class. For now, I trained the models using the default LightGBM hyperparameters. class LightGBMQuantileTransformer(BaseEstimator, TransformerMixin): """ Wrapper class to run all quantile regression models in one "single model" class """ def __init__(self, quantiles, params): self.quantiles = quantiles self.models = {} self.params = params self.params.update({"objective": "quantile"}) def fit(self, X, y, **kwargs): self.X = X for quantile in self.quantiles: self.params.update({"alpha": quantile}) model = LGBMRegressor(**self.params) model.fit(X, y, **kwargs) self.models.update({f"q{str(int(quantile*100)).zfill(2)}": model}) self.model_names = list(self.models.keys()) return self def predict(self, X): preds = [] for model in self.models.values(): pred = model.predict(X) preds.append(pred.reshape(-1, 1)) return np.hstack(preds) The hope is to use the predictions from the q10 and q90 models to generate 80% prediction intervals (PIs), and I'd like to evaluate performance of these PIs by testing the coverage on heldout data. Coverage is defined as the percent of the time that the prediction intervals contain the ground truth value, so we would expect 80% PIs to have 80% coverage. I was seeing PI coverage that was only around 65% on heldout data derived via cross-validation, much lower than the desired 80% coverage expected. I trained quantile regression models for many more quantiles in order to examine whether different PI widths resulted in different coverage. I also compared the coverage of the heldout data to that of the data used during training, and here were the results: For my original 80% prediction interval model set, I wanted to observe the model training process for any signs of overfitting. I tracked several metrics as the tree boosting process was carried out (for a total of 100 trees). We can see that the error on the median prediction (q50) goes down as expected and the prediction intervals narrow, but this comes at the cost of reducing coverage much below the expected coverage on the heldout data. The data evaluated during cross-validation is a strategic subset of the training data and that might explain why it has lower mean absolute error than the training data. But it is undesirable how the coverage of the predictions on the heldout data starts out in the desired range but drops as training proceeds while the train data coverage starts low and then quickly exceeds the 80% expected coverage. Based on these plots, I'm not convinced it is an overfitting issue. I'm also not convinced this is simply an issue of poor hyperparameter choices, though I'm sure I could make improvements through tuning. Why might these quantile-regression-derived prediction intervals not generalize well to new data?
