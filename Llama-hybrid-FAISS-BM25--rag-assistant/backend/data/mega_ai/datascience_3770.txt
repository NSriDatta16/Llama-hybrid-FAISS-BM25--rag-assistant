[site]: datascience
[post_id]: 3770
[parent_id]: 
[tags]: 
How to merge monthly, daily and weekly data?

Google Trends returns weekly data so I have to find a way to merge them with my daily/monthly data. What I have done so far is to break each serie into daily data, for exemple: from: 2013-03-03 - 2013-03-09 37 to: 2013-03-03 37 2013-03-04 37 2013-03-05 37 2013-03-06 37 2013-03-07 37 2013-03-08 37 2013-03-09 37 But this is adding a lot of complexity to my problem. I was trying to predict google searchs from the last 6 months values, or 6 values in monthly data. Daily data would imply a work on 180 past values. (I have 10 years of data so 120 points in monthly data / 500+ in weekly data/ 3500+ in daily data) The other approach would be to "merge" daily data in weekly/monthly data. But some questions arise from this process. Some data can be averaged because their sum represent something. Rainfall for example, the amount of rain in a given week will be the sum of the amounts for each days composing the weeks. In my case I am dealing with prices, financial rates and other things. For the prices it is common in my field to take volume exchanged into account, so the weekly data would be a weighted average. For financial rates it is a bit more complex a some formulas are involved to build weekly rates from daily rates. For the other things i don't know the underlying properties. I think those properties are important to avoid meaningless indicators (an average of fiancial rates would be a non-sense for example). So three questions: For known and unknown properties, how should I proceed to go from daily to weekly/monthly data ? I feel like breaking weekly/monthly data into daily data like i've done is somewhat wrong because I am introducing quantities that have no sense in real life. So almost the same question: For known and unknown properties, how should I proceed to go from weekly/monthly to daily data ? Last but not least: when given two time series with different time steps, what is better: Using the Lowest or the biggest time step ? I think this is a compromise between the number of data and the complexity of the model but I can't see any strong argument to choose between those options. Edit: if you know a tool (in R Python even Excel) to do it easily it would be very appreciated.
