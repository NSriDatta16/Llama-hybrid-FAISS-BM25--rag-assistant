[site]: crossvalidated
[post_id]: 614731
[parent_id]: 614730
[tags]: 
The term is often used by the machine learning community loosely. When people say that “neural network converged” they mean that the training stopped improving. This could mean that they picked some arbitrary $\epsilon$ threshold and some metric that they use for tracking the progress of the training is changing from epoch to epoch by less than $\epsilon$ . In other cases, it could mean that someone eyeballed the plot showing the training progress and decided that it looks “flat enough”. Finally, there are no convergence guarantees as often in mathematics. The training can reach a minimum as measured with validation loss and then the loss can start getting worse as the model starts overfitting. Also reaching some point where it does not improve much does not mean that if you trained longer you would not get a better performing model or that other model would not be better.
