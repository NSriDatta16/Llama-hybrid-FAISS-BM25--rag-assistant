[site]: datascience
[post_id]: 45697
[parent_id]: 44220
[tags]: 
I am working on a similar problem where we are classifying 2 million products into about 1000 categories. I converted the product descriptions using tf-idf vectorization, and then used SVM to run the supervised classification. There is a lot of optimization you can do with the sklearn package in Python for natural language processing. Additionally I struggled because I wanted to include other features beyond just text descriptions. There is a union method in sklearn for accomplishing that. The final model was about 90% accurate in classifying, but out of a 1000 classes there were quite a few with low accuracy rates from the test set. This is likely due to small training sample for those classes. So, just make sure there are enough training samples for each class when you build your model.
