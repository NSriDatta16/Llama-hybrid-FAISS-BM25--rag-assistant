[site]: crossvalidated
[post_id]: 396044
[parent_id]: 
[tags]: 
Validity of pruning algorithm in regression trees

I am reading the book "The elements of Statistical Learning"(pdf available online for free) and in particular I'm trying to better understand the validity of the algorithm presented in section 9.2.2, p.308. Initially we construct the tree in such a way that the $L^2$ -norm: \begin{align} C(T) = \sum_{m=1}^{|T|}\sum_{x_i\in R_m}^{}(y_i-\bar{y}_m)^2 \end{align} is minimized. Here $|T|$ is the number of terminal nodes, $\{R_m\}_m$ are the disjoint regions referring to the terminal nodes and $\bar{y}_m$ is the average value in region $R_m$ .a Then when we want to prune the initial tree $T_0$ , we introduce a cost function depending on a tuning variable $a$ , \begin{align} C_a(T) = C(T) + a|T| \end{align} and we say that for a fixed $a$ we can show that there exists a unique tree $T_a$ minimizing $C_a(T)$ . My questions are the following: 1) I don't understand why C_a(T) is defined in that way, i.e. why this addition a|T| is added to C(T) and not another function depending on $a$ ? 2) How does the cost C_a(T_a) relate to C(T_0)? 3) Why the choice of T_a is a good choice of subtree to consider? Thanks in advance.
