[site]: crossvalidated
[post_id]: 339358
[parent_id]: 
[tags]: 
Why does a AR(1) model that's mean reverting revert back to B0 as opposed to B0 + B1*B0?

In time series analysis, an AR$(1)$ model takes the form: $$x_t = \beta_0 + \beta_1 \cdot x_{t-1} + w_t,$$ where $w_t$ is the white noise term. In order for the model to be stationary and to converge to the mean, $\beta_1$ must be less than one. However, I don't get why such a model converges to B0 as opposed to $\beta_0 + \beta_1 \cdot \beta_0$. Surely, $\beta_1 \cdot \beta_0$ is large enough to matter even if $\beta_1 Is my math wrong?
