[site]: datascience
[post_id]: 126378
[parent_id]: 126377
[tags]: 
Hadley and I offer this critique of your approach: Your data is messy. It's time to tidy it up! Consider these grades on a monthly spelling test: student Jan. Feb. Mar. Alice 89 91 92 Bob 79 84 85 This is messy data, very inconvenient for analysis. What you really want is this: time student grade 2023-01 Alice 89 2023-01 Bob 79 2023-02 Alice 91 2023-02 Bob 84 2023-03 Alice 92 2023-03 Bob 85 Now we have a compound PK of (time, student) mapping to one or more attributes, and we can conveniently pick out time ranges, compute aggregate statistics, and so on. Your "end goal" diagonal that involves windSpeed_mph_3h is the same thing. You want the wind speed forecast data organized with a PK of (time, delta) mapping to a forecast windspeed. For the row that has delta hours of 0 , it's showing an actual measured windspeed, a historic fact of what was observed to happen at that time. Other rows listing the same time will have some non-zero delta , representing a forecast made that many hours ahead of time. Recommend you store the inverter power in KW in a separate RDBMS table, and then JOIN against that when you need to combine things. You might find it convenient to CREATE VIEW, in order to record a JOIN query that you often use. external storage ... takes too long and crashes VS Code. It sounds like you're exhausting internal storage, you're exceeding the amount of installed RAM. A relational database is a terrific way to deal with such challenges. A table index, such a primary key, is an on-disk datastructure that lets us navigate large datasets very rapidly, and supports fast JOIN operations. Postgres and MariaDB are excellent options, but even the very simple sqlite, bundled with python's standard libraries, would be a big step up from your current approach. Remember to put time indexes on your tables. Pandas offers great support for reading and writing table data to an RDBMS.
