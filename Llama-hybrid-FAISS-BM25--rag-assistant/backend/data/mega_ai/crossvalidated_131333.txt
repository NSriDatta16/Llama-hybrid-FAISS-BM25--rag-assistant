[site]: crossvalidated
[post_id]: 131333
[parent_id]: 130985
[tags]: 
All the other answers (especially related to over-fitting) are very good, but I would just add one thing. The very nature of learning algorithms is that training them ensures they learn "something" common about the data they are exposed to. However, what we cannot be directly sure of, is exactly which features about the training data they end up actually learning. As an example, with image recognition, it's very hard to be sure whether a trained neural network has learned what a face looks like, or something else that's inherent in the images. An ANN could have just memorized what the shirts or shoulders or hair look like, for example. That said, using a separate set of testing data (unseen by training) is one way to increase the confidence that you have a model that can be counted on to perform as expected with real-world/unseen data. Increasing the number of samples and feature variability also helps. What is meant by feature variability, is that you want to train with data that has as many variations which still count on each sample as possible. For example, with face data again, you want to show each particular face on as many different backgrounds as possible, and with as many variations in clothing, lighting, hair color, camera angles etc as possible. This will help ensure that when the ANN says "face" it's really a face, and not a blank wall in the background that triggered the response.
