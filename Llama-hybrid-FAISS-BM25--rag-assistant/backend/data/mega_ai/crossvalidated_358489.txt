[site]: crossvalidated
[post_id]: 358489
[parent_id]: 358142
[tags]: 
All cells will have an input of some kind. If the LSTM continues iterating after the sequence ends, one of two circumstances typically obtains: The missing inputs are replaced by padding (usually 0s) The cell begins receiving input from another source. This may be the output of the previous cell, the output of of cells at a deeper layer, or some appropriately shaped vector/tensor computed over the entire sequence length (eg, as maybe computed by some attention mechanism). Often it is taken from the final output of the model at the previous time step. I think your intuition about the number of cells is correct, in that just inputting zero pads (what I think you mean by "no data") and letting the cell run on would mostly result in the degradation of the memory state. However, I'm not aware of any typical architectures that do this. Usually the output is being fed back in as input in this circumstance. However, adding additional layers adds additional memory blocks. The effect of this is complicated, but it does increase the total size of memory available to the model. There's no guarantee that this will improve the temporal depth of memory, but in principle it could have that effect. (Relatedly, some researchers have even tried used "hierarchical" LSTMs to take advantage of deeper layers specifically for increasing temporal depth).
