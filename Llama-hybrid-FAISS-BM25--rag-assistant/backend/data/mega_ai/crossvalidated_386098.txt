[site]: crossvalidated
[post_id]: 386098
[parent_id]: 386093
[tags]: 
The variational Bayes approach to a Bayesian mixture model, as described in detail in the Wikipedia page , is producing a pseudo-posterior distribution on the parameters of the mixture model, including the latent variables $\mathbf{Z}$ , by imposing a certain dependence structure (or graphical model) and estimating the hyperparameters of this model by a maximising algorithm akin to the EM algorithm . Note: variational Bayes is a form of (empirical) Bayesian inference, which simplifies the true posterior into a distribution with more independence assumptions. In the current example, the construction of the variational Bayes approximation relies on an optimisation algorithm that is close to EM, usually employed for maximum likelihood estimation. Reproducing the equations from that Wikipedia page we have that $$q(\mathbf{Z},\mathbf{\pi},\mathbf{\mu},\mathbf{\Lambda}) = q(\mathbf{Z})q(\mathbf{\pi},\mathbf{\mu},\mathbf{\Lambda})$$ means that $\mathbf{Z}$ is given as independent from $(\mathbf{\pi},\mathbf{\mu},\mathbf{\Lambda})$ in the variational Bayes approximation, which is not true for the exact posterior distribution, proportional to $p(\mathbf{X},\mathbf{Z},\mathbf{\pi},\mathbf{\mu},\mathbf{\Lambda})$ . When solving the VB equation $$q^*(\mathbf{Z}) = \mathbb{E}_{\mathbf{\pi},\mathbf{\mu},\mathbf{\Lambda}}[\ln p(\mathbf{X},\mathbf{Z},\mathbf{\pi},\mathbf{\mu},\mathbf{\Lambda})] + \text{constant} $$ the variational Bayes version of the posterior on the latent variables is of the Multinomial kind $$q^*(\mathbf{Z}) = \prod_{n=1}^N \prod_{k=1}^K r_{nk}^{z_{nk}}$$ Similarly, the variational Bayes approximation makes the assumption that the posterior on the parameters factorises as $$q(\mathbf{\pi},\mathbf{\mu},\mathbf{\Lambda})=q(\mathbf{\pi}) \prod_{k=1}^K q(\mathbf{\mu}_k,\mathbf{\Lambda}_k)$$ and the conjugate nature of the prior implies that the variational Bayes posterior is also from the conjugate family. (This is a generic result for variational Bayes approximations in exponential families.) There is no connection with MCMC, but rather EM , as well-described in the Wikipedia page : the reason is that the variational Bayes posterior is made of standard distributions, thus does not require simulation, but has hyperparameters, like the $r_{kn}$ above that need derivation by an optimisation program, hence the call to an EM-like algorithm. The only weak similarity is that both MCMC and EM are iterative, but the number of iterations in EM is determined by convergence to the optimum, while the number of iterations in MCMC is unlimited. Furthermore, MCMC operates on the parameter space, while EM operates on the hyperparameter space.
