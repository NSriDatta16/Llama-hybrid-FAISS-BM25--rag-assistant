[site]: crossvalidated
[post_id]: 315042
[parent_id]: 315027
[tags]: 
Your wording is bizarre. Normalization makes your input commensurate in size with the initializations you pick for your weights/parameters. The mean shift balances your input to have both negatives and positives, which is important when your model has asymmetric functions, such as ReLU neurons or the sigmoid of a logistic regression. It also prevents your functions from being saturated, I.e. have vanishing gradient
