[site]: datascience
[post_id]: 31337
[parent_id]: 
[tags]: 
Replacing VGG by LeNet Caffe model

this code works with a VGG caffe model (VGG_ILSVRC_19_layers.caffemodel). When I tried to change the model to a LeNet caffemodel (below the prototxt file), I got the following error. In 6 module of nn.Sequential: xx/.luarocks/share/lua/5.1/nn/Linear.lua:66: size mismatch, m1: [1 x 594050], m2: [140450 x 500] at xx/torch/pkg/torch/lib/TH/generic/THTensorMath.c:1293 Any suggestions? Many thanks! =========================== Code net = loadcaffe.load(opt.cnn_proto, opt.cnn_model, opt.backend); --for i = 1, 9 do -- Lines removed when using LeNet -- net:remove() --end print(net) if opt.gpuid >= 0 then require 'cutorch' require 'cunn' cutorch.setDevice(opt.gpuid+1) net = net:cuda() end net:evaluate() function loadim(imname) im = image.load(imname) im = image.scale(im,448,448) if im:size(1) == 1 then im2=torch.cat(im,im,1) im2=torch.cat(im2,im,1) im=im2 elseif im:size(1) == 4 then im=im[{{1,3},{},{}}] end im = im * 255; im2 = im:clone() im2[{{3},{},{}}] = im[{{1},{},{}}]-123.68 im2[{{2},{},{}}] = im[{{2},{},{}}]-116.779 im2[{{1},{},{}}] = im[{{3},{},{}}]-103.939 return im2 end local image_root = opt.image_root local file = io.open(opt.input_json, 'r') local text = file:read() file:close() json_file = cjson.decode(text) print(json_file['unique_img_train']) local train_list={} for i,imname in pairs(json_file['unique_img_train']) do table.insert(train_list, image_root .. imname) end local test_list={} for i,imname in pairs(json_file['unique_img_test']) do table.insert(test_list, image_root .. imname) end local batch_size = opt.batch_size local sz=#train_list local feat_train=torch.FloatTensor(sz, 14, 14, 512) print(string.format('processing %d images...',sz)) for i = 1, sz, batch_size do xlua.progress(i, sz) r = math.min(sz, i+batch_size-1) ims = torch.DoubleTensor(r-i+1, 3, 448,448) for j = 1, r-i+1 do ims[j] = loadim(train_list[i+j-1]) end if opt.gpuid >= 0 then ims = ims:cuda() end net:forward(ims) feat_train[{{i,r}, {}}] = net.output:permute(1,3,4,2):contiguous():float() collectgarbage() end local train_h5_file = hdf5.open(opt.out_name_train, 'w') train_h5_file:write('/images_train', feat_train) train_h5_file:close() print('DataLoader loading h5 file: ', 'data_train') local sz = #test_list local feat_test = torch.FloatTensor(sz, 14, 14, 512) print(string.format('processing %d images...',sz)) for i = 1, sz, batch_size do xlua.progress(i, sz) r = math.min(sz, i + batch_size-1) ims = torch.DoubleTensor(r-i+1, 3, 448, 448) for j = 1, r-i+1 do ims[j] = loadim(test_list[i+j-1]) end if opt.gpuid >= 0 then ims = ims:cuda() end net:forward(ims) feat_test[{{i,r}, {}}] = net.output:permute(1,3,4,2):contiguous():float() collectgarbage() end local test_h5_file = hdf5.open(opt.out_name_test, 'w') test_h5_file:write('/images_test', feat_test) test_h5_file:close() ======================== LeNet-deploy.prototxt input: "data" input_shape { dim: 1 dim: 3 dim: 224 dim: 224 } layer { name: "scale" type: "Power" bottom: "data" top: "scaled" power_param { scale: 0.0125000001863 } } layer { name: "conv1" type: "Convolution" bottom: "scaled" top: "conv1" param { lr_mult: 1.0 } param { lr_mult: 2.0 } convolution_param { num_output: 20 kernel_size: 5 stride: 1 weight_filler { type: "xavier" } bias_filler { type: "constant" } } } layer { name: "pool1" type: "Pooling" bottom: "conv1" top: "pool1" pooling_param { pool: MAX kernel_size: 2 stride: 2 } } layer { name: "conv2" type: "Convolution" bottom: "pool1" top: "conv2" param { lr_mult: 1.0 } param { lr_mult: 2.0 } convolution_param { num_output: 50 kernel_size: 5 stride: 1 weight_filler { type: "xavier" } bias_filler { type: "constant" } } } layer { name: "pool2" type: "Pooling" bottom: "conv2" top: "pool2" pooling_param { pool: MAX kernel_size: 2 stride: 2 } } layer { name: "ip1" type: "InnerProduct" bottom: "pool2" top: "ip1" param { lr_mult: 1.0 } param { lr_mult: 2.0 } inner_product_param { num_output: 500 weight_filler { type: "xavier" } bias_filler { type: "constant" } } } layer { name: "relu1" type: "ReLU" bottom: "ip1" top: "ip1" } layer { name: "ip2" type: "InnerProduct" bottom: "ip1" top: "ip2" param { lr_mult: 1.0 } param { lr_mult: 2.0 } inner_product_param { num_output: 5 weight_filler { type: "xavier" } bias_filler { type: "constant" } } } layer { name: "softmax" type: "Softmax" bottom: "ip2" top: "softmax" }
