[site]: crossvalidated
[post_id]: 610129
[parent_id]: 610118
[tags]: 
What you are describing is that you need to losslessly compress and retrieve the data. Did you consider any off-the-shelf caching solution? Since you care about “predicting” the seen data, it's about compressing it, and if I were you I’d start with ready data compression algorithms. If they don't compress the data enough, you could try machine learning, but storing machine learning model also needs storage space, and the 100% accurate model will possibly be one that is complicated, so heavy. When using machine learning, you could simply fit one model per chunk, so there is no problem with not seeing all the data at once. If you insist on having a single model, it would be more complicated and it would need to be able to distinguish somehow between the chunks (so it doesn't predict chunk 2 for chunk 1). So it might be harder to do, but the model itself could potentially be smaller than the per-chunk models if there are patterns that repeat between the chunks. But really, start with regular caching and compression. Decent compression algorithms were designed for such purposes and can do miracles.
