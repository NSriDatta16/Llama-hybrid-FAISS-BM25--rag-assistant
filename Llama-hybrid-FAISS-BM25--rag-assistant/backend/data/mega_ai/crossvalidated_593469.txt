[site]: crossvalidated
[post_id]: 593469
[parent_id]: 593418
[tags]: 
During training a NN is often overparameterised and only fits well because of some explicit or implicit regularisation. To compute the effective degrees of freedom is very difficult. With elastic net ( How can I calculate the number of degrees of freedom in the Elastic Net regularization, specifically in R? ) there is a method that approximates the effective degree of freedom by using the hat matrix which expresses the 'flexibility'/variation of the model around the final fitted point. I wouldn't be surprised if equivalent approaches exist for neural networks. After training a model you have a fixed formula for predictions. Which contains zero parameters. If you test it then there is no parameter fitting anymore. So your F statistic will use the same degrees of freedom in numerator and denominator and the statistic resembles a relative likelihood . Sidenote: the use of a F-statistic is not very rigorous in this case. Assumptions relating to it's use are independent and identical normal distributed error terms. So theoretical calculations that estimate statistical probabilities, like p-values, are likely wrong. The use of the F-statistic makes more sense when you can apply some statistical model for it's sample distribution. Otherwise it is just a random measure. Maybe you could better use a more intuitive measure like comparing some relevant cost function. Related: What does "degree of freedom" mean in neural networks?
