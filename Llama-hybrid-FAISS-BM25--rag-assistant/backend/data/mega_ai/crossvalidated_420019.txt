[site]: crossvalidated
[post_id]: 420019
[parent_id]: 
[tags]: 
How do convolutional neural networks learn from images of different translations and conditions?

when we feed the CNN images of cats in different lighting conditions or colors, Is it the job of the conv layers to learn the different representations(lighting conditions and colors) and map them to one final representation(in the last conv layer) or Is it the job of the fully connected layers to receive those different representations(from the last conv layer) and map them to the label "cat"? And What would be the case if the CNN has no fully connected layers like SqueezeNet? I am not talking about objects' spatial translation.
