[site]: datascience
[post_id]: 106720
[parent_id]: 
[tags]: 
Binary classification with seperate training and testing datasets

I have two datasets (train.csv) and (test.csv) revolving around predicting the death outcome for a disease. Both sets include 20 independent variables (age, weight, etc), but only the train.csv dataset contains the true death outcome (0 for alive, 1 for death). The training dataset has a shape of (650, 21), whereas the test dataset has a shape of (200, 20). I'm also using python, and the dataset is imbalanced with only around 30 deaths in the training set. I have to train the model using the training set, and predict the outcome using the test dataset, as well as display stuff such as performance tables. I then have to put the predicted outcomes into a new .csv file for submission which gets tested against a different dataset and gives me an ROC/AUC score. This submission has to have the same amount of observations as the test dataset, which is 200. I just have a few questions revolving around this. Do I have to use train_test_split on the training dataset? Or is it uneccsary because I already have seperate training/testing datasets? I've created 2 different models using XGBoost and a Linear regression model. However, the accuracy for the training model tends to be really high (0.8-0.98), but when submitting my predictions to test on kaggle, it has a really low AUC/ROC score (0.5 or lower). Is this due to overfitting or is there another issue? Is there any way to get an accurate AUC/ROC score for my predictions before I submit them to kaggle? I only have a limited amount of submissions, and I would like to know I'm atleast submitting something good rather than wasting all my submissions. Sorry if that is worded confusingly.
