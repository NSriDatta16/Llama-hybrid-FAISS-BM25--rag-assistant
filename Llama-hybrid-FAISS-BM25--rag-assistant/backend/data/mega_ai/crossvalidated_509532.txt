[site]: crossvalidated
[post_id]: 509532
[parent_id]: 509519
[tags]: 
This problem has been extensively covered in this forum, to wit: Why is computing the Bayesian Evidence difficult? Bayesian MCMC methods that need to calculate the evidence / normalizing factor Normalizing constant irrelevant in Bayes theorem? Why Normalizing Factor is Required in Bayes Theorem? What does it mean intuitively to know a pdf “up to a constant”? Why is it necessary to sample from the posterior distribution if we already KNOW the posterior distribution? and it is hard to see which aspect has not yet been sufficiently processed for the OP. In short, computing the evidence or marginal likelihood is certainly NOT the only problem in Bayesian computation. When dealing with a single model, it is rarely necessary to compute this constant (and if need be there exist unbiased estimators of $I^{-1}$ ). When comparing different models, there exists a myriad of solutions, covered in the above answers. Numerical integration is very rarely a solution of relevance.
