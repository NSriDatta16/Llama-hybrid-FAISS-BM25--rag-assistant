[site]: crossvalidated
[post_id]: 90981
[parent_id]: 90979
[tags]: 
Imbalanced dataset means that there are much more positive examples than negative or vice versa. If you train soft-margin SVM for such data set resulting hyperplane will equally penalize both negative or positive examples. Since there are much more examples in one of the group the hyperplane will lie closer to the group with smaller examples. When inferring a class label for a new data point it will be classified having the label of the majority group more often because of the way it was trained (hyperplane is further from the group of majority examples). In practice it is often the case that you have much more negative examples than positive. Generic SVM will almost always predict testing data point as negative. One way to cope with that is to have two different parameters $C_1,C_2$ one for positive and one for negative examples. In order to balance between positive or negative we will penalize less frequent examples higher. Let $C$ be original parameter, then we set $$C_1=\frac{\# \text{number of positive examples}}{\text{# total examples}}C$$ $$C_2=\frac{\# \text{number of negatuve examples}}{\text{# total examples}}C$$ There are other methods that try to solve this problem. In the end a way to choose parameters for such non-generic SVM is called parameter fine-tuning.
