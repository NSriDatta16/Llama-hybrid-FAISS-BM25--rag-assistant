[site]: crossvalidated
[post_id]: 551
[parent_id]: 298
[tags]: 
I always tell students there are three reasons to transform a variable by taking the natural logarithm. The reason for logging the variable will determine whether you want to log the independent variable(s), dependent or both. To be clear throughout I'm talking about taking the natural logarithm. Firstly, to improve model fit as other posters have noted. For instance if your residuals aren't normally distributed then taking the logarithm of a skewed variable may improve the fit by altering the scale and making the variable more "normally" distributed. For instance, earnings is truncated at zero and often exhibits positive skew. If the variable has negative skew you could firstly invert the variable before taking the logarithm. I'm thinking here particularly of Likert scales that are inputed as continuous variables. While this usually applies to the dependent variable you occasionally have problems with the residuals (e.g. heteroscedasticity) caused by an independent variable which can be sometimes corrected by taking the logarithm of that variable. For example when running a model that explained lecturer evaluations on a set of lecturer and class covariates the variable "class size" (i.e. the number of students in the lecture) had outliers which induced heteroscedasticity because the variance in the lecturer evaluations was smaller in larger cohorts than smaller cohorts. Logging the student variable would help, although in this example either calculating Robust Standard Errors or using Weighted Least Squares may make interpretation easier. The second reason for logging one or more variables in the model is for interpretation. I call this convenience reason. If you log both your dependent (Y) and independent (X) variable(s) your regression coefficients ($\beta$) will be elasticities and interpretation would go as follows: a 1% increase in X would lead to a ceteris paribus $\beta$% increase in Y (on average). Logging only one side of the regression "equation" would lead to alternative interpretations as outlined below: Y and X -- a one unit increase in X would lead to a $\beta$ increase/decrease in Y Log Y and Log X -- a 1% increase in X would lead to a $\beta$% increase/decrease in Y Log Y and X -- a one unit increase in X would lead to a $\beta*100$ % increase/decrease in Y Y and Log X -- a 1% increase in X would lead to a $\beta/100$ increase/decrease in Y And finally there could be a theoretical reason for doing so. For example some models that we would like to estimate are multiplicative and therefore nonlinear. Taking logarithms allows these models to be estimated by linear regression. Good examples of this include the Cobb-Douglas production function in economics and the Mincer Equation in education. The Cobb-Douglas production function explains how inputs are converted into outputs: $$Y = A L^\alpha K^\beta $$ where $Y$ is the total production or output of some entity e.g. firm, farm, etc. $A$ is the total factor productivity (the change in output not caused by the inputs e.g. by technology change or weather) $L$ is the labour input $K$ is the capital input $\alpha$ & $\beta$ are output elasticities. Taking logarithms of this makes the function easy to estimate using OLS linear regression as such: $$\log(Y) = \log(A) + \alpha\log(L) + \beta\log(K)$$
