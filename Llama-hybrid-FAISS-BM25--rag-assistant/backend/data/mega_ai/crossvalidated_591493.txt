[site]: crossvalidated
[post_id]: 591493
[parent_id]: 
[tags]: 
How to calculate confidence interval for detected signals that were matched by time threshold in time-series?

Data & the goal. There are two time series (A and B): A - exact signal, B - same signal, just distorted and with slightly random shift in its (delta) phase (see the image below). Consider the duration and timing of signals as random. I worked out an algorithm to to detect signals in B data and aiming to validate this detection with signals in the A data. The list of B signals can be larger or smaller because of noise and other technical nuances. I have extracted lists of signals from each time-series with their start and end times: lists of (A1,A2) and (B1, B2). I have matched signals by looking for the lowest absolute difference in their end-timestamps (A2 and B2) and then checked if the difference is below threshold, e.g. 10 min. Problem. Now, I want to estimate confidence interval or similar statistical assessment, which could be used to interpret the results when I would apply the signal detection just having data B (no data A for validation). I think the fact that I am using a threshold to match the signals makes the problem a bit particular; at least in my humble perception. Thus, I would welcome opinions, insights, and especially solutions.
