[site]: crossvalidated
[post_id]: 422032
[parent_id]: 
[tags]: 
Deep item-based recommender objective function

I'm trying to understand the following paper written by researchers at eBay that uses deep learning to overcome the problem of making recommendations when you mostly have one-of-a-kind items. A section of the abstract says: We propose an objective function that optimizes a similarity measure between binary implicit feedback vectors between two items. We demonstrate formally and empirically that a model trained to optimize this function estimates the log of the cosine similarity between the feedback vectors. We also propose a neural network architecture optimized on this objective. I don't understand the importance of estimating the log of cosine similarity. Amazon began using cosine similarity in its item-based recommenders back in 2003, and that cosine similarity is now widely adopted today because it is so simple to calculate. Why would the researchers take something that is very complicated (deep learning) to estimate something that is very simple (cosine similarity)? Surely the best way to estimate cosine similarity is to calculate it directly? Maybe the researchers are trying to approximate it indirectly because of the sparsity, and they want to check that when sparsity is not a problem their approach approximates cosine similarity? If that's the case, then is it reasonable to assume that the paper's deep learning approach is unnecessary when the data is dense?
