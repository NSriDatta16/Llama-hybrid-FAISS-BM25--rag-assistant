[site]: datascience
[post_id]: 102236
[parent_id]: 
[tags]: 
Deep autoencoder: validation loss doesn't change

I'm trying to understand autoencoders and reproduced some code from Keras documentation : from keras.datasets import mnist import numpy as np (x_train, _), (x_test, _) = mnist.load_data() x_train = x_train.astype('float32') / 255. x_test = x_test.astype('float32') / 255. x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:]))) x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:]))) input_img = keras.Input(shape=(784,)) encoded = layers.Dense(128, activation='relu')(input_img) encoded = layers.Dense(64, activation='relu')(encoded) encoded = layers.Dense(32, activation='relu')(encoded) decoded = layers.Dense(64, activation='relu')(encoded) decoded = layers.Dense(128, activation='relu')(decoded) decoded = layers.Dense(784, activation='sigmoid')(decoded) autoencoder = keras.Model(input_img, decoded) autoencoder.compile(optimizer='adam', loss='binary_crossentropy') autoencoder.fit(x_train, x_train, epochs=100, batch_size=64, shuffle=True, validation_data=(x_test, x_test)) And when I start fitting the model I see the next validation loss. Epoch 1/100 938/938 [==============================] - 5s 5ms/step - loss: 0.0270 - val_loss: 0.0040 Epoch 2/100 938/938 [==============================] - 4s 4ms/step - loss: 0.0039 - val_loss: 0.0040 Epoch 3/100 938/938 [==============================] - 4s 5ms/step - loss: 0.0039 - val_loss: 0.0040 ... Epoch 100/100 938/938 [==============================] - 4s 5ms/step - loss: 0.0035 - val_loss: 0.0036 However, there is another val_loss value in the documentation: After 100 epochs, it reaches a train and validation loss of ~0.08 Is this overfitting? Why it happens so fast? Why there is another value in the documentation? I tried different learning_rate: 0.01, 0.0001, and nothing helped. I also tried to use tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss') callback but it didn't stop and I don't understand why.
