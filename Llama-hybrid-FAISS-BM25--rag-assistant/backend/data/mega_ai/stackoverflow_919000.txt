[site]: stackoverflow
[post_id]: 919000
[parent_id]: 918971
[tags]: 
You complicate quite a few things. Collecting stats, controlling spiders, html5 storage, XSS, inter-frame communication, virtual-host setup, third-party ad serving, interaction with remote APIs like google maps. That's not to say these things can't be solved, just that the rise in complexity adds more work and may not provide suitable benefits to compensate. I should add that I went down this path once myself for a classifieds site, adding domains like porshe.site.com, ferrari.site.com hoping to boost rank for those keywords. In the end I did not see noticeable improvement and even worse google was walking the entire site via each subdomain, meaning that a search for ferraris might return porsche.site.com/ferraris instead of ferrari.site.com/ferraris. In short google considered each site to be duplicates but it still crawled each site every time it visited. Again, workarounds existed but I chose simplicity and I don't regret it.
