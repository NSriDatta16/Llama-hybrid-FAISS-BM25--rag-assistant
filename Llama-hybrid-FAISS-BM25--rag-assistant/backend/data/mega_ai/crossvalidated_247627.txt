[site]: crossvalidated
[post_id]: 247627
[parent_id]: 247614
[tags]: 
A couple of answers: Biased forecasts can be caused by "wrong" models. If your time series has a trend, but the model does not include the trend, your forecasts will be too low or too high. Alternatively, forecasts might be biased because you know what you are doing. You might be regularizing your model to combat overfitting, accepting some bias but reducing variance (the bias-variance tradeoff) in order to have a lower overall error. On a "meta" level, your forecasts might be biased because you are optimizing on a wrong point forecast accuracy measure. For instance, the MAD is minimized in expectation by the median of the future distribution, not the mean - so if your future realizations are asymmetrically distributed and you optimized your model for the MAD, you will end up with biased forecasts. This is most problematic for low volume count data or intermittent time series, since these are asymmetrically distributed. See Morlidge (2015, Foresight ) or Kolassa (2016, IJF ) . Whether or not biased forecasts are a problem will depend on your subsequent processes. If you forecast intermittent demands and optimize the MAD, you may end up with a flat zero forecast as MAD-optimal. Any stocking or other decisions made based on this zero forecast will likely be suboptimal. Then again, stocks also depend on safety amount calculations. How these are affected by biased models is the subject of a forthcoming paper by Syntetos & Morlidge in Foresight . I use point forecast accuracy measures that reward unbiased forecasts, like the MSE or scaled versions if I want to compare forecasts on different scales (unless I can reasonably expect future realizations to be symmetrically distributed). I always look at bias, not only at accuracy. Whenever possible, I try to forecast full predictive distributions and derive means or any other functional of interest from these predictive distributions - see my IJF paper noted above.
