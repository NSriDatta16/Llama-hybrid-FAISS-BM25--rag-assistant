[site]: crossvalidated
[post_id]: 178201
[parent_id]: 156722
[tags]: 
Here are some tips for enhancing the performance of VW models: Shuffle the data prior to training. Having a non-random ordering of your dataset can really mess VW up. You're already using multiple passes, which is good. Try also decaying the learning rate between passes, with --decay_learning_rate=.95 . Play around with the learning rate. I've had cases where --learning_rate=10 was great and other cases where --learning_rate-0.001 was great. Try --oaa 16 or --log_multi 16 rather than --ect 16 . I usually find ect to be less accurate. However, oaa is pretty slow. I've found --log_multi to be a good compromise between speed and accuracy. On 10,000 training examples, --oaa 16 should be fine. Play with the loss function. --loss_function=hinge can sometimes yield large improvements in classification models. Play with the --l1 and --l2 parameters, which regularize your model. --l2 in particular is useful with text data. Try something like --l2=1e-6 . For text data, try --ngram=2 and --skips=2 to add n-gram and skip grams to your models. This can help a lot. Try --autolink=2 or --autolink=3 to fit a quadratic or cubic spline model. Try ftrl optimization with --ftrl . This can be useful with text data or datasets with some extremely rare and some extremely common features. Try some learning reductions: Try a shallow neural network with --nn=1 or --nn=10 . Try a radial kernel svm with --ksvm --kernel=rbf --bandwidth=1 . (This can be very slow). Try a polynomial kernel svm with --ksvm --kernel=poly --degree=3 . (This can be very slow). Try a gbm with --boosting=25 . This can be a little slow. VW is extremely flexible, so it often takes a lot of fine tuning to get a good model on a given dataset. You can get a lot more tuning ideas here: https://github.com/JohnLangford/vowpal_wabbit/wiki/Command-line-arguments Regarding the post you linked to: that person used vw with squared loss on an unbalanced classification problem. That's a silly thing to do , and pretty much guarantees that any linear model will always predict the dominant class. If you're worried about class balance, VW supports weights, so you can over-weight the rarer classes. Edit : You have 100 classes and 10,000 training examples? That's an average of 100 observations per class, which isn't that many to learn from, no matter what model you use.
