[site]: crossvalidated
[post_id]: 479747
[parent_id]: 
[tags]: 
How to re-scale the inputs for forward-propagation and backpropagation in the drop out?

Assume $p$ is the keep probability for drop out, for the forward-propagation, we do the scaling for the inputs as $A_r = A/p$ . In the backpropagation, as many other people said ( dropout: forward prop VS back prop in machine learning Neural Network ), we should obtain the gradient as $dA = dA_r/p$ . However, in my view, it should be $dA = dA_r * p$ . Is it correct?
