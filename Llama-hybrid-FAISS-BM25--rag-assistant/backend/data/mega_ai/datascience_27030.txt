[site]: datascience
[post_id]: 27030
[parent_id]: 
[tags]: 
Why do good word embeddings come out of maximizing cosine similarity?

I have an understanding of the technical details of word2vec. What I don't understand is why semantically similar words should have high cosine similarity. From what I know, goodness of a particular embedding is seen in shallow tasks such as word analogy. I am unable to grasp the relationship between maximizing cosine similarity and good word embeddings
