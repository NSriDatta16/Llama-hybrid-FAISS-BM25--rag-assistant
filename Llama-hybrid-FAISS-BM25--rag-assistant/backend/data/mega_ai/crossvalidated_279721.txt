[site]: crossvalidated
[post_id]: 279721
[parent_id]: 279713
[tags]: 
One of the most classical is the Perceptron in 2 dimensions, which goes back to the 1950s. This is a good example because it is a launching pad for more modern techniques: 1) Not everything is linearly separable (hence the need for nonlinear activations or kernel methods, multiple layers, etc.). 2) The Perceptron won't converge if the data is not linearly separable (continuous measures of separation such as softmax, learning rate decay, etc.). 3) While there are infinitely many solutions to splitting data, it's clear that some are more desired than others (maximum boundary separation, SVMs, etc.) For multilayer neural networks, you might like the toy classification examples that come with this visualization . For Convolutional Neural Nets, the MNIST is the classical gold standard, with a cute visualization here and here . For RNNs, a really simple problem they can solve is binary addition , which requires memorizing 4 patterns.
