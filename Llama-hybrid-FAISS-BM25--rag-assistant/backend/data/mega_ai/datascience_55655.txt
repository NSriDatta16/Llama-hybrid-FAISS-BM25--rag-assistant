[site]: datascience
[post_id]: 55655
[parent_id]: 54381
[tags]: 
When reading the paper there actually is some points towards the background of these phenomena: About predicting a word meaning there is only the surprise present, but about predicting the order there was a whole section about the statistical influence of natural language underlying mechanics to the performance of the CBOW. It appears, that order of sentences or even word pairs was easy for CBOW whereas a random permutation with no natural order dropped also the performance. If we go very basics, CBOW (Continuous Basket Of Words) mission is to: Predicting a word given its context. [1] Thus, with even an average (or only because of it) we can identify which word is which, although in a little bit weird way around. Just because that is how it is behaving. Secondly, basket is for training the neural network, not the full algorithm. What is the model obtained would be different of the way it gets its data. What makes me guessing is the drop on ability to identify from larger range of words. As we all are surprised how averages make the trick so, well, maybe the true nature is revealed with larger dimensions. [1] https://www.quora.com/What-are-the-continuous-bag-of-words-and-skip-gram-architectures
