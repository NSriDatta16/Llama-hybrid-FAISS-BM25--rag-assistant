[site]: stackoverflow
[post_id]: 2834341
[parent_id]: 2831631
[tags]: 
Doing it in small chunks with redirects seems like one kludge, but how? That's exactly how I handled a full forum database backup (phpBB) when the built-in export mechanism started hitting the max_execution_time limit. I did it one table at a time, and for the big tables in chunks of 5000 rows. (It turned out that the limiting factor in the whole process wasn't the execution time on the export, but actually the file size that phpmyadmin could handle on the import.) After each chunk of exporting, I returned a page with a meta refresh tag in the header, redirecting the script back to itself with the next block's table number and start row in the query string. '); } ?> The counters were so I could iterate through an array of table names that I'd retrieved with SHOW TABLES. Before I had the wits to cull the gigantic word-match table (which phpBB can rebuild by itself) from the export, this back-up script would take over half an hour to complete.
