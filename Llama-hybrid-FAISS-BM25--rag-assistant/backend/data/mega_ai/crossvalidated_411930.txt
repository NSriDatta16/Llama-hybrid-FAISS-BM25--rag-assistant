[site]: crossvalidated
[post_id]: 411930
[parent_id]: 
[tags]: 
Are loss functions necessarily additive in observations?

In all of the contexts I've seen loss functions in statistics/machine learning so far, loss functions are additive in observations. i.e.: loss $Q_D$ of dataset $D$ is an additive aggregation of losses at observations $i\in D$ : $Q_D(\beta)=\sum_{i\in D}Q_i(\beta)$ . e.g. in the loss that is a simple sum of squared residuals: $Q_D=\sum_i(y_i-X_i\beta)^2$ . This seems sensible, but I am wondering: Are there contexts in statistics/machine learning in which it happens (or reasons in theory why one might want) that a loss function is used that is not additive (or even separable) in observations?
