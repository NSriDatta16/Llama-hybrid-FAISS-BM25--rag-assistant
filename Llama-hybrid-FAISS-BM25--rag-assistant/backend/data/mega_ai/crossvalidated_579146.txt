[site]: crossvalidated
[post_id]: 579146
[parent_id]: 579134
[tags]: 
I think there are a couple of reasons: Logistic regression does not have a deterministic closed form solution and must be solved iteratively, starting from some random initialization. The solutions are stochastic and depend on their random seed (hence the random_state argument). There's no guarantee that a given search will converge on the lowest cost, i.e. optimal, solution. The algorithm, as implemented in sklearn anyway, has L2 regularization applied by default. This penalty tries to smooth the coefficients of the logistic function and essentially prevents the model from finding a perfect (and possibly overfit) solution. I just tried some experiments and I reckon that if you switch to L1 regularization, which tries to push as many coefficients as possible to 0, and ramp up the regularization penalty, then I think you will find the model stops behaving this way. For example, try instantiating the model like this: LogisticRegression(penalty='l1', solver='liblinear', C=0.001) Is it a 'better' model? That's the $64,000 question! Not sure whether to include my experiment or not, since this isn't really a programming question. But it's here if you're interested.
