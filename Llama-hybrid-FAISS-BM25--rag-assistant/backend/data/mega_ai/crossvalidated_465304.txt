[site]: crossvalidated
[post_id]: 465304
[parent_id]: 464774
[tags]: 
To compute the gradients , first think that we unfold the RNN through time as below: Though the notation is different, the essence of your problem can be understood very well with this figure. TO compute gradients, we start from the last time step. $t = \textit{T}$ : $$ \begin{align} \dfrac{\partial L}{\partial h_{T}} &= \dfrac{\partial L}{\partial {\hat y}_T}\dfrac{ \partial {\hat y}_T}{\partial h_T} \\ &= \dfrac{\partial L}{\partial {\hat y}_T} \theta^{'}(h_{T}) \end{align} $$ $$ \frac{∂L}{∂s_{T}}=\frac{∂L}{∂h_{T}} \frac{∂h_{T}}{∂s_{T}}=\dfrac{\partial L}{\partial {\hat y}_T} \theta^{'}(h_{T}) ϕ′(s_{T}) $$ where $\frac{\partial L}{\partial {\hat y}_T}$ is the loss gradient with respect to prediction which can be computed easily. Then for $t = \textit{T} -1 $ , we use the relation that you've derived for $\frac{∂L}{∂s_{t}}$ . Gradients for time intervals $t = 0. \dots , \textit{T} -1$ are computed like this. SO, if you compute gradients backward through time you can compute $\frac{∂L}{∂s_{t}}$ 's as $\frac{∂L}{∂s_{t+1}}$ would be known to you(Your derivation is for $t = 0. \dots , \textit{T} -1$ ). The gradients outside the time intervals are assumed to be zero for this procedure( $t > \textit{T}$ ). So, you should define training time steps carefully.
