[site]: datascience
[post_id]: 108750
[parent_id]: 
[tags]: 
How to build a Neural network architecture where dense layer output goes into LSTM layer input?

Hello Everyone, As you can see the above picture, I want to build a similar architecture, Can you tell me how? The CV1 and CV2 in red box are the output of a dense layer, now I have to put the output of the dense layer along with DV and PROD into the input of an LSTM layer and it should predict SEC. Here FC means Fully connected layer. Initially I created a Multi-input model which is same as below, It contains 3 Inputs and 1 output with denselayer. So the point is, I'm taking three inputs and predicting, each input will have LSTMs, im concatinating them and feeding them to 2 dense layers, the final dense layer will output 'outputs' number of columns. Now I want to take those outputs and feed them into different LSTM layer, i want to add DV and PROD inputs to that LSTM as well, finally that layer should go into dense layer and output the final SEC. Is it possible to build such architecture? See my code, if my code is wrong, please correct it, if you can help me, it would be great. def create_model(outputs, inp_shape_inp1, inp_shape_inp2, inp_shape_inp3, train,y_train, val,y_val, epochs=50, loss='mae'): X_inp1_train, X_inp2_train, X_inp3_train = train X_inp1_val, X_inp2_val, X_inp3_val = val inp1 = Input(shape=inp_shape_inp1) inp2 = Input(shape=inp_shape_inp2) inp3 = Input(shape=inp_shape_inp3) lstm_1_1 = LSTM(64, return_sequences=True)(inp1) lstm_1_2 = LSTM(64, return_sequences=True)(inp2) lstm_1_3 = LSTM(64, return_sequences=True)(inp3) lstm_2_1 = LSTM(32)(lstm_1_1) lstm_2_2 = LSTM(32)(lstm_1_2) lstm_2_3 = LSTM(32)(lstm_1_3) mrg = concatenate([lstm_2_1, lstm_2_2, lstm_2_3]) dense = Dense(12)(mrg) op = Dense(outputs, activation='linear')(dense) model = Model([inp1, inp2, inp3], op) model.compile(optimizer='adam', loss=loss,) early_stopping = EarlyStopping(monitor='val_loss', patience=12,mode='min', restore_best_weights=True) history = model.fit([X_inp1_train, X_inp2_train, X_inp3_train], y_train,validation_data=([X_inp1_val, X_inp2_val, X_inp3_val], y_val), shuffle=False, epochs=epochs, batch_size=28, callbacks=[early_stopping]) return model, history
