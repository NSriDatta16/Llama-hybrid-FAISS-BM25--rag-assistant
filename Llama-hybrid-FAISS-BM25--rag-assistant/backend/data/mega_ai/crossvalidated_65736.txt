[site]: crossvalidated
[post_id]: 65736
[parent_id]: 65683
[tags]: 
There are many possible systematic approaches to choosing your features. The approaches are often called feature selection, feature reduction, feature extraction, dimensionality reduction etc. etc. (really there are a ridiculous number of names for it). Methods used to choose the features can include greedy approaches, evolutionary algorithms, specialist domain knowledge, more mathematically grounded approaches like PCA, etc. If you have a particular problem in mind then I would suggest reading up on some of the literature and getting an idea of the approach that is most suitable for you. For example, some methods will cause the meaning of your original features to be lost (e.g. through combining features), and if this is not wanted then you may only want to consider selecting a subset of your original features. As to the theoretical explanation of why fewer features may be better, you might want to have a look at information on the Hughes effect/Hughes phenomenon/peaking paradox (see e.g. here or Wikipedia ). Basically, the performance of a classifier improves with addition of features until a certain (problem dependent) point. After that adding more features can (possibly) be detrimental to the performance of the classifier. Normally this is an issue for higher dimensional problems, but I though it might be of interest to you anyway.
