[site]: crossvalidated
[post_id]: 478361
[parent_id]: 
[tags]: 
Backpropagation through time for stacked RNNs

I was able to find the partial derivative of the cost function with respects to a single variable without much difficulty. However, this requires propagating backwards through the network for each parameter. Is there a way to do this by propagating backwards through the network once? For example, for a MLP, one could find the partial derivative with respects to the activation levels of neurones by propagation backwards only once, and then finding the partial derivatives of the weights and biases by applying the chain rule. Unfortunately, for a stacked RNN, this proved way less straightforward due to the parameters being the same at each time step. I think it might have something to do with ordered derivatives but canâ€™t seem to find much resources on the topic.
