[site]: crossvalidated
[post_id]: 282217
[parent_id]: 282048
[tags]: 
Some kind of basic feature selection would help certainly. You can select for the most important n-grams (uni- to whatever you want), e.g., with $\chi^2$ selection. A bit "deeper" approach is to detect collocations on an associated corpus, or chunking noun- and, possibly verb-phrases, and use those only those collocations/chunks, instead of working over all n-grams. A similar approach generating even less features is to extract keywords, e.g., with the TextRank keyword extraction algorithm. Finally, keywords can be further enriched again by adding entities from Named Entity Recognition taggers or even joining entities and keywords with dependencies. As you see, there are plenty of options; the real question is how far you are willing to go or technically can go, in terms of NLP-wise pre-processing of your texts (PoS tagging, phrase chunking, NER tagging, or even dependency parsing for some very complex features).
