[site]: stackoverflow
[post_id]: 2113723
[parent_id]: 2113557
[tags]: 
If you are going with a dictionary-based encoding where the decoder has the dictionary as well, there's no absolute minimum. For a frequency-based encoding, however, What you need is to calculate the entropy: E = -(P(0) * log_2(P(0)) + P(1) * log_2(P(1))) E = -(1004/1024 * log_2(1004/1024) + 20/1024 * log_2(20/1024)) E = 0.1388005 So each bit of the input should require 0.1388005 bits of the output on average. In total: 0.1388005 * 1024 = 142.1317 bits. This means that in theory, using an optimal algorithm you can encode any string with 1004 zeros and 20 ones (or the other way around) using 143 bits.
