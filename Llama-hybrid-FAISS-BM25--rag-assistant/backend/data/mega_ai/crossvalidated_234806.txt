[site]: crossvalidated
[post_id]: 234806
[parent_id]: 220816
[tags]: 
So the reason you want a baseline is that $R_\tau$ can be very different from one episode to another under the policy you are training (equation no 3 in the paper). Subtracting a baseline from that return helps reducing the variance and stabilizing the algorithm. See for example John Schulman's MLSS Lecture or David Silver's lecture on policy gradient methods . When training policy gradient methods the baseline can be whatever approximates best the expected reward onward from the step you are in. A natural baseline is the state value function, the value of being in that state $V(s)$ . Usually these functions are learned by parametric estimators (linear function approximators, neural networks, etc.). More on the general theory of using baselines in policy gradient methods you can find here . In the paper you linked they are using a separate LSTM that learns to predict the expected episodic return from that step onward, $R_\tau$ so it is only natural that the objective to be minimized is the squared mean error between the actual return $R_\tau$ and the value predicted by the baseline LSTM, $b_\tau$ . The features the baseline LSTM is using are the full tape at the first time step and then the exact input the RL-LSTM is receiving at each step which is a fairly complex statistic (past actions, current memory, current input, hidden state of RL-LSTM). If you are asking if this is a sufficient statistic to predict the episodic return $R_\tau$ , well, it seems to be working :). For example a similar baseline was learned in this attention model + implementation .
