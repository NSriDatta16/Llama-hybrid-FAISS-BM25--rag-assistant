[site]: crossvalidated
[post_id]: 269397
[parent_id]: 269396
[tags]: 
As long as you are subtracting the two squared errors for each test from each other and calculating the average difference in performance you shouldn't have any issues as your sample size is big enough to assume the mean of the differences is normally distributed. Formally you are testing whether $\bar\epsilon'^2$ is significantly different from zero. Where $\bar\epsilon'^2$ is the average difference in the squared error between your two algorithms. Formally: $H_{0}: \bar\epsilon'^2=0$ (null hypothesis) $H_{A}: \bar\epsilon'^2\neq 0$ (alternative hypothesis) Where $\bar\epsilon'^2=\Sigma_{i}(\epsilon_{ai}^2-\epsilon_{bi}^2)/n$ and $\epsilon_{ai}^2$ is the squared error of algorithm a for on the $i_{th}$ test set and $\epsilon_{bi}^2$ is the squared error of algorithm b for that same test. Make sure you standardise $\bar\epsilon'$ when testing.
