[site]: datascience
[post_id]: 56603
[parent_id]: 56601
[tags]: 
All layers of a neural network take part in the back-propagation process. This includes the convolutional layers and the pooling layers. In general, every step of the network that the input has to go thru, the back propagation goes thru as well (in reverse order). However, not all layers contain trainable parameters. For example standard pooling layers (max-pooling, average-pooling) and standard activation layers (sigmoid, ReLU, softmax) don't have any parameters to adjust. They still take part in the back propagation, contributing their partial derivatives, but they just have no weights that can be updated. The convolutional layers do contain weights that are updated during the process (the parameters of the filter and their bias). Note: I assume that what you refer to as "feed forward part", are the fully-connected layers that are usually placed as the final layers of a network. In standard CNNs, all the network is a "feed forward part", (including convolutional layers) it just means that the input goes thru a sequential pipeline until becoming the final output.
