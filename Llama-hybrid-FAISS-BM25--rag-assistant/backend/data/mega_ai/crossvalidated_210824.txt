[site]: crossvalidated
[post_id]: 210824
[parent_id]: 209477
[tags]: 
You didn't say which type of RNN you use: LTSM (context layer and gates) or classic(only context layer) RNN. Concern 1: I use in my work, batches of 128 sequences and used sequence sizes of 100 time points. When I test the dataset I use batches of 1000 sequences of size 100 time points, and I don't have problems with memory. You size your batch as much memory you have, and this applies to any kind of neural net..recurrent, convolutional, etc. Concern 2: This is problem of missing/corrupt data in a dataset. I wouldn't remove the time points from the sequence just because the data is corrupt, but fix the data. One easy way to fix the data is to compute an average for that feature on the entire dataset, and put it in the dataset where it is missing or corrupt.
