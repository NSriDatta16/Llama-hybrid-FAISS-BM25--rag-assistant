[site]: crossvalidated
[post_id]: 315546
[parent_id]: 310062
[tags]: 
Short answer: When reducing the size of the range for a linear/polynomial fit (which is only an approximation of the true relationship) then you change the characterization of the fit from underfitting - to correct fitting - to overfitting. Long answer: Why does changing the range have an effect The change of the range has an effect of changing the prediction at A=150... because the polynomial model is not accurately describing the data . This leads to, large residuals (which are systematic errors, and not noise), with unreliable behavior and the range plays a role in this. Your polynomial fit in the large range is bad . This leads to unreliable behavior. Especially since you use a linearized fit $$log(log(A)) \sim polynomial(V)$$ you get that the residuals at low amplification have increased weight. For instance, a difference between 1.001 and 1.0001 is tiny on a linear scale but 33% difference on a log(log())-scale. A difference between 1000 and 1100 is large on a linear scale, but les than 1% on a log(log()) scale. Your polynomial fit in the short range is very good. You could say that locally the relationship can be well approximated by a linear relationship or low order polynomial. So this basically makes that the shift from the one range (large) to the other (small) is changing (improving) your estimate. The image below demonstrates this effect better. Note the difference between the green curve which fits well, and the red curve which fits badly. This difference is a bit less clear in your diagrams which shows only one single point, and does not explain what is going on (what is going on is that the red curve polynomial fit is not a good one) Note that the red curve fits actually reasonably in the lower amplification range, that is because the effect explained above: the curve is linearized which increases the weight of the residuals at low values (The standard Excel has the same issue and linearizes when performing an exponential curve fit, which is not always providing satisfying results). I have added an additional curve (the gray broken line), based on a differential equation, anticipating an answer on DeltaIVs question If in this problem I regress $x$ on $y$ instead than $y$ on $x$, do I need to use an error-in-variables model? . The function $A \sim \frac{1}{1-\left( \frac{V}{\theta_0} \right)^{\theta_1}}+\theta_2 +\epsilon$ that is mentioned in that question is not converging well, and trying to fit it anyway seems to be an ill-posed problem. What I found is that an alternative does behave much better, namely $\partial A (a (A-1)^b + c (A-1)^d)^{-1} = \partial V$, fits quite well. What is the best thing to do in this situation Since you have very little noise and the curve is smooth you can use your polynomial fit in the short range, without much troubles . You could put in place some diagnostic checks to see if the fits behave well and improve robustness (e.g. check if there are no outliers that may incidentally mess up your results). The polynomial fit is already such a control to improve robustness. Contrast this with an even simpler and still good model, which would be to calculate the voltage at A=150 just by using the line between the two data points around it. This works very well with no errors, except if occasionally one of those points has an error. If you would have more noise, such that fitting only a few points in a small range is creating a large error, then you might like to use more of the surrounding data. However, this only helps if the model is good. You could say that the use of the larger data range to predict the value at a single point, is effectively extrapolating. This requires you to be much more careful. The fit in the small range can be seen as interpolating. A note on the use of mixed effects. I wonder whether your use of mixed effects is appropriate (not that in your case it creates so much difference). Mixed effects means that your model assumes that the effects of the different device series-numbers is a random effect and that it follows a normal distribution. This is not correct to do when this effect is not a normally distributed random error. For instance if you have a few devices with errors, the mixed effect model is going to predict these errors to be closer to the average than reality (based on a less effective model to detect differences between devices, because it is maximizing a likelihood which assumes a normal distribution of the errors and thus you say it is likely for the errors to be zero and push your estimates to an average). The small R code below demonstrates the effect of random effects. > # simple example to demonstrate what happens with random effects > > # data > x t > # model > fit > # coefficients tend to be closer to the mean coefficient > coef(fit) $t (Intercept) 1 -2.280899 2 2.532367 3 9.848532 attr(,"class") [1] "coef.mer" > > # group residuals do not sum up to zero > residuals(fit)[c(1,3,5)]+residuals(fit)[c(2,4,6)] 1 3 5 -0.43820248 -0.06473446 0.50293694 >
