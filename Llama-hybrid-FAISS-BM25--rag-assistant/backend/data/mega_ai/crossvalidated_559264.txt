[site]: crossvalidated
[post_id]: 559264
[parent_id]: 559251
[tags]: 
Great question! To put it briefly, "Gradient Free Learning" (i.e. "metaheuristics", as pointed out by @user0123456789) is usually used when the "gradient" (i.e. derivative) of the loss function can not be evaluated. This can occur in instances such as : The derivative of the loss function does not exist (e.g. contains "indicator functions", piecewise functions) The derivative of the loss function exists, but is very costly to evaluate (e.g. I have heard talks in which gradient free optimization techniques were suggested for various problems involving reinforcement learning) Discrete Combinatorics/Optimization problems (this is kind of related to the first point, but imagine trying to optimize functions in which the inputs are a set of discrete objects and the output is a value associated with different inputs - for example: travelling salesman problem, knapsack optimization, scheduling, etc.) Gradient Free Optimization Techniques (e.g. Evolutionary Algorithms, Genetic Algorithm, Simulated Annealing, Particle Swarm, etc.) are sometimes preferred for certain types problems such as "games", in which optimal strategies are developed by mutating and combining random strategies according to their performance with respect to some target (e.g. https://en.wikipedia.org/wiki/Neuroevolution_of_augmenting_topologies , https://www.youtube.com/watch?v=OGHA-elMrxI ) The other note that I wanted to add was that in situations where the gradient of the loss function can be evaluated (e.g. classic MLP neural networks), I think there might be some theoretical results that guarantee the probabilistic convergence of Stochastic Gradient Descent (i.e. the opposite of gradient free learning) to a global maximum provided infinite iterations (I could be wrong about this) - with gradient free optimization techniques, as far I know there is no such guarantee (over here, I myself asked a question about the "Schema Theorem", which uses Markov Chains to supposedly guarantee an improvement in results as the number of iterations in the Genetic Algorithm increases https://math.stackexchange.com/questions/4295279/does-the-following-computer-science-optimization-theorem-have-a-proof ). To sum everything up - chances are that if the derivative of your loss function "exists", try using classical gradient based optimization techniques. If the derivative does not exist, consider using Gradient Free based techniques. For example, over here I asked a question about identifying "clusters" in a dataset such that the "proportion of zeros in all columns for a given cluster" is minimized. As far as I can think, there is no standard "gradient" in this problem, making it an ideal choice for "gradient free optimization techniques": https://or.stackexchange.com/questions/7488/mixed-integer-programming-optimization-using-the-genetic-algorithm Hope this helps!
