[site]: crossvalidated
[post_id]: 316244
[parent_id]: 316163
[tags]: 
I don't think the use of either z standardization, nor PCA, nor both (which always seems a bit redundant, and often does very odd transforms) on data with many 0s is a good idea. The zeros usually have some clear semantic, and these transformations will destroy such properties. Don't approach this as a black box and just stack function calls. You need to find a statistical model that explains the data. Your transformation backwards, that is the generation process you assume. So you begin with k values, add low-variance iid Gaussian noise. Then scale and rotate the data, scale and translate again, etc. Instead, you want a model like "with p the value is zero. With 1-p it is drawn from " and then argue about these differences being random by chance to get a similarity function.
