[site]: datascience
[post_id]: 37288
[parent_id]: 37287
[tags]: 
You can use KFold cross validation if you want to average the results of the training. sklearn.model_selection.KFold This will split your data in a specified number of folds (k) and train your data on all but one folds, then validate on the last fold. This operation is done k times and the results are averaged out. Normally, how I do is: Train/Test split Model selection + Hyperparameters tuning using KFold on training set Retrain the final model on the whole training set Evaluate on the test set Note that if you want to check whether your split was 'lucky' or 'unlucky', you can still change the seed, or not give a seed at all and compare the results with different runs. [EDIT] As stated in the comments below, the seed is controlled by the random_state argument and is mainly there for reproducibility. If you want a different train/test split at each run, just leave the default as is. It's always good to check at least twice to see whether you've been particularly lucky or not but it never happened to me ! :)
