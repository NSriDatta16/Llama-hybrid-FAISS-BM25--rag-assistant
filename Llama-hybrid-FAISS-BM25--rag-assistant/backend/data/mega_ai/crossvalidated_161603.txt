[site]: crossvalidated
[post_id]: 161603
[parent_id]: 161278
[tags]: 
The issues that you report are fairly common with large numbers of predictor variables. Multiple regression provides relations of each of your predictor variables to your classification with the other predictor variables taken into account via a linear model. This leads to all sorts of interesting behavior, as seen in many Cross Validated pages including one on a predictor uncorrelated with outcome becoming significant in multiple regression , on two orthogonal predictors affecting each others' significance in multiple regression , on the suppression effect , and even with the sign of a regression coefficient flipping after other predictors are included . If your main interest is in predicting classification of future cases you are probably better off erring on the side of keeping too many predictors. That's even true for those that do not appear "significant" on their own, as their exclusion may affect the relations of the "significant" variables to the classification and hurt the overall performance of your model, as you have seen. If the number of cases relative to the number of predictors leads to worries about overfitting or about perfect separation in your logistic regression, you might consider a technique like ridge regression , combined with standard model validation techniques like cross-validation or bootstrapping, to minimize those problems. The Elements of Statistical Learning , freely available, is a good starting point.
