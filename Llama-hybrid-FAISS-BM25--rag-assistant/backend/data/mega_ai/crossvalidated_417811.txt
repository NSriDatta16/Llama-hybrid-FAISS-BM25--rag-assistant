[site]: crossvalidated
[post_id]: 417811
[parent_id]: 
[tags]: 
Best way to pass distribution estimates as a feature into deep learning

I have been googling a lot and somehow cannot find a good answer. Lets say we have deep neural net model, arbitrary topology. Also we have 10 features and for each features we got 1000 observations for each time step. What is the best way to pass the data into our network? Perhaps some initial central moments, like mean, variance, skewness, kurtozis...? Or should we find the distribution that best explains our features (relying somehow on expert judgment), like lognormal and pass only the two parameters obtained with MLE? Any ideas or experiences? EDIT (on answer): The goal is binary classification. If we input whole 10*1000 as features we obtain the curse of dimensioanlity. Maybe I wasn't clear. We have 10 000 different observations at each time step, while we know that they come from 10 separate distributions. Learning a neural net with 10K input features is at least for my hardware currently impossible. We obtain those samples at each time step as an output of some meta model, and we have 10 different meta models. Sorry for unclear question before. SECOND EDIT: Perhaps we can teach 10 variational autoencoders, that will map our distributions to lets say 3 parameters and then manage to decode the information and we minimize KL loss. Has anyone tried that with unknown distributions? Best JJ
