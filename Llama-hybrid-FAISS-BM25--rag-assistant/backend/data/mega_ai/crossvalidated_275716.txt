[site]: crossvalidated
[post_id]: 275716
[parent_id]: 
[tags]: 
LSTM for classification

I have a dataset which consists of $n_\text{samples}$ different measurements. Each measurement contains $n_\text{features}$ features. These features are for example average_mass , average_charge , speed , position_x , position_y , ... In addition, each measurement was done at a certain time $t$ which is also known. I want to classify the measurements into $C$ classes. For this, I obtained a training set with labels $i \in \lbrace1,2,3,\ldots, C\rbrace$. Therefore, this can be treated as a supervised learning task. Lets say all $n_\text{samples}$ are labeled and the classifier should be able to work on new unseen data. One could think now of different methods to do build such a classifier: Treat each of the $n_\text{samples}$ measurements independently and show them to the classifier. E.g. one could build a RandomForest with the above mentioned features and estimate its performance by using cross validation. This ansatz would completely ignore the time informaion. Use a classifier which makes explicit use of the time information. E.g. one could reshape the feature matrix of shape [n_samples, n_features] into a 3D tensor of shape [n_samples', n_timesteps, n_features] where n_timesteps is the number of consecutive timesteps which are considered as one sequence. n_samples' is now smaller than n_samples , since the orginal n_samples are grouped together in sequences of length n_timesteps . To fix some numbers, lets say there are $C=10$ classes, $n_\text{samples}=10^5$, $n_\text{timesteps}=4$, $n_\text{features}=20$ . For the second method, I used an LSTM. The keras code for the model looks like this: model = Sequential() model.add(LSTM(64, input_shape=(n_timesteps, n_features), return_sequences=True, stateful=False)) model.add(LSTM(32, return_sequences=True, stateful=False)) model.add(LSTM(32, return_sequences=False, stateful=False)) model.add(Dropout(0.2)) model.add(Dense(num_classes)) model.add(Activation("softmax")) model.compile(optimizer="adam", loss=self.loss, metrics=['accuracy']) It turned out that a RandomForest trained with approach 1 (i.e. ignore the time) works way better than the LSTM. I find this astounding as I assumed that since there is a correlation between the measurements in the time direction (position and speed at time 1 will certainly affect the position at time 2), a method which uses the time domain will outperform a classifier which ignores this extra information. Now I have the following questions: Most often LSTMs are used for time series prediction or seq2seq tasks (eg. text translations or speech recognition). Hence I wonder if an LSTM is even a good choice for my problem? Is my network design suitable or are there better ways to construct a LSTM network for my task? What are other alternatives, which make use of the temporal correlations? Different network types?
