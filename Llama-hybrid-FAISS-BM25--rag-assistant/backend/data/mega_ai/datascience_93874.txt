[site]: datascience
[post_id]: 93874
[parent_id]: 93859
[tags]: 
Vanishing gradients can occur because of adding so many layers to network. You can think neural networks as composite functions. During learning process gradients of loss function with respect to weights calculated according to chain rule, and because of large number of layers gradients can be so small by multiplications. It causes insufficient updates of weights towards optimum points of loss function. If your loss function barely decreasing, or there is no change at all then cause of this might be vanishing gradients. Activation functions like tanh, sigmoid also can cause this problem, because their gradients become zero in edge points. Skip connections can ease vanishing gradient problem, because when connections are skipped gradient calculation follow a shorter path, and less multiplications can ease vanishing gradient problem. Resnets use skip connections to ease vanishing gradient problem: https://en.wikipedia.org/wiki/Residual_neural_network . If you observe no decrease in loss function, then you can decrease number of layers gradually. It's a hyperparameter, and you can experiment with different numbers of layers to find optimum number which gives best results.
