[site]: crossvalidated
[post_id]: 93960
[parent_id]: 
[tags]: 
Statistical significance in time series and sub-series

So, when I did first year stats in undergrad, we did an experiment where we tampered with a bunch of coins, to see if it would cause a statistical difference in the results. This is a graph of the ratio of $heads:tosses$ for each series of flips: We took the full data set in each case, and found that there were no significant differences from a null hypothesis of 50% heads. Had we stopped at 100 flips, or 150 flips we would have probably concluded that the Cupped coin was significantly biased. Would this have been invalid, and why? In particular, does it mean anything that the ratio is outside the 95% confidence interval more than 5% of the time?
