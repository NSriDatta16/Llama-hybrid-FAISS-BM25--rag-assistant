[site]: datascience
[post_id]: 75708
[parent_id]: 
[tags]: 
neural network probability output and loss function (example: dice loss)

A commonly loss function used for semantic segmentation is the dice loss function. (see the image below. It resume how I understand it) Using it with a neural network, the output layer can yield label with a softmax or probability with a sigmoid. But how the dice loss works with a probility output ? The numerator multiply each label (1 or 0) of the predicted and ground truth. Both pixels need to be set to 1 to be in the green area. What is the result with a probability like 0.7 ? Does the numerator result in a floating number (ie with ground truth = [1, 1] and predicted = [0.7, 0], the "green" area of the numerator would be 0.7) ?
