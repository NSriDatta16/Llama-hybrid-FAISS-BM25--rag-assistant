[site]: crossvalidated
[post_id]: 162656
[parent_id]: 
[tags]: 
making sense of small eigenvalues

I have a huge dataset with about 1.000.000 matrix entries of size about 300.000 and ran a PCA on them, but the components and the eigenvalues are really small. I am unsure what this means. I ran an Incremental PCA (from sklearn 0.16.1) on the data with batch size 100. Already for the first increment the values were quite small and then kept decresing with every iteration. So for the normalized eigenvalues after looking at all the data I get something like this: array([ 0.01146837, 0.00927029, 0.0067319 , 0.00601581, 0.00507061, 0.00442723, 0.00411958, 0.00397337, 0.00384924, 0.00379049, 0.00339444, 0.00307697, 0.00298565, 0.00279872, 0.00273881, 0.00269498, 0.00264522, 0.00251268, 0.00237168, 0.0021606 , 0.00209025, 0.00203315, 0.00198865, 0.00195354, 0.00192039, 0.001854 , 0.0017387 , 0.00172961, 0.0016596 , 0.00156462, 0.00155386, 0.00150622, 0.00150157, 0.00139544, 0.00134342, 0.00131588]) Can anyone give some intuition what these values mean and why they are so small? Thank you
