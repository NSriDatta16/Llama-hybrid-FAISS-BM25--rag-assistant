[site]: stackoverflow
[post_id]: 5664374
[parent_id]: 5664123
[tags]: 
You build a compiler for a specific instruction set, which is a subset of a chosen an "instruction set architecture (ISA)". (Many instruction sets have I/O instructions, but compilers almost never generate these). There may be several different processor designs that execute this "instruction set architecture" which will work with specific instruction subset you have chosen. There are three kinds of evolutionary events occur in practice. You determine your compiler would be better if used some more instructions from the ISA. For instance, you might decide that the MULTIPLY instruction would allow your compiler to generate faster code than the subroutine call you have used for multiply in the past. In this case, you extend your compiler slightly. The owners of the ISA (Intel, AMD, IBM, ...) add entire new sets of instructions to the ISA. For instance, data parallel operations on a cache-line of data ("SIMD instructions"). You can decide to add some of these to your compiler. This event can be difficult, as new families of instructions generally make different assumptions about how data is layed out and processed. You find some completely different ISA that you want to handle. In this case, you are going to rebuild the back end of your compiler as the insturction set is completely different, in terms of what registers exist, how they are used, etc. Compiler builders often build the compilers to operated in stages. A last stage before generation of actual machine code typically represents the program as an abstract set of operations on pretty low level data (e.g., operations on fixed-word-size values) with fairly standard abstract operations (ADD, MULTIPLY, COMPARE, JUMP, CALL, STORE, LOAD, ...) that have no commitments to the actual ISA (especially no commitements about register or specific machine instructions). Doing it this way allows higher-level optimizations to be done independent of the ISA; just think of this as good modularization. The last few stages are specialized to the ISA; usually on stage to allocate registers, followed by a stage the pattern matches actual instructions against the abstract ones. There are entire books written on optimizing on the higher level, and other books written on the final code generation sates (and often books that address both in seperate chapters). [Both Aho&Ullman Dragon book, and Torczon's Engineering a Compiler are quite good books on both topics). There is a lot of technology allowing one to write down the final instruction sets and registers layouts, and will generate much of the last stages; GCC has such. That technology is complex and wont fit in this sentence; best to go read the books. Once you get a compiler working for the first ISA in this fashion, you can build a variant using the same technology. You end up with two physical compiler, one for each ISA. They share all the front end logic and absract code generation and optimization. They vary completely for the last stages. What you should understand is that building the compiler to take advantage of instructions sets is a complex process.
