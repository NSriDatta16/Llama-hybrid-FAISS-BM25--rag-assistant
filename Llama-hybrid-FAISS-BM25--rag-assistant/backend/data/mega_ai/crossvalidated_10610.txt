[site]: crossvalidated
[post_id]: 10610
[parent_id]: 10608
[tags]: 
Feature selection does not necessarily improve the performance of modern classifier systems, and quite frequently makes performance worse. Unless finding out which features are the most important is an objective of the analysis, it is often better not even to try and to use regularisation to avoid over-fitting (select regularisation parameters by e.g. cross-validation). The reason that feature selection is difficult is that it involves an optimisation problem with many degrees of freedom (essentially one per feature) where the criterion depends on a finite sample of data. This means you can over-fit the feature selection criterion and end up with a set of features that works well for that particular sample of data, but not for any other (i.e. it generalises poorly). Regularisation on the other hand, while also optimising a criterion based on a finite sample of data, involves fewer degrees of freedom (typically one), which means that over-fitting the criterion is more difficult. It seems to me that the "feature selection gives better performance" idea has rather passed its sell by date. For simple linear unregularised classifiers (e.g. logistic regression), the complexity of the model (VC dimension) grows with the number of features. Once you bring in regularisation, the complexity of the model depends on the value of the regularisation parameter rather than the number of parameters. That means that regularised classifiers are resistant to over-fitting (provided you tune the regularisation parameter properly) even in very high dimensional spaces. In fact that is the basis of why the support vector machine works, use a kernel to tranform the data into a high (possibly infinite) dimensional space, and then use regularisation to control the complexity of the model and hence avoid over-fitting. Having said which, there are no free lunches; your problem may be one where feature selection works well, and the only way to find out is to try it. However, whatever you do, make sure you use something like nested cross-validation to get an unbiased estimate of performance. The outer cross-validation is used for performance evaluation, but in each fold repeat every step in fitting the model (including feature selection) again independently. A common error is to perform feature selection using all of the data and then cross-validate to estimate performance using the features identified. IT should be obvious why that is not the right thing to do, but many have done it as the correct approach is computationally expensive. My suggestion is to try SVMs or kernel logistic regression or LS-SVM etc. with various kernels, but no feature selection. If nothing else it will give you a meaningfull baseline.
