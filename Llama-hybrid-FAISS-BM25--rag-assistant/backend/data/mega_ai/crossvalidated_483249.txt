[site]: crossvalidated
[post_id]: 483249
[parent_id]: 483195
[tags]: 
There is a kind of chain rule, $$\DeclareMathOperator{\KL}{KL} \KL(P \lVert Q)=\sum_y P(y) \KL( P(\cdot \mid y) \lVert Q(\cdot \mid y) ) + \KL(P_Y \lVert Q_Y) $$ where $P_Y, Q_Y$ denotes the marginals. So this is the divergence of the marginals plus the average of the conditional divergences. In the case of independence this simplifies to the sum of the marginal divergences. The proof is a simple exercise in conditional probability: $$ \KL(P \lVert Q)= \sum \sum P(x,y) \ln\frac{P(x,y)}{Q(x,y)} = \\ \sum\sum P(x\mid y) P(y) \ln\frac{P(x\mid y) P(y)}{Q(x\mid y)Q(y)} =\\ \sum\sum P(x\mid y) P(y) \left( \ln\frac{P(x\mid y)}{Q(x\mid y)} + \ln\frac{P(y)}{Q(y)} \right)=\\ \sum_y P(y) \sum_x P(x\mid y) \ln\frac{P(x\mid y)}{Q(x\mid y)} + \sum_y P(y)\ln\frac{P(y)}{Q(y)} \cdot \underbrace{\sum_x P(x\mid y)}_{1} =\\ \sum_y P(y) \KL( P(\cdot\mid y)\lVert Q(\cdot\mid y) + \KL(P_Y \lVert Q_Y) $$
