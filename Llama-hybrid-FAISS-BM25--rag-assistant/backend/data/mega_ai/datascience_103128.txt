[site]: datascience
[post_id]: 103128
[parent_id]: 
[tags]: 
The case of (1,478) dim and parameters of neural network to find out

colleagues, actually I am kind'a new to NN, but hard trying.. I have data: Index : 40073 entries (excluded from training, UID) Columns : 484 entries dtypes: bool(468), float64(2), int64(13), object(1) I used only 478 arguments. The Y is moneySpend which can be >= 0 The code is below: newDropped = df.drop(["moneySpend","userAgent", "secondsToBuy", "hoursToBuy", "daysToBuy", "platform"], axis = 1) x_train, x_test, y_train, y_test = train_test_split(newDropped, df["moneySpend"], test_size=0.25, random_state=547) model = Sequential() dnn1.add(Dense(16, input_dim=478, activation='relu')) dnn1.add(Dense(8, activation='relu')) dnn1.add(Dense(1, activation='linear')) model.compile(loss='mse', optimizer='adam', metrics=['accuracy']) tb_callback = tf.keras.callbacks.TensorBoard('./logs', update_freq=1) history = model.fit(x_train.astype('int'), y_train.astype('int'), validation_split=0.33, epochs=100, batch_size=5, callbacks=[tb_callback]) I have kind'a big losses, and no idea what number of neurons to use at each Dense layer (should I use Dense or LSTM?) and what proper number of layers and activation functions to use. Regularization is making training worse, so I turned it off. Any good advices for me, if you can?
