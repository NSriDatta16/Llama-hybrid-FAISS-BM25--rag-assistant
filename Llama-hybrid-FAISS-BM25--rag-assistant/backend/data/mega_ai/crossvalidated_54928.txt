[site]: crossvalidated
[post_id]: 54928
[parent_id]: 54915
[tags]: 
While I don't have a proof for this, I doubt that PCA is a good method to use on binary data. It is really meant for continuous variables as far as I can tell. And actually, most clustering methods are meant so, too! But given that there can be at most $2^5=32$ different values in your data set, why don't you just use the most frequent groups, then assign the remaining observations to the group with the lowest Hamming distance? Alternatively, you could use jaccard similarity for example, and do hierarchical clustering. This approach has a semantic meaning for binary data - Jaccard similarity is well understood. But don't forget: you can have only 32 different records - with 100, you must have plenty of duplicates. At these few attributes, your cluster hierarchy will likely degenerate to levels of "duplicates", "1 difference", "2 differences", "3 differences", "4 differences" and "inverse".
