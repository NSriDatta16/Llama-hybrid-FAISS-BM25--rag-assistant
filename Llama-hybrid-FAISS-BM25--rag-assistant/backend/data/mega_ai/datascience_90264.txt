[site]: datascience
[post_id]: 90264
[parent_id]: 
[tags]: 
Why use embeddings when data about the categories are abundant?

Suppose I'm making a movie recommendation engine and I have tons of data about the movies like genre, ratings, release info, earnings, etc. It makes sense to create an embedding layer for the users since I have no additional info on them and the embeddings will learn the geometry of the users based on their watch preferences. My gut still tells me to make an embedding layer for the movie index as well and concatenate it with the movie-related numerical features. Why though? Can't we just pass in the numerical features of the movies for the model to differentiate? Are the embeddings supposed to capture some additional latent information perhaps not captured in the raw metadata? I know the embeddings get us the nice feature of being able to compare similar movies but I could just do the same this with my numerical features right? This isn't specific to recommendations but generally, why would you create an embedding vector for categorical features when you already have a lot of descriptive data about each item in that categorical feature?
