[site]: crossvalidated
[post_id]: 469091
[parent_id]: 
[tags]: 
Statistical Significance with Multivariate Logistic Regression and Feature Selection

Background: Given a dataset with output y (Bernoulli distribution) and features x_0, x_1, x_2, ..., x_n I want to test which variables are statistically significant to the predictor y . From what I remember from my stats classes, I can run these through a logistic regression model ( glm in R) and get p-values for their statistical significance. Now I understand that given many variables in a model, I should apply a correct such as the Bonferroni correction, etc. to prevent over optimism in my results. This it not my question, as it seems straightforward. Question: My question is rather, if I try all subsets of my features x_0, x_1, x_2, ..., x_n in different combinations, will I need to apply any corrections to account for that? I see that the p-values vary slightly as I add or drop variables from my feature list, and I don't want to accidentally end up p-hacking. Similarly, does selecting a model using AIC/BIC and then putting those variables into the model for hypothesis testing affect the validity of my results? Am I approaching this wrong? Should I be defining these hypothesis beforehand and then testing them? Is it bad faith to throw it in a model and see what is significant? I come from a machine learning background, where I feel like it is commonplace to throw all the data in and see what comes out.
