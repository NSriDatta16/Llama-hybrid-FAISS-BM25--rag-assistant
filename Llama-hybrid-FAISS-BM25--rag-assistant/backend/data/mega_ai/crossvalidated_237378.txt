[site]: crossvalidated
[post_id]: 237378
[parent_id]: 
[tags]: 
Finding time-point when sample average start differing from zero

As the title say, I am after a way to find out the time-point when the average of an ordered sample start differing from zero. Here the details: I have two different psychological tests, and I have tested their association with age in my sample (N = 431). I found out using a mixed linear model that there is an interaction between the type of test and age, i.e. one of the two test has a steeper increase with age than the other. Looking at this scatter plot of the two correlations it is evident that the two test start to diverge between 12 and 15 years of age. What I would like to have is a "statistical hat" for this information. Something that would allow me to write something along this line: "The association between test 1 and age become significantly different (p I thought that one way to do that would be calculate the difference between the score of test 1 and the score of test 2 for every subject, order the observation by age (i.e. basically fitting the linear model TestDifference~Age) and then find out when the model stop crossing the zero line. The fact is that I don't have any idea about how to do this last step. Moreover, I am not completely sure that this is the good way of doing this. Have you got any suggestion ? Should I use a totally different approach ? An answer based on R would be really appreciated.
