[site]: datascience
[post_id]: 15764
[parent_id]: 15352
[tags]: 
What you are looking for is called a metric (or distance, or similarity measure) for HMMs (some people would say SVM kernel for HMMs). Somehow, if you have a distance, you can cluster the HMMs. The most classical distance, defined by Rabiner and Juang is a Monte-Carlo approximation of a Kullback-Leibler divergence for HMM. It requires a long observation sequence $O$ generated from the first HMM $\lambda_1$ and is then computed as: $$ D_{KL}(\lambda_1||\lambda_2) = 1/T * (log(P(O|\lambda_1)-log(P(O|\lambda_2))$$ The advantage of this is that it does not imply any constraint on the number of hidden state. However, $O$ needs to be a long sequence if you want accurate results. There is some research on the topic but not too much, and even less for HMMs with different topology (which is another reason why Google does not give much results...) Edit: This paper proposes a distance measure that does not require the number of hidden states to be the same but is only valid for uni-dimensional observations. A new distance measure for hidden Markov models by Zeng, Duan, and Wu in Experts Systems with Applications 37, p. 1550-1555, 2010. It is rather easy to implement.
