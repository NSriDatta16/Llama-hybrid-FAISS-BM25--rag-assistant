[site]: crossvalidated
[post_id]: 62397
[parent_id]: 
[tags]: 
GARCH, or Generalized AutoRegressive Conditional Heteroskedasticity is a generalization of the ARCH model. It is used to model the time-dependent conditional variance (volatility) of financial time series. A GARCH model represents the current volatility in terms of both past volatility and past errors. E.g. in the standard GARCH( $q,p$ ) model we have $$ \sigma_t^2 = \omega + \sum_{i=1}^q\alpha_i\varepsilon_{t-i}^2 + \sum_{j=1}^p\beta_j\sigma_{t-j}^2 $$ where $\varepsilon_t$ is the error of the conditional mean model and $\sigma_t^2$ is its conditional variance. A GARCH model defines the conditional variance but not the conditional mean of a time series. A GARCH-type conditional variance specification can be combined with an arbitrary specification for the conditional mean, yielding e.g. an ARIMA-GARCH model. The conditional variance equation in a GARCH model is deterministic (the variance is completely determined by lags of own values and of the error term), in contrast to Stochastic Volatility (SV) models. As such, the conditional variance itself does not follow an ARMA model (a frequent misconception), but the squared error term does. GARCH models are mostly used for forecasting return distributions and variances and are instrumental in estimating Value at Risk, Expected Shortfall and other financial risk measures.
