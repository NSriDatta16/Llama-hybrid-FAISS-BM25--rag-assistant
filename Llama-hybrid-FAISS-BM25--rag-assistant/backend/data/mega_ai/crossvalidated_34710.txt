[site]: crossvalidated
[post_id]: 34710
[parent_id]: 34706
[tags]: 
Based on your follow up comment it sounds like you are trying to estimate the coverage probability of a confidence interval when you assume constant error variance when the true error variance is not constant. The way I think about this is that, for each run, the confidence interval either covers the true value or it doesn't. Define an indicator variable: $$ Y_i = \begin{cases} 1 & {\rm if \ the \ interval \ covers} \\ 0 & {\rm if \ it \ does \ not } \end{cases}$$ Then the coverage probability you're interested in is $E(Y_i) = p$ which you can estimate by the sample proportion which I think is what you're proposing. How do I set the number of iteration runs? We know that the variance of a Bernoulli trial is $p(1-p)$, and your simulations will generate IID bernoulli trials, therefore the variance of your simulation based estimate of $p$ is $p(1-p)/n$, where $n$ is the number of simulations. You can choose $n$ to shrink this variance as much as you want. It is a fact that $$p(1-p)/n \leq 1/4n$$ So, if you want the variance to be less than some pre-specified threshold, $\delta$, then you can ensure this by choosing $n \geq 1/4\delta$. In a more general setting, if you're trying to investigate properties of the sampling distribution of an estimator by simulation (e.g. it's mean and variance) then you can choose your number of simulations based on how much precision you want to achieve in an analogous fashion to that described here. Also note that, when the mean (or some other moment) of a variable is the object of interest, as it is here, you can construct a confidence interval for it based on the simulations using the normal approximation (i.e. the central limit theorem), as discussed in MansT's nice answer. This normal approximation is better as the number of samples grows, so, if you plan on constructing a confidence interval by appealing to the central limit theorem, you will want $n$ to be large enough for that to apply. For the binary case, as you have here, it appears this approximation is good even when $np$ and $n(1-p)$ are pretty moderate - say, $20$. Is it true that larger than necessary replications may result in spurious biases? If so, how is that? As I mentioned in a comment - this depends on what you mean by spurious. Larger numbers of simulations will not produce bias in the statistical sense, but it may reveal an unimportant bias that is only noticeable with an astronomically large sample size. For example, suppose the true coverage probability of the misspecified confidence interval were $94.9999\%$. Then, this isn't really a problem in a practical sense, but it you may only pick up this difference if you ran a ton of simulations.
