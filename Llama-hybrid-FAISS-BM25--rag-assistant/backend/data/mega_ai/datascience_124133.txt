[site]: datascience
[post_id]: 124133
[parent_id]: 124127
[tags]: 
The second subscript refers to the position within the sequence of the contextual embedding being computed. The inner subscript range refers to what input tokens are usted as input for the previously mentioned computation. That is, to generate each position, the transformer decoder can use the input tokens from the same position and before. Take into account that the input of the transformer decoder is shifted one position to the right by prepending a (beginning of sequence) token, which implies that, speaking about token positions within the original unshifted sequence, each prediction depends on the previous ones (and not on the current one or the following ones, which are not available at inference time). Note that Transformer decoders are meant for generating text. At each position, we are predicting the next token. The final multiplication by the embedding matrix simply computes a distance between the contextual embedding and each of the embedding vectors, and then the softmax normalizes such distances into probabilities. The closer the contextual embedding to one of the token embeddings, the more probable to be the next token. And, surely enough, if you use a Transformer decoder specifically to get token representations, you should take the representation of the previous position. However, Transformer decoders are seldom used to compute token representations because their computation only depends only on the previous tokens. Transformer encoders are the norm when we want to compute token representations.
