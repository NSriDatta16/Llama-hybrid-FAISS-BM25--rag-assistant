[site]: datascience
[post_id]: 124914
[parent_id]: 
[tags]: 
Minimizing error in cosine similarity

Presume I have a vector space, and I am attempting to compress it into a latent vector space, while minimizing error in cosine similarity between entries. Suppose that I know the actual cosine similarity scores between the latent space. i.e. Let $X \subset \mathbb{R}^n$ be my set of vectors, and $r: X \times X \to [-1, 1]$ be a function. I am trying to find a function $f: X \to \mathbb{R}^m$ such that the error $$\sum_{(x, y) \in X \times X} |sim(f(x), f(y)) - r(x, y)|$$ is minimized. What is a good way of finding $f$ given $X$ , $r$ and $m$ ? I am comfortable using gradient descent or neural networks.
