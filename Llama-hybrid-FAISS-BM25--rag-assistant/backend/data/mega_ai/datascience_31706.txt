[site]: datascience
[post_id]: 31706
[parent_id]: 31685
[tags]: 
To handle class imbalance, do nothing -- use the ordinary cross-entropy loss, which handles class imbalance about as well as can be done. Make sure you have enough instances of each class in the training set, otherwise the neural network might not be able to learn: neural networks often need a lot of data. Assuming you care about global accuracy (rather than the average of the accuracy on each individual class, say), I wouldn't bother with a weighted cross-entropy loss or duplicating images. Your training sounds rather small. To deal with that, you might try starting from an existing pre-trained model and fine-tune the last few layers. Also use image augmentation.
