[site]: crossvalidated
[post_id]: 629175
[parent_id]: 629106
[tags]: 
TO answer your question: yes, from a scientific perspective it is ok to build hundreds of models. For example in fMRI data analysis we fit one model per voxel (3d pixel) per subject in the brain image, which can be around 500k voxels, but it can be much more based on the resolution of the machine. The thing that makes it acceptable/unacceptable is how and if you correct for multiple comparisons. The other things you've mentioned are also true. Yes, if you want to fit many models, it will be time consuming, but you can easily parallelize it, since each model can be run separately. Running dimensionality reduction like PCA/ICA/NNMF etc. is also ok, and commonly done in these situations. What I suggest is to do whatever is commonly done with the data modality you are working with and go from there.
