[site]: crossvalidated
[post_id]: 354719
[parent_id]: 214632
[tags]: 
Neural network optimization is often like traversing a narrow canyon: if you take a step too large in the wrong direction, you can end up bouncing off of the canyon floor up the side and get stuck well outside of the canyon. I think that's what's happened here. Your network made a "misstep" and then got stuck. Two common causes for this are gradient descent step size is too large; gradients can be "too large" for some steps. Gradient clipping can help.
