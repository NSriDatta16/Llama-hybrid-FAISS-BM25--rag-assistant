[site]: datascience
[post_id]: 27142
[parent_id]: 25987
[tags]: 
The issue is that you are not introspecting properly the feature importances. for x in bbc.estimators_: print(x.named_steps['classifier'].feature_importances_) [ 0. 0. 0. 0. 1. 0. 0. 0.] [ 0. 0. 0. 0. 0. 1. 0. 0.] [ 0. 1. 0. 0. 0. 0. 0. 0.] [ 0. 0. 1. 0. 0. 0. 0. 0.] [ 1. 0. 0. 0. 0. 0. 0. 0.] [ 0. 0. 0. 0.5 0. 0. 0. 0.5] [ 0. 0. 1. 0. 0. 0. 0. 0.] [ 0. 0. 0. 1. 0. 0. 0. 0.] [ 0. 0. 0. 0. 1. 0. 0. 0.] [ 0. 0. 1. 0. 0. 0. 0. 0.] As you can see, the XGBoostClassifier reports the feature importances only on 8 features as defined in the parameters list of the BalancedBaggingClassifier. Those 8 features presented to each XGBoostClassifer are in fact randomly selected for each estimator of the ensemble. So you have to first find which features were subsampled and given to each XGBoostClassifier and then indirectly find the real feature importances. names = np.array(names) for x, feat_sel in zip(bbc.estimators_, bbc.estimators_features_): feat_imp = np.nonzero(x.named_steps['classifier'].feature_importances_) print(names[feat_sel[feat_imp]]) ['x11'] ['x11'] ['x7'] ['x11'] ['x11'] ['x16' 'x3'] ['x7'] ['x6'] ['x7'] ['x7']
