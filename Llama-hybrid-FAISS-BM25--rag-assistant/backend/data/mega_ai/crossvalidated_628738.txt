[site]: crossvalidated
[post_id]: 628738
[parent_id]: 
[tags]: 
Assign weights to examples in a highly imbalanced dataset

I have a highly imbalanced dataset and I'd like to train a simple ANN classifier on it. My model currently is a simple 2-layer feed-forward neural network with ReLU activation in between. After a few iterations, the model already "decides" that outputting the most seen labels is good enough and stops learning. To cope with this problem, I'd like to sample the examples in a way that each batch approximately contains the same number of examples of each class. My labels are: labels = np.array([0] * 295 + [1] * 1321 + [2] * 1277 + [3] * 4477 + [4] * 457280) In order to sample the rarest class the most, my current class weights are defined as follows: classes = np.unique(self.labels) counts = np.sum(self.labels[:, np.newaxis] == classes, axis=0) frequencies = counts / counts.sum() class_weights = 1 - frequencies class_weights /= class_weights.sum() However, because of the high imbalance, the weights of classes 0, 1, 2 and 3 are almost equal. Consequently, when I randomly sample a batch, label 4 makes up about 50% of it on average, whereas label 0 only makes up 2%. I also tried a logarithmic weight, hoping it would discount large counts over-proportionally: class_weights = np.log(frequencies) class_weights /= class_weights.sum() This weight is slightly better, but it significantly prefers label 3, which makes up about 45% of examples in a randomly sampled batch, whereas label 0 is around 5% now. Which function could I use to assign roughly equal weights to each class, such that each sampled batch contains about 20% of examples of each class?
