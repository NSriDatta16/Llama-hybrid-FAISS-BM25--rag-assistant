[site]: crossvalidated
[post_id]: 134107
[parent_id]: 134097
[tags]: 
I'd start by doing a little "feature engineering". You know exactly what parts of the image carry semantic information: the positions of the hour, minute and (optionally) second hands. The rest of the image is irrelevant. You could manually mask it out, or you could subtract the average image from each one. If you have enough images (and consistent lighting), only the hands will remain. Now you just have to find the angle of each hand (to determine the time) and its length/width (to identify the hour/minute/second) hand. The Hough Transform (or one of its variants) is a standard technique for identifying lines in an image. Once you've got the two lines, it's trivial to convert their lengths and angles into a time. This seems like a reasonable starting point to which you could add progressively more sophisticated image processing/machine learning techniques to deal with e.g., variations in lighting or movement. You could feed Hough-like features into your model, for example. If you wanted to apply a less ad-hoc algorithm, I'd be tempted to think about it as a regression problem instead of a classification problem. Predicting a continuous value is going to be easier than performing a 720+ way classification (12 hours with 60 minutes each) and a prediction that misses the correct time by 1-2 minutes feels better than one which is off by 6 hours. As an added complication, time is circular on a watch: both 12:59 and 1:01 are reasonably close to 1:00.
