[site]: crossvalidated
[post_id]: 191071
[parent_id]: 190304
[tags]: 
I think the answer was already included in the paper you provided. Barmaley was right, the proof is pretty simple. However, if you don't like mathematical proof, there still some reasonable interpretations. The main purpose for $\xi$ in objective function is that we want to penalize the mis-classification(penalize according to their soft margin). As for those points classified correctly, NO REWARD, NO PENALIZE! e.g. in l2-svm, if the data was misclassification, $y_i(x_i^{T}w)$ will be negative. The constraint $y_i(x_i^Tw) \geq 1 - \xi_i$ ensures that $\xi_i$ must be positive, which means penalize the mis-classification. As for $y_i(x_i^{T}w) > 0$ which means those point were classified correctly, optimal $\xi$ for them is zero (optimal value for min $\xi^2$). That is exactly what we want! (NO REWARD, NO PENALIZE.) On the other hand, let's see why non-negative constraint for $\xi$ is essential for original svm model. Actually, it still punishes the misclassification, but for those correctly classified points will get infinity reward. i.e. if $y_i(x_i^{T}w) > 0$, $\xi_i$ can be $-\infty$ so as to minimize the objective function. To fulfil our goal, we must add nonnegative constraint for $\xi$ to let the model make sense.
