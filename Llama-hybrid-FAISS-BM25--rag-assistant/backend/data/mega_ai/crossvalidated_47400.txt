[site]: crossvalidated
[post_id]: 47400
[parent_id]: 47367
[tags]: 
One of the problems with this approach is that logistic regression and decision trees are very different algorithms, so the set of features that work well with a decision tree are not necessarily going to be the features that work well with a logistic regression model (and vice versa). So my advice would be that this approach is a bit of a hack and there are likely to be better approaches. There is a good tutorial on feature selection: Isabelle Guyon, André Elisseeff, "An Introduction to Variable and Feature Selection", Journal of Machine Learning Research, 3(Mar):1157-1182, 2003. ( www ) I, like Peter Flom (+1) and B_Miner (+1), am quite keen on LASSO/LARS based methods (The Random Forest is also a good algorithm), my own contribution to this can be found here: Gavin C. Cawley and Nicola L. C. Talbot, Gene selection in cancer classification using sparse logistic regression with Bayesian regularization, Bioinformatics, (2006) 22 (19): 2348-2355. ( www ) where the regularisation parameter for a LASSO type penalty is integrated out analytically, so there are no hyper-parameters to tune. As @julieth points out (+1) if you use feature selection, you must perform the feature selection step independently in each fold of the cross-validation procedure, or you will end up with an optimistically biased performance estimate. See this paper for details Christophe Ambroise and Geoffrey J. McLachlan, Selection bias in gene extraction on the basis of microarray gene-expression data, PNAS, vol. 99, no. 10, pp 6562–6566, 2002 ( www ) This paper is a "must read" for anybody that is interested in feature selection!
