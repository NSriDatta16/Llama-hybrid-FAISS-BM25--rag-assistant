[site]: datascience
[post_id]: 25698
[parent_id]: 25693
[tags]: 
One common application is to freeze an embedding layer. Freezing this layer will prevent the embedding from updating its weight which can be a desirable thing, especially for a text embedding layer. There also exist designs where updating a weight during a certain batch is not wanted. For example some GAN implementations only want to train a model during the combined model phase and therefore freezes the Generator and Discriminator layers for batches. You can also see this used for stacked autoencoder where someone may train one layer at a time. Here is a quick link .
