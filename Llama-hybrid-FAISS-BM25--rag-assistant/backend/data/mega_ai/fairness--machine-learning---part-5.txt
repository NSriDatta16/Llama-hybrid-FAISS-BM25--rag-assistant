bes the accuracy of a classification model. In this matrix, columns and rows represent instances of the predicted and the actual cases, respectively. By using these relations, we can define multiple metrics which can be later used to measure the fairness of an algorithm: Positive predicted value (PPV): the fraction of positive cases which were correctly predicted out of all the positive predictions. It is usually referred to as precision, and represents the probability of a correct positive prediction. It is given by the following formula: P P V = P ( a c t u a l = + | p r e d i c t i o n = + ) = T P T P + F P {\displaystyle PPV=P(actual=+\ |\ prediction=+)={\frac {TP}{TP+FP}}} False discovery rate (FDR): the fraction of positive predictions which were actually negative out of all the positive predictions. It represents the probability of an erroneous positive prediction, and it is given by the following formula: F D R = P ( a c t u a l = − | p r e d i c t i o n = + ) = F P T P + F P {\displaystyle FDR=P(actual=-\ |\ prediction=+)={\frac {FP}{TP+FP}}} Negative predicted value (NPV): the fraction of negative cases which were correctly predicted out of all the negative predictions. It represents the probability of a correct negative prediction, and it is given by the following formula: N P V = P ( a c t u a l = − | p r e d i c t i o n = − ) = T N T N + F N {\displaystyle NPV=P(actual=-\ |\ prediction=-)={\frac {TN}{TN+FN}}} False omission rate (FOR): the fraction of negative predictions which were actually positive out of all the negative predictions. It represents the probability of an erroneous negative prediction, and it is given by the following formula: F O R = P ( a c t u a l = + | p r e d i c t i o n = − ) = F N T N + F N {\displaystyle FOR=P(actual=+\ |\ prediction=-)={\frac {FN}{TN+FN}}} True positive rate (TPR): the fraction of positive cases which were correctly predicted out of all the positive cases. It is usually referred to as sensitivity or recall, and it represents the probability of the positive subjects to be classified correctly as such. It is given by the formula: T P R = P ( p r e d i c t i o n = + | a c t u a l = + ) = T P T P + F N {\displaystyle TPR=P(prediction=+\ |\ actual=+)={\frac {TP}{TP+FN}}} False negative rate (FNR): the fraction of positive cases which were incorrectly predicted to be negative out of all the positive cases. It represents the probability of the positive subjects to be classified incorrectly as negative ones, and it is given by the formula: F N R = P ( p r e d i c t i o n = − | a c t u a l = + ) = F N T P + F N {\displaystyle FNR=P(prediction=-\ |\ actual=+)={\frac {FN}{TP+FN}}} True negative rate (TNR): the fraction of negative cases which were correctly predicted out of all the negative cases. It represents the probability of the negative subjects to be classified correctly as such, and it is given by the formula: T N R = P ( p r e d i c t i o n = − | a c t u a l = − ) = T N T N + F P {\displaystyle TNR=P(prediction=-\ |\ actual=-)={\frac {TN}{TN+FP}}} False positive rate (FPR): the fraction of negative cases which were incorrectly predicted to be positive out of all the negative cases. It represents the probability of the negative subjects to be classified incorrectly as positive ones, and it is given by the formula: F P R = P ( p r e d i c t i o n = + | a c t u a l = − ) = F P T N + F P {\displaystyle FPR=P(prediction=+\ |\ actual=-)={\frac {FP}{TN+FP}}} The following criteria can be understood as measures of the three general definitions given at the beginning of this section, namely Independence, Separation and Sufficiency. In the table to the right, we can see the relationships between them. To define these measures specifically, we will divide them into three big groups as done in Verma et al.: definitions based on a predicted outcome, on predicted and actual outcomes, and definitions based on predicted probabilities and the actual outcome. We will be working with a binary