[site]: crossvalidated
[post_id]: 604191
[parent_id]: 604176
[tags]: 
The obvious techniques to try in order to" see what variables are impacting the model? " are: PDP; Partial dependency plots - primarily used to explain overall behaviour but has extensions that work for individual predictions too (ICE) - Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation by Goldstein is a good reference ) LIME; Local interpretable model-agnostic explanations - primarily used to explain individual predictions - "Why Should I Trust You?": Explaining the Predictions of Any Classifier by Ribeiro is a good reference) SHAP; SHapley Additive exPlanations - primarily used to explain individual predictions but works for overall behaviour too - A Unified Approach to Interpreting Model Predictions by Lundberg & Lee is the main reference but I prefer An Efficient Explanation of Individual Classifications using Game Theory by Strumbelj & Kononenko) If you haven't seen it already, check Molnar's Interpretable Machine Learning book. It is amazing work, has greater coverage and will start from the basics and go into more advanced techniques very nicely. It provides a more readable and to-the-point presentation of the papers mentioned above too.
