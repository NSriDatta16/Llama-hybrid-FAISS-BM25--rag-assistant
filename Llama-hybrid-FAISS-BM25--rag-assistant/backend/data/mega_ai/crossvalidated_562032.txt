[site]: crossvalidated
[post_id]: 562032
[parent_id]: 
[tags]: 
Examples in Machine Learning with Non-Differentiable Objective Functions

I was reading the following lecture notes on Gradient Descent and came across the following note: Supposedly, there are some instances in machine learning where the objective function is non-differentiable. I was trying to think of some instances in Machine Learning where the Objective Functions are non-differentiable. After doing some thinking and reading about this online, I think one of the instances where this is true is in L1 Regularization (i.e. the absolute value of the model parameters introduces discontinuities): My Question: Can someone please tell me if my understanding of this concept is correct? In L1 Regularization, the absolute value of the model parameters will likely result in the objective function not being differentiable in the classical sense - and as a result, some alternate method of Gradient Descent will be required to optimize such types of objective functions: thus, we use Subgradient Methods. Is this correct? Thanks! References: https://raghumeka.github.io/CS289ML/gdnotes.pdf https://machinelearningcompass.com/machine_learning_math/subgradient_descent/
