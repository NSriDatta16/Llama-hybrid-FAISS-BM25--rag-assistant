[site]: datascience
[post_id]: 54243
[parent_id]: 53956
[tags]: 
As others also pointed out as long as you have numeric data (or data that can be converted to numeric) you can use some sort of distance measure between the users. Simple solution is the euclidean distance (or some other like minkowski or manhattan). The gotcha with them that they are sensitive for having different scales in your variables. You can solve it by normalizing your data but keep in mind that in this case you will assign equal importance for each feature. You may want to adjust it manually based on your domain knowledge. If you have a high number of features in a sparse space it's worth considering using cosine similarity that will more focus on the direction of your data point (customer features). You may also want to do PCA before thereby reducing your dimensionality and eliminating the fact that certain features can be similar to each other (so they correlate). If you want to experiment with a more sophisticated solution you can try to do an autoencoder . A type of neural network where your input and output are the same (user features) but in the hidden layers you suppress the dimensionality thereby having a more dense representation of the data. In that representation features may also bear a semantic meaning. On this more dense representation you can calculate again some of the distance metrics proposed at the beginning.
