[site]: crossvalidated
[post_id]: 157368
[parent_id]: 
[tags]: 
Convolutional Neural Network process

I have two question regarding regarding Convolutional Neural Network (with Autoencoders for patches generation). Let's assume that I got a dataset with images and I want to perform an object recognition task using Convolutional Neural Networks. In the first step I am going to implement Autoencoders Network in order to generate the patches. A brief description about Autoencoders could be found here . My first question is concerning this proess. In this step my system could be learn weights W which is in fact is learning an identify function. I am confused on how can I visualize those weights and produce the patches images. Every hidden unit(or simple neuron in hidden layer) is just a value. How can I get the results which are presented in the mentioned link? My second question is in CNN architecture. Having calculate patches in the first step, in the convolution layer I have to convolve all patches with the image? Thus if I have 100 patches I will create for every input image 100 convolution size-layer? What if I want a second convolution layer? I have to convolve the results of max-pooling with the same patches I have generated in the Autoenconder step? EDIT: One last question: If this is the architecture of autoenconders: The visualised weights are those between L2 and L3?
