[site]: datascience
[post_id]: 71948
[parent_id]: 68893
[tags]: 
I think this question depends on the type of data you deal with. With images, you can always depict heatmaps of the gradients with Grad-CAM for instance. When deploying, you could bolster the skeptic's trust of the model by providing the gradient heatmap along with the prediction probability.However, this is specific to just one problem domain and I'm not aware of a general answer. In fact, there are several papers discussing the trust of models ( this for instance ). Yet, my answer to this question in general would be that neural networks are extremely high dimensional functions and no one ever asks to inspect the exact error associated with all weights in regression models for example. Knowing that each individual part of a neural network works mathematically and that they can be combined should be enough.
