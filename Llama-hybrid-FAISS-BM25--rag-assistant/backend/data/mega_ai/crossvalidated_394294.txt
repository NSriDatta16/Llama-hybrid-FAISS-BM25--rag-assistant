[site]: crossvalidated
[post_id]: 394294
[parent_id]: 
[tags]: 
Longitudinal Data with Equal Outcomes Within Individual Samples

I need to prepare some data for plugging into a predictive model. The data is in tidy format, but it comes from an audit table, i.e. every change made to a record is recorded and stored as a separate record, and I haven't been able to find any good sources for how to deal with this. The data pertains to sales leads for a company and looks something like this: SalesLeadId OwnerId Project Amount IsActive CloseReasonDate FirstSourceId CreatedDate AuditDate ... 1 546 31 CHECKER 225000 1 NA 3 9/20/2013 17:12 1/17/2017 17:11 ... 2 546 31 CHECKER 225000 1 NA 3 9/20/2013 17:12 1/26/2017 8:59 ... 3 546 31 CHECKER 225000 1 NA 3 9/20/2013 17:12 2/2/2017 7:17 ... 38 760 184 Catalyt 2100000 1 NA 7 12/18/2013 15:23 6/6/2017 8:21 ... 39 760 184 Catalyt 2100000 0 6/6/2017 8:21 7 12/18/2013 15:23 6/6/2017 8:21 ... 40 1081 31 GART 2000000 1 NA 5 4/11/2014 9:42 1/10/2017 9:30 ... 41 1081 31 GART 2000000 1 NA 5 4/11/2014 9:42 1/13/2017 14:56 ... 42 1081 31 GART 2000000 1 NA 5 4/11/2014 9:42 1/18/2017 17:07 ... 43 1081 31 GART 2000000 1 NA 5 4/11/2014 9:42 1/18/2017 17:11 ... My goal is to use this data to predict when a sales lead is going to get closed. My problem is that each sales lead is audited and represented multiple times in the data set such that the set of features are different for different audits, but the current close date, if defined, remains the same. This results in multiple observations for a given sales lead with different feature values but equal target values (here, my target = CloseReasonDate_current - CreatedDate). E.g. if observation 1 for sales lead id 7740 has the feature vector $\overrightarrow{x_1}$ $=$ $[$ 31, 225000, 1, 0, ... $]$ and the target $y$ $=$ 635, observation 35 for the same sales lead may have $\overrightarrow{x_{35}}$ $=$ $[$ 46, 270000, 0, 0, ... $]$ but the outcome will not change e.g. $y$ $=$ 635. One of my ideas was to just use the most recent record for every sales lead. This reduces my data set from n = 24,000 to n = 1,600 (the number of unique sales leads) and I don't know if this is an optimal approach because I am losing important historical data on the sales lead. I feel there may be some sampling technique or way to reshape the data that would provide better results. I have thought of using feature engineering to make it so that I collapse all the observations of every sales lead onto a single row via some aggregation, but I don't know if this makes sense and it will likely result in twice the features I currently have, putting me close to 100 features with a measly 1600 observations. I would appreciate any thoughts, advice, or references on this problem.
