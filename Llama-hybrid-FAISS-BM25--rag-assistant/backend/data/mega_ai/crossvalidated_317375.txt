[site]: crossvalidated
[post_id]: 317375
[parent_id]: 
[tags]: 
Understanding the math behind linear discriminant analysis

I am trying to understand Eigenfaces Vs. Fisherfaces: Recognition Using Class Specific Linear Projection paper. It uses PCA and further uses LDA for dimensionality reduction. I have read about eigenfaces and PCA approach but I am not familiar with LDA and I am unable to understand the mathematics behind the scatter matrices : why to minimize the determinant of projected scatter matrices and how we perform it. Can anyone explain the maths behind LDA and how the generalized eigenvalues come into picture or point to some related tutorial where I can understand the approach?
