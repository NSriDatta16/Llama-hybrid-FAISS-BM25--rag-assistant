[site]: crossvalidated
[post_id]: 466580
[parent_id]: 466573
[tags]: 
Your interpretation about MC sampling is correct. MC is powerful and unbiased but requires too much samples to get a better approximation compared to variational inference. Here $p(x | z)$ is the data likelihood or decoder receiving a stochastic input, which is a parameterized function in our case. If you know the prior and the parameters, you can model the likelihood and can construct MC samples from it. In VAE, likelihood is characterised as a neural net. Just pass your latent variable to the likelihood(decoder) network and get an estimate of your data .Otherwise, you need to know the form of your likelihood, of course . One note, you cannot evaluate the integral for the evidence term as there can be infinitely many latent variables for an input. You can think it as VAE maps input to probabilistic spheres which has infinitely many points inside of it. If there were a exact pair of input and latent pairs, you could compute the integral, however you may have to deal with the numerical computation of it. Being fully Bayesian is not scalable unless you are dealing with certain distributions(conjugate priors) . VAE is a bridge between approximate Bayesian inference and deep learning. If I couldn't explain it properly, let's discuss!
