[site]: crossvalidated
[post_id]: 473426
[parent_id]: 
[tags]: 
How to understand logistic regression cost function formula?

I have found this cost function for logistic regression (source of formula: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression ): $\displaystyle \underset{w,c}{min} \ \frac{1}{2} w^{T} w+C\sum ^{n}_{i=1} log\left( exp\left( -y_{i}\left( X^{T}_{i} w+c\right)\right) +1\right)$ I would like to code this function to python, but I have some issues understanding some parts of the formula: regarding this part: $\displaystyle\frac{1}{2} w^{T} w$ If w is vector of weights of features, how it can be transposed? Isn't that same like doing just w * w ? regarding this part: $\displaystyle x^{T}_{i} w$ Here it's similar, ith vector is multiplied with another vector, so why the transpose symbol? Also, most important question: if two vectors are multiplied, result is vector again. Does it mean that all 'outer' (exp, log, sum) operations are done on vectors? But in that case, function output is vector. How do we can minimize function when output is vector?
