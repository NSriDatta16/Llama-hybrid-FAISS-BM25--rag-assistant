[site]: datascience
[post_id]: 120209
[parent_id]: 
[tags]: 
Is there any concern for a pretrained model to overfitting to a fine-tuning task that has overlapping pretraining and training data?

Let's say my language model is pretrained on a general text corpus, and I want to use it for some specific downstream task that has it's datasets also included in the general corpus, is there any concern for overfitting or bias? I can't seem to find much resources that touch on this issue. I read this paper SciBERT that shows in-domain pretraining of BERT with vocab and corpus extracted from only scientific text would yield better performance on scientific tasks. But isn't this just overfitting? I also read a few papers like the T5 paper that claims in-domain pretraining leads to improvement of fine-tuning tasks as if it is a merit to use pretraining data that is similar to finetuning tasks? Is there not a concern for overfitting? Is it not a concern if the pretraining and finetuning objectives are different enough? Or am I misunderstanding the concept of pretraining and overfitting? Would appreciate if anyone could also provide links to articles that investigate this issue.
