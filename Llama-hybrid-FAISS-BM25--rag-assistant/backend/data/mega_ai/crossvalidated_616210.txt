[site]: crossvalidated
[post_id]: 616210
[parent_id]: 616103
[tags]: 
the difference between train loss and val loss and why val_loss is as low as it is from the start and does it make any sense. Yes, it can make sense. Neural networks, if they have a large complexity, can always fit the training data if given enough time to train. That doesn't mean that some meaningful pattern has been learned that works on other data as well. Your graph is also not different from the typical graphs with curves of training loss and validation loss. (See several here: How to know if model is overfitting or underfitting? ) Typically you have: training loss continuously decreasing with increasing model complexity validation loss initially decreasing (due to less underfitting), subsequently increasing (due to more overfitting) Your curves look the same, but it is just that the overfitting starts right-away (or just after the first step, which does reduce the validation loss slightly) and you have little of the part where the training decreases the bias and underfitting. Your model directly starts to overfit. A similar early overfitting is in this graph where the validation error starts increasing after the cubic polynomial (3rd point along the x-axis in the graph with mean squared error): From question Overfitting, but why is the training deviance dropping? If you believe that your features should allow a simple model that can do the fitting, then you probably need to debug and adapt your model. The question* that was previously added as duplicate is different, in the body text, and is about the case when even the training loss doesn't decrease. However, the answers given to it are very general and can help you with the troubleshooting of your network. * What should I do when my neural network doesn't learn?
