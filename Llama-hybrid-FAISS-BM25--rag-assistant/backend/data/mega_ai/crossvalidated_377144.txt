[site]: crossvalidated
[post_id]: 377144
[parent_id]: 376851
[tags]: 
You have bug in the code. It looks like torch accumulates gradients and before making backpropagation you need to clean them (set to zero) optimizer.zero_grad() # add this line loss.backward() optimizer.step() I added this fix in your code and I get progress during the training. I got loss equal to 0.0005 after 1000 iterations with step 1e-3 . Check this answer from the stackoverflow. You can also check this RNN tutorial on github
