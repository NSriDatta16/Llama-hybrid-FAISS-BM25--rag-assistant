[site]: datascience
[post_id]: 114347
[parent_id]: 114289
[tags]: 
The performance of a classical agent strongly depends on how close the observation approximates the true state of the environment. The state is sometimes defined as all we need to know to model the dynamics forward in time. Therefore, the closer it is possible to infer the dynamics of the environment with the information provided, the better the performance. A common case study in RL is Atari. There, one observation (pixel image) alone would be insufficient to make progress at all. But if you stack consecutive images together, then this can provide sufficient information for an agent developed for MDPs like DQN to learn games like Breakout, Space Invaders or Pong. Now, this is not enough for games that require memory, like Montezuma's Revenge, which would be considered as a POMDP. To address partial observability, a general idea is to use a recurrent neural network (often LSTM or GRU) instead of a feed-forward network in the actor and critic to give the agent additional context if required to make good decisions. The use of an LSTM, plus additional implementation tricks, can give strong results in all Atari games (see R2D2 ). Also, essentially PPO with a (huge) LSTM was also able to reach super-human performance in DOTA2, see OpenAI Five . To take an example in robotics deployed to the real world. Assume we know the positions and velocities of all the joints. Then this information is still partial in practice, as we ignore other factors (like friction, wind speed, terrain). However, it would perform still be able to perform decently. If we had only the positions, then we may be able to infer the velocities by using an LSTM and make it work. If we had only access to the information of one joint, it would probably never work.
