[site]: datascience
[post_id]: 96351
[parent_id]: 96342
[tags]: 
The term "attention weights" seems overloaded to me, as you may refer to the computed attention weights applied to the weighted sum of the values, or you may be referring to the attention head parameters, which are learned during training. I assume you are referring to attention head parameters. With that assumption: During training, the attention head parameters are the same for all sequences in the same optimization step (i.e. in each batch). After each optimization step, the parameters are updated and, therefore, the next batch will use the new parameter values. The attention head parameters are learned during training and, like all the other model parameters, do not change at inference time. Yes, in pretrained Transformers the attention head parameters are part of the pretrained model.
