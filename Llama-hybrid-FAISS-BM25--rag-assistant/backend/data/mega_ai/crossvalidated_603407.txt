[site]: crossvalidated
[post_id]: 603407
[parent_id]: 603385
[tags]: 
It would be useful to know in what context Dr. Harrel made that remark. The classic misadventure of variable selection algorithms is using such an algorithm to identify the hypothesis, then reporting the results of the hypothesis test as if it were prespecified. When formulating a hypothesis from a multivariable regression model, different combinations of adjustment variables comprise different hypotheses even with the same main effect. Developing a predictive model does not usually involve hypothesis testing, so many analysts relax the stringency that usually conserves type 1 error rates - sometimes applying many models and picking which results are best, or manually tuning penalties, weights, or tradeoffs. Variable selection and prediction are closely related. The type 1 error in prediction has traditionally been underemphasized, they believe the associated cost is different and less than in confirmatory studies. In confirmatory studies, type 1 errors amount to adopting ineffective drugs to the marketplace, ruling "guilty" for innocent defenders, implementing ineffective public policies, etc. But one could argue that in big data, repeated type 1 errors are contributing to a fizzling hype. So it's not clear on which side of the fence one should fall. Trying to formally control type 1 error rate for the many ML/DS methods has not played out well, numerous studies show it's quite hard to formally conserve a type 1 error rate in such problems as adding variables to predictive models or other machine learning applications. One method to enforce internal consistency of predictions is split sample validation. This, if anything, benefits the analysis by trimming the $N$ so as not to select too many variables. I think that, in the context of variable selection this would be fine. When a subject enrolls to a trial, the trial results are highly subject to participation bias, a specific form of "selection bias". Trial participants tend to be white, female, and healthier than the general population. However, if your trial participants were identified from an electronic data base, and you have access to electronic database records for all utilizers, then selection bias isn't a problem, and developing an "enrollment" model helps develop weights and probabilities to correct the selection bias in the trial results. As far as selecting variables in the model, it can really be as simple as statistical significance in the logistic model, or you can even use a ROC regression where statistical significance means the ROC is significantly better, variables can be added or removed via stepwise forward or backward selection. You can use an L1 penalty. You can add select models with optimal AIC or BIC...
