[site]: crossvalidated
[post_id]: 638450
[parent_id]: 185486
[tags]: 
It is unclear what biasing a regression model means here and why you want to achieve it. Statisticians advocate for unbiased estimates, consistent at least if biased estimates are unavoidable. In logistic regression, estimates (of coefficients) are at best consistent asymptotically. If the number of predictors are large, variable selection, dimension reduction, and regularization are three typical methods to reach convergence. Variable selection includes stepwise (e.g. step() ), best subset, genetic algorithm (e.g. glmulti() ), and some other approaches that remove unimportant variables and retain important ones. A popular dimension reduction technique is principal component analysis (PCA) that decompose variable variation among several principal components, which seems to fit the current needs of representing genes in multiple activity pathways. It is unclear how chemical information is represented, but averaging enforces equal weights among genes, which is more rigid than what PCA allows. Regularization is another technique for problems of many predictors with few observations common in gene studies that penalize large estimates. In practice, it includes LASSO, ridge, and elastic net. For logistic regression, all can achieved by glmnet() . If the response variable is ordinal instead of binary, ordinal regression allows many more predictors that can possibly exceed the number of observations. I have not used linear discrimination analyses in practice, but I believe it achieves the same objective as logistic regression in estimating probabilities of two or more categories. Others can elaborate whether it is beneficial to conduct linear discrimination analysis in addition to logistic regression.
