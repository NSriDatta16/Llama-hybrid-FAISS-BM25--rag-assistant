[site]: crossvalidated
[post_id]: 625920
[parent_id]: 625846
[tags]: 
It depends on what kind of variables the unavailable ones are. If we are talking about variables that would be unavailable at the time one needs to make a prediction, it is often wrong/misleading to include them. E.g. predicting whether a patient dies based on the outcomes of an autopsy with one outcome being "none conducted" is simply nonsense, but the variable could be useful for explaining what happened/why. If it is variables that would become available later, but not yet, it could make sense to use them (and to see whether one can predict them by what is known/or update the predictions once known1). E.g. predicting whether a project will fail based on the number of pages (or slides) in the full project plan could make sense, but this might not be available at the time of approving the project. However, this might be predictable from the outline/characteristics of the project, or even if not it might be worth updating the prediction once the planning stage is completed (but before some major spending happens). One could argue that perhaps one should just train two models in this case though. If it is variables that in principle are knowable at the time of prediction, but would not be measured in practice, this is a case of "privileged data" and there's methods proposed for what one could do about that (e.g. train model with all variables and then try to distill the model into one that only has the variables that would be available in practice, ideally on new data - but perhaps on synthetic data). In terms of models, boosted trees (and random forests) are commonly used models that are often quite decent, but it's usually a good idea to have something like a regularized regression model just to see whether such a simple model is not better. One certainly should evaluate models properly in the sense that the "to-be-expected" performance would be that with only the smaller set of predictors available. In fact, model evaluation (e.g. through some appropriate form of - possibly nested - cross-validation and/or evaluation on a new test set) would be another thing to very, very carefully think about. Overfitting will be a big potential issue with such a small dataset and more predictors than records, so a validation strategy (that minimizes the negative consequences of overfitting as much as possible and gives an idea of performance with as little bias as possible) needs very careful thinking.
