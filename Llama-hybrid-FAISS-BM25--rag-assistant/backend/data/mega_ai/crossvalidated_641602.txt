[site]: crossvalidated
[post_id]: 641602
[parent_id]: 
[tags]: 
Is using aggregated usage data more efficient than using flattened usage data to build a ML model in anomaly detection?

We're tracking users' hourly usage on our cloud service and have a risk model that uses aggregated usage data plus other signals to identify potential fraudsters. Basically, it's an anomaly detection model. The current approach uses simple filters, like checking if today's usage is suspiciously high on yesterday (around 2 sd from its past history) combined with a high-risk model score, before sending the users to our labeling team for review. However, accuracy is only about 50-60%. I suggested building a new model using deep neural networks and the past 60 days of data to uncover hidden usage patterns. My manager dismissed it, saying our existing usage model made it unnecessary. But then, a colleague proposed using scores from 3 current production models (include the usage one) as features for a new model, which showed some improvement, and suddenly my manager loved the idea. Honestly, I'm confused. Am I missing something, or is there a deeper issue with my manager?
