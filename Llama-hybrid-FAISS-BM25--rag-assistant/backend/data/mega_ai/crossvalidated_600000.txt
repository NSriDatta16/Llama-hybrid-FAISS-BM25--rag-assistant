[site]: crossvalidated
[post_id]: 600000
[parent_id]: 
[tags]: 
Why Aren't "Non-Informative Priors" More Popular?

Recently, I have been reading about how Confidence Intervals in the Frequentist setting are often misinterpreted (e.g. The importance of a correct interpretation of a confidence interval ). Supposedly, many people incorrectly use the following understanding to incorrectly draw conclusions from Frequentist Confidence Intervals. Given the following understanding of Confidence Intervals: If you collect some data, fit a regression model to this data and estimate regression coefficients (e.g. b1) and then calculate the 95% confidence interval for b1 (e.g. between some range x,y) It is incorrect to say "there is a 0.95 probability that this confidence interval contains the true value of b1. However, if you could collect data many times, 95% of these times the original interval (x,y) would contain the true value of b1 (i.e. b1 from the population). The following incorrect understanding is then derived by many people: Someone could just turn the tables and say - that is a 0.95 probability that the data I collected and the confidence interval calculated from this data that I collected is one of those experiments in which the confidence interval bounds the true value of the parameter. Furthermore, - many of these confidence intervals (e.g. https://i.stack.imgur.com/JwPFh.png ) could contain the true value of the parameter AND many of these confidence intervals that contain the true value of the parameter have significant overlap with each other Since most datasets would produce a confidence interval that would bound the true value of the parameter and most confidence intervals from all these datasets overlap-for both of these statements to be simultaneously true -it is not unreasonable to believe that in general, the CI calculated from the data I collected in general has a 0.95 prob of containing the true parameter. This is being said, I have read that in the Bayesian setting, a similar concept called "Credible Intervals" are able to address some of the shortcomings present in "Confidence Intervals". Particularly, a Bayesian Credible Interval can be said to describe the probability of the true population parameter being contained in the resulting interval. This brings me to my question: I read that "Non Informative Priors" often result in the same parameter estimates as MLE (Maximum Likelihood Estimation), because "Non Informative Priors" are said to "let the data speak for itself" and have minimum influence on the parameter estimates. Non Informative Priors are also advantageous in the sense that if you genuinely do not have any strong "prior" beliefs in the distributions of the model parameters and do not want to incur any risk by incorrectly choosing some ill-suited distributions - Non Informative Priors will incur less risk compared to a "standard choice" in "priors" (i.e. Not-Non Informative Priors). At the same time, Credible Intervals in the Bayesian setting seem to have more advantageous interpretations compared to Confidence Intervals in the Frequentist setting. Thus, if both of these statements are true - why don't people just use "Non Informative Priors" to effectively perform Maximum Likelihood Estimation, while simultaneously deriving intervals on their estimates (i.e. Credible Intervals) with more advantageous interpretations? Would this not allow them to get the "best of both worlds"? Currently, my guess is that this (i.e. using Non Informative Priors) is more complicated to perform (e.g. computation) and explain to an audience, thus it ends up being used less. Can someone please comment on this? References: Why does a 95% Confidence Interval (CI) not imply a 95% chance of containing the mean? From a Bayesian probability perspective, why doesn't a 95% confidence interval contain the true parameter with 95% probability?
