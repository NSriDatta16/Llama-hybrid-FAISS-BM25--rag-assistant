ystyle P(T|F)={\frac {P(T)}{P(F)}}} or in terms of information, the relative probability is, P ( T | F ) = 2 − ( L ( T ) − L ( F ) ) {\displaystyle P(T|F)=2^{-(L(T)-L(F))}} Note that this estimate for P(T|F) is not a true probability. If L ( T i ) < L ( F ) {\displaystyle L(T_{i})<L(F)} then the theory has evidence to support it. Then for a set of theories T i = H i {\displaystyle T_{i}=H_{i}} , such that L ( T i ) < L ( F ) {\displaystyle L(T_{i})<L(F)} , P ( T i | F ) = P ( T i ) P ( F | R ) + ∑ j P ( T j ) {\displaystyle P(T_{i}|F)={\frac {P(T_{i})}{P(F|R)+\sum _{j}{P(T_{j})}}}} P ( R | F ) = P ( F | R ) P ( F | R ) + ∑ j P ( T j ) {\displaystyle P(R|F)={\frac {P(F|R)}{P(F|R)+\sum _{j}{P(T_{j})}}}} giving, P ( T i | F ) ≈ 2 − L ( T i ) 2 − L ( F ) + ∑ j 2 − L ( T j ) {\displaystyle P(T_{i}|F)\approx {\frac {2^{-L(T_{i})}}{2^{-L(F)}+\sum _{j}{2^{-L(T_{j})}}}}} P ( R | F ) ≈ 2 − L ( F ) 2 − L ( F ) + ∑ j 2 − L ( T j ) {\displaystyle P(R|F)\approx {\frac {2^{-L(F)}}{2^{-L(F)}+\sum _{j}{2^{-L(T_{j})}}}}} Derivations Derivation of inductive probability Make a list of all the shortest programs K i {\displaystyle K_{i}} that each produce a distinct infinite string of bits, and satisfy the relation, T n ( R ( K i ) ) = x {\displaystyle T_{n}(R(K_{i}))=x} where R ( K i ) {\displaystyle R(K_{i})} is the result of running the program K i {\displaystyle K_{i}} and T n {\displaystyle T_{n}} truncates the string after n bits. The problem is to calculate the probability that the source is produced by program K i , {\displaystyle K_{i},} given that the truncated source after n bits is x. This is represented by the conditional probability, P ( s = R ( K i ) | T n ( s ) = x ) {\displaystyle P(s=R(K_{i})|T_{n}(s)=x)} Using the extended form of Bayes' theorem P ( s = R ( K i ) | T n ( s ) = x ) = P ( T n ( s ) = x | s = R ( K i ) ) P ( s = R ( K i ) ) ∑ j P ( T n ( s ) = x | s = R ( K j ) ) P ( s = R ( K j ) ) . {\displaystyle P(s=R(K_{i})|T_{n}(s)=x)={\frac {P(T_{n}(s)=x|s=R(K_{i}))P(s=R(K_{i}))}{\sum _{j}P(T_{n}(s)=x|s=R(K_{j}))P(s=R(K_{j}))}}.} The extended form relies on the law of total probability. This means that the s = R ( K i ) {\displaystyle s=R(K_{i})} must be distinct possibilities, which is given by the condition that each K i {\displaystyle K_{i}} produce a different infinite string. Also one of the conditions s = R ( K i ) {\displaystyle s=R(K_{i})} must be true. This must be true, as in the limit as n → ∞ , {\displaystyle n\to \infty ,} there is always at least one program that produces T n ( s ) {\displaystyle T_{n}(s)} . As K i {\displaystyle K_{i}} are chosen so that T n ( R ( K i ) ) = x , {\displaystyle T_{n}(R(K_{i}))=x,} then, P ( T n ( s ) = x | s = R ( K i ) ) = 1 {\displaystyle P(T_{n}(s)=x|s=R(K_{i}))=1} The apriori probability of the string being produced from the program, given no information about the string, is based on the size of the program, P ( s = R ( K i ) ) = 2 − I ( K i ) {\displaystyle P(s=R(K_{i}))=2^{-I(K_{i})}} giving, P ( s = R ( K i ) | T n ( s ) = x ) = 2 − I ( K i ) ∑ j 2 − I ( K j ) . {\displaystyle P(s=R(K_{i})|T_{n}(s)=x)={\frac {2^{-I(K_{i})}}{\sum _{j}2^{-I(K_{j})}}}.} Programs that are the same or longer than the length of x provide no predictive power. Separate them out giving, P ( s = R ( K i ) | T n ( s ) = x ) = 2 − I ( K i ) ∑ j : I ( K j ) < n 2 − I ( K j ) + ∑ j : I ( K j ) ⩾ n 2 − I ( K j ) . {\displaystyle P(s=R(K_{i})|T_{n}(s)=x)={\frac {2^{-I(K_{i})}}{\sum _{j:I(K_{j})<n}2^{-I(K_{j})}+\sum _{j:I(K_{j})\geqslant n}2^{-I(K_{j})}}}.} Then identify the two probabilities as, P ( x has pattern ) = ∑ j : I ( K j ) < n 2 − I ( K j ) {\displaystyle P(x{\text{ has pattern}})=\sum _{j:I(K_{j})<n}2^{-I(K_{j})}} P ( x is random ) = ∑ j : I ( K j ) ⩾ n 2 − I ( K j ) {\displaystyle P(x{\text{ is random}})=\sum _{j:I(K_{j})\geqslant n}2^{-I(K_{j})}} But the prior probability that x is a random set of bits is 2 − n {\displaystyle 2^{-n}} . So, P ( s = R ( K i ) | T n ( s ) = x ) = 2 − I ( K i ) 2