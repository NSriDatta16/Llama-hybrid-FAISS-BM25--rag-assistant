[site]: datascience
[post_id]: 108409
[parent_id]: 108408
[tags]: 
Feature selection can have an impact of hyperparameters which are optimal and vice versa. So if you want to squeeze every bit of performance from your model you should do both of the together. But as you rightly mentioned that it may not be feasible and i would suggest the following step : Very loosely optimize hyperparameters, just to make sure you don't assign extremely bad values to some hyperparameters. This can often just be done by hand if you have a good intuitive understanding of your hyperparameters, or done with a very brief hyperparameter optimization procedure using just a bunch of features that you know to be decently good otherwise. Feature selection, with hyperparameters that are maybe not 100% optimized but at least not extremely terrible either. If you have at least a somewhat decently configured machine learning algorithm already, having good features will be significantly more important for your performance than micro-optimizing hyperparameters. Extreme examples: If you have no features, you can't predict anything. If you have a cheating feature that contains the class label, you can perfectly classify everything. Optimize hyperparameters with the features selected in the step above. This should be a good feature set now, where it actually may be worth optimizing hyperparams a bit. Please refer to this link for more details
