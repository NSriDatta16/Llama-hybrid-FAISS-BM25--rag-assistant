[site]: crossvalidated
[post_id]: 440986
[parent_id]: 439277
[tags]: 
To answer your question first: No, I'm not aware of such work in the literature. There are two classes of generative models which may be well-suited for this kind of modeling though. VAE with an autoregressive (linear OR tree-structured decoder ). Basically these types of models define $$p(x) = \int p(z)\prod_i p(x_i|x_{j And $z$ is the latent space. So each variable $x_i$ is generated conditioned on it's predecessors (in the linear generator case), or you can condition on it's parents or markov blanket depending on the exact structure of the bayesian network. The factors $p(x_i|x_{j are modeled by an RNN, and just like in a typical VAE, we have a variational posterior $q(z|x)$ and the ELBO is optimized to train the model. The main problem with these types of models is that the decoder can become too powerful, and $q(z|x)$ just converges to $p(x)$ -- this is called " posterior collapse ". Deep EBMs -- these models use a network which directly outputs $E(x; \theta)$ and define $p(x) \propto \exp(-E(x; \theta))$ . See here for an example. This energy function could even be parameterized with a graph neural network with the same graph as the bayesian network you're trying to copy. This type of model doesn't really have a latent space, but I suspect picking any pooling/bottleneck layer of the network would give good results.
