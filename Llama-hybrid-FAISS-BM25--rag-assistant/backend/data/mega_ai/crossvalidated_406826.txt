[site]: crossvalidated
[post_id]: 406826
[parent_id]: 406334
[tags]: 
Cross-validation allow us to get more than one model but the testing data isn't the same for all of them. You can repeat (aka iterate) cross validation: do it again with new random splits. Each such iteration will give you one prediction for each sample, so after $i$ runs you have $i$ predictions of the same case by $i$ different surrogate models. With that you can measure variance caused by small differences in the surrogate models, i.e. model instability wrt. the data set at hand . Note that this is possible only if more than one case is left out (i.e. no LOO), so a single run doesn't exhaust all possible train/test combinations (even though also LOO results are subject to model instability). You may say that the dependency between tested case and surrogate model doesn't allow to decompose the observed variance into variance due to tested case and variance due to tested surrogate model for the LOO experimental design. If you want to assess model variance/instability for different models built from different data sets taken from the same population , you'll need to get several such data sets - cross validation doesn't cannot provide an estimate of this variance. You can see this in analogy to LOO design having collinearity between surrogate model and tested case: cross validation is an experimental design that has high correlation between training sets and thus surrogate models. Literature: for variance between surrogate models via cross validation: Beleites, C. & Salzer, R.: Assessing and improving the stability of chemometric models in small sample size situations, Anal Bioanal Chem, 390, 1261-1271 (2008). DOI: 10.1007/s00216-007-1818-6 for variance wrt. new data sets of same size and application: Bengio, Y. and Grandvalet, Y.: No Unbiased Estimator of the Variance of K-Fold Cross-Validation Journal of Machine Learning Research, 2004, 5, 1089-1105
