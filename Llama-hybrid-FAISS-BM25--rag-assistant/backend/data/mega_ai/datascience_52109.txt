[site]: datascience
[post_id]: 52109
[parent_id]: 
[tags]: 
using word embedding features with linear prediction models

I have been seeing that word embedding features (e.g. here or there) are used on classification or regression tasks where the classifier/regressor is a linear one: e.g. Linear/Logistic Regressor or GBM. However, I cannot really understand how these regressors/classifiers should interpret the word embedding dimensions and be able to capture the cosine relationship between these dimensions. Can anyone recommend a paper/blog post that can answer my question?
