[site]: crossvalidated
[post_id]: 338038
[parent_id]: 
[tags]: 
Definition of (contiuous-time) Markov chain transition rate

Suppose that the rate at which a Markov chain leaves state i at some time t is $\lambda_{i}$. I.e., What is the rate at which $X_{t}$ leaves state i. Then, $\lambda_{i} = \sum_{j\neq i}q\left ( i,j \right )$ is the rate at which a Markov chain leaves a state i for j. From observation, $\lambda_{i} = \sum_{j\neq i}q\left ( i,j \right )$ looks to be the definition of a marginal probability up to some state j. Could someone kindly explain to me why there is the case? Why would the rate at which the chain moves be determined by how far is it transition previously? It's bizarre. The notation $q(i,j)$ is also not clarified nor defined by the author. From Essentials of stochastic processes by Durrett :
