[site]: crossvalidated
[post_id]: 331476
[parent_id]: 89535
[tags]: 
A classic algorithm for binary data clustering is Bernoulli Mixture model. The model can be fit using Bayesian methods and can be fit also using EM (Expectation Maximization). You can find sample python code all over the GitHub while the former is more powerful but also more difficult. I have a C# implementation of the model on GitHub (uses Infer.NET which has a restrictive license!). The model is fairly simple. First sample the cluster to which a data point belongs to. Then independently sample from as many Bernoullis as you have dimensions in your dataset. Note that this implies conditional independence of the binary values given the cluster! In Bayesian setting, the prior over cluster assignments is a Dirichlet distribution. This is the place to put priors if you believe some clusters are larger than others. For each cluster you must specify prior, a Beta distribution, for each Bernoulli distribution. Typically this prior is Beta(1,1) or uniform. Finally, don't forget to randomly initialize cluster assignments when data is given. This will break symmetry and the sampler won't get stuck. There are several cool features of the BMM model in Bayesian setting: Online clustering (data can arrive as a stream) Model can be used to infer the missing dimensions The first is very handy when the dataset is very large and won't fit in RAM of a machine. The second can be used in all sorts of missing data imputation tasks eg. imputing the missing half of binary MNIST image.
