[site]: datascience
[post_id]: 49066
[parent_id]: 49054
[tags]: 
About dealing with the mean of trials, I should say you can not recover the original trials and therefore you have to trust the results of classifucation model trained on averaged trials. About dealing with a dataset with higher number of features than number of samples, I would suggest the following steps: 1- reduce the dimension of your data from 70 to 2 using PCA or t-SNE and 2- visualize it using a scatter plot to see if your labels are linearly separable in 2D space 3- If the result of PCA linearly separates labels, it means that your labels are kind of linearly separable in the original 70D space. So, you would apply the results of PCA (or even your original features) to linear-SVM. 4- If the result of t-SNE linearly separates labels, it means that your labels are not linearly separable in the original 70D space because unlike PCA, t-SNE is not a linear transformer. In this case, you would apply kernel SVM to your data. 5- If because of lack of data none of aforementioned methods worked, you would try low-variance classifiers such as Random Forest. 6- If Random Forest didn’t work, you would try using Bayesian methods such as Bayesian logistic regression because these methods assume a prior for parameters’ distributions and update it to posterior distributions using training data which compensates the lack of enough training data.
