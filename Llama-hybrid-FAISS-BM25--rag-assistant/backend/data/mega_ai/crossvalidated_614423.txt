[site]: crossvalidated
[post_id]: 614423
[parent_id]: 604519
[tags]: 
...the idea to describe NHST as something like a posterior predictive check, but with a "null hypothesis" pseudo-posterior that is a Dirac $\delta$ density about $H_0$ . Does that make sense...? Yes. I'm not sure whether to call NHST a prior or posterior predictive check, but it is fair to see it as a form of model check. That said, a Bayesian PPC is often used to check "Is my model large enough yet, or do I need to add more nuance?" By contrast, you could says classical NHST is typically used to check "Is my model small enough yet to match the capacity of my sample size / study design, or should I simplify it further because (especially without believing in an informative prior) I just don't have enough data to estimate some parameters with adequate precision?" Ultimately, the usual scientific reason for running a NHST is to answer the question "Is my sample size large enough to rule out sampling variation as being a major concern?" We deliberately set up a too-simple straw-man model, under which the effect we hope to learn about (say, a difference in means between treatment and control groups) isn't a true effect in the population, but could show up as an apparent effect in finite samples: $\mu_1=\mu_2$ , but $\bar{x}_1\neq\bar{x}_2$ . If our sample is small, this "PPC" might lead us to conclude: "Even though we don't believe this straw-man model, the data aren't inconsistent with it. Let's design our next experiment to collect more data, so that we can rule out sampling variation as a reason to disbelieve our results." But if our sample is large enough, we should see that the too-simple model $\mu_1=\mu_2$ typically leads to datasets that don't look like our actual sample. Then we can say, "OK, sampling variation isn't a major concern here. Now we can focus on all the other concerns: Was there random assignment? Are the measurements valid for the construct we are trying to study? etc." Perhaps it's worth framing this a 2nd way too: From the Bayesian point of view, a point prior often makes no sense. If your prior puts all its weight on a single $\theta$ value, the posterior will be the same, so there's no point in collecting data at all. In this sense, classical NHST is not the same as using data to update your prior probabilities for $H_0$ and $H_A$ into posterior probabilities, because it starts with a point prior solely on $H_0$ . But since Bayesian methods are largely meant for updating priors into posteriors, NHST seems like nonsense to many Bayesians. However, if you're a Bayesian who is willing to run a PPC, you are willing to admit your prior might be wrong. Maybe your initial prior is your first attempt at pinning down your beliefs on this topic, and you run the PPC to see if your prior beliefs lead to an adequately realistic model that generates adequately realistic data. If they do, you'll keep using your prior. If they don't, it might convince you that your initial prior was inadequate, and you'll revise your prior (again, NOT the same thing as updating from a prior to a posterior). In that sense, the purpose of NHST is similar to a PPC attempting to find convincing evidence that a prior of "no effect" is unreasonable. You might not actually hold such a prior yourself, but some readers or reviewers might. By reporting a NHST, you hope to tell them: "If we had started with a simple prior of 'no effect,' a PPC would have told us that our prior was inadequate" (if you reject $H_0$ ), or "...was not inadequate" (if you fail to reject $H_0$ ). In either case, NHST is not meant as an answer to the Bayesian's usual question "Which values of $\theta$ should I believe in?" NHST is about the study design, not really about $\theta$ itself. The Greenland & Poole article mentioned in the comments does a nice job of trying to frame p-values in more Bayesian ways, but I don't know how useful that is, because (outside of PPCs) Bayesian methods are simply tackling a very different question than NHST is.
