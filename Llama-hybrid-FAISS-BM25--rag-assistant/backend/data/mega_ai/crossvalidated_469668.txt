[site]: crossvalidated
[post_id]: 469668
[parent_id]: 469654
[tags]: 
In a general linear model, R-squared is $$R^2 = \frac{SS_{reg}}{SS_{tot}} $$ (Note: the above expression only holds if intercepts are included in the model -- see Whuber's comment below) $SS_{reg}$ is the sum of squared differences between the regression and observations $$SS_{reg} = \sum_{i=0}^n(\bar{y}_i-\hat{y}_i)^2$$ and $SS_{tot}$ is the sum of squared differences between the mean and observations $$SS_{tot} = \sum_{i=0}^n(y_i-\bar{y}_i)^2$$ Why must the R-squared value of a regression be less than 1? Under OLS regression, $0 , so $0 . What does R-Squared value more than '1' indicate? The important thing to note here is that $R^2$ is the ratio of explained variance ( $SS_{reg}$ ) to total variance ( $SS_{tot}$ ). There's not always a perfect analogue to this in other models, and other quantities are often reported as $R^2$ (see Pseudo-R2s in logistic regression), however, these values are not always guaranteed to be between 0 and 1. Can a Regression Model with a Small R-squared Be Useful? This depends what you want to use it for. If it is for prediction purposes, it is unlikely to be helpful, however, it is important to note that a large $R^2$ does not guarantee good prediction either. It is simply a measure of how well the model explains the variability in the observed data.
