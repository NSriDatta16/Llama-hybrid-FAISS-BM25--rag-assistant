[site]: crossvalidated
[post_id]: 548004
[parent_id]: 547995
[tags]: 
There are some reasons that could lead to that happening yes, one of such reasons could be due to the increased variance in the data (more outliers or a distributional shift after incorporating the other half of the entire dataset), you might want to employ outlier detection to detect and remove those examples from your dataset. Also, running the train/test once (a validation scheme usually called holdout) is often not a good measure of the actual performance of any model (it may lead to unstable models both in hyperparameters and accuracy), specially in non-deterministic training models like neural networks that are very sensitive to initialization (which may also be the culprit for your problem too). Therefore I'd suggest you to try more robust validation schemes, as at least it will give you a good grasp of what is the actual performance of your model: Holdout with 50% split may be not enough for your model to generalize properly. Usually when validating through holdout, it's recommended to use more data to train than to test, 80/20, 75/25 and even 60/40 splits are way more common. Instead of holdout, use a proper k-cross-validation scheme, you can start with 10-fold cross-validation (divide data into 10 groups, then run the training from start using 1 of the 10 to test and the others to train, each run which group is the test is changed, then evaluate the average performance obtained of all runs), since i don't know the task i'm not able to really propose something tailored for your situation, you might need to do a nested or stratified cross-validation... Once you're satisfied with your validation scheme, I'd devise an environment to test values obtained for each hyperparameters to investigate them as well.
