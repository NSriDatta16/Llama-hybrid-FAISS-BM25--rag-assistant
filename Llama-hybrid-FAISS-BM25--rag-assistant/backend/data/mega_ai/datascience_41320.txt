[site]: datascience
[post_id]: 41320
[parent_id]: 41318
[tags]: 
In general machine learning algorithms, if feeded with large training datasets are able to deal both with outliers and multicollinearity. PCA is a dimension reduction techniques and surely helps with multicollinearity. Naïve Bayes assumes independence of its input features (the word naïve comes from this property). So after PCA Naïve Bayes has more chance to get better results. If you find that PCs are still correlated among the fraud cases I don't think it is an issue. However, you could try to preprocessing your data removing highly correlated variables according to some criteria. The caret library has many functionalities for preprocessing and this tutorial cover interesting stuff besides software applications. https://topepo.github.io/caret/pre-processing.html About outliers I am always skeptic about any outlier removal except in the case could be due to miscoding.
