[site]: crossvalidated
[post_id]: 348087
[parent_id]: 348067
[tags]: 
The embedding layer that you add at the start of your neural network in solution #2 has to be a time-distributed layer. This means that the layer must process each time slice independently and then feed the output into your neural network in a time-series. Generally you will then use the output of the Embedding layer in a recurrent neural network of some sort. As a concrete example, suppose you have inputs of shape (32,128,100000) where the dimensions of this tensor is (batch_size,time,one_hot) - i.e. you have 32 sentences in a batch, and each sentence is 128 words long and each word in the sentence is a 100000 dimensional one-hot vector. The output of an Embedding layer should be (32,128,300) where the last dimension is now the size of the embeddings. Notice that the embedding layer does not modify the time-dimension, hence it is time-distributed. For each time segment, you will embed the word at that time segment from a 100000 dimensional (one-hot) space to a 300 dimensional space.
