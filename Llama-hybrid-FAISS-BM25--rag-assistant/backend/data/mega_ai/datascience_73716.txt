[site]: datascience
[post_id]: 73716
[parent_id]: 73282
[tags]: 
I finally came out with a way to implement bayesian hyperparameter optimization for a time series neural network model using walk-forward validation. You can find a gist with the code here I used some helper functions for the walk-forward validation from this Jason Brownlee book It is basically as follows: original time series data: define a function to convert your time series dataset into a supervised dataset format, another one for the train-test split (ordered in time as done in sklearn.model_selection.TimeSeriesSplit) define your evaluation metric, root mean squared error in this case we also add a differencing function, in case we want to make our time series stationary (by differencing, we can reduce trend and/or stationality) define the walk-forward validation functions ( walk_forward_validation and repeat_evaluate ) define the keras tuner bayesian optimizer, based on a build_model function wich contains the LSTM network in this case with the hidden layers units and the learning rate as optimizable hyperparameters define the model_fit function which will be used in the walk-forward training and evaluation step lastly, find the evaluation metric value and std To keep in mind: this is a proof-of-concept example about how to use keras tuner for time series using walk-forward validation. Further analysis could be done to improve forecasts.
