[site]: crossvalidated
[post_id]: 250034
[parent_id]: 
[tags]: 
Machine learning for regression of a function that is dependent on its own previous state?

I've been taking a machine learning class, and we learned various techniques for machine learning regression. The examples we considered were typically simple functions, such as a linear function $f(x) = ax+b$. But I'm thinking about how to apply machine learning to my own research, and a question that I have been wondering is, are there techniques to perform regression on functions of the form: $$x_{k+1} = f(x_k)?$$ This would be something like a discretization of a differential equation, where you have, for example, $$x_{k+1} = x_k + h f(x_k).$$ I've thought about this idea for a while, and I've even tried to apply methods such as decision trees and SVR to this kind of problem, but I am concerned that I'm doing something that is just ridiculous and mathematically incorrect. I also recognize that there is a problem because in order to have a training set, we would need pairs such as $(x_k,x_{k+1})$, $(x_\ell,x_{\ell+1})$, which is unlikely to happen in any real data collection method. But, I'm interested in looking for into this idea and I'm hoping that it is viable and that someone has already written about it in a paper so I can learn more about it. Thanks!
