[site]: datascience
[post_id]: 61597
[parent_id]: 61579
[tags]: 
After some further research and experimentation, I think I am able to answer my own question. 1. Yes it is feasible! The quickest way to answer my first question was to try it out, which I did. Training a GAN on only a single digit (3 in my example) worked and produced the expected results: Besides the obvious practical example, I was able to find some further articles discussing GAN which mentioned that reducing the classes in the training sample to reduce variance is advised (e.g. in CIFAR-10 concentrate on frogs OR trucks not both). So while I suspected as much it is still nice to confirm that this approach is valid and feasible. 2. I'm not sure it is faster The second part of my question was whether such an approach is better (i.e. more accurate) and faster. I was unable to confirm this because mnist is a very simple example and accuracy is very good, very fast in either case. I learned that speed of model fitting depends on the model complexity and number of epochs, which do not change by focusing on just one digit! So naturally covering all digits individual takes a lot longer than just training one whole model. It might be the case that for more complex classes/image types more complex models and epochs are needed and that cutting down on variance (i.e. just frogs, no trucks) can help cutting down those two main drivers of computation duration but this was not the case for mnist like images. The only direct benefit I was able to find is the greater utility of individual models. Due to the clear separation I am able to generate a specific number on demand (by calling the relevant model) instead of generating only a random number from 0-9.
