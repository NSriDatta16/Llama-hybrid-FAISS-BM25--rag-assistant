[site]: crossvalidated
[post_id]: 459761
[parent_id]: 
[tags]: 
When and why to (log) transform dependent or independent variables in machine learning models?

I know that linear regression (and any other machine learning model) doesn't assume normality in both independent and dependent variables, but assumes normality of the residuals (in case of linear regression). However, I see a lot of times people of Kaggle log-transforming their skewed dependent variable. My questions are: Do people log-transform the skewed dependent variable in order to make the residuals possibly more normal? Or is there another reason? Why do people log-transform independent variables? Is it to make the relation between the dependent and independent more linear? When you log-transform the dependent variable, do you NEED to log-transform the independent variables as well? Or can you only log-transform a skewed dependent variable and let the independent ones untouched? Do only linear models benefit from log-transforming (dependent and independent variables)? Or are there specific machine learning models that benefit from it? Thank you!
