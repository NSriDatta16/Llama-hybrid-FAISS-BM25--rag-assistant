[site]: datascience
[post_id]: 26557
[parent_id]: 24658
[tags]: 
You said you have a bunch of keywords for each category extracted from the data. It is better to explain how did you get them as your data does not contain labels i.e. how do you know the word $w_i$ is a representative of category business? You might have assigned them based on semantics which is not bad but does not give you a level of confidence that they are really the representatives of category. The whole point about clustering is to find this representative words via the algorithm. An example is the famous concept of Stop Words. Everyone thinks that words such as $The$, $is$, $an$ and the rest of common lists are stop words but empirically you always get better classification result including them! That's because in NLP the only reference to indicate a stop word is the data itself. So be careful about it. Assuming your words are really representatives (however I would rely on LDA results), I would design a score for similarity of each question to each category representation list. It could be the category which has the maximum number of representatives in the sentence. It most probably falls in overfitting problem however you will not see that (your problem is unsupervised) but if you insist using them it could be a starting point. In the direction of this idea, another one could be to extract the very extreme words for each category (e.g. "Manchester united" and "Neymar" are more probable to represent sport) then do the same clustering with only extreme words and based on found categories find more representatives and feed them to the algorithm again (iterative manner) and continue till some stopping criterion is satisfied. This looks like above but the difference is that the least input (first set of words) are fed manually and the rest will be found through algorithm. You also may have a look at embedding algorithm such as word2vec or doc2vec and use them with some similarity measure (cosine similarity for example).
