[site]: crossvalidated
[post_id]: 558080
[parent_id]: 
[tags]: 
Implementing Multinomial Logistic Classification

I have a dataset that has probability labels instead of one-hot-encoded labels AND I'd like to keep it that way. The probabilities of each sample is information that I'd like to influence the loss function during backprop. E.g. of dataset Sample (Movie) Horror Comedy Drama SciFy/Fantasy Aliens 0.3 0.2 0.0 0.5 Dumb & Dumber 0.0 0.8 0.2 0.0 The Hobbit 0.1 0.2 0.2 0.5 Above you can see each movie is labeled corresponding to a category with a probability in each category. One-hot-encoding would hard classify Aliens as 100% SciFy/Fantasy, but there's value in knowing that this also belongs to horror and comedy too and in those exact proportions. My usecase is a little more complicated than this as well. I'm not able to simply slap on scikit-learn's logistic regression model and call it a day. I'll need to use a timeseries neural network. I'm able to get decent results with my current model that converts everything to a one-hot-encoded classification, but I believe that it would perform significantly better if y_target utilized more loose class labels reflecting the probabilities in each of those labels. I'm assuming that using those y_target probabilities will work just like any one-hot classification problem and that the loss function will reflect these softer labels, but I am unsure... What I'm thinking is: linear model output --> y_pred --> softmax > y_target Essentially, use a linear output from the model. Then squash it down using softmax. Then calculate the loss with cross-entropy against y_target. y_pred and y_target will both have probability distributions and both will not be one-hot encoded. Can this be done? and is it doing what I think it's doing? Has anyone seen any good examples of something similar?
