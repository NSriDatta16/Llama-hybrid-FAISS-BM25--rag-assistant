[site]: crossvalidated
[post_id]: 295634
[parent_id]: 
[tags]: 
Weighting prior proposals based on distance function in approximate Bayesian computation

The typical approach in approximate Bayesian computation (ABC) is to propose parameters from the prior, simulate data $\chi'_\text{sim}$ and then accept data that minimises the data misfit $\lambda$ with the observed data set $\chi'_\text{data}$: $$\lambda = g\{\eta(\chi'_\text{sim}),\eta(\chi'_\text{data})\}$$ where $g(.)$ is some distance function and $\eta(.)$ are summary statistics. The typical approach is to set some threshold acceptance rate e.g. accept $\alpha$ = 1% of the proposals that have the smallest $\lambda$. However of that 1% some have a smaller data misfit $\lambda$ and are 'better' samples of the posterior than samples that have a larger $\lambda$. An alternative approach to selecting samples from the posterior is to assign all the proposals weights which are proportional to $\lambda$. The approximate posterior distribution is then dependent on all of the proposals (albeit with different weights). Within this context the typical approach of selecting some $\alpha$ is equivalent to selecting a shifted flipped heavy-side weighting function. I am assume that is not a new idea so can you link me to literature that considers this approach? If this is a flawed idea could you explain the issues with this approach?
