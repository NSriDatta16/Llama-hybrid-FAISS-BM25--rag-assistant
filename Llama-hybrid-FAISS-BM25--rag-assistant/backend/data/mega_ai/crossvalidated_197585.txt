[site]: crossvalidated
[post_id]: 197585
[parent_id]: 197555
[tags]: 
I am surprised at the textbook statement as testing hypotheses and comparing models are a most fundamental feature of Bayesian analysis, with a wide variety of possible resolutions that exposes the multiple and sometimes incompatible facets of the problem. (excerpt from our book, Bayesian essentials with R , Chapter 2, p.29:) For the null and alternative hypotheses $$ H_0:\ \theta \in > \Theta_0\text{ and }H_a:\ \theta \in \Theta_1 $$ and under the loss function $$ L_{a_0,a_1} (\theta ,d) = \begin{cases} a_0 & \hbox{if}\quad \theta \in \Theta_0\quad\hbox{and}\quad d=0\,, \cr a_1 & \hbox{if}\quad \theta \in \Theta_1\quad\hbox{and}\quad d=1\,, \cr 0 & \hbox{otherwise.} \cr \end{cases} $$ where $d=0$ denotes the rejection of $H_0$, the Bayes optimal decision associated with a prior $\pi$ is given by $$ \delta^\pi(x) = \begin{cases} 1 & \hbox{if}\quad \mathbb{P}^\pi(\theta \in \Theta_0|x)>a_1\big/{a_0+a_1}, \cr 0 & \hbox{otherwise.}\cr \end{cases} $$ For this class of losses, the null hypothesis $H_0$ is rejected when the posterior probability of $H_0$ is too small, the acceptance level $a_1/(a_0+a_1)$ being determined by the choice of $(a_0,a_1)$. The Bayesian paradigm allows for testing and model comparison, to a larger extent than other statistical paradigms, I would say. What may sound at first like a drawback is that all aspects of this decision have to be spelled out, from the specification of the sampling models under the null and under the alternative hypotheses (which explains why I cannot spell out a strict distinction between hypothesis testing and model choice), to the construction of prior distributions on the parameters of both sampling models, to prior weights on the prior likelihood of both hypotheses, to the impact of selecting the "wrong "model". Outside this Neyman-Pearson decision framework, there are further Bayesian resolutions of the testing issue, like the substitute Bayes factor$$\dfrac{\mathbb{P}^\pi(\theta \in \Theta_0|x)}{\mathbb{P}^\pi(\theta \in \Theta_1|x)}\Big/\dfrac{\mathbb{P}^\pi(\theta \in \Theta_0)}{\mathbb{P}^\pi(\theta \in \Theta_1)}$$that avoids selecting the prior weights but which are not free from foundational drawbacks ; information criteria like BIC , DIC , WAIC and Aitkin's integrated likelihood ; score functions and related information approaches ; posterior predictive assessments like the posterior $p$-value of Gelman et al. and others ; Evans' relative belief ; divergence criteria like ABC$\mu$ ; model averaging ; embedding models like our mixture representation .
