[site]: crossvalidated
[post_id]: 318580
[parent_id]: 309627
[tags]: 
I think I have an answer to this at this point. The first step is to assign probabilistic values for each state at each time step, given just the observations. For example, for the first time step, both I1 and I2 had estimated the state as '1'. Now, given what we know about the accuracy of these instruments, what is the probability that the state was actually '1'? This is the problem of combining multiple pieces of evidence. For a good example, see here : (see formula (5) for the final formula). For the time periods when an instrument output is missing, we use the probability estimated from just one instrument. Thus, we calculate this kind of table by combining the evidence from both the sensors. So, we may have something like: Time_period I1 I2 (prior) probability of state 1 (prior) probability of state 2 1 1 1 0.88 0.12 2 NA 2 0.20 0.80 3 2 2 0.07 0.93 4 1 2 ...etc... 5 1 1 6 NA 2 7 NA 2 8 1 2 9 1 1 NOTE! The probability numbers shown in the above table are just illustrations, and NOT actual calculations! Again, if BOTH instrument outputs are missing for some time step, we can use a 'best guess' estimate (say, probabilities of 0.5 for both state1 & state2). The second part (and again, I am not sure of this part!) is that you use a method known as 'HMM smoothing', as described in 'Machine Learning: A Probablistic Perspective' by Kevin Murphy (2012). In that textbook, chapter 17 is 'Markov and hidden markov models'. In 17.4.1, the 'smoothing' problem is explained. The problem I have described maps well into such a smoothing problem. In 17.4.3, the forwards-backwards algorithm is presented as a way to solve the smoothing problem. Hope this helps someone!
