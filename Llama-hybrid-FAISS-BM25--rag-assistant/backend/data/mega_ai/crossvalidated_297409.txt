[site]: crossvalidated
[post_id]: 297409
[parent_id]: 297352
[tags]: 
An alternative reason could be underfitting of the data. While it is quite unlikely to happen with decision trees, it could still be the case if some of the parameters were chosen poorly. Underfitting is the exact opposite of overfitting: the capacity of your model is too low to fit the training data at all and your model under-performs. It generalizes very well and underperforms on the test data as well. When people make statements such as "avoid overfitting", what they mean is actually to try to minimize the gap between the training and test error rates, sometimes referred as "generalization gap". There is a good explanation of this effect in (Goodfellow et al., 2016) see for instance figure 5.3 . If you get a poor fit on the training data you will of course get poor results on the test data too. Before drawing any conclusion about the performance of the test data, remember to always check the fit on the training data too. References: (Goodfellow et al., 2016) Ian Goodfellow, Yoshua Bengio and Aaron Courville. Deep Learning. MIT Press, 2016.
