[site]: crossvalidated
[post_id]: 222153
[parent_id]: 222058
[tags]: 
I didn't think this through in full detail, so just point out if anything seems a bit weird. Your target variable is a weighted combination of (binary) features, which still essentially is a linear combination of features. The best model for such would be a linear model/generalized linear model (LM), which, given some data, will model this relation pretty well using an appropriate slope and intercept. A support vector machine instead models its regression fit ("tube") using support vectors only, so essentially those samples ending up close to/in the margin. From my understanding, you might be better off using a LM in this case instead (this is the part I didn't think through fully because I lack the imagination of the details of the SVM regression loss function is this setup). One further thing: sort(cor(train.data[,-(1:2)])[,24], decreasing = T) userWeight 1.0000000 Employee_Portal_2 0.7135050 Excelity_Payroll_System_2 0.5450896 Financial_Management_Suite_2 0.4945498 HRMS_1 0.3365252 HRMS_2 0.2991008 [...] yields a pretty similar order to varImp on the SVM model: For SVM models, varImp states: For SVM classification models, the default behaviour is to compute the area under the ROC curve. So, your SVM variable importance will emphasize those variables that have strongly discriminative power on their own by design . In contrast, for linear models, varImp states: Linear Models: the absolute value of the t--statistic for each model parameter is used. model2 Does this look more like what you would expect?
