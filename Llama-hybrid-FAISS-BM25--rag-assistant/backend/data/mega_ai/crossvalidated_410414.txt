[site]: crossvalidated
[post_id]: 410414
[parent_id]: 409645
[tags]: 
First things first Maybe a simple solution is enough? Note that provided you have a reasonable number of measurements, you should be quite OK with just measuring the average total time and choosing the intervention that has the fastest one. The only thing a statistical analysis gives you on top of that is a) a new possibility to mess up and b) some assessment of uncertainty, i.e. whether you need to collect more data to be more certain. Given enough data even the tiniest difference will be significant. You are (I presume) making an engineering choice and not publishing a scientific paper. For the following I assume you want to do statistics. What do you care about? It is also IMHO good idea to think about what difference are you actually interested in. Do you really care about the difference in mean performance? Or do you care about some sort of worst-case scenarios? I would also suggest that for your use case, you might be better served by estimating the pairwise differences with some measure of uncertainty rather than by testing hypothesis - if you see that there is "no significant difference" betweeen treatments A and B it is way less informative than e.g. "95% CI for mean performance of A is (10;30), for B is (15;50) and for the difference A-B is (-45,18)" - in the latter case you are not certain which is better, but you see that B has higher risk of very large absolute times. Bootstrap - dirty but cheap A very simple but useful way to estimate uncertainty is the Bootstrap . A simple scheme you could use is to choose $N$ of the runs at random (with replacement) and compute all the quantities of interest (in your case probably all means and pairwise differences in means). Repeat this a lot - say 1000 times. This will get you 1000 "samples" of each quantity. You can than treat the middle 95% of the samples as your 95% uncertainty interval. $N$ could be equal to your actual number of runs or a bit smaller (say 0.95 * number of runs), you may want to test multiple values of $N$ and see if this affects results. Modelling The full-fledged (and most difficult) approach is to try to find a good model of your dataset. My suggestion would be to try brms or other flexible linear model package. I think a student-t response with log link or a gamma response could have reasonably fat tails to not be led astray by the outliers. Alternatively, you can try to find a transformation that makes your data look more normal and then fit a model with "normal" or "student" families (with identity link) - log(sqrt(x)) might be your friend, see https://en.wikipedia.org/wiki/Variance-stabilizing_transformation for some other ideas. Using brms you can then use posterior predictive checks (PPC) to test whether your response distribution at least roughly matches your data (e.g. ppc_dens_overlay ) and get posterior credible intervals for all the central tendencies and differences of interest, including (with some fiddling) differences in say 95% quantiles of runtimes, should you be interested. I can elaborate a bit more if you think this is a sensible approach for you.
