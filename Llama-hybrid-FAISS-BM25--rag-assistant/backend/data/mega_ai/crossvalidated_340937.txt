[site]: crossvalidated
[post_id]: 340937
[parent_id]: 340584
[tags]: 
To understand the intuition behind, one should probably know the evolution of the algorithms. 1. RNN An RNN network initially wants to model the sequence of inputs. By sharing the network parameters between the individual inputs, and using the output of previous input as part of the input for next computation, the sequence state can be accumulated along with the sequence, virtually achieving a variable-depth network. For this purpose, the most straightforward network design is to build a path between the hidden nodes (i.e., from the output of the hidden node to the input of it in the subsequent time-step). $$ c_t = \sigma( f \cdot c_{t-1} + g \cdot x_t) $$ Here, both $f$ and $g$ are parameters, hence single path suffices. This design is powerful enough to encode all the history information. But it has a problem of vanishing or exploding gradient. 2. Leaky Unit One way among others to overcome the problem is to use the leaky unit that provides linear self-connection in following way: $$ c_t = f\cdot c_{t-1} + (1-f) \cdot \hat x_t $$ Now since $c_t = f^{n}\cdot c_{t-n} + ...$, with $f$ being near one, the far distant history has no problem to be passed along. When $f$ being near zero, the history can be quickly forgotten. Leaky unit is a clever design. Then the problem is how to decide the value of $f$. It can be a constant, or a parameter, or a function of the history info. (It does not make much sense for $f$ to be a function of the input.) When we use a function for $f$, the function is called a gating function. Depending on the choice of $f$, leaky unit can be single or two paths . 3. LSTM The idea of using a gate function of the history info requires introducing another path between steps (i.e., from the output of hidden node to the input of the gating function.) So you have, $$ \begin{align*} f_t & = \sigma(h_{t-1}, x_t) \\ \end{align*} $$ And you know this is LSTM, if you have, $$ \begin{align*} c_t &= f_t \cdot c_{t-1} + i_t \cdot \hat x_t \\ h_t & = o_t \cdot c_t \\ & ... \end{align*} $$ With gating functions, the network can control very well how long and how short the memory should be. LSTM uses the output $h_{t-1}$ to gate not only the hidden node connection, but also the input and, especially the output from $c_{t}$ to $h_{t}$ , which makes the two paths hard to be merged into one. Then $h_{t}$ is supposed to encode best the information for both long and short memory. 4. GRU As such, you definitely can use the cell state to compute the gating functions, but the original setup is more straightforward - with clean separation between gating and cell state. Inspired by the idea, one can come up with different designs. GRU is then designed to use single path, by removing the output gating and output the cell state directly . Then the cell state can be used by the subsequent step for both state update and gating function computation. You can interpret it differently anyway. For example, the reset gate in GRU can be considered as a shifted output gate of LSTM: it shifts from the output of current step (in LSTM) to the input of next step (in GRU). $$ \begin{align*} c_t &= f_t \cdot c_{t-1} + (1-f_t) \cdot \hat x_t \\ f_t & = \sigma(c_{t-1}, x_t) \\ & ... \end{align*} $$ A figure is better than a thousand words. You can further simplify GRU with fewer gates, for example, with the same update gate for reset gate. The point is, from the viewpoint of "path", there is no essential difference no matter if it is one or two.
