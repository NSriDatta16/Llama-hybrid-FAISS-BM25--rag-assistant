[site]: crossvalidated
[post_id]: 69187
[parent_id]: 69179
[tags]: 
If you have a linear activation function, your neural network collapses to a linear model: the composition of a linear function with a linear function remains linear. Thus, your neural net will not be able to perform better than ridge regression. Indeed, you are optimizing something very similar with stochastic gradient descent which will result in sub optimal solutions very often. Ridge regression, on the other hand, can be solved in closed form and you will always get the best result wrt training error. If you want to have better results than ridge regression, you will need to use nonlinear activation functions.
