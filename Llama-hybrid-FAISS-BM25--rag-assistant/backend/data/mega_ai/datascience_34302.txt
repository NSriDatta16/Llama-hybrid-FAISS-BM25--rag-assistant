[site]: datascience
[post_id]: 34302
[parent_id]: 15903
[tags]: 
Is it known why convolutional neural networks always end up learning increasingly sophisticated features as we go up the layers? This is pure mathematics. A neural network, at the end of the day, is a big mathematical function. And the deeper the network, the bigger the function it represents. And by bigger, I obviously mean high-dimensional. The features learned are more sophisticated because they are the results of more sophisticated functions. What caused them create such a stack of features Interestingly enough, conventional neural networks were inspired by our own, actually cat's, biology. Hubel and Wiesel conducted experiments on the visual cortex of cats, and they realized that light was perceived by stacks of optic fields. This is what inspired convolutional layer and a deeper architecture.
