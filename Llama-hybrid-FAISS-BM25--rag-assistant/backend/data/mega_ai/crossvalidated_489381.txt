[site]: crossvalidated
[post_id]: 489381
[parent_id]: 
[tags]: 
Questions about object function and loss function in weighted logistic regression

According to what i learned in machine learning, the loss function is derived by the Maximum likelihood estimation of training data. Taking logistic regression as an example: we got a train data set $\{x^{(i)}, y^{(i)}\}(i=1,..n)$ , and assume the probability $y$ and the feature $x$ satisfy the formula $y = h(\theta^Tx) =\frac{1}{1+e^{-\theta^Tx}}$ . then we have the log likehood function on train data: $ln(L(\theta;x,y)) = \sum_{i=1}^{n}y^{(i)}lnh(\theta^Tx^{(i)}) + (1-y^{(i)})ln(1-h(\theta^Tx^{(i)}))$ and the loss is the negative log likehood function. $l(\theta) = \sum_{i=1}^{n}-y^{(i)}lnh(\theta^Tx^{(i)}) - (1-y^{(i)})ln(1-h(\theta^Tx^{(i)}))$ when i learned the weighted logistics regression, the loss function was given below: $l(\theta) = \sum_{i=1}^{n}-w_1y^{(i)}lnh(\theta^Tx^{(i)}) - w_0(1-y^{(i)})ln(1-h(\theta^Tx^{(i)}))$ the $w_1$ represent the weight on the positive sample and $w_0$ represent the weight on the negative sample.(of course you can make every single sample a specific weight but here we take the simple assumption) here comes my questions: why in weighted logistic regression the loss functions changes but the objective function keep the same as object function in logistic regression? in my opinion the loss function is derived by the likehood function and the likehood function is derived by the objective function, so the the objective function and the loss function are connected, it should not happen that one change but another remains. thanks for any reply!
