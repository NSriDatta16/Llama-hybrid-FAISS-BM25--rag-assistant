[site]: crossvalidated
[post_id]: 501175
[parent_id]: 
[tags]: 
Two image channels, one quantitative, one "relative". Best architecture/preprocessing?

I'm performing semantic segmentation on a multi-channel image with a residual U-NET. I'm getting DICE scores that are ok for this task but I want to do even better. The problem is that the first channel has no intrinsic scale. It's like a photograph, where the intensity level might depend on (random) illumination when the picture was taken, it might have a slowly-varying bias etc...etc... So I want my network to learn to look only at contrasts. So I normalize this channel according to the median intensity of each acquisition. My second channel instead is quantitative. I know that a specific grey-level value has the same meaning in every single subject and acquisition. Therefore I normalize the intensity according to the maximum possible value I can get. The I build a CNN where I pass in input the two channels as they were two color channels. Is this the best thing to do? Or does this confuse the network? To help the network I augment my image by randomly scaling and changing the illumination of the first channel but not the second. Are there different architectures that might be better? Like keeping the two images separate in the decoding path? Any other idea?
