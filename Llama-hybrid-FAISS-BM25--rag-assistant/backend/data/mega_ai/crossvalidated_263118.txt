[site]: crossvalidated
[post_id]: 263118
[parent_id]: 263111
[tags]: 
Interesting question. I'm not familiar with such a definition. You can evaluate that using cross validation. Do cross validation for X times 1.1 In each time measure the accuracy on both the training set and test set Test whether the train accuracy and test accuracy are likely to come from the same distribution (e.g. using Two-Sample t-Test for Equal Means ) Note that this procedure ignores the model complexity. If your model size is tiny with respect to the dataset, it cannot encode much of it. If you see a significant difference between the train and test accuracies there might be another problem there. Example of such a problem is a difference between your train and test datasets. On the other hand, if you have a huge model and the accuracies are similar, you still might have over fitted but also failed to exploit that overfitting. Example of such a case is a large random forest while a one of its trees contributes most predictive power.
