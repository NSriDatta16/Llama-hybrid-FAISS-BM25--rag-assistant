[site]: crossvalidated
[post_id]: 232198
[parent_id]: 231738
[tags]: 
The Fisher test is one way to test whether there are differences in the distribution of cases among cells of a contingency table. If these sample data are much smaller in numbers of cases than your "real" data then a chi-square test might be better. Neither test specifies exactly how the distribution differs. Typically a display of the contingency table is fairly self explanatory, although if you are writing this up for others then you might want to add some discussion based on your knowledge of the subject matter. It seems that you have added a category of "missing" to your 4 numeric categories. That might not be wise. Handling of missing data is tricky, particularly if the probability of a value's being missing is somehow related to its actual value. The missing-data tag on this site has many useful posts. You might also follow the multiple-imputation tag to see if that is appropriate or helpful for your application. In your case, without imputation, I might first determine whether groups A and B differed in the frequency of missing data, then proceed to analyze the non-missing values. If you are interested in some measure of the average difference in scores between groups A and B, and particularly if you are interested in whether there are differences between Test 1 and Test 2, or interactions between the Groups and the Tests, then you should instead follow @whuber's recommendation in a comment and just treat the values as numeric in a linear model or ANOVA. As he notes, even though your data can't be normally distributed, in practice with large numbers of cases and these types of data the normal-theory methods will generally work well enough. After thinking about this, I prefer his suggestion over mine, as you have an ordered numeric score rather than simply a set of arbitrary non-ordered categories. Note that the default for numeric tests in R (whether lm or wilcox.test ) is to ignore missing data. So you should try to understand the nature of the data's missingness, whether you proceed with contingency tables or with numeric data analysis. Finally, note that the "Warning" from wilcox.test was just that: a warning rather than an indication that your results are incorrect. The test was OK, reporting a p-value based on a normal approximation rather than an exact p-value based on the data values. If you had 50 or more cases, your call to wilcox.test would not even have tried to calculate exact p-values. The coin package in R has a wilcox_test function that can calculate exact p-values in the presence of ties, but I see no need here for an exact p-value.
