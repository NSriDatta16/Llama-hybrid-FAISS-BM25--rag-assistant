[site]: datascience
[post_id]: 123538
[parent_id]: 5178
[tags]: 
Generally speaking, there are 2 different approaches. Store data in 1 system and use another system to keep manage the overhead of version control using another system: If you have access to a blob store (like S3 by Amazon), then tools like DVC work pretty well for this. I believe they've already been mentioned in this thread. The benefit here is that blob stores essentially have no storage limits so you don't have to worry about the scale of your data. The downside is the workflow is a bit less intuitive and you have state stored in a few different systems. Use a scalable system that can both version your data & metadata: The other approach is to use a platform can do both. I work at XetHub and this is the approach we're taking. Here's an example repo containing Meta's Llama 2 models, which are basically binary files. Because we deduplicate repetitions in the data, we're able to store 660 GB of files using just 568 GB. You can keep using Git as the interface if you'd like or use our simplified Xet command line interface instead. The downside here is that XetHub can scale to petabytes of data in your repos but we may struggle at the 100 PB+ scale that Amazon S3 can support. Regarding your checklist, XetHub doesn't require you to run a server, shows you diffs in pull requests, etc.
