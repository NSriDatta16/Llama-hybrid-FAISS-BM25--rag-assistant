[site]: datascience
[post_id]: 36397
[parent_id]: 
[tags]: 
Is my model overfitting when I add new features?

I'm working on simple 2-class classification problem. Nearly all features we have used (except one) are about the same for both classes: A random forest classifier confirms that one feature has an "importance" of 40% and the rest have about 8% or less. To try to boost performance, I included 4 new features as an experiment. These features all have VERY different distributions in the two classes (their means are quite different) so they should be useful for discriminating between the classes. However, when I included these, performance dropped significantly. Performance details: We are trying to achieve 95% precision (that is, 95% of things classified as "class 1" are indeed class 1). Using the old feature set, we could achieve about 96% recall (96% of things that ARE class 1 are classified as 1). The new features, however, drive recall down to 95% or below. I strongly suspect the model is now overfitting. However, I can't find any evidence of this. The accuracy of the model on TRAINING data used to be about 1% higher than on TEST data. With the 4 new features, the accuracy is only 0.8% higher - suggesting the model is actually overfitting LESS than before. I am currently using the "best performance" numbers of an SVM. I have also tried these things on a random forest model and the results have been the same. Is it possible I'm measuring overfitting wrong? Or is there another way that adding new features can degrade the performance of a classifier?
