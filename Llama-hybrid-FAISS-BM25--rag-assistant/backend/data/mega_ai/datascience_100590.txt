[site]: datascience
[post_id]: 100590
[parent_id]: 36388
[tags]: 
I dug into this a bit over at this answer , including a Colab notebook . I'll mostly refer you to that answer for the references, but I'll summarize here: At some point in time, xgboost had different behavior for low-cardinality discrete features, splitting at the actual data values; whereas continuous features have always(?) split at midpoints between training data values. The current behavior though is to always split at midpoints, with no special treatment of discrete features. (Unordered categoricals now have some experimental handling, but that's an entirely different matter.) In your quote from the paper: exact split finding algorithm enumerates over all the possible splits on all the features to find the best split "all possible splits" can be understood to mean splits of the training data into two pieces (left and right), and so doesn't need to mean anything about whether the actual feature values or some point between values is used.
