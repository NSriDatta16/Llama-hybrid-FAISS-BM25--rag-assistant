[site]: datascience
[post_id]: 15921
[parent_id]: 
[tags]: 
Questions When Advancing from Vanilla Neural Network to Recurrent Neural Network

I've recently learned how a vanilla neural network would work, with given number of inputs, hidden nodes, and the same number of outputs as inputs. I've been looking at various posts now related to recurrent neural network, and I understand the concept behind it, but I fail to understand certain parts of the logic behind the RNN. Here are four main questions I have: How does back-propagation work in recurrent neural network? Are the weights that lead from the input to the hidden nodes the same for every other step? What about the weights from the hidden nodes to the outputs? How do biases exactly work in a recurrent neural network? Why is tanh function usually used instead of sigmoid function as the activtion function? I realize some of these questions are very basic, but I guess the basics are exactly what I need right now. Even links to related videos or posts will be extremely helpful, and so will google keywords that show the right results. These four questions are hindering me from understanding sample Python codes, so I really need some help.
