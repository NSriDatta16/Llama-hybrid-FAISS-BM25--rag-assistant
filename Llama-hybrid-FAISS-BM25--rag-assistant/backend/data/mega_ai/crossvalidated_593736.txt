[site]: crossvalidated
[post_id]: 593736
[parent_id]: 
[tags]: 
Relation between validation set loss and Test loss in Machine learning

I am very new to Machine learning. I am trying to implement a feedforward neural network to predict release year of songs based on some audio features. I have a train.csv file and dev.csv file. I use train.csv to train the model and side by side evaluate my model after each epoch of training on dev.csv data by calculating RMSE. After fully training the model, i need to submit my predictions on "test" data in Kaggle competition where it evaluates and displays RMSE on my predictions on test data. Now the problem i am facing is, say i train my model for 100 epochs and at the end RMSE on "dev" set is 90. Then when i use this model to make predictions on test set, i get RMSE of 30. Now i tune the model further and manage to reduce my RMSE on dev set to 56, then i expected my RMSE on test set would also decrease. But instead it increased to 38. And this has happened quite a few times, as in, i thought one evaluates the model accuracy on dev set because it is a unseen data and model has not been trained on it, so performance on dev set must also in some way directly correlate to performance on test set. But in this case i am not able to see any correlation. Since my end task is to make RMSE on test set as low as possible, how can i go about it. Because even if reduce dev set loss, it is not refelecting on test set. Is it possible that my dev set data is NOT A RELIABLE source? If so, what techniques can i use to get a dataset on which i can get a "reliable" model performance accuracy?
