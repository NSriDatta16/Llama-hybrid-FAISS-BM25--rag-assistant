[site]: crossvalidated
[post_id]: 315921
[parent_id]: 315710
[tags]: 
It's very useful. In text classification using bag of words you can routinely run into tasks where number of features is much bigger than number of examples. That means when you try to fit linear model, you'll have problems, since the corresponding linear system is underdetermined. L2 regularization is often used to deal with underdetermined linear systems. L1 is used for this too, and it also has an advantage of enforcing sparsity, thus making model simpler to interpret (if you want to read more about that, you can read about maximum a posteriori estimation or bayesian linear models). As an example you can see this page from scikit-learn documentation.
