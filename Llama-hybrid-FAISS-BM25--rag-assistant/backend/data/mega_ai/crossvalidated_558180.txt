[site]: crossvalidated
[post_id]: 558180
[parent_id]: 558158
[tags]: 
This is indeed a good question. I think a lot of people have an incomplete understanding of the exogeneity assumption. Nevertheless, this assumption is crucial, so I will elaborate a bit on that. Let's consider a simple linear regression model with $n$ observations and $k$ regressors: $$ \boldsymbol{y}=\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{u} $$ Most textbooks define strict exogeneity as: $$ E(u_i \vert \boldsymbol{X})=0 \quad, i=1,\dots,n $$ In plain English, this means that no regressor contains useful information for the prediction of $u_i$ . If you are learning about regression for the first time, you probably wonder why we are making this assumption. The simple answer is that it makes our lives easier. A violation of this assumption makes things very complicated. For instance, OLS becomes inconsistent etc. So it would be nice for this condition to hold. Relationship between $E(u_i \vert \boldsymbol{X})=0$ and $Cov(x_{jt},u_i)=0$ First of all, we need to derive some results that follow from the zero conditional mean assumption. The first important result is that the unconditional expected value of the residuals is also equal to zero. $$ E(u_i)=E(E(u_ i\vert \boldsymbol{X}))=E(0)=0 $$ The second important result is that the regressors are orthogonal to the error term for all observations $i=1,\dots,n$ . That means: \begin{align} E(x_{jt}u_i)=0 \quad, i,j=1,\dots,n; \, t=1,\dots,k \end{align} To proof that write: $$ E(u_i \vert x_{jt})=E(E(u_i \vert \boldsymbol{X}) \vert x_{jt})=0 $$ Thus: $$ E(x_{jt}u_i)=E(E(x_{jt}u_i\vert x_{jt}))=E(x_{jt} E(u_i\vert x_{jt}))=0 $$ The key thing to note here is that the regressors are orthogonal not only to the error term of the same observation but also to the error term from the other observations. This allows us to write: $$ Cov(u_i,x_{jt})=\underbrace{E(x_{jt}u_i)}_{=0}-E(x_{jt})\underbrace{E(u_i)}_{=0}=0 $$ Therefore, we can derive from the the orthogonality condition that the regressors are contemporaneously uncorrelated with the error term. However, since you can write $$ Cov(u_i,x_{jt})=E(x_{jt}E(u_i\vert x_{jt}))-E(x_{jt})E(E(u_i \vert x_{jt})) $$ you see that the contrary must also hold, i.e., $Cov(u_i,x_{jt})=0$ implies zero conditional mean $E(u_ i\vert \boldsymbol{X})=0$ . Therefore, both conditions are equivalent to each other. I think that most people are somewhat unfamiliar with conditional expectations and that this is the reason why the latter definition is used more often. An alternative definition However, note that some textbooks have a slightly different definition of exogeneity. They define exogeneity as: $$ E(x_iu_i)=0 \quad, i =1,\dots,n $$ This assumption is weaker than the assumption $E(u_i \vert \boldsymbol{X})=0$ , since this implies that for every measurable function $f(.)$ : $$ E(f(\boldsymbol{x}_i)u_i)=E(E(f(\boldsymbol{x}_i)u_i\vert \boldsymbol{x}_i))=E(f(\boldsymbol{x}_i)E(u_i \vert \boldsymbol{x}_i))=0 $$ Fun fact: To derive asymptotic results, you just need the weaker assumption and the assumption $E(u_i \vert \boldsymbol{X})=0$ is usually violated when dealing with time series data.
