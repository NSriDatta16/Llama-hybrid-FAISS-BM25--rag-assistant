[site]: crossvalidated
[post_id]: 139907
[parent_id]: 
[tags]: 
How to do machine learning (regression/classification) when the samples are of different sizes?

In standard cookbook machine learning, we operate on a rectangular matrix; that is, all of our data points have the same number of features. How do we cope with situations in which all of our data points have different numbers of features? For example if we want to do visual classification but all of our pictures have different numbers of pixels, or if we want to do sentiment analysis but all of our sentences have different amounts of words, etc. I think the normal way would be to extract features of regular size from these irregularly sized data. But I attended a talk on deep learning recently where the speaker emphasized that instead of hand-crafting features from data, deep learners are able to learn the appropriate features themselves. But how do we use e.g. a neural network if the input layer is not of a fixed size?
