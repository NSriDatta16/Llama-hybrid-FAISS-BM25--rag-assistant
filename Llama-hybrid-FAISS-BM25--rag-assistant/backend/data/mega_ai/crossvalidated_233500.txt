[site]: crossvalidated
[post_id]: 233500
[parent_id]: 232829
[tags]: 
Answering in reverse order: c) Changes in apparent significance as other variables are added or removed from a model are common in regressions. Your suggested explanation is one possibility. See this discussion as one of several on this site that examine this phenomenon. b) "Formal inference" in the way you might hope is difficult in LASSO. What you can say is what you found: the texture-only model found no predictor variables that performed better in terms of deviance than the null, intercept-only, model upon 10-fold cross validation. That would seem to be evidence against the usefulness of the texture variables on their own. a) One should be cautious in judging whether methodology is "airtight from a publication point of view." That depends too much on the audience, the journal, the luck of the draw in editors and referees, and the quality of statistical review. You have taken some wise steps: decreasing dimensionality based on subject-matter knowledge (semantic features) or without reference to the outcome variable (principal components of texture features), using LASSO as a principled way to select predictors, with cross-validation to choose the penalty, and using well established software for pre-processing and analyzing the data. The main difficulty I see is with interpretation. "Significance" is problematic in at least 2 ways. First, as with any variable-selection technique, the particular features selected by LASSO can change substantially among bootstrap samples from the same data, raising the issue of whether the selected features are in any way the "true" predictors. That might be less of an issue in your texture-only model with orthogonal predictors from PCA, but could arise in your last, combined, model in which there might be correlations of principal components of the textures with the semantic variables. Second, although LASSO is well accepted as a useful way to select predictor variables, significance testing in LASSO does not yet seem to have generally agreed upon methods. Lockhart et al have proposed a covariance test statistic in the context of linear regression, with some indications that it could also work in logistic regression, which is available in the covTest package in R. There seems, however, to be substantial unresolved discussion about this approach, at least to the eyes of someone not well trained in mathematical statistics. A few other issues to consider: The CI in the cross-validation are based on the number of folds, so you shouldn't worry about overlap. The choice of penalty based on minimum cross-validation deviance is a useful heuristic, but shouldn't be thought of as a statistical significance test. In that respect it is similar to use of the Akaike Information Criterion for selecting the best among competing models without claiming "significance" for the result. Accuracy is, perhaps counter-intuitively, not a very good way to gauge the success of a logistic regression. AUROC is certainly better, and there are others that you can consider. See, for example, this Cross Validated page . You might want to examine the order of entry of the variables as the penalty is relaxed in your final model.
