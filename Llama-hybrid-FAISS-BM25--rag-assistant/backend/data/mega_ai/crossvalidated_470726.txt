[site]: crossvalidated
[post_id]: 470726
[parent_id]: 56950
[tags]: 
An in-depth explanation of skip connections from multiple perspectives can be found here: https://theaisummer.com/skip-connections/ Here I provide the main point from the article: Basically, skip connection is a standard module in many convolutional architectures. By using a skip connection, we provide an alternative path for the gradient (with backpropagation). It is experimentally validated that these additional paths are often beneficial for the model convergence. Skip connections in deep architectures, as the name suggests, skip some layer in the neural network and feeds the output of one layer as the input to the next layers (instead of only the next one). As previously explained, using the chain rule, we must keep multiplying terms with the error gradient as we go backwards. However, in the long chain of multiplication, if we multiply many things together that are less than one, then the resulting gradient will be very small. Thus, the gradient becomes very small as we approach the earlier layers in a deep architecture. In some cases, the gradient becomes zero, meaning that we do not update the early layers at all. In general, there are two fundamental ways that one could use skip connections through different non-sequential layers: a) addition as in residual architectures, b) concatenation as in densely connected architectures. I hope it clarifies your understanding!
