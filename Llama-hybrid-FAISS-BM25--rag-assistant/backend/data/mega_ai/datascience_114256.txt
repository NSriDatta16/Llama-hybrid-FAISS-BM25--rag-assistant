[site]: datascience
[post_id]: 114256
[parent_id]: 66832
[tags]: 
Spark has a library to run distributed machine learning tasks (Spark MLlib). You can use a Spark frontend for R (SparkR) or Python (PySpark) to train your models. Take a look: https://spark.apache.org/docs/latest/sparkr.html#machine-learning Also, you can try to find a ML lib that provides a native API to train a model in Spark. Here's a quick start to train a classification or regression model using the Catboost lib (boosting) with PySpark: https://catboost.ai/en/docs/concepts/spark-quickstart-python Alternatively, you can combine native R algorithms and SparkR User-Defined Functions (UDF) to distribute your tasks: "100x Faster Bridge between Apache Spark and R with User-Defined Functions on Databricks" ( Databricks ). Here's an example how to perform grid search in a distributed way using a Sklearn standardlized algorithm and PySpark (you can adapt it for your needs with R and SparkR!): "Leveraging Machine Learning Tasks with PySpark Pandas UDF" ( Neoway ).
