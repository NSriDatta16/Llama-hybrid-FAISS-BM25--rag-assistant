[site]: crossvalidated
[post_id]: 628074
[parent_id]: 628073
[tags]: 
You are on the right track with your computations. We will define our prior and likelihood distributions by unnormalized probability distributions, then show that the mode of the resulting posterior distribution is the minimizer of a related loss function. In particular, we have $$P(X|\theta) \propto \exp(-L(\theta, X)),\quad P(\theta)\propto \exp(-R(\theta)).$$ The posterior distribution is then given by $$P(\theta|X)\propto P(X|\theta)P(\theta) = \exp(-L(\theta,X)-R(\theta)).$$ We can now show the equivalence between MAP estimation and an unconstrained minimization problem. In particular, the following is true $$\mathrm{argmax}_{\theta}~\exp(-L(\theta,X)-R(\theta)) = \mathrm{argmin}_{\theta}~L(\theta,X)+R(\theta).$$ This follows from the fact that $\log(-x)$ is a monotone decreasing map on $(0,\infty)$ . Without making more assumptions on specific model forms and regularity, it is difficult to say much more in general, but this highlights the roles that the prior and likelihood play in Bayesian inference: High likelihood probability enforces low data misfit, represented here by $L$ , and the priori distribution plays the role of a regularizer, represented here by $R$ . For an example, consider a simple scalar model $X = f(\theta) + \varepsilon$ , where $\varepsilon$ is iid Gaussian noise of the form $\mathcal{N}(0,\sigma^2)$ . Suppose also that, for whatever reason, we wish to enforce a Laplace prior on $\theta$ , i.e., $\theta\sim\mathrm{Laplace}(0,1/\lambda)$ . If we have $N$ realizations of $X$ denoted $x_1,\dots,x_n$ , then our likelihood and prior are given by $$P(X_1=x_1,\dots,X_n=x_n|\theta)\propto\exp\left(-\frac{1}{\sigma^2}\sum_{i=1}^N (f(\theta)-x_i)^2\right), \quad P(\theta)\propto\exp(-\lambda|\theta|).$$ The posterior is then given (up to a normalization constant) by the product of these distributions. The MAP problem is then equivalent to the following minimization problema fter applying the $\log(-x)$ map: $$\min_\theta~\frac{1}{\sigma^2}\sum_{i=1}^N (f(\theta)-x_i)^2 + \lambda|\theta|,$$ which is a LASSO problem for $\theta$ . Notice that this explicitly shows the regularizing effect of the prior: In the limit of zero noise or infinite amounts of data, the prior becomes meaningless and the likelihood dominates. However it biases $\theta$ towards $0$ in most cases, and even allows for a unique solution in the limit of infinite noise when the data is essentially meaningless. This procedure should provide the form you are looking for modulo some normalization constants. I will also note that I have abused notation a bit by conflating probabilities with probability densities for this analysis, but Bayes' Theorem is basically the same for densities w.r.t. Lebesgue measure so the logic is essentially the same with worse notation.
