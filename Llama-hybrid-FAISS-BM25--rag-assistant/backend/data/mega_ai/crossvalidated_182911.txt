[site]: crossvalidated
[post_id]: 182911
[parent_id]: 182907
[tags]: 
Increasing the number of parameters almost always improves the goodness of fit but with increase of parameters comes increase in complexity and the problem of over fitting. AIC (Akaike information criterion) penalizes the model for each increase in the parameter. AIC measures the amount of information lost when we use a model to approximate the truth (actual distribution). Although we don't know what the truth is but still AIC will give the measure of information lost when we have to choose between different models (in relative sense). Note that you can't say anything about the absolute loss of information since you don't know what the truth is. So choosing a model with high AIC compared to small one is not advisable but if the AIC value is close enough, a weighted average of these models can be used for statistical inference. This wikipedia link briefly explains about AIC and the weights.
