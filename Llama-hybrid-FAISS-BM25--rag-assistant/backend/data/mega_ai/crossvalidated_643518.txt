[site]: crossvalidated
[post_id]: 643518
[parent_id]: 
[tags]: 
Transformations for extremely skewed data for Binary Logistic Regression

Good evening everyone, I have a dataset with 5000 observations, and 10 explanatory and 1 response variable (binary 0 or 1), and my task is to make a logistic regression model that can ideally predict well as well as help understand the situation and which variables are most impactful (so not a black box). All of the variables are extremely skewed, although in theory this should not be much of an issue for logistic regression, throughout my EDA and preprocessing stage (analyzing + imputing missing values) I relied on transformations to visualize the data, because without them the plots are practically unreadable. After fitting an initial model and looking at residual plots, and putting transformed variables into the model (as suggested in Simon J. Sheather "A Modern Approach to Regression with R") the transformed terms are far more impactful and significant, showing that so far there is overwhelming evidence in favor of implementing transformation for predictors. TLDR My question is as follows , for variables that have zero values and/or negative values as well as positive ones, what are the best transformations? From bestNormalize package in R, the suggested transformations were orderNorm (ORQ), Yeo-Johnson and arcsinh(x). In cases where there is a very small negative value present, or only zero+positive ones sqrt(x + a) and Log_b(x+a) were suggested too (adding a constant to make values positive, then apply log/sqrt). I am completely unfamiliar with orderNorm and Yeo-Johnson, and want to ask about any theoretical implications or drawbacks about using them.
