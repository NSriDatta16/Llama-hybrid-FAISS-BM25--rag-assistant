[site]: crossvalidated
[post_id]: 425014
[parent_id]: 
[tags]: 
Training models for multiple epochs vs one "super epoch"

This is a conceptual question regarding model training (e.g. CNNs). I have since solved the issue that raised this question, but I was still curious. Preliminaries: In the typical training setting, we have $N$ training examples, which we batch to use with mini-batch SGD (or similar optimization). One run through all training examples is a single epoch. Let's say you plan to run $M$ epochs. Now the question: During the training it's generally good practice to shuffle the data such that the mini-batches are not the same during each epoch. If one is using the mini-batches to update gradients, can one instead train over a "super-epoch" by pre-shuffling the data and feeding it $N\cdot M$ training examples? That is, perform $M$ random shuffles (without replacement) and concatenate to generate $N \cdot M$ examples. Then train for only a single epoch. Is the only downside of this (say, for TensorFlow) that you cannot check the progress of accuracy/etc. of your validation set following each epoch? I know that is a pretty big downside, and I'm not advocating for this method...was just curious if there was anything else I was missing in my understanding.
