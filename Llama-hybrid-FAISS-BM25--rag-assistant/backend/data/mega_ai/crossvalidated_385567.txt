[site]: crossvalidated
[post_id]: 385567
[parent_id]: 296493
[tags]: 
I use exactly this method to estimate the performance of my predictor, and to find an optimal threshold (for a binary classifier) after hyper-parameter tuning, typically when using a gradient boosting machines method, but I still use cross validation for the parameter tuning itself. I have had success using this method as well to determine the coefficients for a logistic regression: do say a 1000 test train splits and train the model for each...then construct the histograms for each of the coefficients and in some cases I would use the mean, in others the mode, to get "averaged" coefficients which provide the most "robust" solution. I have not tried it, but I guess one could perhaps also do hyper-parameter tuning in this way. The idea of the bootstrap as I understand it is to get an estimate of the variability of the data, and this method addresses the same problem. Especially with smaller datasets results can vary considerably from one test-train split to the next, and so if you try to select for example a (classification) threshold for a binary classifier probability score based on a single test-train split, you are on shaky ground. Essentially, once you deploy the model the first batch of data your estimator sees might differ a lot from the test set which you used to optimize the threshold. Repeated test-train splits "simulates" the variability of incoming data, and based on the distribution of thresholds you can then select a more robust threshold.
