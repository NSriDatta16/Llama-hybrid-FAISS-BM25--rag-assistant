[site]: crossvalidated
[post_id]: 296952
[parent_id]: 296944
[tags]: 
This depends on many things, but generally, it is difficult for Q-learning to just "adapt" to a new environment. By adapt, I assume you expect the robot to be able to find the ball without retraining the entire system. Obviously, the most fail safe way to make the algorithm work in any new scenario is to retrain the entire system. The encoding of the state of the environment can be critical. For example, if you encode the state as the $(x,y)$ coordinate position of the robot, then it will not adapt at all when you change the position of the ball. This is because the Q-value of the original position of the ball was probably quite high, but by moving the ball, you have completely changed the Q-values of the game, and using the old Q-values will not make sense. On the other hand, if you have a very sophisticated encoding of the state, including say, a 3D point cloud representing the world and an integrated history of the visual inputs from a camera, it may be possible to train a Q-learning algorithm in a way so that it can adapt to the more diverse examples in your post.
