[site]: datascience
[post_id]: 23776
[parent_id]: 16921
[tags]: 
The original Savitzky-Golay paper addressed smoothing , meaning that you estimate a value for some point in the past, using values from its past and future. It sounds like what you're interested in is filtering : at time step k, you want to reduce noise and come up with a better estimate of the value at the same time step k, without knowing the future. That's important in many fields such as process control, and some fault diagnosis where delays can't be tolerated. The SG approach of calculating the constants for least squares fitting for smoothing has also been done for filtering. A curve is fit through n recent points, and instead of estimating the middle point, the end point is estimated by a least squares fit, reduced to simple multiplications by constants. There are tables of the necessary coefficients and an example at https://gregstanleyandassociates.com/whitepapers/FaultDiagnosis/Filtering/LeastSquares-Filter/leastsquares-filter.htm There is no time delay in the calculations, unlike with smoothing. Also, if the input signal is a ramp, the filter will converge to the correct value once all of the points in the time window reflect that ramp. On the other hand, there are some side effects that may or may not matter to you. Even in the linear case, for a step change in the input, there is overshoot in the filter output. Higher order least-squares filtering can be quirkier, so it is rarely used. But the linear version overall is quite effective, assuming the little bit of overshoot isn't a real problem (for instance, if you don't see step inputs in the data anyway). The link above is part of a more general "Guide to Filtering", that covers some other common filters as well, such as the exponential filter and moving average filters in particular. (Filters don't have time delays, but most do lag the inputs - meaning smear out changes over time. That's a byproduct of filtering out the noise, essentially cancelling out some new noise by a certain amount of averaging against old noise). It also covers general issues for filtering, such as aliasing. There are two other filters that are not covered in the Guide that might be considered close competitors to the SG-style filtering: the alpha-beta filter and double exponential smoothing. These are covered in Wikipedia: alpha-beta filtering has its own page, and double exponential smoothing is part of the page on exponential smoothing. The goal of both is to filter/estimate both the values and the derivative, resulting in a filter that will give correct outputs to a ramp input signal. I've long suspected that these two filters are essentially the same, but never taken the time to look into it.
