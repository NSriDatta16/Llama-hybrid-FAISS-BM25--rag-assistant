[site]: crossvalidated
[post_id]: 638814
[parent_id]: 
[tags]: 
Regression At Scale: Best Practices Around Ensuring Quality of a Large Numbers of Forecasts

Background Often I am forecasting possibly one up to a few dozen variables in a project, but I have an upcoming project that will involve forecasting thousands of variables. I have some ideas of my own, but as always it is wise to try to get some guidance when you're trying something new. Question I would like some reference(s) or guidance on how to perform forecasting on a large number of variables. My main concerns are about ensuring quality of the forecasts. Some Thoughts So Far Some of the variables are related to each other in a hierarchy, so consider taking advantage of partial pooling by having hierarchical models over those sets of variables should improve statistical precision. The problem is a conventional statistical forecast. No mention of making interventions. Many steps (e.g. train/test split, training, evaluation, prediction etc) will be the same as when I do them on a small scale and are quite automatable. Main challenges I see are around things that I would normally do manually, such as visually inspecting EDA and diagnostic plots or summaries. A lot of value can be obtained from that moment, "hmm, that looks weird". Perhaps some data mining or clustering could come in handy. E.g. clustering time series and then focusing on viewing samples from the clusters along with paying attention to time series that do not nicely fit into clusters might help me triage where to put my time/attention/energy.
