[site]: crossvalidated
[post_id]: 475745
[parent_id]: 
[tags]: 
Maximum Likelihood with Least Squared Error

In the book Tom Mitchell - Machine Learning , while deriving Least Squared Error from maximum likelihood, the author considers the training dataset of the form: $ $ where: $$d_i = f(x_i) + e_i$$ Here, $f(x_i)$ is the noise free value of the target function and $e_i$ is the random variable representing noise, which is distributed according to normal distribution with $0$ mean. The author then says that given the noise $e_i$ obeys a Normal distribution with 0 mean and an unknown variance $\sigma^2$ , each $d_i$ must also obey a Normal distribution with variance $\sigma^2$ , centered around the true target value $f(x_i)$ . Can anyone please explain that if the error $e_i$ is Normally distributed, then why should $d_i$ also be Normally distributed ?
