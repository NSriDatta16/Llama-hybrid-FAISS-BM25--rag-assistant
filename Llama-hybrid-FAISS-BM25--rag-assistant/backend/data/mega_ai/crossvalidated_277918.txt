[site]: crossvalidated
[post_id]: 277918
[parent_id]: 277367
[tags]: 
1.If I understood correctly, x can be formed as an M×H matrix, P is H×(CD) , y is (CD)×1 , then exp(x×P×y) would give a M×1 matrix, which is used as an unnormalized probability for each word. 2.It seems dot_article_context is the result of x×P×y , and attention (which is obtained by applying softmax to dot_article_context ) is the probability p . My guess is that the code uses the same value opt.bowDim for D and H , and the P weight matrix is coded here: -- Title context embedding. local title_context = nn.View(D2, 1)( nn.Linear(N * D2, D2)(nn.View(N * D2)(title_lookup))) in the linear layer nn.Linear(N * D2, D2) , so title_context should be the result of P×y . In this case computing P×y first instead of x×P is more efficient since x×P will give a larger matrix.
