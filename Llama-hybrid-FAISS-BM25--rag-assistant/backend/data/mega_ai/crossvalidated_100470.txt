[site]: crossvalidated
[post_id]: 100470
[parent_id]: 100438
[tags]: 
Just a few remarks. In general, if $N$ is large enough then vague priors can't harm, because the posterior distribution is shrunk towards the likelihood. Strictly speaking, looking at your data to choose your prior distribution is not "fully Bayesian", because your priors should come from previous knowledge. However, in the empirical Bayes approach (see Carlin and Louis , chap. 5) the observed data are used to estimate the prior distribution and then proceed as though the prior were known. At last, you could look at Gelman , who recommends weakly-informative prior distributions, and a folded-noncentral- t or a half-Cauchy instead of the inverse-gamma prior for $\sigma^2$.
