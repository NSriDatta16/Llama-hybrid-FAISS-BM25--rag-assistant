[site]: crossvalidated
[post_id]: 110161
[parent_id]: 110153
[tags]: 
The problem in the first part of your question is wrongly formulated given that the conditional density does not exist. According to slide 44, what you are interested in is the Posterior Predictive Distribution , which is a well-defined object in Bayesian inference. Once you have a posterior sample, $\theta_1,\dots,\theta_N$, of the parameters of a model with density $f(\cdot\vert \theta)$ and a certain prior, this is a sample from $\theta\vert \text{Observations}$, then you can approximate the predictive distribution by: $$\pi_{pred}(x_{n+1}\vert Observations)\approx\dfrac{1}{N}\sum_{j=1}^N f(x\vert\theta_j)$$
