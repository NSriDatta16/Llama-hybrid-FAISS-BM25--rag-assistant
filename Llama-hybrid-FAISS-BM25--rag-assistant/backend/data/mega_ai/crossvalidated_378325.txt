[site]: crossvalidated
[post_id]: 378325
[parent_id]: 
[tags]: 
Splitting time series with dummies

I'm doing a linear regression for corporate default rates. However, there is a big financial crisis (2008) in the middle of my time series. You can see the plot below. In order to fit a linear regression to ir, my professor's idea was to use dummies and split the time series for pre and post crisis and examine the significance of the independent variables (GDP growth, interest rate, debt/GDP) on the default rate in the two situations. So the basic idea is creating a dummy for each variable, multiply them with the dummy and this way use twice as many variables. The part I do not completely understand if I should use two series of dummies (2 times 3) and completely split the time series this way, or let one set of the variables be relevant for the whole series? Here are my two equations, which I'm trying to decide between: defrate = c + b(1)* d1 * Y1 + b(2)* d1 * Y2 + b(3)* d1 * Y3 + b(4)*Y1 + b(5)*Y2 + b(6)*Y3 or defrate = c + b(1)* d1 * Y1 + b(2)* d1 * Y2 + b(3)* d1 * Y3 + b(4)* d2 * Y1 + b(5)* d2 * Y2 + b(6)* d2 * Y3 where d1 is a dummy that is 1 for post-crisis values, and 0 for others d2 is the opposite, 0 for post crisis values, 1 for values before that
