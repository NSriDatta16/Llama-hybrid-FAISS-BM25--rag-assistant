[site]: crossvalidated
[post_id]: 228279
[parent_id]: 
[tags]: 
Are there analogs to adversarial boosting for non-image datasets?

As context, adversarial boosting is a technique originally designed to increase robustness of ML methods for image recognition. Start with a training example that is correctly identified, solve an optimization problem to find the minimum perturbation that causes the model to give it another classification, and then add the perturbed example to the training dataset with the original label . For images, the number of pixels modified (or the amount that the color in those pixels was modified) both work well as a measure of perturbation. (That is, a human who looked at the perturbed 2 would say "yeah, that's obviously a 2 instead of a 6.") But what about for other problem domains? What work is there that uses adversarial boosting in, say, NLP? (The one paper I've come across is this one , that I haven't fully digested yet.)
