[site]: datascience
[post_id]: 27066
[parent_id]: 27056
[tags]: 
If I've got the meaning of question, first convolution accepts inputs of size 224*224*3 means that height and width are both 224 and the size of depth or number of activation maps, channels here, is equal to three. The output of this layer will be activation maps with equal height and width as the input, 224 , because it is same convolution. The number of activation maps for this layer which is going to be passed to the next layer is equal to 64 because you have 64 filters in this layer. The point is that each filter is of size 3*3*3 to fit to the input. The output of each filter is an activation map of size 224*224*1. The output of filters come together and construct output of size 224*224*64 which means the input of the next layer will have 64 channels, actually depth here. Consequently, the filters of the second convolution layer is of size 3*3*224 to match the entire input of the previous layer. In other words, you will have 64 filters of size 3*3*64 and each will have an output of size 224*224*1 . Take a look at here which definitely can help you. The purpose of writers of VGG net was to make a network which was just deep enough to perform well on ImageNet data-set. CNNs have lots of hyper parameters which setting them is not based on well behaved math stuff. I mean there is no proof to show which hyper parameter is better than the others. They are found based on experience. They are gained practically. Writers of this paper tried to show that if you use same hyper parameters for convolution layers and just make the network deep and deeper, it was much deeper than AlexNet , you will gain good performance without caring about different set of hyper parameters. For understanding what ConvNets do, there is already an answer here which may help you, it contains the interpretation of different layers in CNNs . They have reached to this setting of convolution layers and architecture by experience and there idea was too use just a deep net without complicated hyper parameters.
