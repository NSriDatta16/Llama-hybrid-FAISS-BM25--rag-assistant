[site]: crossvalidated
[post_id]: 534574
[parent_id]: 
[tags]: 
Where is my picture of off-policy RL wrong?

In off-policy reinforcement learning, we keep two separate networks, one for generating behavior and one as our target network. The behavior net uses some means of exploration like an epsilon greedy policy to generate a set of data which we can optimize it on. Every so often we copy the parameters to the target net. Where is this view wrong? Some questions I don't have a grasp on: Do we ever optimize the target net, or it only ever receives parameter updates via copying the behavior net parameters? Isn't the target net in general going to be less optimal than the behavior net, because the behavior net is continuously improving through via the policy improvement theorem? Seems like with my myopic perspective, the target net is only ever "checkpointing" the parameters from the behavior net
