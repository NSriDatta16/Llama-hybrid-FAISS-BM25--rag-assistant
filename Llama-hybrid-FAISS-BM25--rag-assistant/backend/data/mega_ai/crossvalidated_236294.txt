[site]: crossvalidated
[post_id]: 236294
[parent_id]: 236289
[tags]: 
you are spot on with point # 1. Thats the way to do a cross K-fold validation. But do note that CV can be used to validate a model in itself rather than just use it to compare with other models. At times you will realize that the model is giving vastly different metrics for different sub samples of your training data set. the new data set is always going to give you unpredictable results and thats why the need to constantly retrain your models. For instance if your pre-real data set models gave you 40% and the incoming data is giving you like 20%, you will need to sample from the new data set to see whats so different about it and then add them to your training set. For all you know, your attribute set itself might need to be changed (using SVD / PCA / letting the algo choose). So both models will need to be retrained with the newer sample to see which one performs better now. There's no theoretical answer to this query. Just trial and error :)
