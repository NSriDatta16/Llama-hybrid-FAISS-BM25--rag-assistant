[site]: crossvalidated
[post_id]: 378419
[parent_id]: 378411
[tags]: 
In the case of classification, neural networks learn a coordinate system for the target classes which is a nonlinear mapping of the inputs. See: Can't deep learning models now be said to be interpretable? Are nodes features? These are "features" in the sense that they are an abstraction from the input, but theyâ€™re not really interpretable as some latent phenomenon about the input. CNNs aren't perfect. For example, small modifications to the image can dramatically shift the classifier's disposition. But CNNs are only useful when the input has some sort of structure to it; for an image, that structure is that nearby pixels comprise an important shape, texture or edge. For a time-series, the structure is that yesterday is more similar to tomorrow than last year. FCNs are useful when the data is "tabular" in format, recording different measurements for an observation which are not related (spatially, temporally, or otherwise).
