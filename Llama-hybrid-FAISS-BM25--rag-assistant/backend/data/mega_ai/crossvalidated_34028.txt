[site]: crossvalidated
[post_id]: 34028
[parent_id]: 
[tags]: 
To aggregate and lose resolution OR not to aggregate and suffer with correlated binary data?

I have data from an experiment in which each participant provides a binary response to each presented stimulus, which is either correct (1) or incorrect (0). There are 4 different stimulus types, and 48 stimuli from each group are presented to each participant. I'd like to compare the accuracy achieved between pairs of stimulus types (across all participants). The obvious thing to do is to aggregate responses within each stimulus group for each participant, to arrive at an accuracy score to be used in a repeated measures ANOVA. However, I'm bothered by the fact that this ignores the number of trials that went into computing that accuracy score - significance testing would yield the same significance whether each accuracy score was based on 48 binary responses or 400 binary responses! The other option, then, would be to do work with the raw binary data, but then the data is no longer independent, and so a chi-squared test is out of the question. I'm vaguely aware that logistic regression or generalized estimating equations (gee) could work for this, but I've only seen them used with smaller clusters of correlated data. What is the correct thing to do here?
