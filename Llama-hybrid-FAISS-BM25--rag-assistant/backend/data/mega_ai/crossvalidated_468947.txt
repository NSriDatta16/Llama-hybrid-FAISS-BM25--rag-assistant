[site]: crossvalidated
[post_id]: 468947
[parent_id]: 468942
[tags]: 
Without knowing the details of your model, it's very hard to give specific comments. However, in general a random forest can in principle have relationships of variables and outcomes, and interactions between predictors of arbitrary complexity. In contrast, a logistic regression with only main effects and continuous predictors used in a linear fashion (potentially after some transformation) will tend to be a less complex model that might be able (for better or worse) to react more strongly/in a more nuanced fashion to changes in predictors. Additionally, you did not say whether your example is from the data you trained the model on or not. If it is, then a random forest is typically easier to overfit on the actual data than a logistic regression, but can also (when trained appropriately with well chosen hyperparameters) be somewhat better at predicting new data following the same distribution as the original data. If this is new data, that is a more meaningful evaluation. If you look at enough data that the models never saw, then you will find out whether the RF is overfit or not by seeing its performance on that new data. In the absence of new data, an evaluation using cross-validation may be the best thing you can manage.
