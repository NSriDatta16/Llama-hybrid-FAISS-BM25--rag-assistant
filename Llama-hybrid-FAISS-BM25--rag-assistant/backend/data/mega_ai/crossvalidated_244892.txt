[site]: crossvalidated
[post_id]: 244892
[parent_id]: 
[tags]: 
PCA: covariance matrix for parameter estimation in reduced space

I have a large feature set with data $D$ that I'm trying to reduce the dimensionality of (from about 1500 to about 7), and then estimate some quantities using PCA as a dimensionality reduction. Sorry in advance for the length, and please forgive any misuse of terms. I've tried to be thorough, but let me know if anything is unclear, and I'll make edits. The overall idea is that in order to estimate those quantities $\{P_i\}$, I generate the PCs ($E$) from some training data (for which I know all values of $\{P_i\}$). I project my real data into PC space, and use the distance in PC space to assign a chi-square weight to each of the points from the training observations. Projecting the real data down into PC space involves accounting for measurement error in data-space, by using inverse-variance as a weight $W$. So, if a particular observation had bad data in one or a few axes, I could still use it, down-weighting the known-bad data. As far as I can tell, this is like minimizing $W \cdot |A \cdot E - S|_1$, which I've done linear-algebraically to find the PC weights vector that best represents the weighted data. The "distance" in PC space is found by taking a pre-generated covariance matrix (in data space, $K_d$), setting the diagonal equal to the known variance of an individual observation ($V$ -- but we'll call that whole thing $K_d'$), and dotting it with $E$ and $E^T$ to make a PC covariance matrix, which is inverted to get a sort of distance metric $D_{PC}$. The distance metric is dotted with the difference in the PC weights of each model from the PC weights of the data, to find a $\chi^2$ weight for each model, which is used to build a probability density function for each parameter in $\{P_i\}$. The issue is that when there's zero-weighted data, the variance (the diagonal of $K_d$) blows up, and so $D_{PC}$ starts to have unrealistically large elements. Therefore, "bad" models are not penalized, and all weights go to 1. However, since these points were already taken into account when projecting the data into PC space, I'd like to take out the effect of those down-weights to get a "true" representation of the variance. Is there some way to preserve a combination of the variance information natively encoded in the diagonal of $K_d$ and the more specific variance $V$, without "forgetting" that bad data was down-weighted when projecting onto the PCs? I've tried heuristic solutions like setting elements of $V$ equal to zero (or above some threshold) equal to the inverse-square of average signal-to-noise, but the results are not stable (i.e., I don't get believable results across many different observations, for a single threshold).
