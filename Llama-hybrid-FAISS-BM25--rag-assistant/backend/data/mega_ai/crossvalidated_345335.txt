[site]: crossvalidated
[post_id]: 345335
[parent_id]: 
[tags]: 
Bias variance dilemma (derivation in Haykin's Neural Networks)

I have a question regarding a certain derivation of the bias variance dilemma. Generally, I guess I have understood the derivation in, e.g., Geman's Paper, or in books like Bishop's Pattern Recognition . But now I've read the chapter in Haykin's Neural Network book, and I really don't get it. Maybe it's a kind of different bias variance decomposition. I'm referring now to the 2nd edition, chapter 2.13 "Statistical nature of the learning process" (in the 3rd ed. I think it's in the section "Finite sample considerations"): In formula (2.60) there is a definition of the training error Epsilon(w) for a certain data set T. Then the operator $E_T[]$ is declared as exactly this training error, Haykin calls $E_T[]$ "average operator over the entire training sample". With this definition, the further formulas (2.62), (2.63) can only be right in an approximately sense, if $E_T[]$ is this average operator. The following derivation then looks a lot like in Geman's Paper or Bishop's Book: now the operator $E_T[]$ is used like the ensemble expectation over all possible data sets, but: in the end Haykin defines bias and variance as functions dependent on the weight vector $w$! How can this be? The optimization leads to a mapping $T \rightarrow w$ (sample $T$ encoded in weights $w$ by optimization), and if $E_T[]$ is now used as ensemble expectation operator (over all possible data sets), how can bias and variance then be functions $B(w)$, $V(w)$ depending on the weight vector. $w$ respective $T$ should vanish by the integration in the expectation operator. On the other hand: if $E_T[]$ is still used as an (empirical!) average operator over a certain training set, all the derivations before don't make sense anymore, and the result would be a bias/variance-decomposition for only one special training set. The whole section on this issue looks totally confusing to me, even the definitions don't seem to be consistent throughout this derivation. I tried to figure it out for a long time now, and turn it right somehow. Maybe somebody is familiar with Haykin's book, and can explain, what's going on here...
