[site]: crossvalidated
[post_id]: 239050
[parent_id]: 239020
[tags]: 
Typically, people suggest that the deeper the network the better At some point, adding more layers hurt performances. Example 1: Larsson, Gustav, Michael Maire, and Gregory Shakhnarovich. "FractalNet: Ultra-Deep Neural Networks without Residuals." arXiv preprint arXiv:1605.07648 (2016). https://arxiv.org/abs/1605.07648 Example 2: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. "Deep Residual Learning for Image Recognition". arXiv:1512.03385. https://arxiv.org/abs/1512.03385 Practitioners simply try and see (and in between buy more GPUs). (Similar question has been asked before 2 years ago, and I think it is a good time to bring it up again: Guideline to select the hyperparameters in Deep Learning ) As far as I know, not much progress has been done since then. There have been some new papers, but using known hyperparameter optimization techniques, e.g.: Franck Dernoncourt, Ji Young Lee. Optimizing Neural Network Hyperparameters with Gaussian Processes for Dialog Act Classification , IEEE SLT 2016. L. M. Rasdi Rere, Mohamad Ivan Fanany, Aniati Murni Arymurthy. "Metaheuristic Algorithms for Convolution Neural Network". arXiv:1610.01925. https://arxiv.org/abs/1610.01925 There is one original work though that one of my lab mate made me aware of : Miconi, Thomas. " Neural networks with differentiable structure. ", arXiv (2016): the author introduce the concept of differentiable network structure to include it in the objective function.
