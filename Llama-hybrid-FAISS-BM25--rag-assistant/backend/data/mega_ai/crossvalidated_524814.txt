[site]: crossvalidated
[post_id]: 524814
[parent_id]: 524801
[tags]: 
You don't , so each time you need to validate the results and check if they are not biased. But let's start from the fact that it is not a binary case that the model is biased, or not. If it was that simple, we would just check if the model is biased and fix it, or not release it. Biases can be subtle, for example, Google likely did extensive testing before releasing their model, but still, they missed the case where it classified dark-skin individuals as gorillas . Also, keep in mind that, as discussed by Arvind Narayanan, there are multiple definitions and measures of fairness and it is impossible to satisfy them all at the same time. Gathering data is hard, so it often ends biased . In many cases, you have no way of obtaining big, representative samples of data, especially for data sources like text, videos, voice recordings, or images. Moreover, in some cases even if you had a sample that was random and representative, it could be biased. For example, if you gathered all the police records from a particular region, there could be systematic racial biases because of societal biases (e.g. non-white individuals having a higher chance of being arrested because of being perceived as more likely to be criminals). Models can be biased as well. For the sake of argument, let's assume that on average males write shorter sentences than females. You made a number of decisions based on the results you were observing when training the model on this data. For example, you turned the parameters responsible for the attention span of the model. This worked well for male-generated data, because the sentences were short, but may not work for data consisting of longer sentences, where you may need to look at the broader context. The hyperparameters and other architectural decisions you made are now baked in the model, even if someone re-trains it on the new data, the biases would be there. For more details, you can check the Practical Fairness book by Aileen Nielsen.
