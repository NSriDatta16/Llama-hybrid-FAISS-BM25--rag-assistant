[site]: crossvalidated
[post_id]: 448539
[parent_id]: 
[tags]: 
Where is the divide between information criterion (AIC, BIC, etc...) and cross validation?

I've taken a regression class and am now in a machine learning class. In regression, we talk about model selection using adj-R2 and AIC/BIC. In my machine learning class, we primarily select models using cross validation (or sample splitting) and the validation error. I am not seeing the connection between the two, or more specifically the divide between the two. Seems like cross validation/sample splitting can work on any model, so why even bother with adj-R2 or AIC/BIC? When would we want to use one over the other? And are there situations where AIC/BIC wouldn't work? Thanks
