[site]: datascience
[post_id]: 124986
[parent_id]: 
[tags]: 
Implementing Dropout in Keras

I think I am not conceptually understanding "Dropout" in neural networks. I was under the assumption that a keep rate of 0.8 would set 20% of all the neurons to 0 for each training example. In the code below, dropout is added as its own layer. What is happening here - Is it dropping neurons in the next layers? Or does mentioning dropout implement dropout in the entire model? A clarification would be great. Thanks! tf.keras.models.Sequential([ tf.keras.layers.Flatten(input_shape=(28, 28), name='layers_flatten'), tf.keras.layers.Dense(512, activation='relu', name='layers_dense'), tf.keras.layers.Dropout(0.2, name='layers_dropout'), tf.keras.layers.Dense(10, activation='softmax', name='layers_dense_2') ])
