[site]: crossvalidated
[post_id]: 3354
[parent_id]: 3337
[tags]: 
Interesting question! The approach you describe is certainly very widely used by people using standard ML methods that require fixed-length feature vectors of attributes, to analyse time series data. In the post that you link to, Hyndman points out that there are correlations between the reshaped data vectors (samples). This could be problematic, as k-CV (or other evaluation methods that divide data at random into training and testing sets) assumes that all samples are independent. However, I don't think this concern is relevant for the case of a standard ML methods, that treat attributes separately. For explanation, let me simplify your notation by assuming $n=3$, so the the first few data vectors (labelled alphabetically) will be: \begin{align} A&: (y_1, y_2, y_3; y_4) \\ B&: (y_2, y_3, y_4; y_5) \\ C&: (y_3, y_4, y_5; y_6) \\ \end{align} Clearly, A and B have terms such as $y_2$ in common. But, for A, this is the value of its second attribute whereas for B this is the value of its first attribute.
