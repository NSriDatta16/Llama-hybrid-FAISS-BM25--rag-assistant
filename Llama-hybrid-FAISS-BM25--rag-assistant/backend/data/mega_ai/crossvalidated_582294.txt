[site]: crossvalidated
[post_id]: 582294
[parent_id]: 
[tags]: 
Is it disingenuous to test tree-based regressors on training data for the sake of comparison to a linear model?

I am currently performing an analysis in a domain that, in current literature, is predominantly populated by likelihood-based linear models (MLR, Poisson Regression, etc.). My intent is to implement tree-based regressors (gradient boosting/random forest) for similar tasks and report the predictive performance. I've done my due diligence with respect to proper cross validation, train/test splits, etc. etc., but would like to have at least some acknowledgement of how the performance metrics of my models compare to those linear models that have been empirically implemented. I know this is not a 1:1 comparison, as pure predictive modelling is certainly a different task than coefficient/parameter based analysis, but I want to cover all of my bases if I can. So with this in mind, would it be appropriate to derive metrics such as R squared and MAPE by training entirely on my full dataset (as most linear model implementations technically do), and report these alongside my testing performance?
