[site]: crossvalidated
[post_id]: 215364
[parent_id]: 215204
[tags]: 
I'll just give some R examples to demonstrate what @PeyM87 already mentioned, including feature filters and wrappers, and classic feature correlation (using data from the small mtcars dataset): d Lets assume the target variable is cyl . You could employ classic feature selection using feature filters and feature wrappers . Here's a feature filter example (using caret univariate filters) that states the "most important" features for the target variable: > modelSbf modelSbf$optVariables [1] "mpg" "disp" "hp" "drat" "wt" "qsec" "vs" "am" "gear" "carb" And here a feature wrapper example (using caret recursive feature elimination), which states the actually most important features for the target variable by including model training in the process: > modelRfe modelRfe$optVariables [1] "disp" "mpg" "hp" "wt" "vs" "carb" "drat" "qsec" Note that those approaches give similar but not exact same results. Alternatively, after training a model on predicting the target variable, the variable importance for some models can just be stated: > library(caret) > modelRpart varImp(modelRpart) Overall disp 100.00 hp 91.04 mpg 86.03 wt 69.23 vs 69.07 gear 0.00 drat 0.00 qsec 0.00 am 0.00 carb 0.00 And in case your data would only have numeric features you could also employ classic correlation, PCA, and similar approaches that only work with numeric values. Here's a feature correlation example that shown correlation between features and the target variable: > library(corrplot) > # correlation with target variable > cor(mtcars)[,2] mpg cyl disp hp drat wt qsec vs am gear carb -0.8521620 1.0000000 0.9020329 0.8324475 -0.6999381 0.7824958 -0.5912421 -0.8108118 -0.5226070 -0.4926866 0.5269883 > corrplot(cor(mtcars)) You can also reduce features by selecting them so that the maximum correlation is bounded (exclude the target variable from the process): > toRemove corrplot(cor(cbind(mtcars[,-toRemove], cyl=mtcars[,2])))
