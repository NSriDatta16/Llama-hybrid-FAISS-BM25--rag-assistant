[site]: crossvalidated
[post_id]: 492126
[parent_id]: 
[tags]: 
Bayesian mixture model joint posterior

I am just starting to learn about bayesian mixture models. There is a few clarifications that I want to make which I am not sure myself. The graphical model below describes a gaussian mixture model where $\pi, \Psi, \Sigma, \mu, \boldsymbol{Z}$ are the latent unknowns that we would like to infer about the mixture model. $\mu,\Sigma, Z$ are vectors since we are parameterising by $K$ clusters. From the model described, am i right to say that the joint posterior explicitly is $$p(\boldsymbol{Z},\Sigma,\mu, \pi | \alpha, \mu_0, \Sigma_0, \Psi, \{X_n\}_{i=1}^N) = p(\pi|\alpha)\prod_{k=1}^Kp(\mu_k|\mu_0,\Sigma_0)p(\Sigma_k|\Psi)\prod_{i=1}^Np(z_i|\pi)p(x_i|z_i, \mu,\Sigma)$$ From this full joint posterior, how do i use conditional independence to compute the conditional posterior distributions for each variable to do gibbs sampling ?..
