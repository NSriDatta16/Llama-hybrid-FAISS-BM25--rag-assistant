[site]: crossvalidated
[post_id]: 500236
[parent_id]: 
[tags]: 
How validation set in statistical learning works?

In statistical learning, we split the data into three parts for training, validation, and test, separately. With training data we can get a model $T$ , then we seem to optimize or change the model by validation data. How does that happen (since the model $T$ is sort of fixed)? Updated: I see validation data set is for model selection, namely it doesn't change the model, but pick one from some. But what if we only have one model to train and test? e.g. if we wanna find the value of k and distance in a k-NN model, it seems that we only need to optimize the model instead of selecting one from some. A relevant question: it is said that pixel distance (which I guess is subtracting RGB in each pixel and then add squares of them and them calculate the square root) is never used in kNN for image classification, this sounds reasonable since it seems to me not to provide a good way of measuring how much the objects in two images differ. But then what sort of distance do we define for kNN for image classification? I feel it's difficult for one to directly give such a definition (particularly so if one considers occlusion and disformation), so do we just use machine learning to let the computer decide what type of distance to use?
