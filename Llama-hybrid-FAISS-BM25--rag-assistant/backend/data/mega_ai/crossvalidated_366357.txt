[site]: crossvalidated
[post_id]: 366357
[parent_id]: 366350
[tags]: 
as I understand that above algorithm will pick the important variable by itself and are called ML algorithm because it doesn't require manual intervention to that extent. It's called ML because the algorithm learns by itself some difficult pattern by looking at lots of data, and learning when it gets the wrong prediciton, working towards the goal of a good performance overall. You still need to point it in the right direction. Or at least watch closely what it "learns". It's not magic, thankfully. A good process analysis process always starts with some EDA , because you learn for example which variables are more interesting in terms of what's related to the outcome and what's not. Those algorithms are simply "not good enough" to simply discard the information that they receive when they see lots of noisy variables. Thankfully there's still a need for a Data Scientist afterall. RF and XGB tell you that some variables have low importance, that's correct, but because they use randomization during the process they still might build a tree that uses all noisy variables (not correlated with the outcome) plus one good variable (highly correlated). This is somewhat related with the curse of dimensionality too. So, no, the algorithm won't pick the important variables by itself, it will use everything you throw in the pot to achieve it's goal (high performance). But it will get lost if the hypothesis space is too damn big (which it is for 10k variables). And it won't reach its full potential, which is reached via feature engeneering and feature selection , even after a first model where you give all your variables (for example Lasso followed by Random Forest).
