[site]: crossvalidated
[post_id]: 306913
[parent_id]: 283641
[tags]: 
There is a follow-up paper by Sergej Ioffe (i.e. Batch Renormalization) which discusses this issue: https://arxiv.org/abs/1702.03275 A quote from that paper regarding regular batch normalization: It is natural to ask whether we could simply use the moving averages $\mu, \sigma$ to perform the normalization during training, since this would remove the dependence of the normalized activations on the other example in the mini-batch. This, however, has been observerd to lead to the model blowing up. As argued in [6, the original batch norm paper], such use of moving averages would cause the gradient optimization and the normalization to counteract each other. In that paper the this issue is fixed by introducing an additional affine transformation from the batch statistics to the moving average statistics, the coefficients of which are treated as constants by the optimisation. With this change the moving average can be used both during training and testing time.
