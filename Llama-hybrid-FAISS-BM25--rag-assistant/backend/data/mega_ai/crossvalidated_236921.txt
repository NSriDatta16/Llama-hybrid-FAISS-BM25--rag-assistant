[site]: crossvalidated
[post_id]: 236921
[parent_id]: 168953
[tags]: 
The way you have asked the question, no there is not. It should be trivial to construct an "easy" function and initial condition where you converge to the global optimum in one step, and then any $\gamma\not=0$ will cause the loss to increase. You can, of course, code your optimization routine to simply not follow the gradient beyond the point that it decreases your loss function (line search). If you're studying this for machine learning, you should know going in that the ML community seems to have jettisoned most of the best practices from optimization because they don't scale well enough to large datasets. Often evaluating the loss function even once on the whole dataset is deemed too expensive to be worthwhile, let alone computing exact first and second gradients, doing line searches, etc... (Take this with a grain of salt; I'm new at ML)
