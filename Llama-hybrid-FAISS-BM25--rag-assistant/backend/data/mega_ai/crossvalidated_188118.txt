[site]: crossvalidated
[post_id]: 188118
[parent_id]: 
[tags]: 
Is assuming a vague prior different than assuming nothing?

If you use a vague prior in a Bayesian analysis so that $\Pr(A) = 1\mathbin/n$, then \[ \Pr(\pi\mid x) = \frac{\Pr(x\mid \pi)\Pr(\pi)}{\Pr(x)} = \frac{\mathcal{L}(\pi;x)}{n\Pr(x)} \propto \mathcal{L}(\pi;x) \] Which seems like it's what we would get if we just ignored priors altogether, computed a likelihood and normalized it to a probability. So is assuming a vague prior like assuming nothing? I guess assuming a vague prior is like assuming you can infer a probability distribution from a likelihood. But what are you really assuming in that case? It seems tempting to conclude that a vague prior is like assuming nothing, as this lifts one of the "burdens" of Bayesian analysis, but I remember being told that vague priors are not really the same as assuming nothing. edit: I thought about this more, and I guess assuming a likelihood can just be normalized to a probability is pretty presumptuous, and it becomes clearer if you choose an example where the prior is very obviously not uniform. Eg: I have a machine that can tells me whether or not sun has exploded with a 5% chance of error. One day it tells me the sun has exploded. Has it really? P(machine says exploded | sun exploded) = 0.95, P(machine says exploded | sun hasn't exploded) = 0.05, but it's a bit of a leap to say that there is thus a 95% chance that the sun exploded.
