[site]: crossvalidated
[post_id]: 251955
[parent_id]: 212720
[tags]: 
Can we compare classifiers using ROC curve? If so, the discussion will be on AUC? In this case, should I discuss accuracy, fmeasure... You can. Keep in mind what AUC means, i.e. it reflects the separability of classes over a continuous score. It doesn't reward the separation of hard-to -classify instances nor rewards the certainty of probabilistic scores. Training data ROC curve or testing data ROC curve? Which is more meaningful? and why? Testing data ROC curve, of course. Consider the scenario where your classifier simply memorizes the training data (Random Forests are likely to do this), your training data AUC will be close to 1, but it's a meaningless result, it's simply a just-identified model artifact. Testing-data metrics, on the other hand, are more likely to be generalizable. In my simulation, my testing ROC curve is different from training (Specifically for SVM classifier). SVM's training ROC is very good as compared to testing data curve (See below). Is this correct? If so, how should I analyze/present it? Pointers will be really helpful.The top one is training, where SVM's curve is very good. And bottom one is on testing data, where SVM is not so good, and random forest outperforms. So should I say that random forest is doing a good job? As @jeff said, ignore the training metrics, they aren't useful in your scenario. You could use them only to try to justify a possible overfitting with the SVM, though, but that's it.
