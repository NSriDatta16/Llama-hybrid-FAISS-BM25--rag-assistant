[site]: crossvalidated
[post_id]: 511969
[parent_id]: 
[tags]: 
Optimising Discriminator for GAN

For Generative Adversarial Networks, when trying to optimise for a Discriminator given a fixed Generator, we arrive at the following cost function (as in the original paper) $V(G,D)=\int_{x}p_{data(x)}\log(D(x))+p_{g}(x)\log(1âˆ’D(x))dx$ Given that G is fixed, we can therefore maximise $V(G,D)$ , however instead of taking the derivative of the above function, we instead maximise $a\log(x)+b\log(1-x)$ and therefore end up with $\frac{d}{dD(x)}(p_{data}(x)\log{D(x)}+p_g(x)\log{(1-D(x))} = 0$ Can anyone explain to me why we optimise the integrand instead of the integral? Thank you!
