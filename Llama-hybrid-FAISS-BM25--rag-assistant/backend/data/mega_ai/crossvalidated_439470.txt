[site]: crossvalidated
[post_id]: 439470
[parent_id]: 
[tags]: 
P-value adjustments in regression interpretation

I have a basic question I haven't been able to find a particularly satisfactory answer. P-value adjustments (like the Bonferroni adjustment) are recognized as necessary when performing multiple analyses in a number of contexts, (pairwise comparisons, test of leverage for observations in a regression etc etc.) but I haven't been able to find any commentary on the interpretation of P-values in large regression models. For example: Take a model $ Y=\beta_0 + \beta_1 X_1 + ... +\beta_nX_n $ We are confident in the quality of the model (perhaps we've already run a stepwise model selection method with AIC or BIC). Now what we want to do is consider which $\beta$ -values are "significant", meaning that we can confidently interpret them. Those that are not "significant" we will still include in the model, (because using a measure of model quality like AIC we've already determined we would like to include it) but we will instead consider them as variables we are controlling for. This is a common approach in many social-science methods. For each $\beta_n $ we can obtain a p-value $p_n$ and some significance level $\alpha$ . To determine which regressors are "significant" vs. which are "controls," should we adjust are $\alpha$ based on the number of regressors? I've seen a lot of published research that doesn't do this in my field (education research), and it seems to lead to over-stating the significance of variables. Any references for rational would be appreciated. As an example, see this paper published in a good educational research journal.
