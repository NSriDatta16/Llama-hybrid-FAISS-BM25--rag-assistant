[site]: crossvalidated
[post_id]: 231237
[parent_id]: 231171
[tags]: 
Predictive accuracy always needs to be calculated on unseen data - whether that data is unseen via cross validation splits or via a separate data set. So often the most important point is to avoid leaks between training and test data . This may be easier to achieve with hold out (e.g. by obtaining test cases only after model training is finished) than for resampling. But careful: very often "hold out" or "independent test" are used that are in fact a single random split of the available data set. That procedure is of course prone to the same data leaks that cross validation is. Yes, for simple data, cross validation makes more efficient use of your data. And in small sample size situations, that can be the crucial advantage of resampling. But when you have to deal with multiple confounders and need to split independently for all those confounders, that advantage vanishes very fast because you end up excluding large parts of your data from both test and training set for each surrogate model. Related: Is hold-out validation a better approximation of "getting new data" than k-fold CV? Hold out vs. validation experiment UPDATE: described scenario of 100k (I assume cases) x unknown no of variates. That is certainly not a small sample size situation. In this situation, a random hold out set of 10 % = 10000 cases should have no practically relevant difference to cross validation results. The more so, as a random subset is prone to the same data leaks that cross validation is prone to as well: confounders that lead to clustering in the data. If you have such confounders, your effective sample size may be orders of magnitude below the 100k rows, and any kind of splitting that doesn't take care of those confounders will mean a data leak between training and test and lead to overoptimistic bias in the error estimates. The more efficient use of cases in cross validation is mostly relevant with small data sets where stability of the model is an issue and must be checked (which is easily done by cross validation), and uncertainty of the test result due to small numbers of test cases is large here cross validation is better as a full run will test each case. For theory, I recommend reading up the relevant parts of The Elements of Statistical Learning . These papers have empirical results on bias and variance of different validation schemes (though they deal explicitly with small sample size situations): Kohavi, R.: A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection, Mellish, C. S. (ed.) Artificial Intelligence Proceedings 14$^th$ International Joint Conference, 20 -- 25. August 1995, Montréal, Québec, Canada, Morgan Kaufmann, USA, 1137 - 1145 (1995). Beleites, C.; Baumgartner, R.; Bowman, C.; Somorjai, R.; Steiner, G.; Salzer, R. & Sowa, M. G.: Variance reduction in estimating classification error using sparse datasets, Chemom Intell Lab Syst, 79, 91 - 100 (2005).
