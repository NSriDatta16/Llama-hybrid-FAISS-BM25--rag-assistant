[site]: crossvalidated
[post_id]: 616825
[parent_id]: 174527
[tags]: 
While classical statistics gives you an equation for leverage in a linear model (e.g., the equation given by Agresti (2015), page 59), such an analogue need not exist for the complicated models you are using. The spirit of the leverage given by the referernce is that any one observation might have a huge influence on the model fit, meaning that dropping that observation would result in dramatically different predictions. A classical linear regression allows you to calculate the leverage using a closed-form solution. I do not expect this to exist for the complicated models you use, especially if you get into tuning hyperparameters using cross-validation like I suspect you do (or at least would be common in much machine learning work). Thus, instead of having an equation to calculate the leverage of each observation, you drop the observation, fit the model on all of the other observations, and check by how much the predictions change. I could see a strategy going something like this. Fit the model to all data and record the predicted values. Drop an observation, and fit the model on the remaining data. Compare the predictions on the smaller model to the predictions by the model. Perhaps calculate the variance, mean absolute deviation, or interquartile range of the changes in predictions, where smaller changes indicate less leverage. Drop another observation and repeat, then another obervation, and so on. If you store the measures of leverage, you can plot a distribution or look at summary statistics to see how influential the particular observations are. If there is a tight cluster, then no individual observation is particularly influential. If some observations result in large changes, you can drill down further and determine which observation is responsible. (I think this is what you meant by "leave-one-out" and agree with that stance.) The upside to such an approach is that it is a fairly straightforward coding exercise, rather than a gigantic mess of a mathematical derivation. The downside is that you have to fit a model over and over (oh, the joys of computational statistics). REFERENCE Agresti, Alan. Foundations of Linear and Generalized Linear Models. John Wiley & Sons, 2015.
