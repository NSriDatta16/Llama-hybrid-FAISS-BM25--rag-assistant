[site]: crossvalidated
[post_id]: 494848
[parent_id]: 
[tags]: 
How does XGBoost deal with overfitting?

The question as stated is very general, but I'm asking it because I find weird that I got the following scores using 100 or more trees with XGBoost: AUC on training set 1 (perfect classifier). AUC on test set 0,77. The first AUC on training set seems to indicate overfitting, of course. But the issue is that the second score is decent, and if I reduce the number of trees, I can optimize the AUC on test around 20 trees, getting a value of 0,79 AUC. It's not a huge improvement. IN this case the AUC on training set will not be perfect, but 0,9 approx. So the question is: how can it be that with a model that has learned perfectly the training set you can generalize so well? Isn't weird? I think that with other models the typical situation with overfitting is that the error on test set tends to increase wildly.
