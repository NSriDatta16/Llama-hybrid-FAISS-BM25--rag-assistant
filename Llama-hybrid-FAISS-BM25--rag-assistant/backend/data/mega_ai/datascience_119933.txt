[site]: datascience
[post_id]: 119933
[parent_id]: 119931
[tags]: 
SBERT supports batch inference, which means that it can process multiple sentences at the same time. For instance ( source ): from sentence_transformers import SentenceTransformer model = SentenceTransformer('all-MiniLM-L6-v2') #Our sentences we like to encode sentences = ['This framework generates embeddings for each input sentence', 'Sentences are passed as a list of string.', 'The quick brown fox jumps over the lazy dog.'] #Sentences are encoded by calling model.encode() embeddings = model.encode(sentences) #Print the embeddings for sentence, embedding in zip(sentences, embeddings): print("Sentence:", sentence) print("Embedding:", embedding) print("") The limit for how many sentences you can fit will depend on their length and the amount of memory in your GPU/CPU. Note that the needed amount of memory is quadratic on the sequence length. You should find the appropriate limit for the number of sentences empirically. About projecting the embeddings and clustering, I suggest you use UMAP for the dimensionality reduction and then k-means for clustering .
