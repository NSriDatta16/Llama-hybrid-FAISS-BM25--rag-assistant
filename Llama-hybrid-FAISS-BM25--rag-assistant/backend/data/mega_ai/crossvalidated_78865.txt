[site]: crossvalidated
[post_id]: 78865
[parent_id]: 78862
[tags]: 
Assuming that the regressors are deterministic, and so they do not have a variance of their own (or alternatively that we are considering the variance of the fitted dependent variable conditional on the specific realizations of the regressors that comprise our sample of observations), we have (for a sample of $n$ observations and for $k+1$ regressors including the constant) $$\hat y_i\mid \mathbf x_i = \hat b_0 + \hat b_1x_{1i} + ... \hat b_kx_{ki},\qquad i=1,...,n$$ and so $$\begin{align}\operatorname {Var}(\hat y_i\mid \mathbf x_i) =& \operatorname {Var}(\hat b_0)+\operatorname {Var}(\hat b_1)x_{1i}^2+...+\operatorname {Var}(\hat b_1)x_{ki}^2 +...\\ +&2x_{1i}\operatorname {Cov}(\hat b_0, \hat b_1)+2x_{2i}\operatorname {Cov}(\hat b_0, \hat b_2)+...+2x_k\operatorname {Cov}(\hat b_0, \hat b_k) \\ +&2x_{1i}x_{2i}\operatorname {Cov}(\hat b_1, \hat b_2)+....+2x_{1i}x_{ki}\operatorname {Cov}(\hat b_1, \hat b_k)\\ +&...+ 2x_{k-1}x_k\operatorname {Cov}(\hat b_{k-1}, \hat b_k) \end{align} $$ In other words, we need to take into account the covariances between the estimated coefficients, covariances that in general are not zero, because the estimated coefficients are functions of the same data (and even if the $x$'s are deterministic, "inside" each coefficient estimate lurks the same stochastic error term). Note that some covariances may be positive while others negative. Obviously the above magnitude is different for each observation $i$, which is only natural since it depends on the specific values of the $x_i$'s. One can then average over the whole sample, i.e. consider $$\hat E[\operatorname {Var}(\hat y_i\mid \mathbf x_i)] = \frac 1n \sum_{i=1}^n\operatorname {Var}(\hat y_i \mid \mathbf x_i)$$ while keeping in mind the variance-decomposition formula, which in general reads $$\operatorname {Var}(Y) = E[\operatorname {Var}(Y\mid X)] + \operatorname {Var}[E(Y\mid X)]$$
