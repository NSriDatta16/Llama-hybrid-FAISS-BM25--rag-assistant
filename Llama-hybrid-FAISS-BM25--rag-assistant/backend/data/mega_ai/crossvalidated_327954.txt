[site]: crossvalidated
[post_id]: 327954
[parent_id]: 
[tags]: 
Why does training a GAN discriminator on separate updates for real and generated data work better than a single update?

For each batch I update the discriminator first, and then the combined generator/discriminator together. I noticed that if I make two updates to the discriminator, one for the generated data and one for the training data the system works fine. d_loss_real = discriminator.train_on_batch(real_images, real_labels) d_loss_fake = discriminator.train_on_batch(fake_images, fake_labels) However if I combine both real and generated data into a single dataset and make a single update, the generator isn't able to make any progress and the network doesn't train correctly. Both the discriminator and generator are using Batch Normalization. Can someone tell me why this is happening? I had seen some mention of it being related to batch normalization, but I'm looking for a slightly more in depth answer.
