[site]: datascience
[post_id]: 108258
[parent_id]: 108243
[tags]: 
Since you added that you use linear regression, few ideas (but still a very broad question): How do I know that all of these columns benefit the model's performance? [...] how can I determine which columns/features to keep? Have a look at Introduction to Statistical Learning (ISL) (Chapter 6.1). You can use stepwise (feature) selection for a start. The book comes with "R-Labs", so you can directly see how it is done ( also available for Python ). Is there a method of determining the importance of these columns Yes, use "shrinkage" of (standardized) features to see which features have a "strong" impact. This is Chapter 6.2 in ISL. However, I have no intuition/idea on what to do after the first model fitting. In linear models there is not much to be tuned. You can do feature selection / feature engineering /feature generation but apart from that there are no hyperparameters to be tuned. However, if you are up for a predictive model, make sure you have a proper test strategy. This is explained in Ch. 5 of ISL (" Resampling Methods"). The remaining chapters (i.e. Ch. 7, 8) give a good overview what you can do if you want to go beyond purely linear models. When you face stark non-linear data, you may look at " generalized additive models " (GAM, Ch. 7 in ISL). Often Random Forest is a good choice as well when the parameterization of the data is unclear (Ch. 8 in ISL).
