[site]: datascience
[post_id]: 41848
[parent_id]: 41846
[tags]: 
The answer to your first question: should the test dataset be balanced as well? is, like many answers in data science, "it depends." And really, it depends on the audience for, and interpretability of the model metrics, which is the thrust of your second question: What is the impact of an imbalanced test dataset on the precision, recall, and overall accuracy metrics? Personally, if the metrics will just be used by you to evaluate the model, I would use the sensitivity and specificity within each class to evaluate the model, in which case, I care less about the balance of the classes in the test data as long as I have enough of both to be representative. I can account for the prior probabilities of the classes to evaluate the performance of the model. On the other hand, if the metrics will be used to describe predictive power to a non-technical audience, say upper management, I would want to be able to discuss the overall accuracy, for which, I would want a reasonably balanced test set. That said, it sounds like your test set is drawn independently of the training data. If you are going to balance the training data set, why not draw one balanced data set from the raw data and then split the training and test data? This will give you very similar class populations in both data sets without necessarily having to do any extra work.
