[site]: crossvalidated
[post_id]: 424197
[parent_id]: 424063
[tags]: 
If I'm understanding you correctly, you're trying to create more meaningful features (i.e., denser or more informative variables than what you currently have). This is a great question, and there are many many ways to do feature curation and feature engineering, so a lot of it will deal with experimenting with your data. However, there are techniques (like this paper below) that offer a good starting point to aggregate data and features in a systematic fashion. This paper below specifically uses hiearchical clustering trees when your features are sparse, but there are plenty of other techniques. Yan, X., & Bien, J. (2018). Rare feature selection in high dimensions. arXiv preprint arXiv:1803.06675. https://arxiv.org/abs/1803.06675 Since you mentioned information loss, this paper seems to explain how you can lose information when dichotomizing an discretizing continuous variables. They offer a solution to dos in such a way to minimize information loss. Clarke, Ellis J., and Bruce A. Barton. "Entropy and MDL discretization of continuous variables for Bayesian belief networks." International Journal of Intelligent Systems 15, no. 1 (2000): 61-92. Hope this helps!
