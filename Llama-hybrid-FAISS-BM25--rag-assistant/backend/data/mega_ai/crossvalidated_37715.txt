[site]: crossvalidated
[post_id]: 37715
[parent_id]: 37683
[tags]: 
To expand on Peter Flom's answer (which is echoed in Michael Chernick's subsequent reply), this graphic may help the intuition. The following R code shows how it was produced. Briefly, it generates 400 data points per year, with values of variable $x$ ranging variously from $0$ to $2$ through $2$ to $4$, shifting upwards each year: this is one (mild) form of confounding of $x$ and year . It creates probabilities $y$ according to a logistic model and, for later use, generates binary observations $z$ according to those probabilities. The probabilities are plotted, with color differentiating the years. Finally, the logistic fit to those 2000 observations is overplotted with a dashed line. # Create sample data # logistic Apparently, the effect of this "staircase" of falling curves is to cause the overall model to average its way through the middle, suggesting a much steeper fall than any individual step (year) exhibits. We can do the calculations of the odds ratios and their 95% CIs, too: output The (cleaned up) output, which is in the form (estimate, lower limit, upper limit) for each model, is Overall 0.17 0.14 0.20 year: 1 0.54 0.31 0.94 year: 2 0.53 0.37 0.78 year: 3 0.36 0.25 0.53 year: 4 0.61 0.36 1.03 year: 5 0.47 0.22 1.01 The overall coefficient of 0.17 is lower than the lower confidence limits of any of the annual (year-specific) fits, several of which are evidently not even significant (because their confidence intervals include 1.0). Comments This phenomenon is not special to logistic regression: it is seen in many ordinary regressions and, in that form, has been discussed and illustrated elsewhere on this site. (Finding that discussion might take some clever searching, I'm afraid.) Many textbooks use examples like this to illustrate the value of including and controlling for important variables in statistical models and to discuss confounding. This example uses fairly large (sub)sample sizes: this suggests that the changes in widths of confidence intervals are not wholly due to changes in sample size. In fact, most of the change is because each "stair" is much shallower than the overall trend, and therefore is not as easy to discriminate from no trend at all. There is no multiple testing going on: we're looking only at one covariate.
