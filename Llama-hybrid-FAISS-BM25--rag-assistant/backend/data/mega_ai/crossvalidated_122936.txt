[site]: crossvalidated
[post_id]: 122936
[parent_id]: 
[tags]: 
Under exactly what conditions is ridge regression able to provide an improvement over ordinary least squares regression?

Ridge regression estimates parameters $\boldsymbol \beta$ in a linear model $\mathbf y = \mathbf X \boldsymbol \beta$ by $$\hat{\boldsymbol \beta}_\lambda = (\mathbf X^\top \mathbf X + \lambda \mathbf I)^{-1} \mathbf X^\top \mathbf y,$$ where $\lambda$ is a regularization parameter. It is well-known that it often performs better than the OLS regression (with $\lambda=0$ ) when there are many correlated predictors. An existence theorem for ridge regression says that there always exists a parameter $\lambda^* > 0$ such that mean-squared-error of $\hat{\boldsymbol \beta}_\lambda$ is strictly smaller than mean-squared-error of the OLS estimation $\hat{\boldsymbol \beta}_\mathrm{OLS}=\hat{\boldsymbol \beta}_0$ . In other words, an optimal value of $\lambda$ is always non-zero. This was apparently first proven in Hoerl and Kennard. (Hoerl, Arthur E., and Robert W. Kennard. “Ridge Regression: Biased Estimation for Nonorthogonal Problems.” Technometrics, vol. 42, no. 1, [Taylor & Francis, Ltd., American Statistical Association, American Society for Quality], 2000, pp. 80–86, https://doi.org/10.2307/1271436. ) It is repeated in many lecture notes that I find online (e.g. here and here ). My question is about the assumptions of this theorem: Are there any assumptions about the covariance matrix $\mathbf X^\top \mathbf X$ ? Are there any assumptions about dimensionality of $\mathbf X$ ? In particular, is the theorem still true if predictors are orthogonal (i.e. $\mathbf X^\top \mathbf X$ is diagonal), or even if $\mathbf X^\top \mathbf X=\mathbf I$ ? And is it still true if there is only one or two predictors (say, one predictor and an intercept)? If the theorem makes no such assumptions and remains true even in these cases, then why is ridge regression usually recommended only in the case of correlated predictors, and never (?) recommended for simple (i.e. not multiple) regression? This is related to my question about Unified view on shrinkage: what is the relation (if any) between Stein's paradox, ridge regression, and random effects in mixed models? , but no answers there clarify this point until now.
