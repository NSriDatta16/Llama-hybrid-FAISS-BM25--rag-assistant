[site]: crossvalidated
[post_id]: 409827
[parent_id]: 409149
[tags]: 
Ridge, lasso and elastic net are similar to Bayesian methods with priors centered on zero -- see, for example, Statistical Learning with Sparsity by Hastie, Tibshirani and Wainwright, section 2.9 Lq Penalties and Bayes Estimates : "There is also a Bayesian view of these estimators. ... This means that the lasso estimate is the Bayesian MAP (maximum aposteriori) estimator using a Laplacian prior." One way to answer your question ( what's so special about zero? ) is that the effects we are estimating are zero on average, and they tend to be small (i.e our priors should be centered around zero). Shrinking estimates towards zero is then optimal in a Bayesian sense, and lasso and ridge and elastic nets can be thought of through that lens.
