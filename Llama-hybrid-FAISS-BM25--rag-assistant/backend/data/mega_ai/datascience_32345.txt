[site]: datascience
[post_id]: 32345
[parent_id]: 
[tags]: 
Initial embeddings for unknown, padding?

Last time I've been passing pretrained word embeddings into LSTM to solve text classification problems. Usually, there are additional , replacements for padding and unknown types. Of course, there are no pretrained vectors for them. Solutions I've come up with are Fill them with random values Fill them with zeros Which approach is better/wrong? What is the common practice? Note: I use pytorch+torchtext if it matters.
