[site]: crossvalidated
[post_id]: 210103
[parent_id]: 
[tags]: 
choosing prior parameters for variational mixture of Gaussians

I am implementing a vanilla variational mixture of multivariate Gaussians, as per Chapter 10 of Pattern Recognition and Machine Learning (Bishop, 2007). The Bayesian approach requires to specify (hyper) parameters for the Gaussian-inverse-Wishart prior: $\alpha_0$ (concentration parameter of the Dirichlet prior); $\nu_0$ (degrees of freedom of an inverse Wishart distribution); $\beta_0$ (pseudo-observations for the Gaussian-inverse Wishart distribution); $\mathbf{m}_0$ (mean of the Gaussian distribution). $\mathbf{W}_0$ (scale matrix for the inverse Wishart). Common choices are $\alpha_0 = 1$, $\nu_0 = d + 1$, $\beta_0 = 1$, $\textbf{m}_0 = \textbf{0}$, $\textbf{W}_0 = \textbf{I}_d$, where $d$ is the dimensionality of the space. Unsurprisingly, the posterior can depend strongly on the choice of parameters (in particular, I find that $\textbf{W}_0$ has a large impact on the number of components, much more than $\alpha_0$). For $\textbf{m}_0$ and $\textbf{W}_0$, the choices above make sense only if the data have been somewhat normalized. Following a sort-of empirical Bayes approach I was thinking of setting $\textbf{m}_0$ and $\textbf{W}_0^{-1}$ equal to the empirical mean and empirical covariance matrix of the data (for the latter, I could perhaps only consider the diagonal; also, I need to multiply the sample covariance matrix by $\nu_0$). Would this be sensible? Any suggestion on other reasonable methods to set the parameters? (without going fully hierarchical Bayes and DPGMM) (There is a similar question here , but no answer that is relevant to my question.)
