[site]: crossvalidated
[post_id]: 562856
[parent_id]: 
[tags]: 
Understanding "In Bayesian inference, the difference between data and a parameter is that one is observed (data) and one isn't (parameter)"

In his statistical rethinking course, Richard Mclreath states "In Bayesian inference, the difference between data and a parameter is that one is observed (data) and one isn't (parameter)" I am fairly new to Bayesian inference, I am having a lot of trouble understanding or intuiting this statement. My current intuition is that the data interacts with the prior via the likelihood function, and the prior has a regularizing shrinkage effect on the model estimates. To provide some context to the example in the link above, Richard is demonstrating a sensitivity analysis where there is essentially a column of data where every value is missing, and implies that there is therefore a parameter that needs to be estimated for every single row in the dataset (~4000 parameters!), which I find very confusing, and yet a model can be fit to the data that produces a distribution of estimates for the missing values. Any help understanding the quote, in general or in the context of the example, would be greatly appreciated.
