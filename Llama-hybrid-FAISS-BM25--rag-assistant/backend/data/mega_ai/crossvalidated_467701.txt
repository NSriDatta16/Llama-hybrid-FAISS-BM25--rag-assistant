[site]: crossvalidated
[post_id]: 467701
[parent_id]: 
[tags]: 
Proving that logistic regression on $I(X>c)$ by $X$ itself recovers decision boundary $c$ when $X$ is normal

Backgrounds Suppose that $X \sim \mathcal{N} (0,\sigma^2)$ , and define $C\equiv I(X>c)$ , for a given constant( decision boundary ) $c$ . Now assume we perform a logistic regression : $$\mathrm{logit}(P(C=1)) \sim \beta_0 + \beta_1X $$ Note that for logistic regression , the fitted $\displaystyle -\frac{\hat{\beta_0}}{\hat{\beta_1}}$ corresponds to the mean of underlying logistic distribution. (This is perfect separation case. Please also take a generous look at imperfect separation case at the bottom.) Problem My hypothesis says the value should be the same, or at least similar as the criterion $c$ , i.e. $$ c \approx -\frac{\hat{\beta_0}}{\hat{\beta_1}} $$ I would like to prove or reject the above argument . Simulation It is really hard to analytically derive the distribution of $\displaystyle -\frac{\hat{\beta_0}}{\hat{\beta_1}}$ . Therefore with R , I simulated for various possible sets of $(\sigma, c)$ to test my hypothesis . Suppose we set, for instance, $\sigma: 5,10,15,20$ $c : -5,4,12$ N = 1000 for(sig in c(5,10,15,20)){ for (c in c(-5, 4, 12)){ X = rnorm(N, sd=sig) C = (X > c)*1 DATA = data.frame(x=X, c=C) coef = summary(glm(C ~ X, DATA, family = "binomial"))$coefficients print(sprintf("True c: %.2f, Estimated c: %.2f", c, -coef[1,1]/coef[2,1])) } } Note the true $c$ and the estimated $-\hat{\beta_0}\big/\hat{\beta_1}$ are similar as seen in the following output: [1] "True c: -5.00, Estimated c: -5.01" [1] "True c: 4.00, Estimated c: 4.01" [1] "True c: 12.00, Estimated c: 11.83" [1] "True c: -5.00, Estimated c: -5.01" [1] "True c: 4.00, Estimated c: 3.98" [1] "True c: 12.00, Estimated c: 11.97" [1] "True c: -5.00, Estimated c: -5.01" [1] "True c: 4.00, Estimated c: 3.97" [1] "True c: 12.00, Estimated c: 12.00" [1] "True c: -5.00, Estimated c: -5.01" [1] "True c: 4.00, Estimated c: 3.99" [1] "True c: 12.00, Estimated c: 12.00" Note : there were warning messages for nonconvergence! Try to prove To compute maximum likelihood estimates(MLE), we have the log-likelihood to maximize: $$ \begin{aligned} \widehat{(\beta_0, \beta_1)} &= \mathrm{argmax}_{(\beta_0, \beta_1)} \mathrm{LogLik}(\beta_0, \beta_1) \\[8pt] &\approx \mathrm{argmax}_{(\beta_0, \beta_1)} \mathbb{E}_X \mathrm{LogLik}(\beta_0, \beta_1) \\[8pt] &= \mathrm{argmax}_{(\beta_0, \beta_1)} \mathbb{E}_X \left[ C\cdot(\beta_0 + \beta_1X) - \log[1 + \exp(\beta_0 + \beta_1X) \right] \\[8pt] &= \mathrm{argmax}_{(\beta_0, \beta_1)} \mathbb{E}_X \left[ I(X > c) \cdot(\beta_0 + \beta_1X) - \log[1 + \exp(\beta_0 + \beta_1X) \right] \\[8pt] \end{aligned} $$ Note that $\displaystyle \mathbb{E}_X(I(X>c)) = P(X>c) = 1-\Phi(c/\sigma)$ $\displaystyle \mathbb{E}_X(XI(X>c)) = \mathbb{E}_X \left(Trunc\mathcal{N}(0,\sigma^2,\min=c \right) = \sigma \frac{\phi(c/\sigma)}{1-\Phi(c/\sigma)}$ ( Wiki-Truncated Normal Distribution ) I'm currently finding $\mathbb{E}_X \log(1+\exp(\beta_0 + \beta_1X))$ . However, I'm not sure if it is a valid approach. For instance if $\mathbb{E}_X$ is a linear function of $\beta_0,\beta_1$ then $\mathrm{argmax}_{(\beta_0, \beta_1)} \mathbb{E}_X$ may have no solution. Any help will be appreciated. On imperfect separation The following may obscure my main claim, but I would like to add this. As @Whuber noted I absurdly ignored the warning messages. However, let us say the above is an idealized setting, and suppose there's a white noise in decision: say $C := I(X + W > c), X \perp W, W \sim \mathcal{N}(0, \sigma_W^2)$ . This may eschew some trivialities, but I see the similar tendency here: the recovery of $\displaystyle c \approx - \frac{\hat{\beta_0}}{\hat{\beta_1}}$ , yet with some noise. I would really like to explain what caused this behavior. N = 1000 for(sig in c(5,10,15,20)){ for (c in c(-5, 4, 12)){ X = rnorm(N, sd=sig) C = (X + rnorm(N, sd=5) > c)*1 DATA = data.frame(x=X, c=C) coef = summary(glm(C ~ X, DATA, family = "binomial"))$coefficients print(sprintf("True c: %.2f, Estimated c: %.2f", c, -coef[1,1]/coef[2,1])) } } Without warning messages, [1] "True c: -5.00, Estimated c: -5.35" [1] "True c: 4.00, Estimated c: 4.31" [1] "True c: 12.00, Estimated c: 12.27" [1] "True c: -5.00, Estimated c: -4.91" [1] "True c: 4.00, Estimated c: 3.87" [1] "True c: 12.00, Estimated c: 11.93" [1] "True c: -5.00, Estimated c: -4.72" [1] "True c: 4.00, Estimated c: 3.73" [1] "True c: 12.00, Estimated c: 12.25" [1] "True c: -5.00, Estimated c: -5.16" [1] "True c: 4.00, Estimated c: 4.25" [1] "True c: 12.00, Estimated c: 12.41"
