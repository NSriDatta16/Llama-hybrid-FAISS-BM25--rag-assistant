[site]: stackoverflow
[post_id]: 5462774
[parent_id]: 2953955
[tags]: 
After some time of brain breaking, I figured out my own mistakes... I want to put this new knowledge, so... here I go I made a very big mistake by declaring the Connection objet as a Static object in my code... so obviously, despite I created a new Connection for each new data object I created, every transaction went through a single, static, connection. With that first issue corrected, I went back to the design table, and realized that my process was: Read an Id from an input table Take a block of data related to the Id read in step 1, stored in other input tables Crunch numbers: Read the related input tables and process the data stored in them Save the results in one or more output tables Repeat the process while I have pending Ids in the input table Just by using a dedicated connection for input reading and a dedicated connection for output writing, the performance of my program increased... but I needed a lot more! My original approach for steps 3 and 4 was to save into the output each one of the results as soon as I had them... But I found a better approach: Read the input data Crunch the numbers, and put the results in a bunch of queues (one for each output table) A separated thread is checking every second if there's data in any of the queues. If there's data in the queues, write it to the tables. So, by dividing input and output tasks using different connections, and by redirecting the core process output to a queue, and by using a dedicated thread for output storage tasks, I finally achieved what I wanted: Multithreaded DML execution! I know there are better approaches to this particular problem, but this one works quite fine. So... if anyone is stuck with a problem like this... I hope this helps.
