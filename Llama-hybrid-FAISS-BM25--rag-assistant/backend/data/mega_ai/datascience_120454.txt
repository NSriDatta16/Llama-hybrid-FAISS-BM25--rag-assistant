[site]: datascience
[post_id]: 120454
[parent_id]: 120451
[tags]: 
Consider that before the attention block, you apply a (trainable) linear projection, and that there are many attention heads. This gives the needed degrees of freedom to handle the potential logic of negative cosine similarity that you think the model is lacking. For instance, one head might learn to rotate the query while another one might learn not to rotate it.
