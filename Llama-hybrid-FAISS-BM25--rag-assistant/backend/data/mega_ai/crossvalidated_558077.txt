[site]: crossvalidated
[post_id]: 558077
[parent_id]: 558073
[tags]: 
In regression, the relationship between the feature and the label is examined by t-statistic. The null hypothesis is that there is no relationship and hence the coefficient is 0. If the data suggests otherwise, when the p-value is less than a threshold (e.g. 0.05), you reject the null. This applies no matter if your feature is categorical or continuous. Although when you have categorical feature and have done one-hot-encoding, then you will need to use partial F-statistics, because you have to test for all one-hot-encoded columns of a single feature simultaneously. The feature importance from sklearn is something very different. If you have a classification problem, it measures the average reduction in impurity (basically how much does one feature/node contribute in successfully splitting the dataset into purer subgroups). If you have a regression problem, it measures the average reduction in variance. You can find some further explanations t-statistics in the regression setting on this website . Or if you search on StackExchange, you might find many similar posts talking about this topic.
