[site]: datascience
[post_id]: 92996
[parent_id]: 
[tags]: 
How to improve machine learning model using 2+ datasets

I am building a supervised machine learning model which (for example) predicts heart failure (yes/no). I have two datasets from 2 different labs A and B, which both have decent distribution, aka it's not like A has way more young people than B, but somehow A has much lower rate of heart failure. Having created a separate model for each, they each achieve around 90% or higher accuracy, but my aim is to make one model which can use a concatenated dataset containing information from A and B. Currently this combined model gets 75% accuracy for samples from B and 90% for A. What I have tried: sample in different ways / create 'even' datasets add feature indicating which lab normalise numerical features create categories for numerical features I'm aware the overall results would of course not become as good as the individual models, but how can I make the platforms at least get more similar results? I appreciate any advice / methods I could try out to tackle this type of problem!!
