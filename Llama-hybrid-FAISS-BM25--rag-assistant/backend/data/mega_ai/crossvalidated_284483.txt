[site]: crossvalidated
[post_id]: 284483
[parent_id]: 284459
[tags]: 
It might be the case that one model (say M1 on your case) leads to more extreme predictions compared to the other (M2), meaning that "certainty" (I think this concept is misleading) will be also higher for correctly predicted/classified events. Instead of reporting % of correctly classified events, you could simply compute average predicted proba over the sample and compare this stat between the 2 models. However I would not try to over-interpret the diff in predicted proba. What really matters (in terms of accuracy) is whether events are correctly predicted/classified or not. The predicted proba are not necessarily 100% meaningful. For example if you use Normal errors (instead of Logistic ones) and then estimate a Probit model, you would obtain diff predicted proba.
