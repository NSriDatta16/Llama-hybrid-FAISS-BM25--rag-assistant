[site]: crossvalidated
[post_id]: 387701
[parent_id]: 387698
[tags]: 
Your $\alpha$ reminds me of common relative point forecast accuracy metrics. Such metrics are calculated by taking the error of your focal method and dividing it by the error achieved by some benchmark method, like a naive forecast. The difference is that your $\alpha$ compares one model's performance on your actual data against its performance on randomized data, whereas relative error metric compare different models on the same evaluation data. I think your $\alpha$ might be an interesting way of looking at time series. The problem is that it evaluates time series in the context of a model . (Relative error metrics do the reverse: they evaluate a model, or rather a forecast, in the context of evaluation data.) If the model is misspecified, like non-seasonal for seasonal data, or not including important causal drivers, then the relationship of $\alpha$ to a "predictability" construct will be off. We have a popular thread on How to know that your machine learning problem is hopeless? that may be helpful. In the end, I don't think that there will be useful general cut-offs to declare predictability or non-predictability. Some things are very easy to predict, like the time the sun will rise tomorrow. Some things are harder to predict, like supermarket sales (which I do). Other things yet are very hard to predict, like a roll of a standard die, but we may at least have a probabilistic idea of what will happen, so we can hedge the uncertainty. So it really boils down to whether something is predictable enough for what you plan on doing with the forecast, i.e., for the decision you plan on taking based on the forecast.
