[site]: crossvalidated
[post_id]: 572203
[parent_id]: 
[tags]: 
Learned Loss Attenuation for Classification

In the paper What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision? they propose loss functions that capture aleatoric uncertainty. My question heavily relies on understanding of this paper, so I can go straight to the question without the details. They suggest the following loss objective for regression, where the network outputs two values [ŷ, σ²]: This allows the model to learn to output a large variance in order to attenuate the loss whenever there's aleatoric uncertainty. Now, my question is, can the same principle be used for a classification loss? In the paper they end up defining a more complex loss function, but why not just do the same thing as they did for regression? Just replacing the Mean Squared Error for a classification loss like this: Doesn't the same logic apply?
