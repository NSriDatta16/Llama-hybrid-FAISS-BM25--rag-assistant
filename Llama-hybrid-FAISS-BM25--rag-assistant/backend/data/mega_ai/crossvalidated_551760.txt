[site]: crossvalidated
[post_id]: 551760
[parent_id]: 449735
[tags]: 
Why are frequentists uncomfortable with bayesian statistics when "optimization" algorithms used in frequentist statistics is bayesian? This is a loaded question. The premise that iterative frequentist methods are like the updating of prior information is false. (In addition the reference to statisticians as 'frequentists' makes it sound like a religion or nationality. There are no frequentist people , but there are people that use frequentist methods .) In Bayesian statistics we compute a posterior probability distribution for a parameter that is to be estimated (and in order to do so we need a prior probability distribution). This type of computation, of a probability distribution for the parameter that is to be estimated, does not happen with frequentist methods. The iterative procedures in frequentist methods that you talk about (e.g. iterative re-weighted least squares, or optimization methods to find a MLE, the solution to an equation) are an entirely different thing than this computation of a posterior probability distribution. These iterative procedures require a starting point but it is different from a prior distribution. Why is a prior distribution problematic and this starting point not? The iterative procedures converge to a point/solution that is purely dependent on the data and completely independent on the starting point. There is no influence of the starting point (or only limited, for instance if the computer makes mistakes because of a bad starting point, but in principle the algorithm should lead to a final point/solution independent from the starting point). On the other hand, with a Bayesian method the end result will be dependent on the data and also the prior information. This is different from the iteration in some algorithm to compute/solve an equation. With Bayes rule your solution becomes dependent on the prior. So a lot effort is needed to select a good prior for the analysis and many books and articles are written about it (it might be half if not more of all the literature about Bayesian methods). A criticism here might be that the solution of a frequentist method is not purely dependent on the data but also on the model which is part of the analysis (although this a detour from your thinking about the iterative algorithms). Frequentist methods and Bayesian methods are all containing information outside the data (which makes the methods subjective). The models, assumptions, decisions about analysis, etc. The thing is that with Bayesian methods you have this prior which can be of large influence (if it provides more information than the data). But if we focus on your idea about the iterative algorithms... those have nothing to do with the prior information and subjective input into the statistical analysis.
