[site]: datascience
[post_id]: 60470
[parent_id]: 
[tags]: 
Supervised Learning: time estimation of bike repair

I would like to train a model that estimate the time a given shop would take to repair for a bike using the data below: shops.csv | variable | description | +++++++++++++++++++++++++++++++++++++++ | shop_id | unique shop identifier | | country | country of the shop | | city | city of the shop | | nb_empl | number of employees | | review | google review | repairs.csv | variable | description | +++++++++++++++++++++++++++++++++++++++++++++++++++++++ | repair_id | unique repair identifier | | price | value of the repair in local currency | | type | what to repair (brakes, wheel...) | | bike_model| model of the bike to repair | | size | size of the bike to repair | | shop_id | shop that carries out the repair | | duration | time it took to repair the bike (TARGET)| My first thought was to combine the datasets and treat the problem as a supervised learning problem. Meaning that I would use regression, XGBoost, random forest to fit my model in order to get the estimated time of repair, all the shops in the training set being mixed. But my concern is that my shops might be similar, it doesn't mean that they would spend the same amount of time to repair a bike. My second thought was to treat each shop independently, so basically one model per shop. My questions are: How would you approach the problem yourself? Is there a way to combine my two thoughts together to get a better model?
