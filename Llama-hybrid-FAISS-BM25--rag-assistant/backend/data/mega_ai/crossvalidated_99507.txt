[site]: crossvalidated
[post_id]: 99507
[parent_id]: 97704
[tags]: 
The fastICA approach does require a pre-whitening step: the data are first transformed using PCA, which leads to a diagonal covariance matrix, and then each dimension is normalized such that the covariance matrix is equal to the identity matrix (whitening). There are infinite transformations of the data which result in identity covariance matrix, and if your sources were Gaussian you would stop there (for Gaussian multivariate distributions, mean and covariance are sufficient statistics), in the presence of non-Gaussian sources you can minimize some measure of dependence on the whitened data, therefore you look for a rotation of the whitened data that maximizes independence. FastICA achieves this using information theoretic measures and a fixed-point iteration scheme. I would recommend the work of Hyvärinen to get a deeper understanding of the problem: A. Hyvärinen. Fast and Robust Fixed-Point Algorithms for Independent Component Analysis. IEEE Transactions on Neural Networks 10(3):626-634, 1999. A. Hyvärinen, J. Karhunen, E. Oja, Independent Component Analysis, Wiley & Sons. 2001 Please note that doing PCA and doing dimension reduction are not exactly the same thing: when you have more observations (per signal) than signals, you can perform a PCA retaining 100% of the explained variance, and then continue with whitening and fixed point iteration to obtain an estimate of the independent components. Whether you should perform dimension reduction or not is highly context dependent and it is based on your modeling assumptions and data distribution.
