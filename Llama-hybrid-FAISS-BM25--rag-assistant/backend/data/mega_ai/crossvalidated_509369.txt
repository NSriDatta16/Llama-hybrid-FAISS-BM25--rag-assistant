[site]: crossvalidated
[post_id]: 509369
[parent_id]: 296471
[tags]: 
Some math becomes easier with $e$ as a base, that's why. Otherwise, consider this form of softmax: $\frac{e^{ax_i}}{\sum_j e^{ax_j}}$ , which is equivalent to $\frac{b^{x_i}}{\sum_j b^{x_j}}$ , where $b=e^a$ . Now, consider this function: $\sum_i\frac{e^{ax_i}}{\sum_j e^{ax_j}} x_i$ . You can play with coefficient $a$ making the function less or more soft max. When $a\to\infty$ , it is $\max(x)$ because $\lim_{a\to\infty}\frac{e^{ax_i}}{\sum_j e^{ax_j}}=\mathrm{argmax}(x)$ . When $a=1$ it is $\mathrm{softmax}(x)\cdot x$ - a smoother version of max. When $a=0$ it is as soft as it gets: a simple average $\frac 1 n \sum_i x_i$
