[site]: crossvalidated
[post_id]: 611374
[parent_id]: 611057
[tags]: 
The VAE is a direct implementation of the ELBO, a lower bound on the log likelihood of the data, which makes optimising its training objective a provably valid approach . Any deviation from that would need to be considered carefully as you may otherwise have no idea what optimising the new objective would achieve, i.e. what the model would learn. Aim : train a model of the data $p_\theta(x)$ to fit the true data distribution $p(x)$ , where the model is defined in terms of latent variable $z$ : $p_\theta(x) = \int p_\theta(x|z)p_\theta(z) dz$ Approach : minimise the KL divergence between $p_\theta(x)$ and $p(x)$ i.e. find $\text{argmin}_{\theta} \int_x p(x)\log\tfrac{p(x)}{p_\theta(x)} = \text{argmax}_\theta \int_x p(x)\log p_\theta(x)$ i.e. maximising cross entropy (RHS) is equivalent to minimising the KL divergence (LHS) it is typically intractable to take gradients w.r.t $\theta$ to maximise this, so a lower bound is maximised: $\int p(x)\log p_\theta(x) \geq \int_x p(x)\int_z q(z|x)\{\log p_\theta(x|z) - \tfrac{q(z|x)}{p_\theta(z)}\}\quad$ [ = ELBO in "VAE form"] While the terms of this expression can be thought of intuitively as reconstruction loss + regulariser , this is just intuition, the fact is you want $p_\theta(x)$ to learn $p(x)$ , which maximising the ELBO guarantees. If you "hack around with it", that guarantee may be lost. The proposed alternative "regulariser" may (i) happen to equate to $\tfrac{q(z|x)}{p_\theta(z)}$ for particular choices of those distributions, in which case you are baking in those assumptions (which may be good or bad depending on the data distribution); or (ii) to some extent approximate particular distributional assumptions and "seem to work" (e.g. on a given test set), but, given it is an approximation (and so "wrong" in places) there may be regions in the data space where the VAE performs poorly.
