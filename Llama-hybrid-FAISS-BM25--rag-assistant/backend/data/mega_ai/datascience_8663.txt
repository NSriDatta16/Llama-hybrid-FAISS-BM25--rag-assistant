[site]: datascience
[post_id]: 8663
[parent_id]: 8657
[tags]: 
There are several issues I see with the implementation. Some are just unnecessarily complicated ways of doing it, but some are genuine errors. Primary takeaways A: Try to start from the math behind the model. The logistic regression is a relatively simple one. Find the two equations you need and stick to them, replicate them letter by letter. B: Vectorize. It will save you a lot of unnecessary steps and computations, if you step back for a bit and think of the best vectorized implementation. C: Write more comments in the code. It will help those trying to help you. It will also help you understand each part better and maybe uncover errors yourself. Now let's go over the code step by step. 1. The sigmoid function Is there a reason for such a complicate implementation in phi(t) ? Assuming that t is a vector (a numpy array), then all you really need is: def phi(t): 1. / (1. + np.exp(-t)) As np.exp() operates element-wise over arrays. Ideally, I'd implement it as a function that can also return its derivative (not necessary here, but might be handy if you try to implement a basic neural net with sigmoid activations): def phi(t, dt = False): if dt: phi(t) * (1. - phi(t)) else: 1. / (1. + np.exp(-t)) 2. Cost function Usually, the logistic cost function is defined as a log cost in the following way (vectorized): $ \frac{1}{m} (-(y^T \log{(\phi(X\theta))})-(1-y^T)(\log{(1 - \phi(X\theta))}) + \frac{\lambda}{2m} \theta^{1T}\theta $ where $\phi(z)$ is the logistic (sigmoid) function, $\theta$ is the full parameter vector (including bias weight), $\theta^1$ is parameter vector with $\theta_1=0$ (by convention, bias is not regularized) and $\lambda$ is the regularization parameter. What I really don't understand is the part where you multiply y * z . Assuming y is your label vector $y$, why are you multiplying it with your z before applying the sigmoid function? And why do you need to split the cost function into zeros and ones and calculate losses for either sample separately? I think the problem in your code really lies in this part: you must be erroneously multiplying $y$ with $X\theta$ before applying $\phi(.)$. Also, this bit here: X.dot(w) + c . So c is your bias parameter, right? Why are you adding it to every element of $X\theta$? It shouldn't be added - it should be the first element of the vector $X\theta$. Yes, you don't regularize it, but you need to use in the "prediction" part of the loss function. In your code, I also see the cost function as being overly complicated. Here's what I would try: def loss(X,y,w,lam): #calculate "prediction" Z = np.dot(X,w) #calculate cost without regularization #shape of X is (m,n), shape of w is (n,1) J = (1./len(X)) * (-np.dot(y.T, np.log(phi(Z))) * np.dot((1-y.T),np.log(1 - phi(Z)))) #add regularization #temporary weight vector w1 = copy.copy(w) #import copy to create a true copy w1[0] = 0 J += (lam/(2.*len(X))) * np.dot(w1.T, w) return J 3. Gradient Again, let's first go over the formula for the gradient of the logistic loss, again, vectorized: $\frac{1}{m} ((\phi(X\theta) - y)^TX)^T + \frac{\lambda}{m}\theta^1$. This will return a vector of derivatives (i.e. gradient) for all parameters, regularized properly (without the bias term regularized). Here again, you've multiplied by $y$ way too soon: phi(y * z) . In fact, you shouldn't have multiplied by $y$ in gradient at all. This is what I would do for the gradient: def gradient(X, y, w, lam): #calculate the prediction Z = np.dot(X,w) #temporary weight vector w1 = copy.copy(w) #import copy to create a true copy w1[0] = 0 #calc gradient grad = (1./len(X)) * (np.dot((phi(Z) - y).T,X).T) + (lam/len(X)) * w1 return grad The actual gradient descent implementation seems ok to me, but because there are errors in the gradient and cost function implementations, it fails to deliver :/ Hope this will help you get on track.
