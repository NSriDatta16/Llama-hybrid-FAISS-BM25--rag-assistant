[site]: crossvalidated
[post_id]: 352315
[parent_id]: 
[tags]: 
Aggregating factors: dummy vs. relative frequency

I have a dataset that looks like this: library(dplyr) glimpse(dat) Observations: 6 Variables: 3 $ ID 1, 1, 1, 2, 2, 2 $ Amount 10, 70, 80, 50, 10, 10 $ Product A, B, C, B, E, A As you can see, the ID is duplicated. The issue is that I have to "condense" this data. So, every ID/entity may occur only once. For the continuous variable (Amount), this is not an issue because I can simply calculate the mean per ID. The situation may be different for the factor (Product). In my opinion, there are two options for how to handle this. Creating dummy variables. This means that an ID gets a 1 if he/she ever bought Product A, B, C etc. and a 0 if this isnÂ´t the case. Problem: Frequencies will not be considered -> loss of information. Calculating relative frequencies. This means that an ID gets a value between 0 and 1. This value is dependent on how often he/she bought a certain product previously. Problem: Potentially more computationally expensive. IDs which occur often in the data will probably get low values (0.2, 0.5 etc.) and IDs which occur only once will get very low or high values (0 or 1). For a logistic/linear regression or a tree (random forest/gbm), which one would be the better choice? Is there a general answer or is it dependent on the learner characteristics?
