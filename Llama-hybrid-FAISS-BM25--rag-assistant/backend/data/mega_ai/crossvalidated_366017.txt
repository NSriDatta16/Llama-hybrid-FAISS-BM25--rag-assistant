[site]: crossvalidated
[post_id]: 366017
[parent_id]: 366003
[tags]: 
The typical LASSO model is an ordinary linear model plus the a penalty: $$ \min_\beta L(y,g(X\beta))+\lambda \|\beta \|_1 $$ I want to emphasize that the LASSO model is a linear model, so the coefficients are estimating a function that is linear in its parameters. Extensions of a LASSO to GLMs are still linear in the parameters under estimation. In this sense, basis expansions or variable transformations can be powerful tools to improve the expressive power of your model. Likewise, this great power comes the responsibility to avoid overfitting. In an ideal world, we would know exactly which sets of transformations are the correct ones, and then the modeling task reduces to a linear model. But this is rarely the case, so random forests can do the heavy lifting for us. The random forest model is an ensemble of many different decision trees. It should be obvious that decision trees are nonlinear: for any binary split, the daughter nodes yield distinct constant functions. The effect of many such binary splits is to divide the feature space into a number of axis-aligned rectangles, each with a different estimate. Arbitrarily many binary, axis-aligned splits can approximate a complex boundary by using simpler shapes. The classic example is to consider a binary classification task with a perfect linear decision boundary on the line $x_1 + x_2 > c$. This manifests as a diagonal split. Clearly a single axis-aligned split can't approximate a diagonal very well, but many axis aligned splits, you can make a "stair-step" shape that can approximate the diagonal arbitrarily well . Likewise, the same is true for approximating relationships like logarithms or quadratics or sinusoids. My tangentially-related answer here provides some more elaboration. Can a random forest be used for feature selection in multiple linear regression? These threads address the random forest piece, but not the LASSO piece. does feature engineering matter when doing Random Forest or Gradient Boosting? Including Interaction Terms in Random Forest Effect of categorical interaction terms with random forest machine learning algorithm Random Forests and data transformations
