[site]: crossvalidated
[post_id]: 620921
[parent_id]: 620918
[tags]: 
It means the following. Suppose you have, say, $m$ confidence intervals for $m$ parameters, computed on the same dataset with confidence level $1-\alpha$ each. Statistical theory tells us that each of the intervals has a coverage probability of at least $1-\alpha$ . However, the joint probability that all confidence intervals cover their respective true values can be shown to be greater than or equal to $$ 1- m\alpha. $$ Thus, the larger the number of intervals $m$ , the lower the overall degree of confidence as compared to the desired target $1-\alpha$ . There are plenty of ways to fix this loss of confidence due to multiple comparisons, and one of them, although not the most effective, is the Bonferroni method. The idea of the Bonferroni adjustment is to use in the calculation of the confidence intervals $$\alpha_B = \alpha/m$$ instead of the original $\alpha$ . Indeed, using $\alpha_B$ in the above upper bound we have simply $$ 1-m\alpha_B = 1- m(\alpha/m) = 1-\alpha. $$ As an example, if $m = 4$ , and we desired our intervals to have overall coverage of $95\%$ , so $\alpha =0.05$ , then we can set $\alpha_b = 0.05/4 = 0.0125$ . Note 1. The Bonferroni adjustment gives us a simple way to enlarge our confidence intervals such the probability they all cover the true parameter is as close to $1-\alpha$ as possible. However, each of the adjusted confidence intervals will have coverage that is much greater than $1-\alpha$ ; this is the price you pay in order to get correct overall confidence. Note 2. In practice with high $m$ the Bonferroni method will be too conservative and other correction methods (Holm, Benjamini-Hochberg, etc.) may be more effective.
