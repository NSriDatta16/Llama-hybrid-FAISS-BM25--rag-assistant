[site]: crossvalidated
[post_id]: 33101
[parent_id]: 
[tags]: 
Default priors for mixtures

Suppose we have $p$ dimensional vectors $Y_i$ which we model with $f_Y (y |\theta) = \sum \pi_k N(y | \mu_k, \Sigma_k)$ with $\theta$ being a catch all for the model parameters (the number of components might be a finite known/unknown number or infinite as in Dirichlet process mixtures). The prior on $\pi$ will either be a uniform or Stick-breaking prior with perhaps a hyperprior on the associated precision parameter. What are good default prior choices for the cluster components $(\mu_k, \Sigma_k)$ ? I have been using a conjugate Normal-inverse-Wishart, i.e. $$ (\mu_k, \Sigma_k) \sim \mathcal N(\mu_k |m, \Sigma_k / n_0) \mathcal W^{-1} (\Sigma_k | \Psi, \nu). $$ I then fix $\nu, n_0$ relatively small and either estimate $m$ and $\Psi$ empircally, or specify independent hyperpriors and estimate the key parameters of the hyperpriors empircally (from what I can tell, there is some evidence that people do this since it seems to be what is done in the vignettes for DPpackage ). Ultimately, I'd like to set things up so that the prior should be relatively uninformative (with at least some hope of adding prior knowledge in a systematic way), but there are a lot of parameters floating around and the individual influence of each one isn't always transparent. I've come across some papers on this issue that give guidance for choosing parameters empirically but they mainly focus on $p = 1$ . Given that these models are pervasive in Bayesian nonparametrics, I figure guidelines must exist and that I just haven't found them. Ideally, I'd like answers here to be specific as possible; in particular, I'm most interested in choosing the hyperparameters in the priors/hyperpriors.
