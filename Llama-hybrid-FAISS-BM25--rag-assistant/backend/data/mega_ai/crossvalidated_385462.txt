[site]: crossvalidated
[post_id]: 385462
[parent_id]: 
[tags]: 
Deep Q - Learning Exploration - BestQ Value

I am trying to implement a Deep Q - Network to play Asteroids. Unfortunately, I am not sure how to calculate the Q Value exactly if I am exploring. For example, the Agent is exploring for 1 second (otherwise makes no sense; I cannot let it just explore one step), then, of course, it could have done 0.3s something really good. Unfortunately, it makes a mistake at 0.99s and the reward collapses. I use at the moment the following formula to evaluate the Q - Value: $Q_{new,t} = reward + \gamma Q_{max,t+1}$ But how do I know the max Q Value of the next step? I could consider the best Q - Value the network says, but this is not necessarily true. Edit: You can see the implementation as far (poorly written) here: https://github.com/SuchtyTV/RLearningBird/blob/master/src/main/java/rlgame/Brain.java
