[site]: datascience
[post_id]: 99556
[parent_id]: 98361
[tags]: 
It is straightforward to stack different neural networks with current deep learning frameworks (i.e., PyTorch or TensorFlow). There is no separate output for $r$ , it just one of many layers. It could look like this: You can freeze or not freeze any layers in a stacked neural network. You can decide how far to backpropagate the training updates. The advantages of unfreezing layers is that the model can learn better feature representations. The disadvantages of unfreezing is that training can take longer for very deep neural networks.
