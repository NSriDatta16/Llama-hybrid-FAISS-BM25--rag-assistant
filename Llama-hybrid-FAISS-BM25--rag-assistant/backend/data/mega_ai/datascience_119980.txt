[site]: datascience
[post_id]: 119980
[parent_id]: 
[tags]: 
How can I know the best number of features to use?

I noticed that developing ml models a very important step in feature engineering is adding new features that can explain better the target variable. Recently I experienced a situation where by adding some features the model's performance improved, while adding five more features the performance worsened. So, my question is: is the feature engineering process more an experimentation phase where I need to try different scalers, features and encoding methods? If I am wrong please let me know.
