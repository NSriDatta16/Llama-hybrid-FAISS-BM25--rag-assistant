[site]: crossvalidated
[post_id]: 639169
[parent_id]: 
[tags]: 
Feature selection using backward feature selection in scikit-learn and PCA

I have calculated the scores of all the columns in my dataframe, which has 312 columns and 650 rows, using PCA. I used the following code: all_pca=PCA(random_state=4) all_pca.fit(tt) all_pca2=all_pca.transform(tt) plt.plot(np.cumsum(all_pca.explained_variance_ratio_) * 100) plt.xlabel('Number of components') plt.grid(which='both', linestyle='--', linewidth=0.5) plt.xticks(np.arange(0, 330, step=25)) plt.yticks(np.arange(0, 110, step=10)) plt.ylabel('Explained variance (%)') plt.savefig('elbow_plot.png', dpi=1000) The result is the following image: My main goal is to use only important features for Random forest regression, Gradient boosting, OLS regression and LASSO . As you can see, 100 columns describe 95.2% of the variance in my dataframe. I have 2 Questions: Can I use this threshold (100 Columns) for backward feature selection? Is it best practice to use Random forest for feature selection and train a random forest on the result? Or it would be better to use Backward/forward selection?
