[site]: crossvalidated
[post_id]: 171546
[parent_id]: 171535
[tags]: 
By Chebyshev's inequality we know that probability of some $x$ being $k$ times $\sigma$ from mean is at most $\frac{1}{k^2}$: $$ \Pr(|X-\mu|\geq k\sigma) \leq \frac{1}{k^2} $$ However with making some distributional assumptions you can be more precise, e.g. Normal approximation leads to 68–95–99.7 rule . Generally using any cumulative distribution function you can choose some interval that should encompass a certain percentage of cases. However choosing confidence interval width is a subjective decision as discussed in this thread . Example The most intuitive example that comes to my mind is intelligence scale. Intelligence is something that cannot be measured directly, we do not have direct "units" of intelligence (by the way, centimeters or Celsius degrees are also somehow arbitrary). Intelligence tests are scored so that they have mean of 100 and standard deviation of 15. What does it tell us? Knowing mean and standard deviation we can easily infer which scores can be regarded as "low", "average", or "high". As "average" we can classify such scores that are obtained by most people (say 50%), higher scores can be classified as "above average", uncommonly high scores can be classified as "superior" etc., this translates to table below. Wechsler (WAIS–III) 1997 IQ test classification IQ Range ("deviation IQ") IQ Classification 130 and above Very superior 120–129 Superior 110–119 High average 90–109 Average 80–89 Low average 70–79 Borderline 69 and below Extremely low (Source: https://en.wikipedia.org/wiki/IQ_classification ) So standard deviation tells us how far we can assume individual values be distant from mean. You can think of $\sigma$ as of unitless distance from mean. If you think of observable scores, say intelligence test scores, than knowing standard deviations enables you to easily infer how far (how many $\sigma$'s) some value lays from the mean and so how common or uncommon it is. It is subjective how many $\sigma$'s qualify as "far away", but this can be easily qualified by thinking in terms of probability of observing values laying in certain distance from mean. This is obvious if you look on what variance ($\sigma^2$) is $$ \operatorname{Var}(X) = \operatorname{E}\left[(X - \mu)^2 \right]. $$ ...the expected (average) distance of $X$'s from $\mu$. If you wonder, than here you can read why is it squared .
