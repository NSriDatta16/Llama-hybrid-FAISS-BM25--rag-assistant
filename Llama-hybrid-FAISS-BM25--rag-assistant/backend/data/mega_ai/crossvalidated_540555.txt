[site]: crossvalidated
[post_id]: 540555
[parent_id]: 540460
[tags]: 
There are quite a few algorithms. The simplest is (I believe I have the name right) Wild Binary Regression segmentation. Here our goal is to identify a place to split our time series into 2 and fit a regression for each with connectivity constraints. Then we get the residuals and fit on those and combine the two models (this is gradient boosting) then get residuals and fit again and so on and so forth until some criteria, either a number of changepoints or some criteria which typically takes into account the number of points like maybe the AICC with k replaced with the number of changepoints found. I believe that is the algorithm although I approach it from a pure gradient boosting perspective in this bit of messy code for python: https://github.com/tblume1992/ThymeBoost I do have a much cleaner implementation on pip but there is no documentation for it yet but I have written out some other answers using it. Of course this is in python but the underlying algorithm of boosting with binary-segmented regressions is easy enough to whip up in r. One issue with your data specifically is that the connectivity constraints may not work too nicely since the drop and increases are so drastic. So you may want to relax that constraint to allow it to have a intercept that is different then the other segment and just make sure you add that extra parameter (the new intercept) to whatever cost function that is monitoring everything.
