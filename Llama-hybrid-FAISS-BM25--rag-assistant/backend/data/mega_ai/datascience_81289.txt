[site]: datascience
[post_id]: 81289
[parent_id]: 80747
[tags]: 
As mentioned in the comment, if the question is when does it make sense to use coordinate descent over stochastic gradient descent, then, one advantage with coordinate descent is that, it updates only one parameter at a time. Thus, when the data has a very very large number of features, it might make sense to use Coordinate Descent over stochastic gradient descent as SGD will try to update all model parameters at the same time. Also, for huge amounts of data CD might have lower computational complexity as against SGD. Also, it makes sense to use Coordinate descent for optimization when the function is not differentiable. Although, there are ways to convert non-differentiable functions to differentiable ones, still Coordinate descent might be one of the ways to move forward.For example, coordinate descent is a good technique to optimize a loss with L1 penalty. It is considered state of the art for Lasso regression as well as for linear SVMs.
