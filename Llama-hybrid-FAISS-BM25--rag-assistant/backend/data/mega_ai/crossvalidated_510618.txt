[site]: crossvalidated
[post_id]: 510618
[parent_id]: 
[tags]: 
Can we derive the reward function from the behaviour of a trained agent?

Imagine we train some agent using a reinforcement learning technique where the reward function is only a function of the current state $s_{i}$ and the next state $s_{i+1}$ . After lots of training, we get a (close) to optimal policy. We now use this agent in a set of environments giving us a chain of states $({s_0,s_1,s_2,...,s_n})$ . Are there methods of inversing the behaviour back to the reward function? If so, what are the limitations of such methods?
