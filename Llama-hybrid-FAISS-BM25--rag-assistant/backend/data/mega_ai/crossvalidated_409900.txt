[site]: crossvalidated
[post_id]: 409900
[parent_id]: 
[tags]: 
Features with missing entries are different in train data than in test data

I know there is a number of approaches to preprocess training data with missing entries : dropping features, imputing mean values, etc. I've compared few of such approaches and found that dropping features with missing values gives the lowest Mean Absolute Error for my training data. I applied it for the data and next trained a model with Random Forest Regressor. Next, I wanted to evaluate the trained model for a test data that I haven't seen nor have access before. It came up that the test data contains different features with missing entries than the train data , i.e., features that have all entries in the train data, are missing some entries in the test data. I wonder how can I handle such a situation? Even if I apply same preprocessing to the test data, I will still have features with missing values. And Random Forest Regressor in scikit-learn complains when encountering missing entries. If I drop all features from the test data that have missing entries and the trained model cannot be used because: Number of features of the model must match the input. Model n_features is 36 and input n_features is 25 Is there a workaround for this preprocessing technique or it is a limitation of it?
