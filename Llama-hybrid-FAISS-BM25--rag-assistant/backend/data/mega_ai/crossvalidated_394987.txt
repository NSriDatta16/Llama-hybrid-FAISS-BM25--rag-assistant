[site]: crossvalidated
[post_id]: 394987
[parent_id]: 391280
[tags]: 
I would like to share my understanding here. Here is a thesis and in its related work author has explained Transfer learning and Fine-Tuning. Also, the survey on Transfer Learning is a good read to understand these concepts in detail. Unsupervised pre-training is a good strategy to train deep neural networks for supervised and unsupervised tasks. Fine-tuning can be seen as an extension of the above approach where the learned layers are allowed to retrain or fine-tune on the domain specific task. Transfer learning, on the other hand, requires two different task, where learning from one distribution can be transferred to another. [These points are taken from the related work of this thesis] Now, I think your understanding is correct about Transfer learning and Fine-Tuning. But, Freezing the weights is a choice that you get, if you don't freeze then we call that the network is now fine-tuned on the domain-specific data. And yes, it should usually provide better generalization. On the other hand, if you freeze the weights depends on the problem and type of network you have. For example, IMAGENET layers are widely used to classify images and its layers are frozen (1) as its computationally expensive (2) Imagenet data covers a large distribution of the data and (3) the last layer is usually enough to capture the small variations that a domain-specific image. This is good because of strong representation capacity of Imagenet and may not be true for every model. Hence depending on the case one should empirically answer this question precisely.
