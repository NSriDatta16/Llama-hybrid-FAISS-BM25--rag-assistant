[site]: crossvalidated
[post_id]: 482561
[parent_id]: 433867
[tags]: 
It is actually the opposite of what you said "The sigmoid binary loss would encourage the true label logits to be much higher than 0, and the other logits to be much smaller than 0. I think everyone agrees that this is intuitive." When applying the logit and taking binary cross-entropy loss, we encourage each output component to be independent of each other: one logit is higher doesn't push the other logits to be smaller. This is exactly the opposite of softmax, which encourages one logit to be higher while all others to be smaller. This is why it's counterintuitive, as indicated in the paper.
