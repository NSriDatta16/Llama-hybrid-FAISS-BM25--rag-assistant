[site]: crossvalidated
[post_id]: 120304
[parent_id]: 
[tags]: 
Are there local maxima in the standard 2-pl model or is there proof that there aren't?

I'm looking to use iterative logistic regression to find the parameter values of the 2 pl model: $ P = \sigma(\alpha_{i} (\theta_{s} - \beta_{i})) $ where $ \sigma(x) = \frac{1}{1+e^{-x}} $ I know how to proof that in normal logistic regression the log likelihood of the data is concave and that there will thus be no local maxima in the regression. As an extension of this if I keep $\alpha_{i}$ and $\beta_{i}$ in the model above constant, I can obtain $\theta_{s}$ without running into local maxima and vice versa. My question though is if there could be local maxima over the whole space of paramaters and if not, how to proof this. Edit: Some clarification for those not familiar with IRT and the 2PL model. The 2PL model gives a probability ($P$) that student $s$ will correctly answer item $i$. There is thus an $\alpha$ and $\beta$ parameter for every item and a $\theta$ parameter for every student. Every datapoint is one question that is either answered correctly or incorrectly and the only parameters that influences the prediction $P$ for this datapoint is the parameters of the student $s$ and the item $i$ involved in that question. What is looked for are values for all the parameters such that the likelihood of the data ($\prod_{d \in D} P_{d}^{t_d} (1- P_{d})^{1-t_d}$ where D are all the datapoints and t is 1 if the question was answered correctly or 0 otherwise) is maximized. It is known that the solution is not unique due to the nature of the model. My question is if there are local maxima in $\prod_{d \in D} P_{d}^{t_d} (1- P_{d})^{1-t_d}$ and if not how this can be proven.
