[site]: crossvalidated
[post_id]: 253172
[parent_id]: 
[tags]: 
How should I normalise the inputs to a neural network?

My neural network can have all sorts of inputs from different datasets. For example, with digit recognition using the MNIST dataset, there are 784 inputs (each pixel 28x28) and each value is between 0-255 (single grayscale). However, this would produce math range errors with the sigmoid function because too larger negative values would be produced on later layers. So, all value are divided by 255 to get decimal values between 0-1. Is that correct? Well, does this mean with other datasets, that have big values, you can just divide them all by 10 or 100 to make them smaller? Does that matter? What is this process known as?
