[site]: crossvalidated
[post_id]: 457596
[parent_id]: 455803
[tags]: 
I'm not sure of any literature, as its a pretty specific question, but I tried my hand at a very quick and dirty simulation study. For 1000 cases, I have 10 variables. Each of these 1000 cases has two instances of the same variable, such as we'd see over time. The correlation is either r = .0, .25, .5, .75, or .99. I pivot the data into tidy format, such that it has 2000 rows and 10 columns. So within one column, we have 2000 values with hidden dependencies in observations across 1000 pairs. This is done for each of the 10 variables. The 10 variables are uncorrelated with one another, and are all drawn from normal distributions. I then generate an outcome. To do so, I take the first column of this 2000 x 10 data frame. If values are between -1 and 0 or they are above 2, then I assign it to outcome "A" with .85 probability. Otherwise, it is assigned to outcome "B" with .85 probability. I do this to add some irreducible error to the data. The rest of the nine variables are all, thus, noise. I predict the outcome from all 10 variables in a random forest, and then I pull the Gini coefficient for the variable importance. Since only one variable is predictive, I pull the max value. I run this 500 times for each r value. In this admittedly simple scenario, if correlated predictors mess with the variable importance, then we should see differences as the r increases. That would let us know that unaccounted for dependencies in the data are affecting variable importance. All code can be found at this GitHub Gist. Here are box plots showing correlation on the x-axis and Gini coefficient on the y-axis: We can do a linear model treating the correlation as both a continuous or factor variable, and they both tell us that an increase in correlation means an increase in Gini coefficient: Call: lm(formula = gini ~ cor, data = dat) Residuals: Min 1Q Median 3Q Max -9.6585 -1.9407 -0.2193 1.7701 14.7419 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 99.89959 0.09855 1013.734 Call: lm(formula = gini ~ factor(cor), data = dat) Residuals: Min 1Q Median 3Q Max -10.2650 -1.9187 -0.1948 1.7452 15.1716 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 99.8029 0.1261 791.678 However, there does appear to be heteroskedasticity: The spread gets bigger as the correlation increases. We can do Bartlett's test to see if the null hypothesis that all of these instances are from a population with equal variance. And it looks like that null is rejected: Bartlett test of homogeneity of variances data: gini by factor(cor) Bartlett's K-squared = 52.68, df = 4, p-value = 9.944e-11 We can use a double generalized linear model to look at the mean and variance at the same time. And we see that increased correlation leads to a higher Gini coefficient and a greater spread (dispersion) around that mean, as well: Call: dglm(formula = gini ~ cor, dformula = ~cor, data = dat) Mean Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 99.8632523 0.08826698 1131.377215 0.000000e+00 cor 0.9606741 0.16030576 5.992761 2.361061e-09 (Dispersion Parameters for gaussian family estimated as below ) Scaled Null Deviance: 2535.942 on 2499 degrees of freedom Scaled Residual Deviance: 2500 on 2498 degrees of freedom Dispersion Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) 1.7772280 0.04912051 36.180976 1.212549e-286 cor 0.5751425 0.08064254 7.131998 9.892195e-13 (Dispersion parameter for Gamma family taken to be 2 ) Scaled Null Deviance: 3265.015 on 2499 degrees of freedom Scaled Residual Deviance: 3211.794 on 2498 degrees of freedom Minus Twice the Log-Likelihood: 12253.81 Number of Alternating Iterations: 3 Based on this simple simulation, it looks like it might (a) inflate the Gini coefficient, and (b) create greater spread around that biased number. Very important, though, that this is just one situation, and I would need to simulate many others (correlated predictors, more than one predictor affecting the outcome, dichotomous predictors, continuous outcomes, etc.) to really be able to give you a sweeping conclusion. Hopefully that gives you some idea and gives you some code to mess around with and see how things change when you shift around the data generating procedure. It should also be said that I could generate as many iterations as I want, and even a small deviation of mean differences and variance differences could be statistically significant, given theoretically unlimited power . So that is to say, it might affect the variable importance, but that doesn't mean it is big enough to actually matter when making decisions about inference or prediction in the real world.
