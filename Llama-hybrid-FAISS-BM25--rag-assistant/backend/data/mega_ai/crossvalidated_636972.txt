[site]: crossvalidated
[post_id]: 636972
[parent_id]: 635088
[tags]: 
People usually distinguish two types of uncertainty: aleatoric uncertainty and epistemic uncertainty. Aleatoric uncertainty describes the uncertainty due to pure randomness in the data-generating process, e.g., the noise term $\varepsilon$ in $y=f(x)+\varepsilon$ . You will never be able to predict this noise (because it is random), thus even a perfect model that has correctly learned the true $f(x)$ will display random errors corresponding to $\varepsilon$ vs. the true outcome $y$ . In a logistic regression context, quantifying aleatoric uncertainty is fundamentally impossible . Epistemic uncertainty describes the uncertainty in your fitted model parameters $\theta$ that parametrize the model $f(x)$ . In other words, how precisely do your data determine your fitted model? In a logistic regression context, this is what you can get from the identified covariances / confidence intervals of the parameter estimates. Now, you might think that what you want is a prediction interval (which is distinctly different from a confidence interval) or a prediction set : an interval that contains the true outcome value for this individual data point with a high probability. To get this, one needs to combine aleatoric and epistemic uncertainty estimates. However, prediction intervals/sets are, again, rather useless for logistic regression because the aleatoric uncertainty component cannot be quantified. Finally, the most practical answer to your question is: yes, the number between 0 and 1 that you get is a reasonable measure of model uncertainty if your model is calibrated , which is generally more likely than not for logistic regression (but you should still check it). Read up on the meaning of calibration; this is most likely what you want to look into.
