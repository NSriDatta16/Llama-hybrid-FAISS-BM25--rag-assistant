[site]: crossvalidated
[post_id]: 502641
[parent_id]: 
[tags]: 
Why does the PyTorch tutorial on DQN define state as a difference?

I'm a master's student in EECS working my way towards understanding how DQN [0] works. I'm working towards solving the CartPole-v0 task in as few iterations as possible. First of all I implemented a basic Q-learning algorithm which took forever to converge, then I added decaying learning rate and experimentation proportion which made a whole lot of difference. I'm not interpreting state from the image yet, I just take an observation and discretize it to simplify things - one complication at a time. I'm now trying to add experience replay but keep the Q-matrix approach - I will substitute it with a function-approximation ANN later. I'm wondering why the seventh code snippet in the PyTorch tutorial on DQN, the second under section "training loop" [1] is representing state as a difference between two screens. Right now I'm not doing this, and my replay memory implementation is not helping learning - quite the opposite. Of course, a good RL algorithm is independent from the representation of state and the error is probably somewhere else - especially since my Q-learning algorithm does learn if I avoid using replay memory - but this tickled my curiosity. I skimmed over the DQN paper but did not find any references to such a representation. I'll admit that I haven't read it in detail yet because I don't want to get more confused, so I might have missed it. Is there a specific reason for this kind of representation? Does it only make sense in the context of translating an image to state? Thanks in advance! [0] https://doi.org/10.1038%2Fnature14236 [1] https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html
