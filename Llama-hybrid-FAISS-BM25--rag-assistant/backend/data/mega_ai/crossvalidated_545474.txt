[site]: crossvalidated
[post_id]: 545474
[parent_id]: 534618
[tags]: 
Note that the weights in word embeddings get initialized with zero mean and unit variance. Also, the embeddings get multiplied by sqrt(d) before being added to the positional encoding. The positional encoding is also on the same scale as the embeddings. My hypothesis is that the authors tried rescaling the embeddings by various numbers (as they certainly did with attention), and this particular rescaling happened to work, because it made the embeddings much bigger than the positional encodings (initially). The positional encodings are necessary, but they probably shouldn't be as "loud" as the words themselves.
