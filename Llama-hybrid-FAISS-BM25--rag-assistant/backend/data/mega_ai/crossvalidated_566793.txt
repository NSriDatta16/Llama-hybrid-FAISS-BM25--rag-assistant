[site]: crossvalidated
[post_id]: 566793
[parent_id]: 566696
[tags]: 
No, reducing dimensionality with PCA will only maximize variance, which may or may not translate to linear separability. Here are two visualizations of variance and separability in opposition. In both cases, the discriminative information lies primarily along the low-variance axis, which would get discarded by rote dimensionality reduction. Minimal example of three data points (adapted from whuber's comment ) Example of parallel, elongated Gaussian clusters (adapted from Dikran's answer ) import numpy as np from sklearn.decomposition import PCA # generate gaussian samples (n by 2) rng = np.random.default_rng(4) n = 100 X = rng.standard_normal((n, 2)) @ rng.random((2, 2)) # peel classes apart (+1 and -1 vertically) y = 2 * np.arange(X.shape[0]) // n X[:, 1] = np.where(y, X[:, 1] - 1, X[:, 1] + 1) # project X mu = X.mean(axis=0) X -= mu pca = PCA(n_components=2).fit(X) # explained variance ratios: [0.83267962, 0.16732038] X_pca = pca.transform(X) # reduce dimensionality via PC1 X_pc1 = X_pca[:, [0]] @ pca.components_[[0], :] + mu # reduce dimensionality via PC2 X_pc2 = X_pca[:, [1]] @ pca.components_[[1], :] + mu
