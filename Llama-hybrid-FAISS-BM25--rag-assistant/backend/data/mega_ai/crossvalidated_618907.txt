[site]: crossvalidated
[post_id]: 618907
[parent_id]: 
[tags]: 
Should the data be scaled before normalisation to enable the use of the model as a pre-trained model?

I want to implement a neural network in Pytorch for medical image segmentation. I should normalise my data. Should I apply a min-max scale (range 0 to 1) before applying the normalisation or should I apply the z-score normalisation directly? What is the best approach if the dataset comes from a single source of data? What if the samples come from multiple sources? Imagine I trained a model with some data X. Now I want to use that model as a pre-trained model. I need to normalise my inference data Y before feeding it to the model. However, I can't apply a z-score normalisation in Y data with the training data (X) statistics because the inference data range is different. Y range from [0-1000] and the X ranges from [0-255]. It does not make any sense to normalise Y data with the training data statistics. Therefore, before training the model with the data ranging from [0-255] (X) should I apply a min-max scaling so that the data is now in range [0-1]? Therefore, now I can use the pre-trained model with Y. I just need to scale Y to range [0-1] and then normalise it with X statistics. Is this correct?
