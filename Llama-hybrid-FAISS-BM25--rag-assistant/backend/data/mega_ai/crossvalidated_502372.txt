[site]: crossvalidated
[post_id]: 502372
[parent_id]: 
[tags]: 
How does the python port of libsvm's predict_proba work?

I've followed through the original libsvm code on it's [github][1]. I'm not concerned about the theoretical backfground of how the probability estimates are derived. All that I care about is how to predict the probabilities from the trained model. I know that there's the method svm.SVC.predict_proba but it seems that it doesn't work exactly as the original function in libsvm called svm_predict_probability . For a two-class classification problem, the latter one computes the decision values in a first step, which are then passed to a function called sigmoid_predict: static double sigmoid_predict(double decision_value, double A, double B) { double fApB = decision_value*A+B; // 1-p used later; avoid catastrophic cancellation if (fApB >= 0) return exp(-fApB)/(1.0+exp(-fApB)); else return 1.0/(1+exp(fApB)) ; } I've tried to replicate these 2 steps in python. Here's a minimal working example: import numpy as np from sklearn import svm X_train = np.array([[ 22. , 7.25 , 2. , 1. , 1. , 0. ], [ 38. , 71.2833, 0. , 0. , 1. , 0. ], [ 26. , 7.925 , 2. , 0. , 0. , 0. ], [ 35. , 53.1 , 0. , 0. , 1. , 0. ], [ 35. , 8.05 , 2. , 1. , 0. , 0. ], [ 27. , 8.4583, 2. , 1. , 0. , 0. ], [ 54. , 51.8625, 0. , 1. , 0. , 0. ], [ 2. , 21.075 , 2. , 1. , 3. , 1. ], [ 27. , 11.1333, 2. , 0. , 0. , 2. ], [ 14. , 30.0708, 1. , 0. , 1. , 0. ], [ 4. , 16.7 , 2. , 0. , 1. , 1. ], [ 58. , 26.55 , 0. , 0. , 0. , 0. ], [ 20. , 8.05 , 2. , 1. , 0. , 0. ], [ 39. , 31.275 , 2. , 1. , 1. , 5. ], [ 14. , 7.8542, 2. , 0. , 0. , 0. ], [ 55. , 16. , 1. , 0. , 0. , 0. ], [ 2. , 29.125 , 2. , 1. , 4. , 1. ], [ 23. , 13. , 1. , 1. , 0. , 0. ], [ 31. , 18. , 2. , 0. , 1. , 0. ], [ 22. , 7.225 , 2. , 0. , 0. , 0. ], [ 35. , 26. , 1. , 1. , 0. , 0. ], [ 34. , 13. , 1. , 1. , 0. , 0. ], [ 15. , 8.0292, 2. , 0. , 0. , 0. ], [ 28. , 35.5 , 0. , 1. , 0. , 0. ], [ 8. , 21.075 , 2. , 0. , 3. , 1. ], [ 38. , 31.3875, 2. , 0. , 1. , 5. ], [ 26. , 7.225 , 2. , 1. , 0. , 0. ], [ 19. , 263. , 0. , 1. , 3. , 2. ], [ 24. , 7.8792, 2. , 0. , 0. , 0. ], [ 23. , 7.8958, 2. , 1. , 0. , 0. ]]) y_train = np.array([0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0.]) X_test = np.array([40. , 27.7208, 0. , 1. , 0. , 0. ]).reshape(1,-1) clf = svm.SVC(gamma='scale', C=30,probability=True, random_state=0, kernel='poly',decision_function_shape='ovo') clf.fit(X_train, y_train) clf.predict_proba(X_test) The last command gives me array([[0.48383569, 0.51616431]]) Naively I would expect the following to give me the same result: def sigmoid_predict(dec_val, A,B): fApB = dec_val*A+B if fApB >=0: return np.exp(-fApB)/(1.0+np.exp(-fApB)) else: return 1.0/(1.0+np.exp(fApB)) sigmoid_predict(clf.decision_function(X_test), clf.probA_,clf.probB_) However, in this case the result is array([0.48625056]) which is different from the above one. Im not talking about the fact, that this a 1x1 array instead of a 2x1-arrays. That's fine. But the values don't match. In this specific example its just a minor difference but for other test-data it becomes quite large. So this is definitely not due to some roundoff error. What is happening? Are there some steps I'm missing? I've gone through all the C-Code and couldn't figure out what's missing there. Are there some additional steps in the python port of libsvm, maybe some kind of internal scaling? Any advice? EDIT: I just found that clf._dense_decision_function(X_test) = -clf.decision_function(X_test) and sigmoid_predict(clf._dense_decision_function(X_test), clf.probA_,clf.probB_) gives very similar results to clf.predict_proba(...) (I suppose the slight difference may be attributed to a conversion from float to double when using clf.predict_proba(...)) Still I have no clue why... But it's good enough for now.. [1]: https://github.com/cjlin1/libsvm/blob/master/svm.cpp
