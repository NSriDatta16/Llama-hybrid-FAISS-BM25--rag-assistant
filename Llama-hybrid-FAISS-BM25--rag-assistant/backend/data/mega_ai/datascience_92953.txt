[site]: datascience
[post_id]: 92953
[parent_id]: 
[tags]: 
Does multi-head attention remove the need for self-attention?

The title may be confusing but suppose I were to build Transformer Neural Network with a masking network that utilizes multi-head attention (like that in SepFormer ), would adding self-attention in the encoder and decoder still be necessary?
