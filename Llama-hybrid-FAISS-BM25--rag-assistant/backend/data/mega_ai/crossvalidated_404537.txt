[site]: crossvalidated
[post_id]: 404537
[parent_id]: 83216
[tags]: 
Is overfitting bad when we have really a lot of data? Overfitting with lot of data is still overfitting and overfitting is bad. I don't understand why "very large weights fit the training data very well"? I found an example in Deep Learning by Goodfellow(page 293): Suppose we apply logistic regression to a problem where the classes are linearly separable. If a set of weights $w$ make the model fit the data very well, then it's clear that $2w$ would provide us higher likelihood. And in theory after many iterations of opotimization this increasing would never hald. Is regularization always needed? This question seems equal to: does overfitting always occur? If there exists data the model has never seen during training, the overfitting may occur and hence regularization is necessary. It seems that hardly can we train the model with all possible cases, then normally regularization is always necessary.
