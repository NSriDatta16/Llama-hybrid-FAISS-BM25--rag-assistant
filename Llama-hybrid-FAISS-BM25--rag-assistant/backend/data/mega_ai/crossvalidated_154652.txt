[site]: crossvalidated
[post_id]: 154652
[parent_id]: 
[tags]: 
How does the back-propagation work in a siamese neural network?

I have been studying the architecture of the siamese neural network introduced by Yann LeCun and his colleagues in 1994 for the recognition of signatures ( “Signature verification using a siamese time delay neural network” .pdf , NIPS 1994) I understood the general idea of this architecture, but I really cannot understand how the backpropagation works in this case. I cannot understand what are the target values of the neural network, that will allow backpropagation to properly set the weights of each neuron. In this architecture, the algorithm computes the cosine similarity between the final representations of the two neural networks The paper states: "The desired output is for a small angle between the outputs of the two subnetworks (f1 and f2) when to genuine signatures are presented, and a large angle if one of the signatures is a forgery". I cannot really understand how they could use a binary function (cosine similarity between two vectors) as target to run the backpropagation. How is the backpropagation computed in the siamese neural networks?
