[site]: crossvalidated
[post_id]: 541172
[parent_id]: 541165
[tags]: 
There's no such a thing as a loss function "for" a particular kind of model. You could be using nonlinear regression with different loss functions. There are many loss functions and you can even construct one yourself. The choice depends on the nature of your problem and the data you are dealing with. Recall that minimizing some loss is equivalent to maximizing a likelihood function (e.g. using squared error is an equivalent of assuming Gaussian likelihood function), so it is tightly connected to the assumptions you are making about the distribution of errors. More formally, if you think of the model as of something like $$ y = f(X) + \varepsilon $$ then the choice of model (e.g. linear regression, nonlinear regression, deep neural network, etc) is related to estimating the expectation $E[y] = f(X)$ , while the choice of the loss function impacts how do you treat the residuals $y - f(X) = \varepsilon$ . For example, choosing squared error over absolute error penalizes outliers more, so it would be preferable if this is what you want to achieve. On another hand, absolute error is less prone to outliers, this can be an advantage in another scenario. The most common choice is defaulting to squared error , though it is somehow an arbitrary choice and doesn't have to be the best in all cases.
