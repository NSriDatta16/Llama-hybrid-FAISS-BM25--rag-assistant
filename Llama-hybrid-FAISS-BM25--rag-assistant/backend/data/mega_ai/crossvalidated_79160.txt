[site]: crossvalidated
[post_id]: 79160
[parent_id]: 78747
[tags]: 
Your understanding, while completely legitimate, is fairly "old school". For example, early stopping was used to prevent overfitting before weight decay, which turned out to be a more elegant and effective technique. Also, monitoring the error rate with a single, separate validation set has been supplanted (IMO) by model tuning via resampling (e.g. cross-validation or the bootstrap). That said, there are a lot of different philosophies and opinions about how things should be done. I don't think that you need a validation set and a test set. I would use resampling (as you are) to see how the model is doing on the training set and then, if you think that you've finalized the model, verify the results on the test set. If you are modeling a continuous outcome, you might think about using a linear function between the hidden layer and the outcome (e.g. the nnet option linout ). Also, you will get repeatable results if you fix the random number seed before the call to train (see ?set.seed ). Max
