[site]: crossvalidated
[post_id]: 310839
[parent_id]: 310814
[tags]: 
Their notation is confusing. For instance, they use the hat symbol to denote the normalized variable $\hat x=x-E[x]$, then even worse they use the expectation symbol to denote what is clearly a sample average estimator: $E[x]=\frac 1 N\sum_{i=1}^Nx_i$ I assume that when you wrote $E[b]=b$, you mean the expectation of bias. That's not what they would denote by $E[b]$: they'd mean a simple sample average estimator, the quantity that is usually denoted as $\bar b=\frac 1 N\sum_jb_j$, where $b_j$ is the bias learned from a batch $j$. All they're saying in this paragraph is that you have to be mindful of how you implement normalization, because if you do it wrong then it will interfere with the gradient descent. Their example is first learning the bias, then normalizing. So, you learn the bias $\Delta b$ from a batch, but then when you normalize subsequently you negate the learning by subtracting what you learned. It goes like this. First you learned $\Delta b$, which means you are prepared to output $u+b+\Delta b$. However, you squeezed another operation just before outputting from the layer: you decided to normalize. So, you subtract the sample average, i.e. what they denote unfortunately by $E[u+b+\Delta b]$. This will lead to $E[\Delta b]$ cancelling each other in gradient descent and normalization, i.e.e you didn't learn anything.
