[site]: crossvalidated
[post_id]: 20310
[parent_id]: 20295
[tags]: 
The error of an estimator is a combination of (squared) bias and variance components . However in practice we want to fit a model to a particular finite sample of data and we want to minimise the total error of the estimator evaluated on the particular sample of data we actually have , rather than a zero error on average over some population of samples (that we don't have). Thus we want to reduce both the bias and variance, to minimise the error, which often means sacrificing unbiasedness to make a greater reduction in the variance component. This is especially true when dealing with small datasets, where the variance is likely to be high. I think the difference in focus depends on whether one is interested in the properties of a procedure, or getting the best results on a particular sample. Frequentists typically find the former easier to deal with within that framework; Bayesians are often more focussed on the latter.
