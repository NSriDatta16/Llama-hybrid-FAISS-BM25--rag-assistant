rasure, that increased public awareness of racial bias in automatic speech recognition (ASR) systems. The piece was performed by numerous female and non-binary researchers in the field, including Ruha Benjamin, Sasha Costanza-Chock, Safiya Noble, and Kimberlé Crenshaw. AJL based their development of "Voicing Erasure" on a 2020 PNAS paper, titled, "Racial disparities in automated speech recognition" that identified racial disparities in performance of five commercial ASR systems. Algorithmic governance In 2019, Buolamwini represented AJL at a congressional hearing of the US House Committee on Science, Space, and Technology, to discuss the applications of facial recognition technologies commercially and in the government. Buolamwini served as a witness at the hearing and spoke on underperformance of facial recognition technologies in identifying people with darker skin and feminine features and supported her position with research from the AJL project "Gender Shades". In January 2022, the AJL collaborated with Fight for the Future and the Electronic Privacy Information Center to release an online petition called DumpID.me, calling for the IRS to halt their use of ID.me, a facial recognition technology they were using on users when they log in. The AJL and other organizations sent letters to legislators and requested them to encourage the IRS to stop the program. In February 2022, the IRS agreed to halt the program and stop using facial recognition technology. AJL has now shifted efforts to convince other government agencies to stop using facial recognition technology; as of March 2022, the DumpID.me petition has pivoted to stop the use of ID.me in all government agencies. Olay Decode the Bias campaign In September 2021, Olay collaborated with AJL and O'Neil Risk Consulting & Algorithmic Auditing (ORCAA) to conduct the Decode the Bias campaign, which included an audit that explored whether the Olay Skin Advisor (OSA) System included bias against women of color. The AJL chose to collaborate with Olay due to Olay's commitment to obtaining customer consent for their selfies and skin data to be used in this audit. The AJL and ORCAA audit revealed that the OSA system contained bias in its performance across participants' skin color and age. The OSA system demonstrated higher accuracy for participants with lighter skin tones, per the Fitzpatrick Skin Type and individual typology angle skin classification scales. The OSA system also demonstrated higher accuracy for participants aged 30–39. Olay has, since, taken steps to internally audit and mitigate against the bias of the OSA system. Olay has also funded 1,000 girls to attend the Black Girls Code camp, to encourage African-American girls to pursue STEM careers. CRASH project In July 2020, the Community Reporting of Algorithmic System Harms (CRASH) Project was launched by AJL. This project began in 2019 when Buolamwini and digital security researcher Camille François met at the Bellagio Center Residency Program, hosted by The Rockefeller Foundation. Since then, the project has also been co-led by MIT professor and AJL research director Sasha Costanza-Chock. The CRASH project focused on creating the framework for the development of bug-bounty programs (BBPs) that would incentivize individuals to uncover and report instances of algorithmic bias in AI technologies. After conducting interviews with BBP participants and a case study of Twitter's BBP program, AJL researchers developed and proposed a conceptual framework for designing BBP programs that compensate and encourage individuals to locate and disclose the existence of bias in AI systems. AJL intends for the CRASH framework to give individuals the ability to report algorithmic harms and stimulate change in AI technologies deployed by companies, especially individuals who have traditionally been excluded from the design of these AI technologies [20, DataSociety report]. Support and media appearances AJL initiatives have been funded by