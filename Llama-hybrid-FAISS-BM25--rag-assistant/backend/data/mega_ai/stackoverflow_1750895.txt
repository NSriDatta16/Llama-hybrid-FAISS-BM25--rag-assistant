[site]: stackoverflow
[post_id]: 1750895
[parent_id]: 1750343
[tags]: 
Someone hinted at a possible way to index this thing if you have abundant RAM (or possibly even disk/swap) available. Imagine if you performed a simple 32-bit CRC on a 1 K block extending from each character in the original Gig string. This would result in 4 bytes of checksum data for each byte offset from the beginning of the data. By itself this might give a modest improvement in search speed. The checksum of each 1 K search target could be checked against each CRC ... which each collision tested for a true match. That should still be a couple orders of magnitude faster than a normal linear search. That, obviously, costs us 4 GB of RAM for the CRC array (plus the original Gig for the original data and a little more overhead for the environment and our program). If we have ~16 GB we could sort the checksums and store a list of offsets where each is found. That becomes an indexed search (average of about 16 probes per target search ... worst case around 32 or 33 (might be a fence post there). It's possible that a 16 GB file index would still give better performance than a linear checksum search and it would almost certainly be better than a linear raw search (unless you have extremely slow filesystems/storage). (Adding): I should clarify that this strategy is only beneficial given that you've described a need to do many searches on the same one gigabyte data blob. You might use a threaded approach to building the index (while reading it as well as having multiple threads performing the checksumming). You might also offload the indexing into separate processes or a cluster of nodes (particularly if you use a file-based index --- the ~16 GB option described above). With a simple 32-bit CRC you might be able to perform the checksums/indexing as fast as your reader thread can get the data (but we are talking about 1024 checksums for each 1 K of data so perhaps not). You might further improve performance by coding a Python module in C for actually performing the search ... and/or possibly for performing the checksumming/indexing. The development and testing of such C extensions entail other trade-offs, obviously enough. It sounds like this would have near zero re-usability.
