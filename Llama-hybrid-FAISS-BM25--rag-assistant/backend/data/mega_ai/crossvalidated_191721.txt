[site]: crossvalidated
[post_id]: 191721
[parent_id]: 191686
[tags]: 
There is no standard variable selection method for random forests (RF). The absolute variable importance values have no meaning, but their relative sizes can be useful to comparing different predictors. Deciding how many variables to include can be a little subjective, so many authors have proposed several variable selection algorithms. A few articles are given below: For microarray data, Diaz-Uriarte and Alvarez de Andres [1] suggest reiteratively fitting RFs discarding 20% of variables with the smallest variable importance and choosing the variables that give the smallest out-of-bag (OOB) error rate. Genuer et al. [2] recommend a preliminary elimination that removes variables whose importance is below the minimum prediction value given by a CART model. After a preliminary elimination, a nested collection of RF models or a sequence of RF models is used to select the variables (see paper). Ishwaran et al. [3] propose a new metric called minimal depth which can select variables since the exact distribution is known. The three papers aforementioned have R packages called varSelRF , VSURF , and randomForestSRC , respectively. These articles are a small subset of the literature addressing variable selection using RFs. As a side note, I believe the blog does not use the standard approach to calculate permuted variable importance. I do not know Python that well, but it seems the code permutes each variable in the training sample and compares the permuted and non-permuted prediction error from the random forest. The standard approach is to permute the variables in the OOB sample and compare the permuted and non-permuted prediction error in each tree. The final permuted variable importance is the average difference in prediction error. I personally would suggest using R as there are more tools already available for variable selection using RFs [1] R. Diaz-Uriarte and S. Alvarez de Andres (2006) Gene selection and classification of microarray data using random forest. BMC Bioinformatics [2] R. Geneur, J.-M. Poggi, C. Tuleau-Malot (2010) Variable selection using Random Forests. Pattern Recognition Letters [3] H. Ishwaran, U.B. Kogalur, E.Z. Gorodeski, A.J. Minn, M.S. Lauer (2010) High-dimensional variable selection for survival data. Journal of the American Statistical Association
