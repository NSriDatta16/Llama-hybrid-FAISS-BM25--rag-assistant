[site]: crossvalidated
[post_id]: 184196
[parent_id]: 184192
[tags]: 
If you think your empirical prior (class distribution of training set) is unreasonable during future prediction you can adopt a different prior e.g. a uniform prior (equal intrinsic probability of either class). Yes, you can use classwt(weighting samples by class) but strata(downsampling prevalent class) yields same performance and is better to control the effect of: rf = randomForest(...,#all your other pars inserted as usually strata=train.np.log$targamnt #insert your training targets(as factor here) sampsize = c(344,344) #down sampling for each tree bootstrap replace = TRUE #if set to FALSE sampsize should be c(211,211) ) don't bother colliniarity, as RF have no problems with that. Perhaps try dropping a fraction of the features with lowest permutation importance. don't bother skewed features. Decision tree splits are non-parametric and only mind the ranking(ordinal scale). All your transformations are monotonic, thus the ranking of feature values are unchanged and you will get the exact same model result (when using set.seed(123)) Use a better performance metric than class.error and confusion matrix such ROC plots Decide what trade-of between sensitivity and specificity works the best for your case here's a link to full code solution in R with randomForest and explanation on cross validated You don't need to switch to adaboost, as it is likely inferrior to RF. RF may be inferrior to xgboost, gbm or svm. But basicly, you can handle skewed data with all packages and models.
