[site]: crossvalidated
[post_id]: 582516
[parent_id]: 198762
[tags]: 
This is a really good question and there can be multiple explanations to this phenomenon. I will try to give out a non-exhaustive list for the same. Sparse data ? You mean sparse/binary feature set ? hmmm...this can happen. See in that case your network does not require much parameters to encode this function. Every neural network is a function that encodes the feature set to the set of labels (one hot or whatever !). Now lets talk about regularization because that has soooo much to do about the weight distribution of a network. You are not using dropout. Try using it. It is a great regularization technique which can actually help you to use all of your neurons. Without dropout on Sparse data, this is an expected event. Try using L2, it might have some effect to your weight distribution. You are using batchnorm and still getting this ? Hmm...I dont have an answer right now but if this post generates enough attention I will do a study on dropout vs batchnorm...!!
