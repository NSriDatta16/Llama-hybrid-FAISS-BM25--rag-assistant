[site]: datascience
[post_id]: 95190
[parent_id]: 95177
[tags]: 
First, when working with big data most of the time it's more convenient to work with a random subset rather than the whole thing: usually during the design and testing stages there is no need to work with the full data since optimal performance is not needed. Second, it's often useful to do an ablation study in order to check that using the full data is actually useful for the model. Sometimes training the model with a subset gives the same results as with the full available data, so in this case there's no advantage using all the data. Finally there are indeed cases where one needs to process a large dataset or run a long training process which cannot be done on a regular computer. There are various options depending on the environment: Buy the required hardware (it's rarely the best option but it needs to be mentioned) Use a commercial cloud service such as AWS Some organizations have their own in-house computing servers/clusters. In particular if you're a student it's likely that you should have access to this kind of service through your university, ask around (afaik most decent universities provide it nowadays).
