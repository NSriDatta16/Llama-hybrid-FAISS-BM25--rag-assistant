[site]: datascience
[post_id]: 126211
[parent_id]: 126189
[tags]: 
Hamiltonian Monte Carlo (HMC) comes up mostly in Bayesian statistics. It belongs to a general class of sampling algorithms called Markov Chain Monte Carlo (MCMC). I think there is some confusion about what the purpose of HMC/MCMC is. Some brief, non-rigorous background: The goal of MCMC in the case of Bayesian inference is to generate random samples from the posterior distribution, which we typically cannot derive in closed form. All MCMC algorithms work by iteratively computing a ratio of probabilities that help determine if the sampler should "stay where it is at currently" (and "record" another sample where it is currently at) or "to move" to a different location of the posterior. Where the algorithm proposes to "move" can be determined by a variety of ways, one of which is HMC. With the context above, I think it's not a fair comparison to ask whether HMC or any class of MCMC algorithm is "better than pseudo-random number generation" because the goal is to generate samples from a very specific distribution that we otherwise can't. If we could avoid MCMC altogether (say, if we know the posterior has a closed form like in the case of conjugate priors ), then we would because MCMC is expensive. We could do this by sampling from this known distribution using "pseudo random number generation" (i.e. through the inverse transform probably) as you suggest, but again this is rarely the case. In terms of MCMC being "not random sampling", when everything goes well computationally then MCMC generates a sample that is quite close to being a random sample from the posterior distribution. It is not random in the sense, however, that we are just generating random numbers uniformly; the values it generates are realizations from the posterior as described above.
