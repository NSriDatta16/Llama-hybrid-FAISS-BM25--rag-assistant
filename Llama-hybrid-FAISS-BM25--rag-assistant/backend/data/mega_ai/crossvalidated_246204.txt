[site]: crossvalidated
[post_id]: 246204
[parent_id]: 246193
[tags]: 
Feature selection is just deciding which variable to include in your model. In case of CART (and most Machine Learning methods) feature selection is done by the model itself. So how to do feature selection? Just run the algorithm and let the Gini Index or Entropy decide which variable is useful to include in the tree. Here is your feature selection. Make a plot of the tree to find the included variables. In other models (read non Machine Learning methods) you must do 1) feature selection yourself or 2) decide on an algorithm to do the feature selection for you. An example of 1) is using common sense or a professional opinion to think of features which probably explain the variable you want to explain. Examples of 2) are: stepwize regression based on information criteria or estimate the model using the LASSO method. ps what utobi meant in his comment (I think): CART does the feature selection for you, so the more variables/features you discard before running the CART algorithm, the higher the chances that you discard an important variable. This leads to a loss in performance. For CART it does not matter if you use 10 or 1000 features, it searches for the best of those 10 or 1000 features (however computation time can be an issue with many features).
