[site]: crossvalidated
[post_id]: 415386
[parent_id]: 
[tags]: 
Reinforcement Learning - Why are actor critic methods biased?

I understand that using a state-dependent baseline keeps the expected reward objective unbiased (whilst reducing the gradient variance) like in the equation below: $$ \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \sum _ { t = 1 } ^ { T } \nabla _ { \theta } \log \pi _ { \theta } ( a _ { i , t } | s _ { i , t } ) ( ( \sum _ { t ^ { \prime } = t } ^ { T } \gamma ^ { t ^ { \prime } - t } r ( s _ { i , t ^ { \prime } } , a _ { i , t ^ { \prime } } ) ) - \hat { V } _ { \phi } ^ { \pi } ( s _ { i , t } ) ) $$ However the actor-critic gradient (below) looks very similar i.e. it looks like it has the usual policy gradient with a state dependent baseline of $$ \gamma \hat { V } _ { \phi } ^ { \pi } ( s _ { i , t + 1 } ) - \hat { V } _ { \phi } ^ { \pi } ( s _ { i , t } ) $$ $$ \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \sum _ { t = 1 } ^ { T } \nabla _ { \theta } \log \pi _ { \theta } ( a _ { i , t } | s _ { i , t } ) ( r ( s _ { i , t } , a _ { i , t } ) + \gamma \hat { V } _ { \phi } ^ { \pi } ( s _ { i , t + 1 } ) - \hat { V } _ { \phi } ^ { \pi } ( s _ { i , t } ) ) $$ So my question is : why is the actor-critic algorithm biased given that it can be interpreted as a policy gradient objective with a state-dependent baseline much like the first equation.
