[site]: crossvalidated
[post_id]: 507908
[parent_id]: 
[tags]: 
Are standard VAEs enough for most representation learning tasks?

I've tried VAEs a couple of times in both my applied research and on stock datasets such as MNIST, and it seems like for MNIST it works fairly well with minor hyperparameter tuning (for example architecture, beta in beta-VAEs) but for applied work it seems problems of posterior collapse/degeneracy spring up very quickly and can be pretty difficult to troubleshoot. Is there a sense in the research community that the original standard VAE (i.e. one that relies on a prior of $N(0,1)$ ) has limited uses and will be eventually supplanted completely by more flexible models such as ones that use the VampPrior?
