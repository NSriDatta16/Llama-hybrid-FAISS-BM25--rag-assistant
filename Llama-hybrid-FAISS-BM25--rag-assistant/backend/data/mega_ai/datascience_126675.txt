[site]: datascience
[post_id]: 126675
[parent_id]: 
[tags]: 
Bulding Deep Learning model for multiclassification case

I am soo confused i read a lot of information in forumas and still cna't get what is wrong. my data is around 500.000 rows and 32 columns. my target variables consists of 3 classes (0, 1, 2). Hyperopt supposed to get best parameters that fit my data but no luck :/ # Define the objective function to minimize (in this case, negative accuracy) def objective(params): model = Sequential() # Add input layer model.add(Dense(params['neurons_1'], input_dim=X_train.shape[1], activation=params['activation'], kernel_regularizer=l2(params['l2']))) # Add hidden layers based on the optimized value for i in range(2, int(params['hidden_layers']) + 1): model.add(Dense(params[f'neurons_{i}'], activation=params['activation'], kernel_regularizer=l2(params['l2']))) # Add output layer model.add(Dense(3, activation='softmax')) # Adjust the output size based on your multiclass problem model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # Define EarlyStopping callback early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True) history = model.fit(X_train, y_train, epochs=60, batch_size=params['batch_size'], validation_data=(X_val, y_val), class_weight=dict(enumerate(class_weights)), verbose=0, callbacks=[early_stopping]) val_acc = history.history['val_accuracy'][-1] return {'loss': -val_acc, 'status': STATUS_OK} # Define the hyperparameter search space space = {'hidden_layers': hp.quniform('hidden_layers', 1, 6, 1), # Adjust the range as needed 'learning_rate': hp.loguniform('learning_rate', np.log(1e-5), np.log(1e-3)), 'l2': hp.loguniform('l2', np.log(1e-8), np.log(1e-3)), # Use a default range 'activation': hp.choice('activation', ['relu', 'tanh']), # Add 'tanh' activation 'batch_size': hp.choice('batch_size', [512, 1024, 2048, 4096]) # Add batch size options } # Add neurons for input layer and each hidden layer for i in range(1, 7): #%%% Define with +1 because 1st layer is input one space[f'neurons_{i}'] = hp.choice(f'neurons_{i}', [8, 12, 24, 16, 32, 48, 64, 72, 96, 128]) # Use hyperopt to find the best hyperparameters trials = Trials() best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials)
