[site]: datascience
[post_id]: 43969
[parent_id]: 43963
[tags]: 
Bagging main goal is to minimize variance of your model. Basically, if you have a model that is on average pretty accurate but inconsistent (meaning, it does well for a given data set, poorly generalizations) then bagging may be a way to produce a more consistent estimators. Decision trees are the common example of this because they are the canonical high variance machine learning algorithm. As for your last question, the size of each new training set needs to be the same size as the original training set. The way you achieve this is by random sampling of the original dataset with replacement (meaning the new dataset may have duplicates). The number of new training sets is dependent on the problem. Sometimes 100 is fine other times you need 1000 or so. There isn't a way to just know how many sets you need. It is a parameter that needs to be tuned.
