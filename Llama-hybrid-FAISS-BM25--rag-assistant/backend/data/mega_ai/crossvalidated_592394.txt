[site]: crossvalidated
[post_id]: 592394
[parent_id]: 541498
[tags]: 
Transformer, becuase it uses the attention mechanism with softmax transformation after that using the feedforward with nonlinear transformation. In short it uses different transformations(activation functions) to transform the input from intial representation into final representation if we would explain that in very simple words.
