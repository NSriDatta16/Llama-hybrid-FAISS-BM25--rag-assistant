[site]: datascience
[post_id]: 5508
[parent_id]: 736
[tags]: 
Regarding the approach, SVM with an RBF kernel does a good job, but SVMs can be slowed down by large object sizes, unless you are employing CV with e.g. one tenth of the data randomly assigned to each fold. However, did you ask yourself why you are employing SVMs in the first place? Have you tried multivariate linear regression, $\mathbf{Y}=\mathbf{X}\boldsymbol{\beta}$, where each record of $\mathbf{Y}$ is coded $y_{ij}=+1$ if the $i$th object is in class $j$, and $y_{ij}=-1$ otherwise? If the classification accuracy is appreciably high using linear regression, then your data are linearly separable, and more complex methods such as SVMs and ANNs aren't needed. Step 2 would be to show that k-nearest neighbor, naive Bayes, linear (Fisher) discriminant analysis, polytomous logistic regression, etc., break down and fail. For terminology, you might couch the issue of having more class weights in the context of "lower proportions of objects in certain classes," or "near-zero class size." Skew tends to be used for describing the distribution of a feature's values, as in skewness, fat tails, etc. How many features do you have? Did you try unsupervised clustering (class discovery) on the 100,000 objects before trying supervised classification (class prediction) with SVM? Maybe the 100,000 objects can be grouped into fewer classes than 50, for which the new class membership could be used as the target class during classification analysis. This may alleviate the problem of having near-zero class size.
