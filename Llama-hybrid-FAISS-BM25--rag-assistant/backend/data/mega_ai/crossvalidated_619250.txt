[site]: crossvalidated
[post_id]: 619250
[parent_id]: 
[tags]: 
Can all MDPs be translated into MPs?

Say we have a Markov Decison Process (MDP) with countably infinite state space $S$ , continuous action space $A$ , well-defined transition matrix $T_a(s,s') = P(s'\mid s,a)$ and some stochastic policy $\pi(a\mid s) \in [0,1]$ . Question : If we observe an agent following policy $\pi$ in this system long enough, are the mean transition probabilities guaranteed to converge? I.e., can we assume an MDP with a fixed stochastic policy to behave like a Markov Process (MP) in the long term? Formalised : Does $\mathbb{E}_{a\sim\pi}[T_a(s,s')]$ exist for all $s,s' \in S$ and stochastic policies $\pi: S \times A \to [0,1]$ ? This is trivially true for finite action sets $|A| = |\{a_i\}| = N$ because $\forall s,s' \in S$ , $\mathbb{E}_{a\sim\pi}[T_a(s,s')] = \sum_{i=1}^{N}\pi(a_i\mid s)T_{a_i}(s,s') \in \mathbb{R}$ . Convergence as $N \to \infty$ is also probably provable(?). However, what about continuous action spaces? That is, is it true that $\mathbb{E}_{a\sim\pi}[T_a(s,s')] = \int_{A}\pi(a\mid s)T_{a}(s,s')da \in \mathbb{R}?$ I suppose an intuitive counterexample to this would be the Cauchy distribution which does not have a mean. So, if our agent would follow a stochastic policy where actions are Cauchy distributed, this expected value would not exist? What about deterministic policies $\pi(s) \in A$ ?
