[site]: crossvalidated
[post_id]: 579853
[parent_id]: 
[tags]: 
Bayesian inference on sequential data

I have a dataset that is potentially contaminated. The variable of interest is continuous (non-Gaussian but for simplicity sake let's just assume that it is). Let's say our prior is that the true mean should be 50% of the in-sample mean but we don't know for sure. As we observe more out-of-sample observations, it either confirms or rejects our prior assumption. For example, 1 out-of-sample observation isn't enough evidence to move away from the prior (in-sample x 50%) but let's say 50 out-of-sample observations are plenty to give a reasonable assessment as to whether the prior is right or wrong. I'm interested in the true out-of-sample mean. What's tricky about this problem (at least to me) is the need for sequential update. What would be a good approach to this problem? I've already found this question that doesn't really have an answer. [Edit] One option I've thought about is to assume Gaussian likelihood and prior. The assumed prior is the in-sample mean and precision, but the mean is adjusted by the assumed overstatement. Let prior be $N(\mu_0, \pi_0)$ , where $\pi = 1/\sigma^2$ is precision. The posterior is: $$ \mu_{post}=\frac{\pi_0\mu_0}{\pi_0+\pi}+\frac{\pi x}{\pi_0+\pi} \\ \pi_{post}=\pi_0+\pi $$ I don't know what the precision of each new observation is but I thought a reasonable assumption is to just assume it has the same precision as the in-sample data. This means that the first update my precision doubled, while mean is the average between my prior and the first observation. Then assume my posterior is my new prior for the next update. However, this seems to be adjusting too aggressively to each new observation, particularly at the beginning when I have only a few out-of-sample observations. What I'm expecting to see if at the beginning, my posterior is barely different from my prior, but adjusting closer to the observed out-of-sample data over time.
