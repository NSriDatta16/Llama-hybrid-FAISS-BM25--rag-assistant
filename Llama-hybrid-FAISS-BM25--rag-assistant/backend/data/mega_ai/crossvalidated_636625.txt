[site]: crossvalidated
[post_id]: 636625
[parent_id]: 
[tags]: 
Why doesn't BERT give me back my original sentence?

I've started playing with BERT encoder through the huggingface module. I passed it a normal unmasked sentence and got the following results: However, when I try to manually apply the softmax and decode the output: I get back a bunch of unexpected tensor(1012) instead of my original sentence. BERT is an autoencoder, no? Shouldn't it be giving me back the original sentence with fairly high probability since none of the input words was [MASK] ? Can anyone explain to me what is going on?
