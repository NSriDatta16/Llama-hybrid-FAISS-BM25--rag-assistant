[site]: crossvalidated
[post_id]: 189259
[parent_id]: 189137
[tags]: 
related: What Machine Learning Algorithms are Good for Estimating Which Features are more Important? The problem you posed can be described in a few different ways: I would call this a feature selection or variable importance problem. Alternatively (and perhaps more appropriately, as described in Sophologist's answer) we can constrain this as a prediction problem. Either way, the short answer is that there are multiple different ways to get the importance of different predictors, and that more importantly there are two bigger questions here: how much does a variable (or set of variables) contribute to a particular model's performance, and how relevant is a variable in general? The second question is tricky and I'm not sure there's an easy answer to that. So that being said, and taking inspiration from Sophologist's answer, it would be most straightforward to build models with your feature vectors to figure out how the different features interact with each other to affect the predictions of those models. Without knowing more about the type of features in your data, what your data looks like, and any other goals you might have with the data, it's tough to suggest any one method over another. Like above, and like others have mentioned, a relatively intuitive thing to start with would be to find how all the features interact perform for some model (say, logistic regression), and then inspect the subsets of the features in subsequent iterations. There are a variety of heuristics that you can do to search the subset space for the best subsets. There's some info on the wiki page here . It may actually be the case that removing "less important" features hinders the overall performance of your model! See (this question)[ Should covariates that are not statistically significant be 'kept in' when creating a model? for more clarification on that. As for what part of the data to use, no matter which method you use it would behoove you to consider both the positive and negative data. It should also be stressed that you may not get one universal ranking of variable importance/feature 'strength' across these methods, as many of them are tied to how important that variable is in a specific context (e.g., how it impacts the performance of a random forest). See this blog post for more clarification on these different metrics, and a post here about some of the R feature selection methods with some examples.
