ence dataset and bypasses the reward modeling stage by the following objective: max π θ E [ Ψ ( p ∗ ( y w ≻ y l | x ) ) ] − β D K L ( π θ | | π ref ) {\displaystyle \max _{\pi _{\theta }}\mathbb {E} [\Psi (p^{*}(y_{w}\succ y_{l}|x))]-\beta D_{KL}(\pi _{\theta }||\pi _{\text{ref}})} where p ∗ ( y w ≻ y l | x ) {\displaystyle p^{*}(y_{w}\succ y_{l}|x)} is preference distribution of the chosen responses y w {\displaystyle y_{w}} over the rejected responses y l {\displaystyle y_{l}} . However, since p ∗ {\displaystyle p^{*}} is not observed directly, we sample from a Bernoulli distribution from the offline preference dataset as: p ∗ ( y ≻ y ′ | x ) = E h [ I { h prefers y to y ′ given x } ] {\displaystyle p^{*}(y\succ y'|x)=\mathbb {E} _{h}[I\{h{\text{ prefers }}y{\text{ to }}y'{\text{ given }}x\}]} To solve this objective, IPO minimizes the quadratic loss function: Minimize E ( x , y w , y l ) ∼ D [ ( h π ( x , y w , y l ) − I ( y w , y l ) ) ] 2 = E x , y w , y l ) ∼ D [ I h π ( x , y w , y l ) − ( 1 − I ) h π ( x , y l , y w ) − 1 2 β − 1 ] 2 = E x , y w , y l ∼ D [ h π ( x , y w , y l ) − 1 2 β − 1 ] 2 {\displaystyle {\begin{aligned}{\text{Minimize }}&\mathbb {E} _{(x,y_{w},y_{l})\sim D}[(h_{\pi }(x,y_{w},y_{l})-I(y_{w},y_{l}))]^{2}\\&=\mathbb {E} _{x,y_{w},y_{l})\sim D}[Ih_{\pi }(x,y_{w},y_{l})-(1-I)h_{\pi }(x,y_{l},y_{w})-{\frac {1}{2}}\beta ^{-1}]^{2}\\&=\mathbb {E} _{x,y_{w},y_{l}\sim D}[h_{\pi }(x,y_{w},y_{l})-{\frac {1}{2}}\beta ^{-1}]^{2}\end{aligned}}} where h π ( x , y w , y l ) = log ⁡ ( π θ ( y w | x ) π ref ( y w | x ) ) ) − log ⁡ ( π θ ( y l | x ) π ref ( y l | x ) ) {\displaystyle h_{\pi }(x,y_{w},y_{l})=\log \left({\frac {\pi _{\theta }(y_{w}|x)}{\pi _{\text{ref}}(y_{w}|x))}}\right)-\log \left({\frac {\pi _{\theta }(y_{l}|x)}{\pi _{\text{ref}}(y_{l}|x)}}\right)} and I ( y w , y l ) {\displaystyle I(y_{w},y_{l})} is a function drawn from the Bernoulli distribution from the preference dataset. Here, I ( y , y ′ ) {\displaystyle I(y,y')} is 1 if y {\displaystyle y} is preferred to y ′ {\displaystyle y'} which happens with probability p ∗ ( y ≻ y ′ ) {\displaystyle p^{*}(y\succ y')} , and 0 otherwise. As such, the simplification of the expression directly follows from exploiting the symmetry of y {\displaystyle y} and y ′ {\displaystyle y'} from the Bernoulli such that for each datapoint ( y w , y l ) i ∼ D {\displaystyle (y_{w},y_{l})_{i}\sim D} . In particular this symmetry can be represented as ( y , y ′ , I ( y , y ′ ) ) = ( y w , i , y l , i , 1 ) {\displaystyle (y,y',I(y,y'))=(y_{w,i},y_{l,i},1)} and ( y , y ′ , I ( y , y ′ ) ) = ( y l , i , y w , i , 0 ) {\displaystyle (y,y',I(y,y'))=(y_{l,i},y_{w,i},0)} with E y [ p y ] = 1 2 {\displaystyle \mathbb {E} _{y}[p_{y}]={\frac {1}{2}}} and E [ I ( y , y ′ ) ] = p y {\displaystyle \mathbb {E} [I(y,y')]=p_{y}} . In summary, IPO can control the gap between the log-likelihood ratios of the policy model and the reference by always regularizing the solution towards the reference model. It allows learning directly from preferences without a reward modelling stage and without relying on the Bradley-Terry modelisation assumption that assumes that pairwise preferences can be substituted with pointwise rewards. Thus, it avoids overfitting to the preference dataset especially when preferences are near deterministic and the KL term fails. Kahneman-Tversky optimization Kahneman-Tversky optimization (KTO) is another direct alignment algorithm drawing from prospect theory to model uncertainty in human decisions that may not maximize the expected value. In general, KTO seeks to optimize a class of new loss functions proposed as “human-aware losses” (HALO) formulated under prospect theory to model “human values” of a query, response pair ( x , y ) {\textstyle (x,y)} as v ( r θ ( x , y ) − E Q [ r θ ( x , y ) ′ ] ) {\displaystyle v(r_{\theta }(x,y)-E_{Q}[r_{\theta }(x,y)'])} . A function is defined as a human-aware loss for the value described by the general HALO objective: f ( π θ 