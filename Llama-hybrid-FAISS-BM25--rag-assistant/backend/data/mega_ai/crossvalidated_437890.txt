[site]: crossvalidated
[post_id]: 437890
[parent_id]: 
[tags]: 
Is the PCA estimator used in regression root-n-consistent?

Consider a sample of $n$ observations $(y_i, x_i)$ , $i=1,\ldots,n$ and assume without loss of generality that the samples are centered. The true model is $y_i=x_i^t\beta+\epsilon$ , and the OLS estimator, which is a $\sqrt{n}$ -consistent estimator of $\beta$ is obtained as, $$\hat{\beta}=(X^tX)^{-1}X^ty$$ Now perform a PCA on the matrix $X\in\mathbb{R}^{n\times p}$ . Consider the matrix of principal components $Q\in\mathbb{R}^{p\times p}$ defined in a way such that the first principal component has the largest possible variance, and each succeeding component has the largest possible variance under the constraint that it is orthogonal to the preceding components. From an algebra perpective, $Q$ is the matrix of eigenvectors from $X$ and define an orthogonal change of basis maximizing the variance from $X$ . Consider $Z=XQ\in\mathbb{R}^{n\times p}$ the projection of $X$ into the subspace generated by $Q$ and solve, $$\tilde{\beta}=(Z^tZ)^{-1}Z^ty$$ Where $y_i=z_i^t\beta+\epsilon$ . We can now go back to the original subspace from $X$ (in order, for example, to ease the interpretation of the coefficients) $$\hat\beta=Q\tilde\beta$$ But what happens if we do not select all the principal components from $Q$ but just a subset of the first $d$ , $d components and calculate $Z_d=XQ_d$ ? Now if we solve $$\tilde{\beta_d}=(Z_d^tZ_d)^{-1}Z_d^ty$$ and then project back onto the original subspace $$\beta^*=Q_d\tilde\beta_d$$ Is $\beta^*$ a root-n consistent estimator of $\beta$ ? Is it even a consistent estimator? How does affect the number of components chosen to the consistency?
