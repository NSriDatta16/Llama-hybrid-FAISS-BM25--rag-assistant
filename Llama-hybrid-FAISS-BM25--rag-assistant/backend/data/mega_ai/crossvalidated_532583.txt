[site]: crossvalidated
[post_id]: 532583
[parent_id]: 532571
[tags]: 
Without actually being able to see how you are doing what you are doing, I cannot give a definitive answer. However, for purposes of discussion, we will assume your code is correct. I see a lot of separate issues in your question. First, we do not know if Pearson's correlation coefficient is an appropriate metric. If you were looking at accounting data, such as accounts receivables versus inventories, on a quarterly basis, then it would be appropriate. On the other hand, for many types of financial returns, it would not be. So you should dig into the assumptions of Pearson's coefficient and see if they are satisfied. It is not enough to assume that they are satisfied, they actually must be satisfied. You are using rolling periods, why? If it is because everybody else does it, then that is a bad reason. If a correlation is a stable property of the population, then it is stationary. If you are saying that it is not stationary, why would it happen to correspond to your particular window? Whether you intended to or not, you are asserting that some market changes its mind exactly every three months. If that is not at least an approximately true statement, then the correlations will be bogus. For example, let us imagine that components of the US stock markets break about every 73 days, on average, but sometimes only last 20 days and have lasted as long as 700 days. That will totally blow up your fixed quarterly window. Assuming that your null hypothesis is that there are no correlations in the entire set, and if that null is not rejected, then that implies that your correlations could have been generated by chance effects alone. That would make your statement, "if it were 100% correlated" to be like the case where many people are tossing coins and one tossed a coin as heads ten times in a row and asking "wasn't that one coin tosser the best?" If you see two people walking down the street together, it could imply that they are best friends going to a common destination. It could also imply that they are strangers that intersected and at the moment are going in the same general direction. At some point, one may go to a coffee shop, while the other catches a bus to go home. With very many interactions, that is bound to happen from time to time. The correlation of the two paths only is meaningful in the common destination case, not the happenstance case. Additionally, p-values are not unique. There are very well known examples where two researchers observing the same data will get two different p-values as p-values are model dependent. The intentions of the researcher can determine a p-value. In your case, because you are doing multiple comparisons, there will be a correction factor. The multiple comparison rules are not unique, so two researchers observing the same data can get differing p-values if they employ different rules for multiple comparisons. EDIT Your tool you mentioned in the comments might be inappropriate. It assumes that $x_{t+1}=x_t+\epsilon_{t+1}$ , but for many financial series such as stocks, $x_{t+1}=Rx_t+\epsilon_{t+1},R>1,$ such as stock prices. If the return were less than or equal to one, then equity investors would expect systematic losses. Your tool is assuming that your data does not change, ever. It is a white noise tool. Do you believe your data is made up only of white noise? As a side note, there is a proof in 1958 that the equation $x_{t+1}=Rx_t+\epsilon_{t+1},R>1,$ has no non-Bayesian solution under quadratic loss, though you could use Theil's regression or quantile regression through the median.
