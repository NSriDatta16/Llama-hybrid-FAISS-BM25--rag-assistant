[site]: crossvalidated
[post_id]: 305605
[parent_id]: 184157
[tags]: 
In machine learning, you can get away with many approximations if you can show they are useful. There are some questions and answers on this site stating that in some cases, linear regression can do for classification without using the extensions and adaptations that logistic regression made to it for that purpose. In the case of random forest, know that they already come with approximate label confidences defined by the proportion of trees that classify the record with that label. But who knows, perhaps you are on to something. You would just need to prove it empirically. When you want to show that your way provides better label confidences, you are saying that across the spectrum of possible classification cutoffs, they should reach better classification results than with the other way of estimating label confidences. You could compare classification performance with ROC AUC because that is what a ROC curve represents.
