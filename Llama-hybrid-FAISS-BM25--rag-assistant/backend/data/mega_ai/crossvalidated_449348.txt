[site]: crossvalidated
[post_id]: 449348
[parent_id]: 
[tags]: 
How do I choose a prior for this hierarchical model? (Kruschke book)

I am working through Kruschke's "Doing Bayesian Data Analysis", currently working on the Hierarchical models chapter. The book uses JAGS for MCMC. One of the exercises asks the reader to compare two possible gamma distributions as the the priors for the hyperparameter kappa, in the following JAGS model: model { for ( s in 1:Nsubj ) { z[s] ~ dbin( theta[s] , N[s] ) theta[s] ~ dbeta( omega*(kappa-2)+1 , (1-omega)*(kappa-2)+1 ) } omega ~ dbeta( 1 , 1 ) kappa The data consists of 28 groups,with 10 trials per group, modeled as a binomial z successes out of N trials. Here's a plot of the two candidate gamma priors. The results of running MCMC (note they are on different x and y scales): for gamma(mean=1) mode=19 and tail reaches 250 or so for gamma(mode=1) mode=15 and tail reaches 50 or so I'm puzzled by several aspects of the model and results: The book presents the mean=1 gamma distribution is as "generic, vague", i.e. a weak prior, but its plot concentrates nearly all the probability around 0. I'd expect a "weak" prior to be more like a spread out normal distribution. It's true that the mean=1 has more probability in the tail compared to the mode=1 gamma, but it's very little in absolute terms: > 1-pgamma(50,0.01,0.01) [1] 0.005626756 > 1-pgamma(50, 1.105125 , 0.1051249 ) [1] 0.006654208 > 1-pgamma(100,0.01,0.01) [1] 0.002216235 > 1-pgamma(100, 1.105125 , 0.1051249 ) [1] 3.702478e-05 So, why does the author treat the mean=1 as "noncommital" when it seems quite the opposite? The posteriors for kappa show that the heavy tails for the mean=1 gamma make a huge difference on the tails of the posterior. Why does such a amount of mass in the tail makes make such a big difference in the posterior? Finally (3), the choice of prior for kappa has a strong effect on shrinkage (The book's term for the group estimates being pulled towards the pooled estimate). Because the different groups in the dataset perform very differently, both priors leave the ML estimate for several groups outside the 95% HDI, so by that test they are both problematic. I can see why some shrinkage makes sense, but I can't see how I could apriori justify how much shrinkage there should be, so that doesn't help me decide on a prior. The book suggests that looking at the priors for the group thetas, dictated by the hyper-parameter priors, is a way to decide between the two alternatives. The heavy-tailed mean=1 kappa prior generates a uniform prior on the group thetas while the mode=1 kappa prior generates a (flattened) bell-shaped prior which decays towards 0 and 1. I could argue that either makes sense, but both result in so much shrinkage that several of the group ML estimates (MLE(group)=z/N) are left outside the 95% HDI in the posterior. So they seem equally appropriate, and at the same time, neither seems particularly better than the other. So, what should guide my choice of prior?
