[site]: crossvalidated
[post_id]: 402545
[parent_id]: 395126
[tags]: 
It's reasonable in the sense that there is nothing conceptually wrong with the approach. However as you point out, it's questionable whether this would do better than a straightforward sequence model. I once read somewhere that character level sequence models perform better when the first letter of every word is swapped with the second letter -- presumably because the second letter, usually being a vowel, is easier to predict and condition the rest of the word on. I mention this because if sequence models are so sensitive to the order in which information is presented, perhaps an attention mechanism would help with that, even if on very short sequences.
