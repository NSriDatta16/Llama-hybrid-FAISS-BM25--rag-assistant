[site]: crossvalidated
[post_id]: 337029
[parent_id]: 295880
[tags]: 
A sufficiently powerful function approximator can map any probability distribution to any other probability distribution. In practice, this means that any for reasonable choice of latent distribution, you can train a generator to map that to the distribution of images in your dataset. So it doesn't make any fundamental difference to use uniform distribution over gaussian or vice versa. However, in variational autoencoders, where you have the encoder trying to predict the latent representation of the image $q(z|X)$, then a normal distribution makes things easier to work with, as your predictions can't ever go "out of bounds" as they could if you used a uniform distribution. Of course this doesn't apply to GANs. The variability of the samples generated which you mentioned in your question isn't really a function of the latent distribution, since the network can scale up and down the spread of the distribution with a single layer.
