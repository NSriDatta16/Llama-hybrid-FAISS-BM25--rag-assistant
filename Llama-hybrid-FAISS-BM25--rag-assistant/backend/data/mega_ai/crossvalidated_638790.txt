[site]: crossvalidated
[post_id]: 638790
[parent_id]: 
[tags]: 
Full conditional posteriors

so up to now I dealt with posteriors in the form of: $$p(\theta|x) \propto p(x|\theta) p(\theta)$$ No we started to model a linear regression with the bayesian approach: $$Y \sim MVN(X\beta, \sigma^2I)$$ $$\beta|\sigma^2 \sim MVN(m, \sigma^2M)$$ $$\sigma^2 \sim IG(a_0, b_0)$$ We derived two posteriors (I guess) for the two full conditionals $$p(\beta|\cdot) = p(\beta|\sigma^2, y) \propto p(y|\beta, \sigma^2)p(\beta|\sigma^2)$$ $$p(\sigma^2|\cdot) = p(\sigma^2|\beta, y) \propto p(y|\beta, \sigma^2)p(\beta|\sigma^2)p(\sigma^2)$$ The lecture slide notes that the $p(\sigma^2)$ is not included in the $p(\beta|\cdot)$ since it doesn't include $\beta$ . I don't understand the posteriors. Up to now I only knew the posterior form like $$posterior \propto likelihood \cdot prior$$ . For me it looks like the posterior now needs all the priors, could that be? Like $$posterior \propto likelihood \cdot prior_1 \cdot ... \cdot prior_n$$ Maybe some one can provide some literature or give me a good explanation. Thanks in advance. :) Note: there might be an inaccuracy because the lecture slides used $Y$ and $y$ maybe this is correct but I think it is not
