[site]: crossvalidated
[post_id]: 249513
[parent_id]: 159093
[tags]: 
The one what we really learned is the use of sparse activation and the use of linear rectified activation functions. The later is basically one reason, why we saw an explosion in activity regarding so called neural network since using this kind of activation functions resulted in dramatic degrease of training affords for those artificial computational networks we use to call neural networks. What we learned is why synapse and neurons are build this way and why it is preferable. These linear rectified activation (f(x) := x > a ? x : 0) results in sparse activation (only few of the 'neurons' (weights)) get activated. So what we do while our knowledge extends towards biological functions, we understand why this was selected and preferred by evolution. We understand that those systems are sufficient enough but also stable in terms of error control during training and also preserve resources like energy and chemical/biological resources in a brain. We simply understand why the brain is what it is. Also by training and looking at the strategies we understand about possible flows of information and the involved information processing helping us to construct and assess hypothesis about the very subjects. For example something I can remember from a decade ago was training a system on learning natural spoken language and it the discovery made was the system showed similar problems that reassemble analogic behavior of babies learning speaking a language. Even the differences between learning different kind of languages were similar enough. So by studying this approach and design, it was concluded that the human information processing during language learning is similar enough to draw training recommendations and treatment for language related problems, that it helped in aiding and understanding humans difficulties and developing more efficient treatment (what ever of it really made it in practice is another question). A month ago I read an article about how the 3D navigation and remembering of rat brains really work and by creating computational models about every finding it was a great help to understand what is really going on. So the artificial model filled in the blanks of what was observed in the biological system. It really amazed me when I learned that the neurological scientists used a language that assembled more that of an engineer than a biological person talking about circuits, flow of information and logical processing units. So we are learning a lot from artificial neural networks since it presents us with empiric play grounds we can derive rules and assurance from when it comes to the why the architecture of the brain is what it is and also why evolution prefers this over alternative ways. There are still lots of blanks but from what I read - I just got recently into CNN's etc. but had artificial AI, fuzzy logic and neural networks during university time in the early 2000's. So I had catch up on a decade worth of development and discovery resulting in gratitude for all those scientists and practitioners of the neural network and AI field. Well done people, really well done!
