[site]: datascience
[post_id]: 39828
[parent_id]: 39158
[tags]: 
You are asking for suggestions of a technique which will give the best results in comparison to other methods, that's difficult to say a priori as it will depend on your particular features and dataset and will be guided by trial and error and comparison of different algorithms (this is also related to the No Free Lunch Theorem, see some discussion here ). However, there are certainly many optimization techniques advances in Artificial Intelligence. In this answer I'll cover the ones I'm aware of. Recently there have been several nice advances where new perspectives in optimization models have been developed classic problems Machine Learning. These works have been published by renowned Operations Research academic Dimitris Bertsimas. See for example this link . For example, I find the following publications from the list to be very interesting and promising (you can find free PDF versions at the last link): Bertsimas, D., King, A., & Mazumder, R. (2016). Best subset selection via a modern optimization lens. The annals of statistics, 44(2), 813-852. In this work, the authors present a Mixed Integer Optimization approach for solving the best subset selection problem (i.e. to choose a subset of features out of a larger set in linear regression given a collection of observations). Typically the Lasso technique is used, but in several computational experiments it was outperformed by this method. Moreover, this new theory develops criteria for proving that the adquired solution is indeed optimal (or near-optimal), which was not possible using heuristic methods. Bertsimas, D., & Dunn, J. (2017). Optimal classification trees. Machine Learning, 106(7), 1039-1082. In this case I cite the abstract: State-of-the-art decision tree methods apply heuristics recursively to create each split in isolation, which may not capture well the underlying characteristics of the dataset. The optimal decision tree problem attempts to resolve this by creating the entire decision tree at once to achieve global optimality. In the last 25 years, algorithmic advances in integer optimization coupled with hardware improvements have resulted in an astonishing 800 billion factor speedup in mixed-integer optimization (MIO). Motivated by this speedup, we present optimal classification trees, a novel formulation of the decision tree problem using modern MIO techniques that yields the optimal decision tree for axes-aligned splits. We also show the richness of this MIO formulation by adapting it to give optimal classification trees with hyperplanes that generates optimal decision trees with multivariate splits. (...) We comprehensively benchmark these methods on a sample of 53 datasets from the UCI machine learning repository. We establish that these MIO methods are practically solvable on real-world datasets with sizes in the 1000s, and give average absolute improvements in out-of-sample accuracy over CART of 1–2 and 3–5% for the univariate and multivariate cases, respectively. Furthermore, we identify that optimal classification trees are likely to outperform CART by 1.2–1.3% in situations where the CART accuracy is high and we have sufficient training data, while the multivariate version outperforms CART by 4–7% when the CART accuracy or dimension of the dataset is low. In their computational experiments the model has even outperformed Random Forest and XGBoost, as can be seen on slide 29/41 of this presentation Bertsimas gave earlier this year . This is quite impressive, as these algorithms are among the most frequently used by winning solutions in Kaggle (see for example this link and this one ). Bertsimas, D., & King, A. (2017). Logistic regression: From art to science. Statistical Science, 32(3), 367-384. This case is similar to the first one, but for logistic regression instead of decision trees. Again they achieved new algorithms for training a classic Machine Learning model, getting high quality solutions as well as near-optimality guarantees. Besides this line of work, large scale convex optimization is another area where lots of attention have been drawn. Big data and machine learning/deep learning applications motivate the development of new algorithms and theory to achieve scalable and efficient solutions in this new contetxt. See for example Bottou, L., Curtis, F. E., & Nocedal, J. (2018). Optimization methods for large-scale machine learning. SIAM Review, 60(2), 223-311. All the advances I mentioned are very recent, so don't expect to have access to available and scalable libraries to use them out of the box. Therefore, the usefulness of them for your particular project will depend on (1) the scope of your project (research or more short term objective?), (2) the size of your datasets, and (3) the available time you have for working on it and delivering a solution.
