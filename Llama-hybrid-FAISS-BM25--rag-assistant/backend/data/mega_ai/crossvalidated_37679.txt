[site]: crossvalidated
[post_id]: 37679
[parent_id]: 37569
[tags]: 
A log-normal distribution is fully defined by the pair of parameters $\mu$ and $\sigma$. Since you want to fit this distribution to your data, it's sufficient to estimate these two values. Normally, you would have access to the raw data, and would apply the standard the maximum likelihood estimators (MLEs) for $\mu$ and $\sigma$, which are straightforward: $$\hat{\mu} = \frac{1}{n}\sum_i \ln(y_i) = \langle \ln y \rangle\\ \hat{\sigma}^2 = \frac{1}{n}\sum_i (\ln(y_i)-\hat{\mu})^{2} \enspace .$$ That is, $\mu$ is the mean of the logarithm of your observed data $\{y_i\}$, and $\sigma$ is the standard deviation of the logarithm of the data. But in this case, you don't have the raw data. Instead, you have some sketchy information about the cumulative distribution function (CDF). Very roughly, what you know the fraction of the distribution $\Pr(y)$ that is below some $y$ for some set of values $\{y_i\}$. You can still estimate the log-normal parameters (or those of any other distribution) from this kind of information, but there are subtleties. Two approaches come to mind. The first is a quick and dirty one that will not produce entirely accurate parameter estimates, but will get you close enough to get a sense of what the distribution looks like and, if you want, roughly what the Gini coefficient would be. The second is more complicated and more accurate for the kind of data you have. Quick and dirty approximation Here's the quick and dirty solution. The information you have is a "binned" version of the CDF, represented by a set of pairs $(q_i,y_i)$, where $q_i$ is the fraction of the distribution at or below the value $y_i$ (note: you said that the PPP is an average within the bin, which is technically distinct from the CDF, but for our calculation, that distinction doesn't make a difference). Now, recall that the definition of the mean is $$\langle x \rangle = \sum_i x_i \Pr(x_i)\enspace ,$$ where $\Pr(x_i)$ is the probability of observing $x_i$. We don't have $\Pr(x)$, but we can approximate it using the binned CDF information, like this $$\hat{\mu} \approx \sum_{i=1}^k \Delta q_i \ln x_i$$ where $\Delta q_i=q_{i+1} -q_i$ is the size or width of the $i$th bin, out of $k$ bins. Similarly, for the standard deviation, the definition is $$\sigma = \sum_i (x_i-\langle x \rangle)^2 \Pr(x_i)\enspace,$$ which becomes $$\hat{\sigma} \approx \sum_{i=1}^{k} \Delta q_i (x_i-\hat{\mu})^2 \enspace .$$ To apply these to your data, you'll need to let $x_i=\ln y_i$ since you're working with the log-normal distribution, rather than the normal (or Gaussian) distribution. Coding up these estimators should be fairly easy. In my numerical experiments with these estimators, I consistently get slight errors in the estimates relative to the underlying or "population" values I used to generate synthetic log-normal data. If you use these with your data, you should not treat the estimated values as being highly accurate. To get those, you'd need to apply a more mathematically sophisticated approach, which I'll sketch for you now. Maximum likelihood approach The more complicated and more accurate solution is to derive the maximum likelihood parameter estimate for the particular representation of the log-normal distribution you have, i.e., the binned CDF. The definition of the log-normal PDF is $$\Pr(x) = \frac{1}{x\sigma\sqrt{2\pi}}{\rm e}^{-\frac{(\ln x - \mu)^2}{2\sigma^2} } \enspace ,$$ and the CDF is $$\Pr(x The log-likelihood of your observed quantile information is then $$\ln \mathcal{L} = \sum_{i=1}^k \ln F(x_i\,|\,q_i,q_{i+1})\enspace .$$ The more sophisticated approach would be to estimate $\mu$ and $\sigma$ by maximizing this function over these parameters. This would give you the maximum likelihood estimate for your log-normal model, given the observed information you have. For arbitrary choices of $\{q_i\}$, an analytic solution for the MLE is not possible, but for regularly spaced choices of the bin boundaries, it may be. Regardless, however, you may always numerically maximize the function (which many numerical software packages can do for you, if you whisper the right words to them). What makes this approach more complicated is that you need to get the mathematics correct when you code up the numerical routine to do the estimation with the data. If the accuracy of your answers is really important, then this approach might be worth the extra effort.
