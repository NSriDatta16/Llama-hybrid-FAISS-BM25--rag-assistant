[site]: datascience
[post_id]: 87098
[parent_id]: 
[tags]: 
Dropout after the Embeding layer

I am working on a classification problem. I am using pre-trained GloVe word embedding as input I wanted to know whether adding a dropout after the embedding layer makes sense at all? I have seen a lot of notebooks on Kaggle where dropout layers are added after the embedding layer and were wondering why this is done. model= Sequential() model.add(Embedding(vocab_size+1,embedding_dim,input_length=max_len, weights=[embeddings_matrix],trainable=False)) model.add(Dropout(0.3)) model.add(Bidirectional(LSTM(64,return_sequences= True))) model.add(Bidirectional(LSTM(32))) model.add(Dense(32,activation='relu')) model.add(Dense(1,activation='sigmoid'))
