[site]: crossvalidated
[post_id]: 195467
[parent_id]: 
[tags]: 
Should I use a Bayesian multilevel model to detect outliers?

I have approximately-log-normal price data. The data is hierarchically structured. Let's say there are three levels, so the notation for a log-transformed observation could be $x_{ijk}$, with the indexing decreasing in granularity to the right. A minority of the price data is obviously too high or too low to be valid. What is not obvious are the cut-off points for the definition of a credible price. My current method is stepwise: Step 1: Define outliers as observations that lie below $\frac{1}{3}x_{jk}$ and above $3x_{jk}$, where $x_{jk}$ is the mean across observations the observations in grouping $jk$. Step2: For groups $jk$ with fewer than 10 observations, define outliers instead as lying outside the bounds of $\frac{1}{3}x_k$ and $3x_k$, where $x_k$ is the mean across $x_{jk}$ within group $k$. Step 3: For groups $k$ with fewer than 10 observations, define outliers instead as lying outside the bounds of $\frac{1}{3}x$ and $3x$, where $x$ is the mean across $x_k$. This sounds to me like an informal attempt at defining outliers based on a multilevel model. I wonder if I could fit a multilevel model with an informative population-level hyper-prior distribution, and then define outliers as observations that lie outside 95% or 99% credible intervals of $\mu_{ijk}$, where $\mu_{ijk}$ is the mean parameter of the conditional log-normal distribution for group $ijk$. Are there credible examples of this in the literature? Do you have experience with this outlier-detection method and have some perspective? Are there theoretical justifications for this method? WWAGD? (What Would Andrew Gelman Do?)
