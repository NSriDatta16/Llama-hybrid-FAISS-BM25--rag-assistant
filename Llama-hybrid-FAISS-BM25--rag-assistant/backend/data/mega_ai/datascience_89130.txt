[site]: datascience
[post_id]: 89130
[parent_id]: 25725
[tags]: 
Thiraputhi is correct that Zipf's law can be used to get a fairly decent set of IDF values from an ordered list of the 20,000 most common words. However, Google's n-grams have been available since 2012 and these contain the data you are looking for, although you have to extract it from their unigrams (i.e. 1-grams) dataset using awk or some other programming language or tool. If you go to the top of the repo mentioned by Thiraputhi, they even kind of allude to these Google n-grams, strangely enough, and they also mention that the files in their repo are derived from Peter Norvig's 1/3 million most frquently used words list. Norvig claims on his site that these come from Google's "Trillion word corpus". This may be the same corpus that Google uses to generate their n-grams, I'm not sure. But Norvig's 1/3 million word list contains a column for the count of the word in the corpus, and this is the column you are looking for for your IDF frequency. TF-IDF = frequency (word count) of a term in an indivividual document divided by that word's frequency (word count) in the larger corpus, and this latter term is found in column 2 of Norvig's file. It would be superfluous to approximate this column using Zipf's law when you have the original file containing that column available. Here are the links that answer your question: Google's n-grams, including 1-grams: http://storage.googleapis.com/books/ngrams/books/datasetsv2.html Norvig's more convenient 1/3 million words dataset: https://norvig.com/ngrams/count_1w.txt There are also many other valuable datasets on Norvig's main n-grams page: https://norvig.com/ngrams/ It might be tempting to just use Norvig's very convenient dataset, but I believe it will be more consistent with the scientific method (i.e. 'reproducibilty') to do your own extraction from Google's 1-grams. I believe this should actually yield much more than 1/3 of a million words, as google's n-grams interpret as words quite a lot things that most people will not consider to be words, i.e. 123:45 and similar things. You can leave these in the dataset if you have the processing power to do the lookup, or if you can, turn the dataset into a fast key-value store. There are many open source key-value stores available including Tokyo cabinet and others whose names I have forgotten, and there's also sqlite. So if you can turn it into a fast key-value store or other db somehow, or if you have the processing power, then this might be better than trying to sift through all those rows of data to get just the data that fits your needs. Otherwise you'll have to figure out some rule like 'no hyphens and no colons' or 'strictly alphabetical' or 'strictly alphanumeric' and prune out everything that doesn't fit. Just make sure to document everything you do if this is for some kind of scientific purpose. Edit: You will need the total word counts from your individual documents as well as total word count from the corpus. For google's n-grams, the latter can be found in the files labeled total_counts on the linked page.
