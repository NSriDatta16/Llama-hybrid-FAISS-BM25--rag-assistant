[site]: crossvalidated
[post_id]: 88345
[parent_id]: 
[tags]: 
Better markov chain rank aggregation using an entropy-based approach

Background: Dwork et al. in Rank Aggregation Methods for the Web have proposed a few markov chain-based methods to perform rank aggregation (finding an aggregated ranking of items from a set of N rankings - eg. applications in meta search, group recommendation). There's one flaw with that approach (has been discussed in a couple of other papers) - it does not account for the uniformness of the distribution of the discordance that a candidate optimal list has with the input lists. Simply put, out of, say, 15 voters, if 10 voters rank an item moderately badly, it should be a better indication of said item's inferiority than if 2-3 voters ranked the item VERY badly. This is something I'm trying to incorporate into their markov chain approach. Problem: Now, entropy can be used as a measure of uniformity of data in cases like this. So consider a transition matrix like we have in PageRank calculations, with normalized rows with M(i,j) giving the probability the one will go from i to j. Can we make ranking better by figuring out the quality (uniformity) of "ranks" here, using entropy-based methods? I don't have a formal stats/maths background, so I may be missing a lot of obvious things here. Will really appreciate it if someone can give me some pointers.
