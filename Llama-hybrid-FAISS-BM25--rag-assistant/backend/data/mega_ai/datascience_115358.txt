[site]: datascience
[post_id]: 115358
[parent_id]: 115351
[tags]: 
In machine learning there is something called the no free lunch Theorem , which basically states that there isn't one solution/algorithm that will perform best on every problem. Different algorithms will perform differently on different data. Therefore, you try different algorithms to pick the best, although there are algorithms that are generally more powerful for certain types of problems (...). Hint: You usually pick the best algorithms without hyperparameter tuning in the first round and then compare the best ones again after tuning their hyperparameters since these can make a decisive performance difference - especially for more "complex" algorithms like XGBoost in contrast to Random Forest . Although both algorithms are decision-tree based.
