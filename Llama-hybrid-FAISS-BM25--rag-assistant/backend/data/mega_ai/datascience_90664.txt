[site]: datascience
[post_id]: 90664
[parent_id]: 20179
[tags]: 
Have a look at this experimental data for average prediction speed per sample vs batch size. It very much underlines the points of the accepted answer of jcm69. It looks like this particular model (and its inputs) works optimal with batch sizes with multiples of 32 - note the line of sparse dots that is below the main line of dots. That might be different for other model-GPU combinations, but a power of two would be a safe bet for any combination. The benchmark of ezekiel unfortunately isn't very telling because a batch size of 9 potentially allocates twice as much memory. That a batch size of 9 is therefore faster than a batch size of 8 is to be expected.
