[site]: crossvalidated
[post_id]: 600767
[parent_id]: 600761
[tags]: 
Modeling a binary outcome with binary predictors is fairly standard in logistic regression. Here I have tried to simulate the data you refer to in R and fit it to a logistic regression. #### Create Fake Data #### set.seed(123) choose If you run summary(fit) , you get this readout, which shows you a good amount of information about your model, including the coefficients (which include log odds of each predictor), deviance, AIC, and other info. We can see color has a positive association with the outcome whereas flavor has a negative association: Call: glm(formula = choose ~ color + flavor, family = binomial, data = df) Deviance Residuals: Min 1Q Median 3Q Max -1.145 -1.131 -1.108 1.224 1.248 Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) -0.13261 0.15695 -0.845 0.398 color 0.05582 0.17933 0.311 0.756 flavor -0.03287 0.17924 -0.183 0.854 (Dispersion parameter for binomial family taken to be 1) Null deviance: 691.35 on 499 degrees of freedom Residual deviance: 691.22 on 497 degrees of freedom AIC: 697.22 Number of Fisher Scoring iterations: 3 Probably one of the more important things to obtain is the exponentiated coefficients, which provides odds ratios which may be more useful for interpretation. exp(coef(fit)) This gives us the following readout: (Intercept) color flavor 0.8758064 1.0574089 0.9676636 We can see that color = 1 (whatever that may be, we can call it "red" here) is 1.05 times likely to choose yes (this depends on what your reference value is, here I just say it means yes). Flavor = 1 (perhaps "spicy") slightly decreases the odds of choosing yes. If you are not experienced on logistic regression or using it within R, a great book on this subject is Practical Guide to Logistic Regression by Joseph Hilbe.
