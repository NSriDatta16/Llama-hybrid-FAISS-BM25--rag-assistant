[site]: crossvalidated
[post_id]: 343253
[parent_id]: 343223
[tags]: 
My possibly idiosyncratic view is as follows. If we had an exact, fully-known, prior distribution on the parameters, possibly belief-based, and we knew the true likelihood function, the Bayesian paradigm gives us the optimal way of updating that prior with the likelihood to get a posterior. In real life we don't have either a prior or a likelihood, except in what seems to me to be rare cases, so we apply an intuitive "smoothness in function space" argument that runs as follows. As long as the prior we use for calculation is close to the real, unobservable prior we have, and the likelihood function we use for calculation is close to the real, usually unknowable likelihood function, applying the Bayesian paradigm will get us a posterior that is close to the real, uncalculatable posterior. Applying the Bayesian paradigm even with approximate priors and likelihoods is likely to get us closer (on average) than doing something else, because it eliminates a source of noise in the move from prior to posterior - noise due to using a suboptimal updating algorithm. This, then, is the value of trying to state your prior information as a probability distribution - it allows you to use the optimal updating algorithm, thereby reducing the error in the beliefs you form after you've looked at the data. As a rather lengthy side note, this implies that Bayesian robustness is a desirable feature of our overall process (assigning priors and likelihood functions, performing the update calculations), the more so as our confidence in the accuracy of our constructed / assumed prior and likelihood functions degrades. At some point, we'll have so little confidence in our ability to form any sort of reasonable approximation to one, the other, or both, that we may as well abandon the Bayesian paradigm and do something else. Alternatively, the cost of setting up and executing the Bayesian paradigm may be so great, relative to the gains thereof, that we are, again, better off doing something else, such as running a classical t-test, observing a t-statistic of 19.4, and rejecting the null hypothesis that we created just to make life simpler. Now, as to the influence priors have - that depends on the prior, the likelihood function, and the data. It is quite easy to find all sorts of real-world situations where the data overwhelms the prior, in which case even very different priors lead to very similar posteriors. In these situations, worrying about the likelihood is far more important than worrying about the prior. On the other hand, in situations where getting data is very cost or time-intensive, the prior information may have to be carefully extracted from the relevant experts in order to make the best use of it as we can. (This was the case in my previous job, in which I did reliability analysis for solar panels and trackers, among other things - testing a large, expensive piece of equipment that is supposed to track the sun to derive a mean time to failure is both time-consuming and expensive.) So, the influence of the prior is situational, and that same situationality drives where we should focus our efforts in order to make best use of the optimal updating algorithm that Bayes' Theorem gives us.
