[site]: crossvalidated
[post_id]: 249191
[parent_id]: 
[tags]: 
Does variance only work on normally distributed data (as a measure of dispersion)?

It says in wikipedia The role of the normal distribution in the central limit theorem is in part responsible for the prevalence of the variance in probability and statistics. I understand this as When we use variance/SD as a measure of dispersion, we are actually looking for the "scaling parameter" of a normal distribution, since a random random variable is likely to follow approximately a normal distribution to CLT. In the case that the data is not normally distributed, is variance/SD still a reasonable measure of dispersion? Say the data is uniformly distributed, the average absolute deviation seems to be a better measure of dispersion than the variance, because it can be seen as the "scaling parameter" for the uniform distribution, am I right? Update I mean, say I have two sets of samples, one is {1,1,1,-1,-1,-1} and the other one is drawn from a normal distribution $N(0,1)$, their variances are both 1. The two sets will be considered as of the same degree of dispersion if we use variance as the measure. But it feels like we are forcefully treating them both as Gaussian then work out the distribution parameters and say "yeah they're equal in terms of dispersion".
