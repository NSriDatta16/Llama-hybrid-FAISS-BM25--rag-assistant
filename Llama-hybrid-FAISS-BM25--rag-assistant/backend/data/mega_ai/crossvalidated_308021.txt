[site]: crossvalidated
[post_id]: 308021
[parent_id]: 
[tags]: 
Distance measure between discrete distributions (that contains 0) and uniform

I'm trying to choose a district metric that falls between 0 to 1 and lets me compare the distance between a uniform probability distribution and any given probability distribution (could be random, could be uniform, could have all probability mass at one value, etc.). How I've been computing distance currently is by basically computing the average L1-norm distance between each bin in the two distributions. However, I haven't been able to find any published metric that does this. It seems like similar to the total variation distance and the Hellinger distance, but neither of those average across the total number of bins. Averaging avoids weaknesses of total variation that occur when, for example, given a true distribution of Unif[0,1] total variation distance = 1 if my experimental distribution is Unif[0.1,1] or Unif[0.9,1] (discretized into 10 bins). If I average, I get a distance near 0 for Unif[0,1] and closer to 1 for Unif[0.9,1]. Jenson-Shannon divergence or chi-squared seem like they could work, but I'm not sure how to choose. The Wasserstein metric could work, but since it's not between 0 to 1, it'll make comparing distributions across different distances difficult (e.g., for a different scenario my true distribution might be Unif[0,5] with 50 instead of 10 bins). I would appreciate if someone could 1) point me to a metric that's the average L1 distance between histogram indices (or tell me why this is a poor choice), and 2) Help me figure out how to choose between the above metrics (or point me to intuitive discussions of how to choose between them -- the mostly measure theory based papers I've found comparing the metrics unfortunately don't give me intuition on which to choose for my problem).
