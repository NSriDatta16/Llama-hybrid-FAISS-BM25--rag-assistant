[site]: datascience
[post_id]: 122642
[parent_id]: 
[tags]: 
Tensorflow loss: 0.0000e+00 - accuracy: 0.0000e+00

I was making changes to improve myself in a chatbot code using LSTM. But Loss and truth values are getting ridiculous values. Code: import tensorflow as tf import numpy as np from pandas import read_csv from tensorflow.keras.models import Model from tensorflow.keras.layers import Input, Dense, LSTM, Embedding ,Dropout from tensorflow.compat.v1.keras.layers import CuDNNGRU from tensorflow.keras.optimizers import RMSprop from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.utils import to_categorical mark_start = "ssss " mark_end = " eeee" data_src=[] data_dest=[] df = read_csv('Conversation.csv') for index, row in df.iterrows(): soru = row['question'] cevap = row['answer'] cevap = mark_start + cevap + mark_end data_src.append(soru) data_dest.append(cevap) class TokenizerWrap(Tokenizer): def __init__(self,texts, padding, reverse=False,num_words=None): Tokenizer.__init__(self, num_words=num_words) self.fit_on_texts(texts) self.index_to_word=dict(zip(self.word_index.values(), self.word_index.keys())) self.tokens=self.texts_to_sequences(texts) if reverse: self.tokens = [list(reversed(x)) for x in self.tokens] truncating="pre" else: truncating="post" self.num_tokens=[len(x) for x in self.tokens] self.max_tokens=int(np.mean(self.num_tokens)+2*np.std(self.num_tokens)) self.tokens_padded=pad_sequences(self.tokens,maxlen=self.max_tokens,padding=padding,truncating=truncating) def token_to_word(self,token): word = ' ' if token==0 else self.index_to_word[token] return word def tokens_to_string(self,tokens): words = [self.index_to_word[token] for token in tokens if token!=0] text = ' '.join(words) return text def text_to_tokens(self,text,padding,reverse=False): tokens = self.texts_to_sequences([text]) tokens = np.array(tokens) if reverse: tokens = np.flip(tokens,axis=1) truncating='pre' else: truncating='post' tokens=pad_sequences(tokens,maxlen=self.max_tokens,padding=padding,truncating=truncating) return tokens tokenizer = TokenizerWrap(texts=data_src + data_dest,padding='pre') VOCAB_SIZE = len(tokenizer.word_index)+1 from gensim.models import Word2Vec from re import sub vocab = [] for word in tokenizer.word_index: vocab.append(word) def tokenize(sentences): tokens_list = [] vocabulary = [] for sentence in sentences: sentence = sentence.lower() sentence = sub('[^a-zA-Z]', ' ', sentence) tokens = sentence.split() vocabulary += tokens tokens_list.append(tokens) return tokens_list , vocabulary # encoder_input_data tokenlenmis_sorular = tokenizer.text_to_tokens(data_src,"post") maxlen_questions = max([len(x) for x in tokenlenmis_sorular]) encoder_input_data = tokenlenmis_sorular data_src print(encoder_input_data.shape) # decoder_input_data tokenlenmis_cevaplar = tokenizer.text_to_tokens(data_dest,"post") maxlen_answers = max([len(x) for x in tokenlenmis_cevaplar]) decoder_input_data = tokenlenmis_cevaplar print(decoder_input_data.shape) tokenlenmis_cevaplar = tokenizer.text_to_tokens(data_dest, "post") #padded_answers = pad_sequences(tokenlenmis_cevaplar, maxlen=maxlen_answers, padding='post', truncating='post') onehot_answers = to_categorical(tokenlenmis_cevaplar, VOCAB_SIZE) decoder_output_data = np.array(onehot_answers) print(decoder_output_data.shape) print(decoder_output_data.shape) encoder_inputs = tf.keras.layers.Input(shape=(maxlen_questions ,)) encoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, 200 , mask_zero=True) (encoder_inputs) encoder_outputs , state_h , state_c = LSTM(200 , return_state=True)(encoder_embedding) encoder_states = [ state_h , state_c ] decoder_inputs = tf.keras.layers.Input(shape=(maxlen_answers , )) decoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, 200 , mask_zero=True) (decoder_inputs) decoder_lstm = LSTM(200 , return_state=True , return_sequences=True) decoder_outputs , _ , _ = decoder_lstm (decoder_embedding , initial_state=encoder_states) decoder_dense = tf.keras.layers.Dense(VOCAB_SIZE , activation=tf.keras.activations.softmax) output = decoder_dense (decoder_outputs) model = Model([encoder_inputs, decoder_inputs], output) model.compile(optimizer=RMSprop(learning_rate = 0.003), loss='categorical_crossentropy',metrics=["accuracy"]) model.summary() model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=512, epochs=200) def inference(): encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states) decoder_state_input_h = tf.keras.layers.Input(shape=(200 ,)) decoder_state_input_c = tf.keras.layers.Input(shape=(200 ,)) decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c] decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding , initial_state=decoder_states_inputs) decoder_states = [state_h, state_c] decoder_outputs = decoder_dense(decoder_outputs) decoder_model = tf.keras.models.Model([decoder_inputs] + decoder_states_inputs,[decoder_outputs] + decoder_states) return encoder_model , decoder_model def preprocess_input(input_sentence): tokens = input_sentence.lower().split() tokens_list = [] for word in tokens: tokens_list.append(tokenizer.word_index[word]) return pad_sequences([tokens_list] , maxlen=maxlen_questions , padding='post') enc_model , dec_model = inference() tests = ['Hello', 'Are you a', 'What is your name', 'That is a very long name', 'see you later'] print() for i in range(5): states_values = enc_model.predict(preprocess_input(tests[i])) empty_target_seq = np.zeros((1 , 1)) empty_target_seq[0, 0] = tokenizer.word_index['start'] stop_condition = False decoded_translation = '' while not stop_condition : dec_outputs , h , c = dec_model.predict([empty_target_seq] + states_values) sampled_word_index = np.argmax(dec_outputs[0, -1, :]) sampled_word = None for word , index in tokenizer.word_index.items() : if sampled_word_index == index : decoded_translation += f' {word}' sampled_word = word if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers: stop_condition = True empty_target_seq = np.zeros((1 , 1)) empty_target_seq[0 , 0] = sampled_word_index states_values = [h , c] print(f'You: {tests[i]}') print() decoded_translation = decoded_translation.split(' end')[0] print(f'Bot: {decoded_translation}') print('-'*25) Thank you in advance.
