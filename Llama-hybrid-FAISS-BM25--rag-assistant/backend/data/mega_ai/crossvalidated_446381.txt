[site]: crossvalidated
[post_id]: 446381
[parent_id]: 
[tags]: 
Friedman's Test fails inspite of big differences between the average ranks of algorithms

I performed Friedman's test to validate that a particular algorithm is statistically different from other compared algorithms. For example: I wanted to prove my algorithm A is significantly different from other algorithms across different datasets. So I tabulate the performances similar to, Algorithm A Algorithm B ..... Algorithm N Dataset A Dataset B . . Dataset k Average rank r_A r_B r_N Now if r_A (average rank of algorithm A) is much different than others (i.e. it's value is closer to N or 1 and other ranks are much away from r_A ), then we can guess that the Friedman's test will more likely reject Null Hypothesis . But surprisingly, I figured that the test fails to reject the null hypothesis in spite of having at least one algorithm or group excessively different from others. However when applying Wilcoxon ranksum test the significance is absorbed and rejected the null hypothesis as expected. Numerical example: Average ranks of 7 algorithms are: [5.66, 5.16, 4.66, 4.83, 2.66, 4, 1] Following Demsar-2006 notations: Friedman statistic is computed for k = 7, N = 3 and found to be around 2.433 and Critical value from F-distribution table is around 2.99 . And to reject the null hypothesis, Friedman statistic should be greater than the critical value, hence it failed to reject null hypothesis in spite of the significant difference of the seventh algorithm from others. It is very much appreciated if someone suggests the piece I missed or correct my wrong understanding in any part. Thanks in advance :)
