[site]: datascience
[post_id]: 72083
[parent_id]: 
[tags]: 
Best model structure for input coordinates producing full images

I am trying to produce a machine learning model that works as an interpolator of real data. Essentially what would go into the model is an xy coordinate the result would be an image. This is for characterising a camera at the per pixel level. Ive tried feeding in raw xy coordinates and producing an x*y number of images, using a conv net ( unet ) but that didn't converge well. I then tried feeding in full black images with a single xy pixel that was white, but because the output images were mostly black it cheated by making the whole output black. I then tried cropping the output image to be only of the area of interest but again it struggled to produce any real change and by eye just made broadly white images. Is there a standard way of addressing coordinates in image out? Do I just need to train it for a very long time? Would transfer learning help? Is there a better loss function for images than mse ?
