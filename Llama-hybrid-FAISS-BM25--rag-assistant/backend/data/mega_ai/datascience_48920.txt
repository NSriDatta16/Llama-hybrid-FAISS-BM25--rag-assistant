[site]: datascience
[post_id]: 48920
[parent_id]: 
[tags]: 
Struggling to understand GCNNs (Graph Convolutional Neural Networks)

Although I've worked with CNN's for over a year, I am struggling to understand how GCNNs work paper on their simplification . I've read several papers, and I find myself out of my depth when they talk about Chebyshev polynomials or Fourier spaces. The descriptions talk about using an adjacency matrix as input, and perhaps my primary confusion is how I can supply such a matrix to a convolutional neural network (if that is what is in fact what is done). I can't just convolve over the matrix as if it were an image because spatial similarity in the matrix (i.e. rows/cols that are near to each other) doesn't signify actual closeness between nodes in the graph. The question is therefore, how does an adjacency matrix, fit into the framework of CNNs which uses the spatial filters, and the logistic operations (non-linear regressions) in the layered stack/graph. As this GCNN is less mature than CNNs, which have more explanatory material, some further explanation material as to how these methods differ would be helpful. Are there simple situations in which the usage of GCNN can be used to derive results specific to this method/framework?
