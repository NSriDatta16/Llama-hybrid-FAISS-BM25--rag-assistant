[site]: datascience
[post_id]: 67184
[parent_id]: 67183
[tags]: 
Here are two ways to model the problem. The first one is simpler, the second one is more complex but closer to your original statement of the problem. Store as an input feature You can consider the store as a feature to pass to your LSTM. With two different stores, just add a binary input feature "store" where store A is 0 and store B is 1, for example. Then, you can implement it this way. from keras.models import Sequential from keras.layers import LSTM, Dense timesteps = 20 n_features = 5 model = Sequential() # add +1 to n_features for the store identifier model.add(LSTM(32, input_shape=(timesteps,n_features + 1), return_sequences=True)) model.add(LSTM(32)) model.add(Dense(1,activation="relu")) model.compile(optimizer="rmsprop", loss="mse") One sample of your data at a given timestep will be a vector of the form (feature_1, feature_2, ..., feature_n, store). Multivariate time-series prediction Here we input both time series and aim to predict next values of both stores. So you have a shared-LSTM processing store separately, then concatentate both produced embeddings, and compute the predicted values. from keras.models import Model from keras.layers import LSTM, Dense, Concatenate, Input timesteps = 20 n_features = 5 # Store A and B time-series inputs a_inputs = Input(shape=(timesteps, n_features)) b_inputs = Input(shape=(timesteps, n_features)) # Stacked LSTM lstm_1 = LSTM(32, return_sequences=True) lstm_2 = LSTM(32) # Stacked LSTM on Store A a_embedding = lstm_1(a_inputs) a_embedding = lstm_2(a_embedding) # Stacked LSTM on Store B b_embedding = lstm_1(b_inputs) b_embedding = lstm_2(b_embedding) # Concatenate embeddings and define model outputs = Concatenate()([a_embedding, b_embedding]) outputs = Dense(64)(outputs) outputs = Dense(2, activation="relu")(outputs) model = Model([a_inputs, b_inputs], outputs) model.compile(optimizer="rmsprop", loss="mse") Inspired by section 7.1.5 Shared weight sharing, in Deep Learning with Python by F. Chollet
