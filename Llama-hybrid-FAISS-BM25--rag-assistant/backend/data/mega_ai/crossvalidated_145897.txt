[site]: crossvalidated
[post_id]: 145897
[parent_id]: 
[tags]: 
How to extract the function being approximated by a neural network?

We all know that in general, a neural network takes in a set of training examples having the form $\{x, f(x)\}$ and it aims to approximate the function $f$ thereby "classifying" $x$ to its correct output. This not only applies in function approximation but in virtually any domain. Classification problems are essentially function approximation problems where the function classifies an input $x$ appropriately. This means that in general, for any neural network problem the aim is always to approximate the target function $f(x)$. In practice, we rarely care what $f(x)$ really is, so long as the neural network does its job correctly. For example, if we train a neural network to correctly classify between an apple and an orange given a feature vector $x$, we don't really think about the actual function $f$ being approximated in the process. Namely, the function which essentially does all the magic of classifying between an apple and an orange . My question is, are there ways to actually extract this explicit, $f$ approximate; $\hat{f}$? Both in practice and in theory?
