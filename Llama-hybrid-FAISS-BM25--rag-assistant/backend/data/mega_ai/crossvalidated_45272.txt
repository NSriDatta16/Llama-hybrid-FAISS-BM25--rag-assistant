[site]: crossvalidated
[post_id]: 45272
[parent_id]: 
[tags]: 
When are time averages equal to statistical averages?

I have a data set which comprises N measurements. Each measurement is an 8 dimensional vector representing 8 voltages measured from a machine. I want to compute the covariance matrix of this data. Lets assume I have made the data zero mean, hence I want to find $E[V_i*V_j]$ where $V_i$ is the i-th component of a measurement vector. What I did here was to take the time average of $V_i*V_j$ which is $\frac{1}{N} \sum (V_i*V_j)$ over all measurements. I think in effect this is using the ergodic theorem to relate statistical averages to their time averages. What confuses me though is thinking about what this actually means. To me, my measurements at a point in time are not realizations of a random process- rather they are deterministic measurements (the voltages arise from a deterministic signal and let's disregard measurement noise) and so I'm not sure how to make sense of expectations in the random variable sense. Is my reasoning correct?
