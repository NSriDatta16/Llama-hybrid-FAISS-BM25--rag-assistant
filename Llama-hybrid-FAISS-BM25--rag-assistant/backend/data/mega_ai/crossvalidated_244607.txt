[site]: crossvalidated
[post_id]: 244607
[parent_id]: 
[tags]: 
Regression equation as constraint function in an optimization problem?

I have an idea and I want to check if it is already a thing. I have an optimization problem currently solvable using sequential least squares. If it matters, the objective function and constraints are determined according to this paper but I think my idea is more general. Edited to add this paragraph about the original problem. I am fitting parameters of a dynamical model (e.g. a system of ODEs). The paper linked above describes a method of formulating an optimization problem for this purpose. We have a model of a system, an observation of one state variable of the system, and unknown parameters and other state variables. The method proceeds roughly as follows. First, we restate our system of ODEs as a difference equation (e.g. using RK4 or Euler's method). We add to this a synchronization term that drives the model toward the observed trajectory at every time step. So if our original difference equation is $$ \mathbf{y}_{n+1} = \mathbf{y}_n + \mathbf{F}(\mathbf{y}_n, \mathbf{r}) $$ where $\mathbf{y}_n$ is the system state at step $n$ and $\mathbf{r}$ is the unknown parameter vector, adding the synchronization term we have $$ \mathbf{y}_{n+1} = \mathbf{y}_n + \mathbf{F}(\mathbf{y}_n, \mathbf{r}) + u_n(x_n - y_{1,n}) $$ where $u_n$ is a time-varying synchronization constant and $x_n$ is the observation of the first state variable. We then formulate the optimization as follows. The objective function is $$\frac{1}{N}\sum^n ((x_n - y_{1,n})^2 + u_n^2),$$ in other words we are trying to choose the parameters that minimize the necessary amount of synchronization as well as the squared difference between the observed and model trajectories. The difference equation is enforced as constraint functions, and the optimization solution vector is the concatenation of $\mathbf{y}_n$, $\mathbf{r}$, and $u_n$ for all $n$. So we are asking the solver to find the parameter values, synchronization constants, and the associated unobserved states that correspond to a simulation of the system with the minimal necessary synchronization to match the observed trajectory. This method is called Dynamical Parameter Estimation by Abarbanel. I am now trying to add regression-like relationships among the parameter values $\mathbf{r}$. The rest of the original post follows. Let's say my solution-space vector has 20 elements. I hypothesize that 10 of these are linear functions of 10 known values with unknown intercept $\beta_0$ and coefficient $\beta_1$. E.g., a simple regression. I see two ways to modify the optimization problem to include this assumption. Either replace the 10 values in question with $\beta_0$ and $\beta_1$, leaving a 12-D solution space. Of course, modify the constraint and objective functions accordingly. Or, add $\beta_0$ and $\beta_1$ to the solution vector, resulting in a 22-D solution-space, but reduce the degrees of freedom by adding the regression equations as 10 constraint functions. So far so good, I think. So my first question is whether this is already a thing that someone has done. My second question is extending this from simple regression to mixed models. Imagine now that those 10 values are sampled from a normal distribution with unknown mean and variance. Intuitively this is an equivalent reduction in the overall degrees of freedom as the previous example. Instead of $\beta_0$ and $\beta_1$ we now have $\mu$ and $\sigma^2$. However, it doesn't appear to be so simple, because in order to allow the values to vary we must also include the residuals in the solution vector. The best solution I can think of is to add a penalty term to the objective function. In particular, the log likelihood of the normal distribution computed from the 10 values in question. Thus, in addition to the previous objective, the solver would try to maximize the likelihood that the 10 values come from a normal distribution. This method would not actually reduce the overall degrees of freedom but you could think of the added penalty in the objective function as a "soft" constraint. A remaining question is, if this is all feasible, how to do hypothesis testing, i.e., how to decide if the addition of a fixed or random effect improves the resulting solution. But for now I'll stick to the main question: does this already exist? And if not, does it sound feasible? I'm also a bit over my head if it's not apparent so literature suggestions are welcome as well.
