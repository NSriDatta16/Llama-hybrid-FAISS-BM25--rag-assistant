[site]: crossvalidated
[post_id]: 553334
[parent_id]: 553323
[tags]: 
How do loss functions or different loss functions fit into this picture? Do different loss functions result in a different mechanism for calculating likelihood? This is how I read your question: we could fit regression by minimizing different loss functions, how they can be applied to Bayesian regression? Regression, in non-Bayesian setting, can be fitted by minimizing loss or maximizing the likelihood . Both approaches are equivalent, for example, minimizing squared loss is equivalent to maximizing Gaussian likelihood, absolute loss is equivalent to using Laplace distribution for likelihood, etc. Same in Bayesian regression , you are not minimizing the loss to fit the model, but the choice of likelihood function plays a similar role. Same applies to using regularization. In Bayesian setting, regularization is dealt by choosing appropriate priors. For example, $\ell_2$ penalty is equivalent to using Gaussian priors for the parameters .
