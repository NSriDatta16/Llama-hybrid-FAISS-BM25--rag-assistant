[site]: datascience
[post_id]: 75818
[parent_id]: 75814
[tags]: 
So, I believe the question you are asking is how to re-train the model on a single day's worth of data, rather than training on all 180 day's worth of data. It definitely seems reasonable that you have seen overfitting, this can be due to the complexity of the model (related to depth of network). So changing the model's architecture to suit this classification task is definitely a good idea. For this, without knowing your model architecture, I would look at the overall depth (i.e. number of layers in neural network). To clarify, LSTMs (like RNNs [Recurrent Neural Networks]) are typically used for sequential data as data $x_i \in \textbf{x}$ , where $0 ( $N$ is number of data examples in sequence) is fed sequentially in the model to either output at each time step $i$ or later in the input sequence. One suggestion from the top of my mind is, since a RNN is simply an normal NN with a recurring hidden layer, we can use transfer learning (which you spoke about). Transfer learning essentially when we pre-train a model on usually a large quantity of (generic) data and then we take this pre-trained model and alter the output section of the network's architecture to suit the classification task (in your case, add a softmax layer to output a probability distribution over the 2 classes). We then train this altered model with different data (in your case your single day's worth of data). To prevent the model from losing generalisation performance, we typically freeze parameter updates within the pre-trained model's hidden layers and only allow the altered section of the model to update their parameters. This also has the added benefit of reducing time taken to train the model. So, that would be my suggestion, firstly pre-train your model with the 180 day's worth of data, then alter your pre-trained model to suit this task to classify for a day's worth of day, train the model but only update parameters in the altered section of the model.
