[site]: crossvalidated
[post_id]: 147886
[parent_id]: 147866
[tags]: 
I do not really see how this is to work. The Hausman test is designed for comparing the difference between two estimates, where one of the estimators is consistent under $H_1$ and the other one is not. The classical version assumes that the estimator which is inconsistent under $H_1$ is efficient under $H_0$, whereas the one that is also consistent under $H_1$ is not efficient under $H_0$. In so doing, it provides a test of the underlying moment conditions used to identify a parameter of interest, based on the idea that if we observe a large difference, then we are probably faced with a situation in which the underlying $H_0$ is violated, as, if $H_0$ is true, the estimates should be "close" as measured by the $\chi^2$-distribution. Now, 3SLS is "merely" a system estimator that may be more efficient than estimating the coefficients of each equation by 2SLS if the error terms of the equations are indeed correlated. Both use the same moment conditions, though, and hence are (in-)consistent in the same situations. Indeed, it is called 3SLS because it uses preliminary consistent 2SLS estimates to get residuals to estimate the covariance matrix of the errors. One exception one could think about is looking at the coefficients of one equation only. If we get the moment conditions of another equation wrong, that misspecification may "pollute" the entire system of equations when using 3SLS, whereas doing single-equation 2SLS would not be affected. EDIT: the systemfit package provides an implementation of the test I am referring to in the last paragraph.
