[site]: datascience
[post_id]: 122626
[parent_id]: 
[tags]: 
Using different metrics for training vs validation

Is there any theoretical justification for using a different metric on the validation set to do model selection than what was using for training? For example, one can train the model using some type of differentiable loss (i.e. log-loss) so gradient descent can be ran, then do model selection using some other metric (i.e. F1 score or average precision). Is it the general hope that that the training metric is a good proxy for the validation metric, or that doing better on one means doing better on the other one?
