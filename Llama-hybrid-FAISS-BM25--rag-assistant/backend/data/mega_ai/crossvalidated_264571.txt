[site]: crossvalidated
[post_id]: 264571
[parent_id]: 264539
[tags]: 
I don't think the book contains a formal proof, but the reasoning is as follows: Remember that OLS aims to minimize the RSS (Residual Sum of Squares) $$\text{RSS} = \sum_{i=1}^n (Y_i - f(x_i) )^2$$ In the case of classification, this implies the number of wrong classifications. The kNN formula should be interpreted as averaging the categories of a circle with $k$ data-points around a certain $x_i$. If $k=1$ then each data points is categorized correctly which implies $\text{RSS} = 0$. And this is minimal. 'Formal' proof In the case of $k=1$ then the kNN formula implies for each $x_i$ $$\hat Y_i = 1 \sum_{x_j \in N_{1}(x_i)} Y_j = Y_i$$ This implies $$\text{RSS} = \sum_{i=1}^n (Y_i - \hat Y_i) ^2 = 0$$ This seems good enough, since looking at the theorem ' OLS implies $k=1$ ' and proving this by contradiction would result in a $k \not = 1$ as the result of OLS (and having minimal RSS). However above we noticed how $k=1$ has minimal RSS. (OLS results in unique coÃ«fficients)
