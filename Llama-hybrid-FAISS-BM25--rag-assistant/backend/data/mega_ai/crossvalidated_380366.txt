[site]: crossvalidated
[post_id]: 380366
[parent_id]: 41394
[tags]: 
Perhaps one of the most straightforward and common cases where the Bayesian approach is easier is the quantifying the uncertainty of parameters. In this answer, I'm not referring to the interpretation of confidence intervals vs. credible intervals. For the moment, let's assume that a user is fine with using either method. With that said, in the Bayesian framework, it's straight forward; it's the marginal variance of the posterior for any individual parameter of interest. Assuming you can sample from the posterior, then just take your samples and compute your variances. Done! In the Frequentist case, this is usually only straightforward in some cases and it's a real pain when it's not. If we have a large number of samples vs. small number of parameters (and who really knows how large is large enough), we can use MLE theory to derive CI's. However, those criteria don't always hold, especially for interesting cases (i.e., mixed effects models). Sometimes we can use bootstrapping, but sometimes we can't! In the cases we can't, it can be really, really hard to derive error estimates, and often require a bit of cleverness (i.e., Greenwood's formula for deriving SE's for Kaplan Meier curves). "Using some cleverness" is not always a reliable recipe!
