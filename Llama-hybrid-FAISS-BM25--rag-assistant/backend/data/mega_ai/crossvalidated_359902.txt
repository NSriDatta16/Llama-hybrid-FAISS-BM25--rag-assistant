[site]: crossvalidated
[post_id]: 359902
[parent_id]: 359856
[tags]: 
You don't want to want to get in to manipulating the analysis like this to make the results come out more to your liking. It is statistically wrong, and your audience will recognize it as shenanigans. My advice in this case is to not use a p -value or alpha adjustment for multiple tests, and to accept that this means you may have an inflated probability of a type I error among the family of tests. You don't have a tremendous sample size, and this is a kind of preliminary study it seems, so you probably want to err on the side of not missing effects that may be there, even if you may have to accept some false positives as well. Deciding how to manage this kind of tradeoff is really the responsibility of the analyst. I would also advise you to look at the Cohen's d . (I assume that's what in the d column). The effect size for Writing and Translating is rather large. Combined with the reasonably low p -values, this suggests that there is something worth noting in the results of these Tasks. Also consider the the absolute difference in means. For Writing it was 5 points, which seems considerable for these data. The point is to let the data speak. There's no point in stifling it because someone told you are required to make a Bonferroni adjustment. You just must understand what the consequences are of not doing so: You risk an inflated type-I error rate. Sometimes that's okay, and sometimes that's something to be avoided.
