[site]: crossvalidated
[post_id]: 420072
[parent_id]: 
[tags]: 
Two questions about standardization and overfitting

Question 1 Why neural networks (or more generally, any machine learning models) tend to overfit smaller datasets? The "default" reason is that the information associated with the smaller dataset is not sufficient for models like neural network (which has large capacity) to learn. For example, if there are just a few images available, then the parameters of neural network are barely updated and nothing much is learnt from them. Another reason may come statistical learning theory, which shows that some networks 's error grow with order $O(\frac{1}{\sqrt{m}})$ , where $m$ is number of examples. However, I am not quite satisfied with these reasons. The first one is intuitive enough but lacks quantitative reasoning and latter one is limited in particular type of neural network architecture. Update Hopefully the following two resources could provide more context for my question. A guide to choose suitable machine learning model by scikit-learn (original image could be found here ). Based on range of the amount of data available (less than 50, 50 - 100000, etc), it provides different paths to choose suitable models. As much as I know there numbers are largely empirical, I am wondering what is the theory under the hood. The following is a snapshot from Andrew Ng's deep learning course. When amount of data is small, the performance of neural network may not be as good as traditional machine learning algorithms. Question 2 In computer vision tasks, why it is a general practice that images are just scaled to $[0, 1]$ while normalization $x_i \leftarrow \frac{x_i-\mu_i}{\sigma_i}$ is preferable for optimization purposes? ( $x_i$ is individual pixel value at location $i$ ). The only reason I could think of is convenience. Are there deep reasons behind this practice. Say, is there any quantitative rationale that pinpoint their differences? Any input is appreciated. Thank you in advance!
