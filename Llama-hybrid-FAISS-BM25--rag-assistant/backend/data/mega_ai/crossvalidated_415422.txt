[site]: crossvalidated
[post_id]: 415422
[parent_id]: 368949
[tags]: 
It might worth adding another, perhaps more straightforward example to Stephen's excellent answer. Let's consider a medical test, the result of which is normally distributed, both in sick and in healthy people, with different parameters of course (but for simplicity, let's assume homoscedasticity, i.e., that the variance is the same): $$\begin{gather*}T \mid D \ominus \sim \mathcal{N}\left(\mu_{-},\sigma^2\right)\\T \mid D \oplus \sim \mathcal{N}\left(\mu_{+},\sigma^2\right)\end{gather*}.$$ Let's denote the prevalence of the disease with $p$ (i.e. $D\oplus\sim Bern\left(p\right)$ ), so this, together with the above, which are essentially conditional distributions, fully specifies the joint distribution. Thus the confusion matrix with threshold $b$ (i.e., those with test results above $b$ are classified as sick) is $$\begin{pmatrix} & D\oplus & D\ominus\\ T\oplus & p\left(1-\Phi_{+}\left(b\right)\right) & \left(1-p\right)\left(1-\Phi_{-}\left(b\right)\right)\\ T\ominus & p\Phi_{+}\left(b\right) & \left(1-p\right)\Phi_{-}\left(b\right)\\ \end{pmatrix}.$$ Accuracy-based approach The accuracy is $$p\left(1-\Phi_{+}\left(b\right)\right)+\left(1-p\right)\Phi_{-}\left(b\right),$$ we take its derivative w.r.t. $b$ , set it equal to 0, multiply with $\sqrt{1\pi\sigma^2}$ and rearrange a bit: $$\begin{gather*} -p\varphi_{+}\left(b\right)+\varphi_{-}\left(b\right)-p\varphi_{-}\left(b\right)=0\\ e^{-\frac{\left(b-\mu_{-}\right)^2}{2\sigma^2}}\left[\left(1-p\right)-pe^{-\frac{2b\left(\mu_{-}-\mu_{+}\right)+\left(\mu_{+}^2-\mu_{-}^2\right)}{2\sigma^2}}\right]=0\end{gather*}$$ The first term can't be zero, so the only way the product can be zero is if the second term is zero: $$\begin{gather*}\left(1-p\right)-pe^{-\frac{2b\left(\mu_{-}-\mu_{+}\right)+\left(\mu_{+}^2-\mu_{-}^2\right)}{2\sigma^2}}=0\\-\frac{2b\left(\mu_{-}-\mu_{+}\right)+\left(\mu_{+}^2-\mu_{-}^2\right)}{2\sigma^2}=\log\frac{1-p}{p}\\ 2b\left(\mu_{+}-\mu_{-}\right)+\left(\mu_{-}^2-\mu_{+}^2\right)=2\sigma^2\log\frac{1-p}{p}\\ \end{gather*}$$ So the solution is $$b^{\ast}=\frac{\left(\mu_{+}^2-\mu_{-}^2\right)+2\sigma^2\log\frac{1-p}{p}}{2\left(\mu_{+}-\mu_{-}\right)}=\frac{\mu_{+}+\mu_{-}}{2}+\frac{\sigma^2}{\mu_{+}-\mu_{-}}\log\frac{1-p}{p}.$$ Note that this - of course - doesn't depend on the costs. If the classes are balanced, the optimum is the average of the mean test values in sick and healthy people, otherwise it is displaced based on the imbalance. Cost-based approach Using Stephen's notation, the expected overall cost is $$c_{+}^{+}p\left(1-\Phi_{+}\left(b\right)\right) + c_{+}^{-}\left(1-p\right)\left(1-\Phi_{-}\left(b\right)\right) + c_{-}^{+} p\Phi_{+}\left(b\right) + c_{-}^{-} \left(1-p\right)\Phi_{-}\left(b\right).$$ Take its derivate w.r.t $b$ and set it equal to zero: $$\begin{gather*} -c_{+}^{+} p \varphi_{+}\left(b\right)-c_{+}^{-}\left(1-p\right)\varphi_{-}\left(b\right)+c_{-}^{+}p\varphi_{+}\left(b\right)+c_{-}^{-}\left(1-p\right)\varphi_{-}\left(b\right)=\\ =\varphi_{+}\left(b\right)p\left(c_{-}^{+}-c_{+}^{+}\right)+\varphi_{-}\left(b\right)\left(1-p\right)\left(c_{-}^{-}-c_{+}^{-}\right)=\\ = \varphi_{+}\left(b\right)pc_d^{+}-\varphi_{-}\left(b\right)\left(1-p\right)c_d^{-}= 0,\end{gather*}$$ using the notation I introduced in my comments below Stephen's answer, i.e., $c_d^{+}=c_{-}^{+}-c_{+}^{+}$ and $c_d^{-}=c_{+}^{-}-c_{-}^{-}$ . The optimal threshold is therefore given by the solution of the equation $$\boxed{\frac{\varphi_{+}\left(b\right)}{\varphi_{-}\left(b\right)}=\frac{\left(1-p\right)c_d^{-}}{pc_d^{+}}}.$$ Two things should be noted here: This results is totally generic and works for any distribution of the test results, not only normal. ( $\varphi$ in that case of course means the probability density function of the distribution, not the normal density.) Whatever the solution for $b$ is, it is surely a function of $\frac{\left(1-p\right)c_d^{-}}{pc_d^{+}}$ . (I.e., we immediately see how costs matter - in addition to class imbalance!) I'd be really interested to see if this equation has a generic solution for $b$ (parametrized by the $\varphi$ s), but I would be surprised. Nevertheless, we can work it out for normal! $\sqrt{2\pi\sigma^2}$ s cancel on the left hand side, so we have $$\begin{gather*} e^{-\frac{1}{2}\left(\frac{\left(b-\mu_{+}\right)^2}{\sigma^2}-\frac{\left(b-\mu_{-}\right)^2}{\sigma^2}\right)}=\frac{\left(1-p\right)c_d^{-}}{pc_d^{+}} \\ \left(b-\mu_{-}\right)^2-\left(b-\mu_{+}\right)^2 =2\sigma^2 \log \frac{\left(1-p\right)c_d^{-}}{pc_d^{+}} \\ 2b\left(\mu_{+}-\mu_{-}\right)+\left(\mu_{-}^2-\mu_{+}^2\right) =2\sigma^2 \log \frac{\left(1-p\right)c_d^{-}}{pc_d^{+}}\end{gather*}$$ therefore the solution is $$b^{\ast}=\frac{\left(\mu_{+}^2-\mu_{-}^2\right)+2\sigma^2 \log \frac{\left(1-p\right)c_d^{-}}{pc_d^{+}}}{2\left(\mu_{+}-\mu_{-}\right)}=\frac{\mu_{+}+\mu_{-}}{2}+\frac{\sigma^2}{\mu_{+}-\mu_{-}}\log \frac{\left(1-p\right)c_d^{-}}{pc_d^{+}}.$$ (Compare it the the previous result! We see that they are equal if and only if $c_d^{-}=c_d^{+}$ , i.e. the differences in misclassification cost compared to the cost of correct classification is the same in sick and healthy people.) A short demonstration Let's say $c_{-}^{-}=0$ (it is quite natural medically), and that $c_{+}^{+}=1$ (we can always obtain it by dividing the costs with $c_{+}^{+}$ , i.e., by measuring every cost in $c_{+}^{+}$ units). Let's say that the prevalence is $p=0.2$ . Also, let's say that $\mu_{-}=9.5$ , $\mu_{+}=10.5$ and $\sigma=1$ . In this case: library( data.table ) library( lattice ) cminusminus $cost b, muplus, sigma ) ) + res $cplusminus*(1-p)*(1-pnorm( res$ b, muminus, sigma ) ) + res $cminusplus*p*pnorm( res$ b, muplus, sigma ) + cminusminus*(1-p)*pnorm( res$b, muminus, sigma ) xyplot( cost ~ b | factor( cminusplus ), groups = cplusminus, ylim = c( -1, 22 ), data = res, type = "l", xlab = "Threshold", ylab = "Expected overall cost", as.table = TRUE, abline = list( v = (muplus+muminus)/2+ sigma^2/(muplus-muminus)*log((1-p)/p) ), strip = strip.custom( var.name = expression( {"c"^{"+"}}["-"] ), strip.names = c( TRUE, TRUE ) ), auto.key = list( space = "right", points = FALSE, lines = TRUE, title = expression( {"c"^{"-"}}["+"] ) ), panel = panel.superpose, panel.groups = function( x, y, col.line, ... ) { panel.xyplot( x, y, col.line = col.line, ... ) panel.points( x[ which.min( y ) ], min( y ), pch = 19, col = col.line ) } ) The result is (points depict the minimum cost, and the vertical line shows the optimal threshold with the accuracy-based approach): We can very nicely see how cost-based optimum can be different than the accuracy-based optimum. It is instructive to think over why: if it is more costly to classify a sick people erroneously healthy than the other way around ( $c_{-}^{+}$ is high, $c_{+}^{-}$ is low) than the threshold goes down, as we prefer to classify more easily into the category sick, on the other hand, if it is more costly to classify a healthy people erroneously sick than the other way around ( $c_{-}^{+}$ is low, $c_{+}^{-}$ is high) than the threshold goes up, as we prefer to classify more easily into the category healthy. (Check these on the figure!) A real-life example Let's have a look at an empirical example, instead of a theoretical derivation. This example will be different basically from two aspects: Instead of assuming normality, we will simply use the empirical data without any such assumption. Instead of using one single test, and its results in its own units, we will use several tests (and combine them with a logistic regression). Threshold will be given to the final predicted probability. This is actually the preferred approach, see Chapter 19 - Diagnosis - in Frank Harrell's BBR . The dataset ( acath from the package Hmisc ) is from the Duke University Cardiovascular Disease Databank, and contains whether the patient had significant coronary disease, as assessed by cardiac catheterization, this will be our gold standard, i.e., the true disease status, and the "test" will be the combination of the subject's age, sex, cholesterol level and duration of symptoms: library( rms ) library( lattice ) library( latticeExtra ) library( data.table ) getHdata( "acath" ) acath It worth plotting the predicted risks on logit-scale, to see how normal they are (essentially, that was what we assumed previously, with one single test!): densityplot( ~predict( fit ), groups = acath$sigdz, plot.points = FALSE, ref = TRUE, auto.key = list( columns = 2 ) ) Well, they're hardly normal... Let's go on and calculate the expected overall cost: ExpectedOverallCost b, levels = c( FALSE, TRUE ) ), y )*matrix( c( cminusminus, cplusminus, cminusplus, cplusplus ), nc = 2 ) ) } table( predict( fit, type = "fitted" )>0.5, acath$sigdz ) ExpectedOverallCost( 0.5, predict( fit, type = "fitted" ), acath$sigdz, 2, 4 ) And let's plot it for all possible costs (a computational note: we don't need to mindlessly iterate through numbers from 0 to 1, we can perfectly reconstruct the curve by calculating it for all unique values of predicted probabilities): ps We can very well see where we should put the threshold to optimize the expected overall cost (without using sensitivity, specificity or predictive values anywhere!). This is the correct approach. It is especially instructive to contrast these metrics: ExpectedOverallCost2 b, levels = c( FALSE, TRUE ) ), y ) sens $plot.line$ col ) panel.points( x[ which.min( y ) ], min( y ), pch = 19 ) } ) p2 We can now analyze those metrics that are sometimes specifically advertised as being able to come up with an optimal cutoff without costs, and contrast it with our cost-based approach! Let's use the three most often used metrics: Accuracy (maximize accuracy) Youden rule (maximize $Sens+Spec-1$ ) Topleft rule (minimize $\left(1-Sens\right)^2+\left(1-Spec\right)^2$ ) (For simplicity, we will subtract the above values from 1 for the Youden and the Accuracy rule so that we have a minimization problem everywhere.) Let's see the results: p3 This of course pertains to one specific cost structure, $c_{-}^{-}=0$ , $c_{+}^{+}=1$ , $c_{+}^{-}=2$ , $c_{-}^{+}=4$ (this obviously matters only for the optimal cost decision). To investigate the effect of cost structure, let's pick just the optimal threshold (instead of tracing the whole curve), but plot it as a function of costs. More specifically, as we have already seen, the optimal threshold depends on the four costs only through the $c_d^{-}/c_d^{+}$ ratio, so let's plot the optimal cutoff as a function of this, along with the typically used metrics that don't use costs: res2 $OptThreshold rat, function( rat ) ps[ which.min( sapply( ps, Vectorize( ExpectedOverallCost, "b" ), p = predict( fit, type = "fitted" ), y = acath$sigdz, cplusminus = rat, cminusplus = 1, cplusplus = 0 ) ) ] ) xyplot( OptThreshold ~ rat, data = res2, type = "l", ylim = c( -0.1, 1.1 ), xlab = expression( {"c"^{"-"}}["d"]/{"c"^{"+"}}["d"] ), ylab = "Optimal threshold", scales = list( x = list( log = 10, at = c( 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50 ) ) ), panel = function( x, y, resin = res[ ,.( ps[ which.min( value ) ] ), .( variable ) ], ... ) { panel.xyplot( x, y, ... ) panel.abline( h = resin[variable=="Youden"] ) panel.text( log10( 0.02 ), resin[variable=="Youden"], "Y", pos = 3 ) panel.abline( h = resin[variable=="Accuracy"] ) panel.text( log10( 0.02 ), resin[variable=="Accuracy"], "A", pos = 3 ) panel.abline( h = resin[variable=="Topleft"] ) panel.text( log10( 0.02 ), resin[variable=="Topleft"], "TL", pos = 1 ) } ) Horizontal lines indicate the approaches that don't use costs (and are therefore constant). Again, we nicely see that as the additional cost of misclassification in the healthy group rises compared to that of the diseased group, the optimal threshold increases: if we really don't want healthy people to be classified as sick, we will use higher cutoff (and the other way around, of course!). And, finally, we yet again see why those methods that don't use costs are not ( and can't! ) be always optimal.
