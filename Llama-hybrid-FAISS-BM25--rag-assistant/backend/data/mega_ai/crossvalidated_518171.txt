[site]: crossvalidated
[post_id]: 518171
[parent_id]: 
[tags]: 
Is this notation , $\ell(y,f(x;\theta))= -\log p(y|f(x;\theta))$, correct for the negative log probability loss function of a classifier?

In Murphy’s Probabilistic Machine Learning: An Introduction , he states that the loss function for a probabilistic classifier $f(x;\theta)$ is the following: $$\ell(y,f(x;\theta))= -\log p(y \mid f(x;\theta))$$ where $x$ is the input, $y$ the corresponding true output and $\theta$ the parameter vector of the model $f$ . My question is one of notation: Should it not rather be $-\log p(y \mid f,x,\theta )$ ? I feel that the original notation is saying the probability of the true label given the model output $f(x;\theta)$ taking a specific value $f(x;\theta)$ which doesn’t really capture what the “meaning” of this the probability distribution in this loss function (the probability of the true label $y$ of $x$ , given the model choice $f$ , the input $x$ , and the parameters of the model). Am I splitting hairs / missing something?
