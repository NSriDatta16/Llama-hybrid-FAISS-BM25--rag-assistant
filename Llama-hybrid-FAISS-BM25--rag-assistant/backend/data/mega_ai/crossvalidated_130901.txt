[site]: crossvalidated
[post_id]: 130901
[parent_id]: 130882
[tags]: 
I think it might help to pull back from the mathematics and think about the goal of PCA. In my mind, PCA is used to represent large-dimensional data sets (many variables) in the clearest way possible--i.e. the way that reveals as much of the underlying data structure as possible. For an example, let's consider a data set with 3 variables: height, weight and age of people. If you have this data, you could create 3 separate 2-dimensional scatter plots, Height vs. Weight, Weight vs. Age, and Age vs. Height--and if you graphed them, they would all look like 'squares' with base=X-axis and height=Y-axis. But if you suspect there is an interesting relationship among all 3 variables, you would probably want to create a 3-dimensional plot of the data, with X, Y and Z axes--which would create a 3-D 'cube' plot. This plot might reveal an interesting relationship that you would like to communicate to people. But of course, you can't print a 3-dimensional plot, so you have to project the data onto a 2-dimensional piece of paper, which means you have to choose 2 dimensions to prioritize, at the expense of the third (which would be orthogonal to the piece of paper you are printing on). You could try to include the 3rd dimension visually with the use of color-coding or various sized bubbles for the points. Or, you could rotate the plot (in your mind, or with software) until you find a new projection that expresses as much of the 3-D information in 2-D space as possible--think visually of just rotating the 'cube' around until the underlying data relationship you want to show is a clear as possible for printing on paper. As you rotate the 3-D 'cube' through 2-D space, you create new synthetic axes, and these new synthetic axes are orthogonal to each other--and correspond to the length and width of the paper you are printing on. If you travel along these new synthetic axes, you are moving through multiple dimensions of the original data (height, weight and age) at the same time, but you can travel along the new synthetic X-axis (the width of the paper) without moving along the synthetic Y-axis. We can think of this visually, because our brains understand 3-dimensional spaces, but things quickly become problematic if you are talking about higher dimensional data sets. We can't imagine 9-dimensional 'hyper-cubes' (at least I can't), but we often have to deal with data sets that contain many variables. We can use software (or grueling math) to 'rotate' the 9-dimensional data through space, until we find the new projection that represents as much of the higher-dimensional data structure as possible--for printing on a 2-D page. This is exactly what PCA does. Again, for simplicity, consider the earlier 3-D data set example plotted in a 'cube' space--we would see something like a cloud of points. PCA simply rotates that cloud until it finds the 'longest' straight line possible through that cloud--the direction of this line becomes PC1. Then, with PC1 fixed, the data point cloud is rotated again, along PC1, until the next 'longest' orthogonal axis is found, which is PC2. Then you can print a new 2-D plot (PC1 vs. PC2) that captures as much of the 3-D data structure as possible. And of course you can keep going and find PC3, PC4 and so on, if it helps understand the data. Then, the PCA results will tell you how much of the data variance is explained by the new synthetic principal components, and if the PCA axes capture more of the data variance than you would expect to occur by random chance, we can infer that there is a meaningful relationship among the original measured variables. All the discussion about eigenvectors and matrix algebra is a little bit beside the point in my opinion (and also, I'm not that mathematically inclined)--orthogonal axes are just an inherent part of this type of matrix algebra. So, citing the mathematical foundations of orthogonal axes doesn't really explain why we use this approach for PCA. We use this matrix algebra in statistical analysis because it helps us reveal important characteristics of data structures that we are interested in.
