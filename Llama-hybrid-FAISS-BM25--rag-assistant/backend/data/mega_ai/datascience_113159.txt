[site]: datascience
[post_id]: 113159
[parent_id]: 113119
[tags]: 
I found normal to be confusing and my belief is that often times there is no proper solution. I might be wrong, but I will share my thoughts on that. In order to understand the complexity of the issue we should consider all parties involved or potentially affected by that: the model, the variable relationships from data, the samples involved and how they were collected and the domain knowledge. Not all models are equal. Models can differ in multiple ways. Most models assumes something about the regression function which affects the types of hypotheses they can express. Linear models produces hypotheses in the form of linear combinations, for example, others not. Even if the produced hypotheses are identical, the way they arrive at a result might be different. Scaling can affect that in multiple ways. For a linear regression with intercept a standard scaling (subtract mean, divide by sd) will be absorbed into coefficients, if there is an intercept, while a min max scaling might be a disaster since might break further assumptions like heteroskedastic variance. The opposite is also possible since many times we scale variables to meet some assumptions required by the model. This is the case with Box-Cox transformation and others, used often to make the variance heteroskedastic. The method used to fit the model also play a factor. There are models which needs scaling in small number intervals in order to work, like sigmoid gradients in backpropagation. Other times it does have no effect, like monotone transformation on random forests. When we use data to fit a regression function on independent variable we hope that variables are correlated in some way with the target. But connections exists between variables also. Those connections can be destroyed by scaling or can be improved. For example having longitude and latitude, if scaled separately and the sample is not representative we can destroy a lot. On the other hand we might have two plane coordinate variables, one measured in inch and other measured in centimeters, for whatever reason. A scaling into a similar range might help. There are many cases in which a scaling can help or makes the problem worse. The question is how can we know if we have to apply or not some transformations? If we have strong domain expertise and if we have enough information about the collected measurements we might improve the situation. Knowledge about measurement units, measurement errors, relations between variables and connections (rarely clear like physics laws, but some connections could be made, like human heights and weights, and so on) can help to make decisions. But the truths is about the usefulness of those transformations is impossible to assess. My personal approach when I am considering transformations is to find a reason for any kind of transformation. That reason, as a said, can be motivated by model hypothesis, by fitting, by domain knowledge if any, or anything connected with the process. If I find no reason to do feature scaling, I simply do not. Even if I think there is a strong reason, it remains to understand and justify what kind of scaling, which should be the parameters, and ultimately, I am ready to admit that it might be not useful. In general I think applying blindly feature scaling, like any kind of transformation is a really bad habit. I prefer to struggle to understand, before any change.
