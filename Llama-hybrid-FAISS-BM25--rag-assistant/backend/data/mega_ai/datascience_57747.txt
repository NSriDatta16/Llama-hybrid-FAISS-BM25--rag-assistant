[site]: datascience
[post_id]: 57747
[parent_id]: 57743
[tags]: 
I find it really hard to imagine how tree-based boosting works. I think there are two important components: Boosting can be seen as an ensemble method , so in essence it is not one "small" tree, but many (in fact a huge number of) small trees which learn together. During learning, there is feedback on the learning process (" adaptive boosting "). So observations which are "hard to predict", receive a higher weight (more attantion if you like). This allows the ensemble to learn "deeper" than other methods. The xgboost docs come with a nice introduction on boosted trees . Chapter 10 of "Elements of Statistical Learning" covers boosting. It is worth a look. Below is the AdaBoost routine as covered by the book. There are also other boosting methods which do not work with trees. Here is a simple L2 boosting routine implemented in R .
