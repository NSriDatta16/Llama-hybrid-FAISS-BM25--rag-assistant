[site]: crossvalidated
[post_id]: 479106
[parent_id]: 452826
[tags]: 
FGSM was originally designed to analyse the sensitivity of neural networks to small changes at input. It was not designed to generate adversarial examples . By taking the sign of the gradient for each pixel the authors could modify the entire image with uniform magnitude, but in specific gradient directions. Then they could observe how much modification was required to change an image's class. A good adversarial attack is the L2 attack from Carlini & Wagner. It uses the entire gradient from pre softmax logits to modify the original input, with additional distance constraints.
