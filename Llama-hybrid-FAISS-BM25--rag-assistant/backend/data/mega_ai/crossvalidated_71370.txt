[site]: crossvalidated
[post_id]: 71370
[parent_id]: 71357
[tags]: 
There is at least one case where people do use the mean of the bootstrap distribution: bagging (short for bootstrap aggregating ). The basic idea is that if your estimator is very sensitive to perturbations in the data (i.e., the estimator has high variance and low bias), then you can average over lots of bootstrap samples to reduce the amount of overfitting particular examples. The page I linked to points out that this introduces some bias into your estimate, which is why the sample mean will often make more sense than averaging your bootstrap samples. But if you have something like a decision tree or a nearest neighbor classifier that can change radically in response to small changes in the data, then this bias might not be as big a concern as overfitting.
