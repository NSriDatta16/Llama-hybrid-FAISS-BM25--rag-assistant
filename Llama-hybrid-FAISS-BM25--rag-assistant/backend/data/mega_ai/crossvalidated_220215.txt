[site]: crossvalidated
[post_id]: 220215
[parent_id]: 
[tags]: 
Big difference of accuracy between C-SVC and nu-SVC using SVM

I'm currently dealing with an image classification problems. The objective is to classify the images to 4 classes, with 8000 images in the training set and 14000 images to predict. I'm using the SVM implemented in the kernlab package in R, and there is a huge difference on accuracy between C-SVC and nu-SVC. The kernel is rbf, and I'm using all the default parameters. Using a 5-fold cross validation, the cross validation error for C-SVC is as high as 49%, and for nu-SVC it's 27%. What's even more strange is that the training error(no cross validation) of C-SVC is always around 56%, but for nu-SVC it can go down as low as 5%. So what's strange is: Why the training error of C-SVC can bi higher than it's cross validation error rate? Why the error rate of C-SVC can be so much higher than that of nu-SVC? (it seems that adjusting parameters of C-SVC doesn't help much) And besides, C-SVC and nu-SVC seems to be the same thing mathematically? EDIT 1 I've changed from doing a 4-class classification to doing four 0-1 classification problems, and C-SVC works well. The error rates are almost the same as those of nu-SVC. So maybe the problem has something to do with overfitting and multi-level classification?
