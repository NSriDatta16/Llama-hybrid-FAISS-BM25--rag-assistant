[site]: crossvalidated
[post_id]: 74286
[parent_id]: 
[tags]: 
What method is simulating pvalues from re sampling from the data

A while back I asked a question about correlating times between time stamps and received a response from Peter Ellis that said I could calculate mean distances between codes... This already will give you some sense of which behaviours are clustered together, but you also should check that this isn't plausibly due just to chance. To check that, I would create simulated data generated by a model under the null hypothesis of no relation. Doing this would require generating data for each behaviour's time from a plausible null model, probably based on resampling the times between each event (eg between each yawn) to create a new set of time stamps for hypothetical null model events. Then calculate the same indicator statistic for this null model and compare to the indicator from your genuine data. By repeating this simulation a number of times, you could find out whether the indicator from your data is sufficiently different from the null model's simulated data (smaller average time from each yawn to the nearest stretch, for example) to count as statistically significant evidence against your null hypothesis. I finally possess the skill set to do this and have done so in R but I don't know what this method or technique is called so that I can (a) learn more about it (b) speak intelligently about the theory behind what I'm doing. Some people have suggested this is called a permutation test, others say similar to but not the same as bootstrapping and some have told me it's related to Monte Carlo re sampling. What is this method of resampling, given the NULL is TRUE, called? If you have a reference or two to back up your response that may be helpful but not necessary.
