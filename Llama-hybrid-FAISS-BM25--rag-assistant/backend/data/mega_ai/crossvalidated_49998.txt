[site]: crossvalidated
[post_id]: 49998
[parent_id]: 49993
[tags]: 
In general, gradient based techniques for optimizing neural networks are more specific and optimized for the task than the two generic optimization algorithms you mention, which don't require a gradient. Geoff Hinton mentioned evolution based approaches to optimizing neural networks in his slides on deep learning. He says that they don't really work, and they scale poorly to networks that have many weights. Using the gradient to optimize helps immensely to do efficient training. Successful approaches to training deep neural networks have gone in the direction of approximating the second derivative of the objective function. I am very skeptical that general optimization procedures that don't know about the structure of the neural nets are going to have much success.
