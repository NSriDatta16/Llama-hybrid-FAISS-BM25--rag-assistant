[site]: crossvalidated
[post_id]: 247368
[parent_id]: 
[tags]: 
Evaluating/Tuning hyper parameters of a neural network regression model

I have a fairly reasonable understanding of the theory behind neural networks, regularization, and cross-validation, but I am lacking in the actual experience department. In a nutshell, I am using a neural network regression model to make predictions of prices based off of several variables, most of them being categorical (34 categorical, 4 continuous). Using about 4000 examples. I am using scikit-learn to do this analysis. When it comes time for me to tune my hyperparameters, the "best" score I get from using GridSearchCV doesn't necessarily give me the "best" score from scoring the Neural Network. I assume this is okay because it is doing cross-validation and the bias will be slightly off. Am I correct in assuming I should listen to what GridSearchCV tells me are the "optimal" hyperparameter tunings? How much should I rely on the r^2 as an evaluation of my neural network? Higher is better, but is there anything more to it than that? Are there any graphs I can use to help me make some judgment? Thank you all, and let me know if I am leaving out any pertinent information!
