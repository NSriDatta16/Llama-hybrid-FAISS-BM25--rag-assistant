[site]: crossvalidated
[post_id]: 469291
[parent_id]: 468942
[tags]: 
As it's been pointed out already, to see whether a model suffers from overfitting one needs to choose a cross-validation scheme and compare training and test errors. For Random Forests you have two options. Keep a subset of your data (usually 20%) aside, and train your model on the remaining set. Afterwards, you test the performance on your training and test data and compare these two numbers. This is applicable to any ML-model. Alternatively, in Random Forests one can use Out Of the Bag (OOB) error estimation. As you might well know, Bootstraping is implemented in each tree in Random Forests, i.e. whenever a tree is built, a random subset of the training data is left aside (i.e. not considered in training). This is called an OOB subset. Using the OOB of each tree one can obtain the OOB error estimation. In modern ML API's like Sklearn, obtaining the OOB error estimation is rather straightforward. Assuming the numbers of trees is big enough, this can be shown to be equivalent to Leave-One-Out cross validation. (See An Introduction to Statistical Learning by Gareth James et. al.).
