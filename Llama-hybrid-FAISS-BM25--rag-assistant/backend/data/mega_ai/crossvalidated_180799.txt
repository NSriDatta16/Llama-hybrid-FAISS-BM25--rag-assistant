[site]: crossvalidated
[post_id]: 180799
[parent_id]: 177646
[tags]: 
Yes, it is dimensionality reduction, form your input dimension to number of your hidden networks. This is used in autoencoders, also you can throw away your output layer, and switch it for some other classifier, therefore using your NN as supervised feature learning unit. Visualizing is however tricky, because feature representations are distributed across the network and they don't have spatial properties like SOM or more understandable dim. reduction like PCA. They are however way's how to visualize what are they doing, or extract decision rules.
