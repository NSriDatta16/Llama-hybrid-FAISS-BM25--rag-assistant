[site]: crossvalidated
[post_id]: 28304
[parent_id]: 
[tags]: 
The odds ratio is a measure of association between two binary variables. If $X_1$ and $X_2$ are two conditions and the outcome of interest is coded as $1$, then $p(1|X_1)$ is the probability of that outcome occurring in the first condition. The odds ratio is equal to: $$ {\rm OR} = \frac{ \frac{p(1|X_1)}{(1-p(1|X_1))} }{ \frac{p(1|X_2)}{(1-p(1|X_2))} } $$ or, $$ {\rm OR} = \frac{p(1|X_1)\quad (1-p(1|X_2))}{(1-p(1|X_1))\quad p(1|X_2)\quad} $$ The OR ranges $(0, +\infty)$. When ${\rm OR} 1$ indicates a positive relationship. When the probabilities are exactly equal, $OR = 1$. Due to this asymmetry, the odds ratio is sometimes difficult for people to interpret and they take the natural log of the odds ratio. In that case, a positive log odds ratio implies the event is more likely in the first condition, a negative log odds ratio implies the even is less likely in the first condition, and $\ln(OR) = 0$ implies equality. The odds ratio (and especially the log odds ratio) has a strong connection to logistic regression. The fitted coefficients for continuous variables in a logistic regression model are the log odds ratios associated with a one unit change in the variable in question. (The intercept is the log of the odds [not ratio] of the event when all the variables are $0$.)
