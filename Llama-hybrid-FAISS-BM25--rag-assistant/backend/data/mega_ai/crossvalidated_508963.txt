[site]: crossvalidated
[post_id]: 508963
[parent_id]: 506667
[tags]: 
I realize I read it too quickly and mis-interpreted the question. I think the top answer already addressed the more specific question adequately, but yeah - typically the best observed point is used because there is no surety that the predicted GP maximum is higher than the best current value. It may be, but you know the high performance at the best point you've observed. Going with somewhere you haven't observed yet runs the risk of the GP being inaccurately high and ended up with poorer performance. If you have the computational time, you could always do one last greedy step - though a new GP conditioned on that as well may have a new predicted maximum. In general I think the reason to not use the predicted max is out of precaution against the GP just being off. Old answer with mis-interpreted question: For Bayesian optimization, using the prediction function in a "greedy" sense - next evaluating at the maximum predicted value - can lead to poor optimization performance through inadequate exploration. Part of the value of the Bayesian optimization approach is that it balances attempting to exploit regions of high predicted performance and explore regions of high uncertainty. A model can become stuck in a local optimum if it predicts high performance there and then only searches near there. "A Taxonomy of Global Optimization Methods Based on Response Surfaces", Jones (2001) does an exploration of approaches including comparison of greedy selection with expected improvement and some other approaches and finds the greedy methods can suffer from the issues described above.
