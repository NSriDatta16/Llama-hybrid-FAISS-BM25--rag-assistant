[site]: crossvalidated
[post_id]: 582653
[parent_id]: 582580
[tags]: 
Processes that reduce the magnitude of coefficients are called "regularization". Normally, though, you decide what regularization to do before performing the regression, rather than looking at your regression first, and then tailoring the regularization to a particular goal for a coefficient. Regularization also usually is applied equally to all the coefficients, but it doesn't have to be (and if you don't standardize your variables, then your choice of units can make a nominatively symmetric regularization have an asymmetric effect). If you have reason to think that one of your variables is less likely to be significant, it is valid to use a different regularization hyperparameter for that variable. Regularization is equivalent to a Bayesian prior, the solution mkt brought up, but you'll probably get more useful results looking up how to do regularization than Bayesian priors.
