[site]: stackoverflow
[post_id]: 4260851
[parent_id]: 
[tags]: 
Django ORM: Organizing massive amounts of data, the right way

I have a Django app that uses django-piston to send out XML feeds to internal clients. Generally, these work pretty well but we have some XML feeds that currently run over 15 minutes long. This causes timeouts, and the feeds become unreliable. I'm trying to ponder ways that I can improve this setup. If it requires some re-structuring of the data, that could be possible too. Here is how the data collection currently looks: class Data(models.Model) # fields class MetadataItem(models.Model) data = models.ForeignKey(Data) # handlers.py data = Data.objects.filter(**kwargs) for d in data: for metaitem in d.metadataitem_set.all(): # There is usually anywhere between 55 - 95 entries in this loop label = metaitem.get_label() # does some formatting here data_metadata[label] = metaitem.body Obviously, the core of the program is doing much more, but I'm just pointing out where the problem lies. When we have a data list of 300 it just becomes unreliable and times out. What I've tried: Getting a collection of all the data id's, then doing a single large query to get all the MetadataItem 's. Finally, filtering those in my loop. This was to preserve some queries which it did reduce. Using .values() to reduce model instance overhead, which did speed it up but not by much. One idea I'm thinking one simpler solution to this is to write to a cache in steps. So to reduce time out; I would write the first 50 data sets, save to cache, adjust some counter, write the next 50, etc. Still need to ponder this. Hoping someone can help lead me into the right direction with this.
