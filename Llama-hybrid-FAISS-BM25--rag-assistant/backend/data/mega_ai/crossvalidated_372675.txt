[site]: crossvalidated
[post_id]: 372675
[parent_id]: 
[tags]: 
Optimizing Bayesian estimation as a function of a parameter

GENERAL QUESTION Imagine a system with a statistical variable $X$ and a parameter $\theta$ that we want to determine. Following the standard Bayesian estimation, a sequence of $N$ measurements will be taken, getting the values $\{ x_1, \dots, x_N \}$ . Then, the conditional probability $ P( \theta | x_N,x_{N-1},\dots,x_1 ) $ can be determined as follows: \begin{align} P( \theta | x_N,x_{N-1},\dots,x_1 ) = \frac{1}{\mathcal{N}} \prod_{i=1}^N P( x_i | \theta ) P_0(\theta) \, . \qquad \qquad \mathrm{(1)} \end{align} Here, $ P_0(\theta) $ is a prior distribution of the parameter, $\mathcal{N}$ is a normalization constant and $ P( x_i | \theta ) $ is known. So far, this is just the application of Bayes' theorem $N$ times. But now, let $\tau$ encode an extra degree of freedom that appears in $ P( x | \theta ) \equiv P_{\tau}( x | \theta )$ . $\tau$ is not a property of the system under study, but just a variable whose value $\tau_i$ we can choose at each measurement $i$ . In that case, Eq. (1) turns into: \begin{align} P( \theta | x_N,x_{N-1},\dots,x_1 ) = \frac{1}{\mathcal{N}} \prod_{i=1}^N P_{\tau_i} ( x_i | \theta ) P_0(\theta) \, . \qquad \qquad \mathrm{(2)} \end{align} Question: How can Bayesian inference, Eq. (2), be optimized as a function of $\tau$ ? In other words, is there an optimal choice of $ \{ \tau_1, \dots, \tau_N \}$ for the estimation of $\theta$ ? EXAMPLE This example is a very unrealistic coin-flip experiment, but it can serve to illustrate the question. Let the following rules apply: A coin is tossed $N$ times. The coin is always tossed with the same angular velocity $\theta$ and from the same position (e.g., heads). The angle $\phi$ as a function of time $t$ is given by $ \phi(t) = \theta t $ , see the outline: The coin turns for a time $\tau$ before it hits the ground. We can choose this time $\tau_i$ at each toss $i$ by tossing the coin from the appropriate height. When the coin hits the ground, the probability that it is heads ( $x = 1$ ) or tails ( $x = -1$ ) is given by the following function: \begin{align} P_{\tau}(x|\theta) = \frac{1}{2} \left[ 1 + x \cos( \theta \tau ) \right] \, . \end{align} For example, if the coin hits the ground at an angle $\phi(\tau) = \theta \tau = 2 m \pi$ , $m \in \mathcal{Z}$ (namely in the heads position), the result will be heads. $\phi(\tau) = (2m+1)\pi$ , $m \in \mathcal{Z}$ (getting to the floor as tails) will always yield tails, whereas $\phi(\tau) = (2m+1)\pi/2$ , $m\in\mathcal{Z}$ (that is, the coin hitting the ground with the edge) yields a $50\%$ chance of getting heads or tails. Assume the prior distribution $P_0(\theta)$ to be a Gaussian. In this context, the question is: how should I choose the times $\{\tau_1,\dots,\tau_N\}$ (equivalently, the heights from which the coin is tossed) so that I can estimate the angular frequency $\theta$ better?
