[site]: crossvalidated
[post_id]: 624239
[parent_id]: 
[tags]: 
Why does Kernel PCA works with validation data?

Assume that you have a matrix $X$ and you want to do Principal Component Analysis on that data. But the data contains nonlinearities, so you decided to use Kernel Principal Component Analysis instead. First you take your data and turn it into a kernel. K = exp(-gamma*D.^2) Where $D$ is the L2-norm distance of $X$ matrix. Then you perform regular PCA onto $K$ and find its components $W$ , e.g eigenvectors. Now, you can multiply $W$ with $X$ and get the projection, even if $K$ is used with PCA. Question: Why does this work? I mean, I don't need to take a kernel onto $X$ before multiplying with $W$ to get the projection.
