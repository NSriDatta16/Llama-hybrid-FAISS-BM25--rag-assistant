[site]: crossvalidated
[post_id]: 186945
[parent_id]: 
[tags]: 
What do you do when your test step has dissapointing results?

I'm working with a Random Forest classifier and wanting to wrap my head around the train-validate-test cycle. So, as far as I understand the process of training is as such: 1) Train with training data set 2) Validate: run classifier on separate set of ground truth and fiddle with parameters until desirable results are achieved 3) Test: run classifier on yet another set of ground truth to determine a more realistic measure of accuracy of your classifier. If I'm off the mark please correct me! So, here is where my question arises. If after the test phase the error is still higher than desired then what is the next course of action? It seems like if you redid the process with the same datasets but different parameters until the results became desirable, then you would just be creating the same issue that would have been the case if you didn't use a final test phase--which is that you would be choosing the best case scenario rather than the most likely. I get in theory the separation of validation and test phases, but I don't really see how the problem is mitigated if after the test phase the results are still bad.
