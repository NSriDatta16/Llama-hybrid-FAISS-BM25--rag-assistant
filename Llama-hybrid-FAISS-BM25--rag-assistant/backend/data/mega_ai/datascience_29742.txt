[site]: datascience
[post_id]: 29742
[parent_id]: 22163
[tags]: 
If your weights are diverging then your optimizer or your gradients aren't behaving well. A common reason for diverging weights is exploding gradients , which can result from: too many layers, or too many recurrent cycles if you're using an RNN. You can verify if you have exploding gradients as follows: grad_magnitude = tf.reduce_sum([tf.reduce_sum(g**2) for g in tf.gradients(loss, weights_list)])**0.5 Some approaches to solving the problem of exploding gradients are: Use RELU or ELU activations Use Xavier initialization Use a Deep Residual architecture . This will keep the gradients from being squished by subsequent layers.
