[site]: crossvalidated
[post_id]: 413888
[parent_id]: 
[tags]: 
Reinforcement Learning: A2C agent does not learn

I am trying to implement an A2C algorithm, but for some reasons, my agent does not learn very well. I build a custom environment using Unity ML Agents . The environment is very simple: an agent can control a car in a top-down fashion. Here is an example of me playing that environment: An agent can take 3 actions: turn left, turn right or do not turn at all. An agent can not control the throttle. On each frame an agent receives a reward equals to a distance traveled along the center-line of the road. Episode finishes if the center of the car is off the road, in this case the agent receives a -1 reward. To give an agent a sense of motion a single transition consist of 5 frames stacked together. Here is a sample of a single transition: I used tensorflow and here is how I implemented the computation: X = tf.placeholder(tf.float32, [None, 128, 128, 3]) A = tf.placeholder(tf.int32, [None]) ADV = tf.placeholder(tf.float32, [None]) R = tf.placeholder(tf.float32, [None]) h1 = conv(X, 32, 8, 4) pool1 = maxpool(h1, 2, 2) h2 = conv(pool1, 64, 4, 2) pool2 = maxpool(h2, 2, 2) h3 = conv(pool2, 64, 3, 1) h3 = tf.layers.flatten(h3) h4 = fc(h3, 512) actor = fc(h4, NUMBER_OF_ACTIONS, act=None) critic = fc(h4, 1, act=None) v0 = tf.squeeze(critic) prob = tf.nn.softmax(actor) dist = tf.distributions.Categorical(logits=prob) a0 = tf.squeeze(dist.sample()) value_loss = tf.reduce_mean(tf.square(tf.squeeze(critic) - R)) action_one_hot = tf.one_hot(A, NUMBER_OF_ACTIONS, dtype=tf.float32) neg_log_prob = -tf.log(prob) policy_loss = tf.reduce_mean( tf.reduce_sum(neg_log_prob * action_one_hot, axis=1) * ADV) entropy = tf.reduce_mean(tf.reduce_sum(prob * neg_log_prob, axis=1)) loss = policy_loss + value_loss * VALUE_LOSS_K - entropy * ENTROPY_K adam = tf.train.AdamOptimizer(LR).minimize(loss) I trained an agent for a 20 hours on google cloud using GPU and apparently it does not make much progress. In terms of hyperparameters, I am using learning rate ${\alpha} = 0.0001$ and discount factor ${\gamma} = 0.999$ Some metrics, which I log: Here is the highest reward episode an agent had during the training (it looks rather disappointing): I am not sure what I did wrong and thus it is hard to frame the question. Is there something wrong with my approach or implementation ? Any help or suggestions would be greatly appreciated.
