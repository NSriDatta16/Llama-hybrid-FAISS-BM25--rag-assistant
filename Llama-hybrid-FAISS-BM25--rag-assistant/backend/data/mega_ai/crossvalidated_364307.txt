[site]: crossvalidated
[post_id]: 364307
[parent_id]: 364298
[tags]: 
There are a couple ways to implement random forest classifiers. Suppose $x$ is some data point and there are $k$ classes. Method 1: Each tree predicts the class of $x$ according to the leaf node $x$ falls within. The leaf node output is the majority class of the training points it contains. The predictions of all trees are considered as votes, and the class with the most votes is taken as the output of the forest. This is the original formulation of random forests proposed by Breiman (2001). Method 2: Each tree outputs a vector $[p_1, \dots, p_k]$ representing the predicted probability of each class given $x$. This may be estimated as the relative class frequencies of training points in the leaf node $x$ falls within. The forest output is the average of these vectors across trees, representing a conditional distribution over classes given $x$. Method 2 is nice because it gives probabilistic predictions. But, for some problems, further steps may be needed to ensure that they're well calibrated. For example, see: Niculescu-Mizil and Caruana (2005). Predicting good probabilities with supervised learning.
