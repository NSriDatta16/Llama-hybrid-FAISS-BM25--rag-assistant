[site]: crossvalidated
[post_id]: 369171
[parent_id]: 369164
[tags]: 
I think there's a piece of information missing here. What are you meant to do with your sample of size $n=100$ ? Are you meant to find its mean? As far as I can tell, the question could be asking one of two things. 1: Calculate the expected value, and variance of the sample mean. The expected mean of your sample is the mean of the distribution you are sampling from. Any individual sample's mean will probably be different to the mean of the distribution (unless the sample is very big, then it asymptotes to the distribution mean), but on average, if you took many samples of size $n=100$ and took their mean, the average of these means would tend to the mean of the distribution. Likewise, it's fairly easy to prove that the variance of the sample mean is given by $\frac{\sigma ^{2}}{N}$ , i.e. the variance of the underlying distribution divided by the sample size. 2: What is the theoretically expected mean and variance of the sample? Again, the expected sample mean can easily be shown to be the distribution mean. The expected sample variance can be (somewhat more cumbersomely) to be shown to equal $\sigma^{2}(\frac{N-1}{N})$ in which $\sigma ^{2}$ is the distribution variance. In your case, this means you'd expect your sample variance, on average to be 0.99 as large as the distribution variance. Especially for small samples, one expects the sample variance to be smaller than the variance of the underlying distribution which generated the sample. Edit (as you have clarified your original question). If you estimate the distributional mean with the sample mean, the expected value of the sample mean is the distribution mean (for more info on this: https://en.wikipedia.org/wiki/Gamma_distribution ). The variance on this estimator, i.e., how far from its mean value (which is the same as the true value), this estimator will fall on average, can be shown to be $\frac{\sigma ^{2}}{n}$ . This is related to what I said originally, the variance of the sample mean is given by $\frac{\sigma ^{2}}{n}$ . The variance of the sample mean is a measure of how far the sample mean on average falls from its average value, but because the sample means's average value is the distribution mean, then the variance of the sample mean is also a measure of spread of the sample mean from the true mean, otherwise known as the standard error.
