[site]: crossvalidated
[post_id]: 90139
[parent_id]: 90134
[tags]: 
What about optimization? Let's see if I understand you correctly. You have a model $p(y|x, \theta)$ conditioned on some observation $x$ and a set of parameters $\theta$ and a prior $p(\theta)$ leading to a joint likelihood of $\mathcal{L} = p(y|x, \theta)p(\theta)$ . The parameters are distributed according to a known multivariate normal, i.e. $\theta \sim \mathcal{N}(\mu, \Sigma)$ . You want to find the MAP solution to this problem, i.e. $$ \text{argmax}_{\theta} \mathcal{L}. $$ A special case of this problem is well studied in the neural networks community, known as weight decay. In that case, $\mu=\mathbf{0}$ and $\Sigma = \mathbf{I}\sigma^2$ . As you already noted, the trick is that $\text{argmax}_{\theta} \mathcal{L} = \text{argmax}_{\theta} \log \mathcal{L}$ . When you take the log of the Gaussian density, many ugly terms (the exponential) vanish and you will end up with sth like $\log p(\theta) = -{1 \over 2}(\theta - \mu)^T\Sigma^{-1}(\theta - \mu) + \text{const}$ . If you differentiate that, Sam Roweis' matrix identities will come in handy and let you arrive at $$ -{1 \over 2}{\partial (\theta - \mu)^T\Sigma^{-1}(\theta - \mu) \over \partial \theta} = -\Sigma^{-1}(\theta - \mu). $$ (Please verify, this was done quickly and in my head.) Together with the derivatives of your model, you can use off-the-shelf optimizers to arrive at a MAP solution. Update : Incorporated comment by David J. Harris. Formulas should be correct now.
