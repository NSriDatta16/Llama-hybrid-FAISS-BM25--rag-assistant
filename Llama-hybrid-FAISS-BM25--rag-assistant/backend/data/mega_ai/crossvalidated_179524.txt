[site]: crossvalidated
[post_id]: 179524
[parent_id]: 177339
[tags]: 
I'll try to keep it as generic as possible. You might have continuous, categorical or binary values. If you have categorical values, let's say x1 takes values such as (Monday, Tuesday, Wednesday, etc) then you need to process and encode them somehow. For the example above it's typical to encode monday as [1,0,0,0,0,0,0]. Tuesday would be encoded as [0,1,0,0,0,0,0] and so on. You need to convert everything to numbers. This page from scikit-learn explains it: http://scikit-learn.org/stable/modules/preprocessing.html Feature transformation could then be: Preprocessing -- This is typically: Scaling, Standardizing (z-score) or Normalization. If you're not allowed to look at the data, I'd go with Normalization since it's a procedure that you can perform on streaming data immediately. This guarantees that none of your features will be greater than one or less than zero. You now have some bounds on each feature. This might be your answer, although I'll go further just in case. Dimensionality reduction -- http://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition These methods project the data onto lower dimensions -- helpful with high dimensional data (e.g. think of is as compression, getting rid of non-essential information). Again, for streaming data you might need something like the Incremental PCA. Another option (vs PCA) is the RBM http://scikit-learn.org/stable/modules/neural_networks.html#neural-network Either way, you have no guarantee that you will end up with an optimal solution since you're not taking the y 's (regressor) into consideration. Feature selection -- http://scikit-learn.org/stable/modules/feature_selection.html#feature-selection Several options are (but not limited to): L2 (Ridge regression) L1 (Lasso) and L1+L2 ElasticNet. If you use the L1 norm to penalize then your solutions will be sparse (e.g. you get rid of some features completely, some weights are zero). If you use L2, then the algorithm adjusts the importance (weight) of each feature. You don't have to use linear models (e.g. f(x) = x_1 + x_2 + ... x_n), you can choose whatever combination of features you want, e.g. (x_1^2 + x_2*x_1 + x_2 + etc). Then the regularization is performed on this model. Or you could just use the random forests and you get this for free. Generic answer without knowing what the data 'means' and not making any assumptions about it's distribution: encode non-numerical values to numbers, normalize (unit length) and use a regularized regression model.
