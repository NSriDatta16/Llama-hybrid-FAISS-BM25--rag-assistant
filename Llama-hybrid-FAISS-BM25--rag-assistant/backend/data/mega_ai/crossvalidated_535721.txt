[site]: crossvalidated
[post_id]: 535721
[parent_id]: 240305
[tags]: 
For transformers I think you should do it like this: According to the original paper ( https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf ) they say: Residual Dropout We apply dropout [27] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of P_drop = 0.1. which makes me think they do the following: assert SubLayer is FullyConnected or MultiHeadedSeltAttention (not the output of LN+Add) x = SubLayer(x) x = torch.nn.dropout(x, p=0.1) x = nn.LayerNorm(x) + x So right after the multiheaded attention or fully connected (before the LN+ADD) during the transformer blocks/stack. But for the input just before the input to the actual encoder stack - i.e. together with the table look up embeddings. batch_token_seqs: list[list[tokens]] = tokenize(batch_of_tokens) batch_embeddings: torch.Tensor = table_look_up(batch_tokens) * D**0.5 # note this usually outputs masks, one for the right shift no cheating another for padding. D**0.5 is there for completeness, paper mentions it but doesn't justify it. It's not the same as the MHA division. batch_embeddings = batch_embeddings + pos_embedding batch_embeddings = self.drop_out(batch_embeddings)
