[site]: datascience
[post_id]: 32395
[parent_id]: 32355
[tags]: 
The point is that we can not use test data to choose the best model or give weights to our set of models because test data must be used for the finsl evaluation of our machine learning algorithm. Thus, we have two starategies: 1- Keeping a separate validation data and choose the best model or or give weights to our set of models according to their performance on validation data. This is not the best action because our training data set usually does not include the whole data space, so when we see the new test data which is far from the training set, our model fails. So, our model lacks generalization and is overfitted to validation data and has high variance in terms of expected loss. 2- Training multiple models (trees) and average the results of them which is called bagging. This effectively reduces the variance of the model. In case of random forest which includes random selection of features, the variance is further reduced. This techniques lead to better model generalization (performance on test data). To cut the long story short, if we choose the best model according to its performance on validation data, we run the risk of over-fitting on validation data. To prevent this problem, we should train multiple models and because we do not know anything about the distribution of test data, we should give equal weigth (priority) to these models while averaging them.
