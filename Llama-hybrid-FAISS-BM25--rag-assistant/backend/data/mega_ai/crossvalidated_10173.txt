[site]: crossvalidated
[post_id]: 10173
[parent_id]: 10169
[tags]: 
I would approach this as a hypothesis test (but then I am a Bayesian, and we always tend to do things a bit differently). I take it by homogeneous that you mean homogeneous in the "way" which did not have its totals fixed. If we index the cell counts as $n_{ijk}$ for $i=1,\dots,I$ for the first way, and $j=1,\dots,J$ for the second way, and $k=1,\dots,K$ for the third way. This gives a total of $IJK$ cells in your contingency table. The "saturated model" is to assume that each cell is a multinomial distributed variable, with an individual long run frequency, so $$p(n_{111},\dots,n_{IJK}|\theta_{111},\dots,\theta_{IJK})=n_{\bullet\bullet\bullet}!\prod_{i=1}^{I}\prod_{j=1}^{J}\prod_{k=1}^{K}\frac{\theta_{ijk}^{n_{ijk}}}{n_{ijk}!}\propto\prod_{i=1}^{I}\prod_{j=1}^{J}\prod_{k=1}^{K}\theta_{ijk}^{n_{ijk}}$$ If a row total had been fixed, then we would get: $$p(n_{111},\dots,n_{IJK}|\theta_{111},\dots,\theta_{IJK})=\prod_{i=1}^{I}n_{i\bullet\bullet}!\prod_{j=1}^{J}\prod_{k=1}^{K}\frac{\theta_{ijk}^{n_{ijk}}}{n_{ijk}!}\propto\prod_{i=1}^{I}\prod_{j=1}^{J}\prod_{k=1}^{K}\theta_{ijk}^{n_{ijk}}$$ If a row and column totals had been fixed, then we would get: $$p(n_{111},\dots,n_{IJK}|\theta_{111},\dots,\theta_{IJK})=\prod_{i=1}^{I}n_{i\bullet\bullet}!\prod_{j=1}^{J}n_{\bullet j\bullet}!\prod_{k=1}^{K}\frac{\theta_{ijk}^{n_{ijk}}}{n_{ijk}!}\propto\prod_{i=1}^{I}\prod_{j=1}^{J}\prod_{k=1}^{K}\theta_{ijk}^{n_{ijk}}$$ This shows that whether the row totals are fixed or random, is not relevant to inference about the $\theta_{ijk}$ parameters - as this will simply change the factorial in the denominator. This is called the "likelihood principle". However it is relevant if you are going to be predicting a new set of counts sampled in the same way that you sampled your data. This is usually how the standard statistician will think though (in terms of "if I did this experiment again..."). But for a hypothesis which is not sharp, such as two parameters being equal, but not restricted to be equal to anything the "prior predictive" distribution is what matters (at least from a Bayesian perspective) - and this certainly depends on the sample design! This is quite interesting for the "likelihood principle" is a bit subtle - your interest must be in the particular values of the parameters and not whether two parameters are equal. It is quite easy to see that the prior predictive distribution will depend on the sample design. My answer to this question should be easily adapted to your case. I can provide more details if you are uncertain how to adapt it to your case. You can also use a poisson GLM with a log-link function to do this test, and it amounts to testing that a certain interaction term in the glm is zero, so you can use the Pearson residuals from the output of the glm to do your chi-square test - you can also use the deviance table. UPDATE Having denied the likelihood principle here is actually incorrect on my part, for making inference on the $\theta_{ijk}$ parameters. This is easily seen by first computing odds ratios and then renormalising - the part related to the sampling design will cancel out. What an ass I made of myself, talking about "subtle" and all that! To show this more explicitly, I will write out $A\equiv A(n_{111},\dots,n_{IJK})$ as the constant which depends on how the experiment was done and which totals were fixed and which were random (independent of the $\theta_{ijk}$ parameters). I will show the final answer does not depend on $A$. So we we have: $$p(n_{111},\dots,n_{IJK}|\theta_{111},\dots,\theta_{IJK})=A\prod_{i=1}^{I}\prod_{j=1}^{J}\prod_{k=1}^{K}\theta_{ijk}^{n_{ijk}}$$ For the specific problem I will let $i$ denote gender, $j$ denote treatment, and $k$ denote the response, so we have $I=2,J=3,K=2$. Now I am going to assume that these are exhaustive classes. This can be re-phrased as restricting the domain of generality to those units which could be classified into this table - and not as "something else" which isn't in the table (e.g. a fourth treatment). This places some restrictions on the values of $\theta_{ijk}$, namely that they sum to 1: $$\sum_{i=1}^{2}\sum_{j=1}^{3}\sum_{k=1}^{2}\theta_{ijk}=1$$ And we have a hypothesis to consider: $$H_{0}:\text{Response does not differentiate between treatments and gender}$$ Or in mathematical terms $$H_{0}:\theta_{i_{1}j_{1}k}=\theta_{i_{2}j_{2}k}\;\;\;\forall i_{1},j_{1},i_{2},j_{2},k$$ Now this basically means that there is only one parameter in the table $\theta$, and we have an integral to do (letting $D\equiv n_{111},\dots,n_{IJK}$ and $I$ denote the prior information): $$p(H_{0}|D,I)=\frac{p(H_{0}|I)p(D|H_{0},I)}{p(D|I)}=\frac{p(H_{0}|I)}{p(D|I)}\int_{0}^{1}p(\theta|H_{0},I)p(D|\theta,H_{0},I)d\theta$$ $$=\frac{p(H_{0}|I)}{p(D|I)}A\int_{0}^{1}p(\theta|H_{0},I)\prod_{i=1}^{I}\prod_{j=1}^{J}\theta^{n_{ij1}}(1-\theta)^{n_{ij2}}$$ $$=\frac{p(H_{0}|I)}{p(D|I)}A\int_{0}^{1}p(\theta|H_{0},I)\theta^{n_{\bullet\bullet 1}}(1-\theta)^{n_{\bullet\bullet 2}}d\theta$$ Now nothing in the hypothesis says what the actual rate is, so $p(\theta|H_{0},I)=p(\theta|I)$ - this must depend on the prior information. A conservative choice is the uniform one, this will adopted for all priors, and we have: $$p(H_{0}|D,I)=\frac{p(H_{0}|I)}{p(D|I)}A \frac{\Gamma(n_{\bullet\bullet 1}+1)\Gamma(n_{\bullet\bullet 2}+1)}{\Gamma(n_{\bullet\bullet\bullet}+2)} =\frac{p(H_{0}|I)}{p(D|I)}A \frac{\Gamma(46)\Gamma(276)}{\Gamma(322)}$$ And it looks like $A$ is relevant, but we have yet to calculate $P(D|I)$. In order to do this, we must think about what the alternatives to $H_0$ are. These are fairly obvious: that the response differs by treatment and not by gender; by gender and not by treatment; or by both gender and by treatment $$H_{1T}:\theta_{ij_{1}k}=\theta_{ij_{2}k}\;\;\;\forall i,j_{1},j_{2},k$$ $$H_{1G}:\theta_{i_{1}jk}=\theta_{i_{2}jk}\;\;\;\forall i_{1},i_{2},j,k$$ $$H_{2}:\theta_{i_{1}j_{1}k}\neq\theta_{i_{2}j_{2}k}\;\;\;\forall i_{1},j_{1},i_{2},j_{2},k$$ These could be further broken down into statements about the individual cells, but this is not necessary. As before we will set uniform priors for the unrestricted parameters, and for $H_{1T}$ we get an integral $$\int_{\sum_{ik}\theta_{ik}=1}\prod_{i,k=1}^{2}\theta_{ik}^{n_{i\bullet k}}\prod_{i,k=1}^{2}d\theta_{ik}=\frac{\Gamma(n_{1\bullet 1}+1)\Gamma(n_{1\bullet 2}+1)\Gamma(n_{2\bullet 1}+1)\Gamma(n_{2\bullet 2}+1)}{\Gamma(n_{\bullet\bullet\bullet}+4)}$$ Now if you are like me, you will be able to see the pattern. The probabilities of each are written below: $$p(H_{0}|D,I)=\frac{p(H_{0}|I)}{p(D|I)}A \frac{\Gamma(n_{\bullet\bullet 1}+1)\Gamma(n_{\bullet\bullet 2}+1)}{\Gamma(n_{\bullet\bullet\bullet}+2)}$$ $$p(H_{1T}|D,I)=\frac{p(H_{1T}|I)}{p(D|I)}A \frac{\prod_{i,k}\Gamma(n_{i\bullet k}+1)}{\Gamma(n_{\bullet\bullet\bullet}+4)}$$ $$p(H_{1G}|D,I)=\frac{p(H_{1G}|I)}{p(D|I)}A \frac{\prod_{j,k}\Gamma(n_{\bullet jk}+1)}{\Gamma(n_{\bullet\bullet\bullet}+6)}$$ $$p(H_{2}|D,I)=\frac{p(H_{2}|I)}{p(D|I)}A \frac{\prod_{i,j,k}\Gamma(n_{ijk}+1)}{\Gamma(n_{\bullet\bullet\bullet}+12)}$$ Note that all of these probabilities contain the factor $\frac{A}{p(D|I)}$. So if we were to take odds ratios they would just cancel out. That is both $A$ and $P(D|I)$ cannot change the relative relationships between the 4 hypothesis - thus $A$ is irrelevant for answering this question as is $P(D|I)$. It does not matter whether you had fixed designs or random ones - it only matters that the likelihood function has the form I wrote at the start. So I will give my results in odds ratio form, with $H_0$ the denominator in the odds. Now if we assume that all hypothesis are equally likely, then these also drop out of the odds ratios. To go from odds ratios to probabilities is easy $$P(H_0|D,I)=\left(\sum_{h} \frac{P(H_h|D,I)}{P(H_0|D,I)}\right)^{-1}$$ And the results are given below, which shows that you would have to be crazy to reject $H_{0}$ in favour of the alternatives (the probability has first 60 decimal digits of 9 before the first non 9 digit). I also did a quick $\chi^{2}$ test and this gave values of $\chi^{2}=5.49$ on $5$ degrees of freedom, for a p-value of 0.36 (note that this is inflated because of some cells below 10, so the fit is much better than what $\chi^{2}$ test indicates). $$ \begin{array}{c|c} \text{Hypothesis} & \text{prior odds vs }H_{0} & \text{posterior odds vs }H_{0} \\ \hline H_{0} & 1 & 1 \\ H_{1T} & 1 & 5\times 10^{-61} \\ H_{1G} & 1 & 9\times 10^{-147} \\ H_{2} & 1 & 4\times 10^{-214} \\ \end{array} $$
