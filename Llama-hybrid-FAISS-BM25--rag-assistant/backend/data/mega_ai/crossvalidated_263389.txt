[site]: crossvalidated
[post_id]: 263389
[parent_id]: 166585
[tags]: 
Logistic regression seeks to maximize the log likelihood function $LL = \sum^k \ln(P_i) + \sum^r \ln(1-P_i)$ where $P_i$ is the predicted probability that case i is $\hat Y=1$; $k$ is the number of cases observed as $Y=1$ and $r$ is the number of (the rest) cases observed as $Y=0$. That expression is equal to $LL = ({\sum^k d_i^2} + {\sum^r d_i^2})/-2$ because a case's deviance residual is defined as: $d_i = \begin{cases} \sqrt{-2\ln(P_i)} &\text{if } Y_i=1\\ -\sqrt{-2\ln(1-P_i)} &\text{if } Y_i=0\\ \end{cases}$ Thus, binary logistic regression seeks directly to minimize the sum of squared deviance residuals. It is the deviance residuals which are implied in the ML algorithm of the regression. The Chi-sq statistic of the model fit is $2(LL_\text{full model} - LL_\text{reduced model})$, where full model contains predictors and reduced model does not.
