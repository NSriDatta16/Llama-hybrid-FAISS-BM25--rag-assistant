[site]: datascience
[post_id]: 53601
[parent_id]: 
[tags]: 
Copying embeddings for gensim word2vec

I wanted to see if I can simply set new weights for gensim's Word2Vec without training. I get the 20 News Group data set from scikit-learn (from sklearn.datasets import fetch_20newsgroups) and trained an instance of Word2Vec on it: model_w2v = models.Word2Vec(sg = 1, size=300) model_w2v.build_vocab(all_tokens) model_w2v.train(all_tokens, total_examples=model_w2v.corpus_count, epochs = 30) Here all_tokens is the tokenized data set. Then I created a new instance of Word2Vec without training model_w2v_new = models.Word2Vec(sg = 1, size=300) model_w2v_new.build_vocab(all_tokens) and set the embeddings of the new Word2Vec equal to the first one model_w2v_new.wv.vectors = model_w2v.wv.vectors Most of the functions work as expected, e.g. model_w2v.wv.similarity( w1='religion', w2 = 'religions') > 0.4796233 model_w2v_new.wv.similarity( w1='religion', w2 = 'religions') > 0.4796233 and model_w2v.wv.words_closer_than(w1='religion', w2 = 'judaism') > ['religions'] model_w2v_new.wv.words_closer_than(w1='religion', w2 = 'judaism') > ['religions'] and entities_list = list(model_w2v.wv.vocab.keys()).remove('religion') model_w2v.wv.most_similar_to_given(entity1='religion',entities_list = entities_list) > 'religions' model_w2v_new.wv.most_similar_to_given(entity1='religion',entities_list = entities_list) > 'religions' However, most_similar doesn't work: model_w2v.wv.most_similar(positive=['religion'], topn=3) [('religions', 0.4796232581138611), ('judaism', 0.4426296651363373), ('theists', 0.43141329288482666)] model_w2v_new.wv.most_similar(positive=['religion'], topn=3) >[('roderick', 0.22643062472343445), > ('nci', 0.21744996309280396), > ('soviet', 0.20012077689170837)] What am I missing?
