[site]: crossvalidated
[post_id]: 530809
[parent_id]: 
[tags]: 
PCA: removing dominant vector "directions" (isotropy)

I am currently reading an NLP paper on improving the representation of word vectors in space. The authors show that embedded words are not uniformly distributed in space but are contained in a lower-dimensional subspace. They propose fixing this problem as follows (cf. page 4, algorithm 1): Compute the PCA change-of-basis matrix $U$ (size $k \times m$ ) for the original $n \times m$ matrix $V$ (where $k$ is the reduced dimension and $m$ is the original dimension). Compute $(V - \mu) - VU^TU$ . The result is an $n \times m$ matrix (original size). PCA wasn't applied for dimensionality reduction but to remove dominant directions. Refer also to the code (only a few lines). My questions: $VU^T$ performs a change of basis (correct?). Multiplying by $U$ restores the matrix to its original size. What happens here geometrically/from a linear algebra perspective? It is no longer the original matrix $V$ . Is this a procedure that occurs also in other domains?
