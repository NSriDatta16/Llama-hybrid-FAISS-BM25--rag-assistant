[site]: crossvalidated
[post_id]: 637812
[parent_id]: 
[tags]: 
Transforming discrete optimisation problem into continuous optimisation problem

In Sparse Hilbert-Schmidt Independence Criterion Regression (Poignard and Yamada, AISTATS 2020), the authors consider a way to perform feature selection by taking the subset of features that maximises the sum of dependence between each feature and the target variable minus the sum of the pairwise dependence between each feature. This is a discrete optimisation problem, which corresponds to equation (1) in section 2.2, and which for convenience I reproduce here : $ \hat{\mathcal{S}} = \text{argmax}_{\mathcal{S}}\left[ \frac{1}{|\mathcal{S}|} \sum_{k \in \mathcal{S}} \text{D}\left(\text{X}^{(k)}, \text{Y} \right) - \frac{1}{|\mathcal{S}|^2} \sum_{k \in \mathcal{S}} \sum_{j \in \mathcal{S}} \text{D}\left(\text{X}^{(k)}, \text{X}^{(j)} \right) \right]~~~ (A)$ where the argmax is taken over the set of subsets of the full feature set, and D is a measure of dependence. The authors choose to use the empirical HSIC as that measure of dependence D, which I have no problem with. That optimisation problem is a discrete optimisation problem. Discrete optimisation problems are typically hard to solve. In section 2.4, the authors transform that discrete optimisation problem into a continuous optimisation problem. First they rewrite the original problem (A) in the strictly equivalent form (B) : $\text{argmax}_{\beta \in \{0, 1\}^d} \left[ \frac{1}{\beta^T \mathbf{1}} \sum_{k=1}^d \beta_k \text{D}\left(\text{X}^{(k)}, \text{Y} \right) - \frac{1}{\left(\beta^T \mathbf{1}\right)^2} \sum_{k=1}^d \sum_{j=1}^d \beta_k \beta_j \text{D}\left(\text{X}^{(k)}, \text{X}^{(j)} \right)\right] ~~~~ (B)$ . So far so good, (A) and (B) are just two ways of saying exactly the same thing. However, the authors then "relax" $\beta$ to optimise over a continuous space instead, leading to the continuous optimisation problem (C) : $\text{argmax}_{\theta \in [0, \infty[^d} \left[\sum_{k=1}^d \theta_k \text{D}\left(\text{X}^{(k)}, \text{Y} \right) - \frac{1}{2} \sum_{k=1}^d \sum_{j=1}^d \theta_k \theta_j \text{D}\left(\text{X}^{(k)}, \text{X}^{(j)} \right) - \lambda ||\theta||_1\right]~~~~ (C)$ . This continuous formulation (C) makes sense to me on an intuitive level. However, top of my head I am unable to come up with a formal argument why it is valid to do this relaxation and consider continuous optimisation problem (C) instead of discrete optimisation problem (B) . Could someone explain the condition of validity of this substitution of a discrete optimisation problem into a continuous optimisation problem ? Does that technique have a specific name ? References to literature welcome !
