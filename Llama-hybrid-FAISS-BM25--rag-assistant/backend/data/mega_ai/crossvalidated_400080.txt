[site]: crossvalidated
[post_id]: 400080
[parent_id]: 391051
[tags]: 
The input is the ground truth. In other words, typical discriminative supervised learning tries to learn $p(y|x)$ . On the other hand, language models learn $p(x)$ . As you can see there is no need for any labeled " $y$ " The way this works with autoregressive models such as RNNs is that you decompose it as a bunch of conditional probabilities $p(x) = \prod_j p(x_j|x_{ .
