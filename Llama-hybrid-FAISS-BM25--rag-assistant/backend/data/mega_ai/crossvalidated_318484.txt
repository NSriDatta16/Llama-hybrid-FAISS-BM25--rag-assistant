[site]: crossvalidated
[post_id]: 318484
[parent_id]: 318463
[tags]: 
There's no right answer to this but, maybe, "everything in moderation." While many recent improvements in machine learning, i.e., dropout, residual connections, dense connections, batch normalization, aren't rooted in particularly deep theory (most can be justified in a few paragraphs), I think there's ultimately a bottleneck for just how many such results can make a huge impact. At some point you have to sit down and work out some extra theory to make the next big leap. As well, theory can guide intuition because it can prove the quality or limitations of a model to within a reasonable doubt. This is particularly important for figuring if say, SGD is better than Momentum for a particular problem. That's the nice thing about theory: it forces you to abstract the problem you're solving, and in many cases this can be very beneficial because abstract objects are rigorously defined and you can easily see similarities between two seemingly different objects. The big example that comes to mind is support vector machines. They were originally devised by Vapnik and Chervonenkis in the early 60s, but really took off in the early 90s when Vapnik and others realized that you can do nonlinear SVMs using the Kernel Trick. Vapnik and Chervonenkis also worked out the theory behind VC dimension , which is an attempt at coming up with a complexity measure for machine learning. I can't think of any practical application of VC dimension, but I think the idea of SVMs was likely influenced by their work on this. The Kernel Trick itself comes from abstract-nonsense mathematics about Hilbert spaces. It might be a stretch to say that it's necessary to know this abstract nonsense to come up with SVMs, but, I think it probably helped out quite a bit, especially because it got a lot of mathematicians excited about machine learning. On the subject of ResNet, there's been some really neat work recently suggesting that Residual architectures really don't need to be 100s of layers deep. In fact some work suggests that the residual connections are very similar to RNNs, for example Bridging the Gaps Between Residual Learning, Recurrent Neural Networks and Visual Cortex ", Liao et al. I think this definitely makes it worth looking into deeper because it suggests that theoretically, ResNet with many layers is in fact incredibly inefficient and bloated. The ideas for gradient clipping for RNNs were very well justified in the now famous paper " On the difficulty of training recurrent neural networks " - Pascanu, et. al. While you could probably come up with gradient clipping without all the theory, I think it goes a long way toward understanding why RNNs are so darn hard to train without doing something fancy, especially by drawing analogies to dynamical system maps (as the paper above does). There's a lot of excitement about Entropy Stochastic Gradient Descent methods. These were derived from Langevin dynamics, and much of the theoretical results are rooted firmly in classical theoretical PDE theory and statistical physics. The results are promising because they cast SGD in a new light, in terms of how it gets stuck in local fluctuations of the loss function, and how one can locally smooth the loss function to make SGD be much more efficient. It goes a long way toward understanding when SGD is useful and when it behaves poorly. This isn't something you can derive empirically by trying SGD on different kinds of models. In the paper Intriguing properties of neural networks , the authors summarize that neural networks are sensitive to adversarial examples (defined as calculated, sleight perturbations of an image) due to high Lipchitz constants between layers. This is still an active area of research and can only be understood better through more theoretical derivations. There's also the example of Topological Data Analysis , around which at least one company ( Ayasdi )has formed. This is a particularly interesting example because the techniques used for it are so specific and abstract that even from today, it will still take a lot of time to see where the ideas from this theory end up. My understanding is that the computational complexity of the algorithms involved tends to be quite high (but then again it was equally high for neural networks even 20 years ago).
