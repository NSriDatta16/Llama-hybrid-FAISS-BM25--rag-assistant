[site]: crossvalidated
[post_id]: 530049
[parent_id]: 529926
[tags]: 
To all the other answers I'd add that in Deep Learning, Neural Architecture Search benefits immensely from data efficiency. Think about it: each data point is a trained network . If your NAS setup requires $N$ data points (networks), and each network requires $D$ samples to be trained, that's $ND$ forward- and backpropagations overall: if you reduce $D$ by a ratio of $k$ , that's $k$ -many more architectures that you can explore with the same resources. Of course, it isn't always that straightforward: This CVPR2021 paper by Mundt et al shows that the architectures themselves act as Deep Priors which don't need to be fully trained, if initialized correctly (as a nice counterpoint to the vast few-shot learning literature): Neural Architecture Search of Deep Priors: Towards Continual Learning without Catastrophic Interference
