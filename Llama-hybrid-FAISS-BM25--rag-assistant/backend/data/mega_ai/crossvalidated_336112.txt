[site]: crossvalidated
[post_id]: 336112
[parent_id]: 
[tags]: 
Bayesian and Frequentist linear regressions: different results

I've simulated three normal distributions with different means and standard deviations. Assume these are three groups of subjects. I wanted to see how a normal lm() would compare with a brm() regression. The model was Score ~ Group + (+1|Subject) . I haven't added a random intercept by group on purpose. set.seed(2) df = data.frame(Group = as.factor(rep(c("A", "B","C"), each = 120)), Subject = rep(paste("subject", seq(1, 9), sep = "_"), each = 40), Score = c(rnorm(120, 5, 2), rnorm(120, 7, 4), rnorm(120, 9, 6))) It turns out that the results are quite different. Groups B and A are significantly different in the lm() , but their 95% HDI often includes zero in the brm() output (I understand that results will vary every time a model samples the posterior, but the bottom line is that the t value in the lm() model is 2.544, whereas the HDI in the brm() would lead me to remain skeptical. My question is : why is this happening? In other words, how do these two models compare re. their treatment of heteroscedastic data? Output of each model Frequentist Linear mixed model fit by REML ['lmerMod'] Formula: Score ~ Group + (1 | Subject) Data: df REML criterion at convergence: 2057.9 Scaled residuals: Min 1Q Median 3Q Max -4.1237 -0.6125 -0.0159 0.5968 3.8714 Random effects: Groups Name Variance Std.Dev. Subject (Intercept) 0.4598 0.6781 Residual 17.7157 4.2090 Number of obs: 360, groups: Subject, 9 Fixed effects: Estimate Std. Error t value (Intercept) 5.0640 0.5485 9.232 GroupB 1.9735 0.7757 2.544 GroupC 5.4684 0.7757 7.049 Correlation of Fixed Effects: (Intr) GroupB GroupB -0.707 GroupC -0.707 0.500 Bayesian In this particular run, the HDI doesn't include zero, but you can see it's quite close to it (unlike the t value in the model above). Family: gaussian(identity) Formula: Score ~ Group + (1 | Subject) Data: df (Number of observations: 360) Samples: 4 chains, each with iter = 4000; warmup = 2000; thin = 1; total post-warmup samples = 8000 ICs: LOO = NA; WAIC = NA; R2 = NA Group-Level Effects: ~Subject (Number of levels: 9) Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat sd(Intercept) 0.84 0.53 0.07 2.06 1457 1 Population-Level Effects: Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat Intercept 5.07 0.67 3.76 6.48 2602 1 GroupB 1.97 0.94 0.01 3.90 2744 1 GroupC 5.44 0.95 3.46 7.27 2851 1 Family Specific Parameters: Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat sigma 4.23 0.16 3.92 4.56 6732 1 Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1).
