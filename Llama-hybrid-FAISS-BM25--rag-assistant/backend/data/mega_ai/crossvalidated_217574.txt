[site]: crossvalidated
[post_id]: 217574
[parent_id]: 162988
[tags]: 
Since the original question mentioned the decaying gradient problem, I'd just like to add that, for intermediate layers (where you don't need to interpret activations as class probabilities or regression outputs), other nonlinearities are often preferred over sigmoidal functions. The most prominent are rectifier functions (as in ReLUs ), which are linear over the positive domain and zero over the negative. One of their advantages is that they're less subject to the decaying gradient problem, because the derivative is constant over the positive domain. ReLUs have become popular to the point that sigmoids probably can't be called the de-facto standard anymore. Glorot et al. (2011) . Deep sparse rectifier neural networks
