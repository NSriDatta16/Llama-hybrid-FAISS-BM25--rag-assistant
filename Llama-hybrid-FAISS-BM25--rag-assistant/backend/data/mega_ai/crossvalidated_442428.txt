[site]: crossvalidated
[post_id]: 442428
[parent_id]: 442337
[tags]: 
Bayesian approaches do not involve maximizing a likelihood function, but rather integrating over a posterior distribution. Note that the underlying model may be exactly identical (i.e., linear regression, generalized linear regression), but we also need to provide a prior distribution which captures our uncertainty in the parameters before seeing the data. The posterior distribution is simply the normalized distribution of the prior times the likelihood. I believe that most statisticians these days generally agree that a Bayesian approach is generally superior to an MLE approach for parameter estimation. However, when one has a lot of data, it may not be so much better that it's both the extra computational costs (integrating is harder than optimizing!) and extra effort of coming up with a prior distribution. In fact, one can show that asymptotically, the MLE + normal approximation approaches the posterior distribution under certain conditions.
