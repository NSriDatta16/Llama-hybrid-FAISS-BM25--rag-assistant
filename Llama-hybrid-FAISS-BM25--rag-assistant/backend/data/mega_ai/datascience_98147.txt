[site]: datascience
[post_id]: 98147
[parent_id]: 
[tags]: 
Neural network weight initialization

I was working on recreating the Convolutional Neural Network Le-Net 5. I was getting around 96.5% accuracy on the training set. This was not near the 99.2% the network was meant to be operating at. After a bit of digging I realized I had uniformly distributed the weights and biases initialization as opposed to using a normal distribution. After retesting the network using normal distribution, it was now achieving a 98.1% accuracy. I was wondering why this occurs and also is there is any preferred settings (i.e. when I initialized mine I had a mean of 0 and and a standard deviation of .25)/methods for weight initialization (I have heard of Xavier, but that about as far as my knowledge goes outside of normal distribution initialization)?
