[site]: crossvalidated
[post_id]: 190594
[parent_id]: 190566
[tags]: 
You do not specify a context, so for the following I will assume we are in a (classical) linear model setup: $$ y = X\beta + u $$ Where $u$ are the errors. This might be a time series or panel model, since we are talking about serial (auto) correlation in the error term. If we maintain strictly exogenous regressors we should start by acknowledging that all the important properties of the model are preserved, even if there is serial correlation. That is, the estimator is unbiased and consistent (and consistency even holds under weaker assumptions). So you get will get casual effects and good predictions, which is what we care about. You know this, since serial correlation does not play role in proving this. So why do we even need the assumption of no serial correlation? We need this for the (usual OLS) standard errors to be calculated correctly. Why do care? Because we want to do inference, and because under no serial correlation (and homoskedlasticity) OLS offers the lowest variance of all linear unbiased estimators. If the errors are serially correlated, and a priori this is often impossible to rule out, this is no longer the case. I will again mention that the important properties of the model are STILL present. The problem is with inference only. So to your question, what can we do? You can try do to some manipulation of the data, but often it will not help and it might even destroy the interpretation that you intended. It is also somewhat arbitrary. On the other hand, there are so cases where you can argue for a certain manipulation to fix it (but this should really be considered the exception IMO). Instead I suggest that you simply calculate standard errors that are robust against the problem. Most programs has some option that allows this, so you don't even need to worry about the formula. In any case, once you have the robust errors inference can be carried out in the same fashion as always. Personally I like the bootstrap method(s), because you don't need to impose some arbitrary error structure. But it is not a magic bullet, median regression being a prime example. Clustering is another alternative I often see in applied works. Here we think of each observation as belonging to a well defined group (the cluster), this allows for arbitrary correlation within the cluster but with a form independence between the clusters. This is very feasible in a social science context. In fact, most paper's written and published today will only report robust errors and contain something along the lines: "we report robust errors, this changes the significance level of variable z but... etc.". Actually the usual errors might work pretty well provided the sample is large and the correlation is not too strong. Again I stress that all that you loose is efficiency! Provided you have a large sample, and work in applied science (or the industry) who cares?
