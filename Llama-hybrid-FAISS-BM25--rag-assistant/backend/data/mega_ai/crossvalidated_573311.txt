[site]: crossvalidated
[post_id]: 573311
[parent_id]: 
[tags]: 
Regular *negative* spikes in neural network training loss

I am training an ALL-CNN network using the Adam solver. As the figure shows, the testing seems to converge to an acceptable solution, but there are these regular negative spikes during training that leave me thinking I can do better. Is that so, or has anybody seen similar behavior?
