[site]: datascience
[post_id]: 65455
[parent_id]: 32901
[tags]: 
I'll go through your questions one by one: Should i normalize my numerical data values before feeding to any type of autoencoder? if they are int and float values still i have to normalize? This is strongly suggested, for two reasons. First, if different variables are on different scales, weights distributions will be unequal. Larger scales will dominate smaller scales during application of gradient descent, and this would lead to many parameters to be undertrained, leading to sub-optimal results. Second, your layers have activation functions that are meant to "learn" non-linear patterns in your data. All the commonly used activation functions (Sigmoid, Tanh, all the ReLU family, you name it) tend to be non-linear only around zero. Normalizing your data helps Neural Networks to learn most from them. Which activation function should i use in autoencoder? Some article and research paper says , "sigmoid" and some says "relu" ? This is more an art than a science, however all the activations form the ReLU family have been proved superior to other counterparts. I'd suggest you to go for some kind of ReLU basically always. Some are more fancier but computationally expensive, usually the rank is: ELU > Leaky ReLU > ReLU Should i use dropout in each layer ? like if my artichare for autoencoder looks like Use some amount of dropout, but not too much. Dropout is a regularization technique that helps you preventing overfitting. This technique "turns off" some neurons during training sampled randomly, in order to make all neurons specialize during training (it turns your Neural Network in an ensemble of Neural Networks). However, keep in mind that dropout also represents an information loss! For example, if you set a Dropout() layer with dropout probability of 0.5, you'd loose half of the information at that layer at each iteration! I suggest you to use it, but not at every layer, and be parsimonious with it. For text dataset , If i am using word2vec or any embedding to convert text into vector then i would have float values for each word , should i normalize that data too ? No, those are learned automatically by the model, you don't have to worry about their internal values.
