[site]: datascience
[post_id]: 99492
[parent_id]: 
[tags]: 
Why it is said that cross entropy has no minimum value?

I have been reading Deep Learning book by Ian Goodfellow, et al. ,and in chapter 6 (pages 179-180), the following point is mentioned: One unusual property of the cross-entropy cost used to perform maximum likelihood estimation is that it usually does not have a minimum value when applied to the models commonly used in practice. For discrete output variables, most models are parametrized in such a way that they cannot represent a probability of zero or one, but can come arbitrarily close to doing so. Logistic regression is an example of such a model. For real-valued output variables, if the model can control the density of the output distribution (for example, by learning the variance parameter of a Gaussian output distribution) then it becomes possible to assign extremely high density to the correct training set outputs, resulting in cross-entropy approaching negative infinity. However, I dont understand why, for discrete outputs (i.e. classification), having a probability close to zero or one means our loss function will get close to negative infinity. Also I have the same problem with real valued outputs, as I got confused when realizing why having a probability mass function centered narrowly(low variance) on the true training set labels will make the loss function get close to negative infinity? Could you help me work out the math and the translation of following statements in terms of math equations?
