[site]: crossvalidated
[post_id]: 174438
[parent_id]: 174295
[tags]: 
Intuition A saturating activation function squeezes the input. Definitions $f$ is non-saturating iff $ (|\lim_{z\to-\infty} f(z)| = +\infty) \vee (|\lim_{z\to+\infty} f(z)| = +\infty) $ $f$ is saturating iff $f$ is not non-saturating. These definitions are not specific to convolutional neural networks. Examples The Rectified Linear Unit (ReLU) activation function, which is defined as $f(x)=max(0,x)$ is non-saturating because $\lim_{z\to+\infty} f(z) = +\infty$ : The sigmoid activation function, which is defined as $f(x) = \frac{1}{1 + e^{-x}}$ is saturating, because it squashes real numbers to range between $[0,1]$ : The tanh (hyperbolic tangent) activation function is saturating as it squashes real numbers to range between $[-1,1]$ : (figures are from CS231n , MIT License)
