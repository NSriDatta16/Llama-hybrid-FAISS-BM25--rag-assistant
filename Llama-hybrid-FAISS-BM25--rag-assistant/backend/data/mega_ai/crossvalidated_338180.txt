[site]: crossvalidated
[post_id]: 338180
[parent_id]: 338178
[tags]: 
The paper probably gives the best explanation. The idea is that rather than using the CNN-RNN embeddings directly as inputs to the generator, you want to reprocess them to a smaller size (the raw embeddings are length 1024 but the conditioning vectors are length 128) and also add some randomness/sample more diverse examples from embedding space. You do this by learning 128 means and stddevs from the raw embeddings using a FC layer, then essentially sampling the 128 components of the conditioning vector from 128 independent normal distributions with those means and stddevs
