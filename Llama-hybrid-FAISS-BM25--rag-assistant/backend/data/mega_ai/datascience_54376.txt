[site]: datascience
[post_id]: 54376
[parent_id]: 54296
[tags]: 
As the other answers previously said, in practice it doesn't have much difference which of the two you choose. However, theoretically it's better to scale your input to $[-1, 1]$ than $[0, 1]$ and I'd argue that it's even better to standardize your input (i.e. $μ=0$ , $σ=1$ ). Let me explain why: Deep neural networks, especially in their early days, had much trouble with backpropagation, as they suffered from vanishing/exploding gradients. A popular way to combat this is by initializing the weights of the network in smarter ways. Two initialization techniques have proven to be the most popular: Glorot initialization (sometimes referred to as Xavier initialization) and He initialization , both of which are variations of the same idea. In order to derive their intialization they make a few assumptions, one of which is that the input features have a zero mean. You can read this post for simplified explanation, but I'd suggest reading the two papers mentioned above.
