[site]: crossvalidated
[post_id]: 409035
[parent_id]: 408920
[tags]: 
Typically a dense layer follows the LSTM/RNN layer(s), because the output of the RNN cell is of dimension of your choice, i.e. latent space dimension. Since you've three outputs, the final Dense layer will have three neurons, aiming to regress genre points. The RNN layer's duty is to figure out a compressed, latent representation of your series going back a couple of time steps of your choice. Here is a good (I believe) introductory tutorial for multivariate time series forecasting. Keras itself has a short example on multi-input/multi-output models. One trick that can be useful for the last Dense layer is that if you have bounded outputs (e.g. $[0,5]$ ), scaling the output variable and regressing via sigmoid activation may help, instead of linear activation that can cause overflow issues on some inputs.
