[site]: crossvalidated
[post_id]: 413250
[parent_id]: 413195
[tags]: 
okay, so omitting more data, will probably not be possible In my opinion, there is one common mistake with imputations: people tend to use those imputation that lead to a better model fit. But of course the model is not getting better, they imputed just the fitting values. A way to generally lower that risk are multiple imputations...but it's not possible to combine this method with randomforests (yet), at least as far as I know. My understanding is that for random forest models, the distribution of each predictor is important, so using a simple method like class-mean imputation would be incorrect, because it distorts the distribution of the predictors. I don't think that is correct. I would describe randomforests as 'very forgiving' to imputing a 'mean'. Even if that 'mean' would be terribly wrong, the 'random' part of an 'randomForest' deals with that easily The reason I feel so apprehensive abourt using class mean imputation is that it reduces the internal variance of each class, and I'm worried that that is artificially making each class more homogeneous internally, and more different from the other classes. That would then make the classification results artificially better than they would be if I was using a method that didn't distort the distributions. Is that wrong? It's completely the other way around--let's say you have two observations. X1 is 100kg and height unknown. X2 is 50 kilo and height unknown. You are using a regression-prediction as the imputation. The most likely height for X1 is 1.90m and for X2 1.50m. So your Residuals have a very low variance and might not display the reality, where people do not always have the most likely height for their weight. also , if those two observations get splittet up in training and test, it will be more easy for them to "find the patterns" as their values were created in the same data-generating process. instead if you are using the mean, your error's variance will usuallly get higher
