[site]: crossvalidated
[post_id]: 255603
[parent_id]: 
[tags]: 
How do I use negative examples (in addition to positive ones) for training a multiclass softmax classifier (or a neural net with softmax output)?

Suppose we are training a neural network for multi-class classification, and we use softmax (or hierarchical softmax) as its output layer. For positive examples , we need to maximize the log likelihood of training examples. We can calculate the gradient of the negative log likelihood for each example, and do stochastic gradient descent. My question is, if we also have negative examples (e.g. some prediction the model made in the past that was later judged by human to be incorrect), how do we incorporate them into the training process? It sounds right to me to minimize the likelihood of making these predictions; is it okay to simply negate the gradient? (n.b. initial tests show that this causes the model to diverge.)
