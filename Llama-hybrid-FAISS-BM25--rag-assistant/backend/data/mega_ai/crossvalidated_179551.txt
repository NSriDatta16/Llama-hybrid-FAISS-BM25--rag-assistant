[site]: crossvalidated
[post_id]: 179551
[parent_id]: 179511
[tags]: 
There is a generalized lack of rigor in the use of the word "correlation" for the simple reason that it can have widely differing assumptions and meanings. The simplest, loosest and most common usage is that there is some vague association, relationship or lack of independence between a static pair of random variables. Here, the default metric referred to is usually the Pearson correlation, which is a standardized measure of pairwise, linear association between two continuously distributed variables. One of the Pearson's commonest misuses is to report it as a percentage. It is definitely not a percentage. The Pearson correlation, r , ranges between -1.0 and +1.0 where 0 means no linear association. Other not so widely recognized issues with using the Pearson correlation as the default is that it is actually quite a stringent, non-robust measure of linearity requiring interval-scaled variates as input (see Paul Embrechts' excellent paper on Correlation and Dependency in Risk Management: Properties and Pitfalls here: https://people.math.ethz.ch/~embrecht/ftp/pitfalls.pdf ). Embrechts notes that there are many fallacious assumptions about dependence that begin with assumptions of the underlying structure and geometric shape of these relationships: These fallacies arise from a naive assumption that dependence properties of the elliptical world also hold in the non-elliptical world Embrechts points to copulas as a much wider class of dependence metrics used in finance and risk management, of which the Pearson correlation is just one type. Columbia's Statistics department spent the academic year 2013-2014 focused on developing deeper understandings of dependence structures: e.g., linear, nonlinear, monotonic, rank, parametric, nonparametric, potentially highly complex and possessing wide differences in scaling. The year ended with a 3 day workshop and conference that brought together most of the top contributors in this field ( http://datascience.columbia.edu/workshop-and-conference-nonparametric-measures-dependence-apr-28-may-2 ). These contributors included the Reshef Brothers, now famous for a 2011 Science paper Detecting Novel Associations in Large Data Sets http://www.uvm.edu/~cdanfort/csc-reading-group/reshef-correlation-science-2011.pdf that has been widely criticized (see AndrewGelman.com for a good overview, published simultaneously with the Columbia event: http://andrewgelman.com/2014/03/14/maximal-information-coefficient ). The Reshefs addressed all of these criticisms in their presentation (available on the Columbia conference website), as well as a vastly more efficient MIC algorithm. Many other leading statisticians presented at this event including Gabor Szekely, now at the NSF in DC. Szekely developed his distance and partial distance correlations. Deep Mukhopadhay, Temple U, presenting his Unified Statistical Algorithm -- a framework for unified algorithms of data science -- based on work done with Eugene Franzen http://www.fox.temple.edu/mcm_people/subhadeep-mukhopadhyay/ . And many others. For me, one of the more interesting themes was wide leverage and use of Reproducing Kernel Hilbert Space (RKHS) and the chi-square. If there was a modal approach to dependence structures at this conference, it was the RKHS. The typical intro statistics textbooks is perfunctory in its treatment of dependence, usually relying on presentations of the same set of visualizations of circular or parabolic relationships. More sophisticated texts will delve into Anscombe's Quartet , a visualization of four different datasets possessing similar, simple statistical properties but hugely differing relationships: https://en.wikipedia.org/wiki/Anscombe%27s_quartet One of the great things about this workshop was the multitude of dependence structures and relationships visualized and presented, going far beyond the standard, perfunctory treatment. For instance, the Reshefs had dozens of thumbnail graphics that represented just a sampling of possible nonlinearities. Deep Mukhopadhay had stunning visuals of highly complex relationships that looked more like a satellite view of the Himalayas. Stats and data science textbook authors need to take note. Coming out of the Columbia conference with the development and visualization of these highly complex, pairwise dependence structures, I was left questioning the ability of multivariate statistical models to capture these nonlinearities and complexities.
