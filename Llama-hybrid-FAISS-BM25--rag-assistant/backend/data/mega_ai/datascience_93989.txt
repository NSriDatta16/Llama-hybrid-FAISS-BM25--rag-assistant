[site]: datascience
[post_id]: 93989
[parent_id]: 93981
[tags]: 
You probably mean "MapReduce" by "Hadoop". MapReduce is an older model where each operation took input from storage, computed, wrote results to storage. Of course, many real-world tasks involve lots of operations - read, filter, transform, aggregate, etc. Expressing those in MapReduce would be difficult, and what's more, every single piece of the operation would read and write data, which is slow. Spark is a higher-level framework where the operations can be combined and optimized first before execution. So a bunch of simple operations on data could all happen at once to data, while it's 'in memory', without any need to persist intermediate results. This is also said of Spark because it can easily persist data sets in memory if desired by caching them. This easy access to data in memory is also one reason Spark can be faster. Of course, nobody really used MapReduce for basic SQL-like operations. They'd have used Hive or similar. Hive also had similar optimizations and was likewise, to some degree, 'in memory'. However Hive was SQL-only, whereas Spark can support distributed SQL operations, user code, etc. It had the flexibility of MapReduce with much less overhead for developers, but could also support high-throughput SQL like Hive.
