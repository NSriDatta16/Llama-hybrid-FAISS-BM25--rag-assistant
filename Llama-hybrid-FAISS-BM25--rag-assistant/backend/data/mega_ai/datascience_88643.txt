[site]: datascience
[post_id]: 88643
[parent_id]: 88604
[tags]: 
I think it's always a good idea to start with a basic option. Once you have a decent basic model, you can try to improve performance with something more advanced and you would have a baseline so you know if the performance of the advanced model is really good or just ok. In this case the basic option would be to train 100 binary classifiers, one for every label. The text could be represented as a TFIDF vector. I would suggest removing the rare words, it can significantly improve performance. Then a traditional model like Naive Bayes, Decision Trees or SVM can be used. The number of labels by itself is not an issue, however their distribution matters, especially if some of them have very few instances (expect bad performance on these).
