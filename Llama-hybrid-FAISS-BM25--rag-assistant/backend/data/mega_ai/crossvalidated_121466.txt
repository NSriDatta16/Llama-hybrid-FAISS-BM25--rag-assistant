[site]: crossvalidated
[post_id]: 121466
[parent_id]: 93071
[tags]: 
The cost function derived in logistic regression is quite similar to the cross-entropy cost function in NN terminology . The difference is that in cross-entropy they took a log and divided it by the number of samples. The reason for the log is for the ease of getting the gradient, and dividing by a constant does not affect the learning in terms of gradient decent(Except for Plain gradient descent) . It is used quite often. The default-introduction of the quadratic loss in text books may be due to historical reasons.
