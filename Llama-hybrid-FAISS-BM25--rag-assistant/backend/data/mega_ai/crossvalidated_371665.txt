[site]: crossvalidated
[post_id]: 371665
[parent_id]: 
[tags]: 
Obtaining posteriors for multivariate Normal mixture models

So I want to fit a mixture model $$f(y) = \pi_1 f_1 (y) + \pi_2 f_2 (y)$$ where $\pi_k = P(S = k)$ and $S_i$ is a latent unobserved variable. I assume that, conditional on $S=k$ , we have the model $$y_i = \mu_k + \beta_k x_i + \epsilon_i$$ where $\epsilon_i \sim N_p (0, \Sigma_k)$ . My goal is to estimate parameters $\mu_k, \beta_k, \Sigma_k, \pi_k$ for each $k$ . Firstly, As I understand it, under this model we have likelihood function $$L(\theta |y) = \prod_i \left( \pi_1 \phi_{1,p} \left(y_i-\mu_1 -\beta_1 x_i\right) + (1-\pi_1) \phi_{2,p} \left(y_i-\mu_2 -\beta_2 x_i\right) \right)$$ where $\phi_{k,p} \sim N_p(0, \Sigma_k)$ . Secondly, in order to obtain point estimates of the parameters in a Bayesian framework, I need to obtain a posterior sample from the posterior distribution $$p(\mu, \beta, \Sigma, \pi | y)$$ But as this is difficult, an alternative would be to use Gibbs sampling with posteriors $$p(\mu | \beta, \Sigma, \pi , y)$$ $$p(\beta | \mu, \Sigma, \pi , y)$$ $$p(\Sigma | \mu, \beta, \pi , y)$$ $$p(\pi | \mu, \Sigma, \beta , y)$$ What I don't understand, is how to get from the given model equation and likelihood function to those posteriors. Once I have those posteriors, I am confident I can impliment Gibbs sampling. Since my likelihood is not multivariate normal, it is a mixture of multivariate normals, I am unsure how to proceed, as there are no conjugates listed on the Wikipedia page for mixture distributions. Any help/advice is much appreciated!
