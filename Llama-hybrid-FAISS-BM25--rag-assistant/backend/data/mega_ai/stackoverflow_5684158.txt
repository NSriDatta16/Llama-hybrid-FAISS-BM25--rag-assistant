[site]: stackoverflow
[post_id]: 5684158
[parent_id]: 
[tags]: 
What's the fastest way to access a dataset with Java?

I have a large file, with 1.8 million rows of data, that I need to be able to read for a machine learning program I'm writing. The data is currently in a CSV file but clearly I can put it in a database or other structure as required - it won't need to be updated regularly. The code I'm using at the moment is below. I'm first importing the data to an array list and then I'm passing it to a table model. This is very slow, currently taking six minutes to execute just the first 10,000 rows which is not acceptable as I need to be able to test different algorithms against the data fairly often. My program will only need to access each row of the data once, so there's no need to hold the whole dataset in RAM. Am I better off reading from a database, or is there a better way to read the CSV file line by line but do it much faster? import java.io.File; import java.io.FileNotFoundException; import java.util.ArrayList; import java.util.Scanner; import javax.swing.table.DefaultTableModel; import javax.swing.table.TableModel; public class CSVpaser { public static TableModel parse(File f) throws FileNotFoundException { ArrayList headers = new ArrayList (); ArrayList oneDdata = new ArrayList (); //Get the headers of the table. Scanner lineScan = new Scanner(f); Scanner s = new Scanner(lineScan.nextLine()); s.useDelimiter(","); while (s.hasNext()) { headers.add(s.next()); } //Now go through each line of the table and add each cell to the array list while (lineScan.hasNextLine()) { s = new Scanner(lineScan.nextLine()); s.useDelimiter(", *"); while (s.hasNext()) { oneDdata.add(s.next()); } } String[][] data = new String[oneDdata.size()/headers.size()][headers.size()]; int numberRows = oneDdata.size()/headers.size(); // Move the data into a vanilla array so it can be put in a table. for (int x = 0; x Update: Based on feedback I received in the answers I've rewritten the code, its now running in 3 seconds rather than 6 minutes (for 10,000 rows) which means only ten minutes for the whole file... but any further suggestions for how to speed it up would be appreciated: //load data file File f = new File("data/primary_training_short.csv"); Scanner lineScan = new Scanner(f); Scanner s = new Scanner(lineScan.nextLine()); s.useDelimiter(","); //now go through each line of the results while (lineScan.hasNextLine()) { s = new Scanner(lineScan.nextLine()); s.useDelimiter(", *"); String[] data = new String[NUM_COLUMNS]; //get the data out of the CSV file so I can access it int x = 0; while (s.hasNext()) { data[x] = (s.next()); x++; } //insert code here which is excecuted each line }
