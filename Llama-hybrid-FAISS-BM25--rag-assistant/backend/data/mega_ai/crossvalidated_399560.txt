[site]: crossvalidated
[post_id]: 399560
[parent_id]: 
[tags]: 
Value function and action value function

In an MDP, for any state s , what is the difference between the action value function and value function? I guess they are same because they both are defined w.r.t a policy (say pi). Now if policy is fixed then action on a state is also fixed even though we can have probabilistic transitions. But not sure about this.
