[site]: datascience
[post_id]: 113612
[parent_id]: 
[tags]: 
Gradient tree boosting additive training

In the XGBoost documentation, they specify that the additive training is done given an objective $obj^{(t)}$ defined as $obj^{(t)} = \sum\limits_{i=1}^n \ell(y_i, \hat{y}_i^{(t-1)}+f_t(x_i)) + \sum\limits_{k=1}^t \omega(f_k)$ . In the following lines, it is specified that this objective is equal to $\sum\limits_{i=1}^n \ell(y_i, \hat{y}_i^{(t-1)}+f_t(x_i)) + \omega(f_t)+constant$ though I don't really understand this step - do they mean that $\sum\limits_{k=1}^{t-1} \omega(f_k) = constant$ ? if so, why? and why is this a greedy method?
