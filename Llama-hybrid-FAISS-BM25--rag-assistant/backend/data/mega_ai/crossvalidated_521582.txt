[site]: crossvalidated
[post_id]: 521582
[parent_id]: 
[tags]: 
Controlling the entropy of a distribution

What is the best way to control the entropy of a categorical probability distribution? I have a categorical probability distribution $p_1, p_2, \dots, p_K$ , for $K$ small-ish ( $ ). Assume that the probabilities are generally distinct. I'd like to compute a distribution $p'_{1 \dots K}$ that approximates $p_{1 \dots K}$ in some sense while having entropy approximately equal to a given value $H'$ . What algorithm can I use to compute $p'_{1 \dots K}$ ? Ideally, I'd like a smooth function that can be computed without numerical optimization, since I need to use this in the forward pass of a neural network. I'm flexible about the approximation criterion, as long as it (usually) keeps the rank of the probabilities $p_i > p_j \Longleftrightarrow p'_i > p'_j$ . EDIT: Note that this computation should be performed at test time. I'm not looking for a training time entropy penalty on an output distribution, but to a method to project an arbitrary distribution to a subpace of the probability simplex with an arbitrary given entropy.
