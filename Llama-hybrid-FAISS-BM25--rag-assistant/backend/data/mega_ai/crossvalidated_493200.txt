[site]: crossvalidated
[post_id]: 493200
[parent_id]: 493192
[tags]: 
My general advice would be to transparent in how you report your findings. There are standards in different scientific communities, but there is no single way of performing cross-validation. That being said, let me try my best to answer your three questions: Generally, when using cross-validation, you need to split in to THREE groups. Training, validation, and test. The issue is that generally when training a CNN, you stop training after the model stopes improving when evaluated against the validation data. However, if the validation data is not a fully representative sample of the problem space, then your evaluation metrics will appear better than they are. So, when performing 10-fold cross validation, train on 8 folds, validate on 1, and test on 1. Your metrics against the test fold is what you should be reporting. This is an interesting idea, however, it isn't really cross-validation. Not that there is something intrinsically wrong with this methods, but assuming that your train/test/validation split is sufficiently random, your ten models should be relatively similar. This method of evaluation also isn't a good representation of what you would do in real life. In real life you build one model and have one chance to evaluate it. So cross validation as I explained in (1) is akin to simulating 10 different models, while your suggestion here is just as computationally expensive but only gives you 1 simulation of a real-life scenario. If you follow the process I laid out in (1), then generally you average across the 10 confusion matrixes of the test split against your model. I think we've covered how this isn't best practice, and I'm not sure I entirely understand your suggestion here. Feel free to comment.
