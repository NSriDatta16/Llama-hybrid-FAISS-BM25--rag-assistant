[site]: crossvalidated
[post_id]: 331368
[parent_id]: 328075
[tags]: 
The nice property of the CTC loss is that it trains the network in a segmentation-free manner. At the end of training, the network is able to handle these situations (which might occur in your use-case) without manually coding any special cases: beginning of text in images differs from image to image: no problem, empty area will be marked by CTC-blanks one character occupies multiple time-steps: no problem, they will be merged the size of the region of interest differs from image to image: no problem, the characters are repeated as often as needed So, using CTC loss should be ok for your use-case. However, you could implement a special CTC decoder which only allows valid strings. License plates do not contain random character strings, so maybe it is possible to describe a license plate text with a regular expression!? Then, you could adapt a CTC decoding algorithm such as beam search decoding (1) to only keep those beams describing valid license plates. (1) you could use my decoder as a starting point, throw away the language model and instead score the beams by a regular expression: https://github.com/githubharald/CTCDecoder/blob/master/src/BeamSearch.py
