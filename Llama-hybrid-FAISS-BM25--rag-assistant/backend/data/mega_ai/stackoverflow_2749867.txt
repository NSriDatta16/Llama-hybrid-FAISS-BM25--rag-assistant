[site]: stackoverflow
[post_id]: 2749867
[parent_id]: 2749821
[tags]: 
You've got two sides of this thing that are obvious. The first is how you will store the recipes, which will be models. Obviously Models will not be scraping other sites, since they have a single responsibility: storing valid data. Your controller(s), which will initiate the scraping and storage process, should not contain the scraping code either (though they will call it). While in Ruby we don't go for abstract classes nor interfaces -- it's duck-typed, so it's enough that your scrapers implement a known method or set of methods -- your scraping engines should all be similar, especially in terms of the public methods they expose. You will put your scrapers -- and here's the lame answer -- wherever you want. lib is fine, but if you want to make a plugin that might not be a bad idea either. See my question here - with a stunning answer by famous Rails-guy Yehuda Katz - for some other ideas, but in general: there is no right answer. There are some wrong ones, though.
