[site]: crossvalidated
[post_id]: 358318
[parent_id]: 
[tags]: 
Tossing coin and classical ML estimate

I'm reading Bishop's Pattern recognition and came across with the next on the p.23: Suppose, for instance, that a fair-looking coin is tossed three times and lands heads each time. A classical maximum likelihood estimate of the probability of landing heads would give 1, implying that all future tosses will land heads! By contrast, a Bayesian approach with any reasonable prior will lead to a much less extreme conclusion. Could you explain please, why it is so?
