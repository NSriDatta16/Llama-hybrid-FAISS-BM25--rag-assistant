[site]: crossvalidated
[post_id]: 512016
[parent_id]: 
[tags]: 
Why use KL-Divergence as loss over MLE?

I have came across this statement several time now Maximizing likelihood is equivalent to minimizing KL-Divergence (Sources: Kullbackâ€“Leibler divergence and Maximum likelihood as minimizing the dissimilarity between the empirical distriution and the model distribution ) I would like to know in applications such as VAE, why use KL- divergence then over MLE? In which applications would you choose one over the other? And any specific reason for it given both are equivalent?
