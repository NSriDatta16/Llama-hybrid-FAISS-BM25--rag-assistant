[site]: crossvalidated
[post_id]: 118472
[parent_id]: 118439
[tags]: 
Principal component analysis is reducing your original dataset of p variables to k uncorrelated, linear projections of your original variables. The first step, following the algorithm implementation, is deciding on the number of components to keep which is a somewhat subjective decision. There are multiple ways of deciding how many to keep including cumulative proportion of explained variance criterion - whereby you keep however many are needed to explain ~80% of total variance. Kaiserâ€™s Rule - whereby you retain as many PCs which have a variance larger than the average variance. Look at your summary output. Scree Plot - which is more or less a way to visualize the variance explained by each additional component and selecting the point at which the curve flattens out. This can be done in R if you used princomp with plot(pca_mod, type="lines") . Once you have determined the appropriate number of components, you have successfully reduced the dimensionality of your data! Now, what we can do next is entirely dependent on what your goals are. If you are looking to just explain which variables are important in the data you could then look at the individual component loading vectors. From the loadings you present, it appears that the most important variables are USD.HUG, USD.SEK, and USD.NOK. However, this is typically just to try and explain the PC, this may mean something significant to you or may mean nothing (hence the difficulty in PCA interpretation). You can think of principal components as projections of variables that are immeasurable (e.g. happiness, success, etc.). What you components actually may represent is for you to try and determine if such detail is important to you. If you are trying to reduce the original variables, I would suggest exploring other methods such as with stepAIC or if you prefer a multivariate approach Partial Least Squares typically has the Variable Importance in Projection metric. PCA is generally not used as a variable reduction tool in this sense. There are many variable selection methods out there. Now if you are just trying to get a predictive model with fewer variables (which are now your PCs) you can use PCA. You refit your pca model with princomp or prcomp and specify the optimal number of components are use the predict function. An example of this with the iris dataset has been shown in a similar question here
