[site]: crossvalidated
[post_id]: 595264
[parent_id]: 311810
[tags]: 
I don't have enough reputation for a comment, hence I add it as an answer. ai zhonguo's answer is correct. But one also has to consider that usually, neural networks layers are deterministic transformations of their input, hence you will have an infinitely large mutual information (see e.g. Mutual information between $X$ and $f(X)$ ). Using Bayesian neural networks or some other ways to include randomness could alleviate the issue. Another idea would be to use an alternative (but related) information measure such as $\mathcal{V}$ -information [1]. I once did an internship, where I tried something similar and found MINE [2] (from ai zhonguo's answer) also has serious shortcomings, you can find my technical report here if you are interested. [1] Yilun Xu, Shengjia Zhao, Jiaming Song, Russell Stewart, and Stefano Ermon. A theory of usable information under computational constraints, ICLR 2020 [2] Belghazi, Mohamed Ishmael, et al. "Mutual information neural estimation." International conference on machine learning. PMLR, 2018
