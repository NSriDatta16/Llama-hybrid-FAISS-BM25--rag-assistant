[site]: datascience
[post_id]: 81775
[parent_id]: 
[tags]: 
As RELU is not differentiable when it touches the x-axis, doesn't it effect training?

When I read about activation functions , I read that the reason we don't use step function is because, it is non differentiable which leads to problem in gradient descent. I am a beginner in deep learning , as Relu is almost a linear function and also non differentiable where it touches x-axis , why it performs so much better than tanh or sigmoid functions. And why is it so widely used in Deep learning. As it is non differentiable doesn't it affect in training?
