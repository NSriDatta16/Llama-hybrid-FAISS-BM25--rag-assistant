[site]: crossvalidated
[post_id]: 487886
[parent_id]: 
[tags]: 
Traditional machine learning often makes an assumption that both the training and test data are drawn from the same distribution?

It is said that traditional machine learning often makes an assumption that both the training and test data are drawn from the same distribution. But it isn't clear what this means in a practical sense. For instance, what would this mean in the case of image data, such as people's faces for facial recognition? In that example, why would one set of images of people's faces be "drawn from a different distribution" than another set of images of people's faces? I am looking for an explanation that connects this mathematical language of "drawn from the same distribution" to practical, real-world examples.
