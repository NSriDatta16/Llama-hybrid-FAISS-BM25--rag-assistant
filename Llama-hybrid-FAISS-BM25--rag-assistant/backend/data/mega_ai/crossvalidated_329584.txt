[site]: crossvalidated
[post_id]: 329584
[parent_id]: 
[tags]: 
Why does training a neural network to replicate general univariate random distributions require multiple sources of randomness?

As part of building a neural network (NN) to choose continuous-valued actions in an agent-based simulation, I wanted to first see how easy it would be to train a NN that could produce values drawn from distributions specified using the 4 standardized moments: mean, variance, skewness, and kurtosis. The inputs to my NN are IID uniform random numbers in $(0,1)$. There is only a single fully connected hidden layer with tanh activation and a single linear output node. I trained it using the cross-entropy method. Essentially this should approximate an inverse CDF. It turns out that it is easy to train it to replicate a distribution with a given mean and variance using only a single input and a couple hidden nodes, but to train it to also match a given skewness and kurtosis requires at least 3 inputs. Why is this? I was expecting to need additional hidden nodes, but it seems that at least 3 independent sources of randomness are needed: Input_Nodes| Error -----------| ---------- 1 | 1.377773 2 | 0.3911643 3 | 0.04052494 4 | 0.09945948 5 | 0.02489987 6 | 0.02465258 7 | 0.0424307 8 | 0.05257538 9 | 0.02362013 10 | 0.04844253 where Error is just sum of the mean-squared errors for each moment when tested on a million samples. Any tips on relevant papers would also be appreciated.
