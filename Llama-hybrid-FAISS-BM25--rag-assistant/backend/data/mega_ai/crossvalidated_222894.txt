[site]: crossvalidated
[post_id]: 222894
[parent_id]: 
[tags]: 
Eigenvalue equation for kernel PCA

In Nonlinear component analysis as a kernel eigenvalue problem , Sch√∂lkopf et al start by describing PCA. Given a set of data instances $x_1, \dots, x_M$, with $x_k \in \mathbb{R}^N, k=1,\dots,M$, and the corresponding $N\times N$ covariance matrix $C$, the principal components are found by solving, for each eigenvalue $\lambda \geq 0 $, $$ \lambda v = Cv $$ for $v \in \mathbb{R}^N \backslash \{0\}$. So far so good. However, the next argument states the following: since all solutions $v$ must lie in the span of $x_1, \dots, x_M$, then the following equation is equivalent to the previous one. \begin{array}{c c} \lambda x_k^Tv = x_k^TCv & \mbox{for } k=1,\dots, M \end{array} I see how this equation is equivalent to the first one, in that both sides are basically multiplied by the input data. What I don't see is how that is a consequence of the eigenvectors lying in the span of $x_1, \dots, x_M$. Wouldn't the equations be equivalent regardless of that fact? I suspect that this might be related to the fact that the projection of $x_k \in V$ onto $U=L(v_1, \dots, v_N)$ is only defined when $U\subseteq V$, but I still would like to be sure and know when the equations might not be equivalent.
