[site]: crossvalidated
[post_id]: 201197
[parent_id]: 201186
[tags]: 
"data is insensitive if B is between 1/3 and 3" This is indeed an artificial convention or conventional artifice and as such it is as arbitrary as the $0.05$ used as a beacon for $p$-value based decisions. What is almost always missing from the Bayesian toolbox is a calibration of the Bayes factor, namely to see how it behaves under the null predictive and the alternative predictive distributions, because this provides a range of possible (likely?) values under both models or hypotheses and hence a numerical reference to judge the strength of the observed Bayes factor. If the ranges of the Bayes factors under both models intersect too much, we can then conclude by the inconclusive decision! This Bayesian calibration is more of an idea that came to me when answering the OP question than a standard in Bayesian practice, the closest notion being the posterior predictive calibration in Gelman et al. (2013) Bayesian Data Analysis . (For instance, I do not mention this notion in my books .) "can one really always drive B out of the "insensitive range"" There are fairly generic results that, when the sample size grows to infinity, under some minimal requirements on the regularity of the distribution of the said sample, the Bayes factor goes to zero or to infinity depending on which proposed model is closest in the Kullback-Leibler sense to the true model behind the data. It is therefore the case that more data drives the Bayes factor away from the vicinity of $1$. "add more data, then are able to reject/accept H0 - say you then added even more data, and this leads to a new B-value whereby the opposite conclusion" On the one hand, this is cherry picking: if you keep collecting data until the Bayes factor is above a certain bound, it is no different from collecting data until the $p$ value is below a certain bound and you always end up rejecting whatever hypothesis is to be rejected (or die in the attempt if the data never strongly opposes the said hypothesis). On the other hand, the Bayes factor as such is not intended for this decision rule. It compares two marginal likelihoods as measures of evidence for the two models under comparison. While it a.s. converges to one of the limits $0$ or $\infty$, the theory does not say anything about a decision rule that compares the Bayes factor to a fixed or sequential bound.
