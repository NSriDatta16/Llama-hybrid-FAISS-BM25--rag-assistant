[site]: crossvalidated
[post_id]: 336894
[parent_id]: 
[tags]: 
Using a pre-trained VGG to get image embeddings

Possibly useful context: I am trying to create an implementation of Style2Vec which takes images of fashion items and creates an embedding of them in a vector space. The paper uses a pretrained-VGG to "vectorise" the images which are then used in a skip-gram architecture like Word2Vec, where one item is used as the context item and the rest of the set is used as targets to predict. Questions: If I want to feed the output from the VGG into the embedding layer does the embedding matrix need to have the same number of rows as my VGG vector has columns? Does this also have to equal the number of images in my training set if I hope I hope to read off each line as an embedding? Furthermore, since the output of the VGG is not a one-hot, then the VGG feature vector ends up "selecting" many rows in the embedding matrix - so how do we know what are the embeddings for an image? Do we use the product of the VGG vector and the embedding matrix?
