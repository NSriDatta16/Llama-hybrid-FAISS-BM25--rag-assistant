[site]: crossvalidated
[post_id]: 254082
[parent_id]: 254037
[tags]: 
When there are multiple questions getting at the same issue in a questionnaire, these are typically aded together to get a combined score. The individual questions are Likert items, with their combination providing a Likert scale . This helps get around several types of problems that can arise from depending too heavily on a single response to a single item. So the task of a designer of a questionnaire is more typically to evaluate how well a set of items all address the same issue, rather than to find a single "best" item as this question might suggest. I suppose you could consider this a case of making multicollinearity work for you. Cronbach's alpha is a standard way to evaluate whether a set of items is measuring the same thing, to evaluate their internal consistency. As Michael Chernick has noted in another answer, any plan to choose a "best" item based on p-values for relations to your outcome variable is fraught with difficulty. In addition to the multiple-testing statistical issues, it is likely that the "best" item found in one sample of answers to a set of items will be different from that found in another sample. Try repeating your item-selection protocol on multiple bootstrap samples of your data to illustrate that problem. This page is one of several useful threads on this site about design, validation, and interpretation of Likert items and scales; a search for Likert item shows many more. I understand the motivation to try to use a smaller number of questions, but that can pose problems as discussed on this page . If you have several items that you want to analyze as individual predictors rather than in a combined Likert scale, logistic regression as suggested in the question would be a possible approach, but with some cautions. The responses to these items are likely to have substantial multicollinearity, which can pose problems for any type of regression. Ridge regression is often a useful way to combine multicollinear predictors without running risks of overfitting, in logistic, linear, or Cox regression models. As with other regressions, you could incorporate transformations of the predictor variables if needed to get around issues arising from the distributions of the predictors or, more precisely, of their relations to outcome.
