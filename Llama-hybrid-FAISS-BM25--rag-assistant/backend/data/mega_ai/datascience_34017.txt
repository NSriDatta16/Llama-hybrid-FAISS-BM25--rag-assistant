[site]: datascience
[post_id]: 34017
[parent_id]: 34016
[tags]: 
Looks like to me this is a classic imbalance binary classification problem (see comments above). What loss are you using ? It looks like your model is predicting the non-membership class because it’s minimising it’s averaging loss. Here are some techniques you might wanna try to solve this issue: use regularization over sampling the membership class under sampling the non-membership class select variables, linked to regularization above with l1 and l2 penalties feature engineering: you specified you have 30 features but are all of them useful ? How do you preprocess them to feed the model ? Are they numerical or categorical ? I hope this gives you some ideas about how tackling the problem. Changing the model won’t miraculously solve your issue.
