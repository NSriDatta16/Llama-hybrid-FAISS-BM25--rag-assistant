[site]: crossvalidated
[post_id]: 386274
[parent_id]: 
[tags]: 
How does smoothing an image gives it a different scale?

In some deep learning papers i read about multiscale inputs, so i wanted to read about scale of an image. What i got to know is that fundamentally scale is related to the distance of the object being captured from the camera. source from where i read But to produce images at different scale i see that one smoothes it with gaussian filter, and varying its parameters we get image at different scales. This doesn't intuitively makes sense to me because objects in the image still remain at same size, they just get even more blurred. This way of achieving it seems to not be capturing the fundamental motivation behind image scale. Can someone please clarify ?
