[site]: crossvalidated
[post_id]: 605322
[parent_id]: 
[tags]: 
Trouble understanding expected value (does it assume infinite sample size?) and bias vs consistent

This might be a dumb question.. but I was wondering if someone can help me out with the concept expectation. This question started from trying to understand bias vs consistent. So when we roll a dice, we usually say that the expected value is 3.5 I sort of assumed that getting 3.5 means that we roll the dice infinite number of times. The average value of rolling a dice 10 times will not be 3.5. The more times we roll, the closer we get to 3.5 so I thought 3.5 meant that we're rolling it infinite (or realistically close to infinite) number of times. But, when learning the difference between bias vs consistent estimators, there was this estimator that is biased but consistent. That to me is sort of confusing because going back to the dice example, the expected value of 3.5, in my thought, meant that we are already rolling the dice infinite number of times.. Can some one help me out with this please? Thank you!
