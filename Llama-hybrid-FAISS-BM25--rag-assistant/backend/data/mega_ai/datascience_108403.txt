[site]: datascience
[post_id]: 108403
[parent_id]: 108386
[tags]: 
Reinforcement Learning is a paradigm of learning with reward and includes tons of methods. One categorization is between model-based RL and model-free RL. In model-based the agent knows all the possible contingencies from the current state and can evaluate the expected return which will help it to take an informative decision. Computationally this means that the agent has a model of how the environment changes states given an action. It also knows the outcome of a specific transition. Before the agent moves, it makes an estimation of what to expect given current state. In cognitive terms this is planning. Model-based is also referred to as response-outcome model. The reason for this is that given a change in the outcome (reward) the agent can adapt its response accordingly to optimize return. This is because it has a model of the world that tells it "what happens if". Model-free RL is related to stimulus-response learning or else habitual learning. The agent has no knowledge of "what happens if". Instead it has passed through an extensive period of trial and error till it managed to associate a particular state (stimulus) with a particular action (response) by (computationally) maintaining an estimation of expected return from the current state. TD methods belong to MF RL as they do not use a model in order to predict what happens in future transitions. In other words they do not perform planning. Computationally, TD methods are used to learn value functions (estimators of expected return) by accumulating sampled experience (i.e interacting with the task by selecting actions and observe the outcome). Of course there is a trade-off between exploring new actions or exploiting ones that their outcome has been estimated. In general, if I were to perform MB RL I would think the outcomes of all possible contingencies from my current state, up to a horizon. Then I would compare those and I would select the action that corresponds to the best one. In contrast if I were to perform MF RL I would simply try an action that either I think is good because in the past it led me to a positive outcome, or a random action in order to see if I can get an even better outcome. After the action I would observe the outcome and update how good is that action based on the new evidence. There have been some pathways in the brain that could computationally be described (approximately) by MF and MB RL. There has been research in pathways that combine both (learning a model of the world by experience). If you are interested in RL from computational neuroscience perspective you could search some tutorial on MB/MF RL by Nathaniel Daw or Yael Niv (they have written a lot) and work representative of the field by Peter Dayan - one of the pioneers on the subject - and Samuel Gershman.
