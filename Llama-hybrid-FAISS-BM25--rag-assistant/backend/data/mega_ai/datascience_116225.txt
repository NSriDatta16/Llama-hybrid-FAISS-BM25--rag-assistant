[site]: datascience
[post_id]: 116225
[parent_id]: 116217
[tags]: 
My question is, what is the most appropriate order? PCA creates "new" variables in a different dimensional space. I am not even sure most PCA libraries can handle missing values, and it doesn't make sense too (in this case, all of the output vector would be NaN since the output is a linear combination of all the input variables). So you first need to do any sort of imputation and then try to reduce the dimensions with PCA. when deleting variables with little data, what is an appropriate percentage of NAs per variable? There is no silver bullet solution I think. You can try to see what produces the best outcomes. In my experience, more than 20% NaN is too much (but it depends on the size of your data too, 50% on a dataset with a billion rows might provide enough information while 50% on 100 rows might be too little). In general, I would propose to First drop the features with too many missing data (what is too many is up to you to find) Then impute the missing values using whatever method you want (you mentioned KNN but even simpler methods like mean or mode or constant might work while taking a lot less time) Finally do the PCA (or any other method of feature selection or dimensionality reduction, since you mentioned in the comments that you wanted to drop features, not dimensionality reduction)
