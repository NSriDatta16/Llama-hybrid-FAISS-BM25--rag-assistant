In the history of artificial intelligence (AI), an AI winter is a period of reduced funding and interest in AI research. The field has experienced several hype cycles, followed by disappointment and criticism, followed by funding cuts, followed by renewed interest years or even decades later. The term first appeared in 1984 as the topic of a public debate at the annual meeting of AAAI (then called the "American Association of Artificial Intelligence"). Roger Schank and Marvin Minsky—two leading AI researchers who experienced the "winter" of the 1970s—warned the business community that enthusiasm for AI had spiraled out of control in the 1980s and that disappointment would certainly follow. They described a chain reaction, similar to a "nuclear winter", that would begin with pessimism in the AI community, followed by pessimism in the press, followed by a severe cutback in funding, followed by the end of serious research. Three years later the billion-dollar AI industry began to collapse. There were two major "winters" approximately 1974–1980 and 1987–2000, and several smaller episodes, including the following: 1966: failure of machine translation 1969: criticism of perceptrons (early, single-layer artificial neural networks) 1971–75: DARPA's frustration with the Speech Understanding Research program at Carnegie Mellon University 1973: large decrease in AI research in the United Kingdom in response to the Lighthill report 1973–74: DARPA's cutbacks to academic AI research in general 1987: collapse of the LISP machine market 1988: cancellation of new spending on AI by the Strategic Computing Initiative 1990s: many expert systems were abandoned 1990s: end of the Fifth Generation computer project's original goals Enthusiasm and optimism about AI has generally increased since its low point in the early 1990s. Beginning about 2012, interest in artificial intelligence (and especially the sub-field of machine learning) from the research and corporate communities led to a dramatic increase in funding and investment, leading to the current (as of 2025) AI boom. Early episodes Machine translation and the ALPAC report of 1966 Natural language processing (NLP) research has its roots in the early 1930s and began its existence with the work on machine translation (MT). However, significant advancements and applications began to emerge after the publication of Warren Weaver's influential memorandum, Machine translation of languages: fourteen essays in 1949. The memorandum generated great excitement within the research community. In the following years, notable events unfolded: IBM embarked on the development of the first machine, MIT appointed its first full-time professor in machine translation, and several conferences dedicated to MT took place. The culmination came with the public demonstration of the Georgetown–IBM machine, which garnered widespread attention in respected newspapers in 1954. Just like all AI booms that have been followed by desperate AI winters, the media tended to exaggerate the significance of these developments. Headlines about the Georgetown–IBM experiment proclaimed phrases like "The bilingual machine," "Robot brain translates Russian into King's English," and "Polyglot brainchild." However, the actual demonstration involved the translation of a curated set of only 49 Russian sentences into English, with the machine's vocabulary limited to just 250 words. To put things into perspective, a 2006 study made by Paul Nation found that humans need a vocabulary of around 8,000 to 9,000-word families to comprehend written texts with 98% accuracy. During the Cold War, the US government was particularly interested in the automatic, instant translation of Russian documents and scientific reports. The government aggressively supported efforts at machine translation starting in 1954. Another factor that propelled the field of mechanical translation was the interest shown by the Central Intelligence Agency (CIA). During that period,