[site]: crossvalidated
[post_id]: 373016
[parent_id]: 
[tags]: 
Calculating the optimization problem for a SVM in python

I am trying to understand, how SVMs work internally. Let's say, I have these two vectors $\begin{pmatrix} 1\\ 3 \end{pmatrix}$ and $\begin{pmatrix} 1\\ 1 \end{pmatrix}$ and the labels are 1 and -1. Then the optimization problem would be Min $\frac{1}{2}\vec{w}^T\vec{w}$ with $g_1 = 1 * (\vec{w} \begin{pmatrix} 1\\ 3 \end{pmatrix}+b) - 1\geq 0$ and $g_1 = -1 * (\vec{w} \begin{pmatrix} 1\\ 1 \end{pmatrix}+b) - 1\geq 0$ Right? So, where do I go from here? I saw code that initialitzes $\vec{w}$ to some value and then does gradient descent on it and for each value of it just checks if the contraints are correct for each data point. Is this a valid way to do this? If not, how I implement this in code in a simple way?
