[site]: crossvalidated
[post_id]: 309103
[parent_id]: 
[tags]: 
How do I diagnose overparameterization in Bayesian parameter estimation?

I am fitting a linear mixed effects model (with random slopes and intercepts, for subjects and items) containing several fixed effects and their interactions, using Bayesian parameter estimation. (In particular I am using the "brms" package in R.) I am using non-informative priors, and essentially using this approach to be able to use MCMC sampling to estimate my model. I seem to notice that this approach can handle more fixed and random effects, while still converging, than the "lme4" package. I used to use non-convergence, and perfectly correlated random intercepts, as red flag for overparameterization. However, using this Bayesian approach, my models converge well (diagnosed using Rhat). My question is: assuming my models converge, is there something else I should be checking to guard against overparameterization?
