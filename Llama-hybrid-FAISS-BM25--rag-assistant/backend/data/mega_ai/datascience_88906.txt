[site]: datascience
[post_id]: 88906
[parent_id]: 
[tags]: 
Data scaling for convolutional neural networks and other issues

My project involves deep learning on physiological data gathered from a wearable device. I aim to evaluate the potential CNN usefulness in classifying data collected from the wearable. The wearable collects 6 channels, each on a different scale and sampled from different distributions. The class variable is binary. One category makes up around 3/4 of the labels, and the other - the remaining 1/4. My dataset it quite small - 10s of subjects with ~30 files each. The files each contain ~15k timepoints, so to enrich the dataset, I segmented the data into 1k chunks. To enrich the data even more, I also created chunks from shifted windows. In other words, each chunk is followed by another chunk with a 50% overlap, resulting in a 3D NumPy array that has 2n-1 arrays of 1k timepoints and 6 channels. After creating these chunks, I shuffled them randomly and ran them through a simple CNN. Then, I normalized the data within each chunk and channel - once using simple Z-score transform, and then using MinMax scaling. The network I built in TF2 is very simple, consisting of the following layers: Conv2D(10, (1, 3), activation = 'relu' , input_shape = (1024, 6, 1); Note regarding this layer - the convolution occurs over data channels, but I also tried using convolution over the rows (the temporal dimension), and using a conventional 3x3 kernel too MaxPooling2D(pool_size=(1,3)) Dropout(0.2) Flatten() Dense(100, activation='relu') Dense(1, activation='sigmoid')]) I'm using Adam (default settings), with binary cross-entropy as the loss function. Eighty % of the data went into training, 20% to the test set. I encountered 2 problems: Overfitting when learning and testing on the normalized/standardized data Stagnation of both the training and validation accuracy when using the raw (untransformed) data I'm really a novice in deep learning, however from my viewpoint - the network isn't learning. I tried modifying the network by playing with the dropout proportion, adding batch normalization when working with untransformed data, using other optimizers and learning rates, and adding more dense layers. The bottom line is that nothing seems to work. My questions are, therefore: Should I have scaled the entire dataset prior to segmentation? Is adding additional convolution layers useful in this case? I'd appreciate any answer and viewpoints. Thanks for the patience and time. EDIT 2020-02-05: There's a mistake in the 4th paragraph; I've transformed the data before running it through the CNN, not after. Sorry for that.
