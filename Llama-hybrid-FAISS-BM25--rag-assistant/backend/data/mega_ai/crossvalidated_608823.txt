[site]: crossvalidated
[post_id]: 608823
[parent_id]: 
[tags]: 
Attention mechanism with different hidden states length?

i have two layer with different layer sizes (hidden states) how can i perform encoder decoder type of attention on these layers if the layer sizes are different? because i will do dot product, how? Consider I have two layers: lstm1 = LSTM(20, return_sequences=True, name='lstm') lstm2 = LSTM(40, return_sequences=True, name='lstm') How is possible to apply attention here since dot product would be not possible? Thanks
