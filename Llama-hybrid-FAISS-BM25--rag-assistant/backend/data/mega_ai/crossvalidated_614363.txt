[site]: crossvalidated
[post_id]: 614363
[parent_id]: 585860
[tags]: 
'LSTM are well known that they overcome the "long term dependency"' this is incorrect. See Copy Memory experiments in https://arxiv.org/abs/1901.08428 for evidences that LSTM suffers from long term memory at L=1000 and beyond 'thus $x_t$ is "decently reconstructable" until time t+N'. If RNN is stable (vanishing gradient is inevitable), the best case you would have is not a suddenly vanishing gradient at step t+N+1, but gradients become smaller and smaller as the sequence length increases. Generally, stacking layers means there are more pathway of signal between $y_{t_2}$ back to $x_{t_1}$ . For example, there are $(t_2-t_1)(t_2-t_1-1)+1$ path way for 2 layers. This reduces the chance for vanishing gradient to happen. p/s: In 3, the worst case scenario can still happen when vanishing gradient happens early (near $t_1$ ).
