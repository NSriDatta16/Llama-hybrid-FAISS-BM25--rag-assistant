[site]: crossvalidated
[post_id]: 568406
[parent_id]: 
[tags]: 
Is the non-multicollinearity assumption for OLS multiple regression just an assumption of convenience?

The four assumptions for bivariate regression are: • (L)inearity • (I)ndepdent observations • (N)ormal errors • (E)qual variance And for multiple regression we add a fifth assumption: • no multicollinearity between predictors. From a purely mathematical perspective, true multicollinearity makes the problem unsolvable in the sense that you cannot obtain a unique solution. But to be clear, this means there is perfect correlation between two or more predictors (or some subset of predictors lie on a smaller dimensional space). This, of course, is bad. But, in practice, the assessment of this assumption results in the removal of nearly multi-collinear predictors. To clarify, I understand one pragmatic reason for this: even if your predictor variables are not perfectly correlated, if they are very highly correlated, the resulting design matrix may be computationally indefinite (i.e., not truly indefinite, but not solvable with machine number computational limitations). This, too, is bad. Unfortunately, it seems to me that many textbook authors (particularly in the social sciences) have morphed the fifth assumption to "no NEARLY multi-collinear predictors". (E.g., remove items if the VIF is 10 or more...even though, the predictor variables may not be perfectly multi-collinear.) My question is this: ¿is this "revised" assumption a valid assumption? or ¿is this a hopeful imposition for convenience's sake? Note: This query is motivated for pedagogic purposes for introductory level statistics with applied focus (e.g., an applied statistics course for the social sciences).
