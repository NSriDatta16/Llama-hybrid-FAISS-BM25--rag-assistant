[site]: crossvalidated
[post_id]: 77311
[parent_id]: 
[tags]: 
Wide swings in SVM performance with different training/test sets

I'm trying to train a classifier on 10 classes, using 249 samples and a (currently) 16-dimensional feature vector. I'm using an SVM with RBF kernel, through Python's scikit-learn module. The distribution of classes in those 249 samples is as follows: [6, 45, 51, 38, 34, 23, 12, 15, 19, 6] When building my 200-sample training set, I'm careful to maintain this same relationship. I'm not currently applying any class weighting when training. I've used grid search to optimize my C and gamma values, and the feature vectors are normalized. My problem is this: When I randomly create different test/training sets from the samples (again, always maintaining the class frequency relationship above), the learned SVMs, when cross-validated against the test set, give values for precision/recall/f-score that will be anywhere from ~0.20 to ~0.50. This is making it hard for me to evaluate relative performance changes when trying different feature vectors. I should mention that the data is somewhat subjective; it consists of what I'll call 'squiggly lines' (to avoid the wrath of my employer) that have been graded by an expert we call the 'Golden Eyeball', who allegedly knows how and why some squiggles are better than others. So, we're all quite happy to be within +/- 1 grade of his 'true' grade. My question: I have some learning curve data suggesting that more training examples will help, but can I glean anything else from these differing performances? Or does this just tell me my model is flawed? And a side question: Am I fooling myself in thinking I can choose the SVM with the best performance and say 'Done!'? For the visual: I like making animations to quickly scan through the different SVMs. The one below shows the confusion matrix for 20 different SVMs taught per above. Probably not overly helpful, but I think I can sometimes see 'flashes of brilliance' in some of them that keeps me plugging away. (The size of the blocks above is relative to the proportion of samples in that class; predicted class is on the horizontal axis)
