[site]: datascience
[post_id]: 81233
[parent_id]: 
[tags]: 
Adaboost with other classifier fitting

There is the opportunity to fit decision trees with other decision trees. For example: adaclassification= AdaBoostClassifier(RandomForestClassifier(n_jobs=-1)) adaclassification.fit(X_train,y_train) I got better results with random forest, so improved the result from adaboost with the random forest classifier. However I dont understand what´s happening here? It sounds easy: adaboost uses a random forest to fit it´s classification. But what´s mathematically going on here? Adaboost is made of the residuals as a sequence (boosting). Random forest (bagging) built a forest out of trees.
