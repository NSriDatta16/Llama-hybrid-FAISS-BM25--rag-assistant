[site]: crossvalidated
[post_id]: 184736
[parent_id]: 184561
[tags]: 
I think I've got it. Let $X$ be our data matrix, with dimensions $n\times p$. Then, let the column vectors of $X$ be denoted as $\vec{x}_i$, where $\vec{x}_i \in\mathbb{R}^n, i=1,\ldots,p$. After PCA, we have the principal components decomposition of $X$ as $T=XW$, where $W$ is a $p\times p$ matrix whose columns are the eigenvectors of $X^T X$. Denote the corresponding eigenvalues $\lambda_i, i=1,\ldots,p$. Because the loadings (columns of $T$, which we denote $\vec{t}_i$) are eigenvectors of $X^T X$, we have: $$(X^T X) \vec{t}_1 = \lambda_1 \vec{t}_1$$ Denote the first factor to be $\vec{f}_1$; then, we have: $$X^T \vec{f}_1 = \lambda_1 \vec{t}_1$$ $$\vec{c}_i^T \vec{f}_1 = \lambda_1 t_{(1,i)}$$ for $i=1,\ldots,p$. Our regression model is: $$\vec{c}_i \sim \vec{f}_1$$ $$\vec{c}_i = \beta_i \vec{f}_1 + \vec{\epsilon}_i$$ Substituting, we have: $$\left(\beta_i\vec{f}_1 + \vec{\epsilon}_i\right)^T \vec{f}_1 = \lambda_1 t_{(1,i)}$$ $$\beta_i \vec{f}_1^T\vec{f}_1 + \vec{\epsilon}_i^T\vec{f}_1 = \lambda_1 t_{(1,i)}$$ $$\beta_i = \frac{\lambda_1 t_{(1,i)} - \left(\vec{\epsilon}_i^T \vec{f}_1\right)}{\vec{f}_1^T\vec{f}_1}$$ But, note that: $$\vec{f}_1^T\vec{f}_1 = (A\vec{t}_1)^T(A\vec{t}_1) = \vec{t}_1^TA^TA\vec{t}_1$$ and because $\vec{t}_1$ is an eigenvector of $A^TA$ that is simply equal to: $$\vec{f}_1^T\vec{f}_1 = \lambda_1 \vec{t}_1^T\vec{t}_1 = \lambda_1$$ so finally we arrive at: $$\beta_i = \frac{\lambda_1 t_{(1,i)} - \left(\vec{\epsilon}_i^T \vec{f}_1\right)}{\lambda_1} = t_{(1,i)} - \left(\frac{\vec{\epsilon}_i^T\vec{f}_1}{\lambda_1}\right)$$ which, on expectation, yields the result given above.
