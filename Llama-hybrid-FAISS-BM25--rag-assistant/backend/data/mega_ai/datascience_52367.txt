[site]: datascience
[post_id]: 52367
[parent_id]: 
[tags]: 
What are tokens and tokenizations?

I'm a high school senior who is new to data science, and would like to get into natural language processing. I currently know nothing about NLP, and the information online can be overwhelming. What are tokens? What are they used for / why do we need to tokenize text?
