[site]: crossvalidated
[post_id]: 606250
[parent_id]: 
[tags]: 
Neural network activation functions for continuous outcomes

I understand we should always scale/normalize the variables in ANN models (correct me if I am wrong), but still, I was wondering if certain activation functions, like sigmoid, can be used when the output neuron of a neural network is a continous, not scaled/normalized variable? And if not, would it change if the variable had been normalized? In those lines, I suppose there would be no issues with using activation functions like softplus and ReLU (in addition to linear) if the output neuron is a continous, not scaled/normalized variable, right? Thank you in advance.
