[site]: datascience
[post_id]: 43662
[parent_id]: 43444
[tags]: 
I have no reputation to add a comment. I think Marcelo Silva gave a very nice answer (don't know how to link his name). In "Muñoz-Mas, R., Fukuda, S., Vezza, P., Martínez-Capel, F., 2016. Comparing four methods for decision-tree induction: A case study on the invasive Iberian gudgeon (Gobio lozanoi; Doadrio and Madeira, 2004). Ecol. Inform. 34, 22–34. 10.1016/j.ecoinf.2016.04.011" we used a wrapper approach to simultaneously search for the best hyper-parameters and variables’ subset using cross-validation and a genetic algorithm. We made a nice review in that paper so it maybe deserves a check. The forest-based variant is currently on review. I'm the first author so write me in case you want to discuss anything about trees and forests. An alternative will be to use conditional random forests “Strobl, C., Hothorn, Zeileis, A., 2009. Party on! R J. 1 (2), 14–17. (and linked references)” employing the entire variables subset to then use sequentially only those that proved most relevant to train the decision-tree. Nevertheless, take into account that hyper-parameters play a role in the final decision-tree so they will need also some tuning. In case you are not interested on interpretability I would use a forest instead of a single decision-tree. Good luck.
