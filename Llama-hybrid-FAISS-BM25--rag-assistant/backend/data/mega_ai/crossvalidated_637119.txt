[site]: crossvalidated
[post_id]: 637119
[parent_id]: 
[tags]: 
Understanding the variance of a value vector (RL Theory)

I am self studying "Reinforcement Learning: Theory and Algorithms" by (Agarwal et al. 2022) and would like some help understanding the variance of a value vector. They write that for $V \in \mathbb{R}^{|S|}$ we define the vector $Var_P(V) \in \mathbb{R}^{|S|\times |A|}$ as $ Var_P(V)(s,a) = Var_{P(\cdot | s,a)}(V)$ where $s' \sim P(\cdot | s,a)$ is the probability distribution of the next state $s'$ . I do not understand what to make of $ Var_P(V)(s,a) = Var_{P(\cdot | s,a)}(V)$ - how can we interpret the variance of a vector to be a scalar? I believe it must be a scalar since we have $Var_P(V) \in \mathbb{R}^{|S|\times |A|}$ .
