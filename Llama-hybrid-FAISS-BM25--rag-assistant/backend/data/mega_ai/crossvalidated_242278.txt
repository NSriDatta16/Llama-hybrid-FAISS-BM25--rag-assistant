[site]: crossvalidated
[post_id]: 242278
[parent_id]: 
[tags]: 
Question about variational autoencoder gradient

The following is from Section 2.2 of the Auto-Encoding Variational Bayes paper, It says the gradient of the lower bound w.r.t $\phi$ is a bit problematic because the Monte Carlo estimator exhibits very high variance. Why? Is it because sampling from $q_\phi(z|x)$ is of high variance? What about the gradient w.r.t $\theta$ ? $$\nabla_\theta E_{q_\phi(z|x)}[\log p_\theta(x|z)]=E_{q_\phi(z|x)}[\nabla_\theta\log p_\theta(x|z)]$$ Why is this not described as problematic? A note to myself My original confusion was that since the expectation is essentially an integral, $$\int_z q_\phi(z)f_\theta(z)$$ why its derivative w.r.t $\phi$ is more difficult than the derivative w.r.t $\theta$ ? Now I seem to understand that the gradient w.r.t $\theta$ can be approximated approximate it by sampling from distribution $q_\phi(z)$ , whereas $f_\theta(z)$ is not a distribution so we cannot do the same for $\phi$ . As an alternative we may use the log derivative trick as mentioned in the paper, but it's usually of high variance (as mentioned in the accepted answer).
