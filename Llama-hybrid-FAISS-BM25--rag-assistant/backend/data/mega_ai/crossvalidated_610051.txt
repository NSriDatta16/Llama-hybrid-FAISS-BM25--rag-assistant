[site]: crossvalidated
[post_id]: 610051
[parent_id]: 435031
[tags]: 
Random effects (e.g. a random effect of genre on the intercept and/or on gender) are one option here, where smaller categories (i.e. genres with fewer data points) would be shrunk towards the average genre and larger ones (with more data points) are subject to less shrinkage. One can also approach this with multiple hierarchy levels if some genres are related at some higher level and others are not. Random effects are still pretty interpretable (it's really just a form of data informed shrinkage for regression coefficients), so it's an option for both inference and prediction. If you goal is solely prediction and you don't care so much about interpretability, there's quite few other options usually to do with how you represent the input: Embeddings: e.g. embedding layers within a neural network either trained during training a neural network that you use for this task, taken from such a NN but used in a different model, or trained with some other target & then used, could e.g. be predicting which types of genres get played together on the same radio station the embedding for the genre name from a large language model (or just something like word2vec) anything else that already exist or that you could do Target encoding (kind of a form of random effect, where you reduce a categorical variable to some kind of - possibly regularized - summary of the outcome split by the categories, requires careful definition/handling to avoid overfitting/target leakage) Frequency (or some other encoding): use the count of how often the genre is listened to as a numeric representation Plus, if you primarily want to predict, you also don't need to necessarily use a GLM, but various other models (e.g. LightGBM, neural networks, ensembles of these etc.) could be options.
