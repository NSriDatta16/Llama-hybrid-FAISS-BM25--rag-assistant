[site]: crossvalidated
[post_id]: 97511
[parent_id]: 
[tags]: 
What can be learnt from the sampling distribution of posterior probabilities?

I have an rather open question. In Bayesian statistics you do testing based on some posterior distribution $p(\theta|D)$. E.g. you could try something like $T=P[\theta>0|D]$ and decide based on the threshold $.5$. $T$ is basically some statistics of the data observed - and as such has a sampling distribution. Could I not get frequentists bounds on $T$ that guarantee a certain error niveau using the null? Am I completely mistaken here, is it just useless or does this approach have a tag I could not find? The cool thing would be having a frequentist test proceedure and instead of messing around with 'almost' significant p-values you just report the statistic $T$ itself? Is this related to conditional testing?
