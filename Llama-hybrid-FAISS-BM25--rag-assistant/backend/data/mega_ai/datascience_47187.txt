[site]: datascience
[post_id]: 47187
[parent_id]: 
[tags]: 
How do Bayesian methods do automatic feature selection?

Someone asked me this question and I do not know I answered it correctly. I answered the question in the following way: One type of Bayesian method is Bayesian inference and feature selection has to do with ${L}^{1}$ regularization because it is used extensively for this purpose. So, for ${L}^{1}$ regularization, the penalty $\alpha \Omega (\boldsymbol{w}) = \alpha \sum_{i} |w_{i}|$ used to regularize a cost function is equivalent to the log-prior term that is maximized by MAP Bayesian inference when the prior is an isotropic Laplace distribution. But my question is this an automatic feature selection? ${L}^{1}$ regularization finds the specific subset of the available features to be used. Also is my answer correct to this question? I am just curious to know if my line of thinking makes sense or if it does not. If it does not, please let me know why. Thanks
