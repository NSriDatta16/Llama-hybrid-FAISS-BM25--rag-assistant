[site]: datascience
[post_id]: 82772
[parent_id]: 
[tags]: 
Practical attention models

Attention is all you need is a nice paper that suggests using positional encodings as an alternative to RNNs in their Transformer architecture. GPT-2 and GPT-3 are examples of using this architecture which are trained on input data of a massive scale. Is there a paper and a model that uses positional encodings and outcompetes RNN/LSTM based models for small scale datasets (MBs of text data, not terabytes)? If there are many, which ones are the leading ones in production applications?
