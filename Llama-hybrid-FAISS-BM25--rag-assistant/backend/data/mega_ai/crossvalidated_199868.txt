[site]: crossvalidated
[post_id]: 199868
[parent_id]: 96588
[tags]: 
Let me take a stab at this. The term "fuzzy neuron" is in a sense a fuzzy set in and of itself, given that it has a broad definition encompassing a panoply of different types of neurons, which have different types of "fuzz" added to different internal components. The easiest way to explain it is as a Cartesian product of 1) all of the different types of subcomponents a neuron can have; 2) the type of fuzzy set used and 3) the type of uncertainty the fuzzy set is supposed to reduce. The point of fuzzification is to apply a continuous scale to non-continuous data, in cases where some imprecision can be tolerated and some practical benefit would be derived from treating the data as continuous. In many cases this is not justified because using fuzzy sets requires more computational overhead and skill in interpretation, and can even lead to misleading answers when incorrectly applied. The goal is to reduce one of at least three different types of uncertainty, by letting fuzzy sets capture some of the information lost when we can't quite use a definite continuous scale. Nonspecificity - which has to do with uncertainty about the number of state descriptions an object can have Imprecision in the boundaries of a set, such as whether or not a particular object qualifies as a member; I believe measurement uncertainty is subsumed within this Discord, i.e. conflicting evidence The best discussion of this topic I've seen to date is "Fuzzy sets and Fuzzy Logic: Theory and Applications," where authors George Klir and Bo Yuan cautioned that many other types of uncertainty may be discovered in the future. Since they wrote that 20 years ago, there may well be others to add to this list. Also keep in mind that these types of uncertainty are closely linked to at least five different fields, in which fuzzy terms take on additional nuances in interpretation: Ordinary Fuzzy Sets Possibility theory - in which the membership functions are interpreted as degrees of possibility of an event occurring Probability Information Theory - which is mainly tied to nonspecificity and the Hartley Function Dempster-Shafer Evidence Theory - in which conflicting evidence like discord is weighed and membership functions are interpreted as degrees of plausibility, belief and the like We also have to factor in the fact that there are many different types of fuzzy sets, such as: Interval-Valued fuzzy sets - for situations where we know a value is between two boundaries Level-2 Fuzzy Sets - in which the boundaries of an Interval-Valued fuzzy set are themselves fuzzy. Note that this principle can be extended to each level of boundaries, in an infinite regression Rough sets - which distinguish between objects Fuzzy numbers - which quantify such linguistic terms as "about" or "almost all" Intuitionistic Sets and Vague Sets, which apply different types and numbers of membership functions, rather than just the one used in ordinary fuzzy sets. Q-Sets for the indeterminacy seen in quantum physics, which I believe might be capable of quantifying the existence of Schroedinger's Cat Fuzzy sets to express different types of fuzzy logic There are many others, such as shadowed sets etc. All of the above can be applied to any combination of the following constituent components of neural nets: • Input Values - in some neural nets, we have to feed in the state of some external object on each pass, such as a gameboard. If there's some uncertainty about the values, we could use an ordinary membership function with a probabilistic interpretation, or perhaps use an interval-valued set, if we at least know the limits of the values. • Activation Values • Fuzzy thresholds • Fuzzified connection weights • Output Values or Functions - Such as outputting a fuzzy number • Fuzzy backpropagation and error-correction values, such as sending a fuzzy correction term backwards through the net • Fuzzy control parameters, like inputting a fuzzy number for a Learning Rate or whatever; there's plenty of uncertainty about what the correct parameters ought to be for many neural nets, but fuzzy numbers might be useful in representing the most likely range of values, for example • Fuzzy coordinates in a Kohonen net, or other types that depend on location Note that this principle also extends to whether or not a particular object is of a certain type, including the basic building blocks of a neural net: • Fuzzy connections - Does this object qualify as a connection between two neurons? This is something very different from a fuzzy weight, although the two concepts can be used together. • Fuzzy layers - Does this collection of neurons qualify as a layer? Does a particular neuron belong to it? • Fuzzy neurons (in terms of fuzzy object definition) - Tries to approximate an answer to the question, is it actually a neuron? One practical use of this might be to address the fact that as we multiply membership functions and weights in fuzzy sets and the related theories, they sometimes begin to resemble neural nets themselves. It might also be useful to rate how much an object corresponds to a particular type of neuron, if you're using more than one type in a design or doing biological simulations. All of the above could be applied together in a multiplicity of different configurations. The sky's the limit; therefore, a "fuzzy neuron" can signify many different things, depending on the application. Note how this quickly turns into an exercise in combinatorial explosion; capturing uncertainty in fuzzy sets gives rise to a whole cornucopia of different combinations of the above elements. Ordinary stats and calculations like averages and set joins that have straightforward answers in crisp sets suddenly give way to dozens of choices, which bring their own particular math formulas and nuances in interpretation - many of which have yet to be studied adequately, from what I can gather. The additional expressive power of fuzzy sets is thus a double-edged sword. The best principle to help guide us through the tangle of all these potential permutations is this: ask where the "fuzz" naturally occurs, rather than adding it just for the sake of the buzzword. Only ask where it might be useful to capture previously unquantified information on a continuous scale, at the cost of some loss of accuracy. If it doesn't pay to use fuzzy thresholds in conjunction with probabilistic input states that output fuzzy numbers etc. etc. ad nauseum, then don't. The same is true if the added complexity might lead to erroneous interpretations or misleading conclusions. I hope this helps.
