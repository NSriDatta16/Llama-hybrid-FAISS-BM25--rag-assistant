[site]: crossvalidated
[post_id]: 474750
[parent_id]: 474738
[tags]: 
I like your question a lot. People often talk about overfitting, but may be not too many people realized that intentionally design an overfitting model is not a trivial task! Especially with large amount of data. In the past, the data size is often limited. For example, couple hundreds data points. Then it is easy to have some overfitted model. However, in "modern machine learning", the training data can be huge, say million of images, if any model can overfit it, then that would be already a great achievement. So my answer to your question is, not an easy task, unless you are cheating by reduce your sample size.
