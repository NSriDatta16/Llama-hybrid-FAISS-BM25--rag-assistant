[site]: datascience
[post_id]: 49585
[parent_id]: 49578
[tags]: 
Before we start keep in mind that in most cases it doesn't play much of a difference which of the two you'll choose. Now to answer your question, generally speaking the choice should be made based on what model you want to employ: If you use a distance-based estimator (e.g. k-NN, k-means) it's better to normalize your features so that they occupy the same exact range of values (i.e. $[0,1]$ ). This forces your estimator to treat each feature with equal importance. If you're using Neural Networks , it's better to standardize your features, because gradient descent has some useful properties when your data is centered around $0$ with unit variance. Tree -based algorithms don't require any form of scaling, so its irrelevant if you scale or normalize your features. As a rule of the thumb, I usually standardize the data (unless I'm going to strictly work with distance-based algorithms).
