[site]: crossvalidated
[post_id]: 492355
[parent_id]: 
[tags]: 
Cross-validation for hyperparameter tuning

I've read as many topics regarding hyperparameter tuning as I could, and I developed the following algorithm for hyperparameter tuning & final model building Split the data in train set (80%) & test set (20%) perform a k-fold cross-validation in the train set n times, changing the hyperparameters each time and choosing the ones that performed better in average on the validation sets. build the neural networks that performed better in step 2 (let's say, the top 3) and train with the whole train set (80%) Feed the NNs built in 3 with the test set and pick up the one that performed better. I see no flaws in this process, however recently I've been reading about nested CV and I don't know when to use it instead of the steps proposed here.
