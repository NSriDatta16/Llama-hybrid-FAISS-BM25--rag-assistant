[site]: datascience
[post_id]: 85340
[parent_id]: 
[tags]: 
no decrease loss and val_loss

I try to train a neural network for time series. I use some data from Covid, mainly the goal is knowing 14 days of number of people at hospital to predict the number at J+1. I have use some early stopping to not over fit, but almost one time over two the learning stop at patience+1 and there is no decrease of loss and val_loss. I have tries to move hyperparameters like learning rate but the problem is always here. Any guess? The main code is below and the whole code with data : https://github.com/paullaurain/prediction import os from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error from keras.models import Sequential from keras.layers import Dense from keras.callbacks import EarlyStopping from keras.callbacks import ModelCheckpoint from keras.models import load_model from keras.optimizers import Adam from sklearn.preprocessing import MinMaxScaler # fit a model def model_fit(data, config): # unpack config n_in,n_out, n_nodes, n_epochs, n_batch,p,pl = config # prepare data DATA = series_to_supervised(data, n_in, n_out) X, Y = DATA[:, :-n_out], DATA[:, n_in:] X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1) # define model model = Sequential() model.add(Dense(4*n_nodes, activation= 'relu', input_dim=n_in)) model.add(Dense(2*n_nodes, activation= 'relu')) model.add(Dense(n_nodes, activation= 'relu')) model.add(Dense(n_out, activation= 'relu')) model.compile(loss='mse' , optimizer='adam',metrics=['mse']) # fit es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=p) file='best_modelDense.hdf5' mc = ModelCheckpoint(filepath=file, monitor='loss', mode='min', verbose=0, save_best_only=True) history=model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=n_epochs, verbose=0,batch_size=n_batch, callbacks=[es,mc]) if pl: plt.plot(history.history['loss']) plt.plot(history.history['val_loss']) plt.title('model loss') plt.ylabel('loss') plt.xlabel('epoch') plt.legend(['train', 'test'], loc='upper left') plt.show() saved_model=load_model(file) os.remove(file) return history.history['val_loss'][-p], saved_model # repeat evaluation of a config def repeat_evaluate(data,n_test, config, n_repeat,plot): # rescale data scaler = MinMaxScaler(feature_range=(0, 1)) scaler = scaler.fit(data) scaled_data=scaler.transform(data) scores=[] for _ in range(n_repeat): score, model= model_fit(scaled_data[:-n_test], config) scores.append(score) # plot the prediction id asked if plot: y=[] x=[] for i in range(n_test,0,-1): y.append(float(model.predict(scaled_data[-14-i:-i].reshape(1,14)))) x.append(scaled_data[-i]) X=scaler.inverse_transform(x) plt.plot(X) Y=scaler.inverse_transform(np.array([y])) plt.plot(Y.reshape(10,1)) plt.title('result') plt.legend(['real', 'prdiction'], loc='upper left') plt.show() return scores # summarize model performance def summarize_scores(name, scores): # print a summary scores_m, score_std = mean(scores), std(scores) print( '%s: %.3f RMSE (+/- %.3f)' % (name, scores_m, score_std)) # box and whisker plot pyplot.boxplot(scores) pyplot.show() #setting variable n_in=14 n_out=1 n_repeat=5 n_test=10 # define config n_in, n_out, n_nodes, n_epochs, n_batch, pateince, draw loss config = [n_in, n_out, 10, 2000, 50, 200,True] # compute scores scores = repeat_evaluate(data,n_test, config, n_repeat, True) print(scores) # summarize scores summarize_scores('mlp ', scores) Result:
