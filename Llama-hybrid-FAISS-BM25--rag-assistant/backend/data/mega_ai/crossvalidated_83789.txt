[site]: crossvalidated
[post_id]: 83789
[parent_id]: 
[tags]: 
Choosing number of PCA components when multiple samples for each data point are available

Update : I posted an answer below describing my current attempts to approach this problem. I am facing a perennial problem of identifying significant principal components. This question has been discussed here a couple of times ( one , two and a very impressive thread three ), and the usual advice is to use either so called "parallel analysis" (a very misleading name in my opinion; it is essentially Monte Carlo estimation of eigenvalue spectrum under null) or Laplace/BIC approaches from Minka, 2000, Automatic Choice of Dimensionality for PCA . Apart from that, I am aware of some ways to perform cross-validation over data points (see Bro et al., 2008, Cross-validation of component models ), plus there is of course a good old approach of staring at the eigenvalue spectrum trying to locate an "elbow". All of these methods assume that you have one set of data points, let us say $N$ points from $\mathbb{R}^D$, and nothing apart from that. In my case, however, each point is an average of $M$ points, which are repeated measurements of the same value. This makes me think that I can somehow infer which PCA components are significant, by using the data that goes into averages; maybe by some cross-validation or bootstrapping approach. My question is: how? Update . Following discussion in the comments, I should clarify what I mean by "significant". For this I will introduce some notation. There are $N$ points $\bar{\mathbf{x}}_i \in \mathbb{R}^D$, and each point is an average over $M$ repeated measurements $\bar{\mathbf{x}}_i=\frac{1}{M}\sum_{j=1}^M \mathbf{x}_i^{(j)}$. Each measurement is a noisy observation of a true position of the corresponding point, i.e. $\mathbf{x}_i^{(j)}=\mathbf{a}_i + \mathbf{\xi}_i^{(j)}$ (note that $\xi$ is a $D$-dimensional vector as well, but I cannot make Greek letters bold here). I am interested in principal components of $\{\mathbf{a}_i\}$, but can only perform PCA on $\{\bar{\mathbf{x}}_i\}$, and the small components can be completely distorted by the noise. The larger the noise, the less principal components of $\{\mathbf{a}_i\}$ I will be able to recover. The challenge is to select only those leading PCs that definitely "come from" $\{\mathbf{a}_i\}$. In my answer below I describe two approaches that I currently use. I am not fully happy with either.
