[site]: crossvalidated
[post_id]: 347734
[parent_id]: 347727
[tags]: 
After learning the absolute basics of hypothesis testing and getting to the part about one vs two tailed tests... I understand the basic math and increased detection ability of one tailed tests, etc... But I just can't wrap around my head around one thing... What's the point? I'm really failing to understand why you should split your alpha between the two extremes when your is sample result can only be in one or the other, or neither. The problem is that you don't know the population mean. I have never encountered a real world scenario that I know the true population mean. Take the example scenario from the quoted text above. How could you possibly "fail to test" for a result in the opposite direction? You have your sample mean. You have your population mean. Simple arithmetic tells you which is higher. What is there to test, or fail to test, in the opposite direction? What's stopping you just starting from scratch with the opposite hypothesis if you clearly see that the sample mean is way off in the other direction? I read your paragraph several times, but I'm still not sure about your arguments. Do you want to rephrase it? You fail to "test" if your data doesn't land you in your chosen critical regions. I assume this also applies to switching the polarity of your one-tailed test. But how is this "doctored" result any less valid than if you had simply chosen the correct one-tailed test in the first place? The quote is correct because hacking a p-value is inappropriate. How much do we know about p-hacking "in the wild"? has more details. Clearly I am missing a big part of the picture here. It all just seems too arbitrary. Which it is, I guess, in the sense that what denotes "statistically significant" - 95%, 99%, 99.9%... Is arbitrary to begin with. Help? It is arbitrary. That's why data scientists generally report the magnitude of the p-value itself (not just significant or insignificant), and also the effects size.
