[site]: crossvalidated
[post_id]: 485243
[parent_id]: 485194
[tags]: 
That means your test set mimics the training set better than your validation set. Considering the size of your training dataset (i.e. 703) this is possible. And, that's not guaranteed in production. What if you were using your test set as validation, and your validation set as test? In that case, the situation would be the opposite, and you'd have chosen the model with 4 epochs (ignoring the other epochs for the sake of simplicity). The implication is that overfitting means high variance. So, in production, if your model is tested with a lot of samples, model 10 will have more variance in its decisions than model 4. Considering the gap between training and validation performances, both cases have some degree of overfitting, but situation in model 10 seems to be more serious. Apart from choosing amongst these two, you may also use cross-validation to either select the best epoch, or generate $k$ different models and average/vote their predictions. Considering the size of your data, this approach seems possible and will increase the stability of your predictions.
