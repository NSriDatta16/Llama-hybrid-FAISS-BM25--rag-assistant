[site]: crossvalidated
[post_id]: 474958
[parent_id]: 
[tags]: 
Bayesian interpretation of logistic ridge regression

Most textbooks ( also this blog ) cover the fact that ridge regression, $$ \hat y = \hat \beta X; \\ \hat \beta = \underset{\beta}{\text{argmin}}\ \ \frac{(y-\beta X)^T(y-\beta X)}{\sigma^2} + \lambda \|\beta\|_2^2; $$ can be interpreted as a MAP estimate of a Bayesian model with $N(0, \tau)$ priors on the $\beta$ parameters, where $$ \lambda = \frac{\sigma^2}{\tau^2} $$ What is the equivalent Bayesian interpretation of logistic ridge regression? $$ \hat y = logit^{-1}(\hat \beta X); \\ \hat \beta = \underset{\beta}{\text{argmin}}\ \ -y\ log(\hat y) - (1-y)\ log(1 - \hat y) + \lambda \|\beta\|_2^2; $$ I'm looking for this both out of theoretical interest, and because I would like to use stochastic gradient descent to estimate MAP logistic regression parameters in a context (JavaScript) where I don't have access to any specialised solvers.
