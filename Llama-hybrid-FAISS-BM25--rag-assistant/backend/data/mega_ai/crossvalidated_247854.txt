[site]: crossvalidated
[post_id]: 247854
[parent_id]: 
[tags]: 
Overfitting after feature engineering, data leakage?

I have a dataset with 2 continuous variables : cont and y (the target). I found that y_train, when ordered by cont, is auto-correlated so I would like to create a feature using this property. I use a KNN regression with k = 100 here to create this new feature with the goal of capturing the underlying trend. The problem I have is that the added feature yield overfitting. While traing an xgboost using cross-validation with the same folds as line 1 below, the training error is better but the testing error is worse (compared with training without the new feature). It seems like there is data leakage but I paid attention to not leak information while creating the feature since I trained on var[-fold], y_train[-fold] and built the prediction from var[fold], for each fold. Have you any idea why it's overfitting ? library(FNN) # For the knn regression library(caret) # For the createFolds library(xgboost) folds
