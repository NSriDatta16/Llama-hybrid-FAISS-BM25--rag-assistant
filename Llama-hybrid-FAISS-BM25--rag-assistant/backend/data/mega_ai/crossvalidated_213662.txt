[site]: crossvalidated
[post_id]: 213662
[parent_id]: 213571
[tags]: 
If I understand correctly, this leaves you with two tasks: model tuning and subsequent model selection (you should possibly consider multiple model types when choosing the best suited model for a particular task). For model tuning you could use a hyperparameter grid search, as you already did in the code above. If you get good results for certain ranges of parameters, it would be reasonable to employ a more fine grained parameter grid in those regions. You can approximate the "optimal" parameter configuration iteratively this way, but beware of possible over-optimizing and a resulting overfitting. BTW: more sophisticated approaches than parameter grid searches exist (e.g. genetic algorithms) - you could employ such in case your real data represents a more sophisticated problem where parameter search is quite difficult. As model selection can overfit as well, it is a good idea to evaluate it the same way and time as hyperparameter tuning. A reasonable choice would be to evaluate all desired model types and model hyperparameters using repeated cross validation , then chose an optimal model from the repeatedcv performances, and subsequently compute its real performance on a held-back and yet unseen test data set . # this snipped relies on the snipped in the question! train_model For models with hyperparameters, caret has a built in plotting function to visualize performance over hyperparameter sets. This is where you can see which parts of the grid performed well so far, and which parts you could possible make more fine grained: plot(models$svmLinear, scales=list(x=list(log=3))) plot(models$glmnet, scales=list(x=list(log=3))) Using caret::resamples different models can be compared to each other. This is what could be used to decide upon which model performed best on your task: resample
