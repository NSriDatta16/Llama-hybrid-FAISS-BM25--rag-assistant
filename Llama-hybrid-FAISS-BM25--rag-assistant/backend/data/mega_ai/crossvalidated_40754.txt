[site]: crossvalidated
[post_id]: 40754
[parent_id]: 40731
[tags]: 
Gauss it with ABC Gauss it – start with the simplest version that has (most of) the elements of the real question. ABC – Approximate Bayesian Computation – a direct two stage Monte-Carlo simulation where parameters are first drawn from the prior, then data is drawn using those parameters just drawn from the prior and only those draws that had exactly the same (or approximately) are kept. The distribution of parameters that are kept are a sample from the exact (approximate) posterior. This can be quickly worked out for the linked k~ Bin(N,p) question (yours will require a generation of data that you only saw summaries of). k~ Bin(N,p) question Define a prior for N (preferably with no fixed upper bound but here probably OK) Reps=10^6 N=sample(1:1000,Reps,replace=TRUE) Draw possible k from Bin(N,k) Pk=rbinom(Reps,N,prob=p) #recall p is known and k observed Just keep draws where pk = k Use that kept sample as an approximation of the posterior and say get a density estimate. (If you just want likelihood - no prior - then take c * posterior/prior as an approximation of that) For example say the observed k was 28, this cab be easily run in R and checked against the closed form likelihood which is simple case of Example 4 Integrated Likelihood Methods for Eliminating Nuisance Parameters http://www.stat.duke.edu/~berger/papers/brunero.pdf R code that can be run Reps=10^7 N=sample(1:1000,Reps,replace=TRUE) possible_k=rbinom(Reps,size=N,prob=.1) posterior_k=N[possible_k==28] hh=hist(posterior_k) lik=dbinom(28,1:1000,prob=.1) lines(1:1000,lik/max(lik) * max(hh$counts)) Now write the bugs or jags code, first for the tiny simple version of the question, note the posteriors are approx. the same, and then scale up to real question. Maybe do this in private and then destroy any evidence of having to do this warm up exercise . (Recall that “people go Bats* crazy when they see others doing what they do for a living a different way” quote from the Moneyball movie.) As for posterior probabilities, they are usually just formal and not of relevance to anyone. An exception would be the example by Jim Berger at 2009 International Workshop on Objective Bayes Methodology on HIV Vaccine trial. Really liked this from @Jason Aug 23’11 Bayesian methods are generative, in that they provide a complete "story" for how the data came into existence. Thus, they aren't simply pattern finders, but rather they are able to take into account the full reality of the situation at hand.
