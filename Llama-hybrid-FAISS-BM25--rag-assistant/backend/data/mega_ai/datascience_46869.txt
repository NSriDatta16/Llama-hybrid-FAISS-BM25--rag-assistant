[site]: datascience
[post_id]: 46869
[parent_id]: 
[tags]: 
How much batch effect is too much batch effect?

Algorithms such as ComBat/SVA are powerful tools for the removal of batch effects. Small batch effects can be confidently removed by these methods. But surely there must exist batch effects which are so large that even these algorithm cannot help. (For example, near perfect PCA clustering into the batches.) The only remedy is to re-measure the data. Is there a measure which allows me to distinguish between small and large batch effects? Is there a reliable theoretical foundation for normalization and imputation? What should I read/google?
