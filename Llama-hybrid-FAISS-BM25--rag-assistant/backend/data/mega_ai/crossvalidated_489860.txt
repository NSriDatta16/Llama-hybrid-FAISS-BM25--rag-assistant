[site]: crossvalidated
[post_id]: 489860
[parent_id]: 488890
[tags]: 
In the same way we don't know the form of the outcome model (which is why we use propensity score matching in the first place), we don't know whether regression completely removes all confounding in a matched sample. Matching makes it more plausible for confounding to be removed by regression; this is the main thesis of Ho, Imai, King, and Stuart (2007) , the paper that motivated the development of MatchIt . You should do as much work as you can in the matching phase to ensure bias is eliminated by assessing balance on the covariates not just on their means, but on their entire distributions and their interactions. The cobalt package provides many diagnostics for doing so and works with MatchIt objects. You should also do as much work as you can to flexibly model the outcome; although main effects regression of the outcome of the covariates and treatment may be sufficient, you can be more certain of the elimination of bias by using a flexible model, such as a generalized additive model, regression model with splines, or machine learning method like Bayesian additive regression trees (BART) or generalized boosted modeling (GBM; not the twang implementation). If either the matching is sufficient to eliminate imbalance or the outcome model is correct, the effect estimate will have low bias; this property is known as "double-robustness".
