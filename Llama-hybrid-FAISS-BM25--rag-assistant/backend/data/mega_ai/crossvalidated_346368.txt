[site]: crossvalidated
[post_id]: 346368
[parent_id]: 
[tags]: 
Policy gradient objective function

Following Lecture 4 of the RL course at UC Berkeley, I'm wondering why they use this as the objective function: Wouldn't a more accurate objective function be: $$ \theta* = \arg\max_{\theta} E_{s_1 \sim p_1(s_1)} V_{\theta}(s_1) $$ $$V_{\theta}(s_{t, \; 1 \leq t \leq T}) = E_{a \sim \pi_{\theta}(a_t|s_{1:t})} \; r(s_t, a_t) + V_{\theta}(s_{t+1})$$ $$V_{\theta}(s_{t, \; t > T}) = 0$$ I say that because in the lecture's example, they multiply the entire path probability by the reward for that path, whereas the second objective function weighs earlier rewards only by the probability of the path-so-far. Intuitively, the second objective function makes more sense to me, so I don't see why the first objective function is used.
