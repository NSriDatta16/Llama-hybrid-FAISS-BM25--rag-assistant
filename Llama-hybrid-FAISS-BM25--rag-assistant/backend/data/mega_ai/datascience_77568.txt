[site]: datascience
[post_id]: 77568
[parent_id]: 77550
[tags]: 
So the question is asking why the first two principal components of your encoded text data is encapsulating all of the variation in the data. One potential issue could be the averaging over word vectors. Suppose for a particular feature across word vectors for a particular post f , there could be an array of positive and negative values. When we then apply an average over f we could zero out the dimension and thus cause greater data sparsity, which could explain for what you are seeing (this zero value will exist regardless of whether you multiply this average with the td-idf or not). It could be the case that this sort of thing is happening across multiple dimensions in your text embeddings / feature vectors. With this, you might need to think of another way of deriving a text embedding, maybe used Doc2Vec instead, which follows the same principles as Word2Vec, but instead derives document embeddings, which encapsulates the meaning of a section of text instead of word embeddings, which encapsulates the meaning of an individual word within a section of text.
