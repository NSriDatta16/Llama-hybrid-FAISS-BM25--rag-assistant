[site]: crossvalidated
[post_id]: 521024
[parent_id]: 521021
[tags]: 
To formalize things a little bit, you are solving a binary classification problem where you have a modest level of class disbalance. It would be helpful to get a more detailed description of what you mean by selecting the best method and model. Given that you have a dedicated test dataset and, as far as I understood, you used cross-validation (by the way, which type did you use?) to fine-tune hyperparameters, it seems that there is nothing obviously wrong. That said, since your classes are imbalanced, double-check that you are using stratified splits so as to avoid the cases when one of the splits consists entirely of the first-class objects. It's also worth noting that using accuracy as a metric in such unbalanced scenarios is not the best approach. Getting an accuracy of $0.9$ + and most predictions as zeros makes sense and you should also pay attention to other metrics. What also matters for such tasks is determining how you are going to use your model in the future. It's often the case that trying to concentrate too much on 0/1 outcomes from the beginning does not correlate very much with the really useful possible applications of the model. Try to check other metrics like AUC ROC that can be much more representative of your model's true prediction capability (but please also keep in mind that this is a metric that evaluates your model's ranking talent and is a composite indicator across all possible thresholds, not just one as opposed to accuracy, precision, recall, f1. Another thought regarding the distribution of classes: as long as you are confident that it matches the data in the wild, there is no need to do downsampling or oversampling, but you could try and use the so-called "cost-sensitive learning" and use the class_weight="balanced" parameter available in most sklearn-ish libraries.
