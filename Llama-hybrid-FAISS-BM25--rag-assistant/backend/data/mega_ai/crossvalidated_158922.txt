[site]: crossvalidated
[post_id]: 158922
[parent_id]: 158863
[tags]: 
You should be able to make use of the information that standard deviation is related to mean, and it may yield greater power ... but I wouldn't use a t-test for that. However , you've already tested the data; consider what would have happened if you'd instead had a nice significant result -- would you really have done any more than the unpooled t-test*? *(presumably Welch-Satterthwaite) If the honest answer is you'd have stopped there$^\dagger$, then to start looking for some more powerful procedure now is significance-hunting, plain and simple, and if we account for the effect of that on p-values -- well, your p-values will go back up again. $\dagger$ (on the other hand, if you'd claim you would have searched for a better test even if it was highly significant , then what the heck were you doing calculating it in the first place? Figure out what you're doing before you start playing with the data to see how significant it is -- what else is anyone to make of it when you do?) Leaving aside the significance-hunting for the moment, there are several ways I might think about this; here are three: use a Gamma GLM (which has sd proportional to mean built into the model); take logs (which should yield almost constant standard-deviation); this means you're comparing means-of-logs when you do a t-test, which corresponds to a scale-shift alternative hypothesis (as does the GLM above); alternatively if you're looking for fairly general (tend-to-be-bigger) alternatives you could apply a Wilcoxon-Mann-Whitney test on the original scale (there's no need to transform, it already encompasses the effect of monotonic transforms). However, with 4 observations per group, you have low power , and no amount of cleverness can do much about it. You can improve your estimate of the error-variance by (i) up-front focusing attention on the A vs B comparison alone, but (ii) combining the data into an analysis where their information can be used to get a better handle on the variability, which should improve power. In the case of 2. above you'd use a pre-specified A-B contrast in the context of an ANOVA; in 1. above you'd set up something similar in the GLM. In the comparison of ANOVA vs a t-test (under option 2.) this should improve error df from 6 to about 30, which at 5% significance level on your particular contrast will be like reducing standard errors by 1/7.
