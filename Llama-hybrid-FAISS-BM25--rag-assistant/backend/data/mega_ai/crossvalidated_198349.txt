[site]: crossvalidated
[post_id]: 198349
[parent_id]: 
[tags]: 
Why does the lasso's learning curve suggests that, at low number of samples, there isn't overfiting?

I'm using the lasso penalty in a logistic regression model for classification and I ploted a learning curve using logloss as the performance metric. Here is what I obtained: As the plot shows the training error is always decrasing(in average). But, I was expecting a plot like the one I obtained unsing logistic regression without penalty: That is, I was expecting that: At low number of samples there is overfiting (low train error/high test error) But found that for the lasso penalty At low number of samples both train and test errors are high. The question is: Why does the lasso penalty avoids overfitting? How I potted the curves (pseudocode) for(sampleSize in c(100,200,...,1500)) repeat 100 times { select `sampleSize` random samples from the data train a model calculate Train Error calculate Test Error on the non-selected samples } calculate mean train error calculate mean test error I have gist of the acctual R code I used but I cant post a link because I need at least 10 reputation. So here is an offuscated link. gist.github.com argent0 9b2f82b1072253600aa8 Edit: removed extra word
