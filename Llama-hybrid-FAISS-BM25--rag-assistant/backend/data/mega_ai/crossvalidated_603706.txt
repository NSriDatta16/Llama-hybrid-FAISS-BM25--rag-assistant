[site]: crossvalidated
[post_id]: 603706
[parent_id]: 541487
[tags]: 
Under the assumption that you perform learning rate decay, training twice with 100 epochs gives likely slightly better results. The idea is that the algorithm might get stuck in a local minima and by restarting it is more likely to escape. There are multiple papers on this, e.g. ["A closer look at deep learning heuristics: Learning rate restarts, warmup and distillation", 2018]
