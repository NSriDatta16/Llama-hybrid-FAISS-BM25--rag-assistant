[site]: crossvalidated
[post_id]: 563936
[parent_id]: 563928
[tags]: 
I would express this model as $$\Pr(y_i=1 \vert t_i,s_i=k) = \Lambda \left[ \beta_0 + \beta_1 \mathbb{I}(t_i=1) + \beta_2 \mathbb{I}(t_{i} =2) + \gamma_{k} \right],$$ where $\Lambda(x)$ is the inverse logit function $\frac{\exp(x)}{1 + \exp(x)}$ . The two treatment effects are: $$TE_i(t_i=1) = \Lambda \left[ \beta_0 + \beta_1 + \gamma_{k} \right]-\Lambda \left[ \beta_0 + \gamma_{k} \right]$$ and $$TE_i(t_i=2)= \Lambda \left[ \beta_0 + \beta_2 + \gamma_{k} \right]-\Lambda \left[ \beta_0 + \gamma_{k} \right]$$ These give you the expected change in the probability of getting the question right for each treatment, measured relative to the control outcome for a student at school $k$ . You can also get the effect contrasting the two treatments in the same way. These are called finite differences, to distinguish them from a derivative-based effect you might use for a continuous treatment. People often calculate the average of these in the sample to get down to two single numbers. This can be a useful summary, especially if schools do not matter very much. The logit index functions coefficients are harder to interpret since they are not on the probability scale. They do have an interpretation on the log-odds scale, but in my experience, this is best avoided unless you are communicating with gamblers or that is the standard in your academic discipline. To measure the effect of being in any group, you can use a model where you group the two treatments into one to estimate $$\Pr(y_i=1 \vert t,s_i=k) = \Lambda \left[ \alpha_0 + \alpha \mathbb{I}(t_i =1 \textrm{ or }2) + \delta_{k} \right],$$ and proceed as above. Alternatively, you can use your original model and just multiply the $\beta$ s by the share of treated in each treatment group when you calculate the finite difference: $$TE_i = \Lambda \left[ \beta_0 + \beta_1 \frac{n_1}{n_1 + n_2} + \beta_1 \frac{n_2}{n_1 + n_2} + \gamma_{k} \right]- \Lambda \left[ \beta_0 + \gamma_{k} \right].$$ This is a bit of a strange counterfactual.
