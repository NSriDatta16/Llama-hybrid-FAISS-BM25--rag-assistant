[site]: crossvalidated
[post_id]: 297063
[parent_id]: 297057
[tags]: 
Random forest or gradient boosting (with decision trees) feature importances are very helpful here. [+] By nature, they don't ignore combinations. [+] No need to scale the data. [+] These are greedy methods, so they are much faster than non-greedy subset selection, which is NP-hard. Here [ 1 ] is even an example showing how to use GBC to view partial dependence plots, which could be useful as well. [-] These are greedy (and maybe random) methods, so they might give different answers each time. If possible, using bootstrapping or cross-validation with many iterations you can obtain a distribution over the importances (would recommend this for RF or GBC, but also for whatever you decide to use). Features that are correlated with others should have high dispersion. You must test your reduced model on held out data (i.e., don't use all your data to for the RF and then construct the linear model on a training set that is a subset of the data and then test it on some other subset of the data). Hold out data entirely and use it as test. Ideally, implement the feature selection as a step in the modeling process of a nested cross-validation procedure. This is more expensive, though, and requires more coding. Also, the added insight in terms of feature selection might be marginal. Also, this might be totally useless, but I recently dealt with data that was of similar dimension/ size with many categorical variables. I was able to almost halve the dimensionality simply by removing categorical features that had only been observed, say, fewer than 20 times. It's hard for any model to use those, and they're quite common in many domains.
