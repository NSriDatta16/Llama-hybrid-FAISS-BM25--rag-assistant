[site]: crossvalidated
[post_id]: 437091
[parent_id]: 
[tags]: 
Why is training of extremely deep fully-connected NNs difficult?

Practitioners know that if we increase the number of full-connected layers in Neural Network (NN), then at some points the NN performance starts to degrade. The natural reason is that we have classical overfitting, as we have more parameters to estimate. Among other possible reasons are problems with backpropagation of gradients through deep networks etc. My question is: are there any works that claim to found the ultimate reason for this effect and do they propose a way to fight performance degradation when working with a large number of fully-connected layers? Both statistical-learning-based and empirical-based answers are welcome.
