[site]: stackoverflow
[post_id]: 4754094
[parent_id]: 4751668
[tags]: 
You could do a simple substitution algorithm with a set conversion table that varies based on the power of the digit in the original number. Weight the values in the conversion tables so that vowels and certain consonants are more common. Pick some base large enough to have variety across the places. E.g. (hex based data): value | place | 0 1 2 ... ------|------ - - - 0 | a a a ... 1 | e e e 2 | i i i 3 | o o q 4 | u u w 5 | y q r 6 | q w f 7 | w r g 8 | r f h 9 | t g j A | p h k B | s j c C | d k v D | f l b E | g z n F | h x m ... (This could also be done with simple well chosen formulas for each column...) So, B4B => "suc" 3AA => "ohk" F62 => "iwm" ... Extend this out to enough columns to control the distribution of all the characters well. If your source data does not have a cleanly random distribution you may want to mix up the order from column to column as well. Notice how some characters exist in every column, and some only exist once. Also the vowel to consonant frequency can be tweaked by changing the average ratio in every column. Take large fixed size chunks of the data and run them through the converter, then apply a spacing/punctuation/capitalization algorithm. (There is no guarantee that you won't end up with an all consonant or extremely low vowel count word, but you can have the capitalization algorithm make it all caps to look like an acronym/initialism)
