[site]: datascience
[post_id]: 128044
[parent_id]: 
[tags]: 
My custom neural network is converging but keras model not

in most cases it is probably the other way round but... I have implemented a basic MLP neural network structure with backpropagation. My data is just a shifted quadratic function with 100 samples. I have created a network for regression with 1 hidden layer with 10 neurons with sigmoidal activation function and linear on the output. My algorithm is working perfectly fine, after enough epochs I get MSE around 10, which is good. On the other hand, when I try to train the same model using keras, it just cannot go below 1800 MSE - no matter what optimizers, what learning rate and how many epochs I use. Thats my model architecture which is super simple: model = Sequential([ Dense(10, input_dim=1, activation='sigmoid'), Dense(1, activation='linear') ]) model.compile(optimizer='adam', loss='mean_squared_error') model.fit(X_test, y_test, epochs=10000) I plotted the predictions: Blue - original test set, Green - predictions from my custom network, Orange - keras predictions. My question is what am I doing wrong? How is it possible that NN implementation from keras cannot properly fit such a simple function? ** When I get weights from my model and set them in Keras model it gives the exact same output on test set, so my architecture (and perceptron implemenation) is the same as one from Keras. Edit: Solution by @brewmaster321 unfortunately does not work. Here is the plot of MSE loss for 10000 epochs:
