[site]: crossvalidated
[post_id]: 408716
[parent_id]: 
[tags]: 
Variant of validation with singleton test sets

Is the following approach to model validation somehow reasonable? And is there a name for that approach? We have 110 data points, iid assumption holds and we want to compare two predictive models M1 and M2. We randomly assign the data to a model estimation/training set $E=\{d_1,\dots,d_{100}\}$ and a test set $T=\{d_{101},\dots,d_{110}\}$ . Then we estimate M1 and M2 10 times, based on 10 equally large subsets of $E$ , i.e. $E_1=\{d_1,\dots,d_{10}\}, \dots ,E_{10}=\{d_{91},\dots,d_{100}\}$ and test their performance each at exactly one point of $T$ . We test the fitted models based on $E_1$ at $d_{101}$ the fitted models based on $E_2$ at $d_{102}\dots$ At the end we calculate an average of the (mean) squared errors for both models. As we test at one point only, it is actually just an average of squared errors.
