[site]: crossvalidated
[post_id]: 411216
[parent_id]: 411212
[tags]: 
You often define the MSE (the mean squared error) as the loss function of the perceptron. Then you update the weighs using gradient descent and back-propagation (just like any other neural network). For example, suppose that the perceptron is defined by the weights $W = (w_1, w_2, w_3)$ , which can initially be zero, and we have the input vector $X_i = (x_{i1}, x_{i2}, x_{i3})$ , for $i=1, \dots, n$ , where $n$ is the size of the training dataset, which can be defined as $\mathcal{D} = \{(X_1, t_1), \dots, (X_n, t_n) \}$ . Then, for the $i$ th pair of the training dataset $\mathcal{D}$ , the output of the perceptron is defined as a linear combination $$ y_i = W \cdot X_i $$ where $y_i$ is the output of the perceptron for the training example $X_i$ . The loss for the training example $(X_i, t_i)$ can be defined as follows \begin{align} \operatorname{MSE}(X_i, t_i) &= (y_i - t_i)^2\\ &= (W \cdot X_i - t_i)^2\\ &= ((w_1 x_{i1} + w_2 \cdot x_{i2} + w_3 \cdot x_{i3}) - t_i)^2\\ \end{align} To update the weights $W$ , we need to find the partial derivative of $\operatorname{MSE}(X_i, t_i)$ with respect to each weight, $w_1$ , $w_2$ and $w_3$ . The gradient of $\operatorname{MSE}(X_i, t_i)$ , denoted by $\nabla_W \operatorname{MSE}(X_i, t_i)$ , that is, the vector containing the partial derivatives of $\operatorname{MSE}(X_i, t_i)$ with respect to each element of $W$ , $w_1$ , $w_2$ and $w_3$ , can be calculated as follows \begin{align} \nabla_W \operatorname{MSE}(X_i, t_i) &= \begin{bmatrix} \frac{\partial \operatorname{MSE}(X_i, t_i) }{ w_1}\\ \frac{\partial \operatorname{MSE}(X_i, t_i) }{ w_2}\\ \frac{\partial \operatorname{MSE}(X_i, t_i) }{ w_3} \end{bmatrix}\\ &= \begin{bmatrix} 2((w_1 x_{i1} + w_2 \cdot x_{i2} + w_3 \cdot x_{i3}) - t_i) \frac{\partial}{w_1} (w_1 x_{i1} + w_2 \cdot x_{i2} + w_3 \cdot x_{i3})\\ 2((w_1 x_{i1} + w_2 \cdot x_{i2} + w_3 \cdot x_{i3}) - t_i) \frac{\partial}{w_2} (w_1 x_{i1} + w_2 \cdot x_{i2} + w_3 \cdot x_{i3})\\ 2((w_1 x_{i1} + w_2 \cdot x_{i2} + w_3 \cdot x_{i3}) - t_i) \frac{\partial}{w_3} (w_1 x_{i1} + w_2 \cdot x_{i2} + w_3 \cdot x_{i3}) \end{bmatrix}\\ &= \begin{bmatrix} 2((w_1 x_{i1} + w_2 \cdot x_{i2} + w_3 \cdot x_{i3}) - t_i) x_{i1}\\ 2((w_1 x_{i1} + w_2 \cdot x_{i2} + w_3 \cdot x_{i3}) - t_i) x_{i2}\\ 2((w_1 x_{i1} + w_2 \cdot x_{i2} + w_3 \cdot x_{i3}) - t_i) x_{i3} \end{bmatrix}\\ &= \begin{bmatrix} 2(y_i - t_i) x_{i1}\\ 2(y_i - t_i) x_{i2}\\ 2(y_i - t_i) x_{i3} \end{bmatrix} \end{align} You can then update the weights of the perceptron using a step of gradient descent $$ W \leftarrow W - \gamma \nabla_W \operatorname{MSE}(X_i, t_i) $$ where $\gamma$ is the learning rate. Note that $W$ and $\nabla_W \operatorname{MSE}(X_i, t_i)$ have the same dimensions. This shows the application of stochastic gradient descent, that is, you update the weights for every training example $(X_i, t_i)$ . You can also perform (batch) gradient descent. You would just define the loss function differently, that is, you would define it as \begin{align} \operatorname{MSE}(X_i, t_i) &= \frac{1}{n} \sum_{i=1}^n (y_i - t_i)^2\\ \end{align} Then the derivatives are similarly calculated. To get rid of the $2$ in front of the partial derivatives, you can define the loss functions with a $\frac{1}{2}$ in front. The way you change the weights also depends on the outputs. If they are binary (using a threshold-like method to obtain them), which is usually the case, then you can work out what's going to happen when you perform one step of the gradient descent. Just look at the equations above. But, for example, if $y_i = t_i = 1$ , then $y_i - t_i = 0$ , then indeed the corresponding weight will not be updated. If $y_i = 0$ and $t_i = 1$ , then $y_i - t_i = -1$ , the weight will likely be updated (unless the input is zero). So, the updates of the weights also depend on the values of the outputs and targets, that is, you can define the two classes to be $0$ and $1$ or $-1$ and $1$ (or something else), and this affects the updates.
