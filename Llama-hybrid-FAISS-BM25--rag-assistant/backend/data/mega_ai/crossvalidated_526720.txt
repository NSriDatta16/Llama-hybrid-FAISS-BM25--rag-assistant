[site]: crossvalidated
[post_id]: 526720
[parent_id]: 
[tags]: 
Metrics for edge detection (ODS, OIS, AP)

I have a deep learning model which outputs edge maps and I would like to evaluate the accuracy of the model. Reading some papers I saw that typically the edge detection accuracy is evaluated using three standard measures: Optimal Dataset Scale (ODS), Optimal Image Scale (OIS) and average precision (AP). Also the F-measure of ODS and OIS can be computed. However I couldn't find anywhere how they are defined and computed from predicted edge maps and ground truth maps (2D images).
