[site]: crossvalidated
[post_id]: 351001
[parent_id]: 
[tags]: 
Weight Initialization

For Neural Networks with ReLU activations it is common practice to use the initalization introduced by He et al. to keep the variance of layer activations constant throughout the network. On the other hand, for a classification task, this course suggests that a good initialization should result in the expected loss for random guessing, e.g ln(number of classes) for a softmax classifier. These two criteria are not compatible in the network I am training: When using He initialization, the loss before training is much higher. The necessary variance to arrive at the expected loss is much smaller than sqrt(2/N) . With both initializations I am able to train the network and they reach similar losses after training. However, the learned filters are quite different: For the initialization with the smaller variance, in some of the filters all weights get the same, detecting just how light or dark a patch is, which seems odd. With He, init on the other hand, the loss (which already starts higher) makes a big jump up in the first step before drastically decreasing in the next. This jump can be prevented by smaller learning rates which however slow the whole convergence afterwards. Why is the loss with He initalization higher than random guessing? Which criterion should one prioritize? Each class in my dataset has the same number of examples. I use no kind of regularization.
