[site]: crossvalidated
[post_id]: 551878
[parent_id]: 
[tags]: 
Why not always use Polynomial Regression to solve classification problems?

Consider this simple classification problem: You can solve it using Logistic Regression. But there's another way. As @whuber noted in this answer , in hypothesis $h(x) = \frac{1}{1 + e^{-P(x)}}$ , polynomial $P(x)$ only needs to spit positive numbers for our " $+$ " class, and negative numbers for our " $-$ " class. In this case the perfect decision boundary will be found. So why not just apply polynomial regression to this problem without using gradient descent at all? Here's what I mean. Suppose, your features are $x$ and $y$ and you make some polynomial features as follows: $X = \begin{bmatrix} x & y & x^2 & xy & y^2 \end{bmatrix}$ - this is your design matrix. Your desired outcomes are $y = \begin{bmatrix} 1 & 0 & 0 & 1 & ... & 1 \end{bmatrix}^{\mathrm T}$ . Instead of using Logistic regression, you can just turn all zeros in $y$ into $-1$ like this: $y = \begin{bmatrix} 1 & -1 & -1 & 1 & ... & 1 \end{bmatrix}^{\mathrm T}$ and apply least squares to $X\theta = y$ . By doing so you'll find the polynomial producing positive numbers for " $+$ " class and negative numbers for " $-$ " class. This equation will have an exact solution whenever $X$ is square. In this case we perfectly separate two classes. Here's the result of classification after using Logistic Regression: And here's the result after using Polynomial Regression as I described: They're just the same. So why not prefer OLS Linear Regression with polynomial features over gradient descent in Logistic Regression? Any disadvantages of my approach? I realize that Polynomial Regression used in such way can lead to overfitting, but let's omit this for a moment.
