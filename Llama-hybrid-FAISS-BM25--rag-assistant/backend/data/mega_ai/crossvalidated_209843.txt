[site]: crossvalidated
[post_id]: 209843
[parent_id]: 209775
[tags]: 
The Scikit learn documentation has an example of the "probability calibration" problem , which compares Logistic Regression's probability estimates against those from LinearSVC and NaiveBayes. I added GBRT classifier to the matrix as well, and this is the corresponding graph, which shows that while the un-calibrated GBRT performs slighly poorer than Logistic Regression, the calibrated versions (using 2 different calibration methods) perform better. Just from this experiment alone, it would be hard to make a case for GBRT over LR, however. The source for my Gist which adds GBRT to the scikit-learn's original example.
