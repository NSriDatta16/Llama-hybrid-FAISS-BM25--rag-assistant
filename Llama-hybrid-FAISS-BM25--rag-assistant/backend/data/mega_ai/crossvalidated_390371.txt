[site]: crossvalidated
[post_id]: 390371
[parent_id]: 385688
[tags]: 
The Boruta algorithm is built as a wrapper around Random Forest, but with a more complex way of ranking the feature's importance, based on merging the original dataset with its copy, where the column values are shuffled. This supposedly lessen the possibility of overfitting. As a consequence, the computational cost gap is not irrelevant here. So, in a way, it is alright to say that the Boruta, in the context of feature selection, is an improvement on Random Forest. Nonetheless, my poor analogy is a consequence of the fact that the algorithm itself is pretty straightforward to understand, so I'd ultimately suggest you try and read the paper: Feature Selection with the Boruta Package . By the way, if you're on Python, apart from a nice explanation this blog post points to a really nice and fast implementation.
