[site]: crossvalidated
[post_id]: 287896
[parent_id]: 286634
[tags]: 
I think "A self-organizing state space model" by Kitagawa was the first person to describe what you're doing. What I think you're doing is replacing your initial model's parameters (random but not dynamic) with dynamic ones. Really the new "parameters" are states of a different state space model. And this larger model's parameters you're thinking of as tuning parameters. "Combined Parameter and State Estimation in Simulation-Based Filtering" by Liu and West suggests using more intelligent artificial dynamics for the parameters, improving the idea from above. These ways seem to work fine, but it's a totally different model, so it's really apples to oranges here. You're not really looking at $p(\theta|\text{states},\text{data})$ when you're looking at these nice looking samples. "Following a moving target-monte carlo inference for dynamic bayesian models" by Gilks and Berzuini and "Markov chain monte carlo, sufficient statistics, and particle filters" by Fearnhead suggest the idea of using MCMC moves within a particle filter to “jitter” the sample elements that corresponded to parameters. This is like an extra final step in your particle filter. The goal is to simulate from your parameter posterior. If you use Metropolis-Hastings with the idea from above to simulate from your parameter posterior, the likelihood that's proportional to this distribution is still hard to evaluate because it depends on your increasing dataset. The Fearnhead paper, "Particle filters in state space models with the presence of unknown static parameters" by Storvik, and "Particle learning and smoothing" by Carvalho et al. get around this algorithmic problem by using recursive formulas for sufficient statistics.
