[site]: stackoverflow
[post_id]: 949202
[parent_id]: 949146
[tags]: 
My main question is: How bad would be in this case to detect the most common indexing spiders, and serve them the content in a non-randomized fashion? Most (legitimate/search engine) bots set their user-agent correctly, and hence it is very easy to do something like this, you just need to check the User-Agent HTTP request field and react properly... Whether this solution is the best one, I'm not qualified to debate on. List of User Agents.
