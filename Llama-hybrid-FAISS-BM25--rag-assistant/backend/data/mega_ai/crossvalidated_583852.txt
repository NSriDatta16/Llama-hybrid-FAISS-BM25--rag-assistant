[site]: crossvalidated
[post_id]: 583852
[parent_id]: 
[tags]: 
parallelizing log-sum-exp

I have some approximate likelihoods: $L_1, \ldots, L_n$ . Each is quite expensive to calculate. They're approximate because they use random numbers. Each of them is being calculated on the same data set of size $N$ . Actually, what's really being calculated are the log-likelihoods: $\ell_i$ for $i=1,\ldots,n$ , not the likelihoods. I'd like to average together these likelihoods. If each likelihood is unbiased, then so is the sample mean. This is an important requirement when using the Pseudo-marginal Metropolis-Hastings sampler , for instance. To avoid numerical underflow, from exponentiating numbers close to $-\infty$ , I'd like to use the log-sum-exp trick: $$ \log \left( \frac{1}{n}\sum_{i} L_i\right) = m + \log \sum_{i}\exp[\ell_i - m] - \log n $$ Question Typically $m$ is set to $\max(\ell_1, \ldots, \ell_n)$ . However, if one is using multithreading, calculating the maximum will require more thread communication or memory or locking, and I don't want to waste time calculating some example/pilot likelihoods (remember, they're expensive to compute). Is there a good rule of thumb to use as a substitute? Perhaps $m := -.5N$ ? I reason that, if the data are iid, and each data point likelihood is approx. $r \approx .6$ , then $L_i \approx r^N$ and $\ell_i \approx N \log r \approx -.5 N$ . Another piece of relevant information: I expect these $\ell_i$ to be quite left-skewed. The perfect answer is probably problem-specific, but I was wondering if anyone had some nice anecdotes or resources.
