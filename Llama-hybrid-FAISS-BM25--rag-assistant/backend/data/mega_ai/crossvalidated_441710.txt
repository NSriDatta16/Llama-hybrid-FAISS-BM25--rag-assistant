[site]: crossvalidated
[post_id]: 441710
[parent_id]: 
[tags]: 
On the Mathematical Interpretation of Standard Error

:) My colleagues are using Standard Error in a way that I do not find intuitive. Here is the situation we are dealing with: We take the average for the last 12 months worth of sales data. We take the standard deviation for those same 12 months. This is the part that confuses me : the standard error (SE) is then calculated using the last 6 months instead of 12 to create an "average" for the last 6 months. In effect, the mathematical formula being used is SD / sqrt(6), not SD / sqrt(12). The SE is then used with the average to create the Confidence Interval. Here's where I need guidance : by using 6 months instead of 12, that implies that we care about the dispersion of the mean for 12 months while we only care about the standard error of the mean for 6 of those months. So, isn't this introducing a sort of randomness in that we are not using the full sample size available? We can't "choose" which 6 months to use out of the available 12. My colleagues insist this doesn't matter since it's an average, but that doesn't make sense to me at all. The Standard Error isn't an average, and yet it's being used as such. Phrased another way: do we have to use the full sample size available for calculating SE? My colleagues believe we don't. I believe we have to. Finally, my colleagues refer to this as the Confidence Interval. I thought when we use the Standard Error to create intervals that was called the Margin of Error, not Confidence interval. Any guidance is greatly appreciated! -valentinexoxo
