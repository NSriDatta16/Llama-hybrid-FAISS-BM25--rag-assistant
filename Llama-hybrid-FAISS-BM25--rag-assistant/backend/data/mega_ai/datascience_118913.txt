[site]: datascience
[post_id]: 118913
[parent_id]: 
[tags]: 
How a Random forest "learns" or How loss (objective function value) is propagated back so that a random forest can "Improve"?

Every Blog and Youtube video talks about the same steps: Choose that you have to build N number of tree and do the task 2-5 below for all of those N trees Choose random samples with replacement Choose f features randomly from total F For each tree, at each split find the node with Minimum Gini entropy (or Max Info gain) and split Run the test sample to get a result Aggregate the results No what I'm trying to understand intuitively is that how the next iteration is done? For example, in Linear Regression (assuming two variables), we calculate the difference between predicted and actual values and move the line with theta degrees Or in Neural Networks, we use Gradient Descent and Chain rule so that weights of the matrix at each layer are updated according to what they contributed to the actual prediction in the next iteration. How is it done in Random Forest? What is the learning? How the loss (objective function) is propagated back to the Nodes ? The closest I could Find was this slide:
