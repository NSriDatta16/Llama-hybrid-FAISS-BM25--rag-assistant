[site]: crossvalidated
[post_id]: 635741
[parent_id]: 
[tags]: 
Neural net performs inline with linear regression, how can I improve it?

I have a regression problem with around 1 million samples and 400 features (some not too meaningful and/or are redundant) and 1 target variable. I have been trying very hard to design a neural network architecture that beats linear regression (with regularization) in validation data, but what I'm getting so far is about in-line with linear regression and not meaningfully better. I'm mostly focusing on feed forward networks with some complexity added such as: Multiple dense layers (with varying choices for number of units, normalization, activation, dropout, etc.). With or without residual connections. Parallel networks with some gating, which depends on a subset of features. Something like stratified linear regression. Train an autoencoder first to reduce input dimensionality, then do all of the above. Briefly tried CNN and other more complex layers but that is not my focus. They all perform about the same as linear regression if not worse. What's a bit strange to me is why is it so difficult to beat linear regression, I would think adding a small nonlinearity to a baseline linear model should at least help a little. My question is what would an advanced ML practitioner try other than the above? Any pointers to standard approaches or out of the box ideas are highly appreciated.
