[site]: crossvalidated
[post_id]: 629787
[parent_id]: 
[tags]: 
Incrementally Computing $\Sigma_t v_t$ Without Storing $x_i$, $v_i$, or $\Sigma_t$

Motivation: I have a discussion with friends many days ago, at that time I think this problem is very easy so we directly skip, later I realize I cannot solve it XD. The problem is: Given a sequence of vectors $x_1, x_2, \ldots, x_t$ and $v_1, v_2, \ldots, v_t$ in $\mathbb{R}^p$ in an online fashion, I'm interested in computing the product of the sum of outer products $\Sigma_t$ with $v_t$ , where $$ \Sigma_t=\sum_{i=1}^t x_i x_i^T $$ Specifically, the term of interest is $\Sigma_t v_t$ . However, there are constraints for memory (only O(p)): I cannot store all individual vectors $x_i$ or $v_i$ for $i . Finite them is ok. I cannot store the matrix $\Sigma_t$ . The vector $v_i$ can change from one time step to the next. Given these constraints, is there a way to compute $\Sigma_t v_t$ incrementally as each new $x_t$ and $v_t$ arrives? I'm looking for an efficient approach that uses minimal storage and captures the effect of the changing $v_t$ without explicitly storing past data or the matrix sum. Any suggestions, insights, or relevant literature would be greatly appreciated! When $v_t$ is unchanged over time, we definitely have a solution by introduce some ancillary variables. But I fail for the varying $v_t$ case.
