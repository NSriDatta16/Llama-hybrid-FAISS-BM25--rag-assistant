[site]: crossvalidated
[post_id]: 582366
[parent_id]: 
[tags]: 
Statistical significance of difference between two time series

There are two groups of samples. The number of events on n-th day is measured for each sample. The average is measured for each day for each group of samples Days after first event average events count group 1 average events count group 2 1 1.0000 1.0001 2 0.1000 0.1050 3 0.0500 0.0600 4 0.0050 0.0610 5 0.0005 0.0115 6 0.0006 0.0080 7 0.0001 0.0020 8 0.0003 0.0060 9 0.0001 0.0030 10 0.0000 0.0020 11 0.0000 0.0010 12 0.0000 0.0015 The averages start at about the same value, then drop rapidly for group 1 and drop slower for group two. There is some noise "day 6 for group 1" and "day 4 for group 2". Groups are clearly statistically different: on any day the value in group 1 is lower that in group 2 for the same day. However, the t-test shows very small significance (p>0.94) because the value for the first day is very large and almost equal. What is the correct metrics to show that "two is always greater than one on any given day"? Update I've recorded the averages and the stdevs for each day for samples in each group. Here is what I get Note that at day one every sample is guaranteed to have an event at least once. After that only a small fraction of samples have an event. On each day 0 Should I use this data, and if so - how? Should I just count the number of samples with events on n-th day?
