[site]: crossvalidated
[post_id]: 337953
[parent_id]: 
[tags]: 
The role of Support Vectors in optimization

I believe that I have somewhat an understanding of the objective and loss functions associated with Support Vector Machines (SVM), however, one point is still confusing me: The fact that the margins of the SVM can be characterized only by the Support Vectors is often named as a reason for their efficient optimization. What I do not understand is this: To know what the Support Vectors will be, doesn't the algorithm have to consider all the datapoints in the beginning? I.e. is this sparse representation not only possible after training? Edit: Here is an example: In other words, if all data points other than the support vectors were removed, the algorithm would find the same solution. This property, known as sparseness, has many consequences, both in the implementation and in the analysis of the algorithm From this paper: Cristianini, N. and Scholkopf, B. (2002). Support Vector Machines and Kernel Methods: The New Generation of Learning Machines, page 39. So, I understand how the (hinge) loss function looks like, how we come to this function at all, and how derivation can lead to a solution - but I guess I don't understand how this is algorithmically implemented such that only the support vectors are needed (from when on are only these needed, how exactly does this affect the SVM optimization?)
