[site]: crossvalidated
[post_id]: 314623
[parent_id]: 
[tags]: 
Naive Bayes likelihood

I'm interested in computing the Bayesian Information Criterion (BIC) for a set of Naive Bayes models. The NB can be described as follows, for a two-class $Y \in {0,1}$ with predictors $X = (x_1, x_2, ..., x_k)$ , we have the following joint probability $$ P(Y = y, X_1 = x1, X_2 = x_2, ..., X_k = x_k) \\ = P(Y = y) \prod^k_{j = 1} P(X_j = x_j | Y = y) $$ assuming independence between all attributes. This is also proportional to posterior $$ \theta = P(Y = y| {\bf{X}} ) \propto P(Y = y) \prod^k_{j = 1} P(X_j = x_j | Y = y) $$ I denote the posterior with $\theta$ for short. The BIC has the general formula: $$ -2 ln(\hat{L}) + k \times ln(n) $$ where $\hat{L} = \text{Likelihood}$ or $-2 ln(\hat{L}) = \text{Deviance}$ $k = \text{parameters to be estimated}$ $n = \text{number of observations}$ My question is, whether for a two-class classification problem, the likelihood for the Naive Bayes model is a Bernoulli density, such as $$ L(\theta| \bf{X} ) = \prod^n \theta^{y_i} \times (1 - \theta)^{1-y_i} $$ This would be similar to the logistic regression. Is this assumption correct? Also, I am aware that this type of question has been asked before, but a clear answer has not yet been delivered. Maximum Likelihood formula for Naive Bayes
