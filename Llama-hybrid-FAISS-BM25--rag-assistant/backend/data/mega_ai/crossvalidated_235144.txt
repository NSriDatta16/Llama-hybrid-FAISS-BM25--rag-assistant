[site]: crossvalidated
[post_id]: 235144
[parent_id]: 2234
[tags]: 
There are two variations of the logistic regression which are not yet outlined. Firstly the logistic regression estimates probabilities using a logistic function which is a cumulativ logistic distribution (also known as sigmoid). You can also estimate probabilities using functions derived from other distributions. The most common way apart from the logistic regression is the probit regression which is derived from the normal distribution. For a more detailed discussion between the differences of probit and logit please visit the following site. Difference between logit and probit models set.seed(55) n The second alternative points out a weekness of the logistical function you implemented. If you have a small sample size and/or missing values logistic function is not advisable. Hence an exact logistic regression is a better model. The log odds of the outcome is modeled as a linear combination of the predictor variables. elrm(formula = y ~ x) Furthermore there are other alternatives like to be mentioned: Two-way contingency table Two-group discriminant function analysis. Hotelling's T2. Final remark: A logistic regression is the same as a small neural network without hidden layers and only one point in the final layer. Therefore you can use implementations of neural network packages such as nnet in R. Edit: Some weeks later I realized that there is also the Winnow and the Perceptron algorithm. Both are classifiers which work also for classifications into two groups, but both are fallen out of favor in the last 15 years.
