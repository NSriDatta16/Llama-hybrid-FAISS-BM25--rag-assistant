[site]: crossvalidated
[post_id]: 390395
[parent_id]: 
[tags]: 
How are these two expected values equal?

From understanding machine learning: How are the two expected values equal in the proof of the theorem in the red box below? Notation : $D$ is a distribution on the set $Z$ , $E_{z \text{ ~ }D}$ means the expected value with respect to $z$ having distribution $D$ , $D^m$ is the distribution of choosing $m$ points $(z_1, \dots, z_m)$ , $l(A(S), z)$ is some loss function with respect to some learning algorithm $A$ trained on $S$ and evaluated at $z$ . By their definitions: $$\ \Bbb E_{S \text{ ~ } D^m, z' \text{ ~ } D}[l(A(S), z')] = \Bbb E_S[\Bbb E_{z'}[l(A(S), z')]] = \Bbb E_S [\sum_{z' \in Z}l(A(S), z')D(z')]$$ $$ \Bbb E_{S \text{ ~ } D^m, z' \text{ ~ } D}[l(A(S^{(i)}), z_i)] = \Bbb E_S[\Bbb E_{z'}[l(A(S^{(i)}), z_i)]] = \Bbb E_S [\sum_{z' \in Z}l(A(S^{(i)}), z_i)D(z')]$$ But for these to be equal we must have: $$ \sum_{z' \in Z}l(A(S), z')D(z')= \sum_{z' \in Z}l(A(S^{(i)}), z_i)D(z')$$ I can't see how these are equal. Anyone have any ideas?
