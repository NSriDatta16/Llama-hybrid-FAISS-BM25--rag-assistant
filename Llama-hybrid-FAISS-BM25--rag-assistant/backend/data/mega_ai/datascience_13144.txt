[site]: datascience
[post_id]: 13144
[parent_id]: 13120
[tags]: 
In my work, I have done the same way by averaging the word vectors. But there is another idea I wanted to try. It is with the help of POS tags. First construct a most complicated sentence with all the POS tags as possible and set these POS tags as a template. For each sentence in the twitter corpus, POS tag all the words in it and apply those word vectors respective to the POS tags in the template. So, the unseen POS tags will have zeros. For example: NNP PDT DT NNS VB MD JJS CC PRP RBS is the template. Thus each position will contain 300-dimensional vector totally a 3000-dimensional vector. And if the first tweet's POS tags are NNP VB JJS PRP , then the word vectors are applied on these positions and have vectors on NNP VB JJS PRP positions and 300-dimensional zero vectors on other positions. This method not only solves the problem of representing the sentence by a single vector but also preserves the syntactic structure of the sentence by positioning in the right POS. Ofcourse, there will be problems when there are more than one POS tags or jumbled positions of the tags. This is just one of the possibility.
