[site]: crossvalidated
[post_id]: 77004
[parent_id]: 76995
[tags]: 
Your method should be effective (or you've got a problem with your random number generator). Whether it is efficient is a separate question. You might scale better by taking weighted samples from the $V$ possible word types rather than unweighted samples from all $N$ tokens. As documents get longer, Heaps Law suggests that these are related as $V=kN^\beta$ with $k$ between 10 and 100 and $\beta$ between around 0.5 (all depending on the texts) so the amount of storage increases more slower than $N$ . There are then two steps: figure out how many times each word type occurs in the text sample from a multinomial with these probabilities In case you'd like to do this in python using (numpy and nltk): from __future__ import division import nltk import numpy.random as npr nltk.download('punkt') ## make a tiny document txt = 'A B c c c D E E' ## lower case and tokenise its contents tokens = [word.lower() for word in nltk.word_tokenize(txt)] ## count how many times each word type occurs fd = nltk.FreqDist(tokens) ## construct probabilities for each type probs = [c/fd.N() for c in fd.values()] ## decide how long you'd like the new document to be new_doc_len = 1000 ## new document should have 1000 words ## generate new word type counts new_counts = npr.multinomial(new_doc_len, probs) These new word type counts are of course one bootstrap sample from the original document. You could regenerate the words as well, but there's probably no point since you've removed the inter-word correlations by sampling from the margin.
