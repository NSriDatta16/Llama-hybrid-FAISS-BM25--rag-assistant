[site]: stackoverflow
[post_id]: 524187
[parent_id]: 510412
[tags]: 
In terms of compression and information theory, the entropy of a source is the average amount of information (in bits) that symbols from the source can convey. Informally speaking, the more unlikely a symbol is, the more surprise its appearance brings. If your source has two symbols, say A and B , and they are equally likely, then each symbol conveys the same amount of information (one bit). A source with four equally likely symbols conveys two bits per symbol. For a more interesting example, if your source has three symbols, A , B , and C , where the first two are twice as likely as the third, then the third is more surprising but is also less likely. There's a net entropy of 1.52 for this source, as calculated below. You calculate entropy as the "average surprise", where the "surprise" for each symbol is its probability times the negative binary log of the probability: binary symbol weight probability log surprise A 2 0.4 -1.32 0.53 B 2 0.4 -1.32 0.53 C 1 0.2 -2.32 0.46 total 5 1.0 1.52 The negative of the binary log is used (of course) because logs of values between 0 and 1 (exclusive) are negative.
