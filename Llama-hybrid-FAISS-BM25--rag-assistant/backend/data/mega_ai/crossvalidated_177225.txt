[site]: crossvalidated
[post_id]: 177225
[parent_id]: 177220
[tags]: 
I think you are misunderstanding a lot of things about how to use SVMs. Most parameters you mention are related to the hyperparameterization of common kernel functions (described in detail in the documentation). By default, LIBSVM uses the so-called RBF kernel: $$\kappa(\mathbf{u},\mathbf{v}) = \exp(-\gamma \|\mathbf{u}-\mathbf{v}\|^2),$$ with $\gamma$ a hyperparameter you must choose. If you use a default SVC with RBF kernel, you have to choose good values for the misclassification penalty ( -C ), kernel bandwidth ( -gamma ) and, optionally, class weights ( -wX for class X). To find good values you typically optimize some score function (e.g. cross-validated area under the ROC curve). LIBSVM itself only offers optimizing cross-validated accuracy via grid search, which has several issues. I recommend to have a look at Optunity , a library designed specifically to automate hyperparameter optimization (I'm the lead dev). Its documentation contains a comprehensive example about tuning a support vector classifier in scikit-learn ($\approx$ LIBSVM), available here . If you are new to machine learning, I recommend using libraries with a simple API like Python's scikit-learn , instead of using LIBSVM directly. LIBSVM is essentially meant as a back-end for more high-level libraries, and hence has a very terse user interface. For SVM classification, you should particularly have a look at sklearn's SVC documentation . The other kernel-related parameters (degree, coef0) are entirely irrelevant if you use the RBF kernel. $\epsilon$ is only really relevant if you're doing regression. Whether or not you use shrinking ( -h ) is about using some heuristics in the optimization process, which do not affect the actual resulting model significantly, just the training speed. -b gives probability estimates, but only if you explicitly ask for them (by default, LIBSVM returns only labels). LIBSVM itself is not multithreaded as far as I know.
