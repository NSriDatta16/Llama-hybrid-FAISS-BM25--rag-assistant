[site]: crossvalidated
[post_id]: 292332
[parent_id]: 292291
[tags]: 
Some additional possibilities to avoid overfitting Dimensionality reduction You can use an algorithm such as principal components analysis (PCA) to obtain a lower dimensional features subspace. The idea of PCA is that the variation of your $m$ dimensional feature space may be approximated well by an $l Feature selection (also dimensionality reduction) You could perform a round of feature selection (eg. using LASSO) to obtain a lower dimensional feature space. Something like feature selection using LASSO can be useful if some large but unknown subset of features are irrelevant. Use algorithms less prone to overfitting such as random forest. (Depending on the settings, number of features etc..., these can be more computationally expensive than ordinary least squares.) Some of the other answers have also mentioned the advantages of boosting and bagging techniques/algorithms. Bayesian methods Adding a prior on the coefficient vector an reduce overfitting. This is conceptually related to regularization: eg. ridge regression is a special case of maximum a posteriori estimation.
