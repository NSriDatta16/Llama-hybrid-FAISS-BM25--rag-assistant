[site]: crossvalidated
[post_id]: 604582
[parent_id]: 
[tags]: 
How can I re-write the expectation for a linear quadratic Gaussian problem?

Let $x_0 \sim N(\mu_{x_0},\Sigma_{x_0})$ , where $\mu_{x_0} \in \mathbb R^n$ and $\Sigma_{x_0} \in \mathbb R^{n \times n}$ . Then, let $$ y_0 = Cx_0 + v_0 $$ where $C \in \mathbb R^{n \times n}, v_0 \sim N(0,\Sigma_{v_0})$ , and $\Sigma_{v_0} \in \mathbb R^{n \times n}$ . Suppose that $x_0$ and $v_0$ are independent, such that $y_0 \sim N(C\mu_{x_0},C^T \Sigma_{x_0}C + \Sigma_{v_0})$ . Next, let $$ x_1 = Ax_0 + Bu_0(y_0) + w_0 $$ where $A \in \mathbb R^{n \times n}$ , $B \in \mathbb R^{n \times n}$ , $u_0 : \mathbb R^n \to \mathbb R^n$ is a vector-valued function of $y_0$ , and $w_0 \sim N(0,\Sigma_{w_0})$ is independent of $x_0$ and $v_0$ . In Linear Quadratic Gaussian (LQG) control, we want to solve the following optimization problem $$ \min_{u_0} E_{x_0,w_0,v_0}[x_0^T Q_0 x_0 + u_0^T(y_0)R_0u_0(y_0) + x_1^TQ_1x_1] $$ where the notation $E_{x_0,w_0,v_0}[\cdot]$ indicates that we are averaging over all possible values of $x_0,w_0,$ and $v_0$ , which are the fundamental sources of randomness in the problem, $Q_0$ and $Q_1$ are positive semi-definite matrices, and $R_0$ is a positive-definite matrix In other words, we want to determine the optimal function $u_0$ that minimizes the above cost function. In the books that I've read that derive the solution to this optimization problem, they instead solve the following optimization problem $$ \min_{u_0} E_{x_0,x_1,y_0}[x_0^T Q_0 x_0 + u_0^T(y_0)R_0u_0(y_0) + x_1^TQ_1x_1] $$ Note that the expectation changed from $E_{x_0,w_0,v_0}[\cdot]$ to $E_{x_0,x_1,y_0}[\cdot]$ . I'm not sure if these two optimization problems are equivalent. If they are, I'm not sure how to prove it.
