[site]: crossvalidated
[post_id]: 636854
[parent_id]: 
[tags]: 
Bayesian linear regression: How to enforce constraint on the sum of coefficients?

I have a linear regression problem in which my $X$ matrix is not full rank. Here is a small example: $$X = \left[\begin{array}{rrrr} -1 & 0 & 0 & 1 \\ 1 & 0 & -1 & 0 \\ 0 & 1 & -1 & 0 \\ 0 & 0 & 1 & -1 \\ -1 & 0 & 1 & 0 \\ 0 & 0 & -1 & 1 \\ 1 & 0 & 0 & -1 \end{array}\right] $$ As you can see, each row sum is equal to 0, thus $\mathbb{1}$ is in the nullspace of $X$ , indicating the matrix is not full rank. I would like to "fix" this problem by enforcing the constraint that the sum of all 4 coefficients is equal to 0, thus guaranteeing a unique solution. I can do this in a frequentist OLS pretty easily, by using the technique described here to get a new matrix $X'$ , with one fewer dimension, and then proceed with the normal equations. $$X' = \left[\begin{array}{rrr} -2 & -1 & -1 \\ 1 & 0 & -1 \\ 0 & 1 & -1 \\ 1 & 1 & 2 \\ -1 & 0 & 1 \\ -1 & -1 & -2 \\ 2 & 1 & 1 \end{array}\right] $$ However, I want to use a Bayesian approach by placing weakly informative priors around each of the coefficients. These priors are normal distributions that are not zero-centered. I should note I have priors that all 4 coefficients, including the coefficient for the dropped column. My problem is that when I do Bayesian regression with these priors on the $X'$ dataset, the resulting posterior distribution for the 4th "hidden" coefficient is very wide and becomes almost nonsensical. I suspect this is because most default Bayesian software (I am using brms in R, which calls to Stan) assumes the priors are independent, when that is obviously not the case in my scenario (there has to be some negative correlation). Thus when I sample from the posteriors for the three coefficients, and use those to derive samples for the "hidden" coefficient (by taking the sum and multiplying by -1, as the coefficients must sum to 0), the posterior distribution is very wide, and many, many data points are required to get a tighter distribution that is about as wide as that for the other coefficients. This is a problem because there is nothing "special" about the column that is dropped, yet the posterior for its coefficient will be a lot wider than that for the columns that are directly modeled. How to resolve this? One idea that I had is that I should somehow specify the prior as the entire joint distribution, but I'm not sure how to do this in brms or other software. I did some quick research on using covariance matrices with Bayesian approach, but I am not interested in estimating the matrix itself (placing a prior around it, etc.), moreso just relaxing the starting assumption that the priors are independent. I should also note that in my full setting (~5000 data points, ~700 columns), the joint posterior clearly does have the required correlation between the coefficients, and the derived samples for the "hidden" coefficient are sensible. This issue only occurs when I have relatively few data points. I found this question which pretty much describes the same setting that I have, but it does not address my specific question.
