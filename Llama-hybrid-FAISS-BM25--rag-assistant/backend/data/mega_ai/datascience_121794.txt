[site]: datascience
[post_id]: 121794
[parent_id]: 121782
[tags]: 
Most of your questions cannot be objectively answered. Whether or not a model is good depends on what is the use for it. Seeing how your classes are imbalanced, it definitely affects the metrics you presented. Do you care more about False Positives or False Negatives? What are the consequences of this? How many False Negatives are you willing to allow in order to have less False Positives? Is it common for AUC to be higher than the recall and precision when the data is imbalanced? This is an example of your model not being "as good" (given the caveats I mentioned). High ROC AUC means that your data can be ranked well while varying the threshold, which is to be expected since most of your data belongs in one class. But when considering precision-recall as individual metrics, at least one of those (precision if you have a lot of FP and recall if you have a lot of FN) will be more sensitive to the type of error you have, thus having lower values. For my use case, what is the best metric to use? F1 score is a pretty solid option whenever there are imbalanced classes, because as I mentioned it punishes both FP and FN. But, by its definition, it is an average (harmonic mean) between precision and recall. If you care more about reducing a specific type of error, you can focus more on maximizing the more specific metrics (precision/recall). I passed the probabilities to create ROC, is that the right way? Yes it is. ROC is dependent on classification threshold, thus it needs to know the probability in order to be able to determine where to classify the sample given the specific threshold it checks each time.
