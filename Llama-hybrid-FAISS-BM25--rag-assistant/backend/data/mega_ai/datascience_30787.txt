[site]: datascience
[post_id]: 30787
[parent_id]: 
[tags]: 
Grad Checking, verify by average?

I am running Gradient Checking , to spot any discrepancy between my mathematically-computed gradient and the actual sampled gradient - to reassure my backprop was implemented correctly. When computing such a discrepancy, can I sum-up squares of differences, then take their average? I could then use this average as my estimate of how correctly the network computes the gradient: $$\frac{1}{m}\sum_{i=0}^{i=m}(g_i-n_i)^2$$ or even: $$\sqrt{\sum_{i=0}^{i=m}(g_i-n_i)^2}$$ where $g$ is a gradient from backpropagation, and $n$ is gradient from gradient checking. However, Andrew Ng instead recommends: $$\frac{\vert \vert (g-n) \vert \vert _2 }{ \vert \vert g \vert \vert _2 + \vert \vert n \vert \vert _2}$$ where $\vert \vert . \vert \vert _2$ is the length of the vector. Another post post also recommends an slightly different approach: https://stats.stackexchange.com/a/188724/187816 Why would their approaches be better than mine?
