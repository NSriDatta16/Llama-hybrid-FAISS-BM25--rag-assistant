[site]: crossvalidated
[post_id]: 232821
[parent_id]: 
[tags]: 
When approximating a posterior using MCMC, why don't we save the posterior probabilities but use the parameter value frequencies afterwards?

I'm currently estimating parameters of a model defined by several ordinary differential equations (ODEs). I try this with a bayesian approach by approximating the posterior distribution of the parameters given some data using Markov Chain Monte Carlo (MCMC). A MCMC sampler generates a chain of parameter values where it uses the (unnormalized) posterior probability of a certain parameter value to decide (stochastically) whether it will add that value to the chain or add the previous value again. But, it seems to be the practice that the actual posterior probabilities do not need to be saved, rather is a n-dimensional histogram of the resulting parameter values generated and summary statistics like highest density regions (HDRs) of a parameter posterior ditribution is calculated from this histogram. At least that's what I think i learned from Kruschkes tutorial book on bayesian inference . My Question: Wouldn't it be more straightforward to save the posterior probabilities of the sampled parameter values along with these and approximate the posterior distribution from these values and not from the frequencies of parameter values in the MCMC chain? The problem of the burn-in phase would not arise as the sampler would initially still sample low probability regions more often than they would "deserve" by their posterior probabilities but it would not be anymore the problem of giving unduly high probability values to these.
