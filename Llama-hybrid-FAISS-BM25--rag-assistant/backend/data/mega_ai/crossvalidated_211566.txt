[site]: crossvalidated
[post_id]: 211566
[parent_id]: 
[tags]: 
Representation Bias Term in Neural Network

I've seen different variations of representing the "bias" term in a Neural Network, which is added during the forward propagation phase, and I would like to verify whether my interpretation is correct. In the Coursera course on Machine learning, the term is added in each layer as a constant. A vector of ones, equal to the number of observations in the input layer, is added as an extra term. To me, this resembles classic least squares, where you usually add a constant term as well. In a tutorial of how to implement a Neural Network in Python, the bias term is added explicitly as a separate parameter. I'm now interested in bringing these two representations together. I think I figured it out, would appreciate your comments on my solution. Say we start from the first representation, where the bias term is added as a constant of ones. We want to go from to the input layer to the first hidden layer. The input layer consists of three features: $X = (x_1, x_2, x_3)$, where $X$ is of size $(n,m)$ - so $n$ rows and $m$ columns. The bias term is now added as $x_0 = 1_n$, a vector of ones of length $n$. $X$ now becomes $X^* = (x_0, x_1, x_2, x_3) $, of size $(n, m+1)$. For each node in the hidden layer (call it layer 1), you find its 'value', by finding the dot product of $X^*$ and $\theta_k^{(0)*}$, which is the weight vector for node $k$. So, now you find the input of layer 1 as : $z_k^{(1)} = X^{*'}\theta_k^{(0)*} = (x_0, x_1, x_2, x_3)'(\theta_{k,0}^{(0)}, \theta_{k,1}^{(0)}, \theta_{k,2}^{(0)}, \theta_{k,3}^{(0)}) = x_0\theta_{k,0}^{(0)} + x_1\theta_{k,1}^{(0)} + x_2\theta_{k,2}^{(0)} + x_3\theta_{k,3}^{(0)}$ Now, in the second representation the input for layer 1 is found by the following equation: $z_k^{(1)} = X'\theta^{(0)}_k + b^{(0)}_k$, where: $\theta^{(0)}_k = (\theta_{k,1}^{(0)}, \theta_{k,2}^{(0)}, \theta_{k,3}^{(0)})$ Which can be written as: $z_k^{(1)} = X'\theta^{(0)}_k + b^{(0)}_k = x_1\theta_{k,1}^{(0)} + x_2\theta_{k,2}^{(0)} + x_3\theta_{k,3}^{(0)} + b^{(0)}_k$ Is it now correct to say that $b^{(0)}_k = x_0\theta_{k,0}^{(0)}$? Thanks! If anything is unclear, let me know.
