[site]: datascience
[post_id]: 81766
[parent_id]: 81704
[tags]: 
Loss regularization also prevents the trees from growing; it does not penalize the number of trees. I think you're right that which is better (of course, you could use both) depends on the data and other hyperparameters. You can think though about whether a given node will split, depending on the type of regularization: If you have a large node with some potential split that would create two other large nodes, but that split doesn't decrease the loss much, gamma will prevent it but min_child_weight won't. If you have a node with a potential split with one child being very small but that decreases the loss a lot, gamma will allow it but min_child_weight won't. So, personal opinion (based on that thought experiment, and not experience tuning these two together or against each other): set min_child_weight large enough that you're comfortable that a node with that many samples (in the squared-loss case, and appropriately transformed in other cases) is big enough to absorb noisiness of your population [by which I mean: one sample is surely quite likely to be random noise; an average of 100 might be better, but might not be enough, depending on your data]; after setting that, tune gamma . Better, tune both of them together, but still use the above intuition of your data to give a reasonable range for min_child_weight .
