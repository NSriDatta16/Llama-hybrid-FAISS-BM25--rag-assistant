[site]: datascience
[post_id]: 76677
[parent_id]: 
[tags]: 
Agent Collapse / Overfitting during Training

I'm new to reinforcement learning so please bear with me. I'm training an agent to play ms-Pacman using the actor-critic method. Below are the results of a couple of runs, in both graphs the orange line in the average of the previous 100 values. The left graph is the episode duration and the right is the reward earnt in each episode. In both the runs, the agent steadily learns and then collapses, and is unable to recover. Has the agent reached it's maximum potential with this method or is this a case of overfitting? Hyperparameters for both the runs: Batch Size: 128 Replay Memory Buffer : 500,000 Epsilon Minimum : 0.1 Epsilon Maximum : 1.0 Epsilon minimum is reached at around 2,000,000 steps (approx episode 3,000)
