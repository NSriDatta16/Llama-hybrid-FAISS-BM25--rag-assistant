[site]: crossvalidated
[post_id]: 592936
[parent_id]: 
[tags]: 
What is the purpose behind tuning n_estimators in XGBoost?

I have read multiple tutorials that talk about tuning the number of trees ( n_estimators or num_boosting_rounds ) as a hyperparameter. However, when I train the tuned model, I can see that the loss curves do not fully plateau. My question is, why do we tune the number of trees if it is not going to minimize the loss during the actual training? It seems like we can and should just put a high number and use early stopping (much like the number of epochs in neural networks). Thank you.
