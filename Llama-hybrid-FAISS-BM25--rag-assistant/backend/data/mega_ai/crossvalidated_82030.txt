[site]: crossvalidated
[post_id]: 82030
[parent_id]: 81957
[tags]: 
I will try to to give a more accessible (complementary) answer. Spectral clustering has two steps. First you determine neighborhood edges between your feature vectors (telling you whether two such vectors are similar or not), yielding a graph. Then you take this graph and you "embed" it, or "rearrange it" in a Euclidean space of d dimensions. Each dimension of this embedding is trying to give a solution to an optimization problem. Informally, this optimization problem is to minimize the number of neighboring graph nodes that appear on opposite sides of 0 (subject to a space filling constraint). Thus each node tends to end up next to its neighbors (as determined by the graph structure), which can be a friendlier space for K-means to work in. If you carry the math through, it turns out that the eigenvectors of the Laplace L give a solution to a "relaxed" version of this optimization problem - that's it.
