[site]: crossvalidated
[post_id]: 329373
[parent_id]: 329370
[tags]: 
The algorithm underlying back propagation is gradient descent, so I am guessing your question is "Why cant we just use gradient descent on any function/Taylor approximation of a function". There are a few points to note here 1) Gradient descent is in some sense the last resort in optimization problems. If the problem is too large/problem is non-convex and having some untenable form, this might be our only hope. 2) Gradient descent can converge very slowly in many cases..see slide 10-7 and 10-8 in http://ee364a.stanford.edu/lectures/unconstrained.pdf for an explanation and an example. There are modifications of the algorithm which converge several times faster (like Newton's method). 3) Gradient descent does not guarantee that a function converges to a global minimum or even a local minimum. The point can be a global minimum/local minimum/saddle point. The optimization we do in Neural Networks is sort of a last resort 'Hail Mary'.
