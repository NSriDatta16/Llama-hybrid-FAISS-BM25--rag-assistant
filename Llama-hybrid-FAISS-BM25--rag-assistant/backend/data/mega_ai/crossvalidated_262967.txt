[site]: crossvalidated
[post_id]: 262967
[parent_id]: 
[tags]: 
How does regularisation affect the interpretation of the logistic regression's coefficients?

My interpretation of the usual Logistic regression is that it maximizes the Likelihood of the data given the parameters. I.e., given responses $Y_j$ and random variables $X_{ij}$ , it maximizes the likelihood that the data is generated from $$Y_j = 1 \ \quad \text{if}\quad \ \epsilon + \sum_i \beta_i X_{ij} > 0, \quad \text{else} \quad \ Y_j = 0 $$ where $\epsilon$ is logistic noise. When a regularization is added, e.g. of the form $$ \sum_j (Y_j - \tilde{Y}(X_{j}))^2 + \alpha ||\tilde{Y}(X)||$$ the interpretation I found in the literature is that it corresponds to maximize the posterior of the parameters given the data, $P(\beta|Y)$ , by interpreting $\log P(\beta) = \alpha ||\tilde{Y}(X)||$ (regularization is the prior). Now, in standard textbook material, the interpretation of $\beta_i$ is that the log of the odds ratio increases by $\beta_i$ when $X_i$ increases by 1. My intuition for this is that because in non-regularized logistic regression we are maximizing the Likelihood, the interpretation that Y (i.e. data) increases with $X_i$ seems consistent because the random variable is the data (and the parameters are being conditioned). However, I do not see how this interpretation holds in a Logistic regression with regularization. Specifically, when we maximize the posterior, we end with a quantity about $\beta$ ( $P(\beta|X,Y))$ ). This answer claims that the interpretation is still the same, but gives no justification as of why. If yes, why can we still argue that $\exp(\beta_1)$ is the increase of the odds ratio by one unit of $X$ ? Hand-wavingly, I would argue something as follows: $\langle \exp(X_1) \rangle$ is the increase of the odds ratio when $\beta_1$ is increased by 1.
