[site]: datascience
[post_id]: 42299
[parent_id]: 42174
[tags]: 
The first point I would make (and you are likely aware of this given you specified "lstm" as part of your tags), is that you should use a long-short term memory network for this purpose. Given that you are working with time series data, the risk with a regular neural network is that it treats all the observations as independent when this is not the case with a time series. Instead of training with a completely correct input sequence and its correct output, i was thinking the network should train on its own predicted, not completely correct, inputs as well. This is somewhat of a slippery slope, because one must assume that you are using actual outputs. The model might be falsely trained to believe that the predicted values (which ordinarily would not yield the correct output) do in this case, and this skews your results. The model is only as good as the data that is being fed into it - if the data is wrong then the results will be as well. In general, your error reading looks quite good, although I am not familiar with your data or model. If you wanted to generate dummy data, one possibility could be to use a bootstrap simulation of sorts. For instance, suppose you have a time series with 10% mean and 20% volatility. You then choose to generate a random time series based on these two readings. The neural network could then be further trained on such data, and may potentially improve accuracy. However, I would be cautious of mixing inaccurate inputs with correct outputs - as this has the potential to skew your results.
