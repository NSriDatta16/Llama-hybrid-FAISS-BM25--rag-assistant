[site]: crossvalidated
[post_id]: 533986
[parent_id]: 533983
[tags]: 
I wouldn't say that LIME is a flawed half-solution and that SHAP is a perfect full solution. If anything, I would say both solutions are inherently flawed but perhaps are the best we have. If you are going to use a locally correct linear approximation of your machine learning model for the purpose of explaining predictions, then I would choose whichever software tool has the least bugs and most features that you like. Perhaps SHAP offers some theoretical properties which LIME doesn't, but it's not clear that these imply correctness of explanation. Perhaps they disallow some kinds of fishy explanations. Most people are looking for a quick-fix for understanding their models, and LIME and SHAP do that. Sometimes regulators even require it. Does that mean you truly understand your models? I don't think so. I don't see any reason to use LIME over SHAP unless the idea of locally approximating a function with a linear function and creating augmented examples for the purpose of training appeals to you. Besides for that, I would recommend not using SHAP or LIME if your data is not always (especially if locally - I can think of some examples like if you're using categorical features with int encoding) linear. I think a fair approach to model explanations is one which is very broad and approaches the question from many different angles. Are you looking to see whether there are hidden confounders? unfair biases? There are a lot of sources and usually it is recommended to use a broad range of solutions in order to understand your models and it is not as simple as choosing between lime and shap. Here is an example of IBM explaining their approach https://www.ibm.com/watson/explainable-ai
