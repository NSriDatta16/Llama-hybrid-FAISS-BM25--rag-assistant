[site]: crossvalidated
[post_id]: 191592
[parent_id]: 
[tags]: 
Find a window of ‘best correlation’ – exploratory analysis

I have data from a neurobiological experiment and I'm interested in learning which parts of multiple time-series (e.g. event-related potentials) best explain behavioural data (e.g. accuracy). Rather than testing for a small set of hypotheses based on existing literature, I would like to conduct an exploratory analysis. Let's say there are 20 subjects who perform a certain task in many trials with varying accuracy. For each subject, there is a time-series of 1000 data points times 50 electrodes and a single number representing their overall accuracy (average of all trials). In other words, there is an array of 20x50x1000 values (time-series) which are correlated in time as well as in space (between the electrodes) and a vector of just 20 values representing accuracy. My current approach is computing Pearson's correlation for every data point per every channel (50x1000) in all subjects (20) with the overall accuracy averages (20), however that's 50.000 tests, so naturally many of them are significant by mere chance. Adjusting the p-values could be a viable option but I'm not sure which method to use and I face the risk of rejecting otherwise valid correlations. Another approach that has occurred to me is to construct a moving window of let's say 20 data points (should be about 80 ms), compute its average and then correlate that average to the behavioural data but I'm not sure about a) the rational and b) how the correlations within the data itself influence the results. I've also heard something about cluster-based permutation tests but I'm only finding time-series vs. time-series examples and it isn't particularly clear to me yet. What approach would you suggest, please? An implementation in R would be splendid.
