[site]: crossvalidated
[post_id]: 533519
[parent_id]: 
[tags]: 
Bishop equation 3.67

Equation (3.67) in Bishop's "Pattern recognition and machine learning" is the following $$p(t|\mathbf{x}, D) = \sum_i^Lp(t|\mathbf{x}, M_i, D)p(M_i|D) \quad (3.67)$$ where $t$ is a target variable, $D$ is a dataset, $\mathbf{x}$ is an input vector and $M_i$ is a model in a set of $L$ models that refers to a probability distribution over the observed data $D$ . He writes (3.67) is derived with the sum and product rules of probability. However, when I try I don't get the same result: $$p(t|\mathbf{x}, D) = \dfrac{p(t, \mathbf{x}, D)}{p(\mathbf{x}, D)} = \dfrac{\sum_i^L p(t, \mathbf{x}, D, M_i)}{p(\mathbf{x}, D)} = \dfrac{\sum_i^L p(t, \mathbf{x}, D, M_i)}{p(\mathbf{x}, D, M_i)} \dfrac{ p(\mathbf{x}, D, M_i)}{p(\mathbf{x}, D)} = \sum_i^L p(t| \mathbf{x}, D, M_i)p(M_i|D, \mathbf{x})$$ That is I get a factor $p(M_i|D, \mathbf{x})$ instead of $p(M_i|D)$ . What am I doing wrong? edit: My understanding is that $D$ is a set of data points $\{(\mathbf{x}_1, t_1), (\mathbf{x}_2, t_2), ..., (\mathbf{x}_N, t_N)\}$ and that $\mathbf{x}$ is the N+1:th input $\mathbf{x}_{N+1}$ from which we want to predict what the next $t=t_{N+1}$ is going to be. I'm not entirely sure about this though. For what it means for probabilities to be conditioned upon input vectors, I have seen examples earlier in the book where for example it is assumed that $$t = y(\mathbf{x}, \mathbf{w}) + \epsilon$$ where $\epsilon$ is zero-mean Gaussian noise with precision $\beta$ so that $$p(t|\mathbf{x}, \mathbf{w}, \beta) = \mathcal{N}(t|y(\mathbf{x}, \mathbf{w}), \beta^{-1})$$
