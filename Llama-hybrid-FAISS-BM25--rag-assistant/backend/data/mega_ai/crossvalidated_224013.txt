[site]: crossvalidated
[post_id]: 224013
[parent_id]: 223711
[tags]: 
There's nothing special you need to do for either of these methods to work on a problem of this structure. (There are things you could do to speed convergence, but this is a separate, larger question.) Both methods use update rules based on both $r_{t+1}$ and $Q(s_{t+1},\cdot)$, so high $Q$ values will propagate backward from the goal state. Q-learning takes as input a set of learning episodes—each a set of $(s, a, r, s')$ tuples—typically ending in an absorbing state. Take a look at example 11.10 here . In this example and the associated table, a Q-learner observes the exact same episode until convergence. (This is atypical, but useful to build intuition.) Much like in your example, only a single state has a positive reward, though a few have negative and zero rewards. Check the table and note that as iterations grow, positive $Q$ value propagates backward from the goal state. SARSA acts based on a policy, and updates $Q$ and the corresponding policy as tuples are observed. The update rule is different, since the agent uses $Q$ to select $a'$, but the intuition is similar. As the learner experiences more and more tuples, value will propagate out from the goal state. That's how an agent will learn the sequence using either of these methods. Given infinite experience and certain restrictions, the $Q$ values will converge to values that reflect the optimal policy.
