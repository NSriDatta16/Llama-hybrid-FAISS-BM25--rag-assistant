[site]: crossvalidated
[post_id]: 258196
[parent_id]: 
[tags]: 
How to predict when an outlier is going to occur if it's shown to occur repeatedly?

I know my title seems to go against what an outlier is, but I don't know any other way of phrasing it. Let's say I have a spreadsheet, and a column with the following values: [From newest-to-oldest] 100, 9, 7, 100, 7, 7, 100, 9, 9, 100, 7, 9 If we were to disregard the 100's for a second, the average would seem to be 8; however, every third entry, the value seems to (reliably) jump up by 92+ the average. What I'm trying to figure out is how to predict what the next value will be given what we've seen before. So, if the last few values where: [From newest-to-oldest] 9, 100, 7, 9, 100, 7, ect. It should say "the next value should be 8 ± 1." If the values where instead: [From newest-to-oldest] 9, 7, 100, 7, 7, 100, 9, ect. It should say "the next value should be 100 ± 0." How would I go about doing this? This is all in excel (Google Spreadsheets to be more precise). I originally tried to find the outliers and throw the two sets of data into separate columns. Then, depending on the interval, choose the average from one of the two columns. However, since the 100's aren't really outliers (being that they're so common), using standard methods of finding them didn't work.
