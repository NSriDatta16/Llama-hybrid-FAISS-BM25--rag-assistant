[site]: crossvalidated
[post_id]: 521172
[parent_id]: 
[tags]: 
KDE Classifier: Mean of the bandwidths of features produces better results than a bandwidth for each feature. Why?

I'm building a Univariate Bayesian Kernel Density Estimation Classifier. It works like the Naives Bayes Classifier but instead of using the normal density function, it uses the Kernel Density Estimation generated density function. I calculate de bandwidth by using Sheater-Jones method. I've tried two things: Calculate the bandwidth for every feature (variable). So every feature has a different bandwidth. Calculate the bandwidth for every feature and then use the mean of the bandwidths. The first approach produces poor results (the accuracy of the classifier is just a little better than NB). The second approach produces much better results. The question is: why? The second approach doesn't make sense mathematically. Every feature have a different probability density function, so it makes sense to use an individual bandwith for each different feature. The datasets that I'm using are gene expression data. They have a lot of features (20000-55000) and few samples (20-150). They are from here: https://sbcb.inf.ufrgs.br/cumida Thank you.
