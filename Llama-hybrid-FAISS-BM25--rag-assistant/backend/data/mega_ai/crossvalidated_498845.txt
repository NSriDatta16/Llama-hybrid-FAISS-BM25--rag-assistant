[site]: crossvalidated
[post_id]: 498845
[parent_id]: 
[tags]: 
Autoencoders For Multivariate Time-series Anomaly Detection

I have a multivariate time series of size (1e6, 15) and would like to fit a LSTM autoencoder. I prepare data with multivariate rolling windows (one step rolling) where each sample has (1, 5, 15) dimension. Samples are fed to LSTM network with the input X of size (-1, 5, 15), the first dimension (-1) is the number of samples, the second dimension is number of time steps in each sample and third dimension is the number of features. Then I use reconstruction error between output X ' and input X sample. This method makes no forecast, instead, it reproduces the input signal. what is the best size for the autoencoder output? (1, 5, 15) or (1, 1, 15)? Am I better off building a LSTM forecasting network? This means I will have to predict the next time steps based on previous time steps (needs labels y ) and then minimise prediction error instead of reconstruction error?
