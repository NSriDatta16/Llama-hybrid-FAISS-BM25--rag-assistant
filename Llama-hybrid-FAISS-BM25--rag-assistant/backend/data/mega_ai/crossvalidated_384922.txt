[site]: crossvalidated
[post_id]: 384922
[parent_id]: 384919
[tags]: 
The only reward I supply to my agent is a win/loss reward at the end of the game. During the game, no reward is given. What should be the values provided to the Q-value neural network then? Typically you feed in the state and the output is a $Q$ -value for every action which can be taken from that state. It doesn't matter that reward isn't given at every step. Every update batch sampled from the experiene replay array to the network would then consist of mainly transitions with reward of 0. That is true. but that doesn't prevent the Q-values from "backflowing" from the terminal states to the rest of the states. Consider a game with one action in which you start in state A, then go to B, C, and get 1 reward at D. After one game, your Q-value for D is 1, then after another update your Q-value at C becomes 1 as well, and so on. How does one initialize such network (meaning the weights and biases and learning rate) so it can learn the optimal or at least "correct" policy? Well you should pick any reasonable weight initialization scheme -- hyperparameter tuning must be done as usual. Only target for the action which was selected during exploration/exploitation would be updated according to the formulas above. Targets for the other actions would remain the Q values predicted for the given state we have selected the action in. When you update the parameters of the target network $\theta$ , it necessarily changes all the $Q$ -values, not just the $Q$ -value of the selected action. How does one perform any learning in such case? Is it even possible with DQN? It is possible, although the sparser the rewards get the more difficult the problem is. I remember when replicating some DQN results on the Atari Enduro game, it took 1-2 million steps before the network started getting more than 0 reward.
