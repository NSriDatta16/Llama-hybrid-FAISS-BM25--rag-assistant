[site]: datascience
[post_id]: 82773
[parent_id]: 82772
[tags]: 
Is there a paper and a model that uses positional encodings and outcompetes RNN/LSTM based models for small scale datasets (MBs of text data, not terabytes)? Yes, there are several. Similar to GPT, they still pre-train on terabytes of data. But the embeddings they learn generalize well. Then you can fine-tune on a much smaller dataset. It works much in the same way as transfer learning on a CNN where a model first is trained on ImageNet and then trained on a specific task. It tends to give better results than RNN/LSTMs. If there are many, which ones are the leading ones in production applications? The one that sees most use is definitely BERT . Here is a really nice explanation of how it works. This transformers library from Huggingface makes it really easy to work with BERT and other transformers that have already been pre-trained.
