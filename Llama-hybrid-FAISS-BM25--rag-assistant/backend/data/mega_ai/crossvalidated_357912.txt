[site]: crossvalidated
[post_id]: 357912
[parent_id]: 357623
[tags]: 
Let’s take simple linear regression as a working example. The model is $y = \beta_0 +\beta_1 x + \epsilon$ . Now imagine the existence of an infinity of possible observations. You do not observe them, but they exist. In this infinity of possible observations there are many of them which for the same $x$ you notice a different $y$, this happens due to errors $epsilon$ which are independent of $x$. Again, they exist but you do not observe them. Now you have a training sample. On fitting the line you get some betas and some training errors. The average of those errors is denoted with $\bar{err}$. These is the mean of residuals and you can measure it. Now imagine what would have happened if you would have been received a different set of observations which by chance would have same set of x as in training sample. The y for this new hypothetical sample would be the same? Maybe yes, but most of the time not. Remember that from that infinity of possible set of observations you can get different y on same x. In the new hypothetical sample the new y, denoted with $Y_0$ is a random variable. The average errors for this new hypothetical sample is in sample error. In sample because you have same set of values for x. The in sample error is not observed, thus can’t be computed. But the concept is there and because is a statistic it can be more or less approximated.
