[site]: datascience
[post_id]: 115074
[parent_id]: 115052
[tags]: 
One approach would be to profile the code to empirically find the slowest parts. A quick visual scan of the code you referenced relieved inefficiencies. For example, there are several list comprehensions: labels = [headline[:20] for headline in headlines] docs = [nlp(headline) for headline in headlines] One straightforward way to speed up the code is converting those into generator expressions. Additionally, there are nested for-loops: similarity = [] for i in range(len(docs)): row = [] for j in range(len(docs)): row.append(docs[i].similarity(docs[j])) similarity.append(row) You may not need to do a doc-by-doc comparison.
