[site]: datascience
[post_id]: 75758
[parent_id]: 
[tags]: 
Implement the following loss function without interrupting the gradient chain registered by the gradient tape

I have spent five days trying to implement the following algorithm as a loss function to use it in my neural network, but it has been impossible for me. Impossible because, when I have finally implemented, I get the error: No gradients provided for any variable: ['conv1_1/kernel:0', 'conv1_1/bias:0', 'conv1_2/kernel:0' I'm implementing a semantic segmentation network to identify brain tumours. The network is always returning very good accuracy, about 94.5%, but when I have plotted the real mask with network outputs, I see that the accuracy is 0% because it doesn't plot any white dot: By the way, the mask image only has values between 0.0, black, and 1.0, white, values. So, I have decided to implement my own loss function, which has to do the following: In a nutshell, sum mask image to output and count how many 2.0 values are in this sum. Compare this count of 2.0 values with the count of 1.0 values in the mask image, and get the error. More detailed: Converts the output's value of the model into 0.0 or 1.0. Sum that output to the mask (which values are also 0.0 or 1.0). Counts how many 2.0 are this sum. Counts how many 1.0 are in the mask. Returns the different between them. My question is: Is there a already tensorflow function that do that? Now, I get that network output because I'm using Euclidean Distance. I have implemented the loss function using tf.norm : def loss(model, x, y): global output output = model(x) return tf.norm(y - output) And then, I use this loss function to tape gradients: def grad(model, inputs, targets): with tf.GradientTape() as tape: loss_value = loss(model, inputs, targets) return loss_value, tape.gradient(loss_value, model.trainable_variables) Maybe, what I am trying to do is another kind of distance.
