[site]: crossvalidated
[post_id]: 571641
[parent_id]: 571633
[tags]: 
There's a nice article in Medium article "17 types of similarity and dissimilarity measures used in data science" by Mahmoud Harmouch summarizing a large variety of statistical distance measures. Personally, I prefer staying grounded in Information Theory. The Jenson-Shannon metric begins with the Kullback-Liebler Divergence but makes it a metric by defining an intermediate distribution so that J(P,Q) = J(Q,P).
