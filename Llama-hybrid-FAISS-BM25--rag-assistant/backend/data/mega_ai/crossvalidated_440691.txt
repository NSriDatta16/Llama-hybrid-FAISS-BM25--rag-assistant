[site]: crossvalidated
[post_id]: 440691
[parent_id]: 
[tags]: 
Proving RBM similar to feedforward neural network with single latent layer

In an examination, I came across a question where I was asked to prove that RBMs are similar to a feedforward neural network with a single layer. I have an intuition that the structure is similar (Fully Connected) and that a single code/hidden layer is present in both, along with presence of activation functions. However, I am unable to figure out how do I go about proving this mathematically. Please help me out.
