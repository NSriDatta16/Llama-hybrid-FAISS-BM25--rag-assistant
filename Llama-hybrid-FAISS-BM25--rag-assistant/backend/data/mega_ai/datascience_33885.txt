[site]: datascience
[post_id]: 33885
[parent_id]: 
[tags]: 
eta and learning_rate different in xgboost

I am creating a classification model using xgboost in python. I am using different eta values to check its effect on the model. My code is- for eta in np.arange(0.2, 0.51, 0.03): xgb_model = xgboost.XGBClassifier(objective = 'multi:softmax', num_class = 5, eta = eta) xgb_model.fit(x_train, y_train) xgb_out = xgb_model.predict(x_test) print("For eta %f, accuracy is %2.3f" %(eta,metrics.accuracy_score(y_test, xgb_out)*100)) I expected different accuracies for some eta values, but to my surprise I got same accuracy for each eta. When I printed the model, I got this- >>> print(xgb_model) XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bytree=1, eta=0.5, gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3, min_child_weight=1, missing=None, n_estimators=100, n_jobs=1, nthread=None, num_class=5, objective='multi:softprob', random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, silent=True, subsample=1) Here you can see eta = 0.5 , but learning_rate = 0.1 . While in xgboost docs , learning_rate is an alias for eta. So how is this possible that both have different values?
