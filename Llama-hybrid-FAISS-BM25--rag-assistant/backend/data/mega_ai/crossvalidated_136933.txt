[site]: crossvalidated
[post_id]: 136933
[parent_id]: 
[tags]: 
Cross-validation for best subset selection

Rephrasing my question: I have predictors $\mathbf{X}=(X_1,X_2,X_3,...)$ and want to find the best subset for predicting some variable Y . My interest lies not in linear prediction, but using nearest neighbors. Here are two alternative algorithms and I wonder which one contains a correct application of cross validation: First approach: Rank the $X_i$ in $\mathbf{X}$ by their association with $Y$ Use cross-validation to find the best number $\widehat{p}$ of predictors from the ranked set $\mathbf{X}$. To this end I predict $Y$ with $p=1,2,3$ of the ranked predictors using nearest-neighbor prediction and RMSE as a skill metric. The $p$ with minimal error across all folds is the best $\widehat{p}$ All of this I do on the learning set (which I subdivide for the cross-validation into 5 folds). This gives quite good prediction results on an independent test set of the data. However, in Section 7.10.2 "The Wrong and Right Way to Do Cross-validation" in the Elements of Statistical Learning it says: In general, with a multistep modeling procedure, cross-validation must be applied to the entire sequence of modeling steps. In particular, samples must be “left out” before any selection or filtering steps are applied. ...and it seems I should not do any ranking involving the target variable $Y$ before hand. So I tested an alternative Second approach: Subdivide learning set into 5 folds and in each fold Rank the $X_i$ in $\mathbf{X}$ by their association with $Y$ Predict $Y$ for different numbers $p=1,2,3$ of the ranked predictors Choose $\widehat{p}$ which gave minimal average error across all folds While this seems to be a correct way to use cross validation, the prediction results are mostly worse compared to the first approach. I think the reason is that the ranking leads to different predictors in each fold introducing too much variance for determining $\widehat{p}$... What do you think, is the first approach valid? Note that I am not interested in evaluating the generalization performance... Last question: In a variation, I added a pre-selection to the first approach: Zeroth step: Pre-select a subset $\mathbf{S}$ from $\mathbf{X}$ by exploiting the associations of each $X_i$ with $Y$ and then do the ranking only on $\mathbf{S}$ ...which eliminates some predictors before I do the ranking. This gave even better results, albeit it seems to go against the rule not to do any pre-selection involving the target variable outside of cross-validation... Any ideas?
