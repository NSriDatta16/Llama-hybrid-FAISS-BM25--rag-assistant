[site]: crossvalidated
[post_id]: 460423
[parent_id]: 153599
[tags]: 
To answer a couple of the questions: CNNs definitely are used for NLP tasks sometimes. They are one way to take a variable-length natural language input and reduce it to a fixed length output such as a sentence embedding. Google's Multilingual Universal Sentence Encoder (USE) is one example: https://arxiv.org/abs/1907.04307 https://tfhub.dev/google/universal-sentence-encoder-multilingual/3 Since this question has been asked, there have been a number of new models proposed for NLP that are distinct from those mentioned above such as Transformers and pre-trained neural language models like BERT and some of the other flavors of USE. https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)
