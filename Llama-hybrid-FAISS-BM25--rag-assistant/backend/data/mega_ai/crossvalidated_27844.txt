[site]: crossvalidated
[post_id]: 27844
[parent_id]: 27833
[tags]: 
Do you mean the distribution of response, i.e. you have 70 cases of "YES" and 10000 of "NO"? If so, that is a common problem in data mining applications. Imagine a database with 1,000,000 instances, where only about 1,000 cases are "YES". Response rate of 1% and even less is a common thing in a business predictive modeling. And if you pick a sample to train a model that is a huge problem, especially with assessing stability of given model. What we do is pick a sample with different proportions. In aforementioned example, that would be 1000 cases of "YES" and, for instance, 9000 of "NO" cases. This approach gives more stable models. However, it have to be tested on a real sample (that with 1,000,000 rows). I've tested it with data mining models, such as logistic regression, decision trees, etc. However, I haven't used it with "proper" [1] statistic models. You can search it as "oversampling in statistics", the first result is pretty good: http://www.statssa.gov.za/isi2009/ScientificProgramme/IPMS/1621.pdf [1] "proper" in meaning "not data mining".
