[site]: crossvalidated
[post_id]: 518579
[parent_id]: 518484
[tags]: 
You can apply the "double machine learning" of Chernozhukov et al. (2017) outlined in this paper . To fit your problem into their framework it is useful to write it in the following notation $$ Y_i = D_i\theta + X_i'\beta + \varepsilon_i$$ The variable $D_i$ is your primary variable of interest, $X_i$ are additional controls (including an intercept), and $\varepsilon_i$ is an error term. All three variables are assumed to be i.i.d.. The idea of double machine learning is running auxiliary regressions on different subsets of the variables. This allows you to run an algorithm like Lasso on only a subset of the covariates. (Sample Splitting) Randomly split the sample, let's call it Sample A and B. (Auxiliary Regressions) In sample A, do two lasso regressions: $\qquad$ (a) $Y_i$ on $X_i$ . The estimated coefficients are $\hat{\gamma}^Y$ . $\qquad$ (b) $D_i$ on $X_i$ . The estimated coefficients are $\hat{\gamma}^D$ . (Main Regressions) In sample B, construct the residuals $\hat{U}_i^Y = Y_i - X_i'\hat{\gamma}^Y$ and $\hat{U}^{D} = D_i - X_i'\hat{\gamma}^D$ . Then run the following regression: $$ \hat{U}_i^Y = \hat{U}_i^D \theta + \tilde{\varepsilon}_i $$ You can estimate robust standard errors in step (3) to construct confidence intervals for $\hat{\theta}$ . Note: (i) The coefficients $(\hat{\gamma}^Y,\hat{\gamma}^D)$ are different from $\beta$ and should not be confused. Similarly, $\varepsilon_i \ne \tilde{\varepsilon}_i$ . With this approach you can only estimate $\theta$ . (ii) This procedure works for a wide variety of high-dimensional procedures. You can use Ridge instead of Lasso in the second step, or neural networks, random forest, etc. (iii) In the published paper, they also outline an alternative procedure where you repeat the process, now changing the role of samples $A$ and $B$ . You can then construct $\hat{\theta}$ by obtaining the average of the coefficients obtained from either labelling strategy. This can increase efficiency by utilizing your whole sample. Why does it work? (i) On one hand, Steps (2) and (3) can be viewed as analogs of the Frisch-Waugh-Lovell Theorem , which shows that we can always rewrite a multivariate linear regression in terms of auxiliary regressions with two sets of covariates $(D_i,X_i)$ . On the other hand, Step (3) has a "Neyman-Orthogonality" property. In layman's terms this means that Step (3) is less sensitive to whether you estimated $(\hat{\gamma}^Y,\hat{\gamma}^D)$ accurately. (ii) The additional sample splitting step is necessary to further control for the first stage uncertainty in estimating $(\hat{\gamma}^Y,\hat{\gamma}^D)$ . The formal proof is non-trivial but it is based on the following intuitive idea: The estimation error from Step 2 is uncorrelated with the error in Step 3 because they are constructed from different samples.
