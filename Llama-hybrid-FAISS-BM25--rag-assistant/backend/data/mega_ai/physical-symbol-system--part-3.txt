variety of novel problems, as humans do. Thus, the criticism of the PSSH refers to the limits of AI in the future, and does not apply to any current research or programs. Some claim that large language models are capable of "general intelligent action", however this is arguable. Consciousness vs. intelligent action The PSSH refers to "intelligent action" -- that is, the behavior of the machine -- it does not refer to the "mental states", "mind", "consciousness", or the "experiences" of the machine. "Consciousness", as far as neurology can determine, is not something that can deduced from the behavior of an agent: it is always possible that the machine is simulating the experience of consciousness, without actually experiencing it, similar to the way a perfectly written fictional character might simulate a person with consciousness. Thus, the PSSH is not relevant to positions which refer to "mind" or "consciousness", such as John Searle's Strong AI hypothesis: The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds. Evidence against the hypothesis Nils Nilsson has identified four main "themes" or grounds in which the physical symbol system hypothesis has been attacked. The "erroneous claim that the [physical symbol system hypothesis] lacks symbol grounding" which is presumed to be a requirement for general intelligent action. The common belief that AI requires non-symbolic processing (that which can be supplied by a connectionist architecture for instance). The common statement that the brain is simply not a computer and that "computation as it is currently understood, does not provide an appropriate model for intelligence". And last of all that it is also believed in by some that the brain is essentially mindless, most of what takes place are chemical reactions and that human intelligent behaviour is analogous to the intelligent behaviour displayed for example by ant colonies. Evidence the brain does not always use symbols If the human brain does not use symbolic reasoning to create intelligent behavior, then the necessary side of the hypothesis is false, and human intelligence is the counter-example. Dreyfus Hubert Dreyfus attacked the necessary condition of the physical symbol system hypothesis, calling it "the psychological assumption" and defining it thus: The mind can be viewed as a device operating on bits of information according to formal rules. Dreyfus refuted this by showing that human intelligence and expertise depended primarily on unconscious instincts rather than conscious symbolic manipulation. Experts solve problems quickly by using their intuitions, rather than step-by-step trial and error searches. Dreyfus argued that these unconscious skills would never be captured in formal rules. Tversky and Kahnemann Embodied cognition George Lakoff, Mark Turner and others have argued that our abstract skills in areas such as mathematics, ethics and philosophy depend on unconscious skills that derive from the body, and that conscious symbol manipulation is only a small part of our intelligence. Evidence that symbolic AI can't efficiently generate intelligence for all problems It is impossible to prove that symbolic AI will never produce general intelligence, but if we can not find an efficient way to solve particular problems with symbolic AI, this is evidence that the sufficient side of the PSSH is unlikely to be true. Intractability Common sense knowledge, frame, qualification and ramification problems Moravec's paradox Evidence that sub-symbolic or neurosymbolic AI programs can generate intelligence If sub-symbolic AI programs, such as deep learning, can intelligently solve problems, then this is evidence that the necessary side of the PSSH is false. If hybrid approaches that combine symbolic AI with other approaches can efficiently solve a wider range of problems than either technique alone, this is evidence that the necessary 