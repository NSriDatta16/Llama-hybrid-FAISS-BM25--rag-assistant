, doi:10.1214/aoap/1177005588, JSTOR 2959678. Katehakis, M.; C. Derman (1986), "Computing optimal sequential allocation rules in clinical trials", Adaptive statistical procedures and related topics, Institute of Mathematical Statistics Lecture Notes - Monograph Series, vol. 8, pp. 29–39, doi:10.1214/lnms/1215540286, ISBN 978-0-940600-09-6, JSTOR 4355518. Katehakis, Michael N.; Veinott, Jr., Arthur F. (1987), "The multi-armed bandit problem: decomposition and computation", Mathematics of Operations Research, 12 (2): 262–268, doi:10.1287/moor.12.2.262, JSTOR 3689689, S2CID 656323 External links MABWiser, open-source Python implementation of bandit strategies that supports context-free, parametric and non-parametric contextual policies with built-in parallelization and simulation capability. PyMaBandits, open-source implementation of bandit strategies in Python and Matlab. Contextual, open-source R package facilitating the simulation and evaluation of both context-free and contextual Multi-Armed Bandit policies. bandit.sourceforge.net Bandit project, open-source implementation of bandit strategies. Banditlib, open-source implementation of bandit strategies in C++. Leslie Pack Kaelbling and Michael L. Littman (1996). Exploitation versus Exploration: The Single-State Case. Tutorial: Introduction to Bandits: Algorithms and Theory. Part1. Part2. Feynman's restaurant problem, a classic example (with known answer) of the exploitation vs. exploration tradeoff. Bandit algorithms vs. A-B testing. S. Bubeck and N. Cesa-Bianchi A Survey on Bandits. A Survey on Contextual Multi-armed Bandits, a survey/tutorial for Contextual Bandits. Blog post on multi-armed bandit strategies, with Python code. Animated, interactive plots illustrating Epsilon-greedy, Thompson sampling, and Upper Confidence Bound exploration/exploitation balancing strategies.