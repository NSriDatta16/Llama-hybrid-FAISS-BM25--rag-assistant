[site]: crossvalidated
[post_id]: 530934
[parent_id]: 
[tags]: 
How to avoid having only one class during cross-validation

I have to perform a binary classification. My dataset is quite small 280 samples and quite imbalanced (1:10 ratio). I kept around 100 sample as testing and about 140 for training. My input variables are about 100 and are stationary time series. I would like to use GridSearchCV together with L1-Logistic classifier to perform feature selection with roc_auc as a score, but first I just want to check how the cv_score is with a default L1-Logistic classifier, as follows, clf = LogisticRegression(random_state=42) #default settings kfold= model_selection.TimeSeriesSplit(n_splits=2) #split the data, specific for time series train_cv = cross_val_score(X=X_train,y=np.ravel(y_train),estimator=clf,cv=kfold) print("TRAIN GROUP") print("\nCross-validation accuracy scores:",train_cv) print("Mean score:",train_cv.mean()) # Now predict on the test group print("\nTEST GROUP") y_pred = clf.fit(X_train, np.ravel(y_train)).predict(X_test) y_pred_prob = clf.fit(X_train, np.ravel(y_train)).predict_proba(X_test) print("\nAccuracy score:", accuracy_score(y_test.values, y_pred)) print("\nROC_AUC score:",roc_auc_score(y_test.values, y_pred_prob[:,1])) the result is the following, C:\Users\luigi.simeone\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: Traceback (most recent call last): File "C:\Users\luigi.simeone\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 531, in _fit_and_score estimator.fit(X_train, y_train, **fit_params) File "C:\Users\luigi.simeone\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py", line 1372, in fit raise ValueError("This solver needs samples of at least 2 classes" ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0 the error is clear, during one of the 2 splits, the training set only had one class the 0 one as demonstrated here (blue is train, orange is validation), Now, I guess this is due to the fact that the dataset is imbalanced and not so large but I do not know how to solve it. Can anybody help me to get around this problem? I had an idea, maybe stupid one, what if I convert the first sample from 0 to 1, in order to have at least one sample of the class 1 in the first split, and after I penalise somehow that sample? But I would not really know how to penalise it.
