[site]: datascience
[post_id]: 121305
[parent_id]: 
[tags]: 
Why do varied delimiters on text inputs help training stability?

In the preprint paper Text and code embeddings by contrastive pre-training , the authors describe a Transformer encoder which maps the input, x and y, to embeddings, vx and vy respectively and the similarity between two inputs is quantified by the cosine similarity between their embeddings, vx and vy And they state: We found that using different delimiters leads to more stable training. For x, we use ‘[’ as [SOS]x and ‘]’ as [EOS]x, while we use ‘{’ and ‘}’ as [SOS]y and [EOS]y respectively for y Is there an intuitive explanation for why using different delimiters is important for training stability?
