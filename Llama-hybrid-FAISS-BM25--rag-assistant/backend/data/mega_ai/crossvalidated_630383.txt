[site]: crossvalidated
[post_id]: 630383
[parent_id]: 553196
[tags]: 
I'm actually curious exactly what the internal target encoding of catboost does in this scenario. If I had to do the target encoding by hand e.g., for the accelerated-failure-time-loss-function xgboost , I'd be tempted to use a regularized maximum likelihood estimate of the expected log-event time for the category you are trying to encode. In what follows, I'll ignore exactly how one would split the data to create the target encoding (e.g. split the training data into 5 folds and for each of the folds target-encode based on the other 4 folds), which is obviously very important, but as far as I can see is just the same as for non-survival data. The simplest version of this is of course, if you assume an exponential model and just use $$\log \left( \frac{c_e + \sum_{r \in \text{ category}} \text{event indicator}_r}{c_t + \sum_{r=1 \in \text{ category}} \text{time to event or censoring}_r} \right) $$ within each category. The $c_e$ in the numerator and $c_t$ in the denominator are constants $>0$ that ensure the logarithm can be calculated. One obvious way to pick them is as $c_e = \text{scaling factor} \times \sum_{r \in \text{ all records}} \text{event indicator}_r$ and $c_t = \text{scaling factor} \times \sum_{r=1 \in \text{ all records}} \text{time to event or censoring}_r$ . Depending on how you set the $\text{scaling factor}$ , you shrink the target encoding for smaller categories more or less towards the overall estimate. You probably want $c_e$ to be at least something like $0.5$ to $1.0$ (but, hey, it's a hyperparameter you can tune).
