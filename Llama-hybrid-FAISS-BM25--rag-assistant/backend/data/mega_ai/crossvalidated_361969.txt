[site]: crossvalidated
[post_id]: 361969
[parent_id]: 
[tags]: 
Does it make sense to use an Early Stopping Metric like “mae” instaed of “val_loss” for regression problems?

I am performing a regression on a Dataset and try to replace a mathematical Model with a Neural Network. To avoid overfitting I decided to use the Early Stopping Callback Function of Keras. So far I have been told, that the metrics to monitor for this problem should be "val_loss" but when I try to do that, the neural networks stops training very early. I tried to monitor the mean absolute Error 'mae' and seem to get much better results, but I found no other example doing this so I am not sure if I am making another mistake which I am not aware of. Is there a reason not to do this? Or is there maybe even another metric that i should consider using? dataset = np.loadtxt("output_20180804.out", delimiter=",") X = dataset[0:5000,4:7] Y = dataset[0:5000,0:4] tbCallBack = TensorBoard(log_dir='./Graph{}', histogram_freq=0, write_graph=True, write_images=True) #TensorBoard Monitoring esCallback = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=1, mode='auto') #create Layers visible = Input(shape=(3,)) x = Dropout(.1)(visible) x = Dense(60)(x) output = Dense(4)(x) Optimizer = optimizers.Adam(lr=0.0001 #amsgrad = True ) model = Model(inputs=visible, outputs = output) model.compile(optimizer=Optimizer, loss=['mse'], metrics=['mae'] ) model.fit(X, Y, epochs=100000, batch_size=200, shuffle=True, validation_split=0.35, callbacks=[tbCallBack, esCallback]) print(model.get_weights()) # evaluate the model scores = model.evaluate(X, Y) print("\n%s: %.2f%%" % (model.metrics_names[1], scores[1]*100)) savestring = 'dense_100_epo100000_batch800_val02_20180807.h5' model.save(savestring) model.summary()
