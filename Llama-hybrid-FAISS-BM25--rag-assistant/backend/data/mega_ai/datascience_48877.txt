[site]: datascience
[post_id]: 48877
[parent_id]: 
[tags]: 
How is the standard deviation of VAE's obtained?

I am trying to build a Variational Autoencoder. I was looking at various codes online and found most of them in some way or another copy Francois Chollet (Google researchers) code . Now my main question with this code is this part: As you can clearly see the $log(\sigma)$ is the output from a Dense layer. Where did this assumption come from (the output is $log(\sigma)$ and not $\sigma$ ? How is it possible that we generate a random normal distribution with standard deviation $\sigma$ like this? Is it due to the way the computer generates Normal Distributions?
