[site]: crossvalidated
[post_id]: 491599
[parent_id]: 491594
[tags]: 
In the notation of the posterior predictive distribution $$p(y^* \mid x^*, X, Y) = \int p(y^* \mid x^*, \omega)p(\omega, X, Y)\, \text{d}\omega\tag{1}$$ in the question, the posterior density on the parameter vector $\omega$ should be denoted $p(\omega|X,Y)$ ; $p(\cdot|x^*,x,y)$ is a density function $$p(\cdot|x^*,x,y)\,:\ \mathcal Y \longmapsto \mathbb R^*_+\tag{2}$$ indexed by the triplet $(x^*,x,y)$ ; it is the conditional density function of a random variable, possibly denoted $Y^*$ , given $X^*=x$ and the learning sample $(X,Y)$ ; $y^*$ is the dummy argument of the density, it could be equally, written $z$ , $\zeta$ , $\Upsilon$ , or even ygrec , as well; $y^*$ is preferred for the analogy with the components of the learning set $Y$ but it is not a random variable by default, even though the function $p$ in (2) could be applied to a random variable; $\omega$ is the vector of parameters indexing the conditional sampling probability density $p(\cdot \mid x^*, \omega)$ ; while $\omega$ is a random variable in the Bayesian framework, with prior density $q(\omega|X)$ say, it is not denoted by a capital letter, like $\Omega$ or $W$ . The reason is that (i) this could prove confusing, since $\Omega$ [capital Greek letter] is also traditionally the parameter space and the underlying Borel space of measurable sets, while (ii) Bayesian inference returns the posterior distribution and computes summaries of that distribution, like posterior moments or quantiles. Writing $$\int p(y^* \mid x^*, W)p(W, X, Y)\, \text{d}\tag{3}W$$ is unusual if formally correct because the notation $W$ indicates a random variable but the integral is computed wrt a dummy variable, rarely denoted by a capital letter (and (3) is not a random quantity); $(X,Y)$ is the training dataset and the capitals are intended for vectors and matrices rather than for random variables, and furthermore $Y$ is actually a realisation of a random vector, thus not a random variable (and starting from the model, everything is conditional on $X$ ); the expression $P(Y=y^*|X=x^*)$ is incorrect because $Y$ and $y^*$ (as well as $X$ and $x^*$ ) are objects of different dimensions. For instance $Y$ is made of $n$ replications $y_i$ 's, of the same dimension as $y^*$ . Furthermore, if $Y^*$ is a continuous variable, $$P(Y^*=y^*|X^*=x^*,X,Y)=0$$ In learning terms, $(X,Y)$ is the learning set, $X=(x_1,\ldots,x_n)$ being the predictors and $Y=(y_1,\ldots,y_n)$ the outcomes .
