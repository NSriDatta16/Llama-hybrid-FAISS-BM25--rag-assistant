[site]: crossvalidated
[post_id]: 471936
[parent_id]: 
[tags]: 
Inference time for text genration using fine-tuned gpt2

I have re-trained GPT2 model using over 10 million sentences for QA. And while testing also I am getting very good results. The only problem now I am facing is that I have millions of test data that I need to process at once. Referring to open-ai github , I have made few changes by looping over test sentences to get the prediction rather that getting output for one sentence at one time. Using CPU I am getting ~30sec for predicting output of each sentence. But we all know from a product perspective if we have millions of data, then this speed is not feasible. So how can I reduce inference timing of my model?
