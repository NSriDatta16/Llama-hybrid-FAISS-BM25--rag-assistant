[site]: crossvalidated
[post_id]: 124800
[parent_id]: 
[tags]: 
Variable selection for multiple linear regression

Using all possible subsets we consider the adjusted $R^2$, Akaike's Information Criterion (AIC), corrected AIC ($AIC_c$), and Bayesian Information Criterion. The model with the highest adjusted $R^2$), and lowest AIC, AICc and BIC is usually the best model. When doing stepwise subsets we use backward elimination and forward selection. Based on the criterion we choose we either add predictor variables to reduce the criterion (forward selection) or subtract predictor variables to reduce the criterion (backward elimination). My question is why do we sometimes encounter different models with the two approaches? Is it because in stepwise subsets we only focus on minimizing one criterion? Also does this mean using all possible subsets provides a better model?
