[site]: crossvalidated
[post_id]: 279310
[parent_id]: 277618
[tags]: 
There are several possible reasons. Try the following things and report: normalize the input : actually, this is nearly a standard thing that should be done with neural networks, most of all if you are using sinusoid activation functions Apply transformations on the input : this depends of course strongly on your data and how it is distributed, but you may apply a log on the input data. I guess this could help quite a lot in your case! Remember that you basically can apply any transformation on the input as long as you apply it every time. In general, bijective functions are prefered (compared to, say, max()) as no real loss of information happens. Change your solver, change your cost-function : There are a lot of solvers and some cost-functions. Try them! For example, adam . May change your activation functions : relu usually performs quite well, leakyRelu can be an option (or similar relus).
