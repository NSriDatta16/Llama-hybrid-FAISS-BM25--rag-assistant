[site]: datascience
[post_id]: 116461
[parent_id]: 24921
[tags]: 
My intuition would tell me the sequence is what is most important in reinforcement learning. Most episodes have a delayed reward so most action/states do not have a reward (and are not "reinforced"). The only way to bring a portion of the reward to these previous states is to retroactively break the reward out across the sequence (through the future_reward in the Q algorithm of reward + reward * learning_rate(future_reward)) While your intuition is correct w.r.t to RL in general ( ie it is always better to deal with complete trajectories with all the dependencies ), the problem arises when we use ML algorithms like gradient descent etc to train a model. These methods require IIID samples and hence there is a conflict now. By using a experience replay store, we can give IIID samples to the model. After a long long training it will eventually learn to build dependencies between states itself. This should address the key doubt OP has.
