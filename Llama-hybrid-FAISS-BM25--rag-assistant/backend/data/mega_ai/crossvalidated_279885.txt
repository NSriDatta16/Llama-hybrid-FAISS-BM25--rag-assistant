[site]: crossvalidated
[post_id]: 279885
[parent_id]: 258593
[tags]: 
If I understand the Semi-supervised Sequence Learning paper correctly, the input to the decoder-portion of the autoencoder is the same as it would be for an ordinary language model. The authors state: We also find that recurrent language models [2, 24] can be used as a pretraining method for LSTMs. This is equivalent to removing the encoder part of the sequence autoencoder in Figure 1 Thus, it should be the same input that is fed to the encoder. The difference, then, would be that the hidden state of the decoder is initialized with the last hidden state of the encoder.
