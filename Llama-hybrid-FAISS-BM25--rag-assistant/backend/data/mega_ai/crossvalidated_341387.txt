[site]: crossvalidated
[post_id]: 341387
[parent_id]: 
[tags]: 
Intuition behind multinomial logistic regression

I need some clarification in my understanding of what's going on under the hood of multinomial logistic regression (MLR). I have a nominal (not ordinal!) dependent variable, $Y$ , that takes values $A$ , $B$ and $C$ and single quantitative predictor, $X$ , so I run MLR with $A$ set as reference level of $Y$ . I get intercepts and regression coefficients for $B$ and $C$ , say $b_{0B}$ , $b_{1B}$ , $b_{0C}$ , $b_{1C}$ . All the sources I consulted (like this one ) say that these are coefficients of following equations: \begin{align} \log\left( \frac{P(Y=B)}{P(Y=A)}\right) = b_{0B}+b_{1B}X \\[10pt] \log\left( \frac{P(Y=C)}{P(Y=A)}\right) = b_{0C}+b_{1C}X \end{align} They (the sources) say also that these estimates come from following procedure: Code $Y$ with dummies, say $Y_B$ that is $1$ if $Y=B$ and $0$ otherwise and $Y_C$ that is $1$ if $Y=C$ and $0$ otherwise. Find estimates for two "ordinary" logistic regressions (one for $Y_B$ and one for $Y_C$ ) at once. My question is: since $Y_B$ is $1$ if $Y=B$ and $0$ , shouldn't we interpret $b_{0B}$ and $b_{1B}$ as coefficients of $$ \log\left( \frac{P(Y_B=1)}{P(Y_B=0)}\right) = \log\left( \frac{P(Y=B)}{P(Y=A | Y=C)}\right) = b_{0B}+b_{1B}X \qquad ?$$ Second question (less important): Is the following procedure wrong? Why, what are its drawbacks? Create 3 variables: $Y_A$ that is $1$ if $Y=A$ and $0$ otherwise, $Y_B$ that is $1$ if $Y=B$ and $0$ otherwise and $Y_C$ that is $1$ if $Y=C$ and $0$ otherwise Estimate three ordinary logistic regressions I know that one of these three dummies is redundant, but thanks to it I could see how $X$ affects log odds of each possible value of $Y$ against any others (log odds of choosing $A$ against any other choice, log odds of choosing $B$ against any other choice, log odds of choosing $C$ against any other choice).
