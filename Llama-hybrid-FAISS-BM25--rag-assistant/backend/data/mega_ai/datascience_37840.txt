[site]: datascience
[post_id]: 37840
[parent_id]: 37838
[tags]: 
You have lots of choices here. You did not say whether the Fi are categorical or numeric. If numeric, logistic regression is a simple and obvious place to start. Logistic regression also works with categorical variables, but best if the number of such variables is not large and the number of categories is small. The logistic regression will let you know if anything is going on at all, but it may not give you the best predictive power. For a small number of categorical variables, I would try a log-linear model. Assuming more variables: If categorical, then I would try a decision tree. Decision trees also work with numerical variables or both -- so it's a good second choice either way, but more suited to categoricals. If you are getting nice results from the tree and the number of variables is large, I would then move to a random forest. These don't add much, in my experience, when the number of variables is small (say 30 or less), but they can greatly improve the stability and predictive power of your model when you have a large number of variables. If the variables are numeric, a support vector machine, or a Gaussian classification model, would improve on your logistic regression. Note. I have a statistics background, rather than a machine learning background, so I tend not to take my explanatory variables at face value. It's nice to get the children off the street, so if the number of variables is not super huge, I might try a lasso regression to see if I can drop any of the variables. If a number of your explanatory variables do not actually give much information about the target, they will basically add noise and instability to your model. But I'm old fashioned. A lot of people just throw everything in the hopper and leave it at that.
