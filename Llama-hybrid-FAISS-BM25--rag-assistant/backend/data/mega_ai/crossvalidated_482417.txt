[site]: crossvalidated
[post_id]: 482417
[parent_id]: 192153
[tags]: 
Yes and no. By having a sufficiently deep tree (at least two splits deep) and splitting on both $x_1$ and $x_2$ , tree based model like xgboost (or LightGBM or catboost) can eventually approximate (given enough data) any relationship between $x_1\times x_2$ and your prediction target of interest. Of course, if you know that some function $f(x_1\times x_2)$ is a predictor of your outcome of interest, then specifying a feature that is that exact transformation of $x_1$ and $x_2$ is what you should do. What to do in practice? If you have a lot of data and are not that sure that the interaction matters, perhaps don't specify it as a feature. If you don't have a lot of data and suspect it's an important feature, you probably want to put this feature into xgboost. If you are inbetween - try it out using an appropriate cross-validation scheme.
