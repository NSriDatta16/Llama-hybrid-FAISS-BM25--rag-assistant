[site]: crossvalidated
[post_id]: 21108
[parent_id]: 21106
[tags]: 
Your posterior is correct (I edited a missing $\sqrt{}$). Because you are using conjugate priors you actually do not need Gibbs sampling, you can derive exactly the posterior distribution. This is shown for instance in our book, Bayesian Core , where the entire chapter 3 is dedicated to Gaussian linear regression. If you really want to run a Gibbs sampler (is this an homework?!), the above posterior is all you need. From there, you extract the terms that depend on $\beta_0$, $$ \pi(\beta_0|\beta_{-0},x,\mu)\propto \exp\left[-\beta_0^2/2-\mu\sum_i \{y_i−(β_0+β_1x_{i1}+β_2x_{i2})\}^2/2\right] $$ and this gives you a Gaussian distribution with parameters depending on the data and the other parameters. Then you do the same for $\beta_1$, and $\beta_2$, again ending up with two Gaussian distributions. Then for $\mu$, where the conditional distribution is then a Gamma distribution. You should find further details, if needed, in any Bayesian book, including Bayesian Core , Monte Carlo Statistical Methods , and Bayesian Data Analysis .
