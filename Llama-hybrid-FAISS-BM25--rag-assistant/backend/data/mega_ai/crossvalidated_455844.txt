[site]: crossvalidated
[post_id]: 455844
[parent_id]: 406780
[tags]: 
I think I see your point. If you define a prior probability density by $\pi_{\theta}(x) = \frac{d}{d\theta}F(\theta)$ then under (nice) reparametrization $\lambda(\theta)$ , you get that that $\pi_{\lambda}(\lambda) = \frac{d}{d\lambda}F(\theta(\lambda))$ . So the definition is parametrization invariant. However, this property is nice only if this definition of the prior as $\frac{d}{d\theta}F(\theta)$ has a nice meaning, then, this nice meaning is conserved under reparametrization. That's not the case for lots of priors. A uniform distribution is usually meaningfull through its pdf: it's the same everywhere, and this meaning is not stable under reparametrization. Jeffreys' prior definition is not only parametrization-invariant but also meaningful for a given conditional model . A nice way of seeing this meaning (from Jeffreys (1945) ) is to note that if you have conditional densities $\{f(x|\theta) ; \theta\in\Theta\}$ , then $KL(f(.|\theta) + f(.|\theta')) \approx I(\theta)(\theta' - \theta)^2$ if $\theta$ and $\theta'$ are close (here KL denotes the symmetrized Kullback-Liebler divergence). Then, computing probabilities with Jeffreys' prior corresponding to the conditional model: $$P([\theta, \theta + d\theta]) = \sqrt{I(\theta)}d\theta = \sqrt{KL[f(.|\theta), f(.|\theta + d\theta})].$$ And thus : $$P([\theta + d\theta]) = P([\theta' + d\theta]) \Leftrightarrow KL\left[f(.|\theta), f(.|\theta + d\theta)\right] = KL\left[f(.|\theta'), f(.|\theta' + d\theta)\right]$$ And this relation holds for any parametrization. So one could say that Jeffreys' prior is uniform in the space of the conditional densities with KL metric (this is informal, KL is not a proper metric). In its original argument, Jeffreys sees this prior as curvilinear coordinate on the map $ \theta \mapsto f(.|\theta)$ for KL distance, which (I think) is equivalent. Other more pragmatical arguments for Jeffreys prior could be : It often corresponds to already well used objective priors : uniform for a location parameter, inverse for a location parameter, Haar priors. It gives bias reduction on the Maximum A Posterior (considered as a frequentist estimator) on exponential family models (see Firth, 1993: Bias Reduction of Maximum Likelihood Estimates ). Hope this brings something..
