[site]: datascience
[post_id]: 45306
[parent_id]: 45282
[tags]: 
You should use something like an autoencoder . Basically. you pass your images through a CNN (the encoder) with decreasing layer size. The last layer of this network is the one that produce the embeddings (that is, a lower dimensional representation of your input), and the number of neurons you use here is the length of your vector embedding for the input images. Now, your embeddings are useful only if they actual encode the data in your images. To achieve this, you need another network (the decoder) that takes as input the image embedding and outputs an image with the same dimension of the input. Here, you try to minimize a loss function that tells you the distance between the image you generate from the embedding and the initial image (it might be Euclidean distance between pixel values). Finally, if you need to output the embedding of an image, you just need to pass the image through the encoder network and collect the output. Here's a simple tutorial.
