A flow-based generative model is a generative model used in machine learning that explicitly models a probability distribution by leveraging normalizing flow, which is a statistical method using the change-of-variable law of probabilities to transform a simple distribution into a complex one. The direct modeling of likelihood provides many advantages. For example, the negative log-likelihood can be directly computed and minimized as the loss function. Additionally, novel samples can be generated by sampling from the initial distribution, and applying the flow transformation. In contrast, many alternative generative modeling methods such as variational autoencoder (VAE) and generative adversarial network do not explicitly represent the likelihood function. Method Let z 0 {\displaystyle z_{0}} be a (possibly multivariate) random variable with distribution p 0 ( z 0 ) {\displaystyle p_{0}(z_{0})} . For i = 1 , . . . , K {\displaystyle i=1,...,K} , let z i = f i ( z i − 1 ) {\displaystyle z_{i}=f_{i}(z_{i-1})} be a sequence of random variables transformed from z 0 {\displaystyle z_{0}} . The functions f 1 , . . . , f K {\displaystyle f_{1},...,f_{K}} should be invertible, i.e. the inverse function f i − 1 {\displaystyle f_{i}^{-1}} exists. The final output z K {\displaystyle z_{K}} models the target distribution. The log likelihood of z K {\displaystyle z_{K}} is (see derivation): log ⁡ p K ( z K ) = log ⁡ p 0 ( z 0 ) − ∑ i = 1 K log ⁡ | det d f i ( z i − 1 ) d z i − 1 | {\displaystyle \log p_{K}(z_{K})=\log p_{0}(z_{0})-\sum _{i=1}^{K}\log \left|\det {\frac {df_{i}(z_{i-1})}{dz_{i-1}}}\right|} Learning probability distributions by differentiating such log Jacobians originated in the Infomax (maximum likelihood) approach to ICA, which forms a single-layer (K=1) flow-based model. Relatedly, the single layer precursor of conditional generative flows appeared in . To efficiently compute the log likelihood, the functions f 1 , . . . , f K {\displaystyle f_{1},...,f_{K}} should be easily invertible, and the determinants of their Jacobians should be simple to compute. In practice, the functions f 1 , . . . , f K {\displaystyle f_{1},...,f_{K}} are modeled using deep neural networks, and are trained to minimize the negative log-likelihood of data samples from the target distribution. These architectures are usually designed such that only the forward pass of the neural network is required in both the inverse and the Jacobian determinant calculations. Examples of such architectures include NICE, RealNVP, and Glow. Derivation of log likelihood Consider z 1 {\displaystyle z_{1}} and z 0 {\displaystyle z_{0}} . Note that z 0 = f 1 − 1 ( z 1 ) {\displaystyle z_{0}=f_{1}^{-1}(z_{1})} . By the change of variable formula, the distribution of z 1 {\displaystyle z_{1}} is: p 1 ( z 1 ) = p 0 ( z 0 ) | det d f 1 − 1 ( z 1 ) d z 1 | {\displaystyle p_{1}(z_{1})=p_{0}(z_{0})\left|\det {\frac {df_{1}^{-1}(z_{1})}{dz_{1}}}\right|} Where det d f 1 − 1 ( z 1 ) d z 1 {\displaystyle \det {\frac {df_{1}^{-1}(z_{1})}{dz_{1}}}} is the determinant of the Jacobian matrix of f 1 − 1 {\displaystyle f_{1}^{-1}} . By the inverse function theorem: p 1 ( z 1 ) = p 0 ( z 0 ) | det ( d f 1 ( z 0 ) d z 0 ) − 1 | {\displaystyle p_{1}(z_{1})=p_{0}(z_{0})\left|\det \left({\frac {df_{1}(z_{0})}{dz_{0}}}\right)^{-1}\right|} By the identity det ( A − 1 ) = det ( A ) − 1 {\displaystyle \det(A^{-1})=\det(A)^{-1}} (where A {\displaystyle A} is an invertible matrix), we have: p 1 ( z 1 ) = p 0 ( z 0 ) | det d f 1 ( z 0 ) d z 0 | − 1 {\displaystyle p_{1}(z_{1})=p_{0}(z_{0})\left|\det {\frac {df_{1}(z_{0})}{dz_{0}}}\right|^{-1}} The log likelihood is thus: log ⁡ p 1 ( z 1 ) = log ⁡ p 0 ( z 0 ) − log ⁡ | det d f 1 ( z 0 ) d z 0 | {\displaystyle \log p_{1}(z_{1})=\log p_{0}(z_{0})-\log \left|\det {\frac {df_{1}(z_{0})}{dz_{0}}}\right|} In general, the above applies to any z i {\displaystyle z_{i}} and z i − 1 {\displaystyle z_{i-1}} . Since log ⁡ p i ( z i ) {\displaystyle \log p_{i}(z