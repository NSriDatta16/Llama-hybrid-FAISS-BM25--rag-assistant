[site]: datascience
[post_id]: 123194
[parent_id]: 
[tags]: 
Hack to generate training data for a fantasy language?

It is an impractical task of collecting millions of example sentences with their translations in a "fantasy language" ( conlang ). At most, you can probably have a thousand or 2k sentences before you get tired of it. However, it might be possible in my case to generate sentences given English input, if I greatly restrict the number of types of words used in the English sentences. I could then use ChatGPT or other LLMs to generate potentially 10's of thousands of example sentences, using a restricted vocabulary, and then programmatically (hardcoded) transform those into the fantasy language. That would give me say 10-20k sentences, using a limited vocab but many sentence-level grammar features. Any more sentences than that would get time consuming. Is it possible to use those sentences as training data in building a translation system? Then it could take new words (in theory) and create sentences from that, following its training data examples? Is that a hack that could work? Or what would be your recommendation for building a translation system for a language which doesn't have much in terms of resources (an extremely "low resource" language)?
