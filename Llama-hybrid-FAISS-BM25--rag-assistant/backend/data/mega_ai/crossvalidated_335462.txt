[site]: crossvalidated
[post_id]: 335462
[parent_id]: 335461
[tags]: 
There's a big difference between "little" and "no" variation. If a variable has little variation, it can still have predictive importance. This should be obvious: if all values of a feature larger than some constant $c$ corresponds to one class, and all other values to the opposite class, it's a really good predictor. (Likewise, if it's completely independent of the class label, then it's worthless.) This is true no matter what the variance of the feature. A variable with no variation cannot predict the outcome. It won't be selected as a feature, since splitting on the feature can never provide information gain. Including (a large number of) zero-variation features can harm the model, however, because random forest randomly selects feature subsets at each split; a large number of invariant features means that the model will not have any information to split on if the random features are all worthless.
