[site]: datascience
[post_id]: 88423
[parent_id]: 88417
[tags]: 
Because BERT accepts the artificial assumption of independence between masked tokens, presumably because it makes the problem simpler and yet gave excellent results. This is not discussed by authors in the article or anywhere else to my knowledge. Later works like XLNet have worked towards eliminating such an independence assumption, as well as other potential problems identified in BERT. However, despite improving on BERT's results on downstream tasks, XLNet has not gained the same level of attention and amount of derived works. In my opinion, this is because the improvement did not justify the complexity introduced by the permutation language modeling objective. The same assumption is made by other pre-training approaches, like Electra 's adversarial training. The authors argue that this assumption isnâ€™t too bad because few tokens are actually masked, and it simplifies the approach.
