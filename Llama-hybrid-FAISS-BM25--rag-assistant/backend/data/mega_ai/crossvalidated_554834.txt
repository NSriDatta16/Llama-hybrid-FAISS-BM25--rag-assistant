[site]: crossvalidated
[post_id]: 554834
[parent_id]: 325234
[tags]: 
It is not true that sufficient statistics are only relevant with maximum likelihood estimators. For instance, knowing a sufficient statistic is enough for Bayesian inference, see the calculation in Reducing dimension $P(\theta|y) = P(\theta|s)$ in the posterior distribution . People usually claim that sufficient statistics are a data reduction technique. That is, I can store only the summary data expressed by the form of the observed value of the sufficient statistic and I will have enough information to make inference about the parameter of interest, , in the future. To me, this is true only if we will make inference via maximum likelihood. See the Rao-Blackwell theorem: Understanding the Rao-Blackwell Theorem and at Wikipedia . Some other relevant posts: Maximum likelihood and sufficient statistics Is there a difference between Bayesian and Classical sufficiency? Why a sufficient statistic contains all the information needed to compute any estimate of the parameter? What is the correct posterior when data are sufficient statistics?
