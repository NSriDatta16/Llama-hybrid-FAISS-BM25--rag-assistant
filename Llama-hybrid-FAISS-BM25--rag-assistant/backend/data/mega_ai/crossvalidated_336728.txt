[site]: crossvalidated
[post_id]: 336728
[parent_id]: 297749
[tags]: 
I'll answer from a slightly more general perspective, concerning the nature of how, when, and why we can consider NN outputs to be probability distributions. In the sense that the softmax enforces the outputs to sum to 1 and also be non-negative, the output of the network is a discrete probability distribution over the classes, or at least can be interpreted as such. Hence it is perfectly reasonable to talk about cross-entropies and maximum likelihoods. However, what I think you are seeing (and it is correct), is that the output "probabilities" may have nothing to do with the actual probability of correctness . This is a well-known problem in ML, called calibration . For instance, if your classifier $f_\theta$ of dogs $D$ and cats $C$ says $f_\theta(x_i,C) = P(x_i = C|\theta) = 0.7$, then you would expect that if you took a set of examples $S=\{x_j\}$ all of which had $P(x_j = C|\theta) = 0.7$, then roughly 30% of the inputs would be misclassified (since it was only 70% confident). However, it turns out that modern training methods do not enforce this at all! See Guo et al, On the Calibration of Modern Neural Networks to see some discussion of this. In other words, the "probability" of the output from the softmax may well have nothing to do with the actual model confidence. And this is no surprise: we merely want to maximize our accuracy, and every input example has a probability of 1 of being its target class. There is little incentivizing the model to get this right. If it doesn't need to estimate uncertainty then why should it? Cross-entropy does not rectify this issue; indeed, you are telling it to go to a delta function every time! Lots of recent work on Bayesian neural networks strive to rectify this issue. Such models employ a distribution over parameters given the data $P(\theta|X) = P(X|\theta)P(\theta)/P(X)$, which can be integrated to get an actual probability distribution $P(y_i|x_i,X)=\int P(y_i|\theta,x_i) P(\theta|X) \,d\theta$. This helps guarantee useful uncertainty measurements and better calibration. However, it is more problematic computationally. Hopefully I didn't misunderstand your question!
