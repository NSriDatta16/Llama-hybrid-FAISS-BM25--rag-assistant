[site]: crossvalidated
[post_id]: 563129
[parent_id]: 
[tags]: 
Marginalizing out discrete response variables in Stan

There's been quite a bit of discussion and confusion about how to marginalize out discrete response variables in Stan (e.g. binary or ordinal data). See, for instance: Impute binary outcome variable for GLM using Stan in R https://discourse.mc-stan.org/t/marginalize-missing-binary-outcome-variable-for-glm/12858 https://discourse.mc-stan.org/t/marginalising-out-missing-categorical-response-variable-cases-provides-inaccurate-predictor-estimates/9601/6 https://groups.google.com/g/stan-users/c/9XEOnOiL99g However, I don't think there is, as yet, a clear answer, so I want to open the discussion more broadly here. Model For this example, we'll use a binary logistic regression of response variable $y$ on metric predictor $x$ for $i$ in $\{1,...N\}$ data points: $$ \begin{equation} y_i \sim Bernoulli(\eta_i) \\ \eta_i = \text{logit}^{-1}(\alpha + \beta x_i) \\ \alpha \sim \mathcal{N}(0, 5) \\ \beta \sim \mathcal{N}(0, 1) \end{equation} $$ Now, consider that some of the $y_i$ variables are missing completely at random with probability $\rho$ -- denote them $y_{i}^{miss}$ . In this case, the variables can be safely ignored in the likelihood statement, because we can learn everything we need about the model parameters by only considering $y$ , i.e. $p(y | \eta, y^{miss}) = p(y | \eta)$ . However, for convenience, let's say someone wants to keep the missing outcome variables in their model, and marginalize them out of the likelihood. The likelihood for observed variables is just $p(y_{i} | \eta_i) = Bernoulli(y_{i} | \eta_{i})$ as above. But when the variables are missing, we have two possibilities: either the data is 1 or the data is 0. Option 1: The accepted answer here , and also discussed here suggests that, when the data is missing, we need a statement that is equivalent to: $p(y_{i}^{miss} = 1 | \eta_{i}) p(\eta_{i}) + p(y_{i}^{miss} = 0 | \eta_{i}) p(\eta_{i})$ , which evaluates to $\eta_i \eta_i + (1 - \eta_{i}) (1 - \eta_{i})$ . In Stan, this is what the following line is doing, on the log scale: target += log_mix( eta[i], bernoulli_lpmf(1 | eta[i]), bernoulli_lpmf(0 | eta[i]) ); I simulated some data in R from the following function: set.seed(12345) gen_bernoulli_data and fit the model using cmdstanr : bernoulli_model $sample( data = list( N = N, y = ifelse(d$ missing, -1000, d $y), x = d$ x, missing = d$missing, do_marginalization = 1 ), parallel_chains = 4 ) with the following Stan program: data{ int N; int y[N]; vector[N] x; int missing[N]; int do_marginalization; } transformed data{ int missing_idx[N]; int N_miss = sum(missing); for(n in 1:N) missing_idx[n] = missing[n] * sum(missing[1:n]); } parameters{ real alpha; real beta; real rho; } transformed parameters{ vector[N] eta; real lp[N_miss]; eta = alpha + beta * x; for(n in 1:N){ if(missing[n]){ real lp10[2]; lp10[1] = log(inv_logit(eta[n])) + bernoulli_logit_lpmf(1 | eta[n]); lp10[2] = log1m(inv_logit(eta[n])) + bernoulli_logit_lpmf(0 | eta[n]); lp[missing_idx[n]] = log_sum_exp(lp10); } } } model{ alpha ~ normal(0, 5); beta ~ normal(0, 1); rho ~ beta(1, 1); missing ~ bernoulli(rho); for(n in 1:N){ if(!missing[n]) target += bernoulli_logit_lpmf(y[n] | eta[n]); else{ if(do_marginalization) target += lp[missing_idx[n]]; } } } This results in the following output: As you can see, $\beta$ is massively inflated, although it looks like $\alpha$ is estimated accurately, with quite a bit of uncertainty (which we might expect). Switching off the marginalization to only evaluate the observed data points results in the following, more accurate plot: Option 2 From this discussion , it seems that there is some suggestion that the option 1 is wrong, and the correct way to marginalize is to introduce a new vector, call it $\mathbf{\theta}$ where $\theta_{i}$ is the probability that data point $i$ is missing, and is $0$ otherwise. Now, this feels weird to me, because this seems to be saying we need to evaluate the statement $p(y) = p(y=1) p(y = 1 | \eta) + p(y=0) p(y = 0| \eta) = \theta \eta + (1 - \theta) (1 - \eta)$ but that implies that $p(y=1) \neq \eta$ and $p(y=0) \neq 1 - \eta$ , which is wrong. Or does this imply that we need a statement like $p(y_i=1, \eta) = p(\eta | y_i=1) p(y_i=1)$ and $p(y_i=0, \eta) = p(\eta | y_i=0) p(y_i=1)$ ? In either case, this version seems to work. Using the following Stan code: data{ int N; int y[N]; vector[N] x; int missing[N]; int do_marginalization; } transformed data{ int missing_idx[N]; int N_miss = sum(missing); for(n in 1:N) missing_idx[n] = missing[n] * sum(missing[1:n]); } parameters{ real alpha; real beta; real rho; real theta[N_miss]; } transformed parameters{ vector[N] eta; real lp[N_miss]; eta = alpha + beta * x; for(n in 1:N){ if(missing[n]){ real lp10[2]; lp10[1] = log(theta[missing_idx[n]]) + bernoulli_logit_lpmf(1 | eta[n]); lp10[2] = log1m(theta[missing_idx[n]]) + bernoulli_logit_lpmf(0 | eta[n]); lp[missing_idx[n]] = log_sum_exp(lp10); } } } model{ alpha ~ normal(0, 5); beta ~ normal(0, 1); rho ~ beta(1, 1); theta ~ beta(1, 1); missing ~ bernoulli(rho); for(n in 1:N){ if(!missing[n]){ target += bernoulli_logit_lpmf(y[n] | eta[n]); } else{ if(do_marginalization) target += lp[missing_idx[n]]; } } } we recover the parameters, which are essentially identical to the version of the model without the marginalization: However, this feels odd to me, because while I'd expect to recover the parameters, I might expect there to me more uncertainty when marginalizing over the missing data, especially in the intercept. Is this intuition correct? Summary Can anyone shed light on what the second option above works, and provide the mathematical intuition? EDIT: I should add that there is consensus on how to marginalize out missing discrete covariates. In that case, where $y$ is observed, and there is a discrete covariate $x = \{0, 1\}$ , we want $p(y) = p(y | x = 0) p(x = 0) + p(y | x=1) p(x=1)$ , which is gleaned from the total law of probability. For instance, see https://elevanth.org/blog/2018/01/29/algebra-and-missingness/
