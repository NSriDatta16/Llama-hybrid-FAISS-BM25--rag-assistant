[site]: crossvalidated
[post_id]: 257169
[parent_id]: 257161
[tags]: 
This sounds something like posterior predictive checks in Bayesian statistics. You can come up with several statistics that measure something about your actual data and then calculate those statistics on multiple (say 1,000) simulations and see where the actual value falls in the distribution. For example, you could calculate the total number of events in your real data. Then calculate the total number of events in each of your 1,000 simulations. Then see if the actual number is contained in, say, the (0.025, 0.975) quantile of the simulations. If not, your simulations and actual data aren't consistent on this statistic. If you think your simulations are realistic, this is an indication that they're not as realistic as you might think. You might be able to flip this to make a statement about reality but that makes me nervous. You can do the same thing for various aspects you want to check. You mention clustering at the beginning. Say your actual data has 10 events in the first six months and only two in the last six months: a ratio of 5:1. How about calculate that ratio for your 1,000 simulations and see where in that distribution your actual falls? Or maybe you could look at the maximum number of simultaneous events. Or the percentage of the year in which there were no events. You can compare SD or mean of event lengths, the mean (or max) gaps between events, etc, etc. Find a way to express what would make the simulations realistic, in terms of statistic and see whether the actual is consistent with the simulations or not. I believe this approach might be helpful in your situation, and I believe it's called resampling statistics. [Thinking about my 5:1 ratio remark, I'm not sure if ratios are tricky or not. If you're satisfied that you're getting a reasonable number of events from your simulation, you could perhaps use the difference in the number of events in the first half and the last half of the year instead of a ratio. Not sure.]
