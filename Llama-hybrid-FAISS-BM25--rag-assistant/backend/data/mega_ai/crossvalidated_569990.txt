[site]: crossvalidated
[post_id]: 569990
[parent_id]: 
[tags]: 
FID as a metric to evaluate the quality of synthetic datasets (Non GAN generated) for training models for a given classification task

I am working on a problem of generating synthetic data (algorithmically by blender, not using GANs) to aid the training of some CNN for a classification ask. Ideally, I want to generate an algorithm that generates synthetic samples as close to real samples as possible. But it happens that it is very hard to design this algoritm and define hyperparamters just looking at the results and guessing if they are similar enough to the real dataset. It is also not doable to generate many datasets and decide the hyperparameters according to the final classification metric, since the training the CNN takes too long. So I want to get some good methodology to tune my synthetic generator without having to look at them and without having to go through the costly process of training the downstream model. My idea was trying to calculate the Frechet Distance (FID) between my synthetic dataset and a real data and test the effects of different changes on the algorithmic according to the change on the FID metric. My question is, is it a reasonable idea? I am using this for my master thesis so I feel that I need a reasonable methodology. Another question is, I know about KL divergence. Would that be a better idea?
