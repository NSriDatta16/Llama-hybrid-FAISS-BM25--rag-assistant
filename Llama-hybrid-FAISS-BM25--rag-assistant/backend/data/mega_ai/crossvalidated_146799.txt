[site]: crossvalidated
[post_id]: 146799
[parent_id]: 
[tags]: 
Is Predicted R-squared a Valid Method for Rejecting Additional Explanatory Variables in a Model?

I'm building a model to understand the important drivers from a set of possible drivers for a time series of data. In my case the possible drivers are other time series. Like most statistical models I can always add additional drivers and improve the quality of my fit (measured by variance explained). In this case I'm using forward selection to add additional drivers requiring that the variance explained improve by a least certain percentage to determine whether I should add more drivers at all. This given percentage feels arbitrary depending on its value I may overfit. I was wondering if improvement in Predicted R^2 (definition from minitab.com below) would be a more consistent and better performing method for understanding when to stop adding additional drivers? Predicted R2 is calculated by systematically removing each observation from the data set, estimating the regression equation, and determining how well the model predicts the removed observation.
