[site]: crossvalidated
[post_id]: 633434
[parent_id]: 
[tags]: 
Rationale for box-constrained optimization in adversarial example search

In Section 4.1 of "Intriguing properties of neural networks" by Szegedy et al. , the authors define the optimization problem they solve to find adversarial examples in a deep neural network. I am a bit puzzled by the second constraint: why is it necessary to require (2) $x+r \in [0,1]^m$ ? The authors say that solutions $r$ to such optimization problem yield $x+r$ the closest data point to $x$ that is classified by $f$ to a class different than $f(x)$ . I don't see the role that constraint (2) plays in this. They state that $x\in\mathbb R^m$ , so $x+r$ should also be in $\mathbb R^m$ .
