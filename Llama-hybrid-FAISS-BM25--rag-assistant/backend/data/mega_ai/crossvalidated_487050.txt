[site]: crossvalidated
[post_id]: 487050
[parent_id]: 487034
[tags]: 
XGBoost does not have " a clearly defined DoF " unfortunately. Degrees of freedom are not well-defined for regression trees. This is even more pronounces in the case of gradient boosters where we use potentially hunderds of trees. Adding in the fact that we also regularise our fitting procedures both within a tree and across trees makes the whole concept even muddier. (So we cannot even see it is something like num_of_trees times mean_num_of_nodes_per_tree .) There is some work on data pertubation methods that have been used for regression trees. Those can lead to concepts like Generalised DoF (e.g. Ye 1998 On Measuring and Correcting the Effects of Data Mining and Model Selection ) but that these methods themselves are covariance-based approximations not particular to gradient booster (and let alone XGBoost in particular)). Your initial idea about using cross-validation for comparing model performance is the way to go. :)
