[site]: datascience
[post_id]: 36651
[parent_id]: 
[tags]: 
Relationship between batch size and the number of neurons in the input layer

Regarding LSTM neural networks, I am unable to understand the relationship between batch size, the number of neurons in the input layer and the number of "variables" or "columns" in the input. (Assuming that there is a relationship and despite seeing examples to the contrary, I cannot understand why there is no relationship) For the sake of clarity, I am going to use an example to formulate my query. Let's assume that the dataset contains three columns of input and one column of output. So it will be something like input variable 1 input variable 2 input variable 3 output variable 1 From what I understand, the input layer of the LSTM network has to have 3 neurons corresponding to each of the input variables. It cannot be less or more . Even though I have seen examples like this answer (which appears to be very well described but unfortunately, I am not able to comprehend it.) Now let us say that we have 50 rows of the above 4 columns. That is essentially to me means that we have 50 samples. Now if the batch size is 5, then how many input neurons we have? Is the number of neurons in the input layer independent of the batch size? The way I understand batch size is that it is the number of samples that the neural network will see before updating its weights. So let's assume if we have only three neurons in the input layer, then we will be passing the first row of input variable, followed by the second row of input variable and repeating it until the fifth row before we update the weights. How is that going to be any different than just passing the fifth row?
