[site]: datascience
[post_id]: 58219
[parent_id]: 58184
[tags]: 
One of the basic assumptions governing Machine Learning is that samples from the training set must follow the same underlying distribution as samples from the test set (and so must any other sample you want fed to your model)! This is why, usually, we randomly partition the same dataset into training and test sets. This is actually one of the main reasons ML models underperform in some real world applications. You might have trained your model in a specific dataset, but overtime the data slightly changed its characteristics and new data differ from the old ones that were used to train the deployed model. In this case you need to retrain your model on the new data.
