[site]: crossvalidated
[post_id]: 600224
[parent_id]: 419916
[tags]: 
Much research is done in many fields. In many journals and fields statistical significance is seen as a requirement for a "discovery" worth publishing. This means that there is publication bias ; significant studies (i.e. studies where the confidence interval does not include "no effect", often formalised as parameter zero) are published and insignificant ones disappear, and people look at many things until they find something significant. The consequence of this is that published significant effects are actually biased high; in other words, confidence intervals are too far on one side, away from "no effect". Using credible intervals should force the researcher to think about prior information and plausibility. (In fact this often doesn't happen, supposedly "informationless" priors are used, and the situation may not be better than with confidence intervals.) Andrew Gelman argues on his blog that in many social science studies priors should be chosen so that small and zero effects are more and large effects are less likely, so that credible intervals have a better chance to include zero and small effects, mitigating the publication bias problem. There are various references on the blog to studies that find significant and in fact implausibly large effects using frequentist inference, e.g., that more attractive parents tend to have daughters rather than sons, or that voting behaviour of women depends on how long after ovulation the election takes place. It's somewhat hard to find a specific one using the blog search system but there are many of them. One related posting is this. I admit though that this issue is only somewhat loosely related to the question, as the problem with confidence intervals here is not in the first place that they are wrongly interpreted as credible intervals (and in fact credible intervals may mess things up as well with an unsuitable prior). There is some relation though. The bare fact that the confidence interval has large effect sizes in it shouldn't lead us to believe that these are plausible or true with high probability. They are mathematically correct, but the confidence level is a performance characteristic rather than a measure of plausibility/probability of parameters, and the performance characteristic has limited value or even requires adjustment in case many confidence intervals are in fact run, or they are run conditionally on other diagnoses performed on the same data. Bayesian analyses grant epistemic interpretation, i.e., probabilities assigned to parameters regard our knowledge/expectations of the characteristics of the underlying process rather than the performance of the method. This will however not necessarily solve the problem. In particular, publication and selection bias can still bite if results based on priors are favoured that lead to headline-grabbing claims or if priors are chosen dependent on the data. Furthermore all Bayesian results are of course conditional on prior and model, and can only do better if these are chosen taken information appropriately into account. Often "informationless priors" are used that simply reproduce a problematic frequentist analysis. I should also mention that frequentists argue that we actually should be interested in performance characteristics, and that it is a bug rather than a feature of Bayesian analyses that they don't bother with this.
