[site]: crossvalidated
[post_id]: 360008
[parent_id]: 
[tags]: 
Why caring about the dimensions and the axis of computation when calculating the derivative of the bias vector in backpropagation?

In order to create a linear backward function for my first deep neural network I wanted to calculate the derivative of $b$, the bias vector in the l th layer, $db^{[l]} = \frac{\partial \mathcal{L} }{\partial b^{[l]}} = \frac{1}{m} \sum_{i = 1}^{m} dZ^{[l](i)}$ following an IPython notebook tutorial . Starting innocently with db = (1./m)*np.sum(dZ) I quickly noticed that this wasn't enough to computer it as far as it seemed t create dimension errors. So I added some few parameters taht made the job but I don't understand why : db = (1./m)*np.sum(dZ,axis=1,keepdims=True) Therefore, why do we have to care about the dimensions and the axis of computation when calculating the derivative of the bias vector in backpropagation ?
