[site]: datascience
[post_id]: 100026
[parent_id]: 100021
[tags]: 
The algorithm on Fig. 2 can de rephrased as: Take $T$ sequential rows of data. Omit random parts of the rows ( Random Mask ). Put each masked row separately into a Field Transformer. (It doesn't care about where the row was in the document.) Put the resulting strings together as a sentence into BERT in the same order they were in the document. Receive rows' embeddings that account for where they were in the document. Regarding your question about the row embedding, it is a sequence of characters that were allowed in BERT. Likely, it's a mix of Latin characters and numbers that would not look like an English word.
