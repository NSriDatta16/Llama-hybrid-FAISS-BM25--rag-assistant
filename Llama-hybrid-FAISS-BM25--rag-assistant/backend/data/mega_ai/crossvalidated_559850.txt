[site]: crossvalidated
[post_id]: 559850
[parent_id]: 559808
[tags]: 
Take a simple example of computing seasonal adjustment factor for months across a set of years for a company's sales. Assume there is no linear trend except if years are associated with an inflationary period. Note: In reality, one would work a log transform of the data which assumes a constant percent change relationship across time. Collapsing the month data across years produces good results by month if inflationary periods are rare. If you happen to guess that the year seasonality is in a non-inflationary period, you have the best estimates with the best error estimates. So, the dimensionality reduction (ignoring years) is clearly best. However, if it turns out that you are in an inflationary periods, not so good monthly seasonal adjustment. However, a year model may capture the inflation trend and produce better results. So which model to use, collapsed or full? One approach is to estimate the probability that you could be an inflationary period based on history, Next, what is the operational cost associated with having an average error of X in a months' seasonality. Knowing the difference in cost by month due to modeling error for collapsed vs full for inflationary versus non-inflationary, and the associated probability of each case, one can make a decision that produces the lowest expected cost. This assumes that this exercise repeats over time and starting parameters estimate are good estimates. So, the specific answer relates to the nature of the data, model specification/estimation precision and associated knowledge relating to error cost estimates.
