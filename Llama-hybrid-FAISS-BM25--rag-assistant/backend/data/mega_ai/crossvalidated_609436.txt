[site]: crossvalidated
[post_id]: 609436
[parent_id]: 
[tags]: 
Second differences notion behind GAM penalties

I'm going back through Simon Wood's book on generalized additive models (GAMs) and came back across the definition of the penalty term employed, which is supposed to combat overfitting of smooths. The equation used is the following: $$ \Vert{y-X\beta}\Vert^2 + \lambda \sum_{j=2}^{k-1}\{ f(x^*_{j-1})-2f(x^*_j) + f(x^*_{j+1}) \}^2 $$ The book goes on to explain this formula in the following sentence: The summation term measures wiggliness as a sum of squared second differences of the function at the knots (which crudely approximates the integrated squared second derivative penalty used in cubic spline smoothing: see section 5.1.2, p. 198). When f is very wiggly the penalty will take high values and when f is ‘smooth’ the penalty will be low. If f is a straight line then the penalty is actually zero. So here is what I get from this paragraph: The summation from the right side of the equation, if higher, indicates high wiggliness and will be penalized thereafter. The second differences are some kind of approximation of quadratic, cubic, etc. relations with the variable. There is no penalty assigned when the function is linear because there is no wiggliness to add to the right side of this equation. My question is the following... how exactly does this equation achieve this? I'm not sure if the $\lambda$ component is part of the confusion, because from what I see it is simply labeled as the smoothing parameter on the next page. The three functions within the equation confuse me as to what they are doing exactly, and the exponent on the outside of the brackets also doesn't make it immediately clear what is going on.
