[site]: crossvalidated
[post_id]: 602506
[parent_id]: 579132
[tags]: 
The start vector ( $S$ ) contains a set of weights. You can implement it as a linear layer without bias. Remember dot product is just element wise multiplication and sum. This is what a linear layer without a bias does. In other words you use an nn.Linear() layer in Pytorch where input_features equals the size of $T_i$ and output_features is 1. And your start vector $S$ is the weights of this linear layer. The linear is applied at all the token locations (or in other words, a dot product between $S$ and all $T_i$ s are taken, yielding a scalar value at each token location $i$ ) You apply Softmax on top of it. The softmax scores can be interpreted as the probability of a particular token being start token. At the time of training, you compute cross entropy between the softmax scores and a one hot vector that has a 1 at the location of the correct start token. For the end token, loss is computed in similar fashion using a learnt end token vector. And the total loss is the sum of the cross entropy loss for start token and end token. At the time of finetuning you find the most probable start and end token with a constraint that the position/order of the end token must be greater than or equal to the position of the start token References https://mccormickml.com/2020/03/10/question-answering-with-a-fine-tuned-BERT/ https://d2l.ai/chapter_natural-language-processing-applications/finetuning-bert.html
