[site]: crossvalidated
[post_id]: 338585
[parent_id]: 
[tags]: 
Do shrinkage estimators solve the Neyman-Scott paradox?

I read the following SE question: What problem do shrinkage methods solve? And I wondered if shrinkage estimators provide a consistent estimator of the sample variance in a "mixed-effects" model using fixed effects for cluster adjustment. This is called the Neyman-Scott paradox. This brief note shows how the MLE is inconsistent and (I think) how conditional likelihood solves the problem. It helps that the solution is an elegant one! Roughly, the Neyman-Scott paradox illustrates how maximum likelihood estimators can be inconsistent when the parameter space grows with the sample size. From what I understand, shrinkage estimators, like an L2 penalty, also claim to solve these types of problems. They trade an unbiased estimate (of the mean in this case) for a biased one that has much lower variance. But is the resulting estimate of the variance unbiased? I don't know. Nor do I know how one would show this. Do shrinkage estimators solve the Neyman-Scott paradox? If so, how?
