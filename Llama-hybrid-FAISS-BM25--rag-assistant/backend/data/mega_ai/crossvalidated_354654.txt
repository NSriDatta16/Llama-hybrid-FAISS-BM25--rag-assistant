[site]: crossvalidated
[post_id]: 354654
[parent_id]: 354257
[tags]: 
No, standard error of the mean is not the same as standard deviation of the observations. The "minimum plus 1 standard error rule" described in the Elements of Statistical Learning section 7.10.1 uses the standard error of MSE estimates across the folds. This is the standard deviation as you calculate it divided by $\sqrt{\text{FOLDS}}$. These rules may be seen as useful heuristic/rule-of-thumb, and depending on the setup of your selection you may want to adjust the threshold, e.g. by setting the limit to some multiple of the standard error. It is important to state clearly that you are using the standard error of the fold MSEs, as it would also make sense to formulate the decision threshold on the basis of the pooled predictions over all cross validation folds. However, they should be similar in size, so unproblematic (as long as the description clearly says what was done) as anyways we're just looking at a pragmatic rule-of-thumb. Some more thoughts about your code: Your folds are not independent as you scale once for the whole data set outside the cross validation loop. To obtain independent folds, the scaling offset and factor should be calculated in each fold for the respective training split and applied to training and test splits. The effect of this incorrect preprocessing is probably not large for your MWE, but I've seen a similar incorrect splitting for PCA pre-processing of highly multivariate data leading to errors being underestimated by an order of magnitude. if you look at fold-wise MSEs, by using sapply instead of lapply you immediately get a vector instead of a list, which later on saves you all those unlist s.
