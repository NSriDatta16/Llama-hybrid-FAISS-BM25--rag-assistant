[site]: crossvalidated
[post_id]: 437456
[parent_id]: 
[tags]: 
Bayesian Inference in the presence of multiple hypotheses

"Because [Bayesian Inference] respects the forward flow of time or information, there's no need for nor availability of methods for correcting for multiplicity ... The evidence of one question is not tilted by whether other questions are being asked." https://www.youtube.com/watch?v=8B-IEMJCtEw From Frank Harrel's course. I understand with frequentist inference, the objective is to conserve Type 1 error rate. When we control the family-wise error rate, we consider the family of "questions" being asked and consider a type 1 error for at least one question to be a type 1 error for the family of tests. It also makes sense, therefore, that if we present the 95% CIs for each of the estimands for which the original tests were constructed, we can communicate with a readership about the range of plausible effects that an experiment or data collection process has generated without correction. For a Bayesian, it's easy to see the analogue with estimation and the CI. And we can exonerate ourselves from multiple testing corrections. However, from an inferential standpoint, I don't believe any such claim can be made. But to begin: What is the Bayesian analogue of a Type 1 error? The Bayesian upon collecting compelling evidence might be said to "adopt" an updated/alternative probability model for the parameter, if we are to coerce an inferential/decision rule. The Bayesian might like to control the number of models that are spuriously adopted. In this case, the Bayesian one would need to attenuate the posterior by using an increasingly stringent set of priors depending on the number of tests being used. Is this the basic approach used and/or is there literature developing these ideas more fully ?
