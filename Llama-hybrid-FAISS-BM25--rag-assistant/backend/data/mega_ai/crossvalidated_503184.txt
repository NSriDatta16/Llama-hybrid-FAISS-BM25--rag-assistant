[site]: crossvalidated
[post_id]: 503184
[parent_id]: 
[tags]: 
Decision boundary in a Bayesian classification problem

We are given a classification problem within Bayesian decision theory, with 3 equi-probable classes $w_i,\,i=1,2,3$ and 2 features (or dimensions). The average of the three classes are known to be $\mu_1 = \pmatrix{0 \\ 0}$ , $\mu_2 = \pmatrix{6\\0}$ and $\mu_3 = \pmatrix{-4 \\ 0}$ . The covariance matrix is assumed to be the same for all three classes, given by $\Sigma = \pmatrix{2 & 0 \\0 & 2}$ . Using Bayesian classification, what would be the decision boundary? My attempt : In the Bayesian framework, given a feature vector $x = \pmatrix{x_1\\x_2}$ the decision (or discriminant) function for class $w_i$ is in general given by $$ d_i(x) = -0.5\, \text{ln} |\Sigma_i| - 0.5 (x - \mu_i)^{T} \Sigma^{-1}(x-\mu_i) + \text{ln}(P(w_i)), $$ where $P(w_i)$ is the probability of class $w_i$ . In the problem at hand, the discriminant function simplifies to $$d_i(x)= - 0.5 (x - \mu_i)^{T} \Sigma^{-1}(x-\mu_i). $$ According to my textbook, the decision surface (which I guess is just another term for decision boundary?) is the $x$ for which $d_i(x) = 0$ for all $i$ . Writing this our for $i=1$ gives $$(x_1,x_2)\pmatrix {0.5&0\\0&0.5} \pmatrix{x_1\\x_2} = \frac{x_1^2 + x_2^2}{2} = 0 \Rightarrow x_1 = x_2 = 0.$$ Similarly for $i=2$ you get $x_1 = 6 $ and $x_2 = 0$ and for $i=3$ , $x_1=-4$ and $x_2 = 0$ . I know in advance that the solution to the problem is $x_1=3$ and $x_1= -2$ but this doesn't appear to be deducible from our computations. So what's the trick?
