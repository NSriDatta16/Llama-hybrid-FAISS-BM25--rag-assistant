[site]: crossvalidated
[post_id]: 448614
[parent_id]: 448604
[tags]: 
For any given model, a larger sample facilitates more precise estimation of model parameters and hence more accurate forecasts. In a small sample, a flexible model may underperform compared to a simpler model because of poor estimation precision of the former, but in a larger sample the flexible model may beat the simpler model because of improved estimation precision. AIC takes this into account and will tend to select more complex models for larger samples, ceteris paribus. The model complexity will be balanced against the sample size as follows: $$ \text{AIC}=-2n\cdot\frac{\sum_{i=1}^{n}\ln(\text{likelihood}_i)}{n}+2k $$ where $n$ is the sample size, $\frac{\sum_{i=1}^{n}\ln(\text{likelihood}_i)}{n}$ is the average log likelihood (where the average is taken over the sample points) and $k$ is the number of parameters in the model. There is no need to adjust this for sample size, because the adjustment has already been done within the AIC. If the forecasts from the model are not as accurate as from another, perhaps simpler model with a higher AIC value, it might be because of a change in the data generating process (DGP) over time. What was characteristic to the DGP in 1990 may no longer be true in 2020. It is not a problem of AIC but one of selecting a sample that is relevant w.r.t. what you are trying to forecast.
