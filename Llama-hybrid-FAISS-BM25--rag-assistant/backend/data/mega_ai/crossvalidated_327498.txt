[site]: crossvalidated
[post_id]: 327498
[parent_id]: 327495
[tags]: 
No, variable importance in random forests is completely dissimilar to regression betas. There are actually different measures of variable importance. Most of them rely on assessing whether out-of-bag accuracy decreases if a predictor is randomly permuted. The idea is that if accuracy remains the same if you shuffle a predictor randomly, then that predictor can't be all that important. The different measures typically differ in how they assess accuracy (Gini or other impurity, MSE etc.). If you use R and the randomForest package, then ?importance yields (under "Details"): Here are the definitions of the variable importance measures. The first measure is computed from permuting OOB data: For each tree, the prediction error on the out-of-bag portion of the data is recorded (error rate for classification, MSE for regression). Then the same is done after permuting each predictor variable. The difference between the two are then averaged over all trees, and normalized by the standard deviation of the differences. If the standard deviation of the differences is equal to 0 for a variable, the division is not done (but the average is almost always equal to 0 in that case). The second measure is the total decrease in node impurities from splitting on the variable, averaged over all trees. For classification, the node impurity is measured by the Gini index. For regression, it is measured by residual sum of squares. (Plus, you shouldn't interpret regression betas as variable importance. If at all, look at standardized coefficients.)
