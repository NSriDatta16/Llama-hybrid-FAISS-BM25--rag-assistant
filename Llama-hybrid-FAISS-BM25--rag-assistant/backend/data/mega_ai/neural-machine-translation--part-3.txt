n 2022 and 2023. Usually, NMT models’ weights are initialized randomly and then learned by training on parallel datasets. However, since using large language models (LLMs) such as BERT pre-trained on large amounts of monolingual data as a starting point for learning other tasks has proven very successful in wider NLP, this paradigm is also becoming more prevalent in NMT. This is especially useful for low-resource languages, where large parallel datasets do not exist. An example of this is the mBART model, which first trains one transformer on a multilingual dataset to recover masked tokens in sentences, and then fine-tunes the resulting autoencoder on the translation task. Generative LLMs Instead of fine-tuning a pre-trained language model on the translation task, sufficiently large generative models can also be directly prompted to translate a sentence into the desired language. This approach was first comprehensively tested and evaluated for GPT 3.5 in 2023 by Hendy et al. They found that "GPT systems can produce highly fluent and competitive translation outputs even in the zero-shot setting especially for the high-resource language translations". The 2023 Conference on Machine Translation tested a variety of LLMs for their translation capabilities and found that machine translations were almost all on part with manual translation. Comparison with statistical machine translation NMT has overcome several challenges that were present in statistical machine translation (SMT): NMT's full reliance on continuous representation of tokens overcame sparsity issues caused by rare words or phrases. Models were able to generalize more effectively. The limited n-gram length used in SMT's n-gram language models caused a loss of context. NMT systems overcome this by not having a hard cut-off after a fixed number of tokens and by using attention to choosing which tokens to focus on when generating the next token. End-to-end training of a single model improved translation performance and also simplified the whole process. The huge n-gram models (up to 7-gram) used in SMT required large amounts of memory, whereas NMT requires less. Training procedure Cross-entropy loss NMT models are usually trained to maximize the likelihood of observing the training data. I.e., for a dataset of T {\displaystyle T} source sentences X = x ( 1 ) , . . . , x ( T ) {\displaystyle X=\mathbf {x} ^{(1)},...,\mathbf {x} ^{(T)}} and corresponding target sentences Y = y ( 1 ) , . . . , y ( T ) {\displaystyle Y=\mathbf {y} ^{(1)},...,\mathbf {y} ^{(T)}} , the goal is finding the model parameters θ ∗ {\displaystyle \theta ^{*}} that maximize the sum of the likelihood of each target sentence in the training data given the corresponding source sentence: θ ∗ = a r g m a x θ ∑ i T P θ ( y ( i ) | x ( i ) ) {\displaystyle \theta ^{*}={\underset {\theta }{\operatorname {arg\,max} }}\sum _{i}^{T}P_{\theta }(\mathbf {y} ^{(i)}|\mathbf {x} ^{(i)})} Expanding to token level yields: θ ∗ = a r g m a x θ ∑ i T ∏ j = 1 J ( i ) P ( y j ( i ) | y 1 , j − 1 ( i ) , x ( i ) ) {\displaystyle \theta ^{*}={\underset {\theta }{\operatorname {arg\,max} }}\sum _{i}^{T}\prod _{j=1}^{J^{(i)}}P(y_{j}^{(i)}|y_{1,j-1}^{(i)},\mathbf {x} ^{(i)})} Since we are only interested in the maximum, we can just as well search for the maximum of the logarithm instead (which has the advantage that it avoids floating point underflow that could happen with the product of low probabilities). θ ∗ = a r g m a x θ ∑ i T log ⁡ ∏ j = 1 J ( i ) P ( y j ( i ) | y 1 , j − 1 ( i ) , x ( i ) ) {\displaystyle \theta ^{*}={\underset {\theta }{\operatorname {arg\,max} }}\sum _{i}^{T}\log \prod _{j=1}^{J^{(i)}}P(y_{j}^{(i)}|y_{1,j-1}^{(i)},\mathbf {x} ^{(i)})} Using the fact that the logarithm of a product is the sum of the factors’ logarithms and flipping the sign yields the classic cross-entropy loss: θ ∗ = a r g m i n θ − ∑ i T log ⁡ ∑ j = 1 J ( i ) P ( y j ( i ) | y 1 , j − 1 ( i ) , x ( i ) ) {\displaystyle \theta ^{*}={\u