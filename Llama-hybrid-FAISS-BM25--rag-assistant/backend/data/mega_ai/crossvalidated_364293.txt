[site]: crossvalidated
[post_id]: 364293
[parent_id]: 
[tags]: 
Is stochastic gradient descent biased?

In the paper Mutual Information Neural Estimation , the authors derive the following gradient for the network $$ \nabla_\theta\mathcal V(\theta)=\mathbb E\left[\nabla_\theta T_\theta\right]-{\mathbb E\left[e^{T_\theta}\nabla_\theta T_\theta\right]\over \mathbb E\left[e^{T_\theta}\right]} $$ and say it's biased because, in the second term, the expectations are over the sample of a minibatch. However, based on my knowledge, SGD is high variance indeed but it shouldn't introduce any bias. Is my previous understanding wrong? Furthermore, the authors say that by replacing the expectation in the denominator with an exponential moving average, the bias can be reduced. Why does that make sense?
