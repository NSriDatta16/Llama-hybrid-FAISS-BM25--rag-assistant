[site]: crossvalidated
[post_id]: 455767
[parent_id]: 
[tags]: 
How can I Include extremely large outliers in analytics?

Like most of us stuck at home, I'm taking time to get back up to speed with machine learning with some pet projects and one of my projects includes trying to use machine learning to predict missing data. I'm using corporate financial data as an example with a goal to predict depreciation for companies who do not report it. My problem is, my results are really bad(order of magnitude off actuals), and I was talking to a friend who thought maybe because my data range was so large it maybe resulting in bad results. Here's the distribution of the data: df['depreciation'].describe() count 1.687800e+04 mean 7.816402e+09 std 2.124805e+11 min -3.896300e+10 25% 4.308675e+07 50% 1.422000e+08 75% 4.810000e+08 max 1.632600e+13 Name: depreciation, dtype: float64 I think the problem is most companies who's data I have aren't that huge but firms like amazon and apple's huge numbers create large outliers. I am hesitant to delete them because large firms and small firms may add value to the prediction. I read a few articles on this and most say either normalize(which I did use MinMaxScaler) or drop outliers which I'm hesitant to do. My questions What are my options for working with these large ranges of data? I'm totally new to this so if I'm missing any information please let me know and I'll get it asap. Thank you!
