[site]: crossvalidated
[post_id]: 139211
[parent_id]: 139198
[tags]: 
When doing power analysis, for me it is always helpful to think about "minimally-detectable effect." So when you see in GPower that 20-30 participants will detect an effect size of .5, that is the minimum effect size it will detect. Effect sizes over that will be picked up. So you should ask yourself: "What is the smallest effect size I am comfortable with confidently finding?" There is no magic formula to tell you what a minimum effect size you need to test your hypothesis. That is dependent entirely on your context. Some things to consider: What is your hypothesis about how big the effect is? How big does the effect need to be for it to be meaningful in your field? A small effect size could be very meaningful in some types of research (like education interventions often find small effects, but are supported nonetheless), but only large ones could be worth the time of day in others. What is typical practice in your field? Dig around for other research articles to see what other researchers report. Your field may actually not tell you anything about effect size, in which case you may have to calculate it yourself based on reported results. How standard are your measures? Do other researchers use similar measures? If so, what effect sizes are they finding? What are the stakes of your conclusion? If it is high stakes, big money, you may want to make sure you detect even small effect sizes. There are other things you want to consider as well, these are just the ones that popped into my head. Also, I'm assuming when you say "factor" you mean your independent variables are binary. If they are continuous, the effect size is interpreted differently - it is the continuous, one-unit change, not the average difference between two groups. So you would expect a continuous effect to be smaller than the effect between two groups, therefore you may need to make sure you can detect a small effect in that case.
