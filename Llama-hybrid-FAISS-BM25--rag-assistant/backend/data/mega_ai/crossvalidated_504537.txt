[site]: crossvalidated
[post_id]: 504537
[parent_id]: 504519
[tags]: 
Like Three Diag in his very good answer let's consider this a binomial problem with p being the probability of each single member of the list being dead and having cast a vote. The Bayesian question would be how to estimate our best guess of that probability and for binomial problems that is expecially easy because we can use a conjugated, in this case a beta distribution. I have no idea how this list came together. It may be taken out of thin air or be a very precise report. So we have a rare situation where a flat prior seems in order. For convenience / conjugation let's call it a $Beta(1, 1)$ prior: Now with the magic of conjugation we can update that with n = 50 to a $Beta(1,51)$ assumption: This is usefull because we can answer all kinds of questions with that. You asked How would I go about calculating the likelihood that the list was wrong? Now, what is a wrong list? I tend to say: If 20% of more of that list are true voter fraud, I'd consider that a scandal worth noting here one the other side of the atlantic ocean. But you could set any other limit. The probability of the probability being smaller then 20% is the area from 0 to 20% which is conveniently implemented in R as pbeta(x, 1, 51) . > pbeta(.2, 1, 51) [1] 0.9999886 Given a flat prior, the chance of less then 20% true-fraud values is 99.999%. Now of course that is somewhat dependend on the prior. If some Trumpists started out with a 10 to 1 prior of $Beta(1,10)$ as in They would come up with a largely different posterior probability distribution: But even if you started with a 10-to-1 Beta-Prior the probability of p
