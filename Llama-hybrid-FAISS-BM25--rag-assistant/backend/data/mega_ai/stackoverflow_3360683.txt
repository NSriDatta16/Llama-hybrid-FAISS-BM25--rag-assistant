[site]: stackoverflow
[post_id]: 3360683
[parent_id]: 3360549
[tags]: 
You're talking about Normalisation . As with so many design aspects it's a trade-off. Having duplication within the database leads to many problems - for example how to keep those duplicates in step when updating data. So Inserts and Updates may well go more slowly because of the duplication. Hence we tend to normalise the database to avoid such duplication. That does lead to more complex queries and possibly some retrieval overhead. Modern database products tend to do such queries really well if you take a bit of care to have the right indexes in place. Hence my starting position would be to normalise your data, avoid duplication. Then in a special case perhaps denormalise just pieces where it really becomes essential. For example suppose some part of you database is large, mostly queried rather than updated (eg. historic order information) then perhaps denormalise that bit.
