[site]: crossvalidated
[post_id]: 324646
[parent_id]: 322575
[tags]: 
Say, you have a sample, $(Z^{(m)})_{m=1, \dots, M}$, from $f_{\theta}$. To get a sample from $f_{\theta+\varepsilon}$ you can run an MCMC starting in $Z^{(M)}$. If $\varepsilon$ is small there willl hardly be any burn-in period, because you start the MCMC chain very close to the stationary distribution. However you still need to gather enough samples to get a decent stationary sample. To avoid this sampling, I suggest: Your idea of updating the parameter after each MCMC sample sounds like adaptive MCMC . One normally uses adaptive MCMC to tune a step size in the proposal. From my limited knowledge about the theory, I don't see why it should not be possible to extend to your situation. You can use importance sampling on your sampled $Z^{(m)}$'s; So you simulate a driver set $ Z^{(m)}\sim f_{\theta_0}$. To get a sample from $f_{\theta}$, you can use the weighted sample $(Z^{(m)}, \frac{f_{\theta}(Z^{(m)})}{f_{\theta_0}(Z^{(m)})})$. This method could fail when $\theta$ and $\theta_0$ are too far apart, so remember to keep an eye on an estimate of the effective sample size.
