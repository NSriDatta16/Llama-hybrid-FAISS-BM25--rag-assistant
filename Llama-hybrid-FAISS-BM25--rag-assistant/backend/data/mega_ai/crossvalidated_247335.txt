[site]: crossvalidated
[post_id]: 247335
[parent_id]: 
[tags]: 
Prove that the moment condition fails for AR(1) process with autocorrelated error

Consider the simple time series model: $y_t=\rho y_{t-1} + \varepsilon_t$ where $\varepsilon_t$ is autocorrelated. This results in that the moment condition, $E[y_{t-1}\varepsilon_t]=0$, does not hold, i.e. that $E[y_{t-1}\varepsilon_t]\neq0$. How would you show that the moment condition does not hold?
