[site]: crossvalidated
[post_id]: 638792
[parent_id]: 
[tags]: 
What's the loss that is optimized in InstructGPT RL stage?

In the InstructGPT paper they define objective of RL stage as: They try to maximize this objective using PPO . I have trouble understanding how they plug this objective into the PPO though. Do they just replace each $r_t$ in the computation of PPO advantages $\hat{A}_t$ by the value of this objective? (which would also mean that they use $L^\text{CLIP}$ instead of $L^\text{KLPEN}$ in the final loss, otherwise KL term would be kind of duplicated, won't it?)
