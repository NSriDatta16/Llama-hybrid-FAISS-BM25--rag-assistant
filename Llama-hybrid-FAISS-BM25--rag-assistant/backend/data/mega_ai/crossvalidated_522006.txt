[site]: crossvalidated
[post_id]: 522006
[parent_id]: 
[tags]: 
SVM convex optimisation - What is the role of duality

Consider the support vector classifier problem with slack variables $\xi_i$ . In a reference book (Elements of Statistical Learning, Hastie et al) I am using, the authors first introduced the convex optimisation problem, which is then converted into Lagrangian primal form. Then the primal form is taken derivative wrt $\beta, \beta_0, \xi_i$ and each set to zero, yielding 3 results, in particular $\hat{\beta}$ has the form $\sum_i \hat{\alpha}_i y_i x_i$ . Further on the 3 results are substituted into the primal form. Rearranging it becomes the dual function. In the dual function only the Lagrangian multipliers $\alpha_i$ (introduced in the primal form) are the parameters to solve for. While I understand the solution is both primal and dual optimal if the KKT conditions are satisfied, I do not understand the purpose of introducing and solving the dual function, when the form of $\hat{\beta}$ is easily computed from the primal form. Could it be that it is more efficient to solve the dual problem for $\alpha_i$ in software? In short I would like to know the motivation for finding the hyperplane solution that is both primal and dual optimal, thereby introducing both the primal and dual forms in the book. This is the standard SVC problem solved by duality, so forgive me for not providing the mathematical details.
