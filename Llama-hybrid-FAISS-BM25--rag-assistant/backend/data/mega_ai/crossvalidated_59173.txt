[site]: crossvalidated
[post_id]: 59173
[parent_id]: 59152
[tags]: 
Effect sizes Common standardised effect sizes typically quantify the amount or degree of a relationship or effect. The most common effect size measures are probably cohen's d, Pearson's r, and the odds ratio (particularly for a binary predictor). Less common effect size measures: That said, you can have standardised and unstandardised effect size measures. Any statistic that communicates the degree of relationships and is not especially contaminated by sample size is probably an effect size measure. Thus, Beta coefficients, R-square, covariance, raw mean differences between groups, and so on all capture the degree of effect. That said, I find that some researchers apply effect size measures somewhat blindly and forget that the broader aim is to give readers a sense of the degree of effect. And thus, they often don't realise that measures like mean differences or raw regression coefficients are in some sense an effect size measure. Another example of blind use of effect sizes involves the use of effect size measures that do not have an intuitive interpretation, but have been recommended by some textbook. Not effect sizes: Most test statistics are not effect sizes. E.g., Chi-square test, t-test, z-test, F-test. They get larger both as the size of the population effect increases and as sample size increases. In many respects the whole language of effect sizes has been emphasised in recent years because researchers were focussing too much on how large their test statistics were rather than how large their effect sizes were. This is especially important where you have a large sample size when even small effects can be statistically significant. Most univariate statistics are not effect sizes. For most purposes, effect size is concerned about the relationship between at least two variables. Thus, the sample mean, standard deviation, skew, kurtosis, min, max, and so on are not effect size measures. Statistics not pertaining to degree of relationship are not effect size measures. For example, tests of multivariate normality, the eigenvalues of a matrix, and so on generally are not directly aimed at quantifying an effect in the ordinary sense of the word. Broader considerations Scaling considerations: The utility of a statistic as an effect size measure largely relates to its ability to communicate the size of an effect. Sometimes this is achieved by using familiar standardised measures of effect (e.g., cohen's d). Other times, careful consideration of scaling of the variables can yield an even clearer interpretation of the size of the effect. For example, say I had a study looking at a training program on income levels. I could report that the training program increased income by .2 standard deviations or I could say that the program increased income by $3,500 US dollars. Both are useful; both are effect size measures. The first is standardised (cohen's d), the second is unstandardised (raw group mean differences). Precision in estimating effect sizes: We often extract sample estimates of effect size measures (e.g., cohen's d, pearson's r, etc.). This context can lead to a contrasting of significance testing with effect size measures. Nonetheless, the aim should still be to estimate in a precise and unbiased way, the population effect size. From a frequentist perspective, confidence intervals around effect sizes provide an estimate of precision. From a Bayesian perspective, there are posterior densities on effect sizes. In many cases, care needs to be taken to ensure that you are using an unbiased effect size measure.
