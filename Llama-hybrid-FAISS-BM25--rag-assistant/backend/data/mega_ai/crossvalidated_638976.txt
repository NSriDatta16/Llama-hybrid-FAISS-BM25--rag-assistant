[site]: crossvalidated
[post_id]: 638976
[parent_id]: 
[tags]: 
BERT eval loss increase while performance metrics also increase

I want to fine-tune BERT for Named Entity Recognition (NER). However, when fine-tuning over several epochs on different datasets I get a weird behaviour where the training loss decreases, eval loss increases and performance metrics such as F1, accuracy, precision and recall increase. I have tried scripts from different tutorials, but all display this behaviour. Some sources I have read suggest fine-tuning is a small epochs process, while others report fine-tuning for more than 20 epochs for BERT and NER. Is this behaviour normal for BERT and NER? What are the best practices for avoiding such behaviour. Note: If you want to reproduce my results, you can use this Google Colab code from this Medium post . I changed num_train_epochs to 30 . learning_rate is 5e-5 , although I have tried other similar rates and schedulers but still get similar results. Thanks!
