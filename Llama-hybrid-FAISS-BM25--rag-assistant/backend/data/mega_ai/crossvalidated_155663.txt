[site]: crossvalidated
[post_id]: 155663
[parent_id]: 119922
[tags]: 
So many marketers assume that the goal of "single source" status at the household or individual level with respect to data is the only option. This can be a quite limiting, even procrustean mindset as it sets the bar unrealistically high given that 85% of your data is like swiss cheese wrt missing values. "Addressable" predictions can also be made at a higher levels of aggregation. It's a compromise, but it's a realistic one. Here's one way to accomplish that: treat your data like a fractional factorial experimental design -- I don't mean this literally. FFD's are highly efficient and frugal approaches to obtaining experimentally derived estimates across a wide range of required information when faced with limited time and attention on the part of respondents. They achieve this by setting up a matrix structure where certain participants are exposed to some questions, but not others. It's only on the back-end, once the individual results are available for analysis, that the integration across respondents and questions is possible. Very simply, this involves creating a potentially large covariance matrix of the information relevant to whatever the analysis is that you want to do, bearing in mind that covariance matrices underlie most multivariate techniques anyway (i.e., this isn't a big technical leap). The trick is to be sure to turn off the "listwise deletion" options that are the default for many stats packages and routines. The analysis and model would unfold from that covariance matrix. Since your data is longitudinal, we're also talking about creating transition matrices over time. Given that, my preference would be to use hidden markov model approaches. Steve Scott (Google) and Oded Netzer (Columbia) have written the most coherent papers on this class of models. In addition, there's a Chapman-Hall book out Hidden Markov and Other Models for Discrete- valued Time Series . Finally, the Latent Gold software has modules that will implement a markov chain for you. Given this, imputing isn't required. Moreover, the complicated, messy and uncertain process of projecting from the 15% of complete information to the missing data isn't necessary. You only sacrifice a relatively small amount of specificity leveraging this covariance analysis.
