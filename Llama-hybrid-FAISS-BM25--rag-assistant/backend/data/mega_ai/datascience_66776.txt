[site]: datascience
[post_id]: 66776
[parent_id]: 57696
[tags]: 
Uncertainty is fairly easy, if you have a probabilistic output. Just apply the model to unlabeled data sets and pick the one with highest average uncertainty. In the binary classification case, that's just lowest mean(abs(p - 0.5)) . modAL ( https://github.com/modAL-python/modAL ) has some utilities that could be useful in the multi-class case where there are several possible definitions of 'uncertainty'. See https://modal-python.readthedocs.io/en/latest/content/query_strategies/uncertainty_sampling.html for example. For diversity, I suppose you can measure the average distance or similarity between your train set and each data set. This assumes you have a meaningful distance or similarity metric. Up to you how to normalize them to make them comparable though. Some models may already capture that points not near the training set are inherently less certain, in which case this might be redundant. Some may not (like max margin classifiers maybe), but if you suspect new datasets from different part of the input space behave differently to the training set, maybe those models aren't appropriate anyway.
