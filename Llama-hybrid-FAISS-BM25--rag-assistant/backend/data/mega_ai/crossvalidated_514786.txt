[site]: crossvalidated
[post_id]: 514786
[parent_id]: 514778
[tags]: 
I would start by considering that logistic regression is a method, a model in fact, whereas clustering is a family of methods so you are not really comparing like with like. In any case, logistic regression can be described as supervised . You start with a dataset where you know whether each observation is "0" or "1" and you have a number of predictor variables. You build a model that allows you to estimate the contribution of each predictor to the classification and/or predict the class of future observations given new values of the predictors. Clustering, typically, is unsupervised . You have observations for which a number of predictor variables are known but you don't know the assignment of observations to classes. In fact, you may not even known if and how many classes you have. So you can use clustering to learn about if/how many classes are there. A dataset for clustering may look like this. You have two variables for 20 observations: X1 X2 1 12.6 15.2 2 15.0 16.2 3 14.5 18.7 4 10.7 15.6 5 7.5 15.7 6 2.6 15.0 7 13.5 14.4 8 2.8 17.7 9 15.2 17.8 10 7.1 11.9 11 22.0 21.4 12 21.6 9.8 13 17.1 7.0 14 10.7 10.0 15 20.7 17.9 16 26.2 4.9 17 23.2 16.1 18 18.8 15.9 19 21.4 11.5 20 20.8 10.0 You apply, for example, kmeans clustering and you find that there seem to be two groups: For logistic regression, the dataset may be like this (the same but with classes given): class X1 X2 1 0 12.6 15.2 2 0 15.0 16.2 3 0 14.5 18.7 4 0 10.7 15.6 5 0 7.5 15.7 6 0 2.6 15.0 7 0 13.5 14.4 8 0 2.8 17.7 9 0 15.2 17.8 10 1 7.1 11.9 11 1 22.0 21.4 12 1 21.6 9.8 13 1 17.1 7.0 14 1 10.7 10.0 15 1 20.7 17.9 16 1 26.2 4.9 17 1 23.2 16.1 18 1 18.8 15.9 19 1 21.4 11.5 20 1 20.8 10.0 You can use this dataset to estimate the contribution of X1 and X2 in determining the class and in turn use the estimates to predict new observations: fit |z|) (Intercept) 4.317 4.896 0.88 0.378 X1 0.655 0.358 1.83 0.067 . X2 -0.886 0.531 -1.67 0.095 . ... Predict new observations: # More likely a "1" predict(fit, data.frame(X1= 20, X2= 15), type= 'response') 1 0.984 # More likely a "0" predict(fit, data.frame(X1= 15, X2= 17), type= 'response') 1 0.286 More manually, you can use the estimates from the fitted model ( $\beta$ ) to make predictions on new values $\textbf{X}$ considering that the fitted values are probabilities transformed by the logit function [ $logit(p) = log(\frac{p}{1-p})$ ]. So we need to invert the logit function to obtain a probability [ $logit^{-1}(x) = 1/(1+e^{-x})$ ]. So the probability of being 1 is: $$ P(1) = logit^{-1}(\beta X) $$ In R: invlogit Here's the R code to reproduce: set.seed(1234) class $X1, dat$ X2, col= dat$class+1, pch= 19) km $cluster) plot(km$ X1, km $X2, col= km$ cluster, pch= 19) fit
