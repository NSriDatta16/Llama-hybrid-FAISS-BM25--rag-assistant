[site]: crossvalidated
[post_id]: 453288
[parent_id]: 397222
[tags]: 
Yes, they are different. Local FDR in Efron's literature is easier to understand, and it's a Bayesian idea. We suppose the parameter $\theta$ has a prior distribution, say \begin{equation*} \theta \sim \begin{cases} 0 & \text{with probability } \pi_0 \\ f_1(\theta) & \text{with probability } 1-\pi_0 \end{cases} \end{equation*} We also suppose that we observe a statistic $z \sim g(\theta)$ . The local FDR is basically the posterior probability $p(\theta = 0 | z)$ . There's also a related concept in Efron's literature, the Bayes FDR , which is basically $p(\theta=0|z \in \mathcal{Z})$ where $\mathcal{Z}$ is a rejection region (e.g. when the p-value given $z$ is less than 0.05). Bejamini-Hochberg is more difficult to understand, cos it's a frequentist concept. Suppose you run a large number ( $n$ ) of tests, to test whether $\theta_1=0, \theta_2=0, \ldots, \theta_n=0$ . Then, imagine you repeat this set of tests many (infinitely many) times. With each repeat, although $\boldsymbol{\theta}=(\theta_1, \theta_2, \ldots, \theta_n)$ stays the same, $\boldsymbol{z}=(z_1, z_2, \ldots, z_n)$ are generated anew from $z_i \sim g(\theta_i)$ . The "false discovery rate" for each repeat $k$ corresponds to $S(k) = Pr(z_{ik} \in \mathcal{Z}, \theta_i = 0 | z_{ik} \in \mathcal{Z}) \equiv \dfrac{\#(z_{ik} \in \mathcal{Z}, \theta_i = 0)}{\#(z_{ik} \in \mathcal{Z})}$ , with $Pr(z_{ik} \in \mathcal{Z}, \theta_i = 0 | z_{ik} \in \mathcal{Z}) = 0$ if $\#(z_{ik} \in \mathcal{Z}) = 0$ , and $\#$ denotes "counts of". Note that $S(k)$ is a random variable that varies with $k$ . In Benjamini Hochberg's definition, $FDR=\mathbb{E}(S(k))$ . They are saying if you follow their procedure, given the assumptions (e.g. independence of $z_i$ ), $FDR \leq \alpha$ .
