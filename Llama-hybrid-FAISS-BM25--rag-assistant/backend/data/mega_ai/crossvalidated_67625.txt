[site]: crossvalidated
[post_id]: 67625
[parent_id]: 
[tags]: 
How to generalize Particle Filters (w.r.t. multiple states)

I'm using particle filters for inference in a hidden markov model with an infinite state-space. My current state-variable is multidimensional and there are interdependencies between some dimensions. I thought therefore, that it would be beneficial, if I split it up into sub-states and model ther dependences in a small bayesian network. Thus, I would only maintain particles for each of the smaller-dimensional sub-states and hopefully get better results with fewer particles. I thought, that representing N one-dimensional distributions needs fewer samples than representing the N-dimensional joint-distribution, represented by the Bayesian Network. Does this make sense? This leads to a Dynamic Bayesian Network, but I don't know how sampling, likelihood-weighting and resampling should be adapted for such a particle filter with multiple states. Update: Looks like Nonparametric Belief Propagation is an algorithm (class) that extends particle filtering techniques to arbitrarily structured graphical models.
