[site]: stackoverflow
[post_id]: 1018845
[parent_id]: 1018749
[tags]: 
I can only guess, how the time is calculated. But many hours spent watching the copying window and seeing how the time estimate changes, here is my best estimate: Windows is keeping a list of all the files to be copied It is keeping track of the time and number of files already copied The remaining time is calculated as : Average time per file so far = time passed so far / files already copied Estimated time needed for all files = avg time per file * number of files. The calculation is repeated after a fixed time span has passed (maybe 5 seconds, maybe 30?) It probably is a little more complex that I explained above, I suppose that the size of the file currently being copied, and the percentage that has been copied, goes into the calculation as well. That would explain why we see an estimate when only one file is being copied ;-). So, in a direct answer to your question: It is neither the best nor the worst time, it is just a very weak estimate that is the less exact the more the file sizes differ from each other. Or in other words: It was probably the fastest way (in terms of fast programming, as well as low cpu usage when running) that a programmer could think of that implemented a specified feature. I wouldn't be surprised if it was coded on a Friday afternoon...
