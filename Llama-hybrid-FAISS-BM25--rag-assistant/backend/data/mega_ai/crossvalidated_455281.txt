[site]: crossvalidated
[post_id]: 455281
[parent_id]: 455279
[tags]: 
The statement that $E$ changes for each input observation is correct only for Stochastic Gradient Descent, where you are making updates for each sample individually (i.e. with a batch size of $1$ ). If you are performing SGD with any batch size, then you could theoretically average the gradients of the batches. If you want to be $100\%$ correct and the batch size doesn't divide the training set perfectly, then you could perform a weighted average and give the last batch's cost a contribution relative to its size. If you are performing Gradient Descent (i.e. you can fit the whole training set in memory), then $E$ is the cost for the whole dataset.
