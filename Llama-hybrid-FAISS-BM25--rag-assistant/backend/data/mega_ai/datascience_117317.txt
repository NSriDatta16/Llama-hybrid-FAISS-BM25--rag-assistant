[site]: datascience
[post_id]: 117317
[parent_id]: 
[tags]: 
Why VAE Encoder outputs log variance and not standard deviation?

When talking about VAE (and viewing VAE implementations), the Encoder outputs: Î¼, log(variance) when we train the model (the part of the decoder model) , we convert the log(variance) to Standard deviation: std = exp(0.5 * logvar) (I took the example from here: https://github.com/AntixK/PyTorch-VAE/blob/master/models/vanilla_vae.py ) If we need to convert the log(variance) to Standard deviation, why won't we output the Standard deviation from the encoder instead of making calculation to convert it to Standard deviation ?
