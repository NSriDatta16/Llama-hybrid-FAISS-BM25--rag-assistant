[site]: crossvalidated
[post_id]: 48934
[parent_id]: 48918
[tags]: 
You should use PCA for feature selection before applying a classifier. This usage of PCA in classification has grown common since a 1991 paper on classifying faces using "eigenfaces". The procedure is basically: Select k orthogonal dimensions of maximum variation (given by the eigenvectors of the covariance matrix) for all the data together (after normalization). Project all data points (training and test) onto these k dimensions to get k-dimensional feature vectors. Learn and classify the k-dimensional feature vectors using your favorite classification method. The role of PCA in this scheme is to reduce the complexity of the data handled by the classification stage, and thus perhaps afford stronger classifiers. One failure case for this scheme is when there is large intra-class variation in orthogonal directions to the inter-class variation, and thus the inter-class directions are lost. In practice though, the scheme is very commonly used and found to be successful for a variety of problems.
