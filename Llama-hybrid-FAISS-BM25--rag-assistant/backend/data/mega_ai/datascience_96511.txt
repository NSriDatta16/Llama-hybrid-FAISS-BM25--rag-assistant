[site]: datascience
[post_id]: 96511
[parent_id]: 
[tags]: 
Spatial positional encodings Vs Learned positional encodings(Object queries)

I have been trying to understand facebook's Detection transformer (DeTr) paper. Architecture Most of the explanation about the architecture is straightforward. I don't especially understand the concept of object queries . Details In the paper, A transformer decoder then takes as input a small fixed number of learned positional embeddings, which we call object queries, and additionally attends to the encoder output. There are 2 kinds of positional encodings in our model: spatial positional encodings(fixed) and output positional encodings(Object queries). FYI : Learned positional encodings and output positional encodings are same . Questions What are these "learned" positional encodings(object queries)? how are they different from spatial positional encodings? When are they exactly learned ?
