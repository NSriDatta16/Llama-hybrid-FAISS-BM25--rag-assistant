[site]: crossvalidated
[post_id]: 522339
[parent_id]: 
[tags]: 
convex optimisation formulation of SVM

I am currently learning about Support Vector Machine's (SVM) from the CS229 Stanford Class . In page 16 of the notes, they transformed $$max_{\hat{\gamma}, w, b} \frac{\hat{\gamma}}{||w||} \\ s.t \,\,\,y^{(i)}(w^Tx^{(i)}+b) \geq \hat{\gamma}, \,\, i = 1,....n $$ into $$ min_{w,b} \frac{1}{2}||w||^2 \\ s.t \,\,\,y^{(i)}(w^Tx^{(i)}+b) \geq 1, \,\, i = 1,....n $$ I have a few questions relating to this transformation which I don't really understand from the notes. Why can they set $\hat{\gamma} = 1$ ? This was the crux of why they could perform the transformation They mentioned that the objective function for the first equation is non - convex. I am not familiar with convex optimisation. Is it because of the variable $\hat{\gamma}$ that makes it non-convex. But why is that so ? Regarding the first question, the notes explained it in this manner Let’s keep going. Recall our earlier discussion that we can add an arbitrary scaling constraint on w and b without changing anything. This is the key idea we’ll use now. We will introduce the scaling constraint that the functional margin of w, b with respect to the training set must be 1:
