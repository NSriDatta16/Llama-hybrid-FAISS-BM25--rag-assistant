[site]: crossvalidated
[post_id]: 424173
[parent_id]: 424170
[tags]: 
Cost and loss are often used interchangeably. However, sometimes people deliberately use the term loss to refer to a single training example, and cost is used in the context of the whole training set. For instance, for the $i$ th training example, you may have a loss function $\mathcal{L}(x_i, y_i)$ , and you may write the cost as $\frac{1}{n} \sum_{i}^{n}\mathcal{L}(x_i, y_i)$ . Cost and error can be the same, depending what your loss function is. For example, say you have a logistic regression classifier, the classification cost and classification error are two different things. Here, you would define the cost via the negative log-likelihood loss, and the error is based on the 0/1 loss. The 0/1 loss is intuitive but not differentiable. In other cases, like OLS regression, you minimize the mean-squared error and can also report it as the training error. So in that case they are the same The "risk" is usually the expected loss. However, since that's hard to compute in most cases, people use the term "empirical risk" which is basically again just averaging over the losses (so, basically similar as cost)
