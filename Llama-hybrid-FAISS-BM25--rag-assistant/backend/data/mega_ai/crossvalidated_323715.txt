[site]: crossvalidated
[post_id]: 323715
[parent_id]: 280533
[tags]: 
However, the two models have the same coefficients. Is that correct? I thought that k-fold cross validation would yield a model where the coefficients were averages of the k models developed. Yes, this is correct. You mixed up cross-validation and bootstrapping. The one which utilizes averages of coefficients is bootstrapping (see John Fox short and nice intro ). Cross-validation is when you split your data into n folds, calibrate a model using n - 1 folds, predict for the remaining n fold and then compare how your model predicts on out-of-sample data. If the same model is generated with and without cross validation, what is the advantage of developing a model with the train function rather than using glm directly? train function (as a caret package in general) gives you the same interface for all statistical (GLM, GAM, lasso, etc) and machine learning models (random forests, gradient boosting) etc. That is you do not need to learn how specific packages work. Furthermore, this function allows for running cross-validation automatically without writing a code manually.
