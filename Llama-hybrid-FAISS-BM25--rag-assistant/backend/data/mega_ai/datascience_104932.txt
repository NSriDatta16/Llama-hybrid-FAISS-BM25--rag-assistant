[site]: datascience
[post_id]: 104932
[parent_id]: 104930
[tags]: 
I can give you an answer regarding two of the three additional question. The reason that the activation for the last convolutional tranpose layer is set to tanh is most likely because the input data is also preprocessed to be in the (-1, 1) range, so it's making sure that both the real images and the generated images have a similar distribution. If the activation for the last layer would be different (e.g. a sigmoid activation), the ranges of the two distributions would differ quite a lot making it easy for the discriminator to determine if an image is real or fake. Regarding the second question of setting the trainable attribute to False for the discriminator in the gan_model function, this seems to be related to allowing the discriminator to train when discriminator.train_on_batch is called but not when gan.train_on_batch for the combined model (see also this answer on a github issue )
