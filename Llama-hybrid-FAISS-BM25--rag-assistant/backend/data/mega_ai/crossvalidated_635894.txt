[site]: crossvalidated
[post_id]: 635894
[parent_id]: 364917
[tags]: 
Your target value is only binary, i.e., takes on two values: 50 and 100. If you rescaled these values, you could recode 50 to 0 and 100 to 1. Then use the logistic activation function on the output-side which will yield predicted values in the range [0,1]. Firstly, however: -You said nothing about the scale (range, min, max) of your input values. The basic "vanilla-flavored" feed-forward backpropagation neural network likes all input values for all features to be in the range [-1,1], [0,1] based on normalization, or [~ -5, ~5] based on mean-zero standardization. -You also said nothing about how you are randomizing the connection weights before training. A simple rule of thumb is to use random values in the range [-0.5,0.5]. -You stated nothing about whether you're using cross-validation, leave-one-out, etc. -When using a simple neural network for function approximation (not classification), sometimes the solution will break-down (not converge) if you don't have uniformly-spaced input features values over the range of their values. For example, if the range of a continuous feature is [10,400] and you feed the net values mostly like 20, 30, 230, 380, the network won't learn the data. This is why Latin hypercube sampling (LHS) is commonly invoked when using a neural network for function approximation. Your problem is really a classification problem, since y=f(x) takes on discrete values (50,100) and not values which are continuously scaled.
