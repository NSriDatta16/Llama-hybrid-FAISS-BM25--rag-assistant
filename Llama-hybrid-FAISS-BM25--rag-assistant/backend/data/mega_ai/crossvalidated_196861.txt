[site]: crossvalidated
[post_id]: 196861
[parent_id]: 194448
[tags]: 
In the use of Bayes' Theorem to calculate the posterior probabilities that constitute inference about model parameters, the weak likelihood principle is automatically adhered to: $$\mathrm{posterior} \propto \mathrm{prior} \times \mathrm{likelihood}$$ Nevertheless, in some objective Bayesian approaches the sampling scheme determines the choice of prior, the motivation being that an uninformative prior should maximize the divergence between the prior and posterior distributions—letting the data have as much influence as possible. Thus they violate the strong likelihood principle. Jeffreys priors, for instance, are proportional to the square root of the determinant of the Fisher information, an expectation over the sample space. Consider inference about the probability parameter $\pi$ of Bernoulli trials under binomial & negative binomial sampling. The Jeffreys priors are $$ \def\Pr{\mathop{\rm Pr}\nolimits} \begin{align} \Pr_\mathrm{NB}(\pi) &\propto \pi^{-1} (1-\pi)^{-\tfrac{1}{2}}\\ \Pr_\mathrm{Bin}(\pi) &\propto \pi^{-\tfrac{1}{2}} (1-\pi)^{-\tfrac{1}{2}} \end{align} $$ & conditioning on $x$ successes from $n$ trials leads to the posterior distributions $$ \begin{align} \Pr_\mathrm{NB}(\pi \mid x,n) \sim \mathrm{Beta}(x, n-x+\tfrac{1}{2})\\ \Pr_\mathrm{Bin}(\pi \mid x,n)\sim \mathrm{Beta}(x+\tfrac{1}{2}, n-x+\tfrac{1}{2}) \end{align} $$ So observing say 1 success from 10 trials would lead to quite different posterior distributions under the two sampling schemes: Though following such rules for deriving uninformative priors can sometimes leave you with improper priors, that in itself isn't the root of the violation of the likelhood principle entailed by the practice. An approximation to the Jeffreys prior, $ \pi^{-1+c} (1-\pi)^{-1/2}$, where $0 You might also consider model checking—or doing anything as a result of your checks—as contrary to the weak likelihood principle; a flagrant case of using the ancillary part of the data.
