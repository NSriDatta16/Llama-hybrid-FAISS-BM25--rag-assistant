[site]: datascience
[post_id]: 17348
[parent_id]: 13120
[tags]: 
Let me suggest three simple options: average the vectors (component-wise), i.e., compute the word embedding vector for each word in the text, and average them. (as suggested by others). take the (component-wise) maximum of the vectors. (max, instead of average) take the (component-wise) minimum of the vectors. (min, instead of average) Each of these yields a feature vector that is independent of the length of the text. There is some research suggesting that concatenating the max and the min yields a pretty effective feature space: it's not the absolute optimal, but it's close to optimal, and is simple and easy to implement. See this question on Statistics.SE for details. Here is an alternative idea, inspired by cubone's answer , that as far as I know hasn't been tested before. The idea is to tag the text using a part-of-speech tagger and then use those tags to inform the featurization process. In particular, write down a list of all possible POS tags that could be emitted by the POS tagger. Suppose there are 20 possible tags (CC, DT, JJS, MD, NNP, ...). Then the feature vector will be 20*300 = 6000 elements long: it will have one 300-vector per POS tag, concatenated in some canonical order. The 300-vector for each tag could be computed by averaging the word embedding vectors of all words that are tagged by the POS-tagger with that tag. Or, you could get one 600-vector per POS tag, obtained by computing the min and max over all vectors of words with that tag. This might yield a richer feature space, I don't know if it would yield any improvement, but it's something you could try if you wanted to experiment with different ideas.
