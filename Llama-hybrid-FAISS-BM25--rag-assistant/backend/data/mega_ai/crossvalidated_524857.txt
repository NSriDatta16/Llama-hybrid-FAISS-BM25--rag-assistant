[site]: crossvalidated
[post_id]: 524857
[parent_id]: 524389
[tags]: 
Based on the comment clarifying that measurement error is known, you may consider the implementation for errors-in-variables modeling in the brms package. It seems like you're using MCMC anyway, so HMC from Stan is usually just more efficient. Assuming a generic linear regression in R, you would specify something like this: fit Just to briefly overview that code, the real work is the me() function. This is just short for "measurement error" and has the two arguments out and error . In this case, out is the outcome variable (aka, our variable with measurement error), and error is the error that we know for that variable (on a standard deviation scale). I would say, however, that linear regression assumptions are usually not too impacted by violating perfect measurement accuracy in the outcome variable (usually more an issue with the predictors). The residuals of the model generally cover the measurement error in the outcome. That makes some intuitive sense: the residual is error not explained by the predictors, so there's no reason that the predictors should be able to explain the variance due to measurement error. That said, if you have reason to think that the residuals are related to certain predictors, then you can run those regressions as well. This is sometimes called a variance regression or a distributional model, but it's basically a joint estimation of a linear model for the outcome and the residual variance term. Again, in brms language, this would like like the following: fit There are additional details about distributional models in brms here .
