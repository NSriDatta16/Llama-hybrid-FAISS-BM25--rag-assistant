[site]: datascience
[post_id]: 122531
[parent_id]: 22212
[tags]: 
What is "Policy Collapse"? I would define policy collapse as the phenomenon in which an RL agent is learning, the average reward per episode appears on average to be steadily increasing over time, everything is going well, then all of a sudden, the average reward per episode drops to a much lower value and struggles to recover. What are the causes? Learning in RL is different than supervised learning In supervised learning, you're effectively just learning parameters in order to minimise a loss function, so given a sensible optimisation algorithm and large enough batch size, you would reasonably expect your algorithm to converge In RL, say you're using a policy gradient method such as PPO You're typically learning a value function (expected sum of discounted rewards as a function of the state), but this is difficult because: The value function depends on the policy, which is changing from episode to episode, so your value function is trying to model a moving target You're typically learning your value function with some kind of bootstrapping, and since your value function is also changing from episode to episode, this means your value function is trying to learn a doubly moving target, while each update is also causing the target to move in some way Now, you're also learning a policy as well, for example doing some kind of gradient ascent algorithm using a Monte Carlo approximation of the policy gradient But the environment is typically stochastic, and depending on the nature of the environment and the particular experience of the agent in any given episode, it might be very difficult to calculate a good estimate of the policy gradient (see EG Kakade and Langford, 2002 section 3.2), and a bad estimate of the policy gradient might lead to a decrease in performance In some cases, all it might take is a few consecutive unfortunate episodes, which happen to provide bad estimates for the gradients of the value function and policy, which stack up to change your parameters in the wrong direction to such an extent that they're no longer in the basin of attraction of the local optimum you were previously converging to In this case you will see a policy/performance collapse, and it might take a relatively long time before your policy is able to recover its previous levels of performance
