[site]: crossvalidated
[post_id]: 199909
[parent_id]: 
[tags]: 
Low accuracy in emotion voice recognition

I am training a neural network to do voice emotion recognition with: 100 input layer size. 25 hidden layer size. 6 labels (output layer). I have divided the data set into train set and test set, then extracted features from the voice using MLFCC (Mel Frequency Cepstral Coefficients) which returns a matrix with different sizes. So, I used 100 features of them for each time. The accuracy on the training set is 100%, but when it comes to the test set it is about 30-40%. I still don't know a lot about this domain, but it is obviously overfitting the problem (maybe not, but that is what I have learned). I have done some adjustments to avoid this problem: Increasing the lambda, decreasing the number of features, adding an extra hidden layer. The accuracy got better, but never bigger than 40%. What would be the problem? Here is the implementation of MLFCC: function [cepstra,aspectrum,pspectrum] = melfcc(samples, sr, varargin) if nargin 0 if (dcttype ~= 1) disp(['warning: plp cepstra are implicitly dcttype 1 (not ', num2str(dcttype), ')']); end % LPC analysis lpcas = dolpc(aspectrum, modelorder); % convert lpc to cepstra cepstra = lpc2cep(lpcas, numcep); % Return the auditory spectrum corresponding to the cepstra? % aspectrum = lpc2spec(lpcas, nbands); % else return the aspectrum that the cepstra are based on, prior to PLP else % Convert to cepstra via DCT cepstra = spec2cep(aspectrum, numcep, dcttype); end cepstra = lifter(cepstra, lifterexp); if useenergy cepstra(1,:) = logE; end And here is my implementation: clear ; close all; clc [input,output]=gettingPatterns; input_layer_size = 70; hidden_layer_size = 100; hidden2_layer_size = 25; num_labels = 6; fu = [input output];size(fu) fu=fu(randperm(size(fu,1)),:); input = fu(:,1:70); output = fu (:,71:76); crossIn = input(201:240,:); crossOut=output(201:240,:); trainIn = input(1:200,:); trainOut=output(1:200,:); Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size); Theta2 = randInitializeWeights(hidden_layer_size,hidden2_layer_size); Theta3 =randInitializeWeights(hidden2_layer_size,num_labels); initial_nn_params = [Theta1(:) ; Theta2(:);Theta3(:)]; size(initial_nn_params) options = optimset('MaxIter',1000); % You should also try different values of lambda lambda=1; costFunction = @(p) nnCostFunction(p, ... input_layer_size, ... hidden_layer_size, ... hidden2_layer_size,num_labels, trainIn, trainOut, lambda); [nn_params, cost] = fmincg(costFunction, initial_nn_params, options); num_labels, (hidden_layer_size + 1)); Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ... hidden_layer_size, (input_layer_size + 1)); Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):( (hidden_layer_size * (input_layer_size + 1)))+(hidden2_layer_size*(hidden_layer_size+1))), ... hidden2_layer_size, (hidden_layer_size + 1)); Theta3 = reshape(nn_params(((1 + (hidden_layer_size * (input_layer_size + 1)))+(hidden2_layer_size*(hidden_layer_size+1))):end), ... num_labels, (hidden2_layer_size + 1)); %[error_train, error_val] = learningCurve(trainIn, trainOut, crossIn, crossOut, lambda,input_layer_size,hidden_layer_size,num_labels); pred = predict(Theta1, Theta2,Theta3,trainIn); [dummy, p] = max(trainOut, [], 2); [pred trainOut] fprintf('\nTraining Set Accuracy: %f\n', mean(double(pred == p)) * 100); pred = predict(Theta1, Theta2,Theta3,crossIn); [pred crossOut] [dummy, p] = max(crossOut, [], 2); fprintf('\nTraining Set Accuracy: %f\n', mean(double(pred == p)) * 100); And here is the code for getting the patterns: function [ input,output ] = gettingPatterns() myFolder='C:\Users\ahmed\Documents\MATLAB\New Folder (3)\homeWork\speech'; filePattern=fullfile(myFolder,'*.wav'); wavFiles=dir(filePattern); output=[]; input=[]; for k = 1:length(wavFiles) sampleOutput=zeros(1,6); baseFileName = wavFiles(k).name; if baseFileName(3:5)=='ang',sampleOutput(1)=1;,end; if baseFileName(3:5)=='fea',sampleOutput(2)=1;,end; if baseFileName(3:5)=='bor',sampleOutput(3)=1;,end; if baseFileName(3:5)=='sad',sampleOutput(4)=1;,end; if baseFileName(3:5)=='joy',sampleOutput(5)=1;,end; if baseFileName(3:5)=='neu',sampleOutput(6)=1;,end; output(k,:)=sampleOutput; fullFileName = fullfile(myFolder, baseFileName); wavArray = wavread(fullFileName); [cepstra,xxx]=melfcc(wavArray); [m,n]=size(cepstra); reshapedArray=reshape(cepstra,m*n,1); smalledArray=small(reshapedArray,70); %Normalized Features %x(i)=(x(i)-mean)/std; normalizedFeatures=[]; for i=1:length(smalledArray) normalizedFeatures(i)=(smalledArray(i)-mean(smalledArray))/std(smalledArray); end input(k,:)=normalizedFeatures; end Please note the following: I have done this test on the nn toolbox, I got the same result. The only reason for implementing it myself is to be able to add an extra hidden layer. The implementation of the cost function, forward propagation and backward propagation are 100% correct, so I did not include them in this question.
