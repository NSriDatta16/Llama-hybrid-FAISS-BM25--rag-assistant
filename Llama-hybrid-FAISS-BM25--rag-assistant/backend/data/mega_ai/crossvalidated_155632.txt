[site]: crossvalidated
[post_id]: 155632
[parent_id]: 
[tags]: 
Using Bayesian inference to fit functions

I'm trying to understand Bayes Inference better and was wondering if it is possible to use it to fit a function $f(t)$ where I only know the value $f(t)$ for a few values of $t$. For each value of $t$ I have an ensemble of possible values for $f(t)$. As a example I have an experiment which I can repeat as often as I want but my quantaty of interest can only be measured at a few fixed points in time. As an example lets say $f(t) = sin(\alpha x)$. With $\alpha=1.4$ and the ensemble of each measurment is $Norm(\mu=f(t), \sigma=.01)$. I measure the $f(t)$ at 8 equily distributed points between 0 and $\pi$. I tried to model this in pymc with: alpha = pm.Normal('alpha', 3, 1) tau = pm.Normal('tau', 5, 10) obs = [pm.CommonDeterministics.Lambda('obs_{}'.format(i), lambda alpha=alpha : np.sin(alpha*xx)) for i, xx in enumerate(x)] noises = [pm.Normal('noise_{}'.format(i), o, tau, value=m, observed=True) for i, (m, o) in enumerate(zip(measurements, obs))] This assumes that the priors for $\alpha$ and $\sigma$ (note: pymc uses $\tau = \frac{1}{\sigma^2}$ instead that is the reason I write $\tau$ in the code.) When I run this with the MarkovChainMonteCarlo algorithm from pymc I get the following result. Which is totally off. It does this because it starts to estimate an unreasonably large error $\sigma$ in the observations. So my questions are. Is this because I choose bad priors? Is so can what would be more appropriate priors? Is something like this in general possible using bayes? Update The be clear. I would like to infer the value of $\alpha$ from the observed data. The whole example can be found here
