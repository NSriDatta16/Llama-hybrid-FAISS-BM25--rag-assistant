[site]: crossvalidated
[post_id]: 623782
[parent_id]: 
[tags]: 
Univariate vs. Multivariate Standardization

There are several common methods for scaling input features to machine learning models prior to training the model. The most popular methods seem to be standardization (centering by the mean and dividing by the standard deviation of the variable) and min-max normalization (centering by the minimum value and dividing by the range of the variable). While reviewing information on this topic I've only come across univariate methods and it begs the question of why multivariate aren't very prevalent? For example rather than performing standardization to each variable in a univariate way, it could be performed in a multivariate way. $$ \boldsymbol{z}=\boldsymbol{\Sigma}^{-1/2}(\boldsymbol{y}-\boldsymbol{\mu}) $$ Where $\boldsymbol{y}$ is a vector of original unscaled input features (single observation of training data), $\boldsymbol{\mu}$ is a vector containing the main of the unscaled training data input features, $\boldsymbol{\Sigma}$ is the correlation matrix of training data input features and $\boldsymbol{z}$ is the transformed vector of input features. This should lead to the following for the training data: Each transformed feature having a mean of 0. Each transformed feature having a standard deviation of 1. Each each transformed feature being uncorrelated to other features. Is there a reason this kind of preprocessing isn't more prevalent? EDIT: For this question I'm particularly interested in model accuracy and not inference.
