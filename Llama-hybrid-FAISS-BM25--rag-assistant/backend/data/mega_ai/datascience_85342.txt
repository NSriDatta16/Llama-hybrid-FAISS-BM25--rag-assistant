[site]: datascience
[post_id]: 85342
[parent_id]: 85340
[tags]: 
Please have a look at your weights after training. I assume your Neurons die due to relu activation as they output Zero for Input Unfortunately, the ReLU activation function is not perfect. It suffers from a problem known as the dying ReLUs: during training, some neurons effectively “die,” meaning they stop outputting anything other than 0. In some cases, you may find that half of your network’s neurons are dead, especially if you used a large learning rate. A neuron dies when its weights get tweaked in such a way that the weighted sum of its inputs are negative for all instances in the training set. When this happens, it just keeps outputting zeros, and Gradient Descent does not affect it anymore because the gradient of the ReLU function is zero when its input is negative. From Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow Concepts, Tools, and Techniques to Build Intelligent Systems, Aurélien Géron, 2019 So to combat this problem remove the ReLU activations and use LeaklyReLU instead. So for your case following are the changes: from tensorflow.keras.layers import LeakyReLU # for leakly relu model.add(Dense(8*n_nodes, input_dim=n_in)) model.add(LeakyReLU(alpha=0.05)) model.add(Dense(4*n_nodes)) model.add(LeakyReLU(alpha=0.05)) model.add(Dense(2*n_nodes)) model.add(LeakyReLU(alpha=0.05)) model.add(Dense(n_nodes)) model.add(LeakyReLU(alpha=0.05)) model.add(Dense(n_out)) model.add(LeakyReLU(alpha=0.05)) After changing your code as specified the problem should be fixed. But in general I'm with @meTchaikovsky, for time series data recurrent neural networks are better suited for modelling.
