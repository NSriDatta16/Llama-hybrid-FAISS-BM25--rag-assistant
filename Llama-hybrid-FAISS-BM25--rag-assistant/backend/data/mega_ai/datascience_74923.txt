[site]: datascience
[post_id]: 74923
[parent_id]: 74918
[tags]: 
So, how should I refine the word/sentence embeddings vector given by the BERT model in the case when I have a set completely unlabelled set of documents? What are you looking to achieve with these unlabelled documents? If you are looking to classify them, then there is no way of getting around getting labels and fine-tuning on them. I'm aware that the BERT model is originally trained on unlabelled data, so there must be some way. If you don't need to perform any specific end task (like classification) with the data, and instead is just looking to train BERT using your own data, then there is a way. BERT is self-supervised, meaning that it takes unlabelled data and automatically generates labels that it can train on. The same methods can be used to train on any large language corpus. However, this will only train the embeddings and fine-tuning on labels will still always be needed for classification. I think huggingfaces' blog post is a good starting point if you want train a general BERT on your own dataset. You can read about how BERT is trained on generated labels from text here .
