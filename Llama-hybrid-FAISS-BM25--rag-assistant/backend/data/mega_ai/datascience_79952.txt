[site]: datascience
[post_id]: 79952
[parent_id]: 79934
[tags]: 
Y comes as percent in the format. To put it into the same dimension as X I multiplied it with 10. Is that ok from math/data science perspective? As far as I can tell there's no reason to do that, and why multiply by 10? 5 records is not much but there are a lot of features. I would like to do multiple linear regression. Do you think this feasible with this data set? What would be objections and risks doing that? The fact that there are lots of features makes it harder to work with few instances, not easier. There is a very high risk of overfitting, that is of the model catching patterns which appear by chance in the features. This leads to predictions being also affected by chance, so bad performance. Would upsampling the dataset help me with anything here? Or could I just work with the five records? Upsampling is unlikely to work since it's going to reproduce the patterns in the small dataset, so it's also going to reproduce patterns which appear by chance. With the strange shape of the dataset especially the low number of records do you think that sufficient precision can be reached? It depends what the data represents, if the features happen to be really good predictors for the dependent variable and are not affected by chance, it might work. But these are very optimistic assumptions, in general it's not reasonable to expect good predictions from such a small set of instances. How could I calculated the maximum possible precision/discriminative power possible with this dataset? (I am looking for strong arguments why they should give me access to the complete dataset) In general I would suggest doing a leave-one-out experiment: use 4 instances as training set, 1 instance as test set, repeat 5 times with a different instance as test set every time. Measuring the average performance should give you an idea how far off the predictions are going to be (you could use a very simple evaluation measure such as mean absolute error). However what you have is actually a time series apparently, so it might be worth looking at methods which take time evolution into account.
