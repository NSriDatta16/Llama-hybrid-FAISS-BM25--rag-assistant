[site]: crossvalidated
[post_id]: 156301
[parent_id]: 156098
[tags]: 
An example on how to do vanilla plain cross-validation for lasso in glmnet on mtcars data set. Load data set. Prepare features (independent variables). They should be of matrix class. The easiest way to convert df containing categorical variables into matrix is via model.matrix . Mind you, by default glmnet fits intercept, so you'd better strip intercept from model matrix. Prepare response (dependent variable). Let's code cars with above average mpg as efficient ('1') and the rest as inefficient ('0'). Convert this variable to factor. Run cross-validation via cv.glmnet . It will pickup alpha=1 from default glmnet parameters, which is what you asked for: lasso regression. By examining the output of cross-validation you may be interested in at least 2 pieces of information: lambda, that minimizes cross-validated error. glmnet actually provides 2 lambdas: lambda.min and lambda.1se . It's your judgement call as a practicing statistician which to use. resulting regularized coefficients. Please see the R code per the above instructions: # Load data set data("mtcars") # Prepare data set x Final comments: note, the model's output says nothing about statistical significance of the coefficients, only values. l1 penalizer (lasso), which you asked for, is notorious for instability as evidenced in this blog post and this stackexchange question . A better way could be to cross-validate on alpha too, which would let you decide on proper mix of l1 and l2 penalizers. an alternative way to do cross-validation could be to turn to caret's train( ... method='glmnet') and finally, the best way to learn more about cv.glmnet and it's defaults coming from glmnet is of course ?glmnet in R's console )))
