[site]: crossvalidated
[post_id]: 240154
[parent_id]: 240148
[tags]: 
Bayes Networks represent probability distributions in a very simple manner. That being said, all models apply simplifying assumptions to the problem, so in that sense they do end up simplifying assumptions. Important Ideas: If you really understand the joint distribution of all of the variables you're interested in you're golden. You can then compute conditionals, marginals (their densities, statistics, means, medians). So this is a reasonable goal. Each node in a Bayes Network represents a random variable conditionally distributed on it's parents. Because of the chain rule, and the fact that the Bayes Network is acyclic, the joint distribution's density can be computed as a product of the individual nodes' densities. Causation flows along arrows, and correlation travels in both directions. Then there are important ideas like conditional independence , d-separation , inference . I think your last sentence should say that Bayes Networks and Markov Random Fields are both examples of Artificial Neural Networks with no hidden layer. EDIT: Also, I think your title should be changed, Bayes networks are also directed , where as MRF's are undirected .
