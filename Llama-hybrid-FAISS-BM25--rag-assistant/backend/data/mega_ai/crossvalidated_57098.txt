[site]: crossvalidated
[post_id]: 57098
[parent_id]: 
[tags]: 
Macro and Micro Inter-Coder/Rater Agreement

I have a labeling task with 4 labels for which I'm using 2 coders/annotators/raters. This labeling effort leads to a confusion matrix like the one in https://docs.google.com/drawings/d/1T7pkLLkE7qvKanxqvZO_uQ_q3hz6S3fcTbOnWophx38/edit?usp=sharing I want to measure the agreement between the 2 annotators. How so? Kappa Coefficient works for binary labeling/classification but not for this case. I thought of having like a micro kappa for each 2 labels and then an averaged macro Kappa across all combinations. How dummy does that sound? Thanks
