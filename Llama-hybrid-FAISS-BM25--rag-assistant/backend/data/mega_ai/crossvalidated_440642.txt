[site]: crossvalidated
[post_id]: 440642
[parent_id]: 
[tags]: 
Curious on using black box model interpreters (LIME, SHAP, etc) for production models

So, I'm wondering how many of you have been in a similar situation: We have a XGBoost classifier in production that is performing really well. As an enhancement, I want to add an explainer to provide directional context to the business group that is using this model. My first thought and experiment was to use LIME since it seems to be the most popular for similar use cases but I see a big problem with this: There is a kernel width parameter in most packages that identify how big of a neighborhood you should consider when building a localized model that cant be objectively determined for each prediction. Questions: For those of you who have used LIME in a similar context, how did you overcome this? 2 What other alternatives do I have? Also, if there is another location where this should be submitted, please let me know.
