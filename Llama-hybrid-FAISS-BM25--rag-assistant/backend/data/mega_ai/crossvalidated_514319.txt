[site]: crossvalidated
[post_id]: 514319
[parent_id]: 513600
[tags]: 
in addition to my article resource of different fallacies in academic research: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.579.5429&rep=rep1&type=pdf I screened one/two articles and a blog and a few other things, this is the main summary: Most of the research I found always use the term spurious or non-sense, as i mentioned in my comments. The first resource I found dealt exactly with the danger behind these in terms of time series: The conclusion to the dangers is, that researchers seem to "not prewhitening" or in other words flatten the noise in a time series, to be sure the remaining parts could offer a small glance of a real relationship: On prewhitening: http://hosting.astro.cornell.edu/~cordes/A6523/Prewhitening.pdf The article that deals with the dangers: https://link.springer.com/content/pdf/10.3758/s13428-015-0611-2.pdf Some excerpt: We have shown clearly that cross-correlations between pairs of time series, or even pairs of series derived as averages of sets of series, can be misleadding. The key means of avoiding such spurious cross correlations is to prewhiten the series being cross-correlated. But even then, some spurious correlations may remain, and the results need to be treated with caution. Not only is critical interpretation necessary, but also an awareness that certain kinds of time series may not be appropriate for the prewhitening approachâ€”for example, when data are binomial or the series show only sparse change The article also deals with different methods to engage the danger, perhaps this is useful for you. In addition I looked into the terms of causality, as the researchers you mention clearly draw some causal objectives from their observations. I found this blog which is ourstanding in the term that it highlights, i dunno over 100!? papers and sources on causality and time series granger causality and so on: https://towardsdatascience.com/inferring-causality-in-time-series-data-b8b75fe52c46#4da2 . Although i not completely read through all of it, as you can imagine. Perhaps you found something enlightening if my previous research is not sufficient, so taht you got at least another hint. To summarize my findings a sentence like they make the error/fallacy of not checking for spurious correlations, or the error of not prewhitening or something like that, could be feasible, as long as you have a source that highlights the danger of not checking the data in depth behind two time series. Because it is not significant because it is in a certain area. We have to look at the series at a whole. I do not believe if it correlates on one or two time points you can make the inductive check that the whole series is like that or there is a causality. That should be also summed up under spurious. However my insight does not deal completely with the fact, that the researchers left out some information from the beginning at the end of march (the pollen density), I believe this is the error of purely fraud or dunno. But if you believe the researchers made mistake. I would tend to look into the material I provided. Hope it helps in some way. https://en.wikipedia.org/wiki/Oil_drop_experiment#Fraud_allegations
