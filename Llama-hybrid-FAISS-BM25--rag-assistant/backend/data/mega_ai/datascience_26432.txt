[site]: datascience
[post_id]: 26432
[parent_id]: 26430
[tags]: 
Try it and see what happens. Neural networks don't have enough representational power to learn an XOR operation without at least one hidden layer, so there are definitely some interesting features you can construct with logical operations. The AND operation is equivalent to multiplication, which corresponds to interaction terms linear models. But yeah, it does depend on the model. For example, a decision tree can learn these kinds of features on its own (although it won't necessarily). For exame, an AND operation would correspond to two tests on the same branch.
