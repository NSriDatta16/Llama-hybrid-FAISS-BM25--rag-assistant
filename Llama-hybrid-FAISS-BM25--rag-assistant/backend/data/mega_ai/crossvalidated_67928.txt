[site]: crossvalidated
[post_id]: 67928
[parent_id]: 67924
[tags]: 
A cost function is a function that specifies how bad you're going to feel for making any given error. In the case of regression, you're going to wince when your model's predictions are far from the mark. A good way to measure this pain for a given set of data is by measuring, say, the average squared distance from a model's predictions to what is actually observed . For a single observation $x$, that means $(h_{\theta}(x) - x)^{2}$. If a model's predictions for a set of observations $\{x^{(i)}\}_{i \in I}$ are $\{h_{\theta}(x^{(i)})\}_{i \in I}$, the average squared distance is $\frac{1}{|I|}\sum_{i \in I} (h_{\theta}(x^{(i)}) - x^{(i)})^{2}$. Or, $$ J(\theta_{0}, \theta_{1}) = \frac{1}{|I|}\sum_{i \in I} (\theta_{0} -(1 - \theta_{1})x^{(i)})^{2} $$ If we want to minimize the pain we feel from making mistakes, we should seek to minimize this function. To do that, we minimize over the free parameters $\theta_{0}$ and $\theta_{1}$ using whatever optimization method you'd like, and those values will be the parameters of your model, $h$. There's no reason that this particular cost function must be preferred over any others that seek to measure the same kind of thing, so you can really do linear regression using whatever cost function you'd like. The reason that this form of cost function is the de-facto standard, however, is that it's convex . That is, the function will always have a unique global optimum at some particular values of $\theta_{0}$ and $\theta_{1}$, and it's pretty easy to find that optimum.
