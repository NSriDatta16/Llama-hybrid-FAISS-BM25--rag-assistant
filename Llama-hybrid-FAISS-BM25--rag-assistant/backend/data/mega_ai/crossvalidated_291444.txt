[site]: crossvalidated
[post_id]: 291444
[parent_id]: 291417
[tags]: 
I think it's safe to say there's no situation when one needs an unbiased estimator; for example, if $\mu = 1$ and we have $E[\hat \mu] = \mu + \epsilon$, there has got be an $\epsilon$ small enough that you cannot possibly care. With that said, I think it's important to see unbiased estimators as more of the limit of something that is good. All else remaining the same , less bias is better. And there are plenty of consistent estimators in which the bias is so high in moderate samples that the estimator is greatly impacted. For example, in most maximum likelihood estimators, the estimate of variance components is often downward biased. In the cases of prediction intervals, for example, this can be a really big problem in the face of over fitting. In short, I would extremely hard pressed to find a situation in which truly unbiased estimates are needed. However, it's quite easy to come up with problems in which the bias of an estimator is the crucial problem. Having an estimator be unbiased is probably never an absolute requirement, but having an estimator unbiased does mean that there's one potentially serious issue taken care of. EDIT: After thinking about it a little more, it occurred to me that out-of-sample error is the perfect answer for your request. The "classic" method for estimating out-of-sample error is the maximum likelihood estimator, which in the case of normal data, reduces to the in-sample error. While this estimator is consistent, with models with large degrees of freedom, the bias is so bad that it will recommend degenerate models (i.e. estimate 0 out-of-sample error with models that are heavily over fit). Cross-validation is a clever way of getting an unbiased estimate of the out-of-sample error. If you use cross-validation to do model selection, you again downwardly bias your out-of-sample error estimate...which is why you hold a validation dataset to get an unbiased estimate of the final selected model. Of course, my comment about truly unbiased still remains: if I had an estimator had expected value of the out-of-sample error + $\epsilon$, I would happily use it instead for small enough $\epsilon$. But the method of cross-validation is motivated by trying to get an unbiased estimator of the out-of-sample error. And without cross-validation, the field of machine learning would look completely different than it does now.
