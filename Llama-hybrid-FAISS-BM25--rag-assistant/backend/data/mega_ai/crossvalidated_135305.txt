[site]: crossvalidated
[post_id]: 135305
[parent_id]: 135297
[tags]: 
SVM are not designed to estimate probability of a class. Nevertheless, there is a way to estimate such probabilities, using Platt scaling . There is SVM-like probabilistic method, namely, Relevance Vector Machines . Unfortunately, they're patented by Microsoft, though, apparently, free to use in academia. Finally, if you want to find these x s where SVM is unsure about correct labeling, you can do it even without probability. The decision rule of SVM is $$ class(x) = \begin{cases} 1, & \mathbf w^T x + b > 0 \\ 0, & \mathbf w^T x + b Where $\mathbf w^T x + b$ is SVM's decision function, effectively, a signed distance to the separating hyperplane. If this distance (modulo of it) is too small — SVM is less sure about $x$. You can come up with some kind of threshold on decision functions to find vectors of interest. About MaxEnt vs. (linear) SVM: It's mostly opinion-based, but their loss functions are quite similar SVM's loss is called " Hinge loss ", MaxEnt's — "Logistic". Asymptotically (as signed distance goes to $\pm \infty$) they're the same (modulo scaling constant because of arbitrary log base in Logistic loss), so if you data is well-separable, both should perform fine. Both methods will find a separating hyperplane, though, SVM's one will have a bigger margin (because it's a max-margin classifier). When data is not linear separable, SVM will try to push as many points behind the boundary of distance = $\pm$ 1, thus having 0 loss on them, while logistic loss may tolerate some errors to become even more confident in already correctly labeled samples. This is my opinion, it is not necessarily true :-)
