[site]: crossvalidated
[post_id]: 375252
[parent_id]: 375101
[tags]: 
I will try to give an intuitive example to understand why the arithmetic mean \begin{equation} \overline x_1 = \sum_{i=1}^{n} \frac{x_i}{n} \end{equation} is not as good as \begin{equation} \overline x_2 = \frac{a + b}{2} \end{equation} In the case where $X \sim \mathrm{unif}(\alpha,\beta)$ Imagine that you have 10 observations from $\mathrm{unif}(1,11)$ (There is a candy factory that puts a candy of 1cm, 2cm, ..., 11cm, 1cm, 2cm,..., 11cm,... in separate bags). We know from the above information that the real mean is 6 (For the factory example, the factory would have spend the same amount of sugar if each candy was 6cm) Now, if you don't know any of the above and take a random sample to estimate that, then $\bar x_2$ would only require the smallest and the highest number to appear in your sample and that's it! It would always guess the correct answer with 0 error! $\bar x_1$ on the other hand, would be sensitive to every single value that you get and it will "fluctuate" around the real value. Furthermore, if the highest (or equivalently the lowest) value doesn't appear in your sample, then again $\bar x_2$ will almost always be closer to the real mean compared to $\bar x_1$ . $\bar x_1$ will be better only if your sample is already centralized around 6 which is less likely to happen compared to the other possible scenarios. For the candy factory example. If you try to predict the "average candy" that is in each bag, it's better to get the average between the smallest and the largest candy you had so far than averaging the candies in every single bag you open and change your prediction (and thus error) after every bag.
