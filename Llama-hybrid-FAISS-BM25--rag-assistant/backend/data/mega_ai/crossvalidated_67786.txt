[site]: crossvalidated
[post_id]: 67786
[parent_id]: 
[tags]: 
AIC & number of parameters

I have two questions about AICc and number of parameters. I am using AICc to select the best of three models in a logistic regression - and each model has the same number of parameters. What are some compelling arguments for this method? I have G and C statistics, but these do not allow ranking. The formula for AICc in this case is as follows for for logistic regressions: $K_1$ = sample size $K_2$ = number of parameters (including the intercept and s) $K_3$ = Log Likelihood of model $K_4$ = AICc = Akaike’s Information Criterion logistic regression Let $$K_4 = -2K_3 + 2K_2 +\frac{2K_2(K_2+1)}{(K_1-K_2-1)}$$ Additionally, for a different data set, in which the linear regression models being compared with AICc have different numbers of parameters, how might I interpret two models which have nearly equivalent AICc values: one with a single predictor variable (A), and the other model has an additional variable B (A+B)? B on its own has a significantly larger AICc value, so on its own it is one of the least likely models, but model A+B is nearly as likely as model A. Why isn't the addition of one variable, which is not a likely predictor on its own, causing a greater change in the AICc value when added to A? The formula for AICc is as follows and is used for regression models: $K_1$ = sample size $K_2$ = number of parameters (including the intercept and standard deviation) $K_3$ = residual standard error $K_4$ = AICc = corrected Akaike’s Information Criterion Let $$K_4 = K_1\log \left( \frac{K_3^2(K_1-K_2+1)}{K1} \right) + 2K_2 + \frac{2K_2(K_2+1)}{K_1-K_2-1}$$
