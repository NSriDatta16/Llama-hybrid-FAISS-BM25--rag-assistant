[site]: crossvalidated
[post_id]: 618226
[parent_id]: 
[tags]: 
Is concatenating a single integer sufficient for encoding positional embeddings in transformer models?

In transformer models, positional embeddings are commonly used to encode the positional information of words in a sequence. While sinusoidal positional embeddings are often employed, I'm curious about an alternative approach. Can we simply concatenate a single integer to the word embeddings to represent positional information? Note that this way, we can keep the semantic and positional information separate. Normally, we combine these two by adding the word embeddings and positional embeddings together. Consequently, we need to strike a balance to ensure that the positional encoding does not overpower or dominate the word embedding information. Please correct me if my understanding is wrong and let me about the issues with the proposed positional encoding by index idea.
