[site]: crossvalidated
[post_id]: 103237
[parent_id]: 61309
[tags]: 
Note that the predicted residual sum of squares, PRESS, is got by jack-knifing the sample: there's no sense in calculating it for training & test sets. Calculate it for a model fitted to the whole sample (& compare it to the RSS to assess the amount of over-fitting). For ordinary least-squares regression there's an analytic solution: $$\sum_i \left(\frac{e_i}{1-h_{ii}}\right)^2$$ where $e_i$ is the $i$th residual & $h_{ii}$ its leverageâ€”from the diagonal of the hat matrix $$H=X(X^\mathrm{T}X)^{-1}X^\mathrm{T}$$ (where $X$ is the design matrix). In general cross-validation & bootstrap validation are preferable to splitting a sample into training & test sets: you don't lose precision in the estimates as when fitting on a smaller training set, & the performance measure on the test set will be less variable. How preferable depends on sample size.
