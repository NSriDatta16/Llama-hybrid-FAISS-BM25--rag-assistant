[site]: crossvalidated
[post_id]: 481947
[parent_id]: 481941
[tags]: 
This is an open problem in statistics and machine learning. Several methods to approximate the KL divergence have been proposed. For instance, take a look at the FNN R package: https://cran.r-project.org/web/packages/FNN/FNN.pdf It miserably fails sometimes, but it works in simple cases and with large samples (for samples smaller than 100 it can behave erratically). For instance, consider the distance between a t distribution with $\nu =1,2,3, 100$ degrees of freedom and a normal distribution (R code taken from this link ). With $n=10,000$ samples library(knitr) library(FNN) # Normalising constant K With $n=250$ library(knitr) library(FNN) # Normalising constant K
