[site]: crossvalidated
[post_id]: 346019
[parent_id]: 
[tags]: 
Average time it takes to get a sample above some threshold level in normal distribution

I am sampling a normally distributed noise signal (digital recording) with some arbitrary mean and standard deviation, where a single sample is received each second (1 sample / 1 sec). I am interested in: 1.) The average time it takes to get a sample above +6 $\sigma$ ? 2.) The average time between two successive samples above +6 $\sigma$ ? My attempt Probability for sample to be above +6 $\sigma$ is $ p = 1-\frac{1}{2} (1+erf(\frac{6}{\sqrt{2}})$, then the average time for sample above +6 $\sigma$ to occur is 1/$p$ * 1 [sec]. Assuming memory-less system I think that 1.) and 2.) should be the same. I would also like o know if this problem could be solved in the context of failure rates analysis (mean time between failures) where a sample above some threshold level would define system/component failure.
