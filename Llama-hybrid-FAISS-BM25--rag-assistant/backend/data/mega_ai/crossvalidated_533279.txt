[site]: crossvalidated
[post_id]: 533279
[parent_id]: 
[tags]: 
Unrealistic predictions from ML due to high differences in the PCA results of training and prediction data sets

I have a very high-dimensional training set (~300D) and the predictors are highly correlated. Therefore, I ran PCA on the training data and it turns out that 6 PC s are enough to explain ~99% of the variation in the training set. Hence, I kept only those 6 PC s as my predictors for machine learning ( ML ) model training. The 6 PC s (first 6 examples) look like this: | PC1| PC2| PC3| PC4| PC5| PC6| |----------:|---------:|----------:|----------:|----------:|----------:| | -9.771549| -3.827747| 2.9245668| -0.5665935| 0.1713290| 0.4544728| | 15.460740| 4.113467| -0.1197129| 0.8851271| -0.3868882| 0.7598501| | -17.782880| 7.538710| 6.9293371| -1.2879122| 0.2715957| -0.5893660| | -24.981467| 7.900724| 3.4624333| -1.4603620| -0.3122299| 0.1197103| | 11.228855| 1.998577| -1.0166994| -0.3488780| -0.4519286| -0.0417837| | 9.358533| 1.467957| -0.6097303| -0.4238638| -0.2418337| 0.2840885| I made the exact same preprocessing steps for the validation and testing sets based on the training set (e.g. using the scaling factors - mean and standard deviation of training set, not validation and testing sets explicitly) as this is suggested. The PCA results for validation and testing sets seem very similar although I did not explicitly perform PCA for them. I trained ANN on the training set, and tested on the validation and testing sets and the model seems to work very well on these sets. Now, I have another data set on which I have to make predictions using the trained ANN model. I used the exact same transformation (scaling and PCA based on the training set) to get 6 PCs for this data set. But, the 6 PC s look very different from the PCA results of training and testing sets: | PC1| PC2| PC3| PC4| PC5| PC6| |---------:|---------:|---------:|--------:|---------:|--------:| | -13245.26| -22081.56| -43410.22| 48645.37| -48980.44| 53596.35| | -24146.61| -15982.04| -41161.53| 52007.29| -49452.43| 52105.31| | -17232.47| -18435.81| -39299.39| 47905.61| -51266.76| 52223.69| | -14585.95| -18612.40| -41111.39| 50026.41| -46628.30| 51840.56| | -18974.25| -18848.59| -41187.38| 47901.96| -49822.84| 51848.54| | -30657.77| -12755.27| -41228.71| 52902.16| -52276.29| 53275.37| As you can probably guess, ANN gives me a very unrealistic predictions when I use the model to predict on this data set. And I do not have labels for this data set as the labels are what I have to predict. ANN I have built is a multi-output regression model and although it does really well on the training, validation and testing sets, it gives me very unrealistic predictions for the data set I have to make predictions on. Could these differences in the PCA results indicate that the data sets are from completely different distributions? And should I perform PCA on this data set explicitly and extract the first 6 PC s for prediction? UPDATE : I performed PCA (with scaling and centering) on prediction set (the one without labels) without using the same training set PCA scaling and transformation. Then, I took the first 6 PC s and threw the rest. This time, the result looks much closer to the PCA result than before: | PC1| PC2| PC3| PC4| PC5| PC6| |--------:|---------:|--------:|----------:|---------:|----------:| | 2.669013| -5.466714| 1.466906| -0.3025037| -3.099849| 0.0458244| | 4.783994| -5.949396| 2.503893| -0.3880082| -2.808889| 0.2684732| | 2.974174| -5.047682| 1.463603| -0.3253319| -3.205017| -2.0727434| | 2.291158| -4.516547| 1.893270| -0.4827036| -3.425757| -0.3900811| | 3.801425| -5.513287| 2.325632| -0.3225131| -3.366994| 0.2016989| | 5.836599| -5.920547| 2.343807| -0.2373246| -3.736375| 0.7952611| When I use the trained model on this version of the data set I get somewhat more realistic results (relatively). However, as most of the experts do not recommend this (running PCA explicitly on the "prediction" sets), I am not sure if the results could be realiable?
