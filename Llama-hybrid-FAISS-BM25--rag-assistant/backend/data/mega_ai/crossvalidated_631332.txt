[site]: crossvalidated
[post_id]: 631332
[parent_id]: 534618
[tags]: 
As mentioned in the transformer paper 'Attention is all you need', the embedding matrix in the input and the pre-softmax linear transformation in the linear layer outside the decoder block share the same projection matrix parameters. vocab_size->d in the embedding matrix W, and d->vocab_size in the pre-softmax linear transformation matrix W^T in the linear layer. Sharing the parameters in the matrix related to embedding is called tied-embedding. The parameters are initialized with Xavier with N(0,1/sqrt(d)). So multiplying sqrt(d) keep the word embedding to stay with N(0,1). The original position embedding (cos/sin) can be considered as hard-code. So the the variance of position embedding is zero. Adding position embedding and word embedding (scaled sqrt(d)) still makes the variance of the combined embedding 1.
