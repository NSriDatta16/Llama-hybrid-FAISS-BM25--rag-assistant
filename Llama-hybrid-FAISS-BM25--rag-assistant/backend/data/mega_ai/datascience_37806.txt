[site]: datascience
[post_id]: 37806
[parent_id]: 
[tags]: 
Purpose of backpropagation in neural networks

I've just finished conceptually studying linear and logistic regression functions and their optimization as preparation for neural networks. For example, say we are performing binary classification with logistic regression, let's define variables: $x$ - vector containing all inputs. $y$ - vector containing all outputs. $w_{0}$ - bias weight variable. $W=(w_1,...,w_{2})$ - vector containing all weight variables. $f(x_i)=w_{0}+\sum_{i=1}x_{i}w_{i}=w_{0}+x^{T}W$ - summation of all weight variables. $p(x_{i})=\frac{1}{1+e^{-f(x_i)}}$ - logistic activation function (sigmoid), representing conditional probability that $y_i$ will be 1 given observed values in $x_i$ . $L=-\frac{1}{N} \sum^{N}_{i=0} y_i*ln(p(x_i))+(1-y_i)*ln(1-p(x_i))$ - binary cross entropy loss function (Kullback-Leibler divergence of Bernoulli random variables plus entropy of activation function representing probability) $L$ is multi-dimensional function, so it must be differentiated with partial derivative, being: $$\frac{\partial{L}}{\partial{w}}$$ Then, the chain rule gives: $$\frac{\partial{L}}{\partial{w_1}}=\frac{\partial{L}}{\partial{p_i}} \frac{\partial{p_i}}{\partial{w_1}}$$ After doing few calculations, derivative of the loss function is: $$(y_i-p_i)*x_i$$ So we got derivative of the loss function, and all weights are trained separately with gradient descent. What does backpropagation have to do with this? To be more precise, what's the point of automatic differentiation when we could simply plug in variables and calculate gradient on every step, correct? In short We already have derivative calculated, so what's the point of calculating them on every step when we can just plug in the variables? Is backpropagation just fancy term for weights being optimized on every iteration?
