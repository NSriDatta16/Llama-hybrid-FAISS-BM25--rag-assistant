[site]: datascience
[post_id]: 110771
[parent_id]: 
[tags]: 
How "similarity" is measured in image retrieval?

I know what content based image retireval is. I have read this and this as one of them says: "given a query images, get a rank list that are most similar to the query image, based on the content of the query image. " But my question is how the "similar" images are determined. Assume we are working on Oxford5k dataset. The dataset contains 5k images in 17 classes. So, when I feed one of the images as a query, my algorithm returns a list of ranked images. images come in the top of this list consider similar to the query. I know that we do the retrieval by extracting feature and matching them. But, how I can evaluate whether my method finds better than the others. I am pretty sure that some would say an image is similar to another one provided that they both belong to the same class. But this means we are doing a classification not an image retrieval task. If it is not the case, how I can say method A finds more "similar" images than method B. I am aware about Precision, Recall and mAP. But my question is before these metrics. For example, precision is TP/(TP+FP). But how we determine one image is a true positive (just based on class names?) How we say an image is similar to another one? Considering them as similar images based on belonging to the same class does not make sense. Any clear explanation would be really appreciated. UPDATE 1: My question is actually a question about the literature. How computer scientist working on field of image retrieval, consider an image similar to another one. How they measure their methods and claim theirs are better than the state-of-the-art? using precision metric only? Anyone can interpret as he likes to make his method looks better. For example, I can say an image of white bird is similar to an image of airplane as both have wings and white. And this is a correct, they are similar. By this assumption, I can say my method is better than yours. It means there is no standard benchmark. Really? after more than two decades progress in image retrieval, we have no solid standard to measure similarity and comparison technique to compare two method? just a mAP score which I can argue that anyone can cheat and fabricate better results by playing with the ranks.
