[site]: crossvalidated
[post_id]: 319611
[parent_id]: 319576
[tags]: 
Choosing k for k-means is a big problem. Yet, you cannot do k-means without fixing k. So the "hack" for the probability view is to assume every k is there, just with a different probability, and then do all k at once. Then try to maximize both at the same time. Beware that the book has a "everything is Bayesian" bias. And that usually means "everybody that isn't using Bayesian reasoning is doing it wrong". Then K-means "is not a proper EM algorithm"... That subcommunity can be a bit binary (and I will get downvotes for this...). Also, most "machine learning" books tend to be really not interested in unsupervised learning, and the clustering chapters are often utterly incomplete, missing some of the most popular (and important) methods such as DBSCAN and OPTICS.
