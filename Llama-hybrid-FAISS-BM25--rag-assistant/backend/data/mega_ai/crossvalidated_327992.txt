[site]: crossvalidated
[post_id]: 327992
[parent_id]: 265177
[tags]: 
Hamiltonian Monte Carlo performs well with continuous target distributions with "weird" shapes. It requires the target distribution to be differentiable as it basically uses the slope of the target distribution to know where to go. The perfect example is a banana shaped function. Here is a standard Metropolis Hastings in a Banana function: Acceptance rate of 66% and very poor coverage. Here is with HMC: 99% acceptance with good coverage. SMC (the method behind Particle Filtering) is almost unbeatable when the target distribution is multimodal, especially if there are several separate areas with mass. Instead of having one Markov Chain trapped within a mode, you have several Markov chains running in parallel. Note that you use it to estimate a sequence of distributions, usually of increasing sharpness. You can generate the increasing sharpness using something like simulated annealing (put a progressively increasing exponent on the target). Or typically, in a Bayesian context, the sequence of distributions is the sequence of posteriors: $$ P(\theta|y_1) \;,\; P(\theta|y_1,y_2)\;,\;... \;,\; P(\theta|y_1,y_2,...,y_N) $$ For instance, this sequence is an excellent target for SMC: The parallel nature of the SMC makes it particularly well suited for distributed/parallel computing. Summary: HMC: good for elongated weird target. Does not work with non continuous function. SMC: good for multimodal and not-continuous cases. Might converge slower or use more computing power for high dimensional weird shapes. Source: Most of the images come from a paper I wrote combining the 2 Methods (Hamiltonian Sequential Monte Carlo). This combination can simulate pretty much any distribution we can throw at it, even at very high dimensions.
