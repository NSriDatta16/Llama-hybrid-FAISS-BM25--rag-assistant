[site]: crossvalidated
[post_id]: 424704
[parent_id]: 
[tags]: 
Does it make sense to normalize vectors after PCA for cosine distance?

I start off with word2vec embeddings and process them in the following way: Standardize dimensions to mean 0 and standard deviation of 1 PCA to keep the top k-dimensional eigenvector, whereby whitening is active Normalize eigenvectors to unit length as described in https://stackoverflow.com/questions/41387000/cosine-similarity-of-word2vec-more-than-1 When I then calculate cosine distances between the resulting vectors I still get values greater than 1, which shouldn't be the case? So I am wondering if the normalization to unit length that I am doing in step 3 makes sense at all, after having done the previous steps? If it does make sense, do you have any ideas what could be the cause for getting cosine distance values greater than 1? I am using sklearn for all steps.
