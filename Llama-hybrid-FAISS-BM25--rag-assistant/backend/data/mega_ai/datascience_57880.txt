[site]: datascience
[post_id]: 57880
[parent_id]: 57878
[tags]: 
Typically, neural nets are trained using the backpropagation algorithm. The algorithm searches for optimal weights by making small adjustments in the direction opposite the gradient. Computing the gradient requires a differentiable loss function, so you cannot train a network with backpropagation if your loss function is not differentiable. However, there are other optimization algorithms you can try. Genetic algorithms were once commonly used to find weights for a neural network, and they can be used with virtually any loss function. GAs are also pretty easy to code from scratch. If you want to learn more, this is a pretty accessible blog post (with code), and here is a more in-depth paper on the topic.
