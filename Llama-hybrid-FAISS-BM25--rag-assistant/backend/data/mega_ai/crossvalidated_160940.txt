[site]: crossvalidated
[post_id]: 160940
[parent_id]: 160933
[tags]: 
For me, the point about Bayesian methods is that the parameter $\theta$ is uncertain, and its uncertainty (or degrees of belief about it taking different possible values) can be treated using the axioms of probability. This means it is possible to update beliefs about the distribution of $\theta$ by using the observed data $\mathbf{x}$ by combining the prior distribution for $\theta$ with the likelihood of observing $\mathbf{X}=\mathbf{x}$ given $\theta$ to find a posterior distribution for $\theta$ given $\mathbf{X}=\mathbf{x}$. Again, for me, the hyperparameter $\alpha$ represents the underlying assumptions about $\theta$ which, although they may or may not be true, do not change in the analysis. Writing $p(\theta \mid \alpha)$ rather than $p(\theta)$ for the prior distribution is a useful reminder of these assumptions. I do not particularly like the Wikipedia "formal" description. In particular I would distinguish the density for $\mathbf{X}$ more from the prior and posterior distributions for $\theta$, perhaps by writing $f(\mathbf{x}\mid \theta, \alpha)$ for the conditional distributions of the observations (proportional to the likelihood for the parameter given the observations) rather than $p(\mathbf{x}\mid \theta)$, and $\pi_0(\theta \mid \alpha)$ for the prior rather than $p(\theta \mid \alpha)$. This then leads to the posterior distribution of $\theta$ given the observed data being $$\pi(\theta \mid \mathbf{x},\alpha) = \dfrac{\displaystyle \pi_0(\theta \mid \alpha) f(\mathbf{x}\mid \theta, \alpha) } {\displaystyle \int_{\theta \in \Theta} \pi_0(\theta \mid \alpha) f(\mathbf{x}\mid \theta, \alpha)\, d\theta } $$ which you could rewrite in measure-theoretic terms if you actually wanted to be formal. It might also be helpful to avoid using $\theta$ as a free variable in the numerator and a bound variable in the denominator. If it is helpful to you, you might conceive $\displaystyle \int_{\theta \in \Theta} \pi_0(\theta \mid \alpha) f(\mathbf{x}\mid \theta, \alpha)\, d\theta$ as in some sense $p( \mathbf{x}\mid \alpha)$ after integrating out $\theta$ (or summing it out if it takes discrete values). You might then think of the main expression as a manipulation of something like $p( \mathbf{x}\mid \alpha) \pi(\theta \mid \mathbf{x},\alpha) =p( \mathbf{x}, \theta \mid \alpha) = \pi_0(\theta \mid \alpha) f(\mathbf{x}\mid \theta, \alpha)$. Personally I do not find this particularly useful (and it can tempt you to follow it down the rabbit hole to largely meaningless distractions). Instead I prefer to concentrate on checking that $\displaystyle \int_{\theta \in \Theta} \pi(\theta \mid \mathbf{x},\alpha)\, d\theta =1$.
