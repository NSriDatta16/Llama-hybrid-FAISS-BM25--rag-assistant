[site]: datascience
[post_id]: 108262
[parent_id]: 108261
[tags]: 
There are a couple of common options: Fine-tuning: Given new data (both new tokens and changes in data distribution), unfreeze existing model weights and do additional model training. Almost all of the models are trained with stochastic gradient descent (SGD) which supports online learning. Use sub-word features: Include fragments of language in the model. For example if "ing" is in the model, when new words appear that end in "ing" there is high likelihood they are novel verbs. The most extreme example of sub-word features is including individual characters in the model.
