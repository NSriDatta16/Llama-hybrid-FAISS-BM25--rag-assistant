[site]: crossvalidated
[post_id]: 338449
[parent_id]: 338441
[tags]: 
As soon as it starts to avoid it, it will no longer encounter that event again and so it can't reinforce its learning. This is the not the case always.If we talk about any Reinforcement Learning algorithm e.g Q-Learning, there is a choice for action selection called Explore-Exploit Dilemma (or $\epsilon$-greedy policy) during training period. In this, at every state Agent may either choose to exploit and select the best action it has learnt over the period or choose to explore and select any random action. So, it is also possible that Agent will select a bad action again due to exploration, in that case Agent will reinforce its learning about that bad action. Decision of explore-exploit is taken based on a random variable and this is applicable for training phase only, during testing as you wrote, Agent will avoid bad things and will always choose the best action it has learnt during training. So, with the use of methods like exploit-explore, Agent or machine do learn good as well as bad things during training. Edit : As @neilslater wrote in the comment, Experience-replay is another way in which Agent learns about bad state-action pair, without visiting that state-action pair multiple times. In Experience-replay, Agent's experiences are stored in memory and Agent is trained again on that stored batch of experiences.
