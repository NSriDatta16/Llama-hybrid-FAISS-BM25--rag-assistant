[site]: crossvalidated
[post_id]: 24796
[parent_id]: 24753
[tags]: 
Caveat : I'm not particularly well-versed in Granger causality, but I am generally statistically competent and I have read and mostly understood Judea Pearl's Causality , which I recommend for more info. Is my interpetation directionaly correct Yes. The fact that first hypothesis was rejected and second was not means that you can use $X$ to forecast $Y$. What key insights have I overlooked The really important thing to know in terms of key insights is that Granger-causation is only equivalent to causation (in the more common use of the term) under a fairly restrictive assumption, viz, that there are no other potential causes. If this assumption is not satisfied then Granger-causality is actually Granger-usefulness-for-forecasting. For example, if there is a variable $Z$ that causally influences both $X$ and $Y$, then the conclusion that $Y$ Granger-causes $X$ can be explained as the influence of $Z$ being felt in $Y$ before it's felt in $X$. The p-value of .76 allows me to accept the null for X = f(Y) Warning: esoteric bullshtatistical blathering follows. Technically, in the test of $X = f(Y)$ you can't "accept the null". You can "fail to reject the null" -- that is, you didn't find evidence that would warrant rejecting the null. This is the Fisherian view. Alternatively, you can take the Neymanian view: you don't assert the truth of the null; you just choose to act as if the null were true. (Personally I'm a Jaynesian , but let's not get into that .) I'm a little rusty on my F-test The point of the F-test is that it checks that the lagged values of $X$ jointly improve the forecast of $Y$ (or vice versa). One can imagine predicting $Y$ with two predictors $X_1$ and $X_2$ where $X_2$ is just $X_1$ with a bit of added noise. The F-test would compare a model with just $X_1$ (or just $X_2$) with the model containing both and find no evidence of improved prediction in the larger model. I'm also not sure how to interpret the CCF graph The plots of the auto-correlation and cross-correlation functions provide a rough graphical equivalent to the t-tests used in the testing procedure. In order to understand what is being plotted, it's first necessary to understand correlation as a measure of the linear relationship between two random variables. The cross-correlation function is just the correlation of one time series versus a lagged version of the other, and the auto-correlation is just the cross-correlation of a function and itself. Thus these plots show the time structure of the strength of the linear relationships both internally (auto) and from one to the other (cross). I can see from the autocorrelation plots, for example, that $Y$ is reasonably smooth but has no other particularly strong internal structure, whereas $X$ has an oscillation with a peak-to-peak period of about 120 time steps (because it is negatively correlated with itself at about 60 time steps).
