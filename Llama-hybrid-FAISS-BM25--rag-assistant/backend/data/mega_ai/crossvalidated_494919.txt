[site]: crossvalidated
[post_id]: 494919
[parent_id]: 494133
[tags]: 
To the best of my knowledge, such a modification of the weights in Bayesian Model Averaging to take the similarity (or other relations) between models into account does not exist in the literature. According to me, the main reason is that the problem your are raising (and that you nicely illustrated in your example) should be corrected at the level of models selection, and not at the level of model averaging. As far as I know, a characterization of the "similarity" of the models does not exist, and would anyway be difficult to define. Even a notion as simple and widely used as "nestedness" lacks a rigorous definition in the literature ( reference ) (although we proposed a definition in this recent paper ). Different models might have the same prediction, while being greatly different in their structure and nature. If a phenomenological, a normative, and a physical models all agree on the same prediction, then the evidence for the said prediction is very high, and these models "deserve" to have an important weight in your model averaging (even if they have the same prediction). In your example, the problem is from the choice of the models, not the averaging itself. The family of models 1, 1a, 1b, 1c, 2 is ill-defined : it is like sampling only a small part of your population (around model 1), which will lead to a biased result. However, apart from heuristically checking if your proposed family of models is sound, I don't think there exists (yet) a quantitative criterion or method to avoid this pitfall.
