[site]: crossvalidated
[post_id]: 582684
[parent_id]: 123619
[tags]: 
I have faced the same problem trying to predict a single emotion in the RAVDESS dataset. The thing that helped me is: to provide the model with the initial bias and weights; in this way, the model takes care of the class differences through data. You can setup good initialization bias as follows $$ b_0 = \log_e\left(\text{# negative labels}\right)\\ b_1 = \log_e\left(\text{# positive labels}\right) $$ and good initialization weights for the output layer as follows $$ w_0 = \frac{1}{2} \cdot \frac{\text{# total samples}}{\text{# negative labels}}\\[15pt] w_1 = \frac{1}{2} \cdot \frac{\text{# total samples}}{\text{# positive labels}} $$ where $w_0$ is the weight for the negative class and $w_1$ for the positive one. The meaning is that a better bias initialization helps the initial convergence, instead, a good weight initialization helps because you don't have very many of those positive (negative) samples to work with, so you would want to have the classifier heavily weight the few available examples. You can plug the output_bias values inside the model as follows: b0 = np.log(neg) b1 = np.log(pos) output_bias = tf.keras.initializers.Constant([b0, b1]) ... model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(2, activation="softmax", bias_initializer=output_bias)) and the initial class_weights in this way: ... weight_for_negative = (1 / neg) * (total / 2.0) weight_for_positive = (1 / pos) * (total / 2.0) class_weight = {0: weight_for_negative, 1: weight_for_positive} ... model_history = model.fit(x_traincnn, y_train, batch_size = 128, epochs = 800, validation_data = (x_validcnn, y_valid), #callbacks = [mcp_save, lr_reduce, early_stopping, backup]) callbacks = [mcp_save, lr_reduce, early_stopping, tensorboard], class_weight = class_weight) Hope this will help. Tips: I recommend check also fscore, precision, and recall metrics to better interpret the model. Bibliography: TensorFlow Blog - Imbalanced data classification
