[site]: crossvalidated
[post_id]: 467904
[parent_id]: 467888
[tags]: 
Broadly speaking, Bayesian analyses satisfy the so-called likelihood principle , which means that all the information about parameters $\theta$ from an experiment that observed $X^\star$ is contained in the likelihood $$ L(\theta) \equiv p(X^\star | \theta), $$ which is crucially only evaluated at the observed $X^\star$ . Contrast this with the sampling distribution, $p(X|\theta)$ as a distribution in $X$ . Crucially the data isn’t fixed to the observed value, and we instead consider this as a distribution in $X$ . Take for example the posterior, $$ p(\theta|X^\star) \propto p(X^\star | \theta) \pi(\theta). $$ It doesn’t depend on $p(X|\theta)$ anywhere other than at $X=X^\star$ . So we would find the same posterior distribution for any sampling distribution $f$ so long as $f(X^\star|\theta) =p(X^\star|\theta)$ . The posterior depends on the likelihood function, but not the entire sampling distribution. Whilst the fundamental rules of Bayesian inference satisfy the likelihood principle, a few ideas violate it. For example, a few formal rules for constructing priors, e.g., so called reference priors and Jeffreys priors, use the likelihood function evaluated at all possible experimental outcomes (i.e., they use the sampling distribution). A few hybrid ideas, like the posterior and prior $p$ -value, also violate it. I suppose ABC methods require the sampling distribution, but only as a means to ultimately approximate the likelihood at the observed data. So, with a few exceptions, yes Bayesian statistics bypasses the need for the sampling distribution.
