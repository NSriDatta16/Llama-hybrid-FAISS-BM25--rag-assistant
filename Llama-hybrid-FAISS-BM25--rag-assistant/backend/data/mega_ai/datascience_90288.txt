[site]: datascience
[post_id]: 90288
[parent_id]: 
[tags]: 
Using the curse of dimensionality for encoding non-ordered (nominal) categorical variables of high cardinality

When the dimension is high, all data are approximately at the same distance away from each other. This makes distance-based methods such as k-nearest neighbors less useful if the data are more or less uniformly distributed. This is also referred to as the curse of dimensionality. However, why not to make lemonade out of this dimensionality lemon? Why not to encode nominal categorical variables of high cardinality with randomly distributed points in some high-dimensional space but such that the number of dimensions is much less than we would get with the one-hot encoding? As the distance between any two points is almost the same, no artificial ordering will be introduced. The encoding is very fast. No need to calculate embedding with some neural network. Will this encoding work? Will it be meaningless or introduce any bias? Is anybody using it? Do you know any references?
