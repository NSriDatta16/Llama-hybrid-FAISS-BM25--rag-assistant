[site]: crossvalidated
[post_id]: 620023
[parent_id]: 584390
[tags]: 
I have argued in many places, such as here , in favor of viewing $R^2$ as a comparison of the square loss incurred by your model to the square loss of a model that predicts the pooled mean $\bar y$ every time. To condense the notation, I want to give two definitions. $$ SSRes = \overset{N}{\underset{i=1}{\sum}}\left( y_i-\hat y_i \right)^2\\ SSTotal = \overset{N}{\underset{i=1}{\sum}}\left( y_i-\bar y \right)^2 $$ Then we can view $R^2$ as follows, where the third of the fractions can be viewed as a ratio of the residual and total variance . $$ R^2 = \left(\dfrac{ SSTotal - SSRes }{ SSTotal } \right)= 1-\left(\dfrac{ SSRes }{ SSTotal }\right) = 1-\left(\dfrac{ SSRes/N }{ SSTotal/N }\right) $$ If you take $+\sqrt{R^2}$ , then you wind up with $\sqrt{1-\left(\dfrac{ SSRes/N }{ SSTotal/N }\right)}$ . If you want to view a statistic as comparing the standard deviations of the residuals and the total standard deviation, then the square root should be applied to the numerator and denominator separately. $$ 1-\left(\dfrac{ \sqrt{SSRes/N} }{ \sqrt{SSTotal/N} }\right)=1-\sqrt{\dfrac{ SSRes/N }{ SSTotal/N }} $$ I cannot think of when I have seen this done, but it does not seem outrageous, as long as you are clear about your non-standard calculation and why you want to know this particular value. However: $$1-\sqrt{\dfrac{ SSRes/N }{ SSTotal/N }} \ne \sqrt{1-\left(\dfrac{ SSRes/N }{ SSTotal/N }\right)}$$ A simulation shows this. set.seed(2023) N Consequently, taking $+\sqrt{R^2}$ might not mean quite what you want it to mean. Finally, the sign of $\sqrt{R^2}$ is ambiguous. It makes sense in a simple linear regression when there is just one feature $x$ and $\operatorname{cor}(x,y)$ makes perfect sense. However, if you have multiple features and regression coefficients with both positive and negative signs, the sign of $\sqrt{R^2}$ does not make sense to me. If you want to consider the correlation between the predicted and observed values of $y$ , there are issues with that, as it can miss discrepancies between the predicted and observed values that $R^2$ , as it is defined above, does not miss . However, if you do find yourself in a situation where the correlation between predicted and true values is less than zero (it can happen in some of the more exotic modeling techniques than OLS linear regression with an intercept), you know something is not right about your predictions.
