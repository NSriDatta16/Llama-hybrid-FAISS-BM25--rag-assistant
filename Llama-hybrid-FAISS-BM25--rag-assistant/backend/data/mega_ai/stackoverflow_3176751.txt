[site]: stackoverflow
[post_id]: 3176751
[parent_id]: 3176576
[tags]: 
You want a new program that takes as input one or more criteria, then outputs an estimate of the running time or memory usage. This is a machine learning problem. Your inputs can be listed as a vector of numbers, like this: input = [ A, B, C, D, E ] One of the simplest algorithms for this would be a K-nearest neighbor algorithm . The idea behind this is that you'll take your input vector of numbers, and find in your database the vector of numbers that is most similar to your input vector. For example, given this vector of inputs: input = [ 11, 1.8, 3557, 29, 10 ] You can assume that the running time and memory should be very similar to the values from this run (originally in your table listed above): R0001 12 2 3556 27 9 There are several algorithms for calculating the similarity between these two vectors, one simple and intuitive such algorithm is the Euclidean distance . As an example, the Euclidean distance between the input vector and the vector from the table is this: dist = sqrt( (11-12)^2 + (1.8-2)^2 + (3557-3556)^2 + (27-29)^2 + (9-10)^2 ) dist = 2.6533 It should be intuitively clear that points with lower distance should be better estimates for running time and memory usage, as the distance should describe the similarity between two sets of criteria. Assuming your criteria are informative and well-selected, points with similar criteria should have similar running time and memory usage. Here's some example code of how to do this in R: r1 = c(11,1.8,3557,29,10) r2 = c(12,2.0,3556,27, 9) print(r1) print(r2) dist_r1_r2 = sqrt( (11-12)^2 + (1.8-2)^2 + (3557-3556)^2 + (27-29)^2 + (9-10)^2 ) print(dist_r1_r2) smarter_dist_r1_r2 = sqrt( sum( (r1 - r2)^2 ) ) print(smarter_dist_r1_r2) Taking the running time and memory usage of your nearest row is the KNN algorithm for K=1. This approach can be extended to include data from multiple rows by taking a weighted combination of multiple rows from the database, with rows with lower distances to your input vector contributing more to the estimates. Read the Wikipedia page on KNN for more information, especially with regard to data normalization, including contributions from multiple points, and computing distances. When calculating the difference between these lists of input vectors, you should consider normalizing your data. The rationale for doing this is that a difference of 1 unit between 3557 and 3556 for criteria C may not be equivalent to a difference of 1 between 11 and 12 for criteria A. If your data are normally distributed, you can convert them all to standard scores (or Z-scores) using this formula: N_trans = (N - mean(N)) / sdev(N) There is no general solution on the "right" way to normalize data as it depends on the type and range of data that you have, but Z-scores are easy to compute and a good method to try first. There are many more sophisticated techniques for constructing estimates such as this, including linear regression, support vector regression, and non-linear modeling. The idea behind some of the more sophisticated methods is that you try and develop an equation that describes the relationship of your variables to running time or memory. For example, a simple application might just have one criterion and you can try and distinguish between models such as: running_time = s1 * A + s0 running_time = s2 * A^2 + s1 * A + s0 running_time = s3 * log(A) + s2 * A^2 + s1 * A + s0 The idea is that A is your fixed criteria, and sN are a list of free parameters that you can tweak until you get a model that works well. One problem with this approach is that there are many different possible models that have different numbers of parameters. Distinguishing between models that have different numbers of parameters is a difficult problem in statistics, and I don't recommend tackling it during your first foray into machine learning. Some questions that you should ask yourself are: Do all of my criteria affect both running time and memory usage? Do some affect only one or the other, and are some useless from a predictive point of view? Answering this question is called feature selection , and is an outstanding problem in machine learning . Do you have any a priori estimates of how your variables should influence running time or memory usage? For example, you might know that your application uses a sorting algorithm that is N * log(N) in time, which means that you explicitly know the relationship between one criterion and your running time. Do your rows of measured input criteria paired with running time and memory usage cover all of the plausible use cases for your application? If so, then your estimates will be much better, as machine learning can have a difficult time with data that it's unfamiliar with. Do the running time and memory of your program depend on criteria that you don't input into your estimation strategy? For example, if you're depending on an external resource such as a web spider, problems with your network may influence running time and memory usage in ways that are difficult to predict. If this is the case, your estimates will have a lot more variance.
