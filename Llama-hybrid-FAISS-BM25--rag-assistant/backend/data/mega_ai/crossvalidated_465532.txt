[site]: crossvalidated
[post_id]: 465532
[parent_id]: 465483
[tags]: 
Can you allow $b$ to be any soft policy? Yes, it works from a theoretical stand-point, because a soft policy must have some probability of choosing each action, so there will always be some, maybe small, probability of matching the observed trajectory to a trajectory that the target policy would produce. Estimates can be made accurate in the long term using any soft policy as source data. In practice, $b$ is often the $\epsilon$ -greedy policy with respect to current Q, for the reasons you argue. Assuming $\epsilon$ is relatively low (maybe 0.1 or 0.01) that would mean that the loop typically does run for a range of useful trajectory lengths at the end of each episode. There is indeed a practical concern of choosing $b$ in some way close to $\pi$ for efficient learning. This is also the case, for similar reasons, for all other off-policy algorithms. Some of them may appear to learn more from an inner loop when making exploratory actions, but they all learn most efficiently for some relatively low degree of exploration. Although many off-policy algorithms do process and update values from parts of trajectory that off-policy Monte Carlo Control does not, it is not always clear that the updates will be useful - for instance they might refine values in some part of the state space where an optimal agent would never find itself in practice. Or, perhaps more saliently when comparing with Monte Carlo Control, they might refine values in a biased way because there is no data available yet about what happens when acting under the target policy from a certain state.
