[site]: datascience
[post_id]: 126381
[parent_id]: 126380
[tags]: 
The large number you are seeing is not the maximum length, but the maximum representable integer at that precision. It's there because no maximum length has been set. The original GPT-2 has a maximum length of 1024, and german-gpt2 is just a retraining of the original architecture on German text, so it should be the same. Anyway, you can make sure checking model.config.max_position_embeddings , which, as per you comments, is also 1024. You should indeed care about the truncation. When training, usually you want to use as much text as possible, so normally you split very large text chunks into different samples.
