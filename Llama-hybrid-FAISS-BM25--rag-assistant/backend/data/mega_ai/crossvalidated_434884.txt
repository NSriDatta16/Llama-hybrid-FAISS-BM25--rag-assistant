[site]: crossvalidated
[post_id]: 434884
[parent_id]: 215049
[tags]: 
The origin is that you have to be worried about overfitting with generalized linear models when you have too many predictors per record (or per case in the most rare class in case of time-to-event, logistic regression or softmax-regression). An initial rule of thumb was 10 or more cases per record, but for some situations that may be too much of a requirement . The point is that below some point point estimates are substantially exagerrated, standard errors do not do what you hope etc. and that predictions might be less reliable as a result. However, this rule of thumb primarily serves as a warning for when you may be entering dangerous territory with a (non-pentalized) GLM fit using maximum likelihood. To overcome that, you can of course use regularization and still GLMs (e.g. LASSO logistic regression, elastic net etc.) in such situations (especially if you are primarily after predictions), as long as you tune your hyperparameters sensibly (e.g. cross-validation). Of course, there are plenty of other methods (such as Bayesian methods, gradient boosting, neural networks and so on) that all have their particular ways of achieving some form of regularization (e.g. priors, tuning the number of trees, dropout etc.), which - when used approrpiately - allow us to do useful things with sparse (relative to the amount of predictors) data.
