[site]: crossvalidated
[post_id]: 446955
[parent_id]: 
[tags]: 
Problem in correctly predicting time series using RNN

I have implemented a basic time-series forecasting model using RNN with TensorFlow. I have just understood and copy-pasted the tutorial from the web. The web example was working perfectly fine. However, in my case, as I can understand, the code is picking up the pattern just fine, (not very great even at that), but the forecasting is completely flawed. (please see the figure). Can someone suggest what corrective measures should I try to take? The relevant part of the code is below. r_neuron = 120 # 1. Construct the tensors X = tf.placeholder(tf.float32, [None, n_windows, n_input]) y = tf.placeholder(tf.float32, [None, n_windows, n_output]) ## 2. create the model basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=r_neuron, activation=tf.nn.relu) # Now that the network is defined, you can compute the outputs and states rnn_output, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32) stacked_rnn_output = tf.reshape(rnn_output, [-1, r_neuron]) stacked_outputs = tf.layers.dense(stacked_rnn_output, n_output) outputs = tf.reshape(stacked_outputs, [-1, n_windows, n_output]) ## 3. Loss + optimization learning_rate = 0.001 loss = tf.reduce_sum(tf.square(outputs - y)) optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) training_op = optimizer.minimize(loss) init = tf.global_variables_initializer() iteration = 4000 start_time = time.time() with tf.Session() as sess: init.run() for iters in range(iteration): sess.run(training_op, feed_dict={X: X_batches, y: y_batches}) mse = loss.eval(feed_dict={X: X_batches, y: y_batches}) if iters % 150 == 0: mse_150 = mse print(iters, "\tMSE:", mse) y_pred = sess.run(outputs, feed_dict={X: X_test})
