[site]: crossvalidated
[post_id]: 535937
[parent_id]: 483871
[tags]: 
Here is a minimal example based on two normal distributions (built based on the answers already exist in this thread): import scipy.stats import scipy.spatial import numpy as np # Start with two normally distributed samples # with identical standard deviations; # The two means are 2 standard deviations away from each other x1 = scipy.stats.norm.rvs(loc=0, scale=1, size=1000) x2 = scipy.stats.norm.rvs(loc=2, scale=1, size=1000) # Construct empirical PDF with these two samples hist1 = np.histogram(x1, bins=10) hist1_dist = scipy.stats.rv_histogram(hist1) hist2 = np.histogram(x2, bins=10) hist2_dist = scipy.stats.rv_histogram(hist2) X = np.linspace(-4, 6, 10) Y1 = hist1_dist.pdf(X) Y2 = hist2_dist.pdf(X) # Obtain point-wise mean of the two PDFs Y1 and Y2, denote it as M M = (Y1 + Y2) / 2 # Compute Kullback-Leibler divergence between Y1 and M d1 = scipy.stats.entropy(Y1, M, base=2) # d1 = 0.406 # Compute Kullback-Leibler divergence between Y2 and M d2 = scipy.stats.entropy(Y2, M, base=2) # d2 = 0.300 # Take the average of d1 and d2 # we get the symmetric Jensen-Shanon divergence js_dv = (d1 + d2) / 2 # js_dv = 0.353 # Jensen-Shanon distance is the square root of the JS divergence js_distance = np.sqrt(js_dv) # js_distance = 0.594 # Check it against scipy's calculation js_distance_scipy = scipy.spatial.distance.jensenshannon(Y1, Y2) # js_distance_scipy = 0.493 The difference between the KL-divergence-derived JS distance and scipy's JS distance may have been caused by the very coarse binning used to construct the empirical PDF.
