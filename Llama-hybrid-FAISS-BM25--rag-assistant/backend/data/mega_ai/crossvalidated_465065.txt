[site]: crossvalidated
[post_id]: 465065
[parent_id]: 
[tags]: 
VC dimension upper bound of linear kernel SVM

In the book Statistical learning theory by Vapnik, a theorem is presented regarding the maximal VC dimension of a separating hyperplane classifier (that is, in in a SVM setting). A subset of separating hyperplanes $W$ defined on $X\subset\mathbb R^n$ , where $\|x\|\leq D$ , $x\in X$ , satisfying the constraint $\|w\|\leq A$ for all $w\in W$ has its VC dimension $h$ bounded by $$h\leq\min(D^2A^2, n)+1.$$ Now, let's say we're using linear kernel classifiers with some feature map $\phi$ , that is $$w=\sum_{i=1} ^T\alpha_i\,\phi(x_i)$$ where $x_i$ is from the training set containing $T$ samples, and hence our classification function is $$f(x)=\sum_{i=1} ^T\alpha_i\,\phi(x_i)\cdot\phi(x)+b$$ (omitting the usual sign decision function for now). Question : assuming $\|\phi(x)\|=1$ for all $x$ (this is relevant for the context I'm considering this problem in, but I'll spare you the details), the norm of $w$ is given by $$\|w\|^2=\sum_{ij}\alpha_i\alpha_j\,\phi(x_i)\cdot\phi(x_j)$$ and assuming also $T\ll n$ we can bound the VC dimension as $$h\leq(\max_i|\alpha_i|)^2\,T^2.$$ However when we can assure that all $\phi(x_i)$ are orthogonal to each other, this changes to $$h\leq(\max_i|\alpha_i|)^2\,T\tag{1}$$ since the cross terms vanish. What could be an (intuitive) explanation for this discrepancy? Is it just that the bounds are not tight, or is there something deeper going on? In fact, the bound being higher seems contradictory to me: if all feature vectors were parallel, the decision space would be reduced to an affine line, with $h=2$ . P.s. Any pointers to literature answering this question would be appreciated, of course.
