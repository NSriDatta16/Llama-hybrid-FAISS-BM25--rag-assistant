[site]: crossvalidated
[post_id]: 355604
[parent_id]: 274563
[tags]: 
To develop the point made by Andreas: No weak learner can achieve an error rate better (i.e. lower) than 0.5 in the first round, hence it should be Î±t=0 for all t, making AdaBoost (with decision stumps) fail to solve the XOR problem. Indeed in this particular (degenerate) case the AdaBoost algorithm using the following settings will fail: Estimator = decision tree Max_depth = 1 Max_leaf_nodes 2 The point is that Adaboost is not limited to Decision trees , and can be used together with many other weak estimators such as Logistic Regression Support Vector Machines etc.. In general, AdaBoost is often used with Decision Trees, in which case the tree parameters are adjusted to the problem at hand. You could, in this example, increase the maximum depth to 2 in which case the issue disappears. In Sklearn , the default estimator is a Decision tree with unlimited max depth and 2 leaf_nodes
