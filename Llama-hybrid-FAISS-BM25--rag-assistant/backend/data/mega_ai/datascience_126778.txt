[site]: datascience
[post_id]: 126778
[parent_id]: 116705
[tags]: 
You raise a good point, the same number of parameters in a shallow network perform less than a deeper one! My theory is that it is about the optimization methodology like Backpropagation or SGD. Also here is the AI answer 1- Feature Hierarchy: Deeper neural networks excel at constructing a hierarchy of features, starting from simple and progressing to complex. Reference: LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436-444. This landmark paper describes how deep learning models, particularly convolutional neural networks (CNNs), learn a hierarchy of features, from simple to complex, through their layers. 2- Model Capacity and Expressiveness: Despite having the same number of parameters, deeper networks can leverage their depth to increase their representational power. Reference: Goodfellow, I., Bengio, Y., Courville, A., & Bengio, Y. (2016). Deep learning (Vol. 1). MIT press Cambridge. This comprehensive textbook explains the concept of model capacity in the context of deep learning, highlighting how deeper networks can express more complex functions. 3- Optimization Dynamics: Deeper networks alter the landscape of the optimization problem in ways that can facilitate learning Reference: Sutskever, I., Martens, J., Dahl, G., & Hinton, G. (2013). On the importance of initialization and momentum in deep learning. In Proceedings of the 30th International Conference on Machine Learning (pp. 1139-1147). This paper discusses the challenges and strategies related to the optimization of deep neural networks, including the role of initialization and momentum. 4- Efficient Parameter Utilization: Deep networks can often utilize their parameters more efficiently than shallow ones for certain tasks. Reference: Montufar, G. F., Pascanu, R., Cho, K., & Bengio, Y. (2014). On the number of linear regions of deep neural networks. In Advances in neural information processing systems (pp. 2924-2932). This work investigates the capacity of deep networks to partition input space into linear regions, illustrating the efficient use of parameters in deep architectures. 5- Implicit Regularization: Adding more layers can introduce an implicit form of regularization, as the network must propagate signals through many transformations, encouraging the learning of more generalized features. Reference: Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1), 1929-1958. Although this paper primarily introduces dropout as a regularization technique, it discusses the concept of regularization in deep learning, including implicit forms such as layer depth.
