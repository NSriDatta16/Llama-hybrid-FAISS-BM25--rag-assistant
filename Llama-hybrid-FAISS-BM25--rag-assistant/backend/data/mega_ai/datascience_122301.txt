[site]: datascience
[post_id]: 122301
[parent_id]: 
[tags]: 
When training a sklearn machine learning model, what part of a data from a csv file needs scaling like MaxAbsScaler or MinMaxScaler?

Consider the code below: from sklearn.model_selection import train_test_split from sklearn.preprocessing import MinMaxScaler import pandas as pd data = pd.read_csv("data.csv") x = data.drop("Price", axis = 1) y = data.Price xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 0.2, random_state = 1) It simply reads a csv file and splits it into inputs and outputs, for training and testing, which can now be used to train a price prediction model using some algorithm like Random Forest or KNN or Linear Regression or something else. But let's say I have a feeling that scaling the data (like for example using MinMaxScaler ) will yield better results. So do I scale the entire data . Or do I scale xTrain or xTest or yTrain or yTest ? If yTrain and/or yTest are to be scaled, then the prices predicted would be between 0 and 1 which is not very useful. How do I then use the model to predict actual prices? I have been trying for a few days now, but I can't find a decent explanation on what part of a data is actually scaled and why?
