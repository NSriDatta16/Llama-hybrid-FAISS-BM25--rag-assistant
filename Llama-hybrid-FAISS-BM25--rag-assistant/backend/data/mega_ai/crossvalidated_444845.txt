[site]: crossvalidated
[post_id]: 444845
[parent_id]: 444781
[tags]: 
I think I understand your confusion. Typically, Bayes' rule is written as: $$p(\theta |y) = \frac{ p(y|\theta) p(\theta)}{p(y)}$$ where $p(\theta |y)$ is the posterior for the observed data $y$ given unknown parameters $\theta$ , $p(\theta)$ is the prior distribution, and $p(y)$ is the marginal distribution of $y$ . As far as Bayes' rule is concerned, $p(y)$ is a constant since it doesn't depend on the unknown parameters, so this simplifies to: $$p(\theta |y) \propto p(y|\theta) p(\theta).$$ Now some would refer to $p(y|\theta)$ as the likelihood function. Technically, the likelihood is a function of $\theta$ for fixed data $y$ , say $L(\theta |y)$ . However, the liklelihood is proportional to the sampling distribution, so $L(\theta |y) \propto p(y|\theta)$ . In other words, $p(y|\theta)$ isn't technically the likelihood, but it is proportional to it, and as far as applying the Bayesian methodology is concerned, the distinction is not important. Hence why it is often referred to as the likelihood.
