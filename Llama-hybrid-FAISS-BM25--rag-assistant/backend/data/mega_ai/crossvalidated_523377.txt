[site]: crossvalidated
[post_id]: 523377
[parent_id]: 523348
[tags]: 
The reason has to do with the phrasing of the paper and the nature of the problem itself. By theorem, that result had to happen. When I was an academic, I used to do a seminar for the math club on how your choice of axioms determined your solution in the field of probability and statistics. What you are seeing is the result of axioms conflicting with the nature of the problem. If you are placing money at risk, then you must use a Bayesian method. There is no choice on the matter. The relevant issue is the Dutch Book Theorem and its converse. In 1930, Bruno de Finetti realized that if you could force a bookie to take a loss due to the rules of probability that you were using, they wouldn’t play the game. So, using that as the grounding principle, that a bookie won’t play in a game of “heads you win, tails I lose,” he asked what rules of math follow from that. The result is the Dutch Book Theorem. Lehman and Kemeney proved the converse, independently, around 1955. The implication of the converse was that Frequentist methods will not, with a handful of narrow exceptions, produce valid gambling odds. While it is necessary to use Bayesian methods, it is not sufficient. In the general case, you cannot use uninformative priors. You cannot mimic Frequentist methods if you want to gamble. Each method of solving problems has been designed to be optimal for certain classes of problems but can be very ill-designed for others. To consider a type of problem where this outcome is reversed, remember President Trump’s obsession with hydroxychloroquine. Preliminary work on hydroxychloroquine that I saw showed that it was somewhat effective if you looked in a Bayesian framework but not a Frequentist framework. As evidence mounted, the Frequentist method ended being correct. In a sense, Frequentist methods are prejudiced in that the null is treated as true. As it is ill-advised to put deadly chemicals in your body, medicine treats proposed treatments as either producing no effect or being harmful upon administration. Most Frequentist methods minimize the maximum amount of risk you will be exposed to from getting an unfortunate sample. Getting an unfortunate initial sample is what happened with hydroxychloroquine. As the sample size grew, the two methods converged, but based on initial samples, the two methods disagreed. The Bayesian probabilities were based on the observed data. The Frequentist probabilities required overwhelming evidence because it was concerned with the entire sample space that could be out there instead of just the observed sample. That makes the method very conservative. Even with Frequentist methods, it is inevitable that you will be made a fool of sometimes. However, by choosing a cutoff probability, usually called $\alpha$ , you get to control the maximum frequency with which you will be a fool. It is better to think of the frequencies obtained under null hypothesis methods as depending on the model you chose and a loss or utility function. You shouldn’t use Bayesian methods to solve Frequentist problems, generally. You also shouldn’t use Frequentist methods to solve Bayesian problems, generally.
