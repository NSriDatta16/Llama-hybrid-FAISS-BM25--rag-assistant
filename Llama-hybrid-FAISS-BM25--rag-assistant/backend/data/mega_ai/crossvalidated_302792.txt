[site]: crossvalidated
[post_id]: 302792
[parent_id]: 
[tags]: 
GAMs vs GLMs with feature engineering - is there a practical difference?

I recently came across this tutorial on General Additive Models (GAMs). Quoting the article: The principle behind GAMs is similar to that of regression, except that instead of summing effects of individual predictors, GAMs are a sum of smooth functions. Functions allow us to model more complex patterns, and they can be averaged to obtain smoothed curves that are more generalizable. Is a GLM with smooth predictors (i.e., independent variables) practically any different from a GLM with smooth functions? Does smoothness actually matter? The most competitive data scientists (e.g., those on Kaggle) say that when everyone is competing with the same modeling packages, what sets the best data scientists apart from the rest is the ability to do feature engineering well. Quoting this article : With the extensive amount of free tools and libraries available for data analysis, everybody has the possibility of trying out advanced statistical models in a competition. As a consequence of this, what gives you most “bang for the buck” is rarely the statistical method you apply, but rather the features you apply it to. By feature engineering, I mean using domain specific knowledge or automatic methods for generating, extracting, removing or altering features in the data set. Back to the original article, the term "function" sounds vague. To someone without applied GAM experience, such as myself, "function" sounds like a synonym for a predictor that has simply gone through feature engineering or transformation. This leads me to believe that if I am already doing feature engineering (and if I'm also using GLMs) for a predictive model, then I likely won't get any additional predictive benefit using GAMs. Is that a fair assumption?
