[site]: crossvalidated
[post_id]: 605870
[parent_id]: 605795
[tags]: 
The fact that this distance is going down in your case is a good thing. In general, for two samples with empirical distributions $\mathbb P_n$ and $\mathbb Q_m$ , we have have $$ W(\mathbb P_n, \mathbb Q_m) \to W(\mathbb P, \mathbb Q) $$ as $m, n \to \infty$ . Here, $\mathbb P$ and $\mathbb Q$ are the population versions of the two distributions. In your simulations, the two samples are are coming from the same population $\mathbb P = \mathbb Q = N(0,1)$ , hence $W(\mathbb P, \mathbb Q) = 0$ . As $n,m \to \infty$ , you should expect $W(\mathbb P_n, \mathbb Q_m)$ to approach zero. For you time series, if they are coming from two different populations, $W(\mathbb P, \mathbb Q)$ will be nonzero, so you will converge to something nonzero for large samples. In other words, you want $W(\mathbb P_n, \mathbb Q_m)$ to vary with $m$ and $n$ to get more accurate for larger samples. In general (in the 1-D case), $$\mathbb E[ W(\mathbb P_n, \mathbb Q_m) ] = W(\mathbb P,\mathbb Q) + O\Bigr(\frac1{\sqrt{n \wedge m}}\Bigl).$$ Normalizing $W(\mathbb P_n, \mathbb Q_m)$ to be invariant to the sample size does not make much sense, especially when $\mathbb P = \mathbb Q$ . In case you are curious where that rate comes from, first note that by triangle inequality $$W(\mathbb P_n, \mathbb Q_m) \le W(\mathbb P_n, \mathbb P) + W(\mathbb P, \mathbb Q) + W(\mathbb Q, \mathbb Q_m).$$ Then, use a result like that of Fournier and Guillin on the first and third terms. EDIT1: The same problem is there with the absolute value of Pearson correlation coefficient: import numpy as np np.random.seed(1337) def corrr(n): a = np.random.randn(n) b = np.random.randn(n) c = np.corrcoef(a,b)[0][1] return c for n in [10, 100, 1000, 10000]: print(np.mean([abs(corrr(n)) for r in range(1000)])) which produces the following output: 0.2709428042954994 0.07715058968038786 0.023744887030041146 0.008094465791386103
