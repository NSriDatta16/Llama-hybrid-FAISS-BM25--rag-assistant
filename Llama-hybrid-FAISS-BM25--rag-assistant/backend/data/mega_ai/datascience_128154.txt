[site]: datascience
[post_id]: 128154
[parent_id]: 128105
[tags]: 
From my testing on 100,000+ models throughout a few hundred different problems, I have found the following: Automate your testing across model types and architecture. Also, ensure you have something to track your runs and compare values. (MLFlow, WANDB, etc.) Adding layers will not always help; sometimes, increasing layer weights, regularization, and recurrence have far more impact. There are numerous other techniques, including custom activation functions, for example, that can yield considerable performance improvements. I have seen single-layer, seemingly simplistic models, tuned appropriately, perform as well as complex multi-head attention stacks. It all depends on the problem. Datasets and scaling also significantly contribute to your results. Take time to identify the best approach to your problem. Try different scalers, try other features, see what performs consistently on test data, and measure the difference between the train/validation and test datasets. Use statistical plots to view the deviation and see what metrics inform your model accuracy for the problem at hand. There is no one-size-fits-all approach, at least not yet. Trial and error is the way. Automate the tests, refine, and tune the results. Once you have enough data from runs, you will understand what works well for each input sequence and correlation for the problem you are trying to solve.
