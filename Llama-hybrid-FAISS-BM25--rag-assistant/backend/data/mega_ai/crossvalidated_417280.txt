[site]: crossvalidated
[post_id]: 417280
[parent_id]: 
[tags]: 
the accuracy of covariance between two high-dimensional vectors

Question Is the covariance between high-dimensional vectors less accruate than covariance between two vectors in low-dimensional vecotrs? I am asking this questio to check if there is a need for 'dimensionality reduction ' before coming up with covariance between 2 vectors. Question in more detail For example, lets say we have two daily-return vectors of two companies : Apple and Microsoft . The table below shows synthesized daily-return vectors that I created, just for example. AAPL MSFT 1/1 +0.003 +0.005 1/2 +0.001 +0.002 1/3 +0.005 +0.008 1/4 -0.002 -0.004 1/5 -0.005 -0.009 1/8 -0.002 +0.003 1/9 -0.005 -0.006 1/10 -0.004 +0.001 1/11 +0.005 -0.003 1/12 +0.008 -0.008 My goal is to coming up with more accurate covariance between two companies . We have 10 day-long vectors(10 dimensional) 2 vectors above. Would it be more accurate if we calculate the covariance between these 2 vectors after dimensionality reduction such as PCA or autoencoder ? The reason I am asking you this question is because of the 'curse of dimensionality' . I have read many articles or research papers saying curse of dimensionality in euclidean distances and cosine similarity. As you can see in the Swiss roll picture below, the euclidean distance is not the best distance metric sometimes, in the high-dimensional space. It is also true for cosine similarity because it also uses euclidean distance to calculate the similarity. However, I cannot find any meaningful article or research paper dealing with Covariance between two vectors in high-dimensional space . I would really appreciate it if anyone sheds a light on this subject. You can name research papers, for example, as well.
