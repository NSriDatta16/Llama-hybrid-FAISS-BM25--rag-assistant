[site]: crossvalidated
[post_id]: 629374
[parent_id]: 
[tags]: 
How (or can) you formulate the Fisher information matrix in terms of a loss function, specifically cross-entropy loss?

I recently saw the following formulation of the Fisher information matrix in a paper on Transformer pruning : $$ \mathcal{I} := \frac{1}{|D|} \sum_{(x,y) \in D} \left( \frac{\partial \mathcal{L}(x,y;1)}{\partial m}\right) \left(\frac{\partial \mathcal{L}(x,y;1)}{\partial m} \right)^T $$ $\mathcal{L}(x, y; 1)$ here refers to the loss function. However, the usual formulation of the Fisher information matrix, stolen from Wikipedia, is: $$ \bigl[\mathcal{I}(\theta)\bigr]_{i, j} = \operatorname{E}\left[\left. \left(\frac{\partial}{\partial\theta_i} \log f(X;\theta)\right) \left(\frac{\partial}{\partial\theta_j} \log f(X;\theta)\right) \,\, \right| \,\,\theta\right]. $$ Where $f(X;\theta)$ is the parametrized function for the model. I'm not sure how these formulations are equivalent. While it seems to be presented as fact regardless of the specific loss function in the paper, for what it's worth the loss used is cross-entropy. I've tried deriving one from the other, but I'm not really able to.
