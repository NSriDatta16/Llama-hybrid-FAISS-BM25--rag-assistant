[site]: crossvalidated
[post_id]: 161511
[parent_id]: 
[tags]: 
What is a compact vector equation expression the back-propagation algorithm for convolution neural networks?

I was reading the lecture notes for sparse auto-encoders from Andrew Ng and was saw that it had very nice compact way of expressing back propagation for neural networks: The really nice thing about this explanation is that its compactly expressed using operations with vectors and matrix multiplication (and dot products and the Hadamard product $\cdot$, that simply denotes the element-wise product operator). However, I was wondering if there was a way to similarly express the above equations for convolutional neural nets but instead of matrix multiplication with the convolution $*$ operator. I think that its possible to re-derive it, however, since CNNs are so popular these days and they seem to be widely used, it seemed reasonable to assume that this really wasn't an open problem and that there ought to be something like this.
