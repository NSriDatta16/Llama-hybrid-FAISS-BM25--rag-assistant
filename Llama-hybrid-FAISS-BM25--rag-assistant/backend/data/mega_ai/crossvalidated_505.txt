[site]: crossvalidated
[post_id]: 505
[parent_id]: 490
[tags]: 
Metropolis scanning / MCMC Select few features randomly for a start, train classifier only on them and obtain the error. Make some random change to this working set -- either remove one feature, add another at random or replace some feature with one not being currently used. Train new classifier and get its error; store in dE the difference the error on the new set minus the error on the previous set. With probability min(1;exp(-beta*dE)) accept this change, otherwise reject it and try another random change. Repeat it for a long time and finally return the working set that has globally achieved the smallest error. You may extend it with some wiser control of beta parameter. Simpler way is to use simulated annealing when you increase beta (lower the temperature in physical analogy) over the time to reduce fluctuations and drive the algorithm towards minimum. Harder is to use replica exchange .
