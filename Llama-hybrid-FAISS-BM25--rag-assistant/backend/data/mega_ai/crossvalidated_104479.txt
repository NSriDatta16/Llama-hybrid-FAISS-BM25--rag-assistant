[site]: crossvalidated
[post_id]: 104479
[parent_id]: 34357
[tags]: 
Q-learning also permits an agent to choose an action stochastically (according to some distribution). In this case, the reward is the expected reward given that distribution of actions. I think this fits your case above. Q-learning also permits actions that may fail. Hence, $Q(s, Left)$ might lead you to a state $s'$ that is not the to the left $s$ (e.g. the action "fails" with some probability). In that case, the model (MDP, table of Q-values, automaton) will encode the possibility of failure directly and no distributions or expected values are needed.
