[site]: crossvalidated
[post_id]: 501230
[parent_id]: 483785
[tags]: 
Adding to the elaborate answer of @Tim: VAE z latent is stochastic z, meaning samples will be different for a same $x_i$ sample. In the ideal case you latent representation ( $\mu$ or z) will contain meaningful information, these are the ones I would extract (btw in tensorflow you can extract multiple layers ;) ). $\sigma$ is established to act as a noise component. To make sure that what you extract is useful what you can do is an interpretability test. The Ct-scans input features are the pixels you have, BUT is there some other information you are not using ? For example since you don't give explicit labels do you have some a scan image of a sick patient ? Or could you select 10 images by hand with some specific feature to interpret a bit what neurons are triggered in the latent space? If so what you can do is a correlation test. This can be as simple as neyman pearson or a 2d histogram showing how correlated features are. What you want to achieve in this case is some sense of what is being used from the model to decide. Unfortunately this is easy for cats/docs and harder for more complex datasets, but it's something that you need to do to not have a black box machine. Good luck!
