[site]: crossvalidated
[post_id]: 449936
[parent_id]: 187224
[tags]: 
I'm barely a student in his internship, so I don't have a lot of experience, but I am facing a similar issue with a project I'm having at my work. I use an extremely unorthodox method, but one cannot just do 5000 forecasts manually. The way I would approach this is as follows: I created 2 functions that choose best model fits for SARIMA and ets() wrapper to find best models for each family according to AIC(I use MAPE). I choose the model fits that score best from each of the model family(SARIMA/ETS) and perform out of sample forecasting Then, I compare the out of sample accuracy measures(MAPE/RMSE) between the 2 and choose the best scoring one with its parameters or orders. Ultimately, I forecast the desired horizon When new data comes in, my code automatically takes in consideration the newest data points and thus, updates the models. My method does not take in consideration assumptions. I know this is quite counter-intuitive, but I'm creating a score or so to filter out unsatisfactory forecasts. Although there are more than 5000 different time series, it is known that in many cases the data is either missing or consists of unpredictable series, hence, expectations are not high. My data is imported from SQL Server to RStudio. Here, the code splits each data set into lists of train and test sets that will be used for choosing a model. After choosing the best model, I perform the forecast which will be ultimately pushed into SQL Server to be used for reporting/visualizations.
