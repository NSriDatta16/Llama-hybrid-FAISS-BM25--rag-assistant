[site]: crossvalidated
[post_id]: 217806
[parent_id]: 217798
[tags]: 
The short answer is that likelihood theory exists to guide us towards optimum solutions, and maximizing something other than the likelihood, penalized likelihood, or Bayesian posterior density results in suboptimal estimators. Secondly, minimizing sum of squared errors leads to unbiased estimates of true probabilities. Here you do not desire unbiased estimates, because to have that estimates can be negative or greater than one. To properly constrain estimates requires one to get slightly biased estimates (towards the middle) in general, on the probability (not the logit) scale. Don't believe that machine learning methods do not make assumptions. This issue has little to do with machine learning. Note that an individual proportion is an unbiased estimate of the true probability, hence a binary logistic model with only an intercept provides an unbiased estimate. A binary logistic model with a single predictor that has $k$ mutually exclusive categories will provide $k$ unbiased estimates of probabilities. I think that a model that capitalizes on additivity assumptions and allows the user to request estimates outside the data range (e.g., a single predictor that is continuous) will have a small bias on the probability scale so as to respect the $[0,1]$ constraint.
