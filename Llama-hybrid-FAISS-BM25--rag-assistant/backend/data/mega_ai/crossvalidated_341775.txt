[site]: crossvalidated
[post_id]: 341775
[parent_id]: 
[tags]: 
Rationale for different activation function neural network pretraining vs. supervised training?

I was reading a paper that used neural networks to predict protein conformation, Improving prediction of secondary structure, local backbone angles, and solvent accessible surface area of proteins by iterative deep learning : Linear activation function was used for the hidden layers of auto encoder training whereas sigmoid activation function was employed at the stage of back propagation. Little unclear, but I interpret this to mean they did their sparse autoencoder parameter initialization training with a linear activation, then did the supervised training with a sigmoidal activation. What's the rationale for this? I assumed that NN weights were only meaningful in the context of a particular activation function, and the whole point of the autoencoder-y initialization was to make the weights relevant for the supervised training...?
