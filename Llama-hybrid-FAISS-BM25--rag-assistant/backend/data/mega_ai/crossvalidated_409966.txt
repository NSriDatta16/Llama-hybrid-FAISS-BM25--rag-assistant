[site]: crossvalidated
[post_id]: 409966
[parent_id]: 409960
[tags]: 
Typically neural network weights are initialized at random (for example: Xavier Initialization - Formula Clarification ) while the biases are initialized at 0. Gradient descent applies updates of the form $$x^{(k+1)} = x^{(k)} - \eta \nabla f(x^{(k)})$$ where ${}^{(k)}$ indicates that this is the $k$ th iteration of the procedure and $\eta$ is the learning rate. Stochastic gradient descent only uses a fraction of the data to estimate $\nabla f(x^{(k)})$ . Gradient descent is an imperfect tool. Some discussion: For convex problems, does gradient in Stochastic Gradient Descent (SGD) always point at the global extreme value? How can change in cost function be positive? Why can't a single ReLU learn a ReLU?
