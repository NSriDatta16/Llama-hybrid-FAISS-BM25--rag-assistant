[site]: crossvalidated
[post_id]: 349012
[parent_id]: 348983
[tags]: 
The answer is no: no such (nearly) free lunch can exist, even if the model is perfect. Consider for instance a linear regression using $n$ observations $[x_i,\,Y_i]$. Given a "new" design point $x^\star$ you can compute a $80\%$ prediction interval; It contains one new $Y^\star$ corresponding to $x^\star$ with probability of $80\%$. But it does not contain $80\%$ of a large number of new values $Y^\star_j$ made at $x^\star$, even if the model is perfectly adequate. As suggested by the formulation of your question, you can not guess exactly the distribution of an infinite number of new values from a finite number of observations. To reach the wanted probability, you have to draw a new dataset with observations $[x_i, \, Y_i]$ for each prediction that you make. This is well explained by G.W. Senedecor and W.G. Cochran in the regression chapter of their famous Statistical Methods book. An alternative where the expected coverage rate holds is when the prediction is updated sequentially, thus modifying the prediction interval: the first new couple $[x^\star_j, \,Y^\star_j]$ is included in the data, then the model estimates and the prediction interval are updated before a new prediction is made. Again, this must be repeated for each prediction, and the coverage will be reached only in the long run. This context is classical in time-series analysis.
