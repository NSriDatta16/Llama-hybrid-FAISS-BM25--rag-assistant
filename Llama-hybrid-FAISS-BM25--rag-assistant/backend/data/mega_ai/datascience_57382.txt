[site]: datascience
[post_id]: 57382
[parent_id]: 56676
[tags]: 
Here's an expansion on my comment. To preface, absolutely @DanScally is right that there's no reason to use ML for finding a maximum of a list. But I think your "it might give me an understanding of what machine learning can do in general" is good enough reason to delve into this. You ask about more general machine learning, but I'll focus on neural networks. In that context, we must first ask whether the actual functions produced by a neural network can approximate (or evaluate exactly) $\max$ , and only then can we further inquire whether any of the (common?) training methods can fit a NN approximating $\max$ . The comments, and @MachineLearner's answer brought up universal approximation theorems: on a bounded domain , a neural network can approximate any reasonably nice function like $\max$ , but we can't expect a priori to approximate $\max$ on arbitrary input, nor to exactly calculate $\max$ anywhere. But, it turns out that a neural network can exactly sort arbitrary input numbers. Indeed, $n$ $n$ -bit integers can be sorted by a network with just two hidden layers of quadratic size. Depth Efficient Neural Networks for Division and Related Problems , Theorem 7 on page 955; many thanks to @MaximilianJanisch in this answer for finding this reference. I'll briefly describe a simplification of the approach in that paper to produce the $\operatorname{argmax}$ function for $n$ arbitrary distinct inputs. The first hidden layer consists of $\binom{n}{2}$ neurons, each representing the indicator variable $\delta_{ij} = \mathbf{1}(x_i , for $i . These are easily built as $x_j-x_i$ with a step activation function. The next layer has $n$ neurons, one for each input $x_i$ ; start with the sum $\sum_{j i} (1-\delta_{ij})$ ; that is, the number of $j$ such that $x_i>x_j$ , and hence the position of $x_i$ in the sorted list. To complete the argmax, just threshold this layer. At this point, if we could multiply, we'd get the actual maximum value pretty easily. The solution in the paper is to use the binary representation of the numbers, at which point binary multiplication is the same as thresholded addition. To just get the argmax, it suffices to have a simple linear function multiplying the $i$ th indicator by $i$ and summing. Finally, for the subsequent question: can we can train a NN into this state. @DanScally got us started; maybe knowing the theoretical architecture can help us cheat into the solution? (Note that if we can learn/approximate the particular set of weights above, the net will actually perform well outside the range of the training samples.) Notebook in github / Colab Changing things just a little bit, I get better testing score (0.838), and even testing on a sample outside the original training range gets a decent score (0.698). Using inputs scaled to $[-1,1]$ gets the test score up to 0.961, with an out-of-range score of 0.758. But, I'm scoring with the same method as @DanScally, which seems a little dishonest: the identity function will score perfectly on this metric. I also printed out a few coefficients to see whether anything close to the above described exact fit appears (not really); and a few raw outputs, which suggest the model is too timid in predicting a maximum, erring on the side of predicting that none of the inputs are the maximum. Maybe modifying the objective could help, but at this point I've put in too much time already; if anyone cares to improve the approach, feel free to play (in Colab if you like) and let me know.
