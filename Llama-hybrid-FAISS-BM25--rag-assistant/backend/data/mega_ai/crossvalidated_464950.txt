[site]: crossvalidated
[post_id]: 464950
[parent_id]: 464906
[tags]: 
The short answer is that such a regression is completely meaningless. See this question for a great discussion of it. Basically, since $y$ is always the same value, you don't observe any variation in it, and so intuitively you will always find that the intercept is equal to $y$ , and all the other $\beta = 0$ . Basically, you conclude that changing $x$ has no effect on $y$ . But this is a special case of that, which can happen when $y$ is just independent of $x$ , as any variable you used instead of $x$ would yield the exact same result. Furthermore, as the answers in that question reveal, there's really no concept of $R$ or $R^2$ in this case. Now as to why you may observe an $R^2$ using statistical software, this is most likely because such software numerically solves for regressions, and so you won't exactly get that $\beta = 0$ , but rather that $\beta \approx 0$ . In particular, using using the example of R's code, the documentation for lm calculates R squared as $$R^2 = \frac{mss}{mss+rss}$$ where $mss = \sum_i(y_{i,fit} - \bar{y})^2$ and $rss = \sum_i(y_{i,fit} - y_i)^2$ . But in your example, both of these values are incredibly small (due to approximation) but non-zero and equal as $y = \bar{y}$ , and so you have $mss = rss$ and so $$R^2 = \frac{mss}{mss+rss} = 1/2 =.5$$ which is what I suspect many software might conclude. So what's going on? Well in this case, R used the equality that $mss + rss = tot$ , where $tot = \sum_i ((y_i - \bar{y})^2)$ . So instead of calculating $tot$ to get the denominator of $R^2$ , it instead used $mss + rss$ , but in this case, $tot = 0$ but $mss + rss \neq 0$ due to approximation issues, but again fundamentally the concept of $R^2$ doesn't exist here, and so these equations fail to hold. See below for an example reproducing the problem in R: n = 1000 y = rep(1,n) x = rnorm(n, mean = 100) reg = lm(y~x) r = reg $residuals f = reg$ fitted.values mss = sum((f - mean(f))^2) rss UPDATE (after comments) Let's now consider forcing no intercept, and asked in the comments of this post by OP. First, the intuitive purpose of this regression would be to recover identified information of $X$ without needing $Y$ , and so the regression would be 'useless' in that sense: all the information would be contained in the data $X$ without $Y$ . This should make sense: the observed $Y$ is a constant. On a more fundamental sense, let's look at the model you're considering. Sticking to the simple linear regression case, you're saying that the underlying model is $y = f(x) = \beta x + \epsilon$ , and so $E[y|x=0] = 0$ , but you told me $E[y|x = 0] = y = 1$ as $y$ is a constant, so your model is already mis-specified. Anyway, in the simple linear regression case, we have that without intercept, $y = \beta x$ and $$\hat{\beta} = \frac{\bar{xy}}{\bar{x^2}} = \frac{\bar{x}}{\bar{x^2}}$$ where the last equality follows because $y = 1$ for all observations. You also asked that $\bar{x} = 0$ , so $\hat{\beta} = 0$ in your example regardless, unless $\bar{x^2} = 0$ , in which case it's undefined. So you know exactly what $\hat{\beta}$ is, and what it's identifying, and so the interpretation of a regression is equivalent to just studying $X$ on it's own. As for why various statistical software gives various $R^2$ , I'd check documentation and go from there knowing what you identify. I'd also be remiss not to link this post on the pitfalls of $R^2$ .. at least when I first learned about $R^{2}$ , I certainly gave it way more attention than I should have, thinking it's somehow the 'ideal' measure of a regression.
