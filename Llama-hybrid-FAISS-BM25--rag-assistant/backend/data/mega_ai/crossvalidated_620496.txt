[site]: crossvalidated
[post_id]: 620496
[parent_id]: 
[tags]: 
How to combine a noisy (but unbiased) estimate with a precise (but possibly biased) estimate in A/B tests?

Suppose I want to estimate some set of unknown quantities $\theta_1$ , …, $\theta_N$ . For each $i \in \{1, …, N\}$ , I have two estimators: $\hat{\theta_i}_A$ and $ \hat{\theta_i}_B$ . The goal is to combine these estimators to form an "optimal" estimator, where there is some discretion in defining what "optimality" mean. Ultimately, we need to tradeoff bias and variance. We don't know the distributions of $\hat{\theta_i}_A$ and $\hat{\theta_i}_B$ . However, we know that the means and variances take the following forms (true for all i): $$E(\hat{\theta_i}_A) = \theta_i$$ $$V(\hat{\theta_i}_A) = \sigma_A^2$$ $$E(\hat{\theta_i}_B) = \theta_i + \beta$$ $$V(\hat{\theta_i}_B) = \sigma_B^2$$ $\sigma_B^2 . The variances are known. $\beta$ is unknown. N is not very large — let’s say N is between 15 and 35. In other words, $\hat{\theta_i}_A$ is unbiased and but has variance that is meaningfully greater than $\hat{\theta_i}_B$ . We don't know whether $\hat{\theta_i}_B$ is biased ( $\beta$ is probably close to 0). Stakeholders have a strong desire to avoid estimates that are “too biased,” although they can’t formulate that worry precisely. Current practice is to just use $\hat{\theta_i}_A$ , but there is a general desire to take advantage of $\hat{\theta_i}_B$ to reduce the uncertainty in the estimates. What are some candidates for the “best” way to combine $\hat{\theta_i}_A$ and $\hat{\theta_i}_B$ to get a final estimate for each $\theta_i$ ? Feel free to supply your own definition of “best” or to supply assumptions that you feel are missing. I am looking to survey a variety of reasonable solutions. One way would be to choose a weighted average $$ \hat{\theta_i} = p \hat{\theta_i}_A + (1-p) \hat{\theta_i}_B $$ to minimize MSE -- we'd have to estimate the MSE for $\hat{\theta_i}_B$ to do this. A second version of the the problem is to assume multiplicative instead of additive bias for $ \hat{\theta_i}_B $ — in practice we don’t know whether the bias is additive or multiplicative. I’d be interested in comments on this variation, as well. Here's an example where this could arise: you have a retail website that sells shoes. You want to test N different variations of the website to see whether the variations increase the number of purchases that occur in a week. You can run A/B tests to test the variations. You can compare actual purchases in each experiment arm, but purchases are rare and estimates will be noisy. You have an ML model that predicts purchases, so you could also compare predicted purchases. This will be much less noisy, but the ML model could be biased and stakeholders are afraid to rely on an evaluation based on modeled (rather than actual) outcomes.
