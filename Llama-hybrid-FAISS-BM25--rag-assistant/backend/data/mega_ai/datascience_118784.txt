[site]: datascience
[post_id]: 118784
[parent_id]: 118771
[tags]: 
To add a bit more to @noe 's answer: when you have a small number of features, explainable models can do a lot for you because they usually operate by making a prediction directly using the input features, without any intermediate features. When the data is structured and the number of features is small there isn't much value in choosing a more complex model while losing explainability. With a large number of features that changes. You have two issues. Models that make predictions directly from input data no longer have simple explanatory values. Absent feature engineering, it is probably best to use a more powerful model that can use a smaller number of secondary features in its decision. For example, a two-layer neural network is essentially two layers of logistic regression. So you can still analyze which secondary features are useful for the final layer. Then you can analyze to see what those features correlate within your data set.
