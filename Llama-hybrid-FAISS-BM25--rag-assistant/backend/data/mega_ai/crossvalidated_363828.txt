[site]: crossvalidated
[post_id]: 363828
[parent_id]: 276643
[tags]: 
My suggestion is about meeting point of statistical mechanics and machine learning. Others may suggest meeting points of machine learning and other disciplines. Boltzmann Machines are stochastic graphical models inspired from statistical mechanics. Therefore, you can encounter with "energy" term while working with probabilistic graphical models. It is a scalar term that indicates the configuration of graphical models, and it is a metaphor from physics. I think evolution map of Boltzmann Machines starts from Ising Model [1]. It is a mathematical model for electron spins in atoms, and the definition of energy term is used for descendant graphical models (including Boltzmann Machines). The second stop is Hopfield Networks [2]. They are deterministic model since state transitions of nodes depend on energy difference between two states. The model memorizes input data by using weights within nodes (all of them are visible, no hidden nodes are introduced for this type of model). The last stop is Boltzmann Machines (BM) [3]. They are stochastic counterpart of Hopfield Network. Energy difference between two states determines the probability of state transition. Hidden nodes are introduced in BM so that more representation power is available. [1] https://page.mi.fu-berlin.de/rojas/neural/chapter/K13.pdf (I think that it is excellent book to understand intuition behind graphical models.) [2] http://www.its.caltech.edu/~bi250c/papers/Hopfield-1982.pdf (Original paper by John Hopfield) [3] https://www.enterrasolutions.com/media/docs/2013/08/cogscibm.pdf (Original paper from Ackley and Hinton, the original paper describes all formulas of Boltzmann Machines, you should check appendix part of the paper.)
