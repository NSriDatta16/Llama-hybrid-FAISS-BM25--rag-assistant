[site]: crossvalidated
[post_id]: 298456
[parent_id]: 
[tags]: 
Variable Selection and Model Likelihood

Can I derive any (Bayesian) statements about the probability of a parameter given the data and the marginal likelihood of models containing this parameter? For instance, consider a regression setup $$Y_t=X_t\beta_k + \varepsilon_{t,k}$$ where $X\in\mathbb{R}^{N}$. Data $(Y, X)$ is the same for every model, models differ only with respect to the prior for $\beta_k$: $\pi^k(\beta)=\prod_i^N\pi_i(\beta_i)$ where for some elements $\pi^k_i$ is a point mass at $0$ (the parameter is forced to be 0, or, in other words - it is not included in the regression). The prior for the error term is always centered at $0$. For different models $\mathcal{M}_k$ we can compute posterior model likelihoods $\pi(\mathcal{M}_k|Y,X)$, being identical to the marginal likelihood if we assume flat priors for the model. Assume I computed the model likelihoods for all $2^N$ distinct models, leaving me with a set of $2^{N-1}$ models containing parameter $\beta_k$ for every $k=1,\ldots,N$. Is there some proper way to compute something like $$p(\beta_i\neq0|D) \propto \sum\limits_{i=1}^{2^N}\pi(\mathcal{M}_i|Y,X)\mathbb{1}_{\pi_i(\beta_i)\neq 0}$$ I apology for the potential abuse of notation I just don't now how whether this idea makes sense at all.
