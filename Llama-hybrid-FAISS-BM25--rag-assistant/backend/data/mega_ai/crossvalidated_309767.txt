[site]: crossvalidated
[post_id]: 309767
[parent_id]: 
[tags]: 
For a linear model, $E[y | x,z]=\alpha + \beta x + \gamma z$, the partial derivative with respect to $x$ is $$\frac{\partial E[y | x]}{\partial x} = \beta, $$ so that the coefficient has a direct interpretation as a marginal effect of $x$ on $y$. For nonlinear regression models, this interpretation is no longer possible. For example, if $E[y | x]=\exp \left(\alpha + \beta x + \gamma z \right)$ like in the Poisson model for count data, the marginal effect is a function of both parameters and regressors: $$\frac{\partial E[y | x]}{\partial x} = \exp \left(\alpha + \beta x + \gamma z \right)\cdot \beta$$ It is customary to present such marginal effects evaluated at own values of $x$ and $z$ and averaged for all individuals in the sample (average marginal effect or AME) evaluated at the mean/median/modal values of $x$ and $z$ (marginal effect at representative values or MER) estimated at specific values that are interesting to the analyst These three measures will generally differ in nonlinear models, whereas they will agree in a linear one. Moreover, the sign of the marginal effect may change at different values of regressors: it may be positive for some values of $x$ and negative for others. Similar complication arise for interactions between variables. Finally, for binary ($0/1$) regressors in nonlinear models, many prefer the finite difference rather than the derivative:$$ \Delta E[y | x]=E[y | x=1]-E[y | x=0]$$ (where of course all other quantities that might be involved in evaluating the expectations are held constant).
