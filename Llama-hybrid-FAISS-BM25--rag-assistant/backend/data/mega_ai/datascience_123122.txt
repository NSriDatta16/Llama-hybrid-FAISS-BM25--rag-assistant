[site]: datascience
[post_id]: 123122
[parent_id]: 123117
[tags]: 
Normalization doesn't actually lose any information, since it's only scaling the variable into the rage from 0 to 1 or -1 to 1. But there could be few cases which could be affected: When we normalize a variable, it discards some details about the original scale which might have some meaningful information. For example, suppose you have a dataset of grayscale images where pixel values range from 0 to 255. If you normalize these pixel values to be between 0 and 1, you're essentially reducing the contrast in your images. Now imagine that there are certain features in your images that can only be distinguished at higher contrasts - for instance, subtle differences in shading that distinguish one object from another. By normalizing the pixel values (and thereby reducing contrast), those features might become indistinguishable. In this case, even though normalization doesn't technically "lose" any data (since it's just rescaling it), it does lose important information about contrasts and shades which were crucial for distinguishing certain features in the image. Normalization(MinMaxScalar) or Standardization(StandardScaler) is highly sensitive to outliers so if there are extreme outlier values in your dataset, these can skew the normalized results so that most 'normal' data points appear very close together. A common issue could be after normalization, features will be on different scales than originally measured which may make interpretation less intuitive, especially when communicating findings with non-data scientists. It could not be the problem all the time if you are not dealing with Stakeholders or SMEs.
