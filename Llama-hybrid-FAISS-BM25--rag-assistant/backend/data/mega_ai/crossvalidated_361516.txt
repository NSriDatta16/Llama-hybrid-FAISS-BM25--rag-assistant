[site]: crossvalidated
[post_id]: 361516
[parent_id]: 
[tags]: 
How to handle big vocabulary size with keras tokenizer?

I am actually working on a neural language model developed with keras. I have an encoder and a decoder and the output of the decoder is a dense vector on the vocabulary ..so quite big depending on the vocabulary. It's like a multi-class classification problem where each class is a word. I have 440K unique words in my data and I use the tokenizer provided by Keras. So when creating my target vector data with np.zeros() (sparse matrix, one hot encoded) that is (length of sequence * number of sequence * size of vocabulary), I am facing a memory problem. Do you how can I handle this ? Thank you Reduce size of the training batches Reduce size of vocab Change the output shape Try character level language model Other idea
