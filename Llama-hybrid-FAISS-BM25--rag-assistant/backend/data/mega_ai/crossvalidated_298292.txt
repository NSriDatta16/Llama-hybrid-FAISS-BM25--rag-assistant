[site]: crossvalidated
[post_id]: 298292
[parent_id]: 298190
[tags]: 
@NULL is right that this is a general question that isn't specific to your use case. Let me supplement his answer a little. What you really need to do in any given situation is think very hard about what you want to do and why. You have a situation where you want to build a model with response $Y$, but you believe that a set of $X$-variables will be highly correlated. That may not be a problem. First, why do you think the variables are correlated? Are they all measures of the same underlying construct? Are they partly overlapping, but partly distinct? Are some of them causally related to others? Second, what are your long-run goals? Do you just want to describe the data (their distributions and relationships)? Are you exploring the data to generate hypotheses for future research? Do you want to test a hypothesis? About what, one of the correlated variables relative to another, or about what they have in common, or about something completely unrelated (these are controls and the variable of interest is uncorrelated with them)? Do you want to to make a predictive model? Who is going to be using this to make predictions? What data will they have (e.g., will they have some of the variables, but not others)? Is it likely that values of the different variables will diverge in future cases? Your answers to the questions above will guide how you deal with the variables. Here are some possible strategies: If you want to describe the data, just do so. The collinearity is part of the results to be described. Good exploratory data analysis to generate hypotheses is hard. Try a lot of different models (e.g., different combinations of variables) and think deeply about them. Which are plausible? What would it mean, substantively, if one or another were the true data generating process? Don't only consider the point estimates of your betas, but also compute the models that would result from the true values being towards the extremes of the confidence intervals. If you think these $X$-variables are all just different manifestations of a single latent variable (which is somewhat implied by your description), you could: Combine them (for example, you could run a factor analysis , or maybe a PCA). Or possibly, use all of them. Extracting factors will inevitably leave out some information (and hopefully the measurement error); using all of them will guarantee every nuance is captured. To test them, drop all the correlated $X$-variables as a group and perform a nested model test. Or possibly, just pick one $X$-variable at random if the correlations are close enough to $r = 1.0$. At that point there is little to be gained by bothering with other strategies. If you think the $X$-variables are partly overlapping, you could perform a factor analysis and extract $>1$ factor, or use the $X$-variable you see as most directly measuring the main idea and residualizing the rest so that the resulting set are orthogonal. There are various ways to deal with causally related $X$-variables, and which to use will depend on the nature of the causal pattern you suspect and what you are trying to do. That said, the default approach might be to model the relations among the $X$-variables and $Y$ using something like structural equation modeling. A test of an extracted central factor from your $X$-variables used as an explanatory variable in a multiple regression model will be a good test of the information shared in common amongst your $X$-variables. If you want to test something about two (or more) correlated $X$-variables vis-a-vis each other, that is going to be very difficult to do, and you are just in a difficult situation. On the other hand, if you want to test an exposure completely unrelated to the correlated $X$-variables, just go ahead. The multicollinearity won't have any effect on the test of interest. If you are trying to create a predictive model, consider who will use the model to make predictions, in what situation, and what data will they most likely have access to. If they are likely to have $X_3$, but not $X_1, X_2, X_4,$ or $X_5$, use $X_3$. Predicted means aren't terribly affected by collinearity, so if that's all you care about and the correlations are likely to be similar when the model is used to make predictions, you should be fine. Conversely, if future data may occur in the regions of the $X$-variable space that aren't represented in your dataset, thar be dragons . Making a variety of different models and using model averaging may provide some limited safeguard.
