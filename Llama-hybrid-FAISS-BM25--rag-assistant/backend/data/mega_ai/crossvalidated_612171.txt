[site]: crossvalidated
[post_id]: 612171
[parent_id]: 411676
[tags]: 
The McFadden pseudo $R^2$ takes the stance that the extension of the usual $R^2$ to logistic regression should be $1-\left(\dfrac{L_1}{L_0}\right)$ , where $L_1$ is the log-likelihood of your model and $L_0$ is the log-likelihood of an intercept-only model that predicts the mean every time (see, for instance, UCLA on this). If you apply this idea to a Gaussian likelihood instead of binomial, you wind up with the following. $$ 1-\left(\dfrac{ \overset{N}{\underset{i=1}{\sum}}\left( y_i-\hat y_i \right)^2 }{ \overset{N}{\underset{i=1}{\sum}}\left( y_i-\bar y \right)^2 }\right) $$ The numerator is the log-likelihood of your model, and the denominator is the log-likelihood of an intercept-only model that always predicts the mean $\bar y$ . The above expression winds up being equal to $\text{corr}(y, \hat y)$ in the OLS linear regression case. Further, the expression above represents a comparison of the likelihood (or mean squared error) of your model relative to that of a baseline model, and if you cannot beat the baseline model on a metric of interest, that suggests trouble. Predicting $\bar y$ every time makes sense as a baseline model because, for a model that is supposed to predict conditional means, what better na√Øve prediction of the conditional mean than the overall mean? There are issues with summarizing model performance with just one value, as is suggested in the comments, and Cross Validated has an answer showing why $R^2$ can be high despite a clearly incorrect model. However, $ 1-\left(\dfrac{ \overset{N}{\underset{i=1}{\sum}}\left( y_i-\hat y_i \right)^2 }{ \overset{N}{\underset{i=1}{\sum}}\left( y_i-\bar y \right)^2 }\right) $ is a totally reasonable statistic to calculate, and you can do it for your model.
