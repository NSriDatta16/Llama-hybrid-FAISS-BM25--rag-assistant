[site]: crossvalidated
[post_id]: 285695
[parent_id]: 285622
[tags]: 
For every neural network architecture out there: during backpropagation, all weights get processed and thus possibly modified. So the 'wrongness' of every neuron in a neural network gets calculated (aka error). With this 'wrongness', we can see how much every input connection was responsible for that 'wrongness'. The weights get updated so they are less wrong in the next forward propagation.
