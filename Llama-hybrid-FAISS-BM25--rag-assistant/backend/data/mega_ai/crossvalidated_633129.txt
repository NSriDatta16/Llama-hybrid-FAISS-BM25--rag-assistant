[site]: crossvalidated
[post_id]: 633129
[parent_id]: 633091
[tags]: 
Contrary to popular belief (including beliefs implicit in another answer ), linear regressions can handle extremely complicated relationships between variables, including curves and interactions. In that regard, a linear regression should be able to handle (basically) anything that a seemingly more sophisticated model like a support vector regression can do. To bring some mathematical rigor to this argument, consider the Stone-Weierstrass theorem. A possible drawback to the linear model is that these relationships need to be specified for the linear regression to fit them. If you don't tell the linear model to consider an interaction between variables, for instance, it will not. For better or for worse, complex machine learning models like support vector regressions (random forests, neural networks, etc) go and figure out these complex relationships on their own. That sounds extremely powerful, and there is a sense it which it absolutely is. However, it also becomes easy to fit to coincidences in the data and find apparent patterns that aren't really there, because the model is so flexible and able to go chase what it think are patterns. Linear models will be most useful when you have some sense of what relationships matter. If you know to anticipate some nonlinearity, you can use polynomials or, perhaps better yet, splines to allow for curvature. If you anticipate interactions between variables, you can include them. If you anticipate interactions between nonlinear transformations, you can interact polynomials and splines, too. It is also possible to throw a huge amount of flexibility at the model, such as in the Earth/MARS method, possible penalizing with some regularization. In terms of philosophy, however, this is not so different from the more sophisticated machine learning methods like support vector machines Support vector regressions will be most useful when you do not know what relationships exist between the variables, just that the given variables should be predictive of the outcome (perhaps humans can look at those variables and reliably make accurate predictions, such as speech recognition where speakers of a particular language typically understand each other, based only on the audio signal). Since you are making the model not only figure out the relationship but which relationships to figure out, data requirements increase in order to keep from fitting to coincidences in the training data that cannot be counted on to exist when the model is deployed. Vanderbilt's Frank Harrell, also a Cross Vallidated member , has at least two related blog posts worth reading, even if they aren't about support vector machines in particular. This answer reiterates some of his arguments. Road Map for Choosing Between Statistical Modeling and Machine Learning Musings on Statistical Models vs. Machine Learning in Health Research
