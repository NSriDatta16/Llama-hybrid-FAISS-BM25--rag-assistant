[site]: crossvalidated
[post_id]: 163441
[parent_id]: 162587
[tags]: 
Here's a suggestion: One way to think of your problem is as a mixture model. There are two possible causes of $y_1\ldots y_T$, the data that you see, which could correspond to two generative models for it, indexed by $M$. For example $$ p(y_1 \ldots y_T) = p(y_1 \ldots y_T \mid M=\text{human}) p(M=\text{human}) + p(y_1\ldots y_T \mid M=\text{non-human}) p(M=\text{non-human}) $$ You can assert some $P(M)$ and fit the two likelihoods from training data as a start to see how it might work. Then you're interested in $$ p(M \mid y_1\ldots y_T) $$ which is gotten easily from the model above via Bayes theorem. Turning to the likelihoods, a filter (particle or otherwise) will give you the likelihood of the data under the model which seems to be the the single number you want above. That's because, conceptually at least $$ p(y_1 \ldots y_T \mid M) = \int p(y_1\ldots y_T \mid x_1\ldots x_T, M) p(x_1\ldots x_T) dx_1 \ldots dx_T $$ where $p(x_1\ldots x_T)$ is a construction from a genuine prior $p(x_0)$ and a state dynamics model $p(x_t \mid x_{tâˆ’1})$ that can generate the rest, all conditioned on the observations. If, on the other hand, you think that the average state vector is a good feature to classify humans and non-humans, then use it. You may want to smooth as well then. My point above is only that any filter gives you $P(E | H) = P(y_1\ldots y_T \mid M)$ for free. Thinking of particles is not necessary - just recall how you compute a Likelihood with a Kalman filter. Obviously there's a lot more to do in the scheme above, e.g. fit the parameters of the models that you are filtering with, but that's the shape of a solution at least.
