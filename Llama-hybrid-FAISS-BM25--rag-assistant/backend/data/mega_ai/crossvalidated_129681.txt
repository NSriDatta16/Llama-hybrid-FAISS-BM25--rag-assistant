[site]: crossvalidated
[post_id]: 129681
[parent_id]: 129628
[tags]: 
[Assume, for the moment that we're not talking about composite null hypotheses, since it will simplify the discussion to stick to the simpler case. Similar points could be made in the composite case but the resulting additional discussion would be likely to prove less illuminating] The probability of a type I error, which (if the assumptions hold) is given by $\alpha$ is probability under the notion of repeated sampling. If you collect data many times when the null is true , in the long run a proportion of $\alpha$ of those times you would reject. In effect it tells you the probability of a Type I error before you sample. The p-value is instance-specific and conditional. It does not tell you the probability of a type I error, either before you sample (it can't tell you that, since it depends on the sample), or after: If $p\geq\alpha$ then the chance you made a Type I error is zero. If the null is true and $p Take another look at the two things under discussion: P(Type I error) = P(reject H$_0$|H$_0$ true) p-value = P(sample result at least as extreme as the observed sample value|H$_0$ true, sample) They're distinct things. Edit - It appears from comments that it is necessary to address your second paragraph in detail: The p value seems to give an exact estimate of the probability of falsely rejecting a true null hypothesis Not so, as discussed above. (I assumed this was sufficient to render the rest of the question moot.) α seems to be a maximum acceptable Type I error, In effect, yes (though of course we may choose a lower $\alpha$ that the absolute maximum rate we'd be prepared to accept for a variety of reasons). whereas p is exact. Again, not so; it's not equivalent to $\alpha$ in the suggested sense. As I suggest, both the numerator and denominator in the conditional probability differ from the ones for $\alpha$. Put differently it appears to give the minimum α level under which we could still reject the null. In spite of my earlier caveats, there is a direct (and not necessarily particularly interesting) sense in which this is true. Note that $\alpha$ is chosen before the test, $p$ is observed after, so it's necessary to shift from our usual situation. If we posit the following counterfactual: we have a collection of hypothesis testers, each operating at their own significance level they are each presented with the same set of data then it is the case that the p-value is a dividing line between those testers that reject and those that accept. In that sense, the p-value is the minimum α level under which testers could still reject the null. But in a real testing situation, $\alpha$ is fixed, not variable, and the probability we're dealing with is either 0 or 1 (in a somewhat similar sense to the way people say "the probability that the confidence interval includes the parameter"). Our probability statements refer to repeated sampling; if we posit a collection of testers each with their individual $\alpha$, and consider only a single data set to test one, it's not clear $\alpha$ is the probability of anything in that scenario - rather, $\alpha$ represents something if we had a collection of testers and repeated sampling where the null is true - they'd each be rejecting a proportion $\alpha$ of their nulls across samples, while $p$ would represent something about each sample.
