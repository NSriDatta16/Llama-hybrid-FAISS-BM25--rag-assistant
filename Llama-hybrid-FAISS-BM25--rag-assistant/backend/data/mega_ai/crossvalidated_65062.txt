[site]: crossvalidated
[post_id]: 65062
[parent_id]: 65044
[tags]: 
The Bayesian approach to your problem would be to consider the posterior probability over models $M \in \{ \text{Normal}, \text{Log-normal} \}$ given a set of data points $X = \{ x_1, ..., x_N \}$, $$P(M \mid X) \propto P(X \mid M) P(M).$$ The difficult part is getting the marginal likelihood , $$P(X \mid M) = \int P(X \mid \theta, M) P(\theta \mid M) \, d\theta.$$ For certain choices of $p(\theta \mid M)$, the marginal likelihood of a Gaussian can be obtained in closed form . Since saying that $X$ is log-normally distributed is the same as saying that $Y = \{ \log x_1, ..., \log x_N$ } is normally distributed, you should be able to use the same marginal likelihood for the log-normal model as for the Gaussian model, by applying it to $Y$ instead of $X$. Only remember to take into account the Jacobian of the transformation , $$P(X \mid M = \text{Log-Normal}) = P(Y \mid M=\text{Normal}) \cdot \prod_i \left| \frac{1}{x_i} \right|.$$ For this approach you need to choose a distribution over parameters $P(\theta \mid M)$ – here, presumably $P(\sigma^2, \mu \mid M=\text{Normal})$ – and the prior probabilities $P(M)$. Example: For $P(\mu, \sigma^2 \mid M = \text{Normal})$ I choose a normal-inverse-gamma distribution with parameters $m_0 = 0, v_0 = 20, a_0 = 1, b_0 = 100$. According to Murphy (2007) (Equation 203), the marginal likelihood of the normal distribution is then given by $$P(X \mid M = \text{Normal}) = \frac{|v_N|^\frac{1}{2}}{|v_0|^\frac{1}{2}} \frac{b_0^{a_0}}{b_n^{a_N}} \frac{\Gamma(a_N)}{\Gamma(a_0)} \frac{1}{\pi^{N/2}2^N}$$ where $a_N, b_N,$ and $v_N$ are the parameters of the posterior $P(\mu, \sigma^2 \mid X, M = \text{Normal})$ (Equations 196 to 200), \begin{align} v_N &= 1 / (v_0^{-1} + N), \\ m_N &= \left( v_0^{-1}m_0 + \sum_i x_i \right) / v_N, \\ a_N &= a_0 + \frac{N}{2}, \\ b_N &= b_0 + \frac{1}{2} \left( v_0^{-1}m_0^2 - v_N^{-1}m_N^2 + \sum_i x_i^2 \right). \end{align} I use the same hyperparameters for the log-normal distribution, $$P(X \mid M = \text{Log-normal}) = P(\{\log x_1, ..., \log x_N \} \mid M = \text{Normal}) \cdot \prod_i \left|\frac{1}{x_i}\right|.$$ For a prior probability of the log-normal of $0.1$, $P(M = \text{Log-normal}) = 0.1$, and data drawn from the following log-normal distribution, the posterior behaves like this: The solid line shows the median posterior probability for different draws of $N$ data points. Note that for little to no data, the beliefs are close to the prior beliefs. For around 250 data points, the algorithm is almost always certain that the data was drawn from a log-normal distribution. When implementing the equations, it would be a good idea to work with log-densities instead of densities. But otherwise it should be pretty straight forward. Here is the code that I used to generate the plots: https://gist.github.com/lucastheis/6094631
