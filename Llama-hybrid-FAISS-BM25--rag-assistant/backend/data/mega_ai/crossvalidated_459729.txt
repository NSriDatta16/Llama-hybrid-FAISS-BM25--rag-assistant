[site]: crossvalidated
[post_id]: 459729
[parent_id]: 459724
[tags]: 
A few things before I get to the idea of assessing predictive quality of each of your models. If you're trying to predict dispersal, code it as 1. Its standard procedure to code the "success" or "thing you want to model" as the "positive" case of being 1. But, for now, I'll leave it coded as you have it. Rare events can be tricky in statistical learning. The search term you want to look into is "class imbalance." I wrote a little summary of typical methods for a project for a graduate seminar in statistical computing you can check out at GitHub. But your sample size is too small to do anything meaningful. A dataset of 64 cases with only 3 being in the positive outcome does not give any model much to work with. Especially when some of the most common approaches are things like "throw out data from the class with too many cases until its balanced" (undersampling), which would only leave you with six cases. Since this is for a thesis and learning how to do the modeling, I'll keep going—but just know that the results aren't going to be incredibly useful. You mention violating assumptions of normality and homogeneity of variance—these are assumptions in ordinary least squares regression, but not in a logistic regression like you have here. I'd suggest reading the chapter "Generalized Linear Models" by Coxe, West, and Aiken (2013) in The Oxford Handbook of Quantitative Methods, Volume II for a good background on all of the different generalized linear models. Assessing predictive quality As for actually doing it in R, the people behind the {caret} package—and many other programmers—have begun putting all of the machine learning tools into a collection of packages called tidymodels . Julia Silge has some good introductions to this on her YouTube page, like this one. But what you would do ideally is some type of k-fold cross-validation. Let's say we want to test the specificity and accuracy of your best model. Some annotated code is below. I renamed the dataset to dat . library(tidymodels) # clasification problems want this to be a factor outcome dat $Dispersed_Or_Recruited Dispersed_Or_Recruited) wk % # tell it you want to use logistic regression set_engine("glm") # tell it you want to use glm to do so # add to our workflow wk % add_recipe(dat_rec) %>% add_model(dat_spec) # make splits with 5-fold cross-validation set.seed(1839) # for reproducibility dat_cv % collect_metrics() First, you'll get some warning messages, like "No true negatives were detected." This makes sense, because your data only had 3 negative cases in it. If you break the data down into 5 chunks, you're going to get folds without any negative outcomes in it. This is where you need a bigger sample size. You'll also get some errors about the model not converging—again, because of the class imbalance. But, at the end, you can look at your metrics: # A tibble: 2 x 5 .metric .estimator mean n std_err 1 accuracy binary 0.923 5 0.0344 2 spec binary 0.333 3 0.333 These are the metrics averaged across all folds that converged, hence mean and n . You can see your accuracy is good. The data are so imbalanced that if it just predicts a positive case every time, it will have 95.3% accuracy (61 / 64). The specificity, or spec , is not so great, since there's so little data of the negative class to train on. Again, the outputs here and interpreting them aren't too useful—there's not enough data to do much modeling with. But I hope that gets you started at cross-validation metrics, and give you some things to look into (tidymodels, class imbalance, assumptions of general linear models).
