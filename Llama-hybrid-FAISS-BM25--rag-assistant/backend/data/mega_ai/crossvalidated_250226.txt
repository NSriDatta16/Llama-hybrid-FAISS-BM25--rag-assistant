[site]: crossvalidated
[post_id]: 250226
[parent_id]: 250218
[tags]: 
How are the words inputted into a Word2Vec model? In other words, what part of the neural network is used to derive the vector representations of the words? See Input vector representation vs output vector representation in word2vec What is the objective function which is being minimized? The original word2vec papers are notoriously unclear on some points pertaining to the training of the neural network ( Why do so many publishing venues limit the length of paper submissions? ). I advise you look at {1-4}, which answer this question. References: {1} Rong, Xin. "word2vec parameter learning explained." arXiv preprint arXiv:1411.2738 (2014). https://arxiv.org/abs/1411.2738 {2} Goldberg, Yoav, and Omer Levy. "word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method." arXiv preprint arXiv:1402.3722 (2014). https://arxiv.org/abs/1402.3722 {3} TensorFlow's tutorial on Vector Representations of Words {4} Stanford CS224N: NLP with Deep Learning by Christopher Manning | Winter 2019 | Lecture 2 â€“ Word Vectors and Word Senses. https://youtu.be/kEMJRjEdNzM?t=1565 ( mirror )
