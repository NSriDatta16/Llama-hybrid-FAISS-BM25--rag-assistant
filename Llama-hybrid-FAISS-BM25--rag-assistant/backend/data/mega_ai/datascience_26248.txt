[site]: datascience
[post_id]: 26248
[parent_id]: 26244
[tags]: 
This is a common approach to address class-imbalance. You'll generally see it referred to as "upsampling" or "over-sampling". In addition to simply reusing existing data, you can "augment" your data by transforming it or adding noise, or generate synthetic data. A good demonstration of data augmentation is the winning solution to the kaggle Galaxy Zoo contest. A popular technique for generating new data is SMOTE . Keep in mind: if you change the class proportions in your dataset, your effectively changing the prior for those classes. Depending on your choice of algorithm and what you are trying to accomplish, you may want/need to apply a probability adjustment to the model's output. Another technique you can use is to just pick a different decision threshold. For example, if you're performing a logistic regression and don't care about performing inference on parameters, duplicating the data for one class won't actually change the slope of the decision boundary, it will just shift its location. You could equivalently use the decision boundary learned on the unmodified data and then pick a cutoff other than $P(X=1) \ge 0.5$. A good heuristic is to use the threshold which maximizes Youden's J , which can be trivially calculated from a ROC curve. If you have a specific model evaluation metric you are concerned with, choose your threshold relative to that.
