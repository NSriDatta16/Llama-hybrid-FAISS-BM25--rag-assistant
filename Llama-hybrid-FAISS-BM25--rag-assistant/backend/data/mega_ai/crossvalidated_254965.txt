[site]: crossvalidated
[post_id]: 254965
[parent_id]: 254946
[tags]: 
When there are multiple responses within each combination of values of your predictor variables, you can compare your predicted probabilities directly to the observed outcomes. (You can see an example in my answer here: Test logistic regression model using residual deviance and degrees of freedom .) The tricky issue is what to do when the observations are not grouped. Hosmer and Lemeshow (1980) suggested that you sort all the predicted probabilities and bin them somehow. Then you could compare the number of 'successes' in each bin to the expected number (the sum of the predicted probabilities) in that bin with a chi-squared test . This sounds good, as far as it goes, but there is no principled (i.e., correct) way to bin the predicted probabilities . There are lots of ways of trying to bin them, but no way is necessarily right and the test is very sensitive to the binning. This has been noted by no less than Hosmer, Hosmer, Le Cessie and Lemeshow (1997, SIM ). With this understanding, we can infer the reason you get different results from running the Hosmer and Lemeshow test on your same model in MATLAB and R: The binning is being done differently and the result is sensitive to the binning. Moreover, the test (in both MATLAB and R) is unreliable and should be ignored. For a nice overview of this topic, see: Alison, PD (2014). Measures of Fit for Logistic Regression. SAS Global Forum , paper 1485-2014. ( pdf )
