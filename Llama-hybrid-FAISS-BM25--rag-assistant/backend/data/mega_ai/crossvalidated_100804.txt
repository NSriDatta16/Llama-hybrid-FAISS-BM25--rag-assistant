[site]: crossvalidated
[post_id]: 100804
[parent_id]: 100783
[tags]: 
Nothing good is going to come from this. Your raw data just about fits into memory, assuming you can fit Eclipse, the entire OS, and anything else that's running into 800 Mb or so, which seems unlikely. However, you now need to operate on this data. Plain-vanilla quadratic programming needs approximately $O(n^2)$ space, which is going to be a lot in your case. SMO can dramatically improve on this, as can various SVM approximations (reduced SVM), but you're still going to need some memory, and things are going to get ugly very quickly if you start swapping. In theory, you could buy (a lot) more RAM, or switch to something with a lot less overhead ($k$-NN only really needs $k$ doubles and $k$ indicies into your data set). However, I think the the utterly immense sparsity of your data set is likely to be an even bigger issue. Assuming your features are all binary, you're sampling from a very, very small fraction of the input space (~1/ a fifty-four thousand digit number ). You'd be better off with a more compact representation of your data, both pratically and theoretically. I'd start by eliminating any features that have constant values across the whole data set. This should be pretty simple and I suspect it will remove many of your features. Next, you might consider something like PCA to re-represent the remaning features in a low-dimensional space.
