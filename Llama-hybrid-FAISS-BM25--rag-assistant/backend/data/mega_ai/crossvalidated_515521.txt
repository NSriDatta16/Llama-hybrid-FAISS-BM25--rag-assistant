[site]: crossvalidated
[post_id]: 515521
[parent_id]: 
[tags]: 
Why shouldn't you mix variable size inputs in the same minibatch?

I am trying to build a CNN-LSTM architecure in tf.keras that classifies sequences of varying sizes. My training data is highly variable and I would have to crop/pad sequences in order to create similar sized minibatches of the same sequence length respectively. Hence my question: How important is it to have minibatches of sequences of the same length? Is this due to some inherent property of the way data the tensors are processed (i.e. will I get an error) or is it more a performance consideration (i.e. parallelizability) that can be discarded if needs be? Many thanks! Tom
