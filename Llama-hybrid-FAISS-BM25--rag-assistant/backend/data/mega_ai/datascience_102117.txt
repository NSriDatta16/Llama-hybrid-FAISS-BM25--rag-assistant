[site]: datascience
[post_id]: 102117
[parent_id]: 77736
[tags]: 
For Q1, check out these two stats.SE posts: Why does XGBoost have a learning rate? XGBoost Loss function Approximation With Taylor Expansion They bring up questions about the goodness of the second-order expansion, which is even more strict than asking about radius of convergence, and I think the discussion probably answers this reasonably well. I suspect that the common loss functions also have pretty nice Taylor expansion properties, but I haven't worked any of them out. For Q2, the fact that the selected $\gamma^*$ is not infinite is because of the Taylor approximation. And in this context, that's actually a very useful thing to have, to prevent overfitting! Again have a look at the answer to the second question linked above, it has some additional information about other possible descent options.
