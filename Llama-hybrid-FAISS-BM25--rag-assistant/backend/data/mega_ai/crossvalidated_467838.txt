[site]: crossvalidated
[post_id]: 467838
[parent_id]: 
[tags]: 
Regularizing the difference in the norms of two independent weight matrices in a neural network

Say, there are two neural network layers with weights $W_1$ and $W_2$ . These two layers are part of a larger network but their inputs are completely independent of each other and their outputs could be joined somehow and propagated jointly in subsequent layers. So, a vanilla L2 term discourages the weight matrices from learning too big values. But if I calculate the Frobenius norm of $W_1$ and $W_2$ , subtract one from the other and add that as a regularization term, what would that mean? Mathematically, $L(\centerdot) + \lambda (||W_1||_F - ||W_2||_F) $ where $\lambda$ is some rate parameter. What would this sort of regularization do to the learning? Can we say this term would encourage $W_1$ and $W_2$ to be 'similar'? How would it affect the outputs of these layers?
