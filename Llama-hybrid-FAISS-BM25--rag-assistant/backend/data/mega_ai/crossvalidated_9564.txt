[site]: crossvalidated
[post_id]: 9564
[parent_id]: 9507
[tags]: 
Whenever I implement a new algorithm, I get myself an easy, interpretable dataset on which I can try it out. This has several advantages, for example runtime (use a small dataset) or visualization (make things you want to plot have dimension 2 or 3). Of course the behavior you see could result from the dataset. It feels however unlikely to me. However, a standard way to initialize variables is to just randomize them uniformly in an interval like [-1, 1], [-0.1, 0.1] or something like that. I have seen standard normal as well. I am not sure if it works for SMMs, but for Gaussian Mixture Models and PCA mixtures, it makes sense to run a few iterations of K-Means before you go into EM (and use the centers as responsibilities). Maybe you want to try that.
