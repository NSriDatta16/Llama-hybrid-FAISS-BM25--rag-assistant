[site]: crossvalidated
[post_id]: 569857
[parent_id]: 44383
[tags]: 
The general idea of the "prediction distribution" has been expressed in @jerad's answer. In short, Bayesians regard everything as random, hence seeking for their distribution which preserves all the information. (Of course you can always derive a point estimator from it at the last step, for example by maximum vote principle) Return to the formula, there are actually two equations: Marginal equation: $$ \begin{aligned} p\left(f_{*} \mid x_{*}, X, y\right) &=\int p\left(f_{*} \mid x_{*}, w\right) p(w \mid X, y) d w \end{aligned} $$ Proof: By marginal calculation \begin{aligned} p\left(f_{*} \mid x_{*}, X, y\right) &=\int p\left(f_{*} \mid x_{*}, w, X, y\right) p(w \mid x_{*}, X, y) d w \end{aligned} where by independance, $p(f_{*} \mid x_{*}, w, X, y) = p(f_{*} \mid x_{*}, w)$ and $p(w \mid x_{*}, X, y) = p(w \mid X, y)$ Derivation of normal distribution Notice that $f_{*} = w^T x_{*}$ , so $f_{*} \mid x_{*},w$ is actually deterministic. i.e. $p(f_{*} \mid x_{*},w) = \delta(w^T x_{*})$ , integrate any distribution with a delta function is just an evaluation. Or you can just refer @Pantelis' answer which is simpler.
