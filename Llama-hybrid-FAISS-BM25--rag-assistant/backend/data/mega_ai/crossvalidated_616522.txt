[site]: crossvalidated
[post_id]: 616522
[parent_id]: 
[tags]: 
Average time in which a product random variable becomes zero

Im looking for the optimal time in which a process should be cancelled before it results on financial losses. Say M_n=X_n*Y_n-c(n) for for n =1 to 12 which is the number of hours the process gets going, and c(n) is the cost of the process on the n-th hour. c(n) is deterministic in nature as opposed to X_n, a positive real number and Y_n, a count random variable. I have no background on time series analysis but maybe I could try and apply a linear regression model with time as the independent variable and see on average where, and if, M_n gets equal to zero. What do you think? Maybe I should treat time as continuous and analyse M_t as a Poisson process?
