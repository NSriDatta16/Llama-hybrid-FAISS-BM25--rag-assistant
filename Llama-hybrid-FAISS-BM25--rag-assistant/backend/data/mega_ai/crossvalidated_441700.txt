[site]: crossvalidated
[post_id]: 441700
[parent_id]: 441687
[tags]: 
There are many popular classes of generative models. Autoregressive models : Here we model $\log p(x)$ as a sum of conditional terms $\sum_i \log p(x_i | x_{j . This group includes most natural language models, Transformers , PixelRNN , PixelCNN , and Wavenet . Can be used for image, text, sound, and almost any other domain. Provides an explicit $p(x)$ . Not vulnerable to mode dropping like GANs are. Downside is that it's moderately difficult to train and sample efficiently. Variational Autoencoders : Here we model $p(x) = \int p(x|z)p(z) dz$ . The integral is intractable so we resort to computing a lower bound (called the ELBO ). Can be used to model pretty much any mode of data. Upsides are: it's fast, provides an interpretable latent space, and doesn't drop modes. Downsides are: cannot tractably compute $p(x)$ , can suffer from posterior collapse . The most powerful VAEs (see VQ-VAE2 ) are competitive with the best GANs for image generation. Flow-based models : Here we model $\log p(x) = \log p_z(g^{-1}(x)) + \log \left|J_{g^{-1}}(x) \right|$ , where $g$ is an invertible mapping from $x$ to some latent space $z$ and $J_{g^{-1}}$ is the jacobian of it's inverse. Upsides are: provides an explicit form of $p(x)$ , doesn't drop modes, interpretable latent space, and they are also theoretically appealing. Downsides are: these models tend to be very computationally expensive. Popular models include NICE , Real-NVP , GLOW , and invertible ResNets . Some other interesting lines of research I've seen include Implicit MLE (one sentence summary: minimizing expected distance to the nearest ground-truth point is equivalent to maximizing likelihood) and Generative Latent Optimization (one sentence summary: learn $p(x|z)$ first, decide on $p(z)$ later).
