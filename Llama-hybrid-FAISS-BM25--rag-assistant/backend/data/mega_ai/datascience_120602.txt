[site]: datascience
[post_id]: 120602
[parent_id]: 120601
[tags]: 
You are right that a transformer can take in an arbitrary amount of tokens even with fixed parameters, excluding the positional embedding matrix, whose size directly grows with the maximum allowed input length. Apart from memory requirements (O(nÂ²)), the problem transformers have regarding input length is that they don't have any notion of token ordering. This is why positional encodings are used. They introduce ordering information into the model. This, however, implies that the model needs to learn to interpret such information (precomputed positional encodings) and also learn such information (trainable positional encodings). The consequence of this is that, during training, the model should see sequences that are as long as those at inference time because for precomputed positional encodings it may not correctly handle the unseen positional information and for learned positional encodings the model simply hasn't learned to represent them. In summary, the restriction in the input length is driven by: Restrictions in memory: the longer the allowed input, the more memory is needed (quadratically), which doesn't play well with limited-memory devices. Need to train with sequences of the same length as the inference input due to the positional embeddings. If we eliminate those two factors (i.e. infinite memory and infinite-length training data), you could set the size of the positional embeddings to an arbitrarily large number, hence allowing arbitrarily long input sequences. Note, however, that due to the presence of the positional embeddings, there will always be a limit in the sequence length (however large it may be) that needs to be defined in advance to determine the size of the embedding matrix.
