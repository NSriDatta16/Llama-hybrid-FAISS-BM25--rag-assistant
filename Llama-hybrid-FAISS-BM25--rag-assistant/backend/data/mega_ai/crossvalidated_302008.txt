[site]: crossvalidated
[post_id]: 302008
[parent_id]: 
[tags]: 
How to perform L2 Normalization for machine learning

I need to normalize a set of feature vectors stored in training, validation and test matrices. Doing L2-norm in a vector is quite easy, just divide each vector value by its norm. However, how to do that in matrices in the context of machine learning? should I: Divide each vector (each matrix line) by its norm? or should I divide all elements in the matrix by the matrix norm, considering all the matrix as one big vector? Store the training matrix norm and use it to normalize the elements on the validation and testing matrices?
