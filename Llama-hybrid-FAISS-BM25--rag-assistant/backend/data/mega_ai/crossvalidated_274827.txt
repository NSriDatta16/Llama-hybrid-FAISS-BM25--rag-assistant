[site]: crossvalidated
[post_id]: 274827
[parent_id]: 
[tags]: 
Memory Network overfits

I'm playing with a MemN2N on the babi-dialog-task 1 (API calls). And a very surprising thing I see is that it learns dialogues pretty well, but on the most interesting part, actual API calls, it's pretty terrible (0.7%). And I think it makes sense because there are only 7 distinct interlocutory responses but 159 distinct api_calls in the trainset. UPD : if trained exclusively on API calls, its per-utterance accuracy is at 96% training and 0% which makes me think it does nothing but memorize its training data. My MemN2N hyperparameters are as follows: 20 embedding size 3 hops 12 memory size 100 training epochs My MemN2N input is triplets where story is all turns prior to the current question. I don't use any indices of supporting facts. If you worked with a MemN2N for dialogue tasks, did you notice a similar behavior? And maybe have an idea of how to fix it? UPD : tried an LSTM solution for that (encoded dialogue context --> answer label) - results are the same :( UPD2 : seems like I've found a way around imbalanced class label set - incorporating class weights into the loss function. But what still keeps me surprised is, both MemN2N and LSTM trained to predict exclusively API calls perform particularly bad (2-3% accuracy in general).
