[site]: datascience
[post_id]: 244
[parent_id]: 138
[tags]: 
Coming from a programmers perspective, frameworks rarely target performance as the highest priority. If your library is going to be widely leveraged the things people are likely to value most are ease of use, flexibility, and reliability. Performance is generally valued in secondary competitive libraries. "X library is better because it's faster." Even then very frequently those libraries will trade off the most optimal solution for one that can be widely leveraged. By using any framework you are inherently taking a risk that a faster solution exists. I might go so far as to say that a faster solution almost always exists. Writing something yourself is not a guarantee of performance, but if you know what you are doing and have a fairly limited set of requirements it can help. An example might be JSON parsing. There are a hundred libraries out there for a variety of languages that will turn JSON into a referable object and vice versa. I know of one implementation that does it all in CPU registers. It's measurably faster than all other parsers, but it is also very limited and that limitation will vary based on what CPU you are working with. Is the task of building a high-performant environment specific JSON parser a good idea? I would leverage a respected library 99 times out of 100. In that one separate instance a few extra CPU cycles multiplied by a million iterations would make the development time worth it.
