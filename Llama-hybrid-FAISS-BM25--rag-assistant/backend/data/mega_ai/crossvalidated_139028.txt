[site]: crossvalidated
[post_id]: 139028
[parent_id]: 139015
[tags]: 
There can be many ways of computing "rating" of attributes based on complete or incomplete ranking task. Each way implies its assumption or model, so it is difficult to say which one might be the "best". Most natural seems to convert ranks into scores and do it non-linearly. Imagine that all 1 through 20 ranks are possible. We assume rank 1 is the "best" and rank 20 is the "worst". Intuitively, the greater in the rank number the lesser is its qualitative difference with the adjacent ranks. Like in sport: "distance" between gold and silver medals is the greatest; between silver and bronze is somewhat less; between bronze and 4th place is further diminished; between 10th and 11th places is hardly distinguishable. Thinking this way, any decreasing "exponentially fading" function of rank will basically do. For example, I could recommend considering reciprocal function 1/Rank or Exponential function in the form of (backward) Savage scores . Both functions are shown below. Rank 1/Rank Savage (backward) 1 1.0000 2.5977 2 .5000 1.5977 3 .3333 1.0977 4 .2500 .7644 5 .2000 .5144 6 .1667 .3144 7 .1429 .1477 8 .1250 .0049 9 .1111 -.1201 10 .1000 -.2312 11 .0909 -.3312 12 .0833 -.4221 13 .0769 -.5055 14 .0714 -.5824 15 .0667 -.6538 16 .0625 -.7205 17 .0588 -.7830 18 .0556 -.8418 19 .0526 -.8974 20 .0500 -.9500 Savage (it's a name) scores are particularly interesting because of their properties. They have always mean 0 and variance greater as the number or data rows N (number of ranks 20, in this instance) grows. Whatever is N, the curve of fading is approximately the same. And the fading (the flattening) is slower than with 1/Rank function. Savage score is computed as $S_i = (1/N + 1/(N-1) +â€¦+ 1/(N-R_i+1)) - 1 $ with $R_i$ being the rank. In our example ranking is backward (that is, the largest $R_i=20$ corresponds to rank=1). Since in the OP question the ranking was incomplete - only ranks 1, 2, 3 were used and ranks 4 through 20 were dismissed (say, the rating process was "interrupted") we have to assume some equal score for all the attributes not ranked by the respondent. The mean of scores for ranks 4 through 20 is the obvious candidate for the value. Another (and more risky) approach would be to impute the mean score corrected by expectation given the attributes that were selected (ranked) by the respondent. That is, if attributes A and B are often seen to be selected together then for those respondents who selected A but not B the score for B shall be higher than just the average score between scores 4 through 20. In this example with the task "select three and rank them" , ranks 4 through 20 were dismissed. In the unconstrained task "select whatever applies and rank the selected" each respondent will have his own "tail" of dismissed ranks. You might then use the respondent-specific average to impute as the score for the not chosen items. However, in the unconstrained task it is natural to assume that the importance of the not selected items is zero or absolute minimal for the respondent. Then there should be used the minimal possible score, and not the average or such as above.
