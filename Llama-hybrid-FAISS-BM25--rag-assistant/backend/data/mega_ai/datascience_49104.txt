[site]: datascience
[post_id]: 49104
[parent_id]: 
[tags]: 
How should we manage large database

We currently manage a large volume of economic data in Excel in my organisation. All of the data is downloaded from different online databases into Excel spreadsheets (one for each data frequency including annual, monthly, quarterly) - and then one main spreadsheet organises everything and creates tables that we need regularly. By organise, I mean that many of the things we need are simply identities ( $Z=X+Y$ where we would have only downloaded data on $X$ and $Y$ ) My view is that this could be done much more efficiently in R - where we'd automate the updating of the data and then spit out the tables that we need. But I am not trained at all in data management. Would you all recommend a better way of doing this, or are there pitfalls to using R that I am not considering.
