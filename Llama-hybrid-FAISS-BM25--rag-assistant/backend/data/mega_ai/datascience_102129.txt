[site]: datascience
[post_id]: 102129
[parent_id]: 
[tags]: 
Binary classification with imbalanced dataset, about lightgbm output probability distribution

I trained a binary classifier for an imbalanced dataset. I did two experiments: lightgbm classifier, boosting_type='gbdt', objective='cross_entropy', SMOTE upsample After training the lgbm model, I made predictions on validation dataset. I plotted the probability distribution as follow: lightgbm output probability distribution Plot code: fig = plt.figure() tmp = pd.Series(pred_y) ax = tmp.plot.kde() fig.savefig('xx.png') Standard Scaler, sklearn logistic regression, class_weight='balanced' The output probability distribution of validation dataset is as follow: logistic output probability distribution Why does the lightgbm output probability distribution have some output values near 0.5 and 0.75? Unlike a logistic model, just output probability either near 0, or near 1. For that lightgbm model is a decision forest, and add many outputs from many trees to produce the final probability? Or is it because the dataset is imbalanced? differences
