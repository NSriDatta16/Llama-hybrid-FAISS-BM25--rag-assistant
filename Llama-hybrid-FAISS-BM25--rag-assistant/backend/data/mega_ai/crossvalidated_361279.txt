[site]: crossvalidated
[post_id]: 361279
[parent_id]: 361065
[tags]: 
I am not completely satisfied with my answer, but here goes. To an extent, you are comparing apples and oranges. Your two calls to Arima() use method="ML" , whereas your auto.arima() uses the default, which is method="CSS-ML" . Then again, refitting everything with the default does not make a real difference. Minimizing the AIC is asymptotically equivalent to minimizing the one-step ahead squared prediction error. (I don't have a reference at hand, sorry.) Note that this is an asymptotic result in a suitable statistical sense. It's quite possible for a handpicked model to outperform AIC on a limited length time series. And on a single one, at that. Finally, as you write in a comment , the AirPassengers dataset exhibits strong multiplicative seasonality. ARIMA does not model multiplicative seasonality or trend; it can only deal with additive effects. Your overparameterized model gets the multiplicative trend and seasonality right, but it may also forecast this in a series that does not exhibit such effects. There are reasons why such large models are typically not considered. To model multiplicative effects, allow auto.arima() to use Box-Cox transformations: > (foo accuracy(forecast(foo,h=24,biasadj=TRUE),test) ME RMSE MAE MPE MAPE MASE ACF1 Theil's U Training set -0.7186038 8.915531 6.691014 -0.2079082 2.753580 0.2341638 0.04889565 NA Test set 28.5600533 31.711896 28.884516 6.2710488 6.348486 1.0108644 0.17279165 0.6372069 I cut out the AIC, because that is not comparable to the AIC on nontransformed data. Note that we end up much closer to your large model in terms of the test RMSE, but the model is much more interpretable, and I personally would trust it a lot more than an ARIMA(15,1,15)(4,1,4)[12] one. Incidentally, searching through more possible ARIMA models yields the exact same model: > (bar
