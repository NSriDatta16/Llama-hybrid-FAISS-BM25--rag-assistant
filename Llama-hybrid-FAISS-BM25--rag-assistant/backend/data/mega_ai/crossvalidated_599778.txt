[site]: crossvalidated
[post_id]: 599778
[parent_id]: 
[tags]: 
What's the unified definition of Tikhonov regularization

I met with "Tikhonov regularization" in two textbooks. The first is "Pattern Recognition and Machine Learning" by Christopher M. Bishop. In page 267 of his book, the regularized error function is $$\widetilde E=E+\lambda\Omega$$ where $E$ is the original sum-of-squares error, $\lambda$ is the regularization coefficient, and Tikhonov regularizer $\Omega$ takes the form $$\Omega=\frac{1}{2}\int\|\nabla y({\bf x})\|^2p({\bf x}){\rm d}{\bf x}.\tag{1}$$ The second book is "Neural Networks and Deep Learning: A Textbook" by Charu C. Aggarwal. In section 1.4.1.1 on page 26, Tikhonov regularization is described as penalty $$\lambda\|\overline W\|^2\tag{2}$$ added to the perceptron criterion, the smoothed surrogate loss function of perceptron. So, in Bishop's book, Tikhonov regularization is integral of squared 2-norm of gradient of network mapping function, while in Aggarwal's book, Tikhonov regularization is simply a squared 2-norm of parameter vector. It is not surprising they take different forms, because the models are different and unregularized error functions are different: one is neural network with sum-of-squares error function $\frac{1}{2}\int\int\{y({\bf x})-t\}^2p(t|{\bf x})p({\bf x}){\rm d}{\bf x}{\rm d}t$ and the other is perceptron with smoothed surrogate loss function $\max\{-y_i(\overline W\cdot\overline {X_i}),0\}$ . Forms being different, both of them are claimed to be Tikhonov regularization. Although both of them contain a 2-norm, I don't know what property they share in common which can label both of regularizers $(1)$ and $(2)$ Tikhonov. So my question is: what is the unified definition of Tikhonov regularization, which can specialize to $(1)$ and $(2)$ in respective concrete machine learning settings described above. Thank you.
