[site]: crossvalidated
[post_id]: 320596
[parent_id]: 
[tags]: 
Variance of OLS estimator of $\theta$ in $y_n = \theta x_n + \eta_n$ compared to Cramer-Rao

From Theodoridis' Machine Learning , problem 3.7: Derive the Cramer-Rao bound for the LS estimator, where the training data result from the model $$y_n = \theta x_n + \eta_n\text{, } \qquad n = 1, 2, \dots$$ where $x_n$ and $\eta_n$ are iid samples of a zero mean random variable, with variance $\sigma^2_x$, and a Gaussian one with zero mean and variance $\sigma^2_{\eta}$, respectively. Assume, also, that $x$ and $\eta$ are independent. Then, show that the LS estimator achieves the CR bound only asymptotically. After a lot of work, I have that the Cramer-Rao lower bound is $$\dfrac{1}{I(\theta)} = \dfrac{(\theta^2\sigma^2_x + \sigma^2_{\eta})^2}{2N\theta^2\sigma^4_{x}}$$ where $N$ is the sample size. The OLS estimator of $\theta$ is $$\hat{\theta} = \dfrac{\sum_{n=1}^{N}x_n y_n}{\sum_{n=1}^{N}x_n^2}\text{.}$$ How does one find the variance of this, given that BOTH $x_n$ and $y_n$ have variances? I don't like the answer at https://stats.stackexchange.com/a/105411/46427 , since the formula $$\sigma^2_b = (X^{T}X)^{-1}\sigma^2_e$$ assumes that the values of $X$ are fixed and known; i.e., with no variance. Why is this so? Because since $$\hat{\boldsymbol\beta} = (X^{T}X)^{-1}X^{T}\mathbf{y}$$ we obtain $$\mathrm{Var}\left(\hat{\boldsymbol\beta}\right) = (X^{T}X)^{-1}X^{T}\mathrm{Var}\left(\mathbf{y}\right)X(X^{T}X)^{-1}=\sigma^2_e(X^{T}X)^{-1}$$ if we assume that $X$ is a constant, known matrix - which is not the case here.
