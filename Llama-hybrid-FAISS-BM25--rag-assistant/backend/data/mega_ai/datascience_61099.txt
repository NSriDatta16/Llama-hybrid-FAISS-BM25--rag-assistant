[site]: datascience
[post_id]: 61099
[parent_id]: 
[tags]: 
How could a considerable increase in loss leads to an improvement in accuracy?

I'm experimenting with NLP and at the moment, I'm trying to come up with a translator model for converting English sentences to French counterparts. I'm using this dataset (not that it's relevant): https://github.com/udacity/deep-learning/raw/master/language-translation/data which is composed of more than 137K sentences. My model is an encoder-decoder LSTM with attention implemented in Keras. And here are my plotted validation loss and accuracy charts: The two accuracy metrics are custom ones developed by myself. But they are based on the same categorical_accuracy from Keras. Now, my question is why I'm getting an improvement for the accuracy while the loss value is getting wrose? Also, is such a model trustworthy?
