[site]: crossvalidated
[post_id]: 605904
[parent_id]: 
[tags]: 
Flipping inputs in multilabel classification

I have framed a classification problem as follows: I have $N$ items, and wish to predict a set of relevant tags for each out of $M$ tags. An item can have anywhere from 0 to $M$ applicable tags. To use sklearn.metrics to measure a model's performance on this task, for every item I use a multi-hot encoding of it's predicted and actual relevant tags; an array $[t_1, ..., t_M]$ where $t_i$ indicated whether the $i$ th tag applies to an item. When calculating metrics like precision or recall (or others), sklearn will aggregate TN, TP, FN, FP for each tag, then average or directly output the results. But what if I decided to reframe the problem as: I have $M$ tags, and wish to predict a set of relevant items for each out of $N$ items. A tag can be assigned to anywhere from 0 to $N$ items. Now, despite calculating metrics such as weighted / unweighted average precision and recall on the exact same data as before, I get different results. I am really having a lot of trouble wrapping my head around what has changed. Is there a superior amongst these two approaches, and under what circumstances? For the first approach there's also the option of calculating the precision / recall / etc. of tag assignment to each item, then averaging the scores across all times. Is this better? Worse? Completely different? Thanks in advance for any advice.
