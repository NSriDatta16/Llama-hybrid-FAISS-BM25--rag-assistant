[site]: crossvalidated
[post_id]: 639657
[parent_id]: 
[tags]: 
How can we use ReLU activation in a Normalizing Flow model? More generally, is differentiable almost everywhere enough for a normalizing flow?

In some works, e.g., enter link description here normalizing flow models are considered with ReLU activation. For example, using a planar flow, $f = f_n \circ ... \circ f_1$ , and each $f_i$ has the form $$ z\mapsto z+uh(w^Tz + b)$$ for fixed vectors $u, w$ , fixed scalar $b$ , and activation function $h$ . Further, it says in Kong et al. A ReLU planar/Sylvester flow is invertible under certain bounds on its parameters as ReLU is Lipschitz. It is a fundamental property that normalizing flow models are invertible and differentiable. So if we let $h(z) = ReLu(z) = \max(0, z)$ , then the neural network may be 1-1, but clearly will not differentiable. Is the reason that this is "ok" because the non-differentiable set is measure 0? If so, can we extend normalizing flow models to the class of invertible functions that are differentiable almost everywhere?
