[site]: datascience
[post_id]: 70001
[parent_id]: 69957
[tags]: 
When you compute the ROC, you're varying the decision threshold, while the confusion matrix and those metrics based on it are using a default threshold (probability 0.5 for the logistic regression, and the max-margin boundary of the SVM [which isn't meant to be probabilistic by itself]). So the logistic regression is doing at least something meaningful at lower thresholds , whereas the SVM continues to do badly. That the logistic regression model makes no positive-class predictions is probably just due to the default threshold of 0.5, whereas the SVM's decision boundary doesn't actually separate any positive-class test points. For such an imbalanced dataset, sometimes the auROC can be misleading: if many of the majority class are "easy" to identify as such, then the auROC can be quite high, even if the rest of the majority and minority classes are thoroughly scrambled. (Whether this is "right" or not depends on your point of view, and the context.)
