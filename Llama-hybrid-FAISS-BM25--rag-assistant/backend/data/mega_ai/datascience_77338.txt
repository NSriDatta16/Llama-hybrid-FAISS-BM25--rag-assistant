[site]: datascience
[post_id]: 77338
[parent_id]: 
[tags]: 
Difference between stacked lstm vs attention mechanism in LSTM

What is the difference between stacked lstm vs attention mechanism in LSTM? It seem to me that both produce the same context vector at the end. EDIT: From suggestion by @shepan6, the difference in architecture of stacked ltsm and attention mechanism not too sure i am right here, it seem that both used the notion of being able to select important features to construct the final context vector. if the answer to above is yes, am i right to say the goal is the same for both, just that the mechanism is different Thanks
