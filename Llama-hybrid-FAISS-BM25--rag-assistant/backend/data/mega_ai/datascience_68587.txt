[site]: datascience
[post_id]: 68587
[parent_id]: 68555
[tags]: 
A lot of the values from your data seem to be of the same size, or very small difference, also it seems it is important to deal with noise. The same problem is true for image compression. I think a good approach would be to use Haar-Wavelet Transform which compresses the information by a very huge amount and reduces the noisy data, like it does for image compression. After the transofrm, you need to apply quantization on the resulted data, here you can simply set all values to zero which have the smallest absolute value up until some bound, the remaining values you can also quntize into groups, if their difference is less than some bound. By doing this, you get rid of all noise. From here you would have to specify more precisely, what you want to achieve next. You want to predict z? For this task you can simply use Decision Tree learning methods, using the maximum information gain to decide on which attribute to make the first splits. Random Forests, which use a lot of different Decision Trees, where each one can be seen as an expert on some concrete task, should give very good results on this data, a good quantization method is very important for trees. You can also implement them all by yourself, without using any black box libraries, you learn much more from implementier it yourself. "McGrawHill_-_Machine_Learning_-Tom_Mitchell" Has very well explanations on those algorithms, explaining the mechanics behind all those black box like approaches.
