[site]: crossvalidated
[post_id]: 435056
[parent_id]: 
[tags]: 
Why in Hamiltonian MCMC do we multiply the posterior distribution by the likelihood?

So maybe I am misunderstanding what the author is staying, but I am reading Chapter 14 of Kruschke's Doing Bayesian Analysis . I am reading about the software Stan and how it uses the Hamiltonian MCMC (HMC) to sample from the posterior distribution. To recap, HMC generates a proposal (jumping) distribution based on the gradient of the posterior distribution. To do HMC in Stan, we specify a model by defining both a distribution (or likelihood) for the data and the prior, for example: model { theta ~ beta(1,1) ; // prior y ~ bernoulli(theta) // likelihood } But then I read a blurb that says: In other words, in Stan, we are not directly randomly sampling from the posterior (like we would do in Gibbs where we sample from the conditional posterior). Instead, we determine a proposal point via the gradient of the posterior. What I don't get is why are we multiplying the posterior by the likelihood? I don't understand. Thanks!
