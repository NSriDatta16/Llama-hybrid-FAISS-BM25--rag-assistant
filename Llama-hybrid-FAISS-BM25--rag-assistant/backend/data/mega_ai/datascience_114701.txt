[site]: datascience
[post_id]: 114701
[parent_id]: 
[tags]: 
How do transformers differ from feature selection and regular machine learning?

This is perhaps a simplistic way of thinking, but to me transformers (attention based neural networks) focus on a subset of the input, learning what is important for the problem/prediction as the training goes on. How does this differ from regular feature selection and neural network training on a subset of the input?
