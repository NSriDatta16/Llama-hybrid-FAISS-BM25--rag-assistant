[site]: crossvalidated
[post_id]: 287680
[parent_id]: 241286
[tags]: 
I can precisely relate to your frustration, since for the past 2-3 days, I've been scratching my head with the exact same problem. Every blog post and follow-up paper that I read, just copy-pasted the exact same block diagram, with more focus on the addressing scheme, which to be honest is pretty straightforward. Technically, it was clear to me that you can just use the hidden state or the memory of the LSTM, and pass it through some network to produce the add, erase vectors, and the weights for the soft attention, but no paper ever confirmed it. However, if you'll look at the appendix of the " One-Shot Learning with Memory-Augmented Neural Networks" paper, you'll notice that they briefly mention that they use the LSTM's memory state as the key, as well as the value to write in the memory. So, there you go, this paper sort of confirms that you use the memory state of the LSTM to connect the controller with the read and write heads.
