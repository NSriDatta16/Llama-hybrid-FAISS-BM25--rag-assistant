[site]: crossvalidated
[post_id]: 467900
[parent_id]: 467704
[tags]: 
Interesting points from everyone. Let me contradict some. 1) Why Poisson? Cases generation process is intristically interdependent as a pandemic interaction between ill and healthy, so case occurence in a time interval maybe affected by the previous interval occurences. The dependency may be complicated but strong. UDPATE (as of May 23rd) 1.1) Imagine the physics of the process. a) A person is healthy -> b) They get infected from a covid-positive one -> c) they fill sick and go to a hospital -> d) they get screened after - and very likely - waiting in line, or time table slot -> e) the lab processes tests and determines new positives -> f) a report goes to a ministry and gets summarized for a daily report. I would like to insist again, after long discussion and downvotings I got, that when you see the stage F reports, you should understand that events occurred as a function of a lot of human interactions, and it is important they were accumulated to pass a "bottleneck" of either: their own time to visit a doctor, the doctor appointment time table, or laboratory test processing limits. All of these make it non-Poissonian, as we don't use the Poisson for events that wait in a line. I think that it is mostly about lab tests that are made by humans who work with average capacity and cannot process too many per day. It is also possible that the final reporting stage accumulates information in a sort of buckets. My point is that it is not Poisson, or generalization. It is the "Poisson with waiting in line and data accumulation in time periods". I don't see 100% evidence of "Soviet-style data manipulations". It could be just bulks of pre-processed data up to report. 2) For the Krasnodar region the daily mean seems to be non-stationary. It is not good at all to approach these data from Poisson view, or at least one should take only the stationary part of it. These points are about 2 major Possion distribution assumptions violations. 3) Why 100 tests per day? It is official information that in Russia (and I am in Russia, reading news constantly) there were 7.5 million tests made so far, and about 330,000 cases confirmed (as of May 22nd). The proportion of positives is less than 5%. With this, you should expect at least 2,000 tests per day allowed. This could be real, as the tests are scarce and expensive items and not only in the Krasnodar, Russia, or Europe. It is everywhere the same. @Aksakal (source: https://yandex.ru/covid19/stat?utm_source=main_title&geoId=225 ) 4) Why ever would you think these are "Soviet data"? Look at the World data for new covid cases. It is extremely low-variance if you think it must be Poisson (a sum of Poissons is a Poisson). Is the World "Soviet" (I guess you mean lying?) then? @Ben - Reinstate Monica (source: https://yandex.ru/covid19/stat?utm_source=main_title&geoId=225 ) So, it seems to me that Statistics application in the case of pandemic is a dangerous thing. Lots of assumptions of all kinds must be true to conclude what have been concluded. UPDATE To address the point about the world data under/overdispersion, library(data.table) library(magrittr) dat % .[, date:= as.Date(date)] %>% .[date >= '2020-04-01'] %>% setorder(date) min(dt$V1) max(dt$V1) mean(dt$V1) var(dt$V1) var(dt $V1) / mean(dt$ V1) # huge overdispersion, indeed plot(dt$V1,type='l') acf(dt$V1) I got data for April, 1st till today (as a more stationary, plateu phase). The calculation showed that variance to dispersion ratio is 1083. This is huge overdispersion. My naked-eye analysis was wrong. There is significant weekly autocorrelation present. This can be one of the reasons for higher variance, but is it enough? And why is there a daily pattern? Is it still the Poisson process or lying statistics worldwide?
