[site]: crossvalidated
[post_id]: 482069
[parent_id]: 
[tags]: 
Computational complexity of backpropagation vs. forward propagation

I'm trying to figure out the computational complexity of one hidden-layer neural network with sigmoid activation trained on square-error loss given as such: $$ \hat{y} = W_a \sigma(W_bx) $$ Given that $W_a$ , $W_b$ are matrices. I am trying to understand how to figure out the computational complexity of this simple neural network case, and how this can extend to multi-layer cases as well.
