[site]: crossvalidated
[post_id]: 521563
[parent_id]: 321054
[tags]: 
With respect to Deep Residual Learning for Image Recognition , I think it's correct to say that a ResNet contains both residual connections and skip connections, and that they are not the same thing . Here's a quotation from the paper: We hypothesize that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers. The concept of pushing the residual to zero indicates that the residual connection corresponds to layers that are learned rather than to the skip connection. I think it's best to understand a "ResNet" as a network that learns residuals . In the following image (figure 2 from the paper), the path going through the weight layers and relu activation is the residual connection while the identity path is the skip connection. The authors of Squeeze-and-Excitation Networks seem to have this understanding as well based on figure 3 from their paper. References https://arxiv.org/pdf/1512.03385.pdf https://arxiv.org/pdf/1709.01507.pdf https://tim.cogan.dev/residual-connections
