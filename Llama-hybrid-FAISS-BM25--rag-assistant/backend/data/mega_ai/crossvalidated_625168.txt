[site]: crossvalidated
[post_id]: 625168
[parent_id]: 625166
[tags]: 
Controls in Regression There is a lot of missing information about what you are trying to do from an experimental sense and what you have already done from a statistical perspective. It is unclear from the question, but it looks like you are applying a linear mixed model (LMM) to a question about financial outcomes, but that is unclear. It may be helpful to clarify those points more, but I will simply answer what is the biggest part of your question here, something I have found alarmingly common in my experience. I think it is a common misconception that multiple regression and other methods such as ANCOVA are not analogous because "you can't include control variables in a regression." This is false. Multiple regression by definition provides coefficients which are representations of each predictor's effect on the outcome after controlling for all other predictors. This is one of the reasons we have a linear equation in the first place. For example if we have a multiple regression such as the following: $$ y = \beta_0 + \beta_1 x1 + \beta_2 x2 $$ And their coefficients are derived as: $$ y = 200 + (2 * x1) + (4 * x2) $$ We know that for every unit change in $x1$ , we have a 2 point increase to the intercept, which is 200. For every unit change in $x2$ , we have a 4 point increase to the same intercept. These are independent of each other (as they are not modeled as interactions here). As an example, we can determine the individual effect of each predictor by simply zeroing out their coefficients. For example, if we want to only know the influence of $x2$ , we simply multiply $x1$ by zero, giving us: $$ y = 200 + (4 * x2) $$ Using the equation in this way, we can predict what $y$ should be either by the influence of one predictor alone or the combination of the two predictors. Example in R As an example using the iris dataset in the program R: #### Fit and Summarize #### fit Our coefficients are the following: (Intercept) Sepal.Width Petal.Width 2.2581635 -0.3550346 2.1556105 Giving us the linear equation: $$ \text{Petal Length} = 2.258 + (-0.355 * \text{Sepal Width}) + (2.156 * \text{Petal Width}ï¼‰ $$ We can predict petal length by petal width after controlling for sepal width simply by including it with new data. #### Predict on New Data #### new.data We now know that when the sepals of a flower are 4 inches wide and the petals are 3 inches wide, the predicted petal length should be: 1 7.304857 If we are only interested in the same petal width influence without the effect of sepal width, we arrange it like so: #### Only One #### new.data.2 Giving us a new prediction based on petal width when sepal width is equal to zero: 1 8.724995 You can see here that when we don't account for sepal width, our predictions for petal length are higher than we expected, indicating that the regression used more information when both predictors were included. Edit I have edited this answer now that more info has been provided about what you mean by "control." The short answer to your question is "it depends." I'm not an economist or whatever field you are in, so I can't say for certain if these are variables that usually confound the results. But if that is the case, then you can certainly include them in a regression, or create a baseline model to compare against another model with these controls in place if you feel that is important. I would think that being single, having more children, and race would all have some influence in some part, but whether they merit inclusion is up to you. Keep in mind that you could include a billion different predictors into a model. The key is to find a model of the most theoretical importance that balances parsimony with accuracy. I would recommend reading Hosmer and Lemeshow's book on logistic regression, which has a section dedicated to model selection (and which variables merit inclusion). The relevant discussion is Chapter 4 of their book (Page 89), which has many useful points related to this. Citation Hosmer, D. W., Lemeshow, S., & Sturdivant, R. X. (2013). Applied logistic regression (Third edition). Wiley.
