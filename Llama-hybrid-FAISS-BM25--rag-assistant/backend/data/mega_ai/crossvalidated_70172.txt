[site]: crossvalidated
[post_id]: 70172
[parent_id]: 70168
[tags]: 
This is just a simple computation of the partial derivative and observation, that the derivative on the layer $i$ (from top) can be fully computed using partial derivative for weights in layer $i-1$. This applies to any number of layers, but this leads to so called "vanishing gradient phenomenon" which is a reason for not using multiple hidden layers in general (at least with basic architecture and basic training ). To overcome this issue, deep learning has been proposed in recent years (like for example Deep Convolutional Networks, Deep Belief Networks, Deep Autoencoders, Deep Boltzmann Machines etc.)
