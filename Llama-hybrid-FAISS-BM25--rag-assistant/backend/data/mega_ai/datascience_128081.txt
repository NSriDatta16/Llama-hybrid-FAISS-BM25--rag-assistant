[site]: datascience
[post_id]: 128081
[parent_id]: 
[tags]: 
Why is my LSTM model not predicting well when predicting labels for a new dataset?

I have a 15 timeseries datasets with 25-30 columns and is labeled by following a complex formula applied on the 25-30 columns. When training, I split the datasets as training datasets and unseen datasets (12 datasets for training, 3 unseen datasets for inferencing) only the data from the training datasets are used for training the lstm model. and the data from unseen datasets is used for inferencing after training. After joining the training datasets, preprocessing and reshaping the data for lstm model, oversampling, and finally shuffling the data, I train-test-split the data 70% for training 30% for testing (70% of training datasets will be used for training, 30% of the training datasets will be used for testing). After training the model I get 98% accuracy on training data as well as on testing data. However, when I try to infer it on the unseen labeled dataset (not given to model while training and testing), it performs poorly Things to note :- I am performing the same exact preprocessing and reshaping as I did for training. The data in the unseen dataset is somewhat similar to training data. The model is not overfitting, I looked at the training and validation loss graph, and it is nowhere near overfitting. If it was overfitting, i would not have gotten 98% accuracy on testing data. the training data is diverse enough. I synthetically generated the training data from the original data such that the values in columns are hitting all the ranges i.e. each column has normal distribution of data. I want my LSTM model to learn the underlying formula rather than memorizing the data and its labels. If my model can learn the underlying formula, then no matter what data I give it, it will always follow the formula and always give correct output on unseen data. this is my model:- def create_lstmfcn_model(MaxTimeslice, H, LR, num_classes): ip = Input(shape=(MaxTimeslice, H)) x = LSTM(64, return_sequences=True)(ip) x = LSTM(32, return_sequences=True)(x) x = LSTM(16)(x) y = Permute((2, 1))(ip) y = Conv1D(32, 20, padding='same')(y) y = BatchNormalization()(y) y = Activation('relu')(y) y = Conv1D(16, 15, padding='same')(y) y = BatchNormalization()(y) y = Activation('relu')(y) y = Conv1D(32, 10, padding='same')(y) y = BatchNormalization()(y) y = Activation('relu')(y) y = GlobalAveragePooling1D()(y) x = concatenate([x, y]) x = Dense(units=16, activation='relu', )(x) multiclass_output = Dense(units=num_classes, activation='softmax',)(x) model = Model(inputs=ip, outputs=multiclass_output) model.compile(loss="categorical_crossentropy", metrics=["accuracy"], optimizer=RMSprop(learning_rate=LR)) return model
