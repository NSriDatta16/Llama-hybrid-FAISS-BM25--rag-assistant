[site]: crossvalidated
[post_id]: 239138
[parent_id]: 239134
[tags]: 
Probabilistic forecasts (or, as they are also known, density forecasts) can be evaluated using scoring-rules , i.e., functions that map a density forecast and an observed outcome to a so-called score, which is minimized in expectation if the density forecast indeed is the true density to be forecasted. Proper scoring rules are scoring rules that are minimized in expectation only by the true future density. There are quite a number of such proper scoring rules available, starting with Brier (1950, Monthly Weather Review ) in the context of probabilistic weather forecasting. Czado et al. (2009, Biometrics ) give a more recent overview for the discrete case. Gneiting & Katzfuss (2014, Annual Review of Statistics and its Application ) give an overview of probabilistic forecasting in general - Gneiting in particular has been very active in advancing the cause of proper scoring rules. However, scoring rules are somewhat hard to interpret, and they really only help in comparing multiple probabilistic forecasts - the one with the lower score is better. Up to sampling variation, that is, so it's always better to have a lot of forecasts to evaluate, whose scores we would average. How to include the "updating" of Silver's or others' forecasts is a good question. We can use scoring rules to compare "snapshots" of different forecasts at a single point in time, or we could even look at Silver's probabilistic forecasts over time and calculate scores at each time point. One would hope that the score gets lower and lower (i.e., the density forecasts get better and better) the closer the actual outcome is.
