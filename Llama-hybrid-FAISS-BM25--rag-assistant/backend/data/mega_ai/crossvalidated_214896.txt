[site]: crossvalidated
[post_id]: 214896
[parent_id]: 214890
[tags]: 
I'll tackle your second question first. Your method doesn't sample from the beta distribution, but you're sort of right in that there are simple methods that work well when you know the pdf. Say that the pdf of the beta(3, 6) distribution is $p(x)$ and you want to know $y \equiv \int_0^1 f(x) p(x) dx$. Then you could do something like this: n_draws = 10000; x = unifrnd(0, 1, n_draws, 1); px = betapdf(x, 3, 6); y = f(x)' * px / n_draws; Or even better, don't sample: x = 0:.0001:1; px = betapdf(x, 3, 6); y = f(x) * px' / length(x); However, this only works on the beta distribution because its support is on $[0, 1]$. You wouldn't be able to apply such a method to a distribution with unbounded (or unknown) support. However, if you know the CDF, you can use inverse transform sampling to take draws. Now for your second question: If we know the PDF, why would we need a complicated sampling method? Often, you may know the PDF only up to a constant. Then you can't use inverse transform sampling to take draws. However, you can still use a method like importance sampling or Gibbs sampling. For example, let's say we want a function of the posterior distribution in a Bayesian model, where $\theta$ is the parameter of interest, $x$ is data, and $\Theta$ is the set of all parameters in the support of the prior: $$ p(\theta | x) = \frac{p(x | \theta) p(\theta)}{\int_\Theta p(x | \theta') p(\theta') d \theta'} $$ It may be easy to compute $p(x|\theta) p(\theta)$, but the denominator may be a lot harder. However, you can just note that $p(\theta | x) \propto p(x|\theta) p(\theta)$, and use that fact to use importance sampling, Gibbs sampling, or some other Markov Chain Monte Carlo method to take samples from $p(\theta | x)$. Then you can use those draws to compute a posterior mean or another function of the posterior distribution.
