[site]: datascience
[post_id]: 58948
[parent_id]: 
[tags]: 
Why the Logistic regression model trained with tensorflow performed so poor

I trained a logistic regression model with tensorflow but the accuracy of the model was so poor (accuracy = 0.68). The model was trained using simulated dataset and the result should be very good. is there something wrong with the code ? #simulated dataSet sim_data $mort,predictor = pred,ci=T) predBi = 0.5 crossTab mort) (crossTab[1]+crossTab[4])/sum(crossTab) #choose different cutoff for the accuracy DTaccuracy = cutoff; crossTab $placeholder(tf$ float32, shape(NULL, ncol(x)), name = "X") Y = tf $placeholder(tf$ float32, shape(NULL, 2L), name = "Y") #we will define the parameters. We will randomly initialize the weights with mean “0” and a standard deviation of “1.” We will initialize bias to “0.” W = tf $Variable(tf$ random_normal(shape(ncol(x),2L), stddev = 1.0), name = "weghts") b = tf $Variable(tf$ zeros(shape(2L)), name = "bias") #Then we will compute the logit. logits = tf $add(tf$ matmul(X, W), b) pred = tf $nn$ sigmoid(logits) #The next step is to define the loss function. We will use sigmoid cross entropy with logits as a loss function. entropy = tf $nn$ sigmoid_cross_entropy_with_logits(labels = Y, logits = logits) loss = tf$reduce_mean(entropy,name = "loss") #The last step of the model composition is to define the training op. We will use a gradient descent with a learning rate 0.1 to minimize cost. optimizer = tf $train$ GradientDescentOptimizer(learning_rate = learning_rate) $minimize(loss) init_op = tf$ global_variables_initializer() #Now that we have trained the model, let’s evaluate it: correct_prediction $equal(tf$ argmax(logits, 1L), tf $argmax(Y, 1L), name = "correct_pred") accuracy reduce_mean(tf $cast(correct_prediction, tf$ float32), name = "accuracy") #Having structured the graph, let’s execute it: with(tf $Session() %as% sess, { sess$run(init_op) for (i in 1:5000) { sess$run(optimizer, feed_dict = dict(X=x_train, Y=y_train)) } sess$ run(accuracy, feed_dict=dict(X = x_test, Y = y_test)) }) The accuracy obtained by glm() method is quite good (accuracy=0.95), which is as expected; however, the accuracy was only 0.68 with TensorFlow method. How can I solve the problem?
