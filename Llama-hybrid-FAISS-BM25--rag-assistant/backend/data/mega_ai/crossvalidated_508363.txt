[site]: crossvalidated
[post_id]: 508363
[parent_id]: 508268
[tags]: 
Standard implementation in R One potential confusion comes from the role of the intercept in the correction factor used by lm . If present, it shows up in both the numerator and the denominator of the correction factor. Quoting from an answer in the linked question : Extracting the most relevant line you get: ans $adj.r.squared r.squared) * ((n - df.int)/rdf) which corresponds in mathematical notation to: $$R^2_{adj} = 1 - (1 - R^2) \frac{n-1}{n-p-1}$$ assuming that there is an intercept (i.e., df.int=1 ), $n$ is your sample size, and $p$ is your number of predictors. Thus, your error degrees of freedom (i.e., rdf ) equals n-p-1 . Note the "assuming that there is an intercept" phrase in the first quote above. The above description holds for a situation where the number of predictors p does not include the intercept . That can be checked in the code for the lm.fit() function. The residual degrees of freedom (called df.residual in the lm.fit code but rdf in the summary.lm code) are calculated as df.residual = n - z$rank where n is the number of observations and z$rank is the rank of the fitted model z , the number of linearly independent parameter estimates returned. If the model has sufficient n and there is an intercept, that rank is the number of predictors other than the intercept plus 1 for the intercept . Without an intercept, that rank it is just the number of predictors. So if p is interpreted as the number of predictors not including the intercept , the formula in the above quote holds if the model includes an intercept. If there is no intercept, then df.int = 0 in the code quoted above and the rdf in the denominator is simply n-p . Thus without an intercept you get instead: $$R^2_{adj} = 1 - (1 - R^2) \frac{n}{n-p}$$ So one could say that are two "real adjusted $R^2$ formulas" used in lm depending on whether the intercept is included in the model. Recognizing that the denominator in either case is df.residual = n - z$rank , where z$rank is the rank of your model, should help reduce any confusion. Where the correction factors come from The Wikipedia page provides an explanation for where the $n-p-1$ in the denominator and the $n-1$ in the numerator come from, when the model includes an intercept: The principle behind the adjusted ''R'' 2 statistic can be seen by rewriting the ordinary ''R'' 2 as $$R^2 = {1-{\textit{VAR}_\text{res} \over \textit{VAR}_\text{tot}}}$$ where $\text{VAR}_\text{res} = SS_\text{res}/n$ and $\text{VAR}_\text{tot} = SS_\text{tot}/n$ are the sample variances of the estimated residuals and the dependent variable respectively, which can be seen as biased estimates of the population variances of the errors and of the dependent variable. These estimates are replaced by statistically unbiased versions: $\text{VAR}_\text{res} = SS_\text{res}/(n-p-1)$ and $\text{VAR}_\text{tot} = SS_\text{tot}/(n-1)$ . The extension to the above formula for a no-intercept model seems direct. Other adjustments Although the above standard $R_{adj}^2$ effectively incorporates unbiased estimates of total and residual variance, it is not itself unbiased. In particular, that $R_{adj}^2$ estimate can take on negative values even though the underlying population multiple squared correlation coefficient (between observed and linearly-predicted values) is necessarily non-negative. That point is raised in the OP and in the answer from @Carl. But as Olkin and Pratt (1958) put it in their work on the unique uniformly minimum variance unbiased estimator of $\rho^2$ (page 211): We cannot hope for a non-negative unbiased estimator, since there is no region in the sample space having zero probability for $\rho^2=0$ and positive probability for $\rho^2 > 0$ . There is an extensive literature on attempts to obtain improved adjusted estimates. Karch (2020) recently compared 20 different versions of adjusted $R^2$ , and provided an exact implementation of the Olkin-Pratt estimator. The comparison included both the original estimators and the corresponding "positive-part shrinkage estimators" proposed by Shieh (2008) , which keep non-negative values as is and set negative values to 0. The Olkin-Pratt estimator was the clear choice for an unbiased minimum mean-square error (MSE), but unbiased estimators could improve on MSE. Karch notes (page 7): No estimator had uniformly lowest MSE across all conditions. Even stronger, no estimator was uniformly best according to the maximum or average MSE perspectives. However, across all conditions, the positive-part version of the most widely used Ezekiel (adjusted $R^2$ ) estimator performed best both according to the maximum MSE as well as average MSE perspective. What Karch calls the Ezekiel estimator is the formula used in the standard R implementation described above, called McNemar's formula in the OP. "The real adjusted $R^2$ formula" There is no single "real" formula that will work the best in all situations, as Karch demonstrates. The choice is based on what tradeoffs one wishes to make with respect to bias and MSE, and how you wish to deal with the possibility of negative sample estimates. Nevertheless, Karch has also shown that the standard adjusted $R^2$ formula works remarkably well in practical situations with reasonable ratios of observations to predictors and non-negative population values of $\rho^2$ as low as 0.01. For more extreme situations, with even lower population $\rho^2$ values or barely more observations than predictors, the proposal in the answer from @Carl (inverse Fisher transformation of an adjusted Fisher z-transformed correlation coefficient) might have some advantages.
