[site]: datascience
[post_id]: 108687
[parent_id]: 108669
[tags]: 
There are many options, but there are two common ones: crossentropy for classification and mean squared error (MSE) for regression. $$ \text{Crossentropy}\\ L(y, \hat y) = -\dfrac{1}{N}\sum_{i=1}^N \bigg[y_i\log(\hat y_i) +(1 - y_i)\log(1 - \hat y_i)\bigg] $$ $$ \text{MSE}\\ L(y, \hat y) = \dfrac{1}{N}\sum_{i=1}^N \bigg(y_i - \hat y_i\bigg)^2 $$ In both cases, the predicted $\hat y_i$ is a function of the weights and biases in the model (I show that here ). Also, there is an extension of crossentropy for when there are multiple classes. It follows from maximum likelihood estimation with a multinomial $y_i$ (as opposed to the binomial $y_i$ that yields the equation I gave). However, you can pick lots of other loss functions that have varying degrees of utility. There are analogues of quantile regression, generalized linear models, etc, much as crossentropy and MSE give neural network analogues of logistic and linear regression, respectively. Given a fixed training sample, we can view the cost function as a function of the weights and biases, and thus optimizing this function is just finding the minimum of this function. The word “just” makes this sound easier than it winds up being, and that difficulty leads to computational techniques like using batches for the numerical optimization. However, you are correct that the goal is to find weights and biases that minimize the loss function (same as in least squares linear regression).
