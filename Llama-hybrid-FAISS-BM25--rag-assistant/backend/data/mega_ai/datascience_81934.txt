[site]: datascience
[post_id]: 81934
[parent_id]: 56210
[tags]: 
The other values are maintained from the existing q values. If you are using deep Q network, your replay memory will have a records of you are right the state is enough for all inputs. Bur the action and reward in the replay memory belong to a single output. To train it for all using the minibatch, first you fed the neural network with each state. Then, you record the Q values of the fed states for all outputs. Then, leaving the other Q values intact, you only update the Q values of the selected action with its reward using the bellman equation. Finally, you will have a data set of states and Q values for all outputs. You then train the neural network as the usual supervised learning.
