[site]: crossvalidated
[post_id]: 286511
[parent_id]: 286499
[tags]: 
LSTM, GRU, CNN, NARX etc. are all network architectures . Each of them has certain qualities which can't be learned - shared weights, recurrent connections, self connections, gates - these all must be in place before the learning process starts. The learning process is aimed at tuning the parameters, but the architecture stays unaffected by this process. Ofcourse, you could combine LSTM and a CNN, but regardless of the input, the network will still act as an LSTM and CNN. You could have some special training data, that requires LSTM for parts of that data and CNN for other parts of the data - the network will learn to make the outcome of the CNN part of the network less important than the LSTM part (if they are parallel!), but the architecture does not get affected.
