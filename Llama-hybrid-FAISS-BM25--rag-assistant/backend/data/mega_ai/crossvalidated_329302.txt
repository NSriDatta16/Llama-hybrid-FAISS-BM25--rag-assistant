[site]: crossvalidated
[post_id]: 329302
[parent_id]: 329221
[tags]: 
What you did is simple 5-fold cross validation. However, you cannot use the test set to select the best of the five models. You should select your best split using only the first hundred samples, and then you can evaluate it on the test set. Usually, the following is done: Using cross-validation you tune the hyperparameters of your model (in case of neural networks these are number of layers, number of units per layer, dropout rate etc.) Once you have the optimal hyperparameters set, you use all of the training data to train the final model You evaluate the final model on the test set When in doubt what can you do with the test set, imagine it is new, unseen data that the model will have to classify in the future. That is why you cannot use it to select the best model - once you do, it is no longer "unseen" data.
