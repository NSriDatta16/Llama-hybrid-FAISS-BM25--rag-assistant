[site]: crossvalidated
[post_id]: 595948
[parent_id]: 
[tags]: 
When to use Padding when Randomly Cropping Images in Deep Learning?

I am seeing these two options to process mini-imagenet images during training: Option 1 torchmeta: from torchmeta.datasets.helpers import miniimagenet normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) data_augmentation_transforms = transforms.Compose([ transforms.RandomResizedCrop(84), transforms.RandomHorizontalFlip(), transforms.ColorJitter( brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2), transforms.ToTensor(), normalize]) Option 2 l2l (I think rfs too or something similar in rfs): elif data_augmentation == 'lee2019': normalize = Normalize( mean=[120.39586422 / 255.0, 115.59361427 / 255.0, 104.54012653 / 255.0], std=[70.68188272 / 255.0, 68.27635443 / 255.0, 72.54505529 / 255.0], ) train_data_transforms = Compose([ ToPILImage(), RandomCrop(84, padding=8), # todo: do we really need th padding = 8 for delauny, check notes, check l2l git issues if not post one ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4), RandomHorizontalFlip(), ToTensor(), normalize, ]) which one should I choose? I was going to go for the random crop with padding = 8 since that would make the model more robust to contours that have nothing to do with the image. But I'm honestly being arbitrary. Are there (good) reasons to prefer one vs the other? related: https://github.com/camillegontier/DELAUNAY_dataset/issues/3
