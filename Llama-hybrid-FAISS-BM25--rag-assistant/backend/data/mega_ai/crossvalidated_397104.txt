[site]: crossvalidated
[post_id]: 397104
[parent_id]: 397039
[tags]: 
First of all, the methodology of choosing a model randomly and not evaluating all the data using all the models is indefensible. Especially so in case of linear regression, where you can evaluate hundreds of thousands of models in one second. You do not save any resources and you lose a lot of precious knowledge. With that out of the way, many different approaches are possible. Each approach is based on different assumptions. If you do not need to be very rigorous and just want some quick results, I would use the Friedman test . From the Wikipedia page : Classic examples of use are: $n$ wine judges each rate $k$ different wines. Are any of the $k$ wines ranked consistently higher or lower than the others? here substitute "judges" for "houses" and "wines" for "models". Each house tells you which model computes the price with biggest precision. Of course, you would have to abandon your random model selection. But what if you do not want to abandon the random model selection and, on top of that, are ready to get your hands dirty with some math? In this case, you can formulate the problem in terms of Bayesian statistics. I will illustrate the reasoning for two models, but it is of course generalizable to any number of models. Let us assume that the fractions of all homes that models $M_1, M_2$ estimate correctly is $f_1,f_2$ . Then if in the absence of any data we have no reason to believe that any fraction is more likely than any other, we may express our prior beliefs about $f_1$ and $f_2$ using a uniform probability distribution in $[0; 1] \times [0; 1]$ . In other words, $p(f_1), p(f_2) \sim Beta(1,1)$ . Then, as described in this great answer , with each evaluation of the given model, you either bump up the $\alpha$ or $\beta$ parameter of the proper Beta distribution, depending on whether the model was successful for this particular house or not. At each point in time you can ask what is the probability that $f_1 . Thanks to the uniform joint prior, $p(f_1 | data), p(f_2 | data)$ are independent and we can calculate the given probability using the integral: $$ P(f_1 Where $f_{\alpha, \beta}$ and $F_{\alpha, \beta}$ are the PDF and CDF of the Beta distribution. You can solve this integral numerically to get your probability.
