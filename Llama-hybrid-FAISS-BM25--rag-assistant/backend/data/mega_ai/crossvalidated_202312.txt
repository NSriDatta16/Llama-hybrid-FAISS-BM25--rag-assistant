[site]: crossvalidated
[post_id]: 202312
[parent_id]: 
[tags]: 
Does this pattern indicate over-fitting in machine learning?

I am working on a diagnostics project, and trying to improve the performance of a classifier(s). We have over a million features to choose from, so feature selection is a real challenge. To look for evidence of over-fitting, I analyzed the "agreement" (a form of classification accuracy) for our classifiers on both the training and test data. Do the patterns in the attached figure prove over-fitting? The picture will probably appear to small to read at first glance, but if you download the figure, the resolution is high enough that you can zoom in to look at each graph individually. Some background you may need to know: Feature selection is done on each fold of a cross-validation loop, so features can be different between folds. The data shown was generated by first running the cross-validation to select features as I normally do, then the ranked features were added one at a time (starting with the highest ranked) until all features were added back to the model. As each feature was added, the classifier was trained on the training set to the new group of features, then classification of the training set and the test set was performed. I believe there is evidence of over-fitting in some folds (such as fold 1) but maybe not in others (such as fold 6). The volatility has me concerned, and makes the interpretation more difficult. If I am over-fitting in some folds but not in all, any good ideas on how to do feature selection to stop over-fitting on some folds, while maintaining agreement as high as possible on the others that are not over-fitting? Thanks in advance.
