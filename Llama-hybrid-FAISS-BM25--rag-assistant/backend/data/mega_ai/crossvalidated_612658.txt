[site]: crossvalidated
[post_id]: 612658
[parent_id]: 
[tags]: 
Generalize the 1SE rule to elastic net

When you do LASSO or ridge regression, and pick the hyperparameter using cross-validation, the 1SE rule suggest to select not the best CV result but the one with the most penalization that's still within 1SE of the best value. That's meant to be a good approximation to accounting for the overfitting to the validation set that occurs by picking the hyperparameters on the validation set itself. Once I move to elastic net regression (with both a L1- and a L2-penalty), it is less clear what an equivalent rule would be. That's because there will be a whole curve in 2D-space that will be the boundary of where you achieve within 1SE of the best CV result and it's not really clear what's "more penalization" (more of a L1 or more of a L2 or some combination thereof). Is there any work that has looked into this? Is there some clever averaging approaches? Like take solutions all along the curve and model average them (I guess that's as easy as averaging coefficients taking 0 for models where one is not selected for linear regression, but involves more formal model averaging in case of GLMs with non-linear link functions?)? I'm also interested in whether we know if with two hyperparameters any such way of picking hyperparameters enjoys a similar "approximate optimality" as the 1SE rule.
