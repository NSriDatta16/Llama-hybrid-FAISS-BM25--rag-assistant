[site]: datascience
[post_id]: 76375
[parent_id]: 76304
[tags]: 
I can do this with GridSearchCV() , but is this correct to do with a random forest? Yes, this is perfectly valid. It ignores the oob-score feature of random forests, but that isn't necessarily a bad thing. See e.g. https://stats.stackexchange.com/a/462720/232706 What is the convention to hyper-parameter tune with Random Forest to get the best OOB score in sklearn? Can I just loop through a set of parameters and fit on the same training and testing set? I believe this would be the standard way of tuning using oob score, except that there is no testing set in this case. (You'll probably want a test set for future performance estimation of the final selected model though: selecting hyperparameters based on those oob scores means they are no longer unbiased estimates of future performance, just as in k-fold cross-validation! Your hyperparameter-candidate models shouldn't see that test set.) To take advantage of the various conveniences of the hyperparameter searches in sklearn (parallelization, saved results, refitted best model, etc.), you can hack it as in my answer to another question: https://datascience.stackexchange.com/a/66238/55122 You can't directly use oob score in a GridSearchCV because that's coded to apply your scoring function to the test fold in each split.
