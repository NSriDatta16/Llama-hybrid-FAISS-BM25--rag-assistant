[site]: crossvalidated
[post_id]: 147999
[parent_id]: 147986
[tags]: 
If I understand you correctly, you have multiple samples for which you know the mean age, the percentage that pass a task, and the sample size. So, you can easily calculate the actual number of children that pass the task in each sample. In that case, you could analyze these data using logistic regression. Here is an example, analyzed in R: mage These made-up data look like this: Now we can fit a logistic regression model to these data with: summary(glm(cbind(xi,ni-xi) ~ mage, family='binomial')) This yields: Call: glm(formula = cbind(xi, ni - xi) ~ mage, family = "binomial") Deviance Residuals: Min 1Q Median 3Q Max -1.3894 -0.9942 -0.2090 0.5609 1.6286 Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) -2.61092 0.60499 -4.316 1.59e-05 *** mage 0.20985 0.05365 3.912 9.17e-05 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 26.4329 on 7 degrees of freedom Residual deviance: 9.7293 on 6 degrees of freedom AIC: 41.187 Number of Fisher Scoring iterations: 4 So, for these data, there is a statistically significant increase in the log odds of passing the task as the mean age of the sample increases. Now there may be overdispersion in such data. One way of modeling this is via a mixed-effects logistic regression model. This can be done with the lme4 package. This will fit such a model: library(lme4) id This yields: Generalized linear mixed model fit by maximum likelihood (Adaptive Gauss-Hermite Quadrature, nAGQ = 7) [glmerMod] Family: binomial ( logit ) Formula: cbind(xi, ni - xi) ~ mage + (1 | id) AIC BIC logLik deviance df.resid 15.7 15.9 -4.8 9.7 5 Scaled residuals: Min 1Q Median 3Q Max -1.1192 -0.8742 -0.1753 0.4892 1.4364 Random effects: Groups Name Variance Std.Dev. id (Intercept) 0.02967 0.1722 Number of obs: 8, groups: id, 8 Fixed effects: Estimate Std. Error z value Pr(>|z|) (Intercept) -2.66660 0.69286 -3.849 0.000119 *** mage 0.21442 0.06125 3.501 0.000464 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Correlation of Fixed Effects: (Intr) mage -0.971 The additional variance component is slightly positive, but the conclusion is the same. Since you mentioned meta-analysis -- one can indeed think of this as a sort of meta-analysis of proportions. When adding predictors to a meta-analytical model, the commonly used term for the analysis is meta-regression . In fact, the metafor package provides exactly the functionality illustrated above. So, if you want to think of your data in such a way, then you could do: library(metafor) rma.glmm(measure="PLO", xi=xi, ni=ni, mods = ~ mage, method="FE") rma.glmm(measure="PLO", xi=xi, ni=ni, mods = ~ mage, method="ML") The first call to rma.glmm() will in essence reproduce the results from the logistic regression model, while the second call provides the results from the mixed-effects logistic model. The output for the latter looks like this: Mixed-Effects Model (k = 8; tau^2 estimator: ML) tau^2 (estimated amount of residual heterogeneity): 0.0297 tau (square root of estimated tau^2 value): 0.1722 I^2 (residual heterogeneity / unaccounted variability): 12.51% H^2 (unaccounted variability / sampling variability): 1.14 Tests for Residual Heterogeneity: Wld(df = 6) = 9.0825, p-val = 0.1690 LRT(df = 6) = 9.7293, p-val = 0.1365 Test of Moderators (coefficient(s) 2): QM(df = 1) = 12.2566, p-val = 0.0005 Model Results: estimate se zval pval ci.lb ci.ub intrcpt -2.6666 0.6928 -3.8487 0.0001 -4.0246 -1.3086 *** mage 0.2144 0.0612 3.5009 0.0005 0.0944 0.3345 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 As you can see, these are the same results. Based on such a model, you could plot the results as a forest plot. Maybe something like this: res The light-gray polygons are the predicted proportions for each study/sample.
