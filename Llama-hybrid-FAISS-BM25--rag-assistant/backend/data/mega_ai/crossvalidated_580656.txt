[site]: crossvalidated
[post_id]: 580656
[parent_id]: 580647
[tags]: 
I think your intuitive understanding is correct. As for why it poses a conceptual problem for so many, I can only offer a guess. Most statistical procedures are designed around means: comparing means of groups, evaluating how the mean of one variable changes across gradients in other variables, etc. But we are occasionally very interested in evaluating hypotheses about or predicting extremes: the fastest runner, the best-performing stock, etc. Adapting the statistical procedures and mental toolkit/shortcuts we have developed that is based on means to deal with extremes can be nontrivial. It's often not as simple as doing a quantile regression, because the baseline itself has poor (or perhaps 'unstable' is a better term) statistical properties, as you note in your question. If people have not grappled with the importance of this baseline problem before, it can be challenging to understand because it is fairly fundamental and requires revisiting (and revising) some basic ideas. This can be especially challenging when dealing with time series, which poses its own distinct set of challenges even when we are not interested in extremes. In my experience, even experienced scientists can struggle with the idea. The fact that this seems obvious to you is excellent news - you have developed a mental toolkit which makes it easier for you to deal with this. Often, exposure to concepts like these early on in one's training can make otherwise difficult concepts clear. Choosing when to introduce ideas like this is an important challenge when designing courses for students: too early and it can confuse and seem too abstract, too late and it risks taking extra effort because of having to undo earlier mental models. EDIT: To add a realistic example that might shed some light on this. Imagine a business that tests all its employees for performance on some skill. The results show that there is clear room for improvement and the business could make some money by improving those skills. It would be too disruptive to retrain everyone , so instead it takes the people who score in the lowest 10% and puts them through extra training. At the end of the training, this group is tested again. And voila - their average score is higher than it was in the original test! The training has worked, the business has made a useful investment, and should now make more money - right? Not necessarily. If you understand regression to the mean, you would realise that just testing the bottom 10% a second time should lead to a higher average score, even without any additional training . This is because these individuals' position in the bottom 10% on the first test is because of a combination of (probably poor) ability and random factors that happen to reduce performance in that test. Since they were in the bottom 10%, those random factors were likely more negative in this instance; test them again and the random factors would probably not be quite so negative. They would seem to improve in performance, but it would be illusory. The crucial step here was the selection of the extreme values for extra training/testing - but this decision is entirely understandable ! It's just that it's easy to forget that inference and prediction are much trickier when you have selected an extreme sample (or a non-random sample more generally) . Put yourself in the place of a businessperson trying to improve performance, or even a student trained in simple statistical procedures - wouldn't every action taken by the business in the example seem reasonable?
