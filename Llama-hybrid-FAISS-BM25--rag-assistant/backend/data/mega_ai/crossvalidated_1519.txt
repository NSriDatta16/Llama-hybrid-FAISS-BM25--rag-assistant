[site]: crossvalidated
[post_id]: 1519
[parent_id]: 
[tags]: 
On univariate outlier tests (or: Dixon Q versus Grubbs)

In (most of) the analytical chemistry literature, the standard test for detecting outliers in univariate data (e.g. a sequence of measurements of some parameter) is Dixon's Q test. Invariably, all the procedures listed in the textbooks have you compute some quantity from the data to be compared with a tabular value. By hand, this is not much of a concern; however I am planning to write a computer program for Dixon Q, and just caching values strikes me as inelegant. Which brings me to my first question: How are the tabular values for Dixon Q generated? Now, I have already looked into this article , but I'm of the feeling that this is a bit of cheating, in that the author merely constructs a spline that passes through the tabular values generated by Dixon. I have the feeling that a special function (e.g. error function or incomplete beta/gamma) will be needed somewhere, but at least I have algorithms for those. Now for my second question: ISO seems to be slowly recommending Grubbs's test over Dixon Q nowadays, but judging from the textbooks it has yet to catch on. This on the other hand was relatively easy to implement since it only involves computing the inverse of the CDF of Student t. Now for my second question: Why would I want to use Grubbs's instead of Dixon's? On the obvious front in my case, the algorithm is "neater", but I suspect there are deeper reasons. Can anyone care to enlighten me?
