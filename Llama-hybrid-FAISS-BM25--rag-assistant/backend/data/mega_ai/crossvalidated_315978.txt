[site]: crossvalidated
[post_id]: 315978
[parent_id]: 
[tags]: 
Playing Atari: Q-function converges to zero!

I am trying to replicate the results of DeepMind's DQN paper ( https://arxiv.org/pdf/1312.5602v1.pdf ) for the particular case of Atari Pong. However, I am observing a (maybe not so) weird behavior: after a few game episodes (say, 5 or 6) my Q-network starts outputting zeros regardless of the input state! Initially, I thought this was due to a bug in my code, but after thinking for a while I realized that it somehow makes sense. The rewards $\lbrace r_t \rbrace$ are zero for the vast majority of the states $\lbrace s_t \rbrace$ (except for those where we score or concede a goal, getting a $+1$ or $-1$ reward, respectively) and the Bellman equation is: $Q(s_t, a_t) = r_t + \gamma \max_a Q(s_{t+1}, a)$ Thus, for $r_t=0$, the Bellman equation is satisfied (i.e., the loss is zero) if $Q(s,a)=0 \,\, \forall s,a$. Because zero rewards happen very often in Pong, it makes sense that the model is converging towards that solution... Any hints on how I can solve this? Remark 1: I think I do not have any bug in the Q-learning algorithm, since I could solve the Cart-Pole problem using the same code (with different Q-network and hyperparameters) Remark 2: I am using the network architecture and image preprocessing that were presented in the original paper. I have also implemented some extra details described in the link below, which I found while searching for a solution in this forum. https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/
