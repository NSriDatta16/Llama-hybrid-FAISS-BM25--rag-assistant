[site]: crossvalidated
[post_id]: 268944
[parent_id]: 
[tags]: 
Moulton Factor and Clustering of Standard Errors with Heteroscedasticity

When running a linear regression, standard errors may need to be adjusted for clustering. One way to do so is to multiply the "conventional" variance estimate by the Moulton factor, defined for equicorrelated regressors as $1 + [\frac{V(n_g)}{\overline{n}}+\overline{n} -1]*\rho_e$, where $\overline{n}$ represents the average cluster size (average number of observations per cluster), $n_g$ is the number of observations per cluster, $V$ is the variance and $\rho_e$ is the residual correlation. For a reference see for example Angrist and Pischke's "Mostly Harmless Econometrics" page 311. To my understanding, however, this Moulton factor, which comes up in every treatment of clustering, is derived for homoscedastic errors. In that case the standard errors estimated assuming homoscedasticity are multiplied by the square root of the Moulton factor. This is also what the Stata "moulton.ado" file provided on the website of "Mostly Harmless Econometrics" does. Does the same formula hold for heteroscedastic errors? Is it correct to multiply heteroscedasticity robust standard errors by this same factor? If not, how does one adjust for clustering parametrically (using the Moulton factor) while also dealing with heteroscedasticity?
