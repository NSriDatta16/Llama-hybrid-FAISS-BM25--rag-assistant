[site]: datascience
[post_id]: 92602
[parent_id]: 
[tags]: 
Intuition of "Head" in Attention models (Transformer)?

I keep seeing the "head" in attention models (transformers). Aside from the mathematical formula, could anyone please share the intuition behind the idea "head"?
