[site]: crossvalidated
[post_id]: 474663
[parent_id]: 474640
[tags]: 
Typically, it is unsupervised . But actually it can be either. Let's start with supervised anomaly detection. Supervised anomaly/outlier detection For supervised anomaly detection, you need labelled training data where for each row you know if it is an outlier/anomaly or not. Any modeling technique for binary responses will work here, e.g. logistic regression or gradient boosting. The typical application is fraud detection. Usually, one does not have labelled data, so one has to rely on unsupervised methods with their usual pros and cons. Unsupervised anomaly/outlier detection We have a "reference" training data at hand but unfortunately without knowing which rows are outliers or not. Here, it is tempting to let statistical algorithms do the guess work. Some of the typical approaches are: density based: local outlier factor (LOF), isolation forests. distance based: How far away is a row from the average e.g in terms of Mahalanobis distance? autoencoder: How bad can the row be reconstructed by an autoencoder neural network? model based: model each variable by the others and hunt for high residuals. ... Each of the techniques has its pros and cons. There is no approach that does somehow better than the rest for all types of problems. Note about dimensions and unsupervised detection algos For 1-2 dimensional data, you can plot the data and visually identify outliers/anomalies as points far away from the rest. For very high dimensional data, unsupervised anomaly detection is close to being a hopeless task due to the curse of dimensionality , which - in the sense of anomaly detection - means that every point eventually becomes an outlier.
