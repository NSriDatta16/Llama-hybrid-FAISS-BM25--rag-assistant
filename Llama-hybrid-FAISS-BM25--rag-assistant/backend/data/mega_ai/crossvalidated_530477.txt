[site]: crossvalidated
[post_id]: 530477
[parent_id]: 526571
[tags]: 
The identity transition matrix means the state can never change. What you’ve turned this into is a restricted kind of naive Bayes model over sequences, where each position in the sequence is independent given the class. Comparison to HMM Your model’s generative story is this: Sample a class $c$ . for i in 1…n: Sample an observation $x_i$ given $c$ Note the difference from the HMM generative story; you’ve pushed $c$ out of the loop. for i in 1…n: Sample a class $c_i$ given $c_{i-1}$ Sample an observation $x_i$ given $c_i$ Improving on this You can think of your identity-based model as a mixture of 0th-order Markov chains . That is, each observation doesn’t even depend on the previous observation—the usual Markov assumption! This suggests an easy improvement on the model: use a higher order Markov chain. Instead of having $x_i$ depend only on the class, you can have it depend on the previous observation—or maybe the previous two or three! (Reminder: your observations are which API calls are made.) A connection: language identification (for your bonus question) In natural language processing, n-gram language models are just nth-order Markov models over (e.g.) words. You may want to identify the (unknown) language of text with these models. It becomes a case of statistical inference. You can use that exact inference procedure to identify whether your computer is in a malware state. Is the text I see Polish or Spanish? Is my sequence of API calls malware or benign? Train one Markov chain on only Polish and one on only Spanish. Choose a prior probability for each category. Your model is now $p(c \mid \vec{x}) \propto p(\vec{x} \mid c) p(c)$ —a noisy channel model defined by Bayes’s rule. The first term is your Markov model, and the second is your prior belief. Using the resulting probabilities, you can make a decision about whether your program is malware. Reminder: Your identity transition HMM is a special case of what I just described!
