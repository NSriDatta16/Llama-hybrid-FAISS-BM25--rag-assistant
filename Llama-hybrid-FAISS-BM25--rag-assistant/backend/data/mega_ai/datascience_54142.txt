[site]: datascience
[post_id]: 54142
[parent_id]: 
[tags]: 
How to get the weights of a linear model by solving normal equation?

In chapter 6.1 of the book Deep Learning , the author tries to learn the XOR function by using a linear model (on page 168). Linear Model: $f(\mathbf{x};\mathbf{w},b)=\mathbf{x}^T\mathbf{w}+b$ MSE Loss: $J(\mathbf{w},b)= \frac{1}{4} \sum(f^*(\mathbf{x})-f(\mathbf{x;\mathbf{w},b})) $ , where $f^*(\mathbf{x})$ is the XOR function. Normal equation: According to the same book on page 107, the weights can be obtained by solving the gradient of the loss function, which will result in a normal equation (5.12). $\mathbf{w}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$ My Attempts: Since it is a XOR function, we know that if the input is $\mathbf{X}=\begin{bmatrix} 0 & 0 \\ 0 & 1 \\ 1 & 0 \\ 1 & 1 \\ \end{bmatrix}$ , then the corresponding output will be $\mathbf{y}=\begin{bmatrix} 0 \\ 1 \\ 1 \\ 0 \\ \end{bmatrix}$ . So I just plug everything into the normal equation as shown above. However, the solution I get is $\mathbf{w}= \begin{bmatrix} \frac{1}{3} \\ \frac{1}{3} \\ \end{bmatrix}$ . What am I doing wrong here? And also how to find the bias $b$ ?
