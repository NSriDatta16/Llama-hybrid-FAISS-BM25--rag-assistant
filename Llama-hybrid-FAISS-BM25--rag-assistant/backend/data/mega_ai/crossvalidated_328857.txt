[site]: crossvalidated
[post_id]: 328857
[parent_id]: 327514
[tags]: 
I think you might consider doing a Bayesian analysis of the data with a tool like Stan . This lets you put any knowledge/assumptions of the model that generated your data into account and also lets you extract interpretable information of the form "if the model is (approximately) correct, the probability of increase of more than X points is Y% and the probability of a decrease is Z%". However, the case of a single Likert might be quite tricky to model correctly (see an aside below on how the modelling difficulties could be avoided). A somewhat reasonable model could for example assume that there is some true continuous variable representing user's experience (let's call it $a_{1,2} \in (0,1) $ for pre- and post- condition respectively). For each person $i$, belonging to group $g(i)$, you observe a noisy realization of this (let's say $b_i$) which is Beta-distributed with mean $a_{g(i)}$ and some precision $\tau$. There is a set of thresholds $c_1 ... c_{10} \in (0,1)$ that transform $\beta$ into one of the likert answers. The quantity of interest would be $a_1 - a_2$. You could specify that with a Stan program of the form (not actually tested, but should give you a start) data { int N; //Number of observations int Y[N]; //The answers int group[N]; //1 - pre treatment, 2 - post treatment } parameters { //Mean satisfaction per group real a[2]; //True satisfaction of each person real b[N]; //Thresholds positive_ordered[10] c; //'b' gives the same amount of information about 'a' as 'tau' coin tosses //with probability 'a' real tau; } model { for(i in 1:N) { real expected = a[group[i]]; b[i] ~ beta(tau * expected, tau * (1 - expected)); Y[i] ~ ordered_logistic(b[i], c); } //Priors //We assume both a to be more likely close to 0.5, but with a lot of leeway a ~ beta(2,2); //We assume that 'b' give roughly between (e^0) = 1 and (e^2) ~= 7.4 //coin tosses worth of information about 'a' tau ~ lognormal(1,0.5); } You can obviously use your own assumptions about your data and as long as you can describe them in terms of probability distributions, Stan will let you make inferences with those assumptions. After you fit such a model, you get a set of samples from the posterior distribution and thus it is straightforward to ask questions about any aspect of the model. Stan is well tested and has a good community, you should give it a try :-). An aside: A single Likert question tends to be hard to model, so psychologist usually put multiple Likert questions representing the same construct (some coded negatively) and then take the average (after reversing the negatively-coded questions). The average tends to be a well-behaved, almost normally distributed variable. Also this reduces noise as different people often interpret the same word differently. In your case the items could have looked like: The task was easy to do. I did not know how to proceed with this task. I felt I could complete the task confidently. I did feel lost when trying to complete the task.
