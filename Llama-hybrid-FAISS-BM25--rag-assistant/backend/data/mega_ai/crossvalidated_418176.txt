[site]: crossvalidated
[post_id]: 418176
[parent_id]: 417401
[tags]: 
You might also consider the drivers behind 'Deep-learning-beats-all' trend you mention. Much of the hype around these techniques comes from the superiority of these methods in image recognition and natural language problems. These domains are defined by exceptionally large datasets (e.g. ImageNet > 14 million images, it's possible to find very large text corpora). So just by understanding why these methods are popular in the first place more or less answers why they are less used for time series (since time series datasets are much smaller). As an example of how short important time series datasets can be very small consider that if you wanted to model US GDP, the Federal Reserve has quarterly data going back to 1929, which is only about 360 datapoints!
