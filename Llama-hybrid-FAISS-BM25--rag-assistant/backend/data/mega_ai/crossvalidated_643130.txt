[site]: crossvalidated
[post_id]: 643130
[parent_id]: 
[tags]: 
Questions about the process of feature selection through feature importance

'Shap feature importance' was obtained through xgboost, and variables with the lowest feature importance were removed one by one from 50 variables until only 1 variable remained. As a result of monitoring performance based on auc, we were able to see that performance continued to decrease from when there were 5 variables remaining. And when 10 cross validation was performed, the gap with the auc value of the train set was also drastically reduced. So, when the same method was used using lgbm and randomforest, performance continued to decrease from when there were 5 variables remaining. I wonder what this means. Is it okay to interpret this as having 5 very important variables remaining? If these five variables are so important that the auc value of the model gradually decreases when these five are removed, does this mean that there are still important variables left? Or is it a result of the small number of features compared to the number of data? The number of data is 10,000.
