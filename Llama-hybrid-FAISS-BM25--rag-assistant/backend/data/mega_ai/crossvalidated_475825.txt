[site]: crossvalidated
[post_id]: 475825
[parent_id]: 474958
[tags]: 
As a preliminary note, I see that your equations seem to be dealing with the case where we only have a single explanatory variable and a single data point (and no intercept term). I will generalise this to look at the general case where you observe $n$ data points, so that the log-likelihood function is a sum over these $n$ observations. (I will use only one explanatory variable, as in your question.) For a logistic regression of this kind you have the observable values $Y_i|\mathbf{x}_i \sim \text{Bern}(\mu_i)$ with true mean values: $$\mu_i \equiv \mathbb{E}(Y_i|\mathbf{x}_i) = \text{logistic}(\boldsymbol{\beta}^\text{T} \mathbf{x}) = \frac{e^{\boldsymbol{\beta}^\text{T} \mathbf{x}}}{1+e^{\boldsymbol{\beta}^\text{T} \mathbf{x}}}.$$ The log-likelihood function is given by: $$\begin{align} \ell(\mathbf{y}|\mathbf{x},\boldsymbol{\beta}) &= \sum_{i=1}^n \log \text{Bern}(y_i|\mu_i) \\[6pt] &= \sum_{i=1}^n y_i \log (\mu_i) + \sum_{i=1}^n (1-y_i) \log (1-\mu_i) \\[6pt] &= \sum_{i=1}^n y_i \log (\mu_i) + \sum_{i=1}^n (1-y_i) \log (1-\mu_i) \\[6pt] &= \sum_{i=1}^n y_i \log(\boldsymbol{\beta}^\text{T} \mathbf{x}) - \sum_{i=1}^n y_i \log(1+e^{\boldsymbol{\beta}^\text{T} \mathbf{x}}) - (1-y_i) \log(1+e^{\boldsymbol{\beta}^\text{T} \mathbf{x}}) \\[6pt] &= \sum_{i=1}^n y_i \log(\boldsymbol{\beta}^\text{T} \mathbf{x}) - \sum_{i=1}^n \log(1+e^{\boldsymbol{\beta}^\text{T} \mathbf{x}}). \\[6pt] \end{align}$$ Logistic ridge regression operates by using an estimation method that imposes a penalty on the parameter $\boldsymbol{\beta}$ that is proportionate to its squared norm. (Note that you have stated this slightly incorrectly in your question.) It estimates the parameter $\boldsymbol{\beta} = (\beta_1,...,\beta_K)$ via the optimisation problem: $$\begin{align} \hat{\boldsymbol{\beta}}_\text{Ridge} &= \underset{\boldsymbol{\beta} \in \mathbb{R}^K}{\text{argmax}} \ \ \ \ \ell(\mathbf{y}|\mathbf{x},\boldsymbol{\beta}) - \lambda ||\boldsymbol{\beta}||^2. \\[6pt] \end{align}$$ Since the log-posterior is the sum of the log-likelihood and log-prior, the MAP estimator is: $$\begin{align} \hat{\boldsymbol{\beta}}_\text{MAP} &= \underset{\boldsymbol{\beta} \in \mathbb{R}^K}{\text{argmax}} \ \ \ \ \ell(\mathbf{y}|\mathbf{x},\boldsymbol{\beta}) + \log \pi(\boldsymbol{\beta}). \\[6pt] \end{align}$$ We obtain the result $\hat{\boldsymbol{\beta}}_\text{Ridge} = \hat{\boldsymbol{\beta}}_\text{MAP}$ by using the prior kernel $\pi(\boldsymbol{\beta}) \propto \exp(- \lambda ||\boldsymbol{\beta}||^2)$ so that $\log \pi (\boldsymbol{\beta}) = - \lambda ||\boldsymbol{\beta}||^2 + \text{const}$ in the above equation. Integrating to find the constant of integration gives the prior distribution: $$\pi(\boldsymbol{\beta}) = \prod_k \mathcal{N} \bigg( \beta_k \bigg| 0, \frac{1}{2\lambda} \bigg).$$ Thus, we see that ridge logistic regression is equivalent to MAP estimation if a priori the individual $\beta_k$ parameters are IID normal random variables with zero mean . The variance parameter for this normal distribution is a one-to-one mapping of the "penalty" hyperparameter in the ridge logistic regression --- a larger penalty in the ridge regression corresponds to a smaller variance for the prior. ( Note: For a related question showing LASSO and ridge regression framed in Bayesian terms see here .)
