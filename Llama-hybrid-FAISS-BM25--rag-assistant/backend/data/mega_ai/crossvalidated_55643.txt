[site]: crossvalidated
[post_id]: 55643
[parent_id]: 55599
[tags]: 
For starters you can use a simple A/B Testing significance test (lots of those online). Those will show you the difference between average values and certainty with which the test group is better than control group (or whatever you label them). If you really want to go into it, then I strongly recommend this blog post: How large should your A/B test sample size be? for understanding of how to set up tests and interpret the data properly; and this one: What you really need to know about mathematics of A/B split testing for some basic math behind. I urge you to firstly understand the concept of A/B testing and setting it up properly before actually going any further. The set up for both variations should be equivalent (best way to do it is to have both variations showing randomly for different users), that is eliminating the possibility that one had better exposure than another or daily/weekly fluctuations effect (i.e., if one was tested on Sunday, another on Monday - of course they'll be different). In this particular case of yours, you need not only average, but uncertainty in its value as well. Average doesn't make sense until you know the precision of your estimate. That is, test results can be represented as (be careful if your data is far from normally distributed - check if it is first): Variation A: AVG(a) ± STD(a) Variation B: AVG(b) ± STD(b) Difference: d ± unc(d) Here STD() is standard deviation, d = AVG(b) - AVG(a) is difference between the groups and unc(d) = SQRT(STD(a)^2 + STD(b)^2) is uncertainty in this difference. This uncertainty is crucial as it tells you that you can't distinguish any difference smaller than unc(d) (roughly speaking). So, when you actually calculate all the numbers you'll get something like: difference is -10±3 , which would mean that with high certainty A is better than B and difference is likely to be larger than 7. To dig into the actual confidence intervals go to Standard deviation article on Wikipedia, where it's well explained. I.e., for important tests it is recommended to use two standard deviation intervals (just multiply the uncertainty by 2) to have a strong case supporting the test result (in such case if the difference with its uncertainty interval doesn't overlap with 0, confidence would be ~95%). (also looking at your case, my Variation B would be equivalent to what you call A combined with B)
