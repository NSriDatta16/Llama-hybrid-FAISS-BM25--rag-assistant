[site]: datascience
[post_id]: 121290
[parent_id]: 
[tags]: 
How to properly do a k-fold cross validation?

I am trying to solve binary classification problem using deep neural networks. I want to compare different approaches (model architectures) and I have no hyperparameters which I want to tune. So my question is can I simply use K-fold cross validation here without splitting data to train and test in advance? I mean, I have a dataset and I don't split it to train and test, just take it as it is, do 10-fold splits, for each validation split I compute metrics (let's say accuracy). Then after models have been trained, I aggregate metrics over all splits and compare them. Is this approach valid?
