[site]: crossvalidated
[post_id]: 513688
[parent_id]: 513684
[tags]: 
Embeddings, as other neutral networks, need huge datasets. People usually scrape thousands of books, news articles and web pages for that. For audio files, this would mean thousands of hours of recordings, considering audio quality issues, probably even more. For English, this would mean something like scraping all the audio from Netflix etc. Are you sure you have this data? If not, recording it sounds like a project for years. Moreover, embeddings are usually trained in scenarios like “guess next word”, so the data would be manually labeled by someone to mark beginnings and ends of words. This alone is also a lot of work. The reasons above is why we have a lot of NLP models for English and not many for other languages. It is easy to scrape the Internet for thousands of documents in English. Same for written language: you don’t have audio quality issues to worry about, written language is already recorded in unified form, words are easily separable in most languages etc. With audio there’s much more additional preprocessing needed. You are correct about the other issues mentioned in bullet points by you. You would probably need samples diverse in terms of regional variations of pronunciation, genders, age, education, etc. to make the data representative.
