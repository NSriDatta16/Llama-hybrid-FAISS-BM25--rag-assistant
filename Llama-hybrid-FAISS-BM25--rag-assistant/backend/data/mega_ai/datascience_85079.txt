[site]: datascience
[post_id]: 85079
[parent_id]: 
[tags]: 
Neural Network Loss Function - Mean Square Error: questions about what 'n' signifies

I'm very new to neural networks and have recently learnt about the loss functions used with neural networks. This question is in regards to the mean square error metric, defined as (from the textbook I'm using): $(\frac{1}{n})\sum_{i=1}^{n}(h_{\theta}(x^{i}) - y^{i})^{2}$ Where $h_{\theta}(x^{i})$ gives the predicted value for $x^{i}$ with modelâ€™s weights $\theta$ and $y^{i}$ represents the actual prediction for the data point at index $i$ . Looking up about this function online, I've seen different sources say different things. I can't seem to work out what n actually represents. I understood it as representing the number of neurons in the output layer and hence you'd just be finding the difference between the actual neuron value and the predicted value of the network given the weights. Some of the sources say it represents the number of training samples. If this is the case however, what does $h_{\theta}(x^{i})$ represent? Is it a sum of the output neuron values itself? Also if n is this, wouldn't that mean you'd have to run the function many times over all the training samples to minimize it? Whereas with the previous understanding of n, you could run it on certain samples and not all of them.
