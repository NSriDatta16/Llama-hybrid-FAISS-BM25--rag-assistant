[site]: crossvalidated
[post_id]: 482018
[parent_id]: 478255
[tags]: 
Meta-learning conducts a meta analysis: it looks at multiple analyses (which in turn used different assumptions, datasets, and methods) and tries to explain these with some generalization or perhaps even a meta-model. This general idea has long been used by academics to try to generalize and learn about a complicated topic. In this setup, an episode would be one of the analyses plus its associated dataset and methods. Meta-learning in the machine learning community takes many datasets, methods, assumptions, and results and then builds a model to explain all of those results. Early work like Omohundro (1996) looked at episodes as samples drawn from one larger dataset with each sampled modeled. Vilalta and Drissi (2002) (in a survey of meta-learning) noted that assumptions (aka "bias") are also part of an analysis. The resulting models were then averaged or combined in some manner yielding a meta-model. More recent work like this paper by Sun et al (2017) uses a generalization of that in combining results for completely different datasets, assumptions, and models. An excellent recent survey is given by Hospedales, Antoniou, Micaelli, and Storkey (2020) . From these, we can see that an episode is a tuple of (dataset, method(s), assumptions, estimated model/results) which then becomes an observation in the meta-analysis.
