[site]: datascience
[post_id]: 57030
[parent_id]: 48426
[tags]: 
The paragraph vectors are trained by using the information of words in a paragraph. If we randomly sample without distinguishing paragraph as you suggest, a paragraph vector will be adjusted in vector space by other words which are not components of the paragraph. So it will map/project the paragraph vector by irrelevant syntactical and semantical information of words. Both hierarchical softmax and negative sampling are not one of gradient-based optimizer. These methods just adjust/change the objective function to train much easier and faster. I'm not sure but I think itâ€™s not possible. Check this: doc2vec - How does the inference step work in PV-DBOW
