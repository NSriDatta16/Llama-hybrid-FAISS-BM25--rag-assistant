[site]: crossvalidated
[post_id]: 562002
[parent_id]: 
[tags]: 
Hard attention derivations

I am trying to completely understand the paper Show, Attend and Tell: Neural Image Caption Generation with Visual Attention . I understand the paper conceptually. I am trying to understand the math involved. I am probably weak in Bayesian statistics. I got stuck on the hard attention. There are two formulas that I don't understand. First of all, I don't understand derivation of objective function $L_s$ - It is start of the derivation of variational lower bound on the marginal log-likelihood of observing the sequence of words y given image features a and locations s \begin{align} &= \sum_{s} p(s \mid \mathbf{a}) \log (\mathbf{y} \mid s, \mathbf{a}) \\ &\leq \log \sum_{s} p(s \mid \mathbf{a}) p (\mathbf{y} \mid s, \mathbf{a})\\ &= p(\mathbf{y} \mid \mathbf{a}) \end{align} The second thing that is difficult is the derivations of a gradient in equation (6). I understand what the author of the paper is doing here and where parameter W is later used, but I don't understand the formula itself. Especially multiplications by $p(s |a)$ (which is for some reason missing in equation (7)) and $\log p(y| s,a)$ . \begin{align} \frac{\partial L_s}{\partial W} &= \sum_{s} p(s \mid \mathbf{a}) \left[ \frac{\partial \log p(\mathbf{y} \mid s, \mathbf{a})}{\partial W} + \log p(\mathbf{y} \mid s, \mathbf{a}) \frac{\partial \log p(s \mid \mathbf{a})}{\partial W}\right] \\ \end{align} Could you nudge me in the right direction?
