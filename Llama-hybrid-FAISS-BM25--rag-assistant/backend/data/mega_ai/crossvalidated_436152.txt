[site]: crossvalidated
[post_id]: 436152
[parent_id]: 
[tags]: 
Unrealistically high AUC-ROC score comparing to control feature and other performance measures

I am making a binary classification using regularized logistic regression, with extreme unbalanced data. The target label is Tar and non-target label is non-Tar . I am using both the dummy feature (which is just random value with no prediction power) against the full feature set (which should have some predictive power). However, I also did a subsampling so that it didn't allow the machine learning algorithms to learn much, so that I can compare the results between the dummy and full feature set . I am also testing it on both test dataset and external dataset (which is not used for training, but a completely new dataset which the prediction can apply). Here are the results: >>>> On test data > Feature = dummy >> Class label count, in test set: non-Tar 9915 Tar 9 Name: VAR_RECAT_VFINAL, dtype: int64 >> Class label count, predicted: non-Tar 9924 dtype: int64 >> Classification report: precision recall f1-score support Tar 0.00 0.00 0.00 9 non-Tar 1.00 1.00 1.00 9915 accuracy 1.00 9924 macro avg 0.50 0.50 0.50 9924 weighted avg 1.00 1.00 1.00 9924 >> Custom classification report: ('Accuracy:', 0.999, 'Sensitivity:', 0.0, 'Specificity:', 1.0, 'PPV:', nan, 'NPV:', 0.999, 'F1-score:', nan) >> Custom scores: ('ROC_v0:', 0.442, 'ROC:', 0.442) ////////////////////// ////////////////////// >>>> On external data > Feature = dummy >> Class label count, in external test set: non-Tar 11 Tar 1 Name: VAR_RECAT_VFINAL, dtype: int64 >> Class label count, predicted: non-Tar 12 dtype: int64 >> Classification report: precision recall f1-score support Tar 0.00 0.00 0.00 1 non-Tar 0.92 1.00 0.96 11 accuracy 0.92 12 macro avg 0.46 0.50 0.48 12 weighted avg 0.84 0.92 0.88 12 >> Custom classification report: ('Accuracy:', 0.917, 'Sensitivity:', 0.0, 'Specificity:', 1.0, 'PPV:', nan, 'NPV:', 0.917, 'F1-score:', nan) >> Custom scores: ('ROC_v0:', 0.409, 'ROC:', 0.409) ////////////////////// ////////////////////// >>>> On test data > Feature = full feature set >> Class label count, in test set: non-Tar 9915 Tar 9 Name: VAR_RECAT_VFINAL, dtype: int64 >> Class label count, predicted: non-Tar 9924 dtype: int64 >> Classification report: precision recall f1-score support Tar 0.00 0.00 0.00 9 non-Tar 1.00 1.00 1.00 9915 accuracy 1.00 9924 macro avg 0.50 0.50 0.50 9924 weighted avg 1.00 1.00 1.00 9924 >> Custom classification report: ('Accuracy:', 0.999, 'Sensitivity:', 0.0, 'Specificity:', 1.0, 'PPV:', nan, 'NPV:', 0.999, 'F1-score:', nan) >> Custom scores: ('ROC_v0:', 0.963, 'ROC:', 0.963) ////////////////////// ////////////////////// >>>> On external data > Feature = full feature set >> Class label count, in external test set: non-Tar 11 Tar 1 Name: VAR_RECAT_VFINAL, dtype: int64 >> Class label count, predicted: non-Tar 12 dtype: int64 >> Classification report: precision recall f1-score support Tar 0.00 0.00 0.00 1 non-Tar 0.92 1.00 0.96 11 accuracy 0.92 12 macro avg 0.46 0.50 0.48 12 weighted avg 0.84 0.92 0.88 12 >> Custom classification report: ('Accuracy:', 0.917, 'Sensitivity:', 0.0, 'Specificity:', 1.0, 'PPV:', nan, 'NPV:', 0.917, 'F1-score:', nan) >> Custom scores: ('ROC_v0:', 0.727, 'ROC:', 0.727) All the performance measures (accuracy, precision, recall, F1, etc) seem reasonable for models expected to not able to learn much. The AUC-ROC scores seem reasonable in the dummy feature sets, to be around 0.442 and 0.409 which is close to the 0.5 mark of random guessing. However, I got unbelievably high AUC-ROC scores (0.963 and 0.727) from the full feature set , which made the same predictions and obtained the same scores (all but AUC-ROC score) as the dummy set. Here are the Python code snippets: def custom_classification_report(y_true, y_pred): tp, fn, fp, tn = confusion_matrix(y_true, y_pred).ravel() acc = (tp+tn)/(tp+tn+fp+fn) sen = (tp)/(tp+fn) sp = (tn)/(tn+fp) ppv = (tp)/(tp+fp) npv = (tn)/(tn+fn) f1 = 2*(sen*ppv)/(sen+ppv) return ('Accuracy:', round(acc, 3), 'Sensitivity:', round(sen, 3), 'Specificity:', round(sp, 3), 'PPV:', round(ppv, 3), 'NPV:', round(npv, 3), 'F1-score:', round(f1, 3) ) def custom_scores(y_true, y_pred_score, target_name): false_positive_rate, true_positive_rate, thresholds = roc_curve( y_true, y_pred_score, pos_label='non-'+target_name) roc_auc = auc(false_positive_rate, true_positive_rate) return ('ROC_v0:', round(roc_auc, 3) , 'ROC:', round(roc_auc_score(y_true, y_pred_score), 3)) pipe = make_pipeline(preprocessor, control_panel['ml_algo'][1]['clf']) pipe.fit(X_train, y_train) target_labels = [control_panel['binary_target_label'], 'non-{}'.format(control_panel['binary_target_label'])] predicted_labels = pipe.predict(X_test) predicted_scores = pipe.decision_function(X_test) print('>> Custom classification report:', custom_classification_report(y_test, predicted_labels)) print('>> Custom scores:', custom_scores(y_test, predicted_scores, target_labels[0])) Is it likely a real phenomenon? Or is it more likely this was due to a code error? How can I further test it?
