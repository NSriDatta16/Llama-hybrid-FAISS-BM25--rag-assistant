[site]: crossvalidated
[post_id]: 439754
[parent_id]: 439735
[tags]: 
I am sure that there is a highly complex method you could use that I am not aware of, but let me give you a more simplistic approach. I'm going to split this into two parts: philosophical and practical. Philosophical: It seems like you're struggling to define your population of interest. If the population that you want to make predictions about in the model is only those with a large number of opportunities (which you define as greater than 100), then you're fine the way you are. If your population of interest contains ALL individuals, then throwing out data with low sample size is definitely going to bias your results. Practical: That being said I can think of two ways to adjust for the low samples size, one frequentest, and one Bayesian. First, the Bayesian. It may be appropriate to essential weight your data. I have to admit, I'm confused why you are using a beta distribution as your prior, as your data follows pretty clearly a Bernoulli distribution with n successes out of x trials. Perhaps I am missing something. I might suggest weighting your data against the number of opportunities that an individual had. I'd need a little more info about your model to be mor specific, but it should be a fairly simple process where you simply make $\sum \alpha + \beta \propto n $ consistently for the contributions of all individuals. That being said, given your concern about different clusteres, I would personally favor a weighted regression in this case. Here, you could include your other variables as controls, and again weight by the number of opportunities an individuals had. Weighted regression is really easy in most programs too. I've included some sample R code below. dat I'm sure others will have more complex suggestions, but there are my thoughts.
