[site]: crossvalidated
[post_id]: 278285
[parent_id]: 
[tags]: 
How do I interpret the very different results given by a logistic regression on a subset of variables?

I am trying to understand why the "p" (statistical significance) is so different for two regression models on the same population, one with all variables, the other only with some of the variables. I would expect the results to be similar in terms of statistical significance. Why am I wrong? Detail: Having just completed my first course on statistics ( in French ), my understanding is very imperfect, particularly with respect to multivariate logistics regression. Following this 5-week-long introductory course to statistics, I decided to test my knowledge on the following dataset to try and understand the relationship between all the explanatory variables and the outcome variable (y in this case). The dataset is bank-additional.zip , with the variables described here . In my attempt to understand the relationship between variables, using R , I first re-categorised the data: # Remove all previous objects rm(list=ls()) # Import libraries library(gplots) library(prettyR) library(corrplot) # Read the data b Then, I attempted a first model with all variables: # Compute the binomial multivariate regression model m I, maybe incorrectly, understood that the following variables were not statistically significant: Model: y ~ age + job + marital + education + default + housing + loan + contact + month + day_of_week + duration + campaign + pdays + previous + poutcome + emp.var.rate + cons.price.idx + cons.conf.idx + euribor3m + nr.employed Df Deviance AIC LRT Pr(>Chi) age 1 58.052 154.05 0.227 0.6334552 marital 2 58.500 152.50 0.675 0.7134888 default 1 60.295 156.29 2.471 0.1159764 contact 1 60.146 156.15 2.321 0.1276123 day_of_week 4 64.703 154.70 6.879 0.1424439 campaign 1 58.366 154.37 0.542 0.4615772 pdays 1 59.303 155.30 1.479 0.2239809 previous 1 57.926 153.93 0.101 0.7503597 poutcome 1 59.938 155.94 2.113 0.1460316 While those were: job 11 87.480 163.48 29.656 0.0017941 ** education 6 84.294 170.29 26.470 0.0001819 *** housing 1 62.500 158.50 4.676 0.0305929 * loan 1 63.091 159.09 5.267 0.0217320 * month 9 108.087 188.09 50.262 9.615e-08 *** duration 1 108.622 204.62 50.798 1.024e-12 *** emp.var.rate 1 66.978 162.98 9.153 0.0024828 ** cons.price.idx 1 70.983 166.98 13.159 0.0002862 *** cons.conf.idx 1 84.029 180.03 26.204 3.071e-07 *** euribor3m 1 68.756 164.76 10.931 0.0009455 *** nr.employed 1 69.215 165.22 11.391 0.0007380 *** However, if I try to compute a regression model with only the statistically significant variables, I get a very different result, and I do not understand why this is. m The second model gives me this: Model: y ~ job + education + housing + loan + month + duration + emp.var.rate + cons.price.idx + cons.conf.idx + euribor3m + nr.employed Df Deviance AIC LRT Pr(>Chi) 1673.9 1747.9 job 11 1688.5 1740.5 14.67 0.19809 education 7 1676.6 1736.6 2.74 0.90779 housing 1 1674.0 1746.0 0.10 0.74975 loan 1 1674.2 1746.2 0.29 0.58919 month 9 1755.5 1811.5 81.67 7.52e-14 *** duration 1 2299.6 2371.6 625.73 I would welcome any layman enough explanation, particularly one that that can help me understand when I can or cannot use a model calculated in this manner and why the p values are so different for the subset of variables vs. the full list of variables.
