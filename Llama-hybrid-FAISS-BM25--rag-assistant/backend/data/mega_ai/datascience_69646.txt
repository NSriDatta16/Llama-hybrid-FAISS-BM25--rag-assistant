[site]: datascience
[post_id]: 69646
[parent_id]: 69434
[tags]: 
The OneClassSVM is an unsupervised algorithm that is supposed to learn the normal data distribution. This means that the algorithm will model the boundaries of areas of high likelihood for your data points to be drawn from. Even though you know which points are outliers, the SVM will only know that there is roughly a proportion of outliers of nu , therefore the 14000 boundary is not intuitive at all in terms of distribution. What the SVM will do that explains your multiple lines is: If you have a hole in what you consider the "normal zone" - let's say as an example you have no data points between 2560 and 2985 - the SVM can decide that this is an area of low likelihood for your distribution and therefore build two vectors to exclude it from the learned normal distribution. At the opposite, if you have several points clustered together above 14000, since the SVM has no clue this is supposed to be the anomalous zone , it can detect a zone of high likelihood for the data points to be drawn from, and build two vectors to include it in the learned normal distribution. Now regarding your metrics: You probably have an imbalanced dataset since we are talking about anomaly detection (otherwise it makes no sense!), so you should not use global accuracy here because it is misleading. There is one thing that should alert you: The train AUC is very low, while the test AUC very high and this is not normal . So your algorithm poorly learns the target normal distribution. The high test AUC can be explained, but first let's use some notations: The real normal data distribution will be called *the target normal distribution", in the sense that you want to learn it. It is the interval [0;14000[ The real anomalous area (cannot talk about a distribution in this setting) will be called "the target anomaly area". It is the interval [14000;+infinity[ The learned normal distribution will be called the "learned normal distribution", obviously. The learned anomalous area will be called the "learned anomalous area" So the learned distribution consists in small clusters around areas of higher training data density. Since you (supposedly) have a few outliers and a lot of normal points, the learned distribution still intersects well with the target normal distribution, but not with the target anomalous area , where only a few areas will belong to the learned normal distribution. You can verify this on your graph: there are only a few vectors in the target anomalous area. Now when you apply the algorithm on the test set, it is not likely that a point from the test set in the target anomalous area falls into the intersection of the learned normal distribution and the target anomalous area, because this intersection is very small. But this is only due to the small number of data points in this anomalous area, not to a successful learning process. Hence these points will still be labeled as anomalies and you get a misleading high AUC score. At the contrary, the target normal distribution and the learned normal distribution intersects correctly, because there was more data points to learn from. So most of the normal points in the test set are classified correctly. To sum up, your algorithm learns a completely different distribution from what you excpected . You can try to use a higher gamma value to smooth the kernels. but you will probably not avoid the classification of the areas of higher density above 14000 as normal data points. So watch out for the train AUC! It would be very interesting to investigate these statements with graphs, you could as an example plot the [14000; +infinity[ zone, highlight the decision function boundaries and plot train and test set data points. You should get confirmation of what I am saying above (I will do it later and add the graphs if I find some time!)
