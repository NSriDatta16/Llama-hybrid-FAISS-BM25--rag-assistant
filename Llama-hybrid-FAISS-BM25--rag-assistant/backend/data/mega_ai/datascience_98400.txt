[site]: datascience
[post_id]: 98400
[parent_id]: 80191
[tags]: 
Please be specific on what basis The problem is that the model is largely overfitting has been concluded. The validation loss decreased and did not show any evidence of overfitting. How to Fine-Tune BERT for Text Classification? demonstrated the Further Pre-training as the fine-tuning method and the diagrams of the training exhibit the similar diagram for the successful fine-tunings (a), (b), and (c) except the (d) which is catastrophic forgetting. We find that a lower learning rate, such as 2e-5, is necessary to make BERT overcome the catastrophic forgetting problem. With an aggressive learn rate of 4e-4, the training set fails to converge. Hence your diagrams of training and validation loss would not be the basis to conclude overfitting. Please provide the evidences that the model has too adapted to the training data hence lost the generalized capability, by inferior accuracy on the test data set, etc.
