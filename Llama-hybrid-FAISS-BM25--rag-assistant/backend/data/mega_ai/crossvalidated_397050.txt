[site]: crossvalidated
[post_id]: 397050
[parent_id]: 396986
[tags]: 
In XGBoost library, feature importances are defined only for the tree booster, gbtree . So, I'm assuming the weak learners are decision trees. get_fscore uses get_score with importance_type equal to weight . The three importance types are explained in the doc as you say. I could elaborate on them as follows: weight : XGBoost contains several decision trees. In each of them, you'll use some set of features to classify the bootstrap sample. This type basically counts how many times your feature is used in your trees for splitting purposes. gain : In R-Library docs , it's said the gain in accuracy . This isn't well explained in Python docs. I think, this option could be easily confused with Information Gain used in decision tree node splits. cover : In each node split, a feature splits the dataset falling into that node, which is a proportion of your training observations. So, your selected feature concerns some portion of the dataset.
