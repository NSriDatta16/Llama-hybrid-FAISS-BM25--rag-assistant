[site]: crossvalidated
[post_id]: 563967
[parent_id]: 
[tags]: 
How could an oracle that knows the true distribution of things would still incur some errors?

The ideal model is an oracle that simply knows the true probability distribution that generates the data. Even such a model will still incur some error on many problems, because there may still be some noise in the distribution. In the case of supervised learning, the mapping from $x$ to $y$ may be inherently stochastic,or $y$ may be a deterministic function that involves other variables besides those included in $x$ . The error incurred by an oracle making predictions from the true distribution $p(x, y)$ is called the Bayes error.. Deep Learning , Bengio, Goodfellow and al I don't get why " an oracle that simply knows the true probability distribution that generates the data " would still incur some errors. If you know the distribution you know what the truth is, isn't it? Or can the noise create some outliers?
