[site]: stackoverflow
[post_id]: 3327540
[parent_id]: 1926079
[tags]: 
I think this is a common misconception - size is only one part of the equation when it comes to database scalability. There are other issues that are hard (or harder): How large is the working set (i.e. how much data needs to be loaded in memory and actively worked on). If you just insert data and then do nothing with it, it's actually an easy problem to solve. What level of concurrency is required? Is there just one user inserting/reading, or do we have many thousands of clients operating at once? What levels of promise/durability and consistency of performance are required? Do we have to make sure that we can honor each commit. Is it okay if the average transaction is fast, or do we want to make sure that all transactions are reliably fast (six sigma quality control like - http://www.mysqlperformanceblog.com/2010/06/07/performance-optimization-and-six-sigma/ ). Do you need to do any operational issues, such as ALTER the table schema? In InnoDB this is possible, but incredibly slow since it often has to create a temporary table in foreground (blocking all connections). So I'm going to state the two limiting issues are going to be: Your own skill at writing queries / having good indexes. How much pain you can tolerate waiting on ALTER TABLE statements.
