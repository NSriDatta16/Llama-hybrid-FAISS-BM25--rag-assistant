[site]: crossvalidated
[post_id]: 244445
[parent_id]: 
[tags]: 
Choosing between two models based on cross validation error

I am seeking to compare the cross validation error for two models using R, maybe more in the future. Using the cv.glm function from the boot package produces a number that varies each time. Since it varies I ran it 100 times for each model and plotted the results using this code: library(boot) sink.model This took about 15 seconds to create 100 estimated cross-validated error values for each model. If we overlay them in the same density plot we see that the simpler model has much less variation and an overall smaller mean. Therefore I would choose the simpler model for prediction because it has a lower error rate on average, as well as lower variation. Is there some fundamental flaw in that approach? If so could you maybe point me in the direction of some resource that might explain why? Edit for clarification: I am trying to justify the usage of one model for prediction versus another. I am calculating CV error for each model and then plotting them. I see that each time I do this one distribution has less variance and a lower mean value than the other distribution. I would like to say the model that produces a lower CV error mean with less variance would be the better model to choose for prediction but I would like to know if there is something else that should be considered before coming to that conclusion. Second edit: I am not interested so much in why a simpler model predicts better but whether it is a sound approach to perform several CVs on two different models and then compare those results to each other with the intent of choosing the one with the lower error rates?
