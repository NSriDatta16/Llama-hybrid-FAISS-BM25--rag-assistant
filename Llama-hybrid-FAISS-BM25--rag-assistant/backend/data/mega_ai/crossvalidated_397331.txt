[site]: crossvalidated
[post_id]: 397331
[parent_id]: 390250
[tags]: 
The concept I feel you are talking about is Online learning or Incremental Learning(you can find quite a lot of literature with these terms). This is done by Vowpal Wabbit library where you can train a Regression/Classification where both are probabilistic models. This way you can actually train the model first with large data and keep on updating the model as you obtain new data. If something sticks out with new data you can reset the learning rate to a higher amount and begin training thereby to capture the new distribution which was not present with the old training dataset. Otherwise, keep the learning rate as it is to not induce variance in the model. For more practical experiences with Neural Networks, you can refer to concepts related to transfer learning . Training old+new data using initial weights should technically be faster if both datasets are quite same since you will converge way faster, thinking mathematically ( since it has been seen that all local minima's neural network are equally good). But trying to generalize this point - It depends on the size of the new data It also depends on how different these 2 datasets are in terms of information It also depends on your model complexity.
