[site]: crossvalidated
[post_id]: 435230
[parent_id]: 
[tags]: 
Is the kernel trick unnecessary for for non-linear SVM?

I am just learning about Mercer Kernels, and a question came up. Since using Mercer's theorem, we know that a positive definite kernel matrix can be represented by an inner production of the input vector mapped to new feature space implied by the kernel. A Gram matrix of $X$ is defined a $K(X;k)\in \mathbb{R}^{m\times m}$ such that $K_{i,j}=k(\hat{x}_i,\hat{x}_j)$ . If the matrix $K$ is positive definite, then $k$ is called a Mercer Kernel. By Mercer's Theorem, if we have a Mercer kernel, then there exists a function $\phi: X \to Y $ such that $$k(\hat{x}_i,\hat{x}_j)=\langle \phi(\hat{x}_i),\phi(\hat{x}_j) \rangle $$ The question is, since this is the case, why do we need to use the kernel function at all? Why not just transform the data according to $\phi$ and use the transformed features to train the SVM. Apparently with this approach there should be some difficulty while classifying a new datapoint, but I am not quite finding the issue. Thanks!
