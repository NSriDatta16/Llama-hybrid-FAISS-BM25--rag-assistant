[site]: datascience
[post_id]: 61746
[parent_id]: 61745
[tags]: 
The thing I'm wondering is why I'm getting better results with the neural network without the normalization? From a theoretical standpoint, normalizing your input shouldn't affect accuracy, in a sense that a NN can converge to any range of input values. The reason why we normalize our data when using NNs is that it offers some useful properties, mainly regarding convergence and convergence speed. You can read more details here . Is it because the optimal hyperparameters required for the network with the normalized input are different from when it is not normalized? Yes, I think that is your problem. Some hyperparameters might be optimized for the regular input and might not work well for the normalized one. An example of this could be a higher learning rate, which might work for a higher range of input values but not for a normalized one.
