[site]: datascience
[post_id]: 77985
[parent_id]: 60207
[tags]: 
I agree with @Piotr Rarus (+1); as he said, the no free lunch theorem certainly applies to this. I'd like to add some exploration of the four possible states for perfectly fitting the training data: Impossible due to training function if it is only surjective (onto but not one-to-one). This is of course unlikely, but still feasible. Impossible due to network architecture (e.g. with a single layer, you could not fit nonlinear functions) Possible and trivial (e.g. convex problem space). Possible and hard but many local minima and/or saddle points. Possible but difficult to prove, or even attain within some tolerance, global optimality. Given that you had difficulty reaching low error with local optimizers, it's likely that you are in state 4. Deterministic global optimization is generally quite difficult, but you have some tricks at your disposal. Firstly, if you attain an error of 0, then you know you've found a non-strict global minima. With sufficient computational power, you could try some of these deterministic global optimization methods , or you could even brute-force it by splitting the search-space into a hypercubic lattice, defined by your tolerance, and grid search the parameters. However, assuming you do not have such massive computational resources, and assuming you want to keep this network architecture, you could use more classical techniques for overcoming local minima, such as momentum . Some other classical methods are higher-order, such as the Quasi-Newton method . Unfortunately, given the status quo of deep learning, stochasticism and local optimization is still very much ingrained in network training, particularly in popular frameworks/libraries. Lastly, I'd recommend adding more layers. I don't know how you distributed the 100+ million parameters, but depending on the training set function, it may not have been sufficiently nonlinear.
