[site]: crossvalidated
[post_id]: 99735
[parent_id]: 99558
[tags]: 
You are making very different hypothesis for the two cases. For the first case you get the correlation between the model score and the average human score, while for the second one you do not distinguish annotators, and compare the annotators value with the model score. In this case you are considering that the annotators are all the same, which does not make much sense from the practical point of view but, in my understanding, is still valid from a statistical point of view. What will probably happen is that since different annotators have different perceptions of quality, the points around linear relation will be much more scattered than for the first case, and the correlation coefficient will be smaller. From a broader perspective, what you might consider is to try to understand how the different annotators value relates with the model score through regression analysis. You can do a multivariate linear regression (given the low number of annotators, you cannot go further than linear) on the model score using the annotators score (ann1, ann2, ann3). The score between the different annotators will be strongly correlated, but as seen in a different post , this does not constitute a problem; it is merely a multicollinearity issue, i.e., is equivalent to having a smaller number of measurements.
