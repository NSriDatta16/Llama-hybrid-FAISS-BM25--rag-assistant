[site]: crossvalidated
[post_id]: 360218
[parent_id]: 319928
[tags]: 
Background For concreteness, let's consider an AR(1) with stochastic volatility $$\begin{align}y_t&=\beta y_{t-1}+e^{\frac{1}{2}v_t}\varepsilon_t, &&\varepsilon_t\sim N(0,\,1)\\ v_t&=\rho v_{t-1}+\eta_t,&&\eta_t\sim N(0,\,\sigma^2)\\v_0&\sim N(\bar{v}_0,\,\tau_0^2).\end{align}$$ Given $\theta=[\beta,\,\rho,\,\sigma^2]$ and some data $y_{1:T}$, you want to draw from $p(v_{0:T}\,|\,\theta,\,y_{1:T})$. JPR : This algorithm is based on the Metropolis-Hastings algorithm. In particular, JPR is a Block-Metropolis-Hastings algorithm with Inverse-Gamma proposals. Conditional on a past draw of the volatilities, you loop through $t$-by-$t$, propose new $e^{v_t}$ values from an Inverse-Gamma, and accept or reject them; KSC : This algorithm is based on the Kalman Filter. The AR(1)-SV is a Gaussian but non-linear state-space system (nonlinear because of how $v_t$ enters the measurement equation). But if you define $u_t = y_t-\beta y_{t-1}$ (which is observable conditional on $\beta$), you can do some rearranging: $$\begin{align} u_t&=e^{\frac{1}{2}v_t}\varepsilon_t\\ u^2_t&=e^{ v_t}\varepsilon^2_t\\\ln u^2_t&=v_t + \ln \varepsilon_t^2.\end{align}$$ With this, you've made the measurement equation linear in $v_t$ but non-Gaussian. KSC use a Gaussian mixture approximation to the distribution of $\ln\varepsilon^2_t$ to turn the system into a linear, Gaussian one. Then they use the Kalman filter and smoother to draw the states. PF : A particle filter uses importance sampling to sequentially construct weighted approximations $\{v^{(i)}_{0:t},\,W^{(i)}_t\}_{i=1}^N$ to the sequence of posteriors $p(v_{0:t}\,|\,\theta,\,y_{1:t})$. Comparisons JPR and KSC are both Markov chain Monte Carlo (MCMC) algorithms. They condition on a past draw of the states to generate the current draw. So to generate multiple draws from $p(v_{0:T}\,|\,\theta,\,y_{1:T})$, you have to do so serially , which could be time-consuming; By contrast, a PF is constructing an entire sample from $p(v_{0:T}\,|\,\theta,\,y_{1:T})$, and it can be parallelized; Since PFs are sequential, they can be used for real-time posterior updating. That is, they can be used to iteratively update your posterior draws as new information arrives; JPR has all of the MCMC theory going for it. Asymptotically, it gives draws from the right distribution. But you have to accept some extra Monte Carlo noise. Sometimes you reject proposals; KSC is perhaps cleaner to implement (especially if you have Kalman filter/smoother code lying around), but the approximation error is here to stay. The distribution of $\ln\varepsilon^2_t$ is not a Gaussian mixture, and the error you introduced by treating it like one won't go away asymptotically. Granted, it's small, especially if you use the 10-component mixture from Omori et al (2007) ; A PF is technically targeting $p(v_{0:t}\,|\,\theta,\,y_{1:t})$, but for large enough $T$ simple PFs have a hard time approximating the full posterior. They're generally better at the marginals $p(v_t\,|\theta,\,\,y_{1:t})$. To get quality draws from $p(v_{0:T}\,|\,\theta,\,y_{1:T})$, you'd have to use a particle filter with move-steps or a particle smoother . For more on this, Doucet and Johansen (2012) is a great survey; If you want to use a PF to draw the states as part of a Gibbs sampler, you can’t just naively take a vanilla PF off the shelf and use it. The Gibbs sampler won’t have the right invariant distribution. You’ll have to augment the PF with the so-called “conditional SMC update.” See Andrieu et al (2010) for more on this. What should you do? It depends of course. If you're working with simple, linear Gaussian volatility processes like in Primiceri (2005) or Cogley & Sargent (2005) , JPR or KSC are probably the way to go, especially if you're Gibbs sampling. They are tailored for this context, and tuning these algorithms to perform well is less involved than it would be with a particle filter. That said, Primiceri's original use of KSC was wrong. So read the correction and don't make the same mistake. However, if you're considering more complicated SV processes that JPR or KSC simply weren't meant for, then you’ll have to use a particle filter. And if you're Gibbs sampling, you'll have to use a particle Gibbs sampler as in Andrieu et al (2010) . If for fixed $\theta$ you want to generate many draws from $p(v_{0:T}\,|\,\theta,\,y_{1:T})$ and sequentially update them in real-time, I would use a particle filter.
