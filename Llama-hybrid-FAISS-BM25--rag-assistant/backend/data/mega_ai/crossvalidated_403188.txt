[site]: crossvalidated
[post_id]: 403188
[parent_id]: 
[tags]: 
Early stopping on validation set

There exist cases where one can "overfit" on the validation set. Although it is easier to overfit on the training set, the distributions of the validation and test set may not match, in which case tuning hyperparameters on the validation set could result in subpar performance on the test set. It is common for neural networks to use early stopping on the validation set to determine what is a good place to stop training. However, in the scenario where there is a mismatch between the validation and test distributions, early stopping may result in lower performance. So, what are some alternatives to early stopping that could potentially address this issue?
