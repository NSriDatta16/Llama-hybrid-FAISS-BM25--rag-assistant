[site]: crossvalidated
[post_id]: 161497
[parent_id]: 
[tags]: 
What is the correct way to combine data points with error?

If I have a 2-D data, say $y = f(x)$, with error in the dependent variable, $\delta y$ in this case, and I want to transform this data set to a coarser independent variable grid, $x$, what is the correct way to treat the error propagation? I am currently using a linear interpolation for both $y$ and $\delta y$ but have noticed behavior that seems incorrect. In particular, I can clearly see that interpolating the $y$ values produces the same overall curve (the new $\Delta x$ values are still much smaller than any features in the data), but the error seems to oscillate in an unusual manner. I expect local changes in error to be monotonic but the linear interpolation seems to introduce more variations. The error should generally change in a smooth manner as $x$ changes. The largest error will be at the lowest and highest $x$ values. Thinking about the linear interpolation, my understanding is it averages the error from the two closest points, ignoring the error of all other nearby points. It seems it would be more appropriate to use a broader average (or moving average) to interpolate the error values. To provide more specific context, my original data has 100s of data points but I want to use around 25-50 data points when reporting the data. Below are two example interpolations of the same data, one with 25 points and the other with 50 points. I compare the data to a several model curves (shown in green) and for each model curve calculate the $\chi^2$ value. The bottom plot in each figure shows the point-by-point contribution to the $\chi^2$ comparison between the experimental data and the best fit model. Notice that the one with 50 data points has a $\chi^2=1469$ compared to $\chi^2=132$ for the curve with 25 data points. The shocking thing is that 62% of that $\chi^2$ for the 50 data point curve comes from one value that happens to have small error because the two closest points had small errors (if I calculated the $\chi^2$ with the hundreds of points I started with, that would have been averaged out but with so few points it doesn't). There is clearly an error in what I am doing because the result should not be so dependent on how many points I use for the interpolation. Any tips or resources to examine further would be helpful.
