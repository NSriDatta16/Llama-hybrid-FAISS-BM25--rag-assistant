[site]: datascience
[post_id]: 100184
[parent_id]: 99947
[tags]: 
Transformers (aka "attention models") are being used in place of LSTMs in many areas, as they generally give better results, and/or are quicker to train. They can be used for regression problems, just as easily as for classification or text-generation - just create the final layer accordingly. The catch is they scale as O(NÂ²) in the size of your data. You didn't mention how long the sequences are that you are trying to analyze, but if they are less than 512 items then a transformer should be fine on a single GPU. (And your LSTM is probably doing a poor job with sequences that long anyway.) There is a lot of active research in making versions of the Transformer that scale to larger values of N . If you read the papers, the claims of O(N) or O(NlogN) are a little misleading as they end up batching things together for efficiency; but still they are managing to work with sequence lengths in the thousands or higher. For out of the box implementations https://github.com/huggingface/transformers is a good start. For handling larger values of N, I see both Longformer and BigBird mentioned there; the latter was used not just for text but for DNA sequencing. I'm not aware of any research done on meteorological features, specifically, though. (Please leave a comment if you do find anything!)
