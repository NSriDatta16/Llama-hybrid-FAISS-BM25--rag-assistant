[site]: crossvalidated
[post_id]: 359779
[parent_id]: 
[tags]: 
Using bootstrap for robust estimation

I am hoping to understand the process of bootstrapping outlier-contaminated data, and the effects on (simple) OLS estimators. In particular, we have a DGP, $$Y_t = \beta X_t + \epsilon_t$$ where $\epsilon_t \sim (1-\theta) \mathcal{N}(0,\sigma^2) +\theta \mathcal{N}(0,10\cdot \sigma^2)$ or some other similar mixture style distribution indicating outliers (sorry for any abused notation here). If we estimate $\hat{\beta}$ using standard OLS, then we may have significantly biased estimates due to the presence of overly-influential outlier data. My idea is to bootstrap the data first, estimate a bootstrap $\hat{\beta}^*$, and I believe then a simple average of the $\hat{\beta}^*$ would, in fact, be a resistant estimator, depending on the size $\theta$. Or we could try a robust estimator of the mean of the distribution of bootstrap betas (e.g., trimmed mean with the trimmed amount proportional to $\theta$). Does this seem reasonable? And, if so, would anyone have any idea how to begin? Just for reference, for an original sample of size $N$, the number of Bootstrap samples in total is $C(2N-1,N-1)$. Bootstrap samples with no outliers is $C(2N-\theta N -1, N-1)$ Bootstrap samples with exactly $k$ outliers ($k\leq \theta N$) occurring at least once each, is $C(2N-\theta N+k-1,N-1)$ Even for 10% outliers in a sample of size 100, the probability of having no outliers is 0.08%. There will be many more bootstraps with some level of contamination, but I believe this will not make that much difference in the long-run, or if it does, then using a robust estimator of the mean $\beta^*$ would rule out this influence. I have never seen any papers on this data-augmentation method, but it feels right if only because it's easy to see how to extend it to other inference problems.
