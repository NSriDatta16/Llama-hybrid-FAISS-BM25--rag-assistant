[site]: datascience
[post_id]: 43457
[parent_id]: 
[tags]: 
what happens to the depth channels when convolved by multiple filters in a cnn (keras, tensorflow)

I have a $15$ -channel time series that I want to convolve using a $1$ d CNN ( $1\times n$ time-steps kernel). Now, let's say I want to have, as my first layer, $16$ filters. This would imply to my mind that the output would have a depth of $16 \times 15 = 240$ , because each filter would be applied to each channel independently. However when I implement this in keras, (using Sequential) the filter dimensions in the summary do not reflect this. Here is a code fragment: TIME_RANGE = 31 NUMBER_OF_CHANNELS = 15 model = Sequential() model.add(Conv1D(input_shape = (TIME_RANGE, NUMBER_OF_CHANNELS), filters=16, kernel_size=7, padding='valid', data_format='channels_last')) model.add(ReLU()) model.add(BatchNormalization()) model.add(Conv1D( filters=32, kernel_size=5, padding='valid')) and here is the corresponding summary output: Layer (type) Output Shape Param # ================================================================= conv1d_1 (Conv1D) (None, 25, 16) 1696 re_lu_1 (ReLU) (None, 25, 16) 0 batch_normalization_1 (Batch (None, 25, 16) 64 conv1d_2 (Conv1D) (None, 21, 32) 2592 as you can see output's shape along the time-wise axis decreases as expected due to the no-padding argument, from $31$ to $25$ to $21$ , but the depth just reflects the number of filters-- so where have all my channels gone? At this point in the architecture I was expecting a depth of $15\times 16\times 32 = 7680$ . It seems an implicit $1\times 1$ convolution is occurring somewhere, which I don't think I actually want-- I'd like to do my $1\times 1$ convolutions later on in the network. So what am I missing here?
