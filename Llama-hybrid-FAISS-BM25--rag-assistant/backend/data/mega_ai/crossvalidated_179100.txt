[site]: crossvalidated
[post_id]: 179100
[parent_id]: 
[tags]: 
State space for Markov Decision Processes

I'm currently trying to formulate a MDP for a Reinforcement Learning (RL) task. Having read a variety of papers where RL has been applied I've been left somewhat confused as to what can be a considered part of the state space. I was always under the impression that the state formed the agent's observable world and actions taken by the agent would cause a state transition. However many authors add in state variables that are useful to enable predictions but will not and can not change as the result of an action being selected. For example I've seen authors reference current time as a state variable to allow the agent to associate time varying conditions such as peak times of day when controlling traffic lights. In addition some authors have included things like real time electricity pricing in the state space for planning demand response activities in the home. No action the user takes can possibly cause a change in the price of the electrical unit, but obviously the decision the agent makes is dependent upon it. In short, can I have the following state space {price, currentTime} in my MDP or does it need to be modelling differently.
