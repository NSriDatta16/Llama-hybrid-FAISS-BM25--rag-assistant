[site]: crossvalidated
[post_id]: 245858
[parent_id]: 
[tags]: 
Different definitions of "curse of dimensionality"

I have read two definitions of the curse of dimensionality: The first seems to be more widespread, (I have seen people refer to it on stats.SE in other questions), the other one I only recently encountered. Here they are: The curse of dimensionality was coined to indicate that the number of samples needed to estimate an arbitrary function with a given level of accuracy grows exponentially with the number of variables (i.e., dimensions) that it comprises [H. Samet, Foundations of Multidimensional and Metric Data Structures, pp486] The curse of dimensionality is a name given to the situation where all or some of the important features of a dataset sharply concentrate near their median (or mean) values and thus become non-discriminating. [V. Pestov, An axiomatic approach to intrinsic dimension of a dataset, Neural Networks 21 (2008) 204â€“213] What is the relationship between these definitions? Does the second one agree with the first?
