[site]: datascience
[post_id]: 64772
[parent_id]: 
[tags]: 
Training of a CNN stops at the seventh epoch

I am doing image classification, and I am using transfer learning to do this. My problem is that if I build the network, and then I train it, the training process stops at the seventh epoch, even if I set the number of epochs to be equal to 100. Is there a reason why this could happen? Thanks in advance. [EDIT] I have images of 4 different weather conditions, and they are in .jpg format. My code is the following: trainingset = '/content/drive/My Drive/Colab Notebooks/Train' testset = '/content/drive/My Drive/Colab Notebooks/Test_HWI' batch_size = 31 train_datagen = ImageDataGenerator( rescale = 1. / 255,\ zoom_range=0.1,\ rotation_range=10,\ width_shift_range=0.1,\ height_shift_range=0.1,\ horizontal_flip=True,\ vertical_flip=False) train_generator = train_datagen.flow_from_directory( directory=trainingset, target_size=(256, 256), color_mode="rgb", batch_size=batch_size, class_mode="categorical", shuffle=True ) test_datagen = ImageDataGenerator( rescale = 1. / 255) test_generator = test_datagen.flow_from_directory( directory=testset, target_size=(256, 256), color_mode="rgb", batch_size=batch_size, class_mode="categorical", shuffle=False ) num_samples = train_generator.n num_classes = train_generator.num_classes input_shape = train_generator.image_shape classnames = [k for k,v in train_generator.class_indices.items()] print("Image input %s" %str(input_shape)) print("Classes: %r" %classnames) print('Loaded %d training samples from %d classes.' %(num_samples,num_classes)) print('Loaded %d test samples from %d classes.' % (test_generator.n,test_generator.num_classes)) now, I use as pre-trained network a VGG16, and the code is: from keras.models import Sequential from keras.layers import Dense, Activation, Dropout, Flatten,\ Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D,\ UpSampling2D from keras.layers.normalization import BatchNormalization from keras import regularizers from keras import optimizers from keras import applications from keras.models import Model, Input def load_backbone_net(input_shape): # define input tensor input0 = Input(shape=input_shape) # load a pretrained model on imagenet without the final dense layer feature_extractor = applications.vgg16.VGG16(include_top=False, weights='imagenet', input_tensor=input0) feature_extractor = feature_extractor.output feature_extractor = Model(input=input0, output=feature_extractor) optimizer = 'adam' #alternative 'SGD' feature_extractor.compile(loss=keras.losses.categorical_crossentropy, optimizer=optimizer, metrics=['accuracy']) return feature_extractor def transferNet(feature_extractor, num_classes, output_layer_name, trainable_layers): # get the original input layer tensor input_t = feature_extractor.get_layer(index=0).input # set the feture extractor layers as non-trainable for idx,layer in enumerate(feature_extractor.layers): if layer.name in trainable_layers: layer.trainable = True else: layer.trainable = False # get the output tensor from a layer of the feature extractor output_extractor = feature_extractor.get_layer(name = output_layer_name).output #output_extractor = MaxPooling2D(pool_size=(4,4))(output_extractor) # flat the output of a Conv layer flatten = Flatten()(output_extractor) flatten_norm = BatchNormalization()(flatten) # add a Dense layer dense = Dropout(0.4)(flatten_norm) dense = Dense(200, activation='relu')(dense) dense = BatchNormalization()(dense) # add a Dense layer dense = Dropout(0.4)(dense) dense = Dense(100, activation='relu')(dense) dense = BatchNormalization()(dense) # add the final output layer dense = BatchNormalization()(dense) dense = Dense(num_classes, activation='softmax')(dense) model = Model(input=input_t, output=dense, name="transferNet") optimizer = 'adam' #alternative 'SGD' model.compile(loss=keras.losses.categorical_crossentropy, optimizer=optimizer, metrics=['accuracy']) return model # load the pre-trained model feature_extractor = load_backbone_net(input_shape) feature_extractor.summary() # choose the layer from which you can get the features (block5_pool the end, glob_pooling to get the pooled version of the output) name_output_extractor = "block5_pool" trainable_layers = ["block5_conv3"] # build the transfer model transfer_model = transferNet(feature_extractor, num_classes, name_output_extractor, trainable_layers) transfer_model.summary() and then I train the network: from keras import callbacks # fit the transferNet on the training data stopping = callbacks.EarlyStopping(monitor='val_acc', patience=3) steps_per_epoch = train_generator.n//train_generator.batch_size val_steps = test_generator.n//test_generator.batch_size+1 try: history_transfer = transfer_model.fit_generator(train_generator, epochs=100, verbose=1, callbacks=[stopping],\ steps_per_epoch=steps_per_epoch,\ validation_data=test_generator,\ validation_steps=val_steps) except KeyboardInterrupt: pass
