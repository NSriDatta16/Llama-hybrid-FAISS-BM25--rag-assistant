[site]: crossvalidated
[post_id]: 486623
[parent_id]: 
[tags]: 
Can I change the rows / composition of a dataset without violating modeling assumptions?

I am building a few models to explain an outcome (let's say customer making a purchase during a visit to a site) from a dataset of ~200k rows with ~66 features (after dummying out categorical variables). For background, the natural purchase rate per visit in the broader dataset is ~20%. What I did One thing I found that dramatically increases the AUC of my models is to engineer the rows of the dataset a bit. By this, I mean that I: Took all the purchase = 1 instances Combined them with an equal number of purchase = 0 instances, for which I took unique customer IDs in order to make the population as IID as possible On the training and validation sets, I get a strong AUC, but see a 20% drop in AUC (yes, 20 points) when I move to the test set, which consists of all the site visits from Jan. 2020. What I know As standard practice, we take a random train-test split of the original dataset. On certain occasions, we might engineer the rows/composition of the dataset: Class balancing when there are very few 1 s Rotation of images (data augmentation) Omitting rows with null values My questions Is the practice of manipulating the cohort in the way I described leading to violations in assumptions for standard machine learning methods (RF, GBT, penalized LR)? Is it common to engineer the rows of a dataset when looking at a new problem (instead of just the features)? Or is this practice fair game, and the drop in AUC coming from terrible overfitting? Edit to add: Is my method violating the basic assumption that the population the model is developed on and the population to be tested on should be driven by the same data-generating process?
