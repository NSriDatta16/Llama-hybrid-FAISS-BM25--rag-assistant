[site]: datascience
[post_id]: 124852
[parent_id]: 124833
[tags]: 
There's a library called joblib-spark that you can use to leverage a Spark cluster. It lets you take the Scikit-learn code you've written and train it in a distributed, parallel way across a Spark cluster which helps if you're if you're training large models on medium-sized datasets that can fit in the memory of a single node. You could also use Docker and Kubernetes which would allow you to handle large data in a distributed environment. https://github.com/joblib/joblib-spark , https://docs.docker.com/ https://kubernetes.io/docs/home/
