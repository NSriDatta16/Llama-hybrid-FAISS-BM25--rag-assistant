[site]: crossvalidated
[post_id]: 626483
[parent_id]: 610913
[tags]: 
The queries, keys and values are computed from the embeddings $x$ using matrix multiplication: \begin{align} q &= xW_Q \\ k &= xW_K \\ v &= xW_V, \end{align} where $W_Q$ , $W_K$ and $W_V$ are trainable parameters of the attention layer. Basically the attention layer learns how to map the embeddings to queries, keys and values. Probably the only trainable parameter currently in your attention layer is the matrix $W_O$ , that is used for combining the outputs from the multiple heads (described in 3.2.2 of the paper). You could modify your attention layer to contain three additional trainable parameters - the matrices $W_Q, W_K, W_V$ .
