[site]: crossvalidated
[post_id]: 507025
[parent_id]: 
[tags]: 
Using an Autoencoder for Unsupervised Outlier Detection - Train and Predict on Same Data?

I have a high-dimensional dataset of medical utilization for a public health plan's membership in which I would like to identify outliers. i.e., which individuals are potentially over- and/or under-utilizing different medical services, given everything we know about them? An autoencoder seems like a solid option because of its ability to handle high-dimensional, non-linear data. Unlike the examples that I've been able to find so far, I do not have a known dataset of "normal" utilization to train on first. I was thinking of two different options, and I'm looking for community feedback on which one makes the most sense (or if there is a more appropriate option that I'm not thinking about): Train on all observations, and then apply directly to the same group of observations. My thought here is that since the anomalies I'm looking for are ...anomalies, then their inclusion in the development of the autoencoder wouldn't skew the training compression and expansion processes too much. i.e., the trained model being applied to the data it learned from would still produce a greater loss on the anomalistic observations. Train a different "leave-one-out" model for every focal group. If the purpose of the project is to identify potential over- and/or under-utilization for the purpose of addressing the issues with the health plan's provider partner groups, then I could train a different model for every provider partner group, where the training would be applied on the health plan members assigned to all provider partner groups EXCEPT the focal provider provider group. Once trained, the trained model would be applied to the focal provider group to find areas of difference between the focal provider group's member utilization and the rest of the utilization. For the sake of time and simplicity, I would highly prefer the first approach. I just want to make sure I'm aware of the caveats and traps that it would come along with.
