[site]: crossvalidated
[post_id]: 551400
[parent_id]: 551375
[tags]: 
Another thing to bring up in addition to the answers that already exist: AIC, BIC etc. can be really good (i.e. cheap to evaluate, use all the data, let you do things like AIC-model averaging etc.) in the specific circumstances when you can define them and they are valid. What do I mean by this limitation? E.g. for some model class especially with a lot of regularization it can be very hard to even define these (e.g. what is AIC or BIC - especially in terms of the number of parameters - for a XGBoost, a random forest or a convolutional neural network), even if there are various extensions like DIC. Additionally, your model may be rather mis-specified in important ways (e.g. you are using some kind of time series model like ARIMA, but you kind of know that you are mis-specifying the true underlying correlation of records over time). In those situations, I worry whether my likelihood in AIC or BIC is right and whether it may inappropriately overstate (usually more the worry) the evidence for one model vs. another. Cross-validation is also quite good for optimizing metrics, which are not easy to optimize directly as likelihood function. One example would be optimizing AuROC: while there are some tricks and attempts to define loss functions that directly optimize it, it's not straightforward, but you can fit some model using some standard likelihood function and then make choices based on what optimizes AuROC in cross-validation. These factors above mean that e.g. in prediction competitions like on Kaggle forms of cross-validation are usually the go-to-method for model evaluation/making modeling choices. I may be overly negative about the likelihood ratio test, and, yes, I realize that you could re-phrase AIC model selection in terms of model selection with a particular alpha level (but I'd not recommend model selection, anyway, but rather model averaging), but for the purposes where one would consider AIC, BIC or cross-validation, I don't find them all that useful. Sure, a null hypothesis test is useful when you have a pre-specified model for an experiment (e.g. randomized controlled trial of drug A vs. placebo for disease X), but it's a lot less useful for building a good model that performs well by some metric. I don't really see the distinction for inference models beyond this point. You could clearly define a meaningful cross-validation metric for the example you describe. I suspect many examples where one technique is used and another might be just as good (or even better), come down to historical precedent in certain research communities. E.g. in some areas AIC is super-popular, in others train-test splits and/or cross-validation, others are really keen on hypothesis tests, and to name another option that has not, yet, been mentioned, there's also various forms of bootstrapping.
