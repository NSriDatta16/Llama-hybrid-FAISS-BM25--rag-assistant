[site]: datascience
[post_id]: 123109
[parent_id]: 
[tags]: 
Confusion with FC Layer Neurons and Output Shapes in CNN-based Sentiment Analysis Model

I am currently working on a sentiment analysis model for text data, and I'm using a Convolutional Neural Network (CNN) architecture. This is my first time implementing a CNN, and I'm facing issues with both the forward and backward passes of the Fully Connected (FC) layer. Here's an overview of my model architecture: Convolutional Layer: Input -> Convolution -> ReLU -> Output Pooling Layer: Input -> Max Pooling -> Output Fully Connected (FC) Layer: Input -> FC with ReLU Activation -> Output I have successfully implemented the forward pass, but I'm not confident that the number of neurons in the FC layer is correct. Additionally, the output shape of the FC layer is not as expected, causing issues in the backward pass. Here are the details of my questions: Number of Neurons in FC Layer: I have four output classes in my sentiment analysis task, but I'm unsure about the appropriate number of neurons in the FC layer. Currently, I have it set to 4. Should it be the same as the number of output classes, or should it be different? Expected Output Shape in FC Layer: In the forward pass of the FC layer, I am getting an output shape of (5020, 4), which is not what I am expecting. Based on my understanding, it should be (batch_size, hidden_units) where hidden_units is a hyperparameter. But I'm not sure how to determine the correct hidden_units and how to get the desired output shape. Incorrect Output Shape in FC Layer Backward Pass: In the backward pass of the FC layer, I am expecting the gradient shape to be (batch_size, hidden_units). However, I'm getting (batch_size, output_size), where output_size is the number of output classes. I believe this is incorrect, but I'm having trouble identifying the source of the issue. I have provided my complete CNN model implementation on GitHub: https://github.com/GraphicsMonster/NLP-model-for-sentiment-analysis It includes the forward and backward passes of each layer, but I suspect there might be errors or misunderstandings in my implementation. I would greatly appreciate any guidance on determining the correct number of neurons in the FC layer, understanding the expected output shape, and fixing the backward pass to get the desired gradient shape. If there are any other potential issues in my code, I'm open to feedback. Thank you in advance for your help and suggestions. Let me know if you need any further information or code snippets from my implementation.
