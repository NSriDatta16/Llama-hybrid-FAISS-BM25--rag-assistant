[site]: crossvalidated
[post_id]: 95857
[parent_id]: 95638
[tags]: 
This sounds like a ceiling effect , and it's a potentially major problem. It sounds like you have Likert ratings for dis/agreement with various statements, and strongly agree is your highest rating option. These are ordinal data, and probably also grouped continuous data, in that they presumably represent a latent continuum that has been binned into as many groups as you have response options (see Anderson, 1984) . If many of your respondents are using the maximum response options invariably, you'll have a tough time estimating their real level of agreement relative to other respondents, because as @JeremyMiles says, you don't have much information there. Possible problems include: Real variation among participants that agreed strongly versus "extremely" or "absolutely" I.e., they might have made meaningfully different choices among those options if you had offered them Response biases such as acquiescence and extreme response style See chl's answer on " Factor analysis of questionnaires composed of Likert items ") True response invariance that is indistinguishable from the above, which could otherwise indicate: A meaningful consensus among your respondents, or... A problem with your concept of the construct – maybe it's not a variable, or distributed oddly. If this is only a problem with a modest subset of your sample, you might consider detecting and excluding these participants systematically. I had a lot of invariant responding in my dissertation research , which involved a survey full of Likert scales. In consultation with my former advisor, I established a simple criterion for excluding participants who responded the same way to every item on indices with at least ten items and multiple subscales. This was less than 15% of my sample, but enough to warrant action. Leaving in bad data from respondents who showed no evidence of thought in their choices can bias results, especially when those cases are multivariate outliers with high leverage in regression analysis. If this is a problem throughout your sample, you may have larger design problems that make your data intractable. If you have a small sample, you may not be able to afford the power loss from excluding cases, even on a per-index basis as I did. If you have a large sample, you may be able to accommodate varying degrees of response bias by using methods suited to the ordinal nature of Likert ratings. I have discussed some of these options in answers to several questions, including: Factor analysis of questionnaires composed of Likert items Regression testing after dimension reduction Whether to use original or reverse coded items in factor analysis? Correlational study or ordinal data using 5-point Likert scale Correlating two questionnaires with grouped items A simple recommendation to take from the above that ought to be doable in Amos is to produce a polychoric correlation matrix of your item data (after excluding invariant respondents, if practical) and fit the structural equation model to this (instead of an ordinary covariance matrix). Weighted least squares estimation is also recommended (Wang & Cunningham, 2005) , if available in Amos. References Anderson, J. A. (1984). Regression and ordered categorical variables. Journal of the Royal Statistical Society B, 46 , 1–30. Wang, W. C., & Cunningham, E. G. (2005). Comparison of alternative estimation methods in confirmatory factor analyses of the General Health Questionnaire. Psychological Reports, 97 , 3–10.
