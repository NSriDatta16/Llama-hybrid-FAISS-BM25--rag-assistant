[site]: crossvalidated
[post_id]: 502445
[parent_id]: 
[tags]: 
Negative log likelishood for one hot encoding of text

i have a neural network that takes 32 hex characters as input (one hot as a [32, 16] shape tensor) and outputs 32 hex characters one hotted the same way. What i'm confused about is the best way to go about evaluating loss? Could i take NLLL on the individual classes (each hex character) and then average those?
