[site]: crossvalidated
[post_id]: 57686
[parent_id]: 57669
[tags]: 
Burges's A Tutorial on Support Vector Machines for Pattern Recognition provides you a very detailed introduction to SVMs. Just compare it to logistic regression, the decision function of SVMs for the binary classification case is $$ f(\mathbf{x}) = \text{sgn} (\mathbf{w}^T \mathbf{x} + b) $$ where $\mathbf{w} = \sum_i \alpha_i y_i \mathbf{x}_i$ , $\alpha_i$ is zero for all cases, but the support vectors (those lying exactly at the separating hyperplane), and $y_i \in \{1, -1\}$ are the labels. The reference gives in addition very clear and intuitive geometrical interpretations of these results. See that you it does not provide any score like logistic regression does. It does not model a (conditional) probability density. Please refere to the scikit-learn documentation for a description of this issue, and ways to deal with it.
