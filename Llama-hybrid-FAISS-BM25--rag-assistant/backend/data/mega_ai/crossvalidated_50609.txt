[site]: crossvalidated
[post_id]: 50609
[parent_id]: 
[tags]: 
Validation: Data splitting into training vs. test datasets

I was naively validating my binomial logit models by testing on a test dataset. I had randomly divided the available data (~2000 rows) into training (~1500) and validation (~500) datasets. I now read a post in another thread ( Frank Harrell) that causes me to question my approach: Data splitting is not very reliable unless you have more than 15,000 observations. In other words, if you split the data again, accuracy indexes will vary too much from what you obtained with the first split. How serious is this worry and what are ways around it? The OP speaks of "resampling" but not sure how that works here for validation. Edit: Adding context as per @Bernhard's comment below: Comparing logistic regression models
