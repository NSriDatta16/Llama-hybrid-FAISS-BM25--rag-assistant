[site]: crossvalidated
[post_id]: 219024
[parent_id]: 207777
[tags]: 
Use StandardScaler after vectorization or not? It's highly depends on the final estimator you want to use. For example, decision trees usually are not very sensitive to non-scaled features. SVMs, especially with rbf kernels, are sensitive so it's better to try scaling. As you pointed out sparse matrices can't be scaled with with_centering=True argument (because they lose their sparsity) but you can perform scaling using with_centering=False . Sklearn StandardScaler The other solution would be sparse to dense transformation (if you have enough RAM). from scipy import sparse from sklearn.base import TransformerMixin, BaseEstimator class Sparse2DenseTransformer(BaseEstimator, TransformerMixin): def fit(self, X, Y=None): return self def transform(self, X): return X.todense() cc = Sparse2DenseTransformer().fit_transform( sparse.random(2000, 2000 ) ) sparse.issparse(cc) # False You can add it to your pipeline right before StandardScaler . Is it sensible to scale all of my features after I have joined them in the FeatureUnion or do I scale each of them separately? I believe it doesn't really matter.
