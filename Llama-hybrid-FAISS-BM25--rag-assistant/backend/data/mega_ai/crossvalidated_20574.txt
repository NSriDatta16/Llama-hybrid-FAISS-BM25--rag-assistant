[site]: crossvalidated
[post_id]: 20574
[parent_id]: 
[tags]: 
Maximum likelihood estimation when parameters are functions of another data series

We have two time series: $X_t$ and $R_t$, and a model saying that $R_{t+1} = (\mu(X_t) - \frac{1}{2}\sigma^2(X_t))\Delta T + \sigma(X_t) \sqrt{\Delta T} \epsilon_t$, where $\Delta T$ is given constant and $\epsilon_t$-s are independent normally distributed with zero mean and unit variance. Further we assume that the functions $\mu(x)$ and $\sigma(x)$ are linear for simplicity. I would like to use some standard method (MLE comes to my mind) to estimate parameters of functions $\mu(x)$ and $\sigma(x)$, but I am not sure how to do this. I would be grateful for detailed answers, because I am not really experienced with statistics.
