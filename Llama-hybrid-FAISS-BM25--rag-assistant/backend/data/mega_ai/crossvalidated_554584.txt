[site]: crossvalidated
[post_id]: 554584
[parent_id]: 
[tags]: 
Why not solve SVM with gradient descent instead of quadratic programming?

How is SVM optimization implemented in packages like Scikit-Learn? Clearly, SVM is a quadratic programming problem but why not just use gradient descent to update the parameters? Is it because we want to find the values of $\alpha_i$ and find support vectors? I found this link but it doesn't seem to address my question.
