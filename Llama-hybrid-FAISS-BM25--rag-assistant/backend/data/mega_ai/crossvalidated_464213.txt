[site]: crossvalidated
[post_id]: 464213
[parent_id]: 463223
[tags]: 
In the VAE scenario, the encoder $q(z|x)$ already works for any $x$ (any $x$ that the encoder network has seen during training, of course), so there's no gain in representing it as a mixture of trainset distributions $q(z|x_n)$ with binary weights. However, look at the aggregated posterior , $$ q^\text{agg}(z) = \frac{1}{N} \sum_{n=1}^N q(z|x_n) $$ It is a finite (and non-degenerate) mixture of Gaussians, representing average encoding distribution. In a sense, this is the distribution you should use to sample $z$ from after you trained the model (and not the prior $p(z)$ ). This is because the decoder network was effectively trained on samples from $q^\text{agg}(z)$ and works best on them. In theory, Variational Inference tries to make the aggregated posterior $q^\text{agg}(z)$ and the prior $p(z)$ as close as possible, but in practice it might not succeed. For more, see the paper on VampPrior . Leaving VAE's inference model aside, you can claim that the generative part of the VAE is actually a (possibly infinite) mixture. Indeed, the most popular choice for the decoding distribution $p(x|z)$ is Gaussian, and $p(z)$ is also typically Gaussian. Thus, $$ p(x) = \int p(x|z) p(z) dz = \int \mathcal{N}(x \mid \mu_x(z), \Sigma_x(z)) \mathcal{N}(z \mid 0, I) dz $$ So effectively here we construct a distribution $\mathcal{N}(x \mid \mu_x(z), \Sigma_x(z))$ for every $z \in \mathbb{R}^d$ and then mix all these distributions (uncountably many!) with weights of the standard multivariate Gaussian distribution. Such a mixture is too complicated to work with directly, therefore we resort to Variational Inference.
