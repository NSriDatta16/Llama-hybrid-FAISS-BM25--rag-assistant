[site]: crossvalidated
[post_id]: 325374
[parent_id]: 325075
[tags]: 
Part 1 Can you model $g(s)$ as a polynomial? If so, I believe you could turn $\gamma(\beta)$ into a closed form expression by analytically computing the integral. In general, if you can find some approximation of $g(s)$ that makes the integral analytically solvable, then you're back to ordinary maximum likelihood (without a $\gamma$ at all). Let's say you don't know much about g(s). But you must know something! So put a Gaussian Process prior on it with mean function F(s). Use a tool like stan, sample the function during an MCMC draw and perform numerical integration within the loop. You can see me trying this here (see line 96). Now it's slow, but it is a way to estimate and incorporate uncertainty in $g(s)$. If you have a complex nonlinear expression for $g(s)$ but don't want to go the bayesian route, you could incorporate something like the trapezoidal rule in your maximum likelihood routine, but it too will be slow! Part 2 In order to even speak of dependence between parameters, you have to adopt a bayesian viewpoint, since the classical viewpoint has all parameters as fixed unknown quantities. If you take the classical interpretation, then I think you're okay just estimating $\gamma$ as a constant. In fact, why not use the estimates of $\beta$ and $\gamma$ to gain insight into $g(s)$?
