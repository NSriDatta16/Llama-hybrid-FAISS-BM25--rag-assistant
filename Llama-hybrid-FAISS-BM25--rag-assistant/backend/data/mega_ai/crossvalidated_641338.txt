[site]: crossvalidated
[post_id]: 641338
[parent_id]: 
[tags]: 
Prediction Intervals and Alternatives to NHST for testing forecast accuracy between models

Here Rob Hyndman says In the predictive approach to statistics (McLean, 2000), problems of statistical analysis are viewed as prediction problems, and the central theme is a statistical (probability) model. A key idea in the approach is to use only those models that work, only those producing reasonably good predictions. This is clearly a problem of model selection rather than significance testing. A powerful practical alternative to hypothesis testing for forecasting methods is to select models using information criteria (e.g., Hyndman et al., 2008, ch. 7). The information criterion approach has nothing to do with the testing paradigm: there is no null hypothesis, no arbitrary threshold $\alpha$ levels and no notion of significance. This approach to model selection can be readily extended to model ranking and model averaging so that Descartes' dictum can be used. The approach can also be extended to forecasting methods that are not based on models with computable likelihoods (e.g., Anderson et al., 2000). However, that is altogether another story. It is not that testing the slope coefficient for zero or residuals for normality and independence is never appropriate; or that tests for equality of predictive accuracy are never useful. Rather, there is widespread confusion, inappropriate use and interpretation of significance testing. While statisticians admit the confusion (Hubbard and Bayarri, 2003), and while the discipline of statistics is making a continuing effort to provide the world with a unified, problem-free testing methodology (e.g., Berger, 2003), it is perfectly reasonable for a non-statistician to brush aside significance tests altogether, tending to use non-test alternatives wherever possible. He mentions alternatives for model selection, but not for " tests for equality of predictive accuracy ". What are good alternatives to NHST for testing forecast prediction accuracy between models? Lets say I were to go with a Prediction Interval. Is a slimmer interval better? If I were to go with prediction intervals with a sliding window method, would averaging the prediction intervals be a valid way of comparing?
