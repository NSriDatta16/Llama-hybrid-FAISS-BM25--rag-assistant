[site]: crossvalidated
[post_id]: 586502
[parent_id]: 
[tags]: 
How to normalize count data with possible large values and lots of zeros?

I am working on an anomaly detection problem in network data. My dataset consists of aggregated data over a fixed period of time (1mn window). For numerical features we store averages and for categorical data we store counts, the latter can be very large (up to ~3000 in some cases) but also contain a lot of zeros. The image below shows what the data look like. Another detail is that these counts originate from the same categorical feature. For continuous variables I use standardization, but what should I use for the counts? Using standardization on every column does not bring the values to the same scale, so I thought of standardizing using the mean and std of all count variables since they originate from the same categorical feature. Another solution would be to do a minmax scaling, but that may remove information that can be valuable for the anomaly detection task.
