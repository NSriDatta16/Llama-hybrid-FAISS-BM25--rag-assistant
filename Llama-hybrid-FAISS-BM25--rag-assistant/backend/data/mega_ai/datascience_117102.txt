[site]: datascience
[post_id]: 117102
[parent_id]: 117090
[tags]: 
First thing I'd do is examine the source of the CSVs to determine if there's a way to detect the difference upstream without having to perform string-based calculations (which are relatively computationally expensive compared to others, giving the process a scaling limit). Hopefully the CSVs with headers are coming from a different place than the CSVs without headers, otherwise that's somewhat concerning from a data quality/integrity perspective. If I determined that I needed to inspect the files to get the answer, next thing I'd do is perform a few simple checks meant to work for the majority of cases. If the first row has any blanks, it's either data or a bad file. If the first and second row have even a single field that differs in data type, it's a header. If the file is expected to contain a longer text column, I may also use a heuristic on field length to determine if the first row is likely to contain a value for that field, or compare the coefficient of variation between the field lengths of the first few rows to see if the first row has significantly lower variance. If all else fails, time to get a little weird. Word embeddings will not work if the column names are out of vocab, which is fairly frequently the case in my experience. Edit distance metrics like Levenshtein or Jaro-Winkler aren't going to work because values for the data rows don't need to be similar to each other to be valid, e.g. 123 and 456 are 100% different but could be values for the same column. This is where I'd reach for one of the more obscure tools in my toolkit, string profiles. My preferred profile scheme is forcing all letters to A/a, and all numbers to 0, so Abc-123 becomes Aaa-000 . This transforms the string from carrying semantic meaning to just information about "shape". You can also, if anticipating longer words in the data, truncate the shapes with run-length encoding or something similar. Once you have the shapes, edit-distance metrics become far more viable of a solution. Edit to add: While they'd be seriously overkill for the task, I can't leave out that this is something a large language model would be almost as good at as a person if given a good prompt. Overall, though, it's important to recognize that you could quite easily design a file that is impossible to accurately make a determination on, so there is no magic bullet. You may never encounter an impossible case but you do need to account for them existing either in your process or your documentation.
