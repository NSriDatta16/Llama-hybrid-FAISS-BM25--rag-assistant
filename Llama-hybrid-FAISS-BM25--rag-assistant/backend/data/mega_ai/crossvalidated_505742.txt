[site]: crossvalidated
[post_id]: 505742
[parent_id]: 
[tags]: 
Vector Jacobian product in automatic differentiation

my questions is related to this post Higher Order of Vectorization in Backpropagation in Neural Network @shimao I don't really get the following claim (I know how the chain rule works and what is the essence of how reverse mode autodifferentiation ): "In reality, it's not necessary to compute the jacobian in order to perform backpropagation. All that is needed is the "vector jacobian product", or VJP." As far as I know, you need to compute the Jacobian at each step in order to perform the gradient accumulation. For example suppose we are in the step of computing the derivative of the following operation (very typical in neural networks): $$ t = Wz, \,\,\, z\in \mathbb{R}^{m\times 1}, t \in \mathbb{R}^{n \times 1}, W\in\mathbb{R}^{n \times m} $$ Then we now that the vector derivative (the Jacobian) is given by: $$ \frac{\partial t}{\partial z} = W $$ which is then plug into the chain rule (assume $\frac{\partial y}{\partial o}$ is the already computed gradient), to compute the final derivative given by the chain rule: $$ \frac{\partial y}{\partial o} \frac{\partial t}{\partial z} $$ Hence, I dont really understand that sentence because that sentence says that we dont need to compute the Jacobian to perform the back propagation as we only need a Jacobian vector product; but clearly for this step we do need to obtein the Jacobian of the transformation between $t$ and $z$ . Can someone provide a bit of intuition of what is exactly refering to?. In general I have seen that reverse-mode autodiff can be efficiently implemented through vector Jacobian products (and that doesn't require to compute the Jacobian), but when I think about it I realize that we do need to compute the Jacobian's at each step
