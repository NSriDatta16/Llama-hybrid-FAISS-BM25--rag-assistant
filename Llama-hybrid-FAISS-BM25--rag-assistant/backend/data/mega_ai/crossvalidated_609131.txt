[site]: crossvalidated
[post_id]: 609131
[parent_id]: 569878
[tags]: 
Both approaches have minimal statistical motivation and seem to address a non-problem . (There is a very interesting case described in the answer to the linked question that relates to King & Zeng (2001) , but I would argue that to be an issue of experimental design, rather than of model evaluation.) Yes, class imbalance poses problems to the classification accuracy metric in that a score of $97\%$ might sound great but actually be quite pitiful if you would get $99.9\%$ of the cases correct by classifying as the majority class every time. However, this strikes me as a drawback of classification accuracy as a performance metric, rather than of the reality of your problem having imbalanced classes. (I discuss here and here how to remedy accuracy scores to deal with being high yet pitiful.) Most machine learning methods "classifiers" give outputs on a continuum, and every method I know that does not can be wrestled with to give such a output on a continuum (e.g., Platt scaling for SVMs). Consequently, when you refer to the classification accuracy of a machine learning model, you either mean one of the following: Your model has (close to) $0\%$ accuracy, since dead-on predictions on the continuum are so unlikely. You are referring to the continuous predictions made by your model along with a decision rule that partitions that continuum into discrete buckets that make your classifications. The common decision rule is to classify predictions above $0.5$ as category $1$ and predictions below $0.5$ as category $0$ . When you do the second of the two, as many machine learning practitioners do and what happens when you call something like the predict method in software like sklearn , you are not actually evaluating the model. You are evaluating the model along with the decisions made using model outputs. However, especially if you give no thought to the decision rule, that decision rule might be terrible for your problem. Instead of fiddling with the data to remedy class imbalance issues, the first thought should be to change the decision rule. If you have $1000$ : $1$ imbalance, maybe you want to predict as the minority class whenever the output (which often has an interpretation as a probability) is above $0.001$ instead of $0.5$ . My logic for this is that, the baseline rate of minority-class occurrence is one-in-a-thousand, so if you even have a one-in-two-hundred chance of being in the minority class, that is a sizeable deviation from the norm and might warrant consideration. I discuss this idea in my question here . The more sophisticated approach would be to consider the continuous outputs. In fact, doing so allows you to have more decisions that categories. It might be that your decision rule assigns predictions to category $0$ if the prediction is below $0.2$ , assigns predictions to category $1$ if the prediction is above $0.9$ , and gives an "I don't know" classification for predictions between $0.2$ and $0.9$ , related to the idea presented here about putting "suspected spam" in an email subject line, rather than sending the message to the spam folder or letting it through without such a tag. We want confident and accurate predictions, sure, but those need not be realistic, and part of your job (or someone's job) is to handle that ambiguity. All of this is to say that class imbalance is not inherently a problem. Good statistical methods like evaluating proper scoring rules, assessing the continuous outputs of models, and thinking in terms of event probability vs error cost handle class imbalance fine. Consequently, it does not make much sense to fiddle with your data (downsampling) or the model-fitting process (weighting) to skew the outputs a certain way. Low predicted probabilities of unlikely outcomes seems like a feature, not a bug, of machine learning outputs. Fiddling with the statistics in order to get on the correct size of a software-default cutoff of $0.5$ seems like a poor approach to modeling when you consider what you lose by doing so. Plenty of blogs and even sources that might seem more credible will advocate for these poor statistical methods because the authors are unaware of the statistical subtleties. It's a shame that our field has to fight this noise. (Finally, downsampling strikes me as the worst of all approaches. Not only are you trying to solve something that is not a problem, but you are sacrificing precious data in order to do so. While upsampling, synthesizing points (e.g., SMOTE), and weighting the loss function have their problems, at least they don't discard precious data.) REFERENCE King, Gary, and Langche Zeng. "Logistic regression in rare events data." Political analysis 9.2 (2001): 137-163.
