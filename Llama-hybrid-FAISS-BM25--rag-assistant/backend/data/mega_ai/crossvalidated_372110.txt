[site]: crossvalidated
[post_id]: 372110
[parent_id]: 371981
[tags]: 
Absent an answer to @ReneBT's question, I will speculate that subjects are patients with at least one symptom, and that the number of symptoms can be 1, 2, 3, or 4. If you have 60 patients who are Pos for the marker and 100 who are Neg, then your counts $X_{ij}$ might be similar to those in the matrix below, where the respective columns are Pos and Neg, and the rows give the number of symptoms. MAT [,1] [,2] [1,] 8 29 [2,] 13 32 [3,] 27 18 [4,] 22 21 Chi-squared test of independence. A standard chi-squared test of the independence of rows and columns in R gives results as follows: rslt = chisq.test(MAT); rslt Pearson's Chi-squared test data: MAT X-squared = 17, df = 3, p-value = 0.0007068 So the null hypothesis of independence is rejected. The expected cell frequencies $E_{ij}$ are: rslt$exp [,1] [,2] [1,] 15.23529 21.76471 [2,] 18.52941 26.47059 [3,] 18.52941 26.47059 [4,] 17.70588 25.29412 The following are Pearson residuals $(X_{ij} - E_{ij})/\sqrt{E_{ij}}.$ Roughly, their message is that Pos subjects tend to have proportionately higher numbers of symptoms than Neg ones. rslt$resi [,1] [,2] [1,] -1.853663 1.5508857 [2,] -1.284541 1.0747245 [3,] 1.967808 -1.6463864 [4,] 1.020505 -0.8538158 The squares of the Pearson residuals are the 'contributions' of each cell of the $4 \times 2$ table to the chi-squared statistic, reported in the initial results to be 17. sum(rslt$resi^2) [1] 16.99968 Other possible tests. The test above treats the number of symptoms as a nominal categorical variable. Various other tests could treat them as ordinal categorical variables, or as numerical. Recognizing them as numerical would focus directly on the larger numbers of symptoms in Pos patients rather than on the association of categorical variables. Welch t test. While the fake data presented here are clearly not normal, t tests with as many patients as here could be defended on the basis of the robustness of t tests to non-normal data (absent extreme outliers). A Welch separate variances t test on the numbers of symptoms reported by 60 Pos and 100 Neg patients shows a highly significant difference between the two groups in the average number of symptoms. t.test(pos, neg) Welch Two Sample t-test data: pos and neg t = 3.2586, df = 128.76, p-value = 0.001432 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: 0.2252111 0.9214556 sample estimates: mean of x mean of y 2.883333 2.310000 2-sample Wilcoxon test. Ignoring that it is difficult to imagine there is a simple 'location shift' in the distribution of symptoms from Neg to Pos and also hoping for a satisfactory correction for the ties in the data, many statisticians (presumably @user2974951 among them) would feel comfortable doing a Wilcoxon rank sum test, which also shows a highly significant difference. wilcox.test(pos, neg) Wilcoxon rank sum test with continuity correction data: pos and neg W = 3860, p-value = 0.001736 alternative hypothesis: true location shift is not equal to 0 Permutation test. A permutation test using the Welch t statistic as metric (but not assuming it has a t distribution) would not be subject to the criticisms of Welch t and Wilcoxon SR tests; it gives P-value about 0.002. set.seed(1016) pos = sample(1:4, 60, rep=T, prob=c(.1, .2, .3, .4)) neg = sample(1:4, 100, rep=T, prob=c(.3, .3, .2, .2)) t.obs = t.test(pos, neg) $stat all = c(pos, neg); gp = c(rep(1,60),rep(2,100)) t.prm = replicate(10^5, t.test(all ~ sample(gp))$ stat) mean(abs(t.prm) > abs(t.obs)) [1] 0.00171 Note: The first three lines of the R code above were used to simulate the fake data used throughout.
