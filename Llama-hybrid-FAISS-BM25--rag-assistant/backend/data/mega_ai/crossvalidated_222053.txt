[site]: crossvalidated
[post_id]: 222053
[parent_id]: 222046
[tags]: 
As far as I can see, the issue here is similar to the issue of consistency vs. agreement in the context of interrater reliability. Consider your true value as ratings offered by one rater (who happens to be very good...) and the forecasters as providing their own ratings. Raters are consistent if, e.g., those people who are rated as higher than average by one rater are also rated similarly higher than average by another rater. Consistency can be assessed with correlations. Raters are in agreement if the mean of one rater's set of ratings are similar to the mean of another rater's set of ratings. Agreement can be assessed with tests of differences between means. Consistency and agreement are orthogonal. Two raters can be consistent, but not in agreement, |Rater 1|Rater 2| Subject 1| 1 | 10 | Subject 2| 2 | 20 | Subject 3| 3 | 30 | Subject 4| 4 | 40 | Subject 5| 5 | 50 | Mean| 3 | 30 | r = 1 and in agreement but not (really) consistent, |Rater 1|Rater 2| Subject 1| 1 | 1 | Subject 2| 2 | 5 | Subject 3| 3 | 4 | Subject 4| 4 | 2 | Subject 5| 5 | 3 | Mean| 3 | 3 | r = .1 or both, or neither (you can imagine those numbers for yourself). Ideally, you want both consistency and agreement to be high. Both of these can be assessed using the intraclass correlation coefficient Unless I'm missing something obvious, you could do something similar here between your true values and your forecasts.
