[site]: datascience
[post_id]: 51145
[parent_id]: 47168
[tags]: 
The use of cross-entropy here is not incorrect; it is the cross entropy of some quantity. Given data $(x_1,y_1), ..., (x_N, y_N)$ , with $y_N$ a categorial variable over $K$ classes, we can model the conditional probability $p_k(x)$ for class $k$ , where it satisfies $\sum_{k=1}^K p_k(x) = 1$ for each $x$ . Then the sum $$\frac{1}{N}\sum_{i = 1}^N \sum_{k = 1}^K \mathbf{1}\{y_i = k\}\log p_k(x_i) = \frac{1}{N} \sum_{i = 1}^N \sum_{k = 1}^K q_k(x_i) \log p_k(x_i) $$ is the (conditional) log-likelihood, and also the cross entropy between $p$ and the "one-hot" distribution $q$ that has $P(Y = k | X) = 1$ . Logistic regression has the same equation, except there we model $\log p_k(x_i)$ via a log-linear model. Suppose we have $K = 2$ and code the categorical responses as 1 and 0; then this reduces to $$\frac{1}{N} \sum_{i = 1}^N y_i\log p(x_i) + (1 - y_i) \log(1 - p(x_i))$$ In the classification tree setting, for a binary tree with $|T|$ nodes corresponding to regions $\mathcal{R}_1, ..., \mathcal{R}_{|T|}$ , and where the $m$ th region contains $N_m$ points, we model $p_k(x_i)$ as a constant in each region: $$\frac{1}{N} \sum_{i = 1}^N \sum_{\mathcal{R}_m}^{|T|} N_m \Big( y_i \log p_{m} + (1 - y_i) \log(1 - p_{m}) \Big)$$ $$ = \sum_{\mathcal{R_m}}^{|T|} N_m \left( \frac{ \#\text{$\{y = 1\}$ in $\mathcal{R}_m$} }{N} \log p_m + \frac{ \#\text{$\{y = 0\}$ in $\mathcal{R}_m$} }{N} \log (1 - p_m) \right)$$ $$ = \sum_{\mathcal{R_m}}^{|T|} N_m \left( \frac{ C_m }{N} \log p_m + \frac{ N - C_m }{N} \log (1 - p_m) \right)$$ where $C_m$ is the number of times $y = 1$ in $\mathcal{R}_m$ . Taking a derivative and setting equal to zero shows that the MLE is actually $\hat{p}_m = C_m / N$ , and so this is $$ = \sum_{\mathcal{R_m}}^{|T|} N_m \left( \hat{p}_m \log \hat{p}_m+ (1 - \hat{p}_m) \log (1 - \hat{p}_m) \right)$$ which is just the entropy of $\hat{p}_m$ . Since $C_m$ depends on the split points and the parameters chosen in the tree, so does $\hat{p}_m$ . So either cross entropy or entropy are valid, depending on what you want to talk about. I found this blog post very useful.
