[site]: datascience
[post_id]: 48739
[parent_id]: 48711
[tags]: 
I agree with main points of @Upper_Case well put answer. I like to put forth a perspective that emphasizes on "machine learning" side of the question. For a classification task using kNN, logistic regression, kernel SVM, or non-linear neural networks, the main disadvantage that we are concerned about is decrease in model performance , e.g. decrease in AUC score on a validation set. Other disadvantages of skeweness are often investigated when the damage of skeweness on the quality of result is hard to assess .Ù‘ However, in a classification problem, we can train and validate the model once with the original (skewed) and once with the transformed feature, and then If performance declined, we do not transform, If performance improved, we transform. In other words, damage of skeweness can be easily and objectively assessed, therefore, those justifications do not affect our decision , only performance does. If we take a closer look at the justifications for using lets say log transformation, they hold true when some assumptions are made about the final features that a model or test directly work with. A final feature is a function of raw feature; that function can be identity. For example, a model (or test) may assume that a final feature should be normal, or at least symmetric around the mean, or should be linearly additive, etc. Then, we, with the knowledge (or a speculation) that a raw feature is left-skewed, may perform log transformation to align the final feature with the imposed assumption. An important intricacy here is that we do not, and cannot change the distribution of any raw feature, we are merely creating a final feature (as a function of raw feature) that has a different distribution more aligned with the imposed assumptions. For a classification task using kNN, logistic regression, kernel SVM, or non-linear neural networks, there is no normality, or symmetric assumption for distribution of final features, thus there is no force from these models in this regard. Although, we can trace a shadow of "linear addition" assumption in logistic regression model, i.e. $$P(y=1|\boldsymbol{x})=\frac{1}{1+e^{-(w_1x_1+..+w_dx_d)}}$$ and in neural networks for weighted sum of features in the first layer, i.e. $$y_i=f\left(\boldsymbol{W}_{i,.}\boldsymbol{x}+b\right)=f\left(W_{i,1}x_1+W_{i,2}x_2+...+b\right)$$ I say "a shadow" because the target variable is not directly the linear addition of final features, the addition goes through one or more non-linear transformations which could make these models more robust to the violation of this assumption. On the other hand, the linear addition assumption does not exist in kNN, or kernelSVM, as they work with sample-sample distances rather than feature interactions. But again, these justifications come second compared to the result of model evaluation, if performance suffers we do not transform.
