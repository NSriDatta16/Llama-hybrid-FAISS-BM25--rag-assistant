[site]: crossvalidated
[post_id]: 181232
[parent_id]: 179850
[tags]: 
I think I figured it out myself, if anybody is interested: In classification, the likelihood function (such as the probit or error function) itself is a source of noise, since it will make the output of y non-deterministic. This is actually the same as the likelihood for regression, the gaussian noise again actually came from the likelihood. In logistic regression no noise parameter is needed, since by for example enlarging your vector w will cause less noise, since it will be squashed further through the logistic function. Finally, the GP model is regularized as well, this can be best seen from page 144. For the GP classification models, the parameter controlling the latent variable variation in the covariance function ($\sigma_f$) controls the regularization (again see page 144). This regularization comes from the smoothness assumption that the Gaussian Process model enforces on the latent variable.
