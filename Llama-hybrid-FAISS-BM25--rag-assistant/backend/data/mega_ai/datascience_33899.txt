[site]: datascience
[post_id]: 33899
[parent_id]: 33897
[tags]: 
I am having too many features. No, you don't :). First of all, it is highly likely that not all of them are important for the prediction that you want to make. I would highly recommend using a CART Random Forest for regression of the variable of interest. It literally requires minimal coding if you choose to do it in python using the RF algorithm from the sklearn package. It's big advantage is that it is straightforward to use and understand and, moreover, it provides you with the learnt feature_importances_ of all inputs after training, so that you can exclude the least important ones and speed up the inference/training in the future. -EDIT- To understand the difference between Classification and Regression Decision trees, check this helpful link . The decision tree implementations for regression are commonly the C4.5 , the C5.0 or the CART algorithm. The one that is used by sklearn is CART , please take a look at section 1.10.6 in this link . A good example of how to use the sklearn Decision Tree for regression is this .
