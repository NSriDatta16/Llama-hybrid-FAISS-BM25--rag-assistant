[site]: crossvalidated
[post_id]: 264084
[parent_id]: 60080
[tags]: 
$$\hat{\beta} = ([inv(X'X)]X')(X\beta + \epsilon)$$ $$\hat{\beta} = \beta + ([inv(X'X)]X')\epsilon$$ $\hat{\beta}$ is an unbiased estimator of $\beta$ under two conditions: $X$ is non-stochastic $$E(\hat{\beta}) = \beta + E[([inv(X'X)]X')\epsilon]$$ if $X$ is deterministic, this would reduce to: $$E(\hat{\beta}) = \beta + ([inv(X'X)]X') E[\epsilon]$$ The second term on right hand side, $E[\epsilon]$ is zero under one of the Gauss markov assumption. $X$ is stochastic but independent of error ($\epsilon$) Using this, we can reduce the equation to: $$E(\hat{\beta}) = \beta + inv(X'X)] E[(X')\epsilon]$$ where $E[(X')\epsilon] = 0$ from an assumption that comes from one of the OLS's properties, $E[X'e] = 0$. Reference: https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf Thanks Anurag
