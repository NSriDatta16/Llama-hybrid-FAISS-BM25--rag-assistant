[site]: crossvalidated
[post_id]: 642777
[parent_id]: 641427
[tags]: 
First, your stepwise approach, starting with the full model and then removing "insignificant" interaction terms, will make subsequent inference incorrect. Once you start using the results of a model to alter the model, the assumptions used for calculating p -values no longer hold. See this answer and this answer . I'll continue based on the full model with all pre-specified interactions. Second, there's a problem defining a "main effect" for a predictor that's involved in an interaction. That's the case for any regression model. There's nothing specific here to a negative binomial model, except for the interpretation of coefficient estimates and some details of which versions of tests to perform. Some possibilities follow. If you want to assess the overall importance of C in your full model, then what you want to evaluate is C + A:C + B:C + A:B:C . You could do that via a likelihood-ratio test. It can be more convenient to do a Wald " chunk test " on all coefficients involving C . At the other extreme, evaluating the "main effect" in terms of the individual coefficient for C in your model is a fool's errand. Changing the reference level (the level internally coded as 0) of either A or B would change the individual coefficient estimate for C , as they both interact with C . Changing the reference (0) level is the same as centering a continuous predictor involved in an interaction . The fundamental model and its predictions would be the same, but the value reported for the individual coefficient of C would be different and thus so would the apparent "significance" of its difference from 0. You can perform some type of analysis of variance. If you have a perfectly balanced model, you could call anova() directly on your model.count.nb object to get sequential likelihood ratio tests on all predictors and their interactions, via the anova.negbin() function that the MASS package invokes for such models. If the model isn't fully balanced, however, I think that will give you problems . The Anova() function (note the capital "A") in the car package can avoid the problems of imbalance. Applied to your model.count.nb object, its default "Type-II" test would evaluate, for example, the significance of C after taking into account all terms (including interactions) not involving C . From the help page : Type-II tests are calculated according to the principle of marginality, testing each term after all others, except ignoring the term's higher-order relatives. It's also possible to perform "Type-III" tests, "testing each term in the model after all of the others." (Emphasis added.) This page outlines the different type of ANOVA calculations. To do that with Anova() in the car package you would have to re-code your predictors to use contr.sum() or another set of orthogonal contrasts (not the default "treatment" contrasts), as explained in the help page linked above. Alternatively, the emmeans package can apparently perform Type-III tests without predictor recoding, according to the package's author in an answer illustrating functions in its predecessor lsmeans package. This answer explains a way to get an effect measure for each of A , B and C in your model, evaluated by "averaging [predictor values] over the N cases that were actually observed." If you choose that approach, you can get standard errors and confidence intervals by combining the formula for the variance of a weighted sum of variables with the coefficient variance-covariance matrix.
