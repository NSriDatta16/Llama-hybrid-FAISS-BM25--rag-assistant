[site]: crossvalidated
[post_id]: 589073
[parent_id]: 
[tags]: 
On using the same tokenizer for train and test data

I have used keras.preprocessing text tokenizer to fit on the training data alone, computed the (train) vocab size 'input_dim' and maximum train sequence length 'input_length' before fitting my neural network model with an embedding layer(input_dim, output_dim, input_length). When I try to process the test data and predict my trained model, I am not sure about the effect of applying the tokenizer fitted on the train data to my test data. In particular I did not re-compute the input_dim and input_length for test data, and used trainTokenizer.texts_to_sequences() on the test data straightaway. Processing it with the train tokenizer ensures the same input_dim and input_length, such that the test data has a compatible shape when passed to predict function. However the original test data has greater max sequence length and I wonder if there is information loss in what I have done. But otherwise I do not think it is statistically appropriate to concatenate the train and test data to fit tokenizer and train model. How should I process my test data such that the dimensions are appropriate for feeding to the model, and simultaneously keep test data 'unseen' in developing the model? Any clarification is welcomed!
