[site]: crossvalidated
[post_id]: 4646
[parent_id]: 4643
[tags]: 
Let's take the second question first: in most cases the presence of C makes the parameters unidentifiable and there's no way to estimate anything. Constraining the intercept to be positive won't help at all. The first problem can be solved by Maximum Likelihood. After all, writing $p$ for the probability that a flip of A is heads, $q$ for the probability that B is heads, and $k_i$, $n_i$, and $m_i$ for the numbers of heads, number of flips of A, and number of flips of B in the the $i^\text{th}$ toss, respectively, we know $$\Pr[k_i] = \sum_{j=0}^{k_i}{{{n_i}\choose{j}}{{m_i}\choose{k_i-j}}p^j(1-p)^{n-j}q^{k_i-j}(1-q)^{m_i-k_i-j}}$$ (taking ${r}\choose{s}$ to be zero whenever $r \lt s$ or $s \lt 0$). The product of these expressions (over $i$) is the likelihood to be maximized by varying $p$ and $q$ subject to $0 \le p, q \le 1$. Maximizing this likelihood for large amounts of data does not look appetizing. If we assume both $p$ and $q$ are sufficiently far from $0$ and $1$ and that the numbers of coin flips are usually large enough that all of $n_i p$, $n_i(1-p)$, $m_i q$, and $m_i (1-q)$ exceed $5$ or so, we can apply the Normal approximation to the Binomial. This tells us that, to a good approximation, the distribution of $k_i$ is Normal with mean $p n_i + q m_i$ and variance $p(1-p) n_i + q(1-q) m_i$. Thus, weighted least squares regression (WLS) of $k$ on $(n,m)$ without any intercept term is justified. The weights are inversely proportional to these variances. However, the variances depend on the solution. Several approaches immediately come to mind: Use ML based on the Normal approximation. This likelihood is not as messy as the (more accurate) Binomial likelihood. Guess initial values for $(p, q)$, use these to compute the weights, and perform WLS to update the estimates of $(p, q)$. Replace the initial values with these estimates and iterate until convergence occurs (one hopes). Obtain Bayesian estimates for the parameters. Edit Assuming, as suggested in a comment, that C is flipped approximately the same number of times (say $l$) in each trial, let's analyze this situation using the Normal approximation. Letting the expectation of C be $r$, the expectation of $k_i$ becomes $p n_i + q m_i + r l$ with variance $p(1-p)n_i + q(1-q)m_i + r(1-r)l$. We can consider playing the iterated WLS game by estimating $p$, $q$, and $\rho = r l$ with WLS (constraining the intercept to be nonnegative as originally suggested) and updating the previous values of the parameters with the new estimates $\hat{p}$, $\hat{q}$, and $\hat{\rho} / l$. Thus, when $l$ is known, it appears that $p$, $q$, and $r$ are indeed identifiable and that--assuming the iterative method works--there is an effective algorithm for finding them. Note, too, that the Normal approximation should work well provided the expectation of the $k_i$ is typically between 5 and $n_i + m_i + l - 5$; that is, some of $p$, $q$, and $r$ can be quite small or even zero and this still should work provided that neither heads nor tails is a rare occurrence among the totals . When $l$ is not known or varies we can still estimate $r l$ provided we have a reasonable estimate for the contribution of C to the variance, $r(1-r)l$. "Reasonable" means that when we vary $l$ and $r$ within meaningful ranges, the changes in the total variances $p(1-p)n_i + q(1-q)m_i + r(1-r)l$ do not alter the parameter estimates appreciably. This would be the case, for instance, when the contribution of C to the total number of heads is consistently relatively small. (We don't really care about $r l$--it's a nuisance parameter--but if it is not accurately estimated then there must be some bias in the estimation of $p$ and $q$.)
