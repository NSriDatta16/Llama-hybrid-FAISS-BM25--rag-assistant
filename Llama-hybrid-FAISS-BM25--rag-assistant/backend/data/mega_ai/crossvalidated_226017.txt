[site]: crossvalidated
[post_id]: 226017
[parent_id]: 224964
[tags]: 
I think it's natural that you are confused by this because having quickly glanced at the survey, it seems that the authors use the term goal in two different ways. The first is the more standard one: with reference to a single agent, given a goal of the agent e.g., to solve a maze puzzle, or make money in the stock market, one can define rewards. Then the agent's RL "goal" becomes to maximize aggregate rewards. For a MARL problem assume we already have defined rewards. Then the second meaning of goal, from your quote above, is how to measure the success of the MARL algorithm. The point is that for a single agent one can just maximize aggregate reward, but in a multi-agent setting this is now a (competitive, cooperative or mixed) game. Thus there is not a concept of an optimum (necessarily, though there might be), but instead there is the idea of stable policy profiles (e.g. learning dynamics that have converged, actual Nash equilibria etc.). Thus I believe that the meaning of goal in your question is the goal of the actual MARL algorithm in terms of the properties of the profile of policies for the multiple agents (where for one agent this is easy since it's just an optimization problem - though as mentioned above designing good rewards is still important).
