[site]: crossvalidated
[post_id]: 600439
[parent_id]: 600433
[tags]: 
Three reasons come to mind. Taking the expected value of a transformation need not equal that same transformation of an expected value. Jensen’s inequality is one result related to this. You can do this when you have multiple events correspond to the same features, as you appear to have. For each value of concentration and the “water” indicator variable, you have multiple observations, some of which are successes and some of which are failures. Logistic regression need not operate on such data. It is totally possible, perhaps even more common, to have every success and failure correspond to a unique combination of features. Then you cannot calculate the log-odds, as the event either happened with (probability $1$ ) or did not happen (probability $0$ ). Dividing by zero and taking logarithms of zero is problematic. Why should they be equal? The optimization procedures are totally different. One involves minimizing square loss (corresponding to maximum likelihood estimation for a Gaussian conditional distribution) while the other involves minimizing log loss (corresponding to maximum likelihood estimation for a binomial conditional distribution). For that second one, consider how you would define the log-odds in a table like the one generated by the following code. set.seed(2022) # soon will be time for a new seed value! N
