[site]: crossvalidated
[post_id]: 204489
[parent_id]: 
[tags]: 
Why is xgboost overfitting in my task? Is it fine to accept this overfitting?

My set-up is the following: I am following the guide lines in "Applied Predictive Modeling". Thus I have filtered correlated features and end up with the following: 4900 data points in the training set and 1600 data points in the test set. I have 26 features and the target is a continuous variable. I apply 5-fold cross-validation to train models using the caret package. When I apply a MARS model then I get a mean absolute error (MAE) of approximately 4 on the training set as well as on the test set. However applying XGBgboost (either the tree algorithm or the linear one) I get something like 0.32 (!) on the training set and 2.4 on the test set. Thus if the test error is 8 times the training error then I would say: I have overfit the training data. Still I get a smaller error on test anyways. I use the following parameters on xgboost: nrounds = 1000 and eta = 0.01 (increasing nrounds and decreasing eta could help but I run out of memory and run time is too long) max_depth = 16 : if I compare other posts and the default of 6 then this looks large but the problem is pretty complex - maybe 16 is not too large in this case. colsample_bytree = 0.7 , subsample = 0.8 and min_child_weight = 5 : doing this I try to reduce overfit. If I reduce max_depth then train and test-error get closer but still there is a large gap and the test-error is larger (a bit above 3). Using the linear booster I get the roughly the same train and test error on optimal parameters: lambda = 90 and `alpha = 0: found by cross-validation, lambda should prevent overfit. colsample_bytree = 0.8 , subsample = 0.8 and min_child_weight = 5 : doing this I try to reduce overfit. My feeling is that XGBoost still overfits - but the training error and as far as I can see in the real time test (I have used the XGBoost models and an ensemble of them in reality for 4 days) looks ok-ish (the error is larger than the test error but there are is more uncertainty in real life about the forecast of features and other variables). What do you think: can I accept overfit if (if this is possible) real life performance is superior? Does XGBoost in my setting tend to overfit?
