[site]: crossvalidated
[post_id]: 482450
[parent_id]: 
[tags]: 
indicator function in objective function with $L_2$ norm

I am trying to solve an optimization problem. The objective function is as follows: $\arg \min \lVert\mathbb{A}\mathbf{x} - \mathbf{b}\rVert^2 + \text{other linear least squares terms} + \mathcal{I}(\mathit{x_0 b}) \lVert\mathit{x_n-b}\rVert^2$ where $\mathcal{I}$ is the indicator function that returns $1$ for true condition and $0$ otherwise. $x_0,x_1,...,x_n$ should be between $a$ and $b$ . If $x_0$ or $x_n$ is out of the range, one cost will be added to the objective function. If the indicator function doesn't appear in the objective function, it's simply one linear least squares optimization problem and is simple to solve. Indicator function is not a continuous function and makes the problem difficult. I am not an expert on numerical optimization. I search on the internet and it seems that indicator function is used often in deep learning. Any hints, links and materials are appreciated.
