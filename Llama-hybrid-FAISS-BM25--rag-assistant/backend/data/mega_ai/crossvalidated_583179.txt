[site]: crossvalidated
[post_id]: 583179
[parent_id]: 582380
[tags]: 
Consider this an extended comment, not an answer. Your question is why the training residual mean squared error (RMSE) is five times the test RMSE. The challenge is that you have a 10-step procedure to build the model. There might be an issue in one or more of these steps. It's hard to know since you evaluate the performance once at the very end. Here is an alternative approach. Build a very simple baseline model, with as few steps (lines of code) as possible. Maybe a decision tree or a random forest, omitting the more challenging features. Do you observe TrainRMSE >> TestRMSE? If yes, then you know the issue is with (how you load) the raw data. If no, then add in one of your data-processing/model-building steps. Repeat until you achieve good performance. During this exercise you may stumble upon the original problem. But even this doesn't happen, you would have developed a useful model. Finally, an unrelated comment: Why are you looking at the test dataset while you are building the model? Split the training dataset into a subset for training and a subset for validation.
