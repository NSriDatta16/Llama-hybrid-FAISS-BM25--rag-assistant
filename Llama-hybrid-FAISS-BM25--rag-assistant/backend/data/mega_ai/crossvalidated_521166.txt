[site]: crossvalidated
[post_id]: 521166
[parent_id]: 335454
[tags]: 
This is a really good question. I do not know the exact answer but have a feeling. At the end of one epoch in the training the "input" vector (weights between input data and hidden layer) was the last updated along the process, since we are coming from the back of the model (using back-propagation) and the first weights visited are those between the hidden layer and the output, and so the last weights being updated are those between the input layer and the hidden layer (the so called "input vector") . In addition the weights corresponding to the input vector are closest to the input data (the words in one-hot encoded format), and embedding vectors will be used as input to further NLP tasks. There are additional answers here: Here
