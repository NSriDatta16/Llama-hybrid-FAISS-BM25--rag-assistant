[site]: crossvalidated
[post_id]: 311946
[parent_id]: 311931
[tags]: 
Is there any obvious intuition for why this is the case? Most likely these words appear in different contexts. Is there any well-known way to encoding these relationship-type words in a way that the cosine distance would actually successfully measure semantic similarity? You could try to operate on the embedding space, as in Piotr Migda≈Ç's blog post ( Differences and projections part). Other way to see what is in the blog post is that if you want embeddings of related words to be similar, you need to take a quotient of the word embedding space by the vector/subspace representing the relation (for example king - queen represents gender).
