[site]: crossvalidated
[post_id]: 439248
[parent_id]: 
[tags]: 
Choosing learning rate with 2nd order method - minimizing parabola in one step?

In parabola $(\theta,g)$ values are in line $(g=f'(\theta))$ - we can get slope of this line e.g. by dividing their standard deviations: $$ \mu = \frac{\sigma_\theta}{\sigma_g}=\sqrt{\frac{var(\theta)}{var(g)}}=\sqrt{\frac{mean(\theta^2)-mean(\theta)^2}{mean(g^2)-mean(g)^2}}$$ In minimum $g=0$ - we get in one gradient descent step $(\theta\leftarrow \theta -\mu g)$ to where this line intersects $g=0$ if using above $\mu$ learning rate. Its calculation requires maintaining four (exponential moving) averages to adapt learning rate, e.g. independently for each coordinate in SGD (more details in 5th page here ). There is popular $\sqrt{mean(g^2)}$ denominator e.g. in RMSprop, Adam, but are there considered SGD optimizers using variance? Is it safe to just jump to minima of local parabola approximations? Maybe we should use some smaller e.g. $0.5 \mu$ learning rate? Is there a literature about 2nd order choice of learning rate? Any other its choices?
