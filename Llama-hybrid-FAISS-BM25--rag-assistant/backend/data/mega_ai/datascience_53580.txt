[site]: datascience
[post_id]: 53580
[parent_id]: 
[tags]: 
Timeseries prediction error measurement. How to deal with diffrent time scales?

I have some time series and a prediction model. Now I would like to measure how good/bad the prediction is for different products. The problem is that for each product the time points (frequency of information varies very strong) So as an example on series have updates in second intervals: product price 2014-08-25 01:01:01 A 1.2 2014-08-25 01:01:01 A 1.3 2014-08-25 01:01:02 A 1.2 2014-08-25 01:01:03 A 0.9 2014-08-25 01:01:04 A 1.4 The other series might have updates in days: 2014-08-25 05:00:00 B 10.2 2014-08-26 06:00:00 B 10.2 2014-08-27 06:00:00 B 7.2 2014-08-28 21:00:00 B 10.2 2014-08-29 22:00:00 B 10.2 On the other hand, the prediction continues. So at each time point, the model outputs a price. Right now I tried to resample everything to 60 seconds interval using last prices and take the SMAPE measure since the prices are always greater than 1. I don't like the results yet since the not that frequent time series tend to get bad results compared to more frequent time series. Maybe there is a good way to deal with that. Especially if the error measure could take into account that if the time series is more irregular, the measurement should be more relaxed. I'm thankful for any Idea or hint.
