[site]: crossvalidated
[post_id]: 471733
[parent_id]: 471726
[tags]: 
Most of the machine learning models use some kind of regularization (see other questions tagged as regularization ). In simple words, what regularization does it forces the model to be more simple then it can be. To give few examples: LASSO forces pushing regression parameters towards zero, so practically removing them from the model. Dropout that turns on and off different parts of the neural network, so that it needs to learn how to work with smaller sub-networks, instead of using all parameters, what makes it more flexible. When using bagging , you train multiple models using different, random, subsamples of the data, usually subsampling also the columns, and then aggregate them. So the individual models in your ensemble will need to learn how to use different features, and aggregating multiple models would "cancel out" scenarios where the individual model overfit. Moreover, some recent results show that even without explicit regularization, neural networks, but also some other models, are able to work well in scenarios where they have many more parameters then datapoints, so in cases where they could literally memorize the whole data and overfit. Apparently, that is not the case and the models seem to regularize themselves, but the mechanism is still not known to us. This would suggest that we may not understand why this happends well enough yet.
