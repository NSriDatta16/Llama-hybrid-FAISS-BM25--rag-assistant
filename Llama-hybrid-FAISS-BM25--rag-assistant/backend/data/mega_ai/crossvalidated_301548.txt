[site]: crossvalidated
[post_id]: 301548
[parent_id]: 301532
[tags]: 
No. Rank $k$ PCA of matrix $M$ can be formulated as $\hat{X} = \underset{rank(X) \leq k}{argmin} \| M - X\|_F^2$ ($\|\cdot\|_F$ is Frobenius norm ). For derivation see Eckart-Young theorem . Though the norm is convex, the set over which it is optimized is nonconvex. A convex relaxation of PCA's problem is called Convex Low Rank Approximation $\hat{X} = \underset{\|X\|_* \leq c}{argmin} \| M - X\|_F^2$ ($\|\cdot\|_*$ is nuclear norm . it's convex relaxation of rank - just like $\|\cdot\|_1$ is convex relaxation of number of nonzero elements for vectors) You can see Statistical Learning with Sparsity , ch 6 (matrix decompositions) for details. If you're interested in more general problems and how they relate to convexity, see Generalized Low Rank Models .
