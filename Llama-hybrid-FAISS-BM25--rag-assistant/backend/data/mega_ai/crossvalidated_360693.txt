[site]: crossvalidated
[post_id]: 360693
[parent_id]: 
[tags]: 
Reinforcement learning, question from Sutton's new book

In Sutton's new RL book, in chapter 3, there is an exercise Exercise 3.6 Suppose you treated pole-balancing as an episodic task but also used discounting, with all rewards zero except for âˆ’1 upon failure. What then would the return be at each time? How does this return differ from that in the discounted, continuing formulation of this task? My answer is if it is episodic with discounting, the return at each time step is $$-\gamma^{K_1} - \gamma^{K_2} - \cdots - \gamma^{K_n},$$ where $K_i$ is the number of total time steps until it fails at episode $i$ . And for continuous tasks with discounting, the return at each time step is $-\gamma^K$ , where $K$ is the number of time steps until it fails. I am not sure whether my answer is right or not. Could you please help me?
