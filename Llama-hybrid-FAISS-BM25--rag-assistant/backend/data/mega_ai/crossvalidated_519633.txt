[site]: crossvalidated
[post_id]: 519633
[parent_id]: 
[tags]: 
Leave-one-out Cross Validation: What are best practices for reporting results and developing a final model?

I am developing a simple linear regression model based on a limited set of data that represents monthly meteorological observations over six years (72 points). Of these data, I need a certain number of non-zero observations to make a reasonable regression. Furthermore, I need to capture seasonal variation so want to use complete years of data. So, the actual data available for the regression may be relatively small - I have set a minimum threshold of 11 nonzero observations to make the regression; if there are fewer, I don't make a model. In order to make maximum use of the training data, I would use leave-one-out cross validation (LOOCV) to evaluate the quality of my regression model. For this, I leave out one year's worth of data at a time, fit the regression model with the remaining data, and test the resulting model against the excluded observations. This procedure results in six regression models and six RMSE estimates. My questions are : In generating the final regression model, should I average the fitted model coefficients across the six LOOCV iterations? Or make a seventh iteration that uses all training data? Or something else? What is best practice for reporting the quality of the final fitted model? For example, report the RMSE for each LOOCV iteration separately, or take a mean? Or something else?
