[site]: datascience
[post_id]: 81833
[parent_id]: 81811
[tags]: 
I'll try and provide some intuition for you here, instead of focusing on the mechanics of the math behind the methods. Imagine you are evaluating whether a coin is fair or not, so you collect a sequence of heads and tails as your data set. In MLE, we simply look at the data we collected and find the maximum likelihood... this works well when we have no prior knowledge to leverage (i.e. we have no idea if the coin is fair or not). By contrast, in MAP we take the same likelihood we used in MLE but now multiply by our prior knowledge. For instance, we may strongly suspect that our coin is biased and so we can influence our estimate with that knowledge through a prior distribution. This new estimate is a mixture of what we believe (our prior) and what we measured (our likelihood). Thinking of two extreme cases here would be 1) if we very strongly believe in our prior then we would need to collect a lot of data to influence the resulting estimate away from the prior. Conversely, if we know very little up front (i.e. we have an uninformative prior) then finding the MAP estimate is equivalent to the MLE estimate because our prior did not influence the result. Perhaps a visual representation can help too: MLE is finding the maximum of the green curve. MAP is 1) multiplying the blue curve by the green curve to create the red curve and 2) finding the maximum of the newly created red curve.
