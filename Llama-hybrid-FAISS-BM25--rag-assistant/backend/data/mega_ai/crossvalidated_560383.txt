[site]: crossvalidated
[post_id]: 560383
[parent_id]: 
[tags]: 
How can we explain the "bad reputation" of higher-order polynomials?

We all must have heard it by now - when we start learning about statistical models overfitting data, the first example we are often given is about "polynomial functions" (e.g., see the picture here ): We are warned that although higher-degree polynomials can fit training data quite well, they surely will overfit and generalize poorly to the test data. Why does this happen? Is there a mathematical justification as to why (higher-degree) polynomial functions overfit the data? The closest explanation I could find online was something called "Runge's phenomenon" , which suggests that higher-order polynomials tend to "oscillate" a lot - does this explain why polynomial functions are known to overfit data? I understand that there is a whole field of "regularization" that tries to fix these overfitting problems (e.g., penalization can prevent a statistical model from "hugging" the data too closely) - but just using mathematical intuition, why are polynomials known to overfit the data? In general, "functions" (e.g., the response variable you are trying to predict using machine learning algorithms) can be approximated using older methods like Fourier series , Taylor series and newer methods like neural networks . I believe that there are theorems that guarantee that Taylor series, polynomials and neural networks can "arbitrarily approximate" any function. Perhaps neural networks can promise smaller errors for simpler complexity? But are there mathematical reasons behind higher-order polynomials (e.g., polynomial regression) being said to have a bad habit of overfitting, to the extent that they have become very unpopular? Is this solely explainable by Runge's phenomenon? Reference: Gelman, A. and Imbens, G. (2019) Why high order polynomials should not be used in regression discontinuity designs . Journal of Business and Economic Statistics 37(3) , pp. 447-456. (An NBER working paper version is available here )
