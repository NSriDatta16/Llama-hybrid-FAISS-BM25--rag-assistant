[site]: crossvalidated
[post_id]: 253036
[parent_id]: 252965
[tags]: 
First, I would see if the doctors agree with each other. You can't analyze 50 doctors separately, because you'll overfit the model - one doctor will look great, by chance. You might try to combine confidence and diagnosis into a 10 point scale. If a doctors says that the patient doesn't have cancer, and they are very confident, that's a 0. If the doc says they do have cancer and they are very confident, that's a 9. If they doc says they don't, and are not confident, that's a 5, etc. When you're trying to predict, you do some sort of regression analysis, but thinking about the causal ordering of these variables, it's the other way around. Whether the patient has cancer is the cause of the diagnosis, the outcome is the diagnosis. Your rows should be patients, and your columns should be doctors. You now have a situation that's common in psychometrics (which is why I added the tag). Then look at the relationships between the scores. Each patient has a mean score, and a score from each doctor. Does the mean score correlate positively with every doctor's score? If not, that doctor is probably not trustworthy (this is called the item-total correlation). Sometimes you remove one doctor from the total score (or mean score) and see if that doctor correlates with the mean of all the other doctors - this is the corrected item total correlation. You could calculate Cronbach's alpha (which is a form of intra-class correlation), and the alpha without each doctor. Alpha should always rise when you add a doctor, so if it rises when you remove a doctor, that doctor's rating is suspect (this doesn't often tell you anything different from the corrected item-total correlation). If you use R, this sort of thing is available in the psych package, using the function alpha. If you use Stata, the command is alpha, in SAS it's proc corr, and in SPSS it's under scale, reliability. Then you can calculate a score, as the mean score from each doctor, or the weighted mean (weighted by the correlation) and see if that score is predictive of the true diagnosis. Or you could skip that stage, and regress each doctor's score on diagnosis separately, and treat the regression parameters as weights. Feel free to ask for clarification, and if you want a book, I like Streiner and Norman's "Health Measurement Scales". -Edit: based on OPs additional info. Wow, that's a heck of a Cronbach's alpha. The only time I've seen it that high is when a mistake was made. I would now do logistic regression and look at the ROC curves. The difference between weighting by regression and correlation depends on how you believe the doctors are responding. Some docs might be generally more confident (without being more skillful), and hence they might use the extreme ranges more. If you want to correct for that, using correlation, rather than regression, does that. I would probably weight by regression, as this keeps the original data (and doesn't discard any information). Edit (2): I ran logistic regression models in R to see how well each predicted the output. tl/dr: there's nothing between them. Here's my code: d And the output: > par(mfrow = c(2, 2)) > roc(d$cancer, d$prc, ci = TRUE, plot = TRUE) Call: roc.default(response = d$cancer, predictor = d$prc, ci = TRUE, plot = TRUE) Data: d$prc in 81 controls (d$cancer 0) roc(d$cancer, d$pun, ci = TRUE, plot = TRUE) Call: roc.default(response = d$cancer, predictor = d$pun, ci = TRUE, plot = TRUE) Data: d$pun in 81 controls (d$cancer 0) roc(d$cancer, d$pca, ci = TRUE, plot = TRUE) Call: roc.default(response = d$cancer, predictor = d$pca, ci = TRUE, plot = TRUE) Data: d$pca in 81 controls (d$cancer 0) roc(d$cancer, d$pic, ci = TRUE, plot = TRUE) Call: roc.default(response = d$cancer, predictor = d$pic, ci = TRUE, plot = TRUE) Data: d$pic in 81 controls (d$cancer 0)
