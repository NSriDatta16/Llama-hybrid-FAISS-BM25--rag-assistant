[site]: crossvalidated
[post_id]: 397925
[parent_id]: 313070
[tags]: 
You're seeing both equations because they're used at different points in training. $$\frac{{1}}{2} \sum_{i}^{n} ( y_i - \widehat{y_i})^2 \quad$$ is the error of a single training example, where n is the number of output nodes. The second equation, $$\frac{{1}}{2n} \sum_{i}^{n} ( y_i - \widehat{y_i})^2 \quad$$ is the error of the network across all n training examples, though these are usually broken up into mini-batches to make calculation less expensive. Also, $(\widehat{y_i} -y_i)^2 = (y_i - \widehat{y_i})^2$ . Source: http://neuralnetworksanddeeplearning.com/chap2.html Old question but still the top Google result for "MSE neural network"
