[site]: crossvalidated
[post_id]: 476685
[parent_id]: 476672
[tags]: 
Feature selection is a well-studied area within Machine Learning and Pattern Recognition. In general, pruning features is a non-monotonous process. The relative importance of remaining features change when seemingly redundant features are being removed from your Twitter sentiment classifier. There are ready-to-use webservices available that perform feature selection for you, complete with a graphical analysis and feature ranking. Take a look at classifier service , which has an option for feature selection and feature analysis. Just adding (almost) indefinitely many feature variables to a classifier leads to so-called peaking , which means that after a certain point adding even more feature variables begins to reduce the performance on independent test sets. This reduction in performance is counter intuitive because all the truly informative variables are still presented to the classifier. The performance peak has been exceeded. Peaking is caused by over-generalization because each added feature introduces yet more parameters to the classifier. At a certain point, given a limited training set size , the parameters begin to fit noise in the data. Advanced feature-selection schemes are available including floating search and Markov chain Monte Carlo techniques. Applying these algorithms to feature selection requires specialist knowledge.
