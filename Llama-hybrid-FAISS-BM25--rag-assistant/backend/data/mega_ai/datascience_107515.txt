[site]: datascience
[post_id]: 107515
[parent_id]: 107509
[tags]: 
Vocabulary size, padding length and embedding dimension are like hyperparameters which needs to chosen wisely to get good performance from model Vocabulary Size : The set of unique words used in the text corpus is referred to as the vocabulary. When processing raw text for NLP, everything is done around the vocabulary. When the text corpus is large and you need to limit the vocabulary size to increase training speed or prevent overfitting on infrequent words. To do this most people restrict it to specific number or say apply a threshold for example Vocabulary size is equal to words which have frequency greater than 10. Padding Length : Since LSTM takes input of same length, inputs are padded to the maximum length of the sequence in the batch Embedding Diemnsions : Usually people use multiple of 2 like 128, 256 and 512. Higher the dimension better the capturing of information but more the time required for training
