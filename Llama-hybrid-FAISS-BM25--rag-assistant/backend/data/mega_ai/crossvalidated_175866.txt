[site]: crossvalidated
[post_id]: 175866
[parent_id]: 175736
[tags]: 
Claim 1. When dealing with prediction, averaging different models having the same performance (in terms of your error function) improves the performance. This is due to the convexity of your loss function. As an aside, the less correlated your models are, the better is the gain you achieve averaging them. When you train a batch model, the order of the lines does not matter. In a streaming algorithm, the order of the lines matters. Shuffling the lines and training the model on the different versions of the file will produce different results. As the model have the same parameters, they usually have comparable performance. As they are different (slightly), their average is usually better than every single algorithm. More details about this fact can be found in the chapter 5 of " Online Learning and Online Convex Optimization ", by Shai Shalev-Shwartz. Visiting the batches in a random order somehow accomplishes the same process: it produces different weights that are averaged on every epoch. A related question is this one : RANDOM learning rate in gradient descent . Authors claimed that multiple training epochs performed on the same data set with a randomized learning rate achieves better performance than a deterministic learning rate.
