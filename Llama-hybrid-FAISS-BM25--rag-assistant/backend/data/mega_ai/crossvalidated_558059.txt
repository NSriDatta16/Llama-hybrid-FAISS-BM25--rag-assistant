[site]: crossvalidated
[post_id]: 558059
[parent_id]: 
[tags]: 
How to perform a regression with categorical variables as input and numerical output?

I am a chemist and I am currently working on a machine learning/statistical learning based project. This question is related to a previous question that I asked before, but is more about the learning method. I have a set of molecules, and I have to predict the spectral peak for each molecule. That's the chemistry part. The statistical modelling part is that I have an array for each molecule as input, which have binary data that looks like this: molecule1_fingerprint = [0,1,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,1,...,0,0,0,0,1] Each number indicates the presence or absence of a certain feature in the molecule. I can also use another type of numbers (called hashed fingerprints): molecule1_hashed = [0,2,0,6,1,0,7,15,...,0,7,0] Here similar features are expressed by one number so some adjacent numbers in the original array gets added together, reducing the size of the input. So my input data would look something like this: X_train = [[0,0,0,1,0,0,1,...,0,0,1,0], [0,1,0,0,1,0,1,...,0,0,1,0], : : : : : : : : : : : : : : : : : : : : : : : : : : : : [0,1,0,0,1,1,1,...,0,1,0,0]] My target is would be a floating point number, (I could use integers if required): y_train = [100.6, 20.8, -12.9, 10.8, -19.2, 5.7, ..., 8.9, -40.3] I am not sure how to approach this problem. I have tried using support vector regression, but it gives very poor results (score of -0.003). My intuition from chemistry is that the target y value would have contributions from each of the feature in the input, (i.e. the presence of certain feature would add or subtract a certain amount, and the total value would be obtained by adding those contributions.) Another problem is that I cannot standardize the y values, because I need their exact values to be predicted. So for example, if I standardize for one dataset, there is no guarantee that the standardization would be the same for another dataset. I am using scikit-learn for my calculation. I have uploaded a sample dataset that I am working with, on this github page. The arrays can be loaded with numpy.loadtxt() , and are in the scikit-learn orientation. There are 65 molecules in the dataset, and 732 features. The first 731 features are hashed fingerprints, the last feature can be ignored for now. Any advice about how to choose a method and how to handle the data?
