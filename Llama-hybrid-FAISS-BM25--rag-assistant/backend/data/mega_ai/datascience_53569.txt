[site]: datascience
[post_id]: 53569
[parent_id]: 
[tags]: 
Why categorical cross entropy loss is not correlated with NLP scores?

I'm training a deep network for image captioning which is consist of one CNN and three GRUs. During training epoch by epoch model loss (categorical cross entropy) decreases but when I'm measuring bleu , METEOR , ROUGE , CIDEr and SPICE scores,I get best ones in the first epoch that has worst loss. I don't get why this is happening? And if categorical cross entropy is not a suitable loss function for autoencoder then what should I use instead?
