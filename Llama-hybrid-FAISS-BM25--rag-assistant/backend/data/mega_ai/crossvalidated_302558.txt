[site]: crossvalidated
[post_id]: 302558
[parent_id]: 
[tags]: 
data normalization in neural networks

I read multiple posts about normalizing data in case of neural networks. One question - does normalization also help in reducing values of the input to sigmoid function - z=x1*w1+x2*w3.... ? Andrew Ng recommends ( video ) setting variance of Weights in a layer to be equal to 1. He mentions that this allows z to also take on values on a similar scale - mean 0 and variance 1. on a similar note, wont normalization of inputs help to bring value of z down? if my X1 has range in billions then my weight need to be very small. and it will cause gradient descent to take a lot of iterations. my main question - lets say data has multiple inputs and all of them have 1 variance and similar means, but Xs are in millions. IN such case wont normalization help?
