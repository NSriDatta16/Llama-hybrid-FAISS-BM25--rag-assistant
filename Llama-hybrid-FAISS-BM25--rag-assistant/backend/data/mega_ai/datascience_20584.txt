[site]: datascience
[post_id]: 20584
[parent_id]: 
[tags]: 
XGBoost: predict on only valuable features

I have a somewhat strange case that I can't find an answer to anywhere. It is really only applicable if you have a legitimately large data set. Let me describe. XGBoost returns feature importance for all variables in the model. I have been using a data set with over 1000 features and typically about 25-50% of them come back with non-zero feature importance. Now I want to make predictions with this model. Imagine I am making predictions on A LOT of observations (over 1m, imagine predicting on incoming tweets or something). If I re-train the model, giving it only the columns that the original training picked out as valuable (non-zero importance), then the re-trained model makes predictions MUCH FASTER than the original, and WITH THE EXACT SAME ACCURACY. This last part makes sense, because it is building the exact same trees. Also, not only is it faster, but it takes up a lot less space in memory because you are passing it something like a 1m x 250 matrix instead of a 1m x 1000 matrix. My question is: is there a way to train a model, and then pass the predict function only the features it actually uses without having to re-train the model to except only those features instead of the full matrix? This blog post discusses a similar concept of using XGBoost for feature selection, but it does it by retraining the model each time. The obvious answer is "just re-train it" but if I am using a lot of training observations then training the model takes awhile, so I would rather not have to retrain it if possible. I am doing this in R, but solutions in Python/sklearn would also be helpful. Thanks!
