[site]: datascience
[post_id]: 14874
[parent_id]: 
[tags]: 
Contrasting logistic regression vs decision tree performance in specific example

I have a set of 10,000 integers, and another set of 100. The integers in the first set are mapped to integers in the second set according to some rules (not mathematical rules, think of these values as codes for naming certain items, it is some categorical mapping). The mapping is not necessarily 100 to 1, in some case I may have just 30 or so integers from the first set mapped to an integer in the second set, in other cases 300, but on average of course it is 100 to 1. Using sklearn, I created a decision tree that was able to get over 99% accuracy, as I would expect. When I tried logistic regression, though, accuracy was just 45%. The training sample is about 100,000 example, so, it should be enough to learn. What is going on? Is there something inherently different in the logistic regression method that I am missing?
