[site]: crossvalidated
[post_id]: 441956
[parent_id]: 
[tags]: 
Why do we get a new co-ordinate system when we dot product the transpose of eigen vectors with the transpose of a matrix

I am working on implementing PCA on the MNIST dataset and have calculated the eigen vector and eig Values from the co-variance matrix. Now I want to have a new co-ordinate system represented by PC1 and PC2 just so I can project the datapoints on PC1. I have a code snippet where the new coordinate is created by taking the dot product of the transpose of eigen vectors with the transpose of the standardized data matrix. eigValues, eigVectors = linalg.eigh(Covariance_mat, eigvals= (782, 783)) new_cordinates = np.matmul(eigVectors.T, standardize_mnist.T) new_cordinates Output: `````` array([[-5.2264454 , 6.03299601, -1.70581328, ..., 7.07627667, -4.34451279, 1.55912058], [-5.14047772, 19.29233234, -7.64450341, ..., 0.49539137, 2.30724011, -4.80767022]]) Now, I am not sure about the intuition behind this dot product, which leads to a new 2d coordinate system. I am very new to Machine Learning hence, could someone please help me understand this part.
