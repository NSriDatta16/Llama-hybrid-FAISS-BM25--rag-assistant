[site]: datascience
[post_id]: 53883
[parent_id]: 53882
[tags]: 
In this case, I think it should be fine to use word embedding on both languages since word embedding learns the meaning of individual words regardless of languages...Is this reasonable? Or maybe I do need to separate the languages and build different models for each language? If I think logically you are correct. Word embedding is merely a collection of Tokens, which derived its features on the basis of nearby words in a sentence. So if you have sufficient raw data(mix of both), I think its good to go, though results will explain you more :). However its good to see how such models will behave in case we have mix of LeftToRight(LTR) and RTL languages.
