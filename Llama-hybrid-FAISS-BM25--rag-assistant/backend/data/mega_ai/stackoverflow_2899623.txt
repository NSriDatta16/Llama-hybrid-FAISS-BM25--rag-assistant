[site]: stackoverflow
[post_id]: 2899623
[parent_id]: 2899381
[tags]: 
It's pretty hard to measure an improvement before you've implemented something. There's going to be a certain amount of educated guesswork involved. I'm presuming the business has already established that the app/website is slow and costing money. I'm also assuming you've already ruled out other obvious performance improvements (database round-trips, caching, web front-end payload, etc - without knowing anything about your app.) My first step would be to add a few lines of stopwatch code around the slow code in question, and log the response times over a few thousand operations in a live environment. Compare the average figures you see to the response times that you want to achieve. Then run a code profiling tool on that same code (e.g.dotTrace for .NET) to see where your code is spending most of it's time. Apply the percentage of time spent in parallelizable code to the average times from the stopwatch, and you'll get a good idea of whether it can be made faster. Obviously it's not a case of dividing that figure by the number of cores because there is synchronization overhead, and there will be other tasks running in the real-world. But this should give you a close enough estimate of whether it will be feasible.
