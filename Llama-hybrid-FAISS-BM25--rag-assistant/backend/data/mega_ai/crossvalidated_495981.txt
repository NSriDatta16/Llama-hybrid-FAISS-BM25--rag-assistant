[site]: crossvalidated
[post_id]: 495981
[parent_id]: 
[tags]: 
Update beta distributed prior with data that is a probability

In my experience with Bayesian statistics, beta distributions are typically used to estimate the posterior for parameter, $p$ , of a binomial distribution that has been used to generate some data. But my question is, how should a posterior for data that is generated by sampling probabilities themselves rather than probabilistic outcomes be calculated? To explain, say each piece of my data is randomly drawn from a beta distribution: $x \sim Beta(70, 30)$ My goal is to use this data to calculate the posterior distribution for the most likely probability (i.e., $\frac{a}{(a+b)}$ ), which is $0.7$ in this example. The approach I've taken so far is to start with a prior that is an uninformative beta distribution: $prior \sim Beta(1,1)$ . I then update this prior with each new piece of data as follows: $Beta(a+x, b+(1-x))$ . While this approach seems to do what I want, it's not clear to me how much each piece of new data should reduce the uncertainty in the prior. That is, if I know I'm sampling from a relatively noiseless distribution, then I should gain confidence rapidly with each new piece of data, but I should be more cautious if the incoming data is noisy. I can control the learning rate, $r$ , by changing how I update the prior to: $Beta(r\cdot(a+x), r\cdot(b+(1-x)))$ . But this means setting an arbitrary learning rate. What is the proper Bayesian learning rate to use in this case? Alternatively, is my approach totally off and should I not be using a beta distribution for the prior in this situation?
