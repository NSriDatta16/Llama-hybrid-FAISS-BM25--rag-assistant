[site]: datascience
[post_id]: 60338
[parent_id]: 60323
[tags]: 
Your "commonly agreed on" and "standard" accuracy is meant for binary classification, in which case it agrees with the other formula from sklearn. In that case, "positive/negative" refer to the two classes, so this is also a little different from your version. Your version of it is a sort of average of ( the "mediant" of ) the implicit one-vs-rest classifiers. As such, your score is meaningful, but will generally be larger than the actual common multiclass accuracy metric. For a balanced problem, a constant classifier will get a mediant-of-OVR-accuracy score of $(n-1)^2/n^2$ but an accuracy score of just $1/n$ . (Back to the binary case, to compare your method, you'd have to interpret e.g. "Sum(TN)" as including both diagonal entries, so the "accuracy" there is actually $1/2n$ , which agrees with the mediant-of-OVR score.) As such, your metric is similar to macro-averaged scores (though I've never heard of that for accuracy, only precision/recall/Fbeta). Micro Average vs Macro average Performance in a Multiclass classification setting Finally, as an opinion, since accuracy measures the probability of getting the prediction right, it's easier to interpret; your metric gives credit for not misclassifying a sample into each class it's not put in, hence the inflation. Of course, this also perhaps makes a multiclass model's accuracy sound terrible (sklearn says "this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted").
