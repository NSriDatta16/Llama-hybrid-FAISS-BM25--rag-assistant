[site]: crossvalidated
[post_id]: 234330
[parent_id]: 
[tags]: 
Questions about modeling Markov Decision Process (MDP) or Partially Observable Markov Decision Process (POMDP) for a task

I'm working on a human-robot team task and am trying to model the task as Markov Decision Process (MDP) or a Partially Observable MDP (POMDP) as part of my thesis. I've read a lot of papers on this and am still finding it hard to apply it directly to the task at hand. This is a long post and thank you for everyone who reads it. The task at hand is sorting objects by color that are on a table. I'm limiting the total number of objects to 8 (4 on each side) for now. I have a robot that is one end of the table and human on the other. My state and action space is (copying directly from code): State = namedtuple("State", [ 'n_r', # number of robot's objects on its side 0..MAX_OBJECTS_ACC 'n_h', # number of teammate's objects on its side 0..MAX_OBJECTS_ACC 't_r', # Is the robot transferring? Y/N 't_h', # Is the teammate transferring? Y/N 'b_r', # number of robot's objects held by the robot 0..2 'b_h', # Is robot holding teammates object? Y/N 'e' # Is the task completed? Y/N ] ) n_state_vars = 7 # dictionary which maps index to an action key to a tuple containing an index and explanation for the action task_actions_expl = { 'TR': (0, 'Take robot\'s object from table'), 'K' : (1, 'Keep object on table'), 'R' : (2, 'Receive object from teammate'), 'TH': (3, 'Take teammate\'s object from table'), 'G' : (4, 'Give object to teammate'), 'WG': (5, 'Wait for teammate to receive'), 'WS': (6, 'Wait for state change'), 'X' : (7, 'Exit task'), } Even though there are two agents working at the task (human and robot), I modeled it as a single agent MDP. I was told that this violates the Markov assumption as I can't have an agent "give an object" that magically vanishes after the teammate takes it or "take an object" that magically appears when the teammate gives it without accounting for that. Markov assumption is basically the state of the process in the next time-step totally depends on the current state and action taken by the agent. Isn't that still valid in this case? Is it because I don't take into account the state-space and the actions taken by the other agent and that would alter the environment to be non-Markov? If that is true then is the number of states actually (states of each agent)^2? If the above reason is correct, could I get some ideas on how I could model this? If this was a POMDP should I consider the other agent's state space as "the hidden state(s)" in the POMDP frame work? If so, what would the observation be that gives me info on the hidden state space? Finally, in general for controlling state-space dimensionality, we extract features of the state. But in my head I feel features are the same as observations (in the POMDP) framework. Are they the same or different and if they are different how exactly are they different? I know this is a lot of information and lot of questions (many open ended) to ask on a answers forum, but I've thinking about these questions and reading papers about them for a long time and still haven't gotten a good answer that would make me understand. That's why I came here as a last resort. Any help is appreciated and please let me know if more info is needed. Thanks!
