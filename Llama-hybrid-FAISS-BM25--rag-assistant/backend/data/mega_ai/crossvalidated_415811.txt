[site]: crossvalidated
[post_id]: 415811
[parent_id]: 415804
[tags]: 
Validation sets and overfitting Let us start with your question about why we use a validation set. We use this to get an estimate of the true out-of-sample error of our model. We use a separate set here because, as you know, looking at in-sample residuals will give us a too optimistic idea of our performance. Now, suppose we apply our model $M_1$ to the validation set and obtain an error that does not make us happy... but then we notice something about that validation set that $M_1$ didn't catch very well. Which gives us an idea of how to improve the model. So we go back to our training and test set, tweak the model, optimize it a bit more, finally apply the resulting model $M_2$ to the validation set, and success! $M_2$ performs better on the validation set than $M_1$ ! Hurray! And since we didn't use the validation set in training the model, everything is fine, right? Except, of course, it isn't. We have used the validation set, since it informed our modification of $M_1$ to $M_2$ . We have already started down the slippery slope towards overfitting on the validation set , and we can happily slide down that slope some more by tweaking our model yet more to $M_3$ , $M_4$ , ... Which gives my answer to your question: overfitting is caused by using observations to inform the model (whether through fitting the model to the training set, evaluating the fit on the test set, or "being inspired" by the validation set as in the previous paragraphs), and then expecting the model's performance on these observations to predict its performance on new data. Cross-validation won't help By now, you know that CV won't help us here, because by definition, the data point(s) left out are used in model selection. In fact, it's pretty easy to overfit to LOOCV. Here are some random data with no relationship whatsoever: I'll fit (orthogonal) polynomials, starting with a zero degree horizontal line, up through a 6th degree one: Let's apply LOOCV to these models. Below, I'll plot the LOOCV sums of squared (out-of-bag) errors. As we see, the 5th degree polynomial "performs best", so here is the "best model": As I said, there is actually no structure at all in these data, so anything beyond the horizontal line is pure overfitting. So we see that it's easy to overfit to LOOCV (it only took me running three different random seeds to get this example), and of course it would be easy to construct a somewhat more complicated example of "overfitting to the validation set" in the sense above. The bottom line If you want an estimate of your model's performance on new data, then apply it to previously unseen data... and resist the temptation to tweak the model based on the results . (Or, do tweak it, but then evaluate the tweaked model on new data. And don't be surprised if the un-tweaked model performs better on this new data than the tweaked one.) R code nn
