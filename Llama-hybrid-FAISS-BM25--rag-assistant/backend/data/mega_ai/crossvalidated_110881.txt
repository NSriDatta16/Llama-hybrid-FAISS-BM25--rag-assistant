[site]: crossvalidated
[post_id]: 110881
[parent_id]: 107574
[tags]: 
Great question. To my mind, the issue is whether you're trying to do inference (are you interested in what your coefficients are telling you?) or prediction. If the latter, then you could borrow models from Machine Learning (BART, randomForest, boosted trees, etc.) that will almost certainly do a better job at prediction than logit. If you're doing inference, and you have so many datapoints, then try including sensible interaction terms, polynomial terms, etc. Alternatively, you could do inference from BART, as in this paper: http://artsandsciences.sc.edu/people/kernh/publications/Green%20and%20Kern%20BART.pdf I have been doing some work recently on rare events, and had no idea beforehand how much rare cases can affect the analysis. Down-sampling the 0-cases is a must. One strategy to find the ideal down-sample proportion would be Take all your 1s, let's say you have n1 of them. Set some value z = multiple of the n1 you will draw; perhaps start at 5 and reduce to 1. draw z*n1 0 observations Estimate your model on a sample of your subset data, making sure that you cross-validate on the whole dataset Save the relevant fit measures you're interested in: coefficients of interest, AUC of a ROC curve, relevant values in a confusion matrix, etc. Repeat steps 2:5 for successively smaller zs. You will probably find that as you down-sample, the false-negative to false positive ratio (in your test-set) will decrease. That is, you'll start predicting more 1s, hopefully that are genuinely 1s, but also many that are actually 0s. If there is a saddle point in this misclassification, then that would be a good down-sample ratio. Hope this helps. JS
