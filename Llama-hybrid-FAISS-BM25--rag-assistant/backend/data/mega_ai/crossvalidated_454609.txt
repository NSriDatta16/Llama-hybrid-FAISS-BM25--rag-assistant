[site]: crossvalidated
[post_id]: 454609
[parent_id]: 454571
[tags]: 
Your idea about using a hold-out set for comparing the RMSE is fine. I would note though that if we do not have rather a large hold-out sample, using a repeated cross-validation approach instead of a fixed hold-out set will mitigate finite-sample variance issues; repeated CV is preferable because it allows to also estimate the variability of our test statistics. Other test statistics like the mean absolute error ( MAE ) or the mean absolute percentage error ( MAPE - for positive only data) might also be relevant depending on the application. Regarding visualising the posterior: We can obtain an empirical distribution for each fit through a quantile random forest (see Fig. 3 & 4 from the linked paper). Implementations in R and Python are already available.
