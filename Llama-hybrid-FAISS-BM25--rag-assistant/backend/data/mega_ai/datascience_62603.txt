[site]: datascience
[post_id]: 62603
[parent_id]: 62529
[tags]: 
I have used one inbuild function to highlight the variance of each principal component. Ultimately, the target is to cover more variance with minimal Principle component. X is one dataset with 4 dimensions, initially, PCA will be applied on X with no dimension reduction, which means n_components = 4. pca = PCA(n_components=4) X_pca = pca.fit_transform(X) explained_variance_ratio_ is function which helps. pca.explained_variance_ratio_ after running it. The variance ratio for each principal component came as below. array([0.72962445, 0.22850762, 0.03668922, 0.00517871]) Now, it can be concluded that the first two PCs are covering 95% of the variance. so, 2 dimensions would be optimum choice to go ahead with. pca_2 = PCA(n_components=2) X_pca_2 = pca_2.fit_transform(X) This further can used for learning. and If we want to draw a scatter graph for PC1 and PC2 for more clarity. df_pca_2 = pd.DataFrame(X_pca_2,columns = ['PC1','PC2']) plt.scatter(df_pca_2.PC1,df_pca_2.PC2)
