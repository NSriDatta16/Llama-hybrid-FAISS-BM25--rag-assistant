[site]: crossvalidated
[post_id]: 639460
[parent_id]: 525085
[tags]: 
I think this question has not been answered sufficiently despite being a good question. The curse of dimensionality essentially says that random 3 vectors in sufficiently high dimensional space have roughly the same distance to each other with regards to euclidean distance. This is true, also in the case of dim = 512. However, in vector search for embeddings are key differences: The vectors are not random: We are embedding text and questions. Our goal is to measure if they are related, if yes how they are related. We are not working with euclidean distance and usually normalised vectors, i.e. $dist_{euchlidean} = d(\mathbf{p}, \mathbf{q}) = \sqrt{\sum_{i=1}^{n} (p_i - q_i)^2}$ $dist_{cosine}=d(\mathbf{a}, \mathbf{b}) = 1 - \frac{\mathbf{a} \cdot \mathbf{b}}{\|\mathbf{a}\| \|\mathbf{b}\|} = \mathbf{a} \cdot \mathbf{b}$ , note that the cosine distance is a number between -1 and 1. The cosine distance is measures the cosine angle between two vectors. It is a vastly different concept than the euclidean distance which will measure the length of a vector between those two. While the length of vector will increase with an increased dimension it is not the case for the cosine. Therefore the curse of dimensionality finds no application here.
