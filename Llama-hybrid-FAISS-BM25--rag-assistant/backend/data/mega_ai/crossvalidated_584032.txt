[site]: crossvalidated
[post_id]: 584032
[parent_id]: 
[tags]: 
Can increasing dimentionality improve classification in Neural Networks?

I had a dataset where each data sample (pixel) had 7 features (reflectance at 7 wavelengths). However, running my neural network on the 7 features was not able to reach a high accuracy in classifying the pixels into two groups. Somehow, I came up with this idea of interpolating over the 7 wavelengths so that I got 70 features (reflectance values now at 70 wavelengths) for each pixel. This obviously led to an increase in the data volume, but I found the neural network performed much better with this modification, with significantly fewer misclassifications. So far, I was under the impression that dimensionality reduction helps in machine learning, but in this case the results seems to suggest otherwise. Any ideas why this might happen?
