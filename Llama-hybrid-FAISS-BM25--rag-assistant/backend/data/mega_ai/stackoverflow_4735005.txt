[site]: stackoverflow
[post_id]: 4735005
[parent_id]: 2288728
[tags]: 
No, "legacy code" in this context (C/C++) is not exclusively code that plays ugly tricks with the MSB of pointers. It also includes all the code that uses 'int' to store the difference between two pointer, or the length of a memory area, instead of using the correct type 'size_t' : 'int' being signed has 31 bits, and can not handle a value of more than 2 Gb. A way to cure a good part of your code is to go over it and correct all of those innocuous "mixing signed and unsigned" warnings. It should do a good part of the job, at least if you haven't defined function where an argument of type int is actually a memory length. Still that "legacy code" will apparently work correctly for a very long while, even if you correct nothing. That's because it will break only when you'll allocate more than 2 Gb in one block. Or when you'll compare two unrelated pointers that are more than 2 Gb away from each other. As comparing unrelated pointers is technically an undefined behaviour anyway, you won't encounter that much code that does it (but you can never be sure). For the first case, very frequently even if in total you need more than 2Gb, your program actually never makes single allocations that are larger than that. In fact in Windows, even with LARGEADDRESSAWARE you won't be able by default to allocate that much given the way the memory is organized. You'd need to shuffle the system DLL around to get a continuous block of more than 2Gb But Murphy's laws says that kind of code will break one day, it's just that it will happen very long after you've enabled LARGEADDRESSAWARE without checking, and when nobody will remember this has been done.
