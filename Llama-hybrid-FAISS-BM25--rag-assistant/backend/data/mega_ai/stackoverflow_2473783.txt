[site]: stackoverflow
[post_id]: 2473783
[parent_id]: 
[tags]: 
Is there a way to circumvent Python list.append() becoming progressively slower in a loop as the list grows?

I have a big file I'm reading from, and convert every few lines to an instance of an Object. Since I'm looping through the file, I stash the instance to a list using list.append(instance), and then continue looping. This is a file that's around ~100MB so it isn't too large, but as the list grows larger, the looping slows down progressively. (I print the time for each lap in the loop). This is not intrinsic to the loop ~ when I print every new instance as I loop through the file, the program progresses at constant speed ~ it is only when I append them to a list it gets slow. My friend suggested disabling garbage collection before the while loop and enabling it afterward & making a garbage collection call. Did anyone else observe a similar problem with list.append getting slower? Is there any other way to circumvent this? I'll try the following two things suggested below. (1) "pre-allocating" the memory ~ what's the best way to do this? (2) Try using deque Multiple posts (see comment by Alex Martelli) suggested memory fragmentation (he has a large amount of available memory like I do) ~ but no obvious fixes to performance for this. To replicate the phenomenon, please run the test code provided below in the answers and assume that the lists have useful data. gc.disable() and gc.enable() helps with the timing. I'll also do a careful analysis of where all the time is spent.
