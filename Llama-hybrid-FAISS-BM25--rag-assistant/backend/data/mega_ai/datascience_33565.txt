[site]: datascience
[post_id]: 33565
[parent_id]: 31402
[tags]: 
Desicion trees make no assumptions on relationships between features. It just constructs splits on single features that improves classification, based on an impurity measure like Gini or entropy. If features A, B are heavily correlated, no /little information can be gained from splitting on B after having split on A. So it would typically get ignored in favor of C. Of course a single decision tree is very vulnerable to overfitting, so one must either limit depth, prune heavily or preferly average many using an ensemble. Such overfitting problems get worse with many features and possibly also with co-variance but this problem occurs independently from multicolinearity. While training and predictive power is relatively robust to multicollinearity, feature importance scores will be heavily influenced. So take this into account when trying to interpret the model.
