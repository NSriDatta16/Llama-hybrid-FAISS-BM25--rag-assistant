[site]: crossvalidated
[post_id]: 580968
[parent_id]: 485910
[tags]: 
Here is my version, as @avata has said self attention blocks are simply performing re-average of values. Imagine in bert you have 144 self attention block (12 in each layer). If there is no FFN all will act the same and similar. Adding FFN make each of them behave like a separate small model that can be trained (get parameters). Then the whole process become like training a "stacked ensemble learning" where each model get different weight. This is not the best analogy; but the purpose of FFN is to parameterize self-attention modules. Each of FFN has 3072 hidden dimension in the Bert-base; this means a lot of parameters for learning Bert corresponds to FFN block. Therefore, there has been effort to optimize these modules (either by replacing them or by reducing their size)
