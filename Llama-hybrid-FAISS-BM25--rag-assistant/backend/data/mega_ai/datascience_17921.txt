[site]: datascience
[post_id]: 17921
[parent_id]: 
[tags]: 
Heuristic argument for Weight decay and regularization

I have been reading this book and I have no idea how to make an argument for this problem. http://neuralnetworksanddeeplearning.com/chap3.html Connecting regularization and the improved method of weight initialization L2 regularization sometimes automatically gives us something similar to the new approach to weight initialization. Suppose we are using the old approach to weight initialization. Sketch a heuristic argument that: (1) supposing $λ$ is not too small, the first epochs of training will be dominated almost entirely by weight decay; (2) provided $ηλ≪n$ the weights will decay by a factor of $e^{\frac{−ηλ}{m}}$ per epoch; and (3) supposing λ is not too large, the weight decay will tail off when the weights are down to a size around $\frac{1}{\sqrt{n}}$, where n is the total number of weights in the network. Argue that these conditions are all satisfied in the examples graphed in this section. Can anyone explain a little bit more and guide my direction? Thank you so much for reading it. My argument is that: (1) I am thinking about this formula $w' = w(1-\frac{ηλ}{n}) - \frac{n}{m} \sum \frac{dCx}{dw}$ is related. Since the weight is larger without using new weight initialization (the standard deviation of $w = \frac{1}{\sqrt{total number of weights in the network}}$, $\frac{ηλw}{n}$ will be significantly larger than sum of $\frac{dCx}{dw}$ （the average rate of change of $C$ in respect of weight for a minibatch). (2) By substituting $ηλ+b = n$ into $w'$?? (3) Thank you again.
