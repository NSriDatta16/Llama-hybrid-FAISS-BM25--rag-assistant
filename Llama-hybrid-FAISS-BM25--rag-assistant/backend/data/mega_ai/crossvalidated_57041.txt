[site]: crossvalidated
[post_id]: 57041
[parent_id]: 57034
[tags]: 
As regression problems go, it's actually a fairly complicated algorithm. The answer to your question depends a lot on whether you have access to a reliable general-purpose CG optimization algorithm. If you do , the problem becomes somewhat simpler. If you don't , I wouldn't recommend re-implementing logistic regression from scratch ( though others have tried, see here for a minimal R implementation without a GC routine ) for the reasons explained here . At any rate, the underlying likelihood surface can be nearly flat so you have to be careful about the small prints of the implementation and test it on many corner cases (these are situations where the $X$ are highly correlated or when the two groups are nearly perfectly separable ). A possible (quick and dirty) alternative is to rescale all your $X$'s to be in $[0,1]$ --for example by using the inverse logit function on each of them individually (after they have been standardized first to have mean 0 and unit variance)-- and estimate a fit by OLS (this approach is called the linear probability model). It will not be the same model and the coefficients won't be comparable but the results will be better than doing OLS on the raw data. The advantage here is that implementing OLS is trivial, assuming you have access to a good ruby linear algebra library (googling around I have found quix/linalg )
