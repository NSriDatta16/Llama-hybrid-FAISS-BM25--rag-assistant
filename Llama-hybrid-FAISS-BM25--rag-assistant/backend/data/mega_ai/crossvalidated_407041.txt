[site]: crossvalidated
[post_id]: 407041
[parent_id]: 406496
[tags]: 
Expanding the answer by Demetri Pananos , recall that what we estimate is the posterior distribution of the parameters $$ p(\theta|X) = \frac{ p(X|\theta) \; p(\theta) }{p(X)} $$ So we are not making the predictions at this stage. Unlike with point estimation, we end up in here with estimates of distributions for the parameters. If we had point estimates, to make prediction from the model, we would plug-in the estimated parameters and the data to our model (a function $f$ of data $X$ and parameters $\hat\theta$ ) and return the outputs as our prediction $$ \hat y = f(X; \hat\theta) $$ Since we have distributions of the parameters, we plug-in the distributions and obtain posterior predictive distribution of the predicted values. As you noticed, in many cases the posterior distributions are intracable and instead of finding the distributions, we use MCMC to obtain samples from those distributions. If we have large enough number of samples, we can treat empirical statistics from those samples as estimates from the posterior distribution, for example, to estimate expected value of $\theta$ you would take mean of the samples from the posterior distribution of $\theta$ $$ E[\theta|X] \approx n^{-1} \sum_{i=1}^n \hat\theta_i $$ where $\hat\theta_1,\hat\theta_2,\dots,\hat\theta_n$ are $n$ samples from the posterior distribution. To obtain posterior predictive distribution, you would take the samples of the parameters from the posterior distribution and plug-in them to the model function to obtain samples from the posterior predictive distribution $$ \hat y_i = f(X; \hat\theta_i) $$ Alternatively, if you want to make predictions using some other data, that was not used for training, say data from the test set $X_\text{test}$ , you plug it in as the same way $$ \hat y_{\text{test},i} = f(X_\text{test}; \hat\theta_i) $$ Now, when you have those samples, you can estimate all the statistics from the posterior predictive distribution the same way as you would from the posterior distribution. To obtain point estimates, you can take things such as mean, median, or mode of the posterior predictive distribution, you can obtain interval estimates etc. So answering your question: having the MCMC samples from the posterior distribution of the parameters enables us to calculate all the statistics of interest about the posterior distribution, to visualize it (plot histograms, or kernel density estimates from the samples), and make predictions.
