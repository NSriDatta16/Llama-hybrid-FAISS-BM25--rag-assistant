[site]: datascience
[post_id]: 53622
[parent_id]: 43897
[tags]: 
Prior to jumping to any conclusions, some questions that immediately comes to my mind: What is the correlation between your features and target? Do you have any numerical features too? How large is your feature space (how many independent variables)? How about cardinality of your categorical features (levels)? Intuitively, are your features are good indicators for predicting SalePrice? How your regressor performs, in terms of the distributions of residual? Last but not least, have you tried any other regressors? Initial Guess : It would be that a simple Linear Regression won't work because there is no linear correlation between your features and target (see Assumptions of Linear Regressions ). Practical Suggestion : I would suggest trying a quick and dirty Gradient Boosting Trees for Regression (either sklearn or XGboost or Catboost implementation) and see if you notice any immediate improvement. From your explanation I see that you have quick a few categorical features that you encoded using One-Hot-Encoding (OHE) method via pd.get_dummies() in pandas. I have personally experienced that OHE is not a good idea for most of problems esp. when your have a lot of categorical features and they present high cardinality (i.e. many levels in each categorical feature), and if you search you find such examples that people struggle using OHE. Anyways, here are two very quick implementation of Catboost Regressor in Kaggle 1 , 2 to have a quick start. Good thing about Catboost is that one does not need to encode categorical features, you can pass then as it is, you only need to give the column index of your categorical features (let me know if you have struggle make Catboost up and running!).
