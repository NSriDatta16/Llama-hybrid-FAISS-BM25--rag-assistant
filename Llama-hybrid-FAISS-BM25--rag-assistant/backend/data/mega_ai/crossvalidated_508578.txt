[site]: crossvalidated
[post_id]: 508578
[parent_id]: 
[tags]: 
Adding (meaningful) features does not improve model performance

I am struggling with confusion matrices and their outputs. I thought to follow all the steps right, but unfortunately it seems that something is not going well. I had a dataset built and labelled on my own. It shows a class imbalance so I decided to apply undersampling and oversampling, looking at F1-score and Recall as in many papers and on the web. The steps were: split data in train and test (80/20) apply resampling only on train set apply pre-processing algorithm (BoW, TF-IDF, ...) use different classifiers to get results look at performance using confusion matrices (or alternatively ROC) I tried with different features: in one dataset with less features engineering, i.e., using only features from Text, I got a maximum value of F1-score equal to 68%. With more features, that I thought to be significant for improving the model, I am getting max 64%, that is weird considering the problem (email classification for spam detection). In theory, if I extract features only from text, I get a better score rather than extracting also features from email addresses. I would like to ask you for some tips and suggestions, if you have any, as I think that this cannot be possible, as the expected results should be higher in the second case, when I consider also information from email address (number of dots, suffix, registration date,...). Also, the confusion matrix, has given me weird outputs (look at 0s below, got using Kfold with 3 splits): # With RF Total classified: 4696 Score: 0.9630948387237324 Score length 10 Confusion matrix: [[2172 176] [ 3 2345]] # With LR Total classified: 4696 Score: 1.0 Score length 3 Confusion matrix: [[2348 0] [ 0 2348]] I am thinking at a problem of overfitting or some other issues with model building. An example of results is the following (for Logistic Regression model): Logistic Regression (including also numerical features) Total News classified: 4696 Score: 1.0 Score length 10 Confusion matrix: [[2348 0] [ 0 2348]] Logistic Regression precision recall f1-score support 0.0 0.96 0.97 0.97 585 1.0 0.76 0.67 0.71 76 accuracy 0.94 661 macro avg 0.86 0.82 0.84 661 weighted avg 0.94 0.94 0.94 661 Logistic Regression (with only Text, so no extra numerical features) Total News classified: 4696 Score: 0.65 Score length 3 Confusion matrix: [[2154 194] [ 66 2282]] Logistic Regression precision recall f1-score support 0.0 0.96 0.97 0.96 585 1.0 0.73 0.68 0.71 76 accuracy 0.93 661 macro avg 0.85 0.83 0.84 661 weighted avg 0.93 0.93 0.93 661 For defining X, y, X_train, y_train, X_test and y_test , I used the following code: X = train['Text'] # in case of numerical I added more variables: train[['Text', 'Num1', 'Num2']] y = train['Label'] # Split into train/test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40) train_set = pd.concat([X_train, y_train], axis=1) test_set = pd.concat([X_test, y_test], axis=1) spam = train_set[train_set.Label == 1] not_spam = train_set[train_set.Label == 0] # Oversampling oversample = resample(spam, replace=True, n_samples=len(not_spam), random_state=42) # Returning new train set oversample_train = pd.concat([not_spam, oversample]) train_set = oversample_train.copy() test_set = pd.concat([X_test, y_test], axis=1) valid_set = pd.concat([X_valid, y_valid], axis=1) For selecting and extracting text features, I am running the following: countV = CountVectorizer() ct= ColumnTransformer( transformers=[('bow', countV, 'Text')], remainder='passthrough', ) #logistic logisticR_pipeline = Pipeline([ ('LogRCV',ct), ('LogR_clf',LogisticRegression(solver='lbfgs', max_iter=200)) ]) logisticR_pipeline.fit(train_set.drop('Label', axis=1), train_set['Label']) predicted_LogisticR = logisticR_pipeline.predict(test_set.drop('Label', axis=1)) np.mean(predicted_LogisticR == test_set['Label']) Maybe I missed some steps or switched the order. I thought it was due to a classifier applied to the wrong dataset (train instead of set, but it does not seem to be caused by that). What I think is that it is a problem of overfitting. I would appreciated if you could tell me your thoughts on this. Thank you for all your help. See below the code that is returning the weird results in the confusion matrix. I had tried to follow steps from this link: https://www.pythonheidong.com/blog/article/743892/20cb290436b9ea7533af/ but now I am getting failed to open page. def build_confusion_matrix(classifier): for train_ind, test_ind in k_fold.split(train_set): # I am using my train set train_all = train_set.iloc[train_ind] # I think the error comes from this step train_y = train_set.iloc[train_ind]['Label'] test_all = train_set.iloc[test_ind] # I think the error is also from this piece of code test_y = train_set.iloc[test_ind]['Label'] # train_set or test_set? classifier.fit(train_all,train_y) # this step needs to fit the model # Make prediction c_predictions = classifier.predict(test_text) # it returns a matrix confusion += confusion_matrix(test_y, c_predictions) score = f1_score(test_y, c_predictions) scores.append(score) # Calling function for my classifier build_confusion_matrix(logisticR_pipeline)
