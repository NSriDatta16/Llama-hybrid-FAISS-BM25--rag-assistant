[site]: crossvalidated
[post_id]: 301780
[parent_id]: 301767
[tags]: 
An example in sklearn.manifold documentation that compares the methods on MNIST digits dataset. It seems that nonlinear methods give more meaningful reduction - you can see clusters corresponding to digits. As for when to use nonlinear methods - it depends what you want to use. Using PCA or something similar first is a good idea, as it's more scalable than manifold learning methods. If you're unsatisfied with PCA's results, you can try running nonlinear methods, if they can be actually used (they are not as scalable as basic matrix decomposition methods). For dimensionality reduction there is also another alternative to PCA - some of its generalizations can incorporate prior knowledge (or, in other words, use regularization). Some methods are even designed to work with mixed data (both categorical and continuous) - see PCA on a Data Frame and Generalized Low Rank Models .
