[site]: datascience
[post_id]: 105084
[parent_id]: 105083
[tags]: 
I think "one hot" is the obvious thing to do. 500 features is usually not a problem (if you donâ€˜t have too few observations). In any case you could look into "shrinking" features by using Lasso/Ridge for instance. Probably you could also look into dimensionality reduction, e.g. by using principle components (PCA). You could also do some feature selection in the sense that you "kick out" features (skills) which to not have much predictive power or which are redundant. You could for instance check for (very) high correlation among skills or remove skills with little importance after fitting some random forest.
