|X(A)\neq a,X(Y)=+\}|}{|\{X\in D|X(A)\neq a\}|}}-{\frac {|\{X\in D|X(A)=a,X(Y)=+\}|}{|\{X\in D|X(A)=a\}|}}} That is, an approximation to the difference between the probabilities of belonging in the positive class given that the subject has a protected characteristic different from a {\textstyle a} and equal to a {\textstyle a} . Algorithms correcting bias at preprocessing remove information about dataset variables which might result in unfair decisions, while trying to alter as little as possible. This is not as simple as just removing the sensitive variable, because other attributes can be correlated to the protected one. A way to do this is to map each individual in the initial dataset to an intermediate representation in which it is impossible to identify whether it belongs to a particular protected group while maintaining as much information as possible. Then, the new representation of the data is adjusted to get the maximum accuracy in the algorithm. This way, individuals are mapped into a new multivariable representation where the probability of any member of a protected group to be mapped to a certain value in the new representation is the same as the probability of an individual which doesn't belong to the protected group. Then, this representation is used to obtain the prediction for the individual, instead of the initial data. As the intermediate representation is constructed giving the same probability to individuals inside or outside the protected group, this attribute is hidden to the classifier. An example is explained in Zemel et al. where a multinomial random variable is used as an intermediate representation. In the process, the system is encouraged to preserve all information except that which can lead to biased decisions, and to obtain a prediction as accurate as possible. On the one hand, this procedure has the advantage that the preprocessed data can be used for any machine learning task. Furthermore, the classifier does not need to be modified, as the correction is applied to the dataset before processing. On the other hand, the other methods obtain better results in accuracy and fairness. Reweighing Reweighing is an example of a preprocessing algorithm. The idea is to assign a weight to each dataset point such that the weighted discrimination is 0 with respect to the designated group. If the dataset D {\textstyle D} was unbiased the sensitive variable A {\textstyle A} and the target variable Y {\textstyle Y} would be statistically independent and the probability of the joint distribution would be the product of the probabilities as follows: P e x p ( A = a ∧ Y = + ) = P ( A = a ) × P ( Y = + ) = | { X ∈ D | X ( A ) = a } | | D | × | { X ∈ D | X ( Y ) = + } | | D | {\displaystyle P_{exp}(A=a\wedge Y=+)=P(A=a)\times P(Y=+)={\frac {|\{X\in D|X(A)=a\}|}{|D|}}\times {\frac {|\{X\in D|X(Y)=+\}|}{|D|}}} In reality, however, the dataset is not unbiased and the variables are not statistically independent so the observed probability is: P o b s ( A = a ∧ Y = + ) = | { X ∈ D | X ( A ) = a ∧ X ( Y ) = + } | | D | {\displaystyle P_{obs}(A=a\wedge Y=+)={\frac {|\{X\in D|X(A)=a\wedge X(Y)=+\}|}{|D|}}} To compensate for the bias, the software adds a weight, lower for favored objects and higher for unfavored objects. For each X ∈ D {\textstyle X\in D} we get: W ( X ) = P e x p ( A = X ( A ) ∧ Y = X ( Y ) ) P o b s ( A = X ( A ) ∧ Y = X ( Y ) ) {\displaystyle W(X)={\frac {P_{exp}(A=X(A)\wedge Y=X(Y))}{P_{obs}(A=X(A)\wedge Y=X(Y))}}} When we have for each X {\textstyle X} a weight associated W ( X ) {\textstyle W(X)} we compute the weighted discrimination with respect to group A = a {\textstyle A=a} as follows: d i s c A = a ( D ) = ∑ W ( X ) X ∈ { X ∈ D | X ( A ) ≠ a , X ( Y ) = + } ∑ W ( X ) X ∈ { X ∈ D | X ( A ) ≠ a } − ∑ W ( X ) X ∈ { X ∈ D | X ( A ) = a , X ( Y ) = + } ∑ W ( X ) X ∈ { X ∈ D | X ( A ) = a } {\displaystyle disc_{A=a}(D)={\frac {\sum W(X)X\in \{X\in D|X(A)\neq a,X(Y)=+\}}{\sum W(X)X\in \{X\in D|X(A)\neq 