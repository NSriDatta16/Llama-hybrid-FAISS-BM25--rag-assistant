[site]: crossvalidated
[post_id]: 321656
[parent_id]: 321653
[tags]: 
There aren't really strict rules for this - you could always try both majority voting and stacking and pick whichever works better based on a validation dataset. Oftentimes, people will use a logistic regression for stacking for classification problems: if you want to try stacking, I would start with that approach. That is, feed the output of the different random forest classifiers into the logistic regression. I would assume naively that using the combined feature set would provide better performance, however. I've noticed anecdotally that the performance gains from stacking with an ensemble classifier as one of the base classifiers is rather marginal: I believe it probably will be better to train a single ensemble classifier with a richer feature set, but this is all off the top of my head.
