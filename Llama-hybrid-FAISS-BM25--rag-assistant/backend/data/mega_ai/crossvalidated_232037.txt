[site]: crossvalidated
[post_id]: 232037
[parent_id]: 
[tags]: 
Bayesian inference - posterior in a simple model

Suppose you are measuring $n$ quantities with error. Let $\beta_1,\ldots, \beta_n$ represent the true values and $X_1, \ldots, X_n$ represent the measured values of those quantities. Assume that the errors are centered normal. Let $\sigma_i^2\,, i=1, \ldots, n$ represent the known standard deviation of each measurement. So that the measurements are $$ X_i | \beta_i \sim N(\beta_i, \sigma_i^2)\,.$$ I can recover the above model by writing $$ X_i = \beta_i + \varepsilon_i\,,$$ where $\varepsilon_i \sim N(0, \sigma_{i}^2)$. Now I make the following extension to the model after which I get confused. Suppose that $\beta_i \sim N(\mu, \sigma_b^2)$, with known parameters $\mu, \sigma_b^2$. I want to write down form of the posterior distribution $p(\beta_i | X)$. On the one hand, if the relationship $X_i = \beta_i + \varepsilon_i $ is still in force, then $$ \beta_i = X_i - \varepsilon_i $$ so the posterior is $p(\beta_i | \{X_i\}) \sim N(X_i, \sigma_i^2)$ In particular it does not depend on $\sigma_b, \mu$. On the other hand, if I just proceed by Bayesian theorem then $$ p(\beta_i | \{X_i\}) = \frac{p(\beta_i, \{X_i\})}{P(\{X_i\})} = \frac{p(\{X_i\}|\beta_i) p(\beta_i)}{P(\{X_i\})} \propto p(\{X_i\}|\beta_i) p(\beta_i) = f_1(\{X_i\}| \beta_i) f_2(\beta)\,, $$ with $f_1(\{X_i\}| \beta_i) = \prod_{i=1}^n f(X_i| \beta_i)$ where $ f(X_i; \beta_i)$ is density of $N(\beta_i, \sigma_i^2)$ and $f_2(\beta_i)$ is the density of $N(\mu, \sigma_b^2)$. The results of those two approaches differ, what am I confusing here? Added late : As one of the comments suggested my question is related to this question , but the refereed question asks about the specific form of posterior distribution (why the posterior is normally distributed), this is different from what I was trying to figure out.
