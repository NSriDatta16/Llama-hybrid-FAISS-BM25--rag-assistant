[site]: crossvalidated
[post_id]: 613993
[parent_id]: 557974
[tags]: 
Pearson's correlation, at least its magnitude, between two numerical variables is equivalent to taking the square root of the $R^2$ from an ordinary least squares (OLS) linear regression regression of one variable on the other. If you can develop a regression with categorical variables and calculate something like $R^2$ for that regression, you are set. Fortunately, such regressions and metrics do exist. Logistic regression (binary outcome) and multinomial logistic regression ( $3+$ categories in the outcome) are reasonable analogues of OLS linear regression regression with numerical variables. Further, both models can use categorical or numerical variables. From such regressions, pseudo $R^2$ values can be calculated. My favorite from the link would be the McFadden $R^2$ that uses the likelihood (in the technical sense) of the model compared to the likelihood of a model that always predicts the pooled ( prior ) probability. You could then take the square root of the McFadden $R^2$ to quantify the strength of the relationship between your feature and categorical outcome. Note, however, that univariate variable screening presents problems.
