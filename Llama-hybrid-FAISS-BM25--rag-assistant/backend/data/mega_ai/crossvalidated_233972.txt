[site]: crossvalidated
[post_id]: 233972
[parent_id]: 
[tags]: 
Minimizing the number of examples needed for training a neural network?

So let's say I want to train a neural network for a classification task. It is somehow logical that there should exist a way to pick training examples in a really smart way so that the training can be done with smaller number of examples (hence more efficiently). E.g. a couple of different examples can influence the weight changes in the same way as one example which would then "encompass" all these different examples (parallel to generating corner-case examples while testing software). Can this reasoning even be applied to neural nets? If so, is there any work on this? I can't seem to find any, which might very well be because training neural nets simply can't be thought of in this way). However I did read about one-shot learning, is this somehow related to what I explained and should I investigate more about it? I really hope that there is some other way to do training than just throwing as many examples as possible at the network. Thank you very much!
