[site]: datascience
[post_id]: 56487
[parent_id]: 56477
[tags]: 
It depends on what you want to achieve. The biggest difference is that in the autoencoder (encoder-decoder) the output from the hidden space ("middle LSTM") should often be used somehow, and the hidden space is often much lower (or higher) in dimensionality than the input and output. This way you are first "encoding" the information down to very low amount of data, hoping it contains only the essential information (the gist) you are looking for. I would start by learning about Autoencoders for instance at goodfellows deeplearningbook.org or even at wikipedia . Your implementation above seems correct, however, the important thing is to know why you want to use an auto encoder? Is it to denoise the data or maybe create some sort of compression algorithm? Maybe to do arithmetic in the hidden space?
