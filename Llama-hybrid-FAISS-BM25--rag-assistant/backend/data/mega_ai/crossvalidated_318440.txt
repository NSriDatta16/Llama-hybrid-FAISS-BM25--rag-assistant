[site]: crossvalidated
[post_id]: 318440
[parent_id]: 
[tags]: 
The Kaplan-Meier weighted generalized M-estimation

Consider linear regression model: \begin{equation*} y_i = x_i^T \beta + \sigma \epsilon, \end{equation*} where $y_i = \min(T_i,C_i)$ (failure time and censoring time). Can someone explain me the intuition behind the kaplan meier weighted generalized M-estimation? The solution $(\beta,\sigma)$ that minimizes: \begin{equation*} Q_n(\theta)=\displaystyle \sum_i W_{ni}\left\{\rho \left[\dfrac{\omega_i(y_i - x_i^T \beta)}{\sigma}\right] + \log \sigma\right\}, \end{equation*} where $\rho$ is some 'robust' function, $\omega_i = \left\{ 1 + (x_i - M_n)^TV_n^{-1}(x_i-M_n)/d\right\}^{-1/2}$ with $M_n,V_n$ robust measures of location and scale. Finally, the $W_{ni}$ are the jumps of the kaplan meier distribution of the failure time $T$. Approach: The $\rho$-function is to bound the influence of vertical outliers. The $w_i$ to bound the influence of leverage points. But why is the $\log \sigma $-term?
