[site]: crossvalidated
[post_id]: 277590
[parent_id]: 253244
[tags]: 
First, let's lay out what we have got and our assumptions about the shapes of different vectors. Let, $|W|$ be the number of words in the vocab $y$ and $\hat{y}$ be column vectors of shape $|W|$ x 1 $u_i$ and $v_j$ be the column vectors of shape $D$ X 1 ( $D$ = dimension of embeddings) $y$ be the one-hot encoded column vector of shape $|W|$ x 1 $\hat{y}$ be the softmax prediction column vector of shape $|W|$ x 1 $\hat{y}_i = P(i|c) = \frac{exp(u_i^Tv_c)}{\sum_{w=1}^Wexp(u_w^Tv_c)}$ Cross entropy loss: $J = -\sum_{i=1}^Wy_ilog({\hat{y_i}})$ $U = [u_1, u_2, ...,u_k, ...u_W]$ be a matrix composed of $u_k$ column vectors. Now, we can write $$J = - \sum_{i=1}^W y_i log(\frac{exp(u_i^Tv_c)}{\sum_{w=1}^Wexp(u_w^Tv_c)})$$ Simplifying, $$ J = - \sum_{i=1}^Wy_i[u_i^Tv_c - log(\sum_{w=1}^Wexp(u_w^Tv_c))] $$ Now, we know that $y$ is one-hot encoded, so all its elements are zero except the one at, say, $k^{th}$ index. Which means, there's only one non-zero term in the summation above corresponding to $y_k$ and all others terms in the summation are zeros. So the cost can also be written as: $$J = -y_k[u_k^Tv_c - log(\sum_{w=1}^Wexp(u_w^Tv_c))]$$ Note: above $y_k$ is 1. Solving for $\frac{\partial J}{\partial v_c}$ : $$ \frac{\partial J}{\partial v_c} = -[u_k - \frac{\sum_{w=1}^Wexp(u_w^Tv_c)u_w}{\sum_{x=1}^Wexp(u_x^Tv_c)}]$$ Which can be re-arranged as: $$\frac{\partial J}{\partial v_c} = \sum_{w=1}^W (\frac{exp(u_w^Tv_c)}{\sum_{x=1}^W exp(u_x^Tv_c)}u_w) - u_k$$ Using definition (6), we can rewrite the above equation as: $$\frac{\partial J}{\partial v_c} = \sum_{w=1}^W (\hat{y}_w u_w) - u_k$$ Now let's see how this can be written in Matrix notation.Note that: $u_k$ can be written as Matrix vector multiplication: $U.y$ And $\sum_{w=1}^W (\hat{y}_w u_w)$ is a linear transformation of vectors $u_w$ in $U$ scaled by $\hat{y}_w$ respectively. This again can be written as $U.\hat{y}$ So the whole thing can be succinctly written as: $$U[\hat{y} -y]$$ Finally, note that we assumed $u_i$ s to be a column vectors. If we had started with row vectors, we would get $U^T[\hat{y} -y]$ , same as what you were looking for.
