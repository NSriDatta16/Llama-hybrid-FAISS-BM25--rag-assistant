[site]: crossvalidated
[post_id]: 617293
[parent_id]: 
[tags]: 
Maximize Likelihood in Machine learning

In Bayesian theorem, $p(y|x)=\frac{p(y)p(x|y)}{p(x)}$ , we call p(y) the prior, p(x|y) the likelihood. While in machine learning, many models find the solution for parameters through the Maximum Likelihood Estimation(MLE) and then apply derivative to find the solutions. I thought the MLE here corresponds to the likelihood in Bayesian function, but it seems not true. For example, in Gaussian discriminative model, the MLE target is $p(t,X|\pi,\mu_1,\Sigma)$ , which is a joint distribution. In logistic regression, the MLE target is $p(t|X, w)$ , which looks like the "real likelihood" for me. I am confused about this. The latter MLE is different from the likelihood in Bayesian function, right? How to tell which kind of MLE to use in machine learning optimization?
