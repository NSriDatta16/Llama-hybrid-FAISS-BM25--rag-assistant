[site]: crossvalidated
[post_id]: 358506
[parent_id]: 340084
[tags]: 
I am doing repeated cross-validation/holdout, because the results vary a LOT depending on the splits. To me it seems that your model (which you don't talk about but probably should) is unstable . Instability is a problem when performing K-fold CV or repeated K-fold CV because it leads to high variance and incorrect conclusions. This answer from an influential scientific researcher sheds some light on the intuition behind the issue. I would therefore argue to first reduce instability in your model, either by choosing a different model which is inherently more stable (e.g. L2 regularized regression or regularized SVM, not a decision tree..) and then do what you wanted to do. You can definitely perform feature selection via cross validation, but you have to be very careful about the way it is done. In particular to avoid data leakage, and this may involve double (nested) CV Quoting a Kaggle competitor: If you use feature selection, and you cross-validate in order to estimate your error, you should include the feature selection step in your CV procedure. A few threads where this is discussed: Feature selection and cross-validation https://www.kaggle.com/c/the-analytics-edge-mit-15-071x/discussion/7837 https://www.youtube.com/watch?v=S06JpVoNaA0 https://johanndejong.wordpress.com/2017/08/06/feature-selection-cross-validation-and-data-leakage/ https://www.nodalpoint.com/not-perform-feature-selection/ and many more ....
