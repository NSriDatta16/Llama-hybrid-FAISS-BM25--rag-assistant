[site]: datascience
[post_id]: 37207
[parent_id]: 37206
[tags]: 
You are on the right track. We no longer select an action that we think maximizes the score. Rather we predict what the best action to take is. This can be very effective in large or continuous state spaces where taking an argmax of the score over all possible actions would be prohibitively expensive. Our policy network computes the probability of taking an action, and during training we sample actions from the network. This continues until we receive a reward signal, which we multiply by the predicted log probabilities to propagate the error back through the network over past actions leading up to that reward. This will tend to adjust the past actions' predicted probabilities down for more negative rewards, and up for more positive rewards. Of course not all actions leading up to the reward necessary lead to that good or bad reward, but over many training iterations good actions will tend to lead to more positive reward and negative actions will lead to more negative reward. For more discussions see Andre Karpathy's blog post Deep Reinforcement Learning: Pong from Pixels and RL Course by David Silver - Lecture 7: Policy Gradient Methods
