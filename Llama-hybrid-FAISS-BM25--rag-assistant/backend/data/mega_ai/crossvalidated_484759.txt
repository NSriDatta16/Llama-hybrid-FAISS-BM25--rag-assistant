[site]: crossvalidated
[post_id]: 484759
[parent_id]: 484755
[tags]: 
There is a key difference between logistic regression and neural networks. Neural Networks have multiple local minima and thus it's inherently sensible to kick off your gradient descent multiple times from different initialisations, as well as to use stochastic gradient descent. You would expect to end up in different places depending on where you start. The logistic regression cost function however can be shown to be convex, and thus even if you kick your gradient descent off from different initialisations, you should always end up in the same place, give or take numerical effects associated with (S)GD. It is true that logistic regression is a single layer neural network, but in somewhat handwaving terms, the term which goes through the logistic function is linear in all model parameters (the decision boundary is linear in all model parameters). As soon as you add another layer, the term which goes through the logistic function is a non-linear function of some of the model parameters. This is what starts to make the cost function non-convex (I state vaguely without proof), and that's why even a two-layer neural network will end up in different places if you initialise different and logistic regression is the special case
