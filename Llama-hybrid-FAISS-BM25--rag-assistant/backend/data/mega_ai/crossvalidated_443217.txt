[site]: crossvalidated
[post_id]: 443217
[parent_id]: 443213
[tags]: 
You're mistaken about a few things (and that's okay!). In this case the optimal proposal density...is [available]. I believe this is only true if $f$ , the state transition is Gaussian . It can be nonlinear, which precludes closed-form Kalman filtering, but it must be Gaussian to exploit Gaussian-Gaussian conjugacy. In this case, the proposal $$ p(x_k|x_{k-1},z_k) \propto f(x_k \mid x_{k-1})p(z_k \mid x_k) . $$ You can derive that this is Gaussian using standard Bayesian techniques related to identifying conjugate distributions. In this case, the multiplicative adjustment to the weights are not even functions of the current samples you're simulating because there will be significant cancellations in the numerator and the denominator of the importance weight adjustment. Because as far as I understand sampling from the transitional prior... You aren't sampling from the state transition prior. That algorithm would be called the bootstrap filter. There, the importance weight updates would be functions of your current samples, and so they would end up having higher variance. The upside to this algorithm is that the weight updates would only require that you can evaluate the observation density. This would be handy for when you cannot evaluate the state transition density (but you can sample from it). Different algorithm, though. Calculating the measurement likelihood $p(z_k \mid x_{k-1})$ is what really confuses me That is not the measurement density! You are conditioning on the previous time's state, not the current state! This is only evaluate-able if you derive it by solving the following integral: $$ p(z_k \mid x_{k-1}) = \int \underbrace{p(z_k \mid x_{k})}_{\text{observation density}} \underbrace{f(x_k \mid x_{k-1})}_{\text{state transition}} dx_k. $$ By the way, this example is discussed in Inference in Hidden Markov Models on page 220/221.
