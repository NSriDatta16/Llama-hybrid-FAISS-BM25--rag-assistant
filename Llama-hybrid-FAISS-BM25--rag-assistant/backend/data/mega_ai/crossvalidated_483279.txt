[site]: crossvalidated
[post_id]: 483279
[parent_id]: 483191
[tags]: 
I will use the notation used here: https://stats.stackexchange.com/a/44404/2719 Let's consider this toy dataset: EmpID Feature2 Feature4 Target Ann Female Ref-Yes 0 Ann Female Ref-Yes 0 Bob Male Ref-No 0 Cathy Female Ref-No 1 You can compute the $\Delta$ for Gini impurity for each feature: $$ \Delta(Feature2,Target) = 1 - (3/4)^2 - (1/4)^2 - 3/4\Big( 1 - (2/3)^2 - (1/3)^2\Big) - 1/4 \cdot 0 \approx 0.041 $$ $$ \Delta(Feature4,Target) = 1 - (3/4)^2 - (1/4)^2 - 1/2 \cdot 0 - 1/2 \Big( 1 - (1/2)^2 - (1/2)^2\Big) \approx 0.125 $$ According to this, $Feature4$ seems to be better than $Feature2$ . Thus a decision tree induction algorithm (including Cart and Random Forest) would choose to split the node based on $Feature4$ If you remove the duplicated Ann this will be the dataset and the $\Delta$ : EmpID Feature2 Feature4 Target Ann Female Ref-Yes 0 Bob Male Ref-No 0 Cathy Female Ref-No 1 $$ \Delta(Feature2,Target) = 1 - (2/3)^2 - (1/3)^2 - 2/3\Big( 1 - (1/2)^2 - (1/2)^2\Big) - 1/3 \cdot 0 \approx 0.11 $$ $$ \Delta(Feature4,Target) = 1 - (2/3)^2 - (1/3)^2 - 1/3 \cdot 0 - 2/3\Big( 1 - (1/2)^2 - (1/2)^2\Big) \approx 0.11 $$ The $\Delta$ are the same which implies that the prediction power of the two feature is the same. In general, if you leave such duplicates it would mess up the $\Delta$ calculations.
