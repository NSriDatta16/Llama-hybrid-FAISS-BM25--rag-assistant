[site]: crossvalidated
[post_id]: 306482
[parent_id]: 306474
[tags]: 
I would consider approaching it from an information theoretic point of view. The twist wrt information theoretic approaches is that they are momentless and rooted in metrics such as Kullback-Lieber, mutual information criterion, minimum distance length, distance correlations, and more (take your pick). There are several routines out there that would enable the compilation of a matrix of distance or similarity functions for non-numeric objects such as text strings. Then, based on that matrix use PCA or some related dimension reduction algorithm to create a set of component combinations. Once that's done, the final step would be to develop a "regression" model producing coefficients or weights that could be used to score new text objects and, based on their similarity to the original objects, make a probabilistic assignment(s). The routines are out there. The one I'm most familiar with is Andreas Brandmaier's PDC (permutation distance clustering). In addition, Brandmaier has developed an R module for implementation. In his several papers, he goes into much greater theoretical depth than I am able to in this short note. This approach is worth a look and he has many ungated papers out there about it on his website at the Max Planck Institute in Germany.
