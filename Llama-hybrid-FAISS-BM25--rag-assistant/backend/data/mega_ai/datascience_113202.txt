[site]: datascience
[post_id]: 113202
[parent_id]: 113199
[tags]: 
When ensembling, you need some method of introducing diversity into your models (otherwise all your models will make the same prediction, so ensembling them won't improve the results). Using different training data for each model is one way of introducing this diversity. A common method is to use bootstrapping or bagging, where you randomly sample (with replacement) from your training data. This is what the random forest algorithm does (although it also randomly selects the features for even more diversity). As pointed out by @desertnaut, you do your initial test/training split first , then form your ensemble training sets using only the training data. However, there are several other ways to introduce diversity into your models: Boosting - where the models are trained in sequence. Each model re-weights the training samples, increasing the weight of samples the previous model classified incorrectly and decreasing the weight of those previously classified correctly. This is how AdaBoost works. Use different classifiers - e.g. if you want 3 learners you could ensemble a logistic regression model, an SVM and a neural network. Use different architectures or hyper-parameters, so use SVMs with different kernels or different sized neural networks. If using neural networks, initialise each network differently, so that when trained, each model converges to a different solution.
