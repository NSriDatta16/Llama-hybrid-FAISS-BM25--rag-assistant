[site]: crossvalidated
[post_id]: 482283
[parent_id]: 481990
[tags]: 
The very first inputs to the Transformer are token embeddings: this is just a static lookup for each token. At that stage [MASK] is a token just like any other one. Because of that, the gradient that propagates through the entire network to the input embeddings, including the [MASK] one. The task of the output layer is to guess what was the token that was replaced by the [MASK] token. To do so, the network needs to get the information from the context, but that does not remove the [MASK] embedding from the input.
