[site]: crossvalidated
[post_id]: 233346
[parent_id]: 168513
[tags]: 
Ironically the best Optimizers for LSTMs are themselves LSTMs: https://arxiv.org/abs/1606.04474 Learning to learn by gradient descent by gradient descent. The basic idea is to use a neural network (specifically here a LSTM network) to co-learn and teach the gradients of the original network. It's called meta learning. This method, while proposed by Juergen Schmidhuber in 2000, was only recently shown to out-perform the other optimizers in RNN training. ( see the original paper for a nice graphic)
