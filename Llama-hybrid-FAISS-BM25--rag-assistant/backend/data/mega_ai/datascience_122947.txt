[site]: datascience
[post_id]: 122947
[parent_id]: 122932
[tags]: 
There is no single 'standard' to treat any data. Data science is not about blindly hammering data then sucking into some algorithms which happen to take them. You must understand your goal and your data first. Never throw yourself into the sea of techniques before fully understand the problem to be solved. Ask yourself: Why do we clean the data in the first place? What do you want to do with it after being 'cleaned'? Does these date columns contain any information which helps your objective? If yes, how does each date column related to your objective? Is it possible that some of dates unrelated to your objective? What does each missing date imply? Due to collection error, or is it intentional? Start by answering these questions, to frame the problem and then brainstorm on potential approaches. How to handle e.g. missing dates etc. are technical details which should come last. Never put the cart before the horse. ======================= [Added as per OP's comment] So if in the very unfortunate case where you have absolutely zero information (e.g. feature definition etc.), what to do? Best bet: get more info. Talk to people who may know something. Decrypt the language. Sign up for a crash course of the problem domain. Second best: if for whatever reason we cannot do the above, quit. There are numerous things in life we can do that are more meaningful than this. Worst case: if you really have to do it due to e.g. pointed by a gun or poverty, God bless you. Since you have no info, there is nothing that can guide you. In fact, we don't even know the data is of sufficient quantity and quality to tackle the problem - let's pray it is. The best thing we can do is to do a lot of EDA, observe and make assumptions on what they are, how they inter-related, and guess if/how they are relevant to the target. Then, try every combination of techniques (imputation, feature engineering, families of models) you can think of and wish for the best. This isn't science; even if we luckily find a combo that works perfect on training data, it may fail miserably in the field ( 'if we look hard enough, we can always find some false pattern' ). This is the price to pay for being non-scientific. If we know nothing, nothing can help us.
