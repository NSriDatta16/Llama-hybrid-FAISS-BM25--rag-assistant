[site]: datascience
[post_id]: 84098
[parent_id]: 69590
[tags]: 
Traditional offline evaluations use metrics and methodologies borrowed from machine learning and information retrieval to estimate the performance of recommendations. They could be bias ,but they are chip alternative. Offline evaluations follow a train-test evaluation procedure : 1.split user data into the training set and the test set. 2.train recommendation algorithms on the training set.For each user: 2.1.generate a list of recommendations 2.2.test prediction accuracy or ranking effectiveness For the part with split user data into the training set and the test set normally i remove some part of data when train recommender and then use it as a ground true. In your case i would calculate measures for both RS and compare the result ,of course using the same data. More about evaluation could be read in https://scholarworks.boisestate.edu/cgi/viewcontent.cgi?article=2703&context=td
