[site]: crossvalidated
[post_id]: 420062
[parent_id]: 
[tags]: 
When hyperparameter tuning, do we train on the whole dataset after finding optimal parameters?

Consider the hypothetical: I'm splitting a dataset into a training set, a validation set, and a test set. I've found the hyperparameters that maximize the fit of a model relative to the validation set and report the fit of that model with those hyperparameters on the test set. After that, can I train a model on the whole dataset (no splitting) with the hyperparameters I found earlier? Or are these hyperparameters only useful when we use the original training set outlined in the previous paragraph?
