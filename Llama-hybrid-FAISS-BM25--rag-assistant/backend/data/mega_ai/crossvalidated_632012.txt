[site]: crossvalidated
[post_id]: 632012
[parent_id]: 632010
[tags]: 
You've specified a class of MCMC algorithms referred to as Metropolis-Hastings . With these algorithms, choosing the proposal distribution is the key question when specializing the algorithm. Under very mild conditions for selecting the proposal distribution, the algorithm will cover posterior properly and so your results should be asymptotically correct, as long as you don't make a big mistake like choosing a proposal distribution that doesn't properly cover all possible parameter values. But as they say, asymptotically we're all dead. You can pick a proposal distribution that asymptotically will properly cover the posterior yet even after running your algorithm for 10 million steps does not have a good representation of the posterior due to not moving quickly through the posterior. So making an efficient algorithm mean making one that covers the posterior quickly. This is a bit tricky. TLDR is that the closer the proposal distribution is to the target, the more efficient we will be in standard Metropolis Hastings algorithms. But note that this is a chicken and egg problem: if we knew the shape of the posterior then we'd already have our answer! As such, there can be a bit of trial and error in our problem. For a very simple MH algorithm, we fiddle with the $\sigma$ parameter until we move relatively quickly across the posterior. More advanced algorithms will actually learn the $\sigma$ parameter during the MCMC run. Even more advanced will take advantage of the derivatives of the posterior to move more quickly without proposing values that get rejected too often. If you're just trying to learn about MH algorithms, I'd suggest playing with a few different $\sigma$ 's and get a feeling for how they work (too small: moves too slowly. Too big: rejects proposal too often). If you really want to make an efficient MCMC algorithm, I'd suggest using implementations of the more advanced algorithms. Also one note: you mention that your proposal is not good since it can propose impossible values (i.e. $p > 1$ ). Theoretically, this is not an issue: impossible values will automatically be rejected and you will still have valid inference. However, it may not be computationally efficient since you are wasting so many samples on impossible outcomes, so you might reparameterize your problem ( $\theta = \log(p/(1-p))$ ) if that reduces your rejected sample rate.
