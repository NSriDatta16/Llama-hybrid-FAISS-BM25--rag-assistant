[site]: stackoverflow
[post_id]: 5318372
[parent_id]: 5316474
[tags]: 
I've needed to do almost exactly the same as you, only with a continuous video display from the FireWire camera. In my case, I used the libdc1394 library to perform the frame capture and camera property adjustment for our FireWire cameras. I know you can also do this using some of the Carbon Quicktime functions, but I found libdc1394 to be a little easier to understand. For the video capture loop, I tried a number of different approaches, from a separate thread that polls the camera and has locks around shared resources, to using one NSOperationQueue for interaction with the camera, and finally settled on using a CVDisplayLink to poll the camera in a way that matches the refresh rate of the screen. The CVDisplayLink is configured using the following code: CGDirectDisplayID displayID = CGMainDisplayID(); CVReturn error = kCVReturnSuccess; error = CVDisplayLinkCreateWithCGDisplay(displayID, &displayLink); if (error) { NSLog(@"DisplayLink created with error:%d", error); displayLink = NULL; } CVDisplayLinkSetOutputCallback(displayLink, renderCallback, self); and it calls the following function to trigger the retrieval of a new camera frame: static CVReturn renderCallback(CVDisplayLinkRef displayLink, const CVTimeStamp *inNow, const CVTimeStamp *inOutputTime, CVOptionFlags flagsIn, CVOptionFlags *flagsOut, void *displayLinkContext) { return [(SPVideoView *)displayLinkContext renderTime:inOutputTime]; } The CVDisplayLink is started and stopped using the following: - (void)startRequestingFrames; { CVDisplayLinkStart(displayLink); } - (void)stopRequestingFrames; { CVDisplayLinkStop(displayLink); } Rather than using a lock on the FireWire camera communications, whenever I need to adjust the exposure, gain, etc. I change corresponding instance variables and set the appropriate bits within a flag variable to indicate which settings to change. On the next retrieval of a frame, the callback method from the CVDisplayLink changes the appropriate settings on the camera to match the locally stored instance variables and clears that flag. Display to the screen is handled through an NSOpenGLView (CAOpenGLLayer introduced too many visual artifacts when updating at this rate, and its update callbacks ran on the main thread). Apple has some extensions you can use to provide these frames as textures using DMA for better performance. Unfortunately, nothing that I've described here is introductory-level stuff. I have about 2,000 lines of code for these camera-handling functions in our software and this took a long time to puzzle out. If Apple could add the manual camera settings adjustments to the QTKit Capture APIs, I could remove almost all of this.
