[site]: datascience
[post_id]: 32073
[parent_id]: 
[tags]: 
Implementations of "Growing When Required" approach

Recently I discovered such thing as GWR neural networks. This is a family of neural nets that can expand dynamically: in breadth and/or in depth. As far as I understood, such networks are declared to give the best results in continuous learning tasks, because their main goal is to mitigate catastrophic interference/forgetting. Despite that fact that this approach was new to me, it appears that it was first introduced in far 2002 - more than 15 years ago. But the most interesting here is that I can't find implementations of any of those algorithms in popular frameworks! My main tool for development is Deeplearning4j and I'm quite sure that continuous learning is not implemented there in any way. I also googled for something like "tensorflow growing when required" or "keras dynamically growing networks", but it doesn't give any results. My question is: why there's a situation that this mechanism is not available in popular NN-frameworks out of the box, despite its promising results (at least based on fancy plots from science papers I've read)? Or maybe I'm missing something or googling using wrong terms and it's called somehow differently?
