[site]: crossvalidated
[post_id]: 370711
[parent_id]: 
[tags]: 
Deep learning with a lot of training data

I am building a bidirectional LSTM to do a sequential text-tagging task (particularly, automatic punctuation). Usually, the training is done in iterations, where in each iteration, the entire training data is read, the loss is calculated, and the weights are updated to decrease the loss. But, I have a lot of training data; with even 1% of my training data, an iteration takes half a day. Moreover, I keep getting new training data, so even if I run an iteration on current training data, by the time it ends I will have more. Obviously I can ignore or sub-sample the training data to make the iterations faster, but this seems like a wasteful way to train, since it does not use all available data. Is there a way to train the LSTM, that takes advantage of a large amount, possibly unbounded, of training data? Ideally, I would like to create a classifier that improves with time - when I bring it new samples, it improves its performance, without having to run on previous samples. Is this possible with LSTM? Also asked here: https://ai.stackexchange.com/q/8018/8684 with no answers yet
