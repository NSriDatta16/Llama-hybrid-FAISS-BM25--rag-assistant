d with Visual Intelligence and Multimedia Analytics Laboratory (VIMAL) of the Information Sciences Institute at the University Of Southern California developed two generations of deepfake detectors based on convolutional neural networks. The first generation used recurrent neural networks to spot spatio-temporal inconsistencies to identify visual artifacts left by the deepfake generation process. The algorithm achieved 96% accuracy on FaceForensics++, the only large-scale deepfake benchmark available at that time. The second generation used end-to-end deep networks to differentiate between artifacts and high-level semantic facial information using two-branch networks. The first branch propagates color information while the other branch suppresses facial content and amplifies low-level frequencies using Laplacian of Gaussian (LoG). Further, they included a new loss function that learns a compact representation of bona fide faces, while dispersing the representations (i.e. features) of deepfakes. VIMAL's approach showed state-of-the-art performance on FaceForensics++ and Celeb-DF benchmarks, and on 16 March 2022 (the same day of the release), was used to identify the deepfake of Volodymyr Zelensky out-of-the-box without any retraining or knowledge of the algorithm with which the deepfake was created. Other techniques suggest that blockchain could be used to verify the source of the media. For instance, a video might have to be verified through the ledger before it is shown on social media platforms. With this technology, only videos from trusted sources would be approved, decreasing the spread of possibly harmful deepfake media. Digitally signing of all video and imagery by cameras and video cameras, including smartphone cameras, was suggested to fight deepfakes. That allows tracing every photograph or video back to its original owner that can be used to pursue dissidents. One easy way to uncover deepfake video calls consists in asking the caller to turn sideways. Deepfake detection and regulation Legal experts are actively questioning whether current and emerging regulatory frameworks adequately balance the advancements in deepfake detection with the protection of individual rights. Relevant legislation being scrutinized includes the EU AI Act, the General Data Protection Regulation (GDPR), the Digital Services Act in the European Union, as well as the fragmented state and federal laws in the United States, the Online Safety Act 2023 in the United Kingdom, and China's Administrative Provisions on Deep Synthesis in Internet-Based Information Services (commonly known as the Deep Synthesis Provisions). Scholars are evaluating if these frameworks effectively address the complex interplay between technology, rights, and responsibilities in the context of deepfakes. Prevention Henry Ajder who works for Deeptrace, a company that detects deepfakes, says there are several ways to protect against deepfakes in the workplace. Semantic passwords or secret questions can be used when holding important conversations. Voice authentication and other biometric security features should be up to date. Educate employees about deepfakes. Media Literacy and deepfakes Due to the capability of deepfakes to fool viewers and believably mimic a person, research has indicated that the concept of truth through observation cannot be fully relied on. Additionally, literacy of the technology among populations could be called into question due to the relatively new success of convincing deepfakes. When combined with increasing ease of access to the technology, this has led to the concern amongst some experts that some societies are not prepared to interact with deepfakes organically without potential consequences from sharing misinformation and disinformation. Media literacy has been considered as a potential counter to "prime" a viewer to identify a deepfake when they encounter one organically by engendering critical thinking. While media literacy education can