[site]: datascience
[post_id]: 76304
[parent_id]: 
[tags]: 
GridSearchCV with Random Forest Classifier

I'm working with a supervised learning problem and trying to predict a binary label and using a Random Forest to do so. I'm trying to tune my hyper-parameters to give me a best model based on my data. I can do this with GridSearchCV() , but is this correct to do with a random forest? If I'm using GridSearchCV() , the training set and testing set change with each fold. From my understanding we can we set oob_true = True in RandomForestClassifier() , we are already evaluating on the out-of-bag samples (so CV is kind of already built in RF). What is the convention to hyper-parameter tune with Random Forest to get the best OOB score in sklearn? Can I just loop through a set of parameters and fit on the same training and testing set? Can I use GridSearchCV() , or does that make no sense with RF?
