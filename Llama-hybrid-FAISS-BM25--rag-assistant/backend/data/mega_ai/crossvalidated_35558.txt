[site]: crossvalidated
[post_id]: 35558
[parent_id]: 
[tags]: 
Measurement error in dependent variable

Let $Y_{t} \sim N(0,\sigma_{Y_t}^2) $ (independent var), $X_{t}\sim N(0,\sigma_{X_t}^2)$ (dependent var) be time series variables. However, suppose we don't observe $Y_t$ and instead measure it with some error. That is, we have access to $\hat{Y}_t = Y_t - \nu_t$. Assume that the measurement error is of the form $\nu_t \sim N(0,\sigma_\nu^2)$. Consider that I want to use $\{Y_t,X_t\}$ in a Linear Regression Model to be estimated with OLS for $t=1,2,...,T$. Then what I'll actually be estimating is: $(1a) \hat{Y}_t = a + b X_t + e_t$ $e_t \sim N(0,\sigma_{e_t}^2)$ Or, equivalently: $(1b) Y_t = a + b X_t + (e_t + \nu_t) = a + b X_t + \epsilon_t$ $\epsilon_t \sim N(0,\sigma_{\epsilon_t}^2)$ What I want to know is whether the following is correct: Consider the residual variance co-variance matrix of $(1b)$, $\Sigma := E[\epsilon \epsilon']$, and the analogous population variance covariance matrix $\Omega := E[ee']$ (where "population" is taken to mean model $(1a)$ without any measurement error). We have that $\textrm{diag}(\Sigma)_t = \textrm{diag}(\Omega)_t + \sigma_\nu^2$ for all $t$ because $\textrm{diag}(\Sigma)_t = E[\epsilon_t^2] = E[(e_t + \nu_t)^2] = E[e_t^2] + E[\nu_t^2] = E[e_t^2]+\sigma_\nu^2=\textrm{diag}(\Omega)_t+\sigma_\nu^2$.
