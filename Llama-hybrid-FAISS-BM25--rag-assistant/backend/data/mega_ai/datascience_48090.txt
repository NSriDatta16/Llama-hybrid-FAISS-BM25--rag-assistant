[site]: datascience
[post_id]: 48090
[parent_id]: 47954
[tags]: 
Learning rate decay is implemented in, e.g., XGBoost and LightGBM as callbacks. (XGBoost used to allow the learning_rate parameter to be a list, but that was deprecated in favor of callbacks.) Similar functionality for other hyperparameters should be possible in the same way. xgboost.callback.reset_learning_rate LightGBM callback discussion I played around with these ideas (for learning rate and tree depth) a while back, but didn't get improved performance. But you should try it out; if you do see significant gains, it'd be great to add it as an answer here.
