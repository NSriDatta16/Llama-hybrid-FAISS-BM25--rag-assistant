[site]: crossvalidated
[post_id]: 281497
[parent_id]: 
[tags]: 
Feature importance for xgboost

I'm using XGBoost (extreme gradient boosted trees) for binary classification. I'd like to calculate feature importance scores, to help me understand the relative importance of different features. The XGBoost library supports three methods for calculating feature importances: "weight" - the number of times a feature is used to split the data across all trees. (also called f-score elsewhere in the docs) "gain" - the average gain of the feature when it is used in trees. "cover" - the average coverage of the feature when it is used in trees. That's the entire description given in the documentation. After doing some research, I think "gain" is the information gain, and I don't know what "cover" is. How should I choose among these different methods of calculating feature importance? In my setting, the features are one-hot encodings of categorical data, so all features are zero-or-one values. Is one superior to others, in this setting? I'd like to make sure that whatever method I use is statistically well-grounded and is not 'voodoo'.
