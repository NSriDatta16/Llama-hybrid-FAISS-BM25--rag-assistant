[site]: crossvalidated
[post_id]: 593374
[parent_id]: 
[tags]: 
Average XGBoost models in federated learning

The following talk: video: https://www.youtube.com/watch?v=VUINeZUAlx8 slides: https://mike.place/talks/fl/ (including transcript) explains federated learning by averaging several node-specific models together. It provides a high-level example with neural networks. Using NN, it seems straightforward that any weight/bias in the averaged model will simply take the average of the values of the individual node models. For XGBoost (or more generally for trees), how would such an "averaging" work? Would that even be possible? Would that take the form of a "random forest" of XGBoost models?
