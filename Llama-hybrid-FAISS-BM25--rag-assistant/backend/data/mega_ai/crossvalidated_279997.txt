[site]: crossvalidated
[post_id]: 279997
[parent_id]: 217726
[tags]: 
The problem is simple: your base classifiers (gbm and xgb) are biased with the predictions. If they are trained and tested on the same data, they usually perform better then on test data (although depends on your sample size and more...). What is the stacker left to learn? Biased predictions. So you may use cross-validation for the base classifiers, train them and test them on unseen data (from the training set) and use those unbiased predictions to train the stacker on. This way you do not over-fit. Two details: If you want to have a genius implementation of k-folding, you may consider using the REP package (if you use python). Stacking two boosted decision trees won't give you a real improvement. Better use XGBoost alone, its a far superior classifier. Or stack it with SVM, NN or similar.
