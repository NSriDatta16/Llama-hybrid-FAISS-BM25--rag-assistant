[site]: crossvalidated
[post_id]: 201911
[parent_id]: 201750
[tags]: 
I've found that there isn't an incredible benefit in using downsampling/upsampling when classes are moderately imbalanced (i.e., no worse than 100:1) in conjunction with a threshold invariant metric (like AUC). Sampling makes the biggest impact for metrics like F1-score and Accuracy, because the sampling artificially moves the threshold to be closer to what might be considered as the "optimal" location on an ROC curve. You can see an example of this in the caret documentation . I would disagree with @Chris in that having a good AUC is better than precision, as it totally relates to the context of the problem. Additionally, having a good AUC doesn't necessarily translate to a good Precision-Recall curve when the classes are imbalanced. If a model shows good AUC, but still has poor early retrieval, the Precision-Recall curve will leave a lot to be desired. You can see a great example of this happening in this answer to a similar question. For this reason, Saito et al. recommend using area under the Precision-Recall curve rather than AUC when you have imbalanced classes.
