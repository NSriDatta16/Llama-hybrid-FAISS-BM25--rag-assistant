[site]: crossvalidated
[post_id]: 381273
[parent_id]: 
[tags]: 
Total variation regularization in deep learning

For current deep learning models, we can find basically two kinds of regularization on: Activation Weights The common $L_1$ and $L_2$ on weights can lead to a MAP problem where the regularization terms are from priors; On activation, it can lead to a sparse representation. But my questions are, why it is very rare to see Total Variation (TV) regularization in deep learning (or maybe there are some?)? It is very useful in image denoising and reconstruction. what are the meaning if we add TV to activation or weight?
