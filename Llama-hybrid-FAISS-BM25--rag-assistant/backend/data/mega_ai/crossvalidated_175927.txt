[site]: crossvalidated
[post_id]: 175927
[parent_id]: 
[tags]: 
Original (?) model selection with k-fold CV

When using k-fold CV to select among regression models, I usually compute the CV error separately for each model, together with its standard error SE, and I select the simplest model within 1 SE of the model with the lowest CV error ( the 1 standard error rule, see for example here ). However, I've recently been told that in this way I'm overestimating the variability, and that in the specific case of selecting between two models A and B, I should really proceed in a different way: for each fold $K$ of length $N_K$, compute the pointwise differences between the two models predictions.Then compute the mean square difference for the fold $$MSD_K=\sqrt{\frac{\sum_{i=1}^{N_K}\left(\hat{y}_{Ai}-\hat{y}_{Bi}\right)^2}{N_K}}$$ average $MSD_K$ across folds as usual, and use this CV difference error (together with its standard error) as an estimator for the generalization error. Questions: Does this make sense to you? I know there are theoretical reasons behind the use of CV error as an estimator of generalization error (I don't know which are these reasons, but I know they exist!). I have no idea if there are theoretical reasons behind the use of this "difference" CV error. I don't know if this can be generalized to the comparisons of more than two models. Computing the differences for all pairs of models seems risky (multiple comparisons?): what would you do if you had more than two models? EDIT: my formula is totally wrong, the correct metric is described here and it's much more complicated. Well, I'm happy I asked here before blindingly applying the formula! I thank @Bay for helping me understand with his\her illuminating answer. The correct measure described is quite experimental, so I will stick to my trusted work-horse, the CV error!
