[site]: crossvalidated
[post_id]: 360690
[parent_id]: 
[tags]: 
Skip connections: Benefits of skipping extra layers

In the week 2 assignment of Dr Andrew Ng's Convolutional Neural Networks course , it says: The identity block is the standard block used in ResNets, and corresponds to the case where the input activation (say $a^{[l]}$) has the same dimension as the output activation (say $a^{[l+2]}$). In this exercise, you'll actually implement a slightly more powerful version of this identity block, in which the skip connection "skips over" 3 hidden layers rather than 2 layers. It looks like this: Why is skipping three layers "a slightly more powerful version" of the ResNet identity block? Is there any research on this? Is there an optimal number of layers to skip for various functions?
