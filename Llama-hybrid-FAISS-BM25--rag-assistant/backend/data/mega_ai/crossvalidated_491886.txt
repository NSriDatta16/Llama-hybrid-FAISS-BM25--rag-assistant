[site]: crossvalidated
[post_id]: 491886
[parent_id]: 491576
[tags]: 
The usual approach is to put the padding at the end of the sequence. It is more convenient for RNNs because it trivially ensures that the initial RNN state will the same everywhere. You should keep separately a binary mask of what the valid positions are (and then it does not really matter what the pading values are). When you do the standardtization, you do not want to include the padding values anyway and you need to mask out the padding for computing the mean and variance. If you plan to process the sequence with a Transformer, you will need the mask to tell the self-attention to what positions it can attend. Finally, you will need the mask to compute the loss function, to be able base the loss function on the valid positions only.
