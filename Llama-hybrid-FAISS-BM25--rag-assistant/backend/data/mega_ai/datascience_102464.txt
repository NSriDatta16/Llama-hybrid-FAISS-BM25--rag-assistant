[site]: datascience
[post_id]: 102464
[parent_id]: 
[tags]: 
Is there any mathematical basis for distributed training of a neural network on multiple machines?

Let us say that I have 50 people download a program that engages in supervised training of a neural network I designed. The data samples being provided are pretty random and the correct outputs are calculated and fit. Is there any numerical basis by which the results after running the trainer, could be combined into one model ? It seems it would be pretty complex if it could be accomplished. In response to Nikos' request. So, you define your model in pytorch, layers and such, inputs outputs etc. You create a training program that generates the inputs, and then also calculates the expected outputs. You run your fit loop on these. Now to speed up the process, maybe you think, 'oh I could run multiple instances of the training program on multiple computers, maybe this wasn't intended to drive me crazy last time but i'm sure they tried, anyway wouldn't it be nice if I could do this to speed up the training process since I don't have a cuda enabled card, but I got a bunch of buddies who have spare cpu cycles, could we somehow combined the alterations to the model into a single adjusted trained model after so many epochs ?' Is this possible ? One would think it would be, as with gradient descent we're talking one realllly big equation being modified and tweaked and the general direction of those tweaks would probably end up being the same within the expected range of input and output in this example.
