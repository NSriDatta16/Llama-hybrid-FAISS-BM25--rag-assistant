[site]: crossvalidated
[post_id]: 615730
[parent_id]: 
[tags]: 
Best way to fit a Gaussian Process surrogate model to an RL Reward function

Is there a way to get an estimate of good scaling parameters (namely mean and variance) for a Gaussian Process kernel serving as a surrogate model to a Reinforcement Learning reward function for optimization? Considering the reward function to be a scalar objective of 3 input variables, each having box constraints i.e. l 1 1 1 , l 2 2 2 , l 3 3 3 and r= f(x 1 ,x 2 ,x 3 ). Using the following from sklearn: kernel = ConstantKernel(1.0) * RBF(1.0) gauss_pr = GaussianProcessRegressor(kernel) Also, is it advisable to use a Gaussian Process as a surrogate or would another method be more beneficial? Any help would be appreciated. Thanks.
