[site]: datascience
[post_id]: 30989
[parent_id]: 
[tags]: 
What are the cases where it is fine to initialize all weights to zero

I've taken a few online courses in machine learning, and in general, the advice has been to choose random weights for a neural network to ensure that your neurons don't all learn the same thing, breaking symmetry. However, there were other cases where I saw people initializing using zero weights. Unfortunately, I can't remember what those were. I think it might have been non-neural-network cases, like a simple linear or logistic regression model (simple weights only on the inputs, leading directly to an output). Are those cases safe for zero initialization? Alternatively, could we use random initialization in those cases too, just to stay consistent?
