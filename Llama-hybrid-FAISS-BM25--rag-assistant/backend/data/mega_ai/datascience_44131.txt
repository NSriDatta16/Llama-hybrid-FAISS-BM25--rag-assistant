[site]: datascience
[post_id]: 44131
[parent_id]: 38632
[tags]: 
In particular, DQN is just Q-learning, which uses neural networks as a policy and use "hacks" like experience replay, target networks and reward clipping. In original pape r authors use convolutional network, which takes your image pixels and then fit it into a set of convolutional layers. However there are a couple of statistical problems: DQN approximate a set of values that are very interrelated (DDQN solves it) DQN tend to be overoptimistic. It will over-appreciate being in this state although this only happened due to the statistical error (Double DQN solves it) $$Q(s,a) = V(s) + A(s,a)$$ By decoupling the estimation, intuitively our DDQN can learn which states are (or are not) valuable without having to learn the effect of each action at each state (since it’s also calculating V(s) ). We’re able to calculate V(s). This is particularly useful for states where their actions do not affect the environment in a relevant way. In this case, it’s unnecessary to calculate the value of each action. For instance, moving right or left only matters if there is a risk of collision As @emilyfy said self.target_model.set_weights(self.model.get_weights()) - the updating of the target model.
