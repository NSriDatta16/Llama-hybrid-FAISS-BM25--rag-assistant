[site]: crossvalidated
[post_id]: 522444
[parent_id]: 
[tags]: 
How to avoid underestimation in neural network model when loss targets contain min something positive and model output itself?

Apologies if this has been asked already, but I couldn't find anything about it. Straight to the problem : I want to train a neural network for regression and the target in the loss function is a min of (positive number, the neural network itself). To clarify, the loss looks like this: $$ loss = (\min[pos\_nums, model] - model)^2$$ Where $pos\_nums$ are my positive targets and model is the neural network. That's the loss I should use but doesn't this make the network converge to low values? This problem is similar to what happens in Q-learning: in that case, the objective is the max of something + the network prediction itself, so there you have the maximisation bias problem, which causes the output of the network to become bigger than the actual true value aka overestimation. My case is the opposite, the network would be pushed to converge to zero. How can I avoid that? Using two different networks for target and prediction wouldn't necessarily solve the problem, right? Are you aware of other techniques to avoid minimisation bias? Thanks!
