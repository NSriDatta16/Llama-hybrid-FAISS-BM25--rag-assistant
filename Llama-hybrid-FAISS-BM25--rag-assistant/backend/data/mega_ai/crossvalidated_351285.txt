[site]: crossvalidated
[post_id]: 351285
[parent_id]: 
[tags]: 
Gradient of SVM loss (hinge loss)

I am trying to follow https://mlxai.github.io/2017/01/06/vectorized-implementation-of-svm-loss-and-gradient-update.html He starts with the SVM loss (hinge loss) function defined per input instance $x_i$ as Next he defines the gradient of $L_i$ with respect to w. (1) My first question is, I would have expected the matrix of derivatives to be the transpose of what is showing, so the the first row would be with respect to w11, w12, w13, etc. I guess this is not important but it seems contrary to what I expect. Next he examines the derivative of $L_i$ with respect to the first weight w11. He writes out explicitly the expression for $L_i$ As I understand this, since $w_{11}$ only occurs in one term of this $L_i$ expression, if the argument of the $max()$ is greater than zero in that term, than the derivative will just be $x_{i1}$ . (2) Is this correct? This is what he says I think: Then he shows the similar results for $w_{12}$ , $w_{13}$ and so on and then concludes with an expression which I think involves a derivative of $L_i$ with respect to a vector. I believe that the derivative of $L_i$ with respect to $w_j$ is saying that $w_j$ is the vector $\{w_{j1}, w_{j2}, \dots\}$ . (3)Is this correct? Therefore is it correct to say that the derivative with respect to this vector is a kind of shorthand for doing each of the derivatives. Then, each of these derivatives is stored in a vector? (4) Just to make sure, is the indicator function here a scalar valued function (5) Doesn't this mean that the derivative is either the vector $x_i$ itself or 0? Finally he addresses a special case where $j=y_i$ .This is the case where we are finding the derivative of $L_i$ with respect to $w_{y_i}$ , which is the row of weights for the correct label for instance $x_i$ . As further explanation, Each row is giving a score for a particular class of the class variable. For a given instance, one of those classes is the correct classification of that instance. He is now focusing on that particular row of the W matrix and seeing if he can find out more specific information in this case. (6) I don't understand this case. He is calculating the derivative for a single weight $w_{i1}$ (not great notation) He says it is the number of classes that meet the desired margin is the coefficient of xsubi1. Up until now though, it seems to me that the derivative is either the appropriate component of the $x_i$ vector or 0 and does not involve a sum. However in this case he says it involves a sum. Why is there a sum here?
