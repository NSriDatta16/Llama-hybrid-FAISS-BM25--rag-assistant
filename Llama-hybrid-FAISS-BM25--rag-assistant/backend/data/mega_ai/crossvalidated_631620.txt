[site]: crossvalidated
[post_id]: 631620
[parent_id]: 
[tags]: 
How do I proof or disprove that the size of a document influences classification accuracy?

I have a multinomial Naive Bayes model that classifies a document into 1 of 178 classes. This goes well about 50% of the time, so it's pretty good (for my use case) considering the large set of classes. Now, I want to know if by knowing the size of the input document (in bytes) I can say anything about the confidence of the prediction. Intuitively I would say, smaller documents hold less information about its class and thus will generally result in worse predictions. Assume a pandas dataframe with the following data: "actual", "predicted", "size". Is it possible to statistically prove if the size document has any influence over a correct or incorrect prediction? Extra information: There are about 8000 records There is no clear grouping of document sizes, (in the 8000 records, there are about 5000 different document sizes) I have tried using logistic regression with MLE and OLS, but I am unsure if this is scientifically correct. Logistic recression MLE implementation: I separated my data into correct and incorrect predictions, but this doesn't seem fair, since a correct prediction is inherently much less likely then an incorrect prediction. Furthermore, I read somewhere this assumes a normal distribution of the size variable, but it is not normally distributed. # Add an intercept term data_no_outliers['intercept'] = 1 # Fit logistic regression model logit_model_no_outliers = sm.Logit(data_no_outliers['correct'], data_no_outliers[['intercept', size_column]]) result = logit_model_no_outliers.fit() ---------------------------------------- results: Optimization terminated successfully. Current function value: 0.691922 Iterations 3 Logit Regression Results ============================================================================== Dep. Variable: correct No. Observations: 8532 Model: Logit Df Residuals: 8530 Method: MLE Df Model: 1 Date: Fri, 17 Nov 2023 Pseudo R-squ.: 2.621e-06 Time: 13:46:38 Log-Likelihood: -5903.5 converged: True LL-Null: -5903.5 Covariance Type: nonrobust LLR p-value: 0.8604 ============================================================================== coef std err z P>|z| [0.025 0.975] ------------------------------------------------------------------------------ intercept 0.1030 0.031 3.290 0.001 0.042 0.164 size -0.0008 0.005 -0.176 0.860 -0.010 0.009 ============================================================================== OLS implementation: I am pretty sure this is incorrect, since it doesn't even use the actual data. label_encoder = LabelEncoder() results['prediction_encoded'] = label_encoder.fit_transform(results['prediction']) # Assuming df is your DataFrame X = sm.add_constant(results['size']) # Add a constant term y = results['prediction_encoded'] # Fit a non-parametric regression model smModel = sm.OLS(y, X).fit() -------------------------------- results: OLS Regression Results ============================================================================== Dep. Variable: prediction_encoded R-squared: 0.000 Model: OLS Adj. R-squared: -0.000 Method: Least Squares F-statistic: 0.6955 Date: Fri, 17 Nov 2023 Prob (F-statistic): 0.404 Time: 15:09:45 Log-Likelihood: -42579. No. Observations: 8591 AIC: 8.516e+04 Df Residuals: 8589 BIC: 8.518e+04 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 51.8788 0.415 124.995 0.000 51.065 52.692 size 0.0298 0.036 0.834 0.404 -0.040 0.100 ============================================================================== Omnibus: 1001.822 Durbin-Watson: 1.038 Prob(Omnibus): 0.000 Jarque-Bera (JB): 444.613 Skew: 0.375 Prob(JB): 2.84e-97 Kurtosis: 2.177 Cond. No. 13.0 ============================================================================== ```
