[site]: datascience
[post_id]: 112273
[parent_id]: 112262
[tags]: 
nn.Linear should already learn an additive bias by default. This example implements RNN with no activation likely just for educational purposes, since that makes no sizeable difference from fully connected network (see the proof at https://stackoverflow.com/questions/66726974/pytorch-rnn-with-no-nonlinearity ) and is questionable in other aspects as well ( https://github.com/pytorch/tutorials/issues/193 ). tanh is usually preferred as an activation function for RNN connection since it's less prone to vanishing gradients problem than ReLU (which is a serious concern when running RNNs), allows for both increase and decrease of the hidden states and generally shows better convergence behaviour. The classification output can use any suitable activation.
