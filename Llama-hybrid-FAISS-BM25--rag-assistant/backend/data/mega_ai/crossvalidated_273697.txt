[site]: crossvalidated
[post_id]: 273697
[parent_id]: 273689
[tags]: 
With mixed model with random effect you can serve problem where you have quite simple model (for example, that school's pupils performance depends on sex, age, performance on previous test), but these relations somehow differ in some higher level categorical entities (let say in our example: differ between schools, that is in some schools this relation is somehow stronger, and in other it is weaker or mean level of variable differs between schools). And it is traditional statistical model, which in some cases can provide statistical significance, hypothesis testing etc. However, this is rather not a method that would help you with high number of continuous variables, because such a model with hundreds of variables will rather not converge and will not help selecting subset of variables. It just helps with one or few categorical variables, that have many values (let say variable identifying school in our example), because it allows to take this variable into account without using binary indicator variables in number of number of schools - 1. This would be done by defining one or more random effect on the school level. But it would not help with let's say data set of 5000 variables. In contrast, penalized regression depending on chosen regularization parameter value will help you to select small subset of most important variables to be included in the model from even thousands of variables with no problem and they can be any type (categorical or continuous). However, this is not statistical, but rather machine learning approach, therefore it does not provide statistical tests, and result is somehow arbitrary, as it depends on regularization parameter and penalty form (L-1, L-2 or other), that can be only cross-validated, but can not be derived from data. If your problem is prediction/machine learning problem I would recommend penalized regression, especially, it is very simple to apply, if you know linear regression you would not have problem with understanding it and it would work in nearly any case (wide data, continuous and categorical variables etc.; however categorical variables still should be recoded to binary or converted to factors in R, as penalized regression is the same way linear as mixed models). On the contrary, mixed effect models are complicated, requires many assumptions that are difficult to assess and are computationally demanding for big data sets. But if your problem is more scientific that require statistical significance and hypothesis testing, and you have this kind of problem were data is nested in higher level entities you can use mixed models. In R there are many packages with penalized regression, the best is probably LiblineaR, however it is rather advanced one. Maybe it would be helpful when googling information: penalized regression other name is regularized regression, and penalized regression with L-1 penalty is referred as lasso, while penalized regression with L-2 penalty is referred as ridge regression.
