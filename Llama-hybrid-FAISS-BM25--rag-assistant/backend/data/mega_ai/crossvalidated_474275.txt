[site]: crossvalidated
[post_id]: 474275
[parent_id]: 474192
[tags]: 
As mentioned previously, you need to train on large networks in order to prune them. There are some theories as to why, but the one I'm most familiar with is the "golden ticket" theory. Presented in "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks" by Jonathan Frankle, Michael Carbin the golden ticket theory of neural networks asserts that there is a subset of the network which is already very close and what training does is to find and slightly improve this subset of the network, while downplaying the wrong parts of the network. A real-life analogy of this is that only a few of your lottery tickets will be worth anything but you need to buy a lot in order to find them. There is a connection to the original rationale behind dropout: Train many networks 'in parallel' and some of the time you will be training the only the golden ticket network.
