[site]: datascience
[post_id]: 37434
[parent_id]: 
[tags]: 
Policy-based RL method - how do continuous actions look like?

I've read several times that Policy-based RL methods can work with continuous action space (move left 5 meters, move right 5.5312 meters), rather than with discrete actions, like Value-based methods (Q-learning) If Policy-based methods produce probability of taking an action from a current state $S_t$ , how can such an action be continuous? We might get 4 probabities for our actions, but we still have to choose a single action from from four: $A_1: 10\%$ $A_2: 35\%$ $A_3: 5\%$ $A_4: 50\%$ Thus, it's not obvious how can my action be something continuous like: "turn +19.2345 angles clockwise". Such an action must have already been pre-defined to the "19.2345" value, right?
