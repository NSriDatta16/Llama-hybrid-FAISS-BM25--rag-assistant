[site]: crossvalidated
[post_id]: 197754
[parent_id]: 197721
[tags]: 
A SOM has only an input layer and an output layer, so I guess it could be considered a neural network with no hidden layers. The input layer is completely connected to the output layer, with weights for each edge. These weights change as a part of the algorithm, but not in response to the data, which I believe makes it different from more usual neural networks. Complicating matters, the SOM was invented as a neural network, but it's implemented in two different ways: the batch algorithm is more like a neural network, and the online algorithm is more of a competitive-learning exercise. Intuitively, I think of a SOM as something like K-means where the $k$ centroids are constrained in a 2D manifold. So I don't think of it as a neural network at all. (And I think most people who look at the algorithm wouldn't think of it as a neural network, either. It was originally conceived that way, back when neural networks were first hot, and it can be drawn that way, but in practice I'm not sure it matters.) SOMs fall into a class of algorithms known as Learning Vector Quantization (LVQ), which you might also want to investigate. [Personally, I really like SOM. It's a hybrid that does several things at once: clustering, (non-linear) dimensionality reduction, manifold learning, etc, all in one. It's not included in things like Murphy's Machine Learning book because it's not probabilistic (and it's no longer popular enough to force its inclusion in spite of this), but it's got a physical feel to it that I really like.]
