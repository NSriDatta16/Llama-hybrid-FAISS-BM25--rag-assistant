[site]: datascience
[post_id]: 56675
[parent_id]: 56672
[tags]: 
Yes, unimportant features can hurt the model's performance. This happens in my experience in a few ways: Efficiency - they make the fitting process slower. Particularly if you're one-hot encoding categorical features, and end up with a large and useless sparse matrix. If these reductions in efficiency force you into taking steps that might impact on actually informative features, like a PCA transformation, you're then potentially also directly impacting on predictive ability. A minimum number of samples is needed per feature for models to effectively learn. By including redundant features, this sample:feature ratio is diminished, which makes it more difficult for many models to determine whether or not a feature is a useful predictor. You make it more likely to overfit to the "noisy" features. You can recursively reject features that the model's feature_importances_ routine has decided are unimportant using sklearns recursive feature elimination. Or, in the exploratory phase of building your model, you can assess predictive power using visualisation or hypothesis testing. This nicely highlights the improvement in performance by ditching redundant features: https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py
