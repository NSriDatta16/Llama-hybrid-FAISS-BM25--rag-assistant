[site]: datascience
[post_id]: 29531
[parent_id]: 29527
[tags]: 
A working example of a Variational Autoencoder for Text Generation in Keras can be found here . Cross-entropy loss, aka log loss, measures the performance of a model whose output is a probability value between 0 and 1 for classification. Cross-entropy loss goes up as the predicted probability diverges from the actual label. In the case of character-by-character autoencoder, each character in the vocabulary would be a label. Cross-entropy works if the input and output are the same size, that is the case in character-by-character autoencoder. Often times in text analysis, the input and output sequences are different lengths so a second term, encoder loss , is added to the objective function.
