[site]: crossvalidated
[post_id]: 342652
[parent_id]: 143492
[tags]: 
It depends in the application, in general NN work because it has been shown that given the activation functions, as the number of hidden layers $N \rightarrow \infty$ a single layer feedforward network can arbitrarily approximate all functions $f$ (this is known as the Universal Approximation Theorem https://en.wikipedia.org/wiki/Universal_approximation_theorem ), this has been proven for both feedforward MLPs [Hornik1991] and RBFNs [i've forgotten the reference]. For classification FF MLPs essentially work in the same manner as an SVM, and generate a convex hull by the functions approximated by the network. The training method of the network such as gradient descent do not affect the underlying mathematics of how a network works, only how the weights are found and optimise. For example you can use other non-gradient based heuristic methods to train neural networks. Edit- If you want to go deeper into why NNs work there is literature linking the success of NNs to the quantum mechanics and theories of everything (such as holographic theory), or https://www.youtube.com/watch?v=bLqJHjXihK8&feature=youtu.be
