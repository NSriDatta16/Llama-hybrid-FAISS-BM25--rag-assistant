[site]: crossvalidated
[post_id]: 566739
[parent_id]: 566733
[tags]: 
Even if overfitting is not a concern, dropout could still help. Neural networks are (most of the time) trained with backpropagation, i.e. stochastic gradient descent (SGD). And it is known that the stochastic aspect of SGD often helps it to escape local minima. While the main source of stochasticity is the random blocks in each epoch (with the blocks much smaller than the total dataset), techniques like dropout (might) add a different type of stochasticity that could perhaps improve your results. So, while you can never be certain of anything with DNNs, I would definitely give dropout a try.
