[site]: crossvalidated
[post_id]: 441769
[parent_id]: 
[tags]: 
How does one solve the premise selection problem with imitation learning?

I was reading the following two new papers ( HOList , Graph Representations ) applying Machine Learning to Theorem Proving in Higher Order Logic. The main thing that I am unsure about is the following during imitation learning /supervised learning: How do the authors ensure that their Machine Learning method learns to output arguments that are close to the ones humans chose during training? So these are my own thoughts: I can see how their method would learn to output the right tactics, because there are only 41 tactics and they use a classifier layer and there is a single ID for the human ones. Therefore they use any standard classification error (e.g. cross-entropy loss): $$ l(\hat y, y) = \sum^{41}_{l=1} y_l \log \hat y_l $$ so we just minimize what the difference between what the model does and the human did (for a specific goal in question). However, I can't figure out if their loss function actually encourages the method to output the arguments that the human does. In the GNN paper it says: The model uses sigmoid cross-entropy to score how useful an individual premise is for the given goal. which seems to translate to something like: $$ Sigmoid( NeuralNet(t_i) ) $$ where it just outputs a sigmoid randomly for each theorem perhaps being useful but it doesn't seem to have any relation to what arguments the human used. In the first HOList paper section 4.1 they say what a training example is: Our training examples consist of a goal, a tactic, an arglist, and a negarglist. so: $$ \texttt{ example := (goal, tactic, arglist, negarglist) } $$ but it doesn't say how they compute the loss between the arglist and the scoring that their two tower method uses (otherwise how is their scoring function doing anything that isn't just random?!). The section 5.4 says how they compute tactics and arglist suggestions as follows: The neural network has two separate prediction heads $S$ and $R$ . The goal tower G computes an embedding $G(g)$ of the current goal g and infers a scoring vector $S(G(g))$ for the fixed set of tactics where the tactic classifier $S$ is a linear layer producing logits of a softmax classifier. The premise tower $P$ computes a fixed size embedding $P(t_i)$ of all possible tactic arguments $t_i$ in the scope of the goal to be proved. The ranking of the premises is performed by a combiner network $C$ that takes the concatenation of the goal embedding, the premise embedding and possibly that of the tactic $T_j$ to be applied: $r(t_i) = C(G(g), P(t_i), T_j)$ , where $r(t_i)$ is the score of theorem $t_i$ for its being a useful tactic argument for transforming the current goal $g$ towards a closed proof. I guess what I don't see is how: $$ r(t_i) = C(G(g), P(t_i), T_j) $$ is optimized to actually imitate humans (or the arglist examples it sees). Cross-posted: https://www.quora.com/unanswered/How-does-one-solve-the-precise-problem-with-imitation-learning Related: - https://www.reddit.com/r/MachineLearning/comments/dw7se0/r_holist_an_environment_for_machine_learning_of/
