[site]: crossvalidated
[post_id]: 330448
[parent_id]: 
[tags]: 
Understanding a proof of conditions for vanishing/exploding gradient in RNNs

I'm looking at some of the preliminaries in understanding vanishing/exploding gradients with recurrent neural networks (RNNs), and I see this paper referenced quite a lot: https://arxiv.org/abs/1211.5063 My question pertains to equation (5), and where the matrix transpose comes from. It appears in several places, so it does not appear to be a typo. To copy here (moving the time-step identifiers to superscripts since I do not want to misinterpret as matrix/vector indexes) they start with: $$ \mathbf{x}^{(t)} = \mathbf{W}^{(rec)}\sigma(\mathbf{x}^{(t-1)}) +\mathbf{W}^{(in)}\sigma(\mathbf{u}^{(t)}) + \mathbf{b} $$ where I'll take $\mathbf{x}^{(t)} \in \mathbb{R}^n$ and $\mathbf{W}^{(rec)} \in \mathbb{R}^{n \times n}$. In (5), considering only a single term of the product ($\partial x^{(i)}/\partial x^{(i-1)}$), they state: $$ \frac{\partial \mathbf{x}^{(i)}}{\partial \mathbf{x}^{(i-1)}} = \left(\mathbf{W}^{(rec)}\right)^T diag(\sigma^{\prime}(\mathbf{x}^{(i-1)})) $$ where the $diag$ operation takes the vector $\sigma^{\prime}(\mathbf{x}^{(i-1)})$ and creates a $n \times n$ diagonal matrix. I've worked this out using generic indicial notation, and a simple case where $n=3$, and I don't know where the transpose is coming in. Any ideas?
