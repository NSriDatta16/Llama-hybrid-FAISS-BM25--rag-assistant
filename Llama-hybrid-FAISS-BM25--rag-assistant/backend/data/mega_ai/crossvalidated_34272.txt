[site]: crossvalidated
[post_id]: 34272
[parent_id]: 
[tags]: 
Dirichlet processes for supervised learning?

It seems when I glance around here at the fashionable learning algorithms, things like neural networks, boosted trees, support vector machines, random forests, and friends are promoted for supervised learning problems. Dirichlet processes and their ilk seem to be mentioned mostly in unsupervised learning problems, such as document or image clustering. I do see them get used for regression problems, or as general purpose priors when one wants to do Bayesian statistics in a nonparametric or semiparametric way (e.g. as a flexible prior on the distribution of random effects in certain models) but my limited experience suggests that this doesn't come as much from the machine learning crowd as it does from more traditional statisticians. I've done a small amount of googling on this and I've found a few definitive uses in machine learning of DPs for supervised learning, but it seems like whenever I need to look up something about (say) hierarchical DPs, whatever paper I find the answer in is using it for unsupervised learning. So, are Dirichlet processes and their cousins most effective as priors for flexible clustering models? Are they not competitive with boosting, SVMs, and neural networks for supervised learning problems? Are they useful only in certain situations for these problems? Or is my general impression incorrect?
