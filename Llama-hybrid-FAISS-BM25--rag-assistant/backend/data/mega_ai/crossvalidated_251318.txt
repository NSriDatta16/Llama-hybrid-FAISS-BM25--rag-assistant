[site]: crossvalidated
[post_id]: 251318
[parent_id]: 251296
[tags]: 
Your intuition is right. The logistic regression model gives identical inference to a Pearson chi-sq test of "independence" for categorical data (in the long run). In both cases, the null hypothesis is that the conditional probabilities are equal to the marginal probabilities. You can show with some algebra that this necessarily implies that the odds ratio is 1. The minute differences in the actual logistic regression and Pearson test statistics owes to how they're computed. The $p$values you get in R from calling summary.glm come from a Wald test whereas the Pearson test is a related Score test. The problem with your example is that you have a singular logistic model. One advantage of the score test is that it can provide test statistics for models like this one which "explode". For a more sane example, consider the following: set.seed(123) foo2 Gives us: > waldtest(fit, test='Chisq') Wald test Model 1: Dep ~ Pred Model 2: Dep ~ 1 Res.Df Df Chisq Pr(>Chisq) 1 196 2 199 -3 1.7774 0.6199 > chisq.test(table(foo2)) Pearson's Chi-squared test data: table(foo2) X-squared = 1.7882, df = 3, p-value = 0.6175
