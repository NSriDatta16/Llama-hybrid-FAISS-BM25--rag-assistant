[site]: crossvalidated
[post_id]: 637383
[parent_id]: 
[tags]: 
Differences Between Independent Components from ICA Directly on Samples vs. Mixing Matrix from ICA on Features After PCA Dimensionality Reduction

My understanding is that for Independent Component Analysis (ICA), it is recommended to have more samples than features to avoid underdetermination which might cause convergence or stability issues. However, I would like to understand intuitively and via a proof why the mixing matrix obtained after using Principal Component Analysis (PCA) to reduce the number of samples and then running ICA on the reduced sample data would not yield the same result as the independent components after running ICA on the features with the original sample size. For example, I have a data matrix $ X $ that is $p \times n$ , where $p$ is the number of features, $n$ is the number of samples, and $ p . Additionally, the number of sources $s$ equals the number of features $p$ . Let the Singular Value Decomposition (SVD) of $X$ be $$ X = U \Sigma V^T $$ Normally to find the independent components I would run ICA as follows: Whiten the data using PCA on $XX^T$ (equivalent to using the singular values and left singular vectors to whiten the data) $$ \Sigma^{-1}UX $$ Iteratively rotate this whitend data by an orthogonal matrix $ W_{1} $ until independent components are found $$ S_{1} = W_{1}\Sigma^{-1}U^TX $$ where $ S_{1} $ represents the matrix of $s \times n$ sources and $ A_{1} = W_{1}\Sigma^{-1}U^T $ the $s \times p$ unmixing matrix Alternatively, I could reduce the number of samples in my data to be equal to the number of sources by running PCA on $ X^TX $ and projecting my data onto the top $s$ principal components. The principal components are equivalent to the reduced right singular vectors denoted by $ \hat{V} $ which has dimensions $n\times s$ . Additionally, by using $\hat{V}\Sigma^{-1}$ I can whiten my data and now run ICA as follows: Whiten the data using PCA on $X^TX$ (equivalent to using the top $s$ singular values and right singular vectors to whiten the data) $$ X\hat{V}\Sigma^{-1} $$ Iteratively rotate this whitend data by an orthogonal matrix $ W_{2} $ until independent components are found $$ S_{2} = X\hat{V}\Sigma^{-1}W_{2} $$ where $ S_{2}$ represents the matrix of $p \times s$ sources and $ A_{2} = \hat{V}\Sigma^{-1}W_{2} $ is the $n \times s$ unmixing matrix Here is my attempt at a proof to show that $ A_{2} \not= S_{1} $ $$ S_{1} = W_{1}\Sigma^{-1}U^TX \space ,\space A_{2} = \hat{V}\Sigma^{-1}W_{2} $$ $$ S_{1} = W_{1}\Sigma^{-1}U^TX = W_{1}\Sigma^{-1}U^TU \Sigma V^T = W_{1}\Sigma^{-1} \Sigma V^T = W_{1}\hat{V^T} $$ $\Sigma^{-1} \Sigma V^T = \hat{V^T} $ since $\Sigma$ is not full rank Now I was wondering, is this proof correct. If so, which is the reason that $ A_{2} \not= S_{1} $ : $S_{1}$ lacks $\Sigma^{-1}$ that $ A_{2} $ has $ W_{2} \not= W_{1} $ (I could not find a way to prove that they were equal or unequal) In order for $ A_{2} = S_{1} $ , $ W_{1} = \Sigma^{-1}W_{2} $ must hold which given the constraint that $ W_{1} $ and $ W_{2} $ have to be orthogonal matrices $ \Sigma^{-1}$ would have to be unit normal which would make it guassian and theorefore ICA would fail because the data is assumed to be non gaussian None of these, I did something wrong Additionally, what would be the intuitive reason as to why this does not hold? I would want to guess the answer would sound something like "in the second case we are forcing independence on smaller samples which has less information" or "by using PCA to reduce our samples we are undersampling" but I can not fully form a sound intuition. A real-world example of this I am curious about is really understanding the difference between spatial ICA and temporal ICA on fMRI data because if there was not a difference then you could reduce the number of voxels to be less than the number of timepoints and the result of the mixing matrix from temporal ICA would be the same as the independent components from spatial ICA. Thank you!
