[site]: crossvalidated
[post_id]: 233381
[parent_id]: 
[tags]: 
Variance of the average of independent normal observations with different means and variance

I am a little confused here on finding a probability but I can figure out the end I just need help on the intermediate steps. So we have 55 independent normal observations, which all have the same mean $\mu$. The first 50 observations have variance $\sigma_1^2$ and the last 5 observations have variance $\sigma_2^2$. What is the variance of the average of all 55 observations? I left out some of the numbers here so I can scrutinize it theoretically. I've tried a few things. The sum of both variances. I ruled this out because one of the subsets has a much higher variance than the other, much smaller subset. I felt like this would somehow influence the ending variance/standard deviation, in the way that outliers do. The sum of both variances, weighted by the proportion of the entire set they make up. That is to say, if one of the subsets is 10/11ths of the data and the other is 1/11th, then their respective variances would take up that percentage of the variance of the average of all 55. The sum of both variances, divided by 55, the total number of all observations. The sum of both variances divided by $55^2$. What about the average, while I'm asking? Do I add them together? Do divide $\mu$ by 50 and 5 respectively, and then add them? Do I add $\mu + \mu$ and then divide by 55? Is the average simply $\mu$? Any help would be appreciated.
