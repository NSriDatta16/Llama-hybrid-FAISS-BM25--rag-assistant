[site]: crossvalidated
[post_id]: 434749
[parent_id]: 
[tags]: 
Maximum likelihood as minimizing the dissimilarity between the empirical distriution and the model distribution

I am reading Ian Goodfellow "Deep Learning" book. At page 128 it says One way to interpret maximum likelihood estimation is to view it as minimizing the dissimilarity between the empirical distribution $\hat{p}_{\text{data}}$ , defined by the training set and the model distribution, with the degree of dissimilarity between the two measured by the KL divergence. The KL divergence is given by $$ D_{KL} (\hat{p}_{\text{data}} || p_{\text{model}}) = \mathbb{E}_{\mathbf{x} \sim \hat{p}_{\text{data}}} [\log \hat{p}_{\text{data}} - \log p_{\text{model}}(\mathbf{x})]$$ Starting from the definition of maximum likelihood estimator written in the text: $$\mathbf{\theta}_{ML} = \arg\max_{\theta} p_{\text{model}}(\mathbb{X}; \mathbf{\theta}) $$ Is there a formal proof for this? What is the intuition behind the formulation of maximum likelihood estimator as minimizing the KL divergence between the empirical distribution and the model distribution?
