[site]: crossvalidated
[post_id]: 375304
[parent_id]: 330491
[tags]: 
In some papers the two concepts are considered the same, e.g.: Krueger, Paul, Thomas Griffiths, and Stuart J. Russell. "Shaping Model-Free Reinforcement Learning with Model-Based Pseudorewards." (2017). However, there may be a difference in the way the update is done. Dyna uses the value function and the prediction error directly. It can thus use one single simulated step update. Using replay may be more similar to use montecarlo updates that consider the cumulative reward over a sequence of actions and do not use the value function or the prediction error in the update. Z. Feldman and C. Domshlak, “Monte-Carlo tree search: To MC or to DP?,” in ECAI 2014: 21st European Conference on Artificial Intelligence, 2014, vol. 263, p. 321
