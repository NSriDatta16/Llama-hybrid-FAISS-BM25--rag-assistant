[site]: crossvalidated
[post_id]: 153592
[parent_id]: 153210
[tags]: 
The standard "factor rotation" take on this problem would be to perform PCA, choose some number of components (e.g. three), normalize them to unit variance, and then rotate the loadings according to some specific criterium. Usually rotation criteria optimize loading sparseness, but I guess one can try instead to bring the first loading vector to a specific direction -- in your case $$\mathbf v_1 = [1/n, \ldots, 1/n]^\top.$$ However, note that the subspace spanned by the first three PC directions might not include $\mathbf v_1$ at all! Most likely it does not. If so, then there is no "factor rotation" sensu strictu that can solve your problem. For that reason I think you need to use a more specific approach. My suggestion is to construct your first component (C1, as opposed to PC1) by projecting the data $\mathbf X$ onto the main diagonal vector $\mathbf v_1$ by computing $\mathbf X \mathbf v_1$. Then you can take the "lefover" data and perform a standard PCA on it, taking resulting principal components as your components C2, C3, etc. There are two natural ways to define "leftover" data, depending on what property of PCA you want to be preserved. Preserving the orthogonality of the PC directions Project the data on the hyperplane orthogonal to $\mathbf v_1$, i.e. by defining $$\mathbf X_\text{leftover}=\mathbf X-\mathbf X\mathbf v_1 \mathbf v_1^\top = \mathbf X(\mathbf I - \mathbf v_1 \mathbf v_1^\top).$$ This will produce orthogonal component axes (because PCA will be done in the subspace orthogonal to $\mathbf v_1$), but not necessarily uncorrelated components: C2 and C3 can (and will) be somehow correlated with C1. If the original PCA produces PC1 that is very close to C1, then these correlations will be weak. Preserving the uncorrelatedness of the PCs The set of vectors $\mathbf w$ that are orthogonal to the unit vector $\mathbf v_1$ is defined by the condition $\mathbf w^\top \mathbf v_1$; to project the data onto the subspace spanned by this set we used the projector $\mathbf P = \mathbf I - \mathbf v_1 \mathbf v^\top$. The set of vectors $\mathbf w$ that yield projections uncorrelated with the projection onto $\mathbf v_1$ is defined by the condition $\mathbf w^\top \boldsymbol \Sigma \mathbf v_1$, where I defined the covariance matrix of the data $\boldsymbol \Sigma = \mathbf X^\top \mathbf X / (n-1)$. It follows by analogy that to project the data onto the subspace spanned by this set we need to use the projector $\mathbf P = \mathbf I - \tilde{\mathbf v}_1 \tilde{\mathbf v}_1^\top$, where $\tilde{\mathbf v}_1=\boldsymbol \Sigma \mathbf v_1/\|\boldsymbol \Sigma \mathbf v_1\|$ is simply $\boldsymbol \Sigma \mathbf v_1$ normalized to unit length. This insight allows us to compute the leftover data as follows: $$\mathbf X_\text{leftover}=\mathbf X\left(\mathbf I - \frac{\boldsymbol \Sigma \mathbf v_1 \mathbf v_1^\top \boldsymbol \Sigma}{\|\boldsymbol \Sigma \mathbf v_1\|^2}\right).$$ Again, this will yield uncorrelated components, but not necessarily orthogonal projection directions. Note, as a consistency check, that if $\mathbf v_1$ is a unit eigenvector of $\boldsymbol \Sigma$, then $\tilde{\mathbf v}_1=\boldsymbol \Sigma \mathbf v_1/\|\boldsymbol \Sigma \mathbf v_1\| = \mathbf v_1$, meaning that for standard PCA the two approaches coincide.
