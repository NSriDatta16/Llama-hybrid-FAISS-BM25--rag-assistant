[site]: crossvalidated
[post_id]: 538681
[parent_id]: 536243
[tags]: 
Given D_array I want to be able to determine whether a newly calculated distance value D' is "significant" based on my previous data: the distribution of my D_array . Ideally, I would like to provide some sort of metric of 'significance' such as a p-value. By significance I mean the probability / significance of having gotten a result as extreme as D' . To what might be a good enough approximation, you already have what you need. You display an empirical distribution of D values. If you think that an observation in the top 5% of that distribution is "significant," you could simply use the 95th percentile of the empirical cumulative distribution function as your cutoff. I found that I can use bootstrapping to calculate a 95% confidence interval for D_array , and then essentially ask if D' is outside of the 95% CI range. Bootstrapping can be a useful way to get the sampling distribution of a statistic calculated from a data sample. Bootstrapping doesn't really give you a 95% confidence interval (CI) for the entire D_array . As you do bootstrap sampling with replacement, the average distribution among the bootstrap samples should be the same as the original D_array . You get the confidence interval for a statistic by doing multiple bootstrap samples, calculating the statistic on each of them, and examining the distribution of the statistic among the bootstrap samples. This answer discusses relative advantages of different ways to do that. Perhaps you are interested in a 95% confidence interval for the 95th percentile of the population distribution from which you sampled your D_array . Bootstrapping probably won't help much with that. Quoting from that answer : The first, general, issue is how well the empirical distribution $\hat F$ represents the population distribution $F$ . If it doesn't, then no bootstrapping method will be reliable. In particular, bootstrapping to determine anything close to extreme values of a distribution can be unreliable. This issue is discussed elsewhere on this site, for example here and here . The few, discrete, values available in the tails of $\hat F$ for any particular sample might not represent the tails of a continuous $F$ very well. One way to proceed would be to base your analysis on an model of the distribution of your D_array . Your data look like they might be from a log-normal distribution , which sometimes works well for calculations based on count data. If a log-normal distribution is appropriate, you might be better off fitting such a distribution to your data and basing your cutoff on the upper 5% tail of the fitted distribution. Finally, I fear that you are putting too much emphasis on "statistical significance" rather than practical significance. What is significant for your application ? How will you be using the cases that you identify as "significant" versus "non-significant"? Are you more interested in avoiding false-positive or false-negative findings?* For example, using a cutoff based on the upper 95% CI of the 95th percentile of the distribution would tend to put that tradeoff far toward the side of avoiding false positives. Is that what you really want? Those questions must be answered based on understanding of the subject matter. Once you answer those fundamental questions, statistical analysis can point the way to meeting your objectives reliably. *Large-scale biological studies like RNA-seq typically control the false-discovery rate rather than the false-positive (Type I error) rate.
