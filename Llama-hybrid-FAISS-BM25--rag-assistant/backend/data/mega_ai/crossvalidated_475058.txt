[site]: crossvalidated
[post_id]: 475058
[parent_id]: 475052
[tags]: 
Sensitivity and specificity are two entirely different measures of the usefulness of a test. One is based on a (presumably small) population of subjects who have the disease or condition; the other is based on a (presumably much larger) population of subjects who don't. I can see no valid rationale for averaging the two. As an example, suppose a test has sensitivity 99% but its specificity is 1%, essentially rendering the test useless. A bogus test that just declares 99% of subjects to be 'positive', absent all contact with reality, would do as well. Then how could you justify a definition of 'test accuracy' to say the test is "50% accurate"? Example: Consider a population of 100,000 with 5% prevalence so that 5000 have the disease and 95,000 do not. Especially in developmental stages it is not unrealistic for a test to have 95% sensitivity and 80% specificity. Here are the consequences of testing everyone in the population: 4900 correctly treated or quarantined due to true positive results, and 100 undetected potential 'spreaders' of the disease. 19,000 incorrectly quarantined or treated (by whatever means) due to false positive results, and 76,000 with no direct consequences of testing. Especially considering that any member of the population can get the disease at any point in time, the situation is sufficiently difficult that unjustified simplifications are not likely to be helpful.
