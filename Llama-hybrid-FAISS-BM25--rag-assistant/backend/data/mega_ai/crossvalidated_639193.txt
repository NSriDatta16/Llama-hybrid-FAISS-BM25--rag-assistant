[site]: crossvalidated
[post_id]: 639193
[parent_id]: 
[tags]: 
Model evaluation approach and How it affects the performance of the model

So the task iam working on is supervised video summarization where the model tries to predict if a video frame is important or no using its features and the labels as annotations of frame scores. following related work people usually don't use these annotations directly as labels but transform them from a key-frame-score to a key-shot-summary (will be explained) then these labels are used in training and in evaluation the model predictions for each frame is transformed to the key-shot-summary aswell to further evaluate them. key-shot-summary : a function that transforms a sequence of scores for frames into a sequence of segments that have high scores and these segments are assigned average score of the entire segment then these segments are filtered out using a knapsack with a limit of a certain precentage and finally it returns a 1d boolean array for each frame in the video i am currently trying to understand the difference in evaluating the model using one of 2 approaches Firstly : after using the key-shot-summary for model predictions we evaluate the model using f1-score between that key-shot-summary and the original key-frame-scores(labels) before transformation (which is done normally in related work) it gives a normal evaluation results across epochs and stable f-score. Secondly : we transform the labels to key-shot-summary aswell just like the predictions then we evaluate them (i came up with) but using this causes the f1-score to be higher but very unstable across epochs. here is an image representing the second approach and its result : in this image as you can see the f1-score is really unstable
