[site]: crossvalidated
[post_id]: 467067
[parent_id]: 
[tags]: 
Supplemental material for Elements of Statistical Learning?

I'm a Scientific Computing PhD student with a strong background in linear algebra, calculus, and numerical methods, and some exposure to probability theory. I've taken a grad level machine learning class, and I'd say I understood the material pretty well. I don't really have a stats & probability background aside from the grad courses I've taken that used probability theory, and undergrad stats and AP stats. Anyways, I just started reading ESL yesterday, and I must admit that I'm having a lot of difficulty following. Are there any supplemental material to go along with this book? For example, in Chapter 3, everything made sense until Eq. (3.8). Everything after that in that section is going over my head. When they wrote "The $N − p − 1$ rather than $N$ in the denominator makes $\hat{\sigma}^2$ an unbiased estimate of $\sigma^2$ ," I was like "OK I don't know where this came from, but I'll just take author's word for it." And then I proceeded to the following paragraph, and got stuck again, this time on "We now assume that (3.1) is the correct model for the mean; that is, the conditional expectation of Y is linear in X1, . . . , Xp. We also assume that the deviations of Y around its expectation are additive and Gaussian. Hence..." If this is going to be a common occurrence throughout the book, then I think I'd rather just stop reading it and read up on ML methods online. I'm reading this book for two purposes: (1) preparing for quant interviews (2) to learn ML in more depth
