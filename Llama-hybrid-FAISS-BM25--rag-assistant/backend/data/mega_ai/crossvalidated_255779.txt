[site]: crossvalidated
[post_id]: 255779
[parent_id]: 254254
[tags]: 
My answer depends on what you want to do with the regression. If you are trying to compare the effect of different coefficients, then regression may not be the right tool for you. If you are trying to make predictions using different coefficients that you have proven are independent, then maybe multiple regression is what you should use. Are the factors correlated? If so, a multivariate regression can give you a bad model and you should use a method like VIFs or ridge regression to trim cross-correlations. You should not compare coefficients until the cross-correlated factors are eliminated. Doing so will lead to disaster. If they are not cross-correlated, then multivariate coefficients should be as comparable as univariate coefficients, and this should not be surprising. The outcome might also depend on the software package you are using. I am not joking. Different software packages have different methods for calculating multivariate regression. (Don't believe me? Check out how the standard R regression package calculates R 2 with and without forcing the origin as the intercept. Your jaw should hit the floor.) You need to understand how the software package is performing the regression. How is it compensating for cross-correlations? Is it performing a sequential or matrix solution? I've had frustrations with this in the past. I suggest performing your multiple regression on different software packages and see what you get. Another good example here: Note that in this equation, the regression coefficients (or B coefficients) represent the independent contributions of each independent variable to the prediction of the dependent variable. Another way to express this fact is to say that, for example, variable X1 is correlated with the Y variable, after controlling for all other independent variables. This type of correlation is also referred to as a partial correlation (this term was first used by Yule, 1907). Perhaps the following example will clarify this issue. You would probably find a significant negative correlation between hair length and height in the population (i.e., short people have longer hair). At first this may seem odd; however, if we were to add the variable Gender into the multiple regression equation, this correlation would probably disappear. This is because women, on the average, have longer hair than men; they also are shorter on the average than men. Thus, after we remove this gender difference by entering Gender into the equation, the relationship between hair length and height disappears because hair length does not make any unique contribution to the prediction of height, above and beyond what it shares in the prediction with variable Gender. Put another way, after controlling for the variable Gender, the partial correlation between hair length and height is zero. http://www.statsoft.com/Textbook/Multiple-Regression There are so many pitfalls using multiple regression that I try to avoid using it. If you were to use it, be very careful with the outcomes and double check them. You should always plot the data visually to verify the correlation. (Just because your software program said there was no correlation, doesn't mean there isn't one. Interesting Correlations ) Always check your results against common sense. If one factor shows a strong correlation in a univariate regression, but none in multivariate, you need to understand why before sharing the results(the gender factor above is a good example).
