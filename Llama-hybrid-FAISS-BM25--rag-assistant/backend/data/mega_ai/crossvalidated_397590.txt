[site]: crossvalidated
[post_id]: 397590
[parent_id]: 396829
[tags]: 
why such a penalty is used It is used to penalize the complexity of model (function), or equivalently to encourage the model to be as simple as possible (roughly has fewer weights involved). This way, over-fitting is avoided and generalization of model to unseen data is improved. Why does penalizing larger values of W affect the "bias/variance trade-off" of a model? By forcing the parameters to be close to zero, the variance of model which is a function of its parameters would decrease, since parameters are less free to change compared to when their value is not penalized. On the other hand, the true function may need a wide range of weights to be estimated correctly, i.e. more flexible/complex model. However, by this penalty, we are allowing less flexibility (less complexity) thus moving the model away from the best configuration of weights required to estimate the true function, i.e. more bias.
