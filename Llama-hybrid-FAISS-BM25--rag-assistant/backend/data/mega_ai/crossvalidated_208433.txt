[site]: crossvalidated
[post_id]: 208433
[parent_id]: 
[tags]: 
How well should backpropagation agree with finite difference methods when calculating derivatives of the error function?

I have attempted to write a Neural Network code, and it was suggested in my textbook (Bishop - Pattern Recognition & Machine Learning) that a very useful debugging technique is to check your $\frac{d E}{d w}$ terms computed via back-propagation 'agree closely' with those calculated by a central difference approach. Having coded this up, I find that my derivatives agree to within one part in 10. I have no idea if this satisfies 'close agreement'.
