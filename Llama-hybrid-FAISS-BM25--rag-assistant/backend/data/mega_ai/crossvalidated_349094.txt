[site]: crossvalidated
[post_id]: 349094
[parent_id]: 37406
[tags]: 
Proportionality is used to simplify analysis Bayesian analysis is generally done via an even simpler statement of Bayes' theorem, where we work only in terms of proportionality with respect to the parameter of interest. For a standard IID model with sampling density $f(X|\theta)$ we can express this as: $$p(\theta|\mathbf{x}) \propto L_\mathbf{x}(\theta) \cdot p(\theta) \quad \quad \quad \quad L_\mathbf{x}(\theta) \propto \prod_{i=1}^n f(x_i|\theta).$$ This statement of Bayesian updating works in terms of proportionality with respect to the parameter $\theta$. It uses two proportionality simplifications: one in the use of the likelihood function (proportional to the sampling density) and one in the posterior (proportional to the product of likelihood and prior). Since the posterior is a density function (in the continuous case), the norming rule then sets the multiplicative constant that is required to yield a valid density (i.e., to make it integrate to one). This method use of proportionality has the advantage of allowing us to ignore any multiplicative elements of the functions that do not depend on the parameter $\theta$. This tends to simplify the problem by allowing us to sweep away unnecessary parts of the mathematics, and get simpler statements of the updating mechanism. This is not a mathematical requirement (since Bayes' rule works in its non-proportional form too), but it makes things simpler for our tiny animal brains. An applied example: Consider an IID model with observed data $X_1, ..., X_n \sim \text{IID N}(\theta, 1)$. To facilitate our analysis we define the statistics $\bar{x} = \tfrac{1}{n} \sum_{i=1}^n x_i$ and $\bar{\bar{x}} = \tfrac{1}{n} \sum_{i=1}^n x_i^2$, which are the first two sample moments. For this model we have sampling density: $$\begin{equation} \begin{aligned} f(\mathbf{x}|\theta) = \prod_{i=1}^n f(x_i|\theta) &= \prod_{i=1}^n \text{N}(x_i|\theta,1) \\[6pt] &= \prod_{i=1}^n \frac{1}{\sqrt{2 \pi}} \exp \Big( -\frac{1}{2} (x_i-\theta)^2 \Big) \\[6pt] &= (2 \pi)^{n/2} \exp \Big( -\frac{1}{2} \sum_{i=1}^n (x_i-\theta)^2 \Big). \\[6pt] &= (2 \pi)^{n/2} \exp \Big( -\frac{n}{2} ( \theta^2 - 2\bar{x} \theta + \bar{\bar{x}} ) \Big) \\[6pt] &= (2 \pi)^{n/2} \exp \Big( -\frac{n \bar{\bar{x}}}{2} \Big) \cdot \exp \Big( -\frac{n}{2} ( \theta^2 - 2\bar{x} \theta ) \Big) \\[6pt] \end{aligned} \end{equation}$$ Now, we can work directly with this sampling density if we want to. But notice that the first two terms in this density are multiplicative constants that do not depend on $\theta$. It is annoying to have to keep track of these terms, so let's just get rid of them, so we have the likelihood function: $$L_\mathbf{x}(\theta) = \exp \Big( -\frac{n}{2} ( \theta^2 - 2\bar{x} \theta ) \Big).$$ That simplifies things a little bit, since we don't have to keep track of an additional term. Now, we could apply Bayes' rule using its full equation-version, including the integral denominator. But again, this requires us to keep track of another annoying multiplicative constant that does not depend on $\theta$ (more annoying because we have to solve an integral to get it). So let's just apply Bayes' rule in its proportional form. Using the conjugate prior $\theta \sim \text{N}(0,\lambda_0)$, with some known precision parameter $\lambda_0>0$, we get the following result (by completing the square ): $$\begin{equation} \begin{aligned} p(\theta|\mathbf{x}) &\propto L_\mathbf{x}(\theta) \cdot p(\theta) \\[10pt] &= \exp \Big( -\frac{n}{2} ( \theta^2 - 2\bar{x} \theta ) \Big) \cdot \text{N}(\theta|0,\lambda_0) \\[6pt] &\propto \exp \Big( -\frac{n}{2} ( \theta^2 - 2\bar{x} \theta ) \Big) \cdot \exp \Big( -\frac{\lambda_0}{2} \theta^2 \Big) \\[6pt] &= \exp \Big( -\frac{1}{2} ( n\theta^2 - 2n\bar{x} \theta + \lambda_0 \theta^2 ) \Big) \\[6pt] &= \exp \Big( -\frac{1}{2} ( (n+\lambda_0) \theta^2 - 2n\bar{x} \theta ) \Big) \\[6pt] &= \exp \Big( -\frac{n+\lambda_0}{2} \Big( \theta^2 - 2 \frac{n\bar{x}}{n+\lambda_0} \theta \Big) \Big) \\[6pt] &\propto \exp \Big( -\frac{n+\lambda_0}{2} \Big( \theta - \frac{n}{n+\lambda_0} \cdot \bar{x} \Big)^2 \Big) \\[6pt] &\propto \text{N}\Big( \theta \Big| \frac{n}{n+\lambda_0} \cdot \bar{x}, n+\lambda_0 \Big). \\[6pt] \end{aligned} \end{equation}$$ So, from this working we can see that the posterior distribution is proportional to a normal density. Since the posterior must be a density, this implies that the posterior is that normal density: $$p(\theta|\mathbf{x}) = \text{N}\Big( \theta \Big| \frac{n}{n+\lambda_0} \cdot \bar{x}, n+\lambda_0 \Big).$$ Hence, we see that a posteriori the parameter $\theta$ is normally distributed with posterior mean and variance given by: $$\mathbb{E}(\theta|\mathbf{x}) = \frac{n}{n+\lambda_0} \cdot \bar{x} \quad \quad \quad \quad \mathbb{V}(\theta|\mathbf{x}) = \frac{1}{n+\lambda_0}.$$ Now, the posterior distribution we have derived has a constant of integration out the front of it (which we can find easily by looking up the form of the normal distribution ). But notice that we did not have to worry about this multiplicative constant - all our working removed (or brought in) multiplicative constants whenever this simplified the mathematics. The same result can be derived while keeping track of the multiplicative constants, but this is a lot messier.
