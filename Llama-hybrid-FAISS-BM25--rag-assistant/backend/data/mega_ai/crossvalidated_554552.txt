[site]: crossvalidated
[post_id]: 554552
[parent_id]: 245909
[tags]: 
As $A\geq B\Leftrightarrow B^{-1}\geq A^{-1}$ , we may also establish \begin{align*} W^{-1}(X'X)^{-1}W^{-1}-X'X\geq0 \end{align*} Plug in to get \begin{align*} (X'X+\lambda I)(X'X)^{-1}(X'X+\lambda I)-X'X\geq0. \end{align*} Multiplying out and collecting terms gives \begin{align*} 2\lambda I+\lambda^2(X'X)^{-1}\geq0, \end{align*} which is true because $\lambda>0$ , $X'X$ is p.s.d and hence so is its inverse as is the identity matrix. Also, the sum of two p.s.d. matrices is also p.s.d . Here is a numerical example, in which the regressor has relatively little variation. Hence, the ridge correction matters relatively much (e.g., for a simple regression without intercept model we have $\hat\beta_{\text{ridge}}/\hat\beta_{\text{OLS}}=(1+\lambda/\sum_ix_i^2)^{-1}$ ). I plot the slopes of least squares (coral colors) and ridge (green) of 50 replications of the experiment against a representative draw from the DGP. We observe that, while least squares is, unlike ridge, "correct on average" (unbiased) for the true slope (blue, equal to two), it is rather volatile. Ridge, in turn, is more stable - albeit, being biased, around a wrong value. ls.vs.ridge
