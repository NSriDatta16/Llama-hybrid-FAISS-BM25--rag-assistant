[site]: crossvalidated
[post_id]: 621820
[parent_id]: 
[tags]: 
Moving-optimum optimization

Are there ready-made algorithms to solve the following problem? At each time step $t$ the agent chooses a value $x_t$ . There is a function $f(x, t)$ that gives the error on timestep $t$ after choosing $x$ . $f$ is assumed to change relatively slowly in both $x$ and $t$ . It might reasonably be assumed that for a fixed $t$ , $f$ is a convex, differentiable function of $x$ . Aside from that, $f(x, t)$ is not specifically known. After the agent chooses the value $x_t$ , the agent learns only two pieces of information: $f(x_t, t)$ and $\frac{\partial{f}}{\partial{x}} \Bigr|_{x_t, t}$ . The second piece of information is like the "force" pushing $x$ towards more optimal values on that time step. The goal is to minimize the discounted long-run sum of errors $f(x, t)$ . I don't really know what to call this setting. It's not really reinforcement learning because the agent's choice of $x_t$ only affects the reward on the single time step $t$ , and does not affect the future system state. We can think of $x$ like a particle being pushed towards a moving optimum. Anyway, is there a simple, good way to approach this?
