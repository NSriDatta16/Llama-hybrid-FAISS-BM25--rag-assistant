[site]: crossvalidated
[post_id]: 629345
[parent_id]: 629270
[tags]: 
Q1 The theory used to compute the p value of the test of the null hypothesis of a flat, constant function equal to 0 treats the smoothing parameters (the things that ultimately determine the wiggliness of the smooth functions) as fixed and known. However, these were selected by mgcv during fitting. Hence the p value doesn't account for this extra source of uncertainty, and are, as a consequence a little too liberal, a little too anti-conservative, a little too approximate. And that last point is on top of the approximation for p values in generalized linear models (which apart from the Gaussian, rely on large sample approximations). That said, you really shouldn't be making decisions on the basis of p values in applied science work, the estimated function and effect sizes are more important. Q2 Correct; selecting seWithMean = TRUE will not change the computation of the p value via summary() , nor can it; these are two separate functions. The issue that seWithMean option is trying to correct is visible when smooths are estimated to be linear functions. Then, because of the sum-to-zero constraint applied to all the smooths (which is what allows a constant term — the intercept — in the model), the smooth function must pass through the point $\widehat{f(x)} = 0$ . At that point, there is no uncertainty in the value of the smooth, by definition the value is 0 with no uncertainty about that. From the point of view of creating an interval about the smooth that has appropriate coverage (95% coverage if a 1-0.05 interval is formed), this behaviour is problematic; there is no way that a zero-width confidence interval makes any sense (which is what one would get if one used the standard error of the smooth = 0 at this zero-crossing point) because it implies that the smooth must pass through this point. What seWithMean does is apply the uncertainty in the model constant term to each smooth in turn while forming the interval. This adds the uncertainty in the vertical position to the uncertainty in the smooth function itself. It turns out that this correction provides an interval with the correct coverage, except for the situation where the true function is nonlinear but close to the penalty null space (i.e. the true but unknown function is close to linear). It's not clear to me that this extra uncertainty should be used in computing a p value. That p value is for a test of a specific hypothesis, which isn't affected by the vertical position of the smooth. Hence, from the point of the test of that very specific null hypothesis (i.e. a null of a flat constant function), this extra uncertainty that is needed to make an interval have the right coverage is not necessary when formulating the Wald-like test against the flat function. Q3 The theory needed to correct the covariance matrix for smoothness parameter selection requires the calculation of certain values and those are only done if we fit the model using specific smoothness selection methods. When we fit these GAMs, we are fitting a penalized likelihood model, where we take into account the wiggliness of the estimated smooths. This wiggliness is a penalty; the wigglier the smooth the more complex the function. We use smoothing parameters to control how much price we pay for the wiggliness penalty. If the smoothing parameter is large we pay a heavy penalty for wiggliness and so we'll tend to pick very smooth functions. If we have a small smoothing parameter we pay less penalty for wiggliness and hence we can fit wigglier, more complex functions. The only problem is that when we fit the model we don't know what the values of these smoothing parameters are, and instead estimate them during fitting, but when we compute tests and other statistical values we assume we knew the values of these smoothing parameters all along. Clearly, not knowing the values of these smoothing parameters but acting as if we did know them, would mean we assume our uncertainties are smaller than what they actually are. The unconditional argument is a way to correct, to an extent, for the fact that we did not know the values of these smoothing parameters after all. But the required correction can only be computed for some approaches to smoothness selection. Typically this is when method = "REML" or method = "ML" . The default is method = "GCV" and with that method the correction is not available. Q4 Fundamentally, the credible interval and the p value aren't intended to convey or reflect or test the same quantity of the smooth. What I think is happening in this plot is that we are reasonably confident that the function is not linear and not a constant 0 function, but we are unsure exactly where the non-linear function lies. This latter point is reflected by the width of the interval. Just because you can draw a straight line through the interval doesn't mean a constant function is consistent with the estimated function.
