[site]: crossvalidated
[post_id]: 300060
[parent_id]: 277975
[tags]: 
One good approach suggested in cs231n course is to use transfer learning, i.e. rather than engineering and training CNN architectures from scratch you can use one of the existing pre-trained high-performing architectures such as ResNet50 or InceptionV3, remove several top layers and re-train on your dataset. See the following ipython notebook for an example. It's also worthwhile to develop intuition for why CNN architectures designed the way they are. For example, have a look at VGG-16 paper that shows that increasing depth (the number of filters) with every layer results in higher accuracy, i.e. we obtain a convolutional pyramid . It's common to have a structure where convolutional layer is followed by ReLU activation followed by a max-pooling layer. InceptionV3 architecture uses multi-scale convolutions such as 3x3 and 5x5 simultaneously as well as 1x1 convolutions for dimensionality reduction. ResNet50 introduces skip connections that enable easier training of very deep convolutional networks. Similar skip connections can be seen in the DenseNet architecture.
