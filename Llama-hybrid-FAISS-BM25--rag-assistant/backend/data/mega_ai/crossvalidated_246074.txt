[site]: crossvalidated
[post_id]: 246074
[parent_id]: 246069
[tags]: 
In general, $P(A|B)$ is read as "probability of $A$ given $B$" or in other words, the probability of event $A$ under the assumption of $B$. If there are multiple events that we assume are given, we can simply write $P(A|B,C)$, which mean that we now assume $B$ and $C$. From the paper: In logistic regression, we model the conditional probability $P(y = 1 | w, x)$ as $1/(\exp(âˆ’w^T x) + 1)$. So in this case, the model $P(y=1|w,x)$ is the probability that $y=1$ given the weight vector $w$ and the input vector $x$. Throughout the literature, this is sometimes written as $P(y=1|x)$, where the $w$ is left implicit. It is usually intended to be clear by context that the parameters of the model (in this case, the weights $w$) are part of the conditional probability. Our goal is to come up with a weight vector $\hat{w}$ that maximizes $P(y=1|\hat{w},x)$, so that when we receive a test input $x'$, we can plug it into our model along with the weights $\hat{w}$. If the true value of $x'$ is $y'=1$, we want $P(y'=1|\hat{w},x')$ to be high. Note that we can apply a natural log to the model to obtain $1-\ln(\exp(-w^Tx)+1)$, which will be useful in formulating an optimization problem to learn $w$. With a little work, we obtain the expression $$ \hat{w}=\operatorname{arg}\inf_w\frac{1}{n}\sum_{i=1}^n\ln(\exp(-w^Tx_iy_i)+1) $$ which gives us a way to obtain the weight vector $\hat{w}$ which maximizes the conditional probability using a collection of labeled data $\{(x_i,y_i)\}_{i=1}^n$.
