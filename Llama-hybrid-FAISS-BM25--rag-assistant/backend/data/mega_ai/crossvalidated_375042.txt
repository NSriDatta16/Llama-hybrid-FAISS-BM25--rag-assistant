[site]: crossvalidated
[post_id]: 375042
[parent_id]: 
[tags]: 
Checking the similarity of two classes in a binary classification

So, I have a binary classification problem. The classes are fairly balanced and I have a separate training set and a test set. No matter what I try, both classification accuracy and the f1 score are always hovering around 45-55%. This means that the model is just random guessing and not learning anything. I had this doubt that this could be because these two classes are so similar i.e they are inseparable. Therefore, I ran a Kolmogorov–Smirnov test and got a p-value of 0.87 between two classes for the training set and a staggering 0.98 for the test set. I have 20 independent variables and I know that K-S works only for 1-D data. Therefore, I just flattened the n-d array into a 1-D array so that I can perform a K-S on it. My question is this, can I now conclude that these two classes are statistically so similar and are inseparable using machine learning algorithms ?. Is this claim so naive? Is Kolmogorov–Smirnov test enough of a proof to prove this? Can two classes be truly inseparable even in a higher dimension? i.e via kernel transformation or neural nets. I did use different regularization techniques as well just to make sure that this is not due to overfitting.
