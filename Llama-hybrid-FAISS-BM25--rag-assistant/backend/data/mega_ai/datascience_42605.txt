[site]: datascience
[post_id]: 42605
[parent_id]: 42287
[tags]: 
Nice example of using embeddings with Keras. If I interpret it correctly, there is a big difference between your implementation and the "original word2vec". The original framework operates not with one 'embeddings' vector_size x vector_dim weight matrix as in your case, but with two matrices (or layers): the "projection layer" which maps the input to a vector of dimension vector_dim and the "hidden layer" which maps this vector to probabilities over all vocabulary words. The hidden, or prediction layer, is often discarded after the training, although it can be useful. If your code indeed forces those layers to share weights, then we get something also interesting, but different, it is plausible the embeddings reflect co-ocurrences - see this question which asks specifically what happens if the two layers are shared . Either way 'plant' should be most similar to 'plant' for cosine similarity - or there are some vectors that are exactly the same (normalized). According to the original word2vec paper, hierarchical softmax is used to speed up the training. So if you don't use it and have two weight layers, your version might become too slow. I can recommend for example the gensim library instead that implements the soft-max and should run on a modern laptop in a manageable time with your setup.
