[site]: crossvalidated
[post_id]: 393477
[parent_id]: 
[tags]: 
Estimates radically change when including Random Slopes in Multiple Logistic Regression

I am examining the fixed effects of two within-subject experimental manipulations (i.e., Ambiguity 0 = No / 1 = Yes, and Uncertainty 0 = No / 1 = Yes) on a dichotomized variable (i.e., Punishment, 0 = No / 1 = Yes) through a multiple logistic regression . Since the design includes repeated measures, I consider my subject's ID as grouping factor. My variable Punishment was dichotomized as follows: ##Example for one of the repeated measures: data1 $Punishment.R1 R1>0,1,ifelse(is.na(data1$R1),NA,0)) ##Subset of resultant dataframe: library(dplyr) subset_data1 % select(c("ID","R1","Punishment.R1"))%>%head(20) subset_data1 ## ID R1 Punishment.R1 ##145 1 15 1 ##146 2 14 1 ##147 3 0 0 ##148 4 15 1 ##149 5 10 1 ##150 6 15 1 ##151 7 18 1 ##152 8 12 1 ##153 9 1 1 ##154 10 15 1 ##155 11 6 1 ##156 12 13 1 ##157 13 15 1 ##158 14 12 1 ##159 15 0 0 ##160 16 0 0 ##161 17 15 1 ##162 18 15 1 ##163 19 3 1 ##164 20 15 1 These are the frequencies tables for the 4 different within-subject conditions: #NO Ambiguity / NO Uncertainty table(data1$Punishment.R1)/nrow(data1) ## 0 1 ## 0.1463415 0.8475610 #NO Ambiguity / YES Uncertainty table(data1$Punishment.R2)/nrow(data1) ## 0 1 ## 0.1585366 0.8292683 #YES Ambiguity / NO Uncertainty table(data1$Punishment.R3)/nrow(data1) ## 0 1 ## 0.2682927 0.7256098 #YES Ambiguity / YES Uncertainty table(data1$Punishment.R4)/nrow(data1) ## 0 1 ## 0.2195122 0.7743902 After reshaping my data from wide to long format and creating my two within-subject factors, this is how my dataframe dichm used in following analyses looks like for the first 5 subjects: library(dplyr) subset_dichm % select(c("ID","Round","Ambiguity","Uncertainty","Punishment")) %>% head(20) subset_dichm ID Round Ambiguity Uncertainty Punishment 52 1 Punishment.R1 0 0 1 234 1 Punishment.R2 0 1 1 416 1 Punishment.R3 1 0 1 598 1 Punishment.R4 1 1 1 53 2 Punishment.R1 0 0 1 236 2 Punishment.R2 0 1 1 417 2 Punishment.R3 1 0 1 599 2 Punishment.R4 1 1 1 54 3 Punishment.R1 0 0 0 237 3 Punishment.R2 0 1 0 418 3 Punishment.R3 1 0 0 600 3 Punishment.R4 1 1 0 55 4 Punishment.R1 0 0 1 238 4 Punishment.R2 0 1 1 419 4 Punishment.R3 1 0 1 602 4 Punishment.R4 1 1 0 56 5 Punishment.R1 0 0 1 239 5 Punishment.R2 0 1 1 420 5 Punishment.R3 1 0 1 603 5 Punishment.R4 1 1 1 All the analyses presented below were run in R, using the lme4 package (version 1.1.17). My first step was to allow random intercepts into my model: H1.RI |z|) ##(Intercept) 10.0213 1.0292 9.737 As you can see, I observed a significant negative effect of the Ambiguity manipulation on Punishment. The second model I tested included the effects of both experimental manipulations as random effects to account for potential intra-individual response patterns. See output below: H1.RS |z|) ##(Intercept) 8.042 1.278 6.295 3.08e-10 *** ##Ambiguity1 4.912 1.739 2.825 0.00473 ** ##Uncertainty1 18.325 2.570 7.131 9.96e-13 *** ##--- ##Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 ##Correlation of Fixed Effects: ## (Intr) Ambgt1 ##Ambiguity1 -0.637 ##Uncertanty1 0.252 -0.110 Surprisingly, there is not only a substantial change in the size of my estimates, but a complete reversal of the directionality of the Ambiguity effect (from -2.4270 to 4.912). I am struggling to understand why is this the case, and how should I proceed from here in order to clarify what is happening with my data. I conducted a model comparison which seems to point at the random slope model as the better fit to the data: anova(H1.RI,H1.RS) ##H1.RI: Punishment ~ Ambiguity + Uncertainty + (1 | ID) ##H1.RS: Punishment ~ Ambiguity + Uncertainty + (1 + Uncertainty + Ambiguity | ID) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(>Chisq) ##H1.RI 4 395.45 413.37 -193.73 387.45 ##H1.RS 9 323.99 364.30 -153.00 305.99 81.462 5 4.149e-16 *** ##--- ##Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 I am not really familiar with logistic regression so I might be missing something specific from this kind of analysis that applies to the test of mixed models. Also, I have read in other threads that it could be a case of a Simpson's Paradox (see here ), but I would like to know if there is any other plausible explanation given the data. Any comment, reference, or redirection to a similar thread is highly appreciated. Data can be accessed here: https://github.com/toribio-florez/Dichm-Data-Testing To the recommendations added by @EdM: Indeed, the glm gives your expected log odds estimates, so we can be more certain about code glitches not being the problem here: H1.glm |z|) ##(Intercept) 1.6491 0.1805 9.135 I also followed your recommendations regarding using different optimizers used by, in this case, glmer , but they do not seem to substantially change the estimates of the model: H1.RI |z|) ##(Intercept) 10.0215 1.0287 9.742 |z|) ##(Intercept) 10.0213 1.0278 9.751 Perhaps for glmer it is necessary to specify two optimizers, let me know if this is the case. Finally, we specified our model following our pre-registered hypotheses regarding the independent effects of our manipulations, but it is true that this does not seem to be the case in our data, since the Ambiguity:Uncertainty interaction term reaches statistical significance: H1.RI |z|) ##(Intercept) 10.8506 1.1500 9.436 But when introducing random slopes, I receive the expected error: Error: number of observations (=651) .
