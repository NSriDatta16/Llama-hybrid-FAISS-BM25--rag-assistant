[site]: datascience
[post_id]: 14902
[parent_id]: 
[tags]: 
Does the performance of neural networks depend on the method used to unroll weights ?

Lets say we have weights(theta1 and theta2) of neural net as: theta1 =[1, 2, 3] theta2= [4, 5, 6] If we unroll these weights into a single dimension array in matlab/octave ,we get: theta = [theta1(:);theta2(:)] %theta = [1, 4, 2, 5, 3, 6] If we unroll these weights in a slight different way , for example consider this python code: theta = np.array(theta1,theta2) theta = theta.ravel() #theta = [1,2,3,4,5,6] I have implemented a neural network using gradient descent in octave and it worked fine but the same implementation is not working in python(% accuracy = 10%). The only thing different the python implementation is the way the weights are unrolled. So, does the performance of neural network really depend on the way the weights are unrolled ?
