[site]: crossvalidated
[post_id]: 307760
[parent_id]: 307685
[tags]: 
Here's the way I see it. Let $x = (x_1, x_2)$. The posterior distribution is proportional to the likelihood times the prior: \begin{equation} p(y|x) \propto p(x|y)\,p(y) . \end{equation} The prior is \begin{equation} p(y) = \textsf{N}(y|0,1) \end{equation} and the likelihood is \begin{equation} p(x|y) = \prod_{i=1}^2 p(x_i|y) = \prod_{i=1}^2 = \textsf{N}(x_i|y,\sigma_i^2) . \end{equation} Therefore, \begin{equation} p(y|x) = \textsf{N}(y|m,s^2) , \end{equation} where \begin{align*} m & = s^2\left(\frac{x_1}{\sigma_1^2} + \frac{x_2}{\sigma_2^2}\right) \\ s^2 &= \left(1 + \frac{1}{\sigma_1^2} + \frac{1}{\sigma_2^2}\right)^{-1} . \end{align*} The posterior mean is a weighted average from all three sources of information: the prior and the two signals. (Since the mean of the prior equals zero, this term does not appear explicitly in $m$.) The weights are determined by the precisions of the sources of information. (Precision is the inverse of variance.)
