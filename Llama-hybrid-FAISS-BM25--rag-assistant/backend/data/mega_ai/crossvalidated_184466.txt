[site]: crossvalidated
[post_id]: 184466
[parent_id]: 
[tags]: 
n-gram language model

At the end of the introduction of A Neural Probabilistic Language Model (Bengio et al. 2003) , the following example is given: Having seen the sentence The cat is walking in the bedroom in the training corpus should help us generalize to make the sentence A dog was running in a room almost as likely. I get the general spirit, but they provide this example right after explaining that a n-gram language model gives the probability of occurrence of a word given some other previous (context) words: $P[w_{t}\mid (w_{t-n+1},...,w_{t-1})]$. So switching to sentence probability without transition is a bit confusing. What do they mean by "making a sentence likely" since the models work at the word level? PS: I can understand that if we see The cat is walking in the bedroom in the training corpus, we can estimate $p_{0}=P[bedroom\mid (cat,walking)]$. It is clear that taking word similarity into account, when generalizing we would want $P[bedroom\mid (dog,running)]$ to be roughly equal to $p_{0}$ (since dog and cat, walking and running, are similar). But this has still to do with word probabilities. And also, what doesn't work here is that in A dog was running in a room , $bedroom$ does not occur, so we only deal with $P[room\mid(dog,running)]$.
