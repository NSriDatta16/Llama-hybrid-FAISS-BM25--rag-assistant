[site]: crossvalidated
[post_id]: 302267
[parent_id]: 302263
[tags]: 
I think you have other problems before you optimize the execution time of the algorithm. K-means clustering will lead to approximately spherical clusters in a 3D space because it minimizes the sum of Euclidean distances towards those cluster centers. Now your application is not in 3D space at all. That in itself wouldn't be a problem. 2D and 3D examples are printed in the textbooks to illustrate the concept. The whole point of machine learning algorithms is to also work in higher dimensional spaces that we cannot easily imagine. k-means will produce approximate hyperspheres in 4D, 5D etc. In your case however, dimensionality is extremely high. If we assume grayscale pictures and you have $1280\times 960=1228800$ dimensions. Three times that for color images. You will fall victim to the curse of dimensionality, more precisely the distance concentration effect . All your distances will look almost the same since Euclidean distance is just not designed for spaces with that high dimensions. Other practical problems arise irrespective of the distance concentration effect: take an image with much intricate detail, for example a tree with many leaves. Now take a similar image, say the very next frame from the video that contains the same tree but a little more to the right in the picture because the camera was sweeping across the tree. Euclidean distance may tell you that those two pictures are very different. Due to the camera sweep, small areas that displayed a leaf shadow in the one picture overlap with areas that display the leaf in the sunlight in the other picture and vice versa. Almost no pixels show the same colors in both images. Intuitively, the images are very similar but not according to euclidean distance. So not only does euclidean distance concentrate your distances, in some cases it misinterprets them. To solve this problem, sophisticated distance measures based for example on artificial neural networks have been designed for images. In some cases, you can be fine with euclidean distances nevertheless. That is when the manifold assumption holds, when your credible candidates of images you might come across do not occupy all of the high dimensional space equally but are in fact concentrated in some separated manifolds of much lower dimension embedded in the high dimensional space. In a space with more than a million dimensions like yours, most images would be pure noise and not representative of credible frames you might find in a video. Therefore, you are not really dealing with this vast high dimensional space but only lower dimensional sub-spaces embedded within it. The manifold assumption can counteract the curse of dimensionality. Though, for it to hold, you usually use smaller images (cropped and scaled) and you know more about them than that they are from a same video. For example if you knew that they all are photos of a limited number of different objects taken in similar circumstances, as you would in image classificatio, you could reasonable make the manifold assumption. In any case, even if the manifold assumption were to hold, there is not much reason to believe those manifolds would be in the form of spheres which would warrant k-means.
