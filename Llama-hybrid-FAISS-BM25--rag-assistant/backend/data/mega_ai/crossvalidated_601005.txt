[site]: crossvalidated
[post_id]: 601005
[parent_id]: 
[tags]: 
Is there a "proper" way to keep strictly nonnegative data nonnegative when performing PCA, despite centering?

I have a question that came up in my research and I would really appreciate some guidance from someone wise in the ways of dimensionality reduction. I have a dataset of matrices that are strictly positive, and I've been interested in finding the core trends in these matrices in order to better visualize and analyze them. In doing so, I found that running sparse PCA on the flattened matrices pulls out a set of components that capture what visually appear to be the key patterns I observe in the data, such that each original matrix looks like a combination of some number of the top six or so components in different proportions. Thinking about the data in terms of these proportions (the PCA projections of the data in a six-dimensional space, which I can also plot as a series of six bars) has been really helpful for us making sense of it. All of the top six components I've found are themselves also strictly nonnegative, which matches with the fact that the input data is nonnegative. The challenge comes with the fact that the PCA projections themselves are not nonnegative, which I take to be due to the centering that is performed by sklearn's SparsePCA. I understand why this is the case, but purely for the purpose of interpretability I find it to be quite unintuitive to present the components with negative values; as is, no contribution of the component leads to a highly negative value (of different magnitudes per component), while an embedding of 0 corresponds to an average contribution of the component. I think the more intuitive way to present the data would be to have zero correspond to no contribution and the larger the number the larger the contribution. My instinct is to "correct" for the centering in some way such that we can keep the projections that we plot strictly positive, but if I do this I want to do so in a way that is statistically "proper" and isn't going to screw up our interpretation. Is this feasible? My understanding is that it doesn't really make sense to do PCA without centering, but one idea would to be subtract off the PCA projection of a vector of all zeroes from each projection. This would reset the "zero" in the projections to correspond to "no contribution from that component." It seems to me that this is just resetting the (somewhat arbitrary) origin in the PCA space to correspond to a vector of all zeroes in the input space. Another idea is to simply project the original, uncentered data, rather than the centered data, which would also give a somewhat similar but slightly different projection of the data where "zero" corresponds to no contribution. My instinct is that maybe the first option does a better job at preserving the structure in the data, but I find myself getting a little confused and don't want to do something that doesn't make statistical sense. I would especially appreciate if anyone knows of a similar example where this has been dealt with or discussed! As a final note, I am also aware that there are other methods of dimensionality reduction beyond PCA that might be better suited to this exact question; I've tried my hand at non-negative matrix factorization with sparsity terms (both with sklearn's NMF module and scratching the surface of the nimfa library) but it didn't produce embeddings that seemed very meaningful - in particular, despite adding sparsity terms, the components themselves were really not very sparse and were all over the place, and thus not usefully interpretable. I'd therefore ideally love to keep the Sparse PCA components that we know are interpretable and conceptually straightforward if we can.
