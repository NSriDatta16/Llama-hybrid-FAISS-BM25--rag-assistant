[site]: crossvalidated
[post_id]: 506332
[parent_id]: 285545
[tags]: 
The answer to the title question is "yes," many machine learning models are capable of learning various logical gates. @KarelMacek is correct that the XOR gate is famously not linearly separable, so logistic regression will not be able to learn that one. But your example is the NAND gate, which is linearly separable, so a logistic regression should be able to learn it. The problem is that sklearn applies L2 regularization by default, which is preventing the model from learning the pattern "strongly" enough. You can see from clf_lr.predict_proba(x_test)[:, 1] that the model has learned that $(1,1)$ is less likely to be a 1, but the regularization has prevented the probability from dropping below $0.5$ . You can reduce the regularization strength, or better set penalty='none' , to recover correct predictions.
