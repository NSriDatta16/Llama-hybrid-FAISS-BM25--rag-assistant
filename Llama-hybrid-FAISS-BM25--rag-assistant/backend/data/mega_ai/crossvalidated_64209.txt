[site]: crossvalidated
[post_id]: 64209
[parent_id]: 64201
[tags]: 
To predict on a continuous variable the decision tree must essentially bin the variable and you are left with terminal nodes each with an average value for your prediction. Some programs like RapidMiner force you to bin the value yourself before predicting and some do it on the fly, behind the scenes. But you could always do something like use your tree to score a dataset that contains X values across the full range of X and derive some kind of equation from the prediction results, Y. But the problem with this is decision trees are not phased by non-linear relationships in your data. So in your scored dataset you might see several different "strata". A better solution might be to think of the decision tree as a tool to identify these "linear strata" beforehand and then build a regression model for each, or the most "important", depending on your domain for example. Hope this helps!
