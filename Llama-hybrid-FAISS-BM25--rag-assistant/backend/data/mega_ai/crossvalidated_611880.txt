[site]: crossvalidated
[post_id]: 611880
[parent_id]: 611659
[tags]: 
Tuning hyperparameters to training data is, for the most part, the same as fitting any other parameters to the training data. It's what the training data is for, but care needs to be taken to not overfit the data. Often a model's hyperparameters directly affect the model's complexity so just taking the hyperparameters that lead to the best results on the training data will likely lead to overfitting. In theory, performing 10-fold cross-validation for the hyperparameter search should alleviate this issue, but depending on exactly how you're doing the cross-validation and measuring the performance you could get better or worse results. The training/validation sets should be chosen first. You want these to be the same for each set of hyperparameters in order to reduce the randomness of choosing those sets as a variable. Otherwise, the chance that for a given set of hyperparameters the cross-validation sets are easier to get good results on for whatever reason may affect the final results. Make sure the model is being retrained for each set of hyperparameters and each training/validation set. You should not train on the entire training set and then evaluate on a random validation set that is a subset of the training set. For a given set of hyperparameters after you evaluate the performance for each training/validation set make sure you average them in some way, don't just take the min error. This again has to do with minimizing the effect of favorable random splits for certain parameters.
