[site]: crossvalidated
[post_id]: 147038
[parent_id]: 147030
[tags]: 
iid is generally used to describe a sample $S=\{x_1,\dots,x_n\}$ of random variables $X_1,\dots,X_n$. Being independent refers to how the (physical) process of collecting the sample was performed and it assures the representative fairness of the sampling. Dependent samples introduce bias into the results, e.g. if on a voting-intention study both, husband and wife, were asked about their voting intention, we would get, high likely, dependent answers; thus, miss-representing the population. From a computational point of view, independency significantly simplifies operations due to the factorization $P(X_1\leq x_1,\dots,X_n\leq x_n)=P(X_1\leq x_1)\cdots P(X_n\leq x_n)$, avoiding a possibly curse on dimensionality . For instance, consider the painful computation of a maximum-likelihood estimator based on a dependent sample. However, it is not always the case that we are interested in iid samples. Times series analysis is based on the dependency among the random variables, as many other stochastic processes are. Dependence and conditional dependence structures are at the core of models such as markov chain and, more generally, graphical models.
