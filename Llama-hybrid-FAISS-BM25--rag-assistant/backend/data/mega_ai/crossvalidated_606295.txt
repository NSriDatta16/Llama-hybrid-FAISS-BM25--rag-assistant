[site]: crossvalidated
[post_id]: 606295
[parent_id]: 
[tags]: 
Deriving vectorized back propagation

I'm trying to derive vectorized backpropagation from mostly first principles, but I'm having trouble marrying how this paper explains backpropagation with the derivative of a loss function with respect to a matrix. As the paper states (and I believe I understand), you can view a neural network as a composition of functions. Assuming we have some arbitrary composition of functions $$ f = f_n(f_{n-1}(...f_1(x))) $$ such that $f: R^m \rightarrow R^n$ then we can write the derivative of $f$ with respect to some input $x \in R^m$ as $$ \frac{df}{dx} = \frac{df_n}{df_{n-1}} \frac{df_{n-1}}{df_{n-2}} \dots \frac{df_1}{dx} $$ Forward mode differentiation then consists of going right to left and computing the vector-matrix product at each step, starting with $\frac{d_1}{dx} \cdot x$ . The end result is that we end up computing $$ J_f \cdot x $$ Since our neural network produces a scalar loss (treating the loss as part of the network) we really have a function $f : R^n \rightarrow R$ , and as the paper explains in this case forward propagation requires $n$ iterations to compute the derivative of the loss with respect to every input. And since neural networks have a lot of inputs this is slow. So instead we turn to back propagation, which computes $$ J_f^T \cdot x$$ instead of $$ J_f \cdot x $$ This is much faster for our network, since we have few outputs and a lot of inputs (ie. $m \gg n$ ) This is where my first question shows up. I believe that $$ J_f^T \cdot x = (\frac{df}{dx} = \frac{df}{df_{n-1}} \frac{df_{n-1}}{df_{n-2}} \dots \frac{df_1}{dx})^T \cdot x $$ So we can write $J_f^T$ as $$ \frac{df_1}{df_x}^T \frac{df_2}{df_1}^T \dots \frac{df_n}{df_{n-1}}^T $$ I believe this is where the "backpropagation" part comes from since now we can evaluate this from right to left, which corresponds to going "backward" through the network. Assuming my understanding of back propagation is correct my next question is the following. We don't actually want to compute derivatives with respect to a vector of inputs, we want to compute it with respect to a matrix of weights. If we just assume that my network has no non-linearities (for sake of notational simplicity) then each layer is $$ Z = WX + B $$ Where $Z, W, X, B$ are all matrices, $W$ is a matrix of weights, and $X$ is a matrix where each column is an input vector Using numerator notation for the derivative of a scalar with respect to a matrix (and using the same logic as this standford derivation and this U-Mich slidedeck, slide 108 I get $$ \frac{dL}{dW} = \frac{dL}{dZ} \frac{dZ}{dW} = \begin{bmatrix} \frac{dL}{dw_{11}} \frac{dL}{dw_{21}} \dots \frac{dL}{dw_{n1}} \\ \vdots \\ \frac{dL}{dw_{1n}} \frac{dL}{dw_{2n}} \dots \frac{dL}{dw_{nn}} \\ \end{bmatrix} $$ Solving for these gives me $$ \frac{dL}{dW} = X (\frac{dL}{dZ})^T $$ $$ \frac{dL}{dX} = (\frac{dL}{dZ}^T)W $$ Taking the derivative of my loss function with respect to $W_1$ , and using the above definition of $\frac{dL}{dW}$ and $\frac{dL}{dX}$ I get $$ \frac{dL}{dW_1} = X(((\frac{dL}{dZ}^T W_n)^T W_{n-1})^T W_{n-2} \dots)^T $$ Since back propagation computes the transpose of $\frac{dL}{dW_1}$ I get $$ (X(((\frac{dL}{dZ}^T W_n)^T W_{n-1})^T W_{n-2} \dots)^T)^T $$ $$ (((\frac{dL}{dZ}^T W_n)^T W_{n-1})^T W_{n-2} \dots)X^T $$ But if I look at the Gradients for vectorized operations for the same setup of $Z = WX + B$ their code would compute $$ \frac{dL}{dW_1} = (W_n^T W_{n-1}^T \dots W_2^T)X^T $$ Which isn't the same thing that I get. I assume I'm doing something wrong, but I'm not exactly sure where. So I'd like to know where in my derivation the problem is, and how to fix it so I end up $$ \frac{dL}{dW_1} = (W_n^T W_{n-1}^T \dots W_2^T)X^T $$
