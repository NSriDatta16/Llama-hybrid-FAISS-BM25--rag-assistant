[site]: crossvalidated
[post_id]: 448821
[parent_id]: 
[tags]: 
Hessian Matrix for MultiClass Softmax in Gradient Boosting (XGBoost): $2p_i(1-p_i)$

In the context of MultiClass Softmax, for a particular training instance, label and prediction $y, p \in \mathbb{R}^K$ (K categories). The hessian matrix for Multiclass SoftMax with K categories is a $K \times K$ diagonal matrix with diagonal element $p_i(1-p_i)$ . In the implementation of XGBoost, however, the hessian is $2p_i(1-p_i)$ . Here is the source code XGBoost MultiClass Objective . I would appreciate if someone could clarify why the "2" exists in hessian. Here is a list of past issues raised on GitHub: here , here , here , and here . The background on this topic seem to be "Majorization Minimization (MM)". Here is a list of relevant academic literature: here , and Monotonicity of Quadratic Approximation . However the first paper by Chen et. al is on CRF, and after reading am still confused why the Hessian is $2 p_i(1-p_i)$ . My understanding is that, as long as we have a “majorizing” hessian, we are guaranteed to converge when optimizing quadratic approximation. My guess is that, since Hessian is diagonal, with positive entries (hence positive semi-definite), we could multiply the original hessian by any number greater than one, hence “2” is a choice; or like that in Poisson regression ( XGBoost Poisson ) exp(p+max_delta_step) . Is that the case? What is the acceptable range for this constant and how do we choose this constant? Since $p_i(1-p_i)$ is bounded above by 0.25, can we just set the Hessian to be 0.25 regardless of pi? How should we handle the case where hessian is not positive definite? The principal reason is we do not want Hessian to be zero. However, when $p_i(1-p_i)$ is very small, e.g. 10e-10 , a factor of 2 doesn’t seem to help. For a twice differentiable convex loss function, I am very confused on the choice of constant to multiply by the hessian matrix. This could serve as a future guideline for implementing hessian for custom loss function in Gradient Boosting. It would be very helpful to have some kind of formal derivation but intuitive explanation helps too.
