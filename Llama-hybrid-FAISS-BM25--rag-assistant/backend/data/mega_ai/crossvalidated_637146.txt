[site]: crossvalidated
[post_id]: 637146
[parent_id]: 628497
[tags]: 
One dimensional simple model A simple model that illustrates a bit what is going one is a single parameter model $$X_i \sim N(\theta,1) \quad \text{with the constraint $\theta \geq 0$}$$ For a sample of size $n$ the sufficient statistic is $ T=\bar{X}$ (and we could use this as an unconstrained estimate $\hat{\theta}_{unconstrained} = T$ ) the constrained maximum likelihood estimate is $\hat{\theta}_{constrained} = \max(0,\bar{X})$ . Histograms for the distributions for these, when $n=5$ and $\theta = 0.5$ , look like below. The distribution of the constrained estimate is a rectified Gaussian distribution . That peak at $mle=0$ is a point mass and represents the tail from $T that is now closer to the true value $\theta$ . In this case the boundaries are making the mean squared error smaller. Variance, bias, and combining multiple experiments The sufficient statistic $T$ has no bias but the variance is large and the mean squared error will be larger. $$\text{mean}(T) = \theta\\ \text{var}(T) = \frac{1}{n}\\ MSE(T) = \frac{1}{n}$$ The constrained maximum likelihood estimate $\hat{\theta}_{constrained}$ has a bias, but due to a smaller variance the mean squared error will be smaller. (for derivations see: Expectation and Variance of Gaussian going through Rectified Linear or Sigmoid function ) $$\text{mean}(\hat{\theta}_{constrained}) = \theta (1-p) + \frac{\phi(z)}{\sqrt{n}}\\ \text{var}(\hat{\theta}_{constrained}) = \theta^2 p(1-p) + \frac{\theta}{\sqrt{n}} \phi(z)(2p-1) + \frac{1}{n} (1-p-\phi(z)^2) \\ MSE(\hat{\theta}_{constrained}) = \frac{1}{n} (1-p) - \frac{\theta}{\sqrt{n}} \phi(z) + \theta^2 p$$ Where $z = -\theta \sqrt{n}$ and $p = \Phi(z)$ and $\phi$ and $\Phi$ are the standard Gaussian pdf and cdf. The issue/consideration for the one dimensional simple model is (1) can yield unrealistic estimates, but when applied to a large number of datasets might provide better average estimates on the whole; When we have multiple estimates then the variance will reduce and the error of the average of estimates will approach the bias. So when we use averages of estimates then using biased estimates can be a disadvantage, albeit these estimates having a smaller error when compared individually. Depending on how the model is used we may prefer methods 1 or 2. If we are not using averages then method 2 is better because the mean squared error is larger. If we will use some averaging of several estimates (and we are not able to use all the data at once making a single estimate instead of averaging multiple estimates) then model 1 might be better. It will depend on the ratio between variance and bias. Method 2 might give a better estimate, but that estimate might not be the best statistic for further analysis. In the example above this has been made more salient by method 1 being equivalent to the sufficient statistic, and method 2 is effectively loosing information. The same principle might be true for other methods (like regularised regression) where some estimate has 'improved' properties, but results in a loss of information.
