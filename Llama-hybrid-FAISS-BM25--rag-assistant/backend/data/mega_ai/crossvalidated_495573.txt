[site]: crossvalidated
[post_id]: 495573
[parent_id]: 
[tags]: 
how can i handle class imbalance from different data sets?

I have an imbalanced problem and my datasets are from 4 different countries; 1 of these countries has % of cancer patients at 2%, the rest are at 1%. When i ran a training job using xgboost it seemed to be learning more from the country with 2%. It wasn't learning anything from the countries where we have less. The sampling technique used was to 'stratify by country' where the % of cancer patients was kept the same per country. Then this data was combined as the final training set. how else can i deal with getting a training set? i am thinking of doing the following instead: over sample minority class in the 3 countries with 1% to take it up to 2% then for each of the 3 countries under sample majority class to make it equal to minority for the country with 2%, undersample to make equal with non cancer patients numbers combine all 4 contries data for final training set does the above seem ok ? how else can i do this, or should i combine all datasets together irrespective of country they are from and oversample and under sample? # instantiating over and under sampler X= all_country_data y = label over = RandomOverSampler(sampling_strategy=0.5) under = RandomUnderSampler(sampling_strategy=0.8) # first performing oversampling to minority class X_over, y_over = over.fit_resample(X, y) print(f"Oversampled: {Counter(y_over)}") Oversampled: Counter({0: 9844, 1: 4922}) # now to comine under sampling X_combined_sampling, y_combined_sampling = under.fit_resample(X_over, y_over) print(f"Combined Random Sampling: {Counter(y_combined_sampling)}") Combined Random Sampling: Counter({0: 6152, 1: 4922})
