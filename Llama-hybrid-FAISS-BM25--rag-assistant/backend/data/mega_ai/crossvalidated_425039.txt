[site]: crossvalidated
[post_id]: 425039
[parent_id]: 
[tags]: 
Regression penalisation to keep correlated variables

I am an avid user of logistic regression and more specifically of penalised logistic regression. With standard penalisation (Ridge, Lasso, Elasticnet) the goal is to avoid overfitting and translates with a reduction of the number of variables used. Generally it will remove strongly correlated variables. However I am in a case where I want to keep all my variable mostly unchanged and want to make sure that similar variables play a similar role in the model. Basically if I have two explanatory variables $X_1$ and $X_2$ such that $X_1 = X_2$ for all instance, I should have $\beta_1=\beta_2$ in my calibrated model. I am unsure this is enough to fully qualify what I want to achieve, but I hope you will get the idea. Is there any framework to achieve that trough a specific "penalisation" term ?
