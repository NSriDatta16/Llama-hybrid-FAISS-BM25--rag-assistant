[site]: crossvalidated
[post_id]: 253589
[parent_id]: 137573
[tags]: 
Already asked here Decision trees recursively partitions the input feature/attribute space to reduce the entropy/impurity score. The output on an unseen sample is obtained by finding the leaf node/ decision region containing the sample and calculating the majority class/label withing this leaf node. Here and in the original CART trees as well as the implementation in scikit-learn do the take into account the correlation between the attribute variables. While we do know that this is taken care of by random subspace selection in random forests, which are an ensemble of decision trees. This consists of performing randomly subsampling the attributes without replacement.
