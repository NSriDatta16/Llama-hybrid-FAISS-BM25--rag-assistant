[site]: crossvalidated
[post_id]: 70039
[parent_id]: 70024
[tags]: 
Each of the principal components projects the whole original feature space onto several dimensions, which I will call the latent features. The more an original feature contributes to a latent feature, the more important it is for that feature. Thus, look at the absolute values of the Eigenvectors' components corresponding to the $k$ largest Eigenvalues. The larger they are, the more a specific feature contributes to that principal component. Mind, however, that these will typically be dense. If you want to find some kind of minimal feature space that explains most of the data, you might be interested in sparse pca.
