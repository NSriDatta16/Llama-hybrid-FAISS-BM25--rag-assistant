[site]: crossvalidated
[post_id]: 389244
[parent_id]: 389199
[tags]: 
Question 1: when θ2 is being updated in the first "update step" to θ̂ 2, does the calculation of hθ(x(i)) involve using the old value of θ1 or the updated value of that parameter? You use the old value. You calculate all the gradients with respect to old values, and update the parameters. Commonly, your $\theta_1$ and $\theta_2$ would be in a vector to speed up the calculations, and you calculate the gradient vector consisting of the derivatives for each variable. That way, you wouldn't be exposed to this. Always remember your parameters as a vector, i.e. $\Theta=[\theta_j,..]$ . This time the first "update" step looks like I'm afraid that's not the first update step, because $x^{(r)}$ and $x^{(s)}$ are different samples. In SGD, you choose one sample, e.g. $x^{(r)}$ , and you update all your parameters for that sample (together as above), then proceed to next sample. The first equation you wrote updates $\theta_1$ using $x^{(r)}$ , and the second one updates $\theta_2$ using $x^{(s)}$ . Your Q2 iterates over your incorrect equations of SGD in Q1. When you choose a sample, random or not, you update all the system parameters, such that every member of the system learns something from that sample (unless you're applying some kind of dropout mechanism just like in neural networks). I also advise you to check out the Example section here .
