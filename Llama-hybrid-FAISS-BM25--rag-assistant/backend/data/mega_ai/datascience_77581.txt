[site]: datascience
[post_id]: 77581
[parent_id]: 77566
[tags]: 
So, the question is asking how to model the following problem: predicting question taxonomy, given the question. The first thing that leaps into my mind is to use an encoder-decoder architecture. For this, you would use an encoder to encode the question as a sequence of words/tokens. Here we could use a sequential model, such as an RNN or LSTM. This encoder then encodes the question into "hidden representation". Then the hidden representation is decoded, using a normal feedforward NN with a final 3-node softmax layer, which creates a probability distribution over question taxonomies. For the input, you would convert the tokens into word embeddings and then feed them in one-by-one for each question. For the output, you would just obtain the index with the highest probability as the class label. An SVM is a good idea, however this treats the question as a bag of words (i.e. word order does not matter). So any old random question such as "how what learn learn this that" (even though hugely incoherent") can still be classified.
