[site]: datascience
[post_id]: 121436
[parent_id]: 
[tags]: 
Multiagent observations encoding - LSTM or Transformer?

In multi agent learning problems where agents can join or leave, to address the issue of varying observation space I am trying to build a latent representation either using LSTM or transformer which would then passed as fixed size input to policy training. My idea is to generate these varying size observations by simulation and build an autoencoder out of it. Since memory/context is important to nudge the policy optimisation in the right direction what would you recommend? - LSTM or transformers?
