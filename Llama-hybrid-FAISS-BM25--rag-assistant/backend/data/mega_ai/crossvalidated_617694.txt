[site]: crossvalidated
[post_id]: 617694
[parent_id]: 
[tags]: 
Is it a must for a Null Hypothesis Significance Test (NHST) to be fixed-horizon/fixed-sample?

Null Hypothesis Significance Test (NHST) is a class of statistical tests that comes from blending procedures from Fisher’s Significance Test and Neyman & Pearson’s Hypothesis Test (see, e.g., Lew (2020) ). Many have derided its mere existence (see Cohen (1994) ), but it is nonetheless commonly applied across many fields. Fixed-horizon/fixed-sample tests are statistical tests that require the person to determine and commit to the number of samples before starting the experiment. The question: Is an NHST, by definition, a fixed-horizon/fixed-sample test? Alternatively, are the class boundaries (especially for NHST) well-defined enough for this question to be answerable, or perhaps the underlying philosophical considerations are orthogonal and should not be directly compared? I am looking for references that address this either way (or outright state it is inconclusive). My current understanding: There is a philosophical conflict on NHST's origins regarding sample size determination. As a decision procedure, Neyman & Pearson’s hypothesis test requires one to decide on the sample size before starting the experiment. Fisher's significance test, however, does not require pre-determining the sample size and potentially allows one to wait for more data to arrive (as no decision is involved). It is not 100% clear which of the conflicting stance the NHST hybrid has inherited (or, more precisely, whether the Neyman-Pearson approach has completely wiped out the Fisherian approach on the sample size front). Views in my field (digital experimentation) seems to agree with such implication, but it is not definitive. Existing works on alternatives to NHSTs in the context of continuous monitoring (a.k.a. adaptive/optional stopping or optionally increasing the sample size) have made comments of various precision on this matter: NHST is valid for fixed horizon test. But it is known to underestimate Type-I error when continuous monitoring is used. --- Deng, Lu & Chen (2016) However, this practice of unplanned multiple testing [(adaptively increasing the sample size)] is not allowed in the classical NHST paradigm, as it increases Type I error rates. --- Schönbrodt et al. (2017) , who also cited Armitage, McPherson, & Rowe (1969) The validity of NHST requires that the sample size is fixed in advance, which is often violated in practice. --- Yu, Lu & Song (2020) The main issue is that an experiment following NHST requires a fixed sample size and therefore a fixed time window, which does not allow repeated significant testing, or “continuous monitoring”. --- Ju et al. (2019) Some also used the term Classical/Traditional NHST , perhaps in an attempt to perform more-detailed differentiation. Johari et al. (2017) refer to common test procedures in Web A/B testing as "standard frequentist parameter testing," leaving one wondering whether it is equivalent to NHST at all. Then there are sequential tests. Sequential tests are tests with sample sizes not determined in advance. Many sequential tests bear the hallmark of an NHST --- existence of competing hypotheses, a set of decision rules around the test statistic, p-values, and the lack of requirement to specify a prior. However, it is not universally clear whether (frequentist) sequential tests as part of NHST. If they are, then we have a clear counterexample. Deng, Lu & Chen (2016) regarded sequential testing as a "different theory to allow continuous monitoring in NHST framework", while Schönbrodt et al. (2017) call group sequential tests an "extension of the NHST paradigm". The irony that these quotes may contradict the previous quotes from the same authors is not lost on me. There are other works on sequential analysis (usually the Bayesian ones) which contrast themselves against NHST, though I see that more as a frequentist vs Bayesian contrast rather than the one this question seeks.
