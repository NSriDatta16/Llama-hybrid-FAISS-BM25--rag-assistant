[site]: stackoverflow
[post_id]: 3305112
[parent_id]: 1972396
[tags]: 
I agree with the OP: sometimes you just plain need to pay some attention to efficiency. I don't think the example of providing an API is the best, because that certainly calls for leaning toward safety and simplicity over efficiency. However, a simple example is when processing large numbers of huge binary files that have zillions of records in them, such as when writing a parser. Without using a mechanism such as System.ArraySegment, the parser becomes a big memory hog, and is greatly slowed down by creating a zillion new data elements, copying all the memory over, and fragmenting the heck out of the heap. It's a very real performance issue. I write these kinds of parsers all the time for telecommunications stuff which generate millions of records per day in each of several categories from each of many switches with variable length binary structures that need to be parsed into databases. Using the System.ArraySegment mechanism versus creating new structure copies for each record tremendously speeds up the parsing, and greatly reduces the peak memory consumption of the parser. These are very real advantages because the servers run multiple parsers, run them frequently, and speed and memory conservation = very real cost savings in not having to have so many processors dedicated to the parsing. System.Array segment is very easy to use. Here's a simple example of providing a base way to track the individual records in a typical big binary file full of records with a fixed length header and a variable length record size (obvious exception control deleted): public struct MyRecord { ArraySegment header; ArraySegment data; } public class Parser { const int HEADER_SIZE = 10; const int HDR_OFS_REC_TYPE = 0; const int HDR_OFS_REC_LEN = 4; byte[] m_fileData; List records = new List (); bool Parse(FileStream fs) { int fileLen = (int)fs.FileLength; m_fileData = new byte[fileLen]; fs.Read(m_fileData, 0, fileLen); fs.Close(); fs.Dispose(); int offset = 0; while (offset + HEADER_SIZE fileLen) { /*puke as file has odd bytes at end*/} MyRecord rec = new MyRecord(); rec.header = new ArraySegment(m_fileData, offset, HEADER_SIZE); rec.data = new ArraySegment(m_fileData, offset + HEADER_SIZE, varDataLen); records.Add(rec); offset += HEADER_SIZE + varDataLen; } } } The above example gives you a list with ArraySegments for each record in the file while leaving all the actual data in place in one big array per file. The only overhead are the two array segments in the MyRecord struct per record. When processing the records, you have the MyRecord.header.Array and MyRecord.data.Array properties which allow you to operate on the elements in each record as if they were their own byte[] copies.
