[site]: crossvalidated
[post_id]: 488939
[parent_id]: 
[tags]: 
Does up-sampling lead to lots of false positives in production?

Say we have a dataset with a binary outcome variable that takes the positive case ( outcome = 1 ) roughly 20% of the time. Often, we would modify the training set by down-sampling the 0's such that the training set has something like a 50/50 split in the outcomes between 0's and 1's. When this model is in production, though, wouldn't it experience a lot of false positives? Its perception of the "baseline" or "average" (e.g. intercept in logistic regression) seems to be too high. It seems to me like it would end up predicting too many 1's. Am I off base here?
