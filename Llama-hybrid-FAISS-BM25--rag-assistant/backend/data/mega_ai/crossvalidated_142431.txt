[site]: crossvalidated
[post_id]: 142431
[parent_id]: 142425
[tags]: 
Ensemble classifiers typically use far larger numbers of models in the ensemble, say 100s to 1000s. You then tabulate the 100 or 1000 predictions against the classes as voting score. This is where e.g. the majority vote takes place. Somewhat contrary to @ChisLucian's answer I'd put no particular emphasis on tie-breaking. In my experience it is more practical to have the classifier refusing to predict at all if the voting is not decided enough. Your 1 : 1 : 1 situation (as opposed to, say 2 : 1 : 0 or 0 : 3 : 0) basically means that the models are not sure: the votes are all over the place. So I'd answer with "not sure - no prediction possible". If you happen to use a classifier that inherently predicts scores or even posterior probabilities (e.g. logistic regression), you can use different aggregation rules instead of the majority vote, e.g. use the median or average predicted score for each class and then go for the highest. The "not sure" option can be implemented there as well by requiring a threshold. edited question: how does SVM produce a score output? Does this paper answer your question: http://www.csie.ntu.edu.tw/~cjlin/papers/svmprob/svmprob.pdf (no time right now to read and summarize - hopefully later) IIRC e.g. libsv -b fits a logistic regression on top of the SVM (postprocessing?)
