[site]: datascience
[post_id]: 67710
[parent_id]: 
[tags]: 
Do you need to perform variables reduction for tree-based models?

I know for methods and linear regression, GLM, Logistic regression, we typically run through a lot of variable reduction methods, i.e, forward/backward/stepwise, univariate analysis; variable importance through RF, GBM. I know include too many features in linear model lead to overfitting, but I don't know if this is true for tree-based models. I noticed this while I was building a GLM for a company project and through it might be a good idea to build a GBM and compare both. Does tree-based models like RF and GMB, Catboost require you to drop variables in order to achieve a good model? - Re-build model many times eventually with a small model with fewer variables.
