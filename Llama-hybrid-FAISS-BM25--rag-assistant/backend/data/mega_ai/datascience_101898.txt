[site]: datascience
[post_id]: 101898
[parent_id]: 
[tags]: 
Jacobian calculation: Partial derivatives of network outputs respect to inner layers in feedforward neural network

I'm having hard times in deriving a Jacobian (derivative of final network outputs respect to all network parameters !) for a neural network that you see in the picture below. It's about two level network, that has a hyperbolic tangent sigmoid as a activation function in first layer and a pure linear activation in second layer. My exact topology is: 4 inputs, 11 neurons on first layer, and 2 neurons on last layer (hence 2 outputs in my network) Regarding the jacobian calculation: I'm able to derive the partial derivatives for both outputs respect to all parameters from layer 2 (layer 2 weights and biases). However calculating the partial derivative of the both outputs respect to layer 1 parameters seems to be harder due to nested function form. Chain rule has something to do with these partial derivatives. Could anyone help me to derive the analytic formula for the outputs respect to first layer parameters? What I know already: The pure linear activation function (f2 in figure) can be forgotten because it's basically only a feedthrough function (input = output). The derivative of the hyperbolic tangent sigmoid function is: 1-f1^2
