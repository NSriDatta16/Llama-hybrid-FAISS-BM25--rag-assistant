[site]: crossvalidated
[post_id]: 519530
[parent_id]: 519467
[tags]: 
I have gone through this before as well so I'll share a few learnings we had upgrading from traditional stat time series to boosted trees: Feeding lagged target features to xgboost/lightgbm typically didn't work and added a lot of complexity due to needing to predict multiple times. Calendar features such as week number/ month number could cause a lot of overfitting especially if you have shorter training history. So some things that helped us a lot: We are forecasting at multiple levels such as product/country so doing standard scaling at each level helped tremendously. At each product/country or whatever you are forecasting for pass simple model outputs as features such as the mean, maybe a more localized measure of mean/ ets, a simple measure for seasonality, a simple linear trend, a more local measure for trend/ ets. Also pass some time series features as well such as average gradient, skewness, entropy, lumpiness, kpss, stability, etc. Hyndman and the team which submitted an ensemble for the M4 competition did a lot of great work here. We also did several runs to have a ton of data to learn from using both the fitted results from those simple models as well as forecast results from them. Category/ID features especially helped for low history items. You can supplement your data further with some simulated data just be careful that the model won't learn bad things! Overall this approach was a big improvement over traditional stat and other ML such as LSTM, but definitely takes some work.
