[site]: crossvalidated
[post_id]: 613870
[parent_id]: 613857
[tags]: 
Within the deep learning and adjacent communities, the biggest advantage of SGD is that there is an implementation that is an order of complexity faster than per iteration than standard gradient descent. The basic idea is that taking a sub-sample of data (potentially even 1 row!), you can show that in many cases, you have the expected gradient plus unbiased noise. This is a version of SGD, and it's also one in which the computational cost per iteration does not scale with the sample size of your data. For problems in which there are billions (or trillions!) of examples, clearly this is a make-or-break feature. The fact that SGD may or may not dodge local minima is still of interest, but as an example, a version of SGD that required computation of exact gradient first with noise added on after would be a non-starter for most use cases, even if it were better at dodging local modes. However, there are methods like Adam that are no more expensive than "vanilla SGD" but tame the step sizes to help stabilize algorithm. In a sense, you could think of these methods as one in which we alter the form of the noise in order to have more desirable properties of the algorithm. Likewise, the general class of methods with momentum components are believed to help "blow past" local minimum.
