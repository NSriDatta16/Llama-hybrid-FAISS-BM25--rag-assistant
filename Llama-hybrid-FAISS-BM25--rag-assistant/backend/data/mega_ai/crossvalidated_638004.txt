[site]: crossvalidated
[post_id]: 638004
[parent_id]: 637995
[tags]: 
The argument in the provided text snippet is that a prior over your parameter reduces model complexity. A complex model is one, in which many if not all parameters are non-zero; in contrast, a sparse model has many zero parameters. When estimating parameters, we seek a parameter vector that minimizes the loss function. By adding $-log(p(\theta))$ to the loss function, we add a force that pulls the parameter vector towards points with higher likelihood of the prior when finding a solution (because higher log-likelihood translates to a smaller penalty). In order to force the model to a sparse solution, the prior need to be centered at zero, such that parameter estimates are actually pulled towards zero and thus are basically dropped from the model. In a lot of situations, simple models make (on average) more robust prediction
