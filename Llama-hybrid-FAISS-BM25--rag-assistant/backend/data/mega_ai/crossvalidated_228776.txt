[site]: crossvalidated
[post_id]: 228776
[parent_id]: 
[tags]: 
Assessing forecasting methods when forecasting errors and the forecasts themselves have costs

I'm comparing various methods of making forecasts. The forecasts are mostly of continuous quantities, and then later on I get to see the true value. So one thing I can calculate is the MSE of each forecast, and then average across forecasts. A simple method of assessing the forecasting methods is to look at which one has the lowest mean MSE. However, for my purposes that method is a bit flawed. The reason why is that each forecast gets generated by a individual person, who then needs to be paid for each forecast they make. Some of the methods require fewer total forecasts than others. In real life this means that they'd be cheaper to run. Thus I need some sort of cost function to weight the costs of particular forecasting methods both in terms of the how expensive they are to run (how many forecasts we need to input) and in terms of their ultimate accuracy. Ultimately whatever cost function I apply will be a bit arbitrary, because when the methods are applied in real life the costs of an error will vary from context to context, and the costs of paying each forecaster will vary. Is there some sort of cost function that is appropriate on some abstract grounds, or that is standardly applied in contexts like this? Edit: The textbook by Diebold (1998) supports @Aksakal's assertion that MSE is the most commonly used loss function with respect to forecasting error. Diebold, F. X. (1998). Elements of forecasting. South-Western College Publ.. Chicago
