[site]: crossvalidated
[post_id]: 639918
[parent_id]: 
[tags]: 
Best way to impute missing values in a time series

I have a camera that detects every time it views a car. Each detection is recorded in a database. I then simulate this behavior as a time series by doing an each hour count of the records. The problem is that for some time intervals I have no records and it is impossible to know if it is because no cars have passed or because of some bug in the system. So I have two options: assume that no cars have passed and give zero value to those intervals or assume errors in the system and find a function that best fits the series to impute missing values. The question is which of the two options will be better and how do I know? thanks in advance
