[site]: crossvalidated
[post_id]: 484818
[parent_id]: 484654
[tags]: 
As Robert Long pointed out in a comment, the critical thing is whether you evaluated power adequately before you did the study, and your response to his comment indicates that you did. As you have collected the data, you should analyze the results in the way that you intended to when you did the power analysis. If you are worried about overfitting, the usual problem with too many predictors, you could see whether the results on multiple bootstrap samples of the data provide substantially different results than what you got with the full data set. That's a way to estimate the "optimism" in your original model. Rules of thumb are just that: rules of thumb, attempts to point you to an adequate size of study to allow detection of effects of interest without overfitting. As you asked about this rule of thumb, lets' see how it might apply in practice here. Your "rule of thumb is 1 predictor per 10 observations ." (Emphasis added.) You have 180 observations of your outcome variable, so things are not so dire as you might fear, even based on your rule of thumb. The number of predictors is essentially the number of degrees of freedom (df) you'll be using up with them, so you have to be careful with categorical predictors: they count as 1 predictor for every level beyond the first. In terms of predictors you have task (1 predictor), time (2 predictors if you treat it as categorical), text length (at least 1, more if you fit it with a spline or similar flexible model), and 1 less than the number of your linguistic predictors. You don't say how many of those there are, let's say 4 total for 3 effective predictors. That's 7 to there. Your design, however, sounds like your interests are in interactions: "whether the relationship between the linguistic predictors (fixed factors) and the proficiency scores (dependent variable) varied depending on the speaking task-type (monologic/dialogic)" and "... varied depending on the time." In applying that rule of thumb, each interaction term also counts as a predictor. So if you have 4 linguistic predictors (3 df) it has 3 interaction predictors with task and 6 with time. Add those 9 to the previous 7 and you have 16 total predictors to here. What about the random effects? As this is just a rule of thumb anyway, why not count one for each random effect that you are modeling? So that depends on how many random effects you are modeling: intercepts, slopes with respect to task, slopes with respect to time, slopes with respect to text length... If you are only modeling the variance of the random intercepts, then you have on the order of 17 predictors to go along with your 180 observations.
