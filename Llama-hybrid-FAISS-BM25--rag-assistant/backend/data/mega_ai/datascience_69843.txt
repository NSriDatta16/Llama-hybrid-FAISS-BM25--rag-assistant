[site]: datascience
[post_id]: 69843
[parent_id]: 24921
[tags]: 
The DQN uses experience replay to break correlations between sequential experiences. It is viewed that for every state, the next state is going to be affected by the current action, therefore, taking experiences sequentially would result in instabilities due to internal correlations between experiences. An experience consists of a state, an action, a reward and the next state; all that is needed to learn, to the very least in temporal difference fashion. As such, experience replay enables us to combine Monte Carlo (which uses full episodes) and temporal difference (which uses single experiences) in one but more robust scheme. You realize that what is needed is not the size of a single reward but an indication as to whether the agent is on the right track, which is best given by an average of several experiences. Also, using same network parameters to get both the prediction and the target Q-values is like updating a guess with another guess, which is like "a dog chasing its own tail." The use of a target network which is just a clone (stale copy) of the prediction network would help break this other kind of correlation between targets and prediction. While the prediction network is updated using the experiences, the target network parameters are periodically updated, say after every so many updates of the prediction network, its weights are copied to the target network.
