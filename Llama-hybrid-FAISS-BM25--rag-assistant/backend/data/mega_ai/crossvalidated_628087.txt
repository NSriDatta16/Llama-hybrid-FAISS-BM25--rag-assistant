[site]: crossvalidated
[post_id]: 628087
[parent_id]: 627229
[tags]: 
I'm not sure what you mean by "equivalent", but the output is certainly different. In the attention function, when computing the value of the scores variable the reduction happening in the matmul operation is over a subset of the components of the query and key vectors. For a single head this is the whole set of components, but for a multihead, these subsets form a a nontrivial partition of the set of components. From there on, all the results you'd get would be different. You can see this difference when checking the shape of the attention weights.
