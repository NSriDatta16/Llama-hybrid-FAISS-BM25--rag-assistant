[site]: crossvalidated
[post_id]: 609934
[parent_id]: 609829
[tags]: 
Lots of what we're trying to get at with dimensionality reduction, whether linear or nonlinear, is abstraction of the data away from a raw space and towards a manifold or embedding in which complex phenomena can be described with fewer, more meaningful parameters. PCA is arguably the simplest of these techniques; it will help you reduce correlated variables to a single common dimension. Implicit linear correlations in your dataset aside, lots of things in the real world are expressed as linear combinations of a set of latent variables. For example, you can use SVD (a generalization of PCA) in natural language because documents that contain words like "cat" are also likely to contain words like "pet" in rough proportion. So in a bag-of-words or tf-idf model, the correlation gets called out. Aside from dimensionality reduction (or rather why it works for dimensionality reduction), PCA/SVD attempt to maximally explain the overall variance of the data using fewer components. You can think of this as "stretching" each component apart from the others as much as possible while keeping the component itself coherent (and this is actually explicitly how more advanced dimensionality reduction techniques such as maximum variance unfolding work). That makes it very useful at teasing apart how important each linear factor is within a dataset. That turns out to be particularly useful in recommendation engines over graphs, like the one that won the Netflix prize. If PCA isn't useful today, it isn't because it inherently lacks usefulness, but because we have more powerful nonlinear techniques that have supplanted it.
