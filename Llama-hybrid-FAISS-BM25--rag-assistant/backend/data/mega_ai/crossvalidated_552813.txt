[site]: crossvalidated
[post_id]: 552813
[parent_id]: 552809
[tags]: 
$p(x_{new}|x)$ is the posterior predictive distribution. It can be computed as an integral over the parameter $\theta$ as stated in your question, in fact this is just an application of marginal probability density functions ( https://en.wikipedia.org/wiki/Marginal_distribution ): $p(x_{new}|x)=\int_\theta p(x_{new},\theta|x) d\theta$ . $p(x_{new})$ is the prior predictive distribution. $p(x_{new}|x)$ is never equal to $p(x_{new})$ except in trivial cases, because $x_{new}$ is not independent of $x$ . When the slides mention independence of $x$ and $x_{new}$ , they actually mean independence conditional on $\theta$ i.e. the only 'connection' between $x$ and $x_{new}$ is via the parameter $\theta$ . Conditional independence ( https://en.wikipedia.org/wiki/Conditional_independence ): in this context it means that $p(x_{new}|x,\theta)=p(x_{new}|\theta)$ . This is a standard assumption that is made in Bayesian modelling. It's particularly clear that this is reasonable if, say, the data are (conditionally) independent Bernoulli trials with parameter $p$ . If we know $p$ , then knowing the outcome of any number of Bernoulli trials ( $x$ ) won't tell us anything more about the outcome of future trials ( $x_{new}$ ) (because they're conditionally independent). If we don't know $p$ , then $x$ can tell us a lot about $x_{new}$ (because they're not independent). If conditional independence doesn't hold, we can still proceed but the computations might be more complex. This might happen if, for example, $x$ comprises observation of a continuous time stochastic process over a time interval $[0,T]$ and $x_{new}$ is observations over the future interval $(T,T+1]$ .
