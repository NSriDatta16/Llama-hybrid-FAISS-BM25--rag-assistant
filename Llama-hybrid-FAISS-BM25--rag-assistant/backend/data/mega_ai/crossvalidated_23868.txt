[site]: crossvalidated
[post_id]: 23868
[parent_id]: 23832
[tags]: 
The best way to choose a covariance function is according to prior knowledge of the problem, e.g. is the distribution of data stationary, is it smooth, etc. In the absence of prior knowledge it is common to choose the covariance function (and hyper-parameters) by optimising some suitable model selection criterion, for instance the negative-log-marginal-likelihood (nlml) or some (leave-one-out) cross-validation performance estimate. However, this in no panacea, as both nlml and cross-validation based model selection criteria are evaluated on a finite sample of data and hence have a non-zero variance (if you repeat the analysis on different samples of data you get different values). This means it is possible to over-fit the model selection criterion, and you end up with a poor model, especially if you make many choices by having either a large number of hyper-parameters or a large number of covariance functions to choose from (or both). The nlml also has the disadvantage that if the model is misspecified, then the nlml is not a very good indicator of generalisation performance (as it assumes that the model is not misspecified), so it may end up recommending a bad choice of covariance function, so in principal, cross-validation is likely to be more robust in such situations. I've done some work on this area, and was surprised how susceptible kernel models (and GPs) are to this problem, see G. C. Cawley and N. L. C. Talbot, Preventing over-fitting in model selection via Bayesian regularisation of the hyper-parameters, Journal of Machine Learning Research, volume 8, pages 841-861, April 2007. ( www ) and G. C. Cawley and N. L. C. Talbot, Over-fitting in model selection and subsequent selection bias in performance evaluation, Journal of Machine Learning Research, 2010. Research, vol. 11, pp. 2079-2107, July 2010. ( www ) I've been looking into the problem of data-driven choice of covariance function, and I haven't found a method yet that is generally much better than just using the squared exponential covariance all the time.
