[site]: crossvalidated
[post_id]: 319883
[parent_id]: 186464
[tags]: 
I don't see the need for random forest in this case, and random forest generally requires a much larger sample size than logistic regression. Also, correlations among predictors does not affect predictive accuracy, and complete separation does not disqualify predictions; you'll just see some infinite regression coefficients, and these translate to predicted probabilities of 0 or 1. Better would be to get some discounting using a Bayesian logistic model (using priors with finite variance) or penalized maximum likelihood estimation. For the latter see the R rms package lrm function. It is not appropriate to do a first-stage analysis (using random forest or otherwise) to select predictors for the final stage. The model should be pre-specified whenever possible. You might consider unsupervised learning methods (data reduction, e.g. variable clustering or sparse principal components) in a first step. These methods do not use $Y$, so they are not biased in favor of falsely high predictive discrimination. Keep in mind that you need 96 observations just to estimate the intercept in a binary logistic model, so you are on very thin ice. The n=96 requirement comes from considering the maximum margin of error in the context of a 0.95 confidence interval for estimating a probability with a simple proportion, setting the maximum margin of error to +/- 0.10. The logit of the proportion is exactly the intercept in the logistic model when there are no covariates. Adding covariates makes estimation more difficult. See RMS section 10.2.3 from fharrell.com/links for details.
