[site]: crossvalidated
[post_id]: 242931
[parent_id]: 200133
[tags]: 
In highly skewed data sets beating the default accuracy can be very difficult and the ability to build a successful model may depend on how many positive examples you have and what the goals of your model are. Even with a very strong skew building reasonable models is possible, as an example the ipinyou data set has approx 2.5 million negative examples and only a few thousand positive ones. With a skewed dataset such as the ipinyou, using training using the AUC can help as this looks at the area under the ROC curve and so predicting only one class doesn't improve the score. Other challenges that can be faced using such datasets is the size, so ensuring you can actually process the data is important and may effect the language (Python, R, etc) you use, where the processing takes place (computer or on the cloud), and what algorithms you try to work with. Linear methods may struggle with highly skewed data where as non-linear methods such as random-forest or XGboost can be much more effective. Considering careful feature engineering is also important, also sparse matrices and 1 hot vector encoding may help you uncover the patterns within highly skewed data.
