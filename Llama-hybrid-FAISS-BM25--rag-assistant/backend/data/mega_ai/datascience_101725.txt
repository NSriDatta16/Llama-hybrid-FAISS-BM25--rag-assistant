[site]: datascience
[post_id]: 101725
[parent_id]: 26162
[tags]: 
Decision tree will split your data based on the most relevant features, it is not neccessary to give each decision tree with different feature. Let say an example, You have 3 features named as gender,profession and fare and the output required is some column. The most relevent features can be captured by different approaches, here i taking entropy and information gain as example. Lets say information gained by gender is more, then decision tree will split to two nodes, Male and Female where each node will have some samples splited to each based on the information gain calculated against output column. The information gain and entropy of splited data calculated again to find the next node, the resultant node will be any one of the feature having most information gain. The process will continue untill you reach the maximum depth. The disadvantages of decision tree is overfitting (depth of decision tree will be higher) and high variance, to overcome this we often use early stopping or random forest where the data trained by multiple tree (Where each tree can have shuffled training data) and predict based on the majority votes given by each tree Example image with depth of 4
