[site]: crossvalidated
[post_id]: 70020
[parent_id]: 70019
[tags]: 
If you have access to "the whole data" it means that you already know for each input $x_i$ the desired output $y_i$, so you don't need any machine learning, you just answer $y_i$ once someone asks for $x_i$ (he cannot ask for $x$ such that $\neg \exists_i x_i = x$ as you assume that you already have all the data). Big data is not about having "the whole data", and it does not contradict the statistical assumptions. Big data means exactly this: "we have lots of data in the comparison to the current computational power of the average computer". As a result, for many problems (in fact almost every problem) it still means that we do not know the whole data . In general, big data does not change that much in machine learning as one could expect after reading all these papers with "big data" in the title. It is a well known phenomenon that with increased sample size our models require less and less regularization, as the underlying distributions are almost perfectly represented in the data. One can hear "the best machine learning algorithms are not based on the best models, but on the biggest datasets", but it's only partially true, as a large datasets does not mean a "good dataset", or even "unskewed". The amount of data can actually be a problem instead of help due to its hardly noticeable representations of rare phenomena. Do you think there are limitations concerning generalization for ML algorithms (SVMs, trees, Neural Networks) when the data is huge? Huge? What do you mean by huge? What do you mean by generalization limitations? This is to broad question. The generalization properties are very hard mathematical aspects of machine learning, and in my opinion cannot be addressed in such a "loose" manner.
