[site]: crossvalidated
[post_id]: 472380
[parent_id]: 91512
[tags]: 
It's instructive to see what goes wrong -- the integrals are all very well, but a sample average is always finite, so what is the issue? I'll use the Cauchy distribution, which has no finite mean. The distribution is symmetric around zero, so if it had a mean, zero would be that mean. Here are cumulative averages of two samples of ten thousand Cauchy variates (in red and black). First, the first 100, then the first 1000, then all of them. The vertical scale increases over the panels (that's part of the point) If you had a distribution with a mean, the cumulative averages would settle down to that mean (by the law of large numbers). If you had a mean and variance, they would settle down at a known rate: the standard deviation of the $n$ th mean would be proportional to $1/\sqrt{n}$ . The Cauchy averages are 'trying to' settle down to zero, but every so often you get a big value and the average gets bumped away from zero again. In a distribution with finite mean this would eventually stop happening, but with the Cauchy it never does. The averages don't go off to infinity, as they would for a non-negative variable with infinite mean, they just keep being kicked around by outliers for ever.
