[site]: crossvalidated
[post_id]: 602680
[parent_id]: 602651
[tags]: 
The original motivation for random forests is to make decision trees less overfit, and in that sense fully grown trees are the expectation. However, in practice I find that some pruning still provides a benefit. Stumps are quite unlikely to be optimal though. Unlike boosting, where each subsequent tree improves upon the current fit, bagging weak learners doesn't let them learn from each other. Most likely you'll end up with lots of the stumps being exactly the same: their bags just aren't different enough from each other to choose different splits.
