[site]: crossvalidated
[post_id]: 34661
[parent_id]: 34652
[tags]: 
You should fix $\gamma$ and $C$ initially. Then do $k$-fold cross validation to get a single test error estimate, $terr(\gamma,C)$. Then do a two-dimensional grid search, varying $\gamma$ and $C$ separately to generate a test error matrix. To speed things up people typically use a logarithmic grid, $\gamma,C \in \{ 2^{-n_l}, 2^{-n_l+1}, \dots, 2^{n_u}\}$ (usually $\gamma$ is on a smaller scale). The key I think is to look for some smoothness in the surface around the local minimums (or each 1-dim projection) and not just take the global minimum. Remember for Gaussian kernel SVMs $\gamma$ is like $(p\sigma)^{-1}$ parameter for Multivariate Normal data with $p$ independent components. So if you have an understanding of the variability of predictor distances it can help determine a grid for $\gamma$, especially if feature correlation is not too strong. $C$ is how much slack you get if there is no perfect separating plane so the weaker the relationship between the predictors and class labels the smaller in theory $C$ should be (less penalization for missclassification).
