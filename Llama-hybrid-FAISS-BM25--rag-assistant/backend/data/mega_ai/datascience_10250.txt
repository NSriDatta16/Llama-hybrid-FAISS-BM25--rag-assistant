[site]: datascience
[post_id]: 10250
[parent_id]: 
[tags]: 
What is the difference between (objective / error / criterion / cost / loss) function in the context of neural networks?

The title says it all: I have seen three terms for functions so far, that seem to be the same / similar: error function criterion function cost function objective function loss function I was working on classification problems $$E(W) = \frac{1}{2} \sum_{x \in E}(t_x-o(x))^2$$ where $W$ are the weights, $E$ is the evaluation set, $t_x$ is the desired output (the class) of $x$ and $o(x)$ is the given output. This function seems to be commonly called "error function". But while reading about this topic, I've also seen the terms "criterion function" and "objective function". Do they all mean the same for neural nets? Geoffrey Hinton called cross-entropy for softmax-neurons and $E(W) = \frac{1}{2} \sum_{x \in E}(t_x-o(x))^2$ a cost function .
