[site]: crossvalidated
[post_id]: 333743
[parent_id]: 333737
[tags]: 
Looking at your graph, it appears that $\hat{t} \in \{0.7753975, 2.346194\}$ is a pretty reasonable guess at the MLE(s) of $t$. Running those values through the $\sin$ function to get back to $\mu$ results in $\hat{\mu} = \{0.7, 0.7\}$ or $0.7$, just as it should. So, there is no disagreement between the MLE of $\mu$ and the MLE(s) of $t$. What is happening is that you have created a map from $\mu \to t$ that is not 1-1. In this case, the true value of $\mu$ maps to multiple values of $t$, so not surprisingly you will have multiple maxima when working with $t$. Note, however, that this would be the same if you were doing a Bayesian analysis, unless your prior restricted $t$ to the interval $[-\pi/2, \pi/2)$ or some such. If you did so, for comparability, you should restrict the range of the MLE of $t$ to the same range, in which case you won't get multiple maxima for the likelihood function any more. ETA: In retrospect I focused too much on explanation-by-example and not enough on the underlying principle. One can hardly do better than @whuber's comment in response to the OP in this regard. In general, if you have a parameter $\theta$ and an associated MLE $\hat{\theta}$, and you construct a function $\theta = f(t)$, you've effectively created an alternate parameter $t$. The MLE of $t$, label it $\hat{t}$, will be those values of $t$ such that $f(t) = \hat{\theta}$, i.e., $f(\hat{t}) = \hat{\theta}$.
