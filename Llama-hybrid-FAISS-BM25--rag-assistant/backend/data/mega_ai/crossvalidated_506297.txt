[site]: crossvalidated
[post_id]: 506297
[parent_id]: 491937
[tags]: 
There are other important factors to consider for underfitting and overfitting discussion. They are regularization techniques. For example, L1, L2 regularization, pooling and data argumentation. So, it is not only about number of neurons. In recent work, people like to build a large network, at the same time, put a lot of regularizations on it. For example, it is OK to have a model that number of parameters is greater than number of data points. For large number of neurons in one layer or large number of layers discussion: Theoretically, MLP with one hidden layer, and the hidden layer with infinite number of neurons can approximate any functions. In practice, especially in vision problems, people like to have more layers than large number of neurons in one layer (prefer deep but not wide). Check this post for details. Why are neural networks becoming deeper, but not wider? EfficientNet paper is interesting to read for searching a better network structure. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks
