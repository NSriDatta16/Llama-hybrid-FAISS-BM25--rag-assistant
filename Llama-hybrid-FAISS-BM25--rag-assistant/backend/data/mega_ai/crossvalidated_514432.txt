[site]: crossvalidated
[post_id]: 514432
[parent_id]: 189331
[tags]: 
See section 4.2 of Bishop's Pattern Recognition and Machine Learning on probabilistic generative models. He shows on page 197 that in the two-class case, given classes $ \mathcal{C}_1, \mathcal{C}_2 $ , that $ p(\mathcal{C}_1 \mid \mathbf{x}) $ , the posterior for class $ \mathcal{C}_1 $ (i.e. conditional on input example $ \mathbf{x} \in \mathbb{R}^d $ ), is such that (by Bayes' rule) $$ p(\mathcal{C}_1 \mid \mathbf{x}) = \frac{p(\mathbf{x} \mid \mathcal{C}_1)p(\mathcal{C}_1)}{p(\mathbf{x} \mid \mathcal{C}_1)p(\mathcal{C}_1) + p(\mathbf{x} \mid \mathcal{C}_2)p(\mathcal{C}_2)} = \frac{1}{1 + e^{-a(\mathbf{x})}} = \sigma \circ a(\mathbf{x}) $$ Here $ \sigma : \mathbb{R} \rightarrow (0, 1) $ is the sigmoid function and $ a : \mathbb{R}^d \rightarrow \mathbb{R} $ is such that $$ a(\mathbf{x}) \triangleq \log\frac{p(\mathbf{x} \mid\mathcal{C}_1)p(\mathcal{C}_1)}{p(\mathbf{x} \mid \mathcal{C}_2)p(\mathcal{C}_2)} = \log\frac{p(\mathcal{C_1}, \mathbf{x})}{p(\mathcal{C}_2, \mathbf{x})} $$ In the multiclass case, i.e. with classes $ \mathcal{C}_1, \ldots \mathcal{C}_K $ , we naturally have for $ k \in \{1, \ldots K\} $ , $$ p(\mathcal{C}_k \mid \mathbf{x}) = \frac{p(\mathbf{x} \mid \mathcal{C}_k)p(\mathcal{C}_k)}{\sum_{j = 1}^Kp(\mathbf{x} \mid \mathcal{C}_j)p(\mathcal{C}_j)} = \frac{e^{a_k(\mathbf{x})}}{\sum_{j = 1}^Ke^{a_j(\mathbf{x})}} $$ Here for $ k \in \{1, \ldots K\} $ , $ a_k : \mathbb{R}^d \rightarrow \mathbb{R} $ is such that $$ a_k(\mathbf{x}) \triangleq \log\left(p(\mathbf{x} \mid \mathcal{C}_k)p(\mathcal{C}_k)\right) = \log p(\mathcal{C}_k, \mathbf{x}) $$ The $ a, a_k $ functions can be given parametric forms--for example, in multiclass logistic regression, $ a_k(\mathbf{x}) \triangleq \mathbf{w}_k^\top\mathbf{x} + b_k $ for $ \mathbf{w}_k \in \mathbb{R}^d, b_k \in \mathbb{R} $ . In fact, page 203 states that for class conditional distributions, i.e. $ X \mid \mathcal{C}_k $ , that are members of the exponential family of distributions, the $ a_k $ functions are affine functions of $ \mathbf{x} $ . An example is linear discriminant analysis, which assumes Gaussian class-conditional distributions with a shared covariance matrix, as equation (4.68) on page 199 shows that the $ a_k $ function is affine. The softmax function itself, probabilistic interpretations aside, is a smooth, differentiable approximation to the max function, which of course the other answers have mentioned. It is helpful when using gradient-based methods to minimize an objective function. For example, the binary (multiclass) logistic regression objective is convex and differentiable, with the differentiability partly because of its inclusion of the sigmoid (softmax) function.
