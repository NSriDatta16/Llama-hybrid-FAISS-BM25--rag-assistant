[site]: datascience
[post_id]: 76783
[parent_id]: 74984
[tags]: 
With purely one-hot encoded data this isn’t a problem. For example, the distance between a red square and a blue square in your second example (assuming you’re using Euclidean distance) is 1 in the red dimension and 1 in the blue dimension, so sqrt(1+1) overall (by pythagoras). Similarly, the distance between a red square and a red circle is 1 in the circle dimension and 1 in the square dimension. However, things are messier if you have a mixture of one-hot and continuous features. In these cases you might get interesting results by making your features continuous rather than binary (e.g. describe shape based on number of vertices and colour by rgb colour space, although I know that’s a made-up example). However you decide to engineer your features, you can reduce the risk of any particular feature dominating the clustering by scaling your features appropriately and by using dimensionality reduction to avoid accidental unbalanced feature weighting through colinear features you hadn’t noticed.
