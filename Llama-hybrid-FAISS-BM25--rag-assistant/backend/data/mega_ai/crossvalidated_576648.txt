[site]: crossvalidated
[post_id]: 576648
[parent_id]: 576647
[tags]: 
For an XGBoost model, examining one tree at a time makes little sense, because gradient boosting is an iterative algorithm primarily driven by the "mistakes" done up to that point. Thus the $i$ -th tree on it's own has little practical meaning. For all we know, a seemingly "risky" tree corrects existing biases/lapses of the overall XGBoost model.
