[site]: crossvalidated
[post_id]: 603169
[parent_id]: 603084
[tags]: 
As always, it helps to make plots. Here I plot the cv.glmnet predictions on the left and the LassoCV predictions on the right, against the true values in the hold out set. We see immediately that both implementations do a great job: they explain most of the variability in $y$ and the mean squared error (MSE) is small compared to the total variability in $y$ . We can quantify the difference between the implementations in terms of the proportion of variance explained (PVE) = 1 - MSE / Var $(y_{\text{holdout}})$ . PVE cv.glmnet = .9989 PVE LassoCV = .9993 Note: PVE is not the same as the R-squared, which is computed on the training data. And it's not equivalent to comparing the Lasso to the "naive" model that predicts the average $y$ observed in the training data. So there is a difference but it is small relative to the variability of the outcome variable. I wouldn't use this example to conclude that LassoCV is a better implementation than cv.glmnet in general. PS. An increase of .0004 could be a meaningful improvement and worthwhile to put in practice. However, to conclude that a new model is better than the production model by such a small increment, we would want to compare the models on more than 300 examples.
