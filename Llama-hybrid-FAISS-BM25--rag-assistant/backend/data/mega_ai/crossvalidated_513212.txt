[site]: crossvalidated
[post_id]: 513212
[parent_id]: 503648
[tags]: 
I am not sure what you mean when stating that the KL divergence "has to be asymmetric in order to be compatible with Bayes' Law": Does it have to be compatible with Bayes's Law? And, if so, in what sense? Still, I don't think that this is crucial to your question (and if it is, please be more specific). There are many ways to understand the KL divergence. From a Bayesian perspective, you can think of $Q(x)$ as a prior distribution and $P(x)$ as a posterior distribution in the sense that the data was used to estimate it. $P(x)$ is not necessarily the posterior update of $Q(x)$ , but it could be. Note that even though $Q(x) = 0$ signifies certainty that $x$ will not occur, $Q(x) > 0$ does not mean certainty that $x$ will occur. $Q(x) > 0$ signifies a positive probability that $x$ can occur. As an example, let $Q(x)$ be a mixture of $K$ parametric distributions, $$Q(x) = \sum_{k = 1} ^K w_k Q_k(x|\theta_k),$$ where $w_k>0$ are the weights and $\sum w_k = 1$ . Let the posterior distribution be $$P(x) = \sum_{k = 1} ^K \hat{w}_k P_k(x|\hat{\theta}_k),\,\, \hat{w}_k \geq 0, \sum \hat{w}_k = 1.$$ For example if a sample is drawn from a subset of $K$ populations, but you do not know beforehand which, then some $\hat{w}_k = 0$ . In this scenario, if for some $x$ , $Q(x) = 0$ and $P(x) > 0$ , then $D_{KL} \rightarrow \infty$ seems reasonable. The difference between $P(x)$ and $Q(x)$ tends to infinity, and leads us to the conclusion that $P(x)$ has been sampled outside of the $K$ populations. But if for some $x$ , $Q(x) > 0$ and $P(x) = 0$ , this simply means that some $\hat{w}_k = 0$ , and is in accordance with the overall model: samples are taken from a subset of the $K$ candidate populations.
