[site]: datascience
[post_id]: 45769
[parent_id]: 
[tags]: 
Execution time of the same algorithm for different runs

There is this ongoing discussion with me and my advisor. We have a Deep Learning algorithm (does not matter which one it is, I believe, such as CNN, LSTM etc..). We are using 4 GPU Nvidia machine and we have Tensorflow GPU installed on server. He believes that execution time of each epoch and overall runtime should be the same if we run this algorithm multiple times. We set the seed in order to have reproducible results. Every thing is set the same, number of epochs, batch size, number of batches, the same number of instances... I do not expect the execution times of the same algorithm for different runs to be the same on Tensorflow-GPU due to the computer power, computer architecture, memory availability, and even other internal or external interruptions to the computer during the computation. It depends heavily on a lot of secondary elements, not on the algorithm itself. These are all my assumptions. When I plot the overall run time, every time it is different. I cannot find proper references regarding of this issue. What do you think about it?
