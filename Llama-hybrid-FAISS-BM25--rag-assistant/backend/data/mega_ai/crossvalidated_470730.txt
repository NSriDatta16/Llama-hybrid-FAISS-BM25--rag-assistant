[site]: crossvalidated
[post_id]: 470730
[parent_id]: 470626
[tags]: 
If errors are independent and follow the normal distribution (of any variance but consistent), then the sum of squared errors corresponds to their joint probability/likelihood. $\Pi e^{-x_i^2}=e^{-\Sigma x_i^2}$ So under those conditions minimizing the sum of square errors is the same as maximizing the likelihood. If a cost-minimizing prediction is needed (where the cost metric is different from MSE) the general/accurate approach would be to explicitly minimize the expected cost over the entire distribution of models weighted by their likelihoods (or probabilies if you have prior knowledge). This completely decouples the problem of minimizing expected cost from the problem of estimation in the presence of noise. Suppose you are measuring a constant quantity in the presence of Gaussian noise. Even if your cost metric for future outcomes is MAE, you would rather predict with the mean (minimizing past MSE) than the median (minimizing past MAE), if indeed you know the quantity is constant and the measurement noise is Gaussian. Example Consider the following spread of hits produced by a gun that was mechanically fixed in place. You place a circle of a given size somewhere on the target. If the next shot lands entirely inside your circle, you win, else you lose. The cost function is of the form $f_C(x,y)=sign((x-x_C)^2+(y-y_C)^2-R^2)$ . If you minimize $\sum_i f_C(x_i,y_i)$ , you would place the circle in the blue position, containing entirely the maximum number of past shots. But if you knew that the gun is fixed in place and the error is Gaussian, you would place the circle in the green position, centered on the data's mean/centroid (minimizing MSE), as you are optimizing future expected payoff, not average past payoff.
