[site]: stackoverflow
[post_id]: 3343305
[parent_id]: 3329361
[tags]: 
Someone recommended I use the futures package for this. I tried it and it seems to be working. http://pypi.python.org/pypi/futures Here's an example: "Download many URLs in parallel." import functools import urllib.request import futures URLS = ['http://www.foxnews.com/', 'http://www.cnn.com/', 'http://europe.wsj.com/', 'http://www.bbc.co.uk/', 'http://some-made-up-domain.com/'] def load_url(url, timeout): return urllib.request.urlopen(url, timeout=timeout).read() with futures.ThreadPoolExecutor(50) as executor: future_list = executor.run_to_futures( [functools.partial(load_url, url, 30) for url in URLS])
