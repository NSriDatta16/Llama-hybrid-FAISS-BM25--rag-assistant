[site]: crossvalidated
[post_id]: 465434
[parent_id]: 242063
[tags]: 
4 years later I have yet to see a concrete answer for this. The best I could find is this paper. As @dimpol pointed out, it is useful to think of the neural network as a function with a finite number of parameters. If the number of parameters and the dataset match exactly then the function (neural network) is perfectly over fitted. This is the "storage capacity" so to speak. If we go beyond that, something magical happens. It starts to generalize again. Its yet not fully understood why it happens. https://arxiv.org/abs/1812.11118 Open AI also talks about it https://openai.com/blog/deep-double-descent/ Edit (27/04/2022) Almost 5.5 years later we still dont really know the answer but for large language models, we can estimate the optimal amount of training data that we should use to train the model. Here is the paper from deepmind https://arxiv.org/abs/2203.15556#
