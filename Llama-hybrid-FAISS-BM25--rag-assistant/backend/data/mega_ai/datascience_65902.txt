[site]: datascience
[post_id]: 65902
[parent_id]: 58137
[tags]: 
There is nothing that guarantee the six features will be a subset of the 55. Intuitively, if you parametrize you problem with number of features instead of lambda, it is easier to see how two variables could better explain the output than a single different one. Another problem that can arise from these regularisation methods come from correlated variables. If you take two higly correlated explanatory variables that have similar impact on the output, you may never be sure which one the regularisation process will keep. In the context of correlated variables, it is even easier to see how by almost arbitrarily keeping one variable instead of another you can end up with some instability in variables choosen. Note that certain implementations (glmnet in R) proceeds differently. It proceeds iteratively trough differents values of $\lambda$ and remove variables as $\lambda$ increase... so that features for higher alphas are a subset of features for lower alphas. On another topic, this may hint that you need to do some work on feature engeeniring, as some variable appears to capture redundant information. A good feature engineering work will also be helpfull for model explainability.
