[site]: crossvalidated
[post_id]: 156573
[parent_id]: 
[tags]: 
Should predictive accuracy or, alternatively, minimizing the MSE, be reconsidered?

Ever since Breiman, maximizing predictive accuracy has become a predictive modeling gold standard , of sorts. That it has evolved to this status is understandable: it can be "optimized," is easily calibrated across k-fold samples and, for the most part, is a consistent statistic for internal model validation. However, all too often it is the sole criterion for model value -- a mechanistic and rigidly applied metric -- despite its being prone to p-hacking, gaming, and analyst fraud as well as little or no thought being given to its wider business and/or strategic impact and meaning. Is it time to re-evaluate this metric's relative importance in the statistical, machine learning and predictive modeling industries?
