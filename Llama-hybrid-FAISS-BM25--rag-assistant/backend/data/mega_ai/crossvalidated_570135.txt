[site]: crossvalidated
[post_id]: 570135
[parent_id]: 
[tags]: 
Using SSE to find the optimal model does not yield the correct solution

I am generating an AR(3) model with coefficients $(\phi_1, \phi_2, \phi_3) = (0.35, 0.5, 0.08)$ . Then I use the statsmodels module from Python and its Yule-Walker implementation to get back the coefficients. I find two problems: The estimation is not as accurate as I want When using the Sum of Square Errors formula (SSE) I find that p=4 instead of p=3 yields the least error. It should be p=3. I will add my Python code here for reference: # load libraries from statsmodels.graphics.tsaplots import plot_pacf from statsmodels.graphics.tsaplots import plot_acf from statsmodels.tsa.arima_process import ArmaProcess from statsmodels.tsa.stattools import pacf from statsmodels.regression.linear_model import yule_walker from statsmodels.tsa.stattools import adfuller import matplotlib.pyplot as plt import numpy as np # generate data using ArmaProcess # some syntax taken from the Python statsmodels manual N=1000 # number of samples # random seed np.random.seed(1) # the phi_i coefficients # arparms = np.array([0.35, 0.5, 0.08]) phi_1=0.35 phi_2=0.5 phi_3=0.08 arparms = np.array([phi_1, phi_2, phi_3]) # the beta_i coefficients maparms = np.array([0]) # no moving average considered ar = np.r_[1, -arparms] # add zero-lag and negate> ma = np.r_[1, -maparms] # add zero lag and negate arma_process = ArmaProcess(ar, ma) arma_process.isstationary arma_process.isinvertible arma_process.arroots y=arma_process.generate_sample(N, scale=1) def SSE(data, k, seed, N): # data :to be tested # k possible values for p # seed: to be able to reproduce the data as created initially # N: number of samples # define a loop of possibilities # initialize sse vector ssevec=np.zeros(k+1) for i in range(1,k+1): # random seed np.random.seed(seed) # get the coefficients phi, sigma = yule_walker(data, i, method='mle') # we care only on phi print(phi) # create the new data with this phi arparms = phi # the beta_i coefficients maparms = np.array([0]) # no moving average considered ar = np.r_[1, -arparms] # add zero-lag and negate> ma = np.r_[1, -maparms] # add zero lag and negate arma_process = ArmaProcess(ar, ma) y=arma_process.generate_sample(N, scale=1) # find the SEE sse = np.sum((y - data)**2) ssevec[i]=sse return ssevec[1:] # ignore the zero in the first place k=6 N=1000 seed=1 myssevec= SSE(y, k, seed, N) myssevec # find the ideal order p based on the SSE test min_value = min(myssevec) # skip the zero at the beginning min_index = list(myssevec).index(min_value) # need to cast array into a list print(f' the ideal p is {min_index+1} with an SSE equal to {min_value}') The result is: the ideal p is 4 with an SSE equal to 201.87291336514215 It should be p=3 In addition, I believe this is not a programming issue. Here is a code in R with the same parameters: set.seed(1) data=arima.sim(list(order=c(3,0,0), ar=c(0.35,0.5,0.08)), n=1000) SSE=NULL for (p in 1:5){ m = arima(data, order=c(p,0,0), include.mean=FALSE) SSE[p] = sum(resid(m)^2) print(paste(SSE[p])) } The results for the SSEs are: [1] "1516.22197415653" [1] "1111.96217374379" [1] "1095.65751612625" [1] "1095.65425178515" [1] "1092.39037726777" which indicate that p=5 is optimal for this (p=3) problem. I believe there is something fundamentally wrong in my thinking. I found, interestingly, that for the coefficients: phi_1=0.9 phi_2=-0.6 phi_3=0.3 The results are quite good. It seems that small coefficients create uncertainty in the SSE computation. Thanks
