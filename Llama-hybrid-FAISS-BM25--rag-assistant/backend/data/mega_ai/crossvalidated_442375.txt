[site]: crossvalidated
[post_id]: 442375
[parent_id]: 399940
[tags]: 
Firstly, you can see this pre-processing as a blessing. Since you have lots of choices of how much padding is before and after etc., you can use this as an data augmentation method. If you do that, it may very well be that you approach could work. Secondly, you should consider using a (Mel-)spectrogram. There may be a very good reason that's the standard approach most people use for audio. In that case you could create your features using the pre-trained VGGish model by Google. That neural network uses the spetrogram as an input to 1-D convolutions (along the time axis) with the value being the intensity at the frequency band and the different frequency bands as channels. You then build a neural network with either Google VGGish as the first few layers (only an option if you use tensorflow or convert their network to something else) or you build a new NN with their model's output as your input. You could also just use their model output as the input at different time steps for e.g. a RNN (LSTM) or some kind of transformer model with attention.
