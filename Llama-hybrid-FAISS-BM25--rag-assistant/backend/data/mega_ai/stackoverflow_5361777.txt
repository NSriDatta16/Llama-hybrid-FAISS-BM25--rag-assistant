[site]: stackoverflow
[post_id]: 5361777
[parent_id]: 5361127
[tags]: 
In general, you need a locking mechanism (critical sections are a locking mechanism) whenever multiple threads may access a shared resource at the same time, and at least one of the threads will be writing to / modifying the shared resource. This is true whether the resource is an object in memory or a file on disk. And the reason that the locking is necessary is that, is that if a read operation happens concurrently with a write operation, the read operation is likely to obtain inconsistent data leading to unpredictable behaviour. Stephen Cheung has mentioned the platform specific considerations with regards file handling, and I'll not repeat them here. As a side note, I'd like to highlight another concurrency concern that may be applicable in your case. Suppose one thread reads some data and starts processing. Then another thread does the same. Both threads determine that they must write a result to position X of File A. At best the values to be written are the same, and one of the threads effectively did nothing but waste time. At worst, the calculation of one of the threads is overwritten, and the result is lost. You need to determine whether this would be a problem for your application. And I must point out that if it is, just locking the read and write operations will not solve it. Furthermore, trying to extend the duration of the locks leads to other problems. Options Critical Sections Yes, you can use critical sections. You will need to choose the best granularity of the critical sections: One per whole file, or perhaps use them to designate specific blocks within a file. The decision would require a better understanding of what your application does, so I'm not going to answer for you. Just be aware of the possibility of deadlocks: Thread 1 acquires lock A Thread 2 acquires lock B Thread 1 desires lock B, but has to wait Thread 2 desires lock A - causing a deadlock because neither thread is able to release its acquired lock. I'm also going to suggest 2 other tools for you to consider in your solution. Single-Threaded What a shocking thing to say! But seriously, if your reason to go multi-threaded was "to make the application faster", then you went multi-threaded for the wrong reason. Most people who do that actually end up making their applications, more difficult to write, less reliable, and slower ! It is a far too common misconception that multiple threads speed up applications. If a task requires X clock-cycles to perform - it will take X clock-cycles! Multiple threads don't speed up tasks, it permits multiple tasks to be done in parallel. But this can be a bad thing ! ... You've described your application as being highly dependent on reading from disk, parsing what's read and writing to disk. Depending on how CPU intensive the parsing step is, you may find that all your threads are spending the majority of their time waiting for disk IO operations. In which case, the multiple threads generally only serve to shunt the disk heads to the far 'corners' of your (ummm round ) disk platters. Disk IO is still the bottle-neck, and the threads make it behave as if the files are maximally fragmented. Queueing Operations Let's suppose your reason for going multi-threaded are valid, and you do still have threads operating on shared resources. Instead of using locks to avoid concurrency issues, you could queue your shared resource operations onto specific threads. So instead of Thread 1: Reading position X from File A Parsing the data Writing to position Y in file A Create another thread; the FileA thread: the FileA has a queue of instructions When it gets to the instruction to read position X, it does so. It sends the data to Thread 1 Thread 1 parses its data --- while FileA thread continues processing instructions Thread 1 places an instruction to write its result to position Y at the back of FileA thread's queue --- while FileA thread continues to process other instructions. Eventually FileA thread will write the data as required by Trhead 1.
