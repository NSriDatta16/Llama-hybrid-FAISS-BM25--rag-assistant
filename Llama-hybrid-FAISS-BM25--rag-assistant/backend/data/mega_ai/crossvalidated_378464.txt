[site]: crossvalidated
[post_id]: 378464
[parent_id]: 378456
[tags]: 
This does depend a little on how what intent you have for X_test , y_test , but I'm going to assume that you set this data aside so you can get an accurate assessment of your final model's generalization ability (which is good practice). In that case, you want to determine your hyperparameters using only the training data, so your parameter tuning cross validation should be run using only the training data as the base dataset. If instead you use the entire data set, then your test data provides some information towards your choice of hyperparameters, and your subsequent estimate of the test error will be overly optimistic. Additionally, tuning n_estimators in a random forest is a widespread anti-pattern. There's no need to tune that parameter, larger always leads to a model with the same bias but with less variance, so larger is always no worse. You really only need to be tuning max_depth here. Here's a reference for that advice. But my main concern is hyperparamters that I will get will be biased to the training dataset Yup. That's always true and fundamentally unavoidable, you have to use some set of data to tune the values of those parameters, so in the end, they have to be biased towards performance on some sample. The best you can do is rigorously use cross validation and a test set to minimize that bias, and measure your results. This is what I am not understanding. How will I implement Gridsearch and cross validation both in scikit learn. The basic procedure is: Split raw data into train and test. Use cross validation on the split off training data to estimate the optimal values of hyperparameters (by minimizing the CV test error). Fit a single model to the entire training data using the determined optimal hyperparameters. Score that model on your original test data to estimate the performance of the final model.
