[site]: datascience
[post_id]: 121608
[parent_id]: 
[tags]: 
Influence functions on neural networks: Help with understanding of result and derivation

I'm working through a paper titled " Understanding Black-box Predictions via Influence Functions " where they introduce the notion of influence functions from robust statistics to approximate the change in parameters when removing a training example. They start off by establishing that the parameters that minimize the loss is given by $$ \hat \theta = \arg \min_{\theta \in \Theta} \frac{1}{n} \sum_{i=1}^n L(z_i, \theta) $$ Now they state that we would like to know what happens to the parameters when we remove a single training example $z = (x, y)$ , which would be $\hat{\theta}_{-z} - \hat\theta$ where $\hat{\theta}_{-z}$ is the parameters obtained from training on every example except $z$ . $$ \hat \theta_{-z} = \arg \min_{\theta \in \Theta} \sum_{z_i \neq z} L(z_i, \theta) $$ What they introduce now is that the notion of influence functions can be used to upweight the training example $z$ by some small $\epsilon$ which approximates the effect of removing it which results in new parameters $$ \hat{\theta}_{\epsilon, z} = \arg \min_{\theta \in \Theta} \frac{1}{n} \sum_{i=1}^n L(z_i, \theta) + \epsilon L(z, \theta) $$ Here's where I'm unsure of what happens exactly. They write: A classic result (Cook & Weisberg, 1982) tells us that the influence of upweighting $z$ on the parameters $\hat{\theta}$ is given by $\frac{d\hat\theta_{\epsilon, z}}{d\epsilon} \bigg|_{\epsilon=0} = - \mathbf{H}_{\hat \theta}^{-1} \nabla_{\theta} L(z, \hat \theta)$ What is this classic result? I have been trying to go through their reference but can't seem to find anything that I can relate to the paper. My question is then where this classic result is stated and how it is arrived at ? Additionally, what are the steps that are taken to arrive at $- \mathbf{H}_{\hat \theta}^{-1} \nabla_{\theta} L(z, \hat \theta)$ in their expression? Thanks in advance!
