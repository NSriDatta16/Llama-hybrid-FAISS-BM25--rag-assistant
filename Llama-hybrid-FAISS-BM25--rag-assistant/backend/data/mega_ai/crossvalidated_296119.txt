[site]: crossvalidated
[post_id]: 296119
[parent_id]: 291371
[tags]: 
It could be multicollinearity, but if so that's just icing on the cake. Your basic problem is that your model is effectively saturated. In the context of linear models, a regression is saturated when the number of observations is equal to the number of variables plus one. For instance, if you have one X variable and only two data, you can perfectly fit the data with a straight line. (In fact, when there are fewer data than that, the model is unidentifiable; see: Fitting least squares when number of predictors are larger than instances .) In the OLS regression setting (again), we often say that a model is 'approaching saturation' when you have $ Now in your case, you have a logistic (not OLS) regression model, and you have a thousand observations, so it would seem intuitive to assume that you must be OK. However, the corresponding rule of thumb for binary regression is that you need at least fifteen of the less commonly occurring outcome for every variable in order to avoid 'approaching saturation'. You say that you have only $2\%$ positive cases. Out of one thousand observations, that means you have only $20$ positive cases. Clearly positive is the less commonly occurring outcome here, so you want to have $15$ positive cases for each variable. But, you have $11$ variables! Thus, your model is 'approaching saturation' almost by definition (except that that isn't really a definition, it's an out-and-out hand-wavy rule of thumb). It is worth noting that the model could not even estimate one of the parameters; x24 has been rendered NA (cf., Maximum number of independent variables that can be entered into a multiple regression equation ). Nonetheless, you do have $20$ positive cases and 'only' $11$ variables, so based on what I said above, you might assume that you shouldn't really have complete saturation. And, in fact, you may not have complete saturation, but I'm guessing that your variables are categorical (viz., binary) and that there are instances in which you have more than one of the positive cases lying within a given cell, which reduces the effective number of positive cases via redundancy. Another way to think about that is that you almost certainly have perfect separation in your model, which again increases the saturation of the model. Indeed, you can see the classic hallmarks of separation: you have huge estimated coefficients, even more vastly huge standard errors associated with them, and a very large number of Fisher scoring iterations (namely, 25) were needed to try to fit the model. (To further understand separation in this context, it may help you to read my answers here: Adding interactions to logistic regression leads to high SEs and here: Logistic glm with good predictors is giving p-values = 1 .)
