[site]: crossvalidated
[post_id]: 300592
[parent_id]: 10356
[tags]: 
Okay so I work in Fraud Detection so this sort of problem is not new to me. I think the machine learning community has quite a bit to say about unbalanced data (as in classes are unbalanced). So there are a couple of dead easy strategies that I think have already been mentioned, and a couple of neat ideas, and some way out there. I'm not even going to pretend to know what this means for the asymptotics for your problem, but it always seems to give me reasonable results in logistic regression. There may be a paper in there somewhere, not sure though. Here are your options as I see it: Oversample the minority class. This amounts to sampling the minority class with replacement until you have the same number of observations as the majority class. There are fancy ways to do this so that you do things like jittering the observation values, so that you have values close to the original but aren't perfect copies, etc. Undersample, this is where you take a subsample of the majority class. Again fancy ways to do this so that you are removing majority samples that are the closest to the minority samples, using nearest neighbor algorithms and so forth. Reweight the classes. For logistic regression this is what I do. Essentially, you are changing the loss function to penalize a misclassified minority case much more heavily than a misclassified majority class. But then again you are technically not doing maximum likelihood. Simulate data. Lot's of neat ideas that I've played with here. You can use SMOTE to generate data, Generative Adversarial Networks, Autoencoders using the generative portion, kernel density estimators to draw new samples. At any rate, I've used all of these methods, but I find the simplest is to just reweight the problem for logistic regression anyway. One thing you can do to gut check your model though is to take: -Intercept/beta That should be the decision boundary (50% probability of being in either class) on a given variable ceteris paribus . If it doesn't make sense, e.g. the decision boundary is a negative number on a variable that is strictly positive, then you've got bias in your logistic regression that needs to be corrected.
