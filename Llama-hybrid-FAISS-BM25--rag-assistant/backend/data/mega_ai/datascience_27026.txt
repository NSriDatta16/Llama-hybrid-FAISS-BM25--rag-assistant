[site]: datascience
[post_id]: 27026
[parent_id]: 22565
[tags]: 
The baseline of my answer is certainly, try the simplest approach first, then go on. Machine Learning is not about heavy lifting, it is about smart usage of your tools. The choice of your algorithm depends mostly on your data and the types of anomalies you would expect. Do you have timeseries data with sesonality effects and trends, maybe the twitter anomaly detection package would be a good start. For instance. If your CPU is hot you would expect it to run with high load. If the load is low maybe it is an anomaly or it has suffered from a high load. So one of your questions would be, can you spot anomalies or identify normal behaviour by just looking at one sample? Therefore the, "No-free-Lunch", theorem applies again. That being said, SVM and Isolation Forest are a good start. Even if you have a time series problem, where you want to track short term dependencies. Just include "old" rows to your current set: Link You can even try simpler approaches like Gaussian distribution of your values, calculating the probabilities of each permutation (if you features are small enough) or building a markov modell. Unsatisfying results? Dive deeper, maybe Auto Encoder do the trick, maybe LSTMs, maybe a combination of both. A short summary of unsupervised and semi-supervised ML algorithms I recently used, sorted by complexity (kind of): Median Absolute Deviation (Twitters anomaly detection library) Local Outloer Factor Isolation Forest Elliptic Envelope Autoregressive integrated moving average Auto Encoder Time Series Discordes LSTMs for forcasting Replicator Neuronal Networks Bayesian networks There are certainly supervised approaches but I am not familiar with those.
