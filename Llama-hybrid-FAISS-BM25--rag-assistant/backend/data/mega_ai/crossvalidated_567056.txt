[site]: crossvalidated
[post_id]: 567056
[parent_id]: 
[tags]: 
How do I minimize the cost from errors?

Bayes' Optimal Classifier is known to achieve the minimum error rate for a dataset $x_1, \ldots, x_n, x_i \in \mathbb{R}^d$ . Suppose that each error had a cost associated with it. For example, in a binary classifier, classifying $y_i = 1$ when $y_i = 0$ would have a cost $\alpha$ and classifying the other way would have a cost $\beta$ . If $\alpha = \beta$ , it is known that making decisions based on Bayes' Optimal Classifier also achieves the minimum cost . However, what if $\alpha \neq \beta$ , i.e., different kinds of errors have different kinds of costs? In machine learning, how is making predictions so as to minimize cost achieved? Is it a kind of least squares?
