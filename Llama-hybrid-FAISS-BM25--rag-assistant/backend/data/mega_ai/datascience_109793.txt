[site]: datascience
[post_id]: 109793
[parent_id]: 
[tags]: 
Is it wise to always `StandardScaler()` features? [SOLVED]

My current investigations point to the sklearn.preprocessing.StandardScaler() not always being the right choice for certain types of feature extractions for neural networks. Suppose I want to classify sound events based on spectrogram data. A second of such data could look like this: Visible here is a sine wave of around 1kHz over one second. The settling of the low bands is specific to the feature extraction and not part of the question . The data is a (n,28,40) matrix of dBFS-values, ergo the logarithmic energy levels relative to the maximal digital headroom of the wav-file. If StandardScaler is now applied, the visual representation of the sound now looks like this: ... which basically removes the defining features and amplifies the noise, exactly what is NOT wanted. Would a level-based scaler be the better choice here or is the StandardScaler() just not appearing to benefit the system in this specific case of a sine wave? Note : I am a student and I do not have years of experience. So if the question lacks quality, I ask of you to suggest an improvement before downvoting. Thank you.
