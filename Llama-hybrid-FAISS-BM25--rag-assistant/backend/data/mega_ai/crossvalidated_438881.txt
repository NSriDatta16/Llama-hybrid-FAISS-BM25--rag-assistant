[site]: crossvalidated
[post_id]: 438881
[parent_id]: 438753
[tags]: 
The choice of a prior distribution is based on prior belief, prior information, or some constructive principle, like minimum information, maximum entropy, frequentist matching, leading to "default" or "reference" (rather than "noninformative") priors . However, there is no unique and no better/best choice for a prior as the Bayesian perspective is conditional on this prior. In the case of a variance, $\pi(\sigma^2)=\sigma^{-2}$ is a common choice, as for instance in Jeffreys' approach or as a scale invariant (right Haar measure) prior. Again this is not the unique choice for a prior and it is not better or worse than others (unless an extra-Bayesian criterion is used to compare priors). Even for maximum entropy priors, there is no unicity or optimality as the choice of a maximum entropy prior depends on two items of calibration: the reference measure that scales the entropy the moment constraints on the prior that lead to the functions appearing in the exponential of the density
