[site]: crossvalidated
[post_id]: 145485
[parent_id]: 
[tags]: 
LDA vs word2vec

I am trying to understand what is similarity between Latent Dirichlet Allocation and word2vec for calculating word similarity. As I understand, LDA maps words to a vector of probabilities of latent topics, while word2vec maps them to a vector of real numbers (related to singular value decomposition of pointwise mutual information, see O. Levy, Y. Goldberg, "Neural Word Embedding as Implicit Matrix Factorization" ; see also How does word2vec work? ). I am interested both in theoretical relations (can one be considered a generalization, or variation of the other) and practical (when to use one but not the other). Related: What are some standard ways of computing the distance between documents? - DataScience.SE
