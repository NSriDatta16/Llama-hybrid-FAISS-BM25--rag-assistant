[site]: datascience
[post_id]: 68504
[parent_id]: 68501
[tags]: 
Is my goal to produce the smallest variance between Training error and vaidation error? Normally your goal is to have a model that performs best in production, for that you normally split the train/hold out in a way that reproduce the best in the hold out set. Have a look at this answer from here What would I prefer - an over-fitted model or a less accurate model? They talk a bit about model selection. Is a 100% accuracy on the Traing error a bad thing? When you have this you probably have "overfitting". Your model is predicting unrealistically good results in the training set that most probable it wonÂ´t able to achieve in the test set. It is not necessarily a bad thing, but it is suspicious. With this information in this format - how would one determine the best choice of estimator? For your case, I would recommend choosing the one that achieves best in hold out without overfitting. Note: Your dataset it is way too small. 830 rows of data is almost nothing. In order to validate your model properly I will suggest Leave One Out Cross Validation: "In LOOCV we divide the data set into two parts. In one part we have a single observation, which is our test data and in the other part, we have all the other observations from the dataset forming our training data." from here What is happening in your test is that 2 or 3 rows of your data make a big impact in the result that is why you have a big variance between the different folds and parameters. It is hard to say which model is better when validating this way. In summary , my suggestion is that you do LOOCV and then select the one that has better performance in test. (I am assuming that you can do random split since there is no patter such as it is a time series)
