[site]: crossvalidated
[post_id]: 451920
[parent_id]: 282160
[tags]: 
A model can overfit to cross entropy loss without over overfitting to accuracy . There is a key difference between the two types of loss: Accuracy measures whether you get the prediction right Cross entropy measures how confident you are about a prediction For example, if an image of a cat is passed into two models. Model A predicts {cat: 0.9, dog: 0.1} and model B predicts {cat: 0.6, dog: 0.4}. Both model will score the same accuracy, but model A will have a lower loss. Because of this the model will try to be more and more confident to minimize loss. It works fine in training stage, but in validation stage it will perform poorly in term of loss. For example, for some borderline images, being confident e.g. {cat: 0.9, dog: 0.1} will give higher loss than being uncertain e.g. {cat: 0.6, dog: 0.4} In short, cross entropy loss measures the calibration of a model . Mis-calibration is a common issue to modern neuronal networks. They tend to be over-confident. On Calibration of Modern Neural Networks talks about it in great details.
