[site]: crossvalidated
[post_id]: 260900
[parent_id]: 
[tags]: 
Is there a way to adapt machine learning models knowing ex ante that distributions will shift?

I am currently working on a topic where I know that the distributions of the output and of the covariates will shift. I know for example that some covariates will at least follow the inflation rate. The goal is to predict the output over time, based on time series. As far as I understand machine learning methods, they suppose constant distributions over time. I thought about adapting the standardization of the covariates over time but I am not sure it would work. Is there a method used in this case to adapt the model (regression trees or SVR for example) to take that into account ? I think this issue can be divided in 2 subparts: 1. Is there a way to adapt the model knowing approximately by how much the distribution should shift? 2. Is there a way if we do not know the potential shift?
