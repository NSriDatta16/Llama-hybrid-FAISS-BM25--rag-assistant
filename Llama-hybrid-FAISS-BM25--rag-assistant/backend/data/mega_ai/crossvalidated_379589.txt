[site]: crossvalidated
[post_id]: 379589
[parent_id]: 
[tags]: 
Is the best model always one with best test score, even though it looks overfit?

I'm making a binary classification model using gradient boosting (lightgbm). I usually use learning curves to check if my model is overfitting. The metric I'm using is sklearn's average precision-recall score. When use the default model parameters parameters, I get a test metric of 0.69 but when I look at the learning curve there is a large difference between the train and validation scores with iteration number (as shown in first image). Usually, when this happens, I would reduce model complexity. In this case I reduced max_depth , num_leaves and max_bin . The learning curve is shown below, but its test score is 0.63. My question is, which is the better model. The one with best test score that looks overfit or the one with similar learning curves?
