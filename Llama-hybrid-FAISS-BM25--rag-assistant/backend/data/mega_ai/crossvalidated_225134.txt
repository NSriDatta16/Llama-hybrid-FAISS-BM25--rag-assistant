[site]: crossvalidated
[post_id]: 225134
[parent_id]: 225098
[tags]: 
You can view this as a consequence of how we define "optimal" in most reinforcement learning applications: An optimal policy is that which maximizes expected discounted reward in a Markov decision process. MDPs are RL's core and longest-studied problem, making them a natural starting point. Though natural, this definition may not fit every application. Generalized MDPs replace the $\max\limits_a$ and $\mathop{\mathbb{E}}\limits_{s'}$ operators with other non-expansions. For example, replace $\mathop{\mathbb{E}}\limits_{s'}$ with $\min\limits_{s'}$, and you have a risk-sensitive MDP. Several standard planning and learning algorithms—value iteration, policy iteration, model-based RL and Q learning—can be generalized to work in this framework. ( Szepesvári and Littman .)
