[site]: datascience
[post_id]: 107833
[parent_id]: 107775
[tags]: 
It does not make a difference if the inputs to the transformer are embeddings or vectors of a different kind. I guess the answer to the question if the attention can factor out the positional vectors is probably yes, but I do not think this is the right question. The question that you should ask is: Does the transformer need to know the order/positions of the inputs to do the task well or is the input an unordered set? If yes, is the position already encoded in the input? If yes, adding position embeddings might help, otherwise, probably not. The setup that you describe might be similar to vision-and-language models from NLP, such as UNITER where continuous image-region representations are used as an input to the transformer model. These models do not use traditional additive position embeddings and rather concatenate the image-region representations with the metadata describing the position in the image (x and y coordinates, width, and height). Pre-trained language models such as BERT use learned position embeddings instead of the original sinusoidal ones. This also might an option for you.
