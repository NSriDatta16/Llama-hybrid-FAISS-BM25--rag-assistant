[site]: crossvalidated
[post_id]: 392596
[parent_id]: 392213
[tags]: 
The decoder has different behaviours in training and inference stage. In the training stage, all the inputs for the decoder are known, so the decoder can generate all the outputs at one feed forward operation. The shape of the decoder's output should be ( $maxlen_{target}, d_{model}$ ). Then multiplied with the pre-softmax linear layer, whose shape is $(N_{w}, d_{model})$ , you will get the predicted distribution on the output vocabulary. The equation is shown as follows: $$P_{(N_{w},maxlen_{target})}=W_{(N_{w}, d_{model})}X_{(maxlen_{target}, d_{model})}^{T}$$ As described in [1], the pre-softmax linear layer can also be treated as word embedding, whose parameters can be shared with the input embedding layer. Press, Ofir, and Lior Wolf. "Using the Output Embedding to Improve Language Models." Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers. Vol. 2. 2017.
