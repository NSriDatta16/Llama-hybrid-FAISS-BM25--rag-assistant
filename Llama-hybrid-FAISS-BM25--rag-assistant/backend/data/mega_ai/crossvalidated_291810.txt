[site]: crossvalidated
[post_id]: 291810
[parent_id]: 291777
[tags]: 
I believe this is Xavier normalized initialization (implemented in several deep learning frameworks eg Keras, Cafe, ...) from Understanding the difficulty of training deep feedforward neural networks by Xavier Glorot & Yoshua Bengio. See equations 12, 15 and 16 in the paper linked: they aim to satisfy equation 12: $$\text{Var}[W_i] = \frac{2}{n_i + n_{i+1}}$$ and the variance of a uniform RV in $[-\epsilon,\epsilon]$ is $\epsilon^2/3$ (mean is zero, pdf = $1/(2\epsilon)$ so variance $=\int_{-\epsilon}^{\epsilon}x^2 \frac{1}{2\epsilon}dx$
