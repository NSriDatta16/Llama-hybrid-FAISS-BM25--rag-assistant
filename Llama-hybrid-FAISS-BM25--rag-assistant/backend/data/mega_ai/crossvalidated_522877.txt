[site]: crossvalidated
[post_id]: 522877
[parent_id]: 522874
[tags]: 
As was mentioned in the comments this idea of iteratively learning from previous model errors is at the core of boosting methodologies like Adaboost or gradient boosting. As you theorize the idea is prone to overfitting in certain models like trees but it actually regularizes a model such a linear regression (although I would just do standard regularization like l2 normally). In terms of algorithms which do well with this typically it's trees (xgboost or lightgbm are go-tos for hammers in the data science community) or some approach which partitions your data. This is because each time you refit the model you get new splits and the tree can learn new things whereas in linear regression it just updates your coefficients so you aren't actually adding any complexity or anything. Adding two regression models just averages the coefficients but adding two tree models gives you a new tree. This is similar to bagging predictors, bagging linear models will converge to fitting on the whole set whereas bagging trees actually benefits you in terms of the bias-variance tradeoff. In terms of NNs, I believe there is some theory connecting gradient boosting to residual networks and similar architectures see this question on it. My recommendation is just use lightgbm or xgboost!
