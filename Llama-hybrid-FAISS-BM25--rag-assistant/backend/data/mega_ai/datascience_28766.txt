[site]: datascience
[post_id]: 28766
[parent_id]: 21638
[tags]: 
It depends a bit on what uncertainty measures here. If it means that there is a probability p that it is the given outcome, then it's not that difficult to approach. If you have 4 outcomes and you know that outcome a has 60%, then the other three together have 40%. You could either spread this 40% uniformly over the other three categories, use the prior probabilities to spread it out or use another model to predict how it would be spread out. Once you have a model over the other probabilities, you could use a neural network classification approach where you pass these probabilities instead of one hot encoded targets. Directly passing them to a tree based method will not work although XGBoost multiclas classification might be able to be tweaked to handle this. That said, another approach would be to then resample your dataset to make it significantly bigger, and sample the target from this distribution. If uncertainty is some metric that is more a feeling than a direct mathematical thing, weighting your loss is probably better but it might not necessarily be best to take this linearly.
