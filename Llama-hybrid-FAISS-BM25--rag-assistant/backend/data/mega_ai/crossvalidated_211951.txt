[site]: crossvalidated
[post_id]: 211951
[parent_id]: 
[tags]: 
Why is the intercept of linear regression biased?

Out of curiosity, I conducted the following simulation (code below). Why is it that when the variance of the error term is large coefficient associated with the intercept is biased? Can you recommend some reference that discusses this? Or better yet is there a formal proof of that? rm(list=ls()) set.seed(12345) m The code takes thousand random samples of size 100 from the population and calculates a regression in each. Then I take the average of each of the estimated coefficients, which in theory should be equal to the original model. It works great for the slopes, but not for the intercept. Update I have noticed that with the population set at 10,000, the bias persists. If I increase the size of the population to 1,000,000, the bias disappears. The size of the target population was insufficient. Anyway, @Maarten's answer is a step in the right direction. Update2 Entire population results: > summary(lm(y~x1+x2)) Call: lm(formula = y ~ x1 + x2) Residuals: Min 1Q Median 3Q Max -367.24 -66.37 -0.33 66.64 385.26 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 7.0388165 2.6626801 2.644 0.00822 ** x1 5.0296554 0.0348480 144.331 I updated the question, including simulation results (result). I also present the model results with the entire population, which in effect is biased due to the huge variance. The simulation, as expected, reproduce those results correctly. I repeat, my mistake is in the size of the target population.
