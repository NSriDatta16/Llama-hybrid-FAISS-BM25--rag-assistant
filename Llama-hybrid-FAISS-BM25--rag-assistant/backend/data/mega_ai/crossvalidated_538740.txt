[site]: crossvalidated
[post_id]: 538740
[parent_id]: 92180
[tags]: 
I will try to explain this as per my understanding. The main idea of the section 2.4 Statistical Decision Theory is to provide a framework for developing models(e.g. least-squares regression, k-NN). As a first step(it is what author of this topic is asking about) in that section we consider regression function. Idea of the step : to show that we can use conditional expectation as a linear regression function. So $$f(x) = mx+b = \text{E}(Y|X=x)$$ where $m$ - slope, $b$ - intercept ( $X$ is vector in the example of the book but I hope it doesn't confuse.) I understand this mapping next way. Linear regression is a process to find a line closest to every point on scatter-plot. So to predict $y$ we can use expected value(or mean) of $y$ for given $x$ instead of $mx+b$ . How to prove that we are right with above assumption? 1 . We need a loss function(squared error): $$L(Y, f(X))=(Y-f(X))^2$$ Therefore the expected squared prediction error for our regression function will be: $$EPE(f) = E(L[Y, f(x)]) = E([Y-f(X)]^2) \text{ - this is how we're getting 2.9}$$ 2 . Then how to derive 2.11 from 2.10 and 2.10 from 2.9. Generally we need to follow one of properties for conditional expectation $$E(E[X|Y]) = E[X|Y = y] P(Y = y) \text{ - by law of unconscious statistician}$$ and $$E[X|Y = y] P(Y = y) = E[X] \text{ - by partition theorem from above we get this.}$$ We can do the next steps: 2.9 to 2.10: $EPE(f)= E([Y-f(X)]^2) = \int[y−f(x)]^2Pr(dx,dy)$ - this is by definition of expectation( $E(X)=∫xf(x)dx$ for continuous case) probably except for $Pr(dx, dy)$ There are 3 parts: $\int$ - because we're using continuous random variables $[y−f(x)]^2$ - this is our x from definition $Pr(dx, dy)$ - just notation for $p(x,y)dxdy$ where $p(x,y)$ is probability 2.10 to 2.11: $$\int [y−f(x)]^2Pr(dx,dy) \text{ - 2.10 formula}$$ $$=\int[y−f(x)]^2\mathbf{p(x,y)dxdy} \text{ - from the above}$$ $$=\mathbf{\int_{x}\int_{y}}[y−f(x)]^2p(x,y)dxdy \text{ - just more precise integrals}$$ $$=\int_{x}\int_{y}[y−f(x)]^2\mathbf{p(x)p(y|x)}dxdy \text{ - by multiplication rule we got this}$$ $$=\int_{x}\mathbf{(\int_{y}[y−f(x)]^2p(y|x)dy)}p(x)dx \text{ - just regrouped members}$$ $$=\int_{x}\mathbf{(E_{Y|X}([Y−f(X)]^2|X=x))}p(x)dx \text{ - by definition of conditional expectation}$$ $$=E_{X}[E_{Y|X}([Y−f(X)]^2|X=x)] \text{ - by law of unconscious statistician we get this}$$ So $E_{X}E_{Y|X}([Y−f(X)]^2|X=x)$ is generally $E(E[Y|X])$ . 3 . So far we've worked on $EPE(f)$ and proved that $E([Y-f(X)]^2)$ can be represented like this $E_{X}E_{Y|X}([Y−f(X)]2|X=x)$ Then authors say that it suffices to minimize $EPE$ pointwise for $f(x)$ . $$f(x) = argmin_{c} E_{Y|X}([Y − c]^2|X) = x$$ I thought of simple notations for regression to realize what authors mean. Specifically we can minimize squared error of regression line with partial derivatives. a. we can represent this $$SE_{line} = (y_{0}-(mx_{0}+b))^2+(y_{1}-(mx_{1}+b))^2 +...+(y_{n}-(mx_{n}+b))^2$$ like this $$SE_{line} = n\overline{y^2}-2mn\overline{yx}-2bn\overline{y}+m^2n\overline{x^2}+2mbn\overline{x}+nb^2$$ It is the same actually. b. Then we can find partial derivatives of the above with respect of $m$ (slope) and $b$ (intersect) to find minima for those variables. c. So we can use $m$ and $b$ in $mx+b$ to get predicted $y$ with minimum error. The same idea is in the book. We want to find some $c$ to get minimum for $$E_{Y|X}([Y − c]^2|X = x)\text{ (2.12)}$$ So the best prediction of $Y$ at any point $X$ is the conditional mean(mean of $Y$ 's for $X$ ) when the best is measured by average squared error. Hope it helps.
