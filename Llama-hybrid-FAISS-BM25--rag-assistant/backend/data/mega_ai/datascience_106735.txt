[site]: datascience
[post_id]: 106735
[parent_id]: 106726
[tags]: 
To clear this, we need to understand the difference between input_dim and input_shape , both are useful for two main reasons: Ensuring proper shapes are always passed. Initializing the weights without passing any dummy tensors(can call model.summary() after defining the architecture). input_shape is used to tell the model what tensor shape should it expect. input_dim is used to tell the model the number of dimensions of features. More info about it is present here . Coming back to your question, you'd see what the source of error is if you check model.summary() for both cases. Here's your code modified for brevity: Creating dataset: from numpy import array from keras.models import Sequential from keras.layers import Dense from keras.preprocessing.sequence import TimeseriesGenerator series = array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) timestep = 2 generator = TimeseriesGenerator(series, series, length=timestep, batch_size=8) Using input_shape to define your architecture: model = Sequential() model.add(Dense(100, activation='relu', input_shape=(timestep, 1))) model.add(Dense(1)) model.compile(optimizer='adam', loss='mse') model.summary() Output: Model: "sequential" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 2, 100) 200 dense_1 (Dense) (None, 2, 1) 101 ================================================================= Total params: 301 Trainable params: 301 Non-trainable params: 0 _________________________________________________________________ Using input_dim to define your architecture: model = Sequential() model.add(Dense(100, activation='relu', input_dim=timestep)) model.add(Dense(1)) model.compile(optimizer='adam', loss='mse') model.summary() Output: Model: "sequential_1" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_2 (Dense) (None, 100) 300 dense_3 (Dense) (None, 1) 101 ================================================================= Total params: 401 Trainable params: 401 Non-trainable params: 0 _________________________________________________________________ The problem is with you adding another dimension to your expected shape in input_shape . To correct it, you can pass a single-valued tensor as input_shape=(timestep,) : model = Sequential() model.add(Dense(100, activation='relu', input_shape=(timestep,))) model.add(Dense(1)) model.compile(optimizer='adam', loss='mse') model.summary() Output: Model: "sequential_2" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_4 (Dense) (None, 100) 300 dense_5 (Dense) (None, 1) 101 ================================================================= Total params: 401 Trainable params: 401 Non-trainable params: 0 _________________________________________________________________ You can cross-verify the results by fitting and checking with your own tensor as you did above. The same logic applies to RNN, here's the input shape that it expects.
