[site]: crossvalidated
[post_id]: 592977
[parent_id]: 592069
[tags]: 
Yes, gradient boosted decision trees can make the same splits in multiple trees (or even have several identical trees). That may or may not be a good thing. Let's say there's a single split that truly corresponds to the true underlying data generating process. So, with a low learning rate the best thing XGBoost can do (assuming it can "figure that out" despite any noise in the data) is to have all trees identical and just have that one split in all of them. In this example, it so happens that a low learning rate is not really needed (with a high enough learning rate you could just have a single tree with the one split you need), but if you go with the usual recommendation of just fixing the learning rate to a low value, you can still get a just-as-good model by having lots of identical trees. That sort of answers your question of My understanding of gradient boosting machines is briefly, that additional models are fitted iteratively based on the residuals of the previous model. The answer to that is basically about the learning rate. The final prediction of an XGBoost ensemble of multiple trees is a weighted sum of the predictions of all the trees and the lower the learning rate the less weight each tree has. So, if you have only one tree at the start and that single tree gets everything perfectly right, this still does not count as getting things perfect as far as the residuals are concerned, but if you have many identical trees that would eventually count as getting things (almost) perfectly correct.
