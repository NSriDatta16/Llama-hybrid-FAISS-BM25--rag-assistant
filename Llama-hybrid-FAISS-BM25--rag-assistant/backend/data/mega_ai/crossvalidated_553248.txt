[site]: crossvalidated
[post_id]: 553248
[parent_id]: 
[tags]: 
Quantify likelihoods information within a Bayesian framewrok

Suppose that I have two likelihood functions $L_{1}(N; M)$ and $L_{2}(N; C)$ . Both those likelihoods are functions of the same parameter $N$ . However, their expressions are different, as the data $M$ are finer more informative version of data $C$ , i.e. from data $M$ I can go to data $C$ but not the opposite. I work on a Bayesian framework so I use the same prior for both the likelihood functions which is denoted as $N\sim f(N)$ What I would like to know is, if there is a way to quantify the $\textit{evidence}$ or $\textit{information}$ or $\textit{impact}$ of the likelihoods with the ultimate goal to argue which likelihood function contributes more into the Bayesian inference. What I've thought so far, Measure the distance between the posterior $p_{1}(N|M)$ , and $f(N)$ and compare it with the distance between $p_{2}(N|C)$ and $f(N)$ . The larger the distance the larger the contribution of the likelihood, so the larger the information held into the likelihood. Because $M$ is a more informative data representation compared to $C$ maybe I should take the distance between $p_{1}(N|M)$ and $p_{2}(N|C)$ and see if it is significant. If there it is, then it would mean that the information held in $L_{1}(N;M)$ is larger to $L_{2}(N;C)$ Are there any papers that try to do a similar inference, or compare likelihoods information contribution to a Bayesian framework??
