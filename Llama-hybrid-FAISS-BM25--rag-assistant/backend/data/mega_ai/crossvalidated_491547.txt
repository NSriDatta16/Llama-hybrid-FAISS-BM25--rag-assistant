[site]: crossvalidated
[post_id]: 491547
[parent_id]: 
[tags]: 
Does it make sense to regularize the loss function for binary/multi-class classification?

When discussing linear regression it is well known that you can add regularization terms, such as, $$\lambda \|w\|^2 \quad \text{(Tikhonov regularization)}$$ to the empirical error/loss function. However, regularization seems to be under-discussed when it comes to binary/multi-class training. For example, I've browsed through hundreds of code examples online for CNN training and not one has included a regularization term to the cross-entropy loss function. This makes me wonder a couple of things: does adding regularization to the loss functions for binary/multi-class classification training make sense? if so, what type of regularization makes sense and why? if not, why not? Hope someone can answer.
