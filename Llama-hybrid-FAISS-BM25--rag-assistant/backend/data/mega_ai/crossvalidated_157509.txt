[site]: crossvalidated
[post_id]: 157509
[parent_id]: 157502
[tags]: 
kNN from a Bayesian viewpoint Let suppose that we have a data set comprising $N_{k}$ points in class $\mathcal{C}_{k}$ with $N$ total points, so that $\sum_{k}N_{k} = N$. We want to classify a new point $\mathbf{x}$ by drawing a sphere centred on $\mathbf{x}$ containing precisely $K$ points irrespective of their class. Suppose that such a sphere has volume $V$ and contains $K_{k}$ points from class $\mathcal{C}_{k}$. Then, $$ p(\mathbf{x}|\mathcal{C}_{k}) = \frac{K_{k}}{N_{k}V}$$ provides an estimate of the density associated with each class. Similarly, the unconditional density is given by $$ p(\mathbf{x}) = \frac{K}{NV}, $$ while the class priors are given by $$ p(\mathcal{C}_{k}) = \frac{N_{k}}{N}. $$ We can now combine the three equations using Bayes' theorem to obtain the posterior probability of class membership $$ p(\mathcal{C}_{k}|\mathbf{x}) = \frac{ p(\mathbf{x}|\mathcal{C}_{k}) p(\mathcal{C}_{k})}{p(\mathbf{x})} = \frac{K_{k}}{K}. $$ If we wish to minimize the probability of misclassification, we have to assign the test point $\mathbf{x}$ to the class having the largest posterior probability, corresponding to the largest value of $\frac{K_{k}}{K}$.
