[site]: datascience
[post_id]: 84269
[parent_id]: 
[tags]: 
How to apply pruning on a BERT model?

I have trained a BERT model using ktrain (tensorflow wrapper) to recognize emotion on text, it works but it suffers from really slow inference. That makes my model not suitable for a production environment. I have done some research and it seems pruning could help. Tensorflow provides some options for pruning e.g. tf.contrib.model_pruning .The problem is that it is not a widely used technique and I can not find a simple enough example that could help me to understand how to use it. Can someone help? Only answers that include a coding solution will be considered for the bounty. I provide my working code below for reference. import pandas as pd import numpy as np import preprocessor as p import emoji import re import ktrain from ktrain import text from unidecode import unidecode import nltk #text preprocessing class class TextPreprocessing: def __init__(self): p.set_options(p.OPT.MENTION, p.OPT.URL) def _punctuation(self,val): val = re.sub(r'[^\w\s]',' ',val) val = re.sub('_', ' ',val) return val def _whitespace(self,val): return " ".join(val.split()) def _removenumbers(self,val): val = re.sub('[0-9]+', '', val) return val def _remove_unicode(self, text): text = unidecode(text).encode("ascii") text = str(text, "ascii") return text def _split_to_sentences(self, body_text): sentences = re.split(r"(? ```
