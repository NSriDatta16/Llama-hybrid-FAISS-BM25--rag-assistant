[site]: crossvalidated
[post_id]: 530007
[parent_id]: 526655
[tags]: 
I mean, for example, if u have an input data of mostly positive values maybe the activation function that will work better is ReLu. Not necessarily. The weights to that layer could contain negative values which could result in the hidden unit value sitting in the saturation region of the ReLU. Additionally the bias to the layer can also do the same thing. And anyway, why would you want your hidden units to only be firing in the positive linear region of a ReLU? It's no different to linear activation in that scenario, and the whole point of neural networks is to universally approximate any function (asymptotically) given enough hidden non-linear units. 1 linear layer followed by 1 non-linear layer is identical to just 1 non-linear layer as the linear layer can be 'folded' into the weights of the non-linear layer. Or if u have an input feature with string values maybe you have to one-hot them and use softmax function. For an input or hidden layer there is not much reason to use Softmax. Softmax simply 'converts' the unit values across the layer into a valid probability distribution where all values are in the range $[0, 1]$ and their sum is $1$ . I have never seen it used for input or hidden layers. Softmax is mostly used for an output layer in a multiclass classification problem. In a binary classification problem with only one output logit it makes sense to instead use sigmoid activation. Some activations are special purpose, and more often than not you will be choosing specific output activations based on what you are training your model be, but it regression, binary classification, multiclass classification, etc. Otherwise, for your hidden layers you should experiment with activations based on how they perform in your network, rather than what type of input you're feeding them. Try out a few different activations in your hidden layers and evaluate their performance on a validation set. ReLU is a good place to start, it has displaced sigmoids for hidden layers and it has yet to be replaced in real world use by any of the newer trendy activations like SELU or Swish. SELUs were meant to be 'the next big thing' but they require so much work to actually get the network to behave and in my experience they underdeliver compared to ReLUs with residual connections.
