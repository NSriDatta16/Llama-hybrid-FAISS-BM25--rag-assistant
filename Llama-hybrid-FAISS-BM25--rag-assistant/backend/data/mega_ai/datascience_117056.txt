[site]: datascience
[post_id]: 117056
[parent_id]: 117046
[tags]: 
Take a look at PCA . PCA is used to reduce the dimension of your feature space. You could use PCA to transform your large set of variables into a smaller one that still contains most of the information in the large set. This helps reduce overfitting and simplifies computation. Once you have the features selected, you can use your domain knowledge to explain why certain features may have an impact on your model performance. You may also try different subsets of features to determine which produce the best model. Here's some simple starter code for PCA from sci-kit learn : import numpy as np from sklearn.decomposition import PCA X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) pca = PCA(n_components=2) pca.fit(X) PCA(n_components=2) print(pca.explained_variance_ratio_) >>> [0.9924... 0.0075...] print(pca.singular_values_) >>> [6.30061... 0.54980...]
