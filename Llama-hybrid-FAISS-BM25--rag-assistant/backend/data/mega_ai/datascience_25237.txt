[site]: datascience
[post_id]: 25237
[parent_id]: 25209
[tags]: 
In Value function methods (or Critic methods) we usually choose one of the following options to select our actions after we have estimated the relevant value function: Boltzman function with a (inverse) temperature parameter that controls how much we 'listen' the maximum of the value function to select our actions. $\epsilon$-greedy: selects the action with the maximum value function with probability (1-$\epsilon$) and a random one with probability $\epsilon$. In Policy Gradient (or Actor Methods) we have two approaches: Stochastic Policy Gradients (SPG): Output is a probability over actions. For your algorithm, the output would be the parameters of a pre-specified distribution (usually Gausian) as Neil described. Then you sample that distribution in a similar way you sample the Boltzman distribution. Deterministic Policy Gradients (DPG): Output is the value of an action (e.g. speed, height etc). From the above you can clearly see that when continuous action space is involved, PG offers much more plausible solution. For more information I suggest you the Master thesis PG Methods: SGD-DPG of Riashat. He worked closely with one of the co-authors of Silver's DPG paper and the thesis is very well structured with MATLAB code available. Policy Gradients methods are much harder to understand because of the math involved (and many times the different notation used). However I would suggest you to start from Jan Peters and then get back to Silver's slides (my opinion :) ).
