[site]: crossvalidated
[post_id]: 572322
[parent_id]: 572306
[tags]: 
You should try it both ways and see for yourself, as it'll largely depend on the dataset. However "stretching" the data can be somewhat helpful in this case from an numerical point of view. Your neural network is basically outputting some real-valued $u \in \mathbb{R}$ , then you apply a sigmoid: Since your data ranges from [0.05, 0.2], the last hidden layer should range between approximately [-1.28, -0.60]. If you scaled the data to range [0, 1], then the hidden layer is just anything in (- $\infty$ , $\infty$ ). When it's unscaled, the hidden layer has to be that much more numerically precise/stable compared to the scaled version. If you have a very deep network or complicated architecture, or the data is very noisy, then I suspect there might be some differences. Or maybe not.
