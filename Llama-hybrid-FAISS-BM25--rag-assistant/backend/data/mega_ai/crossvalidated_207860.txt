[site]: crossvalidated
[post_id]: 207860
[parent_id]: 
[tags]: 
Bayesian minimum mean square error estimator

In Fundamentals of Statistical Signal Processing, Estimation Theory , by Steven M. Kay the author shows on p. 312-313 that the estimator $p(A\mid x)$ minimises the Bayesian mean square error when you want to use the data $\boldsymbol{x}$ to estimate the parameter $A$ . At the beginning of the proof he writes $\text{Bmse}(\hat{A})=\int[\int(A-\hat{A})p(A\mid \boldsymbol{x})dA ]p(x)d\boldsymbol{x}$ and says that if we can minimise the expression in brackets for each $\boldsymbol{x}$ then the Bmse will be minimised. He then fixes $\boldsymbol{x}$ and says that $\hat{A}$ is a scalar variable. He also takes the partial derivative of $\int (A-\hat{A})^2p(A\mid \boldsymbol{x})dA$ with respect to $\hat{A}=\hat{A}(\boldsymbol{x})$ . My questions are: Exactly what does it mean that $\hat{A}=\hat{A}(\boldsymbol{x})$ is a scalar variable when we fix $\boldsymbol{x}$ ? Why is it possible to view $\int (A-\hat{A})^2p(A\mid \boldsymbol{x})dA$ as a function of $\hat{A}$ , when previously $\int (A-\hat{A})^2p(A\mid \boldsymbol{x})dA$ was considered a function of $\boldsymbol{x}$ ?
