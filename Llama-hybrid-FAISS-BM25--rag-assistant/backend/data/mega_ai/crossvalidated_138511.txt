[site]: crossvalidated
[post_id]: 138511
[parent_id]: 137917
[tags]: 
The closest I got to an answer on this was the approach taken by the precision_recall_fscore_support function and the classification_report function in scikit-learn. By default, the precision, recall, F1 score and support of each label is returned. That means that if you have only two labels, where one label is the opposite of the other label, then one F1 score is going to be the "opposing F1 score" of the other. >>> from sklearn.metrics import precision_recall_fscore_support >>> import numpy as np >>> y_true = np.array(["dupe", "diff", "diff", "diff", "dupe", "diff"]) >>> y_pred = np.array(["dupe", "dupe", "diff", "diff", "diff", "diff"]) >>> precision_recall_fscore_support(y_true, y_pred, labels=["dupe", "diff"]) (array([ 0.5 , 0.75]), array([ 0.5 , 0.75]), array([ 0.5 , 0.75]), array([2, 4])) >>> print classification_report(y_true, y_pred) precision recall f1-score support diff 0.75 0.75 0.75 4 dupe 0.50 0.50 0.50 2 avg / total 0.67 0.67 0.67 6 So, how can we combine them to create an average? There are multiple ways. classification_report uses a weighted average, and precision_recall_fscore_support lets you choose between the averaging methods micro , macro , samples , weighted . Update: This led me to find this interesting draft paper: Macro- and micro-averaged evaluation measures by Vincent Van Asch.
