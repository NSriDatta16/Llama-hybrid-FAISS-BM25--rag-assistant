[site]: crossvalidated
[post_id]: 358229
[parent_id]: 358227
[tags]: 
You probably need to divide by 3, as formally cross entropy is the average of each point's contribution. To be clear, the $n$ is only necessary when you have test observations. The usual formula is $H:=-\sum_{x}p(x)\log(q(x))$, where the sum (or integral in the continuous case) is over the state space , so if you want to make an empirical estimate of cross entropy over your training set, then the formula is $H:=-\frac{1}{n}\sum_{i=1}^ny_i\log(f(x_i))$, where $x_i$ is each datapoint. For example if $P(1)=0.3$, then you typicall expect to see 30% of your data with $y=1$, so then out of 100 samples, you'd expect 30 $Y$'s, and $30/100=0.3$. The logic is the same as say, finding the average of $n$ numbers from a distribution $p(x)$. Note that many implementations don't bother dividing by $n$, since it is immaterial in the optimization that takes place to extremize $H$.
