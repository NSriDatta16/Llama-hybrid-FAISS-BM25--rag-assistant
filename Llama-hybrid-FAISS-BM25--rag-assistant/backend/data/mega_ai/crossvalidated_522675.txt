[site]: crossvalidated
[post_id]: 522675
[parent_id]: 
[tags]: 
Why is reverse KL more suited for data generation

Here goes a first question! In a paper I'm reading in the context of GAN's (WGAN in particular) I came across the following quote when the authors discuss KL divergence: while maximum likelihood incentivises model distributions under which the data from the empirical distribution is likely, data generation might be more interested in the reverse: model distributions which produce data that is likely under the empirical distribution I have no clue how to interpret that statement. I intuitively understand KL divergence. I understand it is asymmetric so, $KL(P_{\theta_*}, P_{\theta}) \neq KL(P_{\theta},P_{\theta_*})$ , where the true parameter is $\theta_*$ and the data comes from $P_{\theta_*}$ . I understand that I can minimise $KL(P_{\theta_*}, P_{\theta})$ w.r.t. $\theta$ because it is equivalent to minimising $E_{\theta_*}(ln(p_{\theta}(x)))$ w.r.t $\theta$ and I can estimate $E_{\theta_*}(ln(p_{\theta}(x)))$ using the LLN. Intuitively I think this means finding the parameter value that makes the data most likely. But I feel like my way of looking at KL divergence is also limiting my understanding. How would I go about interpreting what the quote called "the reverse": $KL(P_{\theta}, P_{\theta_*})$ . The only reason I was able to estimate $E_{\theta_*}(ln(p_{\theta}(x)))$ is because my data comes from the "true" distribution parametrised by $\theta_*$ . I have no way of calculating $E_{\theta}(ln(p_{\theta_*}(x)))$ because I don't know $\theta_*$ , so I can't use my statistical hammer (LLN). I would appreciate any help. Link to the paper (quote @ middle page 5): https://arxiv.org/abs/1909.02210 Niels
