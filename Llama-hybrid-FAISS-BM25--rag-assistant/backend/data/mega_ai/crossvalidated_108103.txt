[site]: crossvalidated
[post_id]: 108103
[parent_id]: 108086
[tags]: 
Just because an analysis is sensitive to changes to the model doesn't necessarily imply that the researcher tried many models and then published the one that worked. One need only hypothesize the existence of a number of researchers each considering a somewhat similar question, each trying only one analysis, and the first one that tries the 'successful' model is the one who published. The lack of any sensitivity analysis would then be more an indication of incompetence or carelessness rather than actual significance hunting by any one individual. That's not to say it (significance hunting) never happens, it's just that you can't tell by the mere existence of such a situation that it was what went on in any particular case. In practice I see ignorant incompetence or flat carelessness (in the face of the publish or perish mentality, it's no surprise at all - what's the payoff for taking the time to do things with proper care?) much more often than deliberate misconduct; all it takes above that is a large enough cohort of people thinking about similar questions for all those "special case" results that don't really generalize to eventually turn up in the literature. The source of what is effective significance hunting, even when everyone tries only one model is the existence of journals that want to publish 'significant' results. They act to create results that can't be reproduced (that are noise, basically). If the journals want to publish results that are nearly all false positives, all they need do is continue as they are. Even many journals that have an official policy geared toward avoiding these problems nevertheless have a de facto policy of only publishing significant results, because they accept the practices and recommendations of reviewers who insist on them in spite of the stated policy. To get better results, they need to actively encourage a focus on sizes of effects, and concentrate a significant fraction of effort on publishing well-conducted studies that find nothing. [If the professional groups and journal editorial boards - and editors - can't comprehend the scientific importance of not finding anything going on (and the very dire consequences of ignoring the null results), they should probably get out of that business and go into something relatively more scientifically honest, like running a psychic reading phone line. At the least, any pretence of rigor should be dropped.] In the vein of suggesting that may people are not doing it deliberately, here are a few more-or-less unconscious 'dirty tricks' that can inflate rates of finding significant results: doing model/variable selection without accounting for the effect of it. This is probably the biggest one, it can happen at a number of points in the research, and the researcher may not even be conscious that they're doing it (sometimes it's subtle). Indeed, the basic 'modelling cycle' paradigm (like the flowchart found in Box and Jenkins, say) leads to this issue. That's not a criticism of that flowchart, by the way - but one must properly deal with the effect of these procedures. It's worth reading Frank Harrell's book ( Regression Modelling Strategies ), in particular chapter 4. Related information can be found in a number of other places. focusing on model-assessment (particularly looking at things like normality, homoskedasticity or considering the addition of interaction terms or terms to pick up curvature), or just focusing harder on it when nothing is found - and then trying a new analysis when some failure of assumptions is detected - than when a desired result is found. trying to find out if there's a better post-hoc procedure when the first one you tried doesn't give the result you needed.
