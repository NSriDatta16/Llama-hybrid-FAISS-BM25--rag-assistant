[site]: crossvalidated
[post_id]: 571514
[parent_id]: 
[tags]: 
Why does this QR-DQN network not learn?

I referred to https://github.com/senya-ashukha/quantile-regression-dqn-pytorch/blob/master/qr-dqn-solution-cool.ipynb I have no problem solving CartPole problem when using the basic Linear network as follows: def __init__(self, int_states, int_actions, int_quants): nn.Module.__init__(self) self.int_quants = int_quants self.int_actions = int_actions self.layer1 = nn.Linear(int_states, 256) self.layer2 = nn.Linear(256, int_actions*int_quants) def forward(self, state): state = self.layer1(state) state = torch.tanh(state) state = self.layer2(state) return state.view(-1, self.int_actions, self.int_quants) Before applying it to Atari games, I tried solving the same CartPole problem with Conv2D networks. However, it doesn't learn anything this time. I made a (2 channels, 1 row, 2 columns) array and set the 2nd channel of which as state: >>> state array([[[ 0. , 0. , 0. , 0. ]], [[ 0.22120677, 1.75521159, -0.26231307, -2.71662474]]]) I thought that the new network would be able to find out that the 1st channel is meaningless and, ultimately, it would work the same as the Linear network. I'd like to know what I've missed or if there are errors. The full Conv2d code that I tested: import gym import torch import pickle import random import numpy as np import torch.nn as nn import torch.optim as optim import torch.nn.functional as F from collections import deque def huber(x, k=1.0): return torch.where(x.abs() eps: action = self.forward(state).mean(2).max(1)[1] else: action = torch.randint(0, 2, (1,)) return int(action) class Agent(): def __init__(self, int_deqSize, int_batchSize=32): self.memory = deque(maxlen=int_deqSize) self.int_batchSize = int_batchSize self.Zmain = Network(int_states=int_states, int_actions=int_actions, int_quants=int_quants ) self.Ztarget = Network(int_states=int_states, int_actions=int_actions, int_quants=int_quants ) self.Ztarget.load_state_dict(self.Zmain.state_dict()) self.optimizer = optim.Adam(self.Zmain.parameters(), lr=1e-3) def push(self, state, action, stateN, reward, done): self.memory.append((state, action, stateN, reward, done)) self.replay() def sample(self): if len(self.memory) > self.int_batchSize: batch = random.sample(self.memory, self.int_batchSize) states, actions, statesN, rewards, dones = zip(*batch) states = torch.Tensor(np.array(states)) actions = torch.Tensor(np.array(actions)) statesN = torch.Tensor(np.array(statesN)) rewards = torch.Tensor(np.array(rewards)) dones = torch.Tensor(np.array([int(val) for val in dones])) return states, actions, statesN, rewards, dones def replay(self): if len(self.memory) > self.int_batchSize: states, actions, statesN, rewards, dones = self.sample() theta = self.Zmain(states)[np.arange(self.int_batchSize), actions.long()] Znext = self.Ztarget(statesN).detach() Znext_max = Znext[np.arange(self.int_batchSize), Znext.mean(2).max(1)[1]] Ttheta = rewards.unsqueeze(1) + gamma * (1 - dones.unsqueeze(1)) * Znext_max self.Ttheta = Ttheta.unsqueeze(1) self.theta = theta.unsqueeze(1) diff = Ttheta.unsqueeze(1) - theta.unsqueeze(1) self.diff = diff loss = huber(diff) * (tau - (diff.detach() ```
