[site]: crossvalidated
[post_id]: 112490
[parent_id]: 41557
[tags]: 
My opinion is that an efficient and simple solution in practice is perhaps possible for small sample sizes. First to quote Wikipedia on the topic of Median: "For univariate distributions that are symmetric about one median, the Hodgesâ€“Lehmann estimator is a robust and highly efficient estimator of the population median.[21]" The HL median estimate is especially simple for small samples of size n, just compute all possible two point (including repeats) averages. From these n(n+1)/2 new constructs, compute the HL Median Estimator as the usual sample median. Now, per the same Wikipedia article on the median, the cited variance of the median 1/(4*n*f(median)*f(median)). However, for a discrete sample of size n, I would argue that a conservative estimate to assume for the value of the density function at the median point is 1/n, as we are dividing by this term. As a consequence, the variance of the median is expected to be n/4 or lower. For large n, this would be poor, so yes a more complex (and some would suggest subjective) exercise involving re-sampling could be employed to construct bins of the optimal width so as provide a greater probability mass for f(median). Now if the purpose of the variance estimate is to gain a precision estimate on the median, may I suggest employing the following bounds due to Mallow assuming the Median is greater than Mean, namely: Median - Mean is less than or equal to Sigma (or, -Sigma when the Median is less than the Mean). Equivalently, the Median lies between the Mean plus Sigma and the Mean minus Sigma. So, inserting population estimators for the mean and sigma, possibly robust, one can establish a bound for the median that would be consisent with the provided mean and sigma estimates based on the sample population.
