[site]: crossvalidated
[post_id]: 540685
[parent_id]: 540657
[tags]: 
In ML estimation, one maximizes the likelihood of the data given the parameters. In your case: $$ \hat{k}_{\mathrm{ML}} = \arg\max_k p(G_1, G_2, \ldots, G_N, H_1, H_2, \ldots, H_N \mid k). $$ If we assume that $$ G_i = \frac{1}{1+H_i k} + \varepsilon , \quad \varepsilon \sim \mathcal{N}(0, \sigma_\varepsilon ), $$ i.e., we assume Gaussian measurement noise on the $G_i$ and independent measurements, we obtain $$ \begin{align} \hat{k}_{\mathrm{ML}} &= \arg\max_k p(G_1, G_2, \ldots, G_N, H_1, H_2, \ldots, H_N \mid k) \\ &= \arg\max_k p(G_1, G_2, \ldots, G_N \mid k) \\ &= \arg\max_k \prod_{i=1}^N p(G_i \mid k) \\ &= \arg\max_k \log \prod_{i=1}^N p(G_i \mid k) \\ &= \arg\max_k \sum_{i=1}^N\log p(G_i \mid k) \end{align} $$ Furthermore, we have that $$ p(G_i \mid k) = \mathcal{N}(G_i; \mu_i(k)=\frac{1}{1+H_i k}, \sigma_{\varepsilon })=\frac{1}{\sigma_\varepsilon\sqrt{2\pi}} \mathrm{e}^{-\frac{1}{2}(\frac{G_i-\mu_i}{\sigma_\varepsilon})^2}.$$ Plugging that into the above optimization problem, we get $$ \begin{align} \hat{k}_{\mathrm{ML}} &= \arg\max_k \sum_{i=1}^N -\frac{1}{2}\left(\frac{G_i-\mu_i(k)}{\sigma_\varepsilon}\right)^2 -N\log (\sigma_\varepsilon \sqrt{2\pi}) \\ &= \arg\max_k \sum_{i=1}^N -\frac{1}{2}\left(\frac{G_i-\mu_i(k)}{\sigma_\varepsilon}\right)^2 \\ &= \arg\min_k \sum_{i=1}^N (G_i-\mu_i(k))^2 \end{align} $$ A few remarks: This particular example is an ordinary least squares problem a nonlinear least-squares estimation problem (as can be seen in the last equation). If the measurements $H_i$ are also assumed to be noisy, things get more complex. In that case, we're dealing with a nonlinear errors-in-variables regression problem . The optimization problem can still be solved numerically, of course. What happened to the information that $k$ is normally distributed in the general population? That was not used at all in the above derivation, because ML estimation does not consider such priors. If you want to take that into account, you can do maximum a posteriori (MAP) estimation , which is essentially ML estimation + a prior on the parameters. MAP estimation maximizes $p(k \mid G_1, \ldots, G_N, H_1, \ldots, H_N).$ Using Bayes theorem, we have that $$ \begin{align} \hat{k}_{\mathrm{MAP}} &= \arg\max_k p(k \mid G_1, \ldots, G_N, H_1, \ldots, H_N) \\ &= \arg\max_k \frac{p(G_{1:N}, H_{1:N} \mid k) \, p(k)}{p(G_{1:N}, H_{1:N})} \\ &= \arg\max_k p(G_{1:N}, H_{1:N} \mid k) \, p(k) \\ &= \arg\max_k \log p(G_{1:N}, H_{1:N} \mid k) + \log p(k) \\ &= \arg\max_k \log p(k) + \sum_{i=1}^N\log p(G_i \mid k), \end{align} $$ where $p(k)$ denotes prior knowledge about the distribution of $k$ . In your example, that could be the population average. We see that this is exactly what we had above for the ML estimate, except for the additional term $\log p(k)$ .
