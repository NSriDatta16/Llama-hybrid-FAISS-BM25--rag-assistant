[site]: datascience
[post_id]: 48301
[parent_id]: 48294
[tags]: 
The statement "it tests combination of features" is not true . It tests individual features. However, a tree can approximate any continuous function $f$ over training points, since it is a universal approximator just like neural networks. In Random Forest (or Decision Tree, or Regression Tree), individual features are compared to each other, not a combination of them, then the most informative individual is peaked to split a leaf. Therefore, there is no notion of "better combination" in the whole process. Furthermore, Random Forest is a bagging algorithm which does not favor the randomly-built trees over each other, they all have the same weight in the aggregated output. It is worth noting that "Rotation forest" first applies PCA to features, which means each new feature is a linear combination of original features. However, this does not count since the same pre-processing can be used for any other method too. EDIT : @tam provided a counter-example for XGBoost, which is not the same as Random Forest. However, the issue is the same for XGBoost. Its learning process comes down to splitting each leaf greadily based on a single feature instead of selecting the best combination of features among a set of combinations, or the best tree among a set of trees. From this explanation , you can see that The Structure Score is defined for a tree (which is a function) based on the first- and second-order derivatives of loss function in each leaf $j$ ( $G_j$ and $H_j$ respectively) summed over all $T$ leaves, i.e. $$\text{obj}^*=-\frac{1}{2} \sum_{j=1}^{T}\frac{G_j}{H_j + \lambda} + \gamma T$$ However, the optimization process greedily splits a leaf using the best individual feature that gives the highest gain in $\text{obj}^*$ . A tree $t$ is built by greedily minimizing the loss, i.e. branching on the best individual feature, and when the tree is built, process goes to create the next tree $t+1$ in the same way, and so on. Here is the key quote from XGBoost paper : This score is like the impurity score for evaluating decision trees, except that it is derived for a wider range of objective functions [..] Normally it is impossible to enumerate all the possible tree structures q. A greedy algorithm that starts from a single leaf and iteratively adds branches to the tree is used instead. In summary: Although a tree represents a combination of features (a function), but none of XGBoost and Random Forest are selecting between functions. They build and aggregate multiple functions by greedily favoring individual features.
