[site]: crossvalidated
[post_id]: 578760
[parent_id]: 578721
[tags]: 
It's much better to work with the underlying numerators and denominators to use a model for binomial outcomes instead (e.g. some form of logistic regression - note: talking about some "test" and some "regression model" is more or less the same thing, because almost every test can be re-expressed as testing a coefficient from a regression model). That would appropriately reflect that each of these numbers may have different levels of uncertainty around them (e.g. 0.7 could be 7 out of 10, or it could be 700,000 out of 1,000,000, which come with very different uncertainties about the estimated proportions, which your analysis model should capture). Additionally, you might consider beta-regression (that would correctly capture that these proportions must lie between 0 and 1), but it would still completely ignore the uncertainty around the proportions. A t-test of just the numbers in each cell, would completely underestimate the amount of information you have, because it would not be able to "tell" whether to what extent variation between the cells comes from the proportions between different or from the error term. Instead, if you do what the other solution suggests (creating 0 and 1 records according to the underlying denominators), that might be an okay approximation to a better solution, if the underlying denominators are very large and about the same for all the cells. However, then there's no reason to not use a more appropriate model for binomial data.
