[site]: crossvalidated
[post_id]: 166349
[parent_id]: 
[tags]: 
How to scale a skewed-distribution predictor for a neural network?

I’d like to ask what would be the more appropriate way of introducing a predictor variable with a highly skewed distribution into a neural network model. Suppose I had a variable whose distribution resembled a power law – that is, highly skewed to the right with an exponential decrease; and the typical preprocessing of subtracting the mean and dividing by the standard deviation resulted in a distribution where there are values of up to 20 but most of the values lie only in a certain small range. On the other hand, scaling it to be between zero and one would result in most of the values being concentrated at around, say, .1 with the highest deciles occupying all the rest of the range. Applying a log or square root transformation and then scaling it would result in somewhat better-distributed values, but still highly skewed. How would the optimization process be affected by having this kind of skewness in a predictor variable, supposing that most of the other predictors follow a distribution that is close to normal?
