[site]: crossvalidated
[post_id]: 104801
[parent_id]: 103193
[tags]: 
1) As regards your first question, some tests statistics have been developed and discussed in the literature to test the null of stationarity and the null of a unit root. Some of the many papers that were written on this issue are the following: Related to the trend: Dickey, D. y Fuller, W. (1979a), Distribution of the estimators for autoregressive time series with a unit root, Journal of the American Statistical Association 74, 427-31. Dickey, D. y Fuller, W. (1981), Likelihood ratio statistics for autoregressive time series with a unit root, Econometrica 49, 1057-1071. Kwiatkowski, D., Phillips, P., Schmidt, P. y Shin, Y. (1992), Testing the null hypothesis of stationarity against the alternative of a unit root: How sure are we that economic time series have a unit root?, Journal of Econometrics 54, 159-178. Phillips, P. y Perron, P. (1988), Testing for a unit root in time series regression, Biometrika 75, 335-46. Durlauf, S. y Phillips, P. (1988), Trends versus random walks in time series analysis, Econometrica 56, 1333-54. Related to the seasonal component: Hylleberg, S., Engle, R., Granger, C. y Yoo, B. (1990), Seasonal integration and cointegration, Journal of Econometrics 44, 215-38. Canova, F. y Hansen, B. E. (1995), Are seasonal patterns constant over time? a test for seasonal stability, Journal of Business and Economic Statistics 13, 237-252. Franses, P. (1990), Testing for seasonal unit roots in monthly data, Technical Report 9032, Econometric Institute. Ghysels, E., Lee, H. y Noh, J. (1994), Testing for unit roots in seasonal time series. some theoretical extensions and a monte carlo investigation, Journal of Econometrics 62, 415-442. The textbook Banerjee, A., Dolado, J., Galbraith, J. y Hendry, D. (1993), Co-Integration,Error Correction, and the econometric analysis of non-stationary data, Advanced Texts in Econometrics. Oxford University Press is also a good reference. 2) Your second concern is justified by the literature. If there is a unit root test then the traditional t-statistic that you would apply on a linear trend does not follow the standard distribution. See for example, Phillips, P. (1987), Time series regression with unit root, Econometrica 55(2), 277-301. If a unit root exists and is ignored, then the probability of rejecting the null that the coefficient of a linear trend is zero is reduced. That is, we would end up modelling a deterministic linear trend too often for a given significance level. In the presence of a unit root we should instead transform the data by taking regular differences to the data. 3) For illustration, if you use R you can do the following analysis with your data. x First, you can apply the Dickey-Fuller test for the null of a unit root: require(tseries) adf.test(x, alternative = "explosive") # Augmented Dickey-Fuller Test # Dickey-Fuller = -2.0685, Lag order = 3, p-value = 0.453 # alternative hypothesis: explosive and the KPSS test for the reverse null hypothesis, stationarity against the alternative of stationarity around a linear trend: kpss.test(x, null = "Trend", lshort = TRUE) # KPSS Test for Trend Stationarity # KPSS Trend = 0.2691, Truncation lag parameter = 1, p-value = 0.01 Results: ADF test, at the 5% significance level a unit root is not rejected; KPSS test, the null of stationarity is rejected in favour of a model with a linear trend. Aside note: using lshort=FALSE the null of the KPSS test is not rejected at the 5% level, however, it selects 5 lags; a further inspection not shown here suggested that choosing 1-3 lags is appropriate for the data and leads to reject the null hypothesis. In principle, we should guide ourselves by the test for which we were able to the reject the null hypothesis (rather than by the test for which we did not reject (we accepted) the null). However, a regression of the original series on a linear trend turns out to be not reliable. On the one hand, the R-square is high (over 90%) which is pointed in the literature as an indicator of spurious regression. fit |t|) #(Intercept) 28499.3 381.6 74.69 On the other hand, the residuals are autocorrelated: acf(residuals(fit)) # not displayed to save space Moreover, the null of a unit root in the residuals cannot be rejected. adf.test(residuals(fit)) # Augmented Dickey-Fuller Test #Dickey-Fuller = -2.0685, Lag order = 3, p-value = 0.547 #alternative hypothesis: stationary At this point, you can choose a model to be used to obtain forecasts. For example, forecasts based on a structural time series model and on an ARIMA model can be obtained as follows. # StructTS fit1 A plot of the forecasts: par(mfrow = c(2, 1), mar = c(2.5,2.2,2,2)) plot((cbind(x, p1$pred)), plot.type = "single", type = "n", ylim = range(c(x, p1$pred + 1.96 * p1$se)), main = "Local trend model") grid() lines(x) lines(p1$pred, col = "blue") lines(p1$pred + 1.96 * p1$se, col = "red", lty = 2) lines(p1$pred - 1.96 * p1$se, col = "red", lty = 2) legend("topleft", legend = c("forecasts", "95% confidence interval"), lty = c(1,2), col = c("blue", "red"), bty = "n") plot((cbind(x, p2$mean)), plot.type = "single", type = "n", ylim = range(c(x, p2$upper)), main = "ARIMA (0,1,0) with drift") grid() lines(x) lines(p2$mean, col = "blue") lines(ts(p2$lower[,2], start = end(x)[1] + 1), col = "red", lty = 2) lines(ts(p2$upper[,2], start = end(x)[1] + 1), col = "red", lty = 2) The forecasts are similar in both cases and look reasonable. Notice that the forecasts follow a relatively deterministic pattern similar to a linear trend, but we did not modelled explicitly a linear trend. The reason is the following: i) in the local trend model, the variance of the slope component is estimated as zero. This turns the trend component into a drift that has the effect of a linear trend. ii) ARIMA(0,1,1), a model with a drift is selected in a model for the differenced series.The effect of the constant term on a differenced series is a linear trend. This is discussed in this post . You may check that if a local model or an ARIMA(0,1,0) without drift are chosen, then the forecasts are a straight horizontal line and, hence, would have no resemblance with the observed dynamic of the data. Well, this is part of the puzzle of unit root tests and deterministic components. Edit 1 (inspection of residuals): The autocorrelation and partial ACF do not suggest a structure in the residuals. resid1 As IrishStat suggested, checking for the presence of outliers is also advisable. Two additive outliers are detected using the package tsoutliers . require(tsoutliers) resol Looking at the ACF, we can say that, at the 5% significance level, the residuals are random in this model as well. par(mfrow = c(2, 1)) acf(residuals(resol$fit), lag.max = 20, main = "ACF residuals. ARIMA with additive outliers") pacf(residuals(resol$fit), lag.max = 20, main = "PACF residuals. ARIMA with additive outliers") In this case, the presence of potential outliers does not appear to distort the performance of the models. This is supported by the Jarque-Bera test for normality; the null of normality in the residuals from the initial models ( fit1 , fit2 ) is not rejected at the 5% significance level. jarque.bera.test(resid1)[[1]] # X-squared = 0.3221, df = 2, p-value = 0.8513 jarque.bera.test(resid2)[[1]] #X-squared = 0.426, df = 2, p-value = 0.8082 Edit 2 (plot of residuals and their values) This is how the residuals look like: And these are their values in a csv format: 0;6.9205 -0.9571;-2942.4821 2.6108;4695.5179 -0.5453;-2065.4821 -0.2026;-771.4821 0.1242;-208.4821 0.1909;-120.4821 -0.0179;-535.4821 0.1449;-153.4821 0.484;526.5179 1.0748;1722.5179 0.3818;299.5179 -1.061;-2434.4821 0.0996;156.5179 0.4805;663.5179 0.8969;1463.5179 0.4111;461.5179 -1.0595;-2361.4821 0.0098;65.5179 0.5605;920.5179 0.8835;1481.5179 0.7669;1232.5179 1.4024;2593.5179 0.3785;473.5179 -1.1032;-2233.4821 -0.3813;-492.4821 2.2745;4642.5179 0.2935;154.5179 -0.1138;-165.4821 -0.8035;-1455.4821 -1.2982;-2321.4821 -1.9463;-3565.4821 -0.1648;62.5179 -0.1022;-253.4821 0.9755;1882.5179 -0.5662;-1418.4821 -0.0176;28.5179 0.5;928.5179 0.6831;1189.5179 -1.8889;-3964.4821 0.3896;1136.5179 -1.3113;-2799.4821 -0.9934;-1800.4821 -0.4085;-748.4821 1.2902;2482.5179 -0.0996;-657.4821 0.5539;981.5179 2.0007;3725.5179 1.0227;1490.5179 0.27;263.5179 -2.336;-4736.4821 1.8994;4263.5179 0.1301;-236.4821 -0.0892;-236.4821 -0.1148;-236.4821 -1.1207;-2236.4821 0.4801;1163.5179
