[site]: crossvalidated
[post_id]: 233085
[parent_id]: 195550
[tags]: 
To begin understanding how to rank variables by importance for regression models, you can start with linear regression. A popular approach to rank a variable's importance in a linear regression model is to decompose $R^2$ into contributions attributed to each variable. But variable importance is not straightforward in linear regression due to correlations between variables. Refer to the document describing the PMD method (Feldman, 2005)[ 3 ]. Another popular approach is averaging over orderings (LMG, 1980)[ 2 ]. There isn't much consensus over how to rank variables for logistic regression. A good overview of this topic is given in [ 1 ], it describes adaptations of the linear regression relative importance techniques using Pseudo-$R^2$ for logistic regression. A list of the popular approaches to rank feature importance in logistic regression models are: Logistic pseudo partial correlation (using Pseudo-$R^2$) Adequacy: the proportion of the full model log‐likelihood that is explainable by each predictor individually Concordance: Indicates a model’s ability to differentiate between the positive and negative response variables. A separate model is constructed for each predictor and the importance score is the predicted probability of true positives based on that predictor alone. Information value: Information values quantify the amount of information about the outcome gained from a predictor. It is based on an analysis of each predictor in turn, without taking into account the other predictors. References: On Measuring the Relative Importance of Explanatory Variables in a Logistic Regression Relative importance of Linear Regressors in R Relative Importance and Value, Barry Feldman (PMD method)
