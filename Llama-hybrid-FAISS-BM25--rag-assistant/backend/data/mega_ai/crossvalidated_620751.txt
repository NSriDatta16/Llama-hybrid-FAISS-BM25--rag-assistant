[site]: crossvalidated
[post_id]: 620751
[parent_id]: 
[tags]: 
Can I use Shapley values with metadata (i.e. information about observations that I didn't train my model on)?

I'm training a set of models (random forest/XGBoost) for an ordinal regression task. I'm (tentatively) planning to use Shapley values to infer feature performance. I also have some metadata that my model is not trained on--imagine features which are available for curated data, but expensive/infeasible to obtain for real world data (say the age or race of a normally anonymous user). I'd like to determine if these metadata features impact model performance. Rather than just assessing whether R^2 differs across metadata categories, I'd like to generate importance values for the metadata categories (i.e. say by one-hot encoding each category). Has anyone attempted to do something similar / is this even possible with Shapley values? I know that Shapley values can be used to look at interactions that the model wasn't explicitly trained on, so I am wondering what it might look like to feed such a "metadata" variable, let's call it "metadata_col1," to my model explainer and whether it would have the type of meaning I am describing or if I am overlooking something regarding interpretation.
