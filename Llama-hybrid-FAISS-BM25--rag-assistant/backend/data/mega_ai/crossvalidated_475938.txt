[site]: crossvalidated
[post_id]: 475938
[parent_id]: 
[tags]: 
What is the best way to cluster huge amounts (250 million) of high dimensional vectors?

There are a lot of clustering algorithms in machine learning. Most of them work well on reasonable small data. For huge data sets in the hundreds of millions most of them are unusable due to memory consumption or processing time. I am using clustering to find matching pairs of vectors. The problem is that there are either too many vectors in the buckets which makes further processing too expensive or the bucket sizes are fine but the matches are too often in different buckets. I already tried hierarchical k-means and lsh but both did not meet my goals. I also tried the annoy repository which works fine on smaller sets but due to memory consumption I cannot use it. Most of the clustering algorithms work fine for data up to 10 million vectors but are too expensive for larger data. My goal is to cluster the data in buckets that should not contain more than 1000 vectors. Matches should be in same bucket almost certainly. I also thought about preprocessing/transforming the vector data to make clustering easier but did not find anything useful. Are there any alternatives I am missing?
