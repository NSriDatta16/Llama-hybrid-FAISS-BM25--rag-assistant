[site]: datascience
[post_id]: 43345
[parent_id]: 37598
[tags]: 
If you are just starting out, it is pretty unlikely that you come across such big datasets that require massive parallelization either in computing power or data storage means. When you start pushing the limits of what a single machine can do, that's when you deploy a big data solution like Spark and Apache HDFS storage and start writing parallelizable models that scale well with bigger data. In a small scale it might be actually way slower to run some jobs at Spark, because it adds some overhead to the calculations. Spark is best used for large enough, well parallelizable and iterative problems. It is also worth mentioning that it requires some patience to configure the whole cluster to work well.
