[site]: crossvalidated
[post_id]: 384876
[parent_id]: 
[tags]: 
Equity Risk Model using an autoencoder

I am trying to create a statistical equity risk model using an autoencoder in a similar fashion to how one would use PCA to derive the systematic and specific risk components of a stock's returns. I understand that the "specific return" (i.e. idiosyncratic return) is the reconstruction error of the PCA or the autoencoder. But I am unsure of how to feed the returns into the autoencoder. For example, let's say I have N = 3,000 stocks and T = 252 returns for each stock (i.e. the last year's trailing daily returns). I have attempted this several ways and I am able to get reconstruction error by reducing the N dimension or the T dimension. But I am unsure which is correct. My gut says I want to reduce the "N" dimension at each time point t=T in order to get the factor return of a few systematic factors, and then the deviation of a stock's return from these systematic factors is a stock's idiosyncratic (i.e. specific) return. If this is the case then how do I recover the systematic risk part for each stock at each time point? For PCA, this is a bit easier as there is a lot of literature on how somebody can use PCA to estimate the systematic and specific components of an equity risk model using only a "panel of returns" for the given investment universe of interest. Calculate the covariance matrix, get the eigen values, etc. In the case that I mentioned above when T If it helps I am using pytorch in python. I can show any code that you might need to see if how I am messing this up. Here is the DAE that I am using. class DAE(torch.nn.Module): def __init__(self, n_feature, n_hidden, n_output, mean=0.0, std=0.3): super(DAE, self).__init__() self.mean = mean self.std = std self.encoder = torch.nn.Sequential( torch.nn.Linear(n_feature, n_hidden[0]), torch.nn.ELU(), torch.nn.Linear(n_hidden[0], n_hidden[1]), torch.nn.ELU(), torch.nn.Linear(n_hidden[1], n_hidden[2]), torch.nn.ELU(), torch.nn.Linear(n_hidden[2], n_hidden[3]), torch.nn.ELU() ) self.decoder = torch.nn.Sequential( torch.nn.Linear(n_hidden[3], n_hidden[4]), torch.nn.ELU(), torch.nn.Linear(n_hidden[4], n_hidden[5]), torch.nn.ELU(), torch.nn.Linear(n_hidden[5], n_hidden[6]), torch.nn.ELU(), torch.nn.Linear(n_hidden[6], n_output) ) def add_noise(self, x): noise = x.new_tensor(x.data).normal_(self.mean, self.std) return x.add(noise) def forward(self, x, train=True): training = train if training == True: x = self.add_noise(x) encoded_x = self.encoder(x) decoded_x = self.decoder(encoded_x) return decoded_x I also found one "quasi-related" link titled, "How can I determine what features an auto-encoder cares about?" if this helps.
