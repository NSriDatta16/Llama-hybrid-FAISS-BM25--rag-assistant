[site]: crossvalidated
[post_id]: 488176
[parent_id]: 
[tags]: 
Multivariate Jensen-Shannon divergence

This paper says multivariate Jensen-Shannon divergence is $$JS(\mathbf{p}_1,\dots,\mathbf{p}_K) = \frac{1}{m} \sum KL(\mathbf{p}_i || \bar{\mathbf{p}})$$ with $KL$ being the KL-divergence of the multiple probability distributions. Is it accurate compared to bivariate JS-divergence? Would a matrix of bivariate JS-divergences feasible (like the correlation matrix), and what would its diagonal consist of? Besides these, if t-distributed Stochastic Neighbor Embedding (t-SNE) in sklearn.manifold.tsne can be used to form a representation of multivariate KL-divergence for minimization (multivariate rather than one pair at a time), can the same or another algorithm do the same for a multivariate version of Jensen-Shannon divergence?
