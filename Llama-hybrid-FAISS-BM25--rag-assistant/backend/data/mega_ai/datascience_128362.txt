[site]: datascience
[post_id]: 128362
[parent_id]: 
[tags]: 
Temporal mismatch

I am building a predictive model to determine risk for a disease over the course of a hospital stay. I am using medical records from a hospital electronic medical record database. The predictions are made on the same subjects every 4 hours until they are discharged. My initial approach was to use summary measures, i.e., mean, median, min, max for many of the records (excluding the demographics). My assumption was that the pathology of the disease was very slow moving and that the summary measures would capture enough of the variance that I could make accurate predictions on new data. I created a number of models which all perform well when trained on the summary of a person's entire encounter, but perform poorly when used in production making predictions every 4 hours. The models I have tested have used regularization, cross validation, and calibration. Additionally, since the outcome I am trying to predicts is occurring in approximately 1% of all cases, I have used cost-sensitive learning and sampling techniques (including synthetic data). I also scored the models in training based on F1, precision, recall, and calibration metrics (cohen's kappa, brier score, log loss). The training data has over 300k subjects. All of this testing leads me to believe that the time-dependent relationships between the features and outcome are important. I have been reading more about discrete-time survival analysis using logistic regression and other models like random forest. I am wondering if this approach would capture the time dependency in my features and allow me to determine risk for the disease at any given time. In addition, I was also considering an LSTM model, but I have been hesitant as I would lose most of the interpretability of the model. I am wondering if discrete-time survival analysis is actually what I need here or if I should just go down the path of traditional timeseries analysis such as mixed-effects models / LSTM.
