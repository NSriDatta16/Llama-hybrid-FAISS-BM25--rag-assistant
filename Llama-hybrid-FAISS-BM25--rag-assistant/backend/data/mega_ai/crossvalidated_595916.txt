[site]: crossvalidated
[post_id]: 595916
[parent_id]: 594928
[tags]: 
In other words: Can stepwise regression or variable selection using AIC (or similar statistics based on the likelihood or distributional assumptions of the regression model) be justified also in the case of deterministic computer experiments? Here's an argument for likelihood-based statistics. Let $Y = f(X)$ , where $f$ is a deterministic function. Assume you observe a finite number of realizations s.t. $\mathbf{y} = f(\mathbf{X})$ . If you knew both $f$ and $\mathbf{X}$ , then you would know $\mathbf{y}$ with probability one, or very vaguely $P(\mathbf{y} = f(\mathbf{X}) | \mathbf{X}, f) = 1$ . You don't know $f$ , so you place a Gaussian process prior $f | \mathbf{X}\sim\mathcal{GP}(\cdot, \cdot)$ . You integrate out $f$ to obtain the output marginal likelihood, which is MVN Gaussian: $\mathbf{y} | \mathbf{X}\sim\mathcal{MVN}(\cdot, \cdot)$ . Even if the computer code output is deterministic and there's no "innate" variability on it, the output marginal likelihood is multivariate normal due to your uncertainty on $f$ . So, statistics based on this marginal likelihood seem sensible to me. More general, here are a few things that might help you decide how comfortable you feel about using such diagnostic techniques. Your computer model is deterministic, i.e., $Y = f(X)$ . Since the computer model does not have a random error component, there's no aleatoric uncertainty. However, you don't know $f$ , so there's epistemic uncertainty. See Aleatoric and epistemic . Even though the underlying function $f$ might be deterministic, the output value is unknown a priori (before running the simulator). This is sometimes called code uncertainty and is enough to justify the use of a stochastic process. Surrogates are most frequently set up in a Bayesian framework, and so the probabilistic statements are about your uncertainty about the function that you are approximating $f$ and not really about the inherent random noise in your computer code. This applies even when Empirical Bayes is used, which in many contexts can get quite close to simply "find MLE estimates but interpret like a Bayesian model". Bastos and O'Hagan (2009, sec. 3.1) discusses the challenges associated with diagnostics for linear models fitted to a deterministic function. In particular, the training set predictions will be perfect if you use an interpolator, thus the need to use cross-validation or a new data set. Out-of-sample validation introduces some uncertainty. However, they also note that there might be scoring functions for deterministic computer experiments such as the Mahalanobis distance. See 10.1198/TECH.2009.08019 An adjacent note. A linear model might not be commonly used for a surrogate. However, you can cast a linear model as a Gaussian process with linear kernel $\mathbf{XX}^\top$ (Rasmussen and Williams, 2006, sec. 4.2.2). References Bastos, L. S., & O’Hagan, A. (2009). Diagnostics for Gaussian process emulators. Technometrics, 51(4), 425–438. https://doi.org/10/bw62bq Rasmussen, C. E., & Williams, C. K. I. (2006). Gaussian processes for machine learning. MIT Press.
