[site]: datascience
[post_id]: 94148
[parent_id]: 94100
[tags]: 
I think the issue is that while $$ \frac{1}{m} \sum_{i=1}^{m}\log p_{model}(\mathbf{y} | \mathbf{x} ; \boldsymbol{\theta}) \rightarrow E_{}(\log p_{model}(\mathbf{y} | \mathbf{x} ; \boldsymbol{\theta})) $$ is true, the right side is not what is being referred to as the cross-entropy. Indeed, we have no access to the expectation over the true population distribution / data generating process; a few paragraphs down, we find: We can thus see maximum likelihood as an attempt to make the model distribution match the empirical distribution $\hat{p}_{data}$ . Ideally, we would like to match the true data-generating distribution $p_{data}$ , but we have no direct access to this distribution. They also distinguish throughout that the expectations are over $\hat{p}_{data}$ , the training data empirical distribution, not $p_{data}$ , the underlying population distribution. The left side of your limit is already an expectation, but over the finite training data, and that is what is referred to as the cross-entropy. ("Cross-entropy" is a broader term, for any pair of probability distributions. Goodfellow et al note this (my emphasis): Any loss consisting of a negative log-likelihood is a cross-entropy between the empirical distribution deﬁned by the training set and the probability distribution deﬁned by model. So, the answer to your questions is that the premise is incorrect: (this) cross-entropy is the same as negative log-likelihood. Taking your questions with the limiting and population cross-entropy instead, the answer is "we don't have access to the latter". It would be the better target to be sure, but our lack of that information is the point of modeling in the first place.
