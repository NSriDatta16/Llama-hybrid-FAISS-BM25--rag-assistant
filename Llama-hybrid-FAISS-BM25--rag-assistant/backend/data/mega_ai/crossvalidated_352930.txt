[site]: crossvalidated
[post_id]: 352930
[parent_id]: 351589
[tags]: 
Regularization cost for magenta classifier is low but still it seems to overfit the data and vice-versa for the black classifier. What's going on? L2 regularization tend to make the coefficients close to zero. But how does that helps in reducing overfitting? Overfitting isn't about what happens to the training data alone. It's about the comparison of the training data and out-of-sample. If your training loss is low, but your out-of-sample loss is large, you're overfitting. If your training loss is low, and your out-of-sample loss is low, congrats! You have some evidence that your model generalizes well. So if we apply that definition here, it’s obvious that we can’t say anything about whether the model is over- or under-fit because there’s no comparison to out of sample data. Regularization can help with overfitting by discouraging the model from estimating too-complex a decision boundary. The diagonal/magenta decision boundary could be "too-complex" (as measured by $L^2$ regularization omitting the intercept) if the X far away from the other Xs and near the Os is not representative of the process overall (i.e. it is a quirk). The intuition what I think of is that not much weight is given to a particular feature. But isn't it sometimes necessary to focus on one feature (like in the above example x1)? Preventing "too much weight given to a particular feature" isn't what $L^2$ regularization does, but does sound more like $L^\infty$ regularization (which tends to result in weights that are more evenly distributed ). $L^2$ regularization penalizes large coefficients (or encourages coefficients to be nearer to zero). The distinction I'm making is subtle, but the point is that in $L^2$ regularization, a model can "put its eggs all in one basket" and have a small number of large coefficients and many near-zero coefficients. This is desirable when there are only a few highly relevant features. It's unusual to apply regularization to the intercept. If you omit the intercept regularization, the black classifier has lower regularization penalty. This isn't sufficient to permit us to draw any conclusions about which model, though, since we do not have information about out-of-sample generalization (or even about in-sample loss).
