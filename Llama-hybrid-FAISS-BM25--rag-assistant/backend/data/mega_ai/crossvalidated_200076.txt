[site]: crossvalidated
[post_id]: 200076
[parent_id]: 111654
[tags]: 
For me your question is part of a larger one involving classification algorithms in general. This paper Do we Need Hundreds of Classifiers to Solve Real World Classfication Problems? , gets at this more fundamental question in finding that random forests outperforms all other competitors. A link to an ungated copy is here: http://jmlr.csail.mit.edu/papers/volume15/delgado14a/delgado14a.pdf There are other flavors of boosting that haven't been mentioned such as GBM, the generalized boosted model, which has an R package: https://github.com/gbm-developers/gbm Also worth a mention is XGBoost or extreme gradient boosting, which some have found to out perform everything (and despite the Delgado paper's findings). https://cran.r-project.org/web/packages/xgboost/xgboost.pdf
