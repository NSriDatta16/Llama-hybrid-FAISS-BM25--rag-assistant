[site]: datascience
[post_id]: 24821
[parent_id]: 22853
[tags]: 
This is simply trying to convey my intuition, i.e. no rigor. The thing with saddle points is that they are a type of optimum which combines a combination of minima and maxima. Because the number of dimensions are so large with deep learning, the probability that an optimum only consists of a combination of minima is very low. This means 'getting stuck' in a local minimum is rare. At the risk of oversimplifying, it's harder to 'get stuck' in a saddle point because you can 'slide down one of the dimensions'. I think the Andrew Ng video you refer to comes from the Coursera course on Deep Learning by him.
