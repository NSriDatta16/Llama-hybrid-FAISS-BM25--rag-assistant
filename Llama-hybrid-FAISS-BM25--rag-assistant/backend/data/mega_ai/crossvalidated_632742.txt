[site]: crossvalidated
[post_id]: 632742
[parent_id]: 
[tags]: 
Derivation of acceptance probability from Linero, Yang (2018)

I am wondering how this paper Bayesian Regression Tree Ensembles that Adapt to Smoothness and Sparsity by Linero & Yang (2018) derived the acceptance probability for $\sigma$ . The authors give $\sigma$ a half Cauchy prior, $\sigma \sim \operatorname{Cauchy}_{+}(0, \widehat{\sigma}_{\text {lasso }})$ . To update $\sigma$ they say the following: Let $R=\left(R_1, \ldots, R_n\right)$ be the residuals, $R_i=Y_i-f\left(X_i\right)$ . Note that, under a flat prior for $\sigma^{-2}$ , the full-conditional of $\sigma^{-2}$ is $\mathrm{Ga}(N / 2+$ $1,\|R\|^2 / 2$ ). We use this full-conditional under the flat prior as a proposal distribution for $\sigma^{-2}$ ; after adjusting for the Jacobian of the transformation, the acceptance probability becomes $$ A\left(\sigma \rightarrow \sigma^{\prime}\right)=\frac{\operatorname{Cauchy}_{+}\left(\sigma^{\prime} \mid 0, \widehat{\sigma}_{\text {lasso }}\right) \sigma^{\prime 3}}{\text { Cauchy }_{+}\left(\sigma \mid 0, \widehat{\sigma}_{\text {lasso }}\right) \sigma^3} \wedge 1. $$ I get the Jacobian bit of it, but I'm wondering how they managed to get from the Gamma distribution of $\sigma^{-2}$ to the half-Cauchy distribution of $\sigma$ ?
