[site]: crossvalidated
[post_id]: 117442
[parent_id]: 
[tags]: 
Factor analysis using "outliers-only" time series

Some background I run a factor analysis of a time series $Y$ using a standard OLS model with n+1 independent variables $(F,X_1...X_n)$, where $F$ is the main factor (from an explanatory power perspective). $X_i$ and $F$ are highly correlated ($\rho>0.8$) but on certain days $t$, $X_i(t)$ will diverge significantly from its estimated value using the coefficient of its regression against $F$ : $X_i=\beta_i F$. In other words the distribution of $\epsilon_i(t)=X_i(t)-\beta_i F(t)$ has fat tails. The reason why those highly collinear factors are included in the regression is precisely the presence of outliers which have a significant impact on the dependent variable $Y$. My approach To get rid of the multicollinearity problem yet keep the outliers, I create new factors $X_i'$ which are constructed like this: $X_i'(t)=\epsilon_i(t)$ if $\epsilon_i(t)$ is in the top or bottom $n$ percentiles of $\epsilon_i$ $X_i'(t)=0$ otherwise so if $n=5$ for example, $X_i'$ will be zero 90% of the time. I then regress $Y$ against $(F, X_1'...X_n')$ My questions Is there a flaw in the method? Are there alternatives that would help me capture that effect? (I know that I could orthogonalise each $X_i$ vs $(F,X_0...X_{i-1})$ but the results are the same as and as unstable as regressing against the original factors). Note: in practice, $Y$ represents the returns of a stock, $F$ represents the returns of the market and the $X_i...X_n$ represents other factors such as industry indices.
