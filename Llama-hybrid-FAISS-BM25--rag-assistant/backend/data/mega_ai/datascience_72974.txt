[site]: datascience
[post_id]: 72974
[parent_id]: 
[tags]: 
Reason for capping Learning Rate (alpha) up to 1 for Gradient Descent

I am learning to implement Gradient Descent algorithm in Python and came across the problem of selecting the right learning rate. I have learned that learning rates are usually selected up to 1 (Andrew Ng's Machine Learning course). But for curiosity reasons, I have tried alpha = 1.1 and alpha = 1.2 . I can see in the case of alpha = 1.2 , we reach the lower cost faster than the other learning rates (simply because the curve touches the bottom first). Is it safe to say that alpha = 1.2 is the best rate? I plugged in the theta values, where alpha = 1.2 , to predict the price of an item, my implemented function provided the same answer as Sklearn's LinearRegression() in lesser iterations than it did with alpha = 1.0 . Using lower alpha rates would increase the number of iterations. So, why is the learning rate capped at 1? Is it mandatory or suggested? Should I forget about selecting learning rates and let functions like LinearRegression() take care of it automatically in the future? I am new to machine learning and I want to understand the reasoning behind the algorithms rather than calling the functions blindly and playing around with parameters using high-level libraries. Feel free to correct me if I have understood the concepts wrong.
