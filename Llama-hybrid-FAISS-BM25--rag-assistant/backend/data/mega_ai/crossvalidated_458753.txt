[site]: crossvalidated
[post_id]: 458753
[parent_id]: 
[tags]: 
Bayesian regression kernel

I am reading Bishops book "pattern recognition and machine learning". In chapter 3 "linear models for regression" section 3.3.3 "equivalent kernel" equation 3.63 on page 160 is given as follows: $$cov[y(x),y(x')]=cov[\phi(x)^Tw,w^T\phi(x')]=\phi(x)^TS_N\phi(x')=\beta^{-1}k(x,x')$$ where $x$ is a data vector, $\phi$ is a vector of basis functions, $t$ the target values, $w$ the regression weights, $y(x)=\phi(x)^Tw$ , and the posterior of $w$ is given by $p(w|t)\sim N(w;m_N,S_N)$ I do not understand the middle part of the equation given. How is this step below derived? $$cov[\phi(x)^Tw,w^T\phi(x')]=\phi(x)^TS_N\phi(x')$$
