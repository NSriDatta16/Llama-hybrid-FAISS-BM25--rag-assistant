[site]: datascience
[post_id]: 64148
[parent_id]: 64143
[tags]: 
The overall F-stat says if your model is significantly better than the naive model that only has an intercept at the average of all response data pooled together. In other words, it measures if $R^2$ is significantly better than 0. In your case, you can eyeball the $R^2$ and see that 0.897 is going to be better than 0. The F-stat and p-value will be more useful when the $R^2$ has a subtle difference from 0. Another place where an F-stat is useful to regression is testing against a less naive model than intercept-only. In fact, the p-values on each parameter in the regression are equal to the p-value you'd get by doing the F-test comparing the full model to a model with that parameter excluded (assuming the typical nice properties that go along with doing parameter inference in linear regression). The reason not to examine the individual p-values for each parameter is because of false discoveries, the usual business about multiple testing. Doing one F-test of the entire equation removes the possibility of false discovery of a significant parameter, in exchange for not being able to pin down which parameters are significant.
