[site]: crossvalidated
[post_id]: 179675
[parent_id]: 179272
[tags]: 
Given that you assigned a neural-networks tag, I am going to assume that you want to solve this problem with a neural network. I do not know how far your knowledge with respect to neural-networks reaches but you could simply feed the 500 feature values into a single softmax regression layer (for classification) with a modified gradient update. The Softmax layer will have a (n_class x 500) weight matrix which will be optimised with respect to the loss function at each training step. Usually one would compute the gradient for each weight with respect to the loss function, and then update all of them accordingly, however this would not yield shared weights. The following altered gradient update would solve that. Let's say that you structure your features to be f1c1, f1c2, f1c3, .. etc. Then rather then doing a conventional weight update for each weight with respect to its gradient you could update the weights by the average gradient of the same feature for different conditions. This means that that the weights of f1c1, f1c2, f1c3, f1c4, f1c5 would all be updated by (gf1c1 + gf1c2 + gf1c3 + gf1c4 + gf1c5)/5. This would ensure that the weights between the conditions of the same feature are shared, which you can extract at a later stage. Sorry if the explanation is a little messy.
