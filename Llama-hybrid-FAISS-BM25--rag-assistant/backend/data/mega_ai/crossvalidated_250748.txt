[site]: crossvalidated
[post_id]: 250748
[parent_id]: 250743
[tags]: 
is it possible to implement feed forward & back prop for such a network using matrix operations? yes when hidden layers are different sizes, are you forced to iterate over layers to update them with vector operations individually? At each layer of a feed forward neural network you just need one multiplication between the weight matrix of the layer and its input vector (and apply the activation function to each element of the resulting vector). So having a different number of nodes per hidden layer isn't an issue.
