[site]: crossvalidated
[post_id]: 567009
[parent_id]: 567004
[tags]: 
Matching is in art in the sense that you may need to try a variety of specifications to find one that works for your data, if at all (I'll get back to the "if at all" at the end). 1:1 propensity score matching without replacement is almost certainly the worst matching method to use, especially in your case with so few observations. It is the default because other methods can be layered on top of it to create a matching specification that works. You need to try many different matching specifications to find one that achieves balance without destroying your (effective) sample size. Did you try full matching (setting method = "full" )? Did you try cardinality matching (setting method = "cardinality" )? Did you try adding a caliper (using the caliper argument)? Did you try adding an exact matching restriction (using the exact argument)? There are so many ways to customize a matching specification, and all the ones available in MatchIt are described in the Matching Methods vignette . I also highly recommend using the MatchIt tutorial vignette instead of the one you found. My next point is whether matching is the right method for you. Why do you want to use matching over other methods of confounding control, like regression? With such a small sample, it is probable that variability in the effect estimate will be vastly greater than the bias that matching would eliminate that wouldn't be eliminated with regression. If you have a problem with separability number of events per variable, try penalized logistic regression (e.g., using the brglm2 package). It may also be the case that you tiny sampel simply doesn't contain enough information for you to reliably remove confounding and estimate an effect with precision, and you will need to decide on a tradeoff between the two or collecting more data.
