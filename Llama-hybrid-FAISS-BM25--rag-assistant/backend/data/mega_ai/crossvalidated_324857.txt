[site]: crossvalidated
[post_id]: 324857
[parent_id]: 
[tags]: 
In reinforcement learning, what is the formal definition of the symbols $S_t$ and $A_t$?

In both the reinforcement learning course given by David Silver and the latest draft of Richard S. Sutton's RL book what is the formal definition of the symbols $S_t$ and $A_t$? Do these definitions depend on the policy being used until time-step $t$? Some context for the question: I realize that this question might seem trivial, since the authors explicitly define these variables in their textbook/lecture. However, I'm currently trying to make sense of their definitions, but no interpretation I think of seems to yield a coherent/consistent notation. What follows is my line of thought, showing which interpretations I tried and why they seem to be inconsistent. These authors seem to define the following symbols like so: $$S_t\triangleq\text{The state we visit at time-step }t.$$ $$A_t\triangleq\text{The action we take at time-step }t.$$ Where both $S_t$ and $A_t$ are commonly treated as random variables. However, what confuses me is that in order to properly define $S_t$ for $t>0$ it seems necessary to first define all the $S_i,A_i$, for $0\leq i This is already ambiguous, and personally confusing, since the symbol $S_t$ has no mention whatsoever to which policy was used to sample the actions until that point. For example, the same symbol $S_5$ can represent completely different random variables, if they're specified in different contexts with different policies being used (or different starting states). This ambiguity didn't strike me as very impairing, since I thought one could always use, say, a superscript to indicate the policy being used at all the previous time-steps, e.g. $S_5^\pi$. Also, for most of the discussions it was very clear which policy was being used (and almost as clear what was the starting state), so it seemed harmless to drop the extra notation. However, I later encountered definitions like, for instance, the action-value function of a state action pair: $$ Q_\pi(s,a)=\mathbb E_\pi[G_t | S_t=s, A_t=a] $$ This is supposed to be defined for all legal actions $a$ in state $s$. However, if I interpret $S_t$ as $S_t^\pi$ and $A_t$ as $A_t^\pi$, respectively, this definition seems to break down when $a$ is an action that policy $\pi$ would never choose, or $s$ is an unreachable state given policy $\pi$ and some starting state $s_0$ (since we'll be conditioning the expectation on an impossible event). So it seems that these authors are not simply dropping the superscript I mentioned before, and instead $S_t$ and $A_t$ have some other definition.
