[site]: datascience
[post_id]: 72222
[parent_id]: 
[tags]: 
Multi-Class CNN model predicting only one class but still the accuracy is high

Before marking the question as repeated, note that I have read most of them but did not find the solution. I have given all the information on model below so please give advice for it. I am working on Facial Sentiment Analysis. I am using transfer learning with ResNet50 to train a dataset of 28k RGB images. The model is giving validation accuracy of 83.33% (which is constant for all epochs) and predicting only one label. "The problem here is that the label3 (i.e. Neutral label) it is predicting has 31.8% proportion in the data and still the accuracy comes out to be 83.33%." The code and confusion matrix is below. Am I using the model.predict_generator incorrectly? Please note that I passed my test_set in the predict_generator . Data Distribution: Test: (Anger-646), (Disgust-970), (Happiness-1927), (Neutral-2232), (Sadness-836), (Surprise-399) Training: (Anger-1935), (Disgust-2908), (Happiness-5778), (Neutral-6693), (Sadness-2505), (Surprise-1195) Model: Rsnt_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=(64, 64, 3)) for layer in Rsnt_model.layers: layer.trainable=False #av1 = layers.GlobalAveragePooling2D()(Rsnt_model.output) av1=layers.Flatten()(Rsnt_model.output) fc1 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(av1) drp1=Dropout(0.35)(fc1) fc2 = Dense(128, activation='relu')(drp1) drp2=Dropout(0.2)(fc2) bat_norm=layers.BatchNormalization()(fc2) fc3 = Dense(68, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(bat_norm) #drp3=Dropout(0.25)(fc3) fc4 = Dense(34, activation='relu')(fc3) out = Dense(6, activation='softmax')(fc4) tl_model = Model(inputs=Rsnt_model.input,outputs=out) tl_model.compile(optimizer = tf.keras.optimizers.Adam(lr=0.001), loss=tf.keras.losses.BinaryCrossentropy(), metrics = ['accuracy']) I am using BinaryCrossEntropy because of multi-label classification training_test_set code training_set = train_datagen.flow_from_directory( 'train_test_dataset/Train', target_size=(64, 64), batch_size=64, class_mode='categorical', ) test_set = test_datagen.flow_from_directory( 'train_test_dataset/Test', target_size=(64, 64), batch_size=64, class_mode='categorical', ) Epochs Epoch 1/10 335/335 [==============================] - 123s 368ms/step - loss: 1.0045 - accuracy: 0.8329 - val_loss: 0.5025 - val_accuracy: 0.8333 Epoch 2/10 335/335 [==============================] - 113s 338ms/step - loss: 0.4883 - accuracy: 0.8330 - val_loss: 0.5093 - val_accuracy: 0.8333 Epoch 3/10 335/335 [==============================] - 117s 350ms/step - loss: 0.4926 - accuracy: 0.8329 - val_loss: 0.4996 - val_accuracy: 0.8333 Epoch 4/10 335/335 [==============================] - 113s 338ms/step - loss: 0.4909 - accuracy: 0.8332 - val_loss: 0.4978 - val_accuracy: 0.8333 Epoch 5/10 335/335 [==============================] - 113s 336ms/step - loss: 0.4898 - accuracy: 0.8334 - val_loss: 0.4975 - val_accuracy: 0.8333 Code for predict_generator , evaluate_generator and confusion matrix preds=tl_model.predict_generator( test_set ) evals=tl_model.evaluate_generator( test_set ) import numpy as np pred=[] for i in preds: pred.append(np.argmax(i)) lbls=test_set.labels from sklearn.metrics import confusion_matrix conf_mat=confusion_matrix(lbls, pred) print(preds) [[0.07126013 0.16465071 0.23472613 0.39474297 0.08812989 0.04649016] [0.07160447 0.16465573 0.23501842 0.39354676 0.08840203 0.04677264] [0.07140011 0.16469218 0.2348036 0.39425436 0.08823967 0.0466101 ] ... [0.0716239 0.16465151 0.23494086 0.3935586 0.08842354 0.04680166] [0.07143697 0.16465022 0.23501226 0.3940167 0.08823942 0.04664437] [0.07193935 0.16459921 0.23527609 0.39246148 0.08866104 0.04706287]] print(evals) [0.49759348820556293, 0.83333373] print(conf_mat) [[ 0 0 0 646 0 0] [ 0 0 0 970 0 0] [ 0 0 0 1927 0 0] [ 0 0 0 2232 0 0] [ 0 0 0 836 0 0] [ 0 0 0 399 0 0]]
