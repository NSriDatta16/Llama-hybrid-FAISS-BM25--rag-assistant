[site]: crossvalidated
[post_id]: 613866
[parent_id]: 386466
[tags]: 
The null hypotheis $H_0$ of the test is the likelihood $f(Y|\theta)$ is right. You also assume you prior $\pi(\theta)$ is "right". The first condition "if it depends only on observed data" is saying: $T(Y)$ cannot depend on $\theta$ , which is the unknown parameters in the likelihood $f(Y|\theta)$ . The second condition "its distribution is independent of the parameters of the model" is as follows. Let $Y^{obs}$ be your observed data. Under $H_0$ , there is a density for $Y^{obs}$ , which is $\int f(Y^{obs}|\theta)\pi(\theta)d\theta$ , and a density for $T(Y^{obs})$ . Let's call this density as $g(T(Y^{obs}))$ . What you want is $g(T(Y^{obs}))$ does not depend on $\theta$ . That being said, the notation you used " $T(y)|\theta$ " is not entirely right as we do not condition on a $\theta$ but integrate it out from $\int f(Y^{obs}|\theta)\pi(\theta)d\theta$ . Regarding your follow-up question, the practical insight of both paragraphs is, if you couldn't a $T(Y)$ that can test the specific feature of interest and satisfies the above two conditions simultaneously, the posterior predictive p-value you will get does not follow uniform(0,1) but a dome-shaped density. This non-uniform issue is not regarded as an issue by Gelman himself http://www.stat.columbia.edu/~gelman/research/published/ppc_understand3.pdf but a number of statisticians find it unsatisfying as this condition is a key for hypothesis testing, e.g, https://www.tandfonline.com/doi/abs/10.1080/01621459.2000.10474310 https://www.sciencedirect.com/science/article/pii/S0167947314001522
