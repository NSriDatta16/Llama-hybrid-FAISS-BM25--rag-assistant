[site]: crossvalidated
[post_id]: 110422
[parent_id]: 110411
[tags]: 
Here's my attempt at a (self) answer, based on user777's hints. At first I was confused by the P(E|H) term in the Bayesian inference formula, thinking P(E|H) is 0 whenever H > E, and I only care about cases where H > E when I'm computing P(H|E). However, then I reformulated the problem slightly and said "what is the probability that it will take at least H given that it takes at least E". The probably of at least E given at least H is one, so that yields: $\int {p(H|E)}{dH} = \frac{\int{p(H)}{dH}}{\int^E_0{p(x)}{dx}}$ I believe that means that if I compute a random number using the original normal distribution that I should just be able to multiply it by $\int^E_0{p(x)}{dx}$ to get my new random number. Rationale: The normal random number should be $inv(\int{p(x)}{dx})(rand())$. To get a random number corresponding to $p(H|E)$ I need $inv\bigl({\int {p(H|E)}{dH}}\bigr)(rand())$, which should be equal to $\int^E_0{p(x)}{dx}*inv\bigl({\int{p(H)dH}}\bigr)$. That should be exactly equal to normal_rand(sigma, mu)*cdf(w) . Update: This needs to be truncated somehow, as the df is only valid when x > w. I'm not quite sure how to do that. Update 2: I ended up using the info here: http://en.wikipedia.org/wiki/Truncated_distribution to produce a truncated normal distribution. I think I can use that, without the Bayesian update.
