[site]: crossvalidated
[post_id]: 296235
[parent_id]: 296221
[tags]: 
EM is a specific class of algorithms, whereas Bayesian inference (including MAP as a special case) is a general approach to inference. As such, they're at different levels of description. EM and MAP aren't mutually exclusive; EM can be used to compute maximum likelihood (ML) or MAP estimates. MAP is a type of Bayesian inference, but is distinct from 'full' Bayesian inference (not a rigorous term). In (2), you wrote an expression for the joint distribution of $x$ and $y$, given the parameters (this is not the posterior). It won't change for any of the methods you mentioned, nor will the likelihood function. Rather, Bayesian approaches differ from ML in that they impose priors on the parameters and are concerned with the posterior distribution , whereas ML is only concerned with the likelihood function . Say we have data $D$ and parameter vector $\theta$. The likelihood is $p(D \mid \theta)$ and the prior is $p(\theta)$. The posterior is given by Bayes' rule: $$p(\theta \mid D) = \frac{p(D \mid \theta) p(\theta)}{p(D)}$$ We can think of $p(D)$ as a normalizing constant that forces the numerator to integrate to 1. Say we'd like to obtain a point estimate of the parameters. ML seeks the parameters that maximize $p(D \mid \theta)$. MAP seeks the parameters that maximize $p(\theta \mid D)$. In a 'full' Bayesian approach, one might compute the mean of $p(\theta \mid D)$. These approaches also differ in how they deal with interval estimates .
