[site]: crossvalidated
[post_id]: 502467
[parent_id]: 502463
[tags]: 
I've encountered a similar issue my own field. Researchers frequently use standard ordinary least squares regression to predict raw scores on cognitive tests. These tests are strictly positive, have firm upward bounds on their range, and have scores that are integers (and thus not truly continuous). The answer as to whether this is problematic lies in the aims of the model. Note that I'll talk mostly about non-multilevel models just because it's easier; however, all the same things apply when using multilevel (general) linear regressions for panel data. In a technical sense, you are correct: standard regressions that assume unbounded, continuous data are inappropriate if your data are not those two things. A variety of alternative distributions, like the Poisson, are available and useful for that very reason. In the book Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences , Cohen et al. point out that ordinary least squares regression on bounded, discrete data can lead to biased standard error estimates in addition to potentially problematic predictions (e.g., what do we do if the model predicts a negative R&D intensity?). Note, however, that the assumption for ordinary least squares regression is normally distributed residuals, not necessarily a normally distributed outcome variable (though still the issue of bounded data can be a barrier to that assumption). So, there is theoretical ground to argue that these models are incorrect and that the inferences derived from them suffer due to their biased estimates. In the case that these models are being used to test theories, meaning that these inferences on significant predictors are the primary reason for running the models, the results may be spurious due to violations of certain assumptions. Now, the other side of this is the relative robustness of most regression methods. When there is sufficiently large data, the assumptions of ordinary least squares regression become a bit more fuzzy, at least on the practical scale. Obviously, a large sample size does not eliminate the need to check assumptions; however, the relative amount of bias obtained from a model run on data that violate its statistical assumptions can be fairly minimal when there is large data. Essentially, if there is a lot of data with a strong central tendency, then estimates tend to get pulled away from the problematic extreme ranges (e.g., potentially negative estimates) and more toward the average. So, when the goal is to create accurate predictions of the dependent variable, then large enough sample sizes can still perform pretty well even if the statistical model is theoretically inappropriate. As in all things statistics, the question of what is best with your model comes down to what you want to do with it. If you're testing theories and it's important that the predictors' significance testing is unbiased, then there's a lot of weight to ensuring that the selected modelling methods are appropriate for the data you have (assuming as well that your data are themselves typical for what you might observe in whatever population you're making inference on). If all you care about is getting decent predictive accuracy, then the model can be used and abused more liberally as long as you are explaining the whole way through that the aim is accuracy in predictions over accuracy in inferences (in other words: you need to be telling the audience not to look at the predictors too closely or make anything too much of what is or is not significant). If this is for a study that you intend to publish in some way, then I think it's helpful to think first about what you want to tell with the final product. Is it a critique of previous work and a demonstration on the importance of choosing the correct model? Is it a recommendation to utilize a better (albeit perhaps less familiar) regression method and demonstrate its strengths to an audience who may be more inclined to use "simpler" methods? Do you want to extend some theory in your field and need those nice, robust inferences that come from a well-done model? Whatever the goal is, just make sure that it's more than "you did it wrong, and I've done it right" or "I did this more sophisticated model and now look what I found." We all love a shiny new model, but if it doesn't have any utility, then it's just noise in the background. Anecdotally, I compared theoretically-appropriate regressions (usually binomial regressions but some Poisson and negative binomials here and there for good measure) to the standard ordinary least squares regressions for predicting raw scores on a variety of cognitive tests using a decently sized data set (N = 3250). I found no practical difference in the predictive accuracy, R2, or predictor effect sizes (estimates differed on the order of 0.01 on average between the out-of-the-box, point-and-click regressions and my theoretically-informed regressions). That's also my unique case: I doubt the same would be true if your dependent variable often has range as small as 0-2, and my variables are very easy to assume having nice, well-behaved normal distributions in the population.
