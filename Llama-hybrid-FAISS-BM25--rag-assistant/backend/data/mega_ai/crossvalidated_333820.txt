[site]: crossvalidated
[post_id]: 333820
[parent_id]: 332997
[tags]: 
Optimal perturbation doesn't have any special meaning. As used in the paper, for a fixed constant $\epsilon$, the optimal perturbation to the input $x$ is some $\Delta x$ with a max norm of at most $\epsilon$ (meaning the maximum element in $\Delta x$ is at most $\epsilon$) which maximizes $J(x+\Delta x, y, W) - J(x, y, W)$. In logistic regression using cross-entropy as a loss function, the gradient is $\nabla_x J = (\sigma(w^T x +b) - y)w$. If we're trying to maximize the loss of a positively classified example, then $\sigma -y$ is negative, so the sign of the gradient is $-\text{sign}(w)$. Now all we need to convince ourselves that the $\Delta x = \epsilon\, \text{sign}(\nabla_x J)$ is the optimal perturbation in the max norm box. Intuitively, we just want to find the vector with the largest projection onto $\nabla_x J$, and pushing all the components of $\Delta x$ as far as they will go accomplishes this because each component is at an angle at most 90 degrees from $\nabla_x J$ which means it can only increase the size of the projection.
