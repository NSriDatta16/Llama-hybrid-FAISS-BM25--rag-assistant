[site]: crossvalidated
[post_id]: 144983
[parent_id]: 142746
[tags]: 
The objective of EM is to maximize the observed data log-likelihood, $$l(\theta) = \sum_i \ln \left[ \sum_{z} p(x_i, z| \theta) \right] $$ Unfortunately, this tends to be difficult to optimize with respect to $\theta$. Instead, EM repeatedly forms and maximizes the auxiliary function $$Q(\theta , \theta^t) = \mathbb{E}_{z|\theta^t} \left (\sum_i \ln p(x_i, z_i| \theta) \right)$$ If $\theta^{t+1}$ maximizes $Q(\theta, \theta^t)$, EM guarantees that $$l(\theta^{t+1}) \geq Q(\theta^{t+1}, \theta^t) \geq Q(\theta^t, \theta^t) = l(\theta^t)$$ If you'd like to know exactly why this is the case, Section 11.4.7 of Murphy's Machine Learning: A Probabilistic Perspective gives a good explanation. If your implementation doesn't satisfy these inequalities, you've made a mistake somewhere. Saying things like I have a close to perfect fit, indicating there are no programming errors is dangerous. With a lot of optimization and learning algorithms, it's very easy to make mistakes yet still get correct-looking answers most of the time. An intuition I'm fond of is that these algorithms are intended to deal with messy data, so it's not surprising that they also deal well with bugs! On to the other half of your question, is there a conventional search heuristic or likewise to increase the likelihood of finding the global minimum (or maximum) Random restarts is the easiest approach; next easiest is probably simulated annealing over the initial parameters. I've also heard of a variant of EM called deterministic annealing , but I haven't used it personally so can't tell you much about it.
