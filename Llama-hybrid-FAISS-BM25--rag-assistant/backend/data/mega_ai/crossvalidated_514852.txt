[site]: crossvalidated
[post_id]: 514852
[parent_id]: 
[tags]: 
Data normalization of test data in machine learning

I have 117 samples which I used to select and train a model. What I did: 1) pre-processed the 117 samples (normalization, statistics, etc); 2) created 4 folds (random split); 3) performed a nested-repated CV in six algorithms; 4) selected the best With this I saw that my model had good performance and decided to take more samples. Now I have an extra 30 samples and I'm unsure what to do. What I tried: I merged all the data (117 + 30) and did the pre-processing. Then, I separated in two datasets: the training (117 samples) and the testing (the 30 extra samples). I trained the best model again with the best parameters using the training set and tested it in the testing set. Is this the right thing to do?
