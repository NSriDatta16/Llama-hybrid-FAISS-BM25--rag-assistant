[site]: crossvalidated
[post_id]: 127198
[parent_id]: 95892
[tags]: 
Regularized linear regression and regularized logistic regression can be interpreted nicely from a Bayesian point of view. The regularization parameter corresponds to a choice of prior distribution on the weights, for example, something like a normal distribution centered at zero with standard deviation given by the inverse of the regularization parameter. Then via your training data, these distributions are updated to finally give you the posterior distributions on the weights. So, for example, a larger regularization parameter means that, as a prior, we think that the weights should be closer to zero, hence with this setup it's less likely that the posterior distributions will be supported far away from zero -- which agrees with the intuition of what regularization is "supposed to do". For most implementations of regularized regression, the final output of the weights is just the expected value of the posterior distributions. By the way, unregularized regression can be basically interpreted in the same way: it's the limit as the regularization parameter goes to zero.
