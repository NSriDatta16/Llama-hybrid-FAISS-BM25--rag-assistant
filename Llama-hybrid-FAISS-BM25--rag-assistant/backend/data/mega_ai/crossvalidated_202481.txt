[site]: crossvalidated
[post_id]: 202481
[parent_id]: 201497
[tags]: 
Looking at the task described I think you are dealing with a somewhat classical problem of classifying images albeit in more abstract form. In your case you want to classify spectrograms rather than standard images. I would suggest you look at image classification or spectrogram classification tutorials online. Having said this general advice: A standard way to reduce your data would be to use principal component analysis. I think you tried to do somewhat gave up half-way in the notebook you attach. In any case, you would do something like this: Transform your matrix dataset into vectors. (ie. if you have 100 50$ \times$ 50 matrices, you will now have 100 2500-element vectors; see here for more information on the equivalence of two-dimensional PCA to line-based PCA) Use PCA to derive the principal components $\phi$ (You will possibly deal with a $n here and here for some first steps). Use $\phi$ to get the principal component scores $\xi$ (the projected data essentially). Use $\xi$ as surrogate data for your original sample, ie. you classify your sample using these. (I would personally suggesting using a logistic regression first before moving to more exotic beasts (eg. decision trees , boosting , etc.)) The only real restriction you must take care of is the need for the matrices holding your spectrograms to be of the same sizes. You can achieve that in multiple ways; you need to either define a representative utterance time (so you practically interpolate all your sample onto a common time-grid) or you segment your utterance accordingly. Both options are slightly messy because you will need to do quite a bit of data wrangling . I have worked with human utterances in the past and that was somewhat "easy" because there is the concept of a word (usually) but the principal is the same; you want the space over your data are recorded to be the same for all data and if possible to have a physical interpretation. You may want to consider using 2D Wavelets as a basis instead of the principal components suggested by PCA. 2D wavelets are localised on time and frequency so that should give you a more informative representation than say a Fourier polynomial basis that is focusing on frequency only. I am not a frequent Python user but Python seems to have some implementation on this matter already (eg. pywavelets ) if you want to experiment on this. Additional points: Check the noise in your sample. Usually statistical samples are assumed to have the same amount of "noise" in them (by noise I mean unstructured variance). Recording almost always do not have this luxury. Different recording environments, different equipment, etc. can lead to a lot heteroskedasticity . You will probably need to denoise your data first. Check your sample for frequency artefacts. Sound recording are finite representations of a infinite process (a wave). A large diverse sample usually has recording on different sampling frequencies. Before starting you want all your sampling frequencies to be similar because otherwise you risk aliasing artefacts .
