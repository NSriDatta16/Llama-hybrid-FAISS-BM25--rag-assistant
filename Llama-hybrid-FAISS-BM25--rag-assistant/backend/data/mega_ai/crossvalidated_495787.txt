[site]: crossvalidated
[post_id]: 495787
[parent_id]: 495701
[tags]: 
You are indeed not the first one who thinks like this. The pragmatic answer would be: people tried, but the negative-log likelihood seems to work better. There are several relatively successful attempts (mostly in machine translation): Von mises-fisher loss for training sequence to sequence models with continuous outputs A Margin-based Loss with Synthetic Negative Samples for Continuous-output Machine Translation Efficient Contextual Representation Learning Without Softmax Layer I think the main drawback of the methods is that you need to have the word/symbol embeddings in advance, however, for high-resource tasks (such as MT or large-scale representation pretraining), it is usually better when you train the embeddings end-to-end with the rest of the model. Another issue is that SoTA models do not segment the text into standard word tokens, but use sub-word segmentation (BPE or SentencePiece) and current methods for word embeddings (such as Word2Vec or FastText) struggle to learn good embeddings for subwords. So, I believe that if someone solves these two issues, what you propose will be the way to go.
