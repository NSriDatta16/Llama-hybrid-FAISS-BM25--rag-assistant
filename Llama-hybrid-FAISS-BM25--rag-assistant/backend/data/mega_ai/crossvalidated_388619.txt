[site]: crossvalidated
[post_id]: 388619
[parent_id]: 
[tags]: 
Models and standard scores

I'd like to use standard scores to describe a variable $y$ . If I always measured $y$ in a particular place and time, I'd just take the mean and standard deviation of $y$ and then calculate the standard score for $y_i$ like so: $z_i = \frac{y_i - \mu}{\sigma}$ . Let's say that $y(x)$ , though, like in the example below. # Set RNG seed set.seed(33550336) # Create toy data frame df If I just used the mean and standard deviation across all of $x$ to calculate $z_i$ , it would be skewed by where (i.e., at which $x$ ) I observed $y_i$ . To get around this, I fit a linear model which gives an expected value for $y(x)$ , shown as the red line in the plot. I can then calculate the difference between $y_i$ and the model and this would account for the fact that $y(x)$ . # Add model predicted value to data frame df$y_p $anom y - df$y_p # Look at results head(df) #> x y y_p anom #> 1 86.36470 849.751977 869.06456 -19.31258 #> 2 35.22257 450.308201 353.37350 96.93470 #> 3 52.57408 419.659783 528.33726 -108.67747 #> 4 6.67227 -9.700307 65.48683 -75.18714 #> 5 37.51383 419.145838 376.47735 42.66849 #> 6 91.97086 770.256306 925.59425 -155.33794 Created on 2019-01-22 by the reprex package (v0.2.1) This makes sense to me: a positive value would mean the observation is above average for that particular location (i.e., $x$ ). What baffles me is how to calculate the standard deviation with which to normalise the difference between model and observation. If I took the standard deviation over all of $x$ , this is based on the mean over all of $x$ . Presumably, the standard deviation should be constant over $x$ if observations are homoscedastic, but how do I calculate it? Edit Do I use the standard deviation of $y_i$ minus the model?
