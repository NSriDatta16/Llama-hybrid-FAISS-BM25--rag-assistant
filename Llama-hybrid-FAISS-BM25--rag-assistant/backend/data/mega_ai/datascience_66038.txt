[site]: datascience
[post_id]: 66038
[parent_id]: 66033
[tags]: 
Some algorithms perform feature selection inherently - e.g. LASSO , random forests , and gradient-boosted models like XGBoost and LightGBM . If you are using those then there is no need for manual feature selection. However if you go down the feature selection route, it maybe good to start with features which have been suggested by all the approaches you have tried (if there are any). Choosing the "best" feature really depends on your goal and setting. If you are chasing modelling accuracy, then the "best" feature might be the one which improves model accuracy the most when included in the model. If you are trying to reduce the data dimension, the "best" feature might be the one which captures the most information (e.g. a timestamp will capture year, month, day, hour). If you are going to dynamically retrieve features in live production, the best feature might be the one which adds the most accuracy to the model given its availability and cost / time.
