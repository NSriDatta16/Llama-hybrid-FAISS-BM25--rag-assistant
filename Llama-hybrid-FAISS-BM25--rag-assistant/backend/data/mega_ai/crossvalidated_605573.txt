[site]: crossvalidated
[post_id]: 605573
[parent_id]: 
[tags]: 
Is Glorot/He-style variance-preserving *regularization* a known thing?

In the context of deep learning, layer weight initialization techniques of the Glorot / He family strive to select initial random weight values in such a way that the variance of each layer's output is equal to the variance of its input. This helps avoid vanishing/exploding gradient issues and improves training stability. It seems likely to me that this is a desirable property to maintain not only initially, but also during training, as a regularization constraint via an explicit loss term nudging weight distribution to the optimal distribution mandated by Glorot/He. Has this been studied?
