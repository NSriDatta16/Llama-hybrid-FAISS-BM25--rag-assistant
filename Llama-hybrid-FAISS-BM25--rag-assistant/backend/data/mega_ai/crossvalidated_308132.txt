[site]: crossvalidated
[post_id]: 308132
[parent_id]: 
[tags]: 
Word2vec/Doc2vec clustering

Application/Desire : I want to be able to cluster word2vec vectors using density based clustering algorithms (say dbscan/hdbscan; due to too much noise in data) using python or R. I cannot compute pairwise distance b/w vectors as the size is too big (>2.5 million vocab). DBSCAN/HDBSCAN in both R and python does not directly support cosine distance as a metric. Question: Using the vectors (say 250 dimensions), if I am to reduce it using a non-linear dim reduction algo like T-SNE or autoencoder or SOM to say 50 dimensions, can I use euclidean metric to cluster using density based clustering algos? does the dim reduction algos also shift distance metrics? so that I can here use euclidean instead of cosine metric ?? Other suggestions are also welcome
