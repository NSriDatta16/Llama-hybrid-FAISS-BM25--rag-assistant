[site]: datascience
[post_id]: 24454
[parent_id]: 24452
[tags]: 
In perspective of storing data in databases, storing correlated features is somehow similar to storing redundant information which it may cause wasting of storage and also it may cause inconsistent data after updating or editing tuples. If we add so much correlated features to the model we may cause the model to consider unnecessary features and we may have curse of high dimensionality problem , I guess this is the reason for worsening the constructed model. In the context of machine learning we usually use PCA to reduce the dimension of input patterns. This approach considers removing correlated features by someway (using SVD ) and is an unsupervised approach. This is done to achieve the following purposes: Compression Speeding up learning algorithms Visualizing data Dealing with curse of high dimensionality Although this may not seem okay but I have seen people that use removing correlated features in order to avoid overfitting but I don't think it is a good practice. For more information I highly recommend you to see here . Another reason is that in deep learning models, like MLPs if you add correlated features, you just add unnecessary information which adds more calculations and parameters to the model.
