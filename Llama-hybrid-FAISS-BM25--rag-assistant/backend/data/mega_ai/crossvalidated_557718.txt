[site]: crossvalidated
[post_id]: 557718
[parent_id]: 557709
[tags]: 
It’s hard to say what constitutes acceptable performance. For instance, it might sound like awesome performance if your classifier gets $90\%$ of its predictions right, but if the data are the MNIST handwritten digits, such performance is rather pedestrian. (Note, however, that “classification accuracy” is more problematic than it first appears.) Being able to beat some baseline is a good start, however, and it mimics $R^2$ in linear regression. In linear regression, the goal is to predict what value you would expect, given some values of the features. The most naïve way of guessing such a value that is sensible is to guess the mean of your $y$ every time. If you can’t do better than that, then why is your boss paying you when she can call np.mean(y) in Python and do better? (This is the “overall and unconditional distribution” in Stephan Kolassa’s answer.) What you propose uses the same idea. If you know there is a 50/50 chance of each outcome, for your model to be worth using, your model ought to be able to outperform randomly guessing based on that 50/50 distribution of labels. In the kind of problem you are solving, there are many options for analogues of $R^2$ . It is not clear how large any of them should be for your model to satisfy business needs, as this depends on the problem and business requirements (customer demands, regulator demands, investor demands, etc). However, if they show your model is outperformed by a naïve model that always guesses the same value, then you are not making a strong case for your modeling skills. Links of possible interest: Why to put variance around the mean line to the definition of $R^2$? By what is this particular choice dictated? Why getting very high values for MSE/MAE/MAPE when R2 score is very good Why does putting a 1/2 in front of the squared error make the math easier? (Data Science)
