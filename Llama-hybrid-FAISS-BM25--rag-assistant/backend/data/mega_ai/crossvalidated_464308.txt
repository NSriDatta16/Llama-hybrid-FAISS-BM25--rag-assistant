[site]: crossvalidated
[post_id]: 464308
[parent_id]: 457543
[tags]: 
Variational Inference is known to produce a lower bound on the log marginal likelihood: $$ \log p(x) = \log \int p(x|\theta) p(\theta) d\theta \ge \int q(\theta|x) \log \frac{p(x|\theta) p(\theta)}{q(\theta|x)} d\theta $$ Where $q(\theta|x)$ is an arbitrary distribution supposed to approximate the posterior $p(\theta|x)$ . The integral in the lower bound is likely to be intractable but can be estimated with Monte Carlo. The gap between the log marginal likelihood and the lower bound is known to be equal to the KL divergence between $q(\theta|x)$ and $p(\theta|x)$ . Since the lower bound is always smaller than the $\log p(x)$ , one can maximize the lower bound w.r.t. $q$ 's parameters to learn good posterior approximation. However, unless the true posterior can be represented by $q(\theta|x)$ , the gap will be non-zero. To tighten the lower bound further, one can use multiple samples (the so called IWAE lower bound): $$ \log p(x) \ge \int \dots \int \prod_{m=1}^M q(\theta_m|x) \log \left( \frac{1}{M} \sum_{m=1}^M \frac{p(x, \theta_m)}{q(\theta_m|x)} \right) d\theta_1 \dots d\theta_M $$ This lower bound obviously coincides with the previous one for $M = 1$ . Moreover, the more samples $M$ you use, the (strictly) tighter the bound becomes. In particular, in the limit of infinite samples ( $M = \infty$ ) the gap vanishes. Finally, you can combine the multisample lower bound with randomized truncation to obtain an unbiased estimate of $\log p(x)$ . For more details see the SUMO paper . In a related work Goda and Ishikawa propose an improved estimator that guarantees unbiasedness and finite variance under certain conditions. An alternative approach is use coupled Markov Chain to perform Unbiased MCMC (this falls under posterior sampling category).
