[site]: crossvalidated
[post_id]: 531588
[parent_id]: 
[tags]: 
According to Hinton's GLOM paper, why is the covariance structure important in vision models?

In Hinton's recent GLOM paper , he compares between the GLOM architecture & CNNs. In one of the disadvantages of CNNs, he mentions that the operations used in CNNs aren't a nice method for modeling the covariance structure. Why is that? Why is his alternative better? Here is the paragraph I am talking about In CNNs, the activity of a neuron is determined by the scalar product of a weight vector with an activity vector. This is not a good way to model covariance structure which is very important in vision. Taking the scalar product of an activity vector with another activity vector makes powerful operations like coincidence detection and attention much easier. Coincidences in a high-dimensional embedding space are a good way to filter out noise caused by occlusion or missing parts because, unlike sums, they are very robust to the absence of some of the coinciding predictions.
