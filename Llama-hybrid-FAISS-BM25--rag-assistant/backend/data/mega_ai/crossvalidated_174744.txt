[site]: crossvalidated
[post_id]: 174744
[parent_id]: 174738
[tags]: 
Formalizing gung's suggestion, you can estimate the sample mean by inverse probability weighting, also known as the Horvitz-Thompson estimator. It is admissible in the class of unbiased estimators. The H-T estimator can be used to estimate the sum $S = \sum_{i=1}^n y_i$ of sample values in a population using a random subsample, as well as the mean. Let's examine the sum estimator first. To model the subsampling, let $B_i \sim \text{Bernoulli}(p_i)$. Then the sum of the random subsample is $$\sum_{i=1}^n y_i B_i$$ the H-T estimator $\hat{S}$ of the population sum is $$\hat{S} = \sum_{i=1}^n y_i B_i / p_i$$ It is easy to see that $\hat{S}$ is unbiased: $$\mathbb{E}[\hat{S}] = \sum_{i=1}^n y_i \mathbb{E}[B_i] / p_i = \sum_{i=1}^n y_i p_i / p_i = S$$ To estimate the mean $S/n$ we can simply use $\hat{S}/n$ if $n$ is known. Otherwise $n$ can be estimated using inverse probability weighting once again: $$\hat{n} = \sum_{i=1}^n B_i/p_i$$ Both $\hat{S}$ and $\hat{n}$ are unbiased, but $\hat{S}/\hat{n}$ may have some bias. However, it should be small when the variance of numerator and denominator are well-controlled - for example in the large sample limit, provided the $p_i$ are not too small. Here's some R code that shows how the H-T mean estimator works. We assume $n$ is known and compute $\hat{S}/n$, but it is easy to make it calculate $\hat{S}/\hat{n}$ instead. n=1000 pop = 66+2*rnorm(n) incl_prob = runif(n) nTrial = 500 ht_est=numeric(nTrial) for (i in 1:nTrial) { included = as.logical(rbinom(n,1,incl_prob)) ht_est[i] = 1/n * sum(pop[included] / incl_prob[included]) } print(paste0('population mean: ',round(mean(pop),2))) print(paste0('average Horvitz-Thompson estimate: ',round(mean(ht_est),2))) print(paste0('standard error in Horvitz-Thompson estimate: ',round(sd(ht_est),2))) This code makes a single random population of 1000 subjects, subsamples from that population with a subject-dependent probability, then computes the H-T estimator. It does the subsampling & H-T estimation 500 times on the same population to help illustrate the estimator's accuracy. Here's a sample run: [1] "population mean: 65.94" [1] "average Horvitz-Thompson estimate: 65.9" [1] "standard error in Horvitz-Thompson estimate: 5.09" The first number is the population mean. The population is random, but is generated once at the beginning of the code and is fixed thereafter. Each of the 500 estimation trials takes a different random subsample from this single population. The second number is the average of 500 Horvitz-Thompson estimates of the population mean, each from a different random subsample pop[included] of the same fixed population pop . Notice how close it is to the population mean, illustrating the unbiased property of the H-T estimator. The third number is the standard deviation of those 500 estimates. It is an estimate of the standard error for any given H-T estimate of the population mean. You might wonder why the average H-T estimate is so much closer to the population mean than the standard error would suggest. This is because we have averaged 500 H-T estimates together, and the error in these estimates is roughly $\sigma / \sqrt{T}$ where $\sigma$ is the standard deviation (in this case 5.09) and $T$ is the number of trials. In our code $T = 500$ so $\sigma / \sqrt{T} = 0.22$, which is on the order of the actual deviation, $0.4$, between the population mean and the 500 averaged H-T estimates.
