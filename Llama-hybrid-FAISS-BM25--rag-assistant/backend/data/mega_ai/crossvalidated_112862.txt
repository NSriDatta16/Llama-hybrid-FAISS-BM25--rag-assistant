[site]: crossvalidated
[post_id]: 112862
[parent_id]: 112219
[tags]: 
If I understand your question correctly, you're just looking for way to select between GLM models in a way that doesn't depend on having different predictions. If that's the case, you're out of luck in the way of loss-function-like criteria like $R^2$ and MSE (and its various analogues). However you have many options besides. The most classical solution would be a likelihood ratio (LR) test . Since you're fitting several GLMs with the same linear component, comparing AICs or BICs would be redundant since they would reduce to LR tests anyway. You could also directly conduct tests of equality of variance, with either Levene's test or the very similar Brown-Forsythe test , although my personal experience with both starts and stops in a classroom several years ago. A third (and in my opinion superior) approach would be to simulate the marginal distribution of your outcome (by plugging your inputs and ML parameters into the likelihood, then repeatedly sampling) and compare it to the empirical distribution. This is a standard approach in Bayesian statistics, where generating such distributions is natural and classical goodness-of-fit tests are unavailable. One Bayesian term for this is "posterior predictive checking," and in the MLE case the posterior happens to be equal to the likelihood. If your response variable is continuous, obvious tests in this case would be the two-sample versions of the Kolmogorov-Smirnov and Cramér–von Mises tests. If your response variable is discrete, you could use Pearson's Chi-square test . The Chi-square test would also work on binned continuous data. I'm a big fan of binning in cases when observations are clumped, or sparse, or there in cases where it would "wash out" problems in the data. Several other possibilities, based specifically on Bayesian approaches, are listed here . A note about the "CPO" listed in that last link: leave-one-out cross-validation is expensive. A cheaper alternative would be to just compare the averages of your posterior densities, i.e. $\frac{1}{N}\sum_i{(\ln{f{(y_i|\hat{\theta}_{g}^{ML})}})}$ and $\frac{1}{N}\sum_i{(\ln{g{(y_i|\hat{\theta}_{f}^{ML})}})}$. (Note that in Bayesian statistics, it is additionally possible (and desired) to marginalize over the distribution of parameters as well, and this advantage is lost in the MLE use case. But given that MLE makes distributional assumptions on the response variable (and MLE is just a special case of Bayesian estimation anyway), in my mind the most practical way to check those assumptions is to do so directly by checking their implications.)
