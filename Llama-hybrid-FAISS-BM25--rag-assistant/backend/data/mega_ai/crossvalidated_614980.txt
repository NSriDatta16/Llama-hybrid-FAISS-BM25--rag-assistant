[site]: crossvalidated
[post_id]: 614980
[parent_id]: 
[tags]: 
Why the Transformer model does not require negative sampling but word2vec does?

Both word2vec and transformer model compute a SOFTMAX function over the words/tokens on the output side. For word2vec models, negative sampling is used for computational reasons: Is negative sampling only used for computational reasons? https://stackoverflow.com/a/56401065/1516331 So why don't we use negative sampling for the transformer models as well? Is it because: Transformer model was published later than word2vec, so now computational cost of the softmax over the whole vocab is not a big concern; Transformer is an autoregressive model. We need to ensure accurate estimate of the probability distribution as much as possible. Are these true? Any other reasons?
