[site]: stackoverflow
[post_id]: 4261352
[parent_id]: 4251999
[tags]: 
Since reading is so slow, I suppose you can throw some CPU power at it so you can try to make an educated guess of how much to read. That would be basically a predictor, that would have a model based on probabilities. It would generate a sample of predictions of the upcoming message size, and the cost of each. Then pick the message size that has the best expected cost. Then when you find out the actual message size, use Bayes rule to update the model probabilities, and do it again. Maybe this sounds complicated, but if the probabilities are stored as fixed-point fractions you won't have to deal with floating-point, so it may be not much code. I would use something like a Metropolis-Hastings algorithm as my basic simulator and bayesian update framework. (This is just an initial stab at thinking about it.)
