[site]: datascience
[post_id]: 25026
[parent_id]: 24872
[tags]: 
Many neural network examples you see in the literature are doing classification problems, E.g. LeNet, AlexNet, Inception, etc are all image classification problems. In this domain, it's useful for the neural network to give outputs between 0 and 1 because an output between 0 and 1 can be interpreted, in some sense, as a probability. The reason these networks output numbers between 0 and 1 is in the layer activations of the network. The last layer in these networks is usually a softmax layer (or, if you're doing just binary classification, a sigmoid layer). Softmax and sigmoid functions have the nice property that they give outputs between 0 and 1 (softmax has the added nice property that it gives outputs which sum to 1). If you want your neural net to be able to output numbers that aren't between 0 and 1, simply change your activation function. Instead of a softmax last layer, you could use a linear one. In this case, it also makes sense to change the loss function you are using to perhaps something like Mean Squared Error (binary cross entropy, for example, won't work too well on negative numbers). There's nothing stopping you from using a deep neural network to perform regression rather than classification.
