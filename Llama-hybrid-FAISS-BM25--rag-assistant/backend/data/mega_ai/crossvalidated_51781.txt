[site]: crossvalidated
[post_id]: 51781
[parent_id]: 
[tags]: 
Which model for count data over time?

I am currently assessing some results I have from a model I applied on a corpus of text data I have mined. My problem is that my professor have told me to use a certain method, and I do not really know how to attack this problem the most sensible way. The main idea is to assess if there is a relationship between months and/or years on my response variable. My data has the following nature: DV: Binary (Event or non event) IV1: Month where event occurred/not occurred IV2: Year where event occured/not occurred I have a total of 431.000 observations As it is now I have transformed my data so that I have count data for each time period instead of a binary DV. I also did a logit transformation on defined as ln(DV/(1-IV3)), in order to sort out the effect of activity. DV(Event): Events in a time period given IV1 and IV2 IV1(Year): Year 1995 to 2012 IV2(Month): Jan to Dec within IV1 IV3(Activity): Events + Non-events in time period given IV1 and IV2 (Neutralized) I have a total of 176 observations Right now my model is defined as: DV = b1*IV1 + b2*IV2 + error but I am struggling alot with the intuition of whether this make sense. I have been looking into poisson models and zero inflated models, but until now the most sensible I think is to do the logit transformation in order to neutralize activity and the look at year and month as factors in a normal linear regression, but then again I am thinking that I might as well use a logistic regression. Does any of you know how to handle such a case? I am using R, SPSS and rapidminer.
