[site]: datascience
[post_id]: 118740
[parent_id]: 118739
[tags]: 
Well, you missed the old good human evaluation, which is the only actual measure that can actually be trusted in terms of semantic evaluation. Also, in the reference n-gram matching area you missed BLEU , which is the standard evaluation measure in machine translation, and METEOR , which applies stemming and handles synonyms to avoid the surface word problem you mentioned. Apart from that, in the space of embedded vector approaches, it is worth mentioning BERTscore , which seems to work very very well. This is the abstract of the article, published at ICLR'20: We propose BERTScore, an automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTScore correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task to show that BERTScore is more robust to challenging examples when compared to existing metrics. These are some references to better understand BERTscore's measures and their traits: A Fine-Grained Analysis of BERTScore (WMT'21) A Study of Automatic Metrics for the Evaluation of Natural Language Explanations (EACL'21) BERTScore is Unfair: On Social Bias in Language Model-Based Metrics for Text Generation (EMNLP'22) Also worth mentioning the WMT Metrics Shared Task , which aims at devising automatic evaluation metrics for machine translation. There, each year you can find practical comparisons of the submitted evaluation metrics.
