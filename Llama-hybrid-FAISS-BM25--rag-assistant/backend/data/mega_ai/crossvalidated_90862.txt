[site]: crossvalidated
[post_id]: 90862
[parent_id]: 90479
[tags]: 
I think you are (or at least were) confused about several things here. Doing SVD of your data matrix $\mathbf{X}$ amounts to doing the PCA. Decomposition $\mathbf{X}=\mathbf{USV^\top}$ decomposes the data into principal axes (eigenvectors of the covariance matrix) and principal "scores", or "principal components", i.e. projections of the data onto the principal axes. The "other rotation matrix" you were asking about is simply these projections (up to the scaling given by singular values). Note that both $\mathbf{U}$ and $\mathbf{V}$ are orthogonal matrices, meaning that principal axes are orthogonal and principal components have correlation zero. But if you have a linear mixture of some sources, it is usually a non-orthogonal mixture, meaning that truly independent axes (that ICA attempts to find) are (usually) not orthogonal to each other. When you say that it can be shown that with a rotation, an independent rescaling of each of the rotated axes, and a second rotation you can recover the original, independent axes you probably mean that taking your data axes (i.e. the set of basis vectors, $\mathbf{I}$), rotating them, scaling and rotating again, you can arrive to the independent axes $\mathbf{A}$. And it is certainly true. If you know $\mathbf{A}$, then you can do SVD of that matrix, and it will give you exactly these rotations and scaling you were talking about. But if you don't know $\mathbf{A}$, then there is no way you can perform an SVD of anything to recover it. Instead, you need to rely on some assumptions about how the independent components should look like (e.g. they should be as non-Gaussian as possible), and that is what ICA is about.
