[site]: stackoverflow
[post_id]: 5615844
[parent_id]: 5615647
[tags]: 
Yes , a special regex pattern must be written for every site. But I think that 1- the treatments done with Beautiful Soup must be adapted to every site, too. 2- regexes are not so complicated to write, and with a little habit, it can be done quickly I am curious to see what kind of treatments must be done with Beautiful Soup to obtain the same results that I obtained in a few minutes. Once upon a time, I tried to learn beautiful Soup but I didn't undesrtand anything to this mess. I should try again, now I am a little more skilled in Python. But regexes have been OK and sufficient for me until now Here's the code for this new site: import urllib import re url = 'http://allrecipes.com/Recipe/Slow-Cooker-Pork-Chops-II/Detail.aspx' sock = urllib.urlopen(url) ch = sock.read() sock.close() x = ch.find('Ingredients ') patingr = re.compile(' \r\n +(.+?) \r\n') print '\n'.join(patingr.findall(ch,x)) . EDIT I downloaded and installed BeautifulSoup and ran a comparison with regex. I don't think I did any error in my comparison code import urllib import re from time import clock import BeautifulSoup url = 'http://allrecipes.com/Recipe/Slow-Cooker-Pork-Chops-II/Detail.aspx' data = urllib.urlopen(url).read() te = clock() x = data.find('Ingredients ') patingr = re.compile(' \r\n +(.+?) \r\n') res1 = '\n'.join(patingr.findall(data,x)) t1 = clock()-te te = clock() bs = BeautifulSoup.BeautifulSoup(data) ingreds = bs.find('div', {'class': 'ingredients'}) ingreds = [s.getText().strip() for s in ingreds.findAll('li')] res2 = '\n'.join(ingreds) t2 = clock()-te print res1 print print res2 print print 'res1==res2 is ',res1==res2 print '\nRegex :',t1 print '\nBeautifulSoup :',t2 print '\nBeautifulSoup execution time / Regex execution time ==',t2/t1 result 1/4 cup olive oil 1 cup chicken broth 2 cloves garlic, minced 1 tablespoon paprika 1 tablespoon garlic powder 1 tablespoon poultry seasoning 1 teaspoon dried oregano 1 teaspoon dried basil 4 thick cut boneless pork chops salt and pepper to taste 1/4 cup olive oil 1 cup chicken broth 2 cloves garlic, minced 1 tablespoon paprika 1 tablespoon garlic powder 1 tablespoon poultry seasoning 1 teaspoon dried oregano 1 teaspoon dried basil 4 thick cut boneless pork chops salt and pepper to taste res1==res2 is True Regex : 0.00210892725193 BeautifulSoup : 2.32453566026 BeautifulSoup execution time / Regex execution time == 1102.23605776 No comment ! . EDIT 2 I realized that in my code I don't use a regex, I employ a method that use a regex and find() . It's the method I use when I resort to regexes because it raises the speed of treatment in some cases. It is due to the function find() that runs extremly rapidly. To know what we are comparing, we need the following codes. In the code 3 and 4, I took account of remarks of Achim in another thread of posts: using re.IGNORECASE and re.DOTALL, ["\'] instead of " . These codes are separated because they must be executed in different files to obtain reliable results: I don't know why, but if all the codes are executed in the same file ,certain resulting times are strongly different (0.00075 instead of 0.0022 for exemple) import urllib import re import BeautifulSoup from time import clock url = 'http://allrecipes.com/Recipe/Slow-Cooker-Pork-Chops-II/Detail.aspx' data = urllib.urlopen(url).read() # Simple regex , without x te = clock() patingr = re.compile(' \r\n +(.+?) \r\n') res0 = '\n'.join(patingr.findall(data)) t0 = clock()-te print '\nSimple regex , without x :',t0 and # Simple regex , with x te = clock() x = data.find('Ingredients ') patingr = re.compile(' \r\n +(.+?) \r\n') res1 = '\n'.join(patingr.findall(data,x)) t1 = clock()-te print '\nSimple regex , with x :',t1 and # Regex with flags , without x and y te = clock() patingr = re.compile(' \r\n +(.+?) \r\n', flags=re.DOTALL|re.IGNORECASE) res10 = '\n'.join(patingr.findall(data)) t10 = clock()-te print '\nRegex with flags , without x and y :',t10 and # Regex with flags , with x and y te = clock() x = data.find('Ingredients ') y = data.find('h3>\r\n Footnotes \r\n') patingr = re.compile(' \r\n +(.+?) \r\n', flags=re.DOTALL|re.IGNORECASE) res11 = '\n'.join(patingr.findall(data,x,y)) t11 = clock()-te print '\nRegex with flags , without x and y :',t11 and # BeautifulSoup te = clock() bs = BeautifulSoup.BeautifulSoup(data) ingreds = bs.find('div', {'class': 'ingredients'}) ingreds = [s.getText().strip() for s in ingreds.findAll('li')] res2 = '\n'.join(ingreds) t2 = clock()-te print '\nBeautifulSoup :',t2 result Simple regex , without x : 0.00230488284125 Simple regex , with x : 0.00229121279385 Regex with flags , without x and y : 0.00758719458758 Regex with flags , with x and y : 0.00183724493364 BeautifulSoup : 2.58728860791 The use of x has no influence on the speed for a simple regex. The regex with flags , without x and y, takes longer to execute , but the result isn't the same as the others, because it catches a supplementary chunk of text. That's why in a real application, it would be the regex with flags and x/y that should be used. The more complicated regex with flags and with x and y takes 20 % of time less. Well, the results are not very much changed, with or without x/y. So my conclusion is the same the use of a regex, resorting to find() or not, remains roughly 1000 times faster than BeautifulSoup, and I estimate 100 times faster than lxml (I didn't installed lxml) . To what you wrote, Hugh, I would say: When a regex is wrong, it is not faster nor slower. It doesn't run. When a regex is wrong, the coder makes it becoming right, that's all. I don't understand why 95% of the persons on stackoverflow.com want to persuade other 5% that regexes must not be employed to analyse HTML or XML or anything else. I say "analyse", not "parse". As far as I understood it, a parser first analyse the WHOLE of a text and then displays the content of elements that we want. On the contrary, a regex goes right to what is searched, it doesn't build the tree of the HTML/XML text or whatever else a parser does and that I don't know very well. So, I am very satisfied of regexes. I have no problem to write even very long REs, and regexes allow me to run programs that must react rapidly after the analyse of a text. BS or lxml would work but that would be a hassle. I would have other comments to do , but I have no time for a subject in which, in fact, I let others to do as they prefer.
