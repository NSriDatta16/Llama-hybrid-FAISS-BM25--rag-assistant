[site]: datascience
[post_id]: 108268
[parent_id]: 108263
[tags]: 
You are mechanically fine using sentence size as your window size. All context/target word combinations will be treated as positive cases & things will seem to work. Logically speaking you also want to make some fraction of negative context/target word pairs to yield better embeddings. You can use the popular negative sampling technique. Roughly, it probabilistically picks word pairs from your vocab that don’t exist in your positive set. Like choosing grammatically impossible combinations: (“summer”, “actively”) Also, since sentence word counts in natural text varies, using this as a dynamic window size might learn uneven word representations. Not sure if this is good or bad but maybe you could report back what you find?
