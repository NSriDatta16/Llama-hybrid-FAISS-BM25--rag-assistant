[site]: crossvalidated
[post_id]: 481297
[parent_id]: 
[tags]: 
Posterior distribution of $\sigma^2$

In chapter 9 of Jim Albert's Bayesian computation with R it's mentioned that, in the context of Normal Linear Regression, the posterior joint density is: $$g(\beta, \sigma^2 | y) =g(\beta|y, \sigma^2)g(\sigma^2|y) $$ And it's stated (without proof) that the marginal posterior distribution of $\sigma^2$ ( i.e. $g(\sigma^2|y)$ ) is $Inv. Gamma((n-k)/2, S/2)$ where n = # of observations, k = # of parameters, and $S = (y-X\hat\beta)^T(y-X\hat\beta)$ . I was wondering how one get's to this (he assumed uninformative prior of $g(\beta, \sigma^2) \propto 1/\sigma^2$ )? I tried the following but got stuck: $$ g(\sigma^2|y) = \int_{\beta} g(\sigma^2, \beta|y)d\beta \propto \int_{\beta} g(y |\sigma^2, \beta)g(\sigma^2, \beta)d\beta \propto \int_{\beta} g(y |\sigma^2, \beta)\frac{1}{\sigma^2}d\beta \\ =\int_{\beta} (2\pi)^{-n/2}(\sigma^2)^{-n/2-1}e^{-\frac{S/2}{\sigma^2}}d\beta $$ So without doing the integral I can account for the $S/2$ (kind-of, if I estimate $S$ using $\hat\beta$ ), and $n/2$ in the inverse gamma. But how do I integrate, and will I then get $(\sigma^2)^{k/2}$ ?
