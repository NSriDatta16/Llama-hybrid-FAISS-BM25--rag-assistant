[site]: datascience
[post_id]: 94669
[parent_id]: 
[tags]: 
Understanding Object Detection Chart Showing Category Percentages "As Total Detections Increase"

I'm trying to understand the following picture, which comes from the "description of updates" pdf on this site for "Diagnosing Error in Object Detectors" by Derek Hoiem, Yodsawalai Chodpathumwan, and Qieyun Dai . The chart shows the results detected by an object-detector for the category "dog". It displays the percentage of correct (Cor/white) object detection results along with the percentages of various false positives: localization error (Loc/blue), confusion of similar images (Sim/red), confusion with background (BG/purple), and other (Oth/green). The graph also shows a recall curve (the red lines) which differ in their Intersection over Union thresholds. What I don't understand is the X axis. It shows how both the true/false positive percentages and the recall values change as the total detections increase , but what does that mean, for them to increase? In what context are they increasing? If it is merely iterating over the data, wouldn't the graph be different depending on the order in which you encountered the data? I have the vague idea that maybe this is an information retrieval concept? For reference, here is the whole chart and caption. The chart compares the results from 3 object detectors (the 3 columns) across 3 object categories (the 3 rows):
