[site]: crossvalidated
[post_id]: 257843
[parent_id]: 
[tags]: 
Constant error during training

I'm training a fully connected neural network using stochastic gradient descent (SGD). In the following graph I've ploted the training error (in the y axis) vs the epochs (in the x axis). As error function I've used the sum of squared errors $\sum_{i=0}^{N}(y - f(x_i))^2$ . After running the training several times, I've found out that the resulting training error, on average, behaves very similiar to the one shown in the graph: On the beginning it's high as weights have random values, As the weights are corrected the error decreases, For a large number of epocs (in the provided graph ~1500/3500) it seems that the error reached a plateau, The correct weights are finally computed and the error reaches a value close to zero. Given these facts, I have two questions: What aspect of the network apart from the weights can cause this plateau? As I'm using SGD, shouldn't the error decrease, or in any case oscillate, on each epoch? In the end of the training I get correct results, but it I'm curious on trying to find a way of diminishing the number of epochs this plateau lasts, so as to reduce the network training time.
