[site]: crossvalidated
[post_id]: 393600
[parent_id]: 393598
[tags]: 
The numbers inside the vector are not probabilities. Indeed, they are not bounded between 0 and 1. Each number is just a real number, and the whole vector represents the embedding for a given word. To find the similarity between two words you are supposed to compute the cosine similarity (again, not a probability). The way those numbers are computed depends on the implementation: common algorithms are SkipGram and CBOW. Basically a single layer neural network is trained to predict a word given some context (the size of the context is controlled by the window parameter) words (or vice versa), and after some epochs the layer provides word embeddings, that is vector representations of the words. If you increase the value of the size parameter, you obtain a richer embedding for each word. However, it looks like you are trying to do something without having a clue about what you are really doing. There is plenty of articles explaining word2vec, take a look at one of them: https://skymind.ai/wiki/word2vec
