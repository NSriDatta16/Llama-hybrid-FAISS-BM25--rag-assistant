[site]: crossvalidated
[post_id]: 237893
[parent_id]: 
[tags]: 
Should K=2 softmax regression and logistic regression give the same results?

I would like to demonstrate the "over-parameterization" of the softmax function and its relation to the sigmoid function with a practical example. With toy data, it's easy to show that: logits = np.array([.123, .456]) softmax(logits) == np.array([ sigmoid(logits[0] - logits[1]), 1 - sigmoid(logits[0] - logits[1]) ]) With real data, I'm constructing both a vanilla logistic regression model and vanilla k=2 softmax regression model, each without a bias term. All weights are initialized to .0001. I'm running 1 step of gradient descent, using a batch size of 1. Should the two functions make identical predictions? What relationship, if any, should we observe between the weights and/or logits of the respective functions? Empirically, I'm not seeing any relationship between the two.
