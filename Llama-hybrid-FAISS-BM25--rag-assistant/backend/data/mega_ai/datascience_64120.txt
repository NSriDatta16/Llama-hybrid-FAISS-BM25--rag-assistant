[site]: datascience
[post_id]: 64120
[parent_id]: 
[tags]: 
Any good Implementations of Bi-LSTM bahdanau attention in Keras?

From past few weeks I'm trying to learn sequence to sequence machine translation modelling but I couldn't find any good examples/tutorials with bahdanau attention implemented. I did come across a ton of examples where people have implemented attention but it's mostly on gru (or) luong attention (or) outdated code. Has anyone come across any good implementations of bahdanau attention model in keras which you have implemented (or) tried? I really want to learn the coding part but no good material regarding implementation is found? Please help.
