[site]: crossvalidated
[post_id]: 608255
[parent_id]: 606863
[tags]: 
This is how the continuous bag of words is defined. It is not a language model but a model for training word embeddings. Continuous means having a (small) sliding window over the training corpus. The bag of words part means that the model disregards the order of the words in the context window. Neither of these features is particularly good for language modeling where the order of words matters, and the longer the context, the better. One way to view Word2Vec training objectives is that you know that language modeling provides a good training signal for word embeddings. However, it is too computationally expensive when you only care about the embeddings. You want a solution that scales for hundreds of thousands of word forms when computing on the CPU. The solution is to simplify the language modeling objective as much as possible while still getting a reasonable training signal for the embeddings, which leads to CBOW and Skip-gram.
