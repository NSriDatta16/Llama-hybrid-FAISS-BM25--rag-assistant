[site]: crossvalidated
[post_id]: 615658
[parent_id]: 
[tags]: 
Expectation of the logit in logistic regression?

When doing logistic regression with binary cross entropy (or equivalently cross entropy) loss, the model tries to approximate conditional probabilities as $$ P(X\mid C) = \sigma(m(C)), $$ where $X$ is an event, $C$ is some condition, $m$ is our (in case of logistic regression linear) model, and $\sigma(y) = \frac{1}{1+e^{-y}}$ . According to wikipedia , we know that the loss is minimized by setting $$ m(C) = logit(P(X\mid C)) $$ where $$ logit(y) = \log\frac{y}{1-y}. $$ My first question is the following, do we know the expectation of the model? Is it equal to the expectation of the logit? Or, formally, $$ E(m(C)) \stackrel{?}{=} E(logit(P(X\mid C)) $$ Second, if it is equal, can we somehow express this as a function of $I_X$ , the indicator function of X, in the form $$ E(f(I_X)\mid C) = E(m(C)), $$ where we are looking for some $f$ ? Edit: In hindsight this question was not just unclear, but incorrect in its formulas, sorry. What I actually meant to ask is if it's true that the output of the model (the logits) approximate the conditional expectation of something. Such as $$ E(m(C)) \stackrel{?}{=} E(logit(P(X\mid C))\mid C) $$
