[site]: datascience
[post_id]: 57769
[parent_id]: 
[tags]: 
How does a CBoW model convert a word to a vector?

A CBOW model actually takes multiple words as inputs and a targeted central word as the output. So, the trained model actually maps several words to a single one, I mean it takes context words and outputs the central word. But what we expected to get is model mapping a word to its vector representation. It seems the output is coincident but not the input and the mapping. So like in genism, how it really works to map a word to its vector representation? Does it just save all final model's outputs as the central words' vector representation? But the final model's outputs would close to the ground truth's one-hot embedding rather than a vector with context information. For short, my question is: How does a CBoW model convert one word to its vector representation?
