[site]: datascience
[post_id]: 23157
[parent_id]: 12532
[tags]: 
Oddly enough, I found that larger batch sizes with keras require more epochs to converge. For example, the output of this script based on keras' integration test is epochs 15 , batch size 16 , layer type Dense: final loss 0.56, seconds 1.46 epochs 15 , batch size 160 , layer type Dense: final loss 1.27, seconds 0.30 epochs 150 , batch size 160 , layer type Dense: final loss 0.55, seconds 1.74 Related Keras issue 4708 : the user turned out to be using BatchNormalization , which affected the results. This tutorial on LSTM, section Tuning the Batch Size search results for keras lstm batch size effect on result My Neural Network isn't working! What should I do? , point 5 (You Used a too Large Batch Size) discusses exactly this Using too large a batch size can have a negative effect on the accuracy of your network during training since it reduces the stochasticity of the gradient descent. Edit: most of the times, increasing batch_size is desired to speed up computation, but there are other simpler ways to do this, like using data types of a smaller footprint via the dtype argument, whether in keras or tensorflow , e.g. float32 instead of float64
