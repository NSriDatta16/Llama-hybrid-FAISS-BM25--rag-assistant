[site]: crossvalidated
[post_id]: 621580
[parent_id]: 
[tags]: 
Cross validation and feature evaluation

I'm currently trying to perform feature evaluation using the permutation_importance method. Say that my model is GradientBoostingRegressor() and I have a dataset with 182 rows of data. At first, I was thinking that I should use 10-fold cross-validation in this procedure as well, since it gives a less biased estimate of model performance, I was thinking that it would also help us get more accurate values of the feature importance values. So I tried to incorporate 10-fold CV into the procedure. The model would be trained on 9 folds of the datasets, then it would be put into the permutation_importance(...) function with the remaining fold as the test fold and n_repeats is also set as 10. This seemed quite logical to me. The standard deviation of each feature importance value is then calculated as the average of the std dev's over the 10 folds. However, the std dev values look very big. However, when I changed the number of folds from 10 to 2, the std dev values for each feature importance all decrease significantly. I went to look at some ML studies that carried out permutation importance method as part of feature evaluation. But they don't seemed to have mentioned that they did cross-validation as part of the procedure. Thus, I am wondering if there even a need to do any k-fold CV at all when carrying out the permutation importance method. Instead, perhaps all I should have done was just to arbitrarily do a train_test_split to split the dataset into 80% training and 20% test data.
