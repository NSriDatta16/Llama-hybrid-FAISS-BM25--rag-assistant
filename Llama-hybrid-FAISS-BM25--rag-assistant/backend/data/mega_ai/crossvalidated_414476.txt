[site]: crossvalidated
[post_id]: 414476
[parent_id]: 414470
[tags]: 
First, 61% accuracy isn't necessarily so bad. You're outperforming random guessing based on the prior distribution that would give about 50% accuracy. Perhaps you know that 61% is inadequate for your application, but don't dismiss 61% just because it is not as high as 99.9% like people have gotten on MNIST. What it means, though, that your parameters are significant but your accuracy is poor is that you are not accounting for everything influencing class membership. Are there interactions between variables such as $a b$ or higher-order terms such as $a^2$ ? Are there additional covariates that were not included in your data but could influence the outcome, perhaps an $L$ variable? As you read about improving machine learning performance, be careful about the difference between in-sample and out-of-sample performance. You have an issue of bias rather than variance. Consequently, while ridge, LASSO, or elastic net regularization may seem like appealing techniques to explore to boost performance, they are not what you seek.
