[site]: crossvalidated
[post_id]: 146242
[parent_id]: 
[tags]: 
A question about SVM kernels

this is a very basic question about SVM. I was using SVMs that are provided in the scikit for some problems, and noted that they are quite slow for big datasets. I then learned more about the implementation of the SVMs in the coursera course, and there the professor says that the SVM in practice often uses all the input vectors in the training set as the kernel. From my understanding it meant that there's as many "features" (I don't know if "feature" is correct word when SVMs are concerned) as there are input samples. Obviously, that would explain the slowness, because the training time is now quadratic in number of input samples. However, I often see people on forums saying that they used SVMs in a context where it is very clear that the training set is quite large. So my question is: is my understanding of how SVMs work completely wrong, and using all the input samples as the kernel does not make it quadratic, or do people use some other kernels often in practice? And if yes, what are the common kernels that can be used with SVMs?
