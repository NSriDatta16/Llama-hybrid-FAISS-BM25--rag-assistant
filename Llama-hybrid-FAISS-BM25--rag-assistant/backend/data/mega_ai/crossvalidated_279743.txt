[site]: crossvalidated
[post_id]: 279743
[parent_id]: 
[tags]: 
ReLU when outputs should be constrained

I'm working on a neural network, and I was wondering whether, when I want to constrain the output to be in the range (0,1), I should still be using ReLU (assuming I still want the advantages ReLU has in training). Would it eventually learn on its own to ensure that the weights arrive at this result, or would using a function that's already defined as having an output of (0,1) such as the sigmoid function?
