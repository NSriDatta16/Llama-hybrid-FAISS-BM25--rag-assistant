[site]: datascience
[post_id]: 13559
[parent_id]: 13504
[tags]: 
SQL / Big Data Tools Getting and cleaning data is a central part of a data scientist's job. My team (data science at a mid-to-large internet company) regularly deals with data wrangling at scale -- combining terabytes of browsing history with a multitude of other data sources, structured and unstructured, to investigate problems. SQL and Spark/Hive/Hadoop are nearly daily parts of our workflow. We filter new candidates heavily on their ability to use SQL intelligently and favor those who have worked with big data technologies in production environments. On a side note, distributed processing/storage is a fascinating research area and the mechanics behind databases are pretty cool. Perhaps you can make it a bit more interesting for yourself by not just focusing on the semantics of SQL, but also some under-the-hood bits, like multi-pass algorithms for joins and so forth. You could read the Amazon DynamoDB paper or Google's BigTable paper for an intro to big, distributed databases. Programming environment Our team is python heavy, though we are using Spark more and more. It's got a great ecosystem for working with big data and has great machine-learning support via scikit-learn and the like. We often prototype in jupyter notebooks and scale solutions by building python apis that run our jobs or handle incoming requests. Other data science teams here use R. Pick one, do a few projects in it, and do a few projects in the other. You'll figure out which syntax and style works best for you. Try to train yourself not to code like a grad student anymore, either. :)
