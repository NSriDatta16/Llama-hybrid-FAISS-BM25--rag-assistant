[site]: crossvalidated
[post_id]: 445772
[parent_id]: 445726
[tags]: 
Just to state this up-front: most machine learning models just try to predict. They do not find/show causal effects, understand what is going on, model disease mechanisms or medical relationships. I.e. model explanations may not point to what happens in terms of causality/disease mechansim, but only highlight what appears to predict best. Something may be very useful as a predictor, but completely non-causal/not something you should intervene on/not a useful insight. Example: a priest giving the last rites to a Catholic patient is probably a pretty big predictor of mortality risk, but does not cause deaths and is not something you should try to intervene against. Here are a couple of possible explanations for what you see: There are often many correlated predictors (e.g. history of heart attacks, history of PCI, history of CABG, taking a statin, taking a P2Y12, taking low dose aspirin,...). Some possible issues include that other factors correlate with the ones you look at and conditional on the values of these other factors, the particular thing is less predictive of events. There is the possibility that there is a medical intervention. Rich Caruana described a case where a model predicted that amongst patients hospitalized due to pneunomia those with asthma were at a low risk of dying, but that may have been only because guidelines foresee extremely aggressive treatment and close monitoring for such patients. This may be some odd property of your training data that the models have identified. E.g. you may have sampled/obtained the data in such a way that the predictor you look at is actually (relatively speaking) a good sign (as compared to the alternative way of getting into the data). Extreme example: you sampled patients that survived a heart attack >2 years ago, patients with end-stage cancer and patients with NYHA class IV heart failure - in that case patients with a history of a heart attack will tend to be the healthies patients. Obviously, nobody does something like that deliberately, but e.g. if you took everyone in an intensive care unit of a particular hospital, then some pretty severe condition may be relatively mild compared to the other ways you ended up in the ICU. The model just fitted to something in the data for some reason (e.g. random noise in the data, small dataset, overfitting etc.) and this may very well be completely wrong - perhaps you should be thankful to these methods for highlighting it. All of these could be true and one needs to look at the particulars of the case to figure out which of these (or perhaps something else) applies.
