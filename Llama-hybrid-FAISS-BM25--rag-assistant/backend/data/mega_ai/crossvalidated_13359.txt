[site]: crossvalidated
[post_id]: 13359
[parent_id]: 13335
[tags]: 
The problem of estimating probabilities falls under the category of "regression," since the probability is a conditional mean. Classical methods for feature selection (AKA "subset selection" or "model selection") methods for regression include best-k, forward- and backward- stepwise, and forward stagewise, all described in Chapter 3 of Elements of Statistical Learning . However, such methods are generally costly, and given the number of features in your dataset, my choice would be to use glmpath , which implements L1-regularized regression using a modification of the fantastically efficient LARS algorithm. EDIT: More details on L1 regularization. The LARS algorithm produces the entire "Lasso" path for $\lambda$ (the regularization constant) ranging from 0 to $\infty$. At $\lambda=0$, all features are used; at $\lambda=\infty$, none of the features have nonzero coefficients. In between there are values of $\lambda$ for which anywhere from 1 to 199 features are used. Using the results from LARS one can select the values of $\lambda$ with the best performance (according to whatever criteria). Then, using only the features with nonzero coefficients for a particular $\lambda$, one can then fit an unregularized logistic regression model for the final prediction.
