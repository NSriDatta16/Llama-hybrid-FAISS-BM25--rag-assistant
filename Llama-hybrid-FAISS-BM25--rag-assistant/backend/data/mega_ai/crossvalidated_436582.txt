[site]: crossvalidated
[post_id]: 436582
[parent_id]: 
[tags]: 
Comparison of statistical tests exploring co-dependence of two binary variables

Suppose we observe data $(X_i,Y_i)_{i=1,...,n}$ on two binary variables: $X\in\{0,1\}$ and $Y\in\{0,1\}$ . We would like to test if $X$ and $Y$ are co-dependent (related). Standard suggestions in mainstream textbooks are the following: chi-square test for independence of $X$ and $Y$ , Z-test for comparing proportions of $[Y = 1]$ between two groups: $[X = 0]$ and $[X = 1]$ , Z-test for comparing proportions of $[X = 1]$ between two groups: $[Y = 0]$ and $[Y = 1]$ . In addition to that, we can run logistic regressions of $Y$ on $X$ and $X$ on $Y$ . We can check statistical significance of the slope coefficients. There are at least $3$ standard tests for that: likelihood ratio, Wald and deviance. Since we consider two regressions, there are $3 * 2 = 6$ tests added, making the total number $9$ . But wait, we can run probit models too. Et cetera, et cetera, ... Is there one or more references which systematically and rigorously answer(s) the following questions: Which tests are algebraically equivalent and when? Which tests are most powerful, why and when? What is the power function for each test and each sample size? In practical terms, which tests deliver the same verdict almost always?
