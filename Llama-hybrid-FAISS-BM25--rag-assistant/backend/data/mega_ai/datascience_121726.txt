[site]: datascience
[post_id]: 121726
[parent_id]: 
[tags]: 
val_accuracy and val_loss not changing while training transformer

recently i have been trying to learn transformer and using it in caption-generator model. While training for 4 hours val_loss and val_accuracy did not change. loss and accuracy for train_data was atleast moving a little. (this output is from different training session but is quite similar to previous one with 4 hour training) Epoch 1/10 100/100 [==============================] - 123s 566ms/step - loss: 12.8864 - masked_accuracy: 0.0140 - val_loss: 12.9553 - val_masked_accuracy: 0.0216 Epoch 2/10 100/100 [==============================] - 50s 498ms/step - loss: 13.0352 - masked_accuracy: 0.0199 - val_loss: 12.9553 - val_masked_accuracy: 0.0216 Epoch 3/10 100/100 [==============================] - 47s 473ms/step - loss: 13.0575 - masked_accuracy: 0.0197 - val_loss: 12.9553 - val_masked_accuracy: 0.0216 Epoch 4/10 100/100 [==============================] - 42s 419ms/step - loss: 13.0294 - masked_accuracy: 0.0198 - val_loss: 12.9553 - val_masked_accuracy: 0.0216 Epoch 5/10 100/100 [==============================] - 38s 380ms/step - loss: 13.0738 - masked_accuracy: 0.0203 - val_loss: 12.9553 - val_masked_accuracy: 0.0216 Epoch 6/10 100/100 [==============================] - 37s 370ms/step - loss: 13.0334 - masked_accuracy: 0.0190 - val_loss: 12.9553 - val_masked_accuracy: 0.0216 Epoch 7/10 100/100 [==============================] - 36s 363ms/step - loss: 13.0213 - masked_accuracy: 0.0197 - val_loss: 12.9553 - val_masked_accuracy: 0.0216 Epoch 8/10 100/100 [==============================] - 36s 365ms/step - loss: 13.0269 - masked_accuracy: 0.0206 - val_loss: 12.9553 - val_masked_accuracy: 0.0216 Epoch 9/10 100/100 [==============================] - 36s 364ms/step - loss: 13.0469 - masked_accuracy: 0.0193 - val_loss: 12.9553 - val_masked_accuracy: 0.0216 Epoch 10/10 what could be the reason. here is the model code here github open transformer.py and here is the notebook def positional_encoding(length, depth): depth = depth/2 positions = np.arange(length)[:, np.newaxis] # (seq, 1) depths = np.arange(depth)[np.newaxis, :]/depth # (1, depth) angle_rates = 1 / (10000**depths) # (1, depth) angle_rads = positions * angle_rates # (pos, depth) pos_encoding = np.concatenate( [np.sin(angle_rads), np.cos(angle_rads)], axis=-1) return tf.cast(pos_encoding, dtype=tf.float32) # Positional embedding For Image class PositionalEmbedding(tf.keras.layers.Layer): def __init__(self, vocab_size, d_model): super().__init__() self.d_model = d_model self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True) self.pos_encoding = positional_encoding(length=2048, depth=d_model) def compute_mask(self, *args, **kwargs): return self.embedding.compute_mask(*args, **kwargs) def call(self, x): length = tf.shape(x)[1] x = self.embedding(x) # This factor sets the relative scale of the embedding and positonal_encoding. x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) x = x + self.pos_encoding[tf.newaxis, :length, :] return x class Patches(tf.keras.layers.Layer): def __init__(self, patch_size): super().__init__() self.patch_size = patch_size def call(self, images): batch_size = tf.shape(images)[0] patches = tf.image.extract_patches( images=images, sizes=[1, self.patch_size, self.patch_size, 1], strides=[1, self.patch_size, self.patch_size, 1], rates=[1, 1, 1, 1], padding="VALID", ) patch_dims = patches.shape[-1] # (patches.shape) patches = tf.reshape(patches, [batch_size, -1, patch_dims]) return patches class PatchEncoder(tf.keras.layers.Layer): def __init__(self, num_patches, d_model): super().__init__() self.num_patches = num_patches self.projection = Dense(units=d_model) self.position_embedding = Embedding( input_dim=num_patches, output_dim=d_model ) def call(self, patch): positions = tf.range(start=0, limit=self.num_patches, delta=1) # tf.print(positions.shape) return self.projection(patch) + self.position_embedding(positions) # Attention class BaseAttention(tf.keras.layers.Layer): def __init__(self, **kwargs): super().__init__() self.mha = tf.keras.layers.MultiHeadAttention(**kwargs) self.layernorm = tf.keras.layers.LayerNormalization() self.add = tf.keras.layers.Add() class CrossAttention(BaseAttention): def __init__(self, **kwargs): super().__init__(**kwargs) self.last_attn_scores=None def call(self, x, context): attn_output, attn_scores = self.mha( query=x, key=context, value=context, return_attention_scores=True) # Cache the attention scores for plotting later. self.last_attn_scores = attn_scores x = self.add([x, attn_output]) x = self.layernorm(x) return x class GlobalSelfAttention(BaseAttention): def call(self, x): attn_output = self.mha( query=x, value=x, key=x) # tf.print('attn_output: ',attn_output.shape) x = self.add([x, attn_output]) # tf.print('concat: ',x.shape) x = self.layernorm(x) # tf.print('layernorm: ',x.shape) return x class CausalSelfAttention(BaseAttention): def call(self, x): attn_output = self.mha( query=x, value=x, key=x, use_causal_mask = True) x = self.add([x, attn_output]) x = self.layernorm(x) return x class FeedForword(tf.keras.layers.Layer): def __init__(self, d_model, dff, dropout_rate = 0.1): super().__init__() self.seq = tf.keras.Sequential([ Dense(dff, activation = 'relu'), Dense(d_model), Dropout(dropout_rate) ]) self.add = tf.keras.layers.Add() self.layernorm = tf.keras.layers.LayerNormalization() def call(self, x): x = self.add([x, self.seq(x)]) return self.layernorm(x) class EncoderLayer(tf.keras.layers.Layer): def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1): super().__init__() self.self_attention = GlobalSelfAttention( key_dim=d_model, num_heads=num_heads, dropout=dropout_rate ) self.ffn = FeedForword(d_model=d_model, dff=dff,dropout_rate=dropout_rate) def call(self, x): x = self.self_attention(x) x = self.ffn(x) return x class Encoder(tf.keras.layers.Layer): def __init__(self, num_layers, d_model, num_heads, dff, patch_size, num_patches, dropout_rate=0.1): super().__init__() self.d_model = d_model self.num_layers = num_layers self.num_heads = num_heads self.dff = dff self.patch_size = patch_size self.num_patches = num_patches self.dropout_rate = dropout_rate self.patches = Patches(patch_size) # Encode patches. self.encoded_patches = PatchEncoder(num_patches, d_model) self.enc_layers = [ EncoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, dropout_rate=dropout_rate) for _ in range(num_layers)] self.dropout = tf.keras.layers.Dropout(dropout_rate) def call(self, x): # `x` is token-IDs shape: (batch, seq_len) x = self.patches(x) # Shape `(batch_size, seq_len, d_model)`. x = self.encoded_patches(x) # Add dropout. x = self.dropout(x) for i in range(self.num_layers): x = self.enc_layers[i](x) return x # Shape `(batch_size, seq_len, d_model)`. class DecoderLayer(tf.keras.layers.Layer): def __init__(self, d_model, num_heads, dff, dropout_rate=0.1): super(DecoderLayer, self).__init__() self.causal_attention = CausalSelfAttention( num_heads=num_heads, key_dim = d_model, dropout= dropout_rate ) self.cross_attention = CrossAttention( num_heads=num_heads, key_dim = d_model, dropout= dropout_rate ) self.ffn = FeedForword(d_model=d_model, dff=dff,dropout_rate=dropout_rate) self.last_attn_scores = self.cross_attention.last_attn_scores def call(self, x, context): x = self.causal_attention(x) x = self.cross_attention(x=x, context = context) x = self.ffn(x) return x class Decoder(tf.keras.layers.Layer): def __init__(self, num_layers, d_model, num_heads, dff, vocab_size, dropout_rate=0.1): super().__init__() self.num_layers = num_layers self.d_model = d_model self.num_heads = num_heads self.dff = dff self.vocab_size = vocab_size self.dropout_rate = dropout_rate=0.1 self.positional_embedding = PositionalEmbedding(vocab_size=vocab_size, d_model=d_model) self.dec_layers = [ DecoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, dropout_rate=dropout_rate) for _ in range(num_layers)] self.dropout = tf.keras.layers.Dropout(dropout_rate) self.last_attn_scores = None def call(self, x, context): # tf.print('x: ', x.shape) # tf.print('context: ', context.shape) x = self.positional_embedding(x) # tf.print('pos-emb x: ', x.shape) for i in range(self.num_layers): x = self.dec_layers[i](x=x, context=context) self.last_attn_scores = self.dec_layers[-1].last_attn_scores # tf.print('afte tra x : ', x.shape) return x class CaptionGenerator(tf.keras.Model): def __init__(self, num_layers, d_model, num_heads, dff, vocab_size, patch_size, num_patches, dropout_rate=0.1): super().__init__() self.encoder = Encoder( num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff, patch_size=patch_size, num_patches=num_patches, dropout_rate=dropout_rate, ) self.decoder = Decoder( num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff, vocab_size=vocab_size, dropout_rate=dropout_rate, ) self.final_layer = tf.keras.layers.Dense(vocab_size ) self.decoder = Decoder( num_heads=num_heads, num_layers=num_layers, d_model=d_model, dff=dff, vocab_size=vocab_size, dropout_rate=dropout_rate, ) self.final_layer = tf.keras.layers.Dense(vocab_size) def call(self, inputs): # sourcery skip: inline-immediately-returned-variable, use-contextlib-suppress img, txt = inputs img = self.encoder(img) # (batch_size, context_len, d_model) x = self.decoder(x=txt, context=img) # (batch_size, target_len, d_model) # Final linear layer output. logits = self.final_layer(x) # (batch_size, max_len, target_vocab_size) try: # Drop the keras mask, so it doesn't scale the losses/metrics. # b/250038731 del logits._keras_mask except AttributeError: pass # Return the final output and the attention weights. return logits here are accuracy and loss def masked_loss(y_true, y_pred): loss_fn = tf.keras.losses.SparseCategoricalCrossentropy( reduction='none') loss = loss_fn(y_true, y_pred) mask = tf.cast(y_true != 0, loss.dtype) loss *= mask return tf.reduce_sum(loss)/tf.reduce_sum(mask) def masked_accuracy(y_true, y_pred): y_pred = tf.argmax(y_pred, axis=-1) y_pred = tf.cast(y_pred, y_true.dtype) match = y_true == y_pred mask = y_true != 0 match = match & mask match = tf.cast(match, dtype=tf.float32) mask = tf.cast(mask, dtype=tf.float32) return tf.reduce_sum(match)/tf.reduce_sum(mask) ```
