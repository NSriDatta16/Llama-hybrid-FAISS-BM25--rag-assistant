[site]: datascience
[post_id]: 55304
[parent_id]: 32128
[tags]: 
Note that GANs are generally considered either as a way to perform implicit density estimation or defined by the game-theoretic minimax problem they usually imply. The situation you're describing doesn't really seem to fit in either of those, especially the latter, because your generator $G$ and $D$ are not really working against each other; rather, they are working together to maximize the function you are optimizing. In other words, the situation you're describing seems to simply be doing generation that maximizes some objective (in a somewhat complicated manner it appears), rather than realism. Since the objective is readily directly computable (as far as I can tell), we can just do standard optimization maximizing it. The reason why the alternating GAN optimization is needed is because the specific objective of realism is not directly computable nor optimizable without first training the critic . Nevertheless, combining GANs and regression is quite sensible, so maybe I'm misunderstanding your question and notation. Let $X$ be the data space, and let $f:X\rightarrow\mathbb{R}$ be the true value of the function you are trying to maximize (after regressing). Let $R:X\rightarrow\mathbb{R}$ denote a learned regression function approximating $f$ . Denote $G:\mathcal{U}\rightarrow X$ as the generator and $D:X\rightarrow[0,1]$ as the critic (discriminator), aiming for realism. Let me consider two possibly relevant scenarios, assuming a dataset $(x_i,f_i)$ : Simply directly optimize a GAN with an additional loss term maximizing $f$ : $$ \mathcal{L}(G) = (1-\eta)\mathbb{E}_{x\sim G}[\log D(x)] + \eta \mathbb{E}_{x\sim G}[R(x)] $$ for the generator. We can optimize $D$ and $R$ in the standard way together. In other words, we are telling $G$ to generate $x\in X$ that balance realism and high $f$ , with the balance controlled by $\eta\in[0,1]$ . When $\eta=0$ , it is a regular GAN (not caring about $f$ ); when $\eta=1$ , the critic is not used at all and there is no regard for realism - $G$ will learn to generate random samples that maximize $f$ without caring about how realistic they are (i.e., without matching the true distribution $p(x)$ ). Separately, train the regressor $R$ and the GAN generator $G$ . Then we can generate a high $f$ data point by gradient descent in the GAN latent space, i.e. doing: $$ u_t \leftarrow u_{t-1} +\xi \nabla_u R(G(u_{t-1})) $$ so that our output $x=G(u_T)$ has high $f(x)$ . Since $u$ is in the GAN latent space, we expect $x$ to be realistic as well. In both of these simple scenarios, we are using the GAN to maintain realism, while maximizing $f$ . Notice that if we don't care about realism then the GAN is not needed at all. Simply generating samples does not make a model a GAN: there are many generative models, such as Deep Boltzmann machines, VAEs, etc... that can do this. What sets GANs apart is adversarial learning , where two networks work against each other, and it's not clear that's happening in the case you described. I think this question is somewhat relevant to yours as well.
