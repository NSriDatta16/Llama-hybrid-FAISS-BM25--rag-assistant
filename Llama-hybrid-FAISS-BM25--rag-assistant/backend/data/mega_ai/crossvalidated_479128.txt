[site]: crossvalidated
[post_id]: 479128
[parent_id]: 479116
[tags]: 
The bias-variance trade-off decomposes the expected test MSE into contributions from bias (squared), variance of the estimates, and irreducible error. Note how that is defined: ... the expected test MSE ... refers to the average test MSE that we would obtain if we repeatedly estimated [the unknown function] $f$ using a large number of training sets , and tested each at [a particular predictor value] $x_0$ . The overall expected test MSE can be computed by averaging ... over all possible values of $x_0$ in the test set. (Emphasis added.) So it shouldn't be too surprising that in any single test/train split your results don't agree with your preconceptions about how LASSO and OLS should perform. See what happens when you do this on many different initial train/test splits and average over the splits, in the spirit of the definition of the "expected test MSE." These vagaries of splitting small data sets into separate training and test sets are expected. Thus many recommend not to do such single splits unless you have thousands of data points ; use re-sampling with cross-validation or bootstrapping within small data sets. Also, consider how well the unpenalized model works on the whole data set; if it's good enough LASSO might not provide any improvement in the first place.
