[site]: crossvalidated
[post_id]: 393219
[parent_id]: 393208
[tags]: 
Three general remarks: t-SNE is excellent at preserving cluster structure but is not very good at preserving continuous "manifold structure". One famous toy example is the Swiss roll data set, and it is well-known that t-SNE has trouble "unrolling" it. In fact, one can use t-SNE to unroll it, but one has to be really careful with choosing optimisation parameters: https://jlmelville.github.io/smallvis/swisssne.html . Using 1-dimensional t-SNE instead of 2-dimensional is likely to exacerbate this problem, possibly by quite a lot. One-dimensional optimisation is more difficult for t-SNE because points don't have the two-dimensional wiggle space and have to pass right through each other during gradient descent. Given that all pairs of points feel repulsive forces in t-SNE, this can be difficult, and it can get stuck in a bad local minimum. t-SNE is not very good with tiny data sets. It's often easier to get a nice embedding of two million points than of twenty points. Default optimisation parameters might be inappropriate for such a tiny sample size. And by the way, perplexity larger than the sample size does not make mathematical sense (not sure what your R package is doing when you set perplexity larger than $N$ ). With all these caveats in mind, if you are really careful with optimisation parameters, you can manage to preserve the manifold structure of your data set. But this is really not what t-SNE is for. %matplotlib notebook import numpy as np import pylab as plt import seaborn as sns; sns.set() from sklearn.manifold import TSNE x = np.arange(-5, 5.001, .5)[:,None] y = x**2 X = np.concatenate((x,y),axis=1) Z = TSNE(n_components=1, method='exact', perplexity=2, early_exaggeration=2, learning_rate=1, random_state=42).fit_transform(X) plt.figure(figsize=(8,2)) plt.scatter(Z, Z*0, s=400) for i in range(Z.shape[0]): plt.text(Z[i], Z[i]*0, str(i), va='center', ha='center', color='w') plt.tight_layout() It was easy to make it work with n_components=2 , but as I suspected, n_components=1 required some tinkering with optimisation parameters ( early_exaggeration and learning_rate ).
