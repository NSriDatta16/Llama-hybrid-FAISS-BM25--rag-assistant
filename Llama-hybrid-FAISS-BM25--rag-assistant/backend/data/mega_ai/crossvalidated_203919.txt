[site]: crossvalidated
[post_id]: 203919
[parent_id]: 
[tags]: 
SVM: Does C increase variance or stability (bias)?

I was learning about SVM using 2 sources: Andrew Ng Machine Learning course from Coursera and Stanford 'Statistical Learning' (from Trevor Hastie and Robert Tibshirani). And I encountered the following contradiction: Andrew Ng provides this formula for SVM cost function: $$C\sum_{i=1}^m[y^{(i)}cost_1(\theta^Tx^{(i)})+(1 - y^{(i)})cost_0(\theta^Tx^{(i)})] + \frac{1}{2}\sum_{i=1}^n\theta_j^2$$ (He doesn't dive into details about $cost_1$ and $cost_0$, but says they are more or less like log-likelihood loss in logistic regression, but linear.) Then he says that increasing $C$ leads to increased variance - and it is completely okay with my intuition from the aforementioned formula - for higher $C$ algorithm cares less about regularization, so it fits training data better. That implies higher bias, lower variance, worse stability. But then Trevor Hastie and Robert Tibshirani say, quote: And that means that $C$ gets bigger the more stable the margin becomes. Also they show the following picture - the larger margins correspond to larger $C$, they say. Stability implies higher bias, right? So they say the opposite to Andrew Ng. So the question is who is right?
