[site]: datascience
[post_id]: 118035
[parent_id]: 
[tags]: 
Preprocessing advice for large text corpus in natural language generation (NLG)

I have a large text corpus (i.e. 30 million sentences, all in lowercase in the format of Penn Treebank) that I want to use to train a neural network for natural language generation. What preprocessing steps would you recommend here? The sentences originate from formal text (i.e. books). I plan to use named entity recognition in order to replace named entities such as people, locations, and organisations during training and generation, and adding them back in for the final output. Any other suggestions?
