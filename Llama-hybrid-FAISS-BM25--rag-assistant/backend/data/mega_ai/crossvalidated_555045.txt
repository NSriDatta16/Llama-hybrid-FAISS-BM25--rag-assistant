[site]: crossvalidated
[post_id]: 555045
[parent_id]: 554944
[tags]: 
Generally you could start by testing if your data follows the linearity assumption. You can do that by plotting the true y vs your predictions (or by plotting the residual vs your prediction). Another thing you could do, is checking for outliers. You could just try to plot the features, but this might be too time-consuming if you have many features. You could plot true y vs prediction and see which observations are far away from the regression line (of course here you would make a prediction for your whole data set, not just for the testing set like in machine learning). Depending on the software you use, there is probably also a way to extract which observations have the strongest impact on your fitted line for each feature. To spot multicollinearity you can calculate variance inflation factor (VIF) between features. If you are using R or python there should be easy implementations to find the VIF between your features. If the VIF is higher than 10, you might have a problem with your coefficients. But remember, multicollinearity is not impacting your prediction, it rather introduces some bias in your coefficients, because it becomes very hard to isolate the effect of the single features. With features you normally build interaction features or polynomials (if you assume some non-linear relationship) to generate more possible options for the model to learn some connections between your features and your target. To summarize: Test if conditions for linear regression are fulfilled, plot as much as possible to get an understanding of your data. And once you understand the data try to build more features from the existing ones. On top of that you might try to remove outliers.
