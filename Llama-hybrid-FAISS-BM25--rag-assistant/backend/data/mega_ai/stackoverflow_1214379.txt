[site]: stackoverflow
[post_id]: 1214379
[parent_id]: 1214286
[tags]: 
I agree with John that SSIS has a lot of built in intelligence for such scenarios and is probably the best bet to invest your time into. For the record such problems you approach by partitioning your data. I'm not talking about the physical storage partitioning (ie. add table partitioning), but logical, processing partitioning. You partition your 2 mil. records in N partitions, based on whatever criteria you have that can be exploited at the data access level , eg. an indexed column, then allocate N processors that start churning each on its own partition. The idea is to not have the processors overlap in trying to access the same rows. 'Processors' can be threads, or better still ThreadPool queued up worker items that use async database access methods. The big problem is that many times you don't have a suitable partitioning key. In such cases you can do an ad-hoc partitioning like this: with cte as ( select top (@batchSize) * from myTable with (rowlock, updlock, readpast) where ) update cte set output inserted.* The trick is the locking hints used in the select: by forcing and updlock the records are locked for processing by the current processor. By adding the readpast hint each processor will skip records that are already locked by other processors. This way each processor gets its own @batchSize batch of records to process, whatever the processing is. Important to understand that all these comments apply to a processing that involves something outside the database, like doing a web service call, printing a paper slip or anything similar. If the processing is all in the database, then you should just express it as a single T-SQL update and let the query optimizer use parallel queries it as it sees fit.
