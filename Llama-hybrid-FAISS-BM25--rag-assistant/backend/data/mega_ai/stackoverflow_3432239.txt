[site]: stackoverflow
[post_id]: 3432239
[parent_id]: 3431505
[tags]: 
You can use the same approach used in Support Vector Machines . SVMs are originally binary classifiers, several approaches were proposed for handling multiclass data: one-against-all : construct one binary classifier per class, and train with instances in this class as positive cases and all other instances as negative cases (ie: 1-vs-not1, 2-vs-not2, 3-vs-not3). Finally use the posterior probability of each classifier to predict the class. one-against-one : construct several binary classifiers for each pair of classes (ie: 1-vs-2, 1-vs-3, 2-vs-3, ..) by simply training over the instances from both classes. Then you can combine the individual results using a majority vote. Error Correcting Output Codes : based on the theory of error correction (Hamming code and such), it relies on coding the output of several binary classifier using some redundancy to increase accuracy. Note these are generic method and can applied to any binary classifier. Otherwise you can search for a specific implementation of multiclass Adaboost, which I'm sure there are plenty out there.. A quick search revealed this one: Multiclass GentleAdaboosting
