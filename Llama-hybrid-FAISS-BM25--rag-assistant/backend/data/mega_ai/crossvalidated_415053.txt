[site]: crossvalidated
[post_id]: 415053
[parent_id]: 
[tags]: 
How does stochastic gradient descent even work for neural nets?

How does stochastic gradient descent (meaning where you backpropagate and adjust the weights and biases of the neural network after each single sample ) even work? Doesn't that just tell the neural network to learn that particular sample ? Which isn't really what we want? So instead of converging towards a solution that appropiately learns the entire training data, the neural net will oscillate between different solutions that are more optimal for the individual samples? To give you a simple example: say I want my neural network to output $x = 1$ if the input is $1$ and I want it to output $x = 0$ if input is $0$ . Let's say I train it on the input $0$ , then $1$ , then $0$ again, and so on. Each time my input is 0, every weight will get adjusted so as to output 0. But then in the next iteration, every weight will get adjusted so as to output 1, hence counter-acting the previous iteration.
