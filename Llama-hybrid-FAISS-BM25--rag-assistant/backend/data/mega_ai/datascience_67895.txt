[site]: datascience
[post_id]: 67895
[parent_id]: 67892
[tags]: 
First, to directly answer your question, the easiest way to get Feature Importance using scikit learn is this, where model is the variable holding your classifier. print(model.feature_importances_) However, this method only exists on some of the Ensemble models , namely: AdaBoostClassifier AdaBoostRegressor ExtraTreesClassifier ExtraTreesRegressor GradientBoostingClassifer GradientBoostingRegressor RandomForestClassifier RandomForestRegressor RandomTreesEmbedding If you're wondering why, there's a fantastic free book online all about Interpretable Machine Learning . Here's an excerpt: The easiest way to achieve interpretability is to use only a subset of algorithms that create interpretable models. Linear regression, logistic regression and the decision tree are commonly used interpretable models. Once you've chosen the right model, beware of using feature importance! See here, it ranks random data very highly. The article proposes using Permutation Importance instead, as well as Drop-Column Importance. They created a library called rfpimp for doing this, but here's a tutorial from scikit themselves on how to do both of those with just scikit-learn. I've pasted the permutation importance example from that tutorial below: from sklearn.inspection import permutation_importance result = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2) sorted_idx = result.importances_mean.argsort() fig, ax = plt.subplots() ax.boxplot(result.importances[sorted_idx].T, vert=False, labels=X_test.columns[sorted_idx]) ax.set_title("Permutation Importances (test set)") fig.tight_layout() plt.show()
