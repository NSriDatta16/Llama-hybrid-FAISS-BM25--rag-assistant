[site]: crossvalidated
[post_id]: 139154
[parent_id]: 
[tags]: 
Why would a regression model predict super huge numbers?

I have a set of 55 items. Each item is defined by 6 values. I am doing 55-fold cross validation: training a model on 54 items, predicting on the 55th. The 6 values of the 54 items are used in some feature engineering to produce feature sets (we have a couple we are testing). I then predict 1 value at a time, so I have 6 54-length "ground truth" vectors for each fold. Thus, it's 6 models for each fold. 4 of the 6 parameters do not make sense as being negative (they are parameters of a gamma distribution). So, to account for this, I have taken the natural log of the ground truth in the training set, then exponentiate the predicted value by base $e$. I have tried a variety of regression algorithms with a grid search over parameters: SVR, Ridge, and Random Forest Regressors are the current top contenders. However, in all of the models, there is usually a couple parameters that get predicted way off base. As in orders of magnitude off base. I think it's because of the log space but I don't think I can not use log space because of the negative numbers. I'm not sure if I'm thinking about this problem correctly, or if there is something trivial I am not seeing.
