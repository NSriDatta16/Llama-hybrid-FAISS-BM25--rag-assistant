[site]: datascience
[post_id]: 104883
[parent_id]: 104882
[tags]: 
These is what I interpreted from your question: Here is my answer: ROC-AUC Score is basically AUC (Area under the ROC Curve). . IF you need more clarification: ROC : will tell how well the classification model predicts given its threshold changes. AUC : will tell you how the aggregrate measures across all the threshold. AUC will be 0 if all the predictions are wrong and 1 if all the predictions are right. Properties of AUC: AUC will tell you how well the predictions are ranked rather than its absolute value irrespective of scale. It will tell you quality of predictions irrespective of classification threshold (threshold invariant). Use AUC for comparing the two classification models (e.g if logistic regression has a AUC = 0.8 & Random Forest with a AUC=0.9 go for Random Forest) . AUC has its limit: Scale-invariant is not always desirable. Specially when we want to calibrate our outputs. threshold invariant not desirable when we want to prioritize minimizing any of the rate (e.g prioritizing minimizing false positives in SPAM filter model)
