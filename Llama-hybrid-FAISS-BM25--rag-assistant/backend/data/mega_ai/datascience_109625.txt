[site]: datascience
[post_id]: 109625
[parent_id]: 
[tags]: 
NLP text representation techniques that preserve word order in sentence?

I see people are talking mostly about bag-of-words, td-idf and word embeddings. But these are at word levels. BoW and tf-idf fail to represent word orders, and word embeddings are not meant to represent any order at all. What's the best practice/most popular way of representing word order for texts of varying lengths? Simply concatenating word embeddings of individual words into long vector appearly not working for texts of varying lengths... Or there exists no method of doing that except relying on network architectures like the positional encoding in transformer family? By the way, ngram is not a solution to me, as it still fails to solve the problem in representing texts in varying lengths. (Or can it and how? It seems to me ngram is more for next word prediction rather than representing texts with varying lengths.) TIA :)
