[site]: crossvalidated
[post_id]: 381797
[parent_id]: 380226
[tags]: 
No, your performance estimator is likely to be optimistically biased because the test data in each fold of the cross-validation procedure was used to tune the hyper-parameters, and so to some extent the hyper-parameter setting can exploit the noise in the test samples. See my paper: Gavin C. Cawley, Nicola L. C. Talbot, "On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation", journal of Machine Learning Research, 11(Jul):2079âˆ’2107, 2010. ( http://www.jmlr.org/papers/v11/cawley10a.html ) The procedure you have adopted (very common in machine learning) is discussed in section 5.3 "Another Example of Biased Evaluation Methodology". To get an unbiased performance estimate from cross-validation, you need to cross-validate all steps involved in fitting the model, including hyper-parameter tuning and perform them independently in each fold of the cross-validation (this often requires nested cross-validation). The final model is created by performing these steps using the full dataset. The key is to think of cross-validation as a method of estimating the performance of a procedure for fitting a model, rather than of the model itself. Note that the marginal likelihood (or evidence for the model) is a statistic evaluated over a finite set of samples, so it will also have a finite variance, which means that optimising hyper-parameters by marginal likelihood maximisation can still be just as susceptible to over-fitting the model selection criteria as optimising cross-validation based performance estimates. Note in my paper I use GP classification as one of the examples. Marginal likelihood maximisation is not really a very Bayesian approach, marginalisation over the hyper-parameters would be more elegant. My maxim is "optimisation is the root of all evil in statistics", but unfortunately marginalisation is generally difficult or computationally expensive, or both!
