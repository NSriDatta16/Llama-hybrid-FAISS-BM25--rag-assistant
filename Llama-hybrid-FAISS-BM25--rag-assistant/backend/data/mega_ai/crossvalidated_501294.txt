[site]: crossvalidated
[post_id]: 501294
[parent_id]: 501288
[tags]: 
The EM algorithm applies most straightforwardly to this simpler case, with the $Q(\cdot;\cdot)$ function defined by $$Q(\mathbf p,\mathbf p^{(t)})=\sum_{i=1}^N\sum_{j=1}^n \mathbb P_{\mathbf p^{(t)}}(Z_i=j|x_i)\log p_j(x_i)$$ As does a Bayesian approach putting a prior on $(p_1,\ldots,p_n)$ , since the Gibbs sampler step are quite similar to the EM steps.
