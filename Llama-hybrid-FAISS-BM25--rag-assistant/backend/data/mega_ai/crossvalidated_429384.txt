[site]: crossvalidated
[post_id]: 429384
[parent_id]: 
[tags]: 
Are there some measures of information content of Bayesian evidence?

Let us have a binary random variable $X$ with values $a,b$ and a prior distribution $P(a), P(b)$ . Now, let us suppose that we learn a piece of evidence $E$ . $E$ is a random variable with two values, $A, B$ , where $$ P(E = A\mid a) = \alpha, P(E = B\mid a) = 1 - \alpha \\ P(E = A\mid b) = \beta, P(E = B\mid b) = 1 - \beta $$ If we want to determine $X$ , we should seek out the most informative evidence possible. It is clear, that any evidence where $\alpha = \beta$ is equally useless (contains no information). On the other hand, $\alpha = 1, \beta = 0$ is maximally useful (contains the maximum amount of information). I was looking for ways to formalize this "information content" or "discriminative value" of evidence in some nice way that would generalize intuitively to variables with $n$ possible values. Since entropy is the measure of information content of a random variable, I tried to see what is the entropy of $E$ : $$ -(P(a)\alpha + P(b)\beta) \log (P(a)\alpha + P(b)\beta) -\\ (P(a)(1-\alpha) + P(b)(1-\beta)) \log (P(a)(1-\alpha) + P(b)(1-\beta)) $$ Unfortunately this does not express the measure of information in a way that I would expect. If we plug the useless zero information case where $\alpha = \beta$ into the expression we get: $$ -\alpha \log (\alpha) - (1-\alpha) \log (1-\alpha) $$ which is not constant as we would expect. Furthermore, $\alpha = 1, \beta = 0$ reduces to the entropy of $X$ , which is also not useful. What are some other measures of information that measure information in the sense of discriminative value?
