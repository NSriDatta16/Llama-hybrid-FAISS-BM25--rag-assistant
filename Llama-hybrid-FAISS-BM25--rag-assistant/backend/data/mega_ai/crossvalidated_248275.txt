[site]: crossvalidated
[post_id]: 248275
[parent_id]: 
[tags]: 
Classifying sequential data

I am trying to understand what are the best practices to perform supervised machine learning on sequential numerical data. My data looks like following: # sample data for class 'a' +----+-----+----+------+-----+---+ | id | inv | s1 | s2 | s3 | y | +----+-----+----+------+-----+---+ | 1 | 0.0 | 36 | 0.02 | 0.0 | a | | 1 | 0.1 | 92 | 0.01 | 8.8 | a | | 1 | 0.3 | 8 | 0.11 | 8.4 | a | | 1 | 0.4 | 7 | 0.13 | 8.3 | a | | 1 | 0.5 | 12 | 0.07 | 8.5 | a | | 1 | 0.6 | 15 | 0.06 | 8.5 | a | | 1 | 0.7 | 11 | 0.08 | 8.4 | a | +----+-----+----+------+-----+---+ # sample data for class 'b' +----+-----+----+------+-----+---+ | id | inv | s1 | s2 | s3 | y | +----+-----+----+------+-----+---+ | 2 | 0.0 | 16 | 0.10 | 9.0 | b | | 2 | 0.1 | 12 | 0.20 | 9.0 | b | | 2 | 0.3 | 24 | 0.11 | 6.4 | b | | 2 | 0.4 | 28 | 0.16 | 1.3 | b | | 2 | 0.5 | 10 | 0.12 | 1.5 | b | | 2 | 0.6 | 11 | 0.11 | 1.5 | b | | 2 | 0.7 | 12 | 0.01 | 4.4 | b | +----+-----+----+------+-----+---+ So far for my baseline, I have taken some aggregations and did normalization for each id, making structure to look something like: # sample data for class 'a' +----+---------+---------+---------+---+ | id | avg(s1) | avg(s2) | avg(s3) | y | +----+---------+---------+---------+---+ | 1 | 50 | 0.05 | 8.2 | a | | 2 | 20 | 0.15 | 4.0 | b | +----+---------+---------+---------+---+ There are also min, max, variance, avg_lag and other aggregation columns. While it works relatively ok, I am noticing that each class has a set of different patterns that are not being captured using those features. I am currently in the process of trying to use KNN with DTW as a distance measure, but ideally I would like to avoid using instance-based learning method. Some other things: - there are three y classes - id denotes unique number of this sequence - inv denotes sequence of events (in this case, it's distance in cm from beginning if that helps) - s1 , s2 and s3 are three different sensors. There are ~6-10 of them, some values are correlated. - each id has 80-120 rows. - There is relatively large amount of training samples available. What are some other approaches that I can try?
