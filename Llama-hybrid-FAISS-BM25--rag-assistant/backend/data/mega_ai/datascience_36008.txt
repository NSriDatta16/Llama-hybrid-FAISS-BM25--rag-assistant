[site]: datascience
[post_id]: 36008
[parent_id]: 36006
[tags]: 
I would really try not to use ordinal numbers for categorical data. It imposes a false magnitude and ordering in the model, especially when you have 1,000 examples. For example, the difference between Brush Script and Calibri could be very small and the difference between Calibri and Times New Roman UNBELIEVABLY HUGE (assuming lexicographical assignment), when really they're all just different fonts. You could: Try to figure out groupings of similar features that make sense, then one-hot those groupings so you wouldn't end up with too many columns. One-hot the whole thing and then try some dimensionality reduction techniques to get the feature space back down to something sensible. Try to use an autoencoder or neural method to learn an embedding of fixed dimension. One thing you should definitely be careful of is how you combine the result of this process with whatever the other half of your features are.
