[site]: datascience
[post_id]: 124578
[parent_id]: 124577
[tags]: 
First, note that the purpose of next sentence prediction objective is not to contribute to the contextual embeddings part, but to allow other downstream tasks like sentence classification and textual entailment. The contextual embedding signal comes from the part of the loss that predicts the masked token. The model receives information to distinguish the two sentences. As you can see from your quoted paragraph (i.e. The first sentence receives the A embedding and the second receives the B embedding. ) and from figure 2 from the BERT article itself: Therefore, the model knows where the two sentences are (with the different $E_A$ , $E_B$ embeddings and the $E_{[SEP]}$ token. Then, it is up to the model to learn to only use each sequence to predict the masked tokens to avoid using information from a segment that is not related, or even to use information from both segments but only if the model thinks they belong together. Of course, this is not a guarantee that the model does not mix information from an unrelated sentence in the prediction. Actually, in the derived model RoBERTa , created by different authors, the next sentence prediction objective was completely removed.
