[site]: crossvalidated
[post_id]: 626146
[parent_id]: 
[tags]: 
Develop a model for theoretical best performances at different ultra running distances/times

Goal Develop a model for theoretical best performance for running distances from marathon to around 1000 km. Partly to compare the strength of ultrarunning world records, but more importantly, to get base values for prior estimates in a Bayesian rating model. Method Calculate the velocity of the theoretical best performance for fixed-distance (50 km, 100 km and 100 miles) and fixed-time (6 hours, 12 hours, 24 hours and 6 days) ultrarunning standard events and find a regression line between these (monotone spline?). The idea originates from Comparing and Forecasting Performances in Different Events of Athletics Using a Probabilistic Model . Assumption 1 Data suggests that there is an almost linear relationship between distance and velocity from around 10 km to the marathon, for elite runners. But beyond that there seems to be break points, in time, at least around 2 hours and somewhere beyond 24 hours, where velocity “breaks” from its previous curve. The first break point is likely partly due to glycogen depletion and the second partly due to sleep deprivation (and general fatigue). That is, it seems like the model needs to look at the relationship between velocity and time, rather than velocity and distance. Assumption 2 The distribution for each event seems to be close to a normal and/or log-normal distribution. For fixed-time events a normal distribution looks possible, whereas it's for fixed-distance events seems to be closer to a log-normal distribution (which may be a result of assumption 1, that runners in the same event and race are affected differently by the time factor, as time to finish the fixed distance is different). This is based on available data, that includes performances of everyone from top elite runners to recreational runners (unlike other more elite-oriented data). This is also based on research about the distribution of marathon performances, such as Modelling dynamics of marathons – A mixture model approach and Empirical analysis on the runners’ velocity distribution in city marathons . Image taken from Modelling dynamics of marathons – A mixture model approach Data Data for all known recorded performances at the mentioned events is available at the ultrarunning statistics database maintained by DUV: https://statistik.d-u-v.org/ However, distributions of performances at different events are affected by race cutoff times. Here are examples of velocity for the 24-hours and 100 km events. With a 6th degree polynomial linear regression as a rough example of the distribution. I have truncated 100 km at 13h, as that is the most common race cutoff time and slower times are therefore not as relevant in this case (as their frequencies are decreased by race cutoff times). 24-hour performances 2012-2018, truncated at 60 km (0.7 m/s) 100 km performances 2012-2018, truncated at 13h (2.1 m/s) Challenges My first challenge is when trying to find the $\mu$ and $\sigma^2$ of the unknown population distribution, given that the sample data is both censored by different race cutoff times (like 11h, 12h, 13h etc. in a 100 km race) and truncated in the case we select the tail unaffected by race cutoff times. Further, there are spikes at round numbers, like even hours (there’s always some strength left to push yourself under a certain mark if you’re close enough…). After having a good estimate of $\mu$ and $\sigma^2$ for each event, it is possible to continue with the rest of the calculations to get a good estimate of theoretical world best performance. The second challenge is to connect these world best performances in a generalized model, with some kind of monotone spline. After that it will be possible to estimate the theoretical world best performance for any arbitrary distance or time. Any tips on how to solve challenge #1?
