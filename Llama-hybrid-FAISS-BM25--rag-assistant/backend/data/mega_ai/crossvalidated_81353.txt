[site]: crossvalidated
[post_id]: 81353
[parent_id]: 80849
[tags]: 
Sample : while in statistics this refers to a set of cases , in many other disciplines a sample is one physical specimen . Of course, sample size is also ambiguous, refering either to the number of cases in the statistical sample or the physical size (mass, volume, ...) of the specimen. Sensitivity : for medical diagnostics the fraction of diseased cases that is recognized by the test. In analytical chemistry: the slope of the calibration curve (see below). Specificity : in medical diagnostics the fraction of non-disease cases this correctly recognized by the test. In analytical chemistry, a method is specific if there are no cross-sensitivities. Calibration : actually, two meanings are listed already for statistics in the Wiki article. In chemistry and physics, the reverse regression meaning is the usual one. Confusion arises, though: In chemometrics, (forward) calibration models the measured signal $I$ dependent on the concentration $c$: $I = f (c)$. Prediction then solves for concentration $c$: $c = f^{-1} (I)$. Inverse calibration models $c = f (I)$. Thus, the forward model agrees with the causality (concentration of analyte causes signal, not the other way round), but the inverse models the direction that is used for the predictions. (In practice, it is often possible to say that the error on $c$ or the error on $I$ is much larger than the other, and the appropriate modeling direction is/should be chosen from that) I've seen plots of predicted probability over true probability called "calibration plots" (stats people). In analytical chemistry, the corresponding calibration plot would be predicted probability over measured signal (usually some other unit). The plot of predicted over true dependent variable would usually be called recovery curve . Validation set : here I'd like to draw the attention to a potentially confusion use of terms which I think already arises within the different statistics-related fields, even though I again contrast . In the context of nested/double validation or optimization vs. validation/testing, one line of terminology splits training - validation - test and uses the "validation" set for optimization of hyperparameters. E.g. in the Elements of Statistical Learning, p. 222 in the 2nd ed. : ... divide the dataset into three parts: a training set, a validation set, and a test set. The training set is used to fit the models; the validation set is used to estimate prediction error for model selection; the test set is used for assessment of the generalization error of the final chosen model. In contrast, e.g. in analytical chemistry validation is the procedure that demonstrates that the model (actually, the assessment of the final model is only part of the validation of an analytical method) works well for the application, and measures its performance, see e.g. John K. Taylor: Validation of analytical methods, Analytical Chemistry 1983 55 (6), 600A-608A or guidelines by institutions like the FDA. This would be "testing" in the other line of terminology, where the "validation" is actually used for optimization. The crucial difference is, that the "optimization-validation" results are to be used to change (select) the model, whereas changes in a validated analytical method (including the data analytic model) mean that you have to revalidate (i.e. prove that the method still works as it is supposed to work). If you happen to have to talk to chemists, a good reference of the analytical chemistry terminology is Danzer: Analytical Chemistry - Theoretical and Metrological Fundamentals, DOI 10.1007/b103950
