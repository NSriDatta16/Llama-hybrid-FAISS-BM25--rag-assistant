[site]: crossvalidated
[post_id]: 439371
[parent_id]: 439327
[tags]: 
Rect is differentiable in the same way that convolution is differentiable. Even though it assigns 0 weight to all inputs outside of the rectangular window -- in the same way that a convolutional filter assigns 0 weight to all inputs outside of it's receptive field -- the window is slid over the entire range of inputs, so every input contributes to the output. Hard-attention is differentiable with respect to the inputs to the attention mechanism, but non-differentiable with respect to the parameters inside the attention mechanism . local-m attention does not have this problem, because it does not predict the alignment as a hard-attention mechanism does. Instead local-m attention assumes the input is already perfectly aligned with the output. local-m attention does not predict the position $p_t$ . The authors write... " $p_t=t$ ". It's worth noting that the other attention model in the paper, local-p attention, is a soft mechanism.
