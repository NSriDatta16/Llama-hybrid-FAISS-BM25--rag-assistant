[site]: crossvalidated
[post_id]: 558316
[parent_id]: 
[tags]: 
Does gradient clipping in a RNN help the network learn the long term dependencies?

So this was asked in one of the exams and I think that gradient clipping does help in learning long term dependencies in RNN but the answer provided to us was "Gradient clipping cannot help with vanishing gradients, or improve the flow of information back deep in time." From the explanation, the only thing that I can deduce is that only vanishing gradient pose a problem while learning long term dependencies. Is this correct? does exploding gradient have any role in learning long term dependencies?
