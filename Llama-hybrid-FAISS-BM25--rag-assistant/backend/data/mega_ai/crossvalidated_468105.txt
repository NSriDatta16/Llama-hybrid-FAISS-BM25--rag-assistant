[site]: crossvalidated
[post_id]: 468105
[parent_id]: 467701
[tags]: 
Indipendently on the distribution of $X$ , if $C$ is computed in that deterministic way, estimation won't converge because there is no couple of parameters $\beta$ for which likelihood is maximized. It is easy to notice that $\hat c = -\frac{\hat \beta_0}{\hat \beta_1}$ maximises the likelihood at some middle value between last x value before $c$ and first one after it, but you have to keep $\beta_1$ fixed to observe this, and vary just $\beta_0$ , because of the absence of one ML point for in the whole parametric space. I will make this clear now. Let's say we take that value $\hat c$ fixed at the point we just described, for which likelihood is maximized for any given slope $\beta_1$ , and we now vary $\beta_1$ , to see how likelihood varies. Mind that $\beta_0$ will vary together with $\beta_1$ to keep $\hat c$ constant. We will notice that the higher the slope is, the higher the likelihood, without convergence. This always happens when logistic regression is used in a deterministic setting and no misclassifications happen. I will add the mathematical details when I have time, but you can already verify my claims.
