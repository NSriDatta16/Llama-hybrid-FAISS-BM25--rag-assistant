[site]: crossvalidated
[post_id]: 309096
[parent_id]: 309054
[tags]: 
Linear regression will pick apart the two variables. However, even if they are not identical, if they are correlated - as height and weight are - you may get the problem that the statistical significance (measure by the t- or p-value on each coefficient) are not very good, since the effect gets split across two similar variables. Try running with one or the other, separately, to see if that helps if that is your problem. More to the point, if the $R^2$ (you said $R$ but I suspect you mean $R^2$) is .64 that isn't too bad on its own. I am actually surprised that the explanatory power is that good. There is a big difference between having a statistically significant set of coefficients (i.e., they are close the the 'actual' number) and having a high $R^2$ (explaining 64% of what determines the speed). This is one of the things people get wrong very often in statistics. Suppose you have lots of data, and make a statistically significant finding that mothers who eat cheese during pregnancy are more likely to have difficult deliveries. But, maybe 25% of mothers have difficult deliveries overall. If cheese eating makes one more mother out of every thousand have a difficult delivery, the effect is statistically significant but pretty useless since out of 1,000 births it means not eating cheese drops the number of difficult births, on average, from 250 to 249.
