[site]: crossvalidated
[post_id]: 83708
[parent_id]: 83686
[tags]: 
The performance bounds on which the maximal margin classifier is based are independent of the dimension of the feature space (which is the dimension of the input vector in the case of a linear SVM), but instead depend on the margin. This is the reason that the SVM with an infinite dimensional feature space (e.g. that induced by the radial basis function kernel) can still give good generalisation performance. However, this requires careful tuning of the regularisation parameter. Instead of the complexity of the model being defined in terms of the number of model parameters (which is a traditional view in statistics), the parameter C is used to define a nested set of classes of models of increasing complexity. The task is then to produce a model that explains the data from the model class of the least complexity. This is known as structural risk minimisation. In other words the linear SVM is robust to the curse of dimensionality provided the C parameter is tuned very carefully. As @Marc Claesen suggests (+1), if the model still doesn't give good performance, it is probably a non-linear problem and an RBF kernel is a good option. Sadly structural risk minimisation does not apply to the problem of optimising the kernel parameters, but you can't have everything! Proper cross-validation is generally a good guide (but make sure you tune the hyper-parameters independently in each fold). If you want to estimate the performance improvement from gathering more data, the simplest thing to do is to generate a plot of the cross-validation error as a function of the number of training patterns (i.e. a learning curve).
