[site]: crossvalidated
[post_id]: 246444
[parent_id]: 
[tags]: 
Difference between cross-validation and back propagation

sorry if this is a dumb question. I want to know what the difference is? Mainly what how they're objectives differ from each other? I know that in cross validation you divide the training set into equal parts and use one as a test set. Back propagation means that the output is sent in reverse of sorts to recalculate the weights of the neural network. But wouldn't they achieve the same thing? Is it that cross-validation deals with the actual output against future data, while back propagation is for weight optimization?
