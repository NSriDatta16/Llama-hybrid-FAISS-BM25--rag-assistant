[site]: crossvalidated
[post_id]: 365826
[parent_id]: 
[tags]: 
What metrics to look at when experimenting with neural network hyperparameters?

So with other machine learning techniques I generally only look at the validation error when deciding on certain hyperparameters. I've been reading up on neural networks and it seems that hand tuning is the preferred method to decide on hyperparameters. While I've been messing around with them, some techniques do have some desirable effects, but don't help on validation error. One example for my problem is dropout. When I add dropout, the validation error increases slightly and the training error increases greatly. The result of this seems to be that there's less overfitting, but the validation error has gotten worse slightly. So in a situation like this would you still utilize dropout, which decreases overfitting but at the same time worsens validation performance slightly?
