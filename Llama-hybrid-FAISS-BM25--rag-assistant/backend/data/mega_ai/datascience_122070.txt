[site]: datascience
[post_id]: 122070
[parent_id]: 122069
[tags]: 
I think yes, you can achieve this by creating a custom scorer in scikit-learn. By default, the GridSearchCV scorer maximizes the average performance across validation folds, but you can define your own scorer to consider both the average validation and average training performance. To create a custom scorer that combines both average validation and average training performance, you can define a function that takes the true labels, predicted labels, and model as input, and returns a score based on your desired criteria. from sklearn.metrics import f1_score def custom_scorer(estimator, X, y): # Calculate validation score (F1 score) y_pred = estimator.predict(X) validation_score = f1_score(y, y_pred, average='micro') # Calculate training score (F1 score) train_predictions = estimator.predict(X_train) training_score = f1_score(y_train, train_predictions, average='micro') # Combine validation and training scores (average) combined_score = (validation_score + training_score) / 2 return combined_score In this example, we use the F1 score as the performance metric, but you can replace it with any other metric that suits your needs. The model argument represents the current model being evaluated in the GridSearchCV cross-validation loop. Once you have defined your custom scorer, you can pass it to the scoring parameter of GridSearchCV when initializing it: from sklearn.model_selection import GridSearchCV from sklearn.svm import SVC # Create the grid search object with your custom scorer grid_search = GridSearchCV(estimator=SVC(), param_grid=param_grid, scoring=custom_scorer) grid_search.fit(X, y) By using your custom scorer, the GridSearchCV will optimize the hyperparameters based on the combined score of both average validation and average training performance.
