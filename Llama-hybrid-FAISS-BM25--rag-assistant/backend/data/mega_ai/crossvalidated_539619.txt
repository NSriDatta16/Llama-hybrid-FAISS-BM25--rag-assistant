[site]: crossvalidated
[post_id]: 539619
[parent_id]: 
[tags]: 
True Error in Machine Learning

I have read this question and I am confused by a part of the first answer, even though it is asked in the comments. I don't understand why $$L_{(\mathcal{D}, f)}(h^{*}) = 0 \implies L_{S}(h^{*}) = 0$$ Why if the "True Error" equal to $0$ i.e. $L_{(\mathcal{D}, f)}(h^{*}) = 0$ then it's implied that the "Training Error" is also equal to $0$ i.e. $ L_{S}(h^{*}) = 0$ ? After the answer to my previous question , I started understanding what is $\mathcal{D}$ . Now, I think intuitively I can understand what they mean by this implication relation. So here's my intuitive explanation (please correct me if I'm wrong): Since the classifier $h^*$ has zero error when applied to the "population" $\mathcal{X}$ (where $\mathcal{X}$ is the set of input datapoints, and where a "training" $\mathcal{X}$ is the training datapoints), this means that $h^*$ has information about all samples of $\mathcal{X}$ from the population and it has correctly classified them with $0$ error, hence when applied to one of these samples (i.e., in our case the training sample), it will clearly yield $0$ error. I hope my intuitive explanation is understandable. Is my understanding correct and how would I prove this more rigorously? (If it's possible to do so?)
