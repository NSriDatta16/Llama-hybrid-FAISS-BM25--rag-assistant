[site]: crossvalidated
[post_id]: 371243
[parent_id]: 371094
[tags]: 
There could be multiple reasons for why you might be seeing this behavior, but I think part of the solution would be to consider how XGBoost and GBM calculate variable importance and then see if the results make sense given each algorithms calculations. In addition, it would be good to check if you have correlated features and whether the most important features are correlated with the features you think should be the most important. For example, for GBM variable importance is determined by calculating the relative influence of each variable: whether that variable was selected during splitting in the tree building process and how much the squared error (over all trees) improved (decreased) as a result. It would also be helpful to know how many levels you have in each categorical column. In general, you need to be careful with high cardinality variables because GBM's can use them to memorize solutions rather than learn from the data, so it is always good to check whether the most important features in your variable importance table are high cardinality features. Please take a look this notebook on best practices for handling categorical features and see if after reading this you have a better sense of why you are getting the results you are getting: https://github.com/h2oai/h2o-tutorials/tree/master/best-practices/categorical-predictors in particular take a look at this notebook https://github.com/h2oai/h2o-tutorials/blob/master/best-practices/categorical-predictors/gbm_drf.ipynb .
