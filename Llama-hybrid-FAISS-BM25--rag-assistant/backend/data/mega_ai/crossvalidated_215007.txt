[site]: crossvalidated
[post_id]: 215007
[parent_id]: 214664
[tags]: 
I'll illustrate my answer with a simple example. Imagine that your data $X_1,\dots,X_n$ are counts that follow a Poisson distribution . Poisson distributtion is described using a single parameter $\lambda$ that we want to estimate given the data we have. To set up a Bayesian model we use Bayes theorem $$ \underbrace{p(\lambda| X)}_{\text{posterior}} \propto \underbrace{p(X | \lambda)}_{\text{likelihood}} \underbrace{p(\lambda)}_{\text{prior}} $$ where we define likelihood function as a Poisson distributtion parametrized by $\lambda$ and we use as a prior another Poisson distributtion parametrized using hyperparameter $\theta$: $$X_i \sim \mathrm{Poisson}(\lambda) \\ \lambda \sim \mathrm{Poisson}(\theta) $$ Your question is basically about how to find "optimal" $\theta$. Recall that Poisson's distributions parameter is also it's mean. It's maximum likelihood estimator is sample mean, so the "optimal" value for $\theta$ after looking at the data would be to use sample mean. If you did so, than what you would be calculating is given that prior mean is $\theta$ find the optimal value of $\lambda$ such that it maximizes the likelihood -- can you see the circularity? $\theta$ is already an optimal value given the data we have and then we use it to find the optimal value... In such case wouldn't maximum likelihood estimation be more honest way to go? To learn more about choosing priors check How to choose prior in Bayesian parameter estimation that goes into more details about choosing informative priors, i.e. priors based on some knowledge that we had before seeing the data. If we don't have such information, we use weekly informative priors that say very little about what we assume about the parameter of interest (e.g. uniform distribution over some reasonable range). Finally, if you have no ideas about the parameters of your priors you can use hyper-priors , i.e. priors for parameters of priors, and then the Bayesian machinery will find the "optimal" parameters for priors for you (but yes, you need to decide about the values of hyperpriors and this is not always that obvious). Finally, there is an approach called empirical Bayesian method , but as you can see from the example, the risk in here is that we can end up with estimates that are overconfident since we used the same data twice. Check "Bayesian Data Analysis" by Andrew Gelman, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and Donald Rubin for great introduction and multiple examples about choosing priors. "Doing Bayesian Data Analysis " by John K. Kruschke provides nice introduction about hierarchical models and hyperpriors. Finally, "Data Analysis: A Bayesian Tutorial" by Devinderjit Sivia and John Skilling give some discussion about "using the same data twice".
