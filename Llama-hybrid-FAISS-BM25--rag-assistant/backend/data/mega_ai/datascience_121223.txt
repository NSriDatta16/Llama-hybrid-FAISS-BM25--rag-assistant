[site]: datascience
[post_id]: 121223
[parent_id]: 
[tags]: 
My test dataset accuracy score is 1.0 for Random Forest - is the issue to do with my code or my dataset?

I use these classification algorithms to test whether a URL is phishing or legitimate. I have tried feature_importance, cross-validation, and hyperparameter tuning to the best of my ability, but I still get an accuracy score of 1.0. Now I am not sure if the problem is to do with my code or my dataset. I created a dataset that includes 45 attributes and 1223 entries. Below is a screenshot of some of the data: the status attribute is what deems the URL as phishing (1) or legitimate (0) this is my code for the Random Forest classifier: df = newData features = ['length_url', 'length_hostname', 'ip', 'nb_dots', 'nb_hyphens', 'nb_at', 'nb_qm', 'nb_and', 'nb_eq', 'nb_underscore', 'nb_slash', 'nb_colon', 'nb_semicolumn', 'https_token', 'ratio_digits_url', 'ratio_digits_host', 'nb_subdomains', 'prefix_suffix', 'length_words_raw', 'char_repeat', 'shortest_words_raw', 'shortest_word_host', 'shortest_word_path', 'longest_words_raw', 'longest_word_host', 'longest_word_path', 'avg_words_raw', 'avg_word_host', 'avg_word_path', 'nb_hyperlinks', 'ratio_intHyperlinks', 'ratio_extHyperlinks', 'nb_extCSS', 'login_form', 'links_in_tags', 'ratio_intMedia', 'ratio_extMedia', 'iframe', 'popup_window', 'safe_anchor', 'onmouseover', 'empty_title', 'domain_in_title'] X = df[features] y = df.status X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0, shuffle=True) rf = RandomForestClassifier(max_depth=10, n_estimators=50,random_state=0) rf.fit(X_train,y_train) y_pred=rf.predict(X_test) print("Accuracy:",metrics.accuracy_score(y_test, y_pred)) the array features consists of all the attributes of the dataset, excluding url and status . the accuracy score was 1.0 I have tried feature_importance_ using the following code: feature_imp = pd.Series(rf.feature_importances_, index=['length_url', 'length_hostname', 'ip', 'nb_dots', 'nb_hyphens', 'nb_at', 'nb_qm', 'nb_and', 'nb_eq', 'nb_underscore', 'nb_slash', 'nb_colon', 'nb_semicolumn', 'https_token', 'ratio_digits_url', 'ratio_digits_host', 'nb_subdomains', 'prefix_suffix', 'length_words_raw', 'char_repeat', 'shortest_words_raw', 'shortest_word_host', 'shortest_word_path', 'longest_words_raw', 'longest_word_host', 'longest_word_path', 'avg_words_raw', 'avg_word_host', 'avg_word_path', 'nb_hyperlinks', 'ratio_intHyperlinks', 'ratio_extHyperlinks', 'nb_extCSS', 'login_form', 'links_in_tags', 'ratio_intMedia', 'ratio_extMedia', 'iframe', 'popup_window', 'safe_anchor', 'onmouseover', 'empty_title', 'domain_in_title']).sort_values(ascending=False) sns.barplot(x=feature_imp, y=feature_imp.index) plt.xlabel('Feature Importance Score') plt.ylabel('Features') plt.title("Visualizing Important Features", pad=15, size=30) plt.savefig('barplot.png') the output: I also did cross-fold validation: cv1 = KFold(n_splits=10, random_state=1, shuffle=True) model = RandomForestClassifier() scores1 = cross_val_score(model, X, y, scoring='accuracy', cv=cv1, n_jobs=-1) print('Accuracy (k-fold cross validation): %.3f (%.3f)' % (mean(scores1), std(scores1))) cv2 = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1) scores2 = cross_val_score(model, X, y, scoring='accuracy', cv=cv2, n_jobs=-1) print('Accuracy (repeated k-fold cross validation): %.3f (%.3f)' % (mean(scores2), std(scores2))) the output:
