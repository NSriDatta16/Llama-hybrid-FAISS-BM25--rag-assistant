[site]: stackoverflow
[post_id]: 2738804
[parent_id]: 2737541
[tags]: 
Most likely it's not performing well because quicksort doesn't handle lots of duplicates very well and may still result in swapping them (order of key-equal elements isn't guaranteed to be preserved). You'll notice that the number of duplicates per number is 100 for 10000 or 500 for 2000, while the time factor is also approximately a factor of 5. Have you averaged the runtimes over at least 5-10 runs at each size to give it a fair shot of getting a good starting pivot? As a comparison have you checked to see how std::sort and std::stable_sort also perform on the same data sets? Finally for this distribution of data (unless this is a quicksort exercise) I think counting sort would be much better - 40K memory to store the counts and it runs in O(n).
