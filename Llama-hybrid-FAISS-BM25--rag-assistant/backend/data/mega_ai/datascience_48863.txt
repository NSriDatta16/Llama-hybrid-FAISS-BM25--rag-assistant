[site]: datascience
[post_id]: 48863
[parent_id]: 1028
[tags]: 
The Random Forest does overfit. The Random Forest does not increase generalization error when more trees are added to the model. The generalization variance is going to zero with more trees used. I've made a very simple experiment. I have generated the synthetic data: y = 10 * x + noise I've train two Random Forest models: one with full trees one with pruned trees The model with full trees has lower train error but higher test error than the model with pruned trees. The responses of both models: It is clear evidence of overfitting. Then I took the hyper-parameters of the overfitted model and check the error while adding at each step 1 tree. I got the following plot: As you can see the overfit error is not changing when adding more trees but the model is overfitted. The experiment with code examples is described in my blog post .
