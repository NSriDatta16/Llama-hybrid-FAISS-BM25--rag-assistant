[site]: crossvalidated
[post_id]: 91491
[parent_id]: 91473
[tags]: 
Without equations? Yikes. Let's see: The logistic regression model is literally a model for the $p$ parameter of a binomial distribution; with a continuous predictor, each point can have its own distribution. (In the cases where the observations are 0-1, we deal with the Bernoulli special case; this is a common situation.) The $n$ is given, not modelled. So the result is, with a model relating the $p_i$ 's and a known $n_i$ , we can model binomial data in terms of a predictor that describes the mean (and variance) via its model for $p$ . The model may be fit via maximum likelihood estimation, but because of its special form (exponential family), ML is relatively "nice". Because the logistic link is canonical for the binomial family, it's even nicer, since the sufficient statistics are of very simple form - this makes it convenient for dealing with large samples, or even to develop 'online' algorithms. Of course, $p$ , being a probability, lies between 0 and 1. This, naturally, means that when we write a model for it in terms of some other variable, that model should not crash through those limits, so as the independent variable gets sufficiently large or small, the relationship must bend to stay inside the bounds. With logistic regression, that curve (the link function) is a logistic function. Other functions are possible, and many packages implement several (R has three suitable ones built into its glm functionality if I recall right). No equality symbols were harmed in the making of this post.
