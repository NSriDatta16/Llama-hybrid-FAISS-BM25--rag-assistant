{ 1 , … , K } {\displaystyle \{1,\ldots ,K\}} and the kernel is chosen to be the Kronecker delta function, so k ( x , x ′ ) = δ ( x , x ′ ) {\displaystyle k(x,x')=\delta (x,x')} . The feature map corresponding to this kernel is the standard basis vector φ ( x ) = e x {\displaystyle \varphi (x)=\mathbf {e} _{x}} . The kernel embeddings of such a distributions are thus vectors of marginal probabilities while the embeddings of joint distributions in this setting are K × K {\displaystyle K\times K} matrices specifying joint probability tables, and the explicit form of these embeddings is μ X = E [ e X ] = ( P ( X = 1 ) ⋮ P ( X = K ) ) {\displaystyle \mu _{X}=\mathbb {E} [\mathbf {e} _{X}]={\begin{pmatrix}P(X=1)\\\vdots \\P(X=K)\\\end{pmatrix}}} C X Y = E [ e X ⊗ e Y ] = ( P ( X = s , Y = t ) ) s , t ∈ { 1 , … , K } {\displaystyle {\mathcal {C}}_{XY}=\mathbb {E} [\mathbf {e} _{X}\otimes \mathbf {e} _{Y}]=(P(X=s,Y=t))_{s,t\in \{1,\ldots ,K\}}} When P ( X = s ) > 0 {\displaystyle P(X=s)>0} , for all s ∈ { 1 , … , K } {\displaystyle s\in \{1,\ldots ,K\}} , the conditional distribution embedding operator, C Y ∣ X = C Y X C X X − 1 , {\displaystyle {\mathcal {C}}_{Y\mid X}={\mathcal {C}}_{YX}{\mathcal {C}}_{XX}^{-1},} is in this setting a conditional probability table C Y ∣ X = ( P ( Y = s ∣ X = t ) ) s , t ∈ { 1 , … , K } {\displaystyle {\mathcal {C}}_{Y\mid X}=(P(Y=s\mid X=t))_{s,t\in \{1,\dots ,K\}}} and C X X = ( P ( X = 1 ) … 0 ⋮ ⋱ ⋮ 0 … P ( X = K ) ) {\displaystyle {\mathcal {C}}_{XX}={\begin{pmatrix}P(X=1)&\dots &0\\\vdots &\ddots &\vdots \\0&\dots &P(X=K)\\\end{pmatrix}}} Thus, the embeddings of the conditional distribution under a fixed value of X {\displaystyle X} may be computed as μ Y ∣ x = C Y ∣ X φ ( x ) = ( P ( Y = 1 ∣ X = x ) ⋮ P ( Y = K ∣ X = x ) ) {\displaystyle \mu _{Y\mid x}={\mathcal {C}}_{Y\mid X}\varphi (x)={\begin{pmatrix}P(Y=1\mid X=x)\\\vdots \\P(Y=K\mid X=x)\\\end{pmatrix}}} In this discrete-valued setting with the Kronecker delta kernel, the kernel sum rule becomes ( P ( X = 1 ) ⋮ P ( X = N ) ) ⏟ μ X π = ( P ( X = s ∣ Y = t ) ) ⏟ C X ∣ Y ( π ( Y = 1 ) ⋮ π ( Y = N ) ) ⏟ μ Y π {\displaystyle \underbrace {\begin{pmatrix}P(X=1)\\\vdots \\P(X=N)\\\end{pmatrix}} _{\mu _{X}^{\pi }}=\underbrace {\begin{pmatrix}\\P(X=s\mid Y=t)\\\\\end{pmatrix}} _{{\mathcal {C}}_{X\mid Y}}\underbrace {\begin{pmatrix}\pi (Y=1)\\\vdots \\\pi (Y=N)\\\end{pmatrix}} _{\mu _{Y}^{\pi }}} The kernel chain rule in this case is given by ( P ( X = s , Y = t ) ) ⏟ C X Y π = ( P ( X = s ∣ Y = t ) ) ⏟ C X ∣ Y ( π ( Y = 1 ) … 0 ⋮ ⋱ ⋮ 0 … π ( Y = K ) ) ⏟ C Y Y π {\displaystyle \underbrace {\begin{pmatrix}\\P(X=s,Y=t)\\\\\end{pmatrix}} _{{\mathcal {C}}_{XY}^{\pi }}=\underbrace {\begin{pmatrix}\\P(X=s\mid Y=t)\\\\\end{pmatrix}} _{{\mathcal {C}}_{X\mid Y}}\underbrace {\begin{pmatrix}\pi (Y=1)&\dots &0\\\vdots &\ddots &\vdots \\0&\dots &\pi (Y=K)\\\end{pmatrix}} _{{\mathcal {C}}_{YY}^{\pi }}} References External links Information Theoretical Estimators toolbox (distribution regression demonstration).