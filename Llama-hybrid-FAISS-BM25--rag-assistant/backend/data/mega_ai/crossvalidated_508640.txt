[site]: crossvalidated
[post_id]: 508640
[parent_id]: 
[tags]: 
Two (related) questions about forecasting multivariate models with multi-period lags

I am estimating a bunch of different linear or quasi-linear models on several hundred observations of the same multivariate (economic) time series data set. I hope to use the results both for insight into the underlying processes and for forecasting a vector of the series values. Because of the former goal, I am more concerned with parsimony than I would be if only interested in forecasting. Because of the latter, I intend to use the best of the individual model forecasts as inputs to some simple ensemble method, such as the median or weighted mean. All the models are linear or quasi-linear and are estimated in lags of both the dependent and the independent variables. I assume the forecasts can be based on past values only, allowing single equation estimates of each series. I transform the data in various ways (de-trended, differenced, log'ed, log-differenced) which result in different time-series properties, and the models (e.g. VARs, VECMs, VARIMAX, ARDL) are selected to be appropriate based on the properties of the transformed series in question. For each model, all the series are transformed, modeled, and for forecasting back-transformed, in the same way. I am using several different approaches for variable selection as to the lag length of each variable (also as to controlling over-fitting, but my question goes only to the variable selection part). Mainly I am selecting variables by AIC and different sorts of cross-validation (leave one out, n-way) followed by relaxed LASSO. Here is my question: My variable selection tests result in variable sets with two properties that make me nervous. First, for all the equations, the selected sets have lag lengths that vary for the different variables. Second, for some of the equations, some variables with a maximum selected lag length of m exclude one or more lags with a lag of less than m. I am trying to decide whether to impose the requirements that: ( Restriction 1 ) the lags of all variables (possibly excluding the equation-specific dependent-variable lags) must be included or excluded to the same length, applying AIC or cross-validation/LASSO to sets accross all variables with the same lag length, rather than to each series individually; and ( Restriction 2 ) that for each series all lags less than the maximum lag selected must be included. Each of these restrictions increases the possibility of over-fitting. On the other hand, I have reason to believe that these series are causally entwined in multiple ways, so that leaving some out at lag lengths where others are included seems risky. Also, I note that some ofmy models (e.g. VARIMAX) impose these restrictions automatically. I'd love to have some general theoretical result that answers these questions,but if there isn't one, I'd be happy to accept a recommendation about best practices from someone who has done a lot of time series forecasting and has a view on these constraints based on experience and observation. P.S. With respect to both of these restriction, I am only asking if they should be applied within the equation for a given series as dependent variable. I am not contemplating comperable cross-equation restrictions.
