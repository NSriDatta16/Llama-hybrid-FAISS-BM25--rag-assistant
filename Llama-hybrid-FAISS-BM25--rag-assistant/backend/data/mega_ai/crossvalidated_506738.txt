[site]: crossvalidated
[post_id]: 506738
[parent_id]: 
[tags]: 
How does this likelihood function log transformation work?

Suppose I have the following Bayesian inference problem: $$\underset{\{{\theta_i\},\{B^j}\}}{\operatorname{argmax}} P(\{\theta_i\},\{B^j_{il}\}|\{v_{jl}\}) \propto P(\{v_{jl}\}|\{\theta_i\},\{B^j_{il}\}) P(\{B^j_{il}\}) P(\{\theta_i\}) \tag1 \label{eq1}$$ and suppose that given $\{B^j\}$ we have the MAP estimate on $\{\theta_i\}$ : $$\hat{\theta_i} = \frac{\sum_{jl} B^i_{jl} v_{jl}/\sigma_j^2}{1/\sigma_0^2 + \sum_{jl}B^j_{il}/\sigma_j^2} \text{ for } i = 1, \dots, L.$$ In the paper Bayesian Nonparametric Federated Learning of Neural Networks (2020), the authors then say: " We can now cast [the] optimization [problem] [..] with respect to only $\{B_j\}$ [..] Taking natural logarithm we obtain: $$-\frac{1}{2}\sum_i(\frac{\|\hat{\theta_i}\|^2}{\sigma_0^2} + D\log(2\pi\sigma_0^2) + \sum_{jl} B_{il}^j \frac{\|v_{jl} - \hat{\theta_i}\|^2}{\sigma_j^2}) + \log (P(\{B^j\}) \tag2 \label{eq2}$$ Can someone help me understand how the different terms in \eqref{eq2} are found? Is there a simple formula to take the likelihood expressed in \eqref{eq1} into \eqref{eq2}? (Knowing that the $\{v_{jl}\}$ are assumed to be normally distributed) EDIT: So indeed as @FabianWerner said, the second part of equation (2) $-\frac{1}{2}\ D\log(2\pi\sigma_0^2) + \sum_{jl} B_{il}^j \frac{\|v_{jl} - \hat{\theta_i}\|^2}{\sigma_j^2}$ simply comes from writing out the pdf of the normally distributed variable $v_{jl}$ and applying common log transformations. I still haven't figured why the first term and the sum over i $\sum_i(\frac{\|\hat{\theta_i}\|^2}{\sigma_0^2} ..)$ is here though..
