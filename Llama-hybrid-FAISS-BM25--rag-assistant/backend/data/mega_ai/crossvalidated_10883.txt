[site]: crossvalidated
[post_id]: 10883
[parent_id]: 10394
[tags]: 
I guess Gaussian efficiency is something related to computation cost. The efficiency of Gaussian adaptation relies on the theory of information due to Claude E. Shannon. When an event occurs with probability P, then the information −log(P) may be achieved. For instance, if the mean fitness is P, the information gained for each individual selected for survival will be −log(P) – on the average - and the work/time needed to get the information is proportional to 1/P. Thus, if efficiency, E, is defined as information divided by the work/time needed to get it we have: E = −P log(P). This function attains its maximum when P = 1/e = 0.37. The same result has been obtained by Gaines with a different method. I may simply conclude that the higher the Gaussian Efficiency is, less resources (RAM) is needed for computing something like a robust scale estimator of a large sample. Since CPUs are much faster than the rest of computer we prefer to run a trial/error algorithm for times rather doing it at once with saying 128GB of RAM. when the Gaussian Efficiency is high the job will be done in a shorter time.
