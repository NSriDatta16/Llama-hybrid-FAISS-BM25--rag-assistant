[site]: crossvalidated
[post_id]: 327748
[parent_id]: 327746
[tags]: 
No. You can have both high or both low at same time. Here is an illustrate example. picture and article source I also recommend you to read the article where this picture comes from. The reason you have such impression is that in "early age" of machine learning, there is a concept called bias variance trade-off (as @Kodiologist mentioned, this concept is still true and a fundamental concept of tuning models today.) When increase model complexity, variance is increased and bias is reduced when regularize the model, bias is increased and variance is reduced. In Andrew Ng's recent Deep Learning Coursera lecture, he mentioned that in recent deep learning framework (with huge amount of data), people talk less about trade off. In stead, there are ways to only reduce variance and do not increase bias (For example, increase training data size), as vice versa.
