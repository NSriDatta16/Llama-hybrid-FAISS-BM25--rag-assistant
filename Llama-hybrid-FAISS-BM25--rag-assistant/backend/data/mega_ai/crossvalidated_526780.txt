[site]: crossvalidated
[post_id]: 526780
[parent_id]: 
[tags]: 
Mean Average Precision (MAP) object detection metric

I am reading up on object detection metrics from this github repository . I have a slight confusion regarding the precision x recall curve mentioned under the Metrics heading. It says that The Precision x Recall curve is a good way to evaluate the performance of an object detector as the confidence is changed by plotting a curve for each object class Before this section, they mentioned that precision and recall is obtained through IOU intersection between the ground truth and detected bounding box, which is decided by a threshold value. In the quote above, they talk about confidence of the prediction which I believe is the probability of class score. My question is whether the precision x recall curve is obtained by varying the threshold for IOU or by varying the threshold for the probability of class scores? I would think it is the former although I am not too sure about it.
