[site]: crossvalidated
[post_id]: 329689
[parent_id]: 145485
[tags]: 
Other answers here cover the technical differences between those two algorithms, however I think the core difference is their purpose: Those two algorithms were designed to do different things: word2vec ultimately yields a mapping between words and a fixed length vector. If we were to compare it with another well known approach, it would make more sense to do so using another tool that was designed for the same intend, like the Bag of Words (BOW model). This one does the same but lacks some desired features of word2vec like using the order of words and assigning semantic meaning to the distances between word representations. LDA on the other hand creates a mapping from a varied length document to a vector. This document can be a sentence, paragraph or full text file but it is not a single word. It would make more sense to compare it with doc2vec that does the same job and is introduced by Tomas Mikolov here (the author uses the term paragraph vectors ). Or with LSI for that matter. So to directly answer your two questions: None of them is a generalization or variation of the other Use LDA to map a document to a fixed length vector. You can then use this vector in a traditional ML algorithm like a classifier that accepts a document and predicts a sentimental label for example. Use word2vec to map a word to a fixed length vector. You can similarly use these vectors to feed ML models were the input are words, for example when developing an auto-completer that feeds on previous words and attempts to predict the next one.
