[site]: crossvalidated
[post_id]: 628637
[parent_id]: 
[tags]: 
Finding probability that the mean of a sample is below a certain limit

I have a complicated physical model that can produce a certain quantity (real-valued) for a large (in the 1000's) number of points (N). We assume that mean and variance of the model output is a good representation of the actual physical object properties but we don't know as we have not built them yet (please let me know if this a bad assumption). So $\mu_{model}=\mu_{population}$ and $\sigma_{model}=\sigma_{population}$ . From here, I'm trying to find the probability that the mean of a relatively small sample ( $n$ $L$ . First basic approach is to use the square-root law. Basically I can convert my z-score from the model/population into a z-score for my sample: if I had $\frac{\mu-L}{\sigma}=-1.15$ , assuming normal distribution, while a single object would have had 75% chance of having its property below the limit $L$ , the mean of a sample of $n$ objects would have a z-score of $-1.15\sqrt{7}=3.04$ and thus a probability of 99.8% to be below the limit $L$ . I feel like this is the right trend as for a very large sample, the sample mean should converge to the population mean and thus the sample mean should have near certainty to be below the limit. Another approach is to use a t-score. $CDF_t\left(\frac{L-\mu}{\sigma}\right)$ , here again using the population mean and std rather than the sample ones. So basically $CDF_t(1.15,6)=85.3\%$ certainty to be below the limit for the mean of my sample of 7, but what does not feel right is that $CDF_t(1.15,n)$ does not tend to 100% as n goes very large. Another brute force approach would be to build samples of my modeled population and compare the mean of these samples to my limit $L$ . Not sophisticated but maybe I should leverage the fact that I have an actual (large-ish) 1000+ actual members of the population and not make any other assumption. Is any of these methods correct? If not what would you suggest I use? Edit: I have a machine (say a gas turbine) that takes air and fuel at certain (T,p) conditions (bounded) and transform it into a different fluid at different (T,p). This gas turbine has 10's of internal parameters that defines it, we'll call these (x1,x2,...,xn). I'm only interested in one output Y and I have built a model to predict Y = f(air_T, air_p, fuel_T, fuel_p, x1, x2, ..., xn). For a given built machine (which means x1, x2, ..., xn are fixed), we are interested in the statistics of Y across a range of inlet conditions. We assume that there is enough physics in that model that it gets the mean of the quantity Y correct, but probably not the higher order statistics. I want to find the probability that the quantity Y across a small sample of machines (but each across a large number of inlet conditions) has a mean below a certain limit L.
