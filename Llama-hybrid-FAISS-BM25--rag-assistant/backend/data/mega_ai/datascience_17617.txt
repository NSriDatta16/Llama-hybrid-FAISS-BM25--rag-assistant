[site]: datascience
[post_id]: 17617
[parent_id]: 17616
[tags]: 
Yes this problem is extremely well suited for machine learning. However, I think you should be careful as to which algorithms you tend to use. A machine learning algorithm should be structured as follows: feature extraction and then your model. These are two things that should be done separately. Feature Extraction This is the bag of words, n_grams and word2vec. These are all good choices for text examples. I think bag of words is a good choice in your case. However, if this generates a sparse matrix then maybe n_grams can be better. You can test all 3 methods. The Model Theoretically, the more parameters in your model the more data you need to train it sufficiently otherwise you will retain a large amount of bias. This means a high error rate. Neural networks tend to have a very high number of parameters. Thus they require a lot of data to be trained. But, you have 1000 instances!!! Yes. But, you also have 500 classes. So imagine you have a very young child and you want him to be able to correctly classify 500 different types of images. Then you can't just show the kid 2 different examples of each class for him to truly understand what each class really means. As a very general rule of thumb, the number of instances you need to train a model increases exponentially with number of classes. So you will need a MASSIVE amount of data to properly train a neural network model. I would suggest a less intensive model. Moreover, looking at your example, it seems that the classes should be linearly separable. So you can use something really simple, linear regression, logistic regression, naive bayes or knn. These methods would do MUCH MUCH better than a neural network. My Suggestion I would start with bag of words and then use knn. This should be a good starting point. A neural network is 0% recommended for the amount of data you have.
