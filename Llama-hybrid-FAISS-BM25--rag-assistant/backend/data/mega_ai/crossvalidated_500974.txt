[site]: crossvalidated
[post_id]: 500974
[parent_id]: 
[tags]: 
Is this a bad method to detect outliers

I thought of this way of detecting outliers. What are the "bad" properties of this method? For example, say you have a time series, and you want to check if the latest observation is an outlier. Firstly, I limit the time series to the past N elements, so that the time series are always of the same length, and data from way in the past is ignored. Then I calculate the standard deviation with AND without the latest observation. I then flag it as an outlier if that change is big, i.e. if $$(\text{std_with} - \text{std_without}) / \text{std_with}$$ is a big percentage. Is this method bad? What are your criticisms?
