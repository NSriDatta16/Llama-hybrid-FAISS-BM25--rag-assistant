[site]: datascience
[post_id]: 25978
[parent_id]: 25977
[tags]: 
C value is like lambda, the L2/L1 regularization hyper parameter, but in reverse manner. Whenever C is large, means there is high probability that your model overfit the data in hand. Whenever it is small, your model endure some errors in order to avoid producing small margin and consequently you will have a model less prone to overfitting. The first one is called hard SVM and the latter one is soft SVM. In your case, I guess there will be no difference because your model is linearly separable. If X4 was between X2 and X3 you would have different result for each hard and soft. Soft and hard differ in cases which the data is not linearly separable or maybe they are separable but the margin would be narrow or wide if SVM discards some samples. Soft tries to make the separator space, margin, as large as possible by discarding those minority data which hinder a large margin to be crated but hard tries to separate the data to reduce the error as much as possible. In your case, you will get same result. For illustrating more, look at the left figure. It tries to discard the sample which hinders the large margin to be created but large margin, considers all the samples.
