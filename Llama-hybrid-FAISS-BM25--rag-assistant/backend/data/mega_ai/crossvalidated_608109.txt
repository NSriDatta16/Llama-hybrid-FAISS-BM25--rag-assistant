[site]: crossvalidated
[post_id]: 608109
[parent_id]: 
[tags]: 
Autocorrelation function of the logistic map $x_{n+1}=4x_n(1-x_n)$

Is there a proof that for the sequence $x_{n+1}=4x_n(1-x_n)$ , the lag- $m$ autocorrelation for $m=1,2$ and so on, is zero if you start with a random seed $x_0$ , or say $x_0=\frac{1}{3}$ ? It is defined as follows: $$ \rho(m) = \lim_{n\rightarrow\infty} \frac{1}{8n}\sum_{k=1}^n \Big(x_k-\frac{1}{2}\Big)\Big(x_{k+m}-\frac{1}{2}\Big). $$ Just asking as there are plenty of authors spending a lot of time doing computations to show it is very close to zero, in the context of random number generation. I have an elegant proof that it is exactly zero regardless of $m>0$ , I'd like to publish it but I'm wondering if such a fundamental result was not already proved long ago. If you start with a random seed, then $\rho(m)$ is a.s. zero. If you start with $x_0=\frac{1}{3}$ , it would be zero if the number $\pi^{-1}\arcsin(\sqrt{1/3})$ is normal in base 2. Proving the latter is a very hard, unsolved problem, although everyone strongly believe this conjecture (about normalcy) to be true. Update To avoid confusion, here is a different (simplified) version of the problem. If it has been proved (which I suspect, but could not find any reference), would love to see a reference. If not, I have a proof but rather than sharing it, I'd encourage readers to try to prove it. It's not that hard, but not trivial either. Happy to share my proof too if there is interest. Assume $X_0$ has a $\text{Beta}[\frac{1}{2},\frac{1}{2}]$ distribution. Let $X_{k+1}=4X_k(1-X_k)$ . Then $\text{E}[X_0 X_k] = \text{E}[X_0] E[X_k]$ for all $k>0$ .
