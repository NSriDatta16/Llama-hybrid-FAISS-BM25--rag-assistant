[site]: crossvalidated
[post_id]: 421266
[parent_id]: 421092
[tags]: 
The notes you reference have a splendid, clear, insightful discussion of this definition, its meaning, and its application--thank you for bringing them to our attention. Because the job of explanation has been so well done, all that is needed may be to clarify the specific points in your question. It looks like you lack only a definition of a "partition:" A partition of a set $\mathcal{X}$ is a collection of its subsets $\mathcal{A}_i \subset \mathcal{X}$ for which (1) $\bigcup_{i} \mathcal{A_i}=\mathcal{X}$ and (2) $\mathcal{A}_i \cap \mathcal{A}_j = \emptyset$ whenever $i\ne j.$ The first condition says the subsets comprise all of $\mathcal X$ and the second says no two have any elements in common. Thus, every element of $X$ belongs to exactly one element of the partition. Partitions may have any cardinality less than that of $\mathcal X$ itself. When the cardinality is finite, the partition is finite. In the notation of the question, $r$ is its cardinality--the number of distinct subsets in the partition. Finally, $\Theta$ is endowed with a probability measure. Because the definition involves probabilities of the sets ("events") in the partition, they all need to be measurable subsets. Thus: Because $(X(\mathcal{A}_1), \ldots, X(\mathcal{A}_r))$ is a realization of a Dirichlet distribution, it must be a vector of $r$ non-negative components summing to unity. The partitions are literally any possible partitions of $\Theta$ that meet the requirements of being finite and measurable. $r$ is the cardinality of such an arbitrary partition. The only relation between $X(\mathcal{A}_i)$ and $X(\mathcal{A}_j)$ is that required of any Dirichlet random variable: namely, these values are non-negative and they cannot sum to more than $1.$ Finally, don't think of a stochastic process in this context as a "sequence of random variables:" think of it as a randomly-chosen probability distribution on $\Theta.$ It can be approximated arbitrarily closely by binning $\Theta$ (that's what the partitioning does) into sufficiently small bins $\mathcal{A}_i$ and using a suitable Dirichlet distribution to realize the probability associated with each bin, written as $X(\mathcal{A}_i).$ One way to visualize this is to draw a series of these approximations. In each row of the figure I use bar charts to plot the values of $X(\mathcal{A}_i), \ldots, X(\mathcal{A}_r)$ for the partition $$\mathcal{A}_i = \left[\frac{i-1}{r}, \frac{i}{r}\right)$$ of the unit interval $\Theta = [0,1).$ The base measure $H$ is the uniform distribution on $\Theta.$ Each row depicts a single realization of the process $X.$ The rows have differing "inverse variance" parameters $\alpha,$ ranging from $1$ (top) to $100$ (middle) to $10000$ (bottom). In this downward progression you can see the sense in which the process $X$ tends to differ less and less from $H$ (whose density is constant) as $\alpha$ is increased. In the progressions from left to right, as $r$ is increased, you can see how more and more detail of each realization is brought out in the finer partitions. Think of a realization of the process as taking a limit as you move further out to the right. It might be instructive to study the code that produced these realization, so here it is. # # Generate `n` realizations of a Dirichlet (p) random variable. # rdirichlet
