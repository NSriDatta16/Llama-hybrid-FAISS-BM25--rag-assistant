[site]: crossvalidated
[post_id]: 546901
[parent_id]: 546844
[tags]: 
I tried it set.seed(1) x = 1:20 y = x + rnorm(20) lm(y[1:10] ~ x[1:10]) lm(y[11:20] ~ x[11:20]) lm(y~x) and it doesn't work. The coefficients of a regression, is not the same as the average of the coefficients of two half regressions. Call: lm(formula = y[1:10] ~ x[1:10]) Coefficients: (Intercept) x[1:10] -0.1688 1.0547 Call: lm(formula = y[11:20] ~ x[11:20]) Coefficients: (Intercept) x[11:20] -0.5036 1.0485 Call: lm(formula = y ~ x) Coefficients: (Intercept) x -0.03609 1.02158 But while you do not get the exact same result as a single regression. The mean of 4 independent regressions does give you a more precise estimate. Since the average of multiple estimates will have less variance. So yes, you could use the average of the multiple regressions. With some more information one could tell if there are smarter ways to combine the figures. E.g. when the data is not the same then the estimates can have different precision and for that case you could use a weighted estimate in which you do not count every result the same. (For instance, in the above example the intercept does not have the same error in the two splits because the vector $x$ is shifted. And this changes the error . Using the first more precise estimate of the intercept is better than using the average.) Also, with some more information one might get to the conclusion that the groups are not to be averaged. If there is something meaningful different about the groups then they could have different regression lines which should not be grouped.
