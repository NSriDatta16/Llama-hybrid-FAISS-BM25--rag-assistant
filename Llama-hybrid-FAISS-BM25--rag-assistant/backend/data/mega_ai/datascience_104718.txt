[site]: datascience
[post_id]: 104718
[parent_id]: 
[tags]: 
How do I find the gradients of an X -> RELU -> RELU -> Softmax network?

I am trying to use the MNIST digits dataset with logistic regression. I am only using numpy as I would like to implement a simple project myself before using something like pytorch. My network topology is an input layer with 784 input features (rows) by 10,000 columns (training examples), this goes to a 3 node RELU layer, then another 3 node RELU layer and then a 10 node softmax layer for output. So X->RELU->RELU->Softmax I know the cost function is J = -SUM(y_i * ln(y_hat_i)) I can find the derivative of J with respect to y_hat . dJ/dy_hat = y_hat_i - y_i but going back any further than that I run into a wall how do I find dJ/dW3 , dJ/dW2 , dJ/dW1 and dJ/db3 , dJ/db2 , dJ/db1 ?
