[site]: crossvalidated
[post_id]: 252074
[parent_id]: 
[tags]: 
Naive Question regarding feature selection

I have two labelled classes, 500 examples of each, 4500 features for each of them. So the dataframe is 1000X4500. Is it possible to devise a model which uses all the 4500 features and gives 100% accuracy, and then remove out unimportant features one by one till I get an accuracy of 85% (i.e. moving from a completely overfit model to a more generalized one with 85% accuracy)? So basically the purpose is to filter out unnecessary features. Does such a method even exist in machine learning?
