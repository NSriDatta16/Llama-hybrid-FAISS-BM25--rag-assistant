[site]: crossvalidated
[post_id]: 391116
[parent_id]: 391099
[tags]: 
Assume we are given $n$ random variables $X_1, ..., X_n$ that satisfy some conditions (so that the covariances exist and so forth) then to me, PCA means forming the matrix $C = (\text{Cov}(X_i, X_j))_{i,j=1,...,n}$ . This is a symmetric, positive semidefinite matrix so it can be diagonalized using an orthogonal matrix. I.e. we can find an invertible matrix $A \in \mathbb{R}^{n \times n}$ such that $A^T = A^{-1}$ and $$A^{-1} C A = D = \text{diag}(\lambda_1, ..., \lambda_n)$$ One can show that, since $C$ is the covariance matrix of the vector of random variables $X = (X_1, ..., X_n)$ that then the covariance matrix of $AX$ is $A^TCA = D$ . This is simple algebra using the mere definition and the bilinearity of covariance (maybe I got it wrong here and one needs to put $A^T$ in front of $X$ instead of $A$ or so). This means that we get new random variables $Y_1, ..., Y_n$ that are linear combinations of the old ones and such that $$\text{Cov}(Y_i, Y_j) = 0$$ for all $i \neq j$ . However, for $i=j$ , i.e. for the diagonal entries we have $$\lambda_i = D_{ii} = \text{Cov}(Y_i, Y_i) = \text{Var}(Y_i)$$ i.e. the $i$ -th eigenvalue belonging to the $i$ -th eigenvector is the variance of the $i$ -th resulting PCA component (i.e. the new variable $Y_i$ ). What people now do is take a threshold and throw away all $Y_i$ such that $\lambda_i$ is below that threshold. I.e. those variables who having a low variance 'do not contribute much'. However, this is just a heuristic (who sais that just because $Y_i$ has a small variance, it does not contribute much? It may be that this difference is small but unbelievably important ...) and should be used only when necessary, i.e. when you just have 30 or 40 variables and 1 mio. examples and you are using an algorithm that is able to detect uneccessary features then I would not use PCA as this tends to confuse the algorithm...
