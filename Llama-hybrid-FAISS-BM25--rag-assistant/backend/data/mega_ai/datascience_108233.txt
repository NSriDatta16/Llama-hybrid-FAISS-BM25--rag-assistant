[site]: datascience
[post_id]: 108233
[parent_id]: 
[tags]: 
Recommendations for tuning XGBoost Hyperparams?

XGBoost has quite a few hyperparameters to tune: max depth, min child weight, number of iterations, eta, gamma, percent of columns considered, and percent of samples considered. It's computationally infeasible to tune all of these simultaneously in a huge grid search. So, these must be done in some order. Do you have any recommendations? Currently, I first tune Eta and N iterations together, then Max Depth and MCW together, then col-sample and row-sample together, then finally gamma. Do you have other ideas? If you tune it piece-wise like this, how do you decide at what value to fix the hyperparams at the very start? For example, what do you set Max Depth and MCW when you're tuning Eta etc.?
