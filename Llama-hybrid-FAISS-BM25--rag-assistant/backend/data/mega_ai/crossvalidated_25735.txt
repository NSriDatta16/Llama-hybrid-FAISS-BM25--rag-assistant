[site]: crossvalidated
[post_id]: 25735
[parent_id]: 
[tags]: 
What to do when a NN learns a static representation for a large chunk of your dataset?

So I'm currently training a stack of RBMs (Restricted Boltzmann Machines), eventually to be build into a DBN (Deep Belief Net), on a set of gray-scale images featuring objects placed in different locations on a complex background. After training for a good long while, I tried generating from the RBMs and found that for a large percentage of the generated images were identical; they all looked like the background. This makes sense: the background is static across the training set, so learning a high fidelity representation of that is a better idea (from the network's perspective) than learning a low fidelity representation of the objects, since they appear in different positions. My question is this: I want to learn about the whole image, not just the background; what should I change in order get this to happen? I could always lower the learning rate/momentum and keep training, but I've love some opinions as to whether or not that is the best solution. Is there anyway to utilize this learned representation of the image background to learn a better representation of the foreground? I've thought of just subtracting it out, but that seems rather ad hoc. P.S. I've had this problem in the past with traditional autoencoders (i.e. three-layer backpropagation net predicting the input pattern), so I know it isn't unique to deep learning approaches or the particular dataset, just any dataset that contains a static signal amongst more varied information.
