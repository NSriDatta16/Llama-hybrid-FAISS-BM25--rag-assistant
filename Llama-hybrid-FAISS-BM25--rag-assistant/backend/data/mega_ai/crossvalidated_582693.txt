[site]: crossvalidated
[post_id]: 582693
[parent_id]: 582686
[tags]: 
All else being equal, more data is always better. So #3 is clearly the best option . Imbalanced data is not really a problem, and sacrificing more data for balance is throwing away free information (as Stephan Kolassa notes, the cost of data collection could be a concern - I am ignoring that for now). See the following questions for more detailed discussion about this common misconception: Are unbalanced datasets problematic, and (how) does oversampling (purport to) help? When is unbalanced data really a problem in Machine Learning? Does an unbalanced sample matter when doing logistic regression? What is the root cause of the class imbalance problem? This would be a more difficult choice if instead of [1000, 1000, 1000], option #3 was something like [10, 1000, 1000]. In that case, it is arguable whether you would learn enough about that one class from 10 samples to make the additional benefit of 1000 samples from the other 2 classes worth it - so [200, 200, 200] or [100, 1000, 1000] might be better options.
