[site]: crossvalidated
[post_id]: 236332
[parent_id]: 236328
[tags]: 
I am not sure if you confuse consistency and unbiasedness. Consistency: The larger the sample size the closer the estimate to the true value. Depends on sample size Unbiasedness: The expected value of the estimator equals the true value of the parameters Does not depend on sample size So your sentence if you average a bunch of values of $\hat\theta$ , as the sample size gets larger, you get a better approximation of $\theta$ . Is not correct. Even if the sample size gets infinite an unbiased estimator will stay an unbiased estimator, e.g. If you estimate the mean as "mean +1" you can add one billion observations to your sample and your estimator will still not give you the true value. Here you can find a more profound discussion about the difference between consistency and unbiasedness. What is the difference between a consistent estimator and an unbiased estimator?
