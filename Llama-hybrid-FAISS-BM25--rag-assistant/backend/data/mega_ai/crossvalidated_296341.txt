[site]: crossvalidated
[post_id]: 296341
[parent_id]: 223420
[tags]: 
An RNN should also has a time dimension or sequence length for its input. In your example, the sequence length is only 1 for each training sequence so the total time step for each training is only one. That means there is basically no long-term memory that can be built on this network, and you are not using the feature of RNN (especially LSTM). To do it correctly, you need to increase the sequence length in each training sequence (like t_1, t_2, t_3, t_4 ) so that the pattern can be seen in one input. You will need to decide how to cut the entire time series into several pieces with fixed length. If you want the network to learn an indefinite length of sequence like the example you gave, you will need additional setting to make it work. In keras, you should use the stateful mode with batch_size = 1 so that the neural network can still remember the state of previous example. The shuffle needed to turn off so that the order is preserved during the training. Finally, you need to manually reset the state of the neural network model.reset_states() after each epoch of training (since in this case 1 epoch is 1 visit through the time series). See Jason Brownlee's blog for a detail example.
