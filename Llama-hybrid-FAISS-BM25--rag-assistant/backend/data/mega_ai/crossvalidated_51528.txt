[site]: crossvalidated
[post_id]: 51528
[parent_id]: 51501
[tags]: 
The problem you're referring to, often called data sparsity, arises often in language modeling. In particular, if some vocabulary words don't appear in your training corpus then maximum likelihood techniques will lead to a learned model that assigns $0$ probability to observing those words. In language modeling, some type of smoothing method is used on the paramater estimates to solve this problem. One option, perhaps the most principled one, is to put Dirichlet priors on the probability vectors you wish to infer and perform Bayesian inference. Loosely speaking, this essentially has the effect of adding some "pseudo observations" of every vocabulary item to your training set.
