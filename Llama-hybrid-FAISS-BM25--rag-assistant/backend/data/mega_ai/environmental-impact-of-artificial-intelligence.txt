The environmental impact of artificial intelligence includes substantial electricity consumption for training and using deep learning models, and the related carbon footprint and water usage. Moreover, the artificial intelligence (AI) data centers are materially intense, requiring a large amount of electronics that use specialized mined metals and which eventually will be disposed as e-waste. Some scientists argue that AI may also provide solutions to environmental problems, such as material innovations, improved grid management, and other forms of optimization across various fields of technology. As the environmental impact of AI becomes more apparent, governments have begun instituting policies to improve the oversight and review of environmental issues that could be associated with the use of AI, and related infrastructure development. Carbon footprint AI has a significant carbon footprint due to growing electricity consumption, especially due to training and usage. Researchers have argued that the carbon footprint of AI models during training should be considered when attempting to understand the impact of AI. One study suggested that by 2027, energy costs for AI could increase to 85–134 Twh, nearly 0.5% of all current electricity usage. Training large language models (LLMs) and other generative AI generally requires much more electricity compared to running a single prediction on the trained model. Using a trained model repeatedly, though, may easily multiply the electricity costs of predictions. The computation required to train the most advanced AI models doubles every 3.4 months on average, leading to exponential power usage and resulting carbon footprint.Additionally, artificial intelligence algorithms running in places predominantly using fossil fuels for energy will exert a much higher carbon footprint than places with cleaner energy sources. These models may be modified for less environmental impacts at the cost of accuracy, emphasizing the importance of finding the balance between accuracy and environmental impact. BERT, a language model trained in 2019, required "the energy of a round-trip transcontinental flight" to train. GPT-3 released 552 metric tons of carbon dioxide into the atmosphere during training, "the equivalent of 123 gasoline-powered passenger vehicles driven for one year". Much of the energy cost is due to inefficient model architectures and processors. One model named BLOOM, from Hugging Face, trained with more efficient chips and, therefore, only released 25 metric tons of CO2. Incorporating the energy cost of manufacturing the chips for the system doubled the carbon footprint, to "the equivalent of around 60 flights between London and New York." Operating BLOOM daily was estimated to release the equivalent carbon footprint as driving 54 miles. Algorithms which have lower energy costs but run millions of times a day can also have significant carbon footprints. The integration of AI into search engines could multiply energy costs significantly. Another estimate found that integrating ChatGPT into every Google search query would use 10 TWh each year, the equivalent yearly energy usage of 1.5 million European Union residents. Once the model is trained, it consumes significantly less energy, however it still requires a high amount of electricity. Researchers have estimated that a ChatGPT query consumes about five times more electricity than a simple web search. In June 2025, OpenAI executive Sam Altman stated that the average ChatGPT query used about 0.34 Wh (1.2 kJ) of electricity and 8.5×10−5 US gal (0.32 ml) of water. Increased computational demands from AI caused both increased water and energy usage, leading to significantly more demands on the grid. Due to increased energy demands from AI-related projects, coal-fired plants in Kansas Cityand West Virginia pushed back closing. Other coal-fired plants in the Salt Lake City region have pushed back retirement of their coal-fired plants by up to a d