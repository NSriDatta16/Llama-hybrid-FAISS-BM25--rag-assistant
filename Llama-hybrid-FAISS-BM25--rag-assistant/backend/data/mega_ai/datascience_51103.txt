[site]: datascience
[post_id]: 51103
[parent_id]: 51100
[tags]: 
You can make the feature extraction to an intermediate model in keras. It could look something like this: # Build a model for only the feature extraction layers feature_extractor = Sequential() feature_extractor.add(Conv1D(15,60,padding='valid', activation='relu',input_shape=(18000,1), strides = 1, kernel_regularizer=regularizers.l1_l2(l1=0.1, l2=0.1))) feature_extractor.add(MaxPooling1D(2,data_format='channels_last')) feature_extractor.add(Dropout(0.6)) feature_extractor.add(BatchNormalization()) feature_extractor.add(Conv1D(30, 60, padding='valid', activation='relu',kernel_regularizer = regularizers.l1_l2(l1=0.1, l2=0.1), strides=1)) # Keep adding new layers for prediciton outside of feature extraction model x = feature_extractor.output x = MaxPooling1D(4,data_format='channels_last')(x) x = Dropout(0.6)(x) x = BatchNormalization()(x) x = Flatten()(x) prediction_layer = Dense(3, activation = 'softmax')(x) # Make a new model combining both cnn_model=Model(inputs=feature_extractor.input, outputs=prediction_layer) cnn_model.compile(optimizer=opt,loss=loss) Then you train using the full cnn_model but you call predict only on your feature extraction model with feature_extractor.predict(X) . Then you can use the output of the prediction to train your decision tree like this: # Train full network, both feature extractor and softmax part cnn_model.fit(X, y_one_hot) # y needs to be one hot for keras # Predict only the output of the feature extraction model X_ext = feature_extractor.predict(X) dtc = DecisionTreeClassifier(criterion = 'entropy') # Train the decision tree on the extracted features dtc.fit(X_ext, y) # y should be one-dimensional for sklearn
