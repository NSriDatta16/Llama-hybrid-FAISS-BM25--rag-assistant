[site]: crossvalidated
[post_id]: 571254
[parent_id]: 570512
[tags]: 
The problem is one of over-fitting the model selection criterion, see my paper (with Mrs Marsupial) G. C. Cawley and N. L. C. Talbot, Over-fitting in model selection and subsequent selection bias in performance evaluation, Journal of Machine Learning Research, 2010. Research, vol. 11, pp. 2079-2107, July 2010. ( pdf ) Essentially if we make model choices by minimising a model selection criterion based on a finite amount of data, then that minimisation may involve changes that produce genuine improvements in generalisation performance, but it can also involve changes that are exploiting the particular sample of data and do not make performance better (and may make it worse). This is analogous to over-fitting the training data, but at the next level up. So one reason not to use the test data to select the model is that it will give an optimistically biased performance estimate. However, it may be that you don't actually need that for your application. If you have to make choices between models that have a lot of hyper-parameters (and hence a greater risk of over-fitting the model selection criterion on the validation set), then you may want to use the test set (which will then be optimistically biased), or a second validation set, for choosing the final model. However, if the number of hyper-parameters is small, this is unlikely to be necessary and you can just use a single validation set. See J Wainer, G Cawley, Nested cross-validation when selecting classifiers is overzealous for most practical applications, Expert Systems with Applications 182, 115222 ( pdf )
