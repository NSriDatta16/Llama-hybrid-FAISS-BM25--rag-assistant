[site]: crossvalidated
[post_id]: 591516
[parent_id]: 423195
[tags]: 
In general, well you could. You can always write a code in your env.step(a) function that takes any continuous representation of your action and discretisze it. This modification is transparent to DDPG. As far the agent is considered, it just deals with a very peculiar kind of environment. The question, is would it be helpful? Answer: Depending on the representation of the action, it might help a lot . This is particularly useful if your action vector is very large but it factorizes. As an example, imagine that you need to output an integer in [1, 2^K] . A discrete space treatment would require 2^K outputs which becomes prohibitly expensive even with moderate K values. However, you can re-structure your problem so that your DDPG's policy network outputs a K -sized vector with each element in (0,1), which is then discretized by the environment to {0,1}^K and then converted to the integer value. Obviously, you could have many more factorizations (or econdings/decondings) of your action.
