[site]: crossvalidated
[post_id]: 95628
[parent_id]: 95428
[tags]: 
Although Deep Belief Networks (DBNs) and Deep Boltzmann Machines (DBMs) diagrammatically look very similar, they are actually qualitatively very different. This is because DBNs are directed and DBMs are undirected. If we wanted to fit them into the broader ML picture we could say DBNs are sigmoid belief networks with many densely connected layers of latent variables and DBMs are markov random fields with many densely connected layers of latent variables. As such they inherit all the properties of these models. For example, in a DBN computing $P(v|h)$ , where $v$ is the visible layer and $h$ are the hidden variables is easy. On the other hand computing $P$ of anything is normally computationally infeasible in a DBM because of the intractable partition function. That being said there are similarities. For example: DBNs and the original DBM work both using initialization schemes based on greedy layerwise training of restricted Bolzmann machines (RBMs), They are both "deep". They both feature layers of latent variables which are densely connected to the layers above and below, but have no intralayer connections, etc. References "Multiview Machine Learning" by Shiliang Sun, Liang Mao, Ziang Dong, Lidan Wu
