[site]: crossvalidated
[post_id]: 489454
[parent_id]: 
[tags]: 
Can a neural network represent exactly a linear regression?

Let's say we have a neural network with one dense hidden layer with two nodes and there are 4 inputs $x_1, x_2, x_3, x_4$ $y = g(w_{11}f(w_{11}x_1 + w_{21}x_2 + w_{31}x3 + w_{41}x4) + w_{21}f(w_{12}x_1 + w_{22}x_2 + w_{32}x3 + w_{42}x4))$ where $f$ is the activation function for the hidden layer and g is the activation function of the output the weight $w_{ij}$ are subscripted such that the $i$ refers to the input node's number from the previous layer and the $j$ refers to the current layer's node label. My question is this: Clearly if we set $w_{21} = 0$ and let f and g be the identity functions ( $f(x) = x, g(x) = x$ ) then you get $y = w_{11}x_1 + w_{21}x_2 + w_{31}x3 + w_{41}x4$ which is clearly just reducing to a simple linear regression. However, if you want to represent interaction terms of say second order, like $ y = ax_1x_2 + bx_2x_4 + cx_1 + dx_2 +$ etc. has it been shown that a neural network can be made which is mathematically equivalent to the above?? forgive me if there is well known proof/disproof of this conjecture.
