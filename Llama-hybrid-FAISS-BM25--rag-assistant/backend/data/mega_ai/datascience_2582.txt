[site]: datascience
[post_id]: 2582
[parent_id]: 
[tags]: 
Consequence of Feature Scaling

I am currently using SVM and scaling my training features to the range of [0,1]. I first fit/transform my training set and then apply the same transformation to my testing set. For example: ### Configure transformation and apply to training set min_max_scaler = MinMaxScaler(feature_range=(0, 1)) X_train = min_max_scaler.fit_transform(X_train) ### Perform transformation on testing set X_test = min_max_scaler.transform(X_test) Let assume that a given feature in the training set has a range of [0,100], and that same feature in the testing set has a range of [-10,120]. In the training set that feature will be scaled appropriately to [0,1], while in the testing set that feature will be scaled to a range outside of that first specified, something like [-0.1,1.2]. I was wondering what the consequences of the testing set features being out of range of those being used to train the model? Is this a problem?
