[site]: crossvalidated
[post_id]: 159975
[parent_id]: 159957
[tags]: 
Each hypothesis function that outputs the result of logistic regression ( $h_\theta (x)$ ) corresponds to the probability that the inputted feature vector $\vec{x}$ is either a $1$ or a $0$ (where 1 is the 'yes'/'true'/etc. term.) So now that you realize that each model you're creating outputs the probability that you want to know what a 'tie' means. If 2 models both output 0.4 as the probability that the inputted feature vector is part of that class, you could say that between the two the answer is indeterminate, or rather that input $\vec{x}$ is equally likely to be in 'class 1' or 'class 2' (assuming $P(y=1|x;\theta)=P(y=2|x;\theta)=\phi$ where $\phi$ is some number, which happened to be 0.4 in this example) You can think of this visually as regressing a Bernoulli (only 2 outcomes, or regular logistic regression if you want to think of it that way) distribution and having an outcome of 0.5. (source: sourceforge.net ) When the outcome is 0.5 from a logistic model, as with multiclass models, the inputs lie on the line of division that separates the classes, again, they are equally likely to be part of either class. This generalization is true for more classes than 2.
