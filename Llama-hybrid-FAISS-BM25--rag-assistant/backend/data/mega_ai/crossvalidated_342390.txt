[site]: crossvalidated
[post_id]: 342390
[parent_id]: 342387
[tags]: 
The sampler function implements the reparameterization trick, it takes in the mean $\mu$ and standard deviation $\sigma$ and outputs a sample $z$ from the corresponding Gaussian, it follows $z=\mu + \sigma\epsilon$ and is differentiable wrt $\mu$ and $\sigma$. Lambda is just a layer in Keras that allows you to customize a layer by passing in a function as parameter, so Lambda(sampler) instantiates a layer of the sampler function since there's no built-in sampler layer in Keras. The KL divergence term is as part of the loss function in VAE definition.
