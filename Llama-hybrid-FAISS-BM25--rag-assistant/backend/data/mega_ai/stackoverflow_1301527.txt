[site]: stackoverflow
[post_id]: 1301527
[parent_id]: 1301149
[tags]: 
It is possible that your bottleneck might be due to excessive memory use. Consider using iteritems to leverage the power of generators. Since you say your data is sparse, that will probably not be the most efficient. Consider this alternate usage of iterators: dicts = ... #Assume this is your dataset totals = {} lengths = {} means = {} for d in dicts: for key,value in d.iteritems(): totals.setdefault(key,0) lengths.setdefault(key,0) totals[key] += value length[key] += 1 for key,value in totals.iteritems(): means[key] = value / lengths[key] Here totals, lengths, and means are the only data structures you create. This ought to be fairly speedy, since it avoids having to create auxiliary lists and only loops through each dictionary exactly once per key it contains. Here's a second approach that I doubt will be an improvement in performance over the first, but it theoretically could, depending on your data and machine, since it will require less memory allocation: dicts = ... #Assume this is your dataset key_set = Set([]) for d in dicts: key_set.update(d.keys()) means = {} def get_total(dicts, key): vals = (dict[key] for dict in dicts if dict.has_key(key)) return sum(vals) def get_length(dicts, key): vals = (1 for dict in dicts if dict.has_key(key)) return sum(vals) def get_mean(dicts,key): return get_total(dicts,key)/get_length(dicts,key) for key in key_set: means[key] = get_mean(dicts,key) You do end up looping through all dictionaries twice for each key, but need no intermediate data structures other than the key_set.
