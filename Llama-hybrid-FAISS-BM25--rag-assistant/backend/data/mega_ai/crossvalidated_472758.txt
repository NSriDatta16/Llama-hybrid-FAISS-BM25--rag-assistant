[site]: crossvalidated
[post_id]: 472758
[parent_id]: 468707
[tags]: 
Your implementation is ok. I cannot see on the webpage you referred to, where the difference in mtry comes from. I will not be surprised if you end up with slightly different mtry if you train on full data or just on a training part of the data like you did. Most likely if you check the difference in accuracy / ROC or whichever metric used, the difference is small. I hope it helps to think like this. Normally, after obtaining the optimal hyperparameter, you would fit the model on the whole training set, and check its performance on the test. The test dataset is not used at all in the training, so it's good to see whether your model overfits or underfits, or has predictive power. This becomes useful when you want to compare between different models, for example if you want to see whether svm performs better than random forest for your data above.
