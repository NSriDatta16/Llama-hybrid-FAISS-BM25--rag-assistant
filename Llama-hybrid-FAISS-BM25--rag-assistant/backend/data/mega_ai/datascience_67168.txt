[site]: datascience
[post_id]: 67168
[parent_id]: 
[tags]: 
Using a neural network to learn regression in image processing

I have a camera system with some special optics that warp the field of view of the camera, dependent on two variables, $\theta_1$ and $\theta_2$ . Given a specific configuration of these two variables, each pixel on my camera (which is 500x600 resolution) will see a specific coordinate on a screen in front of the camera. I can calculate this for each pixel, but it requires too many computations and is too slow. So, I want to learn a model that fits this function, but computes much faster. I have plenty of input/output data that I have generated, mapping the 500x600 input points to the 500x600 output points for different $\theta_i$ values, and I have already used some 2D polynomial least squares regression to learn these functions. They perform adequately, but I was wondering if a neural net could be used to learn a better function. My question comes down to this: can a neural network learn what basically amounts to a regression problem trying to learn $f_{\theta_1,\theta_2}(px_1,px_2)=(a_1,b_1)$ ? I know that neural nets excel in classification problems, which this is not, but I have also heard that a neural network can "learn arbitrary functions."
