[site]: datascience
[post_id]: 82157
[parent_id]: 24511
[tags]: 
Since the SGD algorithm selects the subset of instances randomly, it is quite possible that it may take few instances many numbers of time per epoch, which may bring the cost function to a global minima. If training instances are shuffled then the chances of selecting repeating instances is much less. Source of inforamation: handson machine learning with scikit learn keras and tensorflow.
