[site]: crossvalidated
[post_id]: 582660
[parent_id]: 
[tags]: 
Given that conditional probabilities are primitives in Bayesian formalism, is $P(A,B)=P(A|B)P(B)$ a definition of joint distribution?

In Probabilistic Reasoning For Intelligent Systems by Judea Pearl , section 2.1.1, page 31, it is mentioned that in Bayesian formalism conditional probabilities are not defined in terms of joint distribution like $P(A|B)=\frac{P(A,B)}{P(B)}$ . Rather they are taken as primitives present in our knowledge-base just like $P(B)$ , say. In fact, it is the joint distribution which, if at all necessary, is calculated as $P(A,B)=P(A|B)P(B)$ . What then the statement $P(A,B)=P(A|B)P(B)$ denote? - a definition of joint distributions in the Bayesian formalism? Edit 1: Quoting another portion from the book, section 2.1.1, page 33 - $P(A|B)=\frac{P(A,B)}{P(B)}$ and $P(B|A)=\frac{P(A,B)}{P(A)}$ (2.14) ...while the mathematician views conditional probabilities as mathematical constructs, as in Eq. (2.14), the Bayes adherent views them as primitives of the language and as faithful translations of the English expression "..., given that I know A". Accordingly, Eq. (2.14) is not a definition but an empirically verifiable relationship between English expressions. Adding to my original query, what sort of an empirical verification can there be for Eq. 2.14 given above?
