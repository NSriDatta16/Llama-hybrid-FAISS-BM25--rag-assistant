providing more information to learn for the distilled model compared to hard targets, and at the same time reducing the variance of the gradient between different records, thus allowing a higher learning rate. If ground truth is available for the transfer set, the process can be strengthened by adding to the loss the cross-entropy between the output y i ( x | 1 ) {\displaystyle y_{i}(\mathbf {x} |1)} of the distilled model computed with t = 1 {\displaystyle t=1} , and the known label y ¯ i {\displaystyle {\bar {y}}_{i}} E ( x | t ) = − t 2 ∑ i y ^ i ( x | t ) log ⁡ y i ( x | t ) − ∑ i y ¯ i log ⁡ y i ( x | 1 ) {\displaystyle E(\mathbf {x} |t)=-t^{2}\sum _{i}{\hat {y}}_{i}(\mathbf {x} |t)\log y_{i}(\mathbf {x} |t)-\sum _{i}{\bar {y}}_{i}\log y_{i}(\mathbf {x} |1)} where the component of the loss with respect to the large model is weighted by a factor of t 2 {\displaystyle t^{2}} since, as the temperature increases, the gradient of the loss with respect to the model weights scales by a factor of 1 t 2 {\displaystyle {\frac {1}{t^{2}}}} . Relationship with model compression Under the assumption that the logits have zero mean, it is possible to show that model compression is a special case of knowledge distillation. The gradient of the knowledge distillation loss E {\displaystyle E} with respect to the logit of the distilled model z i {\displaystyle z_{i}} is given by ∂ ∂ z i E = − ∂ ∂ z i ∑ j y ^ j log ⁡ y j = − ∂ ∂ z i y ^ i log ⁡ y i + ( − ∂ ∂ z i ∑ k ≠ i y ^ k log ⁡ y k ) = − y ^ i 1 y i ∂ ∂ z i y i + ∑ k ≠ i ( − y ^ k ⋅ 1 y k ⋅ e z k t ⋅ ( − 1 ( ∑ j e z j t ) 2 ) ⋅ e z i t ⋅ 1 t ) = − y ^ i 1 y i ∂ ∂ z i e z i t ∑ j e z j t + ∑ k ≠ i ( y ^ k ⋅ 1 y k ⋅ y k ⋅ y i ⋅ 1 t ) = − y ^ i 1 y i ( 1 t e z i t ∑ j e z j t − 1 t ( e z i t ) 2 ( ∑ j e z j t ) 2 ) + y i ∑ k ≠ i y ^ k t = − y ^ i 1 y i ( y i t − y i 2 t ) + y i ( 1 − y ^ i ) t = 1 t ( y i − y ^ i ) = 1 t ( e z i t ∑ j e z j t − e z ^ i t ∑ j e z ^ j t ) {\displaystyle {\begin{aligned}{\frac {\partial }{\partial z_{i}}}E&=-{\frac {\partial }{\partial z_{i}}}\sum _{j}{\hat {y}}_{j}\log y_{j}\\&=-{\frac {\partial }{\partial z_{i}}}{\hat {y}}_{i}\log y_{i}+\left(-{\frac {\partial }{\partial z_{i}}}\sum _{k\neq i}{\hat {y}}_{k}\log y_{k}\right)\\&=-{\hat {y}}_{i}{\frac {1}{y_{i}}}{\frac {\partial }{\partial z_{i}}}y_{i}+\sum _{k\neq i}\left(-{\hat {y}}_{k}\cdot {\frac {1}{y_{k}}}\cdot e^{\frac {z_{k}}{t}}\cdot \left(-{\frac {1}{\left(\sum _{j}e^{\frac {z_{j}}{t}}\right)^{2}}}\right)\cdot e^{\frac {z_{i}}{t}}\cdot {\frac {1}{t}}\right)\\&=-{\hat {y}}_{i}{\frac {1}{y_{i}}}{\frac {\partial }{\partial z_{i}}}{\frac {e^{\frac {z_{i}}{t}}}{\sum _{j}e^{\frac {z_{j}}{t}}}}+\sum _{k\neq i}\left({\hat {y}}_{k}\cdot {\frac {1}{y_{k}}}\cdot y_{k}\cdot y_{i}\cdot {\frac {1}{t}}\right)\\&=-{\hat {y}}_{i}{\frac {1}{y_{i}}}\left({\frac {{\frac {1}{t}}e^{\frac {z_{i}}{t}}\sum _{j}e^{\frac {z_{j}}{t}}-{\frac {1}{t}}\left(e^{\frac {z_{i}}{t}}\right)^{2}}{\left(\sum _{j}e^{\frac {z_{j}}{t}}\right)^{2}}}\right)+{\frac {y_{i}\sum _{k\neq i}{\hat {y}}_{k}}{t}}\\&=-{\hat {y}}_{i}{\frac {1}{y_{i}}}\left({\frac {y_{i}}{t}}-{\frac {y_{i}^{2}}{t}}\right)+{\frac {y_{i}(1-{\hat {y}}_{i})}{t}}\\&={\frac {1}{t}}\left(y_{i}-{\hat {y}}_{i}\right)\\&={\frac {1}{t}}\left({\frac {e^{\frac {z_{i}}{t}}}{\sum _{j}e^{\frac {z_{j}}{t}}}}-{\frac {e^{\frac {{\hat {z}}_{i}}{t}}}{\sum _{j}e^{\frac {{\hat {z}}_{j}}{t}}}}\right)\\\end{aligned}}} where z ^ i {\displaystyle {\hat {z}}_{i}} are the logits of the large model. For large values of t {\displaystyle t} this can be approximated as 1 t ( 1 + z i t N + ∑ j z j t − 1 + z ^ i t N + ∑ j z ^ j t ) {\displaystyle {\frac {1}{t}}\left({\frac {1+{\frac {z_{i}}{t}}}{N+\sum _{j}{\frac {z_{j}}{t}}}}-{\frac {1+{\frac {{\hat {z}}_{i}}{t}}}{N+\sum _{j}{\frac {{\hat {z}}_{j}}{t}}}}\right)} and under the zero-mean hypothesis ∑ j z j = ∑ j z ^ j = 0 {\displaystyle \sum _{j}z_{j}=\sum _{j}{\hat {z}}_{j}=0} it becomes z i − z ^ i N T 2 {\displaystyle {\frac {z_{i}-{\hat {z}}_{i}}{NT^{2}}}}