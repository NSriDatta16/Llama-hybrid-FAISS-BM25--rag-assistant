[site]: crossvalidated
[post_id]: 77508
[parent_id]: 
[tags]: 
Why are kernel methods with RBFs effective for handwritten digits (letters) classification?

The question emerged while reading Ch. 3 of Rasmussen & Williams . In the end of this chapter, the authors gave results for the problem of handwritten digits classification (16x16 greyscale pictures); features are 256 pixel intensities + bias. I was surprised that in such a high-dimensional problem, 'metric' methods, like Gaussian processes with squared exponential kernel, or SVM with the same kernel, behave quite nice without any dimension reduction preceeded. Also, I heard sometimes that SVM is good for [essentially bag-of-word] text classification. Why aren't they suffering from the curse of dimensionality?
