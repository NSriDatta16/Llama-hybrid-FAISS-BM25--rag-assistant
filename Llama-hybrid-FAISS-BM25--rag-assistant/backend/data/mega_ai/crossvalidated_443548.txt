[site]: crossvalidated
[post_id]: 443548
[parent_id]: 
[tags]: 
Random Forest pruning vs stopping criteria

I have recently noticed that SciKit-Learn now supports Cost Complexity Pruning, which is great. Since this has been implemented, should I still use other regression trees/ random forest hyper-parameters to control tree growth, or just use pruning of fully grown trees? I see that in Classification and Regression Trees by Breiman et al (1984) mentions that one should use Pruning over Stopping? Is this still the case if I want to use a hyper-parameter grid search in SciKit Learn?
