[site]: datascience
[post_id]: 99712
[parent_id]: 99692
[tags]: 
What you mentioned is true, for 99% of Scikit-learn's estimators, the data X must be numeric (I think only HistGradientBoosting works with no numerical categorical data) So when working with mixed data types in modeling, Pipelines + ColumnTransformers the answer always is. Try something like this and it would worked no matter the type you have: # You may want to change the preprocessing steps for both numerical and categorical from sklearn.tree import DecisionTreeClassifier from sklearn.pipeline import Pipeline from sklearn.compose import make_column_transformer, make_column_selector as selector from sklearn.preprocessing import StandardScaler from sklearn.impute import SimpleImputer cont_prepro = Pipeline([("imputer",SimpleImputer(strategy = "median")),("scaler",StandarScaler())]) cat_prepro = Pipeline([("imputer",SimpleImputer(strategy = "most_frequent")),("encoder",OneHotEncoder(handle_unknown = "ignore"))]) preprocessing = make_column_transformer((cont_prepro,selector(dtype_exclude = "object")),(cat_prepro,selector(dtype_include = "object")) pipe = Pipeline([("preprocessing",preprocessing),("model", DecisionTreeClassifier())]) pipe.fit(X_train, y_train) If dimensionality of your input matrix is a concern, you can even include a feature selector inside the pipeline like: # adjust the parameters of kbest from feature_selection import SelectKBest pipe = Pipeline([("preprocessing",preprocessing),("selector",SelectKBest()), ("model", DecisionTreeClassifier())]) Just be careful if you want to use PCA, you should only use this in the numerical features, not in the categorical ones
