[site]: datascience
[post_id]: 43333
[parent_id]: 36350
[tags]: 
It is really good to have the intuition about the model & features. But practically it is recommended to follow feature selection procedures. However, I am unsure of what is best practice. How do I go about determining whether this is a feature that can be removed or not? A limited common sense or intuition can be applied here, we will obviously know "mobile number" of the candidate is of no use (It has less predicting power). But we may not know whether mode of travel of the candidate will have its effect on interview presence (Based on the location & commutability). It is necessary to feature engineer (running feature selection algorithms) your data to find out what are some effective features. Look here for the methods involved. I found tree based selection algos worked well for me. But it really depends on the data. Secondly, is there a downside to keeping it even if it has limited predictive power? Aside from creating a more cumbersome model, could it adversely impact the accuracy of a prediction? It is always better to avoid features which are of less/no use to improve the model accuracy unless you have very minimal no of features which is effective. And it also helps you to train your model faster because of dimensionality reduction. Refer Scikit-learn doc for examples
