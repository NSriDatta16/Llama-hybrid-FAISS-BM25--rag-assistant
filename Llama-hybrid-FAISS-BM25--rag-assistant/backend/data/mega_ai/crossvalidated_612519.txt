[site]: crossvalidated
[post_id]: 612519
[parent_id]: 
[tags]: 
Surprising disparity between Confusion matrix values and AUC?

I'm working on getting a read out of a Logistic regression classification model (setup in Python via Scikit-learn's LogisticRegression() wrapped in a OneVsRestClassifier()). I got the confusion matrix running pretty quick, and after a decent amount of effort I got the PR Curve (with a lot of help from https://stackoverflow.com/questions/29656550/how-to-plot-pr-curve-over-10-folds-of-cross-validation-in-scikit-learn ) The Algorithm consists of doing KFoldStratified, balancing across the 6 classes present, and doing Leave-one-out cross validation. My test set has a single example of each label in the X and y that gets fed in. The Confusion Matrix is generated based on that, and then I use clf.predict(X_test) to generate y probabilities. I separate them into independent lists per label, then I use 'precision_recall_curve' to calculate precision and recall on the combined list per class, then the list containing everything. Below is the Confusion Matrix and PR Curves I've generated. I don't understand how class 2, for instance, has a seeming perfect classification on the confusion matrix while having a near 0.5 AUC. I'm definitely only using the testing data to calculate both. Any ideas?
