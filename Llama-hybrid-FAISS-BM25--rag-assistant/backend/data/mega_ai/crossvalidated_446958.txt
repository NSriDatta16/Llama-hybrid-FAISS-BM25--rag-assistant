[site]: crossvalidated
[post_id]: 446958
[parent_id]: 446301
[tags]: 
When we're talking about the estimated parameters of a neural network , a "bias" is any constant that's added to an input. Consider logistic regression, i.e. a neural network without hidden layers and a single, sigmoidal output. This network has the prediction equation $$ \hat{y} = \sigma(w^\top x+b) $$ where $x$ is the input vector, $w$ is the vector of weights and $b$ is the bias . The function $\sigma$ yields probabilities as its output: $0 . For this model, it should be obvious that increasing $b$ while keeping all else equal increases the predicted probability; contrariwise, decreasing $b$ does the opposite. However, the situation becomes more complex when we consider a neural network with hidden layers. This is because changing the bias for a node at the "beginning" or "middle" of the network can increase or decrease the predicted probability, depending on what happens "downstream" of the unit. Finally, the utility of class weights in neural networks is not as simple as presented in your TensorFlow tutorial. The intuition that we have about instance weights in simple settings like logistic regression do not hold when using over-parameterized neural networks. Indeed, neural networks are flexible enough that they can effectively overcome instance weighting. Jonathon Byrd, Zachary C. Lipton. " What is the Effect of Importance Weighting in Deep Learning? " Importance-weighted risk minimization is a key ingredient in many machine learning algorithms for causal inference, domain adaptation, class imbalance, and off-policy reinforcement learning. While the effect of importance weighting is well-characterized for low-capacity misspecified models, little is known about how it impacts over-parameterized, deep neural networks. Inspired by recent theoretical results showing that on (linearly) separable data, deep linear networks optimized by SGD learn weight-agnostic solutions, we ask, for realistic deep networks, for which many practical datasets are separable, what is the effect of importance weighting? We present the surprising finding that while importance weighting impacts deep nets early in training, so long as the nets are able to separate the training data, its effect diminishes over successive epochs. Moreover, while L2 regularization and batch normalization (but not dropout), restore some of the impact of importance weighting, they express the effect via (seemingly) the wrong abstraction: why should practitioners tweak the L2 regularization, and by how much, to produce the correct weighting effect? We experimentally confirm these findings across a range of architectures and datasets. If by "bias" you only mean the ultimate bias applied in the final network layer, it should be clear that you have a choice to achieve a desired trade-off between classification errors: You can adjust the final bias in the neural network to move all predicted probabilities up or down. This will distort the predicted probabilities. The caveat to focusing on hard classifications is that you're discarding all of the fine-grained information conveyed by a predicted probability. Frank Harrell has written a number of posts here and elsewhere about the dangers of using hard-and-fast cutoffs. You can leave the final bias at whatever value minimizes the loss, and instead choose a different threshold to transform predicted probabilities into class designations. That is, instead of having the rule "If $\hat{y} > 0.5$ , predict 'This is an image of a dog,'" you can use the rule "If $\hat{y} > 0.9$ , predict 'This is an image of a dog.'" This option does not distort the predicted probabilities, and instead only changes which observations are allocated to which class. In light of the foregoing, my recommendation is to not use class weights. Let the optimizer find the optimal value of your weights and biases. Ideally, the decisions you make should result from a contextual consideration of the risks involved: if you're wrong, is a healthy person taking an unnecessary course of antibiotics? Or is a healthy person having a limb amputated? Or is a sick person missing out on a treatment? Is the treatment critical to their life, or is it too risky considering their overall health, age and other infirmities? If you need some sort of a hard decision (perhaps because you're creating an automation system requiring minimal human intervention), then you should pick a decision threshold which achieves your desired trade-off of correct predictions against incorrect predictions . That is, any time you're making a decision on the basis of a machine learning model's prediction, you're at risk of either a false positive or false negative. You should carefully assess the risks to your organization of both types of errors, and choose a threshold which balances the costs of both types of errors. In some problems, this means you'll tolerate a higher false positive rate in order to find as many positives as possible; in others, you'll tolerate a higher false negative rate in order that the positives that you uncover are "high confidence" targets.
