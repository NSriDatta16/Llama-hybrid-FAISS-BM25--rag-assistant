[site]: crossvalidated
[post_id]: 268027
[parent_id]: 
[tags]: 
With an LSTM, with training samples on 0->250, should it be able to extrapolate to unseen data(e.g. 250->500)?

I'm currently training on a simple dataset: Training: [0,1,2,3,4,5...250] Test: [251-500] My training cost and expected output decreases and seems correct. However, when I test the model, my network doesn't seem to be able to predict sequences higher than 250. Is there an issue with my model, perhaps not deep enough? class Model: def __init__(self, n_steps, n_hidden, batch_size, feature_len=1): self.x = tf.placeholder(tf.float32, [batch_size, n_steps, feature_len]) self.y = tf.placeholder(tf.float32, [batch_size, 1]) self.lstm = tf.contrib.rnn.BasicLSTMCell(n_hidden) state = self.lstm.zero_state(batch_size, tf.float32) self.w = tf.Variable(tf.random_normal([n_hidden, 1])) self.b = tf.Variable(tf.random_normal([1])) outputs = [] with tf.variable_scope("lstm") as scope: for i in range(n_steps): if i > 0: scope.reuse_variables() output, state = self.lstm(self.x[:, i, :], state) outputs.append(output) self.outputs = outputs self.final_state = state self.final_output = self.outputs[-1] self.result = tf.matmul(self.final_output, self.w) + self.b self.cost = tf.sqrt(tf.reduce_mean(tf.square(self.y[0] - self.result[0]))) opt = tf.train.AdamOptimizer(learning_rate=0.001) gvs = opt.compute_gradients(self.cost) capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs] self.optimizer = opt.apply_gradients(capped_gvs) n_epochs = 2000 n_steps = 5 batch_size = 1 feature_len = 1 data = load_data('add1.txt', seq_len=n_steps, batch_size=batch_size, feature_len=feature_len, normalise_window=False) print(data['x_train'][0]) m = Model(n_steps=n_steps-1, n_hidden=50, batch_size=batch_size) init = tf.global_variables_initializer() with tf.Session() as sess: sess.run(init) x,y = sess.run([m.y,m.result], feed_dict={m.x:data['x_train'][0], m.y: data['y_train'][0]}) print(x,y,x-y) for step in range(n_epochs): for batch_idx in range(data['x_train'].shape[0]): out = sess.run(m.optimizer, feed_dict={m.x:data['x_train'][batch_idx], m.y: data['y_train'][batch_idx]}) # out = sess.run(outputs) # tf.Print(outputs, [outputs], message="TESTING!!") if step % 100 == 0: cost,y,x = sess.run([m.cost,m.result, m.x], feed_dict={m.x:data['x_train'][batch_idx], m.y: data['y_train'][batch_idx]}) print("Cost\n",cost,'\nPredicted\n',y,'\nInput\n',x) # cost, expected,actual = sess.run([m.cost, m.y, m.result], feed_dict={m.x:data['x_train'][batch_idx], m.y: data['y_train'][batch_idx]}) # print("Expected\n",expected,'\nActual\n',actual, '\nCost\n',cost) for i in range(data['x_test'].shape[0]): cost, expected,actual = sess.run([m.cost, m.y, m.result], feed_dict={m.x:data['x_test'][i], m.y: data['y_test'][i]}) print("Expected\n",expected,'\nActual\n',actual, '\nCost\n',cost, data['x_test'][i])
