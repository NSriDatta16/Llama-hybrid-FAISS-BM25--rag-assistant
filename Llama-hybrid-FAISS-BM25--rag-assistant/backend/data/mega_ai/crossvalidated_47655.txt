[site]: crossvalidated
[post_id]: 47655
[parent_id]: 47590
[tags]: 
The following explanation is taken from the book: Neural Networks for Pattern Recognition by Christopher Bishop. Great book! Assume you have previously whitened the inputs to the input units, i.e. $$ = 0$$ and $$ = 1$$ The question is: how to best choose the weights?. The idea is to pick values of the weights at random following a distribution which helps the optimization process to converge to a meaningful solution. You have for the activation of the units in the first layer, $$y = g(a) $$ where $$ a = \sum_{i=0}^{d}w_{i}x_{i}$$. Now, since you choose the weights independently from the inputs, $$ = \sum_{i=0}^{d} = \sum_{i=0}^{d} = 0$$ and $$ = \left = \sum_{i=0}^{d} = \sigma^{2}d $$ where sigma is the variance of the distribution of weights. To derive this result you need to recall that weights are initialized independently from each other, i.e. $$ = \delta_{ij}$$
