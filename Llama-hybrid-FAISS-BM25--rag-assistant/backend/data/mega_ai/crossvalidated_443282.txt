[site]: crossvalidated
[post_id]: 443282
[parent_id]: 
[tags]: 
Can precision and recall of a DNN trained on human-labeled data be higher than precision and recall of the humans who labeled the data?

I was discussing Deep Learning with an academic statistician, who was criticizing the field as "lacking scientific rigor, overhyped and delivering results which are way worse than claimed". In particular, we were discussing whether a DNN classifier could be more accurate than the human experts who labeled the dataset on which it had been trained. The statistician dismissed this occurrence as "obviously impossible", and dismissed any such claims from the Machine Learning community as a clear indication that "you guys don't know what you're talking about [sic]". I replied that I couldn't see any mathematical argument for why a ML classifier couldn't in theory match the Bayes rate $^*$ , or, put it differently, why a classifier trained on a training set $T$ should always be less accurate, on a test set $E$ , than the human(s) who labeled $T$ . All humans (experts are no exception) can and do make mistakes in labeling data, and it's entirely possible that the classifier could learn to "ignore" label noise. However, he countered that "given noisy labels, we [statisticians] know how to estimate the optimal error rate using inter-annotator error estimation [ this quote may not be verbatim...I'm not 100% sure what he was referring to, but I think I got the main idea ], and once this is done properly, there's no way a ML classifier could ever beat our best estimate of human accuracy". Who is right? I think there are actually two different issues here. Sometimes, the human experts who labeled the data and the humans we compare against are different. Consider this Nature paper which claims superior precision and recall (with respect to human experts) for malignant cancer detection in breast cancer mammography: Compared to the first reader, the AI system demonstrated a statistically significant improvement in absolute specificity of 1.2% (95% confidence interval (CI) 0.29%, 2.1%; $P$ =0.0096 for superiority) and an improvement in absolute sensitivity of 2.7% (95% CI âˆ’3%, 8.5%; $P$ = 0.004 for non-inferiority at a pre-specified 5% margin; [..] Compared to the typical reader, the AI system demonstrated statistically significant improvements in absolute specificity of 5.7%(95% CI 2.6%, 8.6%; $P$ $P$ Reading the rest of the paper, it seems that the mammographies had not been labeled using the readers' (radiologists) opinions, but using the outcomes of biopsies, performed later in time. Thus, the classifier was trained on "non-noisy" labels (assuming the error rate of biopsies is order of magnitudes than the error rate of mammography-based screening). In this case, it's entirely possible that one could train a model able to beat the human reader's precision and recall. "Human experts' accuracy" (or "human experts' precision and recall", for what it matters) is an ill-defined concept. Different experts have different precision and recall, and given the average size of most ML datasets, it's pretty obvious that usually single human annotates the whole dataset. Thus, in order to define a single performance to compare against the classifer's precision and recall, we need to "average" the performances of different humans in some way. I don't know how this averaging is performed, but let's assume (a BIG assumption) that the precision/recall points corresponding to different humans fall on a concave curve (ROC curves are often concave, however this is not an actual ROC curve, but a collection of points from different curves, it may well be not concave...). Then, I think averaging these points would result in a point which is below the curve (not sure how to prove it). What do you think? Who is right? $^*$ : of course, I'm not expecting a ML classifier to actually match the Bayes error rate in practice, but then again, I'm not expecting a human to match it either!
