[site]: crossvalidated
[post_id]: 140293
[parent_id]: 
[tags]: 
bias of p-value analysis in a not well fitting model

In the report http://www.stat.berkeley.edu/~breiman/wald2002-3.pdf Breiman says: Three decades ago many statisticians and quantitative social scientists were enamored of multilinear regression and its theory of hypothesis testing on the coefficients. Every statistical package had a regression program variable selection program based on F-to delete and F to enter. It was almost impossible to get a paper published unless you showed that a certain coefficient was signifigant at the 5% level. This was regardless of how well the linear model fit the data and little effort was made to find out. Many conclusions were undoubtedly wrong, and I don't think statisticians now-a-days dispute the error of these ways I would like to have an example obtained with simulated data where a linear regression show that a coefficient is significant while it is not true. And I would like that the error will arise from the fact that the linear model is not fitting well the data. EDIT: R or python code is welcome EDIT: The example should make use of at least 1000 samples EDIT: I found a possible example with simulated data. n = 1000 x1 = seq(-10, 10, length.out = 1000) x2 = sin(x1)/x1 + rnorm(n = n, mean = 0, sd = 2) y = sin(x1)/x1 + rnorm(n = n, mean = 0, sd = .1) lr = glm(y~x1+x2) #wrong model summary(lr) z = sin(x1)/x1 lr = glm(y~z+x2) #real model summary(lr) plot(x1, y, col='blue') points(x1, x2) points(x1, z)
