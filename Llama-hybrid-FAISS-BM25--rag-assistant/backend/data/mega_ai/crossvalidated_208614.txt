[site]: crossvalidated
[post_id]: 208614
[parent_id]: 
[tags]: 
Understanding Neural Networks Dropout - Visible vs. Hidden

I am trying to understand exactly how the dropout method typically works. I have been looking at the original Hinton paper but I can't seem to pull out this final detail. If I understand correctly, the hidden dropout component randomly assigns the activations for a given layer to 0. So an activation matrix (e.g. 3 nodes) like this: node1 node2 node3 0.543 0.453 0.985 0.758 0.535 0.786 would result in something like this (assume dropout_hidden = 0.5 ) node1 node2 node3 0.543 0 0.985 0.758 0 0 Now with the visible_dropout is it the same approach applied to the input variables where different samples have different variables set to zero or is an entire variable for all samples set to zero? For example: var1 var2 var3 var4 var5 var6 sample1 0.444 0.547 0.876 0.245 0.016 0.168 sample2 0.554 0.875 0.222 0.423 0.876 0.187 sample3 0.668 0.888 0.975 0.111 0.324 0.007 Assuming visible_dropout = 0.5 : random masking? var1 var2 var3 var4 var5 var6 sample1 0 0 0.876 0 0.016 0 sample2 0.554 0 0 0.423 0.876 0 sample3 0 0.888 0.975 0 0.324 0.007 or random variable masking? var1 var2 var3 var4 var5 var6 sample1 0.444 0 0.876 0.245 0 0 sample2 0.554 0 0.222 0.423 0 0 sample3 0.668 0 0.975 0.111 0 0 I think it is the former (random masking) but I can't find a confirmation on this anywhere.
