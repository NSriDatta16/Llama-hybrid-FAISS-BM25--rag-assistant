[site]: crossvalidated
[post_id]: 444507
[parent_id]: 
[tags]: 
What are the current methods to check for GAN overfitting?

In generative modeling, the goal is to find a way for a model to output samples of some distribution $p_X$ given a lot of samples $x_1, \ldots, x_n$ . In particular, we want sampling from our model $G$ to satisfy $G(z)$ is a new example $G(z)$ looks like it was sampled from $p_X$ . GAN's approach this by finding a Nash equilibrium where $p_g=p_X$ , where $p_g$ is the distribution implicitly defined by mapping the latent noise $z$ under $G$ . How do we know that $G$ does not simply memorize the input data? For example, if I train a GAN to output pictures of cats, how do I know that the output isn't just a modified picture of one of the cats that was used to train $G$ ? In the original Deep Convolutional GAN (DCGAN) paper, they have the following explanation which I don't find particularly convincing.
