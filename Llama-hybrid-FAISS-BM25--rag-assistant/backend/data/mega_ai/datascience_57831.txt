[site]: datascience
[post_id]: 57831
[parent_id]: 57804
[tags]: 
First, I feel like crossvalidated SE might be a better fit here. Regardless, the answer comes from some mathematical facts of both AR and MA models. I don't really understand what your link is trying to suggest in its explanation. For AR(p) models, it is a fact that the partial autocorrelation function (PACF) cuts off (abruptly hits 0) after lag p. Thus, analyzing a PACF graph can be useful since if the process follows an AR one, we can possibly identify the order if we see that the pacf graph shows significant partial autocorrelation at the first few lags and then cuts off to insignificant (i.e. not enough evidence to suggest non zero lags) lags after. In a similar way, for MA(q) models the autocorrelation function (ACF), NOT the pacf, cuts off (abruptly hits 0) after lag p. Thus, analyzing a graph of the ACF can be useful for identifying the order of an MA(q) model for reasons very similar to the AR(p) model and the pacf graph. For the AR(p) model, we can show the pacf for lags k > p cut to zero by solving the Yule Walker system of equations (if my memory serves correct). For the MA(q) model, we can make use of the backshift operator and/or spectral analysis (this is how I learned it, anyway) to find the autocorrelation function for lags k > q (and again, see that it is zero for all lags after q). This methodology is part of the "Box-Jenkins" methodology, which too be honest is a bit too subjective for my tastes in the sense that we are effectively eyeballing a graph. Rarely is the case (in my own studies, anyway) do we find a time series that perfectly exhibits these cutoffs and so it becomes tricky when picking orders. I prefer instead (and I believe that this is becoming much more popular) to select orders based off information criteria, namely, AIC/AICc/BIC instead due to less subjectivity. One final note; there are other assumptions for these models that also make up the Box-Jenkins method. Namely, ensuring the series is stationary first (no trend or seasonality, which hopefully can be addressed using differencing/seasonal differencing though not always) is an important assumption for all AR/MA/ARMA (and with differencing, ARIMA) models.
