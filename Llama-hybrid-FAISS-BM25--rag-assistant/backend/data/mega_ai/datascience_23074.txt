[site]: datascience
[post_id]: 23074
[parent_id]: 23072
[tags]: 
The hyperplane is a linear combination of the support vectors. In the soft margin case, there is only a limited amount of slack; every input does not get to be support vector. In the nonlinear case, the separating hypersurface may be embedded in an infinite-dimensional space, making it impossible to store. To borrow from the Wikipedia article , the normal vector $w$ is given by $$w = \sum_i c_i y_i \phi(x_i)$$ where $\phi$ is the feature embedding function, and $c_i$ is a Lagrangian dual variable that is zero for points on the correct side of the margin. Instead, test points are classified through a kernel function $k(x_i,x_j) = \left $ like so: $$x \to \mathrm{sgn}(\left + b) \equiv \mathrm{sgn} \left( b+\sum_i c_i y_i k(x_i, x)\right)$$ Notice how we avoided explicitly calculating $w$.
