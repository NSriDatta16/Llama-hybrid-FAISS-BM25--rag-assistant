[site]: datascience
[post_id]: 103697
[parent_id]: 103677
[tags]: 
The answer was to go lower level into the actual transformer config, then force the model to create sequences of 64-128 tokens. Doing this before training forces the model to adapt to this constraint, and obviously, these hard bounds ultimately lead to outputs in the specified range only.
