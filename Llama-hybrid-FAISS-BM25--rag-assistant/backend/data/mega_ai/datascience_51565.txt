[site]: datascience
[post_id]: 51565
[parent_id]: 51404
[tags]: 
Generally, the exact number of embedding dimensions does not affect task performance. The number of dimensions can affect training time. A common heuristic is to pick a power of 2 to speed up training time. Powers of 2 have a good chance to increase cache utilization during data movement, thus reducing bottlenecks. The most common powers of 2 for word embeddings are 128 or 256, depending on which order of magnitude is preferred.
