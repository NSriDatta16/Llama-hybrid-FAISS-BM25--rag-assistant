[site]: datascience
[post_id]: 54731
[parent_id]: 
[tags]: 
Neural Network backprop formula - Matrix dimensions won't match?

I want to start by taking an example for a normal neural network with 2 input nodes, 3 hidden nodes and 2 output nodes. Let the weights between input and hidden nodes are $W_i{_j}$ (2x3) and weights between hidden nodes and output (3x2) are $W_j{_k}$ . Last layer has linear activation. The gradient of error wrt: $$W_i{_j}= W_j{_k} * g'(W_i{_j} * x) * x * err$$ But this does not satisfy the weight update equation as the product gives the size 2x1. But it should be 2x3.
