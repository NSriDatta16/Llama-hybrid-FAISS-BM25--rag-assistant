[site]: crossvalidated
[post_id]: 472978
[parent_id]: 472950
[tags]: 
Your choice of Prediction Interval (PI) width depends on what you plan on doing with the PI. For instance, if you will use it to set safety stocks, you can determine the optimal quantile, be it 80%, 90% or 95%. (In this case, you would usually not use both endpoints of the PI, but only the upper one.) Using a "100% PI" often does not make a lot of sense. On the one hand, it's a function of how many MCMC samples you draw, because the maximum and the minimum can only get more extreme as you collect more samples, but the user will usually have a problem accepting that if you run your data collection longer, you get a higher and higher maximum. Also, the true future distribution may be unbounded (at least in theory), but your "100% PI" will always be bounded, so it will be a poor approximation to infinity. First, don't evaluate your model in-sample. You will always overfit. Instead, use a holdout sample and evaluate forecast accuracy on that. Yes, even if you are not interested in this per se. Just hold out some of your training data. Then you can accept your model and forecast if it is good enough for your application and you can't improve it any more with reasonable effort. Yes, this is subjective. How to know that your machine learning problem is hopeless? I'm not quite sure what you mean by "already measured uncertainties". You can use proper scoring rules to assess full predictive distributions, which your MCMC samples will give you. This is very good practice. I recommend Gneiting & Katzfuss (2014) . Alternatively, you can evaluate your PIs, but this loses a lot of information. One quality measure is the Mean Scaled Interval Score ( Gneiting & Raftery, 2007 ), which was used in the recent M4 forecasting competition (Makridakis et al., 2020]( https://doi.org/10.1016/j.ijforecast.2019.04.014 )). I very much recommend Forecasting: Principles and Practice (2nd ed.) by Athanasopoulos & Hyndman .
