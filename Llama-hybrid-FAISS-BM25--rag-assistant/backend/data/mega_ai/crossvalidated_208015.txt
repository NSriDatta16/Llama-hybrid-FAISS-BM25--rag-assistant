[site]: crossvalidated
[post_id]: 208015
[parent_id]: 207997
[tags]: 
You can see why this works out in one case, but not the other, by following your nose through the arithmetic. Say you have a tree $T$, and the terminal nodes are $T_1, T_2, \cdots, T_k$. Let $N_1, N_2, \cdots, N_k$ denote the number of training data points in each of these terminal nodes. Then we can write the prediction in terminal node $T_j$ as: $$ p_j = \frac{\sum_{i \in T_j} y_i}{N_j} $$ So your training average of the predictions is $$ \frac{1}{N} \sum_j N_j \frac{\sum_{i \in T_j} y_i}{N_j}$$ Since this looks a little gnarly, a word of explanation. Each observation in a terminal node receives the same prediction as all the others in the same node, so to aggregate, we can sum over the terminal node predictions, and repeat each summand $N_j$ times. Now you can see why your equality holds $$ \frac{1}{N} \sum_j N_j \frac{\sum_{i \in T_j} y_i}{N_j} = \frac{1}{N} \sum_j \sum_{i \in T_j} y_i = \frac{1}{N}\sum_i y_i $$ When there are sample weights, the convenient cancellation does not occur, as the prediction in the terminal node is then $$ p_j = \frac{\sum_{i \in T_j} w_i y_i}{\sum_{i \in T_j} w_i} $$ And there is no reason for $\sum_{i \in T_j} w_i = N_j$ to hold. A similar thing happens with class weights.
