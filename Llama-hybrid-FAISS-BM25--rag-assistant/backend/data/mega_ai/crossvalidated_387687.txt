[site]: crossvalidated
[post_id]: 387687
[parent_id]: 
[tags]: 
Posterior distribution of mixture models

In the context of mixture models in Bayesian inference, one can assume that the general form of the joint posterior for a mixture model of $k$ components is $$ \begin{equation} p( \boldsymbol{\lambda} , \boldsymbol{\theta} \mid \boldsymbol{y}) \propto \bigg[ \prod_{i=1}^{n} \sum_{j=1}^{k} \lambda_{j} f(y_{i} \mid \boldsymbol{\theta}_{j} ) \bigg] \ p(\boldsymbol{\lambda} ,\boldsymbol{\theta} ) \ . \end{equation} $$ Incorporating information about the missing variable $Z$ and thus using the augmented likelihood, the posterior accounting for the complete data structure reads (this formula is given in the excellent book " Bayesian Core " by Marin and Robert (2007)) $$ \begin{equation} p(\boldsymbol{\theta}, \boldsymbol{\lambda} \mid \boldsymbol{y}, \boldsymbol{z}) \propto \bigg[ \prod_{i=1}^{n} \prod_{j=1}^{k} \lambda_{ j}^{z_{ij}} f (y_{i} \mid \boldsymbol{\theta}_{j})^{z_{ij}} \bigg] \ \ p(\boldsymbol{\lambda} ,\boldsymbol{\theta} ) \ . \end{equation} $$ However, one can assume the following model decomposition for the joint posterior of all variables if we use independent prior for $\boldsymbol{\theta} = (\boldsymbol{\mu}, \boldsymbol{\sigma}^{2})$ . $$ p(\boldsymbol{y}, \boldsymbol{z}, \boldsymbol{\lambda}, \boldsymbol{\mu}, \boldsymbol{\sigma}^{2}) = p(\boldsymbol{y} \mid \boldsymbol{z}, \boldsymbol{\lambda}, \boldsymbol{\mu}, \boldsymbol{\sigma^{2}}) \ p(\boldsymbol{z} \mid \boldsymbol{\lambda}) \ p(\boldsymbol{\lambda}) \ p(\boldsymbol{\mu}) \ p(\boldsymbol{\sigma^{2}}) \\ = p(\boldsymbol{y} \mid \boldsymbol{\Psi}) \ p(\boldsymbol{\Psi}) \ .\\ $$ where $\boldsymbol{\Psi} = (\boldsymbol{\lambda}, \boldsymbol{\mu}, \boldsymbol{\sigma^{2}})$ I find it hard to see how the expression $p(\boldsymbol{\theta}, \boldsymbol{\lambda} \mid \boldsymbol{y}, \boldsymbol{z}) $ relates to $p(\boldsymbol{y}, \boldsymbol{z}, \boldsymbol{\lambda}, \boldsymbol{\mu}, \boldsymbol{\sigma}^{2})$ . Any hints ?
