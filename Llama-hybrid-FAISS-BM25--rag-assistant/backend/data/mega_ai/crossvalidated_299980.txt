[site]: crossvalidated
[post_id]: 299980
[parent_id]: 299446
[tags]: 
While it's possible to combine word embeddings using weighted average or a concatenation of min / max values across word vectors as described in this post , the output vector loses semantic information. A better alternative is to train a doc2vec model which is an extension of word2vec that uses paragraph vectors as part of the context during training: The word vectors in doc2vec are shared across all paragraphs while the paragraph vectors are unique to each paragraph. The doc2vec model is implemented in gensim . See the following ipython notebook for an example and this quora post for additional explanation of the doc2vec model.
