[site]: crossvalidated
[post_id]: 438441
[parent_id]: 
[tags]: 
Why do people produce large data sets with >90% of features redundant, when analysed computationally?

Why do people produce large data sets with >90% of features redundant, when analysed computationally? I've seen more than one example of this kind of data set. The gathering of the data is perhaps done "semi- by hand", but is it the impossible that those gathering the data could infer "what to leave out and what not". Or are they advised to "gather everything they find"? Or are they unable to reason, w.o. machine learning, what features are meaningful? If the data set has 10s of thousands of features, then it seems like a huge waste of time and effort, if in analysis phase >90% of those get discarded as meaningless.
