[site]: crossvalidated
[post_id]: 521376
[parent_id]: 
[tags]: 
how to do the dim reduction for the tensor samples (vector feature)

How to do the dim reduction (like PCA) for the vector feature ? We can assume that each sample has 3 features $(x_1,x_2,x_3)$ and the dimension of features is 2/3/4 respectively: $$x = (x_1,x_2,x_3) = \Big(\big(x_1^1,x_1^2\big), \big(x_2^1,x_2^2,x_2^3\big),\big(x_3^1,x_3^2,x_3^3,x_3^4\big),\Big),$$ with the label $y.$ We want to train a learner to use $x$ to predict label $y.$ Now I want to reduce the dim of $x$ into 2, which can be either: Features selection: remove some features, e.g. $X^* = (x_1,x_2).$ Features extraction (like PCA, LDA) e.g. $X^* = (\hat{x}_1,\hat{x}_2).$ Here $\hat{x}_i$ can have any dim and is obtained from $x_1,x_2,x_3$ (refer PCA). Actually I am more interested in the 2. Features extraction. Moreover, for any tensor samples, usually how do we take the dim reduction? Or could you refer some references?
