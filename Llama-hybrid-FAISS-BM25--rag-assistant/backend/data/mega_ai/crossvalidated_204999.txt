[site]: crossvalidated
[post_id]: 204999
[parent_id]: 204980
[tags]: 
Let's say that each sample $i$ is represented by values $x_{i,1},x_{i,2},\ldots,x_{i,n}$ where $n$ is total number of features. In your case $n=20,000$. Let's say that $X_1,X_2,\ldots,X_n$ are random variables that take values in feature 1, 2, ..., $n$. Also let's say that each instance/sample $i$ has a classification label that corresponds to it $y_i$. Similarly, $Y$ is a random variable that takes values in the set of labels. Suppose that your testing instance is $212$ (the number you have chosen). This means that you have values $x_{212,1},x_{212,2},\ldots,x_{212,n}$, each describing a feature extracted from instance $212$. Both Naive Bayes and Random Forests try to answer this question: what is the label of $y_{212}$? To answer that question, both Naive Bayes and Random Forests try to find $\hat y_{212}$ which maximizes the conditional joint density function: \begin{equation} \hat y_{212} = \underset{y \in \mathcal{Y}}{\text{arg max }}f_{X_1, X_2, \ldots, X_n| Y}(x_{212,1}, x_{212,2}, \ldots, x_{212,n}| y) \end{equation} That's ugly to look at. So let's rewrite that but with some notational abuse: \begin{equation} \hat y_{212} = \underset{y \in \mathcal{Y}}{\text{arg max }}f(x_{212,1},x_{212,2}, \ldots, x_{212,n}| y) \end{equation} Hell yeah dude. That's the dope. Here is where Naive Bayes and Random Forests differ: Naive Bayes shamelessly assumes that: \begin{equation} f(x_{212,1},x_{212,2}, \ldots, x_{212,n}| y) \approx \prod_{f=1}^n\hat f(x_{212,f}|y) \end{equation} Obviously that assumption is usually incorrect as stuff appear to be more dependent on one another in real life. Anyway NB plugs that shameless approximation into the maximization problem and tries to solve it: \begin{equation} \underset{\text{NB}}{\hat y_{212}} = \underset{y \in \mathcal{Y}}{\text{arg max }}\prod_{f=1}^n \hat f(x_{212,f}|y) \end{equation} So basically NB only tries to estimate $f(x_{212,f})$ by $\hat f(x_{212,f})$ for each feature in isolation, assuming that they are totally independent. Random Forests -on the other hand- do not perform that shameless simplistic assumption of NB. Instead, it dares to estimate the real thing. But since estimating the real thing is a complicated task, it uses a randomized algorithm to explore the space of solutions in a randomized manner so that it becomes computationally feasible. \begin{equation} f(x_{212,1},x_{212,2}, \ldots, x_{212,n}| y) \approx \hat f(x_{212,1},x_{212,2}, \ldots, x_{212,n}| y) \end{equation} So how does Random Forests estimate $\hat f(x_{212,1},x_{212,2}, \ldots, x_{212,n}| y)$? The answer is: by creating many (in your case $500$) randomized histograms , and then using the average histogram to be the $\hat f(x_{212,1},x_{212,2}, \ldots, x_{212,n}| y)$. In other words, those randomized tree splits, are essentially creating randomized partitions in the space where $x_{212}$ is sampled from. Such randomized partitioning eventually implies the creation of some randomized bin in a randomized histogram.
