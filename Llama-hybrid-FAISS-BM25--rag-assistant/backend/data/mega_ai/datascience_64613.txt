[site]: datascience
[post_id]: 64613
[parent_id]: 
[tags]: 
Epochs or Loss Convergence

I was training a Deep Neural Network using transfer learning and the loss value as of now is close to 1.7. I am using a dataset of 31,000 images and I do apply data augmentation techniques to help the model generalize better. However, I had confusion regarding epochs and loss values. Should I be training the model for say 50 epochs for good accuracy or would it be better if I trained until the loss value is as low as possible which will be very close to zero. Which one is recommended to get the best accuracy? Will more epochs cause the model to perform better than the lowest loss value? Any help would be greatly appreciated. Thanks in advance!
