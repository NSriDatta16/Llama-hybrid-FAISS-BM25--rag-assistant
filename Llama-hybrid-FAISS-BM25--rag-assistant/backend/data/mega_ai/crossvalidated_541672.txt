[site]: crossvalidated
[post_id]: 541672
[parent_id]: 525562
[tags]: 
If you have multiple fully connected layers, you end up with a Bayesian network model that has a large number of loops; exact inference in this kind of network structure is difficult. Qualitatively it's that everything ends up being correlated with everything else. Consider a neural network model structure like below Now interpret this figure as a Bayesian network by orienting the edges, say from right to left (i.e. arrows point to the left). In this network structure there are a huge number of overlapping loops. If one wants to execute exact inference using a message passing algorithm, these overlapping loops will result in difficult to evaluate junction trees . Really the problem is less about the depth of the network than about the width -- each node of the junction tree will correspond to an adjacent pair of layer, so the size (number of variables) for the clique potentials will scale like the width of the network layers.
