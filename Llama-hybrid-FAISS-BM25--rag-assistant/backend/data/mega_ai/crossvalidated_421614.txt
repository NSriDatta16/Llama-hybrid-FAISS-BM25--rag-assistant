[site]: crossvalidated
[post_id]: 421614
[parent_id]: 421598
[tags]: 
As you say, in this case the choice doesn't matter; the maximization over $w$ is the same. But you touch on a good point. Typically for regression problems we prefer to write the version with the conditioning on $x$ because we think of regression as modeling the conditional distribution $p(y | x)$ while ignoring the details of what $p(x)$ may look like. Succinctly, we only want to model how $y$ responds to $x$ , and we don't care about how $x$ itself. To see why, consider the model $Y = f(X) + \varepsilon$ , where $\varepsilon \sim \mathcal{N}(0, \sigma^2)$ , we want the estimate function $\hat{f}$ we produce to be as close to $f$ as possible (on average, and measured by e.g. least squares). So suppose we have known data $(x_1, f(x_1)), ..., (x_N, f(x_N))$ . Ultimately we just want to pull $\hat{f}(x_1), ..., \hat{f}(x_N)$ close to $f(x_1), ..., f(x_N)$ . It doesn't matter the distribution $p(x)$ of how the inputs are scattered, as long as $\hat{f}$ is close to $f$ on those values ( $f$ being how $y$ "responds" to $x$ ). Here's how the math works out. From an MLE standpoint the goal is to get our likelihoods $P(x,y|w)$ as large as possible, but as you say, we are not modeling $P(x)$ through $w$ , so $P(x)$ factors out and doesn't matter. From a fitting standpoint, if we want to minimize expected prediction error over $f$ , $$\min_f \text{EPE}(f) = \min_f \mathbb{E}(Y - f(X))^2$$ omitting some computation we obtain the minimizing $f$ to be $$f(x) = \mathbb{E}(Y | X = x)$$ so for least squares loss, the best possible $f$ depends only on the conditional distribution $p(y | x)$ , and estimating it should not require any additional information. However, in classification problems (logistic regression, linear discriminant analysis, Naive Bayes) there is a difference between a "generative" and a "discriminative" model; generative models do not condition on $x$ and discriminative models do. For instance, in linear discriminant analysis we do model $P(x)$ through $w$ ---we also do not use least squares loss in those.
