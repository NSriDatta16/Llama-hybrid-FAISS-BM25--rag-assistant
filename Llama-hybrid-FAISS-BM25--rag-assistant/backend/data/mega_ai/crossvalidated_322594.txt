[site]: crossvalidated
[post_id]: 322594
[parent_id]: 322591
[tags]: 
Maybe you could consider your task from two different perspectives: Supervised approach: If you have a good amount of historical data, I think a supervised method could be a good option. For example random forests or xgboost which are methods that perform good with structured data should be ok. The way to go would be labelling those surveys in "fake"/"no fake" surveys and proceed with the design of the algorithm. Unsupervised approach: If, on the contrary, you don't have that much data or labelling it is not an option for some reason maybe you could try a multidimensional outlier detection. The idea would be to iterate trough all your previous samples and compute the density estimate for all of them. Once this is done you can compute the likelihood that your new survey sample belongs to the density estimated with all your data. Using some threshold value (which you can roughly check empirically if you have several fake samples) or applying multiple hypothesis testing should help you identifying fakes. Of course, it all depends on how your data looks like. Hope this helps you at least to get you started. Good luck!
