[site]: crossvalidated
[post_id]: 152384
[parent_id]: 152373
[tags]: 
Scikit-learn doesn't directly support for categorical features in trees or forests (see comment by one of the core developers on SO , pointing to a currently stagnant implementation attempt ). If you leave them coded as id numbers, this imposes an ordering on the classes, and the forest will ask questions that rely on that ordering and are probably quite unnatural. Because the inputs are treated as real numbers, it'll make splits only like class_id , which makes sense only if the classes actually have a natural order to them. This is actually problematic for the model's expressivity: it's possible to ask first class_id and then class_id > 1003 to pick out a single class, but the greedy method used to learn trees will often make the first split look not very good and so bias against this kind of question. The typical workaround is to use a one-hot encoding , so that each categorical feature can be considered independently. This messes with the sampling procedure in the forests a bit (a given tree may be able to ask about class id 1000 but not 1003), but it's much better than the alternative. I agree with katya that standardizing is unnecessary for random forests. Each node of the tree will consider split points according to some kind of purity measure after the split (in scikit-learn, either entropy or Gini coefficient). If you standardize the variables, you just change the scales of the variables monotonically, which means each possible split in the original space has an exactly equivalent split in the standardized space. It's possible there's some kind of regularization used in forests sometimes that's sensitive to scaling, but as far as I know it should end up with exactly equivalent models.
