[site]: datascience
[post_id]: 98287
[parent_id]: 89362
[tags]: 
You are correct that recommender systems that map similarity to distance is useful. Vector representations are useful because most machine learning learning tools are based on linear algebra. Vector representations encode raw data in form that amenable to machine learning. "Any" vector representation is more useful than no vector representation. For example, one-hot encoding is often more useful than not including the feature. However, distance in one-hot encoding is not related to similarity. What is even more useful are semantic vectors (e.g., word2vec and related techniques). Semantic vectors map contextual meaning into locations in a learned vector space. This is an example of where distance is a proxy for similarity.
