In computer vision, articulated body pose estimation is the task of algorithmically determining the pose of a body composed of connected parts (joints and rigid parts) from image or video data. This challenging problem, central to enabling robots and other systems to understand human actions and interactions, has been a long-standing research area due to the complexity of modeling the relationship between visual observations and pose, as well as the wide range of applications. Description Enabling robots to perceive humans in their environment is crucial for effective interaction. For example, interpreting pointing gestures requires the ability to recognize and understand human body pose. This makes pose estimation a significant and challenging problem in computer vision, driving extensive research and development of numerous algorithms over the past two decades. Many successful approaches rely on training complex models with large datasets. Articulated pose estimation is particularly difficult due to the high dimensionality of human movement. The human body has 244 degrees of freedom and 230 joints. While not all joint movements are readily apparent, even a simplified representation of the body with 10 major parts and 20 degrees of freedom presents considerable challenges. Algorithms must account for substantial appearance variations caused by clothing, body shape, size, and hairstyles. Ambiguities arise from occlusions, both self-occlusions (e.g., a hand obscuring the face) and occlusions from external objects. Furthermore, most algorithms operate on monocular (2D) images, which lack inherent 3D information, exacerbating the ambiguity. Varying lighting conditions and camera viewpoints further complicate the task. These difficulties are amplified when real-time performance or other constraints are imposed. Recent research explores the use of RGB-D cameras, which capture both color and depth information, to address the limitations of monocular approaches. Sensors The typical articulated body pose estimation system involves a model-based approach, in which the pose estimation is achieved by maximizing/minimizing a similarity/dissimilarity between an observation (input) and a template model. Different kinds of sensors have been explored for use in making the observation, including the following: Visible wavelength imagery, Long-wave thermal infrared imagery, Time-of-flight imagery, and Laser range scanner imagery. These sensors produce intermediate representations that are directly used by the model. The representations include the following: Image appearance, Voxel (volume element) reconstruction, 3D point clouds, and sum of Gaussian kernels 3D surface meshes. Classical models Part models The basic idea of part based model can be attributed to the human skeleton. Any object having the property of articulation can be broken down into smaller parts wherein each part can take different orientations, resulting in different articulations of the same object. Different scales and orientations of the main object can be articulated to scales and orientations of the corresponding parts. To formulate the model so that it can be represented in mathematical terms, the parts are connected to each other using springs. As such, the model is also known as a spring model. The degree of closeness between each part is accounted for by the compression and expansion of the springs. There is geometric constraint on the orientation of springs. For example, limbs of legs cannot move 360 degrees. Hence parts cannot have that extreme orientation. This reduces the possible permutations. The spring model forms a graph G(V,E) where V (nodes) corresponds to the parts and E (edges) represents springs connecting two neighboring parts. Each location in the image can be reached by the x {\displaystyle x} and y {\displaystyle y} coordinates of the pixel location. Let p i ( x , y ) {\displaystyle \mathbf {p} _{i}(x,\,y)} be point at i t h {\displaystyle \mathbf {i