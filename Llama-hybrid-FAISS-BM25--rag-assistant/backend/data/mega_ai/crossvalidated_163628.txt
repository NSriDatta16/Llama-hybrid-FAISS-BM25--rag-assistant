[site]: crossvalidated
[post_id]: 163628
[parent_id]: 163516
[tags]: 
For a classification problem with data $X$ and labels $Y \in \mathcal{Y}$, if you know the exact distributions, the classifier least likely to make a mistake (the Bayes classifier ) is given by \begin{align} \hat{y}(x) &= \arg\max_{y \in \mathcal{Y}} P(Y = y \mid X = x) \\&= \arg\max_{y \in \mathcal{Y}} \frac{P(X = x \mid Y = y) P(Y = y)}{P(X = x)} \\&= \arg\max_{y \in \mathcal{Y}} P(X = x \mid Y = y) P(Y = y) .\end{align} Since of course in practice we don't know those distributions, we can replace them by an estimate. The Naive Bayes classifier follows from assuming the components of $X$ are independent, and using some density estimator or another on each dimension of $X$. The $k$-nearest neighbor classifier is basically equivalent to using a $k$-NN density estimator for $P(X = x \mid Y = y)$ (the built-in knn function). Using kernel density estimation gives you a kernel-based classifier ( the np package does this well, though I don't think it has a built-in classifier). These make no independence assumptions. As @Jacques notes, you can do similar thing with so-called Bayesian networks , which assumes an independence structure according to a particular graph; you can attempt to learn that graph from the data. Many other classifiers can also be cast in this framework.
