[site]: datascience
[post_id]: 13120
[parent_id]: 
[tags]: 
How to overcome training example's different lengths when working with Word Embeddings (word2vec)

I'm working on Sentiment Analysis over tweets using word2vec as word representation. I have trained my word2vec model. But when I'm going to train my classifier, I'm facing the issue that every tweet has different length and the classifier (RandomForest) needs all examples to be of the same size. Currently, for every tweet I'm averaging the vectors of all of its words, to end up with a vector representation of the tweet. For example: My word2vec model represents each word as vectors of size 300. I have Tweet1 formed by 10 words, and Tweet2 formed by 5 words. So what I do is, for Tweet1 (v1_Tweet1 + v2_Tweet1 + ... +v10_Tweet1)/10 = v_Tweet1 #avg vector of 300 elements. For Tweet2 : (v1_Tweet2 + v2_Tweet2 + ... +v5_Tweet1)/5 = v_Tweet2 #avg vector of 300 elements. *Being v1_TweetX the vector of the first word of the TweetX and so on. This works 'fine' but I would like to know what other approaches do you take to overcome the different sizes on the train and text examples for the classifier. Thanks.
