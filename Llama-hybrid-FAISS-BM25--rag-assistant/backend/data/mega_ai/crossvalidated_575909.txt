[site]: crossvalidated
[post_id]: 575909
[parent_id]: 575859
[tags]: 
This is an example of separation. I.e. the factor for the treatment group where everyone has no event perfectly splits the data so that for one factor level there are no events and for other levels there are some. As a result, the maximum likelihood estimate of the hazard ratio of this group vs. other groups is 0, which is a problem, because we work on the log-scale and $-\infty$ is not a number computer programs will deal with well (or if you reverse the comparison, this the MLE for the hazard ration becomes $+\infty$ ). What happens internally in the program you use is that it keeps making the estimate smaller (or larger) until it realizes things are not working and then it stops iterating with a warning. One possible solution for this issue is to use Firth's penalized likelihood approach, or to use median unbiased estimates using exact methods, both of which will produce finite estimates (that will be larger in absolute value, but need to be interpreted with caution). Another very reasonable approach is to do a Bayesian analysis, where you e.g. capture in your prior that interventions usually don't have incredibly large effects on survival, so maybe you think that many interventions might only influence the hazard for death by 10, 20 or maybe 50%, and that maybe 80 or 90 or 95% reduction in the hazard of death is starting to be hard to believe, which you could the capture with some sensible prior on the log-hazard ratio such as e.g. a N(0,1) prior or N(0, 2) or Student-t $_{\nu=3}(0, 2.5)$ , or something like that (plus perhaps a prior on what's expected for the untreated based on some historical information).
