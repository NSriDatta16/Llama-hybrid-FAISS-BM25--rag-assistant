[site]: crossvalidated
[post_id]: 414748
[parent_id]: 
[tags]: 
Mathematical explanations of cross-entropy function

I'm new to machine learning and know probability theory but most explanations of machine learning I found are very technically imprecise, and I'm having trouble understanding and describing machine learning concepts in a fully technically correct way that would satisfy a mathematician. For example, I understand what the cross-entropy function does, but could someone explain and rewrite it using probability theory and information theory? $$J = -\frac{1}{N} \sum_{n=0}^{N-1}\sum_{c=0}^{C-1} y_{nc}\log \hat{y}_{nc} $$ As I understand, $\hat{y}_{nc}$ is the probability that example $n$ belongs to class $c$ and $y_{nc}$ equals 1 when example $n$ belongs to class $c$ since it is a one-hot encoding. So does that mean there is a discrete random variable with support $\{0,1,\dots,9\}$ that takes on the probabilities $\hat{y}$ ? But isn't the ground truth deterministic, so is the true probability distribution that $y$ is part of even a probability distribution if there is no randomness in $y$ ? How would this be written using probability distributions?
