[site]: crossvalidated
[post_id]: 191819
[parent_id]: 
[tags]: 
HAC standard error or missing ARMA terms

In the context of regressions, it seems a convention that the HAC estimator should be applied when the residual is serially correlated. But isn't the presence of residual autocorrelations an indication that the model misses lagged dependent variables (or MA terms for that matter)? For example, suppose one is investigating the impact of variable $x_t$ on variable $y_t$ in a time series context. A simple model is $y_t = a + b x_t + \varepsilon_t$. It is often the case that the resulting error term $\varepsilon_t$ is serially correlated. One response to this problem could be just applying the HAC estimator to correct the standard errors of the parameter estimates, while arguing that the parameter estimates are still consistent in the presence of residual serial correlation. However, in this case, one could often add lags of $y_t$ to the model to remove the autocorrelations in the residuals, e.g. $y_t = a + b x_t + c y_{t-1} + \varepsilon_t$. Since the lag $y_{t-1}$ could be correlated with the regressor $x_t$, either by economic reasoning ($x_t$ is often serially correlated) or by sample correlation between the lag $y_{t-1}$ and the regressor $x_t$, it would be simply wrong to ignore the lag $y_{t-1}$ while just correcting the standard errors by HAC. This makes me wonder why one would ever use HAC estimator in regressions? At most, use the White estimator to correct for heteroskedasticidy if it is present. Otherwise, whenever there is residual autocorrelation, investigate adding AR or MA terms but never simply use HAC estimator.
