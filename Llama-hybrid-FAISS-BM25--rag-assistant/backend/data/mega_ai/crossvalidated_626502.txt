[site]: crossvalidated
[post_id]: 626502
[parent_id]: 349139
[tags]: 
Have a look at Why should binning be avoided at all costs? The question is what you want to predict for your problem setup? If you want to predict a value w/ uncertainty, then use adequate regression models. Nowadays you can use standard off the shelf methods for proper regression analysis without having to resort to 'binning' and using classification loss. See e.g. pyro.ai, XGBoostLSS, Tensorflow Probability, amongst other options. Even when you choose to bin, you should use ordinal regression and not standard softmax regression loss / metrics; the latter is just penalizing the errors incorrectly . But then the question arises on why bother at all? If you measure continuous / float data, and your predicted targets are potentially real valued and useful that way, then why bother throw out information ( binning does that ) and work with approximate results. Personally I have only seen this suggestion made when people are not familiar/ comfortable with regression metrics or algorithms; not because it's actually better at solving the prediction task.
