[site]: crossvalidated
[post_id]: 565921
[parent_id]: 
[tags]: 
Cross validation within a bootstrap sample: is leakage a problem here?

I would like to calculate the sampling distribution for logistic LASSO coefficients. One approach to calculating this sampling distribution is described on page 143 of "Statistical Learning with Sparsity" by Hastie, Tibshirani and Wainwright, which is freely available here . Briefly, in section 6.2 the authors advocate applying 10-fold cross validation to bootstrapped data. In more detail, the authors suggest (as I understand it) that we should resample the original data with replacement, and that for each bootstrap sample $(\boldsymbol{X}^*_b, \boldsymbol{y}^*_b)$ we: Fit a lasso path to ( $\boldsymbol{X}^*_b, \boldsymbol{y}^*_b$ ) over a dense grid of values $\Lambda = \{\lambda_l\}_{l=1}^{L}$ . Divide the training samples into 10 groups at random. With the $k^{th}$ group left out, fit a LASSO path to the remaining 9/10ths, using the same grid $\Lambda$ . For each $\lambda \in \Lambda$ compute the mean-squared prediction error for the left-out group. Average these errors to obtain a prediction error curve over the grid $\Lambda$ . Find the value $\hat{\lambda}^*_{CV,b}$ that minimizes this curve, and then return the coefficient vector from our original fit in step (1) at that value of $\lambda$ , which gives us $\hat{\beta}^*_b(\hat{\lambda}^*_{CV,b})$ . Repeat steps 1-6 a large number of times (e.g. $B$ = 1000), to get the set of bootstrapped LASSO estimators $\{\hat{\beta}^*_b(\hat{\lambda}^*_{CV,b})\}^B_{b=1}$ , which gives us our desired sampling distribution. My question I'm confused about step 2. We are dividing the bootstrap sample into 10 folds, but since the bootstrap sample was created by replacement sampling, identical observations may show up in both the training and the testing folds. Doesn't this lead to leakage? Notes: I get that such leakage would lead to the MSE being optimistic for all $\Lambda$ , but it's not clear to me that this would be order-preserving (i.e. that the best-performing $\lambda$ with leakage is the best-performing $\lambda$ without it). The authors mention in a footnote "On a technical note, we implement the bootstrap with observation weights $w^{*}_i = k/N$ , with $k = 0, 1, 2, . . .$ . In cross-validation, the units are again the original $N$ observations, which carry along with them their weights $w^{*}_i$ ". Is this done to, and does it, prevent leakage? Any insight would be greatly appreciated.
