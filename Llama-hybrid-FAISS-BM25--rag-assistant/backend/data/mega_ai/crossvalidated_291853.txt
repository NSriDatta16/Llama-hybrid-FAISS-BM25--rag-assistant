[site]: crossvalidated
[post_id]: 291853
[parent_id]: 255025
[tags]: 
Multi-task Learning and Transfer Learning methods although they have some things in common, they are not the same. Transfer Learning only aims at achieving high performance in the target task by transferring knowledge from the source task, while Multi-task Learning tries to learn the target and the source task simultaneously. A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. This assumption is weak and in many cases may not hold. For example, imagine that we have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In Multi-task Learning there is no mandatory to observe a difference in the distributions of the different tasks. Transfer Learning is a broader topic than Domain Adaptation. The latter is the process that attempts to alter the source domain in a way to bring the distribution of the source closer to that of the target domain. Transfer Learning can include also the case when we want to transfer knowledge from an old task to newer and different one (e.g. in a classification problem that we have different labels), which has some similarities with the old, apart from the above explained situation where we have one task but there is a difference in the distributions of the data (e.g. sample bias etc).
