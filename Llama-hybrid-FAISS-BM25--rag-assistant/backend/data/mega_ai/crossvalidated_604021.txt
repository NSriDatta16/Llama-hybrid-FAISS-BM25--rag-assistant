[site]: crossvalidated
[post_id]: 604021
[parent_id]: 
[tags]: 
Why do language models like InstructGPT and LLM utilize reinforcement learning instead of supervised learning to learn based on user-ranked examples?

Why do language models like InstructGPT and LLM utilize reinforcement learning instead of supervised learning to learn based on user-ranked examples? Language models like InstructGPT and ChatGPT are initially pretrained using self-supervised methods, followed by supervised fine-tuning. The researchers then train a reward model on responses that are ranked by humans on a scale of 1 to 5. After the reward model has been trained using supervised learning, reinforcement learning with proximal policy optimization (PPO) is used to update the language model. Why do they then use RLL with PPO to update the LLM based on the reward signal. Why don't they train / fine-tune the LLM using the labels (ranks 1-5 provided by humans) in a supervised fashion using ranking or (ordinal) regression losses?
