[site]: crossvalidated
[post_id]: 27152
[parent_id]: 27112
[tags]: 
edit see alfa's comment below. I'm not an expert on neural nets, so I'll defer to him. My understanding is different from the other answers that have been posted here. I'm pretty sure that backpropagation involves adding to the existing weights, not multiplying. The amount that you add is specified by the delta rule . Note that wij doesn't appear on the right-hand-side of the equation. My understanding is that there are at least two good reasons not to set the initial weights to zero: First, neural networks tend to get stuck in local minima, so it's a good idea to give them many different starting values. You can't do that if they all start at zero. Second, if the neurons start with the same weights, then all the neurons will follow the same gradient, and will always end up doing the same thing as one another.
