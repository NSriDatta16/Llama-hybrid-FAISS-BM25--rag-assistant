[site]: crossvalidated
[post_id]: 3753
[parent_id]: 3746
[tags]: 
One of the problems with multivariate data is deciding on, and then interpreting, a suitable metric for calculating distances, hence clever but somewhat hard-to-explain concepts such as Mahalanobis distance. But in this case surely the choice is obvious - Euclidean distance . I'd suggest a simple heuristic algorithm something like: Calculate the (unweighted) centroid of the data points, i.e. the (unweighted) means of the 2 coordinates Calculate the Euclidean distance of all the readings from the centroid Exclude any readings that are further than a certain distance (to be determined based on your experience and knowledge of the technology, or failing that a bit of trial and error cross-validation - 100m, 1km, 10km??) Calculate the weighted average of both coords of the remaining points, weighting by the inverse of the HDOP score (or some monotonic function of it - i had a quick look at the wikipedia page linked in the question and think maybe you don't need such a function but i'd need to study it further to be sure) There are clearly several ways to make this more sophisticated, such as down-weighting outliers or using M-estimators rather than simply excluding them, but I'm not sure whether such sophistication is really necessary here.
