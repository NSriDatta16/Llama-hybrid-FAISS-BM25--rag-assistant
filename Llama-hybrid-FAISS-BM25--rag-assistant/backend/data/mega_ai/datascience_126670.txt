[site]: datascience
[post_id]: 126670
[parent_id]: 
[tags]: 
Why do we use the RELU activation function?

I reading about activation functions in feedforward neural networks. ad a really old paper https://web.njit.edu/~usman/courses/cs677_spring21/hornik-nn-1991.pdf . They prove that by using arbitrary bounded and non-constant functions then, we can approximate anything function we want which is the heart of machine learning. So I wonder why RELU is used because it would directly violate the theorem as RELU is unbounded. I understand the reasoning that people use for using RELU that it combats exploding gradients of other activations and seems to work well etc. I am mainly confused about the activation functions that are not bounded as it violates the theorem, maybe I haven't delved deep enough into the literature to learn this
