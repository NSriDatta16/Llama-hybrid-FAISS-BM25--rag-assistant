[site]: crossvalidated
[post_id]: 483329
[parent_id]: 
[tags]: 
What should you do if you have too many features in your dataset, dimensionality reduction or regularization?

I have just started machine learning and was asked this concept-based question, "Suppose you are working on a stock market prediction model and the data you collected have millions of features, what should you do?" I found two possible methods - Regularization and dimensionality reduction. But I was told that regularization is incorrect because it does not affect input data but only the output data. Whereas dimensionality reduction removes unnecessary/useless data that generates noise. My main question is, if excessive features in a dataset could cause overfitting and regularization can help to reduce the complexity of the model, why is regularization not a valid solution? Would sincerely appreciate if anyone could provide some usage examples for both methods.
