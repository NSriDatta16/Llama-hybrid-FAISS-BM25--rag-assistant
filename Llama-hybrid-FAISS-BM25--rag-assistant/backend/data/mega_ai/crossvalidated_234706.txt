[site]: crossvalidated
[post_id]: 234706
[parent_id]: 
[tags]: 
when is linear svm with incremental / decremental learning a good idea?

The data that I want to classify are large 3D medical images (the input vectors are the pixels, order of 1M coefficients). It has been argued that a linear SVM is a good classifier for these data because it reduces the degrees of freedom during training, which is important when the sample size is small compared to the size of one training specimen. Looking into the computational side I thought that for efficiency the number of kernel (inner product) re-computations should be kept low, e.g. , when specimen are removed and inserted during cross-validation; a kernel method would be much more efficient during optimisation, because the size of the kernel matrix is much smaller than the input vectors. So I thought that 'online' SVM trainers that allow efficient re-training after adding or removing a few specimens at a time would be the optimal solution. But there seem to be very few (C++) implementation of this technique, and all of them from before 2010, e.g. : Gert Cauwenberghs' method SVM heavy the Kernel Machine Library Then there are more recent algorithms that support 'warm starting' (like in Dlib ), which I presume is more efficient in cross-validations than learning from scratch each time, but does not use the fact that (for example in leave-one-out) you know exactly which specimen have already been used for training and which have not. Other methods that exploit the relatively simple linear classification, such as the in-/decremental extension of LibLinear are not using the support vector machine framework. Does that mean that using SVM, exploiting the kernel representation for efficiency during training (size kernel
