[site]: crossvalidated
[post_id]: 627953
[parent_id]: 
[tags]: 
SHAP for text input: aggregating values across instances

I am working on a binary text classification task, where the input is text (~100 words long, but length varies). I am using a fine-tuned BERT-based model. My goal is to get insights about which tokens are important for predicting pos/neg class across the test set (global explanation). Since SHAP values are calculated for each token and for each instance (local), I need to aggregate across instances. I have already implemented this in various ways: at the token level: sum SHAP values of individual tokens across instances and divide by number of occurrences of each token. at the ngram level (works with any value of n, but for now I used 3): The SHAP value of an ngram is the sum of the SHAP values of its constituent tokens divided by n. Then, I take the mean across occurrences of each ngram. [Note that ngrams are created from tokens for now so are not always sequences of full words] at the chunk level: I noticed that some consecutive tokens get the exact same SHAP value, so I treat them as a chunk with SHAP values equivalent to those of any of its constituent tokens (since they are all the same). Then, average across occurrences of each chunk. [These end up being large chunks of text in a lot of the cases, and therefore I can't exactly aggregate across instances (no exact match)] I also have two approaches to averaging: per class: average contribution of a token WHEN IT DOES CONTRIBUTE to a class prediction (magnitude of contribution when it contributes) overall: average contribution of a token across all instances where it appears, regardless of whether it contributes to the prediction or not (I can get insights about whether on average it contributes more to pos/neg class) Now I am hitting a wall, questioning whether my approach is sound and what I can do to get more (and accurate) insights. Mind that my end goal is to be able to make (estimated) conclusions such as the presence of the phrase (co-location of tokens) 'blue carrots' is an indicator of the negative class . My questions are: Since SHAP values are relative to the instance, how do I most accurately compare importances across instances which have different lengths? I was thinking of normalising at instance level before I aggregate so that instead of having SHAP values I have the contribution of each token expressed as a percentage (e.g. this/these token/s contributed for 20% to predicting the negative class). However, I think I would first need to create groups of tokens within the sentence (since for longer sentences multiple consecutive tokens get the same SHAP values). How could this be done and would it make sense? How do I aggregate importances across instances to get more interesting insights about which tokens are (overall, for the classification task) indicative of the pos/neg class? [Note that I say tokens but it could be a token by itself, a sequence of tokens, non consecutive co-occurrence of tokens in text.] I would really appreciate it if someone with expertise in using SHAP for text took the time to check if my reasoning and work so far is sound and answer my questions.
