[site]: datascience
[post_id]: 115158
[parent_id]: 115141
[tags]: 
This is a very interesting as well as a fundamental question. Let me answer this from a general context rather than just the SVC / SVM case. You can refer to Duda et.al "Pattern Classification" book which explains this concept very well. Let's take the case of a 2-dimensional dataset with just two features. Say for instance we have two classes and the separation is a simple line. For simplicity consider two classes on either side of the separation line. The closer is a given data point to the boundary the less confident the model is about its class. Similarly, the farther away is data point from the boundary, the more confident is the model about the class. This simply means a data point exactly on the boundary has an equal probability that it belongs to two classes. It can either be class A or class B which implies a 50% chance for both. In such cases, we set such data points to be something called a "reject" class which are either outliers or anomalies. Hope this gives you food for thought.
