[site]: crossvalidated
[post_id]: 252049
[parent_id]: 
[tags]: 
Fastest PCA under certain constraints

I'd like to run PCA on some data, and it looks like there are a lot of ways to do it. What is the best way, given this information? The data is high-dimensional. Min: several thousand, max: around 150 million. Big range! Typically less than 10 million, I would think. The lower number of samples I can do this with, the better, but it could be up to thousands. I want to reconstruct the data (specifically only the most recent row), but with differing amounts of each principal component. For right now I plan on only modifying the first component, but it could be more later on. I may want to make the PCA computation again as time goes on. What is the best choice? I've heard of incremental PCA , or thin SVD tracking , data matrix SVD , or the covariance method . Even some rules of thumb would be useful. Edit: I'm using Python + Numpy.
