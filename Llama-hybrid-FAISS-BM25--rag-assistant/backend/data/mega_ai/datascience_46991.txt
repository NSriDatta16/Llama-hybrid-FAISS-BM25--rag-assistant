[site]: datascience
[post_id]: 46991
[parent_id]: 45900
[tags]: 
They are used for two different purposes. StandardScaler changes each feature column $f_{:,i}$ to $$f'_{:,i} = \frac{f_{:,i} - mean(f_{:,i})}{std(f_{:,i})}.$$ Normalizer changes each sample $x_n=(f_{n,1},...,f_{n,d})$ to $$x'_n = \frac{x_n}{size(x_n)},$$ where $size(x_n)$ for l1 norm is $\left \| x_n \right \|_1=|f_{n,1}|+...+|f_{n,d}|$ , l2 norm is $\left \| x_n \right \|_2=\sqrt{f^{2}_{n,1}+...+f^{2}_{n,d}}$ , max norm is $\left \| x_n \right \|_\infty=max\{|f_{n,1}|,...,|f_{n,d}|\}$ . To illustrate the contrast, consider data set $\{1, 2, 3, 4, 5\}$ which consists of 5 one dimensional data points (each data point has one feature), After applying StandardScaler , data set becomes $\{-1.41, -0.71, 0. ,0.71, 1.41\}$ . After applying any type of Normalizer , data set becomes $\{1., 1., 1., 1., 1.\}$ , since the only feature is divided by itself. So Normalizer has no use for this case. Also, when features have different units, e.g. $(height, age, income)$ , Normalizer is not used as a pre-processing step; although, it might be used as an ad-hoc feature engineering step similar to what a neuron does in a neural network. As mentioned in this answer , Normalizer is mostly useful for controlling the size of a vector in an iterative process, e.g. a parameter vector during training, to avoid numerical instabilities due to large values.
