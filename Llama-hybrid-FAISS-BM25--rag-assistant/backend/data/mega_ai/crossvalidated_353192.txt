[site]: crossvalidated
[post_id]: 353192
[parent_id]: 
[tags]: 
Discriminant Analysis: Explicitly writing Covariance matrix given some sample data

Let $(X, Y) \in \mathbb{R}^d \times \{0, 1\}$ give a random pair wherein the conditional distribution of $X$ given $Y$ is $X | Y \sim \mathcal{N}(\mu_Y, \Sigma_Y)$, $\mu_0 \neq \mu_1 \in R_d$, where $\mathbb{P}(Y=k)=\pi_k (\pi_0+\pi_1=1)$ is known (representative of probabilities of classes of label in binary classification task). I happen to need to calculate (explicitly) the covariance matrices $\Sigma_0, \Sigma_1 $ given a set of random samples from $(X, Y)$. Suppose this sample has $n_0$ values of class $0$, and $n_1$ values of class $k=1$. The marginal density is dependent on the precision matrix $\Sigma^{-1}$, but I want to compute the covariance matrices $\Sigma_0, \Sigma_1$. Lecture 22: Fisher LDA and Bayesian Classification from Dr. Shademehr gives a formula (see 16:45) as follows $\Sigma_0 = \frac{1}{n_0-1} \Sigma_{i \in C_0}(x^{(i)}-\mu^{i})(x^{(i)}-\mu^{(i)})^{T}$. I'm not so sure I understand how to interpret this expression. I see elsewhere it is popular to invoke a "pooled covariance estimate", which upon standardizing the notation seems to be essentially the same expression. I will try not to be confusing because there are essentially 3 indices, one for the sample, one for the class of label, and one for the feature (component of $x$), and i'm seeing each seperate source will use a different dummy variable for each and representing the formulas with different notation conventions, some leaving one or another implicit. So, to make everything explicit, given that the $0$ class sample subset is represented as $S_0 = \{(x^{(0)}, y^{0}), (x^{(1)}, y^{(1)}), \dots , (x^{(n_0)}, y^{(n_0)})\}$, where $x^{(i)}=\langle x^{(i)}_{1}, x^{(i)}_{2}, \dots, x^{(i)}_{d}\rangle$ (components carry subscripts), then my matrix entries will look like what? The way I interpret this formula at first glance is that for each component $1 \dots , i, \dots, d$, we have an analagous component defined by the matrix product of 2 $d$ length vectors defined by the sum over all samples of the expression $x-\mu$. We should have a $d \times d$ matrix with entries that look like look like \begin{align} \Sigma_{0}(q,r)=\frac{(x^{(1)}_{q}+x^{(2)}_{q}+\dots+x^{(n_0)}_{q}-n_0\mu_q)(x^{(1)}_{r}+x^{(2)}_{r}+\dots+x^{(n_0)}_{r}-n_0\mu_r)}{(n_0-1)}? \end{align} But this will be $0$. It will suffice of course to simply focus on the $k=0$ case, as the rest will be defined analogously. Feel free to answer in the form of it should be immediately obvious to you that such and such can't be because ... EDIT: I think this matrix should be given by $n_0^{-1} M_x (M_x)^T$ where the entries of the matrix $M_x$ give the $i$th component of the $j$th sample $M_x = [x^{(j)}_i]$ and $i \in\{1, 2, \dots, d\}, j \in \{1, \dots, n_0\}$; columns indexing with $i$, rows indexing with $j$.
