[site]: crossvalidated
[post_id]: 399087
[parent_id]: 393708
[tags]: 
Wouldn't you consider language to be a type of time series? How about OpenAI Five's representation of the Dota game state as a time series with 20,000 continuous and discrete variables? The tool of choice for such sequence modeling is LSTMs, Transformers, and other autoregressive models. And you can always tack on a latent prior to any of these models, (see "recurrent VAE"), but it's not necessary because any sequence distribution can be modeled as $p(x_0, x_1, \ldots) = p(x_0) \prod_j p(x_j|x_{i . The difficulty with imputation using these models is that you'd presumably be trying to do something like $\max_{X_i \ldots X_j} p(x_0 = X_0, \ldots x_{i-1}=X_{i-1}, x_i=X_i, \ldots, x_j=X_j, \ldots)$ , and due to the autoregressive nature of the model, there's actually no easy way to carry out the maximization. If you just want to sample $X_i \ldots X_j$ from the modeled distribution with the other values fixed, there's no easy way to do that either. Maybe it would not be too expensive to use some form of MCMC sampling.
