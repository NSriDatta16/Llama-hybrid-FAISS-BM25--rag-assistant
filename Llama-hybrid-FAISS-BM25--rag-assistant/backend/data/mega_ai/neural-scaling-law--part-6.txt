nsformers, similar to language transformers, exhibit scaling laws. A 2022 research trained vision transformers, with parameter counts N ∈ [ 5 × 10 6 , 2 × 10 9 ] {\displaystyle N\in [5\times 10^{6},2\times 10^{9}]} , on image sets of sizes D ∈ [ 3 × 10 7 , 3 × 10 9 ] {\displaystyle D\in [3\times 10^{7},3\times 10^{9}]} , for computing C ∈ [ 0.2 , 10 4 ] {\displaystyle C\in [0.2,10^{4}]} (in units of TPUv3-core-days). After training the model, it is finetuned on ImageNet training set. Let L {\displaystyle L} be the error probability of the finetuned model classifying ImageNet test set. They found min N , D L = 0.09 + 0.26 ( C + 0.01 ) 0.35 {\displaystyle \min _{N,D}L=0.09+{\frac {0.26}{(C+0.01)^{0.35}}}} . Neural machine translation Ghorbani, Behrooz et al. studied scaling laws for neural machine translation (specifically, English as source, and German as target) in encoder-decoder Transformer models, trained until convergence on the same datasets (thus they did not fit scaling laws for computing cost C {\displaystyle C} or dataset size D {\displaystyle D} ). They varied N ∈ [ 10 8 , 3.5 × 10 9 ] {\displaystyle N\in [10^{8},3.5\times 10^{9}]} They found three results: L {\displaystyle L} is a scaling law function of N E , N D {\displaystyle N_{E},N_{D}} , where N E , N D {\displaystyle N_{E},N_{D}} are encoder and decoder parameter count. It is not simply a function of total parameter count N = N E + N D {\displaystyle N=N_{E}+N_{D}} . The function has form L ( N e , N d ) = α ( N ¯ e N e ) p e ( N ¯ d N d ) p d + L ∞ {\displaystyle L\left(N_{e},N_{d}\right)=\alpha \left({\frac {{\bar {N}}_{e}}{N_{e}}}\right)^{p_{e}}\left({\frac {{\bar {N}}_{d}}{N_{d}}}\right)^{p_{d}}+L_{\infty }} , where α , p e , p d , L ∞ , N ¯ e , N ¯ d {\displaystyle \alpha ,p_{e},p_{d},L_{\infty },{\bar {N}}_{e},{\bar {N}}_{d}} are fitted parameters. They found that N d / N ≈ 0.55 {\displaystyle N_{d}/N\approx 0.55} minimizes loss if N {\displaystyle N} is held fixed. L {\displaystyle L} "saturates" (that is, it reaches L ∞ {\displaystyle L_{\infty }} ) for smaller models when the training and testing datasets are "source-natural" than "target-natural". A "source-natural" data point means a pair of English-German sentences, and the model is asked to translate the English sentence into German, and the English sentence is written by a natural English writer, while the German sentence is translated from the English sentence by a machine translator. To construct the two kinds of datasets, the authors collected natural English and German sentences online, then used machine translation to generate their translations. As models grow larger, models trained on source-original datasets can achieve low loss but bad BLEU score. In contrast, models trained on target-original datasets achieve low loss and good BLEU score in tandem (Figure 10, 11 ). The authors hypothesize that source-natural datasets have uniform and dull target sentences, and so a model that is trained to predict the target sentences would quickly overfit. trained Transformers for machine translations with sizes N ∈ [ 4 × 10 5 , 5.6 × 10 7 ] {\displaystyle N\in [4\times 10^{5},5.6\times 10^{7}]} on dataset sizes D ∈ [ 6 × 10 5 , 6 × 10 9 ] {\displaystyle D\in [6\times 10^{5},6\times 10^{9}]} . They found the Kaplan et al. (2020) scaling law applied to machine translation: L ( N , D ) = [ ( N C N ) α N α D + D C D ] α D {\displaystyle L(N,D)=\left[\left({\frac {N_{C}}{N}}\right)^{\frac {\alpha _{N}}{\alpha _{D}}}+{\frac {D_{C}}{D}}\right]^{\alpha _{D}}} . They also found the BLEU score scaling as B L E U ≈ C e − k L {\displaystyle BLEU\approx Ce^{-kL}} . Transfer learning Hernandez, Danny et al. studied scaling laws for transfer learning in language models. They trained a family of Transformers in three ways: pretraining on English, finetuning on Python pretraining on an equal mix of English and Python, finetuning on Python training on Python The idea is that pretraining on English should help the mode