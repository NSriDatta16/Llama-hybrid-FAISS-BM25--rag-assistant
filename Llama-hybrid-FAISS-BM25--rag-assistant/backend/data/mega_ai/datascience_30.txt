[site]: datascience
[post_id]: 30
[parent_id]: 19
[tags]: 
Total amount of data in the world: 2.8 zetabytes in 2012, estimated to reach 8 zetabytes by 2015 ( source ) and with a doubling time of 40 months. Can't get bigger than that :) As an example of a single large organization, Facebook pulls in 500 terabytes per day, into a 100 petabyte warehouse, and runs 70k queries per day on it as of 2012 ( source ) Their current warehouse is >300 petabytes. Big data is probably something that is a good fraction of the Facebook numbers (1/100 probably yes, 1/10000 probably not: it's a spectrum not a single number). In addition to size, some of the features that make it "big" are: it is actively analyzed, not just stored (quote "If you aren’t taking advantage of big data, then you don’t have big data, you have just a pile of data" Jay Parikh @ Facebook) building and running a data warehouse is a major infrastructure project it is growing at a significant rate it is unstructured or has irregular structure Gartner definition: "Big data is high volume, high velocity, and/or high variety information assets that require new forms of processing" (The 3Vs) So they also think "bigness" isn't entirely about the size of the dataset, but also about the velocity and structure and the kind of tools needed.
