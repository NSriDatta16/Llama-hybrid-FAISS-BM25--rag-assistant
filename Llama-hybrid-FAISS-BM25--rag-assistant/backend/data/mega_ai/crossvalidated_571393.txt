[site]: crossvalidated
[post_id]: 571393
[parent_id]: 571382
[tags]: 
Gaussian log-likelihood is $$ \log \mathcal{L}(y|\mathbf{X},\boldsymbol{\beta}) = -\sum_i \frac{(y_i - \mathbf{X}_i\boldsymbol{\beta})^2}{\sigma^2} $$ When you are minimizing weighted least squares, the loss function is $$ L(y, \hat y) = \sum_i w_i (y_i - \hat y_i)^2 $$ So in the Bayesian scenario, this basically means that your likelihood becomes $$ \prod_i \mathcal{N}(\mathbf{X}_i\boldsymbol{\beta},~\sigma^2/w_i) $$ i.e. instead of having constant variance $\sigma^2$ , it is multiplied by the inverse of the non-negative weights $w_i$ for each observation, so more weight leads to more precision .
