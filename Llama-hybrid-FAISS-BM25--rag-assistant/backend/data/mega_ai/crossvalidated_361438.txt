[site]: crossvalidated
[post_id]: 361438
[parent_id]: 
[tags]: 
Likelihood term in Bayesian inferencing versus the general definition

In general we say that the likelihood function is defined as some $L(\theta|x)$, so that it is a function over some parameters: $\theta$ given some data: $x$. That is, $\theta$ is free to vary whilst $x$, our data, is fixed. This makes sense. In Bayesian inferencing we claim that the term $P(x|\theta)$ is the likelihood term. However what I don't understand is that this term/function in Bayes' rule has fixed the $\theta$ variable, and so it is no longer the varying term. Instead we have a function varying over our data: $x$. And in reality often our data is given and fixed for us so that $x$ is fixed also. So technically the term $P(x|\theta)$ has both $\theta$ and $x$ non-varying. I therefore am not sure how to interpret the $P(x|\theta)$ term properly. It is not varying over $\theta$ (unless we include the prior $P(\theta)$, but I feel like that would be cheating), and in addition to fixing $\theta$, $x$ is fixed also, so is it correct to even call $P(x|\theta)$ the likelihood term under these realities?
