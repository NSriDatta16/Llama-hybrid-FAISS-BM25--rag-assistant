[site]: crossvalidated
[post_id]: 168703
[parent_id]: 168622
[tags]: 
The reason is because the goals of "traditional statistics" are different from many Machine Learning techniques. By "traditional statistics", I assume you mean regression and its variants. In regression, we are trying to understand the impact the independent variables have on the dependent variable. If there is strong multicollinearity, this is simply not possible. No algorithm is going to fix this. If studiousness is correlated with class attendance and grades, we cannot know what is truly causing the grades to go up - attendance or studiousness. However, in Machine Learning techniques that focus on predictive accuracy, all we care about is how we can use a set of variables to predict another set. We don't care about the impact these variables have on each other. Basically, the fact that we don't check for multicollinearity in Machine Learning techniques isn't a consequence of the algorithm, it's a consequence of the goal. You can see this by noticing that strong collinearity between variables doesn't hurt the predictive accuracy of regression methods.
