[site]: crossvalidated
[post_id]: 539122
[parent_id]: 538980
[tags]: 
This is a feature selection problem in classification with categorial features: your cluster labels are the classes, and the features are your catgorial variables. There are two common approaches to this problem: Test each variable for independence from the cluster label, e.g. with a $\chi^2$ test of independence . Beware however the comment by @whuber that the p-values must not be interpreted as error probabilities for $H_0$ . Nevertheless, you can use the p-values $p_i$ for each variable to compute score values, e.g., $1-p_i$ or $1/(1+p_i)$ . Maximize the joint mutual information between the selected features and the target variable (cluster label), e.g., as described by Bennasar, Hicks & Setchi (2015) . Method 1. is only a scalar method, i.e., it does not consider interactions, but it has the advantage of being easy to implement. As method 2. also takes interactions between variable sinto account, it is, theoretically, preferable. Both methods are filter methods which do not rely on a classification algorithm and do not optimize classification error. If you care about classification accuracy (which your question seems to indicate), a possibly preferable approach would be to use a wrapper method around a classification algorithm that can handle categorial features, e.g., random forest. If you have a large number of features and training samples, Boruta is a very popular method: M.B. Kursa, W.R. Rudnicki: "Feature Selection with the Boruta Package." Journal of Statistical Software 36,11, pp. 1-13 (2010)
