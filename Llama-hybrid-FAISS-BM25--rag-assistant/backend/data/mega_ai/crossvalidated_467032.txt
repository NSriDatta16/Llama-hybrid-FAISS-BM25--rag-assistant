[site]: crossvalidated
[post_id]: 467032
[parent_id]: 467013
[tags]: 
Assuming $E[\mu] = \frac{1}{m} \sum_{j=1}^m \mu^j$ and $m \rightarrow \infty$ . If you're ok with passing to a subsequence $n_m$ so that $$ P( | \frac{1}{n_m}\sum_{i=1}^{n_m} X_{i}^j - \mu^j | 1 - \frac{1}{m^2}, $$ then the result would hold for the subsequence (more precisely, sub-array). In general, it is not true that $$ \epsilon^j_n \stackrel{p}{\rightarrow} 0 \; \mbox{ as } n \rightarrow \infty, \;\; \forall j $$ implies $$ \frac{1}{m} \sum_{j = 1}^m \epsilon^j_n \stackrel{p}{\rightarrow} 0 \; \mbox{ as } n, \; m \rightarrow \infty. $$ Indeed, it is not even true for deterministic sequences, which is a special case. Let $$ \epsilon^j_n = j \cdot (\log (n+1) - \log(n)), $$ then $\epsilon^j_n \rightarrow 0$ as $n \rightarrow \infty$ for all $j$ . Averaging across $j$ gives $$ \frac{1}{m} \sum_{j = 1}^m \epsilon^j_n = O\left(m \cdot ( \log (n+1) - \log(n) ) \right) $$ which does not converge to $0$ as $n, m \rightarrow 0$ . (You can easily cook up a sequence $X^j_n$ so that $\frac{1}{n}\sum_{i=1}^nX_n^j = \epsilon^j_n$ , making it a counter-example to the claim.)
