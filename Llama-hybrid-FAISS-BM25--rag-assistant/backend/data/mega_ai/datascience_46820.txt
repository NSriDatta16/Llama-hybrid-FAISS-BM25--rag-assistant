[site]: datascience
[post_id]: 46820
[parent_id]: 46813
[tags]: 
It all comes down on how backward propagation works. Ultimately you need to know how much each part of the equation contributed to the final error and then modify the values of such part of the equation. In the case of linear regression is a bit more trivial, but when you start stacking one linear regression after other (and that is essentially a neural network) then you need to know how much each coefficient of each layer contributes to the final error. If you were to use a full derivative instead of a partial one, you could not determine the error contribution with such grain-level precision. I understand this is not a good mathematical explanation, but it should give you an intuition of why you need the partial derivative.
