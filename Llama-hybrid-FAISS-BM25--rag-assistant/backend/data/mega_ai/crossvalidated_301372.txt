[site]: crossvalidated
[post_id]: 301372
[parent_id]: 267771
[tags]: 
Batch normalization is not a method of standardizing the inputs; rather, it standardizes the activations of the hidden units. For example, if your network has $l$ layers, batch normalization could be applied separately to each layer by subtracting the mean from and dividing by the standard deviation of the activations of the hidden units in each layer (during forward propagation). Moreover, in practice, additional terms are used to shift the mean and scale the standard deviation of the units in a layer to arbitrary values, which are learned through backprop. These values may be different from those that result from the outputs of the hidden units in a layer because they are learned directly rather than implicitly as a function of those hidden units' activations. Broadly speaking, batch normalization is important because the gradients of the units can be very small or very large, depending on the nonlinearity the network is modeling. This makes it difficult to choose a common learning rate for gradient descent that can be applied to all the units because layers may affect each other exponentially. Normalization is useful because it scales the units outputs to a certain scale which makes gradient descent faster and, in a sense, more efficient. For more information, please see Deep Learning by Goodfellow et al. (2016) or Ioffe and Szegedy (2015, referenced therein).
