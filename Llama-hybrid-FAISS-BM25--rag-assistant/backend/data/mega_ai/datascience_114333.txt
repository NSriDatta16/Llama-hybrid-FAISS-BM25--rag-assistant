[site]: datascience
[post_id]: 114333
[parent_id]: 
[tags]: 
Best practice and starting point for designing a decomposable score metric

I need to generate a 'score' metric (out of 100) to summarise a number of other features, and to allow comparison between sets. Each feature in turn will be some numeric value (possibly another derived score) . My instinct is to just scale each feature to a consistent size, multiple it by a weight factor, add them all together and then scale down to 100. My first concern is that I expect this to experience something like regression to the mean. everything will wash out and look very average. How can i design a score to ensure meaningful differentiation. Secondly, I would like to be able to decompose the score and speak to which sub components have impacted it the most or which are the lowest hanging fruit for improvement. This is easy enough with the simple weighting approach but i feel would be lost with more complex modelling. Is there a normal name for this type of work (to allow me to research) or any know best practices or reading? Any general pointers? Thank you
