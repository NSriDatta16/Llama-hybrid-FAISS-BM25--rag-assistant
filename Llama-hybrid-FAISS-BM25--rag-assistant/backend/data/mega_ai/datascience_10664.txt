[site]: datascience
[post_id]: 10664
[parent_id]: 10640
[tags]: 
You do not need domain knowledge (the knowledge of what your data mean ) in order to do feature engineering (finding more expressive ways of framing your data). As Tu N. explained , you can find "quick and dirty" combinations of features that could be helpful pretty easily. Given an output $y$ and an individual feature $x$ , you can take the following transforms, $x' \in \{e^x, \log(x), x^2, x^3, \tanh(x)\}$ . A quick check of the usefulness of the transformation is if the correlation between $\{y,x'\}$ is higher than the correlation between $\{y,x\}$ . Warning on correlation: Correlation does not show everything and depending on the model you are using (highly non-linear such as NN or RF) and interaction with other variables, a change in correlation could mean nothing. However, if you are using a simple linear model like logistic regression, it is an OK indicator of performance. The best way to evaluate such a transformation, however, as noted by Fokhruz Zaman , would be to build a model with and without your transformed feature, and see how the validation error (on your Cross-Validation folds) evolves. It is rather easy to spot single-feature transformations this way. Those apply to a lot of data, where a more expressive relation between your input and output could be on a different scale. For example, the relationship between Income and "Happiness" appears to be logarithmic, but you would never record the log of a participant income directly. Finding combinations of features is more difficult. For a start, if you want to test every addition of 2 features, and you have $D$ features, you have an order of $D^2$ transformations to test. In order to find such transformations, you can apply a nonlinear model (such as NN or RF) to the problem and try to see what it is that it is learning. If you can identify what an intermediate layer in a NN is doing, you can pre-compute its result and add it as a new feature. It will not need to compute it again, and it will probably try to learn something new. It can be hard to interpret the internal representation of a NN, or even interpret feature importance in a Random Forest. An easier, and probably more suited method for this purpose, the model would be Boosting with decision trees. There are a lot of libraries implementing Boosting, and if you are into Kaggle competition as your post seems to imply, XGBoost seems used by a lot of participants, so you might find some help/tutorials on what I'm going to describe. First, run your boosting algorithm using only stumps , 1-level decision trees. Stumps are very weak, but Boosting makes it a reasonable model. This will act as your baseline. Depending on the library you are using, you should be able to display pretty easily which are the most used features, and you should plot them against the response (or do a histogram if the response is categorical) to identify some pattern. This might give you an intuition on what would be a good single feature transformation. Next, run the Boosting algorithm with 2-level decision trees. This model is a lot more complex than the previous one; if two variables taken together have more power than taken individually, this model should outperform your previous one (again, not in terms of training error, but on validation error!). Based on this, you should be able to extract which variables are often used together, and this should lead you to potential multi-feature transformations. On related material, I would advise the following videos as they are easy to follow Data Agnosticism: Feature Engineering Without Domain Expertise; SciPy 2013 Presentation : An example of how "simple" feature engineering can be. Peter Prettenhofer - Gradient Boosted Regression Trees in scikit-learn : The speaker touches quickly on analyzing feature importance at the end.
