[site]: datascience
[post_id]: 76409
[parent_id]: 76408
[tags]: 
RmsProp is a adaptive Learning Algorithm while SGD with momentum uses constant learning rate. SGD with momentum is like a ball rolling down a hill. It will take large step if the gradient direction point to the same direction from previous. But will slow down if the direction changes. But it does not change it learning rate during training. But Rmsprop is a adaptive learning algorithm. That means it adapts it learning rate using a moving average of it's gradient's square value. As the value of the moving average increases, the learning rate becomes more and more small allowing the algorithm to converge. RMSProp: $ g = \frac{1}{m} \sum_{1}^{m} L(\hat{y},y) $ $ r = \delta r + (1 - \delta) g \circ g $ $ \Delta\theta = - \frac{\epsilon}{\sqrt{r+\delta}} \circ g$ $ \theta = \theta + \delta\theta $ Here m is the minibatch size and r is the moving average value and g is gradient and theta is parameters. SGD With Momentum: $ g = \frac{1}{m}\sum_{1}^{m} L(\hat{y}, y) $ $ v = \alpha v - \epsilon g$ $ \theta = \theta + v $ Here v is the velocity of Momentum. (Adapted From Deep Learning By GoodFellow)
