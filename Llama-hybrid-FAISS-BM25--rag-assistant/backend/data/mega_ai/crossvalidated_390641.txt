[site]: crossvalidated
[post_id]: 390641
[parent_id]: 388933
[tags]: 
The difference between an "endogenous" breakpoint and an "exogenous" breakpoint is that the former is inferred from the data being used in the estimation, but the latter is not. Consider the following chart fragment, copied from [Macrotrends]: 1 We can deduce that there was a breakpoint in this series around 1973 by looking at the data, and if we look at the underlying data we can pinpoint it quite well. It's pretty obvious, but it's still an endogenous breakpoint, and a model fit on the underlying data should reflect the fact that, in addition to the parameters being fit (on whatever model is being used), we are also estimating the precise location of the breakpoint from the same data used to estimate the parameters. However, if we happen to know that on October 17, 1973, Arab oil producers cut production by 5% and instituted an oil embargo against the U.S. and other countries, we no longer need to deduce the existence and location of the breakpoint from the data underlying this plot, which is of West Texas Intermediate crude oil prices. In this case the breakpoint is exogenous - its presence and location is determined from data not used in the model. In the case of something this obvious, we might well assume that there must have been an event and not bother with degrees of freedom adjustment, but that's another topic. Alex's answer is correct and hard to improve upon. I'll expand it to the related issue of t-statistics on the non-breakpoint parameters by constructing an example. The actual data generating process is $y_t = e_t$ , $e_t$ i.i.d. Normal(0,1), but we are going to estimate the model $y_t = \beta x_t + e_t$ instead, with and without a breakpoint also estimated from the data. The breakpoint is a constant shift in $y$ , so the model with breakpoint $\tau$ is $y_t = \alpha 1(t>\tau) + \beta x_t + e_t$ . The trouble comes about because $\tau$ is estimated from the data. Our null hypothesis is that $\beta = 0$ . We generate the $x_t$ from a Normal(0,1) distribution, uncorrelated with the $e_t$ of course: tstat for (i in 1:length(tstat)) { y 1.96) [1] 0.0539 for (i in 1:length(tstat)) { y $residuals) if (std_error coefficients[2,3] } } } mean(abs(tstat)>1.96) [1] 0.0702 > sqrt(0.95*0.05/10000) [1] 0.002179449 Note that the initial t-test for $\beta = 0$ has about the right rejection rate; 0.0539 is 1.79 standard deviations above 0.05, but the rejection rate of 0.0702 when we are simultaneously using the data to choose and estimate the magnitude of the breakpoint is 9.27 standard deviations above 0.05, indicating that we will reject the null hypothesis of $\beta = 0$ too often - even though the breakpoint is on a different variable (the constant term, in this case.) Selection bias. One can make an analogy to selection bias, but it's not clear to me that it's a useful one. The classic selection bias arises when some characteristic of the individual related to the outcome of a test is also related to whether the individual is placed in the control or test group. In this case, we have two groups - the pre-breakpoint and the post-breakpoint group - and, when determining if and where a breakpoint occurs from the data, clearly each data point has some (small) influence on that decision. Consider our example above; if $y_{38} = 2.1$ , it's going to tend to cause the breakpoint estimate to be lower than $\tau=38$ , and will also tend to cause the post-breakpoint estimate of the constant term to be $> 0$ . Therefore each data point both has some influence on whether it's selected into the pre-breakpoint or post-breakpoint group and has some influence on what the estimated value of the relevant variables are in whichever group it's assigned to. As Alex observes, the supF approach constructs a test for whether there is a breakpoint that corrects for the fact that we searched for the "best" breakpoint using the same data as we are using to calculate the likelihood ratio used in the test. It doesn't completely eliminate the selection bias problem - after all, if you test at a 95% level of confidence, you're expecting to have a false positive about 5% of the time, and, even if the null hypothesis is false, the data itself may cause you to mis-estimate where the structural break is. However, it eliminates that part of it caused by the fact that our search for the breakpoint results in biased test statistics relative to the classic situation where we are testing one, and only one, location for the breakpoint.
