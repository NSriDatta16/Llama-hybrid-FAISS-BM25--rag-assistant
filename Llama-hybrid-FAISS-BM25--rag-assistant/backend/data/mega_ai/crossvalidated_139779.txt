[site]: crossvalidated
[post_id]: 139779
[parent_id]: 
[tags]: 
Alternative to AIC for feature selection in classification

I want to know what are the most common methods for feature selection in classification problems (binary and mutli-class). I see in Chapter 6 of Zumel and Mount that they use AIC before they train classification algorithms (trees, logistic regression, kNN) on a classification problem with both categorical and numerical features. They compute AIC as $2\,\left(\log L- \log L_{base}\right)-2^S$ for categorical variables, and $2\,\left(\log L- \log L_{base}\right)-1$ for numeric variables ($L$ is likelihood, $L_{base}$ is likelihood of the saturated model, $S$ is entropy). They keep features with AIC above a certain threshold, which presumably can be modified to improve algorithm performance. What are the alternatives, and when should I consider using them? In specific, how should I do feature selection if I plan to train with a boosting algorithm (adaboost or gbm)? Are there any dangers in throwing all the noisy variables (without any feature selection) to adaboost or gbm, as they seem to not be perfectly immune to overfitting?
