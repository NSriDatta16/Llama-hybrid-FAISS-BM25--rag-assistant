[site]: crossvalidated
[post_id]: 225796
[parent_id]: 225734
[tags]: 
Although there are many good answers to this question I believe some important points where left behind and since this question came up with a really interesting point I would like to provide yet another point of view. Why isn't variance defined as the difference between every value following each other instead of the difference to the average of the values? The first thing to have in mind is that the variance is a particular kind of parameter, and not a certain type of calculation. There is a rigorous mathematical definition of what a parameter is but for the time been we can think of then as mathematical operations on the distribution of a random variable. For example if $X$ is a random variable with distribution function $F_X$ then its mean $\mu_x$, which is also a parameter, is: $$\mu_X = \int_{-\infty}^{+\infty}xdF_{X}(x)$$ and the variance of $X$, $\sigma^2_X$, is: $$\sigma^2_X = \int_{-\infty}^{+\infty}(x - \mu_X)^2dF_{X}(x)$$ The role of estimation in statistics is to provide, from a set of realizations of a r.v., a good approximation for the parameters of interest. What I wanted to show is that there is a big difference in the concepts of a parameters (the variance for this particular question) and the statistic we use to estimate it. Why isn't the variance calculated this way? So we want to estimate the variance of a random variable $X$ from a set of independent realizations of it, lets say $x = \{x_1,\ldots,x_n\}$. The way you propose doing it is by computing the absolute value of successive differences, summing and taking the mean: $$\psi(x) = \frac{1}{n}\sum_{i = 2}^{n}|x_i - x_{i-1}|$$ and the usual statistic is: $$S^2(x) = \frac{1}{n-1}\sum_{i = i}^{n}(x_i - \bar{x})^2,$$ where $\bar{x}$ is the sample mean. When comparing two estimator of a parameter the usual criterion for the best one is that which has minimal mean square error (MSE), and a important property of MSE is that it can be decomposed in two components: MSE = estimator bias + estimator variance. Using this criterion the usual statistic, $S^2$, has some advantages over the one you suggests. First it is a unbiased estimator of the variance but your statistic is not unbiased. One other important thing is that if we are working with the normal distribution then $S^2$ is the best unbiased estimator of $\sigma^2$ in the sense that it has the smallest variance among all unbiased estimators and thus minimizes the MSE. When normality is assumed, as is the case in many applications, $S^2$ is the natural choice when you want to estimate the variance.
