[site]: crossvalidated
[post_id]: 320362
[parent_id]: 320357
[tags]: 
The major advantage of using a multi-layered perceptron is that it is a non linear classifier. Every neuron in the current layer can be computed using the following formula: $$h_i(x_t)=f_i(\mathbf{x}_{t-1})$$ where $\mathbf{x}_{t-1}$ is the subset of the input neurons that will participate in the current neuron computation, and $f_i$ is a non linear function. Generally, the transformation is $g_i(\mathbf{wx_{t-1}+b})$ where $\mathbf{w}$ is the weight that is multiplied to the input neurons to the current layer, and $\mathbf{b}$ is the optional bias. This is however, a linear transormation. The main difference that makes a neural network (or for that matter any multi-layered perceptron) is the non linear function $g_i$ which essentially determines if the neuron under consideration will "$\textit{fire}$" or not. Additionally, in the application of computer vision, early layers determine the low level features such as edges in the input image, and later layers determine the higher level features such as objects, presence of a particular color etc.
