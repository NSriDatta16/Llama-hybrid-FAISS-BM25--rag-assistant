[site]: crossvalidated
[post_id]: 430773
[parent_id]: 268468
[tags]: 
Because each tree grown by xgboost splits y into leaf nodes conditional on X. The prediction (or leaf weight) is the value which minimises the loss function for the split y's - so for example if the loss function is squared error, the leaf weight is the mean of the y's. So say you have 1 tree of depth 1 (i.e. a single split on 1 feature). All this model can return is two discrete values, the mean of the y's for either the left or right leaf. So even a simple linear response takes a large number of splits / trees and will still never produce a smooth function. There are regression trees which apply a linear regression to the X,y in each leaf node but `modern' implementations like xgboost, catboost, lightgbm etc don't do this. Edit: also because of this, trees cannot extrapolate outside the data they were trained with
