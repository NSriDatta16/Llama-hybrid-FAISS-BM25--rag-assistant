[site]: crossvalidated
[post_id]: 453094
[parent_id]: 
[tags]: 
How to derive the gradient of the reparameterized score function estimator?

In the paper Evolution Strategies as a Scalable Alternative to Reinforcement Learning , the authors derive the following gradient of the score function estimator $$ \begin{align} \nabla_\psi\mathbb E_{\theta\sim p_\psi}[F(\theta)]&=\mathbb E_{\theta\sim p_\psi}[F(\theta)\nabla_\psi\log p_\psi(\theta)]\\ &=\nabla_\theta\mathbb E_{\epsilon\sim\mathcal N(0,I)}F(\theta+\sigma\epsilon)\\ &={1\over\sigma}\mathbb E_{\epsilon\sim\mathcal N(0,I)}\{F(\theta+\sigma\epsilon)\epsilon\} \end{align} $$ where $p_\psi$ is a multivariate Gaussian, $F$ is the objective function(e.g., return on reinforcement learning problems) and $\theta+\sigma\epsilon$ is the result of the reparameterization trick. The meaning of $\theta$ changes from samples from $p_\psi$ to the mean of the $p_\psi$ in the second step, following the same convention used by the paper. I'm wondering how the last step is derived?
