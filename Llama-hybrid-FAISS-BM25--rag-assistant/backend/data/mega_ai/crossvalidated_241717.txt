[site]: crossvalidated
[post_id]: 241717
[parent_id]: 241715
[tags]: 
"I thought we wanted the output to be as close to 0 as possible" There are two functions involved here. There's the output of the neural network which you want to be accurate, and there's the cost function which you want to minimize. Perhaps it's the output of the neural network that you want to be close to 0. However, this seems unlikely because a neural network for binary classification has some targets of $1$ and some targets of $0$ so you wouldn't want all the outputs to be zero. The cost function used is often $\Sigma_{i=1}^{m} (y_i-T_i)^2$ That's the sum of the squared difference between the $i^{th}$ output ($y_i$) and the target ($T_i$). Clearly this cannot be negative so at a previous stage Andrew Ng might have said we want the cost function to be close to zero. However, other cost functions can be negative. Also, including regularization might add one some quantities which can be negative and these will allow the cost function to go below zero.
