[site]: stackoverflow
[post_id]: 4989272
[parent_id]: 4989002
[tags]: 
Asssume some event type happens on average once per 10 seconds, and you want to print a simulated list of timestamps on which the events happened. A good method would be to generate a random integer in the range [0,9] each 1 second. If it is 0 - fire the event for this second. Of course you can control the resolution: You can generate a random integer in the range [0,99] each 0.1 second, and if it comes 0 - fire the event for this DeciSecond. Assuming there is no dependency between events, there is no need to keep state. To find out how many times the event happens in a given timeslice - just generate enough random integers - according to the required resolution. Edit You should use high resolution (at least 20 randoms per period of one event) for the simulation to be valid.
