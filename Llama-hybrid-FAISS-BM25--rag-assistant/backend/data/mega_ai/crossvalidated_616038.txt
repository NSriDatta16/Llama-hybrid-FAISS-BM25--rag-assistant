[site]: crossvalidated
[post_id]: 616038
[parent_id]: 616033
[tags]: 
What you observe there has been proved in this arxiv-preprint by Pires & Branco . It may also be published elsewhere but I haven't seen it. Note that there is a problem here, which is that the (sample) Mahalanobis distance is intentionally defined to be affine equivariant; it gives all directions in n-dimensional space the same importance for standardisation, and will standardise in any direction according to the variance of observations along that direction. This means that the distance between any two observations is assessed relative to the overall variance/covariance structure ("Covstructure") of the point cloud. What is shown in the preprint is that if $n\ge M-1$ , information in the data regarding the Covstructure is so scarce that every observation "on average" determines one dimension worth of variance/covariance entries, and in consequence every observation has the same Mahalanobis distance from every other observation (no observation can be shown as in any direction "outlying" with respect to others, as each of them points into an idiosyncratic direction and none is in any sense "between" others, so to say - note that I'm assuming data in "general position", i.e., spanning an as high-dimensional hyperplane as possible). This is a consequence of affine equivariance, which could be seen as the very essence of Mahalanobis distance. Arguably, if you penalise it, it will morph into something that is really quite different, and the name Mahalanobis distance may no longer be justified. The resulting distance will have to focus on some directions in $n$ -dimensional space over some others, when it comes to standardising against the variation of the overall point cloud, which runs counter to the basic idea of Mahalanobis. For example, Euclidean distance with standardised variables is not affine equivariant and will use the variance along the main coordinate axes for standardisation. Variation in other directions due to correlation will be ignored. If you want to penalise in such a way that standardisation is mainly governed by the main coordinate axes, chances are you'll get something of a mix between the Mahalanobis-distance (which is a constant) and standardised Euclidean, which presumably amounts to being more or less equivalent to standardised Euclidean (depending on how exactly you use it). Note also that even Euclidean distance (with and without standardisation) has characteristics in high-dimensional settings that seem undesirable to many, see Hennig, C.: Minkowski distances and standardisation for clustering and classification on high dimensional data. In: Imaizumi, Tadashi, Nakayama, Atsuho, Yokoyama, Satoru (Eds.) “Advanced Studies in Behaviormetrics and Data Science. Essays in Honor of Akinori Okada”, Springer Singapore (2020), p. 103-118.
