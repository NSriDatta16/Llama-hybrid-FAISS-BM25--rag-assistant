[site]: crossvalidated
[post_id]: 493579
[parent_id]: 
[tags]: 
Lost in multiple comparisons: is there a princepled way out?

Suppose I am planning to run some well-powered factorial experiment based on a random sample with several treatments and several levels. I am interested in the effects of all of all levels vs. some baseline for each treatment. One can fit a regression model with dummy variables for all levels of all treatments (except baselines) to obtain an unbiased estimate of these "ATEs". This setting also justifies standard assumptions for a neymanian estimator of the variance and one can apply two sided t-tests for all treatment indicators (for simplicity, H0 of no effect, alpha 0.05). Naturally, a multiple comparisons problem will arise. What is a princepled way to approach it and incorporate it in design analyses prior to conducting a study? Bonferroni and other procrustean alpha corrections assume extreme loss functions and will likely do more harm than good. How would one approach the issue from a bayesian perspective? Links to relevant literature are also greatly appreciated (I am aware of several bio stats papers, e.g. by Sander Greenland, but none of these were particulary eluminating, offering only vague guidance a la: "context specific loss functions have to be considered" but not how to do this)! With the example above, I wanted to set up a case were the interest really lies in testing many different hypotheses for many causally identified parameter estimates. Often, the debate drifts into discussions about what relevant comparisons are.
