[site]: datascience
[post_id]: 75997
[parent_id]: 67074
[tags]: 
When you duplicate a feature, you may have to decrease the learning rate to avoid oscillation or divergence, even for a simple model like linear regression. If we have a one-dimensional input as X and duplicate the feature there, the gradient will be about twice as large than it would otherwise be, and if our learning rate is, say, 0.05, the new step size may be large enough to prevent gradient descent from converging. Even if gradient descent does not diverge, training will be slower. See https://towardsdatascience.com/feature-selection-why-how-explained-part-1-c2f638d24cdb , or this answer by Winks ( https://stats.stackexchange.com/a/191364/267884 ): Gradient Descent works 'best' when the direction of the gradient at each iteration points to the optimal point; that is, you could minimize each $\beta_i$ [a weight that the model learns] separately and get to a good answer. This is possible when the function to optimize is strictly convex. But when inputs are highly correlated, this is no longer the case. Obviously, it is not possible for neural networks since the function is not convex to begin with, but it has effects on reaching the local minimum as well.
