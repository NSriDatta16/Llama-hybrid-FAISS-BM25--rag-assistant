[site]: datascience
[post_id]: 19706
[parent_id]: 19700
[tags]: 
Typically you would use a perplexity value. For example, if your LSTM model is word-based and you have a sentence $[x_1, x_2 . . . x_N]$, and your model predicts the words that appear in that sentence with probability $p(x_i|x_0..x_{i-1})$ (where $x_0$ is a "start token" or whatever you use to start your RNN prediction sequence). Then you might quote a per-word perplexity for that sentence under your model as $- \frac{1}{N}\sum_{i=1}^N log_2(p(x_i|x_0..x_{i-1}))$ Using an LSTM to predict consecutive words, it is practical to construct an array of probabilities $[p_1, p_2 . . . p_N]$ by running the network on the sentence and noting the probabilities for the correct matching word - i.e. $p_i = $ the predicted probability of the correct class $x_i$ at each step, which simplifies the expression: $- \frac{1}{N}\sum_{i=1}^N log_2(p_i)$
