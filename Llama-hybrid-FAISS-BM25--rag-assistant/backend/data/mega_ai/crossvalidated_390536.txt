[site]: crossvalidated
[post_id]: 390536
[parent_id]: 
[tags]: 
Convergence to gradient in limit of variance

I came across this equation in the original GAN paper (pg 2 https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf ): $$\lim_{\sigma \rightarrow 0} \nabla_{\bf x} \mathbb{E}_{\epsilon \sim \mathcal{N}(0, \sigma^2 I)}[{f({\bf x} + \epsilon)}] = \nabla_{\bf x} f({\bf x}).$$ Is this a well known result, or easy to prove? Not sure how to proceed beyond the following: $$\begin{align*} \lim_{\sigma \rightarrow 0} \nabla_{\bf x} \mathbb{E}_{\epsilon \sim \mathcal{N}(0, \sigma^2 I)}[{f({\bf x} + \epsilon)}] &= \lim_{\sigma \rightarrow 0} \int_{\epsilon} \nabla_{\bf x} f({\bf x} + \epsilon) p(\epsilon) d \epsilon \end{align*}. $$ Any tips or sources would be greatly appreciated.
