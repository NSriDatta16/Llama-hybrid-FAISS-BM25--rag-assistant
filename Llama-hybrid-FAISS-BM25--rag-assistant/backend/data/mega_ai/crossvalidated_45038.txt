[site]: crossvalidated
[post_id]: 45038
[parent_id]: 44999
[tags]: 
Actually both of you definitions work in different cases, it depends on how you define your null hypothesis (which is often affected by the way you state your alternative hypothesis, so it does matter). If your null hypothesis is strictly that the parameter(s) equal a given value (or set of values, 1 per parameter), e.g. $H_0: \mu=\mu_0$ then your first definition works (well with $f(x) \le f(x_0)$). This is the 2-tailed test in the traditional simple statistics cases. But often we are interested only in the alternative being in a certain direction, the one-tailed test case. E.g. If I want to prove that my new pain reliever is better than aspirin (takes less time for the headache to go away on average) then I am only interested in 1 tail and my alternative would be $H_a: \mu In practice, most common test statistics follow a unimodal distribution (or are close enough) under the null hypothesis, so both definitions are the same. The only common case I know of where all possible cases with lower likelihood are included in the p-value is Fisher's exact test for tables biger than $2\times2$. So to sum up. Your thinking is generally correct, cases that you suggest are just rare enough that most books/classes only present the simpler version.
