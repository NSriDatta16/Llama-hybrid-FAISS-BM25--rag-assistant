[site]: crossvalidated
[post_id]: 544266
[parent_id]: 323945
[tags]: 
My question is this: Is there an accepted value for the Kolmogorov metric that has been used to justify sufficient normality for the purposes of confidence intervals? No. Different confidence interval procedures have difference sensitivities to normality, and various requirements of the work can influence how much deviation from the theoretical coverage is acceptable. $^{\dagger}$ For instance, while confidence intervals for the mean that are based on the t-test are known for their robustness to deviations from normality, confidence intervals for variance that are based on the F-test are known for their lack of robustness to such deviations. $^{\dagger}$ Is $94.5\%$ coverage good enough for a $95\%$ confidence interval? Under most circumstances, I would say so, but perhaps you have an application where that is inadequate.
