[site]: datascience
[post_id]: 66906
[parent_id]: 66899
[tags]: 
The main problem I see here is that OHE is almost never a good idea with that many categories. With neural networks you will usually get better performance by using embeddings. So instead of X1 -{OHE}-> 10,000 -> {..} -> 1,000 you could go straight to X1 -{embedding}-> 50 , where the embedding dimension should probably a lot lower than 1,000. Either way, more important is that one calculates feature importance in a coherent framework. The easiest one is probably permutation importance (another could be shap values), which basically comes down to (repeatedly) shuffling the input features X1 and X2 and record the impact on Y. This works also for OHE variables if you make sure to shuffle X1 before one-hot encoding (and not shuffling the 10,000 encoded vectors independently). In such a framework X2 doesn't need to be "magnified" for a fair comparison with X1.
