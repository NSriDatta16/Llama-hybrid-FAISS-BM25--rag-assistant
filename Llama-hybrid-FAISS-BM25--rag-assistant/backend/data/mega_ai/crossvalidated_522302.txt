[site]: crossvalidated
[post_id]: 522302
[parent_id]: 
[tags]: 
Multicollinearity in Kernel Ridge Regression using 1-hot features

I am working on an ML model using Kernel Ridge Regression method. My features include a categorical 1-Hot encoded vector, and a proper $\mathbb{R}^n$ vector derived from the 1-hot vector (in embedding like method). My training curves resemble: I would like to explain the behaviour of 1-hot features in training: it performs better in lower data regimes and becomes progressively worse in higher data size (green line is the $\mathbb{R}^n$ feature, errors measured against hold out set). My hypothesis is that as 1-hot features will only have few discrete kernel function values (the $\mathbb{R}^n$ feature has near continuous 0-1 distribution) it is becoming highly multicollinear and therefore due to high variance in coefficients, unseen hold-out samples incur huge errors. But not sure about the uptick. Also online resources seems confusing as many sources say multicollinearity does not influence the predictions (specially in ridge regression), while several claim that it can result in overfitting. Can anyone comment, or point out better resources to read up more on it? (I am a chemist by training only recently treading in theory of statistical learning etc)
