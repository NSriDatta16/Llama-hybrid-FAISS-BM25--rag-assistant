[site]: crossvalidated
[post_id]: 614846
[parent_id]: 
[tags]: 
Unconstrained Biases and Neural Network Regularization

In Bishop's PRML on page 259 he discusses a L2 regularizer for each layer of a 2-layer neural network, given by $$ \begin{equation} \frac{\lambda_1}{2}\sum_{w\in W_1}w^2 + \frac{\lambda_2}{2}\sum_{w\in W_2}w^2 \end{equation} $$ He then explains that this corresponds to the prior distribution $$ \begin{equation} p( w| \alpha_1, \alpha_2) \propto \exp{\left( -\frac{\alpha_1}{2} \sum_{w \in (1)}w^2 - \frac{\alpha_2}{2} \sum_{w \in (2)}w^2 \right)} \end{equation} $$ which is "improper because the bias parameters are unconstrained." I'm having trouble understanding why this distribution is improper, and why he considers the biases to be unconstrained?
