[site]: crossvalidated
[post_id]: 538595
[parent_id]: 
[tags]: 
What are some uses of logistic regression at scale?

Many libraries that scale linear and logistic regression assume a tall-skinny design matrix (many samples, few features), but I don't understand why you would need billions of samples if your data has 250 features. In what scenarios would more data help? It seems like, instead of using more computational resources, you could simply sub-sample the data and achieve comparable accuracy in scenarios where the feature count is relatively small. When feature count is high, say 100k, would having samples on the order of billions help? How do we decide how many samples are needed for a given modeling problem? Perhaps more data helps with imbalanced data? e.g. when performing binary logistic regression for outlier detection, where the occurrence of positive samples is very small. Any help here would be greatly appreciated.
