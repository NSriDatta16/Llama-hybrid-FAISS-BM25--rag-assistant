[site]: crossvalidated
[post_id]: 366966
[parent_id]: 366692
[tags]: 
Even though x1 and x2 appear similar (their correlation is very high, as remarked by @Henry), the construction of x2 as the residual after regressing x on z imposes structure on x2 that drastically affects parameter estimation. On each iteration of your simulation, the x2 values not only sum to zero, they also are uncorrelated with the z values. As a result, in model m2 the slope estimate has mean zero but considerably smaller variance than in model m1 , as you can check by comparing histograms of $\hat\beta_1$ (it's the numerator that gets much tighter). This leads to a t-statistic that has smaller variance than would be expected under the normal-theory null hypothesis, and higher p-values on average. A simpler situation: Consider testing $\mu=0$ for a Normal($\mu=0$,$\sigma^2$) sample. Of course the null hypothesis is true, so P-values will be uniform over $[0,1]$. But suppose we alter the sample by subtracting off the sample mean from every observation. Since the population mean is zero, the sample mean is pretty small, right? The seemingly innocuous act of subtracting off the sample mean leads to a test statistic that is identically zero , and a P-value that is always 1 . Loosely speaking, imposing such constraints yields a population such that normal theory thinks the null hypothesis is really, really true :).
