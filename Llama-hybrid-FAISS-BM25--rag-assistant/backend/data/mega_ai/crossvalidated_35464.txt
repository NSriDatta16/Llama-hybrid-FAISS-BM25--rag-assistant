[site]: crossvalidated
[post_id]: 35464
[parent_id]: 35461
[tags]: 
A couple of approaches: Use the pairwise p-values but adjust them for multiple comparisons using something like the Bon Feroni or False Discovery Rate adjustmetns (the first will probably be a bit over conservative). Then you can be confident that any that are still significantly different are probably not due to the multiple testing. You could create an overall test in the flavor of KS by finding the greatest distance between any of the distributions, i.e. plot all the empirical cdf's and find the largest distance from the bottommost line to the topmost line, or maybe average distance or some other meaningful measure. Then you can find if that is significant by doing a permutation test: group all the data into 1 big bin, then randomly split it into groups with the same sample sizes as your original groups, recompute the stat on the permuted data and repeat the process many times (999 or so). Then see how your original data compares to the permuted data sets. If the original data statistic fall in the middle of the permuted ones then there is no significant differences found, but if it is at the edge, or beyond any of the permuted ones then there is something significant going on (but this does not tell you which are different). You should probably try this out with simulated data where you know there is a difference that is big enough to be interesting just to check the power of this test to find the interesting differences.
