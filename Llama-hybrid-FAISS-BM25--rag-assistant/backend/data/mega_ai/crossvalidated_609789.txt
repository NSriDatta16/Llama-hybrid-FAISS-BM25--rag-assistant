[site]: crossvalidated
[post_id]: 609789
[parent_id]: 
[tags]: 
Putting a constraint on the output of the neural network

I am using a neural network to input some complex numbers and to obtain complex numbers. I converted the input complex numbers into real values by stacking the real part and imaginary parts as a vector. I used the ReLU function as an activation function. I need to put a constraint of unit modulus in the output value. I tried to implement the unit modulus constraint by calculating the amplitude of the complex numbers at the output of the activation function and rescaling the output with it. I used the rescaled version of the outputs to calculate the loss for the neural network. I am not getting satisfactory loss values from the neural network. I would like to clarify two things. Is it a good practice to modify (in my case rescaling) the output of the activation function before supplying it to the loss function? How can I impose the unit modulus constraint for the output in other ways?
