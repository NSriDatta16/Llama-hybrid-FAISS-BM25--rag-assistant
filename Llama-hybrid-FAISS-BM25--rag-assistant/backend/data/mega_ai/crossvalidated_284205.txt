[site]: crossvalidated
[post_id]: 284205
[parent_id]: 284189
[tags]: 
This is probably because there was no normalization done. Neural network are very sensitive to non-normalized data. Some intuition: when we're trying to find our multi-dimensional global minimum (like in the stochastic gradient descent model), in every iteration each feature "pulls" into its dimension (vector direction) with some force (the length of the vector). When the data is not normalized a small step in value for column A can cause a huge change in column B. Your code coped with that using your very low learning rate, which "normalized" the effect on every column, though caused a delayed learning process, requiring much more epochs to finish. Add this normalization code: from sklearn.preprocessing import StandardScaler sc = StandardScaler() x = sc.fit_transform(x) y = sc.fit_transform(y) And simply drop the learning rate param (lr) - letting it choose wisely an automatic value for you. I got the same desired chart as you now :)
