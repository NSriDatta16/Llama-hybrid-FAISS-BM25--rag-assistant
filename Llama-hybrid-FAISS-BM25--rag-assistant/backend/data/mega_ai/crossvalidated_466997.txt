[site]: crossvalidated
[post_id]: 466997
[parent_id]: 453956
[tags]: 
Prior predictive and posterior predictive checks may be helpful in here. In both cases you sample the predictions from the model (the "fake data"), in first case from the prior, in the second case from the posterior distribution, and then compare the distributions of the fake data, with the distribution of the observed data. Prior predictive checks are aimed to diagnosing the prior-data conflict , i.e. the model a priori does not make reasonable predictions that cover the possible range of the values observed in the data, it is ill-defined a priori. In posterior predictive checks you sample from the predictions after estimating the parameters (i.e. from posterior), so you check if the predictions that the model does fit the observed data. In both cases, there are many ways of doing this, depending on particular problem, ranging form eyeballing the histograms, density plots, scatter plots, summary statistics etc, up to defining more formal tests (data falls within the per-specified interval, hypothesis tests to compare the distributions, etc). This is a routine practice in Bayesian modeling. If I understand you correctly, the model that you use as example assumes that your data $X$ comes from a mixture of two Gaussians, with unknown means $\mu_1, \mu_2$ and known variances $\sigma^2_1, \sigma^2_2$ , and known constraint $c$ , such that $\mu_2 = c\mu_1$ . Simple way to test this model is to treat $c$ as free parameter, to be estimated. You know what $c$ should be, so you can come up with a strong, informative prior for it. In such case, it would surprise you if estimated $c$ differed from the true value. If I understand you correctly, that's the property of the model that you want to test. To test the validity of this assumption, you could take samples from the posterior distribution $\hat c_i$ , and compare them to the true value of $c$ , e.g. you would accept the model if at least in in $100\alpha\%$ cases, the predicted values for $c$ would be within the $\pm \varepsilon$ range from the truth $$ \alpha \le 1/n \sum_{i=1}^n \mathbf{1}(|c - \hat c_i| This is not exactly a posterior predictive check, since we may argue if $c$ is data, or not, but it follows the spirit of the kind of checks you would make to test model validity. Accidentally, Michael Betancourt has just published a lengthy Towards A Principled Bayesian Workflow tutorial, where among other things, he discusses importance of prior and posterior checks discussed above.
