[site]: crossvalidated
[post_id]: 305424
[parent_id]: 305367
[tags]: 
Your game is zero-sum - i.e. if one player wins, the other loses. Therefore if player A's reward is +1, player B's reward must be -1 and vice-versa. This allows you to take a variant of your approach (2). But instead of inverting the board or all the reward values when the player turn switches, just have the network switch between maximising or minimising the action value. The agent does need to know whose turn it is taking in that case in order to know whether it wants to take the argmax or argmin action. But this does achieve your goal of having a single model which the agent is able to use to play as either X or O. This is a common game theory approach, called minimax , and is used in many self-play reinforcement learning environments. Technically minimax is a separate algorithm to RL, and can be used in tree search for game solutions with any heuristic. In the case of tic-tac-toe a minimax tree search will work fine without any interim heuristic at all, just the eventual win/draw/loss rewards, due to small size of search tree. Effectively RL learns useful heuristics so that a tree search can have less depth or be more efficient. If you use just single-step greedy look ahead (an action value or afterstate value), then you can use minimax with RL and rely on the RL to learn accurate heuristics, no need for a tree search. In more complex games, then RL, minimax and tree search - plus alpha-beta pruning of the search for efficiency - can be used together. Regarding the neural network, try first using a tabular Q-learning approach (enumerating all after states, and estimating value of each one separately), as that is more reliable and perfectly suitable when there are not many possible game states. This will demonstrate that you have the Q-learning set up correctly before you add the extra complication of a neural network. Neural networks used in Q-learning can be unstable and need careful tuning of hyper-parameters to work, so you'll want to be sure you are adding one into a process that already works. Here is some pseudocode for a basic Q-learning algorithm that could be applied to any two player zero-sum game with perfect information, using notation similar to Sutton & Barto: Input: Reward function $R(s) \in \mathbb{R}$ , returning reward to player A when state changes to $s$ Input: Move function $M(s,p) \subset \mathcal{S}^{+}$ , allowed next states when starting in state $s$ for player $p$ Parameters: learning rate $\alpha$ , exploration rate $\epsilon$ , discount factor $\gamma$ Initialise: $V(s) \in \mathbb{R}, \forall s \in \mathcal{S}$ Initialise: $V(s) \leftarrow R(s)$ for all terminal states. Repeat (episode): $\qquad$ Init state $S$ $\qquad$ Init player $P \leftarrow playerA$ $\qquad$ Repeat (step): $\qquad\qquad$ If $P = playerA$ , then: $\qquad\qquad\qquad$ $S^* \leftarrow \text{argmax}_{s' \in M(S,P)} V(s')$ $\qquad\qquad$ Else: $\qquad\qquad\qquad$ $S^* \leftarrow \text{argmin}_{s' \in M(S,P)} V(s')$ $\qquad\qquad$ $V(S) \leftarrow V(S) + \alpha(R(S) + \gamma V(S^*) - V(S))$ $\qquad\qquad$ $S' \leftarrow S^*$ $\qquad\qquad$ With probability $\epsilon$ , $S' \leftarrow \text{random sample from } M(S,P)$ $\qquad\qquad$ Take the action: $\qquad\qquad\qquad$ $S \leftarrow S'$ $\qquad\qquad\qquad$ If $P = playerA$ , then: $\qquad\qquad\qquad\qquad$ $P \leftarrow playerB$ $\qquad\qquad\qquad$ Else: $\qquad\qquad\qquad\qquad$ $P \leftarrow playerA$ $\qquad$ Until $S$ is terminal The reward function can just be +1 if player A wins, 0 for a draw, -1 if player B wins, occurring on the terminal state. Also, you can just set $V(s) \leftarrow 0$ for all states initially. Typically for a simple game like tic-tac-toe you might set $\epsilon = 0.1, \alpha = 1.0, \gamma = 1.0$
