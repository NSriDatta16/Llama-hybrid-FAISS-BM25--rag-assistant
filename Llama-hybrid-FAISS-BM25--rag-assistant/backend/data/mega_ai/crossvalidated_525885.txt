[site]: crossvalidated
[post_id]: 525885
[parent_id]: 525795
[tags]: 
This is more of a comment really - There are some parts of Taleb's lecture that either I don't understand or I don't agree with. Taleb simulates the same experiment several times and he averages the p-values. In his setup the simulation is such that the average of the p-values is 0.11. His point is that because most of the p-values are well below the average, the p-value is a deceiving statistic. This is shown nicely in your histogram. My objection is that averaging p-values doesn't make sense. I can't tell why formally but I don't think probabilities is something you can average straightaway. Instead I would take the mean of the log-transformed pvalues: set.seed(1234) n The anti-log of this mean in this case is ~0.017, much smaller than 0.11. In my opinion, 0.017 is closer to "gut feeling" since you are simulating samples of 2000 data points, quite large to detect a difference of 0.1 with sd= 1. Other loosely related objections I have: Taleb says that p-values are stochastic - I see that since they are computed from random samples. However, the fact that a metric is stochastic doesn't mean that it is necessarily biased, which I think is what he let the audience understand. Also, the fact that p-values are abused and misunderstood (I agree), doesn't make p-values wrong or deceiving. Testing the same hypothesis multiple times will give small a pvalue in the long run and this is how p-values work, I see nothing wrong with it.
