[site]: crossvalidated
[post_id]: 67770
[parent_id]: 67755
[tags]: 
I will start with two (to my best knowledge) better ways of approaching the problem then subsampling the bigger set: Is it possible to oversample the second class instead? This way you will not lose any information (at the cost of additional computational complexity), Isn't your model able to use imbalanced datasets? Models like Support Vector Machines, Neural Networks etc. can deal with lack of balance on the model's definition/training algorithm execution. If none of above is possible, answer to your question should be rather data specific. I do not think that there can exist any general way of subsampling which "represent as closely as possible", because concept of "closeness" is to broad. Depending on your application you could expect to find a subset that is close in the sense of for example: distribution of points (shape of the distribution) maximum coverage (in the sense of facility location problem, which is $NP$-hard at its own) minimum summarized distances (in some metric) from the subset to the set You should provide much more details regarding your problem and used model for more detailed insight. But the choice will always be biased in some sense. The only generic approach that I can think of is performing cross-validation testing (possibly repeated many times) and selecting the best working part for some quality measure that you are concerned about (accuracy? f1? MCC?). This will also be biased, but biased towards quality of your classifier, which is your aim.
