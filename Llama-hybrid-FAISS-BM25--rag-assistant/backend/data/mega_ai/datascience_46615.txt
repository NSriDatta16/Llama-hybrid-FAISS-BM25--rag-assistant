[site]: datascience
[post_id]: 46615
[parent_id]: 46586
[tags]: 
Comparing models cannot (or should not) be done using a test set alone. You should always have a final set of data held out to estimate your generalization error. Letâ€™s say you compare 100 different algorithms. One will eventually perform well on the test set just due to the nature of that particular data. You need the final holdout set to get a less biased estimate. Comparing models can be looked at the same way as tuning hyperparameters. Think of it this way, when you are tuning hyperparameters, you are comparing models. In terms of requirements comparing random forest with 200 tress vs random forest with 500 trees is no different then comparing random forest to a neural net.
