[site]: datascience
[post_id]: 51859
[parent_id]: 51857
[tags]: 
Since you are preprocessing the data using Librosa ( maybe extracting MFCCs ), I assume that the preprocessing is correct and needs no intervention. On viewing the first two layers, model.add (LSTM (units = 512, return_sequences = True, input_shape = (X_train.shape [1], X_train.shape [2]))) model.add (LSTM (units = 32, return_sequences = False)) You can notice that the difference between the units present in the layers is quite huge ( 512 and 32 ). Hence the 512-dimensional data is packed in a 32-dimensional vector immediately in the next layer. You can try reducing this difference. You can use 64 - 32 units in both the layers. Also, most sources and blogs, say that for recurrent neural networks, RMS optimizer is the best choice. Since we use the hyperbolic tangent activation function, there is a problem with it called exploding gradients. RMSProp optimizer can handle such problems efficiently since the square root of gradients is utilised here. Try using the RMSProp optimizer with a low learning rate. I haven't tried this myself, but you can use recurrent or input dropout in LSTMs. They may help you to overcome the overfitting problem. Also, you can try adding some more Dense layers with Dropout layers in between them. Tip: LSTMs commonly face the problem of overfitting, so you can implement any of the other methods which are meant to eliminate overfitting. Also, as a common step you can try reducing the batch sizd during training.
