[site]: datascience
[post_id]: 55349
[parent_id]: 55323
[tags]: 
In the context of deep learning , normalization usually refers to the process of subtracting the mean and dividing by the standard deviation: $$ \hat{x_i}=\frac{x_i - \mu}{\sigma} $$ This kind of normalization is not related with the norm of a vector. Instead, it refers to the statistical notion you referred aimed at rescaling the values. In a statistical context, this approach is sometimes referred to as "standardization" of a random variable, which makes its mean $0$ and its standard deviation $1$ , assuming the original variable follows a normal distribution . There exist multiple types of normalization in deep learning, depending on what we normalize over. The most used ones are instance normalization, batch normalization and layer normalization. These are explained graphically in the image below, which is borrowed from article Group Normalization : Each subplot shows a feature map tensor, with N as the batch axis, C as the channel axis, and (H, W) as the spatial axes. The pixels in blue are normalized by the same mean and variance, computed by aggregating the values of these pixels. Each type of normalization has a different purpose, and some of them have shown their practical success but it's not clear why they work well. These are some authoritative references for the mainstream normalization strategies: Batch normalization: [Ioffe and Szegedy, 2015] , [Santurkar et al., 2018] Instance normalization [Ulyanov et al., 2016] Layer normalization: [Ba et al., 2016] Note that some of the normalization variants need further processing apart from the value normalization itself. For instance, with batch normalization you need to store $\mu$ and $\sigma$ to use them at inference time.
