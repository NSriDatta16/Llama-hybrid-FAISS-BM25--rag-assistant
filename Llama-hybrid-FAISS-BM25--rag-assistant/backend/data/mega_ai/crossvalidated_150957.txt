[site]: crossvalidated
[post_id]: 150957
[parent_id]: 
[tags]: 
What is the predictive distribution of Bayesian supervised Learning? (rigorous argument)

I was trying to understand the posterior predictive distribution for any supervised predictor (by that I mean any classifier or regression predictor $f$). The exact equation I am unsure of is: $$ p(y_{test} | x_{test} , S ) = \int_{\theta} p( y_{test} | x_{test} , \theta) p( \theta | S) d\theta $$ The exact source of confusion I have is how the above equation was derived from the conditional independence assumptions. These are my thoughts: To figure this out I tried drawing a graphical model for the above model but was unsure of what the correct model looked exactly. Usually, we assume that the data is generated from some true distribution $(x,y) \sim P^*(x,y)$. If that is the case then I decided to draw the following graphical model as follows: So according to my model: $$(x^{(i)} , y^{(i)}) \perp (x^{(j)} , y^{(j)}) \mid \theta$$ if we consider what $p(y_{test} | x_{test} , S )$ really means via marginalization we get: $$ p(y_{test} | x_{test} , S ) = \int_{\theta} p( y_{test}, \theta | x_{test} , S) d\theta = \int_{\theta} p( y_{test} | \theta, x_{test} , S) p( \theta | S) d\theta $$ So how do we get the last step to be equal to: $$ \int_{\theta} p( y_{test} | x_{test} , \theta) p( \theta | S) d\theta $$ why can we cross off/ignore the data $S = \{ (x^{(i)}, y^{(i)})^n_{i = 1}\}$ given $\theta$?
