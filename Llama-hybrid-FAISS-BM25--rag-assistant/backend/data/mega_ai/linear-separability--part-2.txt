 be linearly separable provided these two sets of points are linearly separable. The number of distinct Boolean functions is 2 2 n {\displaystyle 2^{2^{n}}} where n is the number of variables passed into the function. Such functions are also called linear threshold logic, or perceptrons. The classical theory is summarized in, as Knuth claims. The value is only known exactly up to n = 9 {\displaystyle n=9} case, but the order of magnitude is known quite exactly: it has upper bound 2 n 2 − n log 2 ⁡ n + O ( n ) {\displaystyle 2^{n^{2}-n\log _{2}n+O(n)}} and lower bound 2 n 2 − n log 2 ⁡ n − O ( n ) {\displaystyle 2^{n^{2}-n\log _{2}n-O(n)}} . It is co-NP-complete to decide whether a Boolean function given in disjunctive or conjunctive normal form is linearly separable. Threshold logic A linear threshold logic gate is a Boolean function defined by n {\displaystyle n} weights w 1 , … , w n {\displaystyle w_{1},\dots ,w_{n}} and a threshold θ {\displaystyle \theta } . It takes n {\displaystyle n} binary inputs x 1 , … , x n {\displaystyle x_{1},\dots ,x_{n}} , and outputs 1 if ∑ i w i x i > θ {\displaystyle \sum _{i}w_{i}x_{i}>\theta } , and otherwise outputs 0. For any fixed n {\displaystyle n} , because there are only finitely many Boolean functions that can be computed by a threshold logic unit, it is possible to set all w 1 , … , w n , θ {\displaystyle w_{1},\dots ,w_{n},\theta } to be integers. Let W ( n ) {\displaystyle W(n)} be the smallest number W {\displaystyle W} such that every possible real threshold function of n {\displaystyle n} variables can be realized using integer weights of absolute value ≤ W {\displaystyle \leq W} . It is known that 1 2 n log ⁡ n − 2 n + o ( n ) ≤ log 2 ⁡ W ( n ) ≤ 1 2 n log ⁡ n − n + o ( n ) {\displaystyle {\frac {1}{2}}n\log n-2n+o(n)\leq \log _{2}W(n)\leq {\frac {1}{2}}n\log n-n+o(n)} See for a literature review. Support vector machines Classifying data is a common task in machine learning. Suppose some data points, each belonging to one of two sets, are given and we wish to create a model that will decide which set a new data point will be in. In the case of support vector machines, a data point is viewed as a p-dimensional vector (a list of p numbers), and we want to know whether we can separate such points with a (p − 1)-dimensional hyperplane. This is called a linear classifier. There are many hyperplanes that might classify (separate) the data. One reasonable choice as the best hyperplane is the one that represents the largest separation, or margin, between the two sets. So we choose the hyperplane so that the distance from it to the nearest data point on each side is maximized. If such a hyperplane exists, it is known as the maximum-margin hyperplane and the linear classifier it defines is known as a maximum margin classifier. More formally, given some training data D {\displaystyle {\mathcal {D}}} , a set of n points of the form D = { ( x i , y i ) ∣ x i ∈ R p , y i ∈ { − 1 , 1 } } i = 1 n {\displaystyle {\mathcal {D}}=\left\{(\mathbf {x} _{i},y_{i})\mid \mathbf {x} _{i}\in \mathbb {R} ^{p},\,y_{i}\in \{-1,1\}\right\}_{i=1}^{n}} where the yi is either 1 or −1, indicating the set to which the point x i {\displaystyle \mathbf {x} _{i}} belongs. Each x i {\displaystyle \mathbf {x} _{i}} is a p-dimensional real vector. We want to find the maximum-margin hyperplane that divides the points having y i = 1 {\displaystyle y_{i}=1} from those having y i = − 1 {\displaystyle y_{i}=-1} . Any hyperplane can be written as the set of points x {\displaystyle \mathbf {x} } satisfying w ⋅ x − b = 0 , {\displaystyle \mathbf {w} \cdot \mathbf {x} -b=0,} where ⋅ {\displaystyle \cdot } denotes the dot product and w {\displaystyle {\mathbf {w} }} the (not necessarily normalized) normal vector to the hyperplane. The parameter b ‖ w ‖ {\displaystyle {\tfrac {b}{\|\mathbf {w} \|}}} determines the offset of the hyperplane from the origin along the normal vector w {\displaystyle {\mathbf {w} }} . If the trainin