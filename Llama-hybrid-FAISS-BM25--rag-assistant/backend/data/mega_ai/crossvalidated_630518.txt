[site]: crossvalidated
[post_id]: 630518
[parent_id]: 
[tags]: 
[In a Neural Network, how do you compute for a neuron that only has one connection/weight?

Our professor has tasked us with computing the $\beta$ derivatives, weight updates and modified weights for a two-layer feed forward network. We are expected to do this by hand. I feel as if I am getting a solid grasp on backpropagation, but I'm a bit confused by something. The network he gave us has one hidden layer with three neurons and two inputs; $x_1$ and $x_2$ . Where $x1=\begin{bmatrix} 0 \\ 1\end{bmatrix} \;$ and $x_2 = \begin{bmatrix} 1 \\ 0\end{bmatrix}$ and a target vector $y = \begin{bmatrix} 1 \\ 1\end{bmatrix}$ . Now, I know that for computing the output of a neuron we use some sort of activation function $f$ to squish the sum of the inputted weights and vectors. But in this particular network some of the neurons only have one input. So for example, $w_1$ connects $x_1$ to the first neuron. There are no other inputs for this neuron. I know we define this as $f(w_1 \cdot x_1)$ . I'm confused about how you would compute this given you are only computing with one weight. Do you treat the weight as some sort of scalar value such that the neuron's output would be computed as $f(w_1\cdot \begin{bmatrix} 0 \\ 1\end{bmatrix})$ ? I would appreciate any help on this.
