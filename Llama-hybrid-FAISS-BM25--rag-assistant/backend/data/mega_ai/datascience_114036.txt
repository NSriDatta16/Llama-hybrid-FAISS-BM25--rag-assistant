[site]: datascience
[post_id]: 114036
[parent_id]: 114007
[tags]: 
Answer: Transformer based models used for sentence similarity have been trained on huge amounts of data where the text preprocessing part has been handled either at the tokenization step or by the attention mechanism of the transformer. Applying cleaning methods and then using the cleaned text as input will worsen the quality of the embeddings. The inputs now differ from the ones the model has been trained with. The attention mechanism will be the one who will neglect tokens that are meaningless and include ones that are meaningful. In that case, a comma or a number can be meaningful in some context and meaningless in another, hence why we should not clean the text on our own but use it as it is.
