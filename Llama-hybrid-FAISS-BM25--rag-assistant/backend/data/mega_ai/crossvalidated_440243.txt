[site]: crossvalidated
[post_id]: 440243
[parent_id]: 440057
[tags]: 
Since this is a homework problem, I will provide hints and relevant resources to get you started. For your problem you have the following variables, $$ x_{ij} \in [0, 10] \: [ \text{available data, annotation for image (i) by annotator (j)} ] \\ m_j \in \{0, 1\} \: [\text{indicates good or bad annotator}] \\ p(x_{ij} \mid m_j=0) = \frac{1}{10} \: [\text{distribution for bad annotator data}] \\ p(x_{ij} \mid m_j=1) \sim \mathcal{N}(\mu_i, \sigma) \: [\text{distribution for good annotator data}] \\ \beta: [\text{prior probability for good annotator}] $$ Based on this information you can formulate the log-likelihood for your problem as follows, $$ \text{ln}\:p(\textbf{X} \mid \mu , \sigma , \beta ) = \sum_{i=1}^{N}\:\text{ln}\:[\:p(x_{ij}\mid m_j=0) \times p(m_j=0) + p(x_{ij}\mid m_j=1) \times p(m_j=1)\:] \\ = \sum_{i=1}^{N}\:\text{ln}\: [ \frac{1}{10}\times(1-\beta)\: + \: \mathcal{N}(\mu_i, \sigma) \times \beta \:] $$ The EM algorithm is a maximum likelihood solution, which means you need to find expressions for $\mu_i, \sigma, \beta$ by taking the derivative of above equation and setting it to zero. Pattern recognition and Machine learning [ pages 435 - 439 ] by Bishop gives very detailed steps on how to derive the relevant equations for the EM algorithm. Once you derive the equations you can use the chart on pages. 438-439 of the book to estimate the parameters. Hope this helps!
