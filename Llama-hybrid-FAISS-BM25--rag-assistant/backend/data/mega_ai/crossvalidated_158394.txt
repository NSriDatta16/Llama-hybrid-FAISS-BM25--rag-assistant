[site]: crossvalidated
[post_id]: 158394
[parent_id]: 102850
[tags]: 
I think the result might be very similar, unless the classification algorithm you are using is biased. Permuting a single variable does not chance the characteristics of your dataset. If your dataset has $n$ records and $m$ features it will still have $n$ records and $m$ features if you permute one of them. If you delete one feature or set it to 0, the resulting dataset will have $m-1$ features. This is a subtle point: the accuracy on a dataset with $m$ features and the one on a dataset with $m-1$ are not directly comparable. Random forests (RF) usually uses the permutation approach: in order to compute the importance of a feature, we compare the decrease in accuracy after permutation. I guess that if you just delete that feature you are a bit less confident in comparing the resulting accuracy. For example, lets say we have 2 features $F_1$ binary and very predictive and $F_2$ with lots of categories and not predictive at all. RF is known to be biased towards $F_2$ because of the many categories. If you permute it, the characteristics of your dataset do not change and the difference in RF accuracy is just due to the predictiveness of $F_2$; If you delete it, the difference in RF accuracy might be higher because it takes into account that you helped the RF in decreasing its bias.
