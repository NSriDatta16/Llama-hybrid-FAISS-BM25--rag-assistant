[site]: crossvalidated
[post_id]: 406891
[parent_id]: 147594
[tags]: 
Short answer CARTs need help with capturing interactions. Long answer Take the exact greedy algorithm (Chen and Guestrin, 2016): The mean on the leaf will be a conditional expectation, but every split on the way to the leaf is independent of the other. If Feature A does not matter by itself but it matters in interaction with Feature B, the algorithm will not split on Feature A. Without this split, the algorithm cannot foresee the split on Feature B, necessary to generate the interaction. Trees can pick interactions in the simplest scenarios. If you have a dataset with two features $x_1, x_2$ and target $y = XOR(x_1, x_2)$ , the algorithm have nothing to split on but $x_1$ and $x_2$ , therefore, you will get four leaves with $XOR$ estimated properly. With many features, regularization, and the hard limit on the number of splits, the same algorithm can omit interactions. Workarounds Explicit interactions as new features An example from Zhang ("Winning Data Science Competitions", 2015): Non-greedy tree algorithms In the other question, Simone suggests lookahead-based algorithms and oblique decision trees . A different learning approach Some learning methods handle interactions better. Here's a table from The Elements of Statistical Learning (line "Ability to extract linear combinations of features"):
