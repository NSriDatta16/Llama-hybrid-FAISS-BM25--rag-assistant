[site]: crossvalidated
[post_id]: 144826
[parent_id]: 
[tags]: 
What is the difference between conditioning on regressors vs. treating them as fixed?

Sometimes we assume that regressors are fixed, i.e. they are non-stochastic. I think that means all our predictors, parameter estimates etc. are unconditional then, right? Might I even go so far that they are no longer random variables? If on the other hand we accept that most regressors in economics say are stochastic because no outside force determined them with some experiment in mind. Econometricians then condition on these stochastic regressors. How is this different from treating them as fixed? I understand what conditioning is. Mathematically, it means we make all observations and inference conditional on that particular set of regressors and have no ambitions to say that inferences, parameter estimates, variance estimates etc. would have been the same had we seen a different realization of our regressors (such is the crux in time series, where each time series is only ever seen once). However, to really grasp the difference between fixed regressors vs. conditioning on stochastic regressors, I am wondering if anyone here knows of an example of an estimation or inference procedure that is valid for say fixed regressors but breaks down when they are stochastic (and will be conditioned on). I am looking forward to seeing those examples!
