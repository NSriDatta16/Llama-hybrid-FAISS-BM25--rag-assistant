[site]: crossvalidated
[post_id]: 608955
[parent_id]: 
[tags]: 
Why is the confidence interval for difference in two proportions inconsistent with the p-value for this difference in R prop.test()?

I calculated the difference in 2 proportions using the prop.test() using some exemplary data I found in internet. > (ppt The p-value is 0.06457. The confidence interval for the difference in proportions is -0.001220547 to 0.614315785. If I calculate the p-value from the CI manually using the normal distribution: > z_score se $conf.int) / (2 * z_score) > z estimate / se > 2 * (1 - pnorm(abs(z))) [1] 0.05091551 that previous p-value does not agree with the obtained p-value using the provided confidence interval. When I calculate it from the logistic regression using the marginal effects, I get the same confidence interval that prop.test() gives and the p-value I calculated above. data m margins::margins_summary(m) factor AME SE z p lower upper GroupGr2 -0.3065 0.1570 -1.9522 0.0509 -0.6143 0.0012 But when I use ANOVA on this model with Rao test I get the p-value from prop.test() > anova(m, test="Rao") Analysis of Deviance Table Model: binomial, link: logit Response: Status Terms added sequentially (first to last) Df Deviance Resid. Df Resid. Dev Rao Pr(>Chi) NULL 36 51.266 Group 1 3.4809 35 47.785 3.4159 0.06457 . --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 So how is that possible, that prop.test() gives me p-value which does NOT come from the confidence interval provided by the same function? It looks like the confidence interval is Wald's, but the p-value is Rao score test. What is the sense behind it? I was taught that confidence interval should match the p-value. Here it does not, as they are calculated two different methods. It agrees in this scenario in statistical significance, but what if they do not? EDIT: My question is about the possible statistical reasons for using different methods for calculating the p-value and confidence interval in one method. I guess there are some considerations that statisticians agree on here, so I'm curious why this discrepancy is justified? EDIT: The linked answer: P value and confidence interval for two sample test of proportions disagree does not agree with my findings. It's clear, that the confidence interval I got is NOT THE WILSON CI, it's the WALD , so the linked answer is not correct. Again, my question is what is the reason behind reporting WALD confidence interval and Rao score test? The proposed answer, which led to close my post is wrong for 2 proportions, which is evidently visible in my findings. This can be found also here: https://stats.stackexchange.com/a/570528/382831 Another proof: > PropCIs::wald2ci(11, 16, 8, 21, conf.level=0.95, adjust="Wald") data: 95 percent confidence interval: -0.001220547 0.614315785 sample estimates: [1] 0.3065476 EDIT 3: It's NOT about using different distributions (it's the z statistic for the proportions, NOT t statistic, because of the s=p(1-p) mean-variance dependency!), as some people suggested in the comments! Moreover, I didn't do any mistake in the formula: se where 2 is NOT 1.96 or anything else rounded up, as some suggested that I use some "approximation". It's 1/2 of the CI width, "half-CI", the "precision" divided by the z_score: precision/z_score = (CI_length/2) / z_score = CI_length/(2*z_score).
