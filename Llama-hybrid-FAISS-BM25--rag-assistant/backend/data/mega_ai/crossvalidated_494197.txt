[site]: crossvalidated
[post_id]: 494197
[parent_id]: 494193
[tags]: 
Your learning curve looks good, mean squared error is decreasing for both test- and training set, and after a while the network has learned what it can apparently. "Not enough" is subjective, but this looks like it's learning, no obvious problems. That said, I have the following suggestions Scale your inputs to resemble a normal distribution. For input 1 to 3, the effect will be limited, you should just divide by standard deviation (be sure to estimate it from the test distribution). For the other two, apply a log-scaling first, then scale it. Also scale the output variable using a logarithm and scale it Your network seems a bit heavy, reduce the number of layers to two, and reduce the number of nodes in the hidden layers as well. For example, use 5 input - 5 hidden layer - 3 output 5 input - 3 hidden layer - 3 output EDIT: if it is true that inputs and outputs are independent, this whole exercise is pointless. You cannot make good predictions if your inputs don't say anything about the outputs. Good on @Dave for catching that fundamental issue. EDIT: Then the question is; how can the MSE still decrease? I'm guessing then this is because the network is learning the average outputs. If you initialize a network, especially with such unscaled predictors and outcomes, it will do much worse than predicting the average of the outputs. It will adapt the parameters so that it predicts the average for all outputs, which is the best you can do given independent (in other words, useless) information.
