[site]: datascience
[post_id]: 32965
[parent_id]: 
[tags]: 
How do I use rnn to forecast to n periods with limited data?

So this is my 1st time trying to run a small time-series dataset through an RNN , but after a lot of searching, I haven't been able to find, 1. How I can use this to forecast to n periods ? (like in the model.predict(start, end) function in ARIMA.) 2. Is there a better way of doing this using NN ? details on the data is given as comments in the code below. thank you. ''' total timeseries data points = 39 frequency = Months train = 36 test = 3 Need to forecast 3 periods ahead upto 42, currently using forecast horizon as 1 Limited data so used all in a single batch ''' periods = 35 f_horizon = 1 batch_size = 35 x_batches = dfp1[ : (len(dfp1) - (len(dfp1) % batch_size))].reshape(-1, periods, 1) # train data for t periods y_batches = dfp1[1 : len(dfp1) - (len(dfp1) % batch_size) + f_horizon].reshape(-1, periods, 1) # train data for t+1 periods #defining test data set def test_data(testdata): test_x = testdata[ :(len(testdata)-1)] # t test_y = testdata[1:len(testdata)] # t+1 return test_x, test_y test_x, test_y = test_data(dfp3[-4: ]) #since 35 points were used in training, used the last 4 points for testing outputs = 1 inputs = 1 hidden = 100 with tf.device('/cpu:0'): with tf.variable_scope('var', reuse = tf.AUTO_REUSE): x = tf.placeholder(tf.float32, [None, periods, inputs]) y = tf.placeholder(tf.float32, [None, periods, outputs]) #creating a basic rnn cell basicrnn = tf.nn.rnn_cell.BasicRNNCell(num_units = hidden, activation = tf.nn.relu) rnn_output, states = tf.nn.dynamic_rnn(basicrnn, x, dtype = tf.float32) stacked_rnn_output = tf.reshape(rnn_output, [-1, hidden]) stacked_output = tf.layers.dense(stacked_rnn_output, outputs) outputs_ = tf.reshape(stacked_output, [-1, periods, 1]) #using mape for loss fn loss = tf.reduce_mean(tf.abs(tf.divide(tf.subtract(outputs_, y), y))) * 100 learningrate = 0.001 training_op = tf.train.AdamOptimizer(learning_rate = learningrate).minimize(loss) init = tf.global_variables_initializer() epochs = 1501 with tf.Session() as sess: init.run() for i in range(epochs): sess.run(training_op, feed_dict = {x: x_batches, y: y_batches}) if i%100 == 0: mape = loss.eval(feed_dict = {x: x_batches, y: y_batches}) print("ep %s: %s"%(i, mape)) ''' unable to pass the 4 test data points since input size is 35, can I reshape x & y before I pass test input, would it alter the model ? also why is the loss fluctuating, and not steadily decreasing. ''' #y_pred = sess.run(outputs_, feed_dict = {x: test_x}) #print(y_pred)
