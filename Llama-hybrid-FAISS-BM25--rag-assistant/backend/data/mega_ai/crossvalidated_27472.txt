[site]: crossvalidated
[post_id]: 27472
[parent_id]: 18453
[tags]: 
In Chapter 6 of Bayesian Data Analysis by Gelman, Carlin, Rubin, and Stern, they give an example of checking the assumption of independence is binomial trials (it comes on page 163 of the second edition). Specifically, they consider the case where a sequence of observations $y_{1},\cdots,y_{n}$ are modeled as iid Bernoulli with a uniform prior on the probability of success, $\theta$. They argue that observing autocorrelation in the outcome sequence is evidence for non-independence. The way they quantify this is to set up a test statistic, $T$, equal to the number of switches between zero and one in the observed set. They simulate draws from the posterior distribution of their model and compute the test statistic for the drawn simulations, $T_{sim,k}$, where $k$ represents the number of times they repeat the whole process. They plot the histogram of this test statistic $T_{sim,k}$ and overlay the actual value observed in their data, $T$, which reveals that $T$ is far lower than they'd expect under the independence assumption. Finally, they reduce it down to a Bayesian p-value of the form $P(T_{sim}\leq T(y) | y)$, and they compute this p-value by simulating and checking the condition $T_{sim}\leq T(y)$. If your data are not discrete, you could consider computing an autocorrelation function and then testing the hypothesis that some of the coefficients for small lags should be close to zero if the data are independent.
