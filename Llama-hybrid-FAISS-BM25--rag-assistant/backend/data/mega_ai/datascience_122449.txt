[site]: datascience
[post_id]: 122449
[parent_id]: 122431
[tags]: 
To answer your question: Standardizing features is generally a good practice when using machine learning algorithms that are sensitive to the scale of the features, such as Support Vector Machines (SVM), k-Nearest Neighbors (k-NN), and Neural Networks. However, whether to standardize binary and ordinal features is not a straightforward decision and depends on the specific context and the algorithm you are using. For binary features, standardization will transform them into continuous variables, which might not be desirable, especially if they represent distinct categories. For example, if you have a binary feature representing "male" and "female", standardizing it might not make sense as it could lose its interpretability. For ordinal features, standardization could potentially distort the relative distances between the categories. For example, if you have an ordinal feature representing "low", "medium", and "high", standardizing it could change the relative distances between these categories. In general, it's a good idea to experiment with different approaches and see which one works best for your specific problem. You could try standardizing all features, only numerical features, or no features at all, and see which approach gives you the best results. Hope I answered your question.
