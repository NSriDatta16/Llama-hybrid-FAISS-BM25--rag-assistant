[site]: datascience
[post_id]: 92672
[parent_id]: 92670
[tags]: 
One possible solution is using a Stacking classifier as follows: from sklearn.ensemble import StackingClassifier # Assumed you already import all the models you are using dt = DecisionTreeClassifier(max_depth=DT_max_depth, random_state=0) rf = RandomForestClassifier(n_estimators=RF_n_est, random_state=0) xgb = XGBClassifier(n_estimators=XGB_n_est, random_state=0) knn = KNeighborsClassifier(n_neighbors=KNN_n_neigh) svm1 = svm.SVC(kernel='linear') svn2 = svm.SVC(kernel='rbf') lr = LogisticRegression(random_state=0,penalty = LR_n_est, solver= 'saga') estimators = [ ('rf', rf), ('svm1', svm1), ('svn2', svn2), ('xgb', xgb), ('knn', knn), ('lr', lr), ('dt', dt) ] Option 1: stacker = AdaBoostClassifier() model = StackingClassifier( estimators=estimators, final_estimator= stacker ) Option 2: base_model = StackingClassifier( estimators=estimators ) model = AdaBoostClassifier(base_estimator = base_model) Option 2 will be without question the most expensive* of both and as far as I understand, option 2 fits better what you are looking for. Strategy:
