[site]: crossvalidated
[post_id]: 34084
[parent_id]: 33996
[tags]: 
One approach is to use "progressive validation error" for SGD diagnostics as per "Beating the Hold-Out: Bounds for K-fold and Progressive Cross-Validation" Basically, every new case is first plugged into your loss function, and only then is passed to SGD, and the resulting loss function values are averaged across "recent" cases (typically, the loss function is printed out for each 2^Nth case, where N=1,2,3,.. using all data points since the last print out) This gives a decent estimate of the test error and avoids the computational problems you've mentioned (since you don't have to calculate the loss function using all data for every print out).
