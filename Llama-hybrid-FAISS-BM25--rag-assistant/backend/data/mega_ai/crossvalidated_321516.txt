[site]: crossvalidated
[post_id]: 321516
[parent_id]: 321493
[tags]: 
The reward is not "of" or "from" a specific state. It is a separate from the state, and received from the environment after each step. However, to be an MRP, the distribution of rewards must depend only only previous state. As this constraint is also true for the next state, it is OK to consider reward and next state linked, and also ok to evaluate them in any order (provided the problem definition allows for it). It is always ok to evaluate the expected reward in any order if you have the model of the Markov process, although for some problems it will be more awkward. After every time step $t$, in a MRP you will observe a reward $R_{t+1}$ and new state $S_{t+1}$. The precise timing of these observations does not matter much, other than that they occur before moving on to step $t+2$ By convention the reward is associated with the next time step instead of the current one. However, all the formulae for reinforcement learning work fine if you used the current time step for the reward (and still use next $t+1$ for next state). In some literature you may see that approach used. David Silver has simplified things slightly by considering the expected reward on the next time step. It doesn't matter for the correctness of the formulae whether you consider a reward to be gained due to exiting one state or entering another. In some scenarios, if makes more sense to assign different rewards depending on the state entered - so you end up calculating expected reward by iterating over all possible transitions. However, in the lectures you will notice that this is not the case, and the same reward is granted for exiting a state regardless of next state. This is not the most general way of doing things, but makes some of the calculations simpler for demonstration purposes.
