[site]: crossvalidated
[post_id]: 424906
[parent_id]: 424896
[tags]: 
The assumption for two class logistic regression and softmax to have the same values is the follows beta ( logistic regression ) = -(beta1 - beta2) Following code gives the matching numbers def sigmoid1(x): x = np.asarray(x, dtype=np.float) x = 1 / (1 + np.exp(-x)) return x def sigmoid0(x): x = np.asarray(x, dtype=np.float) x = np.exp(-x) / (1 + np.exp(-x)) return x def softmax(x): x = np.asarray(x) x = np.exp(x)/np.sum(np.exp(x)) return x x = .5 b = -2 b1 = 1 b2 = -1 sigmoid0(x*b), sigmoid1(x*b), softmax([x*b1, x*b2]) (0.7310585786300049, 0.2689414213699951, array([0.73105858, 0.26894142]))
