[site]: datascience
[post_id]: 117643
[parent_id]: 117634
[tags]: 
BERT computes a vector per input token. When the text is processed, it gets tokenized first. Your sentence gets tokenized like this: from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained("bert-base-cased") tokenzier.tokenize("My name is Aun") which gives 5 tokens: ['My', 'name', 'is', 'Au', '##n'] BERT uses 30k WordPiece vocabulary. The most frequent words remain intact, and less frequent words (such as Aun) get split into smaller pieces, eventually down to characters. This way, it ensures it can represent every word. As you can see in the Figure that you copied from the BERT paper, there are two technical tokens added: [CLS] and [SEP] ; together with the 5 input tokens, it gets 7 output states. You can use batches of more sentences. Batch size is the first number of the output tensor dimension: you input just one sentence, therefore 1. (Note that with multiple sentences, the tensor will be padded to the maximum length.) The last number, 768, is the dimension of BERT's hidden states. This was an arbitrary design decision of the model authors. The individual dimensions have no straightforward interpretation. For more about BERT's interpretability, I can refer you to a paper called A Primer in BERTology .
