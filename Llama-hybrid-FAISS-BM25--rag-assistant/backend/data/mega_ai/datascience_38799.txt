[site]: datascience
[post_id]: 38799
[parent_id]: 38797
[tags]: 
It's is a good practice to use batches to train neural networks. As Yann LeCun said : Training with large minibatches is bad for your health. More importantly, it's bad for your test error. Friends dont let friends use minibatches larger than 32. Although, it will not help you deal with large images. This is what convolutions are for. If you are using Keras, then the batch implementation is entirely made for you. If you use Tensorflow, then you can use the tf.data api or do your own implementation. The code you provided seem to work for your particular training size, but you may want to adapt it for all kinds of training size (namely using num_iterations = training_size // batch_size ) and making sure that the case where not all examples where shown to the network, the last remaining examples are included as well before ending the epoch... I have had good results with the Adam Optimizer, although you might want to tweak the hyperparameters to get better results.
