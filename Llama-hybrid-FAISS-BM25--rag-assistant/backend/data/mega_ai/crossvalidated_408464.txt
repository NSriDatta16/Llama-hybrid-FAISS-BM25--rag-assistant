[site]: crossvalidated
[post_id]: 408464
[parent_id]: 348341
[tags]: 
This does not address your specific questions, but the motivation behind them. The bigRF package may solve your problem: This is an R implementation of Leo Breiman's and Adele Cutler's Random Forest algorithms for classification and regression, with optimizations for performance and for handling of data sets that are too large to be processed in memory. Also: For large data sets, the training data, intermediate computations and some outputs (e.g. proximity matrices) may be cached on disk using "big.matrix" objects. This enables random forests to be built on fairly large data sets without hitting RAM limits, which will cause excessive virtual memory swapping by the operating system.
