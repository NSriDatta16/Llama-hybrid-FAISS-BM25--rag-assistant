[site]: crossvalidated
[post_id]: 44556
[parent_id]: 44400
[tags]: 
Your model is not identifiable. Consider that if you double $a_1$ and halve $a_2$, or double both $a_0$ and $a_1$ - or many other combinations of increases and decreases, the $Y$ value is unchanged - what could possibly tell you which of an infinite number of possible combinations of $a$'s goes with some particular value of $\frac{Y X_1 X_2}{X_0}$? If you rewrite the original model as $Y = b \frac{X_0}{X_1 X_2}$ where $b=\frac{a_0}{a_1 a_2}$, then you can identify $b$ -- e.g. by the kind of log-transform you suggest, or through a nonlinear least squares model, or through a GLM, or a number of other ways - but you cannot identify the individual $a$â€²s in your formula. The way it's set up, there is literally no information that pins them down individually - recall the various doublings and halvings of $a$'s that don't change $b$. You need something that would identify at least two of them separately (it's like knowing three things add to 100. It tells you nothing about the size of the individual things in the sum, only a relationship between them - but if you have some other way to pin down two of them, you can figure out the third). Only the combination I expressed as $b$ is identifiable with your present set-up. The issue is one of inherent non-identifiability in your model. http://en.wikipedia.org/wiki/Identifiability Fisher's comment here: http://en.wikipedia.org/wiki/Parameter_identification_problem#Estimation_methods_and_disturbances applies to your situation. Edit: Estimation of b by models on the original and the log-scale You notice in a comment that it's possible to write the model as a linear model in $Y$ as well as $\log Y$. You need to think about your error term. You can fit a no-constant regression to the model in original form, which would estimate $b$ (I'd suggest also trying the constant just as a check of the model assumption), and you can fit a model to $\log Y$ which has only an intercept (which would estimate $\log b$). But the assumptions the two models make about the error about the population mean are quite different. Consider the model fitted to the logs re-expressed as a model in the original $Y$s. In terms of the original $Y$s, the error term is multiplicative and the spread of points about the line is assumed proportional to the mean (that is, the points 'fan out' around an increasing mean/'fan in' around a decreasing mean. In the other model, the error is additive and the spread of points is constant when the mean changes. You should pay attention to what your model is saying and see whether it corresponds to the data. You also need to take care if you convert an estimate of $\log b$ back to an estimate of $b$. Means don't carry directly through nonlinear transformations.
