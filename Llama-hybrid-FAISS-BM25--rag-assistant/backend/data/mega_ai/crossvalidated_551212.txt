[site]: crossvalidated
[post_id]: 551212
[parent_id]: 551108
[tags]: 
The question is very long, and covering it in full would require a very lengthy answer. So here I will try to provide my view with very brief bullet points. The question over-emphasises the difference between machine learning and statistics. For a good reflection on those I recommend reading the answer by Michael I. Jordan which was given during his Q&A reddit session: link You use "pure" statisticians in an unusual way. From my experience most "pure" statisticians don't care about concrete data at all, they are more interested in creating estimators and proving properties about these estimators. Applied statisticians (of which machine learning practitioners is one type) then use these estimators on concrete datasets in order to answer practical questions. "Pure" statistician come up with methods and then test how well those methods behave under various contexts. Applied statisticians then use well-behaved estimators that align with the properties of their data. Machine learning practitioners do the same, they use well-tested concepts like cross-validation ( itself a product of statistics ) to estimate the accuracy of their models. p-value is a measure of uncertainty, not accuracy. The concept itself is best understood as a form of enhanced induction, where you test a theory about the real world by only having observed a few facts. For example p-value can be applied to check how certain we are about our measure of accuracy on the test set. Accuracy cannot be a substitute for a p-value. Consider a scenario where we ask if there is a difference between two classes, and the overlap of those classes is 99%. Whatever you do with pure prediction you will only be able to get an accuracy of 51%. But with a big enough sample size you will reach an arbitrary small p-value, stating that those two classes are indeed different. Contrary to the statement that statisticians don't care about model generalisation - it's the opposite. Statisticians care about the generalisation of everything, not just accuracy. That's what things like confidence intervals and p-values try to achieve - to give a hint how well some estimate on a sample generalises to a broader population. In my personal opinion one of the bigger differences between the communities of statistics and machine learning (with exceptions of course) is the overall context of the effort. Statisticians place more emphasis on assumptions and selecting the best tool/model for the job at hand. You understand your data, check some assumptions, have a question, and device the best strategy of answering that single question and nothing else. While machine learning people place emphasis on the best overall possible method. For example, I would bet that a lot ML practitioners have a hope of achieving the perfect model that can mimic the human brain and can learn everything and solve multiple problems without being pre-trained, etc. and seems like that is what a big chunk of the community is working towards.
