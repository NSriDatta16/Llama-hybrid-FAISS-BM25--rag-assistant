[site]: crossvalidated
[post_id]: 525382
[parent_id]: 524339
[tags]: 
MCMC algorithms are used to sample complex distributions, but a lot of point parameter estimation formulas can be reduced to a sampling problem. For example, the integral to compute the expected value of a random variable can be estimated using Monte-Carlo, which reduce the computation of the expected value to a sampling problem. The same for a maximum likelihood estimation: if you sample a random variable long enough, you can build an histogram and thus get an estimation of the maximum likelihood value of the variable. Note that, however, for most point estimation problems, generic MCMC methods are likely to be very slow compared to more efficient procedures. For example you would not use HMC to get a point estimate of a neural network parameters. A variant of the gradient descent algorithms would be much faster. It is important to note thus, that the Stochastic Gardient Descent algorithm, which is the goto method for neural network training today, can be considered to be a Markov Chain algorithm. (Also, The name training in the figure is an approximation. What you are performing at this step is inference, but it correspond to the training phase when compared to a point-estimate model workflow).
