[site]: datascience
[post_id]: 116647
[parent_id]: 116642
[tags]: 
You normally fine-tune pre-trained masked language models in a different "downstream" task, e.g. text classification. MLM and NSP pre-training is not an end, but just a means to profit from a large amount of unlabeled data in your supervised learning problem.
