[site]: datascience
[post_id]: 107338
[parent_id]: 107327
[tags]: 
Usually you first split your dataset into train/test set, and then if your model training process requires a validation set, you can further split your train-set into the final train-set and the validation-set. A simple rule is that the test set never shows up in your model development process, including when you develop your data preprocessing steps (such as your data normalizer). You need a validation set in the following cases: Training a gradient boosted decision tree (lgbm, xgboost, etc.) with early-stopping enabled. Because it needs to evaluate your model with a validation set after each training step to see if the early-stopping criteria is satisfied Training a neural network. This is optional but suggested because you can get the validation score curve in addition to the always-there training score curve to monitor if your model begins to overfit. This is required when you use early-stopping. You are doing cross validation. The idea is to fit the model with the same hyperparameter set N times but at each time it uses a different train-set and validation-set. In this way you know how the same set of hyperparameters works for different data scenarios. One validation set should only serve one purpose, so if you do both 1 and 3, then you first split your data into train/test set. Then at each cross validation round (out of N rounds), you split your train-set into another train-set and the $1^{st}$ validation set. Then in training your GBDT model, you again split your train-set into the final train-set and your $2^{nd}$ validation set. Your $1^{st}$ validation set is for cross validation purposes. Your $2^{nd}$ validation set is for GBDT early-stopping. For your code, I see two potential problems: If you source of data is channel_train , then both train_gen and valid_gen are getting from the same source but choices are different by the random generator. This is a problem because you do not guarantee your train and your valid data is mutually different. You called your valid gen two times. The first time it serves as the purpose of point number 2 which I stated in above. The second time it should have served the purpose of testing -- which is what you asked. But again, it may not certain that the test data and the train data do not overlap. Therefore, you have code that does everything, but you may not have made sure that the train/valid/test data are mutually exclusive.
