[site]: crossvalidated
[post_id]: 6599
[parent_id]: 
[tags]: 
Calibrating a multi-class boosted classifier

I have read Alexandru Niculescu-Mizil and Rich Caruana's paper " Obtaining Calibrated Probabilities from Boosting " and the discussion in this thread. However, I am still having trouble understanding and implementing logistic or Platt's scaling to calibrate the output of my multi-class boosting classifier (gentle-boost with decision stumps). I am somewhat familiar with generalized linear models, and I think I understand how the logistic and Platt's calibration methods work in the binary case, but am not sure I know how to extend the method described in the paper to the multi-class case. The classifier I am using outputs the following: $f_{ij}$ = Number of votes that the classifier casts for class $j$ for the sample $i$ that is being classified $y_i$ = Estimated class At this point I have the following questions: Q1: Do I need to use a multinomial logit to estimate probabilities? or can I still do this with logistic regression (e.g. in a 1-vs-all fashion)? Q2: How should I define the intermediate target variables (e.g. as in Platt's scaling) for the multi-class case? Q3: I understand this might be a lot to ask, but would anybody be willing to sketch out the pseudo-code for this problem? (on a more practical level, I am interested in a solution in Matlab).
