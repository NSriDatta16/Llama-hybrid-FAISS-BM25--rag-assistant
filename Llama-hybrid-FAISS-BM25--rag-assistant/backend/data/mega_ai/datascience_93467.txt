[site]: datascience
[post_id]: 93467
[parent_id]: 
[tags]: 
Anomaly detection for high dimensional categorical data

I have a dataset with around 200+ categorical variables $X_i$ and the sizes of their domain $|X_i|$ range from 2 to 8k. So, if I one-hot encode the combination of these variables, the vector space (around 50k) would become extremely sparse. I am wondering if there is an effective way to encode such data. To be specific, the data contain choices users made when requiring SMS service through websites, something like "source_region", "browser_type", and users' own properties like "Is_Vip". I hope these details help. The number of labels is limited, so the model should be trained in an unsupervised fashion. What I have tried I replaced region-related columns with their corresponding GDPs, as well as lamptitudes and attitudes to add geographical and economic information. And I tried a naive auto-encoder to encode the features and chose those samples with high reconstruction loss as anomalies. But the recall on the test set is only 10%. Though I know the anomalies here are based on the context like users, building a model for every user is impractical. And I've thought about embedding these requests and so we can get something like user vector inspired by recommendation system. But due to the limit of unsupervised models,I've lost my way.
