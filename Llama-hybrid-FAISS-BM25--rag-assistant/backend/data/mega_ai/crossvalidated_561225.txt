[site]: crossvalidated
[post_id]: 561225
[parent_id]: 561206
[tags]: 
NO Compared to your linear model, the random forest emphasizes different aspects of how a feature can influence the outcome. For example, a random forest can pick up on an interaction between variables, which you have to specify in a linear model. Consequently, the feature importance from a random forest and the feature importance to a linear model need not coincide. Further, there are a few issues with your approach. Feature importance and feature selection are unstable. Frank Harrell has discussed this multiple times, such as his keynote at "Why R?" and an answer of his on here . Pre-screening with the linear regression influences downstream inferences. For example, if you screen out a variable in the linear model, you never give the random forest a chance to examine that variable, even though the random forest might have found that variable to be important. If you want p-values or confidence intervals, it gets even worse.
