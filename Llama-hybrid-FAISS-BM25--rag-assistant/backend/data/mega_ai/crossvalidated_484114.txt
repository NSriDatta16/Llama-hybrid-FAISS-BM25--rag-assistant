[site]: crossvalidated
[post_id]: 484114
[parent_id]: 
[tags]: 
Estimating positive and negative predictive value without knowing the prevalence

There is a lot of discussion about the positive predictive value of a test currently. I know that if I know specificity, sensitivity of a test and the prevalence $p$ in the sample, then I can easily calculate the positive predictive value (ppv) and negative predictive value (npv): $ppv = \frac{p\cdot Sens}{p\cdot Sens + (1-p)\cdot(1-Spec)}$ and $npv = \frac{(1-p)\cdot Spec}{(1-p)\cdot Spec+p\cdot(1-Sens)}$ . However, this requires that I know the prevalence, and of course the only way knowing this number is from the test, for which I don't know the ppv etc... However, I was wondering if it shouldn't be possible to also use the positive proportion and the number of tests instead to estimate the ppv and the npv in a Bayesian framework. The thinking is like this: Given sufficiently high specificity and sensitivity, if I have 90 positive out of 100 total tests, it is highly unlikely that all these tests are false positives. Even 80 false positive tests seem very unlikely, if I assume for example a specificity of 95%: There are at most 90 negative cases Probability for seeing 80 out of 90 at 5%: 2.833227e-92 So, such a low ppv is just not consistent with the observation. This led me to the following model in JAGS: rm(list = ls()) #### Model Parameters N Is my thinking correct, and could such a model estimate the ppv and npv without any assumptions about the actual prevalence (flat prior at the p.inf variable). Is such an approach used in practice as well, or is the actual ppv estimated differently? I think it should be possible to define a maximum likelihood version of this model as well, but due to the sum of binomials it will probably be very ugly.
