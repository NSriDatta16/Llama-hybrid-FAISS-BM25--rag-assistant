[site]: crossvalidated
[post_id]: 596643
[parent_id]: 596639
[tags]: 
Yours do not seem to agree with the usual definitions. For instance, James et al. (2013) An introduction to statistical learning: with applications in R , Springer (see Chap. 6) defines both the lasso and the ridge without dividing by the sample size. Nevertheless, as you note, such a division is totally irrelevant as far as the estimates are concerned. One reason that I can think of as to why division by $n$ may make sense is when you want to cast your problem as an $M$ -estimation, in which you minimize the average of your loss plus the penalty term with respect to your parameter. In this case, you might be interested in studying theoretical properties (e.g. consistency, limiting distribution, etc.). The average loss plus the penalty is typically an $\mathcal{O}(1)$ , so under appropriate smoothness conditions, it will have a limiting distribution.
