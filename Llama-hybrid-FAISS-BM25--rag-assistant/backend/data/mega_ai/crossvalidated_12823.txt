[site]: crossvalidated
[post_id]: 12823
[parent_id]: 
[tags]: 
Statistically comparing classifiers using only confusion matrix (or average accuracies)

Is it possible to perform a statistical test to determine if one classifier is better than the other using only the confusion matrices of these classifiers? What about the average accuracies from k-fold cross validation? I have a number confusion matrices and average accuracy for classifiers obtained through k-fold cross validation (done using RapidMiner). The data sets for these classifiers are all the same, though the splitting into folds was all done independently. What I'd like to do is, given two of these classifiers, A and B, be able to test if A is statistically better than B using only the confusion matrices and/or the average accuracies for classifiers A and B. All the statistical tests I've found so far require knowing the number of samples that A classified correctly when B did not, and vice-versa. (McNemar's Test for example). I can generate this data if necessary, but I'd like to avoid it if reasonably possible.
