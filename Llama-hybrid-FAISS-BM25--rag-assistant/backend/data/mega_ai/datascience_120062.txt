[site]: datascience
[post_id]: 120062
[parent_id]: 
[tags]: 
Is it valid changing the classification treshold of neural networks for improving the classification performance?

I'm dealing with text classification using BERT pre-trained model with a multiclass imbalanced dataset. When we use a 0.5 default classification threshold we obtain a f1 measure of around 0.7. But we have noticed that when we decrease the classification threshold we obtain a better performance. If we use different binary classifiers, one for each class as positive, we have different imbalance rates. And we have notice that the optimal classification threshold decreases as the imbalance rate increases. Is this an expected behavior? Besides that. Is it valid to change the classification threshold to the optimal value in order to increase the classifier performance? Best regards.
