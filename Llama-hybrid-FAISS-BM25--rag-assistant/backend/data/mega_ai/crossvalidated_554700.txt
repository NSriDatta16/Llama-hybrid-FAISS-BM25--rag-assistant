[site]: crossvalidated
[post_id]: 554700
[parent_id]: 379374
[tags]: 
Remember the definition of $R^2$ as it is defined by that sklearn function. $$ R^2=1-\left(\dfrac{ \overset{N}{\underset{i=1}{\sum}}\left( y_i-\hat y_i \right)^2 }{ \overset{N}{\underset{i=1}{\sum}}\left( y_i-\bar y \right)^2 }\right) $$ The numerator of that fraction is proportional to the $MSE$ , and the denominator is proportional to the variance of the $y$ observations. Your objection is to getting an $R^2$ that looks good yet also getting an $MSE$ that looks big. If that denominator term is extremely gigantic compared to the $MSE$ , then it is entirely possible to have an $R^2$ that looks like a strong letter grade in school yet corresponds to a high $MSE$ . Remember that, under certain conditions , $R^2$ describes the proportion of variance explained. If you start out with a gargantuan variance, then you can leave a huge amount of variance unexplained while explaining a large proportion of the variance. Let's do a simulation. In R : set.seed(2021) N If you plot these (I can't save and post pictures right now), you will see that, as you increase the variance of $y$ , you increase the $R^2$ . However, variance of $y$ is independent of the $MSE$ . (There's this other issue where you seem to be looking at mean absolute error, rather than mean square error. That is more complicated, because there is not a perfect relationship between $MAE$ and $R^2$ . However, if you're noticing that the deviations between truth and prediction are large, in some sense, that corresponds to a large $MSE$ .)
