[site]: crossvalidated
[post_id]: 186451
[parent_id]: 
[tags]: 
Identification of Bayesian models

In frequentist estimation, a large degree of emphasis in SEM/CFA modeling is placed on whether the model is identified , that is, whether each parameter can be uniquely estimated from the data. The typical idea is that, for p parameters, then you need at least p bits of information to identify the model. Now for the Bayesians, the concept of identification is not so black and white. From my read of the literature (Palomo et al., 2011), Bayesian estimation will always produce a posterior distribution for each parameter, no matter their number . Rather, identification is defined as if Bayesian learning occurs: if the posterior is different from the prior (due to the influence of the data). An unidentified Bayesian model is one in which the prior and posterior are exactly the same, and nothing is learned from the data. Adding more parameters, it seems, reduces the ability to estimate each one, and after a certain point the model becomes unidentified. Model identification is straightforward in a frequentist framework, but it is much more vague in the Bayesian paradigm and provokes a lot of questions. How does adding more parameters reduce the overall information available to estimate the others? Does adding information in the priors increase the ability to estimate the other free parameters (i.e., makes them more identifiable/increased Bayesian learning)?
