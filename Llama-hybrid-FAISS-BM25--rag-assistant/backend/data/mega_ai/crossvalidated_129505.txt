[site]: crossvalidated
[post_id]: 129505
[parent_id]: 
[tags]: 
How do I calculate required Sample Size to get useful confidence interval length for Bernoulli random variables?

I am pulling meta data fields from a population of 100M documents. These meta fields include things like Author Name, Author Location, Keywords, etc. Some documents have all of these meta fields, some documents have a few of these fields, and some documents don't have any. We have no idea what the meta field coverage rates are for the population, but would like a best guess from a sample before we process the entire set ( eg. 45% of documents will include an Author Name ... 5% will include an Author Location ). Basic sample size calculation suggest that I need to analyze 166 random documents to be 99% confident that my sample represents the population ( 99% Confidence Level ), while having a Confidence Interval of plus or minus 10% on the coverage rates we discover. Intuitively, however, this does not seem like the correct method to me. If a meta field occurs very rarely in the full population, it seems like I'd have to adjust my population or change the Confidence Interval for that specific Meta Field. How should I design this test to get valid coverage rates for meta fields representative of the complete population of documents?
