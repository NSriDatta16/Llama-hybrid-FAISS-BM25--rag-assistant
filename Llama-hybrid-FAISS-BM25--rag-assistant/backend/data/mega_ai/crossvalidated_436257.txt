[site]: crossvalidated
[post_id]: 436257
[parent_id]: 297947
[tags]: 
I know this is an old post but it still pops up on Google when you search for intuitions so I would like to add the results of my own questioning. The idea that sigmoid was good for binomial distribution was good for intuition when discovering it, but in fact, with deep learning, we combine so many neurons on so many layers that we are not really looking for binomial distribution, not for each separated neuron! That's why we can move on to shimao's idea of function approximator. Then for the ReLu idea: it approaches very closely how our own brain works! Our neurons propagate an action potential (a peak of excitation) only when the excitation (information from the synapse) has reached a threshold level. If the neuron has received some information from the synapse but not enough, it doesn't do anything. That vision helped me accept that ReLu might be a good idea!
