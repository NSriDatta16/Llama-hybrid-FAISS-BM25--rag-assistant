[site]: crossvalidated
[post_id]: 194746
[parent_id]: 
[tags]: 
Categorical with many levels for NN

I have a dataset which lists the amount of seconds a user held a session by browser_type. For example: user 1|iPhone-safari|50 seconds user 1|PC-safari|400 seconds user 2|android-unknown|5 seconds There are around 100 different levels of this (it is detailed) and I wanted to check the optimal way of entering this information into my neural-network. As far as I understand: it is better to give as much information as possible and let the neural-network decide what function form to impose on the data : For example if I pick out the browser that the user spent the most amount of time in (so associate just one value with each other), then I imposing my assumption that that is what matters (perhaps the top 2 matter more, or the 3rd, etc) If collapse the level of aggregation (for example all iPhones get lumped together, all PCs, etc) then again I am the one deciding what information is more important and that is best handled by the neural-network If encode the 'duration in seconds' as a binary (yes/no) then I am limiting the information to the neural-network again and imposing my own assumption which is not based on the training data. This leads me to believe that the best way to put this data into my NN is to create 100 columns which contain the number of seconds the user spent in the browser (others = 0 seconds). user_id|iPhone-safari|iPhone-other|PC-safari|android_unknown|... 1|50|0|400|... 2|0|0|0|5|... Is that correct? Or is that not fair to ask without saying that I am have around 250,000 observations to train on? OR would the proper approach be to use ALL the attributes and then throw in the attributes aggregated at various levels and use PCA to decide which to use?
