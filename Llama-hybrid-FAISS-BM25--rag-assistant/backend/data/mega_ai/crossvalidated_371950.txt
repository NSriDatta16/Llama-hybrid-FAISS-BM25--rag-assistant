[site]: crossvalidated
[post_id]: 371950
[parent_id]: 371917
[tags]: 
There are a number of alternative effects one can derive from the logistic regression model that do not suffer this same problem. One of the easiest is the average marginal effect of the variable. Assume the following logistic regression model: \begin{equation} \ln\Bigg[\frac{p}{1-p}\Bigg]=X\beta + \gamma d \end{equation} where $X$ is an $n$ (cases) by $k$ (covariates) matrix, $\beta$ are the regression weights for the $k$ covariates, $d$ is the treatment variable of interest and $\gamma$ is its effect. The formula for the average marginal effect of $d$ would be: \begin{equation} \frac{1}{n}\sum_{i=1}^n\Bigg[{\Big(1+e^{-(X\beta + \gamma)}\Big)^{-1} - \Big(1+e^{-X\beta}\Big)^{-1}}\Bigg] \end{equation} This effect would be the average probability difference in the outcome between the treatment and control group for those who have the same values on other predictors (see Gelman & Hill, 2007, p. 101). The corresponding R syntax given OP's example would be: dydx_bin I modified OP's syntax to demonstrate that it is not affected by which variables are in the model, as long as the predictor variable of interest is unrelated to other predictors. I modified the results data frame thus: out And within the simulation, I saved the computed average probability difference: out $treat_11[i] treat_21[i] $treat_31[i] treat_41[i] And the new results: colMeans(out)[5:8] treat_11 treat_21 treat_31 treat_41 0.1019574 0.1018248 0.1018544 0.1018642 The estimated effect was consistent regardless of model specification. And adding covariates improved efficiency as with the linear regression model: apply(out[, 5:8], 2, sd) treat_11 treat_21 treat_31 treat_41 0.02896480 0.02722519 0.02492078 0.02493236 There are additional effects that OP can compute like the average probability ratio between the two groups. The average probability difference computed above is available from the margins package in R and margins command in Stata. The average probability ratio is only available in Stata. Onto the other question about trusting meta-analysis results. For one, the direction of the effect should not be useless. The problem with odds ratios does not affect the sign of the coefficients. So if a bulk of studies have an odds ratio above one, there is no reason to doubt this effect because of this particular problem. As for the exact estimate, there is no reason to believe it. The nice thing is that if constituent studies are randomized controlled trials, then the odds ratios are conservative estimates and the actual results are even larger. This is because the effect OP demonstrated shrinks the odds ratios towards one. So if the bulk of studies have an odds ratio above 1 and the meta-analysis is pointing in this direction, then the actual OR once all relevant covariates are adjusted for is even larger. So these meta-analyses are not entirely useless. But I would rather other effect estimates be used in meta-analysis. The average probability difference is one approach, and there are others. Gelman, A., & Hill, J. (2007). Data analysis using regression and multilevel/hierarchical models. Cambridge University Press.
