[site]: crossvalidated
[post_id]: 140016
[parent_id]: 
[tags]: 
Loss function from Posterior log likelihood

I am trying to implement Bayesian Logistic Regression and was looking at this paper http://www.stat.columbia.edu/~madigan/PAPERS/techno.pdf . The authors in this paper had assumed the $\beta$s to have Gaussian prior with zero mean and variance $\tau$ and had implemented the algorithm. I intend to have my implementation with a mean $\mu$ and variance $\tau$. They have derived the log posterior as $$ l(\beta) = (- \sum_{i=1}^n \ln(1+\exp(-\beta^T x_i y_i)) - \sum_{j=1}^d (\ln\sqrt\tau_j+ \frac{ \ln(2 \pi)}{2} + \frac { \beta_j^2}{2 \tau_j}) $$ and to determine the $\beta_j$ values from this equation, they find the value of $z$ that minimizes $$g(z) = ( \sum_{i=1}^n f(r_i +(z- \beta_j) x_{ij} y_i)) + \frac {z^2}{2 \tau_j})$$ where the $ r_i = \beta^T x_i y_i $ are computed using the current value of $ \beta $ and so are treated as constants, $f(r) = \ln(1+\exp(-r))$, and Gaussian penalty terms not involving $z$ are constant and thus omitted. I do not understand how they came up with the above equation for $g(z)$. The above equations are equations 8 and 10 in the paper. Any help is appreciated.
