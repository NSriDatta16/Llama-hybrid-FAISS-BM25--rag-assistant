[site]: crossvalidated
[post_id]: 505844
[parent_id]: 505514
[tags]: 
Interesting enough, I was thinking yesterday on a very similar question after watching a very similar youtube video . While I can not provide a real answer, I would like to share my attempts. Assume there are 100 distinguishable entities that can be in three distinguishable states A, B, and C. It is known that 90 entities are in state A and 5 entities are in state B and 5 in state C. The Shannon entropy is $H=.9*log(.9)+.05*log(.05)+.05*log(.05)=0.5689956$ , where $log$ is base 2. Now both videos seem to claim that the $H$ is the average number of yes-/no-questions necessary to determine the state of a given entity. The value of $H$ is between 0 and 1, which I interpret that there must be cases where no question needs to be asked. I can only think of one probability distribution where no question is necessary, that is when the all the probability mass is on one state. In that case, the entropy is also 0. However, the entropy is continously increasing, while as soon the probability distribution is not "singular", the average number of questions must be 1 or more. Attempt 1 So maybe $H$ is not the average number of yes-/no-questions asked , but the average number of yes-/no-questions answered with no ? If our search algorithm would be is the entity in state A? is the entity in state B? is the entity in state C?, then in 90% of the case no question would have been answered "no". Combined with the probabilities for the other questions, we get $0.1*1 + 0.1*0.5*2=0.2$ , which is much lower than $H$ . Attempt 2 So maybe $H$ is not the average number of yes/no-questions asked for one given entity, but the average number of questions necessary to determine the states of all entities, divided by the number of entities? This would us allow to incorporate the knowledge of previous entities. The search algorithm could then look like this: The distribution $P$ is $(0.9, 0.05, 0.05)$ . iterate over all entities: If $P$ is singular, no question necessary Question: is the entity in the state with the highest proability? If no: ask a second question. recalculate $P$ , taking into account that some entities have been removed. The mean over all entities of the questions asked in this scheme is way above 1. Attempt 3 So maybe the original interpretation of the claim in the videos is correct, but the equivalence is only up to some factor or transformation? Here is a chart comparing the average number of questions (x axis, "nsearches") to $H$ . Each dot is a different probability distribution (so it is not (0.9, .05, .05) anymore, but moves to a more uniform distribution to the right). Both functions seem to capture the same behaviour (increasing entropy), but their relationsship remains unclear to me. So these are my attempts. I am very interested in a verified interpretation / intuition of Shannon entropy in terms of "search costs".
