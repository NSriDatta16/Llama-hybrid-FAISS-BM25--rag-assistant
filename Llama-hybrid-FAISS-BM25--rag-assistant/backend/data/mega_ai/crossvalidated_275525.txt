[site]: crossvalidated
[post_id]: 275525
[parent_id]: 241985
[tags]: 
Although the issue is almost the same as I answered in this answer , I'd like to illustrate this issue, which also confused me a bit today in the seq2seq model (thanks to @Franck Dernoncourt's answer), in the graph. In this simple encoder diagram: Each $h_i$ above is the same cell in different time-step (cell either GRU or LSTM as that in your question) and the weight vectors(not bias) in the cell are of the same size of (num_units/ num_hidden or state_size or output_size ). RNN is a special type of graphical model where nodes form a directed list as explained in section 4 of this paper: Supervised Neural Networks for the Classication of Structures . We can think of num_units as the number of tags in CRF(although CRF is undirected), and the matrices( $W$ 's in graph in the question) are all shared across all time steps like the transition matrix in CRF.
