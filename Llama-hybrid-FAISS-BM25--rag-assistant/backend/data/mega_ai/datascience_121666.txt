[site]: datascience
[post_id]: 121666
[parent_id]: 
[tags]: 
Why do people say ReLU vs MaxPool order does not change the result?

Upon Googling "Maxpool ReLU order" or similar, I've found many people saying this order does not effect the result, i.e.: MaxPool(Relu(x)) = Relu(MaxPool(x)) Here are a small number of examples of people saying this: https://stackoverflow.com/questions/35543428/activation-function-after-pooling-layer-or-convolutional-layer https://github.com/tensorflow/tensorflow/issues/3180 https://www.quora.com/In-most-papers-I-read-the-CNN-order-is-convolution-relu-max-pooling-So-can-I-change-the-order-to-become-convolution-max-pooling-relu https://towardsdatascience.com/convolution-neural-networks-a-beginners-guide-implementing-a-mnist-hand-written-digit-8aa60330d022 To be clear, I'm completely aware that there could be a slight speed difference, but what I'm asking about here is the computation result, not the speed. For example, consider the following: How can the general consensus be that the ReLU/MaxPool order does not effect the computation result when it's easy to come up with a quick example where is does appear to effect the computation result? For what it's worth, ChatGPT seems to go against the general consensus:
