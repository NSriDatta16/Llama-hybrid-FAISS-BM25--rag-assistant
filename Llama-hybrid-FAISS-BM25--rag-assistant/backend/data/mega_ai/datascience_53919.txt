[site]: datascience
[post_id]: 53919
[parent_id]: 
[tags]: 
Understanding Layers in Recurrent Neural Networks for NLP

In convolution neural networks, we have a concept that inner layers learn fine features like lines and edges, while outer layers learn more complex shapes. Do we have any such understanding for layers in RNNs (like LSTMs), something like inner layers understand grammar while outer layers understand more complete meanings of sentences assuming that we are using the LSTM for some natural language task like text summarization?
