[site]: crossvalidated
[post_id]: 628386
[parent_id]: 628315
[tags]: 
I appreciate both comments to the question, but standardization/g-computation is identified here. The issue is that the formula in this causal structure differs from the usual formula. In the paper by Breskin et al. 2018 they consider a DAG similar to the one provided above (except they have an arrow from $A \rightarrow Y$ ). They give the following equality (translated to the variable names given here): $$ E[Y^a] = \sum_l E[Y | A=a,L=l,C=0] \Pr(L=l | A=a)$$ This is distinct from the usual g-computation formula we usually see. See the proof in the paper for the math and ideas that lead to this form. They also demonstrate this with simulations. A similar proof is given in Example 1 of my paper here . It considers a modification of the DAG shown above, but shares some similarities. Update : I had some extra time so I am returning to this question to show how the corresponding g-computation algorithm can be implemented. I am going to do this with Python and my package delicatessen for fitting the models. The data generating mechanism comes from the Breskin et al. paper referenced above. import numpy as np import pandas as pd from delicatessen import MEstimator from delicatessen.estimating_equations import ee_regression, ee_glm from delicatessen.utilities import inverse_logit # Generating the data np.random.seed(20231107) n = 1000000 d = pd.DataFrame() d['A'] = np.random.binomial(n=1, p=0.5, size=n) d['U'] = np.random.binomial(n=1, p=0.3, size=n) d['W'] = np.random.binomial(n=1, p=0.2 + 0.4*d['A'] + 0.3*d['U'], size=n) d['S'] = np.random.binomial(n=1, p=0.1 + 0.8*d['W'], size=n) d['Y'] = np.random.binomial(n=1, p=0.3 + 0.5*d['U'] - 0.2*d['A'], size=n) d['I'] = 1 ds = d.loc[d['S'] == 0].copy() As a comparison, we can use the full data on Y to assess the causal effect # Truth X = np.asarray(d[['I', 'A']]) y = np.asarray(d['Y']) def psi(theta): return ee_glm(theta, X=X, y=y, distribution='binomial', link='identity') estr = MEstimator(psi, init=[0.5, -0.2]) estr.estimate() which gives the estimated causal effect as -0.20066583 . When ignoring the selection bias, we get an answer of -0.23594802 and when adjusting for W via a model we get -0.25898981 . The following code provides an implementation of g-computation for this type of selection bias problem as an estimating equation # G-computation d['Y'] = np.where(d['S'] == 0, d['Y'], -9999) d['AW'] = d['A'] * d['W'] X = np.asarray(d[['I', 'A', 'W', 'AW']]) y = np.asarray(d['Y']) a = np.asarray(d['A']) s = np.asarray(d['S']) da = d.copy() da['A'] = 1 da['AW'] = d['W'] X1 = np.asarray(da[['I', 'A', 'W', 'AW']]) da['A'] = 0 da['AW'] = 0 X0 = np.asarray(da[['I', 'A', 'W', 'AW']]) def psi(theta): # Subsetting parameters out rd, r1, r0 = theta[0:3] beta = theta[3:] # Outcome nuisance model, Pr(Y | A,W,S=0) ee_out = ee_regression(beta, X=X, y=y, model='logistic') ee_out = ee_out * (1-s) # subsetting to S=0 y1hat = inverse_logit(np.dot(X1, beta)) # Predicted Y^{1} y0hat = inverse_logit(np.dot(X0, beta)) # Predicted Y^{0} # Risk functions ee_r1 = a*(y1hat - r1) # Mean Y^{1} among A=1 ee_r0 = (1-a)*(y0hat - r0) # Mean Y^{0} among A=0 ee_rd = np.ones(y.shape) * (r1 - r0 - rd) # ACE return np.vstack([ee_rd, ee_r1, ee_r0, ee_out]) estr = MEstimator(psi, init=[0., 0.5, 0.5, -0.3, -1., 0.6, 0.3]) estr.estimate() (starting values are mildly informative to speed up the fitting process). Here, the procedure estimates the average causal effect as -0.19898929 which is close to the truth.
