[site]: crossvalidated
[post_id]: 384571
[parent_id]: 
[tags]: 
Bayesian consistency in compact uncountable parameter space

Let $p(y_i \mid \theta)$ be the likelihood we are using of a single data point, $p(\theta)$ be the prior, and $f(y_i)$ the true distribution of the data. Also, let $\theta_0$ be the parameter that minimizes the Kullback-Leibler divergence between $p(y_i \mid \theta)$ and $f(y_i)$ , which is $$ KL(\theta) = E_f\left[ \log\left(\frac{f(y)}{p(y \mid \theta)}\right)\right] = \int \log\left(\frac{f(y)}{p(y \mid \theta)}\right) f(y) \text{d}y. $$ Let $A_{\epsilon} = \{ \theta \in \Theta : \rho(\theta, \theta_0 ) be the $\epsilon$ -ball about $\theta_0$ . Let $B$ be another set that does not contain $\theta_0$ , but is not necessarily disjoint from $A_{\epsilon}$ . It is easy to show that $$ KL(\theta_0) - KL(\theta) = E_f\left[ \log\left(\frac{p(y \mid \theta)}{p(y \mid \theta_0)}\right)\right] but is it true that $$ E_f\left[\log \left(\frac{ p(y \mid \theta \in B) }{ p(y \mid \theta' \in A_{\epsilon}) } \right) \right] Hopefully yes, because it would help me prove consistency in a compact parameter space.
