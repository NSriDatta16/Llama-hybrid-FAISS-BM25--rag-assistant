[site]: crossvalidated
[post_id]: 363169
[parent_id]: 363157
[tags]: 
It appears that the $x_i& indicate the occurrence of an event and that they are independent. Thus, you have just simple independent probabilities for each. Once they are e.g. continuous numbers or non-independent (as is the case in any realistic regression setting) this all no longer works in such a simple fashion. In a regression setting you usually cannot write down the joint distribution of covariates and outcomes, but rather tend to condition on the observed values of the covariates (i.e. you assume they are fixed given numbers - there are of course situations where measurement error in the covariates is a concern and with some simplifying assumptions one can deal with that rather easily in Bayesian regression models). Then we assume some kind of regression equation such as $$\text{logit}( P\{Y=1\} ) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 $$ for a binary outcome $Y$ - possibly with an additional index, if there are multiple observations - and do our Bayesian inference as $$p(\beta_0, \beta_1, \beta_2 | y) = \frac{p(\beta_0, \beta_1, \beta_2) p(y | \beta_0, \beta_1, \beta_2)}{p(y)},$$ where I am denoting a pdf/pmf by $p$ and all of this implicitly assumes the given values of the $x_i$.
