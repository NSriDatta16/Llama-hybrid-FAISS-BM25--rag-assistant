[site]: crossvalidated
[post_id]: 237020
[parent_id]: 
[tags]: 
What's the justification for normalizing inputs when solving using closed form?

In Machine Learning I understand that feature scaling or mean-normalizing your input is useful when trying to learn a model using gradient descent. It's useful in the sense that it speeds up training as the "contour lines" are not elliptical, which makes it easier to find an optimal point in the parameter space. Andrew Ng explains the logic here . However, if your data is "simple" enough so that you can solve it in closed-form, what's the point of normalization? Figuring out the weights $w$ that minimize sum-of-squares error is trivial: $$w=(X^T X)^{-1} X^T Y$$ Is it safe to say normalization doesn't change anything? The speed is the same (same number of steps) and the results are the same. What's the justification for normalization? Or am I wrong? I've compared the errors on normalized and non-normalized data in polynomial regression. For the best fit model, the errors are the same. Computation time is the same (actually there is a minuscule increase for the normalization step). Errors for polynomial degrees far from the best fit, however, are vastly different between normalized and non-normalized. For these, the error on the normalized set are actually very high. In this toy example, $x$ only has one feature and there are 100 samples in the data set.
