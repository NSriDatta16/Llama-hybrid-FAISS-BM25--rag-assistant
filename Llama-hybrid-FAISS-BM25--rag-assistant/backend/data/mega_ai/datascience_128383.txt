[site]: datascience
[post_id]: 128383
[parent_id]: 128375
[tags]: 
How do you plan on using these embeddings? You can definitely use concatenated embeddings for similarity/retrieval, but only when comparing concat embeddings to other concat embeddings. Your point about running text queries makes it sound like you want to run text based retrieval of slides. This wouldn't work with concat embeddings, because your query text vector won't be the same size as your concat embeddings. Concat embeddings only work if you plan on doing retrieval queries with other concat embeddings. If you want to be able to retrieve slides with text queries (or slide text with image queries), you need to have text and image embeddings projected into the same space. In theory OpenCLIP models should do this out of the box, but I have a hunch that powerpoint images are different enough from the standard CLIP model training data that you would get bad performance. To get around this, you want to train a small model (MLP or even linear layer) to map OpenCLIP embeddings to new embeddings. Train this with the standard CLIP loss, just on your own specific dataset. This will give you embeddings that work much better for your data (powerpoints) compared to out of the box CLIP models.
