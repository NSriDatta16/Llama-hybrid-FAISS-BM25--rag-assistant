[site]: datascience
[post_id]: 121659
[parent_id]: 121654
[tags]: 
In the attention blocks, Keys and Values are of the same length, but Queries are not necessarily of the same length as the former. For instance, in the encoder-decoder attention blocks, the Keys and Values come from the encoder representations while the Queries come from the decoder representations (which are computed from a completely different sequence than the encoder ones). In the Transformer model, the only factor limiting the length of the inputs is the positional embeddings (see this answer for details. In the ALiBi paper, they replace them with a non-trainable approach and, experimentally, the approach is shown to extrapolate to longer sequences than those seen during training.
