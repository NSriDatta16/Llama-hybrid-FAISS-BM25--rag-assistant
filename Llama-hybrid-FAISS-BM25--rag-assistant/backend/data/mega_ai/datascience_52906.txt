[site]: datascience
[post_id]: 52906
[parent_id]: 52900
[tags]: 
I think I found the answer. The embedding layer in keras is nothing more than a set of vectors for distinct words. The keras embedding layer initializes the word embedding with some random values (the default values from a uniform distribution)and then updates the values when train the whole network. Therefore, there is no need to compose the model in details, because the backpropagation do it perfectly :)
