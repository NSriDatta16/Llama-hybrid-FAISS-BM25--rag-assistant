[site]: datascience
[post_id]: 107940
[parent_id]: 57769
[tags]: 
CBOW while training tries to predict the main word from the context words. Once the training is done we use the weight matrix of first hidden layer to get Word2vec embedding. The weight matrix is multiplies by the one hot encoding to get the word2vec emebdding. If you multiply a 1 x 10,000 one-hot vector by a 10,000 x 300 matrix, it will effectively just select the matrix row corresponding to the “1”. Here’s a small example to give you a visual. : This means that the hidden layer of this model is really just operating as a lookup table. The output of the hidden layer is just the “word vector” for the input word. References : http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/
