[site]: stackoverflow
[post_id]: 469691
[parent_id]: 468117
[tags]: 
Thanks for the detailed update. I'm satisfied that your current bin-packing strategy is pretty efficient. As to the question, " Exactly how much overhead does an ISO 9660 filesystem pack on for n files totalling b bytes?" there are only 2 possible answers: Someone has already written an efficient tool for measuring exactly this. A quick Google search turned up nothing however which is discouraging. It's possible someone on SO will respond with a link to their homebuilt tool, but if you get no more responses for a few days then that's probably out too. You need to read the readily available ISO 9660 specs and build such a tool yourself. Actually, there is a third answer: (3) You don't really care about using every last byte on each DVD. In that case, grab a small representative handful of files of different sizes (say 5), pad them till they are multiples of 2048 bytes, and put all 2^5 possible subsets through genisoimage -print-size . Then fit the equation nx + y = iso_size - total_input_size on that dataset, where n = number of files in a given run, to find x , which is the number of bytes of overhead per file, and y , which is the constant amount of overhead (the size of an ISO 9660 filesystem containing no files). Round x and y up and use that formula to estimate your ISO filesystem sizes for a given set of files. For safety, make sure you use the longest filenames that appear anywhere in your collection for the test filenames, and put each one under a separate directory hierarchy that is as deep as the deepest hierarchy in your collection.
