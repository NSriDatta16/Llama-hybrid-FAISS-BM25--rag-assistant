[site]: crossvalidated
[post_id]: 565248
[parent_id]: 565244
[tags]: 
The calibration using isotonic regression will do two things for you, firstly re-adjusting for the oversampling, but secondly also helping with mis-calibration. The second point is rather helpful, because it is reasonably well-known that even if you had not oversampled, the calibration of XGBoost is often not right in the sense that on average cases predicted to be a 1 with probability X% do not end up being cases about X% of the time. If it were not for the second issue, you could simply re-scale the log-odds by taking the inverse of he sigmoid function (i.e. log(predicted probability)- log(1 - predicted probability)) and adding a constant. I believe the constant would simply be the difference between 0.15 and 0.45 when transformed to the logit scale, i.e. -0.2006707 and -1.734601. I.e. you would get $$\text{adjusted prediction} = \sigma(\sigma^{-1}(\text{predicted probability})-1.53393).$$ In Python you'd probably use the logit and expit functions from scipy.special . However, that approach is only the way to do it, if your model is already well-calibrated, so what you propose seems better to me.
