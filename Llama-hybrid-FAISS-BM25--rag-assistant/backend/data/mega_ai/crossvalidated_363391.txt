[site]: crossvalidated
[post_id]: 363391
[parent_id]: 315369
[tags]: 
Since you've already explored changing nodesize , ntree and mtry , you're left with two possible explanations for the low R 2 : Most of the variation in your data is random i.e. not explainable by your predictors. You have insufficient data . Unfortunately, there's not a lot we can recommend based on the information you've presented. Random forests are structurally reasonably robust to overfitting because of bagging (but see the side note below), so I wouldn't be surprised if you can't push that R 2 higher. Side note: At mtry = 3 , you're using all your predictors at every split. Since you have only 3 predictors, that negates one of the ways that random forests work: 'feature bagging', or the 'random subspace method'. This is explained well in this answer .
