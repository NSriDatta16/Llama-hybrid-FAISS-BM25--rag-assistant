[site]: crossvalidated
[post_id]: 549384
[parent_id]: 549382
[tags]: 
The main distinction between "individual" and "cohort" predictions has more to do with what you do after you build the model. Survival models are necessarily built on data from cohorts. They can nevertheless make predictions for an individual's survival function based on that individual's set of covariate values. See any survival software's predict() or similar function. A major goal of clinical survival modeling is to try to estimate survival for individual patient counseling or to pick the therapy that has the best chance of extending survival for that particular patient. Consider TNM (tumor-node-metastasis) staging, which might distinguish 4 groups with progressively shorter overall survival after diagnosis of a particular type of cancer. That's based on very large cohorts, and regularly re-evaluated by clinical experts. It's not very precise for any individual patient, but at least it provides a rough answer to the patient's question: "How long do I have to live?" From that perspective, survival modeling has always been about individual survival--at least in terms of trying to figure out which cohort best describes an individual's survival probability. Chapter 10 of Therneau and Grambsch , now 21 years old, discusses "Expected Survival" estimates at both the individual and the cohort level, with individual survival estimates discussed first. For example, although you will sometimes hear it claimed that Cox models aren't intended for individual predictions, the combination of a model-estimated baseline hazard with regression coefficients does provide predictions of survival curves based on individual sets of covariates, as discussed in that chapter. For "cohort" analysis, as explained there, the problem is perhaps more difficult, as you need to make a decision about what type of "average" to make among members of the cohort. Larger data sets might provide more refined estimates of regression coefficients or allow models to incorporate larger numbers of predictors. That might in principle make estimates of things like median survival more precise than previously. Larger data sets, however, do not overcome the fundamental problem with making individual predictions in survival analysis: the typically wide distribution of survival times around the median or other central measure. Survival models effectively start with some baseline form of a survival function and then examine the associations of covariates with properties of that function, for example its position in time, its steepness or shape. You model the entire survival function. A highly precise estimate of an individual's survival would require something close to a step function in the predicted survival function for that individual. In clinical survival functions, at least, you don't typically find step functions, except perhaps for those with very poor prognosis. The variability in clinical survival outcomes at any given set of covariate values is inherent in proportional-hazards (PH) models, like Cox regressions. The baseline survival function for a PH model runs over the entire period from time = 0 to the last event time in the data set. In clinical survival studies that period can be years or decades. An individual with a "favorable" set of covariate values could in principle, under the PH assumption, experience the event at any time during that long period, just with a lower risk of event at any time than an individual with "less-favorable" covariate values. You thus don't get a precise individual survival prediction from a PH model for an individual with "favorable" covariate values, even though you might have a model that describes the survival characteristics of a cohort of such individuals quite well. You can see the issues also in the formula that describes parametric accelerated failure time (AFT) models in survival analysis, $$\log T = -x'\beta + \sigma W, $$ where $T$ is survival time, $x$ is the vector of covariates with corresponding coefficients $\beta$ , $\sigma$ is a scale factor and $W$ is an error term with a specified distribution (e.g., extreme-value, normal, logistic). The distribution of errors $\sigma W$ limits the precision of the estimated survival time $T$ based on $x' \beta$ for an individual. In practice, that distribution is generally quite wide. The term $\sigma W$ captures the un-modeled variability in outcomes, similar to the error term in a linear regression. I suppose that if one could incorporate information about all the interactions among 20,000 genes and environmental, social and economic conditions one might be able to model more of the variability. Yes, that would require very large data sets and would probably benefit from "machine learning" approaches. That still would not, however, capture variability arising from unpredictable accidents or infections--as the last 2 years should make very clear.
