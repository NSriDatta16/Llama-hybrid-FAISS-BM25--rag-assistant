[site]: crossvalidated
[post_id]: 514172
[parent_id]: 514157
[tags]: 
$\newcommand{\E}{\operatorname{E}}$ In your application 1, adding more details gives $$ \E[X_1\mid S(X)] = \frac 1n \cdot n \cdot \E[X_1\mid S(X)] = \frac 1n \sum_{i=1}^n \E[X_1\mid S(X)]. $$ Now since the $X_i$ are an iid sample we have $\E[X_1\mid S(X)] = \E[X_2\mid S(X)] = \dots = \E[X_n\mid S(X)]$ (i.e. it didn't matter that we started with $X_1$ . We could have started with any of the $X_i$ so effectively this is a relabeling and iid samples aren't affected by relabeling) so we replace the $i$ th $\E[X_1\mid S(X)]$ with $\E[X_i\mid S(X)]$ to get $$ \E[X_1\mid S(X)] = \frac 1n \sum_{i=1}^n \E[X_i\mid S(X)]. $$ Then the linearity of expectation gives us $$ \frac 1n \sum_{i=1}^n \E[X_i\mid S(X)] = \E[\bar X_n \mid S(X)] = \bar X_n $$ since $S(X) = \bar X_n$ (more formally, $\bar X_n$ is measurable with respect to the $\sigma$ -algebra generated by $S(X)$ so no "averaging" happens). This shows that the function $\E[X_1\mid S(X)]$ was actually $\bar X_n$ all along. For application 2 it's the same idea in that there's no "averaging" in $\E[X_1\mid X_1]$ , meaning that $\E[X_1\mid X_1]$ is just the function $X_1$ . For application 3, $$ \sum_{j=1}^n \E\left[X_j \mid \sum_{i=1}^n X_i\right] = \E\left[\sum_{j=1}^n X_j \mid \sum_{i=1}^n X_i\right] $$ by linearity. But $\sum_j X_j =\sum_i X_i$ so these are the same function and again there's no "averaging" so $$ \sum_{j=1}^n \E\left[X_j \mid \sum_{i=1}^n X_i\right] = n \bar X_n. $$ Overall it looks like the common thread here is understanding when $\E[X\mid Y] = X$ so I'd recommend reviewing the relevant properties of conditional expectation. This property is sometimes called pulling out known factors . Here's one example of a proof of it with the discrete case. It's also important to remember that $\E[X\mid Y]$ is a function, not a number, so e.g. $\E[X_1\mid X_1]$ is still a random variable. For a bit of a sense of why $\E[X\mid X]=X$ , i.e. no reduction happens, conditional expectation takes a random variable (or more generally a $\sigma$ -algebra) and produces a new random variable (that is unique with probability one) that is simpler in that it has the same "local averages" as the original variable but also doesn't change too rapidly (more technically it has sufficiently simple preimages to be measurable). In $\E[X\mid X]$ we have that $X$ has the same local averages as $X$ and also has the same preimages so $\E[X\mid X]=X$ . Here's a resource for a more rigorous treatment and the wikipedia article on conditional expectation has a lot of the terms and definitions.
