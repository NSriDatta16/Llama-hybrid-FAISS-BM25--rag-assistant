[site]: crossvalidated
[post_id]: 337434
[parent_id]: 
[tags]: 
Why normalizing the data gives worst cross validation error but better testing error?

I have used cross validation on my training set to choose the parameters that gives the lowest error for different machine learning models. I then used these models to predict the target values of my testing set and then calculated the error. I tried normalizing the data and went through the same process as above, but I was surprised to see that my normalized cross validation error was worst than my not normalized cross validation error. However, my normalized testing error was better than my not normalized testing error. Should I go with the normalized or not-normalized data?
