[site]: datascience
[post_id]: 56890
[parent_id]: 56885
[tags]: 
A Batch Normalization layer essentially is normalizing outputs of your hidden units, so their outputs would always have a similar scale. By eliminating such internal covariate shift in outputs of hidden layers, your deeper layers become more independent from their previous layers. Nothing here is CNN specific - Batch Normalization may be applied to Fully Connected and Recurrent neural networks as well, but they are more useful with deep neural networks, which tend to accumulate this shift with each layer activated during Forward Propagation.
