[site]: crossvalidated
[post_id]: 222306
[parent_id]: 222198
[tags]: 
It's necessary to provide links because observed links are part of the assumed model. Note that the link indicator $y$ is shown as shaded (conventional means of denoting observed variable) in the graphical depiction, Fig. 2 of the paper , but as explained on the same page, it's otherwise treated as unobserved: First, while one can fix $y_{d_1,d_2} = 1$ whenever a link is present between $d_1$ and $d_2$ and set $y_{d_1,d_2} = 0$ otherwise, this approach is inappropriate in corpora where the absence of a link cannot be construed as evidence for $y_{d_1,d_2} = 0$. In these cases, treating these links as unobserved is more faithful to the semantics of the data. Also see the second part of the assumed generative process, found on the second page of the paper: For each pair of documents $d, d'$: (a). Draw binary link indicator $$y | \textbf{z}_d, \textbf{z}_{d'} \sim \psi(\cdot|\textbf{z}_d, \textbf{z}_{d'}).$$ The authors consider sigmoid and exponential as link functions, both applied to a linear combination of the element-wise product of (normalized) word counts for the two documents (and an intercept). This means, importantly, that documents that share no common topic assignments will have the smallest probability of observing a link. (The element-wise product will be $0$.) This formalizes our intuition that a link between documents should tell us something about those documents' latent topics. Meaning, if an academic paper cites another paper, we have a certain expectation that they share some subject, be it subject matter, methods, etc. By adding a dependence between the topic vectors when a link is observed, we bias the model toward document pairs with similar topic assignments. It's worth noting here that this is different from training LDA and then predicting links via logistic regression. That would induce no bias between the topics of linked documents. Also, the the model experimentally outperformed LDA followed by regression in predicting both links from words and words from links. (See Fig. 3.) To the question of what happens when no links are given, since the model treats $y$ as unobserved when there is no link observed in the training data, the model should collapse into smoothed LDA. (Whether any implementation accounts for this, I cannot say.)
