[site]: crossvalidated
[post_id]: 179966
[parent_id]: 
[tags]: 
Is Weight Matrix Wij from neuron j to i or from i to j?

I have a question about the Weight matrix from a Neural network. I see different notations for this. I see some publications (e.g. This stanford page ) with: Notation 1: $W^{(l)}_{ij}$ denotes the parameter (or weight) associated with the connection between unit j in layer l, and unit i in layer l + 1. However in others I read (e.g. This one ): Notation 2: We denote by $W_{ij}$ the weight of the connection from unit $u_i$ to unit $u_j$ So this the other way around. My questions: What is the most common notation? For denoting node i to j does $W_{ij}$ has some advantages/disadvantages over $W_{ji}$ ? If I want to vectorize the input vector x multiplied by W (to calculate the input values for the activation function) do I save a transpose so $\textbf{Wx}$ instead of $\textbf{Wx}^{t}$ if I use one or another Weight notation? I am pretty sure the motivation to use $W_{ji}$ is saves 1 transpose on x ? Is this correct? Thanks
