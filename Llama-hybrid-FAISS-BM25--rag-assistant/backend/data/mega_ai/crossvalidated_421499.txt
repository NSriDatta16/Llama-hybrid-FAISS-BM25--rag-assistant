[site]: crossvalidated
[post_id]: 421499
[parent_id]: 421488
[tags]: 
Okay I’ll try and answer your questions one at a time: Depending on your model different aspects of the latent variables will be of interest, but in the general context our aim is to just evaluate the posterior distribution of the latent variables given our data. Depending on exactly what your latent variables represent yes you might then be interested in which ones are most probable. Yes, you’re right here, equation 7 is the numerator in equation 2 and equation 8 is the denominator in equation 2. Equation 7 is just a standard use of conditional probability, namely that $P(A,B) = P(A|B)P(B)$ but fully factorised for all the latent variables. Equation 8 is just the law of total probability, integrating out all the latent variables. Basically yes, I’m not sure how intractable the integral is in this case, but generally you’ll find that marginalising out the latent variables quickly gets computationally intractable especially as the dimension increases. Here the means are latent variables and unknown. In general with Bayesian methods you can always keep going deeper placing priors on any of the parameters, in the end you have to decide on something but you’ll just have to try a few different things
