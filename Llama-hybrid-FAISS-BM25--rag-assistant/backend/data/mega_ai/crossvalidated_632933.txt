[site]: crossvalidated
[post_id]: 632933
[parent_id]: 
[tags]: 
Proving Monotonic Decrease of Kullback-Leibler Divergence in Iterative Method for Stationary Distribution Estimation

Introduction Consider a well-behaved Markov chain with desirable properties (irreducible, aperiodic, positive recurrent), characterized by a transition matrix $P$ and a stationary distribution $\pi$ . The stationary distribution satisfies the equation: $$ \pi = \pi P \tag 1 $$ Here, $\pi$ is a row vector whose summation equals 1. In practice, solving $(1)$ directly for $\pi$ is feasible for small-dimensional matrices, but for larger matrices, an iterative approach is employed. terative Method for Stationary Distribution Estimation: Input: Transition matrix $P$ , iteration error $\epsilon$ Initialize $\pi^{(0)}$ such that the elements sum to 1. Compute $\pi^{(k)} = \pi^{(k-1)} P$ . If the error between $\pi^{(k)}$ and $\pi^{(k-1)}$ is less than $\epsilon$ , stop iteration and output $\pi^{(k)}$ . Otherwise, continue iterating. I hope to show Kullback-Leibler divergence $KL(\pi^{(k)} \mid \pi)$ is monotonically decreasing throughout the iteration process. Numerical Simulation To illustrate this monotonic decrease, a fifth-order transition matrix $P$ is considered. $$ P = \begin{pmatrix} 0 & 0.25 & 0.25 & 0.25 & 0.25\\ 0.2 & 0.2 & 0.2 & 0.2 & 0.2\\ 0 & 0.5 & 0 & 0 & 0.5\\ 0 & 1/3 & 1/3 & 0 & 1/3\\ 0 & 1 & 0 & 0 & 0 \end{pmatrix} $$ The corresponding stationary distribution is $\pi = [0.08759124, 0.4379562, 0.1459854, 0.10948905, 0.2189781]$ Using a random initial distribution $\pi^{(0)} = [0.2, 0.2, 0.2, 0.2, 0.2]$ , the calculated KL divergences are as follows: $$ \begin{align} KL(\pi^{(0)} \mid \pi) = 0.1736967812\\ KL(\pi^{(1)} \mid \pi) = 0.0219343265\\ KL(\pi^{(2)} \mid \pi) = 0.0022525586\\ KL(\pi^{(3)} \mid \pi) = 0.0006414772\\ KL(\pi^{(4)} \mid \pi) = 0.0000681666\\ \end{align} $$ This numerical simulation supports the observed monotonic decrease. The next step is to provide a rigorous proof of this phenomenon. Note that the KL divergence is defined as: $$ KL(\pi^{(k)} \mid \pi) = \sum_{i=1}^n \pi^{(k)}_i \log \frac{\pi^{(k)}_i}{\pi_i} $$ import numpy as np # transition matrix P = np.array([[0, 0.25, 0.25, 0.25, 0.25], [0.2, 0.2, 0.2, 0.2, 0.2], [0, 0.5, 0., 0., 0.5], [0, 1./3, 1./3, 0., 1./3], [0, 1., 0., 0., 0.]]) I = np.eye(P.shape[0]) A = I - P.T b = np.ones_like(P[:, 0]) pi = np.linalg.solve(A, b) pi = pi / np.sum(pi) print("True stationary distribution pi: ", pi) # initial distribution pi0 = np.array([0.2, 0.2, 0.2, 0.2, 0.2]) # compute KL divergence eps = 1e-8 print(f"KL(pi{0: Can anyone help to prove the statement or give a counterexample? Actually K-L divergence is not necessary we can only use L2 norm i.e. show that $|| \pi^{(k)} - \pi ||_2^2$ decreases to 0. Any hint is appreciated.
