[site]: datascience
[post_id]: 102110
[parent_id]: 102107
[tags]: 
BERT does not provide word-level representations, but subword representations. This implies that when an unseen word is presented to BERT, it will slice it into multiple subwords, even reaching character subwords if needed. That is how it deals with unseen words. Therefore, BERT can handle out-of-vocabulary words. Some other questions and answers in this site can help you with the implementation details of BERT's subword tokenization, e.g. this , this or this . On the other hand, word2vec is a static table of words and vectors, so it is just meant to represent words that are already in its vocabulary.
