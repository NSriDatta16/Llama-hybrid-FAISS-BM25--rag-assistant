[site]: datascience
[post_id]: 5349
[parent_id]: 
[tags]: 
Terminology: SOMs, batch learning, online learning, and stochastic gradient descent

I'm not sure which word to use to differentiate a self-organizing map (SOM) training procedure in which updates for the entire data set are aggregated before they are applied to the network from a training procedure in which the network is updated with each data point individually. In the case of other algorithms I'd say stochastic gradient descent , but I'm not sure gradient descent is correct for SOM learning. To my knowledge, SOM learning does not follow any energy function exactly, so that should mean it doesn't do gradient descent exactly, right? Another term would be online learning, but `online' sort of implies I train my SOM in the real world with data points streaming in, eg. from a set of sensors. It may also imply that I use every data point only once, which is not what I want to say.
