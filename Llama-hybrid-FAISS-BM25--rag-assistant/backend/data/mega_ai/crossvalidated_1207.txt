[site]: crossvalidated
[post_id]: 1207
[parent_id]: 
[tags]: 
Period detection of a generic time series

This post is the continuation of another post related to a generic method for outlier detection in time series . Basically, at this point I'm interested in a robust way to discover the periodicity/seasonality of a generic time series affected by a lot of noise. From a developer point of view, I would like a simple interface such as: unsigned int discover_period(vector v); Where v is the array containing the samples, and the return value is the period of the signal. The main point is that, again, I can't make any assumption regarding the analyzed signal. I already tried an approach based on the signal autocorrelation (detecting the peaks of a correlogram), but it's not robust as I would like.
