[site]: datascience
[post_id]: 63896
[parent_id]: 63623
[tags]: 
What you highlight in this question is actually the boundary between regression (predicting continuous variables) and classification (predicting outcomes). Indeed, predicting discrete numerical variables or ordered categorical outcomes can fall in either regression or classification, depending on how you interpret the problem. Some algortihms are natively made for classification, and therefore, when used for regression on a continuous target variable, actually produce a prediction on a discrete approximation of the target variable. As highlighted by Ben, decision trees belong to this category. Based on the approach you suggest, almost any multi-class classification algorithm can be used for regression. Oppositely, algorithms designed for regression can be used for classification purposes by doing the inverse operation. For example, logistic regression, which is a regression algorithm because it predicts a continuous variable, is often used as a classifier when it actually predicts the probability for a given class.
