[site]: datascience
[post_id]: 70061
[parent_id]: 
[tags]: 
tf.keras: val accuracy on mnist dataset drops to random guess

For transfer learning purposes I changed mnist into 224x224 px dataset and applied skeletonizing algorithms. I want to train MNIST dataset on "squeezenet" like CNN, which can be found on github. The issue is that when training from scratch (random weights) after some time accuracy drops to random guess, see: Minimal code for reproducibility: train_datagen = ImageDataGenerator( rotation_range=60, rescale=1.0/255 ) test_datagen = ImageDataGenerator(rescale=1.0/255) train_generator = train_datagen.flow_from_directory( os.getcwd() + str(config.NEW_DATA_MNIST) + str(Path("/train")), target_size=(224, 224), class_mode="categorical", batch_size=32 ) validation_generator = test_datagen.flow_from_directory(os.getcwd() + str(config.NEW_DATA_MNIST) + str(Path("/test")), target_size=(224, 224, class_mode="categorical", batch_size=32 ) base_model = SqueezeNet( input_shape=(224, 224, 3), include_top=False,weights=None) x = base_model.output x = GlobalAveragePooling2D()(x) predictions = Dense(10, activation='softmax', name='predictions')(x) model = Model(inputs=base_model.input, outputs=predictions) model.compile( optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'] ) history = model.fit(self.train_generator, epochs=self.simulation_config.epochs, validation_data=self.test_generator) This is a minimal example, so hopefully it is understandable. To further clarify the problem, I tried multiple optimizers ('sgd', 'adam', 'adadelta'). I use tf.keras and tensorflow-gpu==1.14.0. Anybody has some ideas how to fix this issue?
