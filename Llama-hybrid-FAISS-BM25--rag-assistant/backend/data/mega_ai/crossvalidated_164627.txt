[site]: crossvalidated
[post_id]: 164627
[parent_id]: 164623
[tags]: 
The MLE is known to be consistent under specific conditions , that means that the estimate converges (either in probability or almost surely) to the true value of the parameter $\theta_0$. Bayesian parameter estimation updates the posterior of $f(\theta)$ and makes it narrower and narrower around $\theta_0$. In the end, you obtain a Dirac on $\theta_0$. The only assumption is that prior $f(\theta)$ was not zero for $\theta_0$. Thus, you can see that both methods converge under specific conditions to the real value $\theta_0$. Therefore, they converge to the same results.
