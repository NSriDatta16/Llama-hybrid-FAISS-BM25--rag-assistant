[site]: crossvalidated
[post_id]: 264655
[parent_id]: 191467
[tags]: 
According to the paper One-class document classification via Neural Networks of Manevitz and Yousef it seems to be possible to construct a one-class Naive Bayes classifier, even without a standard deviation. I cite the relevant passage where the authors mention how to implement the core of the classifier: We calculate $p(d|E)$ as the product of $p(w|E)$ for all words in the dictionary that appear in the document $d$ . Each of the $p(w|E)$ is estimated independently using the formula: $p(w|E) = \dfrac{n_w + 1}{n + |dictionary|}$ , where $n_w$ is the number of times word $w$ occurs in $E$ , and $n$ is the total number of words in $E$ . We calculate a threshold $\delta$ by the minimum over all examples in $E$ , of the value $p(d|E)$ for each document in the set of examples. Then we experiment with values $\lambda\cdot\delta$ for $0 as in the previous algorithms using $F_1$ to find the optimal threshold for acceptance. That is, given a new document $d$ , we accept it if the calculated value $p(d|E)$ is larger than the determined $\lambda\cdot\delta$ . For this classifier algorithm we store $\delta$ and $\lambda$ . A more detailed picture of the algorithm is explained in the doctoral dissertation Characteristic Concept Representations of Piew Datta.
