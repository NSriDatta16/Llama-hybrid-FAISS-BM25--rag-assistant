[site]: crossvalidated
[post_id]: 53045
[parent_id]: 
[tags]: 
Monette & Fox's conditional hypotheses

G. Monette and J. Fox provide in these slides a framework for the Type II Analysis of Variance/Deviance tests in terms of conditional hypothesis . My questions are: In this frequentist approach, the "conditional hypothesis" $L_1\beta=0 \mid L_2\beta=0$ is only symbolic (isn't it?). Is there a Bayesian analogue to this approach such as inference on $L_1 \beta$ under the posterior distribution of $\beta$ taken conditionally to $L_2 \beta =0$ ? Monette & Fox rigorously define the conditional hypothesis as a classical hypothesis $L_{1\mid 2}\beta =0$ for a certain matrix $L_{1\mid 2}$ but this matrix depends on the estimated asymptotic covariance matrix of the parameters $\beta$. That sounds strange. Does it actually depends on the true asymptotic covariance matrix and then $L_{1\mid 2}$ is only an estimate of a theoretical matrix ? Even for the true asymptotic covariance that sounds strange because it still depends on the choice of the estimating method. In fact I have never seen the notion of conditional hypothesis before, is it presented in some textbooks ? Update1 Still sick today but here are some thoughts. Consider a classical linear model $y = X\beta+\sigma\epsilon$ and the Jeffreys prior. Then the posterior distribution of $(\beta \mid \sigma)$ is ${\cal N}(\hat\beta, V)$ where $V$ is the asymptotic covariance matrix of the least-squares estimator $\hat\beta$, or (I do not remember), $V$ is this matrix up to a factor close to $1$. Then it is easy to see that $(L_1 \beta \mid L_2\beta=0)$ has the distribution of $L_{1 \mid 2} \beta$ under the conditional posterior distribution $(\beta \mid \sigma)$, where $L_{1 \mid 2}$ is a $V$-orthogonal complement as defined in Monette & Fox's slides. And the Wald statistic $Z_{1|2}$ should be related to the norm of $L_{1 \mid 2} \beta$. For more general models the approach should asymptotically coincide with the Bayesian approach when $\hat\beta$ is taken to be the maximum-likelihood estimate. Too sick to continue... Update2 I really wonder about whether this an old or a recent approach. As shown in my answer to myself here , this is not the way used by SAS. But the "old" anova() R function uses this approach. Indeed, for a generalized least-squares model such as glsfit the type II hypothesis Wald F-test statistic of the variable factor is provided by: > anova(glsfit) Denom. DF: 45 numDF F-value p-value (Intercept) 1 1401.9971 (and for the group factor one has to exchange the order of the factors: glsfit.reverse ) Or is it a new theoretical justification of an old approach ?..
