[site]: crossvalidated
[post_id]: 572043
[parent_id]: 
[tags]: 
Deep learning : Can I make an all positive network (features and weights)?

I am trying to train a deep neural network that will always use positive weights and the user will be obliged to enter a feature vector as input that will always be positive (normalized to 0-1). In other words, I am trying to build a neural network that will always use relu, tanh, and not sigmoid (to avoid negative values) and will apply an activation function to the weights before multiplying to the input. e.g. Y = X * sigmoid(W) instead of Y = X * W I have written a collab notebook in PyTorch: link The idea is that by keeping the network positive and forcing it to apply projections only on the [0,1] space, then at the end of the day, the trained network will somehow be explainable. After training, I keep all weights frozen, zero-out all features except one, and feed it to the network. Since it will only multiply with positive values the output will also show the importance of the feature for the task. Has anyone tackled this issue? In my code, where I train a toy model on the MNIST dataset, the trained model has identified only specific bits of an image as important and the bits seem to be random. Could I somehow force the network to not select only specific bits?
