[site]: crossvalidated
[post_id]: 162855
[parent_id]: 161959
[tags]: 
I believe the term of art for you are seeking is "automatic relevance determination." The conventional linear SVM has a kernel function that is the dot product of the two feature vectors: $$K_1(x,x^\prime)=x^Tx^\prime.$$ One way to differentiate among features would be to estimate coefficients for each feature (i.e. element of $x$) separately, and add the results. The simplest case of this will use a diagonal, positive definite matrix $D$: $$ K_2(x,x^\prime)=x^TDx. $$ So now the result of $K_2$ is a weighted sum of the element-wise product of $x$ and $x^\prime$ various features. Note that this isn't restricted just to the linear kernel. For example, the common radial basis function with Euclidean norm can be modified like so. For $$ K_3(x,x^\prime)=\exp\left( -(x-x^\prime)^TD(x-x^\prime)\right) $$ since the product $(x-x^\prime)^TD(x-x^\prime)$ is a squared distance, with each axis weighted according to $D$. Further generalization of this idea can be had by using non-diagonal (but still positive definite) $D$. Enforcing orthogonality of the data via some method such as PCA will obviate the need for non-diagonal $D$, but perhaps reduce interpret-ability of the outcome.
