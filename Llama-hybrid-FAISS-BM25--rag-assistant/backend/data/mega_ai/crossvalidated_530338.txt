[site]: crossvalidated
[post_id]: 530338
[parent_id]: 
[tags]: 
Basic questions about updating weights in neural networks

I have two basic questions about updating weights in neural networks. In the backpropagation process, is the activation function used in the calculation of updating weights or is it used only in the forward-pass process? In the following simple example: x0, x1, w0, w1 and w2 are 1, 1, 0.5, 0.5 and -2, respectively. If the loss function is $L(y, y') = \frac{1}{2} (y âˆ’ y')^2$ , then to update the weights, I need to calculate: $w(updated) = w(old) - \beta \frac{\partial L}{\partial w_i}$ where $\beta$ the learning rate here is $\frac{1}{4}$ . In this example, I don't know how to calculate the partial derivative of L with respect to the weight w ?
