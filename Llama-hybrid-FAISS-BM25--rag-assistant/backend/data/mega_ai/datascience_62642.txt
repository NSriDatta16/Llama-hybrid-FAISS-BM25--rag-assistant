[site]: datascience
[post_id]: 62642
[parent_id]: 62640
[tags]: 
1) Those extracted features are "combination" of original features. Yes, it's additional information. And yes, it could be redundant, but I'd check that after creating those features by doing standard feature selection operations. Here's a great guide from Kaggle . At the same time, it might be a very good feature engineering decision - completely depends on a dataset. 2) Yes, it matters - redundant information can negatively influence model performance. But you find out to understand whether it's redundant or not. The minimum bad thing you can face - is how much time does training takes (more features - more computation resources). The more serious thing is overfitting. 3) Yes, and it depends on how your model is able to process categorical features. If it cannot do anything specific with them, you should remove the original categorical features. XGboost or CatBoost (those are tree-based both) are able to process categorical features from the box. Generally, tree-based models work good with a lot of features, from my experience. And linear models require right feature selection. Also, here's a link to a great example on Target Feature encoding, it might be also useful, however, you must be very attentive to train/test splits and overfitting using it.
