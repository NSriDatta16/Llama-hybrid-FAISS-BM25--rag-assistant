[site]: datascience
[post_id]: 71560
[parent_id]: 
[tags]: 
Preparing text for modeling in dialogue structure

I'm working on implementing the DialogueGCN code from this paper . Its a model that classifies the 'emotion' from utterances of text within a conversation. As this model takes into account speaker context, the structure of the data is very important. It looks something like this: c = conversation (each conversation is independent of the other u = utterances (ex. u1 = "Hey how are you?" u2 = "good, thanks, and you?" u3 = "confused, my dog is cooking pasta") Utterances are of variable length and conversations have variable amounts of utterances. Each utterance has an 'emotion' label. | corpus | / | \ c1 c2 c3 / | \ / | \ / | \ u1 u2 u3 u1 u2 u3 u1 u2 u3 The paper uses a CNN to generate features from the text data. The CNN model comes from Kim (2014) . There are several implementations of this paper. I am using this Keras implementation . The model architecture essentially boils down to this: ### the model input is 1 utterance z = Embedding(len(vocabulary_inv), embedding_dim, input_length=sequence_length, name="embedding")(model_input) z = Dropout(dropout_prob[0])(z) # Convolutional block conv_blocks = [] filter_sizes = [2,4,5] for sz in filter_sizes: conv = Convolution1D(filters=num_filters, kernel_size=sz, padding="valid", activation="relu", strides=1)(z) conv = MaxPooling1D(pool_size=2)(conv) conv = Flatten()(conv) conv_blocks.append(conv) z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0] z = Dropout(dropout_prob[1])(z) z = Dense(hidden_dims, activation="relu")(z) ### the output of the dense layer is a tensor for the utterance IIUC, each utterance of each conversation is individually input into this CNN and 1 tensor per utterance is the output. Then, all utterances per conversation are concatenated. After processing the whole corpus, the result is a list of concatenated tensors. This is fed into the GCN model. If you are still following me, my question is this: I'm trying to use the GCN model to predict on new set of dialogues. The dialogues are structured in the same format, but are still in text (string) form. I need to vectorize them to feed them into the CNN feature generation model. The DialogueGCN paper doesn't say how this was accomplished. What is the optimal way to vectorize strings when they are structured in the dialogue format where words in each utterance are dependent within a conversation, but words across conversations are independent? In other words, if I am converting each unique word to a unique integer, should I process the entire corpus as one (using the same word:integer mapping for all conversations), or should I process each conversation separately (unique word:int mapping for each convo) because they independent of one another. For instance, the word 'baseball' could mean something different in conversation 1 and conversation 2, depending on context (previous and future utterances).
