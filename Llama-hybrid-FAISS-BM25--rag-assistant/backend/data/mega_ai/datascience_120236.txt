[site]: datascience
[post_id]: 120236
[parent_id]: 120222
[tags]: 
Yes, you can think of hashing as random clustering/grouping. A minor point on the numbers: with 1M ids and 0.5M hash dimension, you will have an average of two ids assigned to each index (not a collision for half the ids), or 0.5M collisions total. how would that be usefull and not throw away (a lot) of information? It is primarily useful computationally, not statistically. With the hash function doing the encoding, you don't need to save anywhere a list of user ids or their mapping to indices, and don't have to look those up when encoding test/production data. It absolutely is throwing away information, a lot depending on the hash dimension. That said, the loss of information might not be so detrimental. A model built on the purer one-hot encoded data will tend to be overfit due to the high dimensionality. Of course, there are other ways to regularize that than randomly grouping. The wikipedia article lists two publications in which classification performance wasn't necessarily reduced (Ganchev and Dredze 2008) and actually improved (Weinberger et al. 2009) (but I haven't read either).
