[site]: crossvalidated
[post_id]: 292272
[parent_id]: 
[tags]: 
Estimators of Shannon entropy (to bound it from above)

I can calculate Shannon entropy from an empirical distribution, where $p_i = \frac{n_i}{n}$ for counts $n_i$ and their sum $n$. However, it seems that on average this value is lesser than actual entropy (simplest case: for a sample of size $n=1$ entropy is always $0$, where in fact it can be anything). I've heard that there is no unbiased estimator of Shannon entropy (unlike, say, Renyi for order $\alpha=2,3,4,\ldots$; e.g. collision entropy is fameously easy to estimate). However, is there a way to make an estimator, for which it's average is not greater than actual (i.e. in the limit of infinite samples) entropy? (It may be also a function of the numbers of bins.) My naive guess is that something like changing distribution to $n_i \mapsto n_i + 1$ and calculating it's entropy should work. (Though, this particular method is not enough, as it would underestimate entropy for the uniform distribution.)
