[site]: crossvalidated
[post_id]: 565504
[parent_id]: 
[tags]: 
Effect of apparent correlation due to clustered data on performance of binary classifier

I am exploring possible features for a binary classifier, probably using a SVM, and have encountered an issue with correlation of features. I have chosen a number of features that may be of interest because they are highly predictive of the wether an event is type 1 or type 2 (having one range of values for one and another range for the other that has minimal overlap). Naturally, because they are highly correlated with the event type, they are highly correlated with eachother across the dataset, which contains roughly equal numbers of type 1 and type 2 events. However, looking at the values within a particular event type, they are not correlated, suggesting that, in reality the values are not correlated with eachother. Now, clearly, as a predictive tool the correlation would be poor at predicting one value from the other as what we actually have is two clusters of uncorrelated data that result in apparent correlation when plotted together, there is no actual relation. However, from a stats perspective across the dataset they are mathematically correlated, and I am struggling to understand if the machine learning models that don't deal well with correlated data, such as SVM's, will struggle because values are mathematically correlated or only if there is an actual predictive trend. Any knowledge or advice around this issue would be highly appreciated.
