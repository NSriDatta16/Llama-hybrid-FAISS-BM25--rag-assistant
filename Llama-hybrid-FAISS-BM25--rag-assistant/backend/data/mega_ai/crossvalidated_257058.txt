[site]: crossvalidated
[post_id]: 257058
[parent_id]: 137798
[tags]: 
What you're doing with elastic, ridge, or lasso, using cross-validation to choose regularization parameters, is fitting some linear form to optimize prediction . Why these particular regularization parameters? Because they work best for prediction on new data. Shrinking coefficient estimates towards zero, introducing bias, (as is done in either Ridge or Lasso) can reduce overfitting and shrink variance . The idea is for your penalty parameters to strike the right balance in order to optimize prediction on new data. Imagine the data generating process is: $$ y_i = f(\mathbf{x}_i, \beta) + \epsilon_i$$ Let $\hat{\beta}$ be our estimate of parameters $\beta$ , and let $\hat{y}_j$ be our forecast for observation $j$ How should you present your results? It depends what your underlying research question is! You may want to step back and think deeply about what question you're trying to answer. What does your audience care about? What are you trying to do? Prediction? Estimate coefficients? Variable selection? It's important to distinguish between two types of research questions: Questions where you predominantly care about prediction, that is you care about $\hat{y}_j$ Questions where you predominantly care about parameter estimates $\hat{\beta}$ . Off the shelf machine learning techniques can be extremely powerful for the former, $\hat{y}$ prediction problems. As you appear to be recognizing though, standard off the shelf machine learning techniques may be extremely problematic for $\hat{\beta}$ , parameter estimate problems: In a high dimensional setting, many different parameterization will give you the same predictions $\hat{y}$ . If number of parameters $k$ is high relative to the number of observations $n$ , you may not be able to estimate any individual parameter well. Algorithms trained on different folds may have significantly different parameter estimates. The emphasis in machine learning is on prediction, not consistently estimating causal effects. (This contrasts with econometrics where typically the main issue is in consistently estimating causal effects). Prediction, estimating some functional form, is different than estimating causation. Police levels may be a good predictor of crime levels, and this doesn't mean police cause crime. And as you recognize, there may be issues in interpreting why some machine learning parameterization works. Is your audience comfortable with a prediction black box? Or is how prediction works central to your question? Lasso and Ridge: classic reasons to use them You can use elastic net for classic machine learning, prediction problems, situations where your main concern is $\hat{y}$ . In some sense regularization allows you to include more predictors but still keep overfitting under control. You can use regularization to prevent overfitting. Eg. ridge regression in the context of polynomial curve fitting can work quite nicely. As @Benjamin points out in his answer, Lasso can also be used for variable selection. Under certain regularity conditions, Lasso will consistently select the appropriate model: irrelevant coefficients will be set to zero. The $L_1$ and $L_2$ penalties, of Lasso and Ridge respectively, bias the coefficient estimates toward zero. If the bias is big, this could be a serious issue if you're trying to interpret coefficient estimates. And to get standard error estimates, you need to do something like bootstrapping; there aren't simple closed form solutions (that I'm aware of). Ridge, lasso, and elastic net have similarities to regular OLS regression, but the regularization and variable selection make inference quite different... What I keep coming back to is that it's quite difficult to interpret the results of running ridge regression, lasso, or elastic net without some more context of what you're trying to figure out! Prof. Sendhil Mullainathan gave a talk on machine learning at the January, 2017 AFA meeting which motivated parts of this post.
