[site]: crossvalidated
[post_id]: 605540
[parent_id]: 605466
[tags]: 
Based on the comment by J-J-J to the question which shows how to replicate the result of the online calculator. The difference between that online calculator and statsmodels samplesize_proportions_2indep_onetail function is in the assumption on the standard deviation of the statistic under the Null hypothesis. Statsmodels uses the pooled estimate (assuming proportions given by the alternative), while the online calculator assumes that the standard deviation is based on the proportion of the control. When I add that option to the statsmodels code, I get the same result as the online calculator: sm.stats.samplesize_proportions_2indep_onetail(0.01, 0.02, 0.8, null_var="prop2") 3292.2657351312073 Essentially, the null hypothesis assumes that the proportions are the same, but the methods differ in what that common value is assumed to be (p1=0.03, p2=0.02 in the example) "prop2": prop1 = prop2 = p2 = 0.02 "pooled": prop1 = prop2 = (p1 + p2) / 2 = 0.025 Note, the null hypothesis that the proportions are equal does not provide a condition on the common value. The common value is a "nuisance parameter" for the hypothesis test. Methods for testing this hypothesis differ in how they treat the nuisance parameter. Which approximation? There is a large number of hypothesis tests for comparing two Binomial proportions. Exact methods are usually very conservative in small samples. So many authors prefer approximate inexact methods that have better average size (rejections under the null). Similar approximations will apply to sample size and power computation for hypothesis tests for two independent proportions. I applies also to other statistics like confidence intervals. For example, statsmodels has the following inexact methods in test_proportions_2indep and similarly in the confidence interval function. diff: - 'wald', - 'agresti-caffo' - 'score' if correction is True, then this uses the degrees of freedom correction ``nobs / (nobs - 1)`` as in Miettinen Nurminen 1985 ratio: - 'log': wald test using log transformation - 'log-adjusted': wald test using log transformation, adds 0.5 to counts - 'score': if correction is True, then this uses the degrees of freedom correction ``nobs / (nobs - 1)`` as in Miettinen Nurminen 1985 odds-ratio: - 'logit': wald test using logit transformation - 'logit-adjusted': wald test using logit transformation, adds 0.5 to counts - 'logit-smoothed': wald test using logit transformation, biases cell counts towards independence by adding two observations in total. - 'score' if correction is True, then this uses the degrees of freedom correction ``nobs / (nobs - 1)`` as in Miettinen Nurminen 1985 In large samples, the answers will be very similar across methods. statsmodels does not have a comprehensive list of power and sample size functions for those methods. Power that uses proportion_effectsize is based on arcsin transformation, in imitation of the R pwr package. This is a variance stabilizing transformation. The variance of proportions depends on the value of the proportion. This means that the variance differs between the null and alternative hypotheses. Statsmodels has two functions samplesize_proportions_2indep_onetail and power_proportions_2indep for the asymptotic "z-test" for the null hypothesis that two proportions are the same. The resulting power and sample size will be only approximate if any of the other test methods are used that maintain average size. (Aside: I am not a fan of "exact" methods. They usually rely on very strong distributional assumptions and cannot be made robust to misspecification.)
