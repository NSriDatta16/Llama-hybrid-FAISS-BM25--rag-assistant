[site]: crossvalidated
[post_id]: 281802
[parent_id]: 249980
[tags]: 
There are some thousands of variants of RNN cell(kernel) and both LSTM and GRU are for processing the input $x_i$ and the output of the previous state $s_{i-1}$ , producing the output and the current state. Even though LSTM preceded GRU and GRU involves less computation, LSTM is just on a par with GRU in performance. Therefore, I believe that stacking LSTM and GRU, or any other cells, might be interesting but would not make a significant difference in improving performance when compared to simply stacking either LSTM cells or GRU cells. As George E. P. Box put it, all models are wrong, but some are useful, you can just have a try to see if it is useful or not.
