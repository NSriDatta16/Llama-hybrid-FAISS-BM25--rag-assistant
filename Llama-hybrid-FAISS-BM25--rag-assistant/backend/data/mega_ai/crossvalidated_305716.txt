[site]: crossvalidated
[post_id]: 305716
[parent_id]: 
[tags]: 
Training from a dataset with hidden positives

I am trying to predict a certain binary variable $Y\in\{0;1\}$ given a feature vector $X$. I want to train a predictor for $Y$ or $P(Y=1|X)$. Assume for simplicity that I would naturally use logistic regression (I will appreciate an answer with actually any model/method you like). I expect that $Y$ depends a lot on $X$ and that the predictive power will be very strong. If some pairs $(x,y)$ were known as a training set, this would be a classical case of supervised learning. But I only have a dataset consisting of two parts $A$ and $B$ such as: For all lines $(x,y)$ in $A$, $y=0$ ($A$ contains only negatives) For all lines $(x,y)$ in $B$, $y$ is unknown, we only know $x$ ($B$ contains both negatives and positives but we don't know who is who) How would you do to train a predictor for $Y$ with this kind of dataset? You can assume that the datasets were not artificially split: they originate from the same dataset where every positive line was put in $B$, every negative line was put in $A$ or in $B$ randomly (independently of $X$) with a certain probability.
