[site]: crossvalidated
[post_id]: 502750
[parent_id]: 
[tags]: 
Use of ignore_index on CrossEntropyLoss() for text models

I have been using PyTorch's CrossEntropyLoss() on a Language Autoencoder. I noticed that most people use ignore_index for ignoring the pad token in loss calculation eg this . From what I understand whenever the Label value is 0 (Corresponding to padding) it will not add to the loss irrespective of what the predicted value is. I have experienced experimentally is that it starts producing random values after the seq(where there should be ideally padding) when using ignore index, which makes sense as it doesnt add to loss. It outputs pad token only when not using it. So why include it, does it help training or make the model more robust?
