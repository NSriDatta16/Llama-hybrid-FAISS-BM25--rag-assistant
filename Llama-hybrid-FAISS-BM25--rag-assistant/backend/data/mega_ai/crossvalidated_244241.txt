[site]: crossvalidated
[post_id]: 244241
[parent_id]: 244239
[tags]: 
Oblique Random Forests The conventional random forest algorithm selects one feature to split at each node. This implies that if we want to achieve some kind of elaborate split (like defining a diagonal in two dimensions), many such splits will be required. On the other hand, if each node is something like a logistic regression, then we can directly accommodate linear combinations of features in a single node. Breiman mentioned this possibility in passing in his original random forest paper, but has not drawn the same attention that "orthogonal" random forest have. " On Oblique Random Forests " Bjoern H. Menze, B. Michael Kelm, Daniel N. Splitthoff, Ullrich Koethe, and Fred A. Hamprecht Rotation Forests Rotation forest uses PCA on the subsample of selected features at each split, and then splits in the orthogonal PCA basis. The effect is kinda like what you suggest here, because the projection into the PCA basis is a linear combination of the inputs. This isn't exactly the same as your suggestion, though, since the parameter estiamtes in the linear combinations of PCA are entirely fixed by the data used to fit them. This stands in contrast to the method you propose, which estiamtes a linear function of the data. Clearly, the cost of doing PCA at each split could be quite high, especially when the number of observations is large. But the upside to rotation forest is that it can ameliorate a weakness of Random Forest. Random Forest struggles when the decision boundary is diagonal in the original space. To the extent that rotation by PCA re-aligns that diagonal, the model will be better able to capture that effect. Here's the paper: Rodr√≠guez JJ1, Kuncheva LI, Alonso CJ. " Rotation forest: A new classifier ensemble method. " IEEE Trans Pattern Anal Mach Intell. 2006 Oct;28(10):1619-30.
