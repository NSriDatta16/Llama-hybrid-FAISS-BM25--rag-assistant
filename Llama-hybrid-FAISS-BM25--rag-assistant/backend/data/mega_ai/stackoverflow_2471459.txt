[site]: stackoverflow
[post_id]: 2471459
[parent_id]: 144309
[tags]: 
'Better' is hard to define... 'Faster' can be answered by benchmarking it with your code, and your hardware. Things like collective & offload optimisation will depend on your exact hardware and is also quite variable with regards to driver stack versions, google should be able to find you working combinations. As far as optimisation work, that somewhat depends on the code, and somewhat on the hardware. Is your code I/O bound to storage? In which case investigation something better than NFS might help a lot, or using MPI I/O rather than naive parallel I/O If you are network bound, then looking at communication locality, and comms/compute overlap can help. Most of the various MPI implementations have tuning options for using local shared memory rather than the network for intranode comms, which for some codes can reduce the network load significantly. Segregation of I/O and MPI traffic can make a big difference on some clusters, particularly for gigabit ethernet clusters.
