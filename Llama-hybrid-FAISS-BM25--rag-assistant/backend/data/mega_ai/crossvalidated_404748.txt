[site]: crossvalidated
[post_id]: 404748
[parent_id]: 404640
[tags]: 
The hypothesis you're testing is about equality of two population means . By replacing one population mean with a sample mean, you would treat as a known constant the random variable representing the mean of one of the samples, and would thereby substantially underestimate the variance of the difference. As a result, you would inflate the significance level from the nominal one. A quick example simulation in R suggests that for equal sample sizes and a nominal 5% significance level your true significance level (i.e. your actual type I error rate) is in the ballpark of 15-16% across a range of typical sample sizes between 20 and 200 -- triple what we wanted it to be. It seems to be more than 5 times the desired level for a 1% test. (We could compute the values these simulations estimate but it doesn't seem essential to discover their exact values) To my astonishment, I have seen this same error made surprisingly frequently, even a few times by researchers. n=20;rowMeans(replicate(10000;{x=rnorm(n);y=rnorm(n);c(t.test(x,y,var.equal=TRUE) $p.value,t.test(x,mu=mean(y))$ p.value)}) (with only 10000 simulations I suggest running it multiple times to get a good sense of what the averages are -- or to increase the simulation size)
