[site]: crossvalidated
[post_id]: 565642
[parent_id]: 
[tags]: 
If going with the opposite prediction of a bad predictor gives good predictions, why not do that?

Let’s restrict our consideration to binary outcomes. I have a friend who is terrible at predicting the future, always predicting the opposite of what winds up happening. For instance, my friend predicted Cincinnati to win the Super Bowl, but they lost. My friend predicted that Croatia would beat France for the 2018 World cup, but France won. Et cetera... That is, my friend's predictions give me excellent predictive accuracy if I go with the opposite of what he predicted. “My friend always gets it wrong, so bet on Los Angeles to beat Cincinnati,” I should have thought a few weeks ago. “Bet on France to beat Croatia,” I should have thought four years ago. What is wrong with applying this logic to a classification or probability machine learning model that consistently predicts the opposite, measured by $AUC\approx 0?$ I realize that the coefficient estimates should not be trusted, but if we care about the prediction above all else, the danger is not so apparent to me. That is, if I get $AUC = 0.2$ from the probability predictions $p_i$ of a model, why not take $1-p_i$ as my final predicted probability? EDIT $AUC , McFadden’s $R^2 …whatever metric convinces you that you’re consistently predicting the opposite of what you’re seeing (though I have only ever seen this suggested when the $AUC$ is less than $0.5$ , which I’m pretty sure is how some popular software packages approach ROC and AUC calculations).
