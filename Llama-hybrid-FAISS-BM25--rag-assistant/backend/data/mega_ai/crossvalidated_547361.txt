[site]: crossvalidated
[post_id]: 547361
[parent_id]: 547344
[tags]: 
One-hot encoding ensures that no implicit order is imposed on the feature while integer/label encoding benefits from it. If there is no inherent ordering, the usual approach is one-hot encoding, however sometimes (e.g. in high cardinality) other options can be preferred. How you encode your features always matters because it changes the model's behavior. For example, in random forests, or simply decision trees, with label encoder it's possible to split the samples into two categories where one side is say Red, Blue and the other side is Green, Yellow if they are ordered as Red, Blue, Green, Yellow (i.e. 1,2,3,4), by splitting wrt value $2.5$ . This is not possible in one split with one-hot encoding. However, it may or may not make sense doing this in the context of the problem. This naturally affects the branching of your tree(s) because of hyper-parameters like max depth. Therefore, we can't say that OHE only matters for models where you multiply your features with coefficients.
