[site]: crossvalidated
[post_id]: 234236
[parent_id]: 233548
[tags]: 
I'll answer for artificial neural networks (ANNs). The hyperparameters of ANNs may define either its learning process (e.g., learning rate or mini-batch size) or its architecture (e.g., number of hidden units or layers). Tuning architectural hyperparameters on a subset of your training set is probably not a good idea (unless your training set really lacks diversity, i.e. increasing the training set size doesn't increase the ANN performance), since architectural hyperparameters change the capacity of the ANN. I would be less concerned tuning the hyperparameters that define the learning process on a subset of your training set, but I guess one should validate it empirically.
