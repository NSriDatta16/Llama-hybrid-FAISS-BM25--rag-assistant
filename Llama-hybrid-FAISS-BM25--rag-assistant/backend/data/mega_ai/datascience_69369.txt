[site]: datascience
[post_id]: 69369
[parent_id]: 69357
[tags]: 
Yes you definitely can. Here's an example: Using Convolutional Neural Networks to Classify Hate-Speech The authors used classic embeddings concatenated with a vector of size 28 representing the presence or not (in a tweet) of each letter of the alphabet (26) plus any digit (1) plus any other symbol (1). So basically for a tweet like 'I love NLP!' the representation would be: (embeddings)(00001000100110110000000)(0)(1) where you have the classic embeddings for each word, then a one hot vector for the alphabet letters, another hot vector (just a binary variable actually) for digits and one last that check for the presence of any other symbol (! in this case). The resulting concatenated vector is then passed to a LSTM, convolution layer or whatever architecture you want to use. Consider that there are infinite other variables that you can concatenate to the embeddings. For example you could concatenate counters for each alphabet letter or any other symbol in a sentence (in this case I suggest to separately normalise these variables between 0 and 1 before concatenating them to the embeddings), one-hot representation of n-grams or dependencies, the total number of words, presence or not and counters of capital letters, presence of entities, sentiment scores obtained through external resources like Vader (see nltk) and so on.
