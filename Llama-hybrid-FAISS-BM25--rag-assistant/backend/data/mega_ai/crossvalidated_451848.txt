[site]: crossvalidated
[post_id]: 451848
[parent_id]: 451801
[tags]: 
However, is there any method to rectify this problem? No, for the simple reason that this is not a problem, but a property of your models. Let's have a look at Figure 2-A (left panel) of the following paper . We have 2 models: A model called "binomial", which is a mixture of several Gaussians; A model called "gaussian", which is just one Gaussian distribution. The binomial model has several parameters, including a value $\sigma$ (which is the variance of each Gaussian in the mixture, and hence the width of each "peak" you see in the figure). If $\sigma$ becomes too high, the peaks in the binomial model will overlap, and it will become very similar to a Gaussian distribution, and data generated from the binomial will be misclassified (the Gaussian model will provide a better fit). That's very similar to your situation, where for $v$ close to 0.5, the complex model (with more parameters) becomes indistinguishable from the simpler model. However, the identifiability of a model is a function not only of its parameters, but also of the experimental protocol (i.e. the number of data points). Intuitively, if you increase the number of data points in your samples, the missclassification shall decrease (although it will remain peaky around 0.5) You might find the following papers interesting: Acerbi, L., Ma, W. J., & Vijayakumar, S. (2014). A framework for testing identifiability of Bayesian models of perception. In Advances in neural information processing systems (pp. 1026-1034). Navarro, D. J., Pitt, M. A., & Myung, I. J. (2004). Assessing the distinguishability of models and the informativeness of data. Cognitive psychology, 49(1), 47-84. Daw, N. D. (2011). Trial-by-trial data analysis using computational models. Decision making, affect, and learning: Attention and performance XXIII, 23(1). I myself am working on this subject for my PhD project. Happy to discuss it any further ! EDIT I took the problem in the wrong way in my answer: I looked at how data generated from a complex model are correctly classified compared to a simple model. Here, the question is why are some data generated from a simple model misclassified as being generated from a complex model. The intuition is indeed that, on average , data generated from a simple model should not be misclassified and attributed to a more complicated model. Indeed, both models, the simple and the complex, are going to fit data equally well, but the Bayes Factor is going to favor the simpler one. I insist on the on average : it is still possible that a simple model will, from time to time, generate weirdly distributed data for which a complex model will provide a very good fit. But that is unlikely, so on average, if data are generated from a simple model, then model selection will choose the simple model. I tried to obtain a formal proof for this intuition : Formal proof of Occam's razor for nested models I think that is indeed what you observe on your first plot: while some data are misclassified, on average the BF is > 1.
