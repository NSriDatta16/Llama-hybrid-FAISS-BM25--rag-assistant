[site]: crossvalidated
[post_id]: 215433
[parent_id]: 174497
[tags]: 
ReLus are one of the reasons why modern Deep Learning works so great. They seem to be very fast regarding evaluation and learning. And yes, you can use them with cross-entropy! The thing is, that you don't want to use ReLus ever as your last activation before the output! There you usually go for softmax or sigmoid (or even not using any activation at all) depending on your problem.
