s better samples, while the discriminator becomes more skilled at flagging synthetic samples. When used for image generation, the generator is typically a deconvolutional neural network, and the discriminator is a convolutional neural network. Relation to other statistical machine learning methods GANs are implicit generative models, which means that they do not explicitly model the likelihood function nor provide a means for finding the latent variable corresponding to a given sample, unlike alternatives such as flow-based generative model. Compared to fully visible belief networks such as WaveNet and PixelRNN and autoregressive models in general, GANs can generate one complete sample in one pass, rather than multiple passes through the network. Compared to Boltzmann machines and linear ICA, there is no restriction on the type of function used by the network. Since neural networks are universal approximators, GANs are asymptotically consistent. Variational autoencoders might be universal approximators, but it is not proven as of 2017. Mathematical properties Measure-theoretic considerations This section provides some of the mathematical theory behind these methods. In modern probability theory based on measure theory, a probability space also needs to be equipped with a σ-algebra. As a result, a more rigorous definition of the GAN game would make the following changes:Each probability space ( Ω , B , μ ref ) {\displaystyle (\Omega ,{\mathcal {B}},\mu _{\text{ref}})} defines a GAN game. The generator's strategy set is P ( Ω , B ) {\displaystyle {\mathcal {P}}(\Omega ,{\mathcal {B}})} , the set of all probability measures μ G {\displaystyle \mu _{G}} on the measure-space ( Ω , B ) {\displaystyle (\Omega ,{\mathcal {B}})} . The discriminator's strategy set is the set of Markov kernels μ D : ( Ω , B ) → P ( [ 0 , 1 ] , B ( [ 0 , 1 ] ) ) {\displaystyle \mu _{D}:(\Omega ,{\mathcal {B}})\to {\mathcal {P}}([0,1],{\mathcal {B}}([0,1]))} , where B ( [ 0 , 1 ] ) {\displaystyle {\mathcal {B}}([0,1])} is the Borel σ-algebra on [ 0 , 1 ] {\displaystyle [0,1]} .Since issues of measurability never arise in practice, these will not concern us further. Choice of the strategy set In the most generic version of the GAN game described above, the strategy set for the discriminator contains all Markov kernels μ D : Ω → P [ 0 , 1 ] {\displaystyle \mu _{D}:\Omega \to {\mathcal {P}}[0,1]} , and the strategy set for the generator contains arbitrary probability distributions μ G {\displaystyle \mu _{G}} on Ω {\displaystyle \Omega } . However, as shown below, the optimal discriminator strategy against any μ G {\displaystyle \mu _{G}} is deterministic, so there is no loss of generality in restricting the discriminator's strategies to deterministic functions D : Ω → [ 0 , 1 ] {\displaystyle D:\Omega \to [0,1]} . In most applications, D {\displaystyle D} is a deep neural network function. As for the generator, while μ G {\displaystyle \mu _{G}} could theoretically be any computable probability distribution, in practice, it is usually implemented as a pushforward: μ G = μ Z ∘ G − 1 {\displaystyle \mu _{G}=\mu _{Z}\circ G^{-1}} . That is, start with a random variable z ∼ μ Z {\displaystyle z\sim \mu _{Z}} , where μ Z {\displaystyle \mu _{Z}} is a probability distribution that is easy to compute (such as the uniform distribution, or the Gaussian distribution), then define a function G : Ω Z → Ω {\displaystyle G:\Omega _{Z}\to \Omega } . Then the distribution μ G {\displaystyle \mu _{G}} is the distribution of G ( z ) {\displaystyle G(z)} . Consequently, the generator's strategy is usually defined as just G {\displaystyle G} , leaving z ∼ μ Z {\displaystyle z\sim \mu _{Z}} implicit. In this formalism, the GAN game objective is L ( G , D ) := E x ∼ μ ref ⁡ [ ln ⁡ D ( x ) ] + E z ∼ μ Z ⁡ [ ln ⁡ ( 1 − D ( G ( z ) ) ) ] . {\displaystyle L(G,D):=\operatorname {E} _{x\sim \mu _{\text{ref}}}[\ln D(x)]+\operatorname {E} _{z\sim \mu _{Z}}[\ln(1-D(G(z)))].} Generative re