[site]: datascience
[post_id]: 122766
[parent_id]: 110180
[tags]: 
You could do that. I believe the authors of the original attention paper believed that it isn't as efficient as trying to inject the positional information into the embedding - you add parameters to the model and that has implications for training and inference speed. I think the intuition is that the difference encoding between "foo" and "bar" after positional encoding becomes ("foo + pos_foo) - ("bar" + pos_bar) = ("foo" - "bar") + (pos_foo - pos_bar). So it should be possible for the model to learn pretty reasonably about how the difference in position modifies relationship between words. It isn't the only or best way of adding positional info, as you allude to. In the conventional positional encoding, position information is fairly "absolute" which makes it hard to generalize to longer contexts and harms ability to learn that a) short distances probably imply more relationship than long and b) the relationship between words is probably similar across the context. Rotary positional encoding tries to address that. Then there is ALiBi, which throws out the idea of embedding positional encoding entirely and just decays the strength of relationship based on distance, and that actually works OK. So, maybe positional info isn't that important to represent so exactly, let alone as additional dimensions in the embedding.
