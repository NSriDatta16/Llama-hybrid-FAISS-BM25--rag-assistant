[site]: datascience
[post_id]: 11859
[parent_id]: 9228
[tags]: 
Classifiers like Decision Tree, Bayesian, Back-propagation, Support Vector Machine come under the category of "Eager Learners" , because they first build a classification model on the training dataset before being able to actually classify an [unseen] observation from test dataset . The learned model is now "eager" (read hungry) to classify previously unseen observations, hence the name. The KNN-based classifier, however, does not build any classification model. It directly learns from the training instances (observations). It starts processing data only after it is given a test observation to classify. Thus, KNN comes under the category of "Lazy Learner" approaches. Based on the above foundational differences, we can conclude the following:- Since KNN performs on-the-spot learning, it requires frequent database lookups, hence, can be computationally expensive. Decision Tree Classifier does not require such lookups as it has in-memory classification model ready. Since KNN performs instance-based learning, a well-tuned K can model complex decision spaces having arbitrarily complicated decision boundaries, which are not easily modeled by other "eager" learners like Decision Trees. "Eager" learners work in batches, modeling one group of training observations at a time. So they are not fit for incremental learning. But KNN naturally supports incremental learning (data streams) since it is an instance-based learner. Further, KNN classifier gives test error rates closer to that of Bayesian classier (the gold standard). As quoted in ISLR : The Bayes error rate is analogous to the irreducible error
