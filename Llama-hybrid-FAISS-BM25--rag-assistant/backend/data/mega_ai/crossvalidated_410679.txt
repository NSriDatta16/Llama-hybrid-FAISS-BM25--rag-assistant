[site]: crossvalidated
[post_id]: 410679
[parent_id]: 
[tags]: 
SGD versus Adamax on XOR operator

I am trying to resolve the xor operator using neural networks, and to accomplish that this is my code: X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) y = np.array([[0], [1], [1], [0]]) model = Sequential([ Dense(2, activation="sigmoid", input_dim=2), Dense(1, activation="sigmoid") ]) model_1.compile(loss="binary_crossentropy", optimizer="adamax") model_1.fit(X, y, batch_size=4, epochs=16000) model_2.compile(loss="binary_crossentropy", optimizer="sgd") # Never converge independently of how many epochs model_2.fit(X, y, batch_size=4, epochs=16000) So, can someone explain why using SGD the model does not converge? Why in this case using adamax is better than SGD? And when to use sgd or adamax.
