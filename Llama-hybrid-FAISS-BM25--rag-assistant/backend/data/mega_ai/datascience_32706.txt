[site]: datascience
[post_id]: 32706
[parent_id]: 32705
[tags]: 
Normalising input data to a neural network is known to improve convergence properties, i.e. the model should converge quicker because the normalised data will not be so likely to produce huge or tiny gradients. Huge gradients make training erratic and less likely to converge (without applying other tricks), while tiny loss gradients lead to neuron death whereby certain neurons may get stuck, unable to get to an optimal weight value. As you pointed out, normalising can be trivially simply to perform, so it might be worthwhile for both the final results and as a learning experience to try it out and compare your results. You might also consider using other techniques such as batch normalisation, which inherently give you similar properties to normalising the entire dataset at the beginning.
