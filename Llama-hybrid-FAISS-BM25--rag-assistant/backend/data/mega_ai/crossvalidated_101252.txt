[site]: crossvalidated
[post_id]: 101252
[parent_id]: 
[tags]: 
Is it helpful to have monotonic features when using a random forest for classification?

I am training a random forest for binary classification. Here is a plot of one of my features, which is an integer giving the number of months since an event. The y-axis gives the proportion of cases ($y_i=1$) for each value of the feature. Error bars indicate the $95\%$ binomial confidence interval. The value on the extreme left hand side occurs when there has been no such event, hence the number of months since the event is undefined. It has currently been encoded with the value of $-999997$ but this is entirely arbitrary. My question is whether it is advantageous when training the random forest to re-encode this so that it fits in with the (roughly) monotonic relationship between the probability that $y_i=1$ and the number of months since the event. I have a vague understanding of how the decision trees that make up the random forest are constructed, and intuitively it seems to me that leaving it as $-999997$ requires a more complex rule. How much of a problem is this?
