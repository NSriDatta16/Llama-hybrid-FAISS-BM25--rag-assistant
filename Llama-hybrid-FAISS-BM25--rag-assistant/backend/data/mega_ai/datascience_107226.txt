[site]: datascience
[post_id]: 107226
[parent_id]: 100274
[tags]: 
With relative position bias, you are extending the concept of self-attention to also encode the distance between any two tokens. Basically you let the model itself learn the relative distance between any 2 tokens instead of feeding that information yourself. Most of the time (as shown in the paper), the model does a good job at figuring out the relationships between various tokens. If that is the case, it is always better to make the model generic by giving it the flexibility rather than providing this data & forcing the model to use it. Usually generalized models perform better. This is the simplest reason to not feed explicit positional embeddings. For more details on the bias parameterization, you can refer to the relevant section in - https://towardsdatascience.com/swin-vision-transformers-hacking-the-human-eye-4223ba9764c3
