[site]: crossvalidated
[post_id]: 544168
[parent_id]: 
[tags]: 
Proving Equivalence between Multivariate distributions and Gaussian Bayesian Networks

I am studying Probabilistic Graphical Models by Daphne Koller. In Chap 7, the authors say the following. I can't convince myself of the highlighted part. Induction typically has a statement for n, using which you prove a statement for n+1. Given the graphical nature of the problem, I can't formulate what the n case means, and how to increment it to a larger (n+1) case. For example, consider the graph shown below. Here, $X_1,\dots,X_5$ are jointly normal, and arrows denote linear combinations, so that, $Y = \sum_{i=1}^4 \beta_i X_i$ . Now using the "base" case, one can argue that $X_1,X_2,X_3,X_4,Y$ are jointly normal, and similarly, $X_5,Y,Z$ are jointly normal. But how does one argue that all the variables ( $X_is, Y, Z$ ) are jointly normal?
