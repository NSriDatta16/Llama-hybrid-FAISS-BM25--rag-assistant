[site]: crossvalidated
[post_id]: 532273
[parent_id]: 532255
[tags]: 
The maximum likelihood problem is not guaranteed to have a solution as the so-called " perfect separation of classes " problem may arise. In that case, the log-likelihood is unbounded. This is very likely to happen with small samples such as yours, in which the number of parameters is not much smaller than the number of observations. A second issue may arise: a poor initialization of the parameter vector makes the exponentials in the logistic function and their derivatives numerically indistinguishable from 0. This prevents a derivative-based algorithm like fminunc from making any progress. This is the so-called "vanishing gradient problem", well-known in the training of neural nets (logistic regression is just a special case of neural-net training). Setting the parameter vector equal to zero should solve the problem (try neighborhoods of zero to check the sensitivity to starting value). Moreover, you should carefully set the gradient tolerance in the stopping criteria. You could also try derivatives-free optimization algorithms like fminsearch (in my experience the latter often works better than fminunc).
