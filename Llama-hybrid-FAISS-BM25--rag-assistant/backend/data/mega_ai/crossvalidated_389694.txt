[site]: crossvalidated
[post_id]: 389694
[parent_id]: 
[tags]: 
ML Combination of Clustering and Regression

I recently faced a question that I, strangely, never thought of... Basically, I am considering a setup in which I want to perform regression on a very rich dataset. The chances to overfit are really high, and I am lead to think that a global model could not work. So, operatively, I am lead to think that what one could do is to define not a global model, with the aim to fit just one single regression model for the whole dataset, but to perform many regressions, eventually with different parameters and feature importance, on properly identified clusters. Said in other words, I am looking for a pipeline to combine a clustering approach to a subsequent local regression. My question is: theoretically, are there methods that have been investigated in this direction? I must confess I am not aware of any of them. I always considered the dataset to start with as a whole, and try at best to train a model curing overfitting with the aim to achieve the best in terms of generalization to a test set. EDIT I am now puzzled in some respects in regard to the fact that ensemble learning such as random forest regression could be regarded, in some respect, as a simultaneous combination of clustering and regression. Indeed, different learners which learn different subsets of the data would highlight different aspects of it and therefore would result in fitting a quite different model for these substantially different parts of the data. Is that correct? So, eventually, a potential answer to the question is: is ensemble learning anyhow related to learning different learners on (slightly) different subsets/clusters of the data? Of course the big difference is that in ensemble learning the subsets of the data are randomly drawn, and not selected according to some feature distribution. But even coped with this randomness, could some ensemble techniques boil down to ending up with different learners for (stochastically identified across random drawing of the subsamples) clusters? That said, it is true that the ensemble philosophy washes out these differences in that it tries to aggregate the N learners into one, so for very heterogenic datasets could not be the strategy. But, that said, it would be nice to know whether there is a technique which puts in practice the first part of what I outlined above, i.e. progressive cluster formation based on the performance of a local regression model on the randomly drawn subset of the training data.
