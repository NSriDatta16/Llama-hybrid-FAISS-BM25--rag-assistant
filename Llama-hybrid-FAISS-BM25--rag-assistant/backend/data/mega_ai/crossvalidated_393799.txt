[site]: crossvalidated
[post_id]: 393799
[parent_id]: 393316
[tags]: 
I would argue that only the error bars matter, but in the given example, the whole thing is probably almost meaningless. The example lends itself to interpretaton as a confidence interval, in which the upper and lower bounds of some degree of certainty are the range of probability. This proposed answer will deal with that interpretation. Majority source -- https://www.amazon.com/How-Measure-Anything-Intangibles-Business-ebook/dp/B00INUYS2U The example says that to a given level of confidence, the answer is unlikely to be above 60% and equally unlikely to be below 50%. This is so convenient a set of numbers that it resembles "binning", in which a swag of 55% is further swagged to a +/- 5% range. Familiarly round numbers are immediately suspect. One way to arrive at a confidence interval is to decide upon a chosen level of confidence -- let's say 90% -- and we allow that the thing could be either lower or higher than our estimate, but that there is only a 10% chance the "correct" answer lies outside of our interval. So we estimate a higher bound such that "there is only a 1/20 chance of the proper answer being greater than this upper bound", and do similar for the lower bound. This can be done through "calibrated estimation", which is one form of measurement, or though other forms of measurement. Regardless, the point is to A) admit from the beginning that there is an uncertainty associated with our uncertainty, and B) avoid throwing up our hands at the thing, calling it a mess, and simply tacking on 5% above and below. The benefit is that an approach rigorous to a chosen degree can yield results which are still mathematically relevant, to a degree which can be stated mathematically: "There is a 90% chance that the correct answer lies between these two bounds..." This is a properly formed confidence interval (CI), anmd it can be used in further calculations. What's more, by assiging it a confidence, we can calibrate the method used to arrive at the estimate, by comparing predictions vs results and acting on what we find to improve the estimation method. Nothing can be made perfect, but many things can be made 90% effective. Note that the 90% CI has nothing to do with the fact that the example given in the OP contains 10% of the field and omits 90%. What is the wingspan of a Boeing 747-100, to a 90% CI? Well, I'm 95% sure that it is not more than 300 ft, and I am equally sure that it is not less than 200 ft. So off the top of my head, I'll give you a 90% CI of 200-235 feet. NOTE that there is no "central" estimate. CIs are not formed by guesses plus fudge factors. This is why I say that the error bars probably matter more than a given estimate. That said, an interval estimate (everything above) is not necessarily better than a point estimate with a properly calulated error (which is beyond my recall at this point -- I recall only that it's frequently done incorrectly). I am just saying that many estimates expressed as ranges -- and I'll hazard that most ranges with round numbers -- are point+fudge rather than either interval or point+error estimates. One proper use of point+error: "A machine fills cups with a liquid, and is supposed to be adjusted so that the content of the cups is 250 g of liquid. As the machine cannot fill every cup with exactly 250.0 g, the content added to individual cups shows some variation, and is considered a random variable X. This variation is assumed to be normally distributed around the desired average of 250 g, with a standard deviation, Ïƒ, of 2.5 g. To determine if the machine is adequately calibrated, a sample of n = 25 cups of liquid is chosen at random and the cups are weighed. The resulting measured masses of liquid are X1, ..., X25, a random sample from X." Key point: in this example, both the mean and the error are specified/assumed, rather than estimated/measured.
