[site]: crossvalidated
[post_id]: 591285
[parent_id]: 
[tags]: 
Variance of a Straight line model [time series with deterministic mean]

I have been given the question - Consider the linear trend model: = 0 + 1 + + 0.5−1. You can assume 0=10, 1=0.2 and ^2=1. Find Var(). I tried solving this question by doing this - var() = var(0 + 1 + + 0.5−1) var() = var(0) + var(1) + var() + var(0.5−1) var() = 0 + (1^2).var() + ^2 + (0.25).^2 var() = (0.04).var() + 1 + 0.25 var() = (0.04).var() + 1.25 I am not sure if this is the right approach. And if it is what exactly is the value of var()? Any help in guiding towards the solution would be appreciated.
