[site]: crossvalidated
[post_id]: 369713
[parent_id]: 271522
[tags]: 
The objetive of an autoencoder is to learn an encoding of something (along with its decoding function). There are many uses for an encoding. In a variational autoencoder what is learnt is the distribution of the encodings instead of the encoding function directly. A consequence of this is that you can sample many times the learnt distribution of an object’s encoding and each time you could get a different encoding of the same object. In this sense, variational autoencoders capture the idea that you can represent something in many ways as long as the “essence” is present in all the encodings. What is “essential” to be represented in each encoding is problem dependent; some problems may require more precision and other problems less. The precision of the encodings can be adjusted by adjusting the neural network used for learning the encoding as well as the cost function used to judge how similar is the reconstruction of the object from the encoding. Variational autoencoders can learn more complex objects than plain autoencoders given the same amount of data with the trade-off of being less precise (although being less precise is not always a bad thing).
