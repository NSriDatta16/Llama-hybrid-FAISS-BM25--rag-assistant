[site]: datascience
[post_id]: 44695
[parent_id]: 44693
[tags]: 
The question whether to use a linear classifier depends less on the number of samples you have in your dataset and more whether your dataset is linearly separable (by the way, SVMs can be non-linear with the kernel trick). Now with regards to confidence in the classification, In SVMs there is a method that calculates the probability that a given sample belongs to a particular class using Platt scaling (" Original Paper "). This is the approach that is used in sklearn's SVM confidence implementation. You can read more about it in the following link: How To Compute Confidence Measure For SVM Classifiers In both SVMs and linear regression models you can calculate the distance of a sample from the border and treat it as a confidence measurement (but it is not exactly that). With decision trees I'm not an expert but a similar question was posted and answered in the following link: Decision tree, how to understand or calculate the probability/confidence of prediction result I would strongly recommend using some known embedding method like the word2vec, since as you mentioned, your dataset is too small for your model to be able to properly learn an encoding of context and vocabulary from.
