[site]: crossvalidated
[post_id]: 586327
[parent_id]: 
[tags]: 
Why are model validation and optimism correction separately conducted?

In this excellent reply , Demetri Pananos estimates a regularized logistic regression model via cross-validation, then bootstraps the sample to derive an optimism-corrected calibration curve for that model. Each iteration uses the same model. The only thing that changes is the sample. In another great reply to a similar question, Pananos develops one bootstrapped routine within which he estimates a model to get optimism-corrected performance metrics. In each iteration, both the estimating model and the sample change. I'm not sure if these two approaches would yield similar corrections for the same dataset. On one hand, bootstrapping emphasizes that it's the strategy, and not the model, that should be repeatedly estimated. Changes in the model specification is a feature here, not a bug. On the other hand, if the bias correction is for a specific model, then why should we allow the model to change between iterations? Is one approach necessarily better than the other?
