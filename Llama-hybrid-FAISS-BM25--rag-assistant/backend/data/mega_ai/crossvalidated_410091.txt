[site]: crossvalidated
[post_id]: 410091
[parent_id]: 
[tags]: 
Empirical Comparison: which ideal data characteristics are best captured by each type of machine learning model?

I have reached the point as a data scientist where the empirical differences between the different types of regression models (leaving out classification only for simplicity) have started to matter more to me, since the theories behind them have already been grasped. The theories behind Support Vector Machines and how their how kernels allow non-linear relationships to be modeled in a linear way, and of neural nets being universal function approximators are very important to know, but how these principles bear out empirically is not as clear. All other things equal, can SVM's capture certain non-linearities as well as neural nets ? What about Gaussian Processes (which, based on the title of the link, are somehow superior to SVM's)? Is there any research on the data characteristics that are best captured by each major type of machine learning model? The ideal data qualities for each model, so to say? As an example, linear regression has the BLUE statement (Gauss-Markov theorem): if certain characteristics are met in data (such as uncorrelated errors), then OLS estimates the best possible linear model. Do machine learning models proper have similar statements under certain conditions? Any help in resolving this matter would be extremely appreciated by me.
