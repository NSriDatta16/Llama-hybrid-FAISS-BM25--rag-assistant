[site]: crossvalidated
[post_id]: 110925
[parent_id]: 
[tags]: 
Does it make sense to do CV-error-weighted model averaging?

We often average models together to create an aggregate prediction model. Some recent research suggests that simple model averages perform as well or better than model averages weighted by functions of information criterion scores. Weighted or simple, model averaging often (but not always) performs better than choosing the model with the best score. Many information criterion scores can be seen as approximations to different cross-validation schemes. Even though one model performs worse in cross-validation than another, it is still informative. I argue that an average model with weights as functions of cross-validation scores makes sense. Yet most people who do cross-validation to tune hyperparameters just choose the value of the hyperparameter that minimizes cross-validation error. In what circumstances might we expect a cross-validation-error-weighted average model to perform better than a single "best" model? I would love it if someone could point me to the most relevant research on this. Thank you.
