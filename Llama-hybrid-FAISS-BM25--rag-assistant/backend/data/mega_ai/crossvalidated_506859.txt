[site]: crossvalidated
[post_id]: 506859
[parent_id]: 224051
[tags]: 
I totally agree with @Matthew Drury and @Cameron Bieganek's analysis of perfect collinearity and degree of freedom. However, I want to argue here that we do not need to avoid perfect collinearity if we are using methods such as gradient descends as our optimizer . (Update, I just realized that there are more situations where we do not need to avoid collinearity: For example, We do not need to avoid perfect collinearity if we do regression with regularizer either. Since it will add a identity matrix $\lambda I$ to the matrix( $X^TX$ ) below invertible again. ) The reason why we might want to avoid perfect collinearity is when we are using linear regression and our loss function is MSE , we could solve the closed form solution, which involves the inverse of a matrix about $X$ , and perfect collinearity would make this matrix non invertible. However, in practice, as the computation of inverse matrix is quite expensive $\mathcal{O}(n^3)$ , we could use other faster method such as gradient descend to compute an approximate solution and this process does not involves the inverse of matrix. Thus, perfect collinearity could be tolerated here.(Maybe this is why they do not have a warning) So, we could either use: one hot with intercept or one hot without intercept or dummy with intercept. and they would generate quite similar result. I run a regression using mpg dateset with sklearn, apply one hot or dummy to the "origin" feature which has three categories and the result is quite similar: (Please pay attention to how the residual and the coef are alike to each other, I marked parameters in front of enumerated category terms with red rectangles, with the others are the parameters of some continuous features. Detailed code could be seen here , sorry for the lack of comment and a large part of the code comes from tensorflow turorials .) Relationship between the three results: BTW, we could also observe that for case of one hot without intercept, the parameters in front of the last three one hot features are actually equals to the element-wise sum of intercept and the parameters in front of the last three one hot features in one hot with intercept, which can be explained by the perfect collinearity. We can also notice that in the dummy with intercept case, the intercept is actually the parameter(-13.71778...) of the second categorical encoded term in the one-hot without intercept case. And the two parameter of the categorical encoded term in the dummy case is the difference between the corresponding parameters wrt the second term parameter, which is consistent with the interpretation of the parameter before categorical terms in econometrics: how much different each of the other categories makes to the output compare to the basic category.
