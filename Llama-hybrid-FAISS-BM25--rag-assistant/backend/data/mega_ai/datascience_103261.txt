[site]: datascience
[post_id]: 103261
[parent_id]: 
[tags]: 
What does "S" in Shannon's entropy stands for?

I see many machine learning texts using the following notation to represent Shannon's entropy in classification/supervised learning contexts: $$ H(S) = \sum_{i \in Y}p_i \log(p_i) $$ Where $p_i$ is the probability of a given point being of class $i$ . I just do not understand what is $S$ because no further explanation about it is provided. Does it has something to do with the feature $S$ in the dateset? $S$ seems to appear again in Information Gain formula: $$ \operatorname{IG}(S,A) = H(S) - \sum_{a \in A} \frac{S_a}{S}H(S_a) $$ I know Information Gain and Entropy concepts, I just would like to understand the mathematical formalism.
