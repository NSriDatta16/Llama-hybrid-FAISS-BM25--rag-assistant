[site]: datascience
[post_id]: 62789
[parent_id]: 62649
[tags]: 
Least squares support vector machines were first introduced by Suykens and Vandewalle in 1999. They are slightly prior to proximal SVM proposed by Fung and Mangasarian in 2001. It appears that the latter did not know about LS-SVM and re-discovered the approach in a very similar manner. Both methods are very close in the aspect that, instead of solving a quadratic programming (quadratic problem optimization with linear constraints), they solve a linear system. The trick to do this is to transform the optimization constraint from an inequation to an equation. Both methods use the same trick. There appear to be minor differences though. In the conclusion of their paper on P-SVM , Fung and Mangasarian note: Least squares are also used in [33] to construct an SVM, but with the explicit requirement of Mercerâ€™s positive definiteness condition [35], which is not needed here. Furthermore, the objective function of the quadratic program of [33] is not strongly convex like ours. This important feature of PSVM influences its speed as evidenced by the many numerical comparisons given here but not in [33]. Where [33] is the original LS-SVM paper by Suykens and Vandewalle. As of today, a quick internet search shows that LS-SVM are much more common. Whether it is because of anteriority / initial popularity, because they perform better, and/or because their implementation is easier, however, I don't know.
