[site]: crossvalidated
[post_id]: 228642
[parent_id]: 
[tags]: 
Likelihood of Linear Discriminant Analysis compared to logistic regression

I've come across an interesting exercise. We are given four classification models for binary response and a $d$-dimensional independent variable: A Linear Discriminant Analysis model where the covariance of all classes is the identity matrix. A Quadratic Discriminant Analysis model with unconstrained covariance matrices. A logistic regression model. A logistic regression model with a polynomial basis function expansion of degree 2. We assume a uniform prior distribution over the two classes. After training, we inspect the log-likelihood obtained with each model $M$: $$ \frac{1}{n}\sum_{i=1}^n \mbox{log} p(y_i|\mathbf{x}_i, \boldsymbol{\hat\theta}, M) $$ Given certain pairs of models (e.g. 1 and 3) we are asked to determine if one of them will always perform better with respect to the log-likelihood on the training set and, in that case, which one. Specifically, the exercise asks about these pairs of models: 1 vs. 3, 2 vs. 4, 3 vs. 4 and 1 vs. 4. I'm not sure how to approach this. Intuitively I could say, for instance, that given that LDA makes more assumptions than logistic regression, we could say that 3 will always attain a higher optimum than 1 and 2, since it searches within a wider function space. However, another point of view is to count the number of free parameters of the model. In that case, model 1 would perform better than model 3, with, since the former has $2d$ parameters (the mean of each class) and the latter only has $d+1$, giving the optimizer more room to maximize in the case of 1. Perhaps I should consider a combination of the two: models 1 and 3 learn a linear function of the input, but model 1 has more free parameters. Models 2 and 4 learn a quadratic function, although the number of free parameters is in general larger in the case of model 2. What is the right way to approach this problem?
