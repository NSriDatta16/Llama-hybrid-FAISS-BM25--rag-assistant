[site]: crossvalidated
[post_id]: 506006
[parent_id]: 
[tags]: 
Measuring frequency of change in time series data with inconsistent time intervals?

Context I'm tracking auction house prices for items in Guild Wars 2 in a database. They provide an API that allows me to get the item's current buy/sell offer quantities and prices at that moment. As a result I have a time series of this data stored in my database at irregular intervals. Average & Standard Deviation Not Quite What I Need I started out using an average and standard deviation to determine which items to buy/sell. My problem is that neither of those include the time between quantities. I want to know how often the buy/sell quantities are changing and that they are in-fact changing, not necessarily by how much. How to Measure Frequency of Change in Aggregate? Ultimately, I want to be able to know if an item's supply/demand is constantly changing from 100 to 99 and potentially back to 100. If I have a bunch of quantities over time that are 100 and 99, that doesn't give me an impression of how often the data is changing at an aggregate-level. Is there some sort of formula, measure, or other solution that can help give me an impression of how often the quantities change at an individual item level? I don't care by how much it changes it could go from 99 to 1,000,000 for all I care, I just want to know that it's changing frequently, slowly, or not at all. Also, will the irregular time intervals matter?
