[site]: crossvalidated
[post_id]: 279745
[parent_id]: 279743
[tags]: 
Your neural network will learn to stay in the range of (0,1) after a while. But the best thing to do is to keep the sigmoid function on just the output layer, and the ReLU on the rest of the layers. That way you will never have an output that is out of bound.
