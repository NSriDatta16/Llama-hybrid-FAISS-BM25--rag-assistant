[site]: datascience
[post_id]: 1194
[parent_id]: 1191
[tags]: 
We are using Singular Value Decomposition in the much same manner as you, except rather than clustering similar items, we are using a reduced-rank matrix to power a recommendation engine based on a term-document matrix in Latent Semantic Indexing. From your brief description, your approach seems sound enough. However, I highly recommend reading Berry, Dumais & O'Brien's Using Linear Algebra for Intelligent Information Retrieval . Key to using SVD is selecting an acceptable rank-k approximation to the original sparse matrix. You should carry out some exploratory analysis to see how much variance can be explained, using the singular values in the diagonal matrix Sigma. This question was brought up in this question on CrossValidated . A lot of the papers I've read suggest anywhere a rank k from 200 to 300 singular values. In a proof-of-concept implementation, we had original sparse matrix of about 10000 rows (unique terms) to about 1000 columns (unique documents), and we were capturing just under 85% of the variance with only 300 singular values. However, that really hinges upon the nature of your data, so your mileage may vary.
