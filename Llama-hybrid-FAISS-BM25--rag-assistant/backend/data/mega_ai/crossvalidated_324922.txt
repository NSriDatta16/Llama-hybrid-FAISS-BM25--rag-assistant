[site]: crossvalidated
[post_id]: 324922
[parent_id]: 324914
[tags]: 
1) It's worth it to use dropout if you have a lot of neurons which are fully connected. So even if you have 2 layers, if you also have say, 100 neurons in each layer, then it's worth a try. Even with a single layer, it's still worth a try since it's a perfectly reasonable form of regularization. 2) Dropout zeros out neurons and has nothing to do with the loss function, which is based on the final output of the network. Instead, you should only be concerned on evaluating the network on your test or validation set, where you need to weigh each neuron's contribution with the dropout probability. This is generally automatically done in most neural network packages and implementations. Otherwise on the training set, the output after a single step of dropout is determined based on which neurons were kept during that step along with which batch of data was used.
