[site]: crossvalidated
[post_id]: 329104
[parent_id]: 
[tags]: 
Indexing the formula for the ith layer of a linear deep neural network

In a paper about deep neural networks they say: In a general feedforward linear network described by an underlying directed acyclic graph, units can be organized into layers using the shortest path from the input units to the unit under consideration. The activity in unit $i$ of layer $h$ can be expressed as: $$S^h_i(I)=\sum_{l I'm a little confused by the upper indexing on $w$. What would it mean if $w$ had an upper index $(h,h-2)$, for instance?
