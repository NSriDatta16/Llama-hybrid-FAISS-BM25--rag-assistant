[site]: datascience
[post_id]: 121657
[parent_id]: 
[tags]: 
Validation Loss not decreasing for RESNet model

I have been trying to train a Resnet model to classify Diabetic Retinopathy images into binary classes. The dataset consists of around 35k images. The val loss and accuracy does seem to behave weirdly for me. Here's my model: input_layer = Input(shape = (224,224,3)) base_model = ResNet50(include_top = False, input_tensor = input_layer, weights = "imagenet") x = GlobalAveragePooling2D()(base_model.output) x = Dropout(0.5)(x) x = Dense(32, activation = "relu")(x) x = Dropout(0.3)(x) x = Dense(32, activation = "relu")(x) # x = Dropout(0.15)(x) out = Dense(2, activation = 'softmax')(x) model = Model(inputs = input_layer, outputs = out) optimizer = keras.optimizers.Adam(learning_rate = 1e-4) es = EarlyStopping(monitor='val_loss', mode='min', patience = 5, restore_best_weights=True) rlrop = ReduceLROnPlateau(monitor='val_loss', mode='min', patience = 3, factor = 0.5, min_lr=1e-6) callback_list = [es] model.compile(optimizer = optimizer, loss = "binary_crossentropy", metrics = ["accuracy"]) history = model.fit(train_batches,epochs = 30, validation_data = val_batches, callbacks = callback_list) Here's the model summary: conv5_block3_out (Activation) (None, 7, 7, 2048) 0 ['conv5_block3_add[0][0]'] global_average_pooling2d (Glob (None, 2048) 0 ['conv5_block3_out[0][0]'] alAveragePooling2D) dropout (Dropout) (None, 2048) 0 ['global_average_pooling2d[0][0]' ] dense (Dense) (None, 32) 65568 ['dropout[0][0]'] dropout_1 (Dropout) (None, 32) 0 ['dense[0][0]'] dense_1 (Dense) (None, 32) 1056 ['dropout_1[0][0]'] dense_2 (Dense) (None, 2) 66 ['dense_1[0][0]'] ================================================================================================== Total params: 23,654,402 Trainable params: 23,601,282 Non-trainable params: 53,120 __________________________________________________________________________________________________ I did try changing the units for my dense layers(tried till 512).Here's the loss and accuracy plot(the less epochs are due to early stopping): What could I specifically do to have the val loss not spike?
