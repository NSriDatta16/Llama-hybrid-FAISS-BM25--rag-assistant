[site]: datascience
[post_id]: 72308
[parent_id]: 
[tags]: 
Multi-feature padding for LSTM

I am trying to train a LSTM on an NER dataset which contains multiple features. But I'm having trouble understanding how to pad multiple features. The dataset contains the following 3 features per row: word , POS , shape . Where POS is part-of-speech tagging and shape is categorical numerical , lowercase , capitalized etc. So for the LSTM my timesteps need to be the same. In this case its the length of the sentence. The words I can just pad. Either by just adding _PAD_ or something, or in the case of distributed embeddings use the min/max embedding technique. But I don't know how to pad the additional (categorical) features. How do I pad the features POS and shape ? Do I just add another category and use that for padding? If yes: in the case of embedded paddings, using the min/max embedding, would the additional padding category for the POS feature not train the LSTM to learn the min/max embedding on the padded POS? An example notebook of the dataset using LSTM can be found here . Which pads the words but removes all the other features. How can I expand on this notebook, by adding POS as a feature and use padding? Should I use one-hot encoding for the categorical values and just use a vector with zero's? If yes, how does one make a vector of zero's in python as one-hot encoding.
