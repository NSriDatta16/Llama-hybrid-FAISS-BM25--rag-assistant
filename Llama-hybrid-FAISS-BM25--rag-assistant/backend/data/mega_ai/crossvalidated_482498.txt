[site]: crossvalidated
[post_id]: 482498
[parent_id]: 482445
[tags]: 
Yes, but only because double machine learning uses a doubly robust estimator underneath the hood. There is nothing about the cross-fitting procedure itself that would lead to the double robustness property. Doubly Robust Estimators As noted, doubly robust estimators allow you to have two chances to specify the nuisance models correctly (nuisance models are the treatment and outcome models). However, there is no reason to think you can do this with a parametric model with only two chances. This is where machine learning comes in. Machine learning is used to weaken the assumption regarding model specification by using semiparametric or nonparametric estimators instead. However, these more flexible nuisance functions have slower rates of convergence than $n^{-1/2}$ . Therefore, approaches like inverse probability weights or do-calculus will have anti-conservative variances (with the potential for bias as well). Doubly robust estimators become necessary since they allow for slower rates of convergence. Cross-fitting Despite this, doubly robusts estimators have required a restriction on the nuisance function estimators. Specifically, that they are Donsker class (i.e. the estimators cannot be overly complex). Double machine learning targets this issue. The cross-fitting procedure allows for the use of non-Donsker class estimators for the nuisance functions. Conclusion The double/debiased machine learning described in Chernozhukov et al. 2016 relies on a doubly robust estimator (e.g. in the context for the average treatment effect it uses augmented inverse probability weights). Therefore, the approach will be doubly robust. However, the double machine learning procedure is meant to solve a specific issue for doubly robust estimators with machine learning for the nuisance functions. Cross-fitting with machine learning (double machine learning) should be used jointly with doubly robust estimators. Pulling from the Discussion section of my paper on arXiv ( Zivich and Breskin 2020 ): The need for doubly-robust estimators with cross-fitting when using data-adaptive machine learning for nuisance function estimation arises from two terms in the Von Mises expansion of the estimator. The first term, which is described by an empirical process term in the expansion, can be controlled by either restricting the complexity of the nuisance models (e.g., by requiring them to be in the Donsker class) or through cross-fitting. Because it can be difficult or impossible to verify that a given machine learning method is in the Donsker class, cross-fitting provides a simple and attractive alternative. The second term is the second-order remainder, and it converges to zero as the sample size increases. For valid inference, it is desirable for this remainder term to converge as a function of $n^{−1/2}$ , referred to as as root-n convergence. Convergence rates are not a computational issue, but rather a feature of the estimator itself. Unfortunately, data-adaptive algorithms often have slower convergence rates as a result of their flexibility. However, because the second-order remainder term of doubly-robust estimators is the product of the approximation errors of the treatment and outcome nuisance models, doubly-robust estimators only require that the product of the convergence rates for nuisance models be $n^{−1/2}$ . To summarize, cross-fitting permits the use of highly complex nuisance models, while doubly-robust estimators permit the use of slowly converging nuisance models. Used together, these approaches allow one to use a wide class of data-adaptive machine learning methods to estimate causal effects.
