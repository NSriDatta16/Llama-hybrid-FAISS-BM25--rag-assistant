[site]: datascience
[post_id]: 55738
[parent_id]: 54748
[tags]: 
Multiclass models in XGBoost consist of n_classes separate forests, one for each one-vs-rest binary problem. At each iteration, an extra tree is added to each forest. But it isn't actually a one-vs-rest approach (as I thought in the first version of this answer), because these trees are built to minimize a single loss function, the cross-entropy of the softmax probabilities. https://discuss.xgboost.ai/t/multiclassification-training-process/29 https://github.com/dmlc/xgboost/issues/806 https://github.com/dmlc/xgboost/issues/3655 In general, the one-vs-rest models are very good at identifying the single class, whereas the multiclass model has to balance performance on all of them. More specifically, I think that the softmax may be responsible for the phenomenon you're displaying. (I'm still thinking about it, but I thought I should post the above for now.) Suppose one of your documents is reasonably likely to be in either of two topics: the probability scores given by the forests are 0.9, 0.85, then More extreme, suppose the individual topic model scores are all 0.9. Now the multiclass ensemble applies softmax and produces equal 1/17 probabilities for each topic! In the other direction, suppose one of your documents is judged unlikely to fit any of the topics: all the individual topic model probability scores are 0.01. In the multiclass ensemble, that gets scaled up to 1/17 (OK, 17 topics makes this a harder sell). Hrm, except how likely is it to get the 0.9 and 0.85, since a training sample in one of these two topics will be pushed toward 0 by the other model... ? Especially when your scores are fairly high, so it's not like the models have huge blind spots. (This part still causes a problem with the correct understanding of how XGBoost works; the log-loss of the softmax probabilities still penalizes being confident about belonging to two different classes...)
