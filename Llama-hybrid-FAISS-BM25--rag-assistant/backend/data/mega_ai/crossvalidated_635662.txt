[site]: crossvalidated
[post_id]: 635662
[parent_id]: 
[tags]: 
Backpropagation in LSTM network

as we have Vanishing Gradient in Vanilla RNN and LSTM is the solution , according to some sources LSTM has Vanishing Gradient too but it doesnt cause any problem in the context of LSTM network cause we reach out to the forget gate during backpropagation and it doesn't matter if we have Vanishing Gradient My question is How do we actually reach Out to this equation in LSTM backpropagation? do we have 2 ways to make backpropagation ? ( if we assume the W as the concatenated weights )
