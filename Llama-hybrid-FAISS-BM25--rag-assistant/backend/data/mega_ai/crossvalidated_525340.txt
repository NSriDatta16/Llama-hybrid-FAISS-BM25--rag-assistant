[site]: crossvalidated
[post_id]: 525340
[parent_id]: 
[tags]: 
XGBoost for Time-Series Forecasting - Issues with Stationarity Transformations

EDIT: I finished the project, which was my first contact point with forecasting, and created an app in R Shiny for predicting the daily Covid vaccinations in Germany. The app lets you play around a bit with forecasts made by tree-based models: https://ferdinandberr.shinyapps.io/VaccinationForecaster/ Please check it out and leave me some feedback, it would be highly appreciated! The original question of how to best deal with trends is still open for me, it sure seems like there are many raw approaches floating around but there is no consensus best practice.. Original Post: I'm trying to forecast daily Covid vaccinations in Germany, especially focussing on using tree-based ensemble methods. One issue with tree-based methods for forecasting is extrapolation, when the time-series is non-stationary and contains a trend (see for example https://towardsdatascience.com/xgboost-for-time-series-youre-gonna-need-a-bigger-boat-9d329efa6814 ). My time series at hand is clearly non-stationary and contains an upward trend: Training an XGBoost model and forecasting ahead many weeks, the result shows that the model did not capture the trend: In order to work around that problem, I want to remove the trend through statistical transformations and see how it affects the forecast accuracy and trend prediction. However, when I difference the data, build the model on that differenced data, forecast ahead and then back-transform the data, something goes wrong: I heavily assume there is a mistake with my back-transformation. I use the model time framework for modeling and predicting, and used the diff_vec/diff_inv_vec commands for (back-)transforming the data. I would be happy if you can point out where I went wrong! Here is the important part of code in R: ## Statistical Transformations ## original % mutate( dosen_differenz_zum_vortag = diff_vec(dosen_differenz_zum_vortag) ) ## TRAIN/TEST SPLIT ## splits % time_series_split(assess = "14 days", cumulative = TRUE) ## MODELING ## # Pre-Processing Recipe (Feature Engineering) recipe_spec % step_timeseries_signature(date) %>% step_rm(contains("am.pm"), contains("hour"), contains("minute"), contains("second"), contains("xts")) %>% step_fourier(date, period = 365, K = 5) %>% step_normalize(date_index.num) # XGBOOST model_spec_xgb % set_engine("xgboost") workflow_fit_xgb % add_model(model_spec_xgb) %>% add_recipe(recipe_spec %>% step_dummy(all_nominal()) %>% step_rm(date)) %>% fit(training(splits)) # Modeltime Table model_table % modeltime_calibrate(testing(splits)) calibration_table ## Re-fit & Forecast ## refit_table % modeltime_refit(data_vaccinations) forecast_vacc_table % modeltime_forecast(h = "15 weeks", actual_data = data_vaccinations) ## Back-Transform and Plot ## forecast_vacc_table %>% mutate(.value = diff_inv_vec(.value, initial_values = original$dosen_differenz_zum_vortag)) %>% plot_modeltime_forecast(.conf_interval_show = FALSE, .interactive = TRUE) Edit: I found one mistake in my code: When back-transforming the differencing, this operation was done over various models for the same forecasting timespan, as there were multiple models (with forecasts) in "forecast_vacc_table". That explains the huge gap between the last actual value in the data and the prediction for the first day in the future with XGBoost - there were multiple forecasts with downward trends in between, and the back-transformed vaccination value for day #1 of model #2 was calculated from day #N of model #1. However, even when correcting this issue, the XGBoost forecast still has the same downward trend. This seems to be due to a fundamental flaw in my approach with fitting the model to the differenced data, but I don't really understand this yet. Edit 2: I think I found problem #2. When using the built model WITHOUT refitting on the two weeks of test data for forecasting, the forecast looks somewhat reasonable: When refitting on the test data, the forecast looks like this: It seems like the last two weeks of vaccination data caused the model to adopt the crazy downward trend. My intuition would be, that the negative "acceleration" (by training on differenced data) in daily vaccinations in those two weeks influenced the model strongly this way. I'm not sure how to interpret this, it still seems like the approach with differencing the data is flawed here. I'm happy about any feedback on this! The question is cross-posted on Kaggle.com: https://www.kaggle.com/discussion/240844
