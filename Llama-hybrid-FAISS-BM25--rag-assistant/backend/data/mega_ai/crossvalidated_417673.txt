[site]: crossvalidated
[post_id]: 417673
[parent_id]: 416581
[tags]: 
The skip-gram model stores two vectors for each word $w$ : $v_w$ and $v'_w$ . The result of word2vec training are the $v_w$ vectors. If you stack vectors $v'_w$ , you get a projection matrix of a linear classifier. Note that the $v_w$ vectors get updated whenever word $w$ occurs in the training data. On the other hand, $v'$ get updated when they are selected randomly with negative sampling which makes the output distribution estimation less reliable than standard softmax and it might be also the reason why vectors $v_w$ are used as the embeddings.
