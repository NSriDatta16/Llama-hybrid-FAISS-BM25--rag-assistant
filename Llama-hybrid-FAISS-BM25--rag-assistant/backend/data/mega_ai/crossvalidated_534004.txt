[site]: crossvalidated
[post_id]: 534004
[parent_id]: 533833
[tags]: 
My goal is similar to Predict, to create a nomogram, which ideally I would like to adjust for an effect that is not in my dataset (large population level data, >10,000 observations, >500 events), but is well-established in literature. My previous conviction was that this will lead to a biased model as we do not know about the interactions of this variable with the others in the cohort... TL;DR Overall, your caution is well advised. Correlations among variable values, interactions between variables with respect to outcome, and time dependencies can get in your way. It might be possible to do accomplish what you wish, if the studies in question adequately represent the underlying population of interest (as seems to be the case with Predict). There are, however, many ways for things to go wrong. The general issues can be gleaned pretty well from the Wishart et al. paper you cite, together with predictor-correlation issues noted in the answer by @wrstks . What is to be gained by adding the "extra" variable? From Wishart et al.: The aim of this study was to incorporate the prognostic effect of HER2 status into Predict. Based on your understanding of the subject matter, is the "extra" variable expected to improve performance enough to make this undertaking worth the investment? Are the data sets providing the estimates for the "extra" variable comparable to yours? From Wishart et al: This model has been validated in case cohorts from British Columbia (Olivotto et al, 2005), the Netherlands (Mook et al, 2009) and the United Kingdom (Campbell et al, 2009). What about your study and the studies on the "extra" variable you wish to draw from? What are the correlations between the "extra" variable and those in your model? From the answer by @wrstks you would need to also consider published correlation values of that variable with your other variables... I'm not so worried about parsimony in a predictive model as @wrstks seems to be (unless you're overfitting, more predictors generally help), but correlations do need to be considered. If there are significant associations among outcome-associated variables you would at least need to make sure that the source models took them into account appropriately. For example, from Wishart et al.: We estimated the hazard ratio for HER2-positive disease compared with HER2-negative disease using a Cox proportional hazards model stratified by study and adjusted for size, grade and nodal status. Separate regression models were used for ER-positive and ER-negative cases. Even then you will at least end up with problems in terms of confidence intervals for your estimates. Correlated outcome-associated variables have correlated errors among their individual coefficient estimates, so it's not clear how to combine the variances of coefficient estimates for correlated variables among different models that treat each individually. That's one reason why later validation of the combined model is important. Are there interactions between the "extra" variable and those in your model in terms of relationships to outcome? That's really tricky and needs serious consideration. For example, the Predict model combined large-scale cancer registry data with much smaller but well controlled randomized trials on breast-cancer therapies to give a model incorporating both standard clinical variables and therapy, as outlined here . The registry data presumably covered the population of interest evenly, and the randomized trials evaluated the effects of therapy in a way that should have averaged out any confounding clinical variables. Thus even if there were interactions between therapy and other variables in terms of outcome, the combined Predict model would at least have averaged the predictions properly over the population of interest. Can you say that about combining your data set with the studies that evaluated the "extra" variable? It's possible that interactions with respect to outcome substantially affect the results of combining predictors, particularly in observational studies. In my work, for example, I have two very strong individual outcome predictors, not strongly correlated with each other but with an outcome-associated interaction such that if either one is a poor prognostic marker then the value of the other basically doesn't matter. Had their individual associations with outcome just been combined from separate studies, results would have been way off. In addition to interactions among clinical variables, you need to consider their interactions with time with respect to outcome. Quoting from Wishart et al.: the hazard ratio for HER2-positive disease decreases over time in women with ER-positive breast cancer, so the log hazard ratio was modelled to vary linearly with time. The effect of HER2 in women with ER-negative breast cancer is not time-dependent. Might similar considerations of outcome associations in terms of interactions among variables or between variables and time apply to your situation? If after that it still makes sense to do so, how do you combine hazard ratios among studies? Cox models describe relative hazards that act around a baseline hazard. So if you want to combine models to provide survival estimates over time as Predict does, you have to consider not just the relative hazards but also the prevalence of the "extra" predictor to match a reasonable baseline hazard. Quoting extensively from Wishart et al. to provide a detailed example: Predict was developed using a case cohort of breast cancer cases of unknown HER2 status, and so the underlying baseline hazard is representative of cases of average HER2 status. The HER2 hazard ratio estimates based on the BCAC data are for HER2-positive cases compared with HER2-negative cases, and so these were rescaled to give an average hazard ratio of unity using an estimated prevalence of HER2 of 9% in ER-positive cases and 25% in ER-negative cases. The [time-dependent] applied hazard ratios for ER-positive cases are shown in Table 3. A fixed hazard ratio of 0.93 for HER2–negative cases and 1.27 for HER2–positive cases compared with HER2 unknown cases was applied to the baseline hazard of the ER-negative model. How do you propose to validate the combined model? Unless you have a large data set that includes all the variables on which to validate your combined model it will be hard to convince anyone that the result is reliable. Quoting again from Wishart et al.: We used the same case cohort from British Columbia, Canada, which we used to validate the original version of Predict. The data set has previously been described (Olivotto et al, 2005), but, in brief, includes data from 1653 patients with information on HER2 status out of a total of 3140 patients with stage I or II invasive breast cancer diagnosed in British Columbia, Canada, from 1989 to 1993... Even then, I would worry about whether some bias had been introduced by omitting the nearly half of patients whose tumors lacked HER2 data. Was there some selection bias involved in terms of the decisions to look for HER2 that wouldn't let this validation set adequately represent the full population of interest?
