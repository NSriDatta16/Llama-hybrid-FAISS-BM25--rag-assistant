ified by the user before model building starts. The search at each step is usually done in a brute-force fashion, but a key aspect of MARS is that because of the nature of hinge functions, the search can be done quickly using a fast least-squares update technique. Brute-force search can be sped up by using a heuristic that reduces the number of parent terms considered at each step ("Fast MARS"). The backward pass The forward pass usually overfits the model. To build a model with better generalization ability, the backward pass prunes the model, deleting the least effective term at each step until it finds the best submodel. Model subsets are compared using the Generalized cross validation (GCV) criterion described below. The backward pass has an advantage over the forward pass: at any step it can choose any term to delete, whereas the forward pass at each step can only see the next pair of terms. The forward pass adds terms in pairs, but the backward pass typically discards one side of the pair and so terms are often not seen in pairs in the final model. A paired hinge can be seen in the equation for y ^ {\displaystyle {\widehat {y}}} in the first MARS example above; there are no complete pairs retained in the ozone example. Generalized cross validation The backward pass compares the performance of different models using Generalized Cross-Validation (GCV), a minor variant on the Akaike information criterion that approximates the leave-one-out cross-validation score in the special case where errors are Gaussian, or where the squared error loss function is used. GCV was introduced by Craven and Wahba and extended by Friedman for MARS; lower values of GCV indicate better models. The formula for the GCV is GCV = RSS / (N · (1 − (effective number of parameters) / N)2) where RSS is the residual sum-of-squares measured on the training data and N is the number of observations (the number of rows in the x matrix). The effective number of parameters is defined as (effective number of parameters) = (number of mars terms) + (penalty) · ((number of Mars terms) − 1 ) / 2 where penalty is typically 2 (giving results equivalent to the Akaike information criterion) but can be increased by the user if they so desire. Note that (number of Mars terms − 1 ) / 2 is the number of hinge-function knots, so the formula penalizes the addition of knots. Thus the GCV formula adjusts (i.e. increases) the training RSS to penalize more complex models. We penalize flexibility because models that are too flexible will model the specific realization of noise in the data instead of just the systematic structure of the data. Constraints One constraint has already been mentioned: the user can specify the maximum number of terms in the forward pass. A further constraint can be placed on the forward pass by specifying a maximum allowable degree of interaction. Typically only one or two degrees of interaction are allowed, but higher degrees can be used when the data warrants it. The maximum degree of interaction in the first MARS example above is one (i.e. no interactions or an additive model); in the ozone example it is two. Other constraints on the forward pass are possible. For example, the user can specify that interactions are allowed only for certain input variables. Such constraints could make sense because of knowledge of the process that generated the data. Pros and cons MARS models are simple to understand and interpret. MARS can handle both continuous and categorical data. MARS (like recursive partitioning) does automatic variable selection (meaning it includes important variables in the model and excludes unimportant ones). However, there can be some arbitrariness in the selection, especially when there are correlated predictors, and this can affect interpretability. Building MARS models often requires little or no data preparation. Code from the book Bayesian Methods for Nonlinear Classification and Regression for Bayesian MARS. Extensions and related co