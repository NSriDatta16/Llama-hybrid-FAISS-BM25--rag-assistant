[site]: crossvalidated
[post_id]: 367253
[parent_id]: 
[tags]: 
Neural Network Bias updating during BackProp

Can it make sense to say that when I update the weights in a positive way in a neural network also the bias is updated in a positive way and that therefore the trend of weight and bias for the activation of a certain neuron are proportional? In other words, let's imagine that I want to activate a neuron that defines a certain function for a feature that recognizes the importance of some values of an input x , such as x0 , x1 , x2 for a given example. Let's suppose that I give an example with very high values for x0 , x1 , x2 ; I can say that this neuron, after training, will be activated. Now let's imagine that I provide another input with a very high value of x0 but low value for x1 and x2. Will this neuron still be activated or will I not reach the activation value because of the bias? Or will another neuron be activated that recognizes the importance of x0? If this neuron is activated, should I deduce that its bias for activation is lower than that of the previous neuron ?
