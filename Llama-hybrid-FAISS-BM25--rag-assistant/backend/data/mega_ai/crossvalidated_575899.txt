[site]: crossvalidated
[post_id]: 575899
[parent_id]: 
[tags]: 
For small sample sizes, is jackknife superior at controlling Type-I error compared to bootstrap?

This question is motivated by the post here: Can bootstrap be seen as a "cure" for the small sample size? In the referenced post, we see that the bootstrap approach does not control type-1 error rate well (i.e. the rate of false positives is higher than the nominal 5%) and is not a cure for small sample size. I was really disappointed to learn this about the bootstrap as I always liked the non-parametric-nature of bootstrap. The natural question that came to mind is: Does jackknife (leave-one-out re-sampling) provide better type-1 error control compared to bootstrap in small samples? Perhaps the jackknife can be the cure for small sample size problems! To answer this, I explored the following two scenarios using simulations: Scenario I: Jackknife in the context of 1-sample t-test with n = 5 The simulation for this scenario can easily be implemented with a slight tweak to @Greg-Snow's response in the above linked post. For the sake completeness, I am providing the code below with some edits: # Comparison of jackknife and bootstrap with respect to Type-1 error control # Code modified from here: https://stats.stackexchange.com/questions/112147/can-bootstrap-be-seen-as-a-cure-for-the-small-sample-size set.seed(1254) simfun qt(0.975,n-1), z=abs(z) > qnorm(0.975), z2 = abs(t) > qnorm(0.975), b_boot = (0 quantile(mean_boot, 0.975)), t_jack = abs(tstat_jack) > qt(0.975,n-1), # jackknife test statistic (assume t-distribution) z_jack = abs(tstat_jack) > qnorm(0.975) # jackknife test statistic (assume z-distribution) ) } out t z z2 b_boot.2.5% t_jack z_jack 0.0473 0.0510 0.1186 0.1612 0.0473 0.1186 The above results demonstrate that jackknife method for computing the standard error is excellent at controlling type-1 error rate and performs better than the bootstrap (4.73% vs 16.1% false positive rate)! I believe similar results will be expected for two-sample t-tests and simple linear regression models (or ordinary least squares).Therefore, for the second scenario, I will explore the performance of jackknife in the context of small-sample logistic regression. Side note: I believe there is a mathematical explanation for the t-statistic and the jackknife statistic being identical in this case ( any references are highly appreciated ). Scenario II: Jackknife in the context of small sample logistic regression (n = 10,20,30,40,50) The logistic regression model considered is as follows: $logit(p(Y = 1)) = intercept + slope* X$ The primary question is: "what is the false positive rate of test-statistics using jackknife and bootstrap methods for variance estimation compared to the test statistic using model based variance (i.e. inverse of information matrix)?" The code used to answer the above question is provided below. # Comparing jackknife and bootstrap for Small-Sample Logistic Regression set.seed(1254) # n: sample size # intcpt: intercept for logistic regression # slope: slope for logistic regression (to study Type-1 error rate data are simulated assuming slope =0) # n_resample: number of times to perform bootstrap re-sampling simfun_logistic qt(0.975,n-2), # assume test statistic is T-distributed z2 = abs(t) > qnorm(0.975), # assume test statistic is Z-distributed(normal) b_boot = (0 quantile(slope_boot, 0.975)), # assume the jack-knife statistic is T-distributed t_jack = abs(tstat_jack) > qt(0.975,n-1), # assume the jackknife statistic is Z-distributed z_jack = abs(tstat_jack) > qnorm(0.975) # improper Z-stat ) } # Run the above function in parallel to cutdown on computation time (the code below took 2.6 hours on my machine with 12 cores library(parallel) library(paramtest) numcores The results for the above scenario can be computed as follows: library(tidyverse) results(power) %>% # note results() is a function inside the paramest package group_by(n.test) %>% summarise(across(c(t,z2,b_boot.2.5.,t_jack.x,z_jack.x),~mean(.x))) n.test t z2 b_boot.2.5. t_jack.x z_jack.x 1 10 0 0 0.075 0.0051 0.0061 2 20 0.0084 0.0214 0.0808 0.0038 0.0068 3 30 0.0265 0.0359 0.0724 0.0136 0.0184 4 40 0.0302 0.037 0.0631 0.0203 0.0249 5 50 0.0388 0.0444 0.0654 0.0263 0.031 As we can see from the simulations above, the model-based (t,z2) and jackknife test-statistic (t_jack, z_jack) are overly conservative (i.e. false positive rate much smaller than the nominal rate of 5%) and perform similarly. However, the bootstrap (b_boot.2.5) fails to control type-1 error rate for small samples (and the performance of bootstrap improves with increasing sample size). Overall, jackknife performs better if the objective is to ensure control of type-1 error rate in the small sample scenarios. The above results are quite surprising to me since bootstrap is often promulgated as a superior approach compared to jackknife (see here for an example: Bootstrap vs. jackknife ). However, based on the above two simulations, it appears to me that bootstrap inflates type-1 error (i.e. "underestimate" the standard error) while the jackknife overcontrols the type-1 error rate("overestimates" the standard error) of the maximum likelihood estimate. And therefore, if one's objective is to ensure control of false positives (type-1 error rate), jackknife seems to outperform the bootstrap approach. And so, the natural question is: Are the results of the above two simulation scenarios generalizable to other models (eg. survival/lifetime models, mixed/longitudinal models, etc...)? In general, does the jackknife (leave-one-out re-sampling) provide better type-1 error control compared to bootstrap in small samples? Any reference articles on these issues will be really appreciated!
