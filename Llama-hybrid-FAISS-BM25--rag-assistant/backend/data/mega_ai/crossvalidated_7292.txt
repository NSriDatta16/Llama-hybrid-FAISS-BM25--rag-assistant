[site]: crossvalidated
[post_id]: 7292
[parent_id]: 
[tags]: 
How do you choose a unit of analysis (level of aggregation) in a time series?

If you can measure a time series of observations at any level of precision in time, and your goal of the study is to identify a relationship between X and Y, is there any empirical justification for choosing a specific level of aggregation over another, or should the choice be simply taken based on theory and/or practical limitations? I have three sub-questions to this main one: Is any non-random variation in X or Y within a larger level sufficient reasoning to choose a smaller level of aggregation (where non-random is any temporal pattern of the observations)? Is any variation in the relationship between X and Y at a smaller level of aggregation sufficient reasoning to justify the smaller unit of analysis? If some variation is acceptable how does one decide how much variation is too much? Can people cite arguments they feel are compelling/well defined for one unit of analysis over another, either for empirical reasons or for theoretical reasons? I am well aware of the modifiable area unit problem in spatial analysis ( Openshaw 1984 ). I don't claim to be expert on the material, but all I am to think so far of it is that a smaller unit of analysis is always better, as one is less likely to commit an ecological fallacy ( Robinson 1950 ). If one has a directly pertinent reference or answer concerning aggregation geographical units I would appreciate that answer as well.
