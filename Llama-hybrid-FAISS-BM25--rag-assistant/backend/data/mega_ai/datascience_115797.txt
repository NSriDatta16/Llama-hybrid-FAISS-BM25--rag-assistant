[site]: datascience
[post_id]: 115797
[parent_id]: 
[tags]: 
Overfitting problem with small model

I have an encoder-decoder architecture where I have used top 3 layers of Swin Transformer and few convolutional layer. I tried different approach: i. Training the Transformer layers as well, on doing so model contains approximately 304,086*2(encoder + decoder) trainable parameters. ii. Freezing the transformer layers approx. 105 * 2 = 210 (encoder + decoder) total trainable parameters. This also shows I have very few layers of CNN. On both the approach the validation loss is higher than the training loss. The above depicted curve is for approach (i). I have 7K trainable data and have used 700 for validation. Also, used L2-Regularization but the results doesn't change.
