[site]: crossvalidated
[post_id]: 581482
[parent_id]: 
[tags]: 
Is Transformer multi-headed attention a form of ensemble?

In Transformer, is it correct to say that multi-headed attention is an ensemble learning and each head is learning a different capability by below? independent random weight initialization for each head and attending only to 512/8 dimension per head (where 8 is number of heads) If incorrect, please help correct what is incorrect? References Attention Is All You Need 3.2.2 Multi-Head Attention Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2. Transformers Explained Visually (Part 3): Multi-head Attention, deep dive What is different in each head of a multi-head attention mechanism?
