[site]: datascience
[post_id]: 30963
[parent_id]: 
[tags]: 
Binarized neural network

I am currently looking at a paper by Hubara et al on binarized neural network. I am stuck in understanding algorithm 2 of the paper. The algorithm uses shift-based (bit-shifting) AdaMax, where AdaMax is an extension of the Adam optimizer. In particular, they are using $$m_{t} = \beta_{1}.m_{t-1} + (1 - \beta_{1}).g_{t} $$ $$v_{t} = \max(\beta_{2}.v_{t-1}, |g_{t}|) $$ $$\theta_{t} = \theta_{t−1} − (\alpha\oslash(1−β_{1}^t)).(m_{t} \oslash v_{t}^{-1} ) $$ where $g_{t}$ is the gradient, $\theta_{t-1}$ is the previous parameter, $\alpha$, $\beta_{1}$, $\beta_{2}$ are learning rate, and the betas of Adam optimizer. They stated that $\oslash$ stands for both left and right bit shift. I know the left shift and the right shift on its own, but I am not sure how do we have both at the same time? Help will be much appreciated. Thank you.
