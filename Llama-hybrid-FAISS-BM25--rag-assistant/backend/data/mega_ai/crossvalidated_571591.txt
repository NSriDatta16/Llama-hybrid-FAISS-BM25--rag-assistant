[site]: crossvalidated
[post_id]: 571591
[parent_id]: 571584
[tags]: 
I haven't seen such a setup before, but it might make sense in some scenarios. Notice that if you make decisions based on exploratory data analysis, or adjust your feature engineering pipeline, you are in fact manually doing the similar kind of job machine learning algorithms would do. For example, you could manually extract some features from the textual data for a standard machine learning algorithm or use a deep learning algorithm that learns the features by itself from the "raw" data. If that is the case, even such trivial tasks as exploratory data analysis or feature engineering can "overfit" the data that you used, so you should validate them on external data. You can also check the talk by Cassie Kozyrkov from Google who makes similar remarks . So if there is a risk that decisions made during features engineering would affect the downstream tasks, it might be wise to validate them independently. Also, keep in mind that with target encoding you are leaking information about the labels to the features, so it is a rather specific scenario where more precautions may be needed. With most of the trivial features engineering (take the logarithm of this feature, one-hot encode this, multiply those two columns, etc) I can't see how this would be useful.
