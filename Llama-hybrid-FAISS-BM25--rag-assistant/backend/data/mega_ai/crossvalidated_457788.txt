[site]: crossvalidated
[post_id]: 457788
[parent_id]: 120080
[tags]: 
The general answer is that auto-associative neural networks can perform non-linear dimensionality reduction. Training the network is generally not as fast as PCA, so the trade-off is computational resources vs. expressive power. However, there was a confusion in the details, which is a common misconception. It is true that auto-associate networks with linear activation functions agree with PCA, regardless of the number of hidden layers. However, if there is only 1 hidden layer (input-hidden-output), the optimal auto-associative network still agrees with PCA, even with non-linear activation functions. For the original proof see the 1988 paper by Bourlard and Kamp . Chris Bishop's book has a nice summary of the situation, in Ch.12.4.2: It might be thought that the limitations of a linear dimensionality reduction could be overcome by using nonlinear (sigmoidal) activation functions for the hidden units in the network in Figure 12.18. However, even with nonlinear hidden units, the minimum error solution is again given by the projection onto the principal component subspace (Bourlard and Kamp, 1988). There is therefore no advantage in using two-layer neural networks to perform dimensionality reduction.
