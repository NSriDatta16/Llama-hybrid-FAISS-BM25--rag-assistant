[site]: crossvalidated
[post_id]: 110296
[parent_id]: 110287
[tags]: 
Keep in mind that in Bayesian inference, you are looking at the entire range of values in the posterior, rather than point-estimate summaries. After convergence, MCMC samples are samples drawn from that posterior density. Using just the latest sample is the same as picking a random point from the posterior and using that as your sample estimate. For an intuitive explanation of why this isn't great, imagine that your posterior is a standard normal distribution. The normal distribution has support over the whole real line, so drawing a random sample from the standard normal distribution could be any real number -- though, it will likely be within 2 units of zero. But if you're using MCMC, you likely don't know the distribution of the posterior, so the only information you have is just some value. You have no way to adjudge whether this value is "large" or "small" or "typical." You would have more information if you had, say, 1000 draws from that posterior. You would know where the high- and low-density regions are. This is why people keep all of their posterior draws. The fact that some individual parameter estimates have very large variance does not mean that the chains haven't converged -- it just means that there is a large amount of uncertainty about the posterior values of the parameter in the model given the data and the prior. This isn't necessarily bad -- some quantities are not precisely estimated. But perhaps your priors are too diffuse, or your model is not fitting the data well. The key part from the definition of $\hat{R}$ is that this metric is a ratio of variance: we have evidence that the chain has converged to its stationary distribution because the ratio of variance in independent chains is roughly 1. The ratio can be 1 even if the variance is large in absolute terms.
