[site]: crossvalidated
[post_id]: 70156
[parent_id]: 70150
[tags]: 
Sometimes the things you see as "data torture" aren't really. It's not always clear beforehand exactly what you're going to do with data to give what you believe are the genuine results of the experiment until you see it. For example, with reaction time data for a decision task, you often want to reject times that aren't about the decision (i.e., when they are going so fast they are obviously just guessing and not making a decision). You can plot accuracy of the decision against RT to see where the guessing is generally occurring. But until you've tested that particular paradigm you have no way of knowing where the cutoffs are (in time, not accuracy). To some observers such a procedure looks like torturing the data but as long as it doesn't have anything directly to do with the hypothesis tests (you're not adjusting it based on tests) then it's not torturing the data. Data snooping during an experiment is ok as long as it's done the right way. It's probably unethical to stick your experiment in a black box and only do the analysis when the planned number of subjects have been run. Sometimes it's hard to tell that there are issues with the experiment until you look at data and you should look at some as soon as possible. Data peeking is strongly disparaged because it's equated to seeing if p Say you want to make sure that your variance estimate is within a known likely range. Small samples can have pretty far out variance estimates so you collect extra data until you know the sample is more representative. In the following simulation I expect the variance in each condition to be 1. I'm going to do something really crazy and sample each group independently for 10 samples and then add subjects until variance is close to 1. Y 1.1) y1 1.1) y2 So, I've just gone bonkers with the sampling and making my variances close to expected and I still don't affect alpha much (it's a little under 0.05). A few more constraints like the N's must be equal in each group and can't be more than 30 and alpha is pretty much right on 0.05. But what about SE? What if I instead tried to make the SE a given value? That's actually a really interesting idea because I'm in turn setting the width of CI in advance (but not the location). se 0.2 | se(y2) > 0.2) { y1 Again, alpha changed a small amount even though I've allowed N's to roam up to 46 from the original 10 based on data snooping. More importantly, the SE's all fall in a narrow range in each of the experiments. It's easy to make a small alpha adjustment to fix that if it's a concern. The point is that some data snooping does little to no harm and can even bring benefits. (BTW, what I'm showing isn't some magic bullet. You don't actually reduce the number of subjects in the long run doing this because power for the varying N's simulation is about the same as for a simulation of the average N's) None of the above contradicts the recent literature on adding subjects after an experiment started. In those studies they looked at simulations where you added subjects after doing a hypothesis test in order to get the p-value lower. That's still bad and can extraordinarily inflate alpha. Furthermore, I really like January and Peter Flom's answers. I just wanted to point out that looking at data while you're collecting it, and even changing a planned N while collecting, are not necessarily bad things.
