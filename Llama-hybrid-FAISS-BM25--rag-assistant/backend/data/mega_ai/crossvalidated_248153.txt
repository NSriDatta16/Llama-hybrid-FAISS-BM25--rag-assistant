[site]: crossvalidated
[post_id]: 248153
[parent_id]: 248146
[tags]: 
The expectation of a distribution isn't really fundamentally different from the expectated value of a variable. Sticking with the variables as they are defined in the example you linked to, you're interested in the likelihood over some vector of parameters $\theta$. The likelihood over this vector depends on the values of certain hidden variables $Z$, as well as some observed data $X$. The problem, which EM tries to solve, is that $\theta$ depends on $Z$, and $Z$ depends on $\theta$. To solve this chicken-and-egg problem, EM does the following: Initialize $\theta$ to some arbitrary value $\theta^{(t)}$ Define the probability distribution over possible states of $Z$ given the current parameter setting, i.e. $p(Z|X;\theta^{(t)})$ Compute the expectation over the log-likelihood function over $\theta$ under $p(Z|X;\theta^{(t)})$ Update $\theta^{(t)}$ to the most likely value under this expectation (the maximization step) Iterate steps 2-4 until conversion The step you're asking about it step 3, and you can think of this as computing the "average log-likelihood" given all possible states of the hidden variables $Z$. That is, given each possible state of $Z$, you figure out the log-likelihood function over $\theta$, you multiply this log-likelihood function by the probability of $Z$ (given the data and your current setting of $\theta^{(t)}$, and then you sum over all these log-likelihood functions weighted by their probabilities. So in short, just as you can think of an expected value as the "average value" of a random variable that you get when summing or integrating over a distribution over that variable, you can think of an expectation of a distribution as the "average shape" of that distribution when you sum or integrate over the distributions over other variables that the target distribution depends on.
