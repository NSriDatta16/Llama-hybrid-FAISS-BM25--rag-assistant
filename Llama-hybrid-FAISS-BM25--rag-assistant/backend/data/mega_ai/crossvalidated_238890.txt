[site]: crossvalidated
[post_id]: 238890
[parent_id]: 238882
[tags]: 
I am wondering why we need to assume an underlying distribution to tell whether two sets of paired values are significantly different or not. You don't. At least not always. Some tests assume a particular distribution (for say the pair-differences). Others do not, or make relatively weak assumptions. Some commonly used tests include the paired t-test you mentioned, the Wilcoxon signed rank test (which assumes symmetry of differences under the null hypothesis -- this will be true (for example) when the distributions of the original pairs have the same shape and spread (at least while the null is true); the test then checks to see if those pair differences are consistent with 0 location difference for the populations) the sign test -- which tests whether $P(X_i>Y_i)$ is $\frac12$. But these are not the only possible tests. Let's say you like the one-sample t-statistic (you want to compare means too) -- but you don't want to assume a normal distribution for the pair-differences. No problem. You can do a permutation test . If you can assume that when there's no difference in means the two sets of values have the same distribution, for example, then when the null hypothesis is true the label of which pair-member was which doesn't carry any information about the distribution - you could swap the labels without changing the distribution of the difference. So we can see whether our sample's particular t-statistic is consistent with the labels having been assigned arbitrarily by reassigning the group labels within each pair ... and doing this over all possible rearrangements (each pair can be flipped or not flipped, so there are $2^n$ such possible rearrangements, each with its own paired t-statistic). If our original sample statistic just looks like a typical one of those t-statistics it suggests that the members of the pairs are similar --- consistent with the group-labels not telling you information about the distribution. This don't flip/flip is identical to either keeping the original sign of the pair difference or taking its negative, so most simply, to get the permutation distribution you take the set of absolute pair differences and allocate all the $2^n$ possible patterns of +/- signs to them and then compute your chosen test statistic each time to get a distribution under the null to compare your sample statistic with. [You could as easily use the difference in means as your test statistic rather than the paired t-statistic; indeed this is fairly common practice, but some people like to use the t-statistic itself.] The Wilcoxon signed rank test and the sign test are both permutation tests of this type, but they use different test statistics. The Wilcoxon signed rank test uses the ranked sizes of the pair-differences (smallest absolute pair difference is rank 1, second smallest is 2, etc) to give the +/- signs to and the sign test allocates the +/- signs themselves (essentially replacing all the absolute differences by 1 before giving them their signs). In large samples it's common not to compute all possible arrangements (which can be an astronomically large number) but to sample them with replacement. This is typically easier to do. There are other tests - such as bootstrap tests for example - that might be used instead. Why not just get the ratios of the two sets? You mean calculate $X_i/Y_i$? It would sometimes make sense (at least for some things, if they take strictly positive values) to use ratios instead of differences ... but how will you figure out whether those ratios are consistent with random variation about 1 or if they indicate a difference from 1 that's not explainable by random variation? i.e. what's your test statistic? The mean of the ratios? The geometric mean of the ratios? Something else? Once you have a test statistic, you need a way to compute its distribution under the null hypothesis. This will involve either making some distributional assumption (as is done with the t-test) or using something like a permutation test (as with the signed rank test and the sign test). We can look at all of these suggestions on a single set of data (here rounded to 2dp). I'm doing this in R but it can easily be done in almost anything. a b 1 29.21 28.26 2 29.91 28.72 3 30.23 29.11 4 29.27 28.70 5 29.58 29.29 6 31.35 28.84 7 27.21 27.48 8 28.55 27.45 9 28.94 28.63 10 28.34 28.30 t test: t.test(a-b) One Sample t-test data: a - b t = 3.1301, df = 9, p-value = 0.01212 alternative hypothesis: true mean is not equal to 0 95 percent confidence interval: 0.2164255 1.3445696 sample estimates: mean of x 0.7804975 Wilcoxon signed rank test: wilcox.test(a-b) Wilcoxon signed rank test data: a - b V = 53, p-value = 0.005859 alternative hypothesis: true location is not equal to 0 sign test: binom.test(sum(a>b),length(a)) Exact binomial test data: sum(a > b) and length(a) number of successes = 9, number of trials = 10, p-value = 0.02148 alternative hypothesis: true probability of success is not equal to 0.5 95 percent confidence interval: 0.5549839 0.9974714 sample estimates: probability of success 0.9 permutation test based on mean difference: # 1. sampling the permutations: md=replicate(10000,mean(sample(c(-1,1),10,replace=TRUE)*(a-b))) p.value=mean(abs(md)>=abs(mean(a-b))) p.value [1] 0.0057 # the sampling versions vary a little if you re-run them. # 2. doing all the permutations of signs. 1st get the 2^10 sets of signs: s=c(-1,1) signs=expand.grid(s1=s,s2=s,s3=s,s4=s,s5=s,s6=s,s7=s,s8=s,s9=s,s10=s) pmd=apply(t(signs)*(a-b),2,mean) #averages for (a-b)* 2^10 sets of signs p.value=mean(abs(pmd)>=abs(mean(a-b))) #p(at least as extreme as ours) p.value [1] 0.005859375 permutation test based on paired t-statistic: # sampling the permutations of sign ts=function(x) t.test(x)$statistic # a function to return the t statistic md=replicate(10000,ts(sample(c(-1,1),10,replace=TRUE)*(a-b))) p.value=mean(abs(md)>=abs(ts(a-b))) p.value [1] 0.006 # complete enumeration of permutations of signs pt=apply(t(signs)*(a-b),2,ts) #averages for (a-b)* 2^10 sets of signs p.value=mean(abs(pt)>=abs(ts(a-b))) p.value [1] 0.005859375 # the same as for mean of differences permutation test based on geometric mean of ratios: # only doing the sampling the permutations of sign version this time md=replicate(10000,mean(sample(c(-1,1),10,replace=TRUE)*(log(a/b)))) p.value=mean(abs(md)>=abs(mean(log(a/b)))) p.value [1] 0.0067 All tests were significant at the 5% level
