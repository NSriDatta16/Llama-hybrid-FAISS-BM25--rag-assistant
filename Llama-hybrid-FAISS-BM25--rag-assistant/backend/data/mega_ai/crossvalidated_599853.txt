[site]: crossvalidated
[post_id]: 599853
[parent_id]: 
[tags]: 
How can we compare biases of two estimators with no parametric form?

I was reading in my textbook that the bias of a statistical estimator $\hat{\theta}_n$ can be quantified as $B(\hat{\theta}_n,\theta)=E[\hat{\theta}_n-\theta]$ . This expectation seems to be w.r.t. to the sampling distribution of the estimator for samples of fixed size $n$ (please correct me if I'm mistaken). So I was wondering if we had two estimators $\hat{\theta}_n^1$ and $\hat{\theta}_n^2$ whose biases decrease with sample size $n$ , how would we compare them. For example, if the bias of $\hat{\theta}_n^1$ starts out smaller than that of $\hat{\theta}_n^2$ until $n=100$ , but after that reduces much slower than that of $\hat{\theta}_n^2$ , how can we compare the two? Can we say that $\hat{\theta}_n^1$ is more biased than $\hat{\theta}_n^2$ because $\hat{\theta}_n^1=O(\hat{\theta}_n^2)$ ? How does this apply to machine learning where we use generic statements such as "linear models have higher bias than neural networks"?
