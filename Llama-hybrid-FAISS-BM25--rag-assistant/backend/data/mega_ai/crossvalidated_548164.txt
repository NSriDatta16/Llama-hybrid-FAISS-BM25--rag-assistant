[site]: crossvalidated
[post_id]: 548164
[parent_id]: 548159
[tags]: 
The boruta algorithm is a feature selection method that is based on random forest. The reason you might care is that lasso is a linear model, so anything that matters for your outcome that isn't linear in the parameters under estimation is at risk of getting eliminated. An example: What are disadvantages of using the lasso for variable selection for regression? In other words, the disadvantage to lasso for feature selection of a nonlinear model is that the lasso assumes a more restrictive set of assumptions than the random forest. Additionally, one might wonder why you care about feature selection at all. Random forest has a kind of feature selection built in, in the sense that it selects the best features to split on when building trees (but this isn't foolproof). In the particular case you outline, where you have a small number of observations overall, a random forest is probably not the right tool. Overfitting is a very real risk. Regularized regression might be the best you can do.
