[site]: crossvalidated
[post_id]: 387388
[parent_id]: 162257
[tags]: 
Andrew Ng used the term "logistic regression" as a model for solving the binary classification problem. As you may saw in the paper he actually never draw the model itself. Let me add few details to the bucket so you may find the reasoning on how I think he constructed the lectures. The model used for the "logistic regression" is a single level perception with with custom number of inputs and one output ranging from 0 to 1. Back in 90's the most appreciated activation function was the sigmoidal activation function, and there is a great mathematical theory as a backup. This is exactly the model Andrew Ng is using since that function ranges from 0 to 1. Also the derivative s'(x) = s(x)(1âˆ’s(x)) , where s(x) is sigmoidal activation function. For the error function he uses L2, although in some papers he may use some other function for that. So to recap, when considering "logistic regression" just consider the single level perception with sigmoidal activation function, custom number of inputs and single output. Just a few notes: There is nothing wrong with the sigmoidal activation function, although for the floating point arithmetic, ReLU dominates hidden layers nowadays, but in the near future posits (or some other arithmetical units) may put sigmoidal activation function back to the table. Personalty, I would use simpler model with the ReLU function to explain the SLP (single level perceptron) since it is more used today.
