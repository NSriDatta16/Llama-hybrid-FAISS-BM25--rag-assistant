[site]: crossvalidated
[post_id]: 596284
[parent_id]: 
[tags]: 
Proof that omitting insignificant features in Linear Regression does not change the result

I am trying to understand whether omitting insignificant features (with weights close to zero) will change the overall result of a multiple linear regression problem and if so by how much. In particular, I would be interested in deriving an explicit error bound (if this is possible at all?). 1) To establish a common ground, here's my setup $\min || y - \hat{y} ||_2^2$ with $y = \begin{bmatrix} y_1 \\ y_2 \\ ... \\ y_N \end{bmatrix}$ and $\hat{y}_i = w_0 + w_1x_1 + ... + w_M x_M$ Means we have N samples of a setup with M feature variables In short, I write : $\min || y - Xw ||_2^2$ with $X = \begin{bmatrix} - x_{(1)} - \\ ... \\ - x_{(N)} - \end{bmatrix}$ being the NxM matrix of N observations of the respective M features The explicit solution via Least Squares is given by the pseudo-inverse : $w_{LS} = (X^TX)^{-1}X^{T}y = X^+ y$ 2) Now, intuition and performance metrics tell me that If $w_i = 0$ , then the corresponding feature $x_i$ is insignificant and can be dropped This is because including $x_i$ will not increase the fit, i.e. the sum of squares around the fit is not getting better (smaller) To determine which features might be insignificant, it makes sense to exclude highly correlated features beforehand. This can be done by recalling the assumption for linear regression that the target $y$ is being normally distributed with some variance $\sigma^2$ . It can be shown that the variance for the weight vector $w_{LS}$ is $\sigma^2 (X^TX)^{-1}$ . Of course, we opt for low variance and this can be analyzed using a SVD of the feature matrix $X = USV^T$ as follows. The variance is given by $Var[w_{LS}] = \sigma^2 (X^TX)^{-1} = \sigma^2VS^{-2}V^T$ . For highly correlated features, the values in $S$ will be small and thus the inverse will be very large due to the power of 2. We can use this analysis to determine correlated features and exclude them, consequently. Then, practically, we could compute the adjusted $R^2$ and $p$ values for the setup with and without the insignificant values and compare them. This would be a purely pragmatic approach though with a little prior analysis. 3) The real-world problem In reality, we will most likely never have perfect $w_i = 0$ From a purely theoretical point of view, can we use the LS solution to further derive an error bound on leaving out a feature per se? I am thinking about something like this: We have that $w_{LS} = (X^TX)^{-1}X^{T}y = X^+ y$ . Using the SVD for the pseudo-inverse, we could split this argument up again. Can we use the singular values in this exact scenario already to determine insignificant variables? And if we decide to omit say $x_i$ , whether being it significant or insignificant, can we derive an error metric, say based on the R^2 ? These are just some (maybe trivial) problems of a student :-) Any discussion is very much appreciated! Thanks!
