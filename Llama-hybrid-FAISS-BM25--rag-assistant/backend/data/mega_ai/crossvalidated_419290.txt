[site]: crossvalidated
[post_id]: 419290
[parent_id]: 419102
[tags]: 
Cross validation is a method for testing generalisation of model, not for calculating models themselves. It does this by holding out successive sets of samples, and in each fold it trains a model on the in-samples, tests on the out samples. The overall model is typically calculated from the entire training set rather than attempting to average across models. If you do not have randomisation in your dataset (which is rare if it has not been not explicitly done), then (since if shuffled the folds will pick in a pattern), meaning there will be a bias between folds depending on the organisation of the dataset and how the folds are selected. A real-life visual example of this selection bias is the Moir√© effect caused by using a pixelated camera detector to record a pixelated screen - if the two patterns do not exactly overlap then you get quite complex interference patterns between them. I suspect it was also the reason that old CGI movies optimised for viewing on large cinema screens didn't down-sample well for DVD (pseudo random shading has a structure). If there are any systematic effects in sub-sampling for folds then there will be biased differences in the data submitted to the model builder in each fold. The outcome is that each fold will be creating completely different models that are not comparable.This means their probabilities are related to different things completely, so it is not surprising that their performance drops when an attempt to combine them is made. Note that even a well randomised selection still presents different data to the model builder each time, so there will still be differences each time. This is why model scores on the training set are based on the overall model, not some aggregate of the folds. The value of each fold is in the performance metrics used to assess the generalsiability, but this is based on the assumption of unbiased folding and comparability of models between folds. As you data shows it is important to ensure that the folding is done in order to ensure the best continuity between folds.This means as the dataset becomes more complex (e.g. lost of groups or subgroups) then the randomisation needs to be more complex to ensure fair distribution of important groups between folds. Other issues include ensuring that any repeated measurement from a sample is always kept within the same fold (otherwise the CV performance metrics are contaminated by reproducicibility variations, which are better handled separately).
