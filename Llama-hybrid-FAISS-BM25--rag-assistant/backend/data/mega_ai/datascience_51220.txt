[site]: datascience
[post_id]: 51220
[parent_id]: 51215
[tags]: 
This is an interesting question to answer as there are multiple reasons why random forests work better than a decision tree. I'll compare how each of classifier/regressor work in each of the below cases So, We have a dataset with 5 features as you said. Let's consider our decision tree classifier is overfitted to that data. Since the model is overfitted, Any small change in data will cause a huge change in classification (Variance problem). But in RF, Since we are using multiple decision trees in a random forest, Any small change in data will not cause dramatic changes in classification as we take a majority vote of all the trees to take a decision. Hence reducing the overfitting (variance) problem. If you notice, We do not feed in the entire dataset at once in a random forest. We perform row sampling with replacement column sampling without replacement at every data feeding step and so your model will be able to generalize much better than a decision tree. Random forests are made up of Decision trees with large depth which has a lot of variance at the start and has reduced variance at the end of learning while. But decision trees you hyperparameter tune them, you don't fix their depths(i.e you don't say whether they are shallow or deep). Hope this helps!
