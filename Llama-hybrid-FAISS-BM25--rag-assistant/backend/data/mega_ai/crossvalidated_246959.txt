[site]: crossvalidated
[post_id]: 246959
[parent_id]: 246069
[tags]: 
For illustration, suppose that there are n elements($x_1$ to $x_i$) determining whether or not a man gets cancer($y=0$ or $y=1$). We have some data collected before as examples. For instance $X_j$ when $x_1 = 1, x_2 = 4.1$, .. y = 1 and $X_{j+1}$ when $x_1 = 0, x_2 = 1.2$, .. and etc then $y = 0$. We modify the linear regression a little bit. In linear regression, the output of $\sum_{i=0}^n \lambda_i * x_i$ is compared to the label y(using the least squared error), while in logistic regression we change the output to a probability using the logistic function $\frac{1}{1+e^X}$, $\frac{1}{1+e^{-(\lambda_0 + \sum_{i=1}^n\lambda_i x_i)}}$. What we go to optimize is the product of all the probabilities of all the examples(as we did for the normal distribution) if the examples are independent to each other by adjusting $\lambda_i$(as we did for $\mu$ and $\sigma$ in normal distribution). This probability is noted as the conditional probability $p(Y=1|X;\lambda)$(note that it is only for Y=1, and the $\lambda$ is the estimator vector $(\lambda_0, \lambda_1,..., \lambda_n)$ and the conditional likelihood is $l(\lambda; Y=1|X)$. We use the former after we have trained the estimator using the given examples(the probability may between 0 and 1, we can adjust all probability larger than 0.5 as 1 otherwise 0). So, just like that in linear regression, $w$ is the estimator(or say constraints or weights) to be trained and $x$ is the training data(in precise the independent variable).
