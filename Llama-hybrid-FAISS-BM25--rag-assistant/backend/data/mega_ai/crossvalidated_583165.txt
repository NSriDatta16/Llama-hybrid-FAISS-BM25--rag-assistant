[site]: crossvalidated
[post_id]: 583165
[parent_id]: 582226
[tags]: 
Your question is about the equality $E_{p(\mathbf x)}\left[\mathbf{x} \mathbf{x}^{T} \mathbf{W}^{T}\right]=E_{p(\mathbf x)}\left[\mathbf{x} \mathbf{x}^{T}\right] \mathbf{W}^{T}$ which is a special case of $E_{p(\mathbf x)}\left[\mathbf{A}(\mathbf x) \mathbf{B}\right]=E_{p(\mathbf x)}\left[\mathbf{A}(\mathbf x)\right] \mathbf{B}$ . Otherwise, how should I verify the equality mentioned in the paper? This equality is a fundamental mathematical result and the paper you mention doesn't need to provide a proof because the statement is seen as common knowledge. It follows directly from the linearity of the expectation and the linearity of matrix multiplication. The matrix multiplication is a weighted sum (per entry) and as such a linear operator. The linearity of the expectation says that even if you don't have a discrete distribution $p(\mathbf x)$ , where the expectation is just a weighted sum $E_{p(\mathbf x)} f(\mathbf x) = \sum_{\mathbf x} p(\mathbf x) f(\mathbf x)$ , the expectation still behaves like a weighted sum in the following respect. You can generally change the order in which you apply weighted sums (and more generally linear operators like expectations), without changing the result. In our case, you can first apply the multiplication with $\mathbf B$ and then the expectation or first apply the expectation and then the multiplication with $\mathbf B$ , without changing the result. If you want to proof the equality in detail, you could compare the $i,j$ -th entry of both sides of the equation, writing out what the matrix multiplication does to each entry. It'll be a weighted sum that can switch places with the expectation . I was expecting that $\mathbf{x} \mathbf{y}^T$ and $ﾏタ2\mathbf{W}^T$ have the same direction and size. Why this is not the case? This sounds as if you try to show equality between $\mathbf{x} \mathbf{x}^T\mathbf{W}^T$ and $ﾏタ2\mathbf{W}^T$ . But the actual equality is between $E_{p(\mathbf{x})}\left[\mathbf{x} \mathbf{x}^T\mathbf{W}^T\right]$ and $ﾏタ2\mathbf{W}^T$ . And I think this is what you meant to write. You can approximate the expectation as an average over sample points $\frac1N\sum_{\mathbf{x}\sim p(\mathbf{x})}\mathbf{x} \mathbf{x}^T\mathbf{W}^T$ and then see whether this average is close to $ﾏタ2\mathbf{W}^T$ . In fact with xy = np.matmul(x.T, y) you compute a sum over the sample dimension. The only issue is that you forgot to divide by n_sample : xy = np.matmul(x.T, y)/n_sample This change doesn't affect the angle but drastically decreases the distance norm. Note that an angle between two vectorized matrices may not be a well-motivated measure, but it should still return zero if the matrices are exactly equal to each other. Since random vectors in high-dimensional spaces are usually close to orthogonal, 75 degrees is not a bad sign. In high-dimensional spaces, it's also normal that you need more than n_sample = 50 sample points for a good approximation of the expectation. For n_sample = 1000 I get about 42. degrees and a distance of about 8. while n_sample = 100000 gives about 5. degrees and a distance of about 0.8 . The values go (on average) closer to zero the more sample points you use.
