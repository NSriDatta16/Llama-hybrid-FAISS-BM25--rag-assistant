[site]: crossvalidated
[post_id]: 285284
[parent_id]: 285278
[tags]: 
Random Forests shine in situations where there are many predictors, in particular if they are highly correlated. In addition, the speedup through the bagging (not all predictors are considered for potential splitting, only a randomly preselected small number of candidates) gets interesting if you have a large dataset. You have only two predictors. That's not many. In particular, the random choice of, e.g., mtry=5 candidate predictors (to use R's randomForest ) nomenclature is meaningless if you only have two predictors altogether. I don't think Random Forests will be helpful. Then again, fitting one involves just a few lines of code in R, so you could just take a look. One thing you might be able to do if you can't increase the number of predictors is to collect more observations. Perhaps there is a subtle interaction between your two predictors that only becomes visible with enough data. (I wouldn't be overly optimistic, though, judging from your description.) Sometimes, your machine learning problem simply is hopeless .
