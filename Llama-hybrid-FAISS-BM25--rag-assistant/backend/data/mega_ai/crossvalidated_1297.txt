[site]: crossvalidated
[post_id]: 1297
[parent_id]: 1292
[tags]: 
Here are a couple I can think of: They can be extremely sensitive to small perturbations in the data: a slight change can result in a drastically different tree. They can easily overfit. This can be negated by validation methods and pruning, but this is a grey area. They can have problems out-of-sample prediction (this is related to them being non-smooth). Some of these are related to the problem of multicollinearity : when two variables both explain the same thing, a decision tree will greedily choose the best one, whereas many other methods will use them both. Ensemble methods such as random forests can negate this to a certain extent, but you lose the ease of understanding. However the biggest problem, from my point of view at least, is the lack of a principled probabilistic framework. Many other methods have things like confidence intervals, posterior distributions etc., which give us some idea of how good a model is. A decision tree is ultimately an ad hoc heuristic, which can still be very useful (they are excellent for finding the sources of bugs in data processing), but there is the danger of people treating the output as "the" correct model (from my experience, this happens a lot in marketing).
