[site]: crossvalidated
[post_id]: 11343
[parent_id]: 11248
[tags]: 
I wouldn't use the means at all for the classifier. You don't need to apply "corrections" or to "smooth out" a Bayesian solution, it is the optimal one for the prior information and data that you have actually used. But the means can be useful for giving you a feel for which combinations of regressor variables are likely to lead to classifying towards a particular category. However this can be a horrendously complicated beast for multinomial regression, as you have a matrix of betas to interpret (one column for each category, except for the reference, which can be thought of as having all betas "estimated" as zero with zero standard error). Given that this seems to be an attempt at a intuitive way to understand what your classifier is doing, let me propose another. I will delete this section if this is not what you were intending. You have your MCMC samples of the beta matrix: call this $\beta_{ij}^{(b)}$ where $i=1,\dots,R$ denotes the multinomial category, $j=1,\dots,p$ denotes the regressor variable (the $X$), and $b=1,\dots,B$ denotes the $bth$ MCMC sampled value. If the categories have different $X$ variables, then simply set those excluded variables' betas to zero in the matrix: $\beta_{ik}^{(b)}=0$ for all $b$ if variable $k$ was not part of the model fit to the ith category, and $\beta_{Rj}^{(b)}=0$ for all $j$ and $b$. The first thing you need is a set of covariates to use $X_{mj}\;\;\;\;m=1,\dots,M$, where $m$ is the "observation number" and $M$ is the number of predictions you are going to make. The data used to fit your model should do for this purpose, so $M=\text{sample size}$. You now calculate the linear predictor for each category for each prediction for each MCMC sample: $$y_{im}^{(b)}=\sum_{j=1}^{p}X_{mj}\beta_{ij}^{(b)}$$ (this may be quicker to code up as a matrix/array operation). Note that $y_{Rm}^{(b)}=0$ for all $b$ and $m$. Then convert this into a probability for the $mth$ observation belonging to the $ith$ category/class, call this quantity $Classify(m,i)$. $$Classify(m,i)=\frac{1}{B}\sum_{b=1}^{B}\frac{\exp\left(y_{im}^{(b)}\right)}{\sum_{l=1}^{R}\exp\left(y_{lm}^{(b)}\right)}$$ Now you plot the value of $Classify(i,m)$ against $X_{mj}$, so you will have a total of $R\times p$ plots. Looking at these should give you a feel for what the classifier is doing in relation to the regressor variables. Note that when it comes to actually classifying a new variable, you only need $Classify(i,m)$ in order to do this - all other quantities from the MCMC are irrelevant for the purpose of classification. What you do need though is a loss matrix which describes the loss incurred from classifying into category $i_{est}$ when the true category is actually $i_{true}$, this will be a $R\times R$ matrix, usually zero on the diagonal and positive everywhere else. This can be very important if correctly identifying "rare" classes is crucial compared to correctly identifying "common" classes.
