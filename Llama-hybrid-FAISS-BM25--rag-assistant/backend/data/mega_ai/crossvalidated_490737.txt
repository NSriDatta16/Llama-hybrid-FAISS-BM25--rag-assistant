[site]: crossvalidated
[post_id]: 490737
[parent_id]: 490725
[tags]: 
Step A doesn't provide a single model; it provides a set of models, one for each value of $\lambda$ , developed on all of x[train,] and y[train] . There is no single model in Step B even for a single value of $\lambda$ . At the default 10-fold cross validation in cv.glmnet , you develop 10 different models for each value of $\lambda$ . Here's what's going on "under the hood." Cross validation tries to estimate what would happen if you repeated a modeling process on different samples from a population. For each fold of 10-fold CV, you build a model on 90% of the cases as an internal "training" set, then evaluate performance on the remaining 10% as an internal "test" set. After all 10 folds, each case has been included in one internal "test" set and 9 internal "training" sets. The performance (here, mean-square error on the 10 internal "test" sets) is averaged over all those 10 models developed at that $\lambda$ value. Those 10 lasso models at a single $\lambda$ value may differ not only in regression coefficients but even in the predictors that are selected for inclusion. That's OK. The point of evaluating over the range of $\lambda$ penalty values, as @chl notes in a comment, is to find an optimal penalty value that best balances off the bias and variance, to minimize the expected mean-square error when you apply your modeling process to the underlying population. That's a key concept to recognize. In many circumstances it's the modeling process that you're evaluating, not the model itself. Then you go back to your original set of models developed in Step A, over a grid of $\lambda$ values, and select the model developed at that optimal value of $\lambda$ . Yes, the details of that model will differ from all of the 10 models developed at that value of $\lambda$ during cross validation. But that choice of $\lambda$ means that the resulting model still should have the best expected performance when applied to new cases from the population.
