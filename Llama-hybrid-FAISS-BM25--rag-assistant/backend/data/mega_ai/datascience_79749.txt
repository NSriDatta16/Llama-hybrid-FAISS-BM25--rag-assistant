[site]: datascience
[post_id]: 79749
[parent_id]: 79737
[tags]: 
The architecture of a RNN is called recurrent because it applies the same function at each step. So all the cells on the graph actually represent the same computation , but not the same state . Each green square in your figure represent the computation. $$ s^{(t)} = f(s^{(t-1)}, x^{(t)}, \theta) $$ Where $f$ is the function of the RNN, $\theta$ are parameters, $s^{(t)}$ is the RNN state at step $t$ and $x^{(t)}$ is the input at step $t$ in the input sequence. What you see represented in the figure is actually what is called the unfolded representation, that is the same RNN cell applied to the input sequence one input vector at a time. I recommend you to read the chapter 10 on RNN Deep Learning book. The following figure is from this book and summarize the idea.
