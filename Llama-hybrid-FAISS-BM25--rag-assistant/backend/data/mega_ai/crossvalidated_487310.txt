[site]: crossvalidated
[post_id]: 487310
[parent_id]: 
[tags]: 
Composite priors in bayesian linear regression?

I'm not certain that "composite" is the right word for this; I've seen blogs tutorials and books that seem to link prior beliefs together. Consider MTCARS data, where miles per gallon (mpg) has a linear relationship with engine displacement, depicted by the equation m = B*x +a where m represents mpg, B the slope, x the input variable displacement, and a (or B0) the y-intercept. I'm following this blog by Jesse Fagan, Suppose that: mpg ~Normal(mu=m,sigma=s) B ~Normal(mu=0,sigma=2) a ~Normal(mu=18,sigma=3) s ~Unif(start=0,stop=20) I'm not an R programmer, however, I'm quite familiar with python. Due to the R code, I'm not certain what's happening in each step. Big picture, I understand that this code accomplishes a grid search. However, I'd like to know explicitly which PDFs are priors and which are likelihoods. It's my understanding that mpg is the likelihood whereas B, a, and s are the priors. Likewise, I understand the "composite prior" to use my vocabulary, to be the product of B,s, and a. Again, due to the R code, I'm not 100% confident in this belief and would like affirmation or correction. Thank you!
