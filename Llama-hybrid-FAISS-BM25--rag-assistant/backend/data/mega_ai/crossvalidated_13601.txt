[site]: crossvalidated
[post_id]: 13601
[parent_id]: 13597
[tags]: 
This may be more in bagging spirit, but nevertheless: If you really have a strong learner, there is no need to improve it by any ensemble stuff. I would say... irrelevant. In blending and bagging trivially, in boosting making a too strong classifier may lead to some breaches in convergence (i.e. a lucky prediction may make the next iteration to predict pure noise and thus decrease performance), but this is usually repaired in proceeding iterations. Again, this is not the real problem. The very core of those methods is to force the partial classifiers to look deeper in the problem. join their predictions to attenuate the noise and amplify the signal. 1) needs some attention in boosting (i.e. good boosting scheme, well behaving partial learner -- but this is mostly to be judged by experiments on the whole boost), 2) in bagging and blending (mostly how to ensure lack of correlation between learners and do not overnoise the ensemble). As long as this is OK, the accuracy of partial classifier is a third order problem.
