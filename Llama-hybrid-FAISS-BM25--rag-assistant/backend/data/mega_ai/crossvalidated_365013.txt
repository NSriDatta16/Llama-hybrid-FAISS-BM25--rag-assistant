[site]: crossvalidated
[post_id]: 365013
[parent_id]: 364998
[tags]: 
The terminology 'repeated measures' has come to be used to mean so many different things in various fields of application that I prefer different terminology. Let's look at fundamental differences between a paired t design and analysis and a two-sample t (independent samples) design and analysis. The quick answer is that the paired design controls variability. Two independent samples. Suppose $n = 25$ observations are randomly sampled from each of two normal populations, with distributions $\mathsf{Norm}(\mu_1 = 80, \sigma = 5)$ and $\mathsf{Norm}(\mu_2 = 82, \sigma = 5),$ respectively. We test $H_0: \mu_1 = \mu_2$ against $H_a: \mu_1 \ne \mu_2.$ Test: The test statistic in this case amounts to $T = \frac{\bar X_1 - \bar X_2}{S_p\sqrt{2/25}},$ where $S_p = \sqrt{(S_1^2 + S_2^2)/2}.$ We reject $H_0$ at the 5% level for $|T| > 2.011.$ qt(.975, 48) [1] 2.010635 Intuitively, the issue is whether we can detect the slight difference between $\mu_1$ and $\mu_2$ through the random 'fog' represented by $\sigma = 5.$ qt(.975, 48) [1] 2.010635 With data: To see how this works in practice, we generate fake data that matches the specific alternative $H_a: \mu_1 - \mu_2 = -2 \ne 0.$ [Computations in R.] set.seed(918); x1 = round(rnorm(25, 80, 5)); x2 = round(rnorm(25, 82, 5)) x1 [1] 80 89 82 81 90 90 72 77 74 81 81 67 83 76 78 80 69 84 78 76 82 81 78 84 94 x2 [1] 85 83 82 77 80 89 84 74 82 77 82 78 82 79 81 81 94 75 83 79 85 83 78 75 91 summary(x1); sd(x1) Min. 1st Qu. Median Mean 3rd Qu. Max. 67.00 77.00 81.00 80.28 83.00 94.00 [1] 6.360818 summary(x2); sd(x2) Min. 1st Qu. Median Mean 3rd Qu. Max. 74.00 78.00 82.00 81.56 83.00 94.00 [1] 4.839766 A pooled 2-sample t test on these samples, using t.test(x1, x2, var.eq=T) , returns $T = -.0801,$ which has $|T| 0.05, which also indicates we cannot reject.] Discussion: So in this particular case, we have not been able to detect a difference $|\mu_1 - \mu_2| = 2.$ Nor are these two particularly unlucky samples: one can show that such data would lead to rejection in about 28% of cases so generated. The observations in vectors x1 and x2 are independent; their correlation is only about $r = 9.12,$ consistent with independence. Each sample comes form a population with standard deviation $\sigma = 5,$ and the variances add. The estimate of the difference $\bar X_1 - \bar X_2$ is estimated by the denominator of the t statistic, for our sample about 1.60. Paired data. By contrast, suppose we have $n = 25$ subjects, with mean $\mu_b = 80$ before 'treatment' and $\mu_a = 82$ after treatment, where the tendency toward increase for each subject is $D \sim \mathsf{Norm}(\mu_d= 2, \sigma_d = 5).$ Now the most directly relevant data are the 'improvement scores' $X_{ai} - X_{bi} = D_i.$ We test $H_0: \mu_a - \mu_b = \mu_d = 0$ against $H_a: \mu_d \ne 0.$ Test: The test statistic is $T = \frac{\bar D}{S_d/\sqrt{n}}.$ We reject $H_0$ at the 5% level if $|T| > 2.064.$ qt(.975, 24) [1] 2.063899 Again the issue is whether we can distinguish whether $\mu_d$ differs from $0$ above the noise represented by $\sigma_d = 5.$ With data: Fake data generated according this this model with an average improvement of $\mu_d = 2$ is as follows: set.seed(901); d = round(rnorm(25, 2, 5)); xb = round(rnorm(25, 80, 5)); xa = xb + d summary(xb); sd(xb) Min. 1st Qu. Median Mean 3rd Qu. Max. 70.00 77.00 81.00 80.92 86.00 96.00 [1] 5.929587 summary(xa); sd(xa) Min. 1st Qu. Median Mean 3rd Qu. Max. 66 77 84 83 87 99 [1] 8.784456 summary(d); sd(d) Min. 1st Qu. Median Mean 3rd Qu. Max. -6.00 0.00 1.00 2.08 4.00 13.00 [1] 4.698581 A paired t test on these data is essentially a one-sample t test on the differences: t.test(d) returns $T = 2.2134,$ which has $|T| > 2.064,$ so we reject $H_0.$ [The P-value is p-value = 0.037, which also indicates rejection. The statement t.test(xa, xb, pair=T) produces essentially the same output.] So in this particular case a paired design has enabled us to detect the effect. One can show that data from our model would reject $H_0$ about half of the time. In the paired model the vectors x_b and x_a are ordinarily significantly correlated, because $X_{bi}$ and $X_{ai}$ are both from the same ($i$th) subject; for our data the sample correlation is about $r = 0.87,$ which is not consistent with independence. Discussion: With the paired design, the before-measurements have relatively large variability. Especially dealing with human and animal subjects, we have to expect high variability. (The after-measurements also have relatively large variability.) But in the paired design, each subject is its own sub-experiment, and the random variable of interest is the change $D_i$ for each subject. Ordinarily, it is mostly the variability in the effect of the treatment that matters in a paired design. (Granted, it can happen that different subjects react to the treatment in surprisingly different ways.) So it is the (usually) relatively small variance $\sigma_d$ that determines how easily we can detect whether $\mu_d \ne 0.$ Notes: (1) Whether a particular computation with two-sample or with paired data happens to give a high or low P-value depends on the nature of the data. (2) If paired data are incorrectly analyzed using a two-sample test, then the advantage of pairing is ignored, and the P-value may be larger than it would have been when correctly analyzed using a paired test. (3) For two-sample data with $n$ subjects in each group $(2n$ subjects altogether), the critical value of a pooled two-sample test will be based on $\nu = 2n-2$ degrees of freedom. For paired data with $2$ observations on each of $n$ randomly chosen subjects, an appropriate paired t test will have a critical value based on $\nu = n - 1$ degrees of freedom. Thus, at the 5% level with the same $n,$ the paired test will have critical values with a (somewhat) smaller absolute value. As $\nu$ increases the 5% critical values decrease in absolute value to a minimum of $\pm 1.96.$ However in practice, you should never be comparing $\nu$ for running a two-sample test with $\nu$ for running a paired test on the same data. One analysis is correct and one is not. Oranges have thicker skins than apples, but if you're making applesauce that is an irrelevant observation. You won't be dealing with oranges.
