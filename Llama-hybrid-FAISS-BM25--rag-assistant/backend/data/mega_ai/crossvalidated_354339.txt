[site]: crossvalidated
[post_id]: 354339
[parent_id]: 354336
[tags]: 
My question is this: is a random forest even still random if bootstrapping is turned off? Yes, it's still random. Without bootstrapping, all of the data is used to fit the model, so there is not random variation between trees with respect to the selected examples at each stage. However, random forest has a second source of variation, which is the random subset of features to try at each split. I thought the whole premise of a random forest is that, unlike a single decision tree (which sees the entire dataset as it grows), RF randomly partitions the original dataset and divies the partitions up among several decision trees. This is incorrect. Random forest bootstrap s the data for each tree, and then grows a decision tree that can only use a random subset of features at each split. The documentation states "The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default)," which implies that bootstrap=False draws a sample of size equal to the number of training examples without replacement, i.e. the same training set is always used. Detailed explanations of the random forest procedure and its statistical properties can be found in Leo Breiman, "Random Forests," Machine Learning volume 45 issue 1 (2001) as well as the relevant chapter of Hastie et al., Elements of Statistical Learning . We can verify that this behavior exists specifically in the sklearn implementation if we examine the source , which shows that the original data is not further altered when bootstrap=False if forest.bootstrap: ...irrelevant... elif class_weight == 'balanced_subsample': ...irrelevant... else: tree.fit(X, y, sample_weight=sample_weight, check_input=False) If bootstrapping is turned off, doesn't that mean you just have n decision trees growing from the same original data corpus? Yes, with the understanding that only a random subsample of features can be chosen at each split. In sklearn, random forest is implemented as an ensemble of one or more instances of sklearn.tree.DecisionTreeClassifier , which implements randomized feature subsampling. Or is it the case that when bootstrapping is off, the dataset is uniformly split into n partitions and distributed to n trees in a way that isn't randomized? No.
