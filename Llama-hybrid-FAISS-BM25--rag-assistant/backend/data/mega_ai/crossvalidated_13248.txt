[site]: crossvalidated
[post_id]: 13248
[parent_id]: 13243
[tags]: 
Nice introductory book on the topic related to different aspects of time series models could be Introduction to Time Series and Forecasting by Brockwell and Davis among many others. Roughly speaking, the characteristic of the autoregressive process of order $p$ is linked to the partial autocorrelation function . Estimating the $AR(p)$ process: $$X_t = \sum_{j=1}^p\phi_jX_{t-j} + \varepsilon_t$$ one common solution is to apply the Durbin-Levinson ( wiki on the math of Levinson recursion) method, where the residual sum of squares of $AR(p) $ $$RSS_p = \mathbb{E} \varepsilon^2= \mathbb{E}(X_t - \sum_{j=1}^p\phi_jX_{t-j})^2$$ is linked to the $RSS_{p-1}$ as: $$RSS_p = RSS_{p-1}(1-\varphi_{pp}^2),$$ with $\varphi_{pp}$ being the partial autocorrelations or the last component of $$\Gamma_p^{-1}\gamma_p = {([\gamma(i-j)]}_{i,j=1}^p)^{-1}[\gamma(1),\gamma(2),\dots,\gamma(p)]^\prime,$$ and $\gamma(.)$ being autocorrelation function. Thus if you apply wrong order autoregression it will cost you in theory the $(1 - \varphi_{pp}^2))$, note that in practice the estimation error also adds here. In small samples it may happen that a smaller model $(AR(1))$ is a better predictor than the true model $AR(2)$ (as the parameters has to be estimated and they are not known!). This is also known as the parsimony property of a smaller model.
