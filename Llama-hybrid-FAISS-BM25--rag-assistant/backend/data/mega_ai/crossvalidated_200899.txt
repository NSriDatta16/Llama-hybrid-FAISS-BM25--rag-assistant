[site]: crossvalidated
[post_id]: 200899
[parent_id]: 36247
[tags]: 
As other people have pointed out, there are a lot of (good) resources online and I have personally done some of them: Ng's Intro to ML class on Coursera Hinton's Neural Networks class on Coursera Ng's deep learning tutorial reading the relevant chapters in the original Parallel Distributed Processing I want to draw attention to the fact that these expositions mostly followed the classical treatment where layers (summation and non-linearity together) are the basic units. The more popular and more flexible treatment implemented in most libraries such as torch-nn and tensorflow, now uses computation graph with auto-differentiation to achieve high modularity. Conceptually it is simpler and more liberating. I would highly recommend the excellent Stanford CS231n open course for this treatment. For a rigorous, learning-theoretic treatment, you may want to consult Neural Networks by Anthony and Bartlett.
