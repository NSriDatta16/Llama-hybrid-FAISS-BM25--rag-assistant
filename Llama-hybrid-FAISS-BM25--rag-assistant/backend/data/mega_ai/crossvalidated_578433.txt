[site]: crossvalidated
[post_id]: 578433
[parent_id]: 578426
[tags]: 
There is the Hannan–Quinn information criterion (HQIC): $$ \text{HQ}=-2L_{\text{max}}+2k\ln(\ln(n)) $$ where $L_{\text{max}}$ is the maximized log-likelihood, $k$ is the number of parameters, and $n$ is the number of observations (the sample size). The penalty is smaller than BIC's $k\ln(n)$ but larger than AIC's $2k$ . HQIC is consistent for model selection like BIC, and its penalty is the minimal possible penalty to achieve consistency. It is not efficient like AIC but not very far from that since $\ln(\ln(n))$ is not that much bigger than 1 (1.5 for $n=100$ , 1.9 for $n=1000$ , 2.2 for $n=10000$ ). According to Wikipedia HQC is asymptotically very well-behaved. Van der Pas and Grünwald prove that model selection based on a modified Bayesian estimator, the so-called switch distribution, in many cases behaves asymptotically like HQC, while retaining the advantages of Bayesian methods such as the use of priors etc. And obviously, HQIC is just as easy to calculate as AIC or BIC.
