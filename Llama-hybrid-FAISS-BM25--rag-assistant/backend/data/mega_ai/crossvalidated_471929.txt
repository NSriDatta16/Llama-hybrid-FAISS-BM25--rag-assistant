[site]: crossvalidated
[post_id]: 471929
[parent_id]: 
[tags]: 
Why isn't activation function applied to the output layer of a neural net?

Im reading this paper momentarily, and in it (section 2.1.) the predicted output $\hat{\textbf{y}}$ of a single hidden layer neural network is given by \begin{align} \hat{\textbf{y}} = \sigma(\textbf{x}\textbf{W}_1)+b)\textbf{W}_2, \end{align} where $\textbf{x}$ is the input vector, $\textbf{W}_1,\textbf{W}_2$ the corresponding weight matrices and $b$ the bias weights. Two questions arise for me: Why isn't the activation function applied to the output layer, as in \begin{align} \hat{\textbf{y}} = \sigma(\sigma(\textbf{x}\textbf{W}_1)+b)\textbf{W}_2) \end{align} Why isn't a bias weight added to the output layer, as in \begin{align} \hat{\textbf{y}} = \sigma(\textbf{x}\textbf{W}_1)+b_1)\textbf{W}_2 + b_2 \end{align} Any intuition about this? Happy weekend, cheers
