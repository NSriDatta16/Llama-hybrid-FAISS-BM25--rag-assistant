[site]: crossvalidated
[post_id]: 208670
[parent_id]: 208184
[tags]: 
If you want N output values instead of 1, you should implement generalPerceptron:forward() such that if it receives an NxM input matrix (i.e. N samples with M features), it outputs N values. I.e. it should perform a matrix multiplication between the input and the weights of the network. EDIT : based on your comment. If you cannot modify forward() , you can just iterate over the input samples one at a time. I.e. you take one input sample, get a prediction using forward() on that input sample only, and use that to calculate the gradient. If you want to do minibatch with MSE, you would do something like this (pseudocode-ish): For each minibatch: sumgrad = 0 For each x_i in this minibatch: yhat = generalPerceptron:forward(x_i) error = 0.5*(target - yhat)^2 -- squared error sumgrad += gradient(error) --accumulate gradient weightupdate(learningRate, sumgrad/size(minibatch)) --update with learning rate and gradient average; not sure of the right Torch functions I.e. you accumulate gradient over minibatch samples and use this for your weight update. A quick Google search results in this Torch tutorial code for minibatch learning.
