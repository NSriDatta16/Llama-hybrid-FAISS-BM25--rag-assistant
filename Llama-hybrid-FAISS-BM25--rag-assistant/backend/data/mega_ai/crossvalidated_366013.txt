[site]: crossvalidated
[post_id]: 366013
[parent_id]: 365961
[tags]: 
This issue is really about Bayesian statistics; there's nothing particularly specific to machine learning about it. In Bayesian models, the entire collection of parameters* will have some joint posterior distribution, so if there's more than one such parameter (as is typical), then the joint distribution of them will be multivariate. However interest may focus on a specific parameter or some subset of parameters considered together or each on their own, and then you'll typically want to look at the marginal posterior distribution for that parameter or subset of parameters. In your example, the intercept and slope will have a bivariate posterior. You may be interested in that joint distribution, or you may be interested in (say) just knowing something about the slope on its own, in which case you'll want the marginal posterior for that. If you have an analytical form for the joint posterior you can integrate the intercept out to get the marginal posterior for the slope. On the other hand if you're sampling the joint posterior (e.g. by MCMC) then you can get a sample from the marginal posterior distribution by simply considering the sampled slope values on their own. So in practice you might either consider both together or any of them on their own -- or you may use both joint and marginal posteriors to address different issues. * or unknowns more generally if we're considering predictions, say
