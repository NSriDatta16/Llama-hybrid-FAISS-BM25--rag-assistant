[site]: crossvalidated
[post_id]: 627243
[parent_id]: 
[tags]: 
On the expressivity of latent variable models

Empirically, we have seen that VAEs can approximate very complex distributions. I am interested in knowing if there are any theoretical results showing how expressive latent variable models can be. For instance, are there any results showing that a particular class of latent variable models (say one with the assumptions of a regular VAE with Gaussian conditional distribution, etc.) can approximate any well-behaved distribution up to some level of accuracy? And if it is so, are there any bounds on the number of samples required to find a good approximation? I am new to variational inference. My current understanding is that we can define a latent variable model over a set $\mathcal{X}$ as $$p(x) = p(z)p(x|z),$$ where $z\in \mathcal{Z}$ is the latent variable. Now, if we consider a class of parameterized distributions over the latent like $p_{\mu, \Sigma}(z) \sim \mathcal{N}(\mu, \Sigma)$ and another class of parameterized distributions over the conditional like $p_{A, \Sigma}(x|z) \sim \mathcal{N}(Az, \Sigma)$ , then we can represent any distribution $p(x)$ that can be written as the product of $p(z)$ and $p(x|z)$ for some value of these parameters. My questions, in other words, would then be: Are there any results that consider some family of parameterized distributions $p(z)$ and $p(x|z)$ (like the ones I presented above) and show that a large class of distributions over $\mathcal{X}$ can be approximated as $p(z)p(x|z)$ ? If the answer to the previous part is positive, are there any bounds on the number of samples from $p(x)$ that are required to identify the best fitting parameters? My intuition is that as we increase the dimensionality of the latent variable $z$ , simple models should be able to approximate arbitrary distribution. (e.g., model any distributions with a mixture of Gaussians)
