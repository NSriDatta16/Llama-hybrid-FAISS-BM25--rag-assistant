[site]: crossvalidated
[post_id]: 472907
[parent_id]: 472822
[tags]: 
Our group recently built an LSTM model in a real world application. At first we had used other approaches, but then we decided to include features that were measurements taken over time, but of variable length - so for one person, we would have 15 measurements (of the same parameter) taken over a 3-month period, for another we would have 20 measurements over a 2-month period, and so on. Other features were present once per person, e.g. gender. In this situation, standard time series approaches turned out to be unusable, since they expected us to have an equal number of measurements per person, taken at equal intervals. LSTM allowed us to build a model predicting if a certain event will occur for a person, using the variable length measurements combined with the once-per-person measurements. We also compared our model to a simpler regression model using only one value per time-varying parameter (I forget what it was, probably the average value over time) and to a regression model using three measurements per time-varying feature per person and treating them as measurements of independent variables. The LSTM model had much better accuracy than both of these models, especially for the class of persons for whom the event occurred. I know that this is just one counterexample, and LSTM is not the only algorithm to deal with that kind of situation - but the way your question is stated lends itself to counterexamples, and statistics/ML would be an impoverished area if we didn't have different tools to choose from.
