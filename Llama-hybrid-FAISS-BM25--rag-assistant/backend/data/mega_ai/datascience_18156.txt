[site]: datascience
[post_id]: 18156
[parent_id]: 
[tags]: 
How to (better) discretize continuous data in decision trees?

Standard decision tree algorithms, such as ID3 and C4.5, have a brute force approach for choosing the cut point in a continuous feature. Every single value is tested as a possible cut point. (By tested I mean that e.g. the Information gain is calculated at every possible value.) With many continuous features and a lot of data (hence many values for each feature) this apporach seems very inefficient! I'm assuming finding a better way to do this is a hot topic in Machine Learning. In fact my Google Scholar search revealed some alternative approaches. Such as discretizing with k-means. Then there seem to be a lot of papers that tackle specific problems in specific domains. But is there a recent review paper, blog post or book that gives an overview on common apporaches for discretization? I couldn't find one... Or else, maybe one of you is an expert on the topic and willing to write up a small overview. That would be tremendously helpful!
