[site]: crossvalidated
[post_id]: 619470
[parent_id]: 
[tags]: 
Hyperparameter tuning and initialisation doubts in multivariate gaussian process model

I'm trying to train a multivariate Gaussian Process model using the code here https://github.com/Magica-Chen/gptp_multi_output . However I noticed how problematic is to initialise the length scales of the covSEard kernel, since it seems that the optimization is always different. Can you give me some advice? Are there good ways to initialise starting from the data itself? I read in an answer that a good idea is to initialise the length scales in a range between the minimum and the maximum distance of two (normalized) points of the dataset. I'm trying to do this, but it is valid also in this case, since a negative log marginal likelihood is used? I also tried random search and grid search but without a repeatable and reliable optimization (even if it is difficult to understand how much a length scale should vary to be defined as reliable: on the observation of my particular situation, it seems that after 1.1-1.5 the effect of the length scales on the predictions is always the same). And also, it is normal that the nlml increases as the number of records increase and is always over 1000?
