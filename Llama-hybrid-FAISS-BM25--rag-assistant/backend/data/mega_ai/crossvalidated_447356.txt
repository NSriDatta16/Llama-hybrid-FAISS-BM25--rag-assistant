[site]: crossvalidated
[post_id]: 447356
[parent_id]: 447350
[tags]: 
The simplest thing to try is using stratification. In the case of $k$ -fold cross-validation this will ensure that, each fold will have (approximately) the same percentage of samples for each class as in the original sample. A somewhat more involved solution would be to use over-sampling, under-sampling or a synthetic sample generation procedure like SMOTE or ROSE. If done carefully (i.e. we ensure that synthetic examples are only used during training, that our test examples still represent the class balance observed in the real data, as well as that our metric is relevant for what we want), it can be quite helpful. Please notice that class imbalance in itself is not a huge problem, CV.SE has a few great threads on the matter, the threads: When is unbalanced data really a problem in Machine Learning? and What is the root cause of the class imbalance problem? are great for a start.
