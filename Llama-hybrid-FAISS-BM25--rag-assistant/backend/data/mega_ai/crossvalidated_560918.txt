[site]: crossvalidated
[post_id]: 560918
[parent_id]: 
[tags]: 
Fast linear regression for binary outcomes

I am currently implementing an algorithm which fits many univariate regression models for binary outcomes. Logistic regression is the natural approach. However, since there is no closed solution for the logistic regression problem, an iterative fitting algorithm such as Newton's method or the BFGS algorithm has to be employed which can be quite exhaustive compared to OLS regression. Since my algorithm fits plenty of regression models (possibly millions of models), a fast as possible regression fit is desirable. Thus, I am planning to implement simple linear regression models which have to be truncated at 0 and 1 for generating proper probability estimates. My question is, if the corresponding least squares problem can be analytically solved or solved via an algorithm considerably faster than gradient-descent-like approaches used for logistic regression. The desired regression model is of the form $$ \mathbb{E}[Y \mid X = x] = \begin{cases} \hat{f}(x) & 0 \leq \hat{f}(x) \leq 1 \\ 0 & \hat{f}(x) 1 \end{cases},\;\;\; \hat{f}(x) = \beta_0 + \beta_1 x. $$ Thus, using least squares, the function to be optimized is given as $$ L(\beta_0, \beta_1) = \sum_{(x,y):\ 0 \leq \hat{f}(x) \leq 1} \left(y - \hat{f}(x)\right)^2 + \sum_{(x,y):\ \hat{f}(x) 1} (y-1)^2. $$ Note that solving the regression problem $\mathbb{E}[Y \mid X = x] = \hat{f}(x)$ and ignoring the truncation at 0 and 1 (but applying it afterwards) does not have to lead to the same regression line which is illustrated in the plot below. The black line depicts the true function and the simulated data according to it. The red line is the fitted (ordinary) regression line which does not consider the 0-1 truncation in the fitting process.
