[site]: crossvalidated
[post_id]: 292667
[parent_id]: 220878
[tags]: 
Very interesting idea. There could be some relation to "Gaussian Process Latent Variable Model" (GPLVM) which tries to map from a low dimensional space to the observed space via gaussian processes (instead of neural networks). The reconstruction error is used as objective function while the latent representations are optimized. As far as I know, local minima is a problem but it works in general well for high dimensional data with not so many samples. Also in some way related is "NeuroScale" which is similar, since it maps from latent space to observed space using a neural network and since the latent representations are found with optimization. The objective is in this case based on the similarity of pairwise distance in latent and observed space. One major problem with your approach is, that the model is very flexible. Imagine, that you random initialized all latent positions. It is possible that a neural network learns a perfect model without even changing these latent positions. This problem is also known for autoencoders. Here, it is helpful to have an additional regularization condition. Slightly changes in the input should still lead to similar latent representations. This concept can be found in "contractive autoencoders" or in some way in "denoising autoencoders". I would suggest to train the neural network using dropout and an l2-regularization on the latent space positions.
