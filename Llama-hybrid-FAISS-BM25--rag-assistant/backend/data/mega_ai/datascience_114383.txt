[site]: datascience
[post_id]: 114383
[parent_id]: 
[tags]: 
Online Learning/Continual Learning for tree-based Algorithms

Every example I come across any kind of iterative learning on Random Forest/XGBoost/LightGBM , it just continuously grows the number of estimators for new batches of data by n_tree / n_estimators / num_boost_rounds every time .fit() gets applied [...]. Most of them seem to rely on iterative learning for training on very large datasets that can't be loaded into memory at once. However, I want to implement a continuous learning pipeline (with LightGBM; Python) that takes newly available data on a daily basis in order to update an existing model (without the need to retrain on the whole [growing] dataset; stateful). The initially mentioned approach would imply that my model will ever increase its tree count. Is it possible to train tree-based algorithms so that the estimators (split thresholds) themselves get updated/adjusted in contrast to only adding estimators?
