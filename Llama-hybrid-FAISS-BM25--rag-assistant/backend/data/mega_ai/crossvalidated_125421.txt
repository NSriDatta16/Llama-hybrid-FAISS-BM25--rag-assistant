[site]: crossvalidated
[post_id]: 125421
[parent_id]: 
[tags]: 
Softmax maximum likelihood problem: arbitrary constant

I'm doing a multiclassification with a softmax function. The probability of a sample $j$ belonging to class $k$ is given by the softmax: $p_k(\mathbf{x}_j;\mathbf{w}_1,\mathbf{w}_2,...,\mathbf{w}_K)=\frac{\exp[f(\mathbf{x}_j,\mathbf{w}_k)]}{\exp[f(\mathbf{x}_j,\mathbf{w}_1)] + \exp[f(\mathbf{x}_j,\mathbf{w}_2)] + ... + \exp[f(\mathbf{x}_j,\mathbf{w}_K)] }$ Typically I use a linear model for $f$, so that $f(\mathbf{x},\mathbf{w}) = w_0 + w_1x_1 + ... w_Kx_K$ but there are options for the choice of $f$ such as polinomials or feed forward neural networks or radial basis functions. The optimal values of the parameters $\textbf{w}$ are found by the maximum likelihood estimation. This gives a task of minimizing a negative log-likelihood with respect to $\mathbf{w}$: $L = -\sum\limits_{j=1}^N\sum\limits_{k=1}^{K} \delta(c_j - k)\log p_k(\mathbf{x}_j;\mathbf{w}_1,\mathbf{w}_2, ...,\mathbf{w}_K$), where first sum is over training samples and the second sum is over classes. The $c_j$ is the class label of sample $j$. The $\mathbf{x}_j$ is a vector of features describing sample $j$. The problem is that the model $f$ may contain an arbitrary additive constant, $f' \rightarrow f + const$ which is cancelled in the fraction for the probability. This makes the solution of optimization problem non-unique. I've implemented the gradient descent optimization for linear model and found that depending on the initial estimate I end up with a completely different values for model parameters, but of course the estimated probabilities are identical. Is there any way to modify the cost function or model, so that the solution of the optimization problem becomes unique?
