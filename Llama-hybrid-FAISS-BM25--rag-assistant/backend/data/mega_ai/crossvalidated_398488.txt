[site]: crossvalidated
[post_id]: 398488
[parent_id]: 396165
[tags]: 
Some manifold learning algorithms (Isomap, LLE, LTSA, Hessian Eigenmaps, I think they are collectively called spectral dimensionality reduction) should in principle do this - this is because they actually use $k$ NN graph. This results in algorithm that preserves local structure rather than global one (they aim at distorting smaller distances less than big distances, kind of reverse of what PCA does). These algorithms share similar structure: construct a pairwise distance/similarity matrix $M$ perform some kind of matrix decomposition on $M$ (like SVD for example). For constructing $M$ these algorithms use neighborhood graph: run $k$ NN for some $k$ and then make graph where $x,y$ are connected if $x$ is in $k$ -neighborhood of $y$ or vice versa. Then they run an algorithm for finding distances in this graph.
