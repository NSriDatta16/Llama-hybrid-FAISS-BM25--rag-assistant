[site]: datascience
[post_id]: 67628
[parent_id]: 67566
[tags]: 
I would suggest you to scale your data using standard scaler and do it before you split it into X and Y here in your case. Why ? Please check this answer on stats sc. Also, keep the target variable ( Front in your case) as it is. So, according to me, the right choice looks like this, however you can try and experiment with min max scaler too: from sklearn.preprocessing import StandardScaler import pandas as pd df = pd.read_csv(r"D:\path\data.csv", sep=",") X = df[["Title words", "Hour posted", "Author age", "Is Text", "Subreddit age"]] Y = df['Front'] scaler = StandardScaler() scaled_X = scaler.fit_transform(X) scaled_X = pd.DataFrame(scaled_X, columns = X.columns) X_train, X_test, y_train, y_test = train_test_split(scaled_X, Y, test_size=0.2, random_state=42) To start, you can try with logistic regression and move on to svm, or maybe neural networks, if your data is big enough. from sklearn.linear_model import LogisticRegression lr = LogisticRegression(class_weight='balanced', C = 0).fit(X_train, y_train) y_pred = lr.predict(X_test) precision, recall, fscore, support = score(y_test, y_pred, pos_label= 1, average='binary') Now, you can choose a good performance metric for the same, looking at the distribution of the class label, whether they are balanced/imbalanced etc.
