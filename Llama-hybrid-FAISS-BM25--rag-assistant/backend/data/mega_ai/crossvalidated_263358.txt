[site]: crossvalidated
[post_id]: 263358
[parent_id]: 263293
[tags]: 
Adding noise reduces the quality of Bayesian results as it does for Frequentist and Likelihoodist methods. It will also slow down the model. This can be seen with a simple, degenerate example. Consider a case of data consisting of five points (1,1), (2,2), (3,3), (4,4) and (5,5). The slope is 1 and the intercept is zero. There is 100% certainty as to the parameters, if the model is valid. The posterior will be the Dirac delta function. Now adding noise creates an ordinary posterior with less certainty as a necessity. Furthermore, anything which spreads the uncertainty increases computation time. Where increases in variability do improve Bayesian methods is when it identifies signal rather than noise. Imagine a training set that only had green and brown eyed individuals. How would it handle its first blue-eyed person outside the training set? By having a blue-eyed person in the data set, this increase in natural variability improves the degree to which the model matches reality. This will speed up processing speed. It will narrow the variability.
