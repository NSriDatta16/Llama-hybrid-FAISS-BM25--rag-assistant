[site]: crossvalidated
[post_id]: 441270
[parent_id]: 441266
[tags]: 
Almost, but not quite. A Bernoulli random variable can only take values $0$ and $1$ , leading to the straightforward calculation $$E(X) = 1\cdot P(X=1) + 0\cdot P(X=0) = P(X=1).$$ If we condition on an event $B$ , almost nothing changes: $$E(X|B) = P(X=1|B).$$ So for a markov chain, it is true that $$E(X_t|X_1, X_2, \cdots X_{t-1}) = E(X_t|X_{t-1}) = P(X_t=1|X_{t-1} = x_{t-1})$$ However, this is not what you have written and generally speaking $E(X_t)$ is not the same as $E(X_t|X_{t-1})$ . Example, suppose $X_1 \sim \text{Bern}(1/2)$ and $X_t \sim \text{Bern}(p_{x_{t-1}})$ for $t \geq 2$ , where $p_0 = 0$ and $p_1=1$ . Then for any $t$ we have $$E(X_t) = \frac{1}{2}$$ but we also have $$E(X_t|X_{t-1}) = P(X_t=1|X_{t-1}=x_{t-1}) = \begin{cases} 0, & x_{t-1}=0 \\[1.2ex] 1, & x_{t-1}=1 \end{cases}$$
