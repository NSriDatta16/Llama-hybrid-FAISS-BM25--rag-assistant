[site]: datascience
[post_id]: 74842
[parent_id]: 32638
[tags]: 
A quick experiment suggests that xgboost makes its splits at midpoints between values of a feature in its training set: https://github.com/bmreiniger/datascience.stackexchange/blob/master/74634.ipynb The history turns out to be relatively complex, and possibly ongoing. In this github issue there's an indication that the midpoint splits are only for small-cardinality discrete features. That code is gone in the current (at time of posting) version of the code. It was removed in pull 5251 , commit 969ebbc , with further discussion pushed into issue 5096 . In the opposite direction, the function AddCutPoints didn't always exist, but the same discrete/continuous optimization shows up here , commit Jan 2017. So it seems that historically left-endpoints were used (at least for continuous data), supporting the arXiv paper, but that currently midpoints are used across the board. The experiment above finishes with the Wisconsin Breast Cancer dataset, which has well over 16 values for the relevant features, and still has splits appearing at midpoints. Finally, just a couple words about the monotonicity. First, as the arXiv paper mentions, this is entirely a question of how the model will apply to unseen data and a feature value that lands between values in the training data. Using left endpoints will always put such a value to the right side of the split, and so any monotonically increasing transformation won't affect that. A monotonically decreasing transformation will exactly reverse that. A linear(/affine) transformation will preserve the behavior when using midpoints (and also either left/right endpoints, depending on the sign of the transformation).
