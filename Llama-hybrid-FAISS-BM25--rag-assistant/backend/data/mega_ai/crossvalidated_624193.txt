[site]: crossvalidated
[post_id]: 624193
[parent_id]: 
[tags]: 
Prediction intervals and bias-variance tradeoff

I was looking for literature which connects prediction intervals with the bias-variance trade-off. Obviously both concepts deal with describing a mean squared deviation: the bias variance tradeoff deals with $E_{Y,x,D}[(Y-\hat{Y}(x)_D)^2]=E_{x}[E_{Y,D}[(Y-\hat{Y}(x)_D)^2]]$ , which takes the average over (potentially deterministic) x, different datasets D from the same distribution, and resulting variable y. Now $E_{Y,D}[(Y-\hat{Y}(x)_D)^2]$ looks quite related to the quantity $Var_{Y,D}[Y-\hat{Y}(x)_D]=E_{Y,D}[((Y-\hat{Y}(x)_D)-E[Y-\hat{Y}(x)_D])^2]=E_{Y,D}[(Y-\hat{Y}(x)_D)^2]$ (assuming an unbiased estimator in the last equality) which is repeatedly used in the construction of prediction intervals. Both the prediction interval and the variance-bias tradeoff indicate a minimal mean squared error dictated by the measurement noise $Var(Y)=\sigma^2$ . So I really wonder why I cannot find papers/books discussing this close relationship. Could somebody please tell me a) literature or b) point out the weaknesses in my argument above? PS: I realize that the bias variance tradeoff typically has this additional average over x. Surely, this averaging will have an effect when talking about heteroscedastic noise, i.e. $\sigma(x)$ (this is not the scope of this question) I know that predicition intervals do not have to be symmetric, so higher moments than just the variance might be needed for constructing a prediction interval with correct coverage (but this is not the scope of this question) Typically, we estimate prediction intervals, but let us assume correctly calibrated prediction intervals, having close to correct coverage. Thanks in advance for your thoughts!
