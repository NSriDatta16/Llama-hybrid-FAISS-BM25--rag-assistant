[site]: crossvalidated
[post_id]: 532330
[parent_id]: 
[tags]: 
If Mutual Information is $\text{KLD}(P(X, Y) || P(X)P(Y))$, why is $\text{KLD}(P(X)P(Y) || P(X, Y))$ never mentioned/used?

Mutual Information is defined as the Kullback-Liebler Divergence between $P(X, Y)$ and $P(X)P(Y)$ . Using definitions of KLD, this can be intuitively understood as the cost in average code length if we obtain data from the $P(X, Y)$ distribution but instead encode it optimally assuming the $P(X)P(Y)$ distribution. It can also be thought of as the expected information gain from samples to reject the null hypothesis of $P(X, Y)$ to the alternative $P(X)P(Y)$ . However, why don't we ever discuss/use the expected information gain from samples to reject the null hypothesis of $P(X)P(Y)$ to the alternative of $P(X, Y)$ ? The KL Divergence is not symmetric, and thus $\text{KLD}(P(X)P(Y) || P(X, Y))$ also defines a divergence in which $X$ and $Y$ are interchangeable. Why is this metric not used as much (never?) in literature? Also, the MI has a convenient relation to the (conditional) entropy: $H(X) = H(X \mid Y) + I(X;Y)$ as seen in this figure taken from Information Theory and its Relation to Machine Learning on arXiv.org: Does such a relation exist for the reverse KLD described above?
