[site]: crossvalidated
[post_id]: 307469
[parent_id]: 307407
[tags]: 
There are many ways to do what you want. The simplest is to take the average of all individual forecasts (empirically, it doesn't showed to be worse than theoretically optimal combinations). Another way is to make in-sample forecasts: Split you data (from time 1 to T) into estimation period (from 1 to M) and validation period (from M+1 to T); Use rolling windows or expanding windows schemes to produce forecasts on the validation period, and thus producing T-M forecast errors $y_{t}-y^{(forecast)}_{t}$. Use this forecast error series to calculate some relevant measure (for instance, Root Mean Squared Forecast Error or Mean Absolute Error). You can construct weights in such a way that best individual models receive more weight. For example, $weight_{k}=RMSE^{-1}(model_{k})/ \sum_{i} RMSE^{-1}(model_{i})$. There are more elaborated techniques such as Dynamic Model Averaging or Selection that will do what you want. The estimation can be tricky if you are not familiar with bayesian inference though. Here the weights are the predictive probability of an individual model. It should be mentioned that the Model Confidence Set is also used in several applied studies. I hope it helps your studies!
