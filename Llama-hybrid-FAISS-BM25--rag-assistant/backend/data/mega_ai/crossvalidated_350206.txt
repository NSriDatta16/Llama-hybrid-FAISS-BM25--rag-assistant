[site]: crossvalidated
[post_id]: 350206
[parent_id]: 349815
[tags]: 
I will give the summary of the text mentioned about this problem in Chapter 5, Bayesian Statistics, Machine Learning: A probabilistic perspective - by Murphy . Let us say we observed some data $X$, and we want to comment about the posterior distribution of parameters $p(\theta|X)$. Now, the point estimate of the mode of this posterior distribution, which is widely known as the MAP, has certain drawbacks. Unlike the mean or median, this is an 'untypical' point, in the sense that it does not consider all the other points while being estimated. In the case of estimating the mean/median, we take all the other points into consideration. So, as expected, in highly skewed posterior distributions, the MAP(and, by extension, the MLE) does not truly represent the actually posterior. So, how do we summarize a posterior using a point estimate such as Mean/Median/Mode? This is where people use decision theory - essentially a Loss function $L(\theta, \hat{\theta})$ which is the loss one incurs if the truth is $\theta$ and $\hat{\theta}$ is our estimate. We can choose a variety of Loss functions and our objective here is to minimize the expected value of Loss function. If the Loss function $L(\theta, \hat{\theta})$ is set as $\mathbb{I}(\hat{\theta}\ne\theta|x)$, an Indicator function for the all times when we CAN NOT estimate the truth, then minimizing the expected value of Loss function wrt $\theta$ is equal to Maximizing this function $\mathbb{I}(\hat{\theta}=\theta|x)$ wrt $\theta$. From this, it is intuitive to guess that Posterior mode minimizes the expected value of loss function. The details of this calculation can be seen in the above answer .
