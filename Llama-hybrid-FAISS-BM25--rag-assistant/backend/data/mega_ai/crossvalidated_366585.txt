[site]: crossvalidated
[post_id]: 366585
[parent_id]: 365928
[tags]: 
Your description of regularization is correct but overfitting and getting stuck in a local minimum are not the same thing. You could get stuck in a local minimum yet generalize well. A good way to think about the difference between overfitting and local minima is to think of the following setup: imagine you have a problem where the solution space has a lot of local minima and doesn't have a clear optimal gradient direction to the solution. You set up three identical neural networks in terms of amount of layers and parameters. However the second network adds weight decay and the third adds both weight decay as well as using an evolution strategy to calculate and optimize the gradients (rather than backpropagation with gradient descent). While both the second and third network should generalize better than the first, the third might also be able better escape local minima and reach a better global solution. Also, regularization strategies don't necessarily converge faster, it depends very much on the problem and strategy.
