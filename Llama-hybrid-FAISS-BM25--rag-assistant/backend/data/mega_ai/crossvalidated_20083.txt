[site]: crossvalidated
[post_id]: 20083
[parent_id]: 
[tags]: 
Density of normal distribution as dimensions increase

The question I want to ask is this: how does the proportion of samples within 1 SD of the mean of a normal distribution vary as the number of variates increases? (Almost) everyone knows that in a 1 dimensional normal distribution, 68% of samples can be found within 1 standard deviation of the mean. What about in 2, 3, 4, ... dimensions? I know it gets less... but by how much (precisely)? It would be handy to have a table showing the figures for 1, 2, 3... 10 dimensions, as well as 1, 2, 3... 10 SDs. Can anyone point to such a table? A little more context - I have a sensor which provides data on up to 128 channels. Each channel is subject to (independent) electrical noise. When I sense a calibration object, I can average a sufficient number of measurements and get a mean value across the 128 channels, along with 128 individual standard deviations. BUT... when it comes to the individual instantaneous readings, the data does not respond so much like 128 individual readings as it does like a single reading of an (up to) 128-dimensonal vector quantity. Certainly this is the best way to treat the few critical readings we take (typically 4-6 of the 128). I want to get a feel for what is "normal" variation and what is "outlier" in this vector space. I'm sure I've seen a table like the one I described that would apply to this kind of situation - can anyone point to one?
