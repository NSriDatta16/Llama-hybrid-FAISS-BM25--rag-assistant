[site]: crossvalidated
[post_id]: 475849
[parent_id]: 475832
[tags]: 
You are correct in that frequentist mixed models look and feel very Bayesian, but point estimates and inference are different. In the standard formulation of a non-GLM mixed model we have $$ y = X\beta + Z\gamma +\varepsilon $$ where $\beta$ corresponds to the fixed effects and $\gamma\sim\mathcal N(\mathbf 0, \Omega)$ is the random effects. $\varepsilon\sim\mathcal N(\mathbf 0,\sigma^2I)$ and $ \Omega$ is parameterized with a vector $\theta$ . Thus our parameters become $(\beta,\theta,\sigma^2)$ and $\gamma$ is a random variable. In a standard mixed model we integrate $\gamma$ out to get the marginal likelihood and then optimize this over $(\beta,\theta,\sigma^2)$ and inference uses the fact that this is a MLE which is asymptotically normal (except for some possible issues when some entries of $\theta$ are at the edge of their space). If we're doing REML we additionally put a uniform prior on $\beta$ and integrate that out, which again feels very Bayesian, but we still actually do inference by maximum likelihood. So despite the appearance of priors and integration, when it comes to inference it's all just maximum likelihood on fixed but unknown parameters (and for the ones we thought of as random, we "predict" them). If we were doing this as a Bayesian model we'd want to have priors on $(\beta,\theta,\sigma^2)$ and now we'd think of $(\beta,\gamma,\theta,\sigma^2)$ as our unknowns and they're all jointly random. We would probably then use something like MCMC to sample from the joint posterior of these RVs and then we could marginalize $\gamma$ out or something else. We'd also probably choose to use the posterior mean rather than the posterior mode which is more like maximum likelihood.
