[site]: crossvalidated
[post_id]: 506216
[parent_id]: 505171
[tags]: 
The objective would look exactly the same as above -- the KL divergence is still closed form (albeit the form now includes $m$ and $s$ ), and $p_\theta(z)$ doesn't really show up elsewhere. A really easy way to think about this is to imagine you add a "scaling layer" at the very end of your encoder which adds $m$ to your mean and $\log s^2$ to your log variance. Then at the very start of the decoder, you add an "inverse scaling layer" $z \mapsto \frac{z - m}{s}$ . And then everything else just works as before with a standard normal prior for $z$ . It's not too hard to convince yourself that this VAE is equivalent to a VAE without the scalings, but with the fancier prior.
