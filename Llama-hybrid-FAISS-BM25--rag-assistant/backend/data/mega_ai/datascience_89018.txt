[site]: datascience
[post_id]: 89018
[parent_id]: 89014
[tags]: 
All these parameters are trainable. Note that in normal Transformers it is typical to have fixed (non-trainable) positional embeddings, but in BERT they are learned . Note also the "pooler" component, which is an extra projection that was not mentioned in the paper, but which the authors commented on later .
