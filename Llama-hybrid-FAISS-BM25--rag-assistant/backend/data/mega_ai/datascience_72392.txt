[site]: datascience
[post_id]: 72392
[parent_id]: 
[tags]: 
Which of the following is a consequence of Selecting model complexity on test data? (multiple right answers)

Selecting model complexity on test data (choose all that apply): A. Allows you to avoid issues of overfitting to training data B. Provides an overly optimistic assessment of performance of the resulting model C. Is computationally inefficient D. Should never be done This question is part of a machine learning course I am taking on Coursera that I just realized has poor instructor support and hence I am unable to receive any help. I am confident that I have understood every little detail of the material that this question is based on. But I have no idea why my answer is wrong, i.e.: A and B Here is my explanation for why I picked A and B: Analyzing this question's 4 options: A. Preventing overfitting and under-fitting of training data is the goal of using tuning parameters to pick the right model complexity, hence using test data for this should achieve the goal/purpose. B. Given that tuning parameter is picked according to model performance on test data as question states, when generalization error is estimated for picked model, it is estimated on the same test data, and hence gives an overly optimistic result. C. Nothing implies that the process is computationally inefficient (although it is practically inefficient because of double-dipping into test data). D. Nothing implies it should NEVER be done, because if test data set is containing almost all of the data in the world, even if complexity is fitted to it, it could still serve as good approximation of generalization error (not overly optimistic). Does this logic above not hold? I am having extreme trouble with this question. Is it because I am assuming that there is no validation set in this case (I don't see how that would change the right answer, though). Someone please help me.
