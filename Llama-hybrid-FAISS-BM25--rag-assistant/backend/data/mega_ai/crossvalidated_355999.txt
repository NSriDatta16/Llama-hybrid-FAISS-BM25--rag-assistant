[site]: crossvalidated
[post_id]: 355999
[parent_id]: 355994
[tags]: 
A good place to look might be the Deep Mind paper on the topic, " Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm " by David Silver et al. The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case. From the Methods section (p. 13): A move in chess may be described in two parts: selecting the piece to move, and then selecting among the legal moves for that piece. We represent the policy $\pi(a|s)$ by a $8 \times 8 \times 73$ stack of planes encoding a probability distribution over $4,672$ possible moves. Each of the $8\times 8$ positions identifies the square from which to “pick up” a piece. The first 56 planes encode possible ‘queen moves’ for any piece: a number of squares [1..7] in which the piece will be moved, along one of eight relative compass directions {N, NE, E, SE, S, SW, W, NW}. The next 8 planes encode possible knight moves for that piece. The final 9 planes encode possible under-promotions for pawn moves or captures in two possible diagonals, to knight, bishop or rook respectively. Other pawn moves or captures from the seventh rank are promoted to a queen. To address your questions: A move in chess is defined by the square the move starts from and the square the move ends. I've calculated all possible moves which are: diagonal moves (280), straight (horizontal/vertical) moves (896), knight (L-shape) moves (336) and castle moves (2 for agent only). All of these moves define all the possible moves all pieces in check can make. I am wondering if these should also be used as the number of output nodes where each of these nodes represents one move? It seems that they represent moves in a different way than your enumeration. They start from allotting an output neuron to each piece's starting point, and then destinations for that piece. Special moves, like pawn promotion, are likewise represented as "destinations". The first sentence of this passage suggests that some "filtration" is applied to screen out illegal moves, or illogical moves (like moving a piece 0 squares), from considerations of the move's "profit". So your intuition seems to be correct: make some assessments for all moves first, and then exclude all impossible moves. Am I correct with this assumption of the neural network of AlphaZero chess? You're partially right. The above passage (p. 13) supports that the illegal moves are discarded out of hand. The predicted value for a particular move is estimated in a different manner, however. Instead of having a "value" for each "move" neuron, the "value" for a particular move is estimated by Monte Carlo tree search. The authors write (pp. 2-3): Instead of a handcrafted evaluation function and move ordering heuristics, AlphaZero utilises a deep neural network $(\mathbf{p}, v) = f_\theta(s)$ with parameters $\theta$ . This neural network takes the board position $s$ as an input and outputs a vector of move probabilities $\mathbf{p}$ with components $p_a = Pr(a|s)$ for each action $a$ , and a scalar value $v$ estimating the expected outcome $z$ from position $s$ , $v \approx \mathbb{E}[z|s]$ . AlphaZero learns these move probabilities and value estimates entirely from self-play; these are then used to guide its search. Instead of an alpha-beta search with domain-specific enhancements, AlphaZero uses a general-purpose Monte-Carlo tree search (MCTS) algorithm. Each search consists of a series of simulated games of self-play that traverse a tree from root $s_{root}$ to leaf. Each simulation proceeds by selecting in each state $s$ a move a with low visit count, high move probability and high value (averaged over the leaf states of simulations that selected a from $s$ ) according to the current neural network $f_\theta$ . The search returns a vector $\pi$ representing a probability distribution over moves, either proportionally or greedily with respect to the visit counts at the root state.
