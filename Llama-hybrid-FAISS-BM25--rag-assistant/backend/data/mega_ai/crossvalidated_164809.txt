[site]: crossvalidated
[post_id]: 164809
[parent_id]: 164808
[tags]: 
The results will be sensitive to the splits, so you should compare models on the same partitioning of the data. Compare these two approaches: Approach 1 will compare two models, but use the same CV partitioning. Approach 2 will compare two models, but the first model will have a different CV partitioning than second. We'd like to select the best model. The problem with approach 2 is that the difference in performance between the two models will come from two different sources: (a) the differences between the two folds and (b) the differences between the algorithms themselves (say, random forest and logistic regression). If one model out-performs the other, we won't know if that difference in performance is entirely, partially, or not at all due to the differences in the two CV partitions. On the other hand, any difference in performance using approach 1 cannot be due to differences in how the data were partitioned, because the partitions are identical. To fix the partioning, use cvTools to create your (repeated) CV partitions and store the results.
