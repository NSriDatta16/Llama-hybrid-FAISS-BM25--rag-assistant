[site]: crossvalidated
[post_id]: 608932
[parent_id]: 478930
[tags]: 
As @vivek says, random forests often don't need cross-validation, though you can use cross-validation to choose the 'number of predictors per node' tuning parameter. Even there, you'd probably just use the out-of-bag prediction error that random-forest implementations typically give you for free rather than cross-validation. Boosted trees tend to have more tuning parameters that are worth adjusting (especially XGBoost, which has ALL THE PARAMETERS). The basic approach is the same as cross-validation for a cost-complexity tuning parameter in regression or lasso or CART. You pick a set of values for the tuning parameter, and for each value, use cross-validation to estimate your favorite out-of-sample prediction error metric (eg, mean squared prediction error). You then choose the tuning-parameter value that minimises this metric and re-run the boosting algorithm with that chosen value. If you want to tune multiple parameters this can be slow: some people use a 'greedy' algorithm where they tune one parameter then fix it and tune another parameter, then fix that and tune a third parameter, and so on. This will work ok as long as the best value for one parameter isn't too sensitive to the values of other parameters. In principle, the sort of experimental designs used for response-surface optimisation should get you better results, but I haven't seen people doing it very much.
