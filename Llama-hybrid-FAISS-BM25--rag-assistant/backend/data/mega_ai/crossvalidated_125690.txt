[site]: crossvalidated
[post_id]: 125690
[parent_id]: 125684
[tags]: 
This is a reasonable question (+1) that stems from the terminological ambiguity and confusion. In the context of PCA, people often call principal axes (eigenvectors of the covariance/correlation matrix) "loadings". This is sloppy terminology. What should rather be called "loadings" in PCA, are principal axes scaled by the square roots of the respective eigenvalues. Then the theorem you are referring to will hold. Indeed, if the eigen-decomposition of the correlation matrix is $$\mathbf R = \mathbf V \mathbf S \mathbf V^\top$$ where $\mathbf V$ are eigenvectors (principal axes) and $\mathbf S$ is a diagonal matrix of eigenvalues, and if we define loadings as $$\mathbf A = \mathbf V \mathbf S^{1/2},$$ then one can easily see that $$\mathbf R = \mathbf A \mathbf A^\top.$$ Moreover, the best rank-$r$ approximation to the correlation matrix is given by the first $r$ PCA loadings: $$\mathbf R \approx \mathbf A_r \mathbf A_r^\top.$$ Please see my answer here for more about reconstructing covariance matrices with factor analysis and PCA loadings.
