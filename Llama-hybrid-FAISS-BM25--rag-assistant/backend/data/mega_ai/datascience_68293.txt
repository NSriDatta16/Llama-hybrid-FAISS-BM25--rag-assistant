[site]: datascience
[post_id]: 68293
[parent_id]: 
[tags]: 
Macro F1 result higher than accuracy for imbalanced dataset

In one of the research papers on fake news detection, the authors collected a fake news binary dataset (fake vs. real news) consists of 16,817 real articles and 5,323 fake ones. The authors presented the results using accuracy, precision, recall and F1 without specifying which kind of averaging they applied on the F1 metric ( macro , micro , weighted , etc.). Here are the results: If you can notice for the last system, the accuracy value is 0.689 and the F1 value is 0.717 which is higher than the accuracy. Thus, give the imbalanced status of the dataset, is it possible that the authors averaged the classes in the F1 metric using macro way? For me, this "impossible" to happen, and I argue that probably they used weighed F1 score.
