[site]: crossvalidated
[post_id]: 428011
[parent_id]: 178504
[tags]: 
$\newcommand{\bx}{\mathbf{x}}$ $\newcommand{\by}{\mathbf{y}}$ I personally prefer the Monte Carlo approach because of its ease. There are alternatives (e.g. the unscented transform), but these are certainly biased. Let me formalise your problem a bit. You are using a neural network to implement a conditional probability distribution over the outputs $\by$ given the inputs $\bx$ , where the weights are collected in $\theta$ : $$ p_\theta(\by~\mid~\bx). $$ Let us not care about how you obtained the weights $\theta$ –probably some kind of backprop–and just treat that as a black box that has been handed to us. As an additional property of your problem, you assume that your only have access to some "noisy version" $\tilde \bx$ of the actual input $\bx$ , where $$\tilde \bx = \bx + \epsilon$$ with $\epsilon$ following some distribution, e.g. Gaussian. Note that you then can write $$ p(\tilde \bx\mid\bx) = \mathcal{N}(\tilde \bx| \bx, \sigma^2_\epsilon) $$ where $\epsilon \sim \mathcal{N}(0, \sigma^2_\epsilon).$ Then what you want is the distribution $$ p(\by\mid\tilde \bx) = \int p(\by\mid\bx) p(\bx\mid\tilde \bx) d\bx, $$ i.e. the distribution over outputs given the noisy input and a model of clean inputs to outputs. Now, if you can invert $p(\tilde \bx\mid\bx)$ to obtain $p(\bx\mid\tilde \bx)$ (which you can in the case of a Gaussian random variable and others), you can approximate the above with plain Monte Carlo integration through sampling: $$ p(\by\mid\tilde \bx) \approx \sum_i p(\by\mid\bx_i), \quad \bx_i \sim p(\bx\mid\tilde \bx). $$ Note that this can also be used to calculate all other kinds of expectations of functions $f$ of $\by$ : $$ f(\tilde \bx) \approx \sum_i f(\by_i), \quad \bx_i \sim p(\bx\mid\tilde \bx), \by_i \sim p(\by\mid\bx_i). $$ Without further assumptions, there are only biased approximations.
