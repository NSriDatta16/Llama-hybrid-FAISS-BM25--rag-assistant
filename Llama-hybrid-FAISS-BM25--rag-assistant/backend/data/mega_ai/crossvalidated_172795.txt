[site]: crossvalidated
[post_id]: 172795
[parent_id]: 
[tags]: 
scaling for SVM destroys my results

I'm applying standard 0-1 scaling of features before SVM classification for financial data but the results are worse. This is the results before scaling NORMAL DATA AVERAGE RESULTS Profit PF avMC avPP avRC totTP totFP PF>1 algosnum SS SSl 4389060.90 6.85 -0.00 60.69 0.50 16086 10973 5 8 1 5 and this is after scaling NORMAL DATA AVERAGE RESULTS Profit PF avMC avPP avRC totTP totFP PF>1 algosnum SS SSl 2256204.80 2044.51 -0.07 52.53 0.46 14577 12220 4 8 1 5 Scaling is performed in 0-1 range, test data is scaled according to scaling factor of train data. From the above results you can see that precision went down (avPP) from 60.69 to 52.53, average Mathew Correlation Index from 0 to -0.07 number of true positives went down from 16086 to 14577 and number of false positives grown from 10973 to 12220. The presented result is an outcome of 80 classifications on different financial instruments data for 80 data sets 20000x200 so i think result is quite significant. So my question is: In such situation how I should proceed? Shall I stick to scaling? Or maybe I should generate different data set to check if this behavior is consistent? What sort of analysis of my features I can make? My data set is a mix of binary and continuous features in different scales.
