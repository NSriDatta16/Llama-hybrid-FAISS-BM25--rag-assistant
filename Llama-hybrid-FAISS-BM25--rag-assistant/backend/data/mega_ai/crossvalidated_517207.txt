[site]: crossvalidated
[post_id]: 517207
[parent_id]: 516914
[tags]: 
I think that the confusion you are experiencing is mainly due to notation and a bit of philosophy around bayesian probability. In general, we define a model $M$ having parameters $\theta$ . This structure $M$ is "a structure", it can be a linear regression, a Tree, a Random Forest, A Neural Net, etc... $\theta$ are the parameters of the model. They are the unknowns that exists in a parameter space, and that will allow you to find the model for your specific $X$ . However that definition is already somehow arbitrary, for instance we don't count hidden layer of NN or depth of Tree for Random Forest to be parameters but hyperparameters. In essence there is no difference, but we consider that "they are the model". Why do we do this trick structure/hyperparameter/parameter ? Well mainly because otherwise no problem would be solvable. If your model $M$ can be any model (you don't make a choice, but condition over all the possible structure of models), it would be an impossible problem to solve. So you set a structure $M$ and then optimize within that structure. More recently, there is an entire field of research called Neural Architecture Search or AutoML that aims at finding the best model (mainly hyperparameters) but this is a very complex task. It's important to note that the 2 formulas are actually equivalent, it is more about interpretation. The fact that every term in the first formula is conditioned on $M$ means that $M$ acts as a deterministic value. It could be a random variable but in this case, you always work under the assumption that $M$ is fully known. In the second formula, $M$ is not there because simply it is useless to add it. There is no dependence, $M$ is a feature of the world that can't be change, it is not a random variable, no need to condition on it. The way to use both formulas remains the same regardless of the interpretation. Why don't we write $P(M|\theta)$ ? In theory, one could completely compute $P(M|\theta)$ , but we don't. First because it does not answer any useful question, in real life $M$ will always be specified before $\theta$ . Second because as we've seen, there is no need to have a distribution over $M$ , you always live in a "sub-world" where $M$ is either a fixed value of random variable or unchangeable. Finally, because that does not really make sense. Choosing a specific $M$ will specify your vector $\theta's$ . For instance choosing linear regression will make $\theta's$ coefficient of the linear reg, but choosing a tree will make $\theta's$ being splits along a feature. I hope this helps!
