[site]: crossvalidated
[post_id]: 41232
[parent_id]: 41104
[tags]: 
I'm actually doing an experiment with this right now. I work in text classification, so my training set is typically on the order of several hundred thousand features, and I'm looking at comparing a linear SVM (optimized for the c-parameter) against the weka implementation of random forests. I'm finding that, for my data, about 74 trees, and 32 features, thus far, seems to give pretty good performance. Of course, increasing these values tends to increase the AUC I observe, but it's in the thousandths digit place, generally. I'm still trying to understand how this algorithm is handling my data, but I suspect, based on the Breiman paper , that the more generally-useful features there are in your training set, the less important the number of trees parameter becomes. If you read the paper (and it's a really great paper), each tree consists of a random sampling of the features in your data, so, if there are a great many helpful ones in your set, you're more likely to find something useful in any particular tree. That said, I think it's always a good idea to optimize an algorithm for one's particular data. For my experiments, I've set aside a training/optimization set on which I am performing cross-validation across different parameter values. I'd be interested to hear what you find!
