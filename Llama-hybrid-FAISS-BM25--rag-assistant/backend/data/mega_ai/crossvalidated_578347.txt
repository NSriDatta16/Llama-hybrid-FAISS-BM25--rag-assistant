[site]: crossvalidated
[post_id]: 578347
[parent_id]: 578335
[tags]: 
There's a bunch of potential limitations, but for most of these there also extensions/versions/approaches that address these: It's only for binary/binomial outcomes: there's extensions to ordered and unordered categories. Partial or perfect separation can happen (e.g. one predictor - or a combination of multiple ones - can split results perfectly in all 1s or 0s): potential solutions incl. Firth penalized likelihood, exact or Bayesian logistic regression There's issues with inference for sparse data (low or very high number of cases) in terms of e.g. estimation, confidence interval coverage, type I error etc.: similar solutions to the previous bullet help. People mostly thing of the version of LR, where you assume predictors have a linear effect ("cannot model non-linear effects"): Splines or other non-linear terms are the obvious solution, although to some extent you still have to specify them (rather than things like XGBoost or neural networks "automatically creating their own non-linear functions"). You need to create your own features (often stated in contrast to, say, neural networks, although to be fair these also benefit from being given strong features): to a considerable extent true. Although you could regard a neural network as a particular form of LR (i.e. trained using gradient descent, with, if there's multiple layers, a particularly weird spline across many features...). Logistic regression does not work all that well with certain types of input such as images or text. There are of course techniques that can be used: e.g. TF-IDF features for text help, and using embeddings from (pre-trained) neural networks (e.g. word2vec embeddings, or pooling layer outputs from some convolutional neural network) as inputs can be useful. Hyperparameter tuning: per default there are no hyperparameters for logistic regression that one can tune. Even with elastic net logistic regression just has 2 penalty parameters (and perhaps one more if we considered the relaxed elastic net), while other models like XGBoost, LightGBM, tabular neural networks etc. have a lot more hyperparameters that one can tune to potentially get more effective regularization. However, note that in some settings, it can be very hard to find something better than a well-constructed logistic regression . For what it's worth, there's all the cautionary advice about "you better use the 1-SE-rule instead of picking the seemingly best parameters from cross-validation, or else you may have overfit" about LASSO/ridge/elastic-net LR, which appears to have a lot of merit. LR does not deal well with high dimensional categorical variables by dummy coding them/does not account for them being a-priori similar/ignores that observations from related subjects are correlated: random effects/hierarchical logistic regression and various other version for particular applications (e.g. spatial data) can do this (as well as a bunch of other useful things). Non-linear interactions are rarely modelled in practice, and it gets increasingly difficult to specify these. However, if you suspect them for some combination of predictors, it can be done. LR cannot reflect what I already know about the problem: Bayesian logistic regression let's you specify prior distributions to reflect your prior knowledge. The linear model vs. logistic regression (some people like to model event occurrence 1/0 using linear regression) discussion. The interpretability of odds ratios debate, where some people argue that risk differences are preferable, some people prefer the number needed to treat and some rate ratios (all 4 options have some merit in some circumstances, of course). Plus, you can fit a logistic regression using the logit link and get different effect measures from the model fit under certain assumptions. The collapsability of the odds ratio debate: i.e. odds ratio is non-collapsible, while a rate ratio is, which has some implications on covariate adjustment (and what happens when it's mis-specified). Logistic regression does not sound cool enough when making research proposals and people are looking for buzzwords like machine learning or neural networks. However, the solution is easy: logistic regression also meets the typical definitions of machine learning and/or artificial intelligence, so just describe it as such (I cannot remember the origin, but there's a joke on this that goes:"It's artificial intelligence when you talk to the investors, machine learning in the job advert and logistic regression when you do it).
