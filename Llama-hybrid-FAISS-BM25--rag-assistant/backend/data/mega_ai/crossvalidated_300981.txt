[site]: crossvalidated
[post_id]: 300981
[parent_id]: 
[tags]: 
Shorter confidence interval for $\sigma^2$ in regression

Consider the linear regression model $Y=X_1\beta_1+X_2\beta_2+\epsilon$ where $Y,\epsilon$ are $n\times1$ vectors, $X_1$ is $n\times p_1$ matrix with full column rank, $X_2$ is $n\times p_2$ matrix with full column rank, and $\beta_i$ are $p_i$ dimensional vectors, for $i=1,2$. We assume the classical set up i.e. $\epsilon\sim N_n(0,\sigma^2I)$, and $X_1,X_2$ fixed. Let $\sigma^2_{full}$ be the estimate of $\sigma^2$ under the full model. Now suppose in reality, $\beta_2=0$ i.e. the model is really $Y=X_1\beta_1+\epsilon$. Let $\sigma^2_{red}$ be the estimate of $\sigma^2$ under reduced model. I want to show that the confidence interval for $\sigma^2$ based on $\sigma^2_{full}$, on an average, is longer than the confidence interval for $\sigma^2$ based on $\sigma^2_{red}$. So I know that $\sigma^2_{full}=\dfrac{Y'(I-P_{(X_1|X_2)})Y}{n-p_1-p_2}$ and $\sigma^2_{red}=\dfrac{Y'(I-P_{X_1})Y}{n-p_1}$. I can find the confidence intervals based on the chi-squared distributions but I cannot show that the expected length of the CI based on $\sigma^2_{full}$ is greater than the expected length of the CI based on $\sigma^2_{red}$. Any help in this direction would be helpful.
