[site]: crossvalidated
[post_id]: 40448
[parent_id]: 39283
[tags]: 
Building $k$ clusters and then $k$ corresponding models is absolutely feasible. The pathologic case noted in the comments wherein the clusters perfectly separate the outcome variables would pose difficulties for classifiers is a theoretical problem, but one which I think is unlikely (especially in a high dimensional case). Furthermore, if you could build such clusters, you could then just use those clusters for prediction! In addition, if the process begins with $N$ samples, the classifiers can only use $N/k$ samples. Thus, a more powerful approach would be to use the clusters in building a single classifier that incorporates the heterogeneity in the clusters using a mixture of regressions. In model-based clustering, one assumes the data are generated from a mixture distribution $Y_i \sim N(\mu_i, \sigma_i^2)$ where $i=1$ with probability $\pi$ and $i=2$ with probability $1-\pi$ and $\mu_1 \neq \ \mu_2$ and $\sigma_1^2 \neq \sigma_2^2$. A mixture regression is an extension that allows one to model the data as being dependent on co-variates; $\mu_i$ is replaced with $\beta_i X_i$, where the $\beta_i$ have to be estimated. While this example is for a univariate, Gaussian case, the framework can accommodate many data (multinomial-logit would be appropriate for categorical variables). The flexmix package for R provides a more detailed description and of course a relatively easy and extensible way to implement this approach. Alternatively, in a discriminative setting, one could try incorporating cluster assignments (hard or soft) as a feature for training the classification algorithm of choice (e.g. NB, ANN, SVM, RF, etc.)
