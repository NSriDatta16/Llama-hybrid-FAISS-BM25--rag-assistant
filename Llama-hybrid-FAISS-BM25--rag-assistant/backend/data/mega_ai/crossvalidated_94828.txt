[site]: crossvalidated
[post_id]: 94828
[parent_id]: 
[tags]: 
Standard deviation of a sample when you know the average

I'm coding an app, a part of which is graphing values from a database. The graph plots the average value of every 10% of the values up to 100% so there are ten points along the x-axis. The graph shows a trend of the lifetime of the stats. Hopefully this makes sense. I need to decide on a scale for the y-axis of the graph based on the overall average of the values that are graphed. Is there a standard formula I can apply to this average to determine a scale which is likely to ensure that most if not all of the range is covered and appear on said graph?
