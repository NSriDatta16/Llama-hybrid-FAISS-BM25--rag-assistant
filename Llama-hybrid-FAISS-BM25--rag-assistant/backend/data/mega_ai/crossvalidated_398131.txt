[site]: crossvalidated
[post_id]: 398131
[parent_id]: 398114
[tags]: 
I too haven't understood their reasoning, I always assumed it was a typo or something... The way I see it we if we have $n$ hidden units in a Neural Network with a single hidden layer and we apply dropout keeping $r$ of those, we'll have: $$ \frac{n!}{r! \cdot (n-r)!} $$ possible combinations (not $2^n$ as the authors state). Example : Assume a simple fully connected neural network with a single hidden layer with 4 neurons. This means the hidden layer will have 4 outputs $h_1, h_2, h_3, h_4$ . Now, you want to apply dropout to this layer with a 0.5 probability (i.e. half of the outputs will be dropped). Since 2 out of the 4 outputs will be dropped, at each training iteration we'll have one of the following possibilities: $h_1, h_2$ $h_1, h_3$ $h_1, h_4$ $h_2, h_3$ $h_2, h_4$ $h_3, h_4$ or by applying the formula: $$ \frac{4!}{2! \cdot (4-2)!} = \frac{24}{2 \cdot 2} = 6 $$
