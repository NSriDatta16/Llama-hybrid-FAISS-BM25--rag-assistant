[site]: stackoverflow
[post_id]: 2302694
[parent_id]: 2301926
[tags]: 
xslt tends to be comparatively quite fast even for large files. For large files, the trick is not creating the DOM first. Use a URL Source or a stream source to pass to the transformer. To strip the empty nodes and unwanted attributes start with the Identity Transform template and filter them out. Then use XPATH to search for your required tags. You could also try a bunch of variations: Split the large XML files into smaller ones and still preserve their composition using the XML-Include. It is very much similar to splitting large source files into smaller ones and using the include "x.h" kind of concept. This way, you may not have to deal with large files. When you run your XML through the Identity Transform, use it to assign a UNID for each node of interest using the generated-id() function. Build a front-end database table for searching. Use the above generated UNID to quickly pinpoint the location of the data in a file.
