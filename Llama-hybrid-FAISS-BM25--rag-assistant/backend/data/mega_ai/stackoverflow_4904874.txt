[site]: stackoverflow
[post_id]: 4904874
[parent_id]: 
[tags]: 
Why does list length reduce to sqrt(n) after each comparison in interpolation search?

According to the book I'm reading, interpolation search takes O(loglogn) in average case. The book assumes that each compare reduce the length of the list from n to sqrt(n) . Well, it isn't difficult to work out the O(loglogn) given this assumption. However, the book didn't talk more about this assumption except that it says this is correct. Question: can anyone give some explanation on why this is true?
