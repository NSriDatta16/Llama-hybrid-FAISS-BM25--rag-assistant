[site]: crossvalidated
[post_id]: 451900
[parent_id]: 451660
[tags]: 
It looks good. Minor comments: Next, we know in a plain feedforward neural network, once trained, the weights are frozen. So this means for all training images we wont be able to get an output probability for dog label as 1 for all dog training images and neither will we be able to get an output probability for cat label as 1 for all cat images. I agree with this, although there is nothing mathematically wrong with getting an output probability of 1. It can happen. It just doesn't usually happen in practice. The other thing to note is that treating the output of (non-Bayesian) neural networks as probability distributions is a rather heuristic (albeit common) thing to do. For instance, see this answer . If you observe above, my understanding is that this is not a generative kind of a model which otherwise would have looked at how nature would have generated the training images once it selects a label as either dog or cat for every image. Instead this a discriminatory approach of given an image how do I predict the probabilities between the labels. Yes, it is a discriminative approach, only caring about $p(y|x)$ , rather than the more general (but harder to learn) $p(x,y)$ .
