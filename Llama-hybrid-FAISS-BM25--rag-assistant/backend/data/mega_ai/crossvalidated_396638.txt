[site]: crossvalidated
[post_id]: 396638
[parent_id]: 
[tags]: 
Why does `xgboost` find such an unbalanced split in my data?

I'm using xgboost for regression. The data and the python script used for analyzing the data are uploaded here . The data file is a pickled python list that can be loaded with pickle . The dimension of each instance is $96$ , and there are $11450$ instances in total. The target values are in the range $[-1, 1]$ . Because both the feature dimension and the number of instances are pretty large, I will not show the statistics of the dataset. For the purpose of regression, here is a piece of code I used (which you can view the full version from the link above) from xgboost import XGBRegressor model_exact = XGBRegressor(max_depth = 5, n_estimators = 1, slient = False, min_child_weight = 1, tree_method = 'exact') model_exact.fit(x_train, y_train, eval_set=eval_set, eval_metric="mae", early_stopping_rounds=30) As you can see from the code, I'm using only one gbtree whose max_depth = 5 for regression. After I train the model, I found that the root node of the very first tree divides the whole dataset into two very unbalanced parts. Here is the result: min_child_weight = 1 tree_method = 'exact' max_depth = 5 --> I only show the first level booster[0]: 0:[f25 On the other hand, if I set min_child_weights = 5 , xgboost divides the whole dataset only once min_child_weight = 5 tree_method = 'exact' max_depth = 5 booster[0]: 0:[f84 Similar result is obtained when I set tree_method to approx . Why xgboost cannot evenly divide this dataset? Is there some kind of flaw in this dataset? The reason that I ask this question is xgboost overfits severely on this dataset, I don't know whether it is a result of the unbalanced split that I described.
