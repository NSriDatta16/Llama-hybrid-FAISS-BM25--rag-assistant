[site]: datascience
[post_id]: 126346
[parent_id]: 
[tags]: 
Optimal Number of Epochs for Training Transformer Network on Time series data? Early Stopping and Model Selection Strategies

I have a transformer network that is trained on time series data. The task is to predict if a variable will increase a certain percentage in the next 7 days. The input is data from the 90 previous days. The training data is data from 2000 up till 2023, while validation data is from 2023 and until todays date. There is no overlapping data in the training and validation set. I train the model on the training data and make note of the epoch number $n_E$ where the validation loss is minimized. Then I concatenate the training and validation data and retrain for $n_E$ epochs to get the final model. However, when concatenating the data I have more batches in the resulting training set and I am concerned for overfitting and that there is a lot of variance in the epoch number $n_E$ . Despite this, I think that I will loose some relevant new information if I do not retrain on the concatenated data set, or if I leave out a final test set. I think cross validation will not be feasible due to the time it takes to train the model. Any suggestion of a robust way of performing the training? Appreciate any help that can make my model perform when in production
