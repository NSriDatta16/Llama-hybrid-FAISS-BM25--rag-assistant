[site]: datascience
[post_id]: 22470
[parent_id]: 
[tags]: 
Python implementation of cost function in logistic regression: why dot multiplication in one expression but element-wise multiplication in another

I have a very basic question which relates to Python, numpy and multiplication of matrices in the setting of logistic regression. First, let me apologise for not using math notation. I am confused about the use of matrix dot multiplication versus element wise pultiplication. The cost function is given by: $J = - {1\over m} \sum_{i=1}^m y^{(i)}log(a^{(i)})+(1 - y^{(i)})log(1-a^{(i)})$ And in python I have written this as cost = -1/m * np.sum(Y * np.log(A) + (1-Y) * (np.log(1-A))) But for example this expression (the first one - the derivative of J with respect to w) ${\partial J \over{\partial w}} = {1 \over{m}} X(A-Y)^T$ ${\partial J\over{\partial b}} = {1\over{m}} \sum \limits_{i = 1}^m (a^{(i)}-y^{(i)})$ is dw = 1/m * np.dot(X, dz.T) I don't understand why it is correct to use dot multiplication in the above, but use element wise multiplication in the cost function i.e why not: cost = -1/m * np.sum(np.dot(Y,np.log(A)) + np.dot(1-Y, np.log(1-A))) I fully get that this is not elaborately explained but I am guessing that the question is so simple that anyone with even basic logistic regression experience will understand my problem.
