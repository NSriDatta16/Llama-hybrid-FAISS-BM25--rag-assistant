[site]: datascience
[post_id]: 122450
[parent_id]: 
[tags]: 
Large Language model for regression on urls

I am trying to fit a BERT model for a URL regression task. I have a URL as a feature and I have to predict a metric M for it. Keeping a learning rate like $10^{-5}$ , the model is overfitting in about 3-4 epochs (The validation loss goes low for only the first 3-4 epochs and it does not go much lower than where it was after first epoch), no matter how large the data is (I have tried more than a hundred thousand data points). The validation loss starts going up after 3-4 epochs. If I let it overfit for 10-15 epochs, it can give good predictions on the training dataset. Decreasing the learning rate makes it overfit in more epochs but in the end but the validation loss doesn't decrease after a certain value. Any suggestions on what I might be doing wrong and how I can improve the predictions?
