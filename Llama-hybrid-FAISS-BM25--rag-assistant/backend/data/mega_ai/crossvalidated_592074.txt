[site]: crossvalidated
[post_id]: 592074
[parent_id]: 591643
[tags]: 
Let's think in terms of bias and variance for a second. Let's assume there is no confounding so that bias and variance refer only to the properties of the model. By using a sufficiently flexible model like a Random Forest, we reduce our estimate's bias at the cost of variance. But variance can be combatted by using more data, and there will come a point where using more data does not appreciably change the model estimates. Its hard to pinpoint when that happens, but we know that variance should (in principle) decrease like $1/n$ ( $1/\sqrt{n}$ if we're talking standard error). All this to say, you don't need all the data, you just need enough . How do you define enough? Hard to say. One way you could do this is to subsample as you've mentioned and just see how the predicted risks vary sample to sample. This is a direct estimate of the variability in the predicted risk due to sampling. If after dozen samples or so you see the variation in risk is minimal (e.g. is there a qualitative difference between predicting p = 0.49 and p=0.5? It depends) then you can be confident that the size of the sample results in relatively stable predictions. Then, take a simple random sample from your data, fit your model, and evaluate it using whichever method you choose. I've purposefully left out comments on fixing the relative frequency of cases to non cases to be something other than what is observed naturally. That question has been addressed before, and I don't feel like rehashing it. You can simply search for "imbalanced data" here and a bunch of answers will come up.
