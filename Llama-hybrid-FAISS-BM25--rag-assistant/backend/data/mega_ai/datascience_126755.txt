[site]: datascience
[post_id]: 126755
[parent_id]: 
[tags]: 
Strategies for Encoding Large Datasets in Symbolic Music Generation for BERT-type Model

I am creating a BERT-type model for symbolic music generation. An observation of my database is a musical piece. Actually, is a "viewpoint" of the piece: (note spell, duration with respect to a quarter note) : e.g. ("A", 1.5), ("B", 1)... So each piece is represented by a sequence of tuples. The process I am following is basically this: clean-up, pre-tokenization, tokenization, model training and generation. In the "pre-tokenization", apart from other things, I am "encoding" each piece, meaning that I change each "event" (e.g. ("A", 1.5)) by a simple character (e.g. "a"). I do this process manually, saving in a dictionary the mapping between events and characters, so that later I can revert it. After this encoding, I tokenize the database. I am using BPE tokenization, through youtokentome package. I have an issue with the encoding done in the "pre-tokenization" stage: for big databases, I run out of symbols (i.e. I have already used all ASCII characters, but I would need more to account for all distinct elements in the database). What can I do? I could use two characters (e.g. "1a", "1b", ..., "2a", "2b", ...) but I fear that this may pose a problem because the tokenizer sees two characters (e.g. "2a") and may tokenize as, for example, and , when in truth those two tokens do not make sense as they correspond to a single event. Can this happen? What alternatives do I have? and perhaps more importantly, Is the "encoding" I am doing really necessary? Or would the tokenizer processes well a string of the type "('A', 1.5), ('B', 1), ..."?
