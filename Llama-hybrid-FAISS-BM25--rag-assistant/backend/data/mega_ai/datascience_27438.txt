[site]: datascience
[post_id]: 27438
[parent_id]: 27421
[tags]: 
Back-propagation technically refers to computing the gradient of the loss function with respect to the parameters. According to Section 6.5 of the Deep Learning book: The term back-propagation is often misunderstood as meaning the whole learning algorithm for multi layer neural networks. Actually, back-propagation refers only to the method for computing the gradient, while another algorithm,such as stochastic gradient descent, is used to perform learning using this gradient. The weights are updated right after back-propagation in each iteration of stochastic gradient descent. From Section 8.3.1: Here you can see that the parameters are updated by multiplying the gradient by the learning rate and subtracting. The SGD algorithm described here applies to CNNs as well as other architectures.
