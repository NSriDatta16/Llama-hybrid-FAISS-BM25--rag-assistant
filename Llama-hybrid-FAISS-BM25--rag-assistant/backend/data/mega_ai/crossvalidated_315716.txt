[site]: crossvalidated
[post_id]: 315716
[parent_id]: 
[tags]: 
Why do we typically characterize a stochastic process by its mean and covariance? How else could we?

I have three questions below. Please also feel free to point out flaws in my understanding! A time series of $N$ samples $(x_1, x_2, \cdots, x_N)$ is a realization of some discrete-time stochastic process $\{X_t\}$ indexed by $t(=1, 2, \cdots, N)$. In this case, $(X_1, X_2, \cdots, X_N)$ is a random vector that is fully described by the $N$-dimensional joint distribution function $F(x_1, x_2, \cdots, x_N)$. Question 1: Why can't we just talk about the random vector $(X_1, X_2, \cdots, X_N)$ and its joint distribution to characterize the sample that we observed? When and why do we need to resort to the concept of 'stochastic process'? To fully characterize the stochastic process one would need to specify $F$. But in reality this is too difficult. Question 2: Is it too difficult to specify $F$? Why exactly? So we typically resort to 'second-order characterizations'. That is, we specify all (possibly) joint moments of $F$ of order up to $2$. This turns out to be: (order 1) means: $\mu(t) = E[X_t]$ for all $t$ (order 2) covariance: $\gamma(r, s) = E[(X_r - E[X_r])(X_s - E[X_s])]$ for all $r, s$ If $\{X_t\}$ is a Gaussian process, then a second-order characterization also fully specifies $F$. So then we are done. Question 3: But, what if $\{X_t\}$ is not Gaussian? What information is lost by a second-order characterization? What are some techniques to retain that information? I have briefly read about copulas and suspect they are useful in this case, since they allow to model the marginal distribution (thus non-Gaussian features like skewness, heavy-tailedness) and dependence seperately. Also heard somewhere that higher-order moments are useful. But I have nothing tangible to follow-up on!
