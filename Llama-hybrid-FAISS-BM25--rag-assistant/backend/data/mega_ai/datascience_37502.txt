[site]: datascience
[post_id]: 37502
[parent_id]: 26396
[tags]: 
Exploding gradients are very common with LSTMs and recurrent neural networks because when unfold, they translate in very deep fully connected networks (see the deep learning book and more particularly section 10.7 The Challenge of Long-Term Dependencies for the problem of vanishing/exploding gradients). In order to visualise the weights with Tensorflow, the best method is to use tensorboard and plot the histogram and distribution of the weights during learning. For a general idea on how to do it with tensorflow (I guess it is possible to do as well by applying the same principles with tflearn, although I never used it myself), check this and this More specifically for LSTMs, here . And you'll probably need something to help you interpret it, here . Hope it helps!
