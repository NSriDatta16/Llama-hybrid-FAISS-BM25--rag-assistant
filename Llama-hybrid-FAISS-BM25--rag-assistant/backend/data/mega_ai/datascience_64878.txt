[site]: datascience
[post_id]: 64878
[parent_id]: 
[tags]: 
How to distinguish informative and non-informative feature - Feature importance?

I have a dataset with 5K records focused on binary classification problem. I have more than 60 features in my dataset. When I used Xgboost , I got the below Feature Importance plot. However I am not sure how to find out whether all of these are informative? Questions 1) Yes, I can select top 15/20/25 etc. But is that how it is done? Is there any minimum F-score that we ought to look for? 2) Or is it like I select top 10 features, check for accuracy and again add 2-3 features in every round and verify accuracy manually. Is this how it is done? 3) How would you people go about it? I tried with full dataset, the accuracy is only about 86%. when I tried with 15-20 features, it's only about 84. So manual feature selection is the only way to improve further? Can you help me please?
