[site]: datascience
[post_id]: 41682
[parent_id]: 41663
[tags]: 
The problem is that the model has no idea what the image should look like outside of the 128x128 crop you feed it with, so it fails to extrapolate appropriately. One simple fix is to train the model on bigger (and overlapping!) crops, like 144x144, and then crop out 8 pixels from each side. However, even this approach does not guarantee absence of any visible edges, as it's hard for the model to enforce this "boundary consistency" without direct access to its output from the previous patch. This could be approached with more powerful decoders, like PixelRNN / PixelCNN
