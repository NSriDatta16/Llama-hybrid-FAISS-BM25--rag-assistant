[site]: crossvalidated
[post_id]: 474258
[parent_id]: 
[tags]: 
Fitting models to data with exponential range

I want to create a regression model of data with exponential range (y values 2,4,8,...,512). X values are vectors of dozens...hundreds of parameters, and some of them clearly have nonlinear relation to y, some of them do not. My main interests are the low values (x* = argmin y) that I want to be able to predict accurately, whereas in the high range I don't care if the value is 512 or 256. I have tested several different models (e.g. random forest regression, different neural network implementations, and Gaussian process regression) with and without transforming y values to log2(y) (which creates heteroscedastic variation), but the models persistently fail to predict the low values accurately. Here is an example of how the models tend to behave (this is not real data but an example I made up to illustrate how the low y values tend to have higher relative error than the intermediate/high values): Why the models behave like that, seemingly regardless of the model type I choose? What are either the most appropriate models for this kind of data, or methods for transforming it/weighing it/redefining model training procedure to improve accuracy in the low value region?
