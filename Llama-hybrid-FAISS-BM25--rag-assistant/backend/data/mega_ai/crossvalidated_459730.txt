[site]: crossvalidated
[post_id]: 459730
[parent_id]: 350211
[tags]: 
It depends on how you assume the model for the likelihood. In other words, in variational autoencoders you seek to minimize the ELBO (empirical lower bound), which contains $KL(q||p)$ which is managed by the encoder and a second term known as the reconstruction error $E_{q}[log p(x|z)]$ managed by the decoder and requires sampling, here is where the choise of the model of $p(x|z)$ comes into play. If you assume it follows a normal distribution you will end up with a MSE minimization since $p(x|z)$ can be reformulated as $p(x|\hat{x}) \sim \mathcal{N}(\hat{x},\sigma)$ , if you assume a multinoully distribution you will use cross entropy. Just a side note taken from Goodfellow's book: Many authors use the term “cross-entropy” to identify specifically the negative log-likelihood of a Bernoulli or softmax distribution, but that is a misnomer. Any loss consisting of a negative log-likelihood is a crossentropy between the empirical distribution defined by the training set and the probability distribution defined by model. For example, mean squared error is the cross-entropy between the empirical distribution and a Gaussian model.
