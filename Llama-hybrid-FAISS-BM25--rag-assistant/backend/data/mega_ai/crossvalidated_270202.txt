[site]: crossvalidated
[post_id]: 270202
[parent_id]: 265123
[tags]: 
Interestingly, the more general concept than expected value is location . Thus, the concept of expected value has subtle implications that are somewhat confusing. It is reasonable to question what it means to have 3.5 as anything to do with an anticipated outcome for a die. The answer is that although the average value of rolled dice outcomes is 3.5, that the expected value concept only signifies mean or average value, and is only an expectation for a limited class of functions, that specific to the question here does not include die roll outcomes. To put it another way, although the average roll outcome is 3.5, So what? True enough, one can invent a context (in some alternate universe), where an average value has meaning, but, die outcomes $\leq 3$ pays $\$1$, and outcomes $\geq 4$ loses $1, works as well as an average, with the advantage of actually having outcomes in this universe. The reason for the inordinately restricted association between the term "expected value" and "mean value" appears to be historical rather than semantically correct, or even particularly cogent. That is, the context in which a calculated expected value is consistent the expectation of a location characterizing behaviour in a data set is limited to only certain distributions of data, and not others. That this is historical is supported by the notion of statistical moments. It is widely acknowledged that the first proof of the central limit theorem up to modern standards of rigor was given by Chebyshev in 1887. His argument introduced the method of moments. . Now the first moment of $f$ was, for Chebyshev, the mean value of a Borel set . The concept of a mean value being thus an expected value for the normal distribution, that is, the density function to which the central limit theorem applies is thus tracable to Chebyshev 1887. Such is the strength of the central limit theorem that it became a parenthetical expression to associate expected value with a mean value, as opposed to a more general measure of location. But what about data distributions that are not normal for which other measures are more stable and/or more representative of that data? For example, the mid-range value or average extreme value of data from a uniform distribution is more accurate and stable, i.e., precise and converges faster than the mean or median of that distribution. For log-normal distributions, e.g., (much of the treatment of) income data, the anti-log of the mean of the logarithm of data (A.K.A. geometric mean , e.g., moderate income data), rather than the data mean (e.g., mean income) itself, would be more indicative of what an individual thinking (or anticipated datum) to become inserted into that data might have as an anticipated outcome. That this is well-known is illustrated by the phrase, "I am anticipating a 5-figure salary." Here is an example of this for actual incomes. Another example, Pareto distributions, also used for income calculations, ( see 80/20 law , and high income data ) often have an undefined expected value (infinite first moment of $\alpha \beta ^{\alpha } t^{-\alpha -1}$ when $\alpha \leq 1$), such that for such distributions, it would be a mistake to anticipate an outcome to be an expected value. In that case, see Pareto distribution , the median, geometric mean, and harmonic mean are better measures of location, not only because the $\alpha \leq 1$ requirement is removed, but also because they are less variable even when $\alpha \gt 1$. Further information is found here in Clauset A, Shalizi CR, Newman ME. Power-law distributions in empirical data. SIAM review. 2009;51:661-703 , and here .
