[site]: crossvalidated
[post_id]: 225164
[parent_id]: 224483
[tags]: 
Your precise questions: Assuming they did standard stuff: Yes, their overall estimate would be the time series average of regression coefficients estimates for each time period. The standard error of this estimate is not the time series average of the regression standard errors, you don't use them at all! Instead, read my full answer to see how you compute standard errors for the Fama-Macbeth procedure. Background: Two stylized facts about stock market returns: There's huge cross-sectional correlation. If Home Depot goes up, probably so does Lowes. In fact, probably so do seemingly unrelated stocks like Intel. Control for industry, region etc... and residuals are still cross-sectionally correlated. There's little autocorrelation (i.e. there's little correlation in returns over time). Implications for analyzing Panel Data: When running some panel regression: $$ r_{i,t} = {\mathbf{x}}_{i,t} \cdot {\bf{b}} + \epsilon_{i,t}$$ The $\epsilon_{i,t}$ are generally assumed to be independent across time (and that's pretty much OK), but if you don't take into account cross-sectional correlation within a time period ($E[\epsilon_{i,t} \epsilon_{j,t}] \neq 0$), your standard errors will be horribly wrong! Two general approaches to account for cross-sectional correlation: Cluster standard errors by time. Fama-Macbeth procedure (at first seems odd but once you think about it, it's incredibly intuitive). To do Fama Macbeth, you first run a cross-sectional regression each time period, producing a time series of estimates $\{\hat{\bf{b}}_t\}$. If each time period is independent, then we can then use the extremely basic techniques we all learned in Statistics 1: use the sample mean as an estimator. Sample mean: $$\hat{b} = \frac{1}{T} \sum_t \hat{b}_t$$ Sample standard deviation: $$\sigma = \sqrt{\frac{1}{T-1} \sum_t \left( \hat{b}_t - \hat{b}\right)^2 }$$ Standard Error of sample mean: $$ SE = \frac{1}{\sqrt{T}} \sigma $$
