[site]: datascience
[post_id]: 113758
[parent_id]: 
[tags]: 
KNN using Mahalanobis distance gives low score

I want to get average score of all possible K but the average accuracy I'm getting is much lower than what's given to me. avg_acc = 0 best_acc = 0 for i in range(1, 10): avg_k_acc = 0 X_train, X_test, Y_train, Y_test = train_test_split( x, y, random_state=50, test_size=i * 0.1 ) for k in range(1, len(X_train)): clasifier = KNeighborsClassifier( n_neighbors=k, algorithm='brute', metric='mahalanobis', metric_params={ 'VI': np.cov(np.linalg.inv(np.cov(X_train.T)), rowvar=False) } ) clasifier.fit(X_train, Y_train) y_pred = clasifier.predict(X_test) for z in y_pred: if z == 0: z = 1 else: z = 0 cm = confusion_matrix(Y_test, y_pred) acc = f1_score(Y_test, y_pred) avg_k_acc = acc + avg_k_acc avg_k_acc = avg_k_acc / len(X_train) avg_acc = avg_acc + avg_k_acc print(avg_acc / 9)
