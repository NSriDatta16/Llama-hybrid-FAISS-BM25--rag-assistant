[site]: datascience
[post_id]: 19233
[parent_id]: 19119
[tags]: 
As you already noticed, Topic Models aren't reproducible due to the probabilistic algorithm. I did some topic models on a large corpus (10k documents, 30M words), and generally, between two different topic models about 10% of the topics were incomparable. My corpus is mainly English, but contains some Latin texts, and for a given number of topics I had between 1 and 3 topics consisting of Latin language words predominantly. What to do now depends on what you want to learn from your Topic Model. When you want to "interpret" the Topic Model (using methods from the humanities), the best thing is to commit to one topic model (i.e., fixing the random seed). When you want to study some derived measures (like topic diversity), make sure that the derived measures taken from different topic models are "stable", i.e., centred around one value with small statistical fluctuations. You can also try to create an ensemble of topic models and try some techniques to get an "average topic model" out of it. I have no experience yet in this business and cannot suggest techniques here.
