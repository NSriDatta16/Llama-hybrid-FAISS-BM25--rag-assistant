[site]: crossvalidated
[post_id]: 400407
[parent_id]: 
[tags]: 
Sampling Big Data for Machine Learning

In practice, how does one go about sampling a from big data set (eg. +/- 50 million distinct observations) to perform ML using Python? Most non-parametric models (e.g., SVM, ensemble models) start to push computer resources with much smaller sets (e.g., 200 features, 200K observations). How is this done in practice in industry? Other questions here get at this but don't explicitly ask. So this is not a duplicate. Thanks in advance.
