[site]: crossvalidated
[post_id]: 392309
[parent_id]: 392234
[tags]: 
When speaking about standard convolutional layers in CNNs, the kernel will be of spatial size K x K and have the same depth as the input layer, while the number of kernels used in the layer will determine the depth of the output. For example: Say your input is of size 10x10x3 (10x10 RGB image) and your kernel has a spatial size of 4x4, so the full size of the kernel will be 4x4x3 and have 49 trainable parameters (4x4x3 + one for the bias term). Now to calculate the output, you take spatial strides with the kernel's tensor over the input's tensor, performing element-wise multiplication at every position and adding the bias term at the end, so for every spatial position you will have: And for every position you will need to perform non-linear activation like ReLU so: Lets say you use a stride of a single pixel in each of the 2 spatial directions and you do not use any padding so the final output of your layer for this kernel will be 6x6x1 , meaning you need to use those same 49 parameters at 36 different spatial positions of the input. Now you will probably want a layer with a depth of more than 1, so lets say you have 5 such kernels, each with its own 49 parameters, summing to a total of 245 trainable parameters. You need to repeat the above process independently for each of the 5 kernels and the final output of the layer will be of size 6x6x5 . There are also models that use strides in the channel dimension as well, but those are considerably less common. When you want to connect the output of a convolutional layer to a fully-connected layer, you can simply flatten it to a single vector. So in our example, you will get a flattened vector of size 180 (6x6x5).
