[site]: datascience
[post_id]: 23345
[parent_id]: 23344
[tags]: 
Is there any better approach apart from writing custom web scraper/parser? Probably not. Although this problem might be tractable using machine learning and natural language processing techniques in future, it would be far more work, and a much larger challenge, than the typical 80/20 rule you get from a web scraper plus regular expressions. E.g. you write a web scraper with your best guess at the rules for extracting names. It finds 800 names correctly out of 1000. You analyse the problem sites, and add custom CSS selectors and tweak some of the regular expressions. That finds 150 of the remaining 200. Repeat the analysis and fix, you are down to maybe 10 problem sites, each requiring an individual fix. It is dull, repetitive work. The alternative of attempting this problem using machine learning and NLP: First get the true labels for 10,000+ sites . . .
