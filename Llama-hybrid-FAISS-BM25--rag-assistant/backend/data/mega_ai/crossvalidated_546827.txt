[site]: crossvalidated
[post_id]: 546827
[parent_id]: 
[tags]: 
Can the sufficient statistics simplify Bayesian model selection?

When selecting a model within the Bayesian framework we can choose the model having highest probability given the data. This is proportional to the marginal likelihood or evidence for the data when we assume equally likely prior probabilities on the models. The integration over a particular model's parameters to calculate the evidence associated with that model can be difficult. Rather than use the evidence for the data, wouldn't it be easier to use the 'evidence' for the sufficient statistics associated with the data under that model? e.g., rather than integrate over the mean and precision parameters of a Gaussian model for the evidence of a particular observed data set, why not use the empirical mean and covariance of that data set (i.e., it's sufficient stats under this model) and maximize that match? Here the parameters are the sufficient stats so no integration is required, the prior distribution is the distribution of the sufficient stats, and the prior model selected is the one that maximizes the probability of the empirical mean and cov (MAP for sufficient stats). This seems a natural solution to my intuitive question: What about the observed dataset is selecting the best model(prior)? Doesn't the data inform the model (parameters) through it's sufficient stats? And then wouldn't the prior(model) thereby selected be the simplest, selected via this MAP procedure, be a Dirac delta function of hyper-parameters (mean, cov) = (m_MLE, 0)? See also Models vs parameters in Bayes models .
