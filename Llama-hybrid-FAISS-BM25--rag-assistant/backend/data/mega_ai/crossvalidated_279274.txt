[site]: crossvalidated
[post_id]: 279274
[parent_id]: 279268
[tags]: 
When you perform $k$-fold cross validation, you split the data equally and randomly into $k$ splits. Now you, Take $i^{th}$ split as validation set, and combine the rest $k-1$ splits Train on the $k-1$ splits combined, test on the validation set Do this for $i = 1,..., k$ and note the average error. Repeat all these steps for each potential set of features, and then choose the set that gave you the lowest average error. Note that this requires you to go through $2^n$ combinations, where $n$ is the total number of features. If you can assume independence among the features, you can select them in a greedy fashion: you start with choosing just one feature. See which one among the $n$ features gives you the lowest error, and then keeping that constant, add one more from the remaining, do this $n-1$ times for the $n-1$ remaining features, and so on, until the error either doesn't decrease anymore, or the decrease is too low to offset the cost of increasing your feature space.
