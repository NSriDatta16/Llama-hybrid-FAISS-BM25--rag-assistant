[site]: crossvalidated
[post_id]: 457758
[parent_id]: 457726
[tags]: 
If you want to include prior information into your model, using bayesian model is the standard way to go. In Bayesian setting, ridge regression is equivalent to using Normal priors . In this thread you can find an introduction to Bayesian linear regression. As about priors, using $\mathcal{N}(0, 10)$ prior for the parameters is equivalent to imposing your prior knowledge. On another hand, if you knew that the bounds are hard, i.e. parameters cannot go beyond them, you'd use a bounded prior, e.g. truncated normal distribution. Be warned however that the Bayesian formulation of ridge regression does not behave exactly the same, there were studies showing that using stronger priors may be needed to achieve the regularization effect.
