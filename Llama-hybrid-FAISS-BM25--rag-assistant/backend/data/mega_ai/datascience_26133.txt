[site]: datascience
[post_id]: 26133
[parent_id]: 26129
[tags]: 
Without reading your code (sorry!) I can suggest that you drop the verbs-only approach and use a word embedding to account for word similarity. Your feature dictionary is bound to find very few "hits" in the documents, unless you have a humongous dataset, and even then YMMV because you are just using verbs. Also: what's the justifications for using verbs? "I liked" and "I didn't like" both contain the verb "to like" and they mean opposite things. Same goes for hate (because you only have binary classification: no nuances) and so on. A popular python implementation of word2vec is gensim , but you could use that of tensorflow or some other embedding like the (allegedly superior) conceptnet numberbatch . If you want to summarize whole documents into numbers you can try doc2vec (aka paragraph2vec, paper here ), also available in gensim, tensorflow, etc. You will need a big dataset. Once you have this up and running you can try other classifiers.
