[site]: datascience
[post_id]: 89681
[parent_id]: 49468
[tags]: 
In attention mechanisms, you take an expectation of a representation of data V with respect to some probability mass function, thus computing the context vector, which is essentially a summary statistic (weighted mean) of your data: \begin{align} c&=\mathbb{E}_p[V] \end{align} The big question is how you determine the elements of $p$ , the probability vector. In classic attention, you have a domain $S$ , which is a set of locations. Let's say we are in a language setting, in which case we have a sequence $X=\{x_1,\cdots,x_n\}$ of vector representations of words and we have domain $S=\{1,\cdots,n\}$ . The weights $p$ are $p(t|X)$ . For example, assume we have the sentence 'He ate all of the pies.' and we investigate $t=3$ . The attention weight for $t=3$ essentially ask how important position $3$ is, given the whole sequence. Self attention instead has the domain $S$ include both location/positional and observation value information, and instead of conditioning on the entire sequence, asks about the importance of one word (and its position), conditional on some other word and its location (vector representations of both), a query. This is $p(x_k|x_q)$ . So instead of asking, 'how important is position $3$ given the sequence?', it asks 'how important is "all" at position $3$ given we have "pies" at position $6$ '? Essentially you're replacing conditioning on the entire sequence with pairwise comparisons.
