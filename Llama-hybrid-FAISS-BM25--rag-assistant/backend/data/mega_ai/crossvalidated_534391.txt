[site]: crossvalidated
[post_id]: 534391
[parent_id]: 532822
[tags]: 
We are comparing the $Y$ -values predicted by the full and partial models for a specific data point that has a specific set of values for the predictors $X_1$ , $X_2$ , and $X_3$ : For this data point the difference between the predicted values is the difference between the specific contribution of $X_1$ to the expected $Y$ -value for that data point ( $x_{1} \hat{\beta}_1$ ), and the average contribution of $X_1$ to the expected $Y$ -values across all data points ( $\bar{x}_1\hat{\beta}_1$ ). This means that predictions from the full and partial models will be quite similar for those data points where the value of $X_1$ is close to its average. The further $X_1$ is from its average, the larger the difference between the two predictions. At least the above statement is true if we assume that the omitted variable, $X_1$ , has zero correlation 1 to the included variables, $X_2$ and $X_3$ . In this case the estimates of the slope parameters $\beta_2$ and $\beta_3$ will be identical for the partial and full models 2 . However, the estimates of the intercept parameter, $\alpha$ , will differ. Specifically the $\alpha$ estimate for the partial model will include the average effect of the omitted variable (in the equations below I use the subscript "Partial" for parameters from the partial model, and no subscript for parameters from the full model): $$ \begin{aligned} &\hat{\alpha}_{Partial} = \hat{\alpha} + \bar{x}_1\hat{\beta}_{1}\\ &\hat{\beta}_{2,Partial} = \hat{\beta}_{2}\\ &\hat{\beta}_{3,Partial} = \hat{\beta}_{3} \end{aligned} $$ Here $\bar{x}_1$ is the mean of the variable $X_1$ in the sample that is being examined. Given these relationships we can derive an expression for the difference between the predicted values for the full and partial models: $$ \begin{aligned} \Delta_{Full,Partial} &= \hat{y} - \hat{y}_{Partial}\\ &= \hat{\alpha} + x_{1} \hat{\beta}_{1} + x_{2} \hat{\beta}_{2} + x_{3} \hat{\beta}_{3} - \hat{\alpha}_{Partial} - x_{2} \hat{\beta}_{2,Partial} - x_{3} \hat{\beta}_{3,Partial}\\ &= \hat{\alpha} + x_{1} \hat{\beta}_{1} + x_{2} \hat{\beta}_{2} + x_{3} \hat{\beta}_{3} - \hat{\alpha} - \bar{x}_1\hat{\beta}_{1} - x_{2} \hat{\beta}_{2} - x_{3} \hat{\beta}_{3}\\ &= x_{1} \hat{\beta}_{1} - \bar{x}_1\hat{\beta}_{1} \end{aligned} $$ Thus, as stated above, the difference in predictions for a given data point is the difference between the specific contribution of $X_1$ to $\hat{Y}$ for that data point, and the average contribution of $X_1$ to $\hat{Y}$ across all data points. Essentially, the partial model "knows" nothing about the specific value of $X_1$ for any given data point, and can therefore not account for the specific contributions of this variable. Instead the partial models adds the average contribution of $X_1$ (across all data) to all predictions. The full model, however, will know exactly how much $X_1$ contributes to the exptected value of $Y$ for each data point. 1 Note: this means zero correlation in the specific sample that is being examined. It does not matter if the underlying population parameters are correlated or not. Also note that you are unlikely to have zero sample correlation for real-world data, except in cases where the experimenter has control of the independent variables and their values have been chosen explicitly to be uncorrelated. Nevertheless, looking at the situation with zero correlation is helpful in getting some intuition about the meaning of the difference in predictions. 2 For instance, see: Eric A. Hanushek and John E. Jackson, Statistical Methods for Social Scientists, Academic Press, 1977, section 4.3. Also: The Phantom Menace: Kevin A. Clarke, Omitted Variable Bias in Econometric Research, Conflict Management and Peace Science, 22:341-352, 2005
