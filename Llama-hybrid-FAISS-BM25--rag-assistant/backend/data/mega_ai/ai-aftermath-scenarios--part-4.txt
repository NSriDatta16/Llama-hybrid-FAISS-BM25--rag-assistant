dvanced technology on the human race. Boxed AI People ask what is the relationship between humans and machines, and my answer is that it's very obvious: Machines are our slaves. The AI box scenario postulates that a superintelligent AI can be "confined to a box" and its actions can be restricted by human gatekeepers; the humans in charge would try to take advantage of some of the AI's scientific breakthroughs or reasoning abilities, without allowing the AI to take over the world. Successful gatekeeping may be difficult; the more intelligent the AI is, the more likely the AI can find a clever way to use "social hacking" and convince the gatekeepers to let it escape, or even to find an unforeseen physical method of escape. Human-AI merger Kurzweil argues that in the future "There will be no distinction, post-Singularity, between human and machine or between physical and virtual reality". Human extinction If a dominant superintelligent machine were to conclude that human survival is an unnecessary risk or a waste of resources, the result would be human extinction. This could occur if a machine, programmed without respect for human values, unexpectedly gains superintelligence through recursive self-improvement, or manages to escape from its containment in an AI Box scenario. This could also occur if the first superintelligent AI was programmed with an incomplete or inaccurate understanding of human values, either because the task of instilling the AI with human values was too difficult or impossible; due to a buggy initial implementation of the AI; or due to bugs accidentally being introduced, either by its human programmers or by the self-improving AI itself, in the course of refining its code base. Bostrom and others argue that human extinction is probably the "default path" that society is currently taking, in the absence of substantial preparatory attention to AI safety. The resultant AI might not be sentient, and might place no value on sentient life; the resulting hollow world, devoid of life, might be like "a Disneyland without children". Zoo Jerry Kaplan, author of Humans Need Not Apply: A Guide to Wealth and Work in the Age of Artificial Intelligence, posits a scenario where humans are farmed or kept on a reserve, just as humans preserve endangered species like chimpanzees. Apple co-founder and AI skeptic Steve Wozniak stated in 2015 that robots taking over would actually "be good for the human race", on the grounds that he believes humans would become the robots' pampered pets. Alternatives to AI Some scholars doubt that "game-changing" superintelligent machines will ever come to pass. Gordon Bell of Microsoft Research has stated "the population will destroy itself before the technological singularity". Gordon Moore, discoverer of the eponymous Moore's law, stated "I am a skeptic. I don't believe this kind of thing is likely to happen, at least for a long time. And I don't know why I feel that way." Evolutionary psychologist Steven Pinker stated, "The fact that you can visualize a future in your imagination is not evidence that it is likely or even possible." Bill Joy of Sun Microsystems, in his April 2000 essay Why the Future Doesn't Need Us, has advocated for global "voluntary relinquishment" of artificial general intelligence and other risky technologies. Most experts believe relinquishment is extremely unlikely. AI skeptic Oren Etzioni has stated that researchers and scientists have no choice but to push forward with AI developments: "China says they want to be an AI leader, Putin has said the same thing. So the global race is on." References See also Existential risk from artificial general intelligence