[site]: crossvalidated
[post_id]: 137842
[parent_id]: 137481
[tags]: 
If you are only selecting the hyperparameter for the LASSO, there is no need for a nested CV. Hyper-parameter selection is done in a single/flat CV interaction. Given that you have already decided to use LASSO and given that you have already decided which features to keep and give to the algorithm (the LASSO will likely remove some of the features but that is the LASSO optimization not your decision) the only thing left is to choose the $\lambda$ hyperparameter, and that you will do with a flat/single CV: 1) divide the data into training\learning sets $L_i$ and test sets $T_i$ and chose the $\lambda^*$ that minimizes the mean error for all $T_i$ when trained with the corresponding $L_i$. 2) $\lambda^*$ is your choice of hyperparameter. DONE. (This is not the only method to select hyperparameters but it is the most common one - there is also the "median" procedure discussed and criticized by G. C. Cawley and N. L. C. Talbot (2010), "Over-fitting in model selection and subsequent selection bias in performance evaluation", Journal of Machine Learning Research, 11 , p.2079 , section 5.2.) What I understand you are asking is: how bad is to use the error I computed in step 1 above (the minimal error that allow me to select $\lambda^*$) as an estimate of the generalization error of the classified with that $\lambda^*$ for future data? Here you are talking about estimation not hyper-parameter selection!! I know of two experimental results in measuring the bias of this estimate (in comparison to a true generalization error for synthetic datasets) the Cawley and Talbot paper above Varna and Simon (2006), "Bias in error estimation when using cross-validation for model selection", BMC Bioinformatics , 7 , 91. both open access. You need a nested CV if: a) you want to choose between a LASSO and some other algorithms, specially if they also have hyperparameters b) if you want to report a unbiased estimate of the expected generalization error/accuracy of your final classifier (LASSO with $\lambda^*$). In fact nested CV is used to compute an unbiased estimate of the generalization error of a classifier (with the best choice of hyperparameters - but you dont get to know which are the values of the hyperparameters). This is what allows you to decide between the LASSO and say an SVM-RBF - the one with the best generalization error should be chosen. And this generalization error is the one you use to report b) (which is surprising, in b) you already know the value of the best hyperparameter - $\lambda ^*$ - but the nested CV procedure does not make use of that information). Finally, nested CV is not the only way to calculate a reasonable unbiased estimate of the expected generalizationn error. There has been at least three other proposals Ding et al. Bias correction for selecting the minimal-error classifier from many machine learning models BioInformatics 30(22) has their one proposal and compare it with two others the weighted mean correction and Tibshirani-Tibshirani procedure.(see references in the paper)
