[site]: datascience
[post_id]: 87501
[parent_id]: 87500
[tags]: 
I would use all the features and see how the separateness of my clusters behave according to some metric, for example, silhouette score Additionally, it is very important to scale your data prior to clustering since kmeans is a distance-based algorithm. heart_data = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00519/heart_failure_clinical_records_dataset.csv") from sklearn.cluster import KMeans from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.metrics import silhouette_score Features = heart_data.drop(["DEATH_EVENT"], axis = 1).columns X = heart_data[Features] sc = [] for i in range(2, 25): kmeans = Pipeline([("scaling",StandardScaler()),("clustering",KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0))]).fit(X) score = silhouette_score(X, kmeans["clustering"].labels_) sc.append(score) plt.plot(range(2, 25), sc, marker = "o") plt.title('Silhouette') plt.xlabel('Number of clusters') plt.ylabel('Score') plt.show() You could also try different combinations of features so that score is maximum For visualization purposes you can use a decomposition technique from sklearn.decomposition import PCA import matplotlib.pyplot as plt plt.style.use("seaborn-whitegrid") pca = Pipeline([("scaling",StandardScaler()),("decompositioning",PCA(n_components = 2))]).fit(X) X2D = pca.transform(X) plt.scatter(X2D[:,0],X2D[:,1], c = kmeans["clustering"].labels_, cmap = "RdYlBu") plt.colorbar(); Last but not least, I recommend to use a manifold projection such as UMAP in your data, It might help on your task by generating "well-defined" clusters (might but not necessarily, nonetheless it is worthy to try) Look, by using UMAP the results seems to improve: code: # pip install umap-learn heart_data = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00519/heart_failure_clinical_records_dataset.csv") from sklearn.cluster import KMeans, DBSCAN from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.metrics import silhouette_score from umap import UMAP Features = heart_data.drop(["DEATH_EVENT"], axis = 1).columns X = heart_data[Features] sc = [] for i in range(2, 25): kmeans = Pipeline([("scaling",StandardScaler()),("umap",UMAP()),("clustering",KMeans(n_clusters=i, init='k-means++',random_state=0))]).fit(X) score = silhouette_score(X, kmeans["clustering"].labels_) sc.append(score) plt.plot(range(2, 25), sc, marker = "o") plt.title('Silhouette') plt.xlabel('Number of clusters') plt.ylabel('Score') plt.show() from sklearn.decomposition import PCA import matplotlib.pyplot as plt plt.style.use("seaborn-whitegrid") kmeans = Pipeline([("scaling",StandardScaler()),("umap",UMAP()),("clustering",KMeans(n_clusters=3, init='k-means++',random_state=0))]).fit(X) pca = Pipeline([("scaling",StandardScaler()),("umap",UMAP()),("decompositioning",PCA(n_components = 2))]).fit(X) X2D = pca.transform(X) plt.scatter(X2D[:,0],X2D[:,1], c = kmeans["clustering"].labels_, cmap = "RdYlBu") plt.colorbar(); Plot show first and second principal components of umap projection (It is simply a projection of how all the features would look in 2D space) Colours are the cluster id. i.e. for every colour we see in which cluster the algorithm (k-means) assigned each observation to.
