[site]: datascience
[post_id]: 109747
[parent_id]: 109672
[tags]: 
This is essentially a difficult, unsolved question in deep learning for which there is no unified answer, yet. This is because firstly the architecture of a network has "uncountably" many degrees of freedom and secondly because the mathematical foundations are not developed enough in some domains. Now a little more concrete: If you have time to experiment, you could for instance do a small study: one model with few large layers one model with many narrow layers The important point is that both models should have approximately the same number of parameters. Test the performance and compare. In domains like image recognition where CNNs are used, there seems to be a tendency to use more layers with smaller kernels, but many channels. Smaller kernels decrease the number of parameters and larger channels increase the number of parameters. Smaller kernels essentially mean less multiply-adds. Equivalently for linear fully-connected layers, this would mean less neurons per layer, because this means less multiply-adds. Also keep in mind; with increasing number of neurons per layer, the matrices that need to be multiplied grow exponentially. So, it may be computation-wise smarter to start out with more linear layers with fewer neurons than vice versa. In all of the experiments, start with a very tiny (like, really tiny) model that underfits and then gradually increase the complexity of the model. This takes some experiemental effort. Do not start with an overly large model that takes hours to compute!
