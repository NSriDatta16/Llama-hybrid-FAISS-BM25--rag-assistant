[site]: crossvalidated
[post_id]: 419930
[parent_id]: 419920
[tags]: 
Gone through the paper and their methodology, they are using a technique called 'word embeddings' and 'word2vec'. For more detail, here is an example below: Suppose we have a sentence at the encoder like: I will have an apple juice. For this sentence, each word will be associated with index id in the dictionary. Encoder network will learn the importance of each vector by creating a table with all the input words. On the base of the importance, it will now learn the to predict the input sentences. Suppose, another sentence came like I will have an orange ______. Now, network will compare with the table and find that orange and apple have the same importance. So it will automatically fill the blanks with the ' juice ' word. This is what they are saying according to conditional probability, that if already they have an input vector x and y is the predicted output sentence. Then P(y|x) , probability of a output sentence 'y' will be calculated directly looking upon 'x' which is previously enter in the network. Furthermore, they are using Trigrams technique which is predicting the ith word in a sentence with the help of its neighbor words (i.e. i-1 and i+1). This is a very useful technique in machine translation applications where the input source data is full of grammatical mistakes. This can learn the words through the neighborhood and then provide the context of the source sentence. You can go through this link for further details on this: https://www.tensorflow.org/tutorials/representation/word2vec
