[site]: crossvalidated
[post_id]: 519481
[parent_id]: 519477
[tags]: 
I would generally avoid leave-one-out cross-validation for performance evaluation as it tends to have a high variance. I use it a lot for model selection (e.g. hyper-parameter tuning) because for many models it can be evaluated very cheaply. Whether bias is important depends on what you want the performance estimate for. If you just want to choose between competing models, then the bias is irrelevant if it is essentially the same for all models. Jacques Wainer and I investigated the choice of resampling techniques for model selection and found it really didn't make much difference in practical terms. Jacques Wainer and Gavin Cawley, "Empirical Evaluation of Resampling Procedures for Optimising SVM Hyperparameters", Journal of Machine Learning Research 18 (2017) 1-35 ( pdf ) I tend not to bother looking at the performance in each fold of the cross-validation, I've never found it very useful. For classification problems, misclassification error is a fairly brittle metric and small changes in the model can produce comparatively large changes in performance. While performing additional re-samplings (e.g. repeated k-fold cross-validation) will reduce the standard error of the mean, it won't change the variance of the folds themselves. So it may give a more precise estimate of performance, but it won't necessarily produce a useful improvement in performance. This is because for small and medium sized datasets, the difference in the simple k-fold cross-validation performance and the refined repeated k-fold cross-validation may be too small to be of practical significance, even if it is statistically significant (however this does depend on misclassification costs, if you are performing a medical screening test a false-negative might cost someone their life!). There is also the point that it is possible to over-fit the model selection criterion if there are too many choices to be made. A more reliable, less noisy estimate may be less susceptible to over-fitting. FWIW I tend to use leave-one-out or 5- or 10-fold cross-validation for model selection purposes and repeated split-sample validation for performance estimation (where the cross-validation for model selection is nested in each iteration of the performance estimation). Repeated k-fold cross-validation seems to have an inelegant asymmetry, which is probably irrelevant, and it seems just as easy to perform the same number of truly random test-training splits of the same sizes. I also like bootstrapping for performance estimation, especially when using bagging as a "belt-and-braces" approach to avoiding over-fitting and it gives an out-of-bag estimate for the ensemble.
