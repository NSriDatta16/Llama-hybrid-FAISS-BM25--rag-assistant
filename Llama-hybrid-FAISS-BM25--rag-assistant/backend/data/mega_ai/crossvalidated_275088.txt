[site]: crossvalidated
[post_id]: 275088
[parent_id]: 241723
[tags]: 
The Bengio et al article " On the difficulty of training recurrent neural networks " gives a hint as to why L2 regularization might kill RNN performance. Essentially, L1/L2 regularizing the RNN cells also compromises the cells' ability to learn and retain information through time. Using an L1 or L2 penalty on the recurrent weights can help with exploding gradients. Assuming weights are initialized to small values, the largest singular value $\lambda_1$ of $W_{rec}$ is probably smaller than 1. The L1/L2 term can ensure that during training $\lambda_1$ stays smaller than 1, and in this regime gradients can not explode. This approach limits the model to single point attractor at the origin, where any information inserted in the model dies out exponentially fast. This prevents the model to learn generator networks, nor can it exhibit long term memory traces.
