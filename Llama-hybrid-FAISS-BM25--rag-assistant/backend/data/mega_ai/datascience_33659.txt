[site]: datascience
[post_id]: 33659
[parent_id]: 
[tags]: 
Difference between Sum of Squares and Maximum Likelihood Linear Regression

I'm new in Machine Learning and one of the first arguments I'm studying is linear regression. I understood that , in few words , the idea to use the Linear Regression is to learn an hypothesis that can map a new input x in a good approximation of y. In order to do this , if my hypothesis is : h(x) = wx + w0 I have to update my parameters minimizing an error function like Least Squares and optimize the w vector with the help of an optimization algorithm like Gradient Descent. I understood how this works , but sometimes I see this "Maximum Likelihood Estimation" and I did not understand if it is another way to estimate the w parameters or something else.
