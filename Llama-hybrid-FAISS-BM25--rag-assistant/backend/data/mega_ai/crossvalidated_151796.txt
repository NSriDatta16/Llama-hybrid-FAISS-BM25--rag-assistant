[site]: crossvalidated
[post_id]: 151796
[parent_id]: 151730
[tags]: 
There's a couple of these questions floating around that essentially have the same answer. There are essentially two approaches to testing what are model constraints. Given a regression $Y = \beta_0 + \beta_1X_1 + \beta_2X_2 $, you can 1. Modify ("constrain") the regression structure and perform some kind of test Write out what you want to test and substitute into the regression formula , e.g. $\beta_1 = \beta_2$ which substitutes in as $Y = \beta_0 + \beta_1 X_3 $ where $X_3 = X_1 + X_2$ You now have two models, the original and restricted, and you perform a likelihood ratio test between the two. This is the method discussed by @Sid and @Analyst using lratiotest Alternatively in the same spirit: $\beta_1 = \beta_2$ equivalent to $\beta_1 - \beta_2 = 0$ which can be rewritten as $\beta_1 - \beta_2 = \alpha$ Substitute this back into the original regression formulation $ Y = \beta_0 + (\alpha + \beta_2) X_1 + \beta_2X_2 $ $ Y = \beta_0 + \alpha X_1 + \beta_2 X_3 $ after rearranging Here, a test of $\alpha$ is a test of $\beta_1 = \beta_2$ in the original regression. This is the method shown by @Glen_b here . This method is usable with other hypothesis as well, like $\beta_1 = 0.5$ or $\beta_1 = 3\beta_2$; just substitute as appropriate. 2. Perform what are known as "general linear hypothesis" or "regression Wald tests" after estimation. They have more names, another common one is "linear contrasts", and I've seen others as well. Once you have your vector of regression coefficients $\mathbf{B} = [\text{b x 1}] = [\beta_0, \beta_1, \beta_2]'$ and their var-covariance matrix $\mathbf{V} = [\text{b x b}] = [\text{stuff}]$ You construct a vector defining your hypothesis. So in this case, where we want to test $\beta_1 - \beta_2 = 0$, we have $H = [\text{1 x b}] = [0,1,-1]$ resulting in a hypothesis of $HB = [0,1,-1] \times [\beta_0, \beta_1, \beta_2]'$ If you wanted to test against something other than 0, then you would subtract the constant as such $HB - c$ Then you calculate the test statistic $S = (HB - c)'(HVH')^{-1}(HB -c)$ This is distributed $~\chi^2_1$ for logistic regression (in linear regression, it is an F-statistic). In MATLAB, it looks like linhyptest is what you want for this. As with #1, this is usable with any other hypothesis that involve linear combinations of the coefficients. $\beta_1 = 0.5$ or $\beta_1 = 3\beta_2$ fit easily into this framework; construct $H$ and $c$ as appropriate. I have a preference for the second method. It has the advantage of not requiring refitting a second model, reducing the work in sorting out how the second model should be formulated, especially if you have multiple hypothesis. In addition, multiple hypothesis can be tested jointly as well by "stacking" the $H$ and $c$ vectors, turning $H$ into a $\text{q x b}$ and $c$ into a $\text{q x 1}$, with the resulting test statistic having $q$ degrees of freedom instead of 1.
