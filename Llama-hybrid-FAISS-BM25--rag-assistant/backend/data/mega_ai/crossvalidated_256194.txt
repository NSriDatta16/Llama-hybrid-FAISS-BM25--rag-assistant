[site]: crossvalidated
[post_id]: 256194
[parent_id]: 256189
[tags]: 
You drop the marginal density $p(x)$ (the normalizing constant ) because it is a function of the data which are fixed (in the Bayesian context ) but that leads the posterior density $p(\theta|x)$ to lose some properties like integration to 1 (improper density )over the domain of $\theta$ but this is not a big deal since we are usually not interested in integrating the function but in maximising it , so multiplying this function with the constant does not change the $\theta$ that corresponds to the maximum point (MAP). Now given a binomial likelihood over $r$ success ($Y$ as you denote it ) in $n$ Bernoulli trials each independent and conditional on unknown success parameter $\theta \in [0,1]$ with prior density $\theta \sim Beta(\alpha,\beta)$ . If you drop the constants in the likelihood and the prior you get the kernel of beta density(the posterior ) that means the posterior is proportional (not equal )to the likelihood * prior such that $$p(\theta|r,n) \propto {\theta}^r (1-\theta)^{n-r} * {\theta}^{\alpha -1}(1-\theta)^{\beta-1} = {\theta}^{r+\alpha-1}(1-\theta)^{n-r+\beta-1 }$$ Now to make this density proper(new beta density ) we multiply it with the constant $c$ which will ensure that this posterior density integrate to 1 such that : $$p(\theta|r,n)= c{\theta}^{r+\alpha-1}(1-\theta)^{n-r+\beta-1 }$$ Note no proportionality anymore, now let $c$ be as follows $$c=\frac{\Gamma(n+\alpha+\beta)}{\Gamma(r+\alpha)\Gamma(n-r+\beta)}$$ That means $$ \int_{0}^{1}{\theta}^{r+\alpha-1}(1-\theta)^{n-r+\beta-1 }d\theta=c^{-1}$$ $$\theta|r,n \sim Beta(\alpha+r,\beta +n-r)$$ then $E(\theta|r,n)=\frac{\alpha+r}{\alpha+n+\beta}$ .
