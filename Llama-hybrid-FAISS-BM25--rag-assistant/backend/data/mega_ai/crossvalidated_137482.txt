[site]: crossvalidated
[post_id]: 137482
[parent_id]: 137481
[tags]: 
Any complex learning algorithm, like SVM, neural networks, random forest, ... can attain 100% training accuracy if you let them (for instance through weak/no regularization), with absolutely horrible generalization performance as a result. For instance, lets use an SVM with RBF kernel $\kappa(\mathbf{x}_i,\mathbf{x}_j) = \exp(-\gamma\|\mathbf{x}_i-\mathbf{x}_j\|^2)$. For $\gamma=\infty$ (or some ridiculously high number), the kernel matrix becomes the unit matrix. This results into a model with $100\%$ training set accuracy and constant test set predictions (e.g. all positive or all negative, depending on the bias term). In short, you can easily end up with a perfect classifier on your training set that learned absolutely nothing useful on an independent test set. That is how bad it is.
