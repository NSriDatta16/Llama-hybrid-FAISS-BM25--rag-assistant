[site]: crossvalidated
[post_id]: 336885
[parent_id]: 
[tags]: 
Principal Component Analysis and input feature distribution

I am performing PCA on dataset of shape 300,1500 using scikit learn in Python 3. I have following questions in the context of PCA implementation in scikit learn and generally accepted approach. 1) Before doing PCA do I remove highly correlated columns? I have 67 columns which have correlation > 0.9. Does PCA automatically handle this correlation I.e ignores them? 2) Do I need to remove outliers before performing PCA? 2b) if I have to remove outliers how best to approach this. Using z-score for each column when I tried to remove outliers (z-score >3) I am left with only 15 observations. It seems like wrong approach. 3) Finally is there ideal amount of cumulative explained variance which I should be using to choose Principal components. In this case around 150 components give me 90% cum explained variance
