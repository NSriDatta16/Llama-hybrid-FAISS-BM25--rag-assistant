[site]: datascience
[post_id]: 106219
[parent_id]: 
[tags]: 
Improve machine learning model for small dataset

I'm trying to find the best sorting model on a small dataset (about 600 records). I can't increase this dataset because it's real data from 600 cities and that's the scope of my study. UPDATE: data contains information about cities (temperature, human development index, etc) and I'm aiming to use them for modelling the relationship with cases of a disease. The target variable is the class according to the disease incidence (for example: For model comparison I'm using accuracy and analyzing the confusion matrix. The algorithm is RandomForest I would like your opinion on what I could improve in this case. Here's what I'm doing: Separate 20% of the dataset for model validation: train_test_split(X,y, test_size=0.20, random_state=42) Run GridSearch for the other 80% and get the best gsc.best_estimator_ model I apply this model in the 20% validation and check the accuracy As you might notice, the accuracy is 0.40, which I think is very low. I've done other tests with other algorithms and combination of features and there are few variations (accuracy between 0.35 and 0.45). What do you recommend I could do in this case to try to improve? Update 2: I reviewed all the features, including some others that might be relevant to the subject of my study and excluding others that were not supported in the scientific literature. Now the accuracy is around 0.56 from GridSearch, which is far from what I would like (>0.7), but it is already a considerable improvement. I tested it with 3 different algorithms (Random Forest, MLPClassfier and XGBoost), with RandomForest being a little better than the neural network. As you can see in the new confusion matrix, class 2 is the one with the worst performance. It has fewer records, but it's not that big of a difference for class 1, for example. Any hints here of what I could look for in relation to this class? Thank you very much.
