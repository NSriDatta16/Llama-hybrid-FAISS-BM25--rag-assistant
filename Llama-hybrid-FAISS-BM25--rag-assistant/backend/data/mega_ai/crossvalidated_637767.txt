[site]: crossvalidated
[post_id]: 637767
[parent_id]: 637725
[tags]: 
Component subtraction would be appropriate for these circumstances. First, run PCA on the $\mathbf{Y}$ matrix of correlated outcomes. Then identify the major (greatest eigenvalue) and then generate the principal component (PC) score vector associated with that eigenvalue/eigenvector pair. This will result in an $n \times 1$ matrix $\mathbf{F}_y$ of PC scores. Now regress $\mathbf{Y}$ on $\mathbf{F}_y$ using multivariate normal regression with a constant term, and get the residuals, which are equal to the all the $y-\hat{y}$ . The residual matrix will now be called $\mathbf{E}_y$ , and will represent the original y-values with strong correlation removed. Do the same for the $\mathbf{X}$ matrix of correlated inputs. That is, identify the major eigenvalues and then generate the PC score vector associated with that eigenvalue/eigenvector pair. This will result in an $n \times 1$ matrix $\mathbf{F}_x$ of PC scores. Now regress $\mathbf{X}$ on $\mathbf{F}_x$ using multivariate normal regression with a constant term, and get the residuals, which are equal to the all the $x-\hat{x}$ . The residual matrix will now be called $\mathbf{E}_x$ , which represent the original predictor values with strong correlation removed. Finally, the answer will result from regressing $\mathbf{E}_y$ on $\mathbf{E}_x$ using multivariate normal regression. A key point is that you only want to use the major eigenvalue/eigenvector pair, and don't want to generate PC scores for eigenvalues $\lambda_j>1$ and then run the regressions, since the resulting residuals will have the entire signal in $\mathbf{Y}$ or $\mathbf{X}$ wiped out.
