[site]: datascience
[post_id]: 111724
[parent_id]: 111717
[tags]: 
Here are some of the main approaches I'm aware of. One method is to use Bayesian machine learning, which learns a probability distribution over the entire parameter space (see Joris Baan's A Comprehensive Introduction to Bayesian Deep Learning ). However, these methods tend to be computationally expensive. For classification problems, the most common approach is to use a classifier that can output class probabilities (such as the cross-entropy loss). While this probability can be interpreted as uncertainty, it usually is not well calibrated. By calibrated we mean the model uncertainty reflects the prediction results. For example we would expect that 80% of samples that are predicted with >= 80% certainty are correctly classified. So a calibration step can be added after the classification step. For regression problems, a na√Øve approach is to train multiple models, either by bagging, or for deep learning models, using different weight initialisations. Then the variance of the predictions from each model can be interpreted as the uncertainty. For instance, we can use an ensemble, where we also use the mean of the predictions as the ensemble prediction. However, this gives over-confident estimations of uncertainty. For deep learning models, there are a couple of other approaches that I am aware of. The first is called Monte-Carlo drop-out ( Gal and Ghahramani's Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning ), which can be applied to any deep learning model that uses dropout. This method uses the randomness from dropout to estimate variance or uncertainty in predictions and can be applied to both regression and classification models. The next is to change the loss function to the negative log likelihood (NLL) function. When used for regression, it provides an estimate of both the mean and variance. So models using this method have two outputs - one for the mean and the other for the variance. An early work on this is Nix and Weigend's Estimating the mean and variance of the target probability distribution , which uses separate MLPs for the mean and variance. A more recent work ( Lakshminarayanan et al.'s Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles ) applies this technique to any neural network, and combines it with ensembling, which further improves the uncertainty estimates.
