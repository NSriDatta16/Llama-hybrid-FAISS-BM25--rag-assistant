[site]: datascience
[post_id]: 76444
[parent_id]: 
[tags]: 
How can I build a self-attention model with tf.keras.layers.Attention?

I have completed an easy many-to-one LSTM model as following. from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense from tensorflow.keras.layers import LSTM from tensorflow.keras.layers import Dropout model=Sequential() model.add(LSTM(2**LSTM_units,input_length=data.shape[1],input_dim=data.shape[2],return_sequences=True)) model.add(Dropout(dropout)) model.add(Dense(1)) model.fit(data,res) prediction=model.predict(test) Where data.shape is (dates, time_interval, factors) and res.shape is (dates,1). I would like to replace LSTM with Attention. How can I get the similar result by using tf.keras.layers.Attention?
