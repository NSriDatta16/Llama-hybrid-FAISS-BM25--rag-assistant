[site]: crossvalidated
[post_id]: 343283
[parent_id]: 
[tags]: 
RL-gradient bandit algorithm: What's the intuition behind $\mathbb 1_{|A=a}-\pi(A)$?

I'm reading the online draft of "reinforcement learning: an introduction". At section 2.8(Page 59), it introduces gradient bandit algorithm and defines the preference update rule as below (I rewrite it in a slightly different way, but the meaning should stay the same.) $$H(A)=H(A) +\alpha(R_t-\bar R_t)(\mathbb {1}_{|A=a}-\pi(A))$$ where $a$ is the action just taken. What makes me very uncomfortable is the last term, $\mathbb 1_{|A=a}-\pi(A)$. I've read the inference at the end of the section and have some sense of how those $\mathbb 1_{|A=a}-\pi(A)$ is derived. Although that has explained a lot theoretically, I still want to get some intuitive explanations behind this. Thanks in advance!
