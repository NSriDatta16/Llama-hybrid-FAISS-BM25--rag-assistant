[site]: datascience
[post_id]: 75939
[parent_id]: 
[tags]: 
pandas memory issues

What are some good "best practices" for dealing with large datasets in pandas and numpy? Typically i read data from csv files and use numpy and pandas to prepare them for machine learning and AI models. Sometimes I run out of memory when loading large datasets into a numpy array or a pandas dataframe, wondering what are the best ways to minimize the use of memory and avoid the "out of memory" error.
