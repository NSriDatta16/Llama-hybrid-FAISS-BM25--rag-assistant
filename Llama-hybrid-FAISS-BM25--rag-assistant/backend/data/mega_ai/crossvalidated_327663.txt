[site]: crossvalidated
[post_id]: 327663
[parent_id]: 
[tags]: 
Is pretraining necessary for deep neural network?

With techniques such as batch normalization and Xavier normal initialization, is it still necessary to pretrain the neural network to get a better normalization for unsupervised task? In here, the pretraining procedure refers to training the model in an unsupervised way to force the neurons to have a good representation of the data without the labels. My model overfits heavily, does pretraining help with reduce the overfitting here?
