[site]: crossvalidated
[post_id]: 570088
[parent_id]: 570087
[tags]: 
The second matrix multiplication doesn't scale anything. The attention mechanism as described in the Attention Is All You Need paper, is what translates to the equation (1) from the paper $$ \mathrm{Attention}(Q, K, V) = \mathrm{softmax}\Big(\frac{QK^T}{\sqrt{d_k}}\Big)V $$ where scaling is done in the "Scale" step and the step is about dividing by the scaling factor $\sqrt{d_k}$ . As for matrix multiplication, the first multiplies query $Q$ with keys $K$ to produce attention weights at the penultimate step (softmax) that are multiplied by the values $V$ , so to produce an attention-weighted result. There is also an optional masking step . I don't understand your example with the furniture as it doesn't seem to refer to the neural network using attention. The transformer model is a model designed for natural language processing tasks, to process sequences (sentences, longer texts) while your example doesn't seem to have anything in common with sequence data, there doesn't seem to be any temporal dimension in the data, so I can't see how it is relevant for the problem.
