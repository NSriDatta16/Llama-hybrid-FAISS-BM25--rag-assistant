[site]: datascience
[post_id]: 64899
[parent_id]: 
[tags]: 
Why is orthogonal weights initialization so important for PPO?

I have implemented PPO to solve Atari environments. For the longest time I couldn't figure out why my model would not converge as fast as other open source solutions. Eventually it boiled down to this single weights initialization (in PyTorch) for the CNN: init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.constant_(x, 0), nn.init.calculate_gain("relu")) which is then applied to each of the layers. In other words, the neural network HAS to be initialized orthogonally. Once I added this, my model converges as expected. Why is it that orthogonal initialization is so much superior to xavier initialization for PPO?
