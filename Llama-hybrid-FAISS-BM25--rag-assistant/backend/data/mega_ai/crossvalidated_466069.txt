[site]: crossvalidated
[post_id]: 466069
[parent_id]: 
[tags]: 
How does a Markov Chain converge to a distribution we don't know?

As succinctly stated in this answer : it is possible to design a Markov chain with a stationary distribution equal to the posterior distribution, even though we don't know exactly what that distribution is. To anyone without a rather deep understanding of the statistics behind Markov Chains, this sounds like pure magic. How can you construct a process that guarantees convergence to a distribution you don't know ? All the information I've found on the topic is quite mathematically involved, and not suitable for an introductory course for non-statisticians. How would you explain this property of Markov Chains in a simple way?
