[site]: crossvalidated
[post_id]: 576815
[parent_id]: 
[tags]: 
Moore-Aronszajn Theorem and Mercer theorem for the kernel trick

I have been reading about the RKHS and the kernel trick in Machine Learning mainly from https://ngilshie.github.io/jekyll/update/2018/02/01/RKHS.html (1) and https://arxiv.org/pdf/2106.08443.pdf (2). But in (1), it is stated that because the quadratic kernel is positive semi-definite so by Moore-Aronszajn, it is a reproducing kernel for an RKHS with the feature map $\varphi(x) = K_x$ so instead of computing the feature map, we can directly compute the kernel. While in (2), the author uses Mercer theorem to say something similar i.e for a positive semi-definite kernel, there exist feature maps but we do not need to explicitly compute these feature maps to find the inner product. My question is that how do these two theorems differ in the context of the kernel trick and to what extent?
