[site]: crossvalidated
[post_id]: 239782
[parent_id]: 
[tags]: 
Purpose of a 'time constant' in a recurrent neural network?

I have been trying to understand both the reasoning and method of programming/implementing a network as described in this paper: Running Across the Reality Gap: Octopod Locomotion Evolved in a Minimal Simulation Specifically in equation (1) they specify the use of a "time constant" $T_j$: (1) $T_jA'_j = I_j-A_j + \sum_iW_{ij}O_i$ (2) $O_j = \dfrac{1}{1 - e^{\mathrm{threshold} - A_j}}$ I don't understand why $T_jA'_j$ is on the left hand side of (1)? Also in (2) should that not be $A'_j$ instead of $A_j$. I don't understand how $T_j$ the time constant is supposed to be used in the network or it's purpose as opposed to using a typical weighted network. So if I was to attempt to program it how do you use it in evaluating the network in sequence? Given it's a recurrent neural network it could be related to the timing of evaluation of neurons in the network (i.e. the feedback of the hidden neurons to themselves) but it isn't clear at all. Also I assume the $-A_j$ term in (1) is effectively a connection of the neuron back to itself with a $-1$ weight? Is there some reasoning why feedback of the negative of the previous state is a good idea? And that (2) is some variant of a sigmoid activation with a bias used for some reason. From looking at the Wikipedia definition of Time constant , it seems that it could be some sort of circuit delay/time-based-falloff but I am not sure how that would be realistically implemented in software?
