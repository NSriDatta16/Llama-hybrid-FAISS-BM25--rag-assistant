[site]: crossvalidated
[post_id]: 569833
[parent_id]: 
[tags]: 
How are online reinforcement learning algorithms evaluated?

In online reinforcement learning (RL), we have a behaviour policy. Let this policy be an epsilon-greedy policy. Suppose that I run Q-learning for some episodes and evaluated it by plotting "sum of rewards received in an episode vs. episode". For "sum of rewards received in an episode" (I am aware that there are other types of metrics but this is not the focus of my question), do I use the rewards that I obtained with the behaviour policy (i.e., epsilon greedy) or with an evaluation policy (e.g., greedy policy)? The former (lets call it [A]) is simply the rewards that the RL agent receives while learning online while the latter [B] has to be obtained by pausing the learning process and run an additional episode with the greedy policy induced by the latest Q-function. It seems that recent deep RL papers use [B] or has this always been the practice all along? Is there anything wrong with using [A]? I think both approaches show different aspects of learning. [A] couples the RL algorithm's (e.g., Q-learning) performance with the behaviour policy and [B] to a lesser extent (since the observations still depend on the behaviour policy).
