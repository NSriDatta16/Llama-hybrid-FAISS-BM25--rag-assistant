[site]: crossvalidated
[post_id]: 63277
[parent_id]: 63263
[tags]: 
The first question to address is: what's your distance metric? If you're comfortable with Euclidean space, by all means use that. However, you may want to transform these data onto an orthogonal basis using some kind of SVD and that can be done easily with any statistical software. Given these data have been transformed into a suitable domain, you can estimate a probability density for these data using some kind of parametric or nonparametric estimation. Roughly normal data are adept to estimation via maximum likelihood, but density smoothers like the boxcar, or (better) a radial basis kernal smoother, will give you an estimate of the probability density ($\hat{f}$) over your domain ($\Omega$). With these in place, we can evaluate a new observation in terms of its probability from having originated from that distribution. With a new observation taking values $x, y, z$, integrate the probability density over values of the support for which the density is less than the one you observed. This is well behaved for unimodal distributions. That is, $$\mathcal{F}(x, y, z) = 1-\iiint_{r, s, t :\hat{f}(r, s, t) This has a direct interpretation like a p-value (very roughly and blending Bayesian / Frequentist ideas): with this point, assuming it is generating from a known distribution (\hat{f}), what's the probability of observing another point as improbable or more improbable given it comes from this distribution? If this value is sufficiently small, we would rule that it is unlikely to have originated from the same distribution, though there is a chance that results in a Type I error. curve(dnorm, from=-5, to=2) points(x=-1.8, y=dnorm(-1.8), col='red', pch=20) polygon( x=c(-5, seq(-5, -1.8, by=.1), -1.8), y=c(0, dnorm(seq(-5, -1.8, by=.1)), 0), col='black' ) polygon( x=c(1.8, seq(1.8, 2, by=.1), 2), y=c(0, dnorm(seq(1.8, 2, by=.1)), 0), col='black' ) text(-1.8, dnorm(-1.8), paste("p(this observation | dist'n holds) = ", round(pnorm(-1.8)*2, 2)), pos=2)
