[site]: crossvalidated
[post_id]: 439241
[parent_id]: 439102
[tags]: 
Yes, it's possible, since you can write M-estimation in terms of a loss function (the rho function), to which you can add a penalty, reducing it to another optimization problem. However some M-estimators can have multiple modes on the likelihood, which L1 or L2 regularization won't necessarily remove. While M-estimation arises from likelihood ideas, it doesn't always correspond to maximizing a likelihood. Some M-estimators do. The Huber does, for example. However, some don't - like the Tukey biweight (indeed any M-estimator whose $\psi$ -function cuts off/goes to 0 at some finite value can't be MLE for any density). The psi function can redescend and you may still have it correspond to likelihood (e.g. it does with the Cauchy) but for the $\psi$ -function to correspond to a density, it can't descend too quickly -- or the corresponding density would not be integrable. When it does correspond to a density, you can then cast the M-estimation problem into a Bayesian framework. It might perhaps be possible to get it in a Bayesian framework in some other circumstances but I can't think of any that don't involve doing something odd like imposing a proper prior which might perhaps combine with some M-estimators to emulate a different improper prior times a valid likelihood (not corresponding to the M-estimator you started with). I wouldn't count that sort of thing myself
