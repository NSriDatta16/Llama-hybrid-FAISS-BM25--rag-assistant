[site]: crossvalidated
[post_id]: 455141
[parent_id]: 140080
[tags]: 
For another perspective, this will examine the left inverse of a finite data matrix $A$ . We can consider the data to be a sample rather than a theoretical distribution. While any distribution -- even continuous -- will have a covariance matrix, you can't generally talk about a data matrix unless you get into infinite vectors and/or special inner products. So we have a finite sample in an n-by-m data matrix $A$ . Let each column be one random variable. Then it's $n$ samples and $m$ random variables. Let $A$ 's columns (the random variables) be linearly independent (this is independence in the linear algebra sense, not as in independent random variables). Let $A$ be mean-centered already. Then, $$ C = \frac{1}{n}A^TA $$ is our covariance matrix. It's invertible since $A$ 's columns are linearly independent. And we'll use later that $C^{-1} = n(A^TA)^{-1}$ The left inverse of $A$ is $B = (A^TA)^{-1}A^T$ . And we have $BA = I_{m-by-m}$ . What do we know about $B$ ? It's m-by-n. There's a row of $B$ corresponding to each column of $A$ . Because $BA = I$ , we know the inner product of the $i$ th row of $B$ with the $i$ th column in $A$ equals 1 (diagonal of $I$ ). An inner product of the $i$ th row of $B$ with a $j$ th ( $i \neq j$ ) column of $A$ is 0 (off-diagonal of $I$ ). The right-most term in the expression for $B$ is $A^T$ . Therefore $B$ 's rows are in the rowspace of $A^T$ , the column space of $A$ . by (4) and the fact that $A$ 's columns are mean-centered, $B$ 's rows must also be mean-centered. Let $x_i$ be the $i$ th column of $A$ . The only vectors that have a non-zero inner product with the $x_i$ , zero inner product with all other $x_j$ , and are linear combinations of the columns of $A$ , are vectors parallel to the residual of $x_i$ after projecting it into the space spanned by all the other $x_j$ . Call these residuals $r_{i}$ . And call the projection (the linear regression result) $p_i$ . So the $i$ th row of $B$ must be parallel to $r_i$ (6). Now we know its direction, but what about magnitude? Let $b_i$ be the $i$ th row of $B$ . $$ \begin{align} 1 & = b_i \cdot x_i &&\text{by (2)} \\ & = b_i \cdot (p_i + r_i) &&\text{$x_i$ is the sum of its projection and residual}\\ & = (b_i \cdot p_i) + (b_i \cdot r_i) &&\text{linearity of dot product} \\ & = 0 + (b_i \cdot r_i) &&\text{by (3), and that $p_i$ is a linear combination of the $x_j$s ($j \neq i$)} \\ & = (c_i r_i) \cdot r_i &&\text{for some constant $c_i$, by (6)} \\ \end{align} $$ Therefore, $c_i = \dfrac{1}{r_i \cdot r_i} = \dfrac{1}{\|r_i\|^2}$ , so $b_i = \dfrac{r_i}{\|r_i\|^2}$ . We now know what each row of $B$ looks like. Notice $BB^T = ((A^TA)^{-1}A^T)(A((A^TA)^{-1})^T) = (A^TA)^{-1} = \frac{1}{n}C^{-1}$ We can look at any $i,j$ th element $C^{-1}_{ij} = n(BB^T)_{ij} = n (b_i \cdot b_j) = n\dfrac{r_i \cdot r_j}{\|r_i\|^2\|r_j\|^2}$ The $(r_i \cdot r_j)$ part of that should tell you we're getting close to covariances and correlations of these residuals. Conveniently, the diagonal elements look like $C^{-1}_{ii} = n\dfrac{r_i \cdot r_i}{\|r_i\|^2\|r_i\|^2} = n\dfrac{1}{\|r_i\|^2}$ . This quantity is exactly 1 over the variance of the residual $r_i$ , $\dfrac{\|r_i\|^2}{n}$ (the $n$ makes it a variance instead of a squared vector magnitude). Then to get partial correlations you just need to combine the elements of $C^{-1}$ in the way others have shown. Gilbert Strang lecture on left inverses Gilbert Strang lecture on projection, residuals
