[site]: crossvalidated
[post_id]: 282544
[parent_id]: 
[tags]: 
Why does reducing the learning rate quickly reduce the error

I was reading about learning rate change in a machine learning course. The following statement and image appear: Turning down the learning rate reduces the random fluctuations in the error due to the different gradients on different mini-batches. So why if we reduce the learning rate, the error is quickly reduced afterwards? Also, why does the smaller learning rate converge slower after that?
