[site]: datascience
[post_id]: 47886
[parent_id]: 23159
[tags]: 
This question is very interesting. I do not know the exact reason but I think the following reason could be used to explain the usage of the exponential function. This post is inspired by statistical mechanics and the principle of maximum entropy. I will explain this by using an example with $N$ images, which are constituted by $n_1$ images from the class $\mathcal{C}_1$ , $n_2$ images from the class $\mathcal{C}_2$ , ..., and $n_K$ images from the class $\mathcal{C}_K$ . Then we assume that our neural network was able to apply a nonlinear transform on our images, such that we can assign an 'energy level' $E_k$ to all the classes. We assume that this energy is on a nonlinear scale which allows us to linearly separate the images. The mean energy $\bar{E}$ is related to the other energies $E_k$ by the following relationship \begin{equation} N\bar{E} = \sum_{k=1}^{K} n_k E_k.\qquad (*) \label{eq:mean_energy} \end{equation} At the same time, we see that the total amount of images can be calculated as the following sum \begin{equation} N = \sum_{k=1}^{K}n_k.\qquad (**) \label{eq:conservation_of_particles} \end{equation} The main idea of the maximum entropy principle is that the number of the images in the corresponding classes is distributed in such a way that that the number of possible combinations of for a given energy distribution is maximized. To put it more simply the system will not very likeli go into a state in which we only have class $n_1$ it will also not go into a state in which we have the same number of images in each class. But why is this so? If all the images were in one class the system would have very low entropy. The second case would also be a very unnatural situation. It is more likely that we will have more images with moderate energy and fewer images with very high and very low energy. The entropy increases with the number of combinations in which we can split the $N$ images into the $n_1$ , $n_2$ , ..., $n_K$ image classes with corresponding energy. This number of combinations is given by the multinomial coefficient \begin{equation} \begin{pmatrix} N!\\ n_1!,n_2!,\ldots,n_K!\\ \end{pmatrix}=\dfrac{N!}{\prod_{k=1}^K n_k!}. \end{equation} We will try to maximize this number assuming that we have infinitely many images $N\to \infty$ . But his maximization has also equality constraints $(*)$ and $(**)$ . This type of optimization is called constrained optimization. We can solve this problem analytically by using the method of Lagrange multipliers. We introduce the Lagrange multipliers $\beta$ and $\alpha$ for the equality constraints and we introduce the Lagrange Function $\mathcal{L}\left(n_1,n_2,\ldots,n_k;\alpha, \beta \right)$ . \begin{equation} \mathcal{L}\left(n_1,n_2,\ldots,n_k;\alpha, \beta \right) = \dfrac{N!}{\prod_{k=1}^{K}n_k!}+\beta\left[\sum_{k=1}^Kn_k E_k - N\bar{E}\right]+\alpha\left[N-\sum_{k=1}^{K} n_k\right] \end{equation} As we assumed $N\to \infty$ we can also assume $n_k \to \infty$ and use the Stirling approximation for the factorial \begin{equation} \ln n! = n\ln n - n + \mathcal{O}(\ln n). \end{equation} Note that this approximation (the first two terms) is only asymptotic it does not mean that this approximation will converge to $\ln n!$ for $n\to \infty$ . The partial derivative of the Lagrange function with respect $n_\tilde{k}$ will result in $$\dfrac{\partial \mathcal{L}}{\partial n_\tilde{k}}=-\ln n_\tilde{k}-1-\alpha+\beta E_\tilde{k}.$$ If we set this partial derivative to zero we can find $$n_\tilde{k}=\dfrac{\exp(\beta E_\tilde{k})}{\exp(1+\alpha)}. \qquad (***)$$ If we put this back into $(**)$ we can obtain $$\exp(1+\alpha)=\dfrac{1}{N}\sum_{k=1}^K\exp(\beta E_k).$$ If we put this back into $(***)$ we get something that should remind us of the softmax function $$n_\tilde{k}=\dfrac{\exp(\beta E_\tilde{k})}{\dfrac{1}{N}\sum_{k=1}^K\exp(\beta E_k)}.$$ If we define $n_\tilde{k}/N$ as the probability of class $\mathcal{C}_\tilde{k}$ by $p_\tilde{k}$ we will obtain something that is really similar to the softmax function $$p_\tilde{k}=\dfrac{\exp(\beta E_\tilde{k})}{\sum_{k=1}^K\exp(\beta E_k)}.$$ Hence, this shows us that the softmax function is the function that is maximizing the entropy in the distribution of images. From this point, it makes sense to use this as the distribution of images. If we set $\beta E_\tilde{k}=\boldsymbol{w}^T_k\boldsymbol{x}$ we exactly get the definition of the softmax function for the $k^{\text{th}}$ output.
