[site]: crossvalidated
[post_id]: 383638
[parent_id]: 383634
[tags]: 
I don't think there's necessarily a one size fits all answer for this. What you describe in a way fits what many neural networks used to do (see e.g. VGG-19/-16). However, note that the second approach has simply more trainable parameters through the extra layers. As a result, I would expect it to do better, if you have enough data. I assume you have limited data (and are thus doing transfer learning), so it's hard to know whether in your specific case this helps (extra capacity needed to learn all the generalizable things about your training data) or harms (too many parameters versus samples leading to overfitting) performance on your test set. The best way to find out is likely to do a train-validation-test split, optimize the architecture vs. the validation data, retrain on train+validation with the chosen architecture and to then evaluate on test.
