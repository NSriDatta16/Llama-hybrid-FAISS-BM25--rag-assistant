[site]: crossvalidated
[post_id]: 32722
[parent_id]: 
[tags]: 
Predictive models with large numbers of missing values in the features

I have been trying to train an algorithm to predict if an account will close or not using thousands of data points and many features. I am using data from the month before the account closed but the problem is that accounts have been around for different amounts of time. So, whereas for one account I might only have performance data up to 1 year, for another account I might have 1 month, 3 month, 1 year, 3 year, 5 year and even 10 year. There were 94 features but I cut it down to 19 to start playing around with it. I am looking for some help on how I can build an algorithm that incorporates the different amounts of data for each account. I was thinking about using a neural network because I have always been interested in them but I am open to any suggestions. Basically, then, I have many missing values in my features. If I simply omit observations with many missing values, my data set becomes far too small to be useful. Is there a standard way of handling this type of missing data problem, or a particular algorithm or model than handles it well?
