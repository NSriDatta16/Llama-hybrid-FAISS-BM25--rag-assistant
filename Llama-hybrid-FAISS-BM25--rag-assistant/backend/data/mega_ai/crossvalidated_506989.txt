[site]: crossvalidated
[post_id]: 506989
[parent_id]: 506577
[tags]: 
Q-learning, in operation, looks at the state, finds it in the policy table, and then acts as the policy table dictates. You can think of it like a hash or a dictionary that says "if state is x then do action y". Classification is really nominal regression, it makes a continuous value which is mapped to a discrete space via a threshold. Lets say we have a finite and fixed-order sequence of weighted coins, and we want to predict the next toss with better accuracy than a random guess. A small example would be a coin "A" that is 99% (mosly heads), followed by "B" that is 60% (mostly heads), finally followed by a coin "C" that is 10% (mostly tails). We could make a sequence of training data by flipping them and making a long sequence of flips and taking the first three as input and the fourth as output. There would be three "templates" in the policy. A, B, C $\longrightarrow$ A B, C, A $\longrightarrow$ B C, A, B $\longrightarrow$ C or (H), (H), (T) $\longrightarrow$ (H) (H), (T), (H) $\longrightarrow$ (H) (T), (H), (H) $\longrightarrow$ (T) While I can do python, I prefer R. This is a stats question, not a software question. If you want a particular language instead of a particular statistical idea, then please ask in SO. This is a nice vignette on Q-learning in R: link The following code generates data for training the learner for the rules above. N_flips $Reward Reward) and here is the structure of the data.frame that it outputs > #display coins > str(coins) 'data.frame': 8950 obs. of 4 variables: $ State : chr "110" "110" "110" "110" ... $ Action : chr "h" "t" "h" "t" ... $ NextState: chr "110" "110" "110" "110" ... $ Reward : num 1 0 1 0 1 1 0 1 1 0 ... Here is the code that converts this structure into a policy. Mileage is going to vary. The control parameters need to be adjusted based on your problem and your data. # Define reinforcement learning parameters control It takes a little while to run, but gives the following policy: > print(pol) 110 111 010 011 100 101 000 001 "h" "t" "t" "t" "h" "h" "h" "h" So what does that mean? How do we interpret it? We thought we should get this: (H), (H), (T) $\longrightarrow$ (H) (H), (T), (H) $\longrightarrow$ (H) (T), (H), (H) $\longrightarrow$ (T) The above policy does this: 110 $\longrightarrow$ "h" 101 $\longrightarrow$ "h" 011 $\longrightarrow$ "t" So it estimated the policy for typical states, but also for non-mean but realized states. We didn't plan for a "111" but the data had that, and our policy says we should predict the next case as a head. Now you likely have an image of cats or dogs. They are strings of binary inputs. The fundamental problem with it is that your state is vastly larger than your number of samples. You have a megapixel aka megabit image with a single classification. To get number-of-samples equals number-of-unknowns you would need on the order of millions of training images. The compute cost and memory cost for that is large. What the first deep learning classifiers did was to convert an image of $224*224*3*8=1.2 * 10^6$ bits into a list of 1000 feature/location scores. This is a compression that is much more than the order of 3 orders of magnitude in size. It is moving the space from being $2^{1.2e6}$ discrete states, aka $10^{360e3}$ potential configurations, into a space that is 1000 continuous values.
