[site]: crossvalidated
[post_id]: 617741
[parent_id]: 349138
[tags]: 
This answer applies to finetuning pretrained transformers in NLP, but not computer vision. Contrary to Ng's advice and the currently accepted answer, it's standard practice to finetune the entire transformer, more-or-less regardless of the amount of training data. See the standard text classification tutorial , for example. A more compelling example is that SetFit 1 achieves excellent accuracy on many text classification benchmarks after finetuning all 100M+ parameters of a transformer using as few as 50 observations. Some theory for why finetuning the whole model works well can be found in this paper : Aghajanyan, Armen, Luke Zettlemoyer, and Sonal Gupta. "Intrinsic dimensionality explains the effectiveness of language model fine-tuning." arXiv preprint arXiv:2012.13255 (2020). This answer focuses on empirical results. Some notes before presenting experiments: None of the training algorithms mentioned in this answer rely on layer-wise learning rates, in case you were concerned about that. As usual, the learning rate + scheduler is just another hyperparameter you tune based on folklore and experiments. In all of the experiments, unfreezing a "layer" refers to unfreezing a transformer's attention block. An attention block technically contains two big layers: multi-headed attention (which contains many weight matrices) and a linear layer (1 huge weight matrix). Here are 2 mini empirical analyses which contain plots where the x-axis is the # of frozen layers and the y-axis is accuracy: The first GPT paper 2 : see the left plot of Figure 2 The paper doesn't vary training sizes for that task, so it's hard to say how affordable different amounts of unfreezing are for a smaller training set. This blog post for BERT Surprisingly, there doesn't appear to be a strong interaction effect of # unfrozen layers and training set size on accuracy; you can unfreeze somewhat liberally. The blog post doesn't contain training scores, so it doesn't provide evidence that more unfreezing causes greater complexity. The GPT paper does provide this evidence. And in my experience training transformers for classification and similarity tasks, this has been the case. The plots are slightly dubious to me because it looks like freezing all 12 BERT encoder blocks (not including the tacked-on linear layer) gets majority accuracy, i.e., nothing is really learned. Typically, freezing all of the encoder blocks does not perform this terribly. More on that later. (From the blog post) SST-2 benchmark: (From the blog post) CoLA benchmark: Going even further, there's evidence 3 that re-initializing some of BERT's attention blocks before training improves performance, even with just a few thousand training observations: In other words, intentionally forgetting some of what was learned during pretraining can improve performance on the target task. So don't be too concerned about seemingly immodest increases in variance / decreases in bias. These quantities are not intuitive for modern NNs. You have to run experiments. (That paper is probably the most thorough analysis of BERT fine-tuning that I've seen. You may find other experiments in there to be insightful.) It's also important to not just count layers when thinking about complexity; pay attention (pun intended) to what the layers are doing. When classifying text using transformers, a linear layer is tacked on to a pooled or specifically chosen output from the pretrained model, which consists of many attention blocks which do the heavy lifting. Freezing all but the linear layer may do fine. But freezing all but the linear layer and the last attention block may end up doing significantly better, as the step in model complexity is significant. Empirically, freezing subsequent attention blocks can yield diminishing returns. Finally addressing your question: Is unfreezing more layers always better Yes for modern pretrained transformers in NLP. There aren't many caveats to that answer, which is surprising. But keep in mind that you can save a great deal of training time and memory at little statistical cost by unfreezing fewer layers . Re statistical cost, here's a passage from the original BERT paper 4 describing an experiment where they don't finetune BERT at all: . . . we apply the feature-based approach by extracting the activations from one or more layers without fine-tuning any parameters of BERT. These contextual embeddings are used as input to a randomly initialized two-layer 768-dimensional BiLSTM before the classification layer. Â¶ Results are presented in Table 7. BERTLARGE performs competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the entire model. This demonstrates that BERT is effective for both finetuning and feature-based approaches. Based on my own classification experiments, you don't even need to train a BiLSTM on BERT features to compete with finetuning BERT. Fitting $l_2$ logistic regression on mean-pooled token embeddings (or the [CLS] token embedding for BERT, or the last token embedding for autoregressive models) from the last attention block is a statistically stable and CPU-friendly baseline. Feature extraction approaches are also great for ML applications where you need to run a suite of classifiers for each input, as you can share the output of a single model's forward pass. Because of these benefits, I wouldn't be too keen on unfreezing layers for simpler tasks or for datasets which are presumably close to pretraining data. These days, for NLP and vision models, Low-Rank Adaptation , is a great way to achieve high performance while introducing a small number of trainable parameters. 5 References Tunstall, Lewis, et al. "Efficient Few-Shot Learning Without Prompts." arXiv preprint arXiv:2209.11055 (2022). Radford, Alec, et al. "Improving language understanding by generative pre-training." (2018). Zhang, Tianyi, et al. "Revisiting few-sample BERT fine-tuning." arXiv preprint arXiv:2006.05987 (2020). Devlin, Jacob, et al. "Bert: Pre-training of deep bidirectional transformers for language understanding." arXiv preprint arXiv:1810.04805 (2018). Hu, Edward J., et al. "Lora: Low-rank adaptation of large language models." arXiv preprint arXiv:2106.09685 (2021).
