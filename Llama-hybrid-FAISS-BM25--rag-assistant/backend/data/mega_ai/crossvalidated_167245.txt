[site]: crossvalidated
[post_id]: 167245
[parent_id]: 164935
[tags]: 
A binary SVM tries to separate subjects belonging to one of two classes based on some features, the class will be denoted as $y_i \in \{-1,+1\}$ and the features as $x_i$, note the $y_i$ is a single number while $x_i$ is a vector. The index $i$ identifies the subject. The SVM solves a quadratic programming problem and finds, for each subject a lagrange multiplier $\alpha_i$, many of the $\alpha$'s are zero. Note that the $\alpha_i$ are numbers, so for each subject $i$ you have a number $y_i$, a feature vector $x_i$ and a lagrange multiplier $\alpha_i$ (a number ). You have also choosen a kernel $K(x,y)$ ($x$ and $y$ are vectors ) for which you know the functional form and you have choosen a capacity parameter $C$. The $x_i$ for which the corresponding $\alpha_i$ are non-zero are the support vectors. To compute your decision boundary, I refer to this article . Formula (61) from the mentioned article learns that the decision boundary has the equation $f(x)=0$, where $f(x)=\sum_i \alpha_i y_i K(x_i, x) + b$ and as the $\alpha_i$ are only non-zero for the support vectors, this becomes (SV is the set of support vectors): $f(x)=\sum_{i \in SV} \alpha_i y_i K(s_i, x) + b$ (where I changed $x_i$ to $s_i$ as in formula (61) of the article, to indicate that only support vectors appear). As you know all the support vectors, you know the (non-zero) $\alpha_i$, the corresponding (number) $y_i$ and the vectors $s_i$, you can compute this $f(x)$ if you know your kernel $K$ and the constant $b$. So we still have to find $b$ : The equations (55) and (56) of the article I referred to, learn that for an arbitrary $\alpha_i$ with $0 The latter holds for any of the $\alpha_i, 0 choose one such an $\alpha_i$ that is smaller than $C$ and positive , then you can compute $b$ from the corresponding observation: take an $m$ for which $0 So to summarise: Take the subjects $k$ that correspond to the support vectors, i.e. for which the Lagrange multiplier $\alpha_k$ is not zero, for each such subject you have the Lagrange multiplier $\alpha_k$, the class $y_k \in \{-1,+1\}$, and the feature vector $x_k$ (denoted as $s_k$ to make it clear that it are support vectors); Take one subject for which the Lagrange multiplier is strictly smaller than $C$, and strictly positive, assume this is object $m$ and use it to compute $b$ as $b =\frac{1}{y_m} - \sum_{k \in SV} \alpha_k y_k K(x_m, s_k)$; The equation that identifies the decision boundary is $f(x)=0$ where $f(x)=\sum_{k \in SV} \alpha_k y_k K(s_k, x) + b$
