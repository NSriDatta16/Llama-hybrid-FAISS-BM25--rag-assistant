[site]: crossvalidated
[post_id]: 460296
[parent_id]: 459694
[tags]: 
I would recommend using a classifier based with Multilingual BERT or XLM-RoBERTa. They are implemented in the Huggingface's Transformers package, you can just follow a tutorial for BertForSequenceClassification . Note that this might be rather computationally demanding and you might need to use a GPU (or use, for instance, Google Colab.) If you want a computationally cheaper solution, you can try aligned word embeddings, for instance from FastText . In that case, I would just compute an average over the word embeddings for each name. This means for a description with words $w_i, \ldots w_n$ and embedding table $V$ do: $$\sum_{i=1..n} V[w_i] / n.$$ Then, you can simply concatenate the vectors (i.e., the French one and the English one) and train a classifier on top of that. Running machine translation into and comparing monolingual embeddings (either contextual from BERT or English-only word embeddings) is also an option, but requires running an MT system which is an order of magnitude computationally more demanding than classification or embedding comparison.
