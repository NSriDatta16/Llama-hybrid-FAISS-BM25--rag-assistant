[site]: datascience
[post_id]: 126312
[parent_id]: 126311
[tags]: 
First one clarification: GPT-2 does not deals with words, but with tokens. A token represents a string from a letter to a full word. You can check the vocabulary file here . The tokens in the vocabulary where defined by an algorithm called byte-pair encoding (BPE); you can check the questions about BPE in this very site. Now, the answers to your questions: When calculating the word embedding using outputs.last_hidden_state, does this mean that the word embedding only uses the token embedding and positional embedding of GPT-2, without feeding them to the decoder blocks after that ? No, it means the opposite: it uses the states of the last layer of the Transformer decoder, after going through all the layers of the model. Note that these embeddings are close to the fixed token embeddings. You may obtain more useful embeddings if you concatenate the hidden states of the last few layers. Is this embedding also known as contextualized embedding ? Well, your embeddings are contextual indeed. However, you are computing them with a causal language model, which for each token prediction only takes into account the previos tokens. Therefore, the context used for each token embedding is the part of the sentence that came before that token, not the following part. How does this embedding better than RNN architectures, such as LSTM or bidirectional-LSTM embedding ? LSTMs are also causal, so they also only take into account the previous tokens as context. Bidirectional LSTMs are a concatenation of the results of LSTMs in opposite directions; in this case, you obtain context from both sides, but such context has not been computed "at once", but in separate halfs (one half is computed from the context before each token, the other half from the context after it), therefore is not actually fully contextual.
