[site]: datascience
[post_id]: 57203
[parent_id]: 57176
[tags]: 
Now I can answer my own question: Based on the comments of forum contributors David Waterworth and asmgx, the bias of an artificial neuron does get adjusted during training. The Wikipedia article https://en.wikipedia.org/wiki/Artificial_neural_network was mistaken when it said the optimization produces a sequence of weights $w_0, w_1, ... , w_p$ , where each $w_i$ is a vector in $R^e$ , where e is the total number of connections in the artificial neuron network. Vectors of this size do not have place to hold the biases of the neurons. The correct description should be: Each $w_i$ is a vector in $ R^g $ , where $g=e+f$ , where $e$ is the total number of connections in the artificial neuron network, and $f$ in the total number of neurons that have biases. This article https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6 also agrees with David Waterworth and asmgx: "The process of fine-tuning the weights and biases from the input data is known as training the Neural Network ."
