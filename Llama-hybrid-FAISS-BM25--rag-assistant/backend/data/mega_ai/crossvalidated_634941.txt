[site]: crossvalidated
[post_id]: 634941
[parent_id]: 
[tags]: 
Difference between Principal Component as closest linear surface and regression line as best linear approximation in two-dimensions

I am currently working on a presentation on PCA as part of a seminar programme and one slide is about giving an interpretation on the first constructed Principal Component on an arbitrary two-dimensional dataset. In one of my reference books called "An Introduction to Statistical Learning with Applications" by Gareth James, Daniela Witten, Trevor Hastie, Robert from 2015, the authors give an alternative interpretation on p. 379 regarding the first PC (or PCs all together in a generalized form) as being "the line in the p-dimensional space that is closest to the n observations (using average squared Euclidean distance as a measure of closeness)". I assumed that this would mean that adding the first PC line to a two-dimensional scatterplot of my data, which was obtained from PCA in R on my scaled and centered original dataset, would be equivalent to adding the regression line from a simple linear regression on the scaled and centered dataset (= best linear approximation), but values/results in my example speak against it. In numbers, when I performed PCA with prcomp(x) in base R on my data, it yielded the loading vector $\phi_1 = (-0.7071068, -0.7071068)$ for the first PC, but a simple linear regression yielded a slope parameter of $\hat{\beta}_1 = 0.7843085$ . I would therefore have to ask whether there is something wrong with my approach/thinking, does this interpretation regarding PC only approximately hold true or did I mess up in coding? My code example: ex.df Edit: Apparently, when performing PCA on a scaled dataset in the two-dimensional case, the "loading" will only reflect the eigenvectors as was described here: Identical loadings in a PCA , which makes the example useless. Nonetheless, it is still an interesting question in my opinion
