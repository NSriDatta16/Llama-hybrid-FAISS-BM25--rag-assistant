[site]: crossvalidated
[post_id]: 570811
[parent_id]: 570804
[tags]: 
I think your understanding is mostly correct. :) Regarding your particular questions: Brier score (the mean squared difference between the predicted probability and the actual outcome) and AUC-PR (Area Under the Curve for Precision-Recall; sometimes called average precision score ) are two more metrics one could consider for a classification problem. They are well-suited for "imbalanced" tasks too. Random cross-validation vs. stratified cross-validation should have little impact if the sample is large enough. Via stratification, we will ensure that each fold has very similar (usually identical) class proportions. Unless we see large performance metric fluctuation in per fold performance than can be attributed to sampling variation, stratification isn't that beneficial. So to begin with, do not do it, allow for the sampling variation to be part of the validation schema and if that sampling variation becomes too much of an issue, revisit the use of stratification. Using random folds in each experiment vs fixing a set of folds is a good point to raise. I would suggest you fix the set of folds. It is quite trivial to implement (just set the seed before sampling/instantiating a sampling iterator) and ensures that we compare like for like. Again it should matter "too much", especially if we do repeated $k$ -fold. On that last point, please see my answer in this thread: " Statistical significance when comparing two models for classification " it contains a number of references that will be relevant when comparing classifiers' performance.
