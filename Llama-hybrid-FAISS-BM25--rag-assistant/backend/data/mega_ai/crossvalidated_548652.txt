[site]: crossvalidated
[post_id]: 548652
[parent_id]: 548650
[tags]: 
The obvious solutions are bootstrapping (generically applicable to many settings), various Bayesian neural network approaches (e.g. Monte-Carlo drop-out = training a neural network with drop-out and then leaving drop-out on during inference to get varying results) and fitting a neural network that predicts e.g. the 2.5th, 50th and 97.5th percentile of the predictive distribution (i.e. quantile regression - there's a few useful tricks on can use to enforce an appropriate ordering of the quantiles, see e.g. here ). A number of Kaggle competitions have involved providing good prediction intervals and looking at how people solved this issue when using neural networks is worthwhile (see e.g. here or here )
