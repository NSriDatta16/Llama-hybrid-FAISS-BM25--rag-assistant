tric space. In TransH, the relation embedding is on a different hyperplane depending on the entities it interacts with. So, to compute, for example, the score function of a fact, the embedded representation of the head and tail need to be projected using a relational projection matrix on the correct hyperplane of the relation. TransR: A modification of TransH that uses different spaces embedding entities versus relations, thus separating the semantic spaces of entities and relations. TransR also uses a relational projection matrix to translate the embedding of the entities to the relation space. TransD: In TransR, the head and the tail of a given fact could belong to two different types of entities. For example, in the fact ( Obama , president of , USA ) {\displaystyle ({\text{Obama}},{\text{president of}},{\text{USA}})} , Obama is a person and USA is a country. Matrix multiplication is an expensive procedure in TransR to compute the projection. In this context, TransD uses two vectors for each entity-relation pair to compute a dynamic mapping that substitutes the projection matrix while reducing the dimensional complexity. The first vector is used to represent the semantic meaning of the entities and relations, the second to compute the mapping matrix. TransA: All the translational models define a score function in their representation space, but they oversimplify this metric loss. Since the vector representation of the entities and relations is not perfect, a pure translation of h + r {\displaystyle h+r} could be distant from t {\displaystyle t} , and a spherical equipotential Euclidean distance makes it hard to distinguish which is the closest entity. TransA, instead, introduces an adaptive Mahalanobis distance to weights the embedding dimensions, together with elliptical surfaces to remove the ambiguity. Translational models with additional embeddings It is possible to associate additional information to each element in the knowledge graph and their common representation facts. Each entity and relation can be enriched with text descriptions, weights, constraints, and others in order to improve the overall description of the domain with a knowledge graph. During the embedding of the knowledge graph, this information can be used to learn specialized embeddings for these characteristics together with the usual embedded representation of entities and relations, with the cost of learning a more significant number of vectors. STransE: This model is the result of the combination of TransE and of the structure embedding in such a way it is able to better represent the one-to-many, many-to-one, and many-to-many relations. To do so, the model involves two additional independent matrix W r h {\displaystyle W_{r}^{h}} and W r t {\displaystyle W_{r}^{t}} for each embedded relation r {\displaystyle r} in the KG. Each additional matrix is used based on the fact the specific relation interact with the head or the tail of the fact. In other words, given a fact ( h , r , t ) {\displaystyle (h,r,t)} , before applying the vector translation, the head h {\displaystyle h} is multiplied by W r h {\displaystyle W_{r}^{h}} and the tail is multiplied by W r t {\displaystyle W_{r}^{t}} . CrossE: Crossover interactions can be used for related information selection, and could be very useful for the embedding procedure. Crossover interactions provide two distinct contributions in the information selection: interactions from relations to entities and interactions from entities to relations. This means that a relation, e.g.'president_of' automatically selects the types of entities that are connecting the subject to the object of a fact. In a similar way, the entity of a fact inderectly determine which is inference path that has to be choose to predict the object of a related triple. CrossE, to do so, learns an additional interaction matrix C {\displaystyle C} , uses the element-wise product to compute the interaction between h {\displaystyle h} and r {\d