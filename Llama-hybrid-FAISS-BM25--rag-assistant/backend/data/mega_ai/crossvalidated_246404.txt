[site]: crossvalidated
[post_id]: 246404
[parent_id]: 
[tags]: 
Linear Constraint in SVM optimization

$\newcommand{\loss}{\operatorname{loss}}$Recently I am surveying different SVM optimization algorithms. I came across a strange scenario: When we formulate the SVM primal problem like the following, $$\min_w \frac{1}{2}w^Tw + C\sum_{i=1}^{m}\loss(w, x_i, y_i),$$ $$\text{such that }\quad y_i(w^Tx_i + b) \ge 1 - c_i,$$ because of the linear constraint, we will have the following constraint in the dual formulation: $$\alpha^Ty = 0,$$ and this means that we need to optimize at least two variables at a time. But some papers will formulate the SVM problem in the unconstrained version, simply: $$\min_w \frac{1}{2}w^Tw + C\sum_{i=1}^{m}\loss(w, x_i, y_i).$$ Then because we no longer have the linear constraint, we can apply method such as coordinate descent, which updates only one variable at a time. I am confused, what is the difference between these two formulations of SVM?
