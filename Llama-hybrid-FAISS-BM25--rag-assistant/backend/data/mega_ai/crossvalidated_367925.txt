[site]: crossvalidated
[post_id]: 367925
[parent_id]: 367921
[tags]: 
The ability to embed structural and algorithmic priors into the model. The simplest example of this is convolutional neural networks applied to image data. The structural prior is that nearby regions of the image are more closely related / relevant to each other compared to far-away regions. Graph convolutional networks extends this "locality" prior to arbitrary graph/network structures. 1D and 3D convolutional networks extends this prior to sound / 1D signal data and 3D scans respectively. Powerful quadratic programming solvers have been developed. It is possible to literally embed such a QP solver as part of a neural network , inducing an algorithmic prior which says "find solutions which make use of QP". Value Iteration Networks forces a prior which says "make use of this well known RL algorithm to solve this RL problem". Computer vision scientists can build 3D geometry into a neural network , enforcing the prior "we live in 3D euclidean space, and here is our camera model" into the architecture of the network.
