[site]: datascience
[post_id]: 102958
[parent_id]: 
[tags]: 
How to improve the result of f1 on imbalanced dataset

I have a dataset in which these are the distribution of the data: Neutral. 15000 Negative 3000 positive 2000 And I am mostly interested to improve the performance on the negative category. I would say neutral and positive are not important for me. And I am using Bert model . What I have tried so far: undersample data: result was poor on negative category Augment data with different approaches available in NLPaug . The result not only did not improve but it dropped by 4 percent Class weight. Gave more weight to the negative class however did not affect the result and in some scenarios dropped I tried to change the batch_size epoch etc... and it just had 0.5percent improvement Now my question is that what could be the problem here? (is there anything I need to check in my dataset?) And what else I can try to improve my model?, this is the general result I have so far Negative 65 positive 72 neutral 90 And this is my confusion matrix: Pred_negative Pred_neutral Pred_positive True_negative 138 101 3 True_neutral 53 1408 24 True_positive 2 25 69 I need to improve the negative category by at least 5 percent.
