[site]: crossvalidated
[post_id]: 193671
[parent_id]: 
[tags]: 
Levels of "hyperparameterization" in Hierarchical Modeling

Suppose we have observations $y$ that we wish to model as having being randomly sampled from a distribution with parameter $\theta$. General Bayesian approach assumes a prior distribution over $\theta$, and uses Bayes' Theorem to update the belief about $\theta$ after having seen $y$ (i.e. calculate $p(\theta\mid y)$). Hierarchical modeling envisions the distribution of the population statistic $\theta$ as itself being parametrized by a hyperparameter $\phi$ whose value is unknown. A full Bayesian treatment requires assuming a prior distribution over $\phi$, and talking about the joint prior $p(\theta, \phi)$ and the joint posterior $p(\theta, \phi\mid y)$. What is not clear to me in the context of hierarchical modeling is what justifies the idea of using only one level of "hyperparameterization"? In the simple situation described above, why does it not make sense to assume a second level hyperparameter $\gamma$ that characterizes the distribution of $\phi$, and so on?
