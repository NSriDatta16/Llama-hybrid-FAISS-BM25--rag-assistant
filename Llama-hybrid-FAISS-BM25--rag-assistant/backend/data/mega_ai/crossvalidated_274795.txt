[site]: crossvalidated
[post_id]: 274795
[parent_id]: 
[tags]: 
predictive distribution of bayesian logistic regression

In Bishop's book, he talks about Bayesian Logistic regression and how to compute it. I understood approximating posterior distribution using Laplace approximation. But I don't quite understand predictive distribution part. Specifically: The predictive distribution for class $C_1$, given a new feature vector $\phi(x)$, is obtained by marginalizing with respect to the posterior distribution $p(w|t)$... $$p(C_1|\phi,t) = \int{p(C_1|\phi,w)p(w|t)dw} \approx \int{\sigma(w^T\phi)q(w)dw} \ \ \cdots (1)$$ To evaluate the predictive distribution, we first note the function $\sigma(w^T\phi)$ depends on $w$ only through its projection onto $\phi$. Denoting $a=w^T\phi$, we have $$\sigma(w^T\phi) = \int{\delta(a-w^T\phi)\sigma(a)da}$$ where $\delta$ is the dirac delta function. From this we obtain $$\int{\sigma(w^T\phi)q(w)dw} =\int{\sigma(a)p(a)da}$$ where $$p(a) = \int{\delta(a-w^T\phi)q(w)dw}$$ ... Thus our variation approximation to the predictive distribution becomes $$p(C_1|t) = \int{\sigma(a)p(a)da} = \int{\sigma(a)N(a|\mu_a, \sigma^2_a)da}\ \ \cdots (2)$$ What exactly is the problem with equation (1) that we need to substitute it with dirac functions? At the end, he gets equation (2) but both (1) and (2) have a sigmoid function and a normal distribution. Why all the hassle to change from (1) to (2)? I guess I don't quite understand what it means when he says function $\sigma(w^T\phi)$ depends on $w$ only through its projection onto $\phi$.
