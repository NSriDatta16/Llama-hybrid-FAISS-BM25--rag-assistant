[site]: crossvalidated
[post_id]: 287342
[parent_id]: 
[tags]: 
What is the benefit of word embedding wrapper over simply adding an extra layer in RNNs

I am trying to build a sequence to sequence model using tensorflow. Tensorflow provides some functions for learning embeddings for words - tf.nn.embedding_lookup and tf.contrib.rnn.EmbeddingWrapper can help with that. The question is, will learning an embedding matrix for the vocabulary be any different than adding an extra layer in the RNN. If we add an extra layer, we can imagine the weight matrix for that layer, which would be of the shape [vocabSize,layerSize], can be treated as the embedding matrix. I am new to this field, so please correct me if I've asked something fundamentally wrong.
