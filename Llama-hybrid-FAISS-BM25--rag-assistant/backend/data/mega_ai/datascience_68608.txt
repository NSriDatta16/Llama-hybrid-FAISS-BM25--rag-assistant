[site]: datascience
[post_id]: 68608
[parent_id]: 
[tags]: 
LSTM for text with different sentences size, but same input-output sizes

Hello fellow Data Scientists I'm trying to use a LSTM (using word embeddings) to generate a system that can tag each word of a sentence. For this, I give it a set of sentences of different sizes and using padding I make the sentences all the same size, so that the LSTM can process them. The problem is, the LSTM results, after excluding the tags that correspond to the padding portion give outputs of different size than the input. So my question is, is this an architecture problem or it might be the case that more training (epocs word bigger batches) is needed? Thanks for the help! Edit: More details: In the following picture I show my LSTM Architecture Upon running for a sentence of size n (using one hot encoding and word embeddings) I get a result, that is usually of diferente size; by result I mean final output of the lstm and corresponding translation into words. Also, there are the variables I have at the moment, with which I've played around: BATCH_SIZE = 164 EPOCHS = 50 LSTM_NODES = 128 NUM_SENTENCES = 3000 MAX_NUM_WORDS = 250000 EMBEDDING_SIZE = 300 Sample input: 1 Cada 2 obra 3 consome 4 1,5 5 tonelada 6 de 7 aço 8 ( 9 US $ 10 6 11 mil 12 ) 13 mais 14 US$ 15 10 16 mil 17 de 18 mão-de-obra 19 . Sample output: 1 c-am-prd* 2 c-am-prd* 3 (c-v*) 4 (am-rec*) 5 (c-v*) 6 (c-v*) 7 (c-v*) 8 (c-v*) 9 am-dis*
