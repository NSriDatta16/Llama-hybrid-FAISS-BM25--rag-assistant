[site]: datascience
[post_id]: 118779
[parent_id]: 
[tags]: 
Machine learning / statistical model of a deterministic process: how large must my training set be to ensure almost perfect accuracy?

This may be a silly question, but if I got a deterministic process, for instance, a function (in the mathematical sense) that happens to be computationally expensive to evaluate, and I decided to approximate it with logistic regression, an artificial neural network, or any kind of machine learning model: Is there several (random and continuous) training instances that can ensure "almost" perfect accuracy for my learning model? If yes, how to compute this number? In general, can I expect at least "better" models if the process is deterministic? Is overfitting a problem in this case? My intuition somehow says no, but I cannot prove it. Thanks in advance!
