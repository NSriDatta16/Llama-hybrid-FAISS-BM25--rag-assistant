[site]: datascience
[post_id]: 103053
[parent_id]: 102927
[tags]: 
If the data preparation steps take significant processing time, then best practice would be to make that interim data set. If it is not significant, then repeat the data preparation steps for each model you build, as it is simpler and gives you more flexibility. An example of the former: say, you have NLP training data, some in PDFs, some in docx, some in html, some in epub, etc. But all your models will require either plain text sentences or plain text paragraphs. It would be sensible to first run various scripts to extract each paragraph as a single row in a text file (for instance). As an example of the latter, don't tokenize into sentences: that should be a pipeline step you do for each model, as needed, because it is relatively quick. (And also only some models require it.) Another example is where you have a very large set of images, that are going to be processed over a cluster. If you plan to augment the data by also training on the mirror image of each, what you will find is that loading the data into the cluster takes a significant amount of time, but flipping an image is very quick. If you were to make all those mirror images in advance, you would have twice as much data, so the bottleneck stage would now take twice as long. However, if those images were all hi-res photos, but all your models want to work with them resized into 256x256, I would prepare that in advance. Let's assume, on average, the new file size is 1/10th. Your total storage requirements have only gone up 10% (if you kept the originals), but the bottleneck stage of loading into the cluster will now be 10x quicker.
