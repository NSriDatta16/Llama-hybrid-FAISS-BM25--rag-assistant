[site]: crossvalidated
[post_id]: 561166
[parent_id]: 561164
[tags]: 
Consider a neural network model with 100 weights. If we think only about getting the sign of the weights right and don't worry for the moment about their magnitude. There are 2^100 combinations of the signs of these weights, which is a very large number. If we sample 60 random weight vectors, we will have seen only an minuscule proportion of that space, not even enough to be confident that we have at least one solution for which a given seven weights have the right sign. So even for a small neural network, random sampling has a vanishingly small chance of getting all of the signs of the weights right. Now of course, the structure of the neural net means that there are symmetries that mean there are multiple equivalent solutions (e.g. flipping the signs of all of the input weights of a neuron and its output weights), but this doesn't cut down the number of equivalent combinations of signs very much. I suspect part of the problem is indeed that the distribution of performance is very sharply peaked around the best solutions. So even if 60 samples gets you into the top 5% of solutions, if the search space is very large, and the optimum of the cost function is very localised, then a top 5% random solution may still be nonsense and you need, perhaps, a top 0.0005% solution or better to have acceptable performance. If random search was an effective way of training neural networks, then I would expect someone to have found that out, rather than ~50 years of gradient descent. Random search is useful for hyper-parameter search though, but mostly because the dimension is lower, and the models are trained on the data using gradient descent, so you are choosing from a set of plausibly good solutions, rather than random ones. In that case, most of the search space has goodish solutions, and the optimum of the cost function is not highly localised (at least not for kernel learning methods).
