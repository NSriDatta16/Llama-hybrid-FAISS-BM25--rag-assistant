[site]: crossvalidated
[post_id]: 614090
[parent_id]: 97083
[tags]: 
The optimal value function $v_*(s) := \max_\pi v_\pi(s)$ fulfills the Bellman optimality equation $$ v_\pi(s) = \max_a R(s,a) + \gamma \sum_{s'} P(s'|s,a) v_\pi(s').$$ Also we know by Banach's fixed point theorem that any other value function $v_\pi$ that fulfills this equation is equal to $v_*$ , which implies that $\pi$ must be an optimal policy as well. If we have the statements in the previous paragraph we can argue as follows: the policy iteration algorithm stops once the policy improvement doesn't change $\pi$ . That is the exactly the case, when $v_\pi$ fulfills the Bellman optimality equation. Thus $\pi$ must be equal to an optimal policy, once policy iteration stops.
