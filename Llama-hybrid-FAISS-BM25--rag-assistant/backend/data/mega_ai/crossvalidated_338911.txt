[site]: crossvalidated
[post_id]: 338911
[parent_id]: 338688
[tags]: 
$$ P(D, M) = \underbrace{P(D|M)}_\text{likelihood} \,\underbrace{P(M)}_\text{prior} $$ by the definition of conditional probability , same definition can be applied to obtain $$ \underbrace{P(M|D)}_\text{posterior} = \frac{P(D, M)}{P(D)} $$ The constant $P(D)$ can be dropped, as described in numerous threads, e.g. here , here , here , here , here , or here , what leads to $$ P(M|D) \propto P(D, M) = P(D|M)\,P(M) $$ It can be dropped from the computation if you are not interested in estimating the conditional probabilities directly, e.g. when using Naive Bayes algorithm , where you are only interested in finding the highest peak in the probability, or when using MCMC algorithms in Bayesian setting, that can deal with sampling from unnormalized distributions. When using maximum likelihood approach (see also this thread ), while being interested in estimating a parameter, given observed some data $P(M|D)$, we use as a proxy the conditional distribution of observing the data, given the parameter $P(D|M)$ and call this the likelihood function . Maximum likelihood does not use the priors $P(M)$, while Bayesians consider them as important part of the model. So technically yes, you could drop $P(M)$ as well if you don't care about the priors.
