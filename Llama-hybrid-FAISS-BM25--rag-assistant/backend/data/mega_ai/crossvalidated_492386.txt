[site]: crossvalidated
[post_id]: 492386
[parent_id]: 
[tags]: 
How to define loss function for Discriminator in GANs?

To train the discriminator network in GANs we set the label for the true samples as $1$ and $0$ for fake ones. Then we use binary cross-entropy loss for training. Since we set the label $1$ for true samples that means $p_{data}(x) = 1$ and now binary cross-entropy loss is: $$L_1 = \sum_{i=1}^{N} P_{data}(x_i)log(D(x)) + (1-P_{data}(x_i))log(1-D(x))$$ $$L_1 = \sum_{i=1}^{N} P_{data}(x_i)log(D(x))$$ $$L_1 = E_{x \sim P_{data}(x)}[log(D(x))]$$ For the second part, since we set the label $0$ for fake samples that means $p_{z}(z) = 0$ and now binary cross-entropy loss is: $$L_2 = \sum_{i=1}^{N} P_{z}(z_i)log(D_{G}(z)) + (1-P_{z}(z_i))log(1-D_{G}(z))$$ $$L_2 = \sum_{i=1}^{N} 1-P_{z}(z_i)log(1-D_{G}(z))$$ $$L_2 = E_{z \sim \bar{P_{z}(z)}}[log(1-D_{G}(z))]$$ Now we combine those two losses and get: $$L_D = E_{x \sim P_{data}(x)}[log(D(x))] + E_{z \sim \bar{P_{z}(z)}}[log(1-D_{G}(z))]$$ When I was reading about GANs I saw that the loss function for discriminator is defined as: $$L_D = E_{x \sim P_{data}(x)}[log(D(x))] + E_{z \sim P_{z}(z)}[log(1-D_{G}(z))]$$ Should not it be $E_{z \sim \bar{P_{z}(z)}}$ instead of $E_{z \sim P_{z}(z)}$ ?
