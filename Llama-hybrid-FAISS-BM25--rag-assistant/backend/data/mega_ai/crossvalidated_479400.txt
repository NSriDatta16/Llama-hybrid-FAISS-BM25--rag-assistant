[site]: crossvalidated
[post_id]: 479400
[parent_id]: 479244
[tags]: 
From the description of your problem, it's clear that you have: Low n/p ratio, since no. of observations are small and features are relatively high. Class imbalance, since event rate is approx. 3%. Both the cases are undesirable to any modelling procedure. You can separately address both the problems. To increase n/p ratio: "Feature selection is an important scientific requirement for a classifier when p is large." â€” Page 658, The Elements of Statistical Learning. To reduce redundancy in your dataset, you can look correlation among the numerical features and remove some if there high correlation or use wrapper methods that select features based on their contribution to a model when predicting the target variable(Recursive feature engineering). You can also use projection methods like SVD or PCA to get better representation of your data in low dimensional space. I think random forest should work well for feature selection. Try using it after resampling your data for class imbalance. To address class imbalance: You can try oversampling technique to improve class balancing. Change your performance metric. This is very important, try to use F1 score which does better model evaluation if your data is imbalanced. Try SMOTE to generate synthetic examples. Decision trees often perform well on imbalanced and high categorical datasets. The splitting rules that look at the class variable in building trees can force both classes to be addressed. Use class weights to give more weights on minority class observations. There is no one method which will always work in this kind of problem. So, you may want to try different alternatives to address both the issues.
