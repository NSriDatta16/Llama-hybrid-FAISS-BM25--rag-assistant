[site]: crossvalidated
[post_id]: 402410
[parent_id]: 402403
[tags]: 
This topic has been already discussed from several angles: Neural network training without early stopping Is epoch optimization in CV with constant mini-batch size even possible? How to correctly retrain model using all data, after cross-validation with early stopping However, I think that none of the answers covers your question fully, so I will summarize: You should not use the validation fold of cross-validation for early stoppingâ€”that way you are already letting the model "see" the testing data and you will not get an unbiased estimate of the model's performance. If you must, leave out some data from the training fold and use them for early stopping. However, this does not help you too much, for two reasons: Optimal stopping epoch may have large variance between different folds and there is no guarantee that taking the mean will be optimal in any way. If you decide to train on the whole dataset, length of an "epoch" will change: Epoch is defined as "using the whole dataset once", so how many weight updates happen in one epoch depends on the training set size and the batch size. Early stopping generally aims at limiting the maximal number of weight updates, so optimizing "epoch count" on a dataset of different size makes no sense. Thus, if anything, optimize early stopping in terms of weight updates, not epochs. Finally, I think the best approach is not to use cross-validation for early stopping tuning, instead tune all the other hyperparameters and then during the final training leave aside a small validation set which you use for early stopping.
