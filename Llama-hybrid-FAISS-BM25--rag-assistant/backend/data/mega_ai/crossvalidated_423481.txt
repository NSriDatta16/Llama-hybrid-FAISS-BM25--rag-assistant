[site]: crossvalidated
[post_id]: 423481
[parent_id]: 422672
[tags]: 
Okay, I think I've figured this one out. First some notation. We have an observation $i$ in group $j$ where the outcome of interest $Y$ follows a Bernoulli distribution and you'll be placing a linear predictor on it's $p$ parameter using the inverse logit link: \begin{align} Y_{ij} &\sim \text{B}(p_{ij})\\ p_{ij} &= \text{logit}^{-1}(\alpha + \beta c_{ij} + \gamma_j), \end{align} where $\alpha$ is the global intercept, $\beta$ is the change associated to changes in the condition $c_{ij}$ and $\gamma_j$ is the group-specific intercept. It can be shown that the specific value we choose for contrasts (e.g. $-\frac{1}{2}$ and $\frac{1}{2}$ or $-1$ and $1$ ) will only change the scale of $\beta$ , so I'll pick the first one because it leaves $\beta$ unchanged compared to dummy coding and hence makes the math easier. It's easy to check that the choice of coding won't affect the meaning of the slope, which is the difference between each of the conditions in a given group: \begin{align} (\alpha + \beta + \gamma_j) - (\alpha + \gamma_j) &= \beta\\ (\alpha + 0.5\beta + \gamma_j) - (\alpha - 0.5\beta + \gamma_j) &= \beta \end{align} The presence of group-specific effects didn't make a difference there. If we had used $1$ instead for the contrasts, the difference for the second equation would be $2\beta$ , meaning the model would return a slope that is scaled to half of that under dummy coding. Now, does this mean there are no differences at all? Not necessarily. It seems reasonable to ask that, regardless of coding, our model should predict the same values given a group and condition. Let $p_{0j}$ be the predicted probability under dummy coding and $p_{-j}$ the predicted probability under contrast coding. Since we don't yet know what the effect of coding will be on the global and group intercepts, let's also distinguish those: $\alpha_D$ and $\gamma_{Dj}$ will be the parameters under dummy coding while $\alpha_C$ and $\gamma_{Cj}$ the ones for contrast coding. We then have \begin{align} p_{0j} = p_{-j}\\ \Rightarrow \text{logit}^{-1}(\alpha_D + \gamma_{Dj}) = \text{logit}^{-1}(\alpha_C - 0.5\beta + \gamma_{Cj})\\ \Rightarrow \alpha_D + \gamma_{Dj} = \alpha_C - 0.5\beta + \gamma_{Cj} \end{align} This relates our dummy coding parameters to the contrast ones. But there's two of them and a single equation, so we need more information to assign them unique values. For the sake of argument, let's see what happens if we set the group intercepts to be equal: $$\alpha_D = \alpha_C - 0.5\beta$$ This is the result you're already familiar with, where $\alpha_C$ is bigger than $\alpha_D$ by half a $\beta$ , or we can say it's placed at the "average" of both conditions. If instead we want to keep the global intercepts unchanged, we'd have $$\gamma_{Dj} = \gamma_{Cj} - 0.5\beta$$ which by itself isn't any big revelation. Until we remember that those group intercepts have a distribution! That distribution is commonly assumed to be a zero-centered normal. But $\beta$ in general won't be zero, so the assumption that $\gamma_{Dj}$ comes from $N(0,\sigma)$ means that $\gamma_{Cj}$ must come from $N(0.5\beta,\sigma)$ . Now, what's going to happen when you try to fit the model? Since a zero-centered normal is the default assumption for every package I've seen, this means the global intercept would be the one to "absorb" the changes introduced by contrast coding. Now for some simulations ## Simulation function datasim $coefficients[,1], summary(freq_contr)$ coefficients[,1], summary(freq_contr2)$coefficients[,1]), nrow = 2) # [,1] [,2] [,3] # [1,] -0.2283242 0.2948815 0.2948815 # [2,] 1.0464066 1.0464027 0.5232013 summary(freq_dummy) $varcor summary(freq_contr)$ varcor summary(freq_contr2)$varcor # Groups Name Std.Dev. # group (Intercept) 0.33464 # group (Intercept) 0.33464 # group (Intercept) 0.33464 matrix(c(summary(bayes_dummy) $fixed[,1], summary(bayes_contr)$ fixed[,1], summary(bayes_contr2)$fixed[,1]), nrow = 2) # [,1] [,2] [,3] # [1,] -0.2312706 0.2963076 0.2960156 # [2,] 1.0481774 1.0492645 0.5235235 summary(bayes_dummy) $random summary(bayes_contr)$ random summary(bayes_contr2)$random # Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat # sd(Intercept) 0.3616682 0.07577461 0.2328071 0.524049 1281.155 1.001547 # sd(Intercept) 0.36224 0.07594461 0.2316291 0.5347441 1396.271 1.001551 # sd(Intercept) 0.3632387 0.07466388 0.2350172 0.5263185 1571.552 1.001344 I'd say this confirms the reasoning we developed previously. As for the small differences in the Bayesian models, how can we know they are are solely attributable to Montcaro error? It's pretty simple: just run the same model a couple of times and you'll notice that the same-model differences are enough to make the inter-model differences overlap zero. I didn't show the results for that, but I did it a few times and feel pretty confident that the differences are just MC error.
