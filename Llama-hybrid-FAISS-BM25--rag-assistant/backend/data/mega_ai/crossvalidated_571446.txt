[site]: crossvalidated
[post_id]: 571446
[parent_id]: 
[tags]: 
Reward shaping experiments - interpreting results

I've been training some agents playing Tetris, and I have ran some experiments with different reward structures: I'm mostly interested in the potential-based rewards agent. I'm looking for some guidance on how to interpret that learning curve? It looks like its taking big steps at ~12000 and ~22000 epochs, and it's kind of shaped like a staircase. What could this signify about my agents learning?
