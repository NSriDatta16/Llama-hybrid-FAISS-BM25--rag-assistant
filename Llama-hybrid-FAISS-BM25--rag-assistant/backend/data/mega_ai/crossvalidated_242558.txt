[site]: crossvalidated
[post_id]: 242558
[parent_id]: 
[tags]: 
Python implementation of indicator function in Softmax gradient

I hope this is the right place for this question. I am following the Stanford Deep Learning tutorial http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/ trying to implement gradient decent with softmax. For the indicator function in the equation below, \begin{align} \nabla_{\theta^{(k)}} J(\theta) = - \sum_{i=1}^{m}{ \left[ x^{(i)} \left( 1\{ y^{(i)} = k\} - P(y^{(i)} = k | x^{(i)}; \theta) \right) \right] } \end{align} I am thinking of creating a numpy array that will hold the indicator for all the elements of the input X, which I can then implement. First of all, I'm not sure that creating an array to hold the indicators is the right way to go, but here is my implementation so far: indicator = [[1 if X[i,j]==y[i] else 0 for j in range(X.shape[1])] for i in range(X.shape[0])] where X is the input and y is the labels. This implementation is erroneous, in addition to being quite slow. I wonder if someone could set me in the right direction. Thanks!
