[site]: crossvalidated
[post_id]: 531219
[parent_id]: 
[tags]: 
Maximum Likelihood and the Empirical Distribution

I am reading Goodfellow et al. "Deep Learning" book (2016) . In chapter 5, where they are explaining Maximum Likelihood, they imply the empirical distribution $\hat p_{data}$ as a uniform distribution in which each data point has a chance of $1/m$ where we have $m$ data points. As we do not know the true underlying distribution from which our i.i.d observations have been drawn, we must find model parameters $\theta$ that approximates $\hat p_{data}$ the best (a.k.a minimizes the divergence between these two distributions). What I am wondering about is that why we assume that the empirical distribution is uniform? isn't this very limiting? the underlying distribution might be any distribution and I do not understand why to make such an assumption and proceed with it. I appreciate any hints on the intuition behind this.
