[site]: crossvalidated
[post_id]: 486761
[parent_id]: 
[tags]: 
Policy improvement

I am studying Sutton&Barto's book Reinforcement Learning: An introduction , available for free. On page 80 is given an algorithm called Policy Iteration, this algorithm has a step called Policy Improvement. My understanding is that the policy is assumed to be deterministic; that is, for every feasible $s$ there exists an $a$ such that $\mathbb{P}(A_t = a\mid S_t =s) = 1$ . In the step Policy Improvement one has to determine $$ \text{arg max}_a \sum_{s',r} p(s',r\mid s,a)\cdot (r + \gamma \cdot V(s')), $$ where the expression $p(s',r\mid s,a)$ is equivalent to $\mathbb{P}(S_t =s',R_t=r\mid S_{t-1} = s, A_{t-1} = a )$ . However, the latter expression can be re-expressed as follows \begin{align} \mathbb{P}(S_t =s',R_t=r\mid S_{t-1} = s, A_{t-1} = a )&=\frac{\mathbb{P}(S_t =s',R_t=r, S_{t-1} = s, A_{t-1} = a)}{\mathbb{P}(S_{t-1} = s, A_{t-1} = a)}\\[1em] &=\frac{\mathbb{P}(S_t =s',R_t=r, S_{t-1} = s, A_{t-1} = a)}{\mathbb{P}(A_{t-1} = a\mid S_{t-1} = s)\cdot \mathbb{P}(S_{t-1} = s)} \end{align} The probability $\mathbb{P}(A_t = a\mid S_t =s)$ appears in the denominator, hence $p(\cdot,\cdot\mid s,a)$ is well-defined for a single pair $(s,a) = (s,\pi(s))$ ? This means that the argmax should return $\pi(a)$ ? What am I missing?
