[site]: datascience
[post_id]: 14211
[parent_id]: 14208
[tags]: 
Mean centering is one of many related techniques to preprocess data for downstream analysis in multivariate methods. It might sound odd at first, but it means exactly what it says: the vector has a mean of zero. In pseudocode, (sum(vector) / len(vector)) == 0 . In multivariate data, this typically is applied along each column in a dataset so each column can be more easily compared to another within a similar range of data. After mean centering, each row only includes how it differs from the average sample from that variable in the original data. Typically, samples are also scaled to have unit variance as well, allowing you to more readily compare the data across continuous variables with different ranges. For example, if you had a dataset of patients with variables height, weight, age, household_income , despite each variable being a continuous value each of these variables will be in different range. Height might be between 60 -- 75 inches, weight between 100 -- 300 lbs, and so on. Why do all this? Removing the mean and standardizing the variance will help downstream methods not 'learn' the mean and variance of your data, making it easier to find relationships between variables. Many assume that your data is centered / scaled / normalized in some way and will behave poorly if you don't do so.
