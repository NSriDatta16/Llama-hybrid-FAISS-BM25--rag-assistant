[site]: crossvalidated
[post_id]: 568121
[parent_id]: 568019
[tags]: 
On the first slide, (5) is the conditional probability that the unknown class $Y$ associated with observed $\mathbf x$ is equal to $l$ , which is denoted as $\pi_l(\mathbf x)$ because it is conditional on $\mathbf x$ . There is little sense of considering it as a Bayesian posterior quantity, because (5) does not involve different prior weights on the different classes. (With enough licence, the prior attached to (5) is the uniform prior over classes.) Even though notation has changed between slides, (7) is a generalisation of (5) when there is a (general) prior distribution on the class $Y$ distribution, with $$\pi_l = \mathbb P(Y=l)$$ and $$p_l(\mathbf x)=\log f_l(\mathbf x)$$ (The conditioning on $Y=l$ is redundant with the index on $l$ .)
