[site]: datascience
[post_id]: 113417
[parent_id]: 113400
[tags]: 
The attention mechanism is based on transformers that also include fully connected layers indeed. The difference comes mainly from the RNNs that are the base of the attention mechanism. However, this attention is quite limited . It could be difficult to understand how it works precisely, even if you get the idea step by step. That's why, a good way to understand how they work is to use playgrounds like those ones: https://distill.pub/2016/augmented-rnns/ https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb#scrollTo=OJKU36QAfqOC
