[site]: crossvalidated
[post_id]: 636613
[parent_id]: 
[tags]: 
OLS and Linear and Nonlinear Models

I understand what linear (w.r.t. coefficients) models are. For example, the power model $Y=k X^p$ is nonlinear.. $Y= \beta ^X$ is also nonlinear, etc. Ordinary-least-squares (OLS) is one of the many methods used to estimate the coefficients for a model like $Y=b_1 X+b_0$ (linear). We get nice formulas that estimate the coefficients and, as long as certain key assumptions about residualsare met, the estimates are solid and we can perform reliable hypothesis testing on the parameters using the sample data $(X,Y)$ . Why can OLS not be used for ALL linear models to get the unknown coefficients? Logistic regression uses MLE instead of OLS. Sometimes we have to do variable transformations to convert a particular linear model, or even a nonlinear one, to assume the straight-line form $Y=b_1 X+b_0$ . We can then apply OLS to the transformed model and get clean estimation formulas. Why can OLS not be applied directly to nonlinear models? Given any linear or nonlinear model $g(X)$ , OLS could be used to estimate the coefficients that minimize Sigma [Y_true - g(X)]^2. Let's assume $Y=g(X)=k X^p$ , we could simply minimize $\Sigma [Y_{true} - k X^p ]^2$ and get equations that can be solved numerically for the parameters. Depending on the model, OLS provides in some cases analytical formulas for the coefficients while in other cases we need to resort to numerical methods to find the optimal coefficients. But it seems like we can apply OLS to any type of model....Is the issue with the OLS assumptions (homoscedasticity, errors with zero mean, etc.)? Thank you!
