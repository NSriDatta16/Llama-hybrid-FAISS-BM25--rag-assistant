[site]: crossvalidated
[post_id]: 171461
[parent_id]: 164679
[tags]: 
Assuming that "adding Russia" means just adding it to your vocabulary, there is no simple meaningful way to get a corresponding word vector. Remember that you have learned your vectors not from a list of relevant words (vocabulary) but a large corpus. word2vec is a distributional method and therefore relies on observing the context of Russia, not the word itself. You could try to fix the learned embedding and in a second run over the corpus, learn only the vector 'Russia'. However, that won't be much cheaper than learning the whole embedding from scratch. Alternatively, try to find an embedding technique that learns a parametric mapping from words to vectors. But I am not entirely sure that exists...
