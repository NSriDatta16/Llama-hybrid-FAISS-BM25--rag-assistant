[site]: datascience
[post_id]: 63285
[parent_id]: 63270
[tags]: 
As already noted before, there is no way to find the ideal size of a neural network. It also depends on what kinds of layer you use, not just the size of the dataset. But I suppose the number of trainable parameter at least should be smaller than the size of the dataset (I mean number of examples x number of features). It is probably best practice to find a working architecture - something that guesses better than pure chance, and after that add or remove layers/neurons to reach the optimal number of parameters. Another way to argue that it is small is to use another, larger dataset that is used for image classification and compare the results on the same network. You can try cats and dogs for example. You can find several tutorials on this dataset, including one by Sentdex . If you are doing image classification, I strongly recommend you look into transfer learning . There are several pre-trained neural networks which are trained for very long time on large datasets. You can easily achieve over %95 accuracy in with very little training.
