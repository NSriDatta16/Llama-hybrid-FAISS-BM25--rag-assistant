[site]: datascience
[post_id]: 54849
[parent_id]: 
[tags]: 
Policy Gradient methods not converging to useful mean values

I am getting familiar with Policy Gradient methods, specifically Advantage Actor Critic (A2C). My target problem use clipped continuous state and action spaces and I have therefore been training my system using Normal distributions over the actions. My neural network generates mu and sigma values for each of the actions and I select new actions by choosing values from the provided distribution. What I am unclear on is executing the policy when it is not being trained. In other RL approaches when we are not training we would want to pick the greedy solution, so as to generate the action-state path with the highest possible (and hopefully optimal) reward. In discrete action spaces we accomplish with the argmax function. But due to the intractability of performing argmax over a continuous environment I have been implementing my own solution: just using the value of mu for the given actions. Unfortunately, this solution hasn't been as effective as I anticipated. A closer inspection of my policy shows that in the initial states my policy has a mean located over actions that do nothing (ie. velocity = 0.0) at the limits of my clipped continuous space. As such when trying to execute my greedy policy the system does atrociously. However, in these states I possess a very high sigma/std dev. and therefore, when I train my system it quickly, but randomly, propels itself into other states (ones which generate a positive value for mu). And while I can simply select actions according to the probabilistic-greedy approach instead of the pure-greedy approach I feel that this is not a complete solution for my problem. Most notably for the reliance on random actions to reach the goal. In conclusion, I'd like to know: first, is my approach to a greedy solution appropriate? and second, are there any theoretical approaches I should consider to either limit sigma or encourage mu to move? Thank you EDIT: I have managed to have some luck by clipping the sigma value to be between 1e-7 and 0.5. While, the mean has moved the sigma now remains solidly at 0.5 and has not decreased. Are there more standard ways to accomplish the desired implementation?
