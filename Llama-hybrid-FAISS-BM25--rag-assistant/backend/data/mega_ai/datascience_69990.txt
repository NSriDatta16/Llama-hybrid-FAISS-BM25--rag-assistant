[site]: datascience
[post_id]: 69990
[parent_id]: 
[tags]: 
using feature selection to improve model performance

I have a highly sparse dataset that I am using to predict a continuous variable via a random forest regression. I have achieved an acceptable level of performance following cross-validation, and I am now thinking of potential ways I might further improve accuracy. Given that my dataset is highly sparse, I was thinking that recursive feature elimination (using the cross validated version in sklearn) might be a good way to go. My understanding is that this will give me the 'optimal' number of features and thus may reduce issues related to over-fitting. My question is, is it then appropriate to re-run the analysis with these optimized features, or am I in someway biasing the model? I have a test set that has not been used at all in training/validation, so I am assuming that as long as I don't leak info between training and testing, I should be good. But I'm unclear if these assumptions are correct. Is this a suitable use of RFE, or should i be considering a different path? For info, my training/validation dataset is 370 rows, with approx. 900 features.
