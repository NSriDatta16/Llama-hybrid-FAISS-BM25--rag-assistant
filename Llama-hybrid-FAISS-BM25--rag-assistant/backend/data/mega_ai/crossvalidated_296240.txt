[site]: crossvalidated
[post_id]: 296240
[parent_id]: 296220
[tags]: 
I will piggy-back on @Tim's answer, but with a little more detail. As Tim stated, with a flat prior, your point estimates will be very similar. If your Bayesian point estimate is the maximum a posterior (MAP) estimate, it will always be exactly the same as using the MLE by definition. If your posterior distribution is symmetric and unimodal, then the posterior mean will also be exactly the as the MLE estimate. Furthermore, if the posterior distribution is Gaussian, then using the MLE for point estimates will give you the same point estimates as the MLE and using the Fisher's information to describe the covariance will give you back the exact same distribution as the posterior distribution. In fact, MLE theory tells us that under certain conditions, the log likelihood (or log posterior density with a flat prior) will asymptotically approach a quadratic function. If the log posterior distribution is quadratic, this turns out to be exactly the same as the posterior being Gaussian. In otherwords, asymptotically , using the MLE for inference is exactly the same as using the full posterior distribution with a flat prior. What this implies is that if you are going to use a flat prior, you should only consider using Bayesian MCMC methods if you are worried that asymptotic approximations are not precise enough. When is this a problem? I'm sure there's plenty of other examples that don't fit this rule, but I would say the most common case you should be concerned with is the case when the number of samples per parameter fit is low (recall that one of the necessary assumptions for MLE asymptotics is that the number of parameters fit divided by observations goes to 0). For example, consider mixed effects models. In these cases, we have at least one parameter per subject (although many observations per subject). In this case, it is common practice to use ReML rather than MLE estimates to obtain the variance estimates, as the MLE variance estimates are well known to be downward biased, and problematically so when you have so many parameters to estimate. However, using a Bayesian approach, there's no such issue: just sample from the posterior and don't worry about it!
