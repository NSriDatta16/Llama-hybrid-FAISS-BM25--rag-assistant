[site]: crossvalidated
[post_id]: 223630
[parent_id]: 
[tags]: 
Fractional output dimensions of "sliding-windows" (convolutions, pooling etc) in neural networks

As far as I can see, ResNet-152 ( paper , visualization , Caffe Model ) expects inputs with dimensions 224x224x3, and its first layer does 64 convolutions, each against a 7x7 kernel with a padding of 3 and stride=2. Since $\frac{\text{input}+2\times\text{padding}-\text{filter}}{\text{stride}}=112.5$, its output's dimensions should be 112.5x112.5x64 (right?). This must be converted to an integer, and it looks like Caffe "truncates toward zero" and the output's dimensions are actually 112x112x64 ( here's the code ), and I loaded the model with Caffe and verified it). This seems to be different than Caffe's strategy for pooling layers ( code ), which ceils the result, instead of truncating it. My questions: What's the conventional way of treating settings of kernel/padding/stride that results "fractional" dimensions (e.g. 112.5 as above). Are the common frameworks (Caffe, TensorFlow, Torch...) consistent about it? Is the inconsistency between the behaviour of convolutional layers and pooling-layers of Caffe is "by design"? If so, why?
