[site]: crossvalidated
[post_id]: 419842
[parent_id]: 185639
[tags]: 
I'd like to add some detail to the accepted answer, because I think it's a bit more nuanced and the nuance may not be obvious to someone first learning about RNNs. For the vanilla RNN, $$\frac{\partial h_{t'}}{\partial h_{t}} = \prod _{k=1} ^{t'-t} w \sigma'(w h_{t'-k})$$ . For the LSTM, $$\frac{\partial s_{t'}}{\partial s_{t}} = \prod _{k=1} ^{t'-t} \sigma(v_{t+k})$$ a natural question to ask is, don't both the product-sums have a sigmoid term that when multiplied together $t'-t$ times can vanish? the answer is yes , which is why LSTM will suffer from vanishing gradients as well, but not nearly as much as the vanilla RNN The difference is for the vanilla RNN, the gradient decays with $w \sigma'(\cdot)$ while for the LSTM the gradient decays with $\sigma (\cdot)$ . For the LSTM, there's is a set of weights which can be learned such that $$\sigma (\cdot) \approx 1$$ Suppose $v_{t+k} = wx$ for some weight $w$ and input $x$ . Then the neural network can learn a large $w$ to prevent gradients from vanishing. e.g. In the 1D case if $x=1$ , $w=10$ $v_{t+k}=10$ then the decay factor $\sigma (\cdot) = 0.99995$ , or the gradient dies as: $$(0.99995)^{t'-t}$$ For the vanilla RNN, there is no set of weights which can be learned such that $$w \sigma'(w h_{t'-k}) \approx 1 $$ e.g. In the 1D case, suppose $h_{t'-k}=1$ . The function $w \sigma'(w*1)$ achieves a maximum of $0.224$ at $w=1.5434$ . This means the gradient will decay as, $$(0.224)^{t'-t}$$
