[site]: crossvalidated
[post_id]: 372446
[parent_id]: 372422
[tags]: 
First of all, it depends on the model. If you use a simple decision tree with commonly known settings, your label values (0,1,2 etc.) don't event matter. So, using the same seed, with single-threaded applications of course, random forest will also give the inverted result, because it uses decision trees under the hood. Generalizing this concept for gradient based methods is not healthy. Your gradient updates will have different numerical values and you go somewhere else in the parameter space. You could even fall in a different local minimum, which is highly possible in neural nets, and obtain slightly or totally different class assignments. A model with convex loss function (e.g. logistic regression) might converge to inverted labels in the long run, which is intuitive; but, we're always stopping our algorithm at some point, in which it isn't guaranteed that you'll get exactly $1-y$ for your points. If it were guaranteed, then you'd also get inverted labels at each learning iteration.
