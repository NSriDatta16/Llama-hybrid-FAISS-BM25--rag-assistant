[site]: crossvalidated
[post_id]: 89492
[parent_id]: 89373
[tags]: 
This is a surprisingly tricky issue. If you train and test your 'combinations' only once, statistical testing is straightforward: you can compare each pair of 'combinations' using McNemar's test , (see this question ). Then you can correct the p-values for running multiple tests by FDR or Bonferroni, depending how strict you want to be. However, you want to average across many splits. This makes the variance of the accuracy statistic difficult to estimate, since the accuracies of different splits are not independent from each other. One way to deal with this problem is to ignore it, treating the 100 accuracies (generated from 100 splits) as 100 independent observations. If you choose this path, you can use Wilcoxon signed-rank test to compare pairs of 'combinations' (and then correct for multiple comparisons for testing several pairs). However, there is a danger of an optimistic bias - your p-values will be probability too small. The extent of the problem will depend on your particular dataset and algorithms. If you want to avoid this optimistic bias and still test accuracy averaged over different splits, you'll have to look for a more sophisticated approach: Nadeau and Bengio, 2003 was mentioned in this question as an attempt at this problem.
