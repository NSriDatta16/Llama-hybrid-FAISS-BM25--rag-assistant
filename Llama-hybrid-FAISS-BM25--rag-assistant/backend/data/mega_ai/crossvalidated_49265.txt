[site]: crossvalidated
[post_id]: 49265
[parent_id]: 
[tags]: 
What's the deal with autocorrelation?

To preface this, I have a pretty deep mathematical background, but I've never really dealt with time series, or statistical modeling. So you don't have to be very gentle with me :) I'm reading this paper about modeling energy use in commercial buildings, and the author makes this claim: [The presence of autocorrelation arises] because the model has been developed from time series data of energy use, which is inherently autocorrelated. Any purely deterministic model for time series data will have autocorrelation. Autocorrelation is found to reduce if [more Fourier coefficients] are included in the model. However, in most of the cases the Fourier model has low C.V. The model may, therefore, be aceptable for practical purposes that does (sic) not demand high precision. 0.) What does "any purely deterministic model for time series data will have autocorrelation" mean? I can vaguely understand what this means--for example, how would you expect to predict the next point in your time series if you had 0 autocorrelation? This isn't a mathematical argument, to be sure, which is why this is 0 :) 1.) I was under the impression that autocorrelation basically killed your model, but thinking about it, I can't understand why this should be the case. So why is autocorrelation a bad (or good) thing? 2.) The solution I've heard for dealing with autocorrelation is to diff the time series. Without trying to read the author's mind, why would one not do a diff if non-negligible autocorrelation exists? 3.) What limitations do non-negligible autocorrelations place on a model? Is this an assumption somewhere (i.e., normally distributed residuals when modeling with simple linear regression)? Anyway, sorry if these are basic questions, and thanks in advance for helping.
