[site]: crossvalidated
[post_id]: 406190
[parent_id]: 
[tags]: 
Logistic regression - residual sum of square

In Elements of Statistical Learning https://web.stanford.edu/~hastie/Papers/ESLII.pdf Section 4.4.3 about Logistic Regression infers from the Newton-Raphson procedure for fitting linear logistic model a relationship between so-called adjusted response $z_{i}$ and maximum-likelihood parameter estimates $\hat{\beta}$ : $z_{i}=x_{i}^{T}\hat{\beta}+\frac{(y_{i}-\hat{p_{i}})}{\hat{p_{i}}(1-\hat{p_{i}})}$ This stems directly from the expression of adjusted response in the expression of the Newton step in the previous section : $\textbf{z}=\textbf{X}\beta^{old}+\textbf{W}^{-1}(\textbf{y}-\textbf{p})$ It is then said, equation 4.30 The weighted residual sum-of-squares is the familiar Pearson chisquare statistic $\sum_{i=1}^{N}\frac{(y_{i} − \hat{p}_{i})^{2}}{ \hat{p}_{i}(1 − \hat{p}_{i})}$ a quadratic approximation to the deviance. Why isn't the residual sum of squares $\sum_{i=1}^{N}\frac{(y_{i} − \hat{p}_{i})^{2}}{ (\hat{p}_{i}(1 − \hat{p}_{i}))^{\textbf{2}}}$ ? How do we justify that the sum "follows" a Chisquare statistics ? Why would this give a quadratic approximation of the deviance ? ( deviance is defined as $-2\times$ loglikelihood ) How is this result convenient ? References to good books/texts in statistics around those topics are welcome.
