[site]: crossvalidated
[post_id]: 173196
[parent_id]: 173040
[tags]: 
Here is a more detailed answer concerning the deterministic components. The literature has differing opinions on what these mean, and the two views are summarized in my other answer and Richard Hardy's comment. Below, I want to explain why I have my view. In particular, I want to argue that, in order to get the Dickey-Fuller distribution in the case of a Dickey-Fuller regression with constant, we need to adopt the view that the drift term vanishes under the null. Else, we would be in the case considered by West (Econometrica 1988) . He indeed departs from the (null) model $y_t=\alpha+y_{t-1}+u_t$, which indeed yields a linear trend under the null, or, from writing $\Delta y_t=\alpha+u_t$, a mean in first differences. He then however shows that in this case, the t-statistic will follow an asymptotic normal distribution under the null. Hence, not the Dickey-Fuller distribution. Heuristically, because the linear trend dominates the unit root under the null. Now to the proof (which is basically from Hamilton's book , see pp. 490) that we do indeed get the Dickey-Fuller distribution in the constant case under the model I postulate below, i.e., that says that under the alternative, we have a stationary AR(1) around a non-zero mean but still a driftless random walk under the null. (The most basic model $y_t=\rho y_{t-1}+u_t$ assumes a zero mean - see above table - , which is not realistic for most economic time series. That is, one would reject in favor of the model $y_t=\rho y_{t-1}+u_t$, $|\rho| Let us therefore consider the model \begin{align*}y_t&=\psi+\tilde{y}_t\\ \tilde{y}_t&=\rho \tilde{y}_{t-1}+u_t \end{align*} Rearrange this to $y_t-\psi=\tilde{y}_t$, insert and solve for $y_t$ to get $$ y_t=\psi(1-\rho)+\rho y_{t-1}+u_t $$ So $y_t$ is a random walk if $\rho=1$ and a stable $AR(1)$ under the alternative. To account for this non-zero mean, we include an intercept in our regression $$ y_t=\alpha+\rho y_{t-1}+u_t $$ We look at the sampling error under $H_0:\rho=1$ (and thus $\alpha=0)$. The OLSE is $$ \begin{pmatrix} \hat{\alpha}_T \\ \hat{\rho}_T \\ \end{pmatrix}=\begin{pmatrix} T & \sum_ty_{t-1} \\ \sum_ty_{t-1} & \sum_ty_{t-1}^2 \\ \end{pmatrix}^{-1} \begin{pmatrix} \sum_ty_t \\ \sum_ty_{t-1}y_t \\ \end{pmatrix} $$ Hence, we can show that $$ \begin{pmatrix} \hat{\alpha}_T \\ \hat{\rho}_T-1 \\ \end{pmatrix}=\begin{pmatrix} T & \sum_ty_{t-1} \\ \sum_ty_{t-1} & \sum_ty_{t-1}^2 \\ \end{pmatrix}^{-1} \begin{pmatrix} \sum_tu_t \\ \sum_ty_{t-1}u_t \\ \end{pmatrix} $$ We have a coefficient on a "standard" variable (a constant) as well as one on a nonstationary one ($y_{t-1}$). It turns out we need to assign two different convergence rates, $$ \Upsilon:=\begin{pmatrix} \sqrt{T} & 0 \\ 0 & T \\ \end{pmatrix} $$ We can then rewrite the previous display as \begin{align*} \Upsilon\begin{pmatrix} \hat{\alpha}_T \\ \hat{\rho}_T-1 \\ \end{pmatrix}&=\Upsilon\begin{pmatrix} T & \sum_ty_{t-1} \\ \sum_ty_{t-1} & \sum_ty_{t-1}^2 \\ \end{pmatrix}^{-1}\Upsilon \Upsilon^{-1}\begin{pmatrix} \sum_tu_t \\ \sum_ty_{t-1}u_t \\ \end{pmatrix}\\ &=\left[\Upsilon^{-1}\begin{pmatrix} T & \sum_ty_{t-1} \\ \sum_ty_{t-1} & \sum_ty_{t-1}^2 \\ \end{pmatrix}\Upsilon^{-1}\right]^{-1} \Upsilon^{-1}\begin{pmatrix} \sum_tu_t \\ \sum_ty_{t-1}u_t \\ \end{pmatrix} \end{align*} where $$ \Upsilon^{-1}=\begin{pmatrix} 1/\sqrt{T} & 0 \\ 0 & 1/T \\ \end{pmatrix} $$ Hence, $$ \begin{pmatrix} \sqrt{T}\hat{\alpha}_T \\ T(\hat{\rho}_T-1) \\ \end{pmatrix}=\begin{pmatrix} 1 & T^{-3/2}\sum_ty_{t-1} \\ T^{-3/2}\sum_ty_{t-1} & T^{-2}\sum_ty_{t-1}^2 \\ \end{pmatrix}^{-1} \begin{pmatrix} T^{-1/2}\sum_tu_t \\ T^{-1}\sum_ty_{t-1}u_t \\ \end{pmatrix} $$ Using standard results on weak convergence to functionals of Brownian motion, we obtain \begin{align*} \begin{pmatrix} 1 & T^{-3/2}\sum_ty_{t-1} \\ T^{-3/2}\sum_ty_{t-1} & T^{-2}\sum_ty_{t-1}^2 \\ \end{pmatrix}&\Rightarrow\begin{pmatrix} 1 & \sigma\int_0^1W(r)d r \\ \sigma\int_0^1W(r)d r & \sigma^2\int_0^1[W(r)]^2d r \\ \end{pmatrix}\\ &=\begin{pmatrix} 1 & 0 \\ 0 & \sigma \\ \end{pmatrix}\begin{pmatrix} 1 & \int_0^1W(r)d r \\ \int_0^1W(r)d r & \int_0^1[W(r)]^2d r \\ \end{pmatrix}\\ &\qquad\times\;\begin{pmatrix} 1 & 0 \\ 0 & \sigma \\ \end{pmatrix}\\ &\equiv \Sigma \begin{pmatrix} 1 & \int_0^1W(r)d r \\ \int_0^1W(r)d r & \int_0^1[W(r)]^2d r \\ \end{pmatrix} \Sigma \end{align*} Similarly, using well-known weak convergence results for the second entry, $$ \begin{pmatrix} T^{-1/2}\sum_tu_t \\ T^{-1}\sum_ty_{t-1}u_t \\ \end{pmatrix}\Rightarrow \sigma\Sigma\begin{pmatrix} W(1)\\ 1/2\{W(1)^2-1\}\\ \end{pmatrix} $$ Together, this yields \begin{align*} \begin{pmatrix} \sqrt{T}\hat{\alpha}_T \\ T(\hat{\rho}_T-1) \\ \end{pmatrix}&\Rightarrow\sigma\begin{pmatrix} 1 & 0 \\ 0 & 1/\sigma \\ \end{pmatrix} \begin{pmatrix} 1 & \int_0^1W(r)d r \\ \int_0^1W(r)d r & \int_0^1[W(r)]^2d r \\ \end{pmatrix}^{-1}\\ &\times\; \Sigma^{-1}\Sigma\begin{pmatrix} W(1)\\ 1/2\{W(1)^2-1\}\\ \end{pmatrix}\\ &=\begin{pmatrix} \sigma & 0 \\ 0 & 1 \\ \end{pmatrix} \begin{pmatrix} 1 & \int_0^1W(r)d r \\ \int_0^1W(r)d r & \int_0^1[W(r)]^2d r \\ \end{pmatrix}^{-1}\\ &\times\; \begin{pmatrix} W(1)\\ 1/2\{W(1)^2-1\}\\ \end{pmatrix}\\ \end{align*} The inverse is $$ \Delta^{-1}\begin{pmatrix} \int_0^1[W(r)]^2d r & -\int_0^1W(r)d r \\ -\int_0^1W(r)d r & 1 \\ \end{pmatrix}$$ where $$ \Delta\equiv\int_0^1[W(r)]^2d r-\left[\int_0^1W(r)d r\right]^2 $$ All in all, this means that the second element of the scaled error converges to \begin{align*} T(\hat{\rho}_T-1)&\Rightarrow\frac{1/2\{W(1)^2-1\}-W(1)\int_0^1W(r)d r}{\Delta}\\ &=\frac{1/2\{W(1)^2-1\}-W(1)\int_0^1W(r)d r}{\int_0^1[W(r)]^2d r-\left[\int_0^1W(r)d r\right]^2} \end{align*} Using analogous arguments, we can find a $t$-ratio type distribution, $$ t_{T,\alpha}\Rightarrow\frac{1/2\{W(1)^2-1\}-W(1)\int_0^1W(r)d r}{\left\{\int_0^1[W(r)]^2d r-\left[\int_0^1W(r)d r\right]^2\right\}^{1/2}} $$ This is the distribution that is used to produce the p-values in for example the above output - implying that this is the model we need to have in mind if these are p-values we wish to trust in.
