[site]: crossvalidated
[post_id]: 291583
[parent_id]: 
[tags]: 
Support vector regression with one large outlier

I have a question regarding support vector regression, best summarized by the chart below on simulated data of a linear function with a bit of noise. In essence, why does increasing epsilon rotate the fitted line away from the outlier? Note: the simulated data is the red dots. Green line is a naive lm fit, blue lines are svr predictions for varying values of epsilon. Motivation: trying to understand SVR in the face of a single large outlier. Code here: rm(list = ls()) library('e1071') library('tidyverse') n % arrange(x) res % ggplot(aes(x = x, y = pred, group = eps, color = eps)) + geom_line(aes(colour = eps), size = 1.2) + geom_point(data = df, aes(x = x, y = y), alpha = 1, col = 'red') + geom_line(data = dfLM, aes(x = x, y = predLM), colour = 'green', linetype = 2, size = 1.2) In particular the second link listed below (part way down) says, based on the Smola paper (link 3): In SVR we want to maximize the prediction error in a defined precision, epsilon for better generalization. Here if we minimize the prediction error instead of maximize , the prediction result on unknown data is more likely to be over-fitted. Thoughts greatly appreciated...finding SVR slightly unintuitive vs. SVM for classification. P.S. Links that may be useful (I don't have enough reputation points to post all as hyperlinks) - remove the @@ which I inserted to be allowed to post them: https:@@//stats.stackexchange.com/questions/13194/support-vector-machines-and-regression https:@@//stats.stackexchange.com/questions/5945/understanding-svm-regression-objective-function-and-flatness https:@@//alex.smola.org/papers/2004/SmoSch04.pdf
