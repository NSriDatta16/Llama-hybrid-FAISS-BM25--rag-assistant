[site]: crossvalidated
[post_id]: 262029
[parent_id]: 
[tags]: 
Random forest retraining on only correctly predicted records

A colleague of mine suggested the following procedure for a categorization problem: 1) Train a random forest 2) Select only the records from the training data set which the random forest predicted correctly (based on in sample predictions) 3) Train a random forest on this subset 4) Use this random forest to make predictions for new data (out of sample) He claims that this procedure is helpful for removing noise from the data as wrongly predicted records in the training sample are likely to be noise. However, I think that this is not a good procedure because I cannot imagine how predictive capability of the model should improve by excluding wrongly predicted data. We just retrain a new random forest on data we could already predict correctly with the first one, and we cannot know whether the wrongly predicted records are really noise or whether our model just didn't capture them correctly. Furthermore, I haven't heard or read anything about such an approach before. Does anybody have any thoughts on this? I'd also highly appreciate any hints to papers in which this procedure has been applied before.
