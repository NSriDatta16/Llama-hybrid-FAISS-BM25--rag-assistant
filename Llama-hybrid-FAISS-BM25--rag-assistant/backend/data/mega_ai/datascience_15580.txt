[site]: datascience
[post_id]: 15580
[parent_id]: 15553
[tags]: 
I always consider features selection as a step to a final result. Hereunder, I somehow mix features selection and dimensionality reduction, which might have some goals and can be confused. Some typical uses: reduction of computations in machine learning: the quality of the selection is a factor of the final learning result and also, obviously, the speed to get that learning done visualization/understanding of the data, where you combine eventually multiple dimensions. It is good when it doesn't hides interesting stuffs, and when that's understandable simplification of the learning results, still to make them understandable (eg root cause analysis). Good if simple but still sufficient in terms of quality controlling over fitting, as the previous reply suggests ... So, I don't think there's general rule (as always in ML), but this is a case by case problem. Just a personal belief...
