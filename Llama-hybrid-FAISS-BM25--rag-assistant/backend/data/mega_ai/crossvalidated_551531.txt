[site]: crossvalidated
[post_id]: 551531
[parent_id]: 36298
[tags]: 
A decision tree that is very deep or of full depth tends to learn the noise in the data. They overfit the data leading to low bias but high variance. Pruning is a suitable approach used in decision trees to reduce overfitting. However, generally random forests would give good performance with full depth. As random forests training use bootstrap aggregation (or sampling with replacement) along with a random selection of features for a split, the correlation between the trees (or weak learners) would be low. That means although individual trees would have high variance, the ensemble output will be appropriate (lower variance and lower bias) because the trees are not correlated. If you still want to control the training in a random forest, go for controlling the tree depth instead of pruning.
