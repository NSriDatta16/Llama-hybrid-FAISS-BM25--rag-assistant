[site]: datascience
[post_id]: 103005
[parent_id]: 
[tags]: 
Can a reformer model really handle long-range dependency?

I read this article about new attention model called Reformer . Here is the main strength of this model: The Reformer pushes the limit of longe sequence modeling by its ability to process up to half a million tokens at once as shown in this demo. Is it actually true? Because later in jupyter notebook I see the following in a config: config = { "attention_head_size": 64, "attn_layers": ["local", "lsh", "local", "lsh", "local", "lsh"], "lsh_attn_chunk_length": 64, "local_attn_chunk_length": 64, "lsh_num_chunks_before": 1, "lsh_num_chunks_after": 0, "local_num_chunks_before": 1, "local_num_chunks_after": 0, } This config tells that they use chunks, so attention can span only 64 tokens and 1 chunk before it. Does it mean that there is no any long-range dependency can be learned by this model?
