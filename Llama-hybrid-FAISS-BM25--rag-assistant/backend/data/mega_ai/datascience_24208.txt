[site]: datascience
[post_id]: 24208
[parent_id]: 24179
[tags]: 
Increasing the number of data points and using KernelPCA. Increasing the number of points for the PCA model increased also the scores of that model, something I overlooked. Furthermore, going to a KernelPCA component analysis also looked promising here (using 'rbf' kernel and n_dimensions=20 for the kernel model). All of the following graphs are produced using the raw data (so no sorting, editing... happened). Optimizing the KernelPCA transformer with gridsearchCV yields that a Pipeline with KernelPCA model with parameters {'kernelpca__kernel': 'cosine', 'kernelpca__n_components': 21, 'svc__C': 2154.4346900318847} gives best results (where C could possibly be tweaked a bit further). However, this results in an accuracy of over 98% which is very satisfying. The final model looks like this: kernelmodel = make_pipeline(KernelPCA(21, kernel='cosine'), Imputer(strategy='median'), svm.SVC(C=2000)) Where the imputer is added to account for som NaNs that were created (?) within the KernelPCA. And results in the following graph Other recommendations are welcome!
