[site]: crossvalidated
[post_id]: 412402
[parent_id]: 
[tags]: 
Is there any way to make the notion of a "policy" in reinforcement learning less abstract?

In many reinforcement learning related literature, I see the author suddenly introduces an abstract function called policy $\pi$ which maps from the state to actions. In other words, $\pi$ is a function, such that $\pi: s \mapsto a$ . Because the domain and range of this function are usually discrete, therefore this function do not have the usual properties when it comes to functions, such as continuity, differentiability, Lipischitzness, smoothness, etc. Therefore there is nothing general that we can say about this function, am I correct? To make matters worse, often authors will freely use the term "expert policy", $\pi^{\text{expert}}$ . The expert here usually being a human, so this expert policy is literally a function that models how the human brain works or react to a highly complicated environment. I am pretty sure there is no closed form description of such an object. There is virtually nothing we can ever say about this function either. Is there any way or any existing literature that attempts to make the concept of a policy less abstract? What is the simplest policy that you can have for a non-trivial scenario? Overall, I find it difficult to appreciate this concept and see how theory can connect to practice if I don't see a closed-form examples of a policy.
