[site]: datascience
[post_id]: 10881
[parent_id]: 10803
[tags]: 
Turian, Joseph, Lev Ratinov, and Yoshua Bengio. " Word representations: a simple and general method for semi-supervised learning ." Proceedings of the 48th annual meeting of the association for computational linguistics. Association for Computational Linguistics, 2010. define distributional representations and distributed representations as follows: A distributional word representation is based upon a cooccurrence matrix $F$ of size $W×C$, where $W$ is the vocabulary size, each row $F_w$ is the initial representation of word $w$, and each column $F_c$ is some context. Sahlgren (2006) and Turney and Pantel (2010) describe a handful of possible design decisions in contructing $F$, including choice of context types (left window? right window? size of window?) and type of frequency count (raw? binary? tf-idf?). $F_w$ has dimensionality $W$, which can be too large to use $F_w$ as features for word w in a supervised model. One can map $F$ to matrix f of size W × d, where $d A distributed representation is dense, low-dimensional, and real-valued. Distributed word representations are called word embeddings. Each dimension of the embedding represents a latent feature of the word, hopefully capturing useful syntactic and semantic properties. A distributed representation is compact, in the sense that it can represent an exponential number of clusters in the number of dimensions. FYI: What's the difference between word vectors, word representations and vector embeddings?
