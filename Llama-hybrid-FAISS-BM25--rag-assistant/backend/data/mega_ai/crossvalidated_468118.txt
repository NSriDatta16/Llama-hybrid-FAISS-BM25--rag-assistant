[site]: crossvalidated
[post_id]: 468118
[parent_id]: 
[tags]: 
Train/test split and leave-one-out

I need to build and evaluate a classifier over 100 examples. There are reasons for which I need to use a leave-one-out cross validation approach, and I have a doubt on how I should proceed. Which, among the 2 following options, is the correct one? a) perform a LOO by creating 100 folds over 1-VS-99 and consider the average performance on the 100 folds as the performance for my classifier b) split the 100 examples on 70 for training and 30 for test, then apply the LOO strategy only to the 70 training examples (with 1-VS-69 folds), and then evaluate the model by applying it on the 30 test examples
