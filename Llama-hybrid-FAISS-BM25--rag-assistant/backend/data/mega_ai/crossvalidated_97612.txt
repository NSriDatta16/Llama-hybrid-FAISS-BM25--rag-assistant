[site]: crossvalidated
[post_id]: 97612
[parent_id]: 97555
[tags]: 
I would like to bring to your attention also that in the original SMOTE paper, the good results were based on both combining SMOTE and random under-sampling. This is because applying SMOTE to achieve an equal balance with the majority class is not necessarily the best case for the classifier and as your results show. Thus, you may under-sample the majority to different percentages of the original majority class and then (say 25%, 50%, 75%) , apply SMOTE to minority samples with different numbers of synthetically generated samples (say 2, 3, 4). You end up with a combination of cases and you may choose the one showing better cross-validated results.
