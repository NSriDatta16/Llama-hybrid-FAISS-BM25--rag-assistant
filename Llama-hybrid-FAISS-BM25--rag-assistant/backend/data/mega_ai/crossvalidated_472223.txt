[site]: crossvalidated
[post_id]: 472223
[parent_id]: 472214
[tags]: 
This is uncommon and can be quite risky, especially when there are multiple local minima for the cost function. You might even end up in the neighbourhood of a local maximum after averaging them out! This is similar (not the same since subsets are not bootstrap samples) to ensembling (e.g. random forests). Not very common, and not preferable when training is expensive. It might be useful when you want to reduce the variance of your model, but there are other ways to reduce it as well. This is the most common approach. Preferable since in the end you can claim one model and it'll be cheaper than training $K$ models. Moreover, in this one, your model sees the whole data together.
