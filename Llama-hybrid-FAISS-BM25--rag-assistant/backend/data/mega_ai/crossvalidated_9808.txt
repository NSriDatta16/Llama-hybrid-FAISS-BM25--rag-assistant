[site]: crossvalidated
[post_id]: 9808
[parent_id]: 8511
[tags]: 
Don't forget the rms package, by Frank Harrell. You'll find everything you need for fitting and validating GLMs. Here is a toy example (with only one predictor): set.seed(101) n This yields: Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) 0.8959 0.1969 4.55 5.36e-06 *** x -1.8720 0.2807 -6.67 2.56e-11 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 258.98 on 199 degrees of freedom Residual deviance: 181.02 on 198 degrees of freedom AIC: 185.02 Now, using the lrm function, require(rms) mod1b You soon get a lot of model fit indices, including Nagelkerke $R^2$, with print(mod1b) : Logistic Regression Model lrm(formula = y ~ x) Model Likelihood Discrimination Rank Discrim. Ratio Test Indexes Indexes Obs 200 LR chi2 77.96 R2 0.445 C 0.852 0 70 d.f. 1 g 2.054 Dxy 0.705 1 130 Pr(> chi2) |Z|) Intercept 0.8959 0.1969 4.55 Here, $R^2=0.445$ and it is computed as $\left(1-\exp(-\text{LR}/n)\right)/\left(1-\exp(-(-2L_0)/n)\right)$, where LR is the $\chi^2$ stat (comparing the two nested models you described), whereas the denominator is just the max value for $R^2$. For a perfect model, we would expect $\text{LR}=2L_0$, that is $R^2=1$. By hand, > mod0 lr.stat (1-exp(-as.numeric(lr.stat$stats[1])/n))/(1-exp(2*as.numeric(logLik(mod0)/n))) [1] 0.4445742 > mod1b$stats["R2"] R2 0.4445742 Ewout W. Steyerberg discussed the use of $R^2$ with GLM, in his book Clinical Prediction Models (Springer, 2009, § 4.2.2 pp. 58-60). Basically, the relationship between the LR statistic and Nagelkerke's $R^2$ is approximately linear (it will be more linear with low incidence). Now, as discussed on the earlier thread I linked to in my comment, you can use other measures like the $c$ statistic which is equivalent to the AUC statistic (there's also a nice illustration in the above reference, see Figure 4.6).
