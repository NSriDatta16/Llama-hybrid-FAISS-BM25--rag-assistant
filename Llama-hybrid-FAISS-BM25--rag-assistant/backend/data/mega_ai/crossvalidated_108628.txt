[site]: crossvalidated
[post_id]: 108628
[parent_id]: 108577
[tags]: 
This won't be rigorous, but it should give you a feel for why it might often tend to occur: 1) Imagine you were calculating not the $n-1$ denominator variance, but the $n$-denominator version (this only gives a scaling factor, so it doesn't impact the shape you see... and that scaling factor goes to 1 in the limit) 2) Consider that as sample sizes become large, the distribution of $X_i-\overline{X}$ approaches the distribution of $X_i-\mu$ (e.g. via Slutsky's theorem ). 3) Now consider $Y=(X_i-\mu)^2$; by the Central Limit theorem $\sqrt{n}(\overline{Y}-E(Y))$ converges to a normal distribution, as long as the conditions hold (e.g. you need $\text{Var}(Y)$ to exist). Further note that $E(Y)=\sigma^2$. So - in essence because the sample variance is effectively just a kind of average - you might not be surprised to see sample variance to approach normality (centered at the population variance) as sample sizes become large. In Asymptotic Statistics , A. W. van der Vaart pursues a somewhat more rigorous argument ( see end p26-p27 ) by writing the $n$-denominator form of variance as a function of $\overline{X}$ and $\overline{X^2}$ and looking at the multivariate CLT applied to the pair. However, you have to be careful - in some situations at least - this approximate normality doesn't hold. All the conditions must hold for the argument to go through. (In particular, there's something a little unusual about $(\overline{X}, \overline{X^2})$ for the Bernoulli($\frac{1}{2}$) case.) Consider the Bernoulli distribution with $p=\frac{_1}{^2}$; for that case, as sample sizes become large, sample variance doesn't approach normality. In that case, at least, the distribution of the sample variance - even in quite large samples - doesn't become more normal.
