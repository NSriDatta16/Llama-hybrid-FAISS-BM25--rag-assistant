[site]: crossvalidated
[post_id]: 587618
[parent_id]: 317073
[tags]: 
When there is little information, gradients of the loss function will tend to change slower, hence a smaller hessian. In the MLE framework, the negative of the hessian is known as the observed Fisher information. Ignoring the sign, a larger hessian will mean that more information is available. You don't want splits to happen when there is too little information. This shortage of information manifests in different ways for different loss functions, some of which were already described in another answer: smaller sample size for ordinary least squares regression and similar for logistic regression but now also weighted by the impurity $p(1-p)$ expected by the current model (so smaller and purer samples will be the less informative ones). Also notice that since the score of a leaf is related to $\frac{\sum grad}{\sum hess}$ , a very small $\sum hess$ will make the ratio unstable, which is another way this lack of information manifests.
