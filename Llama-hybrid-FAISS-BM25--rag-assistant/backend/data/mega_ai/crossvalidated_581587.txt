[site]: crossvalidated
[post_id]: 581587
[parent_id]: 
[tags]: 
Proper masking in MultiHeadAttention layer in Keras

I am new to Transformers and I am trying to create a very simple model (not NLP area) for processing data of variable length (not sequence data because for my problem order in data does not matter). Basically, max length of data that I defined (number of vectors) is 10, and each vector has dimension 2. Because of problem domain, different inputs have different number of vectors, but the rest of input tensor is always padded with some value (e.g. -10000 because 0 has certain meaning for my data). Below is example of 1-batch input with 4 vectors that have some meaning and other vectors with -1.0e+5 pad value. array([[[ 1.7e-01, -2.2e-01], [ 1.7e-01, 1.8e-01], [-3.7e-01, 3.7e-01], [-3.7e-01, 8.0e-02], [-1.0e+05, -1.0e+05], [-1.0e+05, -1.0e+05], [-1.0e+05, -1.0e+05], [-1.0e+05, -1.0e+05], [-1.0e+05, -1.0e+05], [-1.0e+05, -1.0e+05]]]) Now, I am using Keras MultiHeadAttention layer that has the option of masking part of the input for attention weigths. Call argument for this option is attention_mask described in Keras docs ( https://keras.io/api/layers/attention_layers/multi_head_attention/ ): a boolean mask of shape (B, T, S), that prevents attention to certain positions. The boolean mask specifies which query elements can attend to which key elements, 1 indicates attention and 0 indicates no attention. Broadcasting can happen for the missing batch dimensions and the head dimension So the mask should be tensor of zeros and ones, with ones at positions for which attention will be calculated. For my problem queries, keys and values are all the same (input data), and the model looks like this: def build_multihead_attention_model(): input_layer = Input(shape = (10, 2), name = 'input') mask = ...mask somehow caluctaed for input_layer multihead_layer = MultiHeadAttention(num_heads=1, key_dim=3) attention_output = multihead_layer(input_layer, input_layer, attention_mask = mask, return_attention_scores = True) model = Model(inputs = input_layer, outputs = attention_output) return model I tried to find some easy way how to calculate this mask depending on the input layer (number of input vectors that are not padded vectors), but I wasn't successful - I haven't find a single example for this general case of variable size input. How should this mask be calculated? Input data are just numbers, not words or not embeddings. Order in data does not matter, but padded vectors are at the end of the input tensor. Is there already a layer for this that could be used, like Masking layer in Keras? Can someone please provide a convenient way how to do this in Keras? P.S. If mask looks for example like this: [[1. 1. 1. 1. 1. 0. 0. 0. 0. 0.] [1. 1. 1. 1. 1. 0. 0. 0. 0. 0.] [1. 1. 1. 1. 1. 0. 0. 0. 0. 0.] [1. 1. 1. 1. 1. 0. 0. 0. 0. 0.] [1. 1. 1. 1. 1. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] then should all attention weights (for all attention heads) be also 0 for all zero-positions in the mask?
