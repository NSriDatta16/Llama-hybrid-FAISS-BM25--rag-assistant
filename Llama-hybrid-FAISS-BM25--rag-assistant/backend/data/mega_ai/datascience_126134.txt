[site]: datascience
[post_id]: 126134
[parent_id]: 
[tags]: 
CDF/PDF vs Monte Carlo

I’m a developer reading the book Bayesian statistics the fun way In chapter 15 they use hypothesis testing using Monte Carlo simulation to pick random values from two intercepting beta distributions I want to know if they could have used PDF/CDF instead? Why did they choose Monte Carlo when you can get a solid value from CDf/PDF method of getting the probability from a distribution? And compare which one is better in the case needed? We’ve covered the basics of parameter estimation pretty well at this point. We’ve seen how to use the PDF, CDF, and quantile functions to learn the likelihood of certain values, and we’ve seen how to add a Bayesian prior to our estimate. Now we want to use our estimates to compare two unknown parameters. The accurate answer to which email variant generates a higher click-through rate lies somewhere in the intersection of the distributions of A and B. Fortunately, we have a way to figure it out: a Monte Carlo simulation Clearly, our data suggests that variant B is superior, in that it garners a higher conversion rate. However, from our earlier discussion on parameter estimation, we know that the true conversion rate is one of a range of possible values. We can also see here that there’s an overlap between the possible true conversion rates for A and B. What if we were just unlucky in our A responses, and A’s true conversion rate is in fact much higher? What if we were also just lucky with B, and its conversion rate is in fact much lower? It’s easy to see a possible world in which A is actually the better variant, even though it did worse on our test. So the real question is: how sure can we be that B is the better variant? This is where the Monte Carlo simulation comes in.
