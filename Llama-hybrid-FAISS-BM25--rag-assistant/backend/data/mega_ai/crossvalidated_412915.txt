[site]: crossvalidated
[post_id]: 412915
[parent_id]: 
[tags]: 
Shannon's Entropy, manual vs. analytic results differ

Let's say that I have information string A, B, C, D, E. All letters are equally probable (1/5). So Shannon's formula would give us. # Original Formula sum(p * log(1/p,2)) # Which can be simplified in this case as 0.2*log(5,2)*5 = 2.321928 However, if I manually try to count the number of bits that have to asked, I come up with 2.4. ABCDE / \ ABC DE / \ / \ AB C D E / \ A B 2 letters require 3 questions and 3 letters require 2 questions. This means that the average number of questions should be (3+3+2+2+2)/5 = 2.4 Can anyone explain where I am going wrong?
