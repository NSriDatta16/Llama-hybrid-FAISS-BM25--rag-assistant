[site]: crossvalidated
[post_id]: 447333
[parent_id]: 
[tags]: 
In image segmentation, is Dice score usually reported as an average between classes?

Dice Similarity or Dice Score is a common evaluation metric for segmentation projects with high class-imbalance. It measures the overlap agreement between discrete classes from two images, ranging between 0 - 1 per class. Usually papers report one single Dice score between 0-1, but it is unclear if this refers to the average of all classes, or just the score for the class of interest. For example if my task is about segmentation into two classes (foreground / background), where background is 99% of the image, then an algorithm than only outputs background will have dice scores of [0.99, 0.0] for background foreground, giving an average of 0.5. I find it is pretty much never specified in those cases if the reported dice score refers to the average, some weighted average, or just the foreground. The lack of information makes me think there is some "obvious" or default way of doing this? Example: https://ieeexplore.ieee.org/abstract/document/8438531 --> "achieving a mean Dice similarity coefficient (DSC) of 0.72"
