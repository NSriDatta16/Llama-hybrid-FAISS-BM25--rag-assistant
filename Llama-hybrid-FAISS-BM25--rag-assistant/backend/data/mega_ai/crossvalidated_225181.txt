[site]: crossvalidated
[post_id]: 225181
[parent_id]: 
[tags]: 
The impact of the number of dimensions on classification/predictive performance

For the sake of simplicity, let's say we have a rectangular dataset where the columns are fields/variables and the rows are observations/events. According to the curse of dimensionality, the higher the number of fields, the poorer the classification/clustering of data points (the distance function used may not be able to cope with the high number of dimensions; all pairs of data points will be equally far/close). Of course, there are other side-effects of having high dimensions too. However, in some algorithms and learning approaches, it seems that to improve classification/prediction is to, in fact, increase the dimensions. For example, with support vector machines we often add dimensions to make data points separable. Another example, we often try feature engineering to add dimensions to improve classification. So, what's the general story or reasoning behind dimensions? Does it simply depends on when to increase or reduce? Is there a general framework and/or study on determining when to add or remove features? Clearly, I have seen research to support either directions to improve classification/prediction. But I wonder if there are any more coherent studies pitting these directions against one another and understanding their behavior in general.
