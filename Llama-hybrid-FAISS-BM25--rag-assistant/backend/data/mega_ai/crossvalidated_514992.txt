[site]: crossvalidated
[post_id]: 514992
[parent_id]: 
[tags]: 
How to decode an output from a phoneme recognition model?

I created a phoneme recognition model (based on a pre-embedding with Wav2vec, and some layers on top of it) that takes as input an audio signal, and outputs a softmax matrix of size $N \times C$ , where $N$ depends on the length of the audio signal, and $C$ is fixed and denotes the number of possible phonemes. To train the model, I used the ctc_loss of Pytorch. However, I don't know how I should decode the softmax matrix to obtain the best phoneme sequence prediction for a given audio signal. Should I take the class with the highest probability at each time step? Or consider a dynamic programming approach to obtain the best path overall? In any case, I will need to reduce the length of the path. What is the strategy to merge consecutive phonemes? And is all of this directly implemented somewhere? Thanks in advance.
