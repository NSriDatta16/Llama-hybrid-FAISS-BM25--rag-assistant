[site]: crossvalidated
[post_id]: 372720
[parent_id]: 243412
[tags]: 
There is another angle of this paper which can be helpful if you are already applying a Bayesian analysis and don't care about the statistical significance part. Suppose $P$ is the posterior CDF of the quantity $\beta$ (effect size) you are interested in estimating. In the Bayesian situation, taking some liberty with notation and switching to talk about probability density functions, you will have a likelihood function based on some observable quantity $V$ , and some pure prior of $\beta$ : $$ p(\beta | V) \sim p(V | \beta)p(\beta) $$ Here $V$ is likely to be a vector quantity, in the simplest case being a vector of multiple independent observations from which the usual product of likelihood terms arises, turning into a sum of log terms, etc. The length of that vector $V$ would be a parameterization of the sample size. In other models, say where $p(V | \beta)$ is Poisson, it might be rolled up into the Poisson parameter, which also expresses a parameterization of sample size. Now suppose you make a hypothesis $\beta^{plausible}$ based on literature review or other means. You can use your assumed data generating process $P(V | \beta)$ with $\beta = \beta^{plausible}$ to generate simulations of $V$ , which represent what data you would see if your model is well specified and $\beta^{plausible}$ is the true effect size. Then you can do something sort of stupid: turn around and act like that sample of $V$ is the observed data, and draw a bunch of samples of $\beta$ from the overall posterior. From these samples, you can compute the statistics as mentioned in the paper. The quantities from the linked paper, type S error and exaggeration ratio, already represent pretty much the same thing. For that effect size, given your model choices, these will tell you for a given parameter of sample size chosen for $V$ , what the posterior probability of the wrong sign is and what the expected (in the posterior) ratio will be between the effect size produced by the model and the assumed plausible effect size, as you vary whatever aspect of $V$ relates to sample size. The trickiest part is interpreting posterior "power" as the posterior probability that the estimated value of $\beta$ is at least as large as the hypothetical value $\beta^{plausible}$ . This is not a measure of capacity to reject the null hypothesis, since the size of this probability would not be used as a significance measure in the frequentist sense. I don't really know what to call it, except to say that I have had several applications in practice where it is a very helpful metric to reason about for study design. It basically offers you some way to see how much data you need to provide (assuming your data is generated perfectly from a process utilizing $\beta^{plausible}$ ) for a particular assumption about likelihood and prior shapes to result in some "sufficiently high" posterior probability of an effect of a certain size. Where this has been most helpful for me in practice is in situations where the same general model needs to be repeatedly applied to different data sets, but where nuances between the data sets might justify changing the prior distribution or using a different subset of literature review to decide what's a pragmatic choice of $\beta^{plausible}$ , and then getting a rough diagnostic about whether these adjustments for different data sets would result in a case where you'll need severely much more data to have non-trivial probability in the posterior concentrated in the right part of the distribution. You have to be careful that nobody misuses this "power" metric like it is the same thing as a frequentist power calculation, which is quite hard. But all of these metrics are quite useful for prospective and retrospective design analysis even when the whole modelling procedure is Bayesian and won't refer to any statistical significance result.
