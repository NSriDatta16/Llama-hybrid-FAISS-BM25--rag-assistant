[site]: datascience
[post_id]: 101881
[parent_id]: 
[tags]: 
All machine learning models are giving the same accuracy

I have a small dataset (2000 rows) and am testing different algorithms for binary classification. The data set is very small but I do not have the option of increasing the dataset. I have tested a multilayer perceptron (deep learning), xgbooster, logistic regression and they all give an accuracy of $\pm 60$ % $(59, 58, 62, 61, \text{etc.})$ no matter what I change in parameters they all give a similar accuracy. I know the dataset is small, but I would like to understand why this is happening, and how I can potentially fix it. I am even trying to remove some features from the model, all still giving $\pm60$ % Here is the code for the deep learning model model = Sequential() model.add(Dense(1, input_dim=X.shape[1], activation='relu')) model.add(Dense(4, activation='relu')) model.add(Dense(1, activation='sigmoid')) history = model.fit(X_train, y_train,epochs=60, batch_size=20,validation_split=0.1) Attaching some sample learning curves for the deep learning model
