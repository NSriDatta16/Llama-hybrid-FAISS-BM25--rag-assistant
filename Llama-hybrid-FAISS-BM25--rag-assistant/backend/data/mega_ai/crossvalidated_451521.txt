[site]: crossvalidated
[post_id]: 451521
[parent_id]: 
[tags]: 
Feature Selection - Overfit?

I have a dataset of 100 patients and 1500 features. I split 80 train 20 test first and then use the train set to get the best hyperparameters / best feature subset doing the following: I randomly split the train set into 70% train 30% "test" (I don't use it) X times and pass it through a classifier (Random Forest, L1 Logistic Reg, etc). Then, I take the most important features (from feature importance in RF or weights in LR) passing a certain threshold (mean/median of weights/importance). For each split, I save this list and make a counter. After X iterations, I have a counter like {feature 1: Y times, feature 2: Z times, etc}. Then I perform backwards elimination on the top features using the complete train set (the 80% of the complete dataset) and use K-fold stratified CV to find the hyperparameters for each BE iteration. My mainly concern lies in the feature counter I am creating. I am using 70% of 80% of the total dataset to get these features. Note that I never use the 20% test set I set aside at the very beginning. Am I overfitting at any step?
