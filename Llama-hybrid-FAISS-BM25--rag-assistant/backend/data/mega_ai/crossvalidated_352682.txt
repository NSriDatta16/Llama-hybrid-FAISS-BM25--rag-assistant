[site]: crossvalidated
[post_id]: 352682
[parent_id]: 352652
[tags]: 
You are extremely perceptive in the way you've asked the question. To begin with, the second definition is completely wrong. For it to be in a dictionary of social research methods is quite astounding. First of all, a type I error is a result or an action, not a probability (the dictionary should have said "type I error probability or type I error rate (but see below)" if they wanted to address a risk. But a more fundamental problem with definition 2. is that null hypothesis statistical tests (NHST) assume the null hypothesis is true. The probability of being wrong about rejecting the null hypothesis is the prob. the null is true when you reject. Frequentists can't know that but have already assumed the null is true. You are also very perceptive in using words like "measure of error" or "astrayness". Below I try to use slightly better wording. "Errors" in NHST are not really errors in the traditional sense. The p-value is "the degree to which the data are embarrassed by the null hypothesis" and is a measure of "surprise" given by the data if H0 is true. Type I error probability is the probability of making an assertion of a nonzero effect if the effect is truly zero. Once you understand the difference between assertions and conclusions, things get more clear. These issues have been written about by William Briggs, Steve Goodman, and many others, and I am putting together training materials that goes into details and gives a lot of references. I go into detail about some of the concepts in some of my blog articles here . William Briggs has decoded Ronald Fisher's statement "either an event of low probability has been observed or the null hypothesis is false" to be logically equivalence to "an event of low probability has been observed". So p-values tell us less than we need. Cohen has shown that proof by contradiction doesn't really work when uncertainties are involved; it only works with logical certainties. The shortest definition of p-values and type I error probabilities I can come up with are as follows. The p-value is the probability that someone else's data will be more impressive than yours if their data were generated with the null hypothesis in force. If you set a rejection levels of $\alpha=0.05$ then type I error is the probability that the p-value will be less than 0.05. This is by definition 0.05 if the p-value is computed exactly correctly, there was only one look at the data, and the sample size is a constant. When multiplicities exist as when doing sequential testing, and if each test is done at a nominal $\alpha$ level, the type I error will be $> \alpha$. If you use a hard-and-fast rule that you'll make an assertion of an effect if p Pure subjective Bayesian statistics could be said to not require complex notions of inference and counterfactuals and use only observed quantities in calculations. Bayesian posterior probabilities allow you to carry along probabilities that assertions hold, to make optimum decisions. Once you think probabilistically, notions of assertions, conclusions, inductive vs. deductive reasoning, etc. can be somewhat ignored. Posterior probabilities are fully conditional on the data observed so far, and quantities such as P(unknown effect > 0 | data) will be low when the data are tilted away from a positive effect. One minus this probability is the probability that the effect is zero or is in the wrong direction. So Bayesian posterior probabilities have a dual purpose - providing both evidence for and evidence against unknown effects. With NHST you can only amass evidence against H0, never evidence in favor of H0.
