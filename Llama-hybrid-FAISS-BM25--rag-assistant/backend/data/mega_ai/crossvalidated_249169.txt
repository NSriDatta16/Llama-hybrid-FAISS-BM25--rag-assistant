[site]: crossvalidated
[post_id]: 249169
[parent_id]: 249160
[tags]: 
One easy example is the linear kernel $k(x, y) = x^T y$, so that $k(x, x) = \lVert x \rVert^2$, or the polynomial kernel $k(x, y) = (x^T y + c)^d$. In general, this kind of kernel is known as nonstationary : its values vary over the input space. There are an infinite number of them, but I'm not aware of any in common usage other than the two I just listed. There's an interesting subclass of these kernels discussed by: Genton (2001). Classes of Kernels for Machine Learning: A Statistics Perspective . Journal of Machine Learning Research 2 299â€“312. ( official pdf ) locally stationary kernels, originated perhaps by Silverman, of the form $$ K(x, y) = K_1\left( \frac{x + y}{2} \right) K_2\left( x - y \right) ,$$ so that the normal stationary kernel $K_2$ is scaled by a function of the average location $K_1$. The following paper I haven't read discusses some other options: Paciorek and Schervish. Nonstationary Covariance Functions for Gaussian Process Regression. NIPS 2004. ( official pdf )
