[site]: datascience
[post_id]: 109795
[parent_id]: 109759
[tags]: 
Some ideas are: a) Generate many many random sentences until you find your words (will be extremely slow, but the samples will be unbiased). b) For very short sentences you can iterate over all positions and all possibilities to fill unknown positions like (today, neural networks,?), (?, today, neural networks), (neural networks, ?, today), etc. and use the language model to compute the probability of each sentence, then select the one with highest probability or one at random with weights equal to probabilities. Actually if you can generate all possible positions then you should be able to use a pretrained BERT model directly since it is trained on a similar Masked Language Model task. c) Train a new language model from scratch, or fine tune a pretrained model (like BERT), with inputs (or sentence pairs) like today, freenode, neural networks # Today I joined the freenode channel for neural networks. You can sample the words or n-grams from existing sentences. This might require a large corpus, some work and some cost. d) If you have access, try existing large models like gpt-3. For example, I tried transformer.huggingface.co, with the last sentence autocompleted by the transformer: Please make a sentence containing the given words. Words: horse, tree, walked. Sentence: The horse walked around the tree. Words: freenode, today, neural network. Sentence: The freenode sat on the tree. Update Same experiment on https://6b.eleuther.ai/ Sentence: The freenode today uses a neural network to classify images.
