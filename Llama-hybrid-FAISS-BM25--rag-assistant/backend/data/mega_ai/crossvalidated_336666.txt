[site]: crossvalidated
[post_id]: 336666
[parent_id]: 
[tags]: 
prior for initial values of Kalman Filter

I'm studying Carter and Kohn's (1994) implementation of the Gibbs sampler for Bayesian analysis of state space models. In their paper, they assume the starting value, call it $\beta_0$, of the state vectors has a proper distribution. Applications of their algorithm (such as the time-varying vector autoregression of Primiceri (2005)) also list priors for the initial values of the state vector such as $\beta_0 \sim N(0, \Omega_0)$ for the state space model $$ y_t = \beta_t y_{t-1} + e_t$$ $$ \beta_t = \beta_{t-1} + \epsilon_t,$$ where $e_t$ and $\epsilon_t$ are independent of each other and normally distributed. Another common prior is centered on the ordinary least squares estimate of $\beta$, $\hat{\beta}$. I don't understand how a prior on $\beta_0$ is implemented here. Is the prior implemented through the initial value of the Kalman Filter? That is to say, a prior for the starting value $\beta_0$ where $\beta_0\sim~N(0, \Omega_0)$ begins the filter recursions with state vector $\beta_0 = 0$ and prediction covariance $\Omega_0$? Or is there something I'm missing? Another post has referenced the use of priors in the time-varying parameter VAR, but it doesn't explain the derivation of the implementation/derivation of the conditional posterior from the prior. Thanks for your time!
