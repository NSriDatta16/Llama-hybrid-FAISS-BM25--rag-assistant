[site]: crossvalidated
[post_id]: 78609
[parent_id]: 
[tags]: 
Outlier detection in very small sets

I need to get as accurate as possible a value for the brightness of a mainly stable light source given twelve sample luminosity values. The sensor is imperfect, and the light can occasionally "flicker" brighter or darker, which can be ignored, hence my need for outlier detection (I think?). I've done some reading up on various approaches here and can't decide on which approach to go for. The number of outliers is never known in advance and will often be zero. Flicker is generally a very large deviation from the stable brightness (enough to really mess with any average taken with a big one present), but not necessarily so. Here's a sample set of 12 measurements for completeness of the question: 295.5214, 277.7749, 274.6538, 272.5897, 271.0733, 292.5856, 282.0986, 275.0419, 273.084, 273.1783, 274.0317, 290.1837 My gut feeling is there are probably no outliers in that particular set, although 292 and 295 look a little high. So, my question is, what would be the best approach here? I should mention that the values come from taking the euclidean distance of the R G and B components of the light from a zero (black) point. It would be programmatically painful, but possible, to get back to these values if required. The euclidean distance was used as a measure of "overall strength" as I'm not interested in the color, just the strength of output. However, there's a reasonable chance that the flickers I mentioned have a different RGB composition to the usual output. At the moment I am toying with some sort of function that will repeat until a stable membership of allowed measures is reached by: Finding the standard deviation Putting everything outside say 2 SDs into an ignore list Recalculating the average and SD with the ignore list excluded Re-deciding who to ignore based on the new average and SD (assess all 12) Repeat until stable. Is there any value in that approach? All comments gratefully accepted!
