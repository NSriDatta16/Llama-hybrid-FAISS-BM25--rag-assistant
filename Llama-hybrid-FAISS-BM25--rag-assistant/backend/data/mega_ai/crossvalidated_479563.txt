[site]: crossvalidated
[post_id]: 479563
[parent_id]: 479432
[tags]: 
There are a few important distinctions I would like to make in this answer. The first is between a DAG and a parametric model. A DAG is a nonparametric system of structural equations, meaning that arrows do not necessarily represent main effects in a linear regression of an outcome on its causes. $X$ , $U_2$ , and $U_3$ may come together to form $Y$ in any number of ways, including linear or nonlinear forms, interacting or not. That is, the arrows from $X$ , $U_2$ , and $U_3$ to $Y$ represent the structural equation $$Y=f(X, U_2, U_3)$$ but they say nothing about what $f(.)$ looks like. It's possible that $f(X, U_2, U_3)$ is $\beta_0 + \beta_1 X + \beta_2 U_2 + \beta_3 U_3$ , but it could be any other form as well. Nothing about the DAG implies it is of this form or another. Statistical theory for causal inference does not depend on the functional form of $f(.)$ or of other relations in the DAG. The implications of the DAG, such as the backdoor path from $X$ to $Y$ is closed by conditioning on $U_2$ and $U_3$ , for example, are nonparametric . That means that by nonparametrically conditioning on the adjustment sets, the nonparametric association between is unbiased. Your question amounts to, "What does it mean to nonparmaterically condition on an adjustment set?" The answer is not linear regression. There are two ways of nonparametric conditioning to recover causal relationships: standardization and inverse probability weighting (IPW) . See Hernán and Robins (2006) for a nice introduction to these techniques. I'll briefly describe them here. Importantly, what I'm about to describe is not what you should do in your dataset. These methods in their purest form assume you have population data. Standardization involves conditioning on an adjustment set by creating strata based on a complete cross of every unique level of the variables in the set. For example, If $U_2$ had two unique values, and $U_3$ had three unique values, you would create six strata based on a complete cross of their levels. From here, you can compute any association between $X$ and $Y$ within each stratum, and that association represents a causal relationship. For example, you could compute the difference between the mean of $Y$ for those with $X=1$ in and the mean of $Y$ for those with $X=0$ . You could also compute a risk ratio or an odds ratio if $Y$ was binary. In each stratum, the association is unbiased. You can think of the phrase "conditional on" to mean "within strata of". If you want a single number that represents the marginal causal association (i.e., as opposed to six numbers that each represents a conditional association), you can take the sum of the conditional associations weighted by the proportion of individuals within each stratum (assuming the measure of association is collapsible). With IPW , you again form strata of the adjustment set. In each stratum, you compute the proportion of units at each level of the treatment. This is called the propensity score (PS). You can use a formula to turn the PS into inverse probability weights and then compute an association between $X$ and $Y$ using the weights (e.g., a difference in weighted means, or a ratio of weighted odds). The weighted association is unbiased for the marginal causal relationship between $X$ and $Y$ . Everything I've described so far is about populations and is only somewhat related to how you would arrive at an unbiased estimate of the causal relationship between $X$ and $Y$ with sample data . Generally, the nonparametric population versions of standardization and IPW are not available in your sample, so you have to use sample versions of them, and often it's not possible to apply the nonparametric formulas because there are not enough units within each stratum of a full cross of every covariate to estimate either the association between the treatment and outcome or the probability of treatment (this is called the "curse of dimensionality"). Instead, you have to make some simplifying functional form assumptions, which may be based in theory or on the data itself. Linear regression is a parametric, sample version of standardization that makes extremely strict assumptions about functional form. The traditional parametric sample form of IPW, which involves using logistic regression to estimate propensity scores, also makes extremely strict functional form assumptions. There is an entire field of statistics devoted to figuring out new ways of enhancing the sample versions of standardization and IPW, which I briefly discuss in this answer . I highly recommend Hernán and Robins' (2020) book , which is what I read to learn about this topic. They make very clear the distinction between what a DAG tells you about causal relationships between variables and how to use models to estimate measures of association in a sample, which I guess is the distinction that I want you to take away from this. In summary, a DAG makes implications about what variables you need to condition on to recover causal associations nonparametrically in the population. Standardization and IPW are two ways of conditioning on variables to nonparametrically recover a causal association in the population. In sample data, there are a variety of statistical methods that can be used to estimate a conditional association, including OLS and versions of IPW, both of which often make extremely strict and likely incorrect functional form assumptions.
