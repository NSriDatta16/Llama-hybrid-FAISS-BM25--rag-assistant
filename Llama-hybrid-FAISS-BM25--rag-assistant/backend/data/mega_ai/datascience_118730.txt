[site]: datascience
[post_id]: 118730
[parent_id]: 118718
[tags]: 
1- Establish general rules and patterns with a random sample of ~2% of total rows. Even if the data is significant, you have always to get "the big picture" using a random and representative sample. 2- Apply algorithms such as PCA or KMeans. But you should also use dimensional reduction algorithms such as T-SNE or UMAP. Business intelligence tools like Power BI or Qlik are interesting because they are built to visualize significant data easily. In addition to that, the algorithms' results can be plugged into them. 3- Once you identify data groups, you can increase the quantity. But always keep in mind whether it is useful or not. Do you want to extract general information from each group, or do you want to look precisely at it? 4- Eventually, once you master algorithms in small datasets, you can use the algorithms in all data or big data groups without making too many errors. One of the main issues in dealing with large datasets is the long processing time. If you try the algorithms on the whole dataset from the start, you will probably get poor results because the parameters are wrong. But if you start with simpler, fewer but representative random data, you will finally have a coherent and progressive approach to reach the 3M rows much faster with fewer errors.
