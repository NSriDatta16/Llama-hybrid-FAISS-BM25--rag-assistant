[site]: crossvalidated
[post_id]: 453170
[parent_id]: 
[tags]: 
How do I account for numerical overflow with Adaptive MCMC?

EDIT: I tested Forgottenscience's solution below and it works; however, note that I found the working acceptance criterion to be that if $\log\alpha \geq \log u$ , the point is accepted, where $u\sim\mathcal{U}(0,1)$ . I'm using Adaptive MCMC (i.e. Haario et. al 2001, link to the paper here ) to optimize the covariance matrix/Metropolis-Hastings "step sizes" for a multi-dimensional proposal distribution, in order to use this proposal distribution with a Metropolis Hastings sampler. Due to the nature of the likelihood function that I need to use, I've found that at the best-fit set of parameters, which I use as the starting point for my Markov chain for adaptive MCMC, the likelihood evaluates to numerical infinity/overflow (as well as sets of parameters near this best fit). I know that for a non-adaptive MH sampler, I could use log-likelihood methods e.g. this post , but I've found that when I follow this solution for adaptive MCMC, the MH step sizes/proposal distribution variances that Adaptive MCMC iterates to are far too small, and I'm getting a MH acceptance ratio that is way too big (about 0.98) when running the MH samplers with those step sizes. Has anyone encountered a similar problem before? Thanks! EDIT: My likelihood is of the form $\displaystyle\mathcal{L}\propto \prod_{n=1}^{N}{ \left(\frac{m_{t,n}^2\Sigma_{x,n}^2+\Sigma_{y,n}^2}{m_{t,n}^2\Sigma_{x,n}^4+\Sigma_{y,n}^4}\right)^{\displaystyle w_n/2}\times\exp\left\{{-\frac{1}{2}w_n\frac{\left[y_n-y_{t,n}-m_{t,n}(x_n-x_{t,n})\right]^2}{m_{t,n}^2\Sigma_{x,n}^2+\Sigma_{y,n}^2}}\right\}}$ , where there are $N$ datapoints $(x_n,y_n)$ , weighted by $w_n$ ; $(\Sigma_{x,n}, \Sigma_{y,n})$ are parameters related to the combined $x-$ and $y-$ uncertainties of the $n^\text{th}$ datapoint and the model I'm fitting to with this likelihood, and $m_{t,n},x_{t,n},y_{t,n}$ are parameters that are different for each datapoint. I've pinpointed the reason why this likelihood blows up; I'm working with about 700 datapoints, many of which have small values of $(\Sigma_{x,n}, \Sigma_{y,n})$ , which is making the prefactor term $\left(\frac{m_{t,n}^2\Sigma_{x,n}^2+\Sigma_{y,n}^2}{m_{t,n}^2\Sigma_{x,n}^4+\Sigma_{y,n}^4}\right)^{w_n/2}$ blow up, especially if the weight $w_n$ is high. Essentially, I can't modify these data or parameters, so it's unavoidable that this likelihood blows up. The only solution I can think of would be to somehow instead use $\ln\mathcal{L}$ (a summation instead of a product) for the Adaptive Metropolis algorithm, but I'm not sure how this would work with the Metropolis Hastings acceptance ratio.
