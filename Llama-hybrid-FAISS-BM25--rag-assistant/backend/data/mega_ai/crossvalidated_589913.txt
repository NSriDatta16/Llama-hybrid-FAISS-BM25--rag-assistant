[site]: crossvalidated
[post_id]: 589913
[parent_id]: 589902
[tags]: 
Most Bayesian applications I've seen in this setting do not use resampled data for parameter estimation but only for accuracy assessment of models developed using regular Bayesian whole-sample methods. Regarding the use of resampling or leave-out-one methods in Bayesian modeling, this is indeed a sampling-based procedure and not a parameter space-only procedure. To understand, first consider a pure Bayesian assessment as shown in http://hbiostat.org/rmsc/titanic.html#bayesian-analysis where you'll see a table that has the highest posterior density uncertainty interval for C (concordance probability = AUROC in the binary Y case here) of [0.861, 0.872]. This is a very narrow interval. How do we interpret it? It assesses the uncertainty in model performance on this sample of X and Y, where the only unreliability comes from unreliability in estimating regression coefficients $\beta$ . This is useful but does not tell us about the likely future performance of the model on a new set of data X. To be able to do that, Bayesian model performance measures use sampling-based ideas instead, the most popular being a leave-out-one method because of the speed with which it can be approximated from using all the posterior draws, say from the Stan system.
