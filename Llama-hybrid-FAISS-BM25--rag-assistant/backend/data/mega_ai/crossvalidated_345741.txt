[site]: crossvalidated
[post_id]: 345741
[parent_id]: 344945
[tags]: 
There are (at least) two possible models associated with this question: A truncated Gaussian mixture$$p{\cal N}^+(\mu_1,\sigma_1,t)+(1-p){\cal N}^+(\mu_2,\sigma_2,t)$$where $t$ is the truncation point and ${\cal N}^+(\mu_2,\sigma_2,t)$ denote the truncated Normal distribution with density $\sigma_1^{-1}\varphi(\sigma_1^{-1}[x-\mu_1])\Phi^{-1}(\sigma_1^{-1}[\mu_1-t])$ A censored Gaussian mixture$$p{\cal N}(\mu_1,\sigma_1)+(1-p){\cal N}(\mu_2,\sigma_2)$$where all $n_t$ observations less than $t$ are not observed (but accounted for). If $n_t$ is unknown, this model is equivalent to the first one. If we adopt a Bayesian approach with a prior $\pi(\theta)$ on the parameter $\theta=(\mu_1,\mu_2,\sigma_1,\sigma_2,p,t)$, a Gibbs sampler can be constructed by allocating the observations $x_1,\ldots,x_n$ to either component with probabilities $$p\sigma_1^{-1}\varphi(\sigma_1^{-1}[x_i-\mu_1])\Phi^{-1}(\sigma_1^{-1}[\mu_1-t])$$ and $$(1-p)\sigma_2^{-1}\varphi(\sigma_2^{-2}[x_i-\mu_2])\Phi^{-1}(\sigma_2^{-1}[\mu_2-t])$$ (up to a constant), thus creating a latent vector $(z_1,\ldots,z_n)$; simulating $(\mu_1,\mu_2,\sigma_1,\sigma_2,p)$ conditional on $t$,$(z_1,\ldots,z_n)$ and $x_1,\ldots,x_n$ (which may involve a Metropolis-Hastings step); simulating $t$ conditional on everything else, which involves a Metropolis-Hastings step. In the case of the censored mixture (#2), the $n_t$ censored observations are also to be simulated on step 1.
