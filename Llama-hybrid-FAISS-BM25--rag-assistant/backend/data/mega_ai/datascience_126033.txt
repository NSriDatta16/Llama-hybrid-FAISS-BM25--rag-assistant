[site]: datascience
[post_id]: 126033
[parent_id]: 
[tags]: 
How does a spare mixture of experts route each token separately to an expert?

As far as I can understand the routing mechanism in the the sparsely-gated mixture-of-experts layer routes each token to its own expert. Considering that the output of the attention layer is a three dimensional tensor(batch_size, seq_len, embd_dim), how does this routing happen especially considering that each batch is supposed to be processed independently?
