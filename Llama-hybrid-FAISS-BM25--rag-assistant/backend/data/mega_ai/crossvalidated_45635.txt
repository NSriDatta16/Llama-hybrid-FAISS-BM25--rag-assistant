[site]: crossvalidated
[post_id]: 45635
[parent_id]: 41394
[tags]: 
(I'll try what I thought would be the most typical kind of answer.) Let's say you have a situation where there are several variables and one response, and you know a good deal about how one of the variables ought to be related to the response, but not as much about the others. In a situation like this, if you were to run a standard multiple regression analysis, that prior knowledge would not be taken into account. A meta-analysis might be conducted afterwards, which might be interesting in shedding light on whether the current result was consistent with the other findings and might allow a slightly more precise estimate (by including the prior knowledge at that point). But that approach wouldn't allow what was known about that variable to influence the estimates of the other variables. Another option is that it would be possible to code, and optimize over, your own function that fixes the relationship with the variable in question, and finds parameter values for the other variables that maximize the likelihood of the data given that restriction. The problem here is that whereas the first option does not adequately constrain the beta estimate, this approach over-constrains it. It may be possible to jury-rig some algorithm that would address the situation more appropriately, situations like this seem like ideal candidates for Bayesian analysis. Anyone not dogmatically opposed to the Bayesian approach ought to be willing to try it in cases like this.
