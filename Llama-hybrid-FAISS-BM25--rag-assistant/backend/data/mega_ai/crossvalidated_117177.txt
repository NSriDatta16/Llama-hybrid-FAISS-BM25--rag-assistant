[site]: crossvalidated
[post_id]: 117177
[parent_id]: 
[tags]: 
Implementing the 0.632+ bootstrap method using the Weka Java API

I am trying to implement the 0.632+ bootstrap estimator (as proposed by Efron and Tibshirani 1997 ) in order to perform certain benchmarks and compare it with other cross-validation methods, such as the k -fold CV. Currently, all of my machine learning code is written in Java and all (sub)sampling, stratification, machine learning, and cross-validation is performed using the WEKA Java API. Therefore I would also like to implement this estimator using the same API. I have performed an extensive search and haven't found any useful algorithm or pseudocode regarding the 0.632+ estimator. Unfortunately, I am unable to decypher the original paper and rewrite the mathematical formulas into software code. Could please someone explain the basic steps that are needed in order to perform the 0.632+ estimator? Currently, the only thing that I have implemented is the basic (0.632??) bootstrap method, in which I sample n instances (with replacement) from a dataset containing n instances. Then, in average, I get the train set, containing 63.2% (unique) instances from the dataset. And the missed-out instances are assigned to the test set. Do I get it right until this part? Then, as far as I can understand, I have to measure the training error. This is where I get lost. I have a Computer Science background (if this even matters).
