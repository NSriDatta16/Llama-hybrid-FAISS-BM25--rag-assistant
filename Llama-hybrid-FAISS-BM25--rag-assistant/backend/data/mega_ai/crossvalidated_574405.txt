[site]: crossvalidated
[post_id]: 574405
[parent_id]: 574262
[tags]: 
This is mostly an extended comment on the answer from @Christian Hennig (+1), which covers the critical points and which I think you should accept. It's very tempting to think that a "significant difference" "supports" your hypothesis. That's not strictly how frequentist significance tests work. A small p-value argues against a null hypothesis, but (as pointed out in Point 2 of Christian Hennig's answer) it doesn't necessarily argue for any specific alternative hypothesis. A lack of "significance" doesn't mean the predictor is unimportant, just that your combination of data and model couldn't document its "significance" by an arbitrary (if widely used) criterion. If (before seeing the data) you suspected that the association of success with Trial number might depend on Condition , then it would have made sense to start with a model containing the interaction. To evaluate if the fixed effects in that model add anything to a null model, compare that model against one with only the random intercepts--as Christian Hennig recommends in Point 3. If the null hypothesis of no fixed effects isn't tenable, then at least you have found something of potential interest. I suspect that you have, even in the model without the interaction term that showed neither predictor on its own to be "significant." You are in a common situation where "the data are too weak to tell you which variable exactly does the trick," as he nicely put it. In that situation, attempts to improve the model with additional terms (splines, interaction terms, etc.) might not add enough to make up for the extra degrees of freedom that you use up, in terms of "significance." Do, however, see whether the model makes sense in terms of your data. One way to interpret the interaction term coefficient of -0.230 for ConditionExperimental:Trial is in the context of the slope of +0.247 for Trial , which is the change per Trial in ConditionControl . That suggests an increasing probability of success with Trial in ConditionControl , but essentially no increasing probability of success with Trial in ConditionExperimental ( 0.247 - 0.230 = 0.017 ). Is that how your data look? Does that make sense based on your knowledge of the subject matter? Comparing the data against the model is typically more useful than just staring at reported coefficient values. Your results only seem counterintuitive because you are so highly focused on the arbitrary p = 0.05 cutoff for "significance." The interaction term might not be "significant" by that criterion, but that doesn't mean it's unimportant in the context of your study. This is one reason why many come to prefer Bayesian models that can provide credible intervals for parameters without the all-or-none fixation on arbitrary null-probability cutoffs.
