[site]: crossvalidated
[post_id]: 391298
[parent_id]: 
[tags]: 
Estimating likelihood from the Residual Sum of Squares

I'm start studying Bayesian statistics, but I've found that I'm having troubles with the likelihoods. Let's say that I have a vector of observations $y$ and I want to calculate how likely it is $y$ given a parameter $\theta$ , that is, $P(y|\theta)$ . I wonder if there is a conceptual problem here, but the thing is that we do not have a probability density function for $y$ . Rather, we have a quite simple and common expectation as follows: $E(y)=k Â· exp(\theta \cdot x)$ In this linear model, $k$ is a constant and $x$ an independent variable. We do not solve this by taking logarithms and using linear regression, but by numerical methods, but this is out of the topic. In any case, we estimate $\theta$ numerically by minimizing the residual sum of squares ( $RSS$ ): $RSS = [E(y)-y]^{2}$ I have the notion that minimizing $RSS$ maximizes likelihood ( i.e. the estimated $\hat{\theta}$ is the MLE), but I cannot find a way to estimate that likelihood $L(\theta|y) = P(y|\theta)$ from these data, which will be necessary to solve Bayes. In addition, if I were interested in solving a particular case, say $P(y|\theta>0.2)$ , how one would proceed in this case? I just cannot imagine how to calculate $RSS$ in this scenario.
