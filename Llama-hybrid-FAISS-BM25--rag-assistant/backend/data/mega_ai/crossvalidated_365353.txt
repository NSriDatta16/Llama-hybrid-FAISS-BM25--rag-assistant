[site]: crossvalidated
[post_id]: 365353
[parent_id]: 365209
[tags]: 
Some points in addition to @EdM's nice answer. To give you a feeling for the small sample size trouble you're in (and I thoroughly know very similar situations) have a look at confidence intervals for estimating a proportion (such as accuracy, sensitivity, specificity, predictive values - you name it): Brown, L.; Cai, T. & DasGupta, A.: Interval Estimation for a Binomial Proportion Statistical Science, 2001, 16, 101-133 may be a good starting point. In R, binom::binom.confint() calculates confidence intervals using a variety of approximations. If just 1 of the 10 patients reserved in the hold-out step is misclassified, the point estimate is 90 % correct, but the 95 % confidence interval ranges all the way from a bit below 60 % to 100 %, with 2 misclassified it would cover everything between guessing (50 %) and almost perfect (95 %). My pragmatic point of view from situations with n and p similar to yours is: the results for the various approximations may look quite different, but these differences IMHO occur in situations where the interpretation in any case is that the model and/or the data set* is unusable in practice. Only, I stay clear of the normal approximation - our situations are clearly outside the recommended conditions where it can be used. * Sometimes it is useful to make a back-of-the-envelope calculation before starting to dig into the data: if 100 % accuracy would have a confidence interval that is too wide to be useful, back to the lab is the only possible answer. Conclusion from that is that your verfication/validation of the final model really should use a resampling strategy (cross validation or out-of-bootstrap). (see here for considerations when independent test sets may be a better choice - as opposed to a hold-out that was split from the data just as a cross validation splitting) I refrain from data/validation-driven optimization in such situations: these optimizations crucially need to be able to assess performance with a precision that allows to distinguish between the best and good models. No way to get that here. The situation is somewhat better if you look at so-called proper scoring rules . But they don't work miracles, neither. Still, I agree with EdM to go for a LASSO, random forest, or any other type of model and feature selection where you can use external knowledge for guidance. In the end, it may be better to fix the desired complexity (e.g. number of features to be kept) beforehand: that will leave you with a model and its validation results. Not necessarily the optimal model - but data-driven optimization is at least equally unlikely to achieve this in the present situation. There is no universal best set of features. There may be a best set of features to train that classifier: good feature selection needs to take into account data (data generating processes) as well as the needs/strengths/weaknesses of the classifier to be trained. It really is part of the training.
