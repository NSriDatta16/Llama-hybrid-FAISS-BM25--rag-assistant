[site]: crossvalidated
[post_id]: 301585
[parent_id]: 301583
[tags]: 
The short answer is yes, Naive Bayes can predict many different classes (A vs B vs C vs ...), and is not limited to just two classes ("disease" vs "no disease"). What you're describing is a multinomial Naive Bayes classifier . I suppose that I should use NB classifier Why not SVM or random forest ? There are lots of different classifiers, with advantages and disadvantages for different data and needs. If you provide more information about what you're doing we can comment on which, if any, are best for your case. Edit: Responding to your comments: Assume that I want to predict probability P(X|Y) If that's all you want, why not just calculate $P(X|Y)$? A Naive Bayes classifier compares probabilities and selects the most probable class. If you're interested in guessing classes, i.e. classifying, there are ways to do it that don't explicitly calculate $P(X|Y)$. But I have problem how to estimate this probability when I have X more "complex". In my data I would like to predict conditional probability one of some diseases (let's say number d3) when this disease is present under some symptom... To add more correct: I would like to use classes of all unique diseases and if they were or not as subclasses in ONE model. If you do really want to use a classifier, whatever method you use, explicitly define the classes you'd like to be predicted. All classes are mutually exclusive: the classifier will guess one or another , but not combinations of classes (there may be methods to do the latter, but I'm not aware of how to do that with a classifier). I'm not totally sure what you mean. If there are N unique diseases and M combinations of diseases and symptoms, then you have M+N classes. Label each row in the training set with a number between 1 and M+N. The classifier will then guess one of these numbers for unlabelled, i.e. testing, rows.
