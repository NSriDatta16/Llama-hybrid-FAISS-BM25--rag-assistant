[site]: crossvalidated
[post_id]: 467393
[parent_id]: 
[tags]: 
Augmenting training data with cases that won't be in future data

Background: I am working on coding survey responses, where the respondent writes in a description of their job. So the person might write in "McDonald's Employee" and get coded to something like 1002 which would be the code for fast food for example. The person might also write in something less clear like "I flip burgers" which should also be coded to 1002. For the clear-cut cases we have a reference file that automatically codes these. For the write ins that are not clear-cut we would like to use machine learning to code them (in particular we are using FastText). My question is: even though our future data will not contain these clear-cut cases as they will be automatically coded, should we still use them to train our model. Would they add valid information to the model even though we never expect to have to code those cases using the model? I suppose having them in our train/test split will bias optimistically our error rate as it will contain many east to predict cases we will never see, but if we could account for this would it be appropriate?
