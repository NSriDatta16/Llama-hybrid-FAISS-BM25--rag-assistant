[site]: datascience
[post_id]: 8057
[parent_id]: 6910
[tags]: 
No. Map reduce by definition works at one record at a time . Thus, you cannot compute a distance/similarity matrix without abusing the programming model. Methods such as SVM do require pairwise similarities. Such abuses are really common though (which is why map reduce is dead). Most easily, you can map every object to the key 0, and do all the work in the "reducer". More clever approaches divide the data into k partitions, so they can run at least some work in parallel. But all of these approaches are ugly hacks that hack around a deliberate design decision made for map reduce: in map reduce, every record must be processed independently, to allow the platform to automatically optimize parallelism and recovery. In above hack, the map phase is abused to split the data into the manually chosen parallelism, and then the reducers are simple parallel jobs. It's no longer mapreduce, but the abstract model of parallel programming on partitioned data. After all the early hype, mapreduce is now dead. Tools such as Apache Mahout no longer accept mapreduce but expect you to move on to more flexible pardigms that don't need such abusive hacks.
