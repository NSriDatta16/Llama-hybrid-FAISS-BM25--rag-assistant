[site]: crossvalidated
[post_id]: 442264
[parent_id]: 
[tags]: 
Using 'causal' padding ensures conv layers does not peek into the future when making predictions

This Sequential model starts with an explicit input layer (this is simpler than trying to set input_shape only on the first layer), then continues with a 1D convolutional layer using "causal" padding: this ensures that the convolutional layer does not peek into the future when making predictions (it is equivalent to padding the inputs with the right amount of zeros on the left and using "valid" padding). Géron, Aurélien. Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (Kindle Locations 11707-11710). O'Reilly Media. Kindle Edition. I quoted WaveNet and RNN part of the book. I don't understand how the causal paddings ensure the bold sentence. Any help would be appreciated.
