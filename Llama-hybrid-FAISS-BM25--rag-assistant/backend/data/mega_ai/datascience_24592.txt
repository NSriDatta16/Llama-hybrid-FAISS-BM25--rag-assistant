[site]: datascience
[post_id]: 24592
[parent_id]: 
[tags]: 
Do Convolution Layers in a CNN Treat the Previous Layer Outputs as Channels?

Lets say you have a max pooling layer that gives 10 downsampled feature maps. Do you stack those feature maps, treat them as channels and convolve that 'single image' of depth 10 with a 3d kernel of depth 10? That is how I have generally thought about it. Is that correct? This visualization confused me: http://scs.ryerson.ca/~aharley/vis/conv/flat.html On the second convolution layer in the above visualization most of the feature maps only connect to 3 or 4 of the previous layers maps. Can anyone help me understand this better? Related side question: If our input is a color image our first convolution kernel will be 3D. This means we learn different weights for each color channel (I assume we aren't learning a single 2D kernel that is duplicated on each channel, correct)?
