[site]: crossvalidated
[post_id]: 603816
[parent_id]: 603810
[tags]: 
Instead the correct interpretation of confidence intervals seem to be upon repeated samples of the data from the likelihood ('repeated experiments'), (1−α) of the confidence intervals generated (which will differ every experiment) will contain the true value θ Your understanding of the confidence interval is complicated by your thinking of it as a Bayesian interval. For a frequentist, an interval does not contain a true value (or not), rather it summarizes an interval or range into which an estimated frequency of results would occur if the study were replicated. Consider the estimation of normal variance without a degree of freedom correction: this is the maximum likelihood estimator, and it is biased. If I construct a 95% CI for this value using the appropriate $\chi^2$ statistics, and I replicate the study an infinite number of times, my biased estimators will fall in the interval 95% of the time as stated. These replicates will not, however, contain the true variance 95% of the time... the coverage will be slightly less than that. It is standard pedagogy to emphasize that the confidence interval does not reflect a probability of containing a true value - the only reason I can imagine for the persistence of this misunderstanding seems to be a lazy approach to pedagogy. Results about frequentist properties of Bayesian estimators are tremendously interesting for a number of reasons. In particular, it may not be desirable to appeal to the Bayesian interpretation of probability, but rather to develop new ways of estimating values that were previously intractable. Mixed models and models for missing or truncated data come to mind. It's somewhat well known that the EM algorithm can maximize a likelihood, and that there are semiparametric regression techniques for some of these cases, but what about a Bayesian estimator? Could an informative prior be useful? Could the Bayes estimators in these cases improve the MSE? Can they overcome issues with singularities or unstable likelihood functions on the boundary? Bayesian statistics are, among many things, an interesting estimation technique. I think the best way to understand this is that our preference for frequentist versus Bayesian interpretations of probability are, at times, subjective, and certainly do not apply for every type of problem. Given that, it should be easy to conceptualize frequentist problems (like a decision rule with well understood error rates, or applying standard NHST) where a Bayesian estimation technique could be useful.
