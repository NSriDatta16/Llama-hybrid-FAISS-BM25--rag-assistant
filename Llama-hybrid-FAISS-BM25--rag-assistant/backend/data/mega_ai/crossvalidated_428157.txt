[site]: crossvalidated
[post_id]: 428157
[parent_id]: 221402
[tags]: 
TL;DR: Discount factors are associated with time horizons. Longer time horizons have have much more variance as they include more irrelevant information, while short time horizons are biased towards only short-term gains. The discount factor essentially determines how much the reinforcement learning agents cares about rewards in the distant future relative to those in the immediate future. If $\gamma = 0$ , the agent will be completely myopic and only learn about actions that produce an immediate reward. If $\gamma = 1$ , the agent will evaluate each of its actions based on the sum total of all of its future rewards. So why wouldn't you always want to make $\gamma$ as high as possible? Well, most actions don't have long-lasting repercussions. For example, suppose that on the first day of every month you decide to treat yourself to a smoothie, and you have to decide whether you'll get a blueberry smoothie or a strawberry smoothie. As a good reinforcement learner, you judge the quality of your decision by how big your subsequent rewards are. If your time horizon is very short, you'll only factor in the immediate rewards, like how tasty your smoothie is. With a longer time horizon, like a few hours, you might also factor in things like whether or not you got an upset stomach. But if your time horizon lasts for the entire month, then every single thing that makes you feel good or bad for the entire month will factor into your judgement on whether or not you made the right smoothie decision. You'll be factoring in lots of irrelevant information, and therefore your judgement will have a huge variance and it'll be hard to learn. Picking a particular value of $\gamma$ is equivalent to picking a time horizon. It helps to rewrite an agent's discounted reward $G$ as $$ G_t = R_{t} + \gamma R_{t+1} + \gamma^2 R_{t+2} + \cdots \\ = \sum_{k=0}^{\infty} \gamma^k R_{t+k} = \sum_{\Delta t=0}^{\infty} e^{-\Delta t / \tau} R_{t+\Delta t} $$ where I identify $\gamma = e^{-1/\tau}$ and $k \rightarrow \Delta t$ . The value $\tau$ explicitly shows the time horizon associated with a discount factor; $\gamma = 1$ corresponds to $\tau = \infty$ , and any rewards that are much more than $\tau$ time steps in the future are exponentially suppressed. You should generally pick a discount factor such that the time horizon contains all of the relevant rewards for a particular action, but not any more.
