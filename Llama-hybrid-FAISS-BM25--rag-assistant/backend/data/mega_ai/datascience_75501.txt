[site]: datascience
[post_id]: 75501
[parent_id]: 75439
[tags]: 
The fact is that in Unsupervised algorithms, you never know. That is their main bottleneck. Unsupervised algorithms (Clustering, Dimensionality Reductions, etc.) are based on assumptions. When an assumption is made, then it will be translated into a math algorithm and applied. Choosing the right thing, as you said, is possible only if you know how is the distribution and/or topology of your data beforehand. But unfortunately it does not happen most of the time. Higher dimensional the data is, more difficult it gets to guess its structure. If you are using it as a feature extraction step for a supervised task, then the right way is to evaluate the impact of each on your Supervised learning through a statistical model selection (e.g. cross validation). If you are using them for an unsupervised task like clustering then you may choose some practical criteria (there is NO theoretical one i.e. there is NOT any theoretical justification for clustering task). For example you can visualize them in 2 or 3 dimensions and try to inspect if clusters are right (for instance by some known samples from your data. If you know two extreme cases of different samples, a better clustering puts them in far clusters, etc.) Again I would emphasize that there is no universally true evaluation for unsupervised tasks like clustering. Hope it helped!
