[site]: crossvalidated
[post_id]: 446116
[parent_id]: 445856
[tags]: 
The task you are referring to is called span identification and is commonly employed in textual question answering (QA; see, e.g., Chen, Bolton, and Manning 2016 ; Devlin et al. 2018 - particularly, figure 4). If you have supervised data for span identification (as in extractive QA), you could fine-tune a pre-trained BERT model to identify the start and end tokens of a span (step 1). Then, pass the words that are within the span through another BERT model fine-tuned to classify sentences into different classes (step 2). If you don't have the annotations for true span, you may want to consider jumping straight into step 2 and not worry about extracting the span, as the latter would primarily only provide additional interpretability anyway. Note that you will need to append padding tokens to the ends of your sentences to make sure they are all of the same length.
