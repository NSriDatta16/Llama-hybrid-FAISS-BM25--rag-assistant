[site]: stackoverflow
[post_id]: 3376696
[parent_id]: 
[tags]: 
Strategy to handle large datasets in a heavily inserted into table

I have a web application that has a MySql database with a device_status table that looks something like this... deviceid | ... various status cols ... | created This table gets inserted into many times a day (2000+ per device per day (estimated to have 100+ devices by the end of the year)) Basically this table gets a record when just about anything happens on the device. My question is how should I deal with a table that is going to grow very large very quickly? Should I just relax and hope the database will be fine in a few months when this table has over 10 million rows? and then in a year when it has 100 million rows? This is the simplest, but seems like a table that large would have terrible performance. Should I just archive older data after some time period (a month, a week) and then make the web app query the live table for recent reports and query both the live and archive table for reports covering a larger time span. Should I have an hourly and/or daily aggregate table that sums up the various statuses for a device? If I do this, what's the best way to trigger the aggregation? Cron? DB Trigger? Also I would probably still need to archive. There must be a more elegant solution to handling this type of data.
