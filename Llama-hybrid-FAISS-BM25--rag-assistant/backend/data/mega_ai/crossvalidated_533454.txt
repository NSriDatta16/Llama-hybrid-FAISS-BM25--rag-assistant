[site]: crossvalidated
[post_id]: 533454
[parent_id]: 532722
[tags]: 
The reason behind generating unreasonable loss values, in this case, is the use of the final Softmax layer and Cross-Entropy Loss together. So I just removed the Softmax layer and used a fully connected layer as the last layer in my CNN architecture and the problem was solved. I've found my answer in the following links: Should I use softmax as output when using cross entropy loss in pytorch? Can softmax be used with cross entropy?
