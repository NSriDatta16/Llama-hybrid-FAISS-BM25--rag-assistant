[site]: crossvalidated
[post_id]: 488823
[parent_id]: 
[tags]: 
Why diff-in-diff over regression with control variable for measuring treatment intervention in time series?

Let's suppose we have a time series data set, with an intervention period for the treatment where the mean of the independent variable increases by 10. The goal is to measure the effect of the treatment, and return the average daily (or whatever time unit) effect size in the post period. Simulated in R. set.seed(1) x1 Additionally, let's assume covariate x1 is highly correlated in the pre-period and not given the treatment. Why can't we just regress our $y$ on a dummy variable for pre/post + covariate x1 ? If x1 is highly correlated shouldn't it produce unbiased estimates of the conditional means for pre and post-period? I have read that autocorrelation is an issue when using ordinary least squares (OLS) regression on time series and it increases the false positive rate, even with a highly correlated covariate. How can I generate simulated data to prove that it increases that false positive rate? How does a difference-in-difference (DiD) model fix this issue? I'm using google's CausalImpact , but I think the idea is the same. EDIT: Clarifying covariate x1
