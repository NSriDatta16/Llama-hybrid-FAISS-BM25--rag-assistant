[site]: datascience
[post_id]: 114920
[parent_id]: 114913
[tags]: 
There is a decent amount of information missing in your problem statement, like what you are trying to predict, how, the granularity, the potential cost of being wrong and all. So giving you a useful solution is not really possible. However, from a purely learning perspective I can give some general suggestions. Your initial skepticism about imputing across years, or decade even is correct. This would not be meaningful or helpful. To understand why, think of this simple situation, we are looking at weather data in St Cloud Minnesota. Imputation might be useful if I had 6-months of data and I had say, 10 non-consecutive days of missing temperatures. I could average the two days on either side and have what is a relatively meaningful value. Even if the temperature had climbed or fallen in that period the starting and ending points would be meaningful to compute that middle value. Example: June 6 : 75 June 7 : - June 8 : 77 Taking the average, 76 is not too horrible. And if the temperature rose from 75 to 77, 76 is quite the plausible value for that one day. Now imagine I had 40 years of data, but only 1 year worth of temperatures. Imagine the daily mean temperature across that year was 57 degrees. When you impute that to the other 39 years, how meaningful is it? There may be only a handful of days in a year when it is 57 degrees in St Cloud MN, those days are unlikely in June, July August, September, December, January, February & March, yet for 39-years you would have that one value for all days. This kind of smoothing just makes the variable useless because your model will smooth it even more and the signal will be completely lost, but there is a very good chance a destructive false signal will be created in the process. If you had 39-years of data, it might be meaningful (after evaluating for warming or cooling climate trends) to impute some completely lost year in the middle using the mean temperature for the day of the year (100th day, 320th day etc. ) across that same day in the other 39, but generally speaking big bundles of consecutive missingness are difficult to navigate in time series. You could chunk the data and build an ensemble of ensembles. But you would in essence be using a 5-year period from 12-years ago in parallel with a 5-year period 30-years ago in conjunction with the last 5-year period as ensemble models. In this case each would have only the variables available in that period, so that would be a bagged model of sorts. However it would be created without the randomness associated with bagging or the replacement. This would introduce biases based on why the missingness exists as well as what is missing. A true bagged model uses repeated samples with replacement to sidestep the bias. And more pragmatically, this is not how time evolves and how influence in a system is exerted on future values of a system over time. In my opinion (and I am not the last great voice of data science reason) you would be better off using fewer complete (or reasonably imputable) variables for a lesser period of time to build your model, even if it means a modest forecasting capability, because it would be built on real signal. The keys are time proximity and meaningful variables. We always talk about more data being better, but what people do not say about that data is important. More (relevant, high-quality, consistent, of known provenance, highly correlated with out outcome variable) data is better. Simply more, is simply not better. And when looking at correlated data (time series or spatially correlated) strength of signal in your chosen variables and proximity (either temporally or spatially) are typically more meaningful than covering a greater span of time or space (so long as you can account for periodicity in the data which you have). Try thinking about the problem you are trying to solve with the data, divorced of the models and the complexity of your data situation and think about what you absolutely think you need to have to know what 'comes next' in your system. It may be less than you think in time and variables (it also may not).
