[site]: crossvalidated
[post_id]: 477691
[parent_id]: 
[tags]: 
Do variational approximations capture the flow of influence or "conditional independence" relationships in graphical models?

Probabilistic Graphical Models (PGMs) are used to model all sorts of complex decision processes, such as medical diagnoses or robot positions, etc. In common machine learning textbooks, like Christopher Bishops book on pattern recognition or Koller's text, the idea of the "flow of influence" or conditional independence seems to be a big feature/benefit of these methods. In other words, these models can make better decisions when dealing with data where a mixture of generative processes may exist. I am a bit confused about the inference methods for PGMs versus the ability of these inference methods to capture the conditional independence relationships between variables. In both the Bishop, Koller, and Murphy texts, the decription of Bayesian networks (directed) and Markov random fields, is immediately followed by the discussion about "exact" inference and Belief Propagation . Then variational methods are kept for a subsequent chapter. I can see how belief propagation or message passing methods execute inference by following the pattern of conditional independence relationships in the network. But my question is, do the approximate inference methods, like maximizing the ELBO, evidence lower bound, also capture this pattern of conditional independence relationships? Also, under what conditions do variational inference methods fail to capture the flow of influence in the original network. My assumption is the variational methods should factor in the conditional independence structure of the network, but there should be issues with the complexity of the underlying distribution, or with the mis-specification of the candidate factorized distribution for VI that could get in the way of accurate representation. Part of my confusion also stems from the fact that many of the popular "Probabilistic Programming" packages, like Pyro, or Tensorflow Probability, or even PyMC3 don't support any belief propagation/message passing methods, and their tutorials never really discuss conditional independence and the flow of influence. So I was just hoping to find an explicit answer to this question.
