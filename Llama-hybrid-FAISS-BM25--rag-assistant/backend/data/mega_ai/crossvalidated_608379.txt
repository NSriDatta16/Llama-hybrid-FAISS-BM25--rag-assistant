[site]: crossvalidated
[post_id]: 608379
[parent_id]: 
[tags]: 
Neural Networks - Can I Use Any Activation for the Output Layer?

I'm new to neural networks, and in almost everything I'm reading, the activation function recommended on the output layer follows a specific pattern: If the network does binary classification (1 output node), use sigmoid If the network does multiclass classification (>1 output nodes), use softmax If the network does regression, don't use an activation function (linear) Which I completely understand - for example, binary classification is a probability and is never going above 1 or below 0, so of course it makes sense to use sigmoid. My question is though, when I'm doing regression, can't I just use the activation function that best fits my range of output values instead of using linear? For example, let's say I'm trying to predict the price of a stock - wouldn't a ReLU activation function make a lot more sense to use on the output layer over a linear activation function, since the price can never be negative? Or for another example, let's say I normalized my output values between 1 and -1 - Wouldn't I want to use a TanH activation on the output?
