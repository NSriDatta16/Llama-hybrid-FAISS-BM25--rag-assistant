[site]: crossvalidated
[post_id]: 616439
[parent_id]: 
[tags]: 
Cross validation with GLMMs; best way to partition train and test data with regard to random effects?

I have some analysis I'm working on and I'm having a hard time nailing down the correct approach to take. I am modeling the dynamics of frog choruses by looking at what predicts the outcome of calling interactions between different chorus members. Interactions can result in call overlap or not, so I have coded each interaction as 1 or 0 and am using a GLMM to model what predicts whether an interaction will result in overlap or not. Predictor vars are size of both males in the interaction, and acoustic properties of their calls etc. I have 6 choruses total, each with 6 males, so I'm including IDs of both males involved in each interaction as random effects, nested within chorus ID. I have data on many interactions (~3000 total in the dataset, ~500 per chorus). I feel confident enough about this step. But, since I am including quite a few variables that may influence probability of overlap, I want to use multimodel inference to determine which features are most predictive of overlap, and then produce an averaged model from the top model set. I'd like to test how generalizable the results of this model are by doing some sort of cross validation. I figure I'll leave 10% of the data out to test the average model with after it has been trained on the training set. My big question is: how do I take into account the random effects present in my model when choosing what 10% of the data to leave out for testing? Is a completely random 10% fine? Or do I need to structure the test data by accounting for the levels of the random effects in some way? e.g., by ensuring the test set contains at least one example from all pairings of males or something? Another idea I had was to train the model on data from 5 of the choruses and then test it with data from the 6th. Would that work? My concern with this option is that the chorus I'd be testing on has a very small sample of males (6), meaning that by chance their idiosyncrasies could give them random effects that fall somewhere weird on the distributions of population-level random effects, which could make the model predict them very poorly even if it could produce good predictions at a population level. Thank you for any information anyone can provide! I've been having a hard time finding much on this.
