[site]: datascience
[post_id]: 76676
[parent_id]: 76658
[tags]: 
Sorry that this isn't a concrete answer, but I can offer some advice. It sounds like you have a problem of many weak relationships. In this case, I think xGBoost or RandomForest would yield better results than Logistic Regression. Also remember that preprocessing your data and creating new features might help more than choosing a different algorithm. Consider different options for encoding your categorical variables into numbers. Look at python's category_encoders and try leave-one-out, response encoding, and others. Consider your imputation strategy for missing data - Using -99999 for missing might work well for xgBoost, but won't work so well with your regression. Consider using logloss as your optimization metric, or at least AUC. (Not just accuracy) Above all else - see if you can find more data. For example - Join other freely available data: i.e. if your data has zip code, can you join economic data by zip to add more features? Leverage data you "ignored": i.e. do you have free form text data? Try parsing that into a sparse matrix using TF-IDF. Lastly, are the xGBoost models performing badly overall, or are they overfitting and performing badly on your holdout? Look for techniques such as CrossFold Validation.
