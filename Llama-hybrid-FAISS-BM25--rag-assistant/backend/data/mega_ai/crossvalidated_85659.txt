[site]: crossvalidated
[post_id]: 85659
[parent_id]: 
[tags]: 
Why would feature scaling cause overfitting?

I'm fitting a logistic regression to a simple two-feature dataset. The feature values range from about 1 to 100. When I scale the features (using scikit-learn StandardScaler), the in-sample performance improves significantly, but the out of sample performance drops. Why would feature scaling cause overfitting?
