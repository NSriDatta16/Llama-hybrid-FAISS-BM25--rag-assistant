[site]: crossvalidated
[post_id]: 174226
[parent_id]: 
[tags]: 
Statistical significance with insufficient data

There is an article in Wikipedia which talks about p-values . In the example section it gives this example: One roll of a pair of dice Suppose a researcher rolls a pair of dice once and assumes a null hypothesis that the dice are fair, not loaded or weighted toward any specific number/roll/result; uniform. The test statistic is "the sum of the rolled numbers" and is one-tailed. The researcher rolls the dice and observes that both dice show 6, yielding a test statistic of 12. The p-value of this outcome is 1/36 (because under the assumption of the null hypothesis, the test statistic is uniformly distributed) or about 0.028 (the highest test statistic out of 6Ã—6 = 36 possible outcomes). If the researcher assumed a significance level of 0.05, this result would be deemed significant and the hypothesis that the dice are fair would be rejected. In this case, a single roll provides a very weak basis (that is, insufficient data) to draw a meaningful conclusion about the dice. This illustrates the danger with blindly applying p-value without considering the experiment design. You obviously would not publish a paper on the result, yet the p-value is statistically significant. What are some measures to prevent this type of "error"? PS. It would be great if both a frequentist and a Bayesian methods presented.
