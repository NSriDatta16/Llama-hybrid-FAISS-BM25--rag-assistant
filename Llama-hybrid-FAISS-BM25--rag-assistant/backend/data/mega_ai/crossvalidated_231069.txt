[site]: crossvalidated
[post_id]: 231069
[parent_id]: 230973
[tags]: 
In auto differentiation systems mostly an operator (e.g. addition, subtraction) is defined together with its differentiation. So after you write a function by stacking a series of operators, the system can figure out by itself how the corresponding derivatives should be computed, usually by using computation graphs and the chain rule. Auto differentiation is beneficial for gradient based optimization (e.g. training a neural network using gradient descent), as it saves us from working out the math, implementing the code and verifying the derivatives numerically case by case. Here's how to define an operator (op) in Theano and Tensorflow .
