med "AI-complete", i.e. requiring all of the different types of knowledge that humans possess (grammar, semantics, facts about the real world, etc.) in order to solve properly. Classical approach of machine translation – rules-based machine translation. Computer-assisted translation – Interactive machine translation – Translation memory – database that stores so-called "segments", which can be sentences, paragraphs or sentence-like units (headings, titles or elements in a list) that have previously been translated, in order to aid human translators. Example-based machine translation – Rule-based machine translation – Natural-language programming – interpreting and compiling instructions communicated in natural language into computer instructions (machine code). Natural-language search – Optical character recognition (OCR) – given an image representing printed text, determine the corresponding text. Question answering – given a human-language question, determine its answer. Typical questions have a specific right answer (such as "What is the capital of Canada?"), but sometimes open-ended questions are also considered (such as "What is the meaning of life?"). Open domain question answering – Spam filtering – Sentiment analysis – extracts subjective information usually from a set of documents, often using online reviews to determine "polarity" about specific objects. It is especially useful for identifying trends of public opinion in the social media, for the purpose of marketing. Speech recognition – given a sound clip of a person or people speaking, determine the textual representation of the speech. This is the opposite of text to speech and is one of the extremely difficult problems colloquially termed "AI-complete" (see above). In natural speech there are hardly any pauses between successive words, and thus speech segmentation is a necessary subtask of speech recognition (see below). In most spoken languages, the sounds representing successive letters blend into each other in a process termed coarticulation, so the conversion of the analog signal to discrete characters can be a very difficult process. Speech synthesis (Text-to-speech) – Text-proofing – Text simplification – automated editing a document to include fewer words, or use easier words, while retaining its underlying meaning and information. Component processes Natural-language understanding – converts chunks of text into more formal representations such as first-order logic structures that are easier for computer programs to manipulate. Natural-language understanding involves the identification of the intended semantic from the multiple possible semantics which can be derived from a natural-language expression which usually takes the form of organized notations of natural-languages concepts. Introduction and creation of language metamodel and ontology are efficient however empirical solutions. An explicit formalization of natural-languages semantics without confusions with implicit assumptions such as closed-world assumption (CWA) vs. open-world assumption, or subjective Yes/No vs. objective True/False is expected for the construction of a basis of semantics formalization. Natural-language generation – task of converting information from computer databases into readable human language. Component processes of natural-language understanding Automatic document classification (text categorization) – Automatic language identification – Compound term processing – category of techniques that identify compound terms and match them to their definitions. Compound terms are built by combining two (or more) simple terms, for example "triple" is a single word term but "triple heart bypass" is a compound term. Automatic taxonomy induction – Corpus processing – Automatic acquisition of lexicon – Text normalization – Text simplification – Deep linguistic processing – Discourse analysis – includes a number of related tasks. One task is identifying the discourse structure of connec