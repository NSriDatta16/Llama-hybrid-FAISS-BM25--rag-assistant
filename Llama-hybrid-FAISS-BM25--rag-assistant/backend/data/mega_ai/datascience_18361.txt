[site]: datascience
[post_id]: 18361
[parent_id]: 
[tags]: 
Reproducing cutoff in xgboost.train() with XGBClassifier()

(Cross-posted from https://stackoverflow.com/questions/43415724/reproducing-cutoff-in-xgboost-train-with-xgbclassifier ) I've gotten xgboost to generate good predictions by using xgboost.train(). X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=.6) xgtrain = xgb.DMatrix(X_train, y_train) param = {'max_depth':7, 'silent':1} bst = xgb.train(param, xgtrain, num_boost_round=2) y_pred = bst.predict(xgtest) y_pred = [1. if y_cont > .28 else 0. for y_cont in y_pred] y_true = y_test This approach wasn't producing good results (I am trying to maximize the f1 score) until I realized that the f1 score increases significantly when setting a threshold on the outputs. This threshold turned out to be .28. Here are some of the predictions before I set the cutoff and convert to 0s and 1s: [ 0.25447303 0.25383738 0.24621713 ..., 0.24621713 0.24621713 0.24621713] But now I want to tune my parameters (using GridSearchCV()), which means that I'll need to reproduce what I did in xgboost.train() above with XGBClassifier(). I realize that things might get tricky because the (default) objective function in xgboost.train() is none, and for XGBClassifier() it's 'binary:logistic'. XGBClassifier() returns the class and not the probability, which is useful in most cases but not here. I've tried predict_proba() with XGBClassifier() and then setting a cutoff, but it seemed pretty useless since the probabilities I were getting were super close to 0 and 1: [[ 9.99445975e-01 5.54045662e-04] [ 9.89062011e-01 1.09380139e-02] [ 9.95234787e-01 4.76523908e-03] How can I finish the code below do the equivalent of xgboost.train() but with XGBClassifier? When I've tried XGBClassifier without a cutoff, I would get a horrible f1 score. X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=.6) rf = XGBClassifier(max_depth=7, learning_rate=0.1, n_estimators=100, silent=True, objective='binary:logistic', nthread=-1, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, seed=0, missing=None) rf = rf.fit(X_train, y_train)
