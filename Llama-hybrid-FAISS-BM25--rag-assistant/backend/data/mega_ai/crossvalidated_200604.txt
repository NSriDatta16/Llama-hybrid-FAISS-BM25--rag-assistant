[site]: crossvalidated
[post_id]: 200604
[parent_id]: 200522
[tags]: 
Why not compare the models based on their out-of-sample performance? Of course, you can do that. I suppose that the advantage of AIC is faster computation and less coding (while AIC is often automatically reported as part of model diagnostics, cross validation for time series might not be readily available in your favourite software). I tried both approaches and most of the time AIC and out-of-sample SSE do not yield the same result. You do not seem to have implemented the cross validation properly. First, you split the data only once while you are supposed to split it multiple times. Second, you assessed forecasting performance based on one trial of forecasting multiple different horizons rather than multiple trials of forecasting one fixed horizon. Perhaps therefore you got the discrepancy between AIC and cross validation When implementing cross validation in a time series setting, you may make use of rolling windows. You would take observations from $t$ to $t+m$ where $m$ is the window length and roll $t$ from 1 to $T-m-1$ where $T$ is the sample size. You would estimate your model in each rolling window and predict one period ahead. You would then collect these predictions and compare them the to the actual values. That would give you an out-of-sample metric of forecasting performance when using cross validation in a time series setting. See also Hyndman and Athanasopoulos "Forecasting: principles and practice", section 2.5 (scroll all the way down) and Bergmeir et al. "A note on the validity of cross-validation for evaluating time series prediction" (2015, working paper). at least visually, those [models] selected by SSE perform better It could be that the model residuals did not quite have the assumed distribution or the model had some other faults invalidating its AIC in some way. That is one argument why out-of-sample forecast accuracy could be preferred over AIC in model selection.
