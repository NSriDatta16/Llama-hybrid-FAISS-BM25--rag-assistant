[site]: crossvalidated
[post_id]: 209979
[parent_id]: 
[tags]: 
Question about reinforcement learning's training data

I am trying to create a simple reinforcement learning model to play the 2048 game. According to the " Playing Atari with deep reinforcement learning " the loss function I use is Loss = R[n] + ratio * max(inc_score|a + Q(S[n+1]|a)) - Q(S[n]) R[n] -> current score S[n+1] -> The board of next movement Q(S[n]) -> Current board's Q score with a 3 layers NN inc_score -> The score could get after a specific action I randomly generated training data in the form of: R[n], S[n], S[n+1]|a Inc|a (Include 4 possible actions, left, right, up, down, I randomly pick one in each step) And update the parameters with a standard SGD. But the NN looks never getting converge. My Question is Q1: Is my understanding correct for this case and RL? Q2: Is that OK to use randomly generated training data, or I have to generate the training the data according to the current Q value.
