[site]: datascience
[post_id]: 63011
[parent_id]: 63004
[tags]: 
It is quite true that papers or books use notations that sometimes seem obvious to people who are used to dealing with the mathematical aspects, but are meaningless for the others. Ways of understanding the math include: Following theoretical courses or trainings Reading dedicated books Asking people around you Asking people on forums such as this one, or Cross Validated for stats formulae Getting it by yourself upon re-reading parts of the paper/book you didn't get at the first time There are some notations/conventions that are implicitly accepted in data science / machine learning papers, such as: Using $X$ as input, $y$ as output, $\theta$ as model parameters Using $\hat{y}$ for the estimator of the true $y$ Assuming that vectors are column vectors The list would be too long to include here. Regarding the example above, what we face is a constrained optimization . The $max$ statement means that we are looking for a maximum value of the expression that follows. What is below (namely, the $\Delta_{ij}$ values) the $max$ is the list of "free" parameters that change the value of the expression. The $max$ statement is prefixed by $arg$ , which means that we do not have interest in the expression's maximum value, but rather in the $\Delta_{ij}$ set that leads to that value. Then we face a $s.t.$ statement, because this is no ordinary optimization, we also have to respect the several constraints expressed after $s.t.$ . Those can be inequations, equations, belonging constraints, etc., either explicit ( $\Delta_{ij} > 0$ ) or more implicit.
