[site]: crossvalidated
[post_id]: 380438
[parent_id]: 380424
[tags]: 
I wish there had been an easy recipe, but unfortunately no. The alternatives you listed are among the many intuitive heuristics for feature selection, e.g. random forest / xgboost feature importances, lasso coefficients, logistic regression weights, correlation coefficients etc. You need to do model selection after having some set of polynomial features. By the way, usage of single variable polynomial features in decision tree based algorithms sometimes might not have an impact on your performance because these transformations do not change the total ordering of the variables if odd-powered and therefore decision boundaries might be similar, i.e. $x_1 , so the boundary will be still between the two.
