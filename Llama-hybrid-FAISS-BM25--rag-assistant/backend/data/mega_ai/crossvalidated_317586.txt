[site]: crossvalidated
[post_id]: 317586
[parent_id]: 
[tags]: 
Convolutional neural network: why is my training accuracy suddenly dropping?

I am trying to understand this behaviour in my shallow CNN (only one layer followed by a sigmoid classification layer. I am training on about 2000 images per class. My learning rate was 0.01 and I was using binary cross-entropy. The final test accuracy was only 54% (essentially no greater than chance). What I don't understand is the pattern of training accuracy - I thought perhaps my learning rate was too high so I lowered it to 1e-3. Again the network rapidly gains training accuracy followed by a validation accuracy of 1 and then starts dropping like stone on the next Epoch (and then up again). I wonder if by binary cross entropy is returning a NaN value but I am not sure how to check for that and I don't have an intuition about the impact that may have. [INFO] training network... Epoch 1/4 44/44 [==============================] - 391s 9s/step - loss: 0.0620 - acc: 0.9943 - val_loss: 1.0960e-07 - val_acc: 1.0000 Epoch 2/4 44/44 [==============================] - 459s 10s/step - loss: 7.0132 - acc: 0.5625 - val_loss: 5.5104 - val_acc: 0.6562 Epoch 3/4 44/44 [==============================] - 246s 6s/step - loss: 16.0302 - acc: 0.0000e+00 - val_loss: 16.0302 - val_acc: 0.0000e+00 Epoch 4/4 44/44 [==============================] - 250s 6s/step - loss: 0.3628 - acc: 0.9774 - val_loss: 1.0960e-07 - val_acc: 1.0000 EDIT: Just to add, when I changed the learning ratee to 1e-3, I got the exact same result as above - as in precisely the same losses and accuracies.
