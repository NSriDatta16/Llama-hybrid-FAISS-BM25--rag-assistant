[site]: datascience
[post_id]: 74330
[parent_id]: 74328
[tags]: 
Well may be you should understand how its done in a Perceptron first and then fill the gaps in, but still don't understand how they got to this equation. in Gradient descent we want to calculate the steepest descent along the surface, which we can do by calculating the derivative of E with respect to each component of weights.so we can write it like this. Now the negative of this gives me the steepest. Now, in back propagation, to put it in very simple terms, we have all these weights(now from one unit to another since its a network) which we want to tune in such a way it minimizes the error for the complete network. for that we calculate the error in forward pass and then accordingly go back and tune the weights . But this time i have to to figure out how to do it when there are weights from one node to another(your article is trying to show you this). but good news is i have this equation which tells for how i can do it for a weight. So now, Its basically straight forward using this equation, just needs to track of all these weights from one unit to another. So now i have weights like this and its weights sum And we again solve it by like- Now , in your article they are doing the same but wrote this equation just to explain that how a weights change can influence the error in the whole network, so just the mathematical representation . for chain rule you can read- https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-2-new/ab-3-1a/v/chain-rule-introduction Please read Tom Mitchell's chapter on Neural Networks. Here is another answer to it- Compute backpropagation
