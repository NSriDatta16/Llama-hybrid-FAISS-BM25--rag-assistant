[site]: crossvalidated
[post_id]: 592080
[parent_id]: 
[tags]: 
Probabilistic machine learning models: parameter uncertainty

Consider models such as DeepAR , ngboost and other frameworks to the general problem of predicting the parameters of some parametric distribution with some black-box function, call it f(X). The predicted parameters are chosen to minimize the negative loglikelihood of the assumed parametric distribution. These models, specifically ngboost, seem to generate accurate probabilistic forecasts, at least according to the paper. However, even after talking with the package creators on this topic, I still don't understand how we can call these intervals "prediction intervals" when parameter uncertainty is ignored. In a Bayesian setting, we integrate over our posterior uncertainty in order to derive the posterior predictive. This is quite intuitive. In a frequentist setting, defining prediction intervals is harder in general, but in cases such as ordinary linear regression, there is a closed form solution that clearly captures both uncertainty around the mean (i.e. parameter uncertainty) and the inherent uncertainty in the process. For other frequentist models such as GAMs and GLMs, we can produce prediction intervals by bootstrapping the sampling distribution (or utilizing the CLT by assuming a multivariate normal distribution for the parameters), and then effectively recreate the Bayesian definition of the posterior predictive distribution by integrating these bootstrap samples against the likelihood of the model. See here as an example, along with this. However, it is clear that in both DeepAR and ngboost, neither of these probabilistic models do anything like this. It would appear that both of these models just predict the parameters of some parametric distribution via. black box learners f(X), and then prediction intervals are defined as the quantiles of the implied distributions. This would be, in my mind, the exact same thing as plugging in the predicted mean and estimated sigma parameters from an ordinary least squares model, and using the quantiles of this distribution as a "prediction interval". This clearly ignores uncertainty in estimating both mu and sigma. I've had this dilemma for quite some time now, where I don't really understand how some of these state of the art probabilistic machine learning models supposedly produce "prediction intervals", even though they ignore the inherent uncertainty in estimating the parameters of the distribution they assume. Any thoughts?
