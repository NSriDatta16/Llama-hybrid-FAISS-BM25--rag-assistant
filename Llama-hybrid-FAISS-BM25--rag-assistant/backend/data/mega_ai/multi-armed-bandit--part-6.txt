nts in a multi-armed bandit setting. A. Badanidiyuru et al. first studied contextual bandits with budget constraints, also referred to as Resourceful Contextual Bandits, and show that a O ( T ) {\displaystyle O({\sqrt {T}})} regret is achievable. However, their work focuses on a finite set of policies, and the algorithm is computationally inefficient. A simple algorithm with logarithmic regret is proposed in: UCB-ALP algorithm: The framework of UCB-ALP is shown in the right figure. UCB-ALP is a simple algorithm that combines the UCB method with an Adaptive Linear Programming (ALP) algorithm, and can be easily deployed in practical systems. It is the first work that show how to achieve logarithmic regret in constrained contextual bandits. Although is devoted to a special case with single budget constraint and fixed cost, the results shed light on the design and analysis of algorithms for more general CCB problems. Adversarial bandit Another variant of the multi-armed bandit problem is called the adversarial bandit, first introduced by Auer and Cesa-Bianchi (1998). In this variant, at each iteration, an agent chooses an arm and an adversary simultaneously chooses the payoff structure for each arm. This is one of the strongest generalizations of the bandit problem as it removes all assumptions of the distribution and a solution to the adversarial bandit problem is a generalized solution to the more specific bandit problems. Example: Iterated prisoner's dilemma An example often considered for adversarial bandits is the iterated prisoner's dilemma. In this example, each adversary has two arms to pull. They can either Deny or Confess. Standard stochastic bandit algorithms don't work very well with these iterations. For example, if the opponent cooperates in the first 100 rounds, defects for the next 200, then cooperate in the following 300, etc. then algorithms such as UCB won't be able to react very quickly to these changes. This is because after a certain point sub-optimal arms are rarely pulled to limit exploration and focus on exploitation. When the environment changes the algorithm is unable to adapt or may not even detect the change. Approximate solutions Exp3 Source: EXP3 is a popular algorithm for adversarial multiarmed bandits, suggested and analyzed in this setting by Auer et al. [2002b]. Recently there was an increased interest in the performance of this algorithm in the stochastic setting, due to its new applications to stochastic multi-armed bandits with side information [Seldin et al., 2011] and to multi-armed bandits in the mixed stochastic-adversarial setting [Bubeck and Slivkins, 2012]. The paper presented an empirical evaluation and improved analysis of the performance of the EXP3 algorithm in the stochastic setting, as well as a modification of the EXP3 algorithm capable of achieving "logarithmic" regret in stochastic environment. Algorithm Parameters: Real γ ∈ ( 0 , 1 ] {\displaystyle \gamma \in (0,1]} Initialisation: ω i ( 1 ) = 1 {\displaystyle \omega _{i}(1)=1} for i = 1 , . . . , K {\displaystyle i=1,...,K} For each t = 1, 2, ..., T 1. Set p i ( t ) = ( 1 − γ ) ω i ( t ) ∑ j = 1 K ω j ( t ) + γ K {\displaystyle p_{i}(t)=(1-\gamma ){\frac {\omega _{i}(t)}{\sum _{j=1}^{K}\omega _{j}(t)}}+{\frac {\gamma }{K}}} i = 1 , . . . , K {\displaystyle i=1,...,K} 2. Draw i t {\displaystyle i_{t}} randomly according to the probabilities p 1 ( t ) , . . . , p K ( t ) {\displaystyle p_{1}(t),...,p_{K}(t)} 3. Receive reward x i t ( t ) ∈ [ 0 , 1 ] {\displaystyle x_{i_{t}}(t)\in [0,1]} 4. For j = 1 , . . . , K {\displaystyle j=1,...,K} set: x ^ j ( t ) = { x j ( t ) / p j ( t ) if j = i t 0 , otherwise {\displaystyle {\hat {x}}_{j}(t)={\begin{cases}x_{j}(t)/p_{j}(t)&{\text{if }}j=i_{t}\\0,&{\text{otherwise}}\end{cases}}} ω j ( t + 1 ) = ω j ( t ) exp ⁡ ( γ x ^ j ( t ) / K ) {\displaystyle \omega _{j}(t+1)=\omega _{j}(t)\exp(\gamma {\hat {x}}_{j}(t)/K)} Explanation Exp3 chooses an arm at random with probability ( 1 − γ ) {\disp