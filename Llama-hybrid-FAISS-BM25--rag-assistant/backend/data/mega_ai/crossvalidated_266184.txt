[site]: crossvalidated
[post_id]: 266184
[parent_id]: 
[tags]: 
Is training multiple Random Forests equivalent to a repeated 3-fold cross-validation?

In the book "Elements of Statistical Learning" (Friedman, Hastie, Tibshirani) , the authors suggest that bootstrap and the resulting out-of-bag data for a random forest are equivalent to a 3-fold cross-validation. This makes sense and is pretty straightforward to implement as far as we intend to use cross-validation for assessing "model performance". But what if I actually want to use CV for "model selection", i.e. try different parameter (number of trees, max_features etc.) combinations to decide upon the best one? Then, one could imagine that we use RFs as follows to implement a repeated 3-fold CV: for each combination_of_parameters in parameter_set: rf My question is can I compare the CV scores across the different "combination_of_parameters" and use it to choose the best model? The reasoning here would be that the code snippet above is actually a form of repeated 3-fold CV? My concern is that since rf is initialized multiple times, the models and the bootstrap sets will differ, thus making the resulting scores not comparable. The goal here is to exploit RFs to achieve simultaneously training and CV without implementing a "dumb", time-consuming repeated CV schema from scratch. Thanks,
