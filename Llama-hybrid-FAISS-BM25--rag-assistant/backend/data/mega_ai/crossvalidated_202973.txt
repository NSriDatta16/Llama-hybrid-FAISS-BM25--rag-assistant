[site]: crossvalidated
[post_id]: 202973
[parent_id]: 
[tags]: 
What would be a proper statistical test for multiple models and model selection?

Suppose I have a data set of N observations (n = 1...N) for out-of-sample estimation and values of ($y_n$). I have also I statistical models (i= 1...I) which every model has its own estimate on each data point ($\hat{y}^i_n$). In addition I have a model selection method $\phi$ which would pick a model's estimate among the model set as its own according to its assessment on previous performance of the models ($\hat{y}^\phi_n = \min_i\{\hat{f_i}(y_n), i \in I\} $). My claim should be "model selection's performance is better than all models it picks estimates from". I am trying to find a proper method to describe the statistical power of the model selection method, compared to individual models in the model set. All individual models follow different assumptions, distributions and dependence structure. Some are iid, some have heteroskedasticity. Actually, there is no restriction on models except it should yield an estimate. Some The models are employed on time series but what they do is asset pricing on different assets and contracts. But for a broaded audience I will make the following analogy. Suppose you have a machine that predicts the scores on basketball matches. It does not only predict the final score, it also predicts a distribution of the scores throughout the time. It also predicts which player will score when. Suppose you have many machines of this sort and all have different predictions. All of them had been right on some occasion (That is what statistics is after all right? No model is perfect.). I am trying to figure out which machine is better at predicting what and when, using the previous performance of the machines. I can say stuff like 'oh machine A was good at predicting scores occured in the last 10 mins, but for the last 2 months model B became better'. It turns out my estimates using the machines are better than any machine could do it alone in the long run. I checked for several error terms starting with MAPE and MSE. But I want to show that it is not a coincidence but a statistically significant fact. I have a fair sample size (~100k) over a good enough time period (5 years). I fiddled with some thoughts about proportion of $\phi$ selecting the model with the lowest error and some logistic regression on that according to the criteria it uses to pick the models. But I lack the comprehensive knowledge on this domain of statistics. ps. R package suggestions are also appreciated.
