[site]: crossvalidated
[post_id]: 68640
[parent_id]: 59363
[tags]: 
I am very new to Bayesian statistics, but it seems to me that all of these distributions (and if not all of them then at least those that are useful) share the property that they are described by some limited metric about the observations that define them. I.e., for a normal distribution, you don't need to know every detail about every observation, just their total count and sum. To put it another way, assuming you already know the class/family of distribution, then the distribution has strictly lower information entropy than the observations that resulted in it. Does this seem trivial, or is it kind of what you're looking for?
