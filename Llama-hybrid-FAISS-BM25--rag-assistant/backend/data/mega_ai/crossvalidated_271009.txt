[site]: crossvalidated
[post_id]: 271009
[parent_id]: 270819
[tags]: 
I do not understand the need for this construction if you can simulate from $P(\phi|x)$ simulate from $P(\theta|\phi)$ since neither an approximation nor an MCMC implementation is then required. If you cannot analytically sample from P(θ|ϕ) , a regular Gibbs sampler will work as well: simulate from $P(\phi|x,\theta)\propto P(\phi|x)\times P(\theta|\phi)$ [which may require one [& only one] Metropolis-within-Gibbs step] simulate from $P(\theta|\phi)$ [which may require one [& only one] Metropolis-within-Gibbs step] simulate from $P(\phi|x,\theta)\propto P(\phi|x)\times P(\theta|\phi)$ &tc... This is less costly than the pseudo-marginal version, which requires $n$ $\phi_i$'s for one proposal of $\theta$. Note: The link to pseudo-marginal MCMC made by lacerbi is however correct in that if one manages to get an unbiased estimator of the posterior density (up to a constant) there are valid MCMC implementations based on such estimates. Which means your proposal is not correct because you use the same $\phi_i$'s for numerator and denominator in the Metropolis ratio. The random variables used to approximate the posterior density should be considered as auxiliary or latent variables and hence be only simulated for the proposal, while being preserved from the earlier stage for the current value: to quote from Darren Wilkinson's blog , "The key to understanding the pseudo-marginal approach is to realise that at each iteration of the MCMC algorithm a new value of w is being proposed in addition to a new value for x." Meaning that the acceptance ratio keeps the previous value of w for the previous value of x. In other words, the $\phi_i$'s are not to be re-simulated for computing the unbiased estimator at the previous value of $\theta$.
