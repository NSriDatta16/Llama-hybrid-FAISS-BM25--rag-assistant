[site]: crossvalidated
[post_id]: 496185
[parent_id]: 52104
[tags]: 
It seems that the question was not at all about the implementation/structural differences between (a) the softmax (multinomial logistic) regression model and (b) the OvR "composite" model based on multiple binary logistic regression models. In a nutshell, however, skipping all the formulas, these differences can be summarized like this: Training : the softmax regression model uses the cross entropy cost function , while the OvR "composite" model based on multiple binary logistic regressors trains completely independent binary logit classifiers using the logistic regression cost function . Trained model representation : not much difference - in softmax each class gets its own parameter vector, and these vectors are stored together in a common parameter matrix, while in OvR logit there are exactly as many separate parameter vectors, one for each positive class. Evaluation : the softmax regression model uses the softmax function that predicts a probability for each class considering the scores for other classes, while the OvR "composite" model based on multiple binary logistic regressors calculates the scores/probabilities of classes completely independently and then just picks the label with the highest score. It also seems that there was no need to explain the differences between the binary, the OvR/OvO "composite" models and the "native" multilabel classifiers like the multinomial logistic regressor (aka the softmax regressor). I think the question was more about THE ACCURACY : The softmax regression ( LogisticRegression(multi_class="multinomial") in scikit-learn) is more flexible when setting the linear decision boundaries among the classes. Here is a two-dimensional three-class illustration of this: https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_multinomial.html The above example really could have benefitted from the confusion matrices, so here they are (normalized): This is not a hard classification problem - the instances from the three classes are barely mixed, so we should expect very high accuracy for all the classes. But OvR Logit stumbles when identifying the "middle" class. Generally speaking, OvR Logit will perform poorly when there is low distinction for some class by the feature values alone. It only likes "edgy" classes. For binary classification this is not a disadvantage when compared to Softmax/multinomial, since the latter also sets a linear boundary between the two classes. Or imagine three clusters that are at approximately the same distances from each other (i.e. each class cluster is on the vertex of an equilateral triangle). In such a case the accuracy of both OvR Logit and Softmax will be good for all the classes. However, imagine one of the three clusters being at or near the straight line between the centers of the other two clusters... The accuracy of OvR Logit for that "middle" class will be poor. Softmax/multinomial regressor will do well (even though its decision boundaries are still straight lines).
