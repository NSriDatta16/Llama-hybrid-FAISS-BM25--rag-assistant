[site]: crossvalidated
[post_id]: 515635
[parent_id]: 
[tags]: 
Why were/are LSTMs popular for so long for NLP?

Talking with a professor at uni about NLP methods, I asked why LSTMs (and any RNN variant for that matter) were so popular for so long, given the vanishing gradient problem. More specifically, treating each alphanumeric character as a unique token integer {a:1, b:2...} then encoding a text string as such 'cat' -> [3,1,20] in a multilayer perceptron should work well, given that the network is sufficiently deep. He noted that this idea has already been considered using CNNs on such an encoding. I'm curious why RNN/LSTM variants seem to be more popular. Semi-related transformers seem to address this shortcoming of RNN/LSTM architectures, but this topic creeps the scope of my question. Anyway, is there a reason that LSTM/RNN architectures were more popular than MLP/CNN approaches to NLP?
