[site]: datascience
[post_id]: 122532
[parent_id]: 
[tags]: 
Error in seq2seq translation when passing predicted output to rnn due to input shape not always being the same

I'm working on a language translator and I'm getting an error I'm unsure about. During the decoding process when using argmax on the predicted output I am sometimes getting an RuntimeError RuntimeError: input must have 3 dimensions, got 4 On the iterations that work the input has a shape of torch.Size([64]) but some iterations the input This is my encoder and decoder models. class Encoder(nn.Module): def __init__(self, input_size, embedding_size, hidden_size, num_layers, p): super(Encoder, self).__init__() self.hidden_size=hidden_size self.mnum_layers=num_layers self.dropout = nn.Dropout(p) self.embedding = nn.Embedding(input_size, embedding_size) self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p) def forward(self, x): # x shape: (seq_length, N) embedding=self.dropout(self.embedding(x)) # embedding shape: (seq_len, N, embedding_size) outputs, (hidden, cell) = self.rnn(embedding) return hidden, cell class Decoder(nn.Module): def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, p): super(Decoder, self).__init__() self.hidden_size=hidden_size self.num_layers=num_layers self.dropout=nn.Dropout(p) self.embedding=nn.Embedding(input_size, embedding_size) self.rnn=nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p) self.fc=nn.Linear(hidden_size, output_size) def forward(self, x, hidden, cell): x = x.unsqueeze(0) # x shape:(1, N) embedding=self.dropout(self.embedding(x)) # embedding shape: (1, N, embedding_size) outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell)) prediction=self.fc(outputs) # prediction shape: (1, N, vocab_len) prediction.unsqueeze(0) return prediction, hidden, cell This is the Seq2seq model. class Seq2Seq(nn.Module): def __init__(self, encoder, decoder): super(Seq2Seq, self).__init__() self.encoder=encoder self.decoder=decoder def forward(self, source, target, teacher_force_ratio=0.7): batch_size = source.shape[1] target_len = target.shape[0] target_vocab_size = len(english.vocab) outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device) hidden, cell = self.encoder(source) # get start token input = target[0] for t in range(1, target_len): teacher_force = random.random() This is what the seq2seq print statements say teach force True input shape torch.Size([64]) output shape torch.Size([1, 64, 5893]) best guess shape torch.Size([1, 5893]) target[t] torch.Size([64]) ... teach force False input shape torch.Size([64]) output shape torch.Size([1, 64, 5893]) best guess shape torch.Size([1, 5893]) target[t] torch.Size([64]) teach force True input shape torch.Size([1, 5893]) I'm using Spacy and the de_core_news_sm to en_core_web_sm datasets using Mutli30k to split the data into training, validation and test datasets. spacy_german = spacy.load("de_core_news_sm") spacy_english = spacy.load("en_core_web_sm") def tokenizer_german(text): return [token.text for token in spacy_german.tokenizer(text)] def tokenizer_english(text): return [token.text for token in spacy_english.tokenizer(text)] german = Field(tokenize=tokenizer_german, lower=True, init_token=' ', eos_token=' ') english = Field(tokenize=tokenizer_english, lower=True, init_token=' ', eos_token=' ') train_data, validation_data, test_data=Multi30k.splits(exts=('.de', '.en'), fields=(german, english)) german.build_vocab(train_data, max_size=10000, min_freq=2) english.build_vocab(train_data, max_size=10000, min_freq=2) This is the traceback Traceback (most recent call last): File "language_translator.py", line 194, in output = model(input_data, target) File "/Users/yeliab/anaconda3/envs/xnap-example/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl result = self.forward(*input, **kwargs) File "language_translator.py", line 113, in forward output, hidden, cell = self.decoder(input, hidden, cell) File "/Users/yeliab/anaconda3/envs/xnap-example/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl result = self.forward(*input, **kwargs) File "language_translator.py", line 78, in forward outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell)) File "/Users/yeliab/anaconda3/envs/xnap-example/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl result = self.forward(*input, **kwargs) File "/Users/yeliab/anaconda3/envs/xnap-example/lib/python3.8/site-packages/torch/nn/modules/rnn.py", line 579, in forward self.check_forward_args(input, hx, batch_sizes) File "/Users/yeliab/anaconda3/envs/xnap-example/lib/python3.8/site-packages/torch/nn/modules/rnn.py", line 530, in check_forward_args self.check_input(input, batch_sizes) File "/Users/yeliab/anaconda3/envs/xnap-example/lib/python3.8/site-packages/torch/nn/modules/rnn.py", line 174, in check_input raise RuntimeError( RuntimeError: input must have 3 dimensions, got 4 I'm really unsure why the input shape is sometimes different.
