[site]: crossvalidated
[post_id]: 545431
[parent_id]: 
[tags]: 
Why do we need the concept of Risk in Bayesian Decision theory?

I'm studying Bayesian decision theory as introduction to machine learning and I see the concept of Risk in a lot of places. In the course I read, they define risk as: Risk is the expected error measured by a loss function. Which makes me think, why do we need this special construct of Risk? Isn't loss function already denoting the error? Same with Bayes Risk, which is the lowest of all errors. Why not call it lowest loss possible? Am I missing something here?
