[site]: crossvalidated
[post_id]: 272327
[parent_id]: 
[tags]: 
Training on censored data with varying sequence length

I have a sequence of daily events, along with times between events. Each such sequence is labeled by $i$ and consists of $n_i$ events. For every $i$, each event $j$ has a list of features $x_{ij}$, and I would like to predict the time $y_{ij}$ till the next event, given the entire past history. This is a pretty classic problem in censored survival time. While not explicitly relevant to what's below, the model is an LSTM. My question is more geared towards how to train this model correctly. Here's what worries me: In general, the last event in each such sequence is censored. When training this model, my loss function is the average loss over each consecutive pair in the sequence. So for a sequence $i$, the loss function looks something like: $L_i=\frac{1}{n_i}\sum_{k=1}^{n_i}f(\hat{y}_{ij},y_{ij}|x_{i1},x_{i2},\cdots,x_{in_i},c_{ij}),$ where $c_{ij}$ is 1 if event $ij$ is censored and 0 otherwise and $\hat{y}_{ij}$ is the predicted time. To be clear, here's a time diagram of a given sequence $i$, with the dashes representing how many days pass between events, and the | representing the end of the sequence (so $x_{i4}$ is censored): $x_{i1}$--------$x_{i2}$---$x_{i3}$-------------------$x_{i4}$----| So then I feed these sequences into my model. However, I could also generate more sequences from my model. For example the above sequence can make: $x_{i1}$--------$x_{i2}$-|--$x_{i3}$-------------------$x_{i4}$---- where now $x_{i2}$ is censored and we throw out $x_{i3},x_{i4}$. I would think that this procedure would make my predictive model more robust to different sequence lengths, especially if there are imbalances of sequence length in my training set. But I'm finding it hard to convince myself that it would improve my model, as opposed to just training on existing sequences. What exactly does one do in classical survival time modeling?
