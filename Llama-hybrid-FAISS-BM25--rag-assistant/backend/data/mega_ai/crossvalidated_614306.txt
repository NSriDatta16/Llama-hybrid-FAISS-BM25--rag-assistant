[site]: crossvalidated
[post_id]: 614306
[parent_id]: 526158
[tags]: 
I dispute that the classes are so inseparable. For instance, in perplexity 4, observations around $(50, -20)$ are almost certainly going to be red, yet points around $(40, 30)$ seem to be an even mix of red and green. Since green is so outnumbered by red, a $50/50$ chance of green in a region is quite remarkable! Now, t-SNE can create groupings that do not exist just as it can miss groupings that do exist, so that perplexity 4 plot is not necessarily indicative of how the real data look in many dimensions. Nonetheless, having clusters like this strikes me as at least a positive sign. One of the issues perhaps leading you to have poor results is the concern with a threshold-based, improper scoring rule like $F_1$ score. Among the issues, $F_1$ score does not evaluate the XGBoost model. The $F_1$ score evaluates the XGBoost model along with a decision rule based on a threshold that might be wildly inappropriate for your task. A standard software default is a threshold of probability $0.5$ . In all four of the plots, there are few regions where there will be a probability of $0.5$ of the point being green. You might find yourself having better luck evaluating the raw outputs of your model instead of applying a software-default threshold. At the very least, you can tune the threshold to change your $F_1$ score. Finally, it might be that the classes are simply quite similar and cannot be separated do a large extent on your data. I will leave some links on class imbalance and the drawbacks of threshold-based performance metrics. Are unbalanced datasets problematic, and (how) does oversampling (purport to) help? Profusion of threads on imbalanced data - can we merge/deem canonical any? Why is accuracy not the best measure for assessing classification models? Academic reference on the drawbacks of accuracy, F1 score, sensitivity and/or specificity
