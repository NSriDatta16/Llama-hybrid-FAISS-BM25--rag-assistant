[site]: stackoverflow
[post_id]: 5418646
[parent_id]: 767697
[tags]: 
Based on this work I tried to convert a H264 signal to a SWF stream that could be easily be displayed in Flash. Here is the recipe. (This recipe is for Linux.) Download Live555 streaming media, from http://www.live555.com/liveMedia/ The src file you have is usually named live555-latest.tar.gz Unpack and compile: Unpack: tar xzvf live555-latest.tar.gz . This will create a directory named live. cd live ./genMakefiles linux (if you have a 32 bit system) or ./genMakefiles linux-64bit if your system is 64-bit) make , and after a while you'll have a brand new compiled code Live55 has a lot of good stuff, but we are only interested in the "testProgs" directory, where openRTSP resides. OpenRTSP will let us receive a signal and send it to ffmpeg, a program wich feeds ffserver. Ffserver is a server that receives the signal from ffmpeg and converts it to SWF (and other formats). Download, unpack, configure and install ffmpeg Download ffmpeg from http://www.ffmpeg.org/ . The version I tested is 0.6.1: http://www.ffmpeg.org/releases/ffmpeg-0.6.1.tar.gz Unpack: tar xzvf ffmpeg-0.6.1.tar.gz . This will create a directory named ffmpeg-0.6.1 cd ffmpeg-0.6.1 All the funny video streaming things are packaged in VideoLan. So you better install VideoLan right now. Go to http://www.videolan.org/ and see how easy is to install it. You may be surprised that the package dependencies contains ffmpeg libraries. After installing VideoLan do ./configure and then make . After 3 or 4 hours you will have mmpeg and mmserver compiled and working. Now we are almost ready to stream the whole world. First of all, let's try to get openRTSP working. Go to your "live" directory (remember 3.2) and do: cd testProgs Try this: ./openRTSP -v -c -t rtsp:// : / First of all, you'll see logs which says something like: - opening conection blah blah. - sending DESCRIBE blah blah. - receiving streamed data. If all goes OK, your console will start to print a lot of strange characters very quickly. These characters are bytes of video, but you can't see it (now). If you don't see your screen printing characters, there is something wrong with your configuration. Check the steps up to now. We got the signal! Now let's send it to an useful component: ffmpeg, which is bound to ffserver. We need to create a configuration file for ffserver. Use your favorite editor to create this text file: Port 8090 BindAddress 0.0.0.0 MaxHTTPConnections 2000 MaxClients 1000 MaxBandwidth 1000 CustomLog - NoDaemon File /tmp/feed1.ffm FileMaxSize 200K ACL allow 127.0.0.1 Feed feed1.ffm Format swf VideoFrameRate 25 VideoSize 352x288 VideoIntraOnly NoAudio Format status ACL allow localhost ACL allow 192.168.0.0 192.168.255.255 Name the file, for example, ffserver.conf. Save it anywhere, for example in the same directory of ffserver. So, ffserver will be bound to the port 8090, for input and output. tag configures the input stream. The name of the configured feed in this case is feed1.ffm. Remember it for step 14. contains configuration for the output stream. In this case the name will be testFlash.swf (remember too), and the format will be SWF. The video frame rate will be 25 and the size 352x288, and it won't contain audio. The last stream is a HTML file (stat.html) that will show you the status of the server. Start ffserver: ./ffserver -f ffserver.conf (or wherever you have left the config file). The -f parameter indicated that you will load the confugration from a custom file. Open a navigator and go to http://localhost:8090/stat.html . A status page of the server will show up, and we'll see a line of information about our testFlash.swf stream. It seems very quiet now, so let's feed this stream with the output of openRTSP (from step 7). Do this: /openRTSP -v -c -t rtsp:// : / | /ffmpeg -i - http://localhost:8090/feed1.ffm The first path (before the "|" is the same as step 9. "|" is a symbol that connects the output of openRTSP (the sequence of video signal, aka strage chars) to be the input of ffmpeg. "-I -" means that the input of mmpeg is taken from the pipe "|" and http://localhost:8090/feed1.ffm is the destination (output) of ffmpeg, wich is basically the input of ffserver. So with this command we have connected openRTSP -> ffmpeg -> ffserver When you enter this command a lot of information will be shown. Is important to note that the input params and the output params are shown, and these params NEED to be "compatible". In my case, this will be shown: Input #0, h264, from 'pipe: ': Duration: N/A, bitrate: N/A Stream #0.0: Video: h264, yuv420p, 352x288, 25 fps, 25 tbr, 1200k tbn, 50 tbc Output #0, ffm, to 'http://localhost:8090/feed1.ffm': Metadata: encoder: Lavf52.64.2 Stream #0.0: Video: FLV, yuv420p, 352x288, q=2-31, 200 kb/s, 1000k tbn, 25 tbc Stream mapping: Stream #0.0 -> #0.0 And then the stream begins to play. You will see in the last line numbers CONSTANTLY changing, telling you the live frame rating in each moment. Something like frame= 395 fps= 37 q=31.7 Lsize = 1404kB time=15.80 bitrate = 727.9kbits/s If you don't see this line of metrics, then there is something wrong with your output configuration. Go back and change the parameters of testFlash.swf. Everything is done. You can see the video in http://localhost:8090/testFlash.swf . You can use this URL to embed a Flash movie or, as in my case, a Flex application.
