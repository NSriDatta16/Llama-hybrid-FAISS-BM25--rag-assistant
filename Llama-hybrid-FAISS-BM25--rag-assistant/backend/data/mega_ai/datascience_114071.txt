[site]: datascience
[post_id]: 114071
[parent_id]: 
[tags]: 
Avoid leakage in NLP extraction

What is best practice for applying traditional NLP extraction techniques a pre-processing for ML models? Given a pipeline: Collect raw data. Parse full data set with a variety of traditional NLP techniques, to create model-compatible features (e.g. one-hot encoded matrix of entity extraction). Train a ML model on the data. My intuition says you must split the data inbetween step 1 and 2, for example, only running TF-IDF or NMF on your training set. But, I have seen a lot in papers and production, that non-deep learning NLP techniques are often used before a data split.
