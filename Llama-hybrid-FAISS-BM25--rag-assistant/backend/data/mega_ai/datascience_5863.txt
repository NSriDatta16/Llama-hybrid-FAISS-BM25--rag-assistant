[site]: datascience
[post_id]: 5863
[parent_id]: 
[tags]: 
Where does the sum of squared errors function in neural networks come from?

Training a basic multilayer perceptron neural network boils down to minimizing some kind of error function. Often the sum of squared errors is chosen as a this error function, but where does this function come from? I always thought this function was chosen because it makes sense intuitively. However, recently I learned that this is only partly true and there is more behind it. Bishop wrote in one of his papers that the sum of squared errors function can be derived from the principle of maximum likelihood . Furthermore he wrote that the squared error therefore makes the assumption that the noise on the target value has a Gaussian distribution. I am not sure what he means with that. How does the sum of squared errors relate to the maximum likelihood principle in the context of neural networks?
