[site]: crossvalidated
[post_id]: 381031
[parent_id]: 
[tags]: 
Question about sample size for each class for machine learning classifiers

I'm trying to use a machine learning classifier (SVM in particular) on data that I generate. Unlike other applications, the data is not given to me but rather I have the flexibility to generate how much I need, albeit with costs. My data only has 2 classes (class A and class B). If I were to respect the probability distribution of the data, I'd end up with an imbalanced class. Class A occurs with a significantly smaller probability than the other, something like fraud detection. To address this, I am planning to generate more samples for Class A, i.e. oversampling it. I must also say that generating samples for Class A is much more costly than generating samples for Class B (I have an approximation that I trust for Class B). If I were to oversample Class A, setting costs of sampling aside, would the ideal situation be for both classes to have the same number of samples? Or, can I still oversample Class A but with fewer samples than Class B and use weighted SVM to address the imbalance? Also, for the test data, should the proportion of samples in each class be equivalent to the proportions in the training data? I would appreciate thoughts on the issue, if you have any experience with this situation.
