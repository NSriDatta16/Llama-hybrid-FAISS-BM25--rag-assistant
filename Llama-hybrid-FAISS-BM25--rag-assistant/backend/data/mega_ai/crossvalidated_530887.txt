[site]: crossvalidated
[post_id]: 530887
[parent_id]: 
[tags]: 
Notation of the Likelihood Term in Bayesian Neural Networks

I see that in Bayesian neural networks likelihood function is defined in two ways: $p(W|D) = Z^{-1} p(D|W)p(W)$ or $p(W|y,x)=Z^{-1}p(y|x,W)p(W)$ Are there a slight difference in interpreting $p(D|W)$ and $p(y|x,W)$ ? is the first one read as the probability of our data given the parameters (weights of the NN) and the second one, the probability of the output label given the input and the parameters? to me, the first notation is more intuitive. I wonder if there is no difference among them, why author choose to go with the second one?
