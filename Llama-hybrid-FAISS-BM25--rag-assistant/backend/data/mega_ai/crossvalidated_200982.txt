[site]: crossvalidated
[post_id]: 200982
[parent_id]: 
[tags]: 
Do Bayesian priors become irrelevant with large sample size?

When performing Bayesian inference, we operate by maximizing our likelihood function in combination with the priors we have about the parameters. Because the log-likelihood is more convenient, we effectively maximize $\sum \ln (\text{prior}) + \sum \ln (\text{likelihood})$ using an MCMC or otherwise which generates the posterior distributions (using a pdf for each parameter's prior and each data point's likelihood). If we have a lot of data, the likelihood from that is going to overwhelm any information that the prior provides, by simple mathematics. Ultimately, this is good and by design; we know that the posterior will converge to just the likelihood with more data because it is supposed to. For problems defined by conjugate priors, this is even provable exactly. Is there a way to decide when priors don't matter for a given likelihood function and some sample size?
