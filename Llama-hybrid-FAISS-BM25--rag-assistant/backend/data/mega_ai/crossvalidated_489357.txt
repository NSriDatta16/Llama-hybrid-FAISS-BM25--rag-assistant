[site]: crossvalidated
[post_id]: 489357
[parent_id]: 265740
[tags]: 
A simple approach would be to treat each event type as independent, and then to build one model per event type. If you expect events to happen on a regular schedule, then an informative feature could be time-since-last-event . To evaluate the viability of such a model one should do some Exploratory Data Analysis and plot the histograms of such features and analyze whether outliers are present and visible. If it looks reasonable then one could fit a model to the features. If the features are continuous and normally distributed, the distance from the normalized distribution might be a decent anomaly score. That is easy to compute for a single feature (univariate). For multiple features one would use something like EllipticEnvelope or GaussianMixtureModel. One could build a multi-event anomaly scoring model using the anomaly scores fro the per-event models. I see that there are also failure/complete status for events. One might summarize those over a time-period and compute the (time-averaged) failure-rate. Either per-event-type or across all. This one could also build an anomaly detection model on. Perhaps just simple thresholding.
