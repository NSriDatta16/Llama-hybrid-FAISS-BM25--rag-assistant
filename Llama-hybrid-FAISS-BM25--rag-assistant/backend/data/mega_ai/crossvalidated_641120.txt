[site]: crossvalidated
[post_id]: 641120
[parent_id]: 637687
[tags]: 
We can predictively manipulate our inference regardless of what data we will get. If I decide to bug the person in the room one more time, I know beforehand that my best estimate would be N=k+1 . So I don't even need to ask the person anymore. I can already update my belief! As Cliff noted , this isn't necessarily true. Assuming $k samples with $k$ different results $d_1,...,d_k$ , you're always equally likely to get $d_{k+1}$ very close to one of the previous $k$ samples. This is true even for a certain range of $k>N$ values, as there's no guarantee that upon hitting $k=N$ requests you have been "exposed" to all $N$ options. The only thing that is guaranteed here is that for $K\ge N+1$ you have at least one cluster with two or more samples, due to the pigeonhole principle. Taking no assumptions other than $N\le100$ , you can solve this with Bayesian GMM and the Gibbs sampling algorithm. As you have known variance for the Gaussian models themselves and a strong assumption regarding the mixture probabilities. However we still take the probabilities vector $\pi$ as unknown so we don't limit ourselves to a fixed number of clusters (you'll see later why this works). We denote $\theta=\{\pi,\{\mu_i\}_{i=1}^{100}\}$ our vector of parameters. An initial prior for $\pi$ would be $\pi\sim Dirichlet(\alpha_1,...,\alpha_{100})$ . Of course $\forall i:\alpha_i>0, \pi_i\ge0, \sum_i\pi_i=1$ . As you have a uniform assumption regarding the identities in the keystroke, you might want to take here $\alpha_1=...=\alpha_{100}$ and give it a high value (which reflects your high certainty in the Uniform manner). Next the expectation vector with prior $\mu_i\sim U[1,10^6]$ . Using the Bayes theorem we get $P(\mu|D)\propto P(D|\mu)P(\mu)$ (the denominator is a normalization constant which we can overlook for the moment). As the likelihood $P(D|\mu)$ is Gaussian and the prior $P(\mu)$ is uniform, the posterior $P(\mu|D)$ is a truncated Gaussian. We start by choosing initial values for $\pi_i,\mu_i$ (for all $i$ values) and denote them $\pi_i^{(0)},\mu_i^{(0)}$ . We then work iteratively, at each iteration $t+1$ we calculate the following steps, each step is calculated for all $k$ samples: Responsibility $q_{ji}^{(t+1)}$ is the predicted probability of the sample $D_j$ belonging to the $i^{th}$ cluster after step $t$ : $$q_{ji}^{(t+1)}=\frac{\pi_i^{(t)}\mathcal{N}(D_j;\mu_i^{(t)},1)}{\sum_{i=1}^N \pi_i^{(t)}\mathcal{N}(D_j;\mu_i^{(t)},1)}$$ Affiliation $$k_j^{(t+1)}\sim Multinomial \left(q_{j1}^{(t+1)},q_{j2}^{(t+1)},...,q_{jN}^{(t+1)}\right)$$ Cluster sizes: $$N_i^{(t+1)}=\sum_{j=1}^k\mathbb{I}\{k_j^{(t+1)}=j\}$$ Sampling the probabilities vector: $$(\pi^{(t+1)}|k^{(t+1)},D)\sim Dirichlet\left(N_1^{(t+1)}+\alpha_1,...,N_N^{(t+1)}+\alpha_N\right)$$ Calculating the expectations vector: Let $D^{i,(t+1)}$ be the subset of samples belonging to the $i^{th}$ cluster at this step: $$(\mu_i^{(t+1)}|k^{(t+1)},D^{i,(t+1)})=\frac{1}{N_i^{(t+1)}}\sum D^{i,(t+1)}$$ What happens eventually is that if the actual number of clusters is smaller that what we've initialized to, we'll get those clusters with 0 members and close to 0 mixture probability. You can also verify the number of clusters using the evidence function. Upon receiving a new sample $k+1$ you can re-run the whole process or (with some risks) initialize to the previous values. Note that for the whole process, we have sticked with the basic assumptions and took nothing more.
