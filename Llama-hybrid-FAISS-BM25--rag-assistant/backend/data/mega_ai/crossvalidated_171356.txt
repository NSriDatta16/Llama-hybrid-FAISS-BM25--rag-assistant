[site]: crossvalidated
[post_id]: 171356
[parent_id]: 112250
[tags]: 
Equation 7 in Hersbach inspires me to notice that the RPS (as a discrete version of the continuous RPS, or CRPS) is a sum of several Brier quadratic probability scores (BS) evaluated over several probability thresholds. (Hersbach goes on to develop an interpretable decomposition of the CRPS for ensemble forecasts.) That is \begin{equation} \mathrm{RPS}=\sum\limits_{i=1}^{r} BS(i), \end{equation} where $BS(i)$ is the Brier score for a single forecast of the probability that the outcome of interest is one of the first $i$ (out of $r$ possible) outcomes. Seeing the RPS as a sum of Brier scores is potentially interesting because the Brier score is a sum three interpretable components (see wikipedia , or page 754 here ): $$BS = reliability - resolution + uncertainty$$ Reliability is a measure of the absence of bias. Resolution is somewhat analogous to the R-squared in regression (but don't look for an exact analogy since there isn't a clear definition for R-squared for predictions with binary outcomes). Uncertainty is somewhat analogous to residual standard error in regression. If you see the RPS as a sum (or mean) of Brier scores and if you like the Brier score decomposition mentioned above, then surely there is a way to write the RPS as a something like $$RPS = \overline {Rel} - \overline {Res} + \overline {Unc} $$ where the terms on the right-hand-side are the means of the Brier score decomposition components over the $r$ underlying Brier scores. Thus, in a heuristic sense, I'm guessing that the closest thing you'll get to an "R-squared" with the RPS is an "average BS R-squared" if you take the time to do the decomposition. For a relatively rigorous discussion of this decomposition for the RPS, see equation (8b) in Candille and Talagrand .
