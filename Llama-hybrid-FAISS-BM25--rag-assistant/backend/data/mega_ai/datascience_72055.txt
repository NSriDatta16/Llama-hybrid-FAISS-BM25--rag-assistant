[site]: datascience
[post_id]: 72055
[parent_id]: 58795
[tags]: 
To make it a fair test, you need to ensure that neither model has seen any of the data in test_02 . The closer you can get each subset of the data to match the distribution of the overall combined data set, the better. If you steer by that, the approach you suggested is fine. A more direct way of answering the question of whether you need a new model is to work out the average accuracy of a new model trained on the updated data set (with the new samples) by using k-fold cross-validation, and then compare that to the accuracy of the average model trained on the old data and tested on the combined data .
