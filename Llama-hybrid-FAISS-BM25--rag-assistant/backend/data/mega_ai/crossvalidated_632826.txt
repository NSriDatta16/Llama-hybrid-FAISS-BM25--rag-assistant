[site]: crossvalidated
[post_id]: 632826
[parent_id]: 
[tags]: 
How to analyze data which has varying amounts of measurements recorded?

I'm doing research which involves data pertaining to the physical characteristics, recorded at consistent intervals, of mice, with the mice's time at death being recorded as well. As such, some mice have more timepoints recorded than others. How should I approach modelling this? I've considered using a hierarchical linear model, but I admittedly am not especially knowledgeable about this sort of analysis and was curious if anyone had suggestions. There are a large number of total datapoints for several hundred individual mice, so I had initially planned on using a neural network before I saw the variety of recorded data. Is there a way I could implement this approach in these circumstances? Or would this be unnecessary? Thanks for any help. Edit: Basically, I have information pertaining to these mice over time, with data being collected every 4 weeks or so, and including blood glucose levels, food intake, and other similar metrics. There is also discrete information, such as the genetic strand of the mice, and lastly a "time of death" value. Because the mice died at very different times, certain things, such as food intake, have data corresponding to shorter or longer time periods (so every 4 weeks for 18 months versus every 4 weeks for 5 months depending on how long it lived). I'd like to make a model to determine which of these factors are most significant in how long these mice lived for, i.e. which provides the best information for predicting how long these mice will live. Hope this clears things up.
