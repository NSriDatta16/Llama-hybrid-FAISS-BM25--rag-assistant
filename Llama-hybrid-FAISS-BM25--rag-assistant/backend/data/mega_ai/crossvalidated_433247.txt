[site]: crossvalidated
[post_id]: 433247
[parent_id]: 
[tags]: 
How does standard deviation error change/improve as I average more measurements?

Let's say I can measure something with a know standard deviation of $\sigma$ . Let's say I measure it $n$ times and take the mean of the $n$ measurements to determine a more accurate mean value. What is the new $\sigma$ for my mean measurement? I can intuitively know that it'll be smaller than the original $\sigma$ but still greater than $0$ , but how does one calculate the new $\sigma$ value? Bonus question (but much harder I suspect): What if still have the $n$ measurements, but they all have slightly different $\sigma$ . How do I now compute/derive the new $\sigma$ (given that the $n$ measurements all had slightly different $\sigma$ ). (In case it helps, $n$ will typically be ~ $40$ , but it can be as little as $10$ or as much as $500$ ; also, the different $\sigma$ do vary, but not by much: the smallest $\sigma$ and the biggest $\sigma$ will differ by at most ~ $2\times$ )
