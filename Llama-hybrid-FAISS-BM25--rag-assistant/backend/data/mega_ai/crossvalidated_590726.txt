[site]: crossvalidated
[post_id]: 590726
[parent_id]: 
[tags]: 
calibration according to Kuleshov

i was reading this paper https://arxiv.org/pdf/1807.00263.pdf in which Kuleshov gives a definition of a well calibrated neural network in a regression task I would like to ask one thing: why does he makes a special cases for when $x_t,y_t$ are i.i.d. realizations of random variables $X, Y \sim \mathbb{P}$ ? when we do regression don't we always assume that both the independent and dependent variables are jointly distributed? also why is a sufficient condition?
