[site]: crossvalidated
[post_id]: 580826
[parent_id]: 
[tags]: 
Oja's rule on convolutional layer

The Oja's rule for updating weights is defined as: $$ \Delta \textbf{w} = \textbf{w}_{n+1} - \textbf{w}_{n} = \eta \textbf{y}_n(\textbf{x}_{n} - \textbf{y}_{n}\textbf{w}_{n}) $$ where $\textbf{x}$ is vector of inputs, $\textbf{y}$ is the vector of outputs and $\textbf{w}$ is the weight matrix that connects the inputs $\textbf{x}$ and outputs $\textbf{y}$ (In neural networks, this connection is reffered to as a fully connected layer). Is there an established way to implement the Oja's rule for convolutional layers?
