[site]: crossvalidated
[post_id]: 109582
[parent_id]: 
[tags]: 
How to understand this objective function in deep learning

I'm going through Christopher Manning's tutorial from NAACL 2013 "Deep Learning for NLP (without Magic)" and he gets to the point where he's showing how to do unsupervised pre-training. He's saying that we compute two different scores $s$ and $s_c$ and then we want to train the network so that $s > s_c$. He claims that the objective function to use is $$ J = \text{max}(0, 1 - s + s_c) $$ I don't see how minimizing this function ensures that $s > s_c$. It seems to me that the value is greater than zero whenever the sum of $s$ and $s_c$ is greater than 1 and it doesn't matter which one is greater than the other. Could someone help clear this up for me? What am I missing? The video is here: http://techtalks.tv/talks/deep-learning-for-nlp-without-magic-part-1/58414/ and the part I'm asking about is at about the 50:30 mark. The slides are at http://nlp.stanford.edu/courses/NAACL2013/NAACL2013-Socher-Manning-DeepLearning.pdf and the slide in question is number 50.
