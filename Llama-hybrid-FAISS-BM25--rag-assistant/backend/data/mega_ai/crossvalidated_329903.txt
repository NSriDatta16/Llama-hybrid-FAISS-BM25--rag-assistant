[site]: crossvalidated
[post_id]: 329903
[parent_id]: 328630
[tags]: 
If you're familiar with linear operators then you may like my answer as most direct path to understanding the phenomenon: why doesn't least norm regression fail outright? The reason is that your problem ($n\ll p$) is the ill posed inverse problem and pseudo-inverse is one of the ways of solving it. Regularization is an improvement though. This paper is probably the most compact and relevant explanation: Lorenzo Rosasco et al, Learning, Regularization and Ill-Posed Inverse Problems . They set up your regression problem as learning, see Eq.3., where the number of parameters exceeds the number of observations: $$Ax=g_\delta,$$ where $A$ is a linear operator on Hilbert space and $g_\delta$ - noisy data. Obviously, this is an ill-posed inverse problem. So, you can solve it with SVD or Moore-Penrose inverse, which would render the least norm solution indeed. Thus it should not be surprising that your least norm solution is not failing outright. However, if you follow the paper you can see that the ridge regression would be an improvement upon the above. The improvement is really a better behavior of the estimator, since Moore-Penrose solution is not necessarily bounded. UPDATE I realized that I wasn't making it clear that ill-posed problems lead to overfitting. Here's the quote from the paper GÃ¡bor A, Banga JR. Robust and efficient parameter estimation in dynamic models of biological systems . BMC Systems Biology. 2015;9:74. doi:10.1186/s12918-015-0219-2: The ill-conditioning of these problems typically arise from (i) models with large number of parameters (over-parametrization), (ii) experimental data scarcity and (iii) significant measurement errors [19, 40]. As a consequence, we often obtain overfitting of such kinetic models, i.e. calibrated models with reasonable fits to the available data but poor capability for generalization (low predictive value) So, my argument can be stated as follows: ill posed problems lead to overfitting (n Moore-Penrose psudo-inverse (or other tools like SVD), which you refer to in the question as $X^+$, solves an ill-posed problem therefore, it takes care of overfitting at least to some extent, and it shouldn't be surprising that it doesn't completely fail, unlike a regular OLS should Again, regularization is a more robust solution still.
