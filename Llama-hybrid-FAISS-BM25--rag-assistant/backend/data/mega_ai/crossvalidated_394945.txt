[site]: crossvalidated
[post_id]: 394945
[parent_id]: 394927
[tags]: 
In fairness, the paper's details are somewhat light on explaining this. The goal of the neural network here is to segment multiple types of lung nodules from CT scans. The proposed method is to initially use a U-Net architecture to generate the mask. This is fairly standard in biomedical imaging. The major difficulty with lung segmentations is small training sets, and absolutely massive images. For example an 80x80x80 CT scan has 512K pixels, compared to say a downsized 299x299 photo image in a simple classifier, which has just 90K. So overfitting is a big problem. The novelty here is that, instead of directly measuring the error on the mask generated by the output of the U-Net, the authors instead choose to measure loss at the neck of the Unet, and then use a conditional GAN to verify whether the generated mask is real. Specifically, the conditional GAN is fed the original image for conditioning, presumably to ensure the mask is sensible on the input. At the neck of the U-Net is a final bank K convolutional filters (kernels), acting on a spacial field of size $IxJ$ . The output of the kernels will therefore be $IxJxK$ , and the idea is to use max-pooling across the $K$ 'th dimension, to pin whether or not pixel $(i,j)$ corresponds to the interior of a lung nodule. So each kernel is its own classifier, and we treat each pixel as a "bag of K classifications", using max-pooling to resolve whether or not there's a nodule at that pixel. One critical detail that's missing here (for me, at least), is that there's no mention of kernel strides, so presumably the $IxJ$ field is a substationial reduction from the original image of size $WxH$ , so that each "pixel" in the $IxJ$ region will have some relatively large receptive field on the original image. Additional: Take pixel $(i,j)$ in $M$ . This pixel has some receptive field $A$ which is a collection of pixels $(x,y)$ in the original image. Again, I'm not sure on this exact detail but, it seems like if $A$ has at least one positive pixel, then $(i,j)$ should report a positive classification. Otherwise it's negative. The authors use two loss functions, one for positive pixels, another one for negative ones. The positive loss function is a max pool of the kernels. The negative loss functions is the mean. I'm guessing the reason for this choice is as follows. When a pixel is positive, we are looking at the maximum of all kernels, so that gradient descent will only alter the weights on the current-maximum kernel, whereas the gradient will be 0 on all other kernels. In other words, for positive pixels, only one kernel, the maximum one, will be updated. Whereas, on negative pixels, the authors use mean_pooling, because this will allow gradient descent to update all kernels simultaneously. This should significantly speed up training. I'm guessing that at classification time, you're only looking at the max pool results, and if they exceed your threshold, you'll say a region is positive.
