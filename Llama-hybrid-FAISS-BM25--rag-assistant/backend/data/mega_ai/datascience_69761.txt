[site]: datascience
[post_id]: 69761
[parent_id]: 
[tags]: 
Q-learning, state transition, immediate rewards (trading logic)

I've been thinking about how to correctly calculate rewards for several weeks now. Here is a grid example: ################################################################################## # | 0 | 1 | 2 | --------------- 0 |...|...|...| 1 |...|.A.|.G.| 2 |...|...|...| ################################################################################## - the Agent(A) is currently at State(t) = [1,1] - if he moves right -Goal(G)- he would get a Reward(t) of 100 - if the Agent(A) goes another way, he gets a Reward(t) of 0 - the selected Action(t) at State(t) depends ONLY on State(t) (no exploration) - BUT the Reward(t) is calculated depending on State(t+1)? - depending on Action(t) and based on State(t) we do the Transition to State(t+1) ################################################################################## Is that correct up to here? If not, then I have misunderstood something and therefore I am not making progress. I try to implement the whole thing in a trading logic, I am not sure if I am doing it right. Imagine a simple time series and 3 Actions (B=BUY/H=HOLD/C=CLOSE): # 0 1 2 3 4 5 6 7 8 9 . . . . . . . . . 19 (index/position) ################################################################################## # 1 3 5 8 7 9 2 3 6 4 . . . . . . . . . 8 (data/value/price) # . . B H H C . . . . . . . . . . . . . done # t^ buy open ################################################################################## The Agent opens a BUY on Index(t)=2 at Price(t)=5, the Reward(t) is 8(t+1) - 5(t) = 3. I open a trade on State(t), the Reward(t) depends on the next value(t+1)? Is that correct? And if i close the trade? The next value doesn't matter, as I'm closing the trade at the current Price(t)? In the examples here, the difference(entry.price - exit.price) is used, which is very confusing: https://github.com/miroblog/deep_rl_trader/blob/master/TraderEnv.py#L94 https://github.com/edwardhdlu/q-trader/blob/master/train.py#L36 I see 2 problems, the drawdown while holding the positions is ignored and the sparse rewards are more difficult to learn, what do you think? Thanks in advance! EDIT: The main question is: In a grid world, the Action(t) is done at State(t), BUT the Reward(t) depends on the next State(t+1), because only if this is the goal, it gets a reward. In a trading environment, i can find a lot of different versions, some guys are using future values, some are using the difference of entry/exit/current close price or other stuff like sharpe/sortino/... What is correct in a trading environment and why? or How do i calculate the rewards for a trading bot?
