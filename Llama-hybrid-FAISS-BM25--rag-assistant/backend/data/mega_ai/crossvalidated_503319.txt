[site]: crossvalidated
[post_id]: 503319
[parent_id]: 
[tags]: 
Why are log odds modelled as a linear function?

I think I already have the answer, however, I wish for some confirmation that I am not missing anything here. This sort of asks the same thing, but I want to double-check. Logistic regression can be motivated via generalized linear models . GLM, in essence, says that we model the transformed (“linked” so to speak) expected value $\mu$ of a variable $Y$ given covariates/features as a linear function. Let's call the link function $g()$ . In case of the classical linear regression model this function would simply be the identity function. If $Y$ is binary, the expected value is equal to $p = P(Y = 1)$ . In the logistic regression model, we model the log-odds as a linear function: $$ \log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1x_1 + \dots + \beta_Kx_K$$ So the assumption is that the log-odds are adequately described by a linear function. The logit function, however, clearly is not a linear function . Yet, it is reasonably approximated by a linear function if we truncate the probability range to something like $0.05 . Question: why do we model the log-odds as a linear function when it is nonlinear for small and large probabilities? My answer would be that since we are interested in the expected value, we assume (!) that the relevant range of probabilities we are trying to estimate does not contain these “extreme” probabilities. Hence, in essence, we simply ignore the nonlinearity. Correct?
