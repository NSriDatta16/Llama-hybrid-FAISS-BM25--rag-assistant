[site]: datascience
[post_id]: 77817
[parent_id]: 77813
[tags]: 
I've often had LogisticRegression "not converge" yet be quite stable (meaning the coefficients don't change much between iterations). Maybe there's some multicolinearity that's leading to coefficients that change substantially without actually affecting many predictions/scores. Another possibility (that seems to be the case, thanks for testing things out) is that you're getting near-perfect separation on the training set. In unpenalized logistic regression, a linearly separable dataset won't have a best fit: the coefficients will blow up to infinity (to push the probabilities to 0 and 1). When you add regularization, it prevents those gigantic coefficients. So, with large values of C , i.e. little regularization, you still get large coefficients and so convergence may be slow, but the partially-converged model may still be quite good on the test set; whereas with large regularization you get much smaller coefficients, and worse performance on both the training and test sets. If you're worried about nonconvergence, you can try increasing n_iter (more), increasing tol , changing the solver , or scaling features (though with the tf-idf, I wouldn't think that'd help). I'd look for the largest C that gives you good results, then go about trying to get that to converge with more iterations and/or different solvers.
