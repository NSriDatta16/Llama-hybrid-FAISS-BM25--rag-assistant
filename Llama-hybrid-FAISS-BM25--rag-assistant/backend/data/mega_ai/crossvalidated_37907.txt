[site]: crossvalidated
[post_id]: 37907
[parent_id]: 37904
[tags]: 
Classification and regression trees do not have the same type of multicollinearity issues that you have in multiple linear regression. Splits are based on best-split criteria from which you have choices with the Gini index being the most commonly used one. In fact, I think it is beneficial to have highly correlated variables available for selection in building the model. This makes it possible to use good surrogate splits when certain variables used in the constructed tree are missing for a particular data point that you want to predict the outcome for in the case of regression or for the classification of a new case where some covariate is missing. Now Random Forest creates an ensemble of trees and if variables are highly correlated one may appear in one tree while a variable highly correlated with it may be absent in that particular tree but the situation might reverse for another tree. Since you are doing ensemble averaging and bootstrap bagging I think there is even less of an issue with highly correlated variables in Random Forest than there would be just using CART.
