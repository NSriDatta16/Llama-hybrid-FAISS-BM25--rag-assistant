[site]: crossvalidated
[post_id]: 343794
[parent_id]: 
[tags]: 
Usefulness of standard deviation/alternatives for highly variable measurements

EDIT: SOME ADDITIONS TO CLARIFY ORIGINAL TEXT If I remember correctly I heard some mention of standard deviation for precipitation means of sums is pretty useless due to the highly variable nature of preciptation quantities. Let's say that climatologists have calculated standard deviations for means of sums of monthly precipitation for every month of the year for 30 years of measurements. A monthly sum equals the total amount that has fallen during that month. So a monthly sum equals one measurement in this case. So if you take the average of the month of july over 30 years you have 30 measurements. If the standard deviation of these mean values are bigger than the mean values themselves it tells us there is a relatively high spread in the dataset. This is another way of saying that the coefficient of variation is big. But what would be considered big in this specific case? Are these sizes of the coefficient of variation normal for this type data? Lets assume here that all the coefficients of variations are above 100 %. Probably an irrelevant question in this forum. Now when the difference between the average values of two 30-year-periods are calculated, each period introduces its own standard deviation. And the resulting standard deviation for the difference would be even bigger than the largest standard deviation between each of the normal periods. I believe this is called error propagation (please correct me if this is the wrong english terminology). If the resulting standard deviation is bigger than the difference of the mean values, it means that the difference between the mean values may be very far away from the true value. In other words a pretty "non-accurate" mean in this case right, which for certain/quite many observations would yield fictitious differences of means? Precipitation can vary greatly in some regions of the world, for example due to large scale weather fluctuations like ENSO or other natural variaton. So perhaps 30 years is to low for averaging precipitation data due to high variability in some locations. The World Meteorological Organization recommends averaging over periods of thirty years. And this is common practice. Of course there are weaknesses by doing so and deviations from this practice exist. For instance some claim that thirty years is too low for certain climatic parameters due to their variable nature. This kind of answers part of my own question here. But if the precipitation data is only available for 30 years, are there any alternatives to standard deviation that would be recommended/considered more useful? I think I have heard some mention that precipitation data from different locations may have different have different distributions. However is standard deviation only useful/make sense for normal distributions? As a sidequestion: would the mean value be more accurate, with lower coefficient of variation if one has one million or billion years of measurements of data, even when each data point (spread) is highly variable? EDIT 2: SHORT VERSION If the data is not normally distributed what does coefficient of variation above 100 % tell us? What are the alternatives for detecting variation, if the alternatives are better/equally good (this is especially attractive to know about if the coefficient of variation is useless in my case)? Looking for answers which preferably are relevant to above example. Links to relevant studies are highly appreciated. Answers/research that provide intuitive examples/explanations are also highly appreciated. Of course answers to the other questions also are appreciated.
