[site]: datascience
[post_id]: 122995
[parent_id]: 
[tags]: 
Constructing an LSTM autoencoder for variable-lentgth sequences

I would like to construct an LSTM autoencoder model for sequence anomaly detection where the sequences can be varying in length. I understand based on this answer that padding and masking can be used to handle variable-length inputs, but how can I require that the output of the autoencoder is the same length as the input? I am using Keras.
