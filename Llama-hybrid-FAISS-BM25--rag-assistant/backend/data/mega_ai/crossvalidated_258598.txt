[site]: crossvalidated
[post_id]: 258598
[parent_id]: 258585
[tags]: 
Just generate a dataset $y$ is always drawn from $N(0;100)$ $x$ is drawn from $N(-1;1)$ or $N(1;1)$ Two clusters, but k-means cannot find them because of it's sensitivity to scale. Because k-means is least-squares, it is particularly sensitive here. But in fact, no distance-based method will find this either, unless you scale the data, or use weights (e.g. a linear SVM can learn appropriate weights, and Mahalanobis distance computes weights based on covariance). The underlying assumption of k-means that breaks with scaling is that a difference of $x$ in attribute 1 is exactly as much as a difference of $x$ in attribute 2 - and that a difference of $2x$ is actually $4$ times as severe (because it's a least squares approach!) If you scale your data differently, this can change substantially.
