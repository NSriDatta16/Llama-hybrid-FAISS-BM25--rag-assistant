[site]: crossvalidated
[post_id]: 386386
[parent_id]: 386358
[tags]: 
I know nothing about "PCA mixtures EM algorithms", but I can answer how I solve this problem with general EM algorithms, such as mixture of multivariate Bernoulli models. Usually, the EM algo involves calculating terms of the form $P(z|\underline{x}, \underline{\theta})$ in which z is your latent variable, $\underline{x}$ are your features and $\underline{\theta}$ are your parameters which you're trying to optimise your log-likelihood wrt. This is also usually the most challenging stage of the algorithm, from a numerical standpoint. Assuming your latent variable is discrete: $P(z|\underline{x}, \underline{\theta})=\frac{P(\underline{x}|z, \underline{\theta})}{\sum_{k}P(\underline{x}| z=k, \underline{\theta})}$ Numerical instabilities occur because if you have many features, $P(\underline{x}|z, \underline{\theta})$ will always be small, so you are dividing a small number by the sum of many other small numbers. One way to mitigate this is to calculate $P^{*}=max_{k}\left[P(\underline{x}|z=k, \underline{\theta})\right]$ , and then write $P(z|\underline{x}, \underline{\theta})=\frac{P(\underline{x}|z, \underline{\theta})}{P^{*}}\left(\sum_{k}\frac{P(\underline{x}|z=k, \underline{\theta})}{P^{*}}\right)^{-1}$ These ratios are easier to calculate if you have a way of calculating $\log P(\underline{x}|z, \underline{\theta})$ which does not involve directly evaluating $P(\underline{x}|z, \underline{\theta})$ . For example, if your likelihood function stipulates that all features are independent on one and other, than the likelihood of any data point is a product over features, thus the log likelihood is a sum over logarithms and that will be easy to evaluate with no numerical instabilities. Likewise if your likelihood is some exponential family, taking the logarithm is the same as evaluating the exponent which should be nice and finite. To calculate $\frac{P(\underline{x}|z, \underline{\theta})}{P^{*}}$ , you can evaluate $e^{\log P(\underline{x}|z, \underline{\theta}) - \log P^{*}}$ instead If there is a big difference between $max_{k}\left[P(\underline{x}|z=k, \underline{\theta})\right]$ and $min_{k}\left[P(\underline{x}|z=k, \underline{\theta})\right]$ , and it can happen that the difference is more orders of magnitude than the difference between the largest and smallest float in python, in my experience, then for some j, $\frac{P(\underline{x}|z=j, \underline{\theta})}{P^{*}}$ will be zero, even when using the logarithm trick, which is fine, it's the algorithm telling you that there is effectively zero probability that cluster j (or latent variable value j) created this data point. Is this useful for a "PCA mixture EM algorithm" ?
