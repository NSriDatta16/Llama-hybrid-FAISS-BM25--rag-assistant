[site]: crossvalidated
[post_id]: 50644
[parent_id]: 
[tags]: 
Why is Bayesian error concave down as a function of prior P(w1)?

Given a two categorization problem, why Bayesian error is concave down as a function of prior $P(w_1)$? $w_1$, $w_2$ are two underlying concepts with prior $P(w_1)$ and $P(w_2)$. We observe the evidence $x$, which is related to $w_1$ and $w_2$ through conditional probability $p(x|w_1)$ and $p(x|w_2)$. Based on $x$, we will do action $a_1$ or $a_2$, and the cost according to each action is $c(a_i|w_j)$, i.e. $c(a_1|w_1)=0$ (guess correctly) and $c(a_2|w_1)=1$ (guess wrong). And finally Bayes error is the expectation of $\text{cost}(\text{action}|x)$.
