[site]: crossvalidated
[post_id]: 84270
[parent_id]: 
[tags]: 
Feature Selection Methodology

I have a question regarding methodology in Machine Learning. Assume we have a classification problem, the size of training set is 100 every instance in training set is represented by 200 features. In order to eliminate meaningless features we evaluate Information Gain / Mutual Information of every feature and eliminate those with low mutual information. Then in order to evaluate the model we use 10-fold cross validation. The question is after all these manipulation whether the calculated evaluation can be used as real evaluation, in other words, are we allowed to perform all these manipulation and get reasonable model with reasonable evaluation. Ideas : Initially the entire process looked for me reasonable, but then I though if I am allowed to eliminate features based on the Infogain of train set or better to do it on validation set, on the other hand, in any case the trained model has access only to train set, so if features has low infogain it will have a minor weight in the model. I would appreciate if you could share your thoughts.
