[site]: crossvalidated
[post_id]: 133427
[parent_id]: 133418
[tags]: 
I don't believe there is any difference. The goals of scaling are 1) to make sure that all features have similar magnitude, to avoid the svm loss function only caring about large features, and 2) to avoid computational problems with large feature magnitudes. All three scalings you suggested satisfy #1. Arguably [-1,1] does the best job of being right in the middle of the range of signed floats (for #2), but any differences between these three scalings will be trivial. I should mention that if your features have a broad distribution (e.g. a Gaussian distribution) rather than coming from a fixed interval, scaling to a fixed range may not be a good idea, since then the extreme values will be at -1 and 1 and most data points will be very close to zero. A better approach in that case is to scale the data such that the mean is zero and the variance is one, ensuring that the majority of the features will have values of order $10^0$. As whuber suggested below, if the situation is even worse and you have a non-Gaussian distribution with significant outliers, then you may want to normalize based on another measure of dispersion like IQR . As was brought up in the comments, changing the scaling of your features may also require changing the scale of other hyperparameters, such as the soft-margin cost.
