[site]: datascience
[post_id]: 118575
[parent_id]: 60222
[tags]: 
The answer is: average embeddings of the parsed tokens . Mind that the tokenizer may be customized/different from what you expect. Here you find a full example import spacy import numpy as np nlp = spacy.load("en_core_web_md") txt= 'ChatGPT could automatically compose comments submitted in regulatory processes. It could write letters to the editor for publication in local newspapers. It could comment on news articles, blog entries and social media posts millions of times every day." https://t.co/rXL8WgJ4hV' vec1 = nlp(txt).vector vec2 = np.array([t.vector for t in nlp(txt)]).mean(0) np.testing.assert_almost_equal(vec1,vec2)
