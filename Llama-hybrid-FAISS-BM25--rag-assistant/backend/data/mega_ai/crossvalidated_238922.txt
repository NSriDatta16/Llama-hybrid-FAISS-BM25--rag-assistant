[site]: crossvalidated
[post_id]: 238922
[parent_id]: 238846
[tags]: 
Simulation results will depend on how the parameter is sampled in the simulation. I don't think there is any dispute over whether the posterior probabilities will be calibrated (in the frequency sense) if the prior probabilities are, so I suspect a simulation will not convince anyone of anything new. Anyway, in the sequential sampling case mentioned in the question (third paragraph) can be simulated "as is" by drawing $\mu$ from the prior, drawing samples given this $\mu$ until $p(\mu>0\mid \textrm{samples})>0.95$ or some other termination criterion occurs (another termination criterion is needed since there is positive probability that the running posterior probability will never exceed $0.95$). Then, for every $p(\mu>0\mid \textrm{samples})>0.95$ claim, check whether the underlying sampled $\mu$-parameter is positive and count the number of true positives vs. false positives. So, for $i=1,2,\ldots$: Sample $\mu_i \sim N(\gamma, \tau^2)$ For $j=1,\ldots^\ast$: Sample $y_{i,j} \sim N(\mu_i, \sigma^2)$ Compute $p_{i,j} := P(\mu_i>0 \mid y_{i,1:j})$ If $p_{i,j}>0.95$ If $\mu_i>0$, increment true positive counter If $\mu_i\leq0$, increment false positive counter Break from the inner for loop $\ast$ some other breaking condition, such as $j\geq j_{\max}$ The ratio of true positives to all positives will be at least $0.95$, which demonstrates calibration of the $P(\mu>0 \mid D)>0.95$ claims. A slow-and-dirty Python implementation (bugs very possible + there is a potential stopping bias in that I debugged until I saw the expected calibration property holding). # (C) Juho Kokkala 2016 # MIT License import numpy as np np.random.seed(1) N = 10000 max_samples = 50 gamma = 0.1 tau = 2 sigma = 1 truehits = 0 falsehits = 0 p_positivemus = [] while truehits + falsehits 0.95: p_positivemus.append(p_positivemu) if mu>0: truehits += 1 else: falsehits +=1 if (truehits+falsehits)%1000 == 0: print(truehits / (truehits+falsehits)) print(truehits+falsehits) break print(truehits / (truehits+falsehits)) print(np.mean(p_positivemus)) I got $0.9807$ for the proportion of true positives to all claims. This is over $0.95$ as the posterior probability will not hit exactly $0.95$. For this reason the code tracks also the mean "claimed" posterior probability, for which I got $0.9804$. One could also change the prior parameters $\gamma,\tau$ for every $i$ to demonstrate a calibration "over all inferences" (if the priors are calibrated). On the other hand, one could perform the posterior updates starting from "wrong" prior hyperparameters (different than what are used in drawing the ground-truth parameter), in which case the calibration might not hold.
