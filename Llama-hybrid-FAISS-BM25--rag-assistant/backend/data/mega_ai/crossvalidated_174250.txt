[site]: crossvalidated
[post_id]: 174250
[parent_id]: 174246
[tags]: 
I think you should focus on dimensionality reduction. The following two facts give me that intution: It is not unusual that KMeans fails in such a high dimensional space, as the "distance between any two points in a given (high dimensional) dataset converges" . PCA can also fail if the principal axes of the classes are parallel to each other ("ADIDAS problem"). In this case Linear discriminant analysis (LDA) for dimensionality reduction could help. You could also give Local Linear Embedding/Laplacian Eigenmaps a try - these are dimensionality reduction techniques that want to preserve the structure of the high dimensional dataset in the lower dimensions. Unfortunately I haven't used scikit yet, so I don't know which techniques are implemented there...
