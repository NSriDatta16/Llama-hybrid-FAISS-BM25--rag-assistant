[site]: crossvalidated
[post_id]: 618969
[parent_id]: 
[tags]: 
Random Forest Variable Importance as a way to weight Nearest Neighbor Variables

I'm employing a nearest neighbor algorithm to find a real NFL game that is the most similar to my projected stats. Not all statistics have the same predictive-importance when projecting the outcome of an NFL game (e.g. total passing yards will predict the final score more than, say, number of penalties), so I want to weight the variables in my KNN by how well they predict the team's final score. My thought was to use all of the variables in my KNN to run a random forest to predict final score. From there I could use the Importance values for each variable as weights in the KNN model. I chose a random forest instead of a linear regression because form my understanding I don't need to normalize Node Purity from a random forest whereas I would with a linear regression. Where I'm stuck with is how exactly to use those weights. Can I just multiply the raw importance values after normalizing the variables in the KNN? Example code of my process with made up data: library(randomForest) library(dplyr) library(class) # Make dummy dataset of real game statistics set.seed(123) passingYds % select(-gameID, -finalScore) %>% normalize(., gameStats) testNFLknn % normalize(., gameStats) ### WEIGHT THE NORMALIZED DATA BY VARIABLE IMPORTANCE ### trainNFLknn_weighted % filter(gameID == predClass) gameStats %>% filter(gameID == predClass_weighted) My projected data of passingYds = 320 , rushingYds=95 , and penalties=2 results in an unweighted KNN selection of gameID=4 , whereas the weighted prediction is gameID=2 . gameID=2 is a much more reasonable selection than gameID=4 based on the eye-test which leads me to believe that I'm on the right track. But I wanted to confirm that what I'm doing is "best practice", could anyone give me some feedback?
