[site]: crossvalidated
[post_id]: 570623
[parent_id]: 534449
[tags]: 
See section 3 of the paper Emulating computer models with step-discontinuous outputs using Gaussian processes or section 15.4.5 of "Machine Learning: a probabilistic perspective" by Kevin Murphy. Write your one-layer neural network of length $L$ and activation function $\sigma$ as $$f(x) = b + \sum_{j=1}^L v_j \sigma(w_j x + b_j)$$ where the weights $w_j$ and biases $b_j$ are i.i.d. Gaussians with appropriate variances; similarly $v_j$ are i.i.d. Gaussian and $b$ is a mean-zero Gaussian. Denote all the model parameters as $\theta$ , then $$\mathbb{E}_\theta[f(x)] = 0 \quad \text{ and} \quad \text{Cov}(f(x), f(x^\prime)) = \sigma_b^2 + L \sigma_v^2 \mathbb{E}_{w, b}[\sigma(w x + b) \sigma(w x^\prime + b)].$$ Take $\sigma_v^2 = \omega^2/L$ for a constant $\omega^2$ , then by the central limit theorem, since $f(x)$ is the sum of appropriately scaled i.i.d. r.v.s we have that $f(x_1), \ldots, f(x_n)$ is asymptotically (in L) jointly Gaussian with mean zero and covariance function $$k(x, x^\prime) = \omega^2 \mathbb{E}_{w, b}[\sigma(w x + b) \sigma(w x^\prime + b)]$$ for any finite collection of points $x_1, \ldots, x_n$ . Now intuitively, this means that $f(\cdot)$ is a Gaussian process, but you need some limiting argument to be rigorous, since you're considering convergence as $L \rightarrow \infty$ for all finite-dimensional marginals.
