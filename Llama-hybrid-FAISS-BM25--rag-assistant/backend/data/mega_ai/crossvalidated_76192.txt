[site]: crossvalidated
[post_id]: 76192
[parent_id]: 63359
[tags]: 
Often when you're dealing with categorical data, it's most useful to convert the elements of your dataset to use a "one-hot" code for your inputs. In the poker example that you gave, the cards in a deck certainly fall into this category of input, since the face value of the card might not map linearly onto the card's value for your hand. Actually, the cards example is a very interesting one, because (at least from my naive perspective) there are many different features of each card that might be relevant to predicting a card's value. A naive one-hot encoding would use a vector of length 52 (assuming no jokers) for each card in the deck. The ace of spades might be represented as [1, 0, 0, ...] (for a total of 51 zeros and 1 one), the two of spades might be represented as [0, 1, 0, ...], and so forth. However, each card has a suit that is orthogonal to its value, so you might be able to factor the encoding for each card into two one-hot vectors, concatenated together. The suit could be a vector of length 4, and the value of the card could be a vector of length 13. Then the ace of spades might be represented as [1, 0, 0, 0] + [1, 0, 0, ...] (for a total of 15 zeros and 2 ones), the two of spades might be [1, 0, 0, 0] + [0, 1, 0, ...], and so on. If you want to use such an encoding as an input for your neural network, then you'd need many different input units. For example, if you wanted to train your neural network to map two cards onto some output value, then your network would need either 34 inputs (if you use the factored encoding) or 104 inputs (if you used a naive one-hot encoding). Certainly a disadvantage of such an input encoding is that your training data will be larger, but the encoding itself might identify valuable information about the inputs that might otherwise be difficult for your network to locate.
