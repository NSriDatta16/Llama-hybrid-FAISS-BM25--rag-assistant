[site]: crossvalidated
[post_id]: 489447
[parent_id]: 489381
[tags]: 
The objective in logistic regression is to maximize likelihood of data which is assumed to be Bernaulli-distributed. $$L(\theta)=\prod_i p_i^{y_i} (1-p_i)^{1-y_i}$$ where $p_i$ is given by the logistic function $g(z) = \frac 1 {1+e^{-z}}$ , $z=\theta^Tx$ . Taking negative log of this quantity gives the loss function as you have mentioned. Class-weighted logistic regression assigns $w_+$ weights to positive samples and $w_-$ weights to negative samples. But let us assume the general case where all samples have a weight $w_i$ . In terms of likelihood this means each sample is now given a probability of occurence (as opposed to other samples) as $w_i$ . (If $\sum_i w_i \neq 1$ then $w_i\leftarrow\frac{w_i}{\sum_i w_i}$ ). The likelihood of each sample is exponentiated by this probability. So the likelihodd of all samples together becomes: $$L(\theta)=\prod_i (p_i^{y_i} (1-p_i)^{1-y_i})^{w_i}$$ You can see how taking the negative log of this would give us the loss function for weighted logistic regression: $$J(\theta) = -\sum_i w_i [y_i \ln(p_i) + (1-y_i)\ln(1-p_i)]$$ where $p_i$ is the same as unweighted scenario. Class weighted logistic regression basically says that $w_i$ is $w_+$ if $i^{th}$ sample is positive else $w_-$ . It is trivial to see that this will indeed lead to the class-weighted logistic regression loss function as you mention.
