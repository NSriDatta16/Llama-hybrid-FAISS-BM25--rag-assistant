[site]: crossvalidated
[post_id]: 568491
[parent_id]: 568485
[tags]: 
Probably because the bias defined by your code is not a very good criterion. For example, if the differences are 0.1, 0.1, -0.1, -0.05, 0 , then according to your definition, the bias would be $(0.1+0.1-0.1-0.05 + 0)/5=0.01$ . In another case, 0.5, 0.5, 0.5, -0.75, -0.75 would give zero bias, even though the absolute values of differences are larger. This very property of the bias defined by your code does not match our intuition for a good criterion. Instead, the mean squared error (MSE) is used more often. Also, even if you replace the bias with MSE, model2 can still appear to be better by pure chance. To mitigate such risk, you can repeat the simulation under the same setting but using different random seeds for, say, 10000 times and look at the average MSE.
