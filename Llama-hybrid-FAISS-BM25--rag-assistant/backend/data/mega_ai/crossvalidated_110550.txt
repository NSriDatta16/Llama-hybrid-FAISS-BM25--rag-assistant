[site]: crossvalidated
[post_id]: 110550
[parent_id]: 71700
[tags]: 
ROC Analysis was designed for dealing with only two variables: noise and no noise, so using it for 3 or more variables makes little sense. However, you for any multi-classification problem it's possible to use a bunch of binary classifiers and do so-called One-Vs-All Classification E.g. consider the IRIS data set: there are 3 classes: setosa, versicolor, and virginica. So we can build 3 classifiers (e.g. Naive Bayes): for setosa, for vesicolor and for virginica. And then draw a ROC curve for each and tune the threshold for each model separately. AUC in such a case could be just the average across AUCs for individual models. Here's a ROC curve for the IRIS data set: AUC in this case is $\approx 0.98 = \frac{1 + 0.98 + 0.97}{3}$ R Code: library(ROCR) library(klaR) data(iris) lvls = levels(iris$Species) testidx = which(1:length(iris[, 1]) %% 5 == 0) iris.train = iris[testidx, ] iris.test = iris[-testidx, ] aucs = c() plot(x=NA, y=NA, xlim=c(0,1), ylim=c(0,1), ylab='True Positive Rate', xlab='False Positive Rate', bty='n') for (type.id in 1:3) { type = as.factor(iris.train$Species == lvls[type.id]) nbmodel = NaiveBayes(type ~ ., data=iris.train[, -5]) nbprediction = predict(nbmodel, iris.test[,-5], type='raw') score = nbprediction$posterior[, 'TRUE'] actual.class = iris.test$Species == lvls[type.id] pred = prediction(score, actual.class) nbperf = performance(pred, "tpr", "fpr") roc.x = unlist(nbperf@x.values) roc.y = unlist(nbperf@y.values) lines(roc.y ~ roc.x, col=type.id+1, lwd=2) nbauc = performance(pred, "auc") nbauc = unlist(slot(nbauc, "y.values")) aucs[type.id] = nbauc } lines(x=c(0,1), c(0,1)) mean(aucs) Source of inspiration: http://karchinlab.org/fcbb2_spr14/Lectures/Machine_Learning_R.pdf
