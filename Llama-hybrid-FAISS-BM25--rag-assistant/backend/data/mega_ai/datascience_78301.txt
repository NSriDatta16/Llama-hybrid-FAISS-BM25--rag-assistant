[site]: datascience
[post_id]: 78301
[parent_id]: 78298
[tags]: 
Global Explanation: The overall importance of a feature in a decision tree(and also applied to random forest and GBDT) can be computed in the following way: ‘weight’: the number of times a feature is used to split the data across all trees. ‘gain’: the average gain across all splits the feature is used in. ‘cover’: the average coverage across all splits the feature is used in. ‘total_gain’: the total gain across all splits the feature is used in. ‘total_cover’: the total coverage across all splits the feature is used in. This is extracted from the xgboost API . Local Explanations If you want to get individual examples of why a prediction was made you can use either Shapley Values Lime There are plenty of ways to achieve a better model explainability and accountability. I recommend you this book.
