[site]: crossvalidated
[post_id]: 82273
[parent_id]: 54895
[tags]: 
This is a surprisingly frustrating thing to pin down. Shannon looked at this in one of the earliest infomation theory papers ( Shannon, 1951 ) and estimated the entropy of printed text at around 1 bit/character, using a neat 'guessing game' paradigm. In the same paper, he estimates the entropy of a word at around 12 bits. Shannon, however, used a relatively small data set[*] and it turns out that the entropy depends on many factors. @Lmorin mentioned time above, but other relevant factors include the topic (children's books have a limited vocabulary, for example), modality, context, author's style, and so on! The general term for $P(\textrm{word})$ is a language model and computational linguists/natural language processing researchers spend a lot of time building them because they're very useful [**]. The models contain the per-character or per-word probability. A language model also often contains information about transitions between words. A trigram (or 3rd-order model) looks like $P(\textrm{Word}_n | \textrm{Word}_{n-1}, \textrm{Word}_{n-2}$). However, the probabilites usually aren't taken directly from the data---it's exceedingly sparse---so there are various smoothing/interpolation/back-off methods designed to produce reasonable probability distributions. Any decent NLP textbook should have a chapter on language modelling. You might start with Chapter 6 of Manning and Schutze's "dice book" or Chapter 4 of Jurafsky and Martin . However, language models are so useful that they'll also show up in contexts as diverse as speech recognition, information retrieval, and even bioinformatics. This slide deck might be a good place to start if you want to read more. There's also a fair bit of literature about human language models. Noam Chompsky famously ranted about how "the notion of 'probability of a sentence' is an entirely useless one, under any known interpretation of this term.” but a lot of people have subsequently disagreed. If you're interested in this, you may want to look for papers on 'statistical learning' (not machine learning; psychologists use the term a bit differently). [*] It was the 50s and he was presumably doing most of this manually, so…fair enough! [**] In particular, it can help resolve ambguities. Suppose you can't tell if a blob is actually a 'T' or an 'I' by itself. If one alternative produces a common word and one doesn't (Iherefore vs Therefore), it's pretty clear which one you should pick.
