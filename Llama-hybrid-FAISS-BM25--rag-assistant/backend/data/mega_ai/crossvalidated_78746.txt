[site]: crossvalidated
[post_id]: 78746
[parent_id]: 78424
[tags]: 
I'm not very knowledgeable on convolutional networks but when you apply back propagation to regular neural networks you differentiate the error value of the network with respect to each connections weight. Where the error value is the difference between the actual network output and the desired output. In order to back propagate this error value, the activation functions also have to be differentiated; but this is still with respect to connection weights. So I would be expecting to see your functions f being differentiated with respect to the connection weight values, and not with respect to a convolution kernel k. Though my lack of knowledge of convolutional networks could be hiding some subtlety of the question.
