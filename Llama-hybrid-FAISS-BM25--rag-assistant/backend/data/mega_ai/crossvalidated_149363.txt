[site]: crossvalidated
[post_id]: 149363
[parent_id]: 147260
[tags]: 
Over there at stackoverflow I've found this question . There, this other question and its answer points out there are different methods in kernlab to compute the decision when class probabilites are included. I'll requote it: The kernlab help pages (?predict.ksvm) link to paper Probability estimates for Multi-class Classification by Pairwise Coupling by T.F. Wu, C.J. Lin, and R.C. Weng. In section 7.3 it is said that the decisions and probabilities can differ: ...We explain why the results by probability-based and decision-value-based methods can be so distinct. For some problems, the parameters selected by δDV are quite different from those by the other five rules. In waveform, at some parameters all probability-based methods gives much higher cross validation accuracy than δDV . We observe, for example, the decision values of validation sets are in [0.73, 0.97] and [0.93, 1.02] for data in two classes; hence, all data in the validation sets are classified as in one class and the error is high. On the contrary, the probability-based methods fit the decision values by a sigmoid function, which can better separate the two classes by cutting at a decision value around 0.95. This observation shed some light on the difference between probability-based and decision-value based methods... EDIT: Max Kuhn himself addressed this issue here .
