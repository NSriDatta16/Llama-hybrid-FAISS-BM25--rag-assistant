[site]: crossvalidated
[post_id]: 413011
[parent_id]: 412980
[tags]: 
I'm an undergraduate in Statistics, so I won't call myself an expert, but here are my two cents. Models don't explain themselves; humans interpret them. Linear models are easier to understand than neural networks and random forests because they are closer to how we make decisions. Indeed, ANNs imitate the human brain, but you don't decide which restaurant to go tomorrow by doing a series of matrix multiplications. Instead, you weight some factors in your mind by their importance, which is essentially a linear combination. "Explanatory power" measures how well a model gets along with humans' intuition, whereas "predictive power" measures how well it aligns with the underlying mechanism of the process in interest. The contradiction between them is essentially the gap between what the world is and how we can perceive/understand it. I hope this explains why "some models do a good job of explaining, even though they do a poor job at predicting". Ian Stewart once said, "If our brains were simple enough for us to understand them, we'd be so simple that we couldn't." Unfortunately, our little human brains are actually very simple compared to the universe, or even a stock market (which involves a lot of brains :). Up to now, all models are products of human brains, so it must be more-or-less inaccurate, which leads to Box's "All models are wrong". On the other hand, a model doesn't have to be technically correct to be useful. For example, Newton's laws of motion has been disproved by Einstein, but it remains useful when an object is not ridiculously big or fast. To address your question, I honestly can't see the incompatibility between Box and Shmueli's points. It seems that you consider "explanatory power" and "predictive power" to be binomial properties, but I think they sit at the two ends of a spectrum.
