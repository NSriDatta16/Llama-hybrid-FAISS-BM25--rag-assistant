[site]: crossvalidated
[post_id]: 79559
[parent_id]: 79547
[tags]: 
MCMC is a strategy for generating samples $x(i)$ while exploring the state space $X $using a Markov chain mechanism. These are irreducible and aperiodic Markov chains that have $P_{target}(\theta)$ as the invariant distribution. This mechanism is constructed so that the chain spends more time in the most important regions. In particular, it is constructed so that the samples $x(i)$ mimic samples drawn from the target distribution $P_{target}(\theta)$. The answer to your question is: MCMC is used when we cannot draw samples from $P_{target}(\theta)$ directly, but can evaluate $P_{target}(\theta)$ up to a constant of proportionality . To clarify this, let us denote $P_{target}(\theta) = P(\theta | D)$ where $D$ is the data and $P(\theta | D)$ is our posterior target distribution. Normally, calculating the exact $P(\theta | D)$ requires: $ P(\theta | D) = \frac{P(D|\theta) * P(\theta)}{P(D)} $ As you can see, our target distribution: $ P(\theta | D) \propto P(D|\theta) * P(\theta)$ up to a constant of proportionality. We use this product (of the likelihood and the prior) as the target distribution in a Metropolis algorithm. The acceptance criterion of the algorithm only needs the relative posterior probabilities in the target distribution and not the absolute posterior probabilities , so we could use an unnormalised prior or unnormalised posterior when generating sample values of $\theta$. Section 2 of this paper gives examples to situations when sampling from the posterior $P_{target}(\theta)$ is tricky. 4 scenarios in short: 1) Bayesian inference and learning (see my comment to another answer on the page), 2) Statistical mechanics, 3) Optimisation, 4) Penalised likelihood model selection.
