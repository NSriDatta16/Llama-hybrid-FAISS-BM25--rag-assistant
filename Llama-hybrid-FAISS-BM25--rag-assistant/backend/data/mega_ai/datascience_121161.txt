[site]: datascience
[post_id]: 121161
[parent_id]: 60396
[tags]: 
I am not entirely sure but this is what I understand. The softmax layer is only used at training phase, to learn all the model parameters, especially the final ReLu layer and video embeddings. At inference time we do not care about the softmax output really. The final ReLu layer has a size of 256 as you can see at Table 1 in the paper. They say that the final ReLu layer can be seen as a user vector. I do not know why, but since the learnt ReLu final layer has the same size as the video embeddings, they are mapped in the same space to compute the approximate nearest neighbours for the user vector. So they try to find the most similar video embeddings to the user vector.
