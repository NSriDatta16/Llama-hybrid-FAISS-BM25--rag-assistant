[site]: crossvalidated
[post_id]: 50204
[parent_id]: 50198
[tags]: 
Akaike's information criterion and (log)-likelihood You are right that the (log-)likelihood and Akaike's information criterion can be used to discriminate between different models. However, if the two models have the same number of parameters the approaches are equivalent. Akaike's information criterion is defined by (see link) $$ \mathit{AIC} = 2k - 2\ln(L), $$ where $k$ is the number of parameters of the model and $L$ is the likelihood function. In your case, the number of parameters is $k=2$ for both models such that it is sufficient to consider the likelihood function. Fitting curves There are a number of methods to fit curves to data, e.g. least-square fitting or maximum likelihood fitting . Different methods are used in different situations: least-square fitting is appropriate if you want to determine the relationship between two variables. Maximum likelihood fitting is appropriate if you want to determine parameters of a probability distribution. Because the original problem mentions "I have a problem with fitting my data set with a Gauss (Normal) distribution" I will take a maximum-likelihood approach in the following. Fitting the parameters (assuming a probability distribution) The likelihood function is defined as the probability to observe a given sample (your data values) given a set of parameters ($sigma$ and $mu$). Consider a single data point with value $x$. Assuming that your model is Gaussian, the probability to observe this point is $$p(x|\sigma,\mu)=\frac{1}{\sqrt{2\pi}\sigma} \exp\left(-\frac{1}{2}\frac{(x-\mu)^2}{\sigma^2}\right).$$ Assuming that your samples are independent and are members of a vector $\bf{x}$, the probability to observe them is $$ \begin{align} p({\bf x}|\sigma,\mu)&=\prod_{i=1}^Np(x_i|\sigma,\mu)\\ &=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^N(x_i-\mu)^2\right). \end{align} $$ We can maximise this probability (the likelihood) by differentiating with respect to $\sigma$ and $\mu$. You can look up the details here . We thus find $$\hat{\mu} = \overline{x} \equiv \frac{1}{n}\sum_{i=1}^n x_i, \qquad \hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (x_i - \overline{x})^2.$$ I have used the following python code to fit the parameters (assuming that the second value of each tuple is a count). import numpy as np data = np.array([[90.00, 2.0], [97.40, 5.0], [104.8, 14.0], [112.2, 12.0], [119.6, 11.0], [127.0, 6.0], [134.4, 3.0], [141.8, 1.0], [149.2, 2.0], [156.6, 1.0]]) #Determine the total number of points number_of_points = np.sum(data[:,1]) #Determine the mean using a weighted average mu = np.sum(map(lambda (value, count): value * count, data)) / number_of_points #Determine the standard deviataion sigma = np.sqrt(np.sum(map(lambda (value, count): (value - mu)**2 * count, data)) / number_of_points) print "\\mu = {0}, \\sigma = {1}".format(mu, sigma) The results are $\mu = 115.056140351, \sigma = 14.1192790625$. Note that we do not need to determine $m$ because it is fixed by the condition that the probability distribution needs to be properly normalised. Comparing results The following code evaluates the log-likelihood of the different parameter values def ll(data, mu, sigma): #Determine the total number of points number_of_points = np.sum(data[:,1]) #Determine the variance var = np.sum(map(lambda (value, count): (value- mu)**2 * count, data)) / number_of_points return - number_of_points*(.5 * np.log(2*np.pi)\ + np.log(sigma) + var / (2 * sigma ** 2)) print "First result" print ll(data, 111.86913960269014, 11.968861052746961) print "Second result" print ll(data, 111.86914614035226, 11.96881745593664) print "My result" print ll(data, mu, sigma) The results are First result -235.552835278 Second result -235.552923018 My result -231.789343246 Thus, the result obtained by Volker is marginally better than your SAGE result. However, neither of them maximises the log-likelihood. Note that all of the above assumes that we are fitting a probability distribution.
