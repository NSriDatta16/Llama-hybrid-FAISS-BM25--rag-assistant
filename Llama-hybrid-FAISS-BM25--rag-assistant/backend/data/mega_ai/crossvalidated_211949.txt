[site]: crossvalidated
[post_id]: 211949
[parent_id]: 211419
[tags]: 
Precision is one of the two natural parameters of the normal distribution. That means that if you want to combine two independent predictive distributions (as in a Generalized Linear Model), you add the precisions. Variance does not have this property. On the other hand, when you're accumulating observations, you average expectation parameters. The second moment is an expectation parameter. When taking the convolution of two independent normal distributions, the variances add. Relatedly, if you have a Wiener process (a stochastic process whose increments are Gaussian) you can argue using infinite divisibility that waiting half the time, means jumping with half the variance . Finally, when scaling a Gaussian distribution, the standard deviation is scaled. So, many parameterizations are useful depending on what you're doing. If you're combining predictions in a GLM, precision is the most “intuitive” one.
