[site]: datascience
[post_id]: 53605
[parent_id]: 
[tags]: 
How can I tune my neural network to solve a group of equations?

I am trying to use neural networks to solve the following group of equations: $\sqrt{(x_1 - x)^2 + (y_1 - y)^2}$ - $\sqrt{(x_2 - x)^2 + (y_2 - y)^2}$ = $d_{12}$ $\sqrt{(x_1 - x)^2 + (y_1 - y)^2}$ - $\sqrt{(x_3 - x)^2 + (y_3 - y)^2}$ = $d_{13}$ Where x and y are unknown variables and x1, y1, x2, y2, x3, y3, d{12} and d{13} are all known. As I am only interested in a certain range of all the x stuff and y stuff and the x and y do not need to be exactly accurate, I think I can solve the equations by a neural network which is believed to be a function approximator. My idea is randomly generating 50000 data points. Each data point includes whole randomly generated x1, y1, x2, y2, x3, y3, x and y, then I can use the equations above to have d{12} and d{13}. I feed x1, y1, x2, y2, x3, y3, d{12} and d_{13} into a fully connected neural network as input, and train the nets to output x and y. The architecture of my current networks: I have 4 hidden layers The numbers of neurons in each layer are: 128, 256, 512, 1024, 2048, 4096, 4096, 2048, 1024, 512, 256, 128. The activation functions are all Relu, the output layer doesn't have activation function since it is a regression problem. In each layer, I did batch normalization. The batch size is 256, and I train the network for 200 epochs. Unfortunately, this network doesn't perform well, the error for the output is huge. I suspect maybe the network stucks in some local minima or the function that the neural network wants to mimic is far more complicated. I am wondering if anybody has ideas about why it is that and how to tune it.
