[site]: datascience
[post_id]: 56700
[parent_id]: 
[tags]: 
Is it normal that a classifier always wrongly predicts the same samples?

I'm trying to improve the accuracy of a classifier, a random forest one. I built different models with the same hyperparameters but with different random seeds, trained them with the same training data, used the same test daata to make the predictions and compared the results. I discovered that 50% of the errors were always made on the same samples. Therefore, do these samples which are always wrongly predicted deserve a particular attention or is it kind of logic ? I hope the question is clear enough.
