[site]: crossvalidated
[post_id]: 602227
[parent_id]: 602223
[tags]: 
The answer depends upon the type of Reinforcement Learning algorithm you'd want to use. In case you use an online RL algorithm like DQN, you'd want a simulator that presents you with new data from the environment ( state, action, reward, new state ) during every time step. You can very easily adapt gym to your simulator. The advantage of doing that, is that it allows you to reference various RL algorithms and libraries, since all of them use the gym API to interact with the environment. If you are working with offline RL, you'd need a complete dataset ,in advance, similar to the ones used in supervised learning. Finally, I doubt your agent will learn anything if your dataset is purely random. Although that once again depends on your problem complexity and the type of RL algorithm, you implement.
