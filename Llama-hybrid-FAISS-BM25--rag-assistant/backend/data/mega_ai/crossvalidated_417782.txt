[site]: crossvalidated
[post_id]: 417782
[parent_id]: 412460
[tags]: 
One possible explanation is that in the scores corresponding to your OLS estimates there are also some negative autocorrelations. These would cause the standard errors to be smaller than under independence. Consider the following very basic artificial example in R using the sandwich package for Newey-West standard errors: Under independence the conventional standard error and the Newey-West standard error are very close. set.seed(1) y0 However, if we create a time series with positive autocorrelation (here AR1 with 0.8) the conventional standard error is too small and the Newey-West standard error is much larger. ypos Conversely, if we create a series with negative autocorrelation (here AR1 with -0.8), then the conventional standard error is too large and the Newey-West standard error much smaller. yneg In a more complex regression model the situation may not be that simple and the effects might also not be the same for all regression coefficients. However, it is not completely unusual to get Newey-West standard errors that are smaller than the conventional standard errors.
