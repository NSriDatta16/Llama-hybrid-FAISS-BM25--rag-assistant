[site]: datascience
[post_id]: 9271
[parent_id]: 9262
[tags]: 
First of all, sklearn.metrics.mutual_info_score implements mutual information for evaluating clustering results, not pure Kullback-Leibler divergence! This is equal to the Kullback-Leibler divergence of the joint distribution with the product distribution of the marginals. KL divergence (and any other such measure) expects the input data to have a sum of 1 . Otherwise, they are not proper probability distributions . If your data does not have a sum of 1, most likely it is usually not proper to use KL divergence! (In some cases, it may be admissible to have a sum of less than 1, e.g. in the case of missing data.) Also note that it is common to use base 2 logarithms. This only yields a constant scaling factor in difference, but base 2 logarithms are easier to interpret and have a more intuitive scale (0 to 1 instead of 0 to log2=0.69314..., measuring the information in bits instead of nats). > sklearn.metrics.mutual_info_score([0,1],[1,0]) 0.69314718055994529 as we can clearly see, the MI result of sklearn is scaled using natural logarithms instead of log2. This is an unfortunate choice, as explained above. Kullback-Leibler divergence is fragile, unfortunately. On above example it is not well-defined: KL([0,1],[1,0]) causes a division by zero, and tends to infinity. It is also asymmetric .
