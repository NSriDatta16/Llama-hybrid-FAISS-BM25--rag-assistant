[site]: datascience
[post_id]: 18546
[parent_id]: 
[tags]: 
information leakage when using empirical Bayesian to generate a predictor

Consider the following problem: I want to predict the next bat of a set of baseball player. I have a training data set, where it contains the historical bat records (0-1 encoded, which is our target variable) of the set of baseball player, player's id, their name, salary, etc, as predictors. and I have an independent test data set. I am using the log loss function as the assessment of the model. I am not sure whether the problem I describe below is specific to log loss function. Note that one can just use All the training data set with just the player id and their bat records and use empirical Bayesian method to compute the bat rate of each player, e.g., form a prior such as binomial and use the observed bat records to compute the posterior probability of each player. Then use this probability to make prediction on the test data set. This method is reasonable especially if you don't have other predictors like player's age, salary, etc. First, now with other predictors, one can run some sophisticated machine learning models. Here for feature engineering, I have the following question. I could also compute the empirical Bayesian probability I just described for each player and incorporate this probability as one of the predictors. I've been told that this will lead to information leakage , since this predictor is a function of the target variable that I want to predict. Hence, this will lead to overfitting and make the generalizability of the model poor. I vaguely understand that but still hope someone could explain this idea clearly . My puzzle is, if just using empirical Bayesian probability alone (not using any machine learning model) to make predictions is a reasonable idea, then why put this variable in a machine learning model would cause the information leakage problem? In fact, I did just put this empirical Bayesian probability into my set of predictors and the model performs much worse on the test data set than I kick this predictor out of my test predictors. I checked the training error rate, which is incredibly well, which suggests strong overfitting issue. I want to understand why the overfitting occurs here, what is the underlying mechanism that drives this overfitting. Second, I found a method online to deal with this problem which does not explain why but it works pretty well. The idea is the following: randomly divide the training data set into 5 folds (like cross validation) and label them as folder 1,2,3,4,5, then for each iteration (totally 5 iterations), we do the following: for iteration 1, we use the fold 2,3,4,5 to compute the empirical Bayesian probability of bat for each player, then map the results to the players in folder 1 . For example, player Kevin, his bat probability in All training data set is 0.23, but using the fold 2,3,4,5, his bat probability is 0.34. Then this 0.34 is assigned to fold 1 whenever Kevin appears (assume we only have 1 Kevin). If some player only appears in fold 1 but not in the rest folds, then just assign 0.5 to this guy. In iteration 2, we use the fold 1,3,4,5 to compute the empirical Bayesian probability of bat for each player, then map the results to the players in folder 2. We do this for all iterations. Eventually, after this procedure, we will generate a new column with the bat probability for all players in the training data set using this "strange procedure". I checked it with my model, it improves the performance on the test data set a lot . And I also run this procedure for 3 fold and 10 fold, they are all pretty good comparing to (1) not include the empirical Bayesian probability as a predictor or (2) compute the empirical Bayesian probability using ALL training data set and put it into the set of predictors. But 3 and 10 folds are not as good as 5 fold. Here my question is why this "strange procedure" of computing the empirical Bayesian probability works? It seems that it overcomes the overfitting issue on the training data set by only using part of the training data set to compute the probability . Note that if you just want to estimate the bat probability of each player, then using part of the data is definitely worse than using the whole data, isn't it? So according to my experiment, it seems that a "worse" (or less precise) estimation of the bat probability when putting as a predictor to predict the bat probability is better than using a more precise bat probability. This seems very counter intuitive. Third, note that many machine learning models have hyper parameters that needs to be set before running the model, e.g., the lambda in logistic regression. Now let's say I want to tune the lambda in logistic regression using cross validation. I also want to include the empirical Bayesian probability as a predictors. How should I conduct the cross validation procedure? I seems that we cannot just randomly divide the data into K fold and do it. For simplicity, let's say we just set a small proportion of the training data as the validation data set: then I believe the correct procedure is: split the training data set first, then construct the empirical Bayesian probability of bat (this procedure should not be reversed) . However, my question is: after I first split the data, then in the non-validation data set, how should I construct that empirical Bayesian probability bat column? apparently according to the second point of this post, we cannot just go head and compute it. We should also run that "strange procedure" as described in the second point on the non-validation data set? I guess I am still questioning why the "strange procedure" in the second point works
