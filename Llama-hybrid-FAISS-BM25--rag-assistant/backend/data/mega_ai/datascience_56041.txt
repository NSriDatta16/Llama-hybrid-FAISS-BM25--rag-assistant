[site]: datascience
[post_id]: 56041
[parent_id]: 
[tags]: 
Gradient boosting, where did the constant go?

In the very early papers on gradient boosting, the ensemble would include a constant and a sum of base learners i.e. $F(X) = a_0 + \sum\limits_{i} a_i f_i(X)$ The constant is fitted first (i.e. if the loss is squared error, the $a_0$ will be the unconditional mean of the target) Somewhere along the way this got dropped, i.e. as far as I can see a number of the current "state of art" gbdt algorithms fit a constant (xgboost, catboost etc). I believe the R mboost package does though. Why is this, does anyone have any reference where it is discussed (I'm performing regression so particularly interested in this aspect). In particular with a regression tree as a base-learner, can it learn the constant more efficiently than the original algorithms? One of my motivations is that when I boost with no constant, for a problem where the target is always positive, it seems to converge from below so it almost always has a bias in the residuals unless you have enough data such that you can boost for long enough to fit the highest values of the targets without overfitting (specifically I fit the relationship between temperature and electricity demand). Edit: Some references [Prokhorenkova et. al.] "CatBoost: unbiased boosting with categorical features" - Algorithm 1 intialises M to 0. I've used catboost a lot, I can confirm that explicitly setting the starting value to the mean produces a differerent result. [Chen et. al.] "XGBoost: A Scalable Tree Boosting System" - Equation 1 excludes the constant. [Sigris] "Gradient and Newton Boosting for Classification and Regression" - there's no mention of the constant alough it's not explicitly excluded
