[site]: datascience
[post_id]: 35778
[parent_id]: 35741
[tags]: 
I don't like the reduce_sum version of the kl-loss because it depends on the size of your latent vector. My advise is to use the mean instead. Moreover it is a notorious fact that training a VAE with the kl loss is difficult. You may need to progressively increase the contribution of the kl loss in your total loss. Add a weight w_kl that will control the contribution : Loss = recons_loss + w_kl * kl_loss You start with w_kl=0 and progressively increase it every epoch (or batch) to 1. This is a classic trick. Your learning rate seems good, maybe you can try a little higher (4e-4). If you don't like the tricks, the Wasserstein auto-encoder may be your friend.
