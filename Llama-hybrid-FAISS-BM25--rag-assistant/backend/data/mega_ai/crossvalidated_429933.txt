[site]: crossvalidated
[post_id]: 429933
[parent_id]: 
[tags]: 
Does a CNN fully memorize ground truth if it has more parameters than training pixels?

ResNet consists of 25M trainable parameters. If only 30% of 600 $512 \times 512$ images is annotated, there are $600 * 512 * 512 * ~0.3 = 47,185,920$ ground truth pixels. A parameter is a floating point value of 32 bits, while an RGB colour pixel takes 24 bits. This means that the network is able to fully encode the whole ground truth into its parameters: $47,185,920 / 24 * 32 = 62,914,560$ , which is more than the number of ResNet's parameters. This leads to the network hardcoding its training data instead of inferring patterns and/or features, which does not bode well for its generalization. Is this reasoning correct? And is this what happens when training with too little ground truth?
