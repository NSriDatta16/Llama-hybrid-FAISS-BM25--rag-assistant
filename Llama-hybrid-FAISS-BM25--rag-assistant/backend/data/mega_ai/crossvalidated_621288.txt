[site]: crossvalidated
[post_id]: 621288
[parent_id]: 
[tags]: 
Log likelihood decreases with posterior obtained after fitting GP

Possibly related to Log posterior probability in MCMC is decreasing but I do not have a MCMC process and the details there are not sufficient for me to understand fully (I'm a mathematician with basic statistical knowledge). I am playing around with Bayesian Optimisation (in Julia) and wanted to set up a standard scheme where I fit a gaussian process to my data to obtain a posterior from which to sample the next point to evaluate. Problem is that after fitting the gaussian process, the log likelihood on the data is lower by several orders of magnitude. I expected the log likelihood given by the posterior to increase. MWE: using AbstractGPs using Random using Plots # Fix rng rng = MersenneTwister(0) # Construct a zero-mean Gaussian process with a matern-3/2 kernel. f = GP(Matern32Kernel()) # Specify some data x = randn(rng, 10) y = randn(rng, 10) # Construct prior fx = f(x, 0.1) # Sample from the prior y_sampled = rand(rng, fx) # log likelihood of our data from the prior is expected to be low # (poor fit) logpdf(fx, y) # log likelihood on the values sampled from the prior is # expected to be high (good fit) logpdf(fx, y_sampled) # Construct posterior on the data f_posterior = posterior(fx, y) # Log likelihood from the posterior is expected to be higher # than from the prior (better fit) logpdf(f_posterior(x), y) scatter( x, y; xlim=(-0.6, 1.6), xlabel="x", ylabel="y", title="posterior (default parameters)", label="Train Data", ) plot!(-0.6:0.001:1.6, f_posterior; label=false, ribbon_scale=2) With this I get logpdf(fx, y) = -46.6 which is expected low. logpdf(fx, y_sampled)=-4.8 which is higher (expected) but still lower than I expected from some data sampled from the same distribution. And lastly I get logpdf(f_posterior(x), y) = -14207.1 which is what is baffling me the most. The scatter plot shows that the posterior indeed does a fitting of the data, so why would the log likelihood be a thousand fold lower? Note that the exact same behaviour is showing in the docs for AbstractGPs package here (see first section "Setup"): https://juliagaussianprocesses.github.io/AbstractGPs.jl/stable/examples/0-intro-1d/ which is really puzzling. Am I wrong in expecting that the log likelihood should have increased? Or is the package AbstractGPs wrong?
