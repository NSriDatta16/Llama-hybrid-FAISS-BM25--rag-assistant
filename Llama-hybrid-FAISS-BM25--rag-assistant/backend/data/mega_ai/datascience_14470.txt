[site]: datascience
[post_id]: 14470
[parent_id]: 13912
[tags]: 
Since no one has mentioned RAM-efficient methods yet: Instead of loading everything into RAM, you can use online/out-of-core learning. Python's Scikit-Learn for example has the SGDClassifier class. Set its loss function to "log" and you get logistic regression. Using the partial_fit function you can feed it small batches of data that you read straight from the database (or from some CSV file,...). Vowpal Wabbit might also be worth a try. Its made for out-of-core learning - hardly uses any RAM and you won't find anything that's much faster. You could also use Python's Keras library to build a neural network (or in the simplest case just logistic regression), which you can also feed with small batches of data instead of loading everything into RAM. Compared to the other two recommendations a neural network could also learn non-linear dependencies. Besides that, try to start with fewer samples - plot the learning curves with 10k, 100k, 1M samples and see if 100M samples are even necessary to get a good score.
