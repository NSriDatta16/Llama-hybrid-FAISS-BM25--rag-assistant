[site]: datascience
[post_id]: 110191
[parent_id]: 110024
[tags]: 
First, let's clarify some things: Each sequence in a batch is totally independent of the rest of the sequences from the same batch. This behavior is fixed and cannot be changed. The initial state is actually a batch of initial states, where each initial state in the batch applies to the associated sequence in the input batch. You normally want to supply the initial state to the RNN under three different circumstances: When you want to initialize it to zeros. When a different part of the model computes it. This happens, for instance, in encoder-decoder architectures without attention, where the initial state of the decoder is the final state of the encoder. When you want to implement truncated back-propagation through time . This means that your data presents dependencies to past values that are farther away than the training batch maximum sequence length, but you don't want to back-propagate that much. In such cases, you mark your RNN as "stateful" and then it uses the state of the last batch as the initial state of the subsequent batch. For this to actually be useful, the batches have to be prepared in a way that the ending of the sequences in the last batch matches with the beginning of the sequences in the subsequent batch. This answer explains more details about the stateful flag in LSTMs. Now, the answer: "sample" here means sequence. All samples in a batch are always independent of one another. by default, the initial state is reset at every batch; this means that each sequence in the batch is computed independently from the sequence in the same position of the previous batch. You can change the behavior of the RNN to make it use as initial state the last state of the previous batch, meaning that the initial state for each sequence of the new batch is the last state of the sequence at the same position within the previous batch. If, as you suggested, they would have written "Normally, the internal state of a RNN layer is reset every time it sees a new sequence ..." (instead of saying "batch"), it would be very confusing, because a single batch contains multiple sequences, and you just can't keep the state across sequences in the same batch so using "normally" would be incorrect there.
