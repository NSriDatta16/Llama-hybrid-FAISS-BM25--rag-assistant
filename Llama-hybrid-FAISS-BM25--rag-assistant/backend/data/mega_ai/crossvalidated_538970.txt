[site]: crossvalidated
[post_id]: 538970
[parent_id]: 538911
[tags]: 
The time series nature of your data means that your observations lack independence. There are ways to deal with time series data, but the usual hypothesis tests like a t-test assume independence. When independence of observations is violated, there is weird behavior. The type I error rate is too high for a true null hypothesis, and the type II error rate is too high for a true null hypothesis. I will give a simulation and graphs showing this. library(ggplot2) library(MASS) set.seed(2021) N On the left, we see that, when the null is true, the independent data give a uniform distribution of p-values, but the dependent data give a distribution skewed towards rejection, which is undesirable. The test is too powerful. On the right, we see that, when the null is false, the independent data give a distribution of p-values more skewed towards rejection than the dependent data. The test with independent data is more powerful. Thus, the dependent data give too many type I errors and too many type II errors, the worst of both worlds! (Zooming in to $\alpha$ levels of interest with a command like xlim(0, 0.1) does not change the story, and I have played with this with other tests like Wilcoxon, only to get the same result that both error rates are too high.)
