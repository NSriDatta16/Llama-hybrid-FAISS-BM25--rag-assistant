[site]: crossvalidated
[post_id]: 578060
[parent_id]: 578057
[tags]: 
Asymptotic efficiency is both simpler and more complicated than finite sample efficiency. The simplest statement of it is probably the Convolution Theorem, which says that (under some assumptions, which we'll get back to) any estimator $\hat\theta_n$ of a parameter $\theta$ based on a sample of size $n$ can be written as $$\sqrt{n}(\hat\theta_n-\theta)\stackrel{p}{\to}Z+\Delta$$ where $Z\sim N(0,I^{-1})$ has variance equal to the inverse Fisher information per observation and $\Delta$ is pure noise independent of $Z$ . The distribution $Z$ is the best possible limiting distribution of an estimator. If $\Delta$ is zero, so that $\sqrt{n}\hat\theta_n-\theta)\stackrel{p}{\to}Z$ , we say $\hat\theta_n$ is asymptotically efficient. The variance bound is the same as the Cramer-Rao one, so if $\hat\theta_n$ is unbiased and efficient in the finite-sample sense for every $n$ it will be asymptotically efficient. Asymptotic efficiency is a much broader concept, though: it allows for estimators to be biased as long as the limiting asymptotic distribution is unbiased. For example, maximum likelihood estimators are asymptotically efficient under fairly weak assumptions, but they are only finite-sample efficient for a few specially nice models. There are some tricky issues with asymptotic efficiency. It's possible for an estimator to beat the Convolution Theorem at a single point or even on a dense set of measure zero . That's why the Convolution Theorem needs assumptions: it's true for regular estimators (which don't have special weird points) and it's true almost everywhere in $\theta$ for arbitrary estimators, and there's a stronger result called the Local Asymptotic Minimax theorem that really nails down all the loopholes. Aad van der Vaart's Asymptotic Statistics has nice coverage of these details.
