[site]: datascience
[post_id]: 15725
[parent_id]: 
[tags]: 
How to interpret the AUC score in this case?

I just run a random forest model on a imbalance dataset. I got the set of AUC and the confusion matrix. The AUC seemed not bad but actually the model predict every instance as positive. So how it happened and how to use AUC properly? The ROC Curve as below: I plot out the predicted probability of positive class in test set. The probability was within a tight range (0-0.4).
