[site]: datascience
[post_id]: 110606
[parent_id]: 110593
[tags]: 
You have many 1000s of neural network parameters that need to be set up correctly to generate a policy function for your game. In addition, many of the parameters are co-dependent - a "good" set of weights for one neuron in layer 1 will not be effective unless weights in layer 2 make use of it correctly - so cannot be searched and optimised independently. This is much too hard a search problem for a basic GA to optimise. Simple policy functions built out of neural networks can be found using GAs, but you have to radically change the architecture compared to deep learning. A well-established GA/NN combination that should work for your problem is NEAT . There are a few Python implementations, including neat-python . NEAT does things differently to the approach you have tried. Key differences are: The neural networks have far fewer neurons and weights. This is usually OK for simple control systems. It may not work so well if you wanted the policy function to be driven from screenshots of the game (if that is your eventual goal, I recommend investigating reinforcement learning and algorithms like DQN which have been demonstrated to solve these kinds of problem). The NEAT algorithm makes and tracks "innovations" to neural network architecture (e.g. adding new neurons and links between them), so that it can perform crossover and mutation whilst respecting the co-dependent nature of weights in the NN. If you don't want to try NEAT, you could try heavily simplifying your 3-layer network. Probably just 10 neurons per hidden layer will be enough for the policy function, and would reduce the size of search space by a few orders of magnitude. Also if your mutation rate is evaluated per weight, you will probably want it to be lower, e.g. 0.01 instead of 0.10 - too many mutations will swamp any forward progress made from selection with random behaviour and cause performance to plateau.
