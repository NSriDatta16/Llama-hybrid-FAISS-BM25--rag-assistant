[site]: datascience
[post_id]: 120642
[parent_id]: 
[tags]: 
Doubt in ELMO, BERT, Word2Vec

I read an answer on Quora where a NLP Practioner stated that using ELMO and BERT embeddings as input to LSTM or some RNN will defeat the purpose of ELMo and BERT. I am not sure I agree with the above statement. Normally we pass words to LSTM to obtain context specific represtations and I am aware of this. But, we pass word2vec instead of one-hot because the contextual representation after LSTM processed it will be better. Similarly common sense states that, if we give ELMO or BERT word embeddings to LSTM, It should output more context rich words than word2vec. Aint I right? I am aware that once the context is obtained we can fine-tune it straight away for some downstream tasks. But why not use it this way in which we pass the context embeddings of ELMo and BERT to an LSTM ? Doubt #2 : I saw a post where the author used ELMo Embeddings with average vectors for each word for logistic regression and tree based models. While this worked for them, In general, It doesn't make sense ? because, In Logistic regression, Each parameter is fixed to an input. Like, Theta1*X1. So if X1 is of different word every time, It should ideally be more confusing to the model to fix that parameter compared to TFIDF where we have a fixed index for each word ?
