[site]: crossvalidated
[post_id]: 381111
[parent_id]: 381110
[tags]: 
It might help to go through this block by block. train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=2018) ## some config values embed_size = 300 # how big is each word vector max_features = 50000 # how many unique words to use (i.e num rows in embedding vector) maxlen = 100 # max number of words in a question to use Here, you are configuring the features of your array so that Keras knows how to treat the text it is analysing. Before doing so, you have split your data into training data (data which you use to train the model), and test data (data which you use to validate the results of the model against the actual). ## fill up the missing values train_X = train_df["question_text"].fillna("_na_").values val_X = val_df["question_text"].fillna("_na_").values test_X = test_df["question_text"].fillna("_na_").values You are filling up missing values in this instance, and once again, you are doing so across the partitioned training and test data (I note that you seem to have validation and test data), it might be worth determining whether you are referring to the same thing by this and whether they are in fact duplicates. ## Tokenize the sentences tokenizer = Tokenizer(num_words=max_features) tokenizer.fit_on_texts(list(train_X)) train_X = tokenizer.texts_to_sequences(train_X) val_X = tokenizer.texts_to_sequences(val_X) test_X = tokenizer.texts_to_sequences(test_X) A tokenizer breaks up a series of strings, e.g. splitting up a sentence into words, or a paragraph into sentences. Each piece that has been broken up is known as a token . ## Pad the sentences train_X = pad_sequences(train_X, maxlen=maxlen) val_X = pad_sequences(val_X, maxlen=maxlen) test_X = pad_sequences(test_X, maxlen=maxlen) Here, we are ensuring that all sequences in the list have identical length. This is to ensure that when you actually run a neural network, the number of observations in the array are identical.
