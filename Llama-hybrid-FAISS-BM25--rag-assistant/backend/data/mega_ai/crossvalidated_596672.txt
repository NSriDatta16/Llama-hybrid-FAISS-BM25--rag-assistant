[site]: crossvalidated
[post_id]: 596672
[parent_id]: 596397
[tags]: 
Some models are known not to scale well with data (e.g. SVM, Gaussian processes). You can check this by comparing their big $O$ time complexity and memory usage. GPUs don't magically make arbitrary code faster. For GPUs to help, you would need the code to be written in a way that utilizes GPUs. This would not be the case for scikit-learn implementations . For LibSVM there is an GPU optimized implementation , but this means you need to switch to it instead of using the regular LibSVM used by scikit-learn. What often would help is switching to a computer with more CPUs and memory (8 GB is not much for many problems), so it still may be the case for switching to the cloud, but not for machines with GPUs, but with more memory and CPUs. Convergence is a different story. For example, lasso regression (a linear model, by the way) has no problem with large datasets, but convergence will depend on the choice of hyperparameters (the $\alpha$ penalty in this case).
