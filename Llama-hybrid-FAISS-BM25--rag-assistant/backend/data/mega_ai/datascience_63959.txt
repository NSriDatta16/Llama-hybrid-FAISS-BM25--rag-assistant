[site]: datascience
[post_id]: 63959
[parent_id]: 
[tags]: 
why an advanced LSTM model produce the same results as a simpler one?

I have implemented the model proposed in this article which is a text classification model that uses sentence representation rather than only word representation to classify texts. model=tf.keras.Sequential() embeding_layer=layers.Embedding(self.vocab_size,self.word_vector_dim,weights=[word_embeding_matrix],trainable=False,mask_zero=False) model.add(TimeDistributed(embeding_layer)) model.add(TimeDistributed(tf.keras.layers.LSTM(50))) model.add(tf.keras.layers.Bidirectional(costumized_lstm.Costumized_LSTM(50))) model.add(layers.Dense(6,activation='softmax')) opt=tf.keras.optimizers.Adam(learning_rate=0.001) model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy',self.f1_m,self.precision_m, self.recall_m]) self.model=model and I use a dataset with 40000 documents with 6 different labels to train it. (30000 for train and 10000 for the test). I uses a pretrained word embeding and the input for this model is (sample,sentences,words). it achieves 84% accuracy. the problem is that I can achieve this accuracy very easily with this simple model: model=tf.keras.Sequential() embeding_layer=layers.Embedding(self.vocab_size,self.word_vector_dim,weights=[word_embeding_matrix],trainable=False,mask_zero=False) model.add(embeding_layer) model.add(tf.keras.layers.Bidirectional(layers.LSTM(50))) model.add(layers.Dense(6,activation='softmax')) opt=tf.keras.optimizers.RMSprop(learning_rate=0.001) model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy',self.f1_m,self.precision_m, self.recall_m]) self.model=model this one is not based on sentence representation and the input for this model is (sample, words). what is the first model ? is my implementation wrong? what should I do? the training process for both models is as below picture. I also have used every trick to overcome overfitting but I haven't got any results. any suggestions please?
