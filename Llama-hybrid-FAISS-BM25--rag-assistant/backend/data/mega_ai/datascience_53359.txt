[site]: datascience
[post_id]: 53359
[parent_id]: 
[tags]: 
Keras Model always predicts the same class

I am currently trying to build a CNN classifier which takes a ector representing the log of an ECG spectrogram together with its class. What I am currently experiencing is the fact the model always predicts the class '1'. Here is my code: def LoadData(filename): with open(filename, 'rb') as f: x, y = pickle.load(f) xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size=.2) return xTrain, xTest, yTrain, yTest def AugGenerator(xTrain, xTest, yTrain, yTest): imagegen = ImageDataGenerator() trainGen = imagegen.flow(xTrain, yTrain, batch_size=20) testGen = imagegen.flow(xTest, yTest, batch_size=20) return trainGen, testGen def CNN(blockSize, blockCount, inputShape, trainGen, testGen, epochs): model = Sequential() # Conv Layer channels = 32 for i in range(blockCount): for j in range(blockSize): if (i, j) == (0, 0): conv = Conv2D(channels, kernel_size=(5, 5), input_shape=inputShape, padding='same') else: conv = Conv2D(channels, kernel_size=(5, 5), padding='same') model.add(conv) model.add(BatchNormalization()) model.add(Activation('relu')) model.add(Dropout(0.15)) if j == blockSize - 2: channels += 32 model.add(MaxPooling2D(pool_size=(2, 2), padding='same')) model.add(Dropout(0.15)) # Feature aggregation across time model.add(Lambda(lambda x: K.mean(x, axis=1))) model.add(Flatten()) # Linear classifier model.add(Dense(4, activation='softmax')) model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy']) # F1? model.fit_generator(trainGen, validation_data=testGen, steps_per_epoch = len(trainGen) / 20, validation_steps = len(testGen) / 20, epochs=epochs, verbose=1) return model if __name__ == '__main__': xTrain, xTest, yTrain, yTest = LoadData('./LogSignalsAndLabels.pk1') trainGen, testGen = AugGenerator(xTrain, xTest, yTrain, yTest) # model = CNN(4, 6, (285, 33, 1), trainGen, testGen, 1) # model.save('./cnn_model.h5') model = keras.models.load_model('./cnn_model.h5') model.summary() print('Evaluation...') y_predict = model.predict_generator(testGen, steps = len(testGen)).argmax(axis=1) print(y_predict) yTest = yTest.argmax(axis=1) f = open('./evaluation.txt', 'w') f.write('model: CNN, epochs: {} \n confusion_matrix: \n {}'.format(1, confusion_matrix(yTest, y_predict))) f.close() The generators trainGen and testGen both contain a matrix of vectors with length 285, representing the spectrograms, together with their labels one hot encoded, e.g: [1., 0., 0., 0.] . After print(y_predict) shows only ones, giving a confusion matrix of this form: model: CNN, epochs: 1 confusion_matrix: [[ 0 1034 0 0] [ 0 125 0 0] [ 0 508 0 0] [ 0 39 0 0]] For completeness, when I converted the categories into a numeric form, I worked with this way: y[y=='N'] = 0 y[y=='A'] = 1 y[y=='O'] = 2 y[y=='~'] = 3 y = keras.utils.to_categorical(y) Where y is the column of the original labels. What am I missing? Edit: the using of only one epoch is for question purposes.
