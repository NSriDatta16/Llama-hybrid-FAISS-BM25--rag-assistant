[site]: crossvalidated
[post_id]: 61857
[parent_id]: 61809
[tags]: 
Overview Many researchers have discussed the many problems with stepwise regression (e.g., @FrankHarrell (2001) in section 4.3). In particular Harrell notes that "it yields $R^2$ values that are biased high" (p.56). There are several possible interpretations of this statement, based on what you assume is the estimand. If you assume the estimate is some form of $\rho^2$, then the following can be said: While this is true for some combinations of data generating process, sample size, set of predictors and p-value criterion of predictor entry, it is not true in all cases. Specifically, $R^2$ from stepwise regression is not inherently biased in a particular direction when estimating $\rho^2$ . The p-value criterion for entry of predictors in the stepwise regression can be used to modulate the expected value of stepwise $R^2$ (i.e., the estimator of $\rho^2$). Specifically, as the p-value of entry approaches zero, then the probability of any predictor being included in the final model approaches zero, and the expected value of stepwise $R^2$ will approach zero. With a p-value of entry of one, all predictors will be retained, and stepwise $R^2$ will display the same bias that $R^2$ shows with all predictors. The bias is monotonically related to the p-value of entry. Thus, there will be a p-value of entry which results in an unbiased estimate of $\rho^2$. I've run a few simulations under different conditions. The p-value of predictor entry which yielded an approximately unbiased estimate often ranged between .05 and .0001. However, I haven't yet read any simulations which explicitly explore this or provide advice on what kind of bias to expect from published stepwise $R^2$ values using a given p-value of entry and given the features of the data. That said, for practical purposes, adjusted $R^2$ is specifically designed to estimate $\rho^2$. Thus, it is more suited to estimating $\rho^2$ than merely hoping that the p-value of entry in a stepwise regression happens to be correct in order to result in an approximately unbiased estimate . Simulation The following simulation has four uncorrelated predictors where population r-square is 40%. Two of the predictors explain 20% each, and the other two predictors explain 0%. The simulation generates a 1000 datasets and estimates stepwise regression r-square as a percentage for each dataset. # source("http://bioconductor.org/biocLite.R") # biocLite("maSigPro") # provides stepwise regression function two.ways.stepfor library(maSigPro) get_data The following code returns the r-square with an alpha for entry of .01, .001, .0001, and .00001. set.seed(1234) simulations The following results indicate the bias for each of the five alpha of entries. Note that I've multiplied r-square by 100 to make it easier to see the differences. mean(rsquares01) - 40 mean(rsquares001) - 40 mean(rsquares0001) - 40 mean(rsquares00001) - 40 sd(rsquares01)/sqrt(simulations) # approximate standard error in estimate of bias The results suggest that alpha of entries of .01 and .001 result in positive bias and alpha of entries of .0001 and .00001 result in negative bias. So presumably an alpha of entry around .0005 would result in an unbiased stepwise regression. > mean(rsquares01) - 40 [1] 1.128996 > mean(rsquares001) - 40 [1] 0.8238992 > mean(rsquares0001) - 40 [1] -0.9681992 > mean(rsquares00001) - 40 [1] -5.126225 > sd(rsquares01)/sqrt(simulations) # approximate standard error in estimate of bias [1] 0.2329339 The main conclusion I take from this is that stepwise regression is not inherently biased in a particular direction. That said, it will be at least somewhat biased for all but one p-value of predictor entry. I take @Peter Flom's point that in the real world we don't know the data generating process. However, I imagine a more detailed exploration of how this bias varies over, n, alpha of entry, data generating processes, and stepwise regression procedure (e.g., including backwards pass) could substantially inform an understanding of such bias. References Harrell, F. E. (2001). Regression modeling strategies: with applications to linear models, logistic regression, and survival analysis. Springer.
