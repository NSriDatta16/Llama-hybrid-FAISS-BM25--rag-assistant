[site]: crossvalidated
[post_id]: 265992
[parent_id]: 18102
[tags]: 
Please forgive me if I am not understanding the question, but I believe your "systems" are "strategies" that are being backtested or implemented. I cannot directly answer your question because I am not certain exactly what it is, so I will try and answer the one I think you are asking. First, let me give you some observations. You have a massive model set if you are looking at 100k x 100k. I am assuming you did some form of combinatoric solution if that is the case. Ignoring the computational issues, this is problematic at many levels. I have done extensive research on the capital markets and the data set is quite small because of the fact that the data points are not independent of each other. They share an extensive amount of information. Indeed, because of the competitive nature of the market actors must be updating relative valuations on a constant basis. Any attempt at a strategy that ignores the underlying non-price information is highly suspect and will result in a high false discovery rate. The second problem with this is that your best choice for model selection is Bayesian model selection, but, in this case, your strategy size exceeds your degrees of freedom, to borrow a Frequentist idea. If a corporation is thought of as an information stream, then you cannot have more strategies than your smallest number of separate companies at any one time in your set. Indeed, due to nuisance parameters, you need even less. An important problem you face is that you cannot use squared distance. It can be shown that the integrals diverge over the probability distribution for each conceptual portfolio. You can use mean absolute deviation. It has theoretical support under Theil's regression as well. Your final challenge will be the cost of liquidity. If your data is not real portfolios that have had the cost of liquidity taken out by a market maker, then you need to model those costs. I would use Ashok Abbott's chapter in The Valuation Handbook to model these. This will separate your portfolios as well. I was thinking about how I would do an exploratory analysis to differentiate portfolios. With that many, speed is important and Bayesian methods are slow. I would start by regressing portfolios values against their prior values, making adjustments for market closures. I would probably regress $\log(v_{t+1}^i)$ on $\log(v_t^i)$ using ordinary least squares, adjusting for days closed. I would ignore $\alpha$ because analysis of $\alpha$ in least squares style algorithms is at best problematic. I would then find the portfolio with the median slope and if a tie, then with the median $\alpha$ among the ties. I would use this portfolio as my standard portfolio. I would then use this portfolio as a predictor for the remaining portfolios. I would regress $\log(v_{t+1}^k)$ on $\log(v_t^i)$. Any portfolio that can be significantly predicted by this standard portfolio should be in that cluster and any that cannot be predicted by this standard portfolio should be in another cluster. I would then take those without significant prediction and repeat the process, creating new clusters. I would not use returns in my regression, only portfolio values. Returns are not data, they are transformations of data. If for some reason, you choose not to take the log of the value data, then you will need to use Theil's method of regression, otherwise you will get incorrect results with ordinary least squares. This method differs from simply looking at the final value in that the portfolios do not need to start on the same date, though your standard portfolios do need to be long lived, and it better accounts for single idiosyncratic shocks. This is not a canonical solution. This should allow you to create a small set of segregated portfolios that you can then analyze separately using other analysis. Do note that I have a lot of reservations regarding this method and I am hoping it will get a lot of criticism because I didn't spend a lot of time thinking about this. Your problem is that $\frac{v_{t+1}}{v_t}-1$ is the translation of a ratio, so you have a ratio distribution. If you assume normality for the appraisals of the underlying prices, then you have a Cauchy distribution, which would have to be truncated at -100%. This creates no mean or variance, ruling out most solutions. The log solution gives you a biased solution, but the bias is probably consistent across portfolios and it is faster than Theil's regression. Another concern is that your cut-off point for statistical significance will determine your number of clusters and that you cannot determine your false discovery rate. With luck, someone will tear this answer apart.
