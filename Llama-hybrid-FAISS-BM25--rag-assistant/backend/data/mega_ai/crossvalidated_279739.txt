[site]: crossvalidated
[post_id]: 279739
[parent_id]: 
[tags]: 
Is a standard error equal to a maximum likelihood sampling distribution?

Let's say we want to estimate $\mu$ in a normal model. I'm interested in the estimation error. Using R code as an example; samples This gives the following estimate Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 0.3333 0.5432 0.614 0.566 Now if I fit a stan model and sample, I obtain the following estimates mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat mu 0.33 0.03 0.89 -1.40 -0.11 0.33 0.80 2.08 1066 1 s 1.89 0.04 1.01 0.90 1.29 1.64 2.17 4.49 776 1 I am using flat priors for $\mu$ and $\sigma$, so I expect the point estimate, which is the maximum likelihood estimate in both cases, to be equal, which is the case. The standard error does not seem to be equal. Can I interpret the standard error given by lm in a Bayesian / Maximum Likelihood Estimator setting? Why are they not equal?
