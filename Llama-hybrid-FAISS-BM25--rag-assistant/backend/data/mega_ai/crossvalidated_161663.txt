[site]: crossvalidated
[post_id]: 161663
[parent_id]: 
[tags]: 
Taming large datasets. How to keep historical data with aggregation techniques?

I will be collecting huge amounts of computer performance data and the datasets will get large very quickly. This necessitates reducing the size of the data by compressing the historical data using aggregation techniques. This allows historical data to be available for comparison, and keeps the data size manageable. With time series data what kind of aggregation techniques are advisable. Are there any useful resources (textbooks, papers, articles etc.) that discuss this? Initial thoughts are measures of central tendency like mean, median. Measures of dispersion like stddev etc and frequency histograms. Any others. What kind of periods should be aggregated over? Hourly, daily etc.? What level of detail allows useful comparison? Thanks
