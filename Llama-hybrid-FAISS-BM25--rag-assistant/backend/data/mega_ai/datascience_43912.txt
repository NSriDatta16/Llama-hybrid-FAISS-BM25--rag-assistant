[site]: datascience
[post_id]: 43912
[parent_id]: 34441
[tags]: 
When doing logistic regression you start calculating a bunch of probabilities $p_i$ and your target is maximize the product of those probabilities (as they're considered independent events). The higher the result of the product the better is your model. As we are dealing with probabilities we are multiplying numbers between 0 and 1, therefore, if you multiply a lot of those numbers you would get smaller and smaller results. So we need a way to move from probabilities multiplication to a sum of other numbers. Then is when $ln$ function enters in to play. We can use some of this function properties such as: $ln(a b) = ln(a) + ln(b)$ . When our prediction is perfect i.e. 1, the $ln(1) = 0$ . $ln$ lower than 0 are growing negative numbers e.g. $ln(0.9) = -0.1$ and $ln(0.5) = -0.69$ . So we can move from maximizing the multiplication of probabilities to minimizing the sum of the $-ln$ of those probabilities. The resulting cross-entropy formula is then: $$ - \sum_{i=1}^m y_i ln(p_i) + (1-y_i) log (1-p_i) $$ If $y_i$ is 1 the second term of the sum is 0, likewise, if $y_i$ is 0 then the first term goes away. Intuitively cross entropy says the following, if I have a bunch of events and a bunch of probabilities, how likely is that those events happen taking into account those probabilities? If it is likely, then cross-entropy will be small, otherwise, it will be big.
