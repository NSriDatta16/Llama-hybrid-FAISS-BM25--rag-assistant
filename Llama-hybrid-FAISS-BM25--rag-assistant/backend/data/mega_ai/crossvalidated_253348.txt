[site]: crossvalidated
[post_id]: 253348
[parent_id]: 253347
[tags]: 
This is definitely one for stats.stackexchange. You should think through and perhaps articulate why this surprises you? You might have context we don't. We have a gradually increasing sample size. So, presuming the values come from some kind of stationary process, the mean and standard deviation will gradually converge to the full population's value. When there are larger values pop up in the increasing sample, they will spike both the mean and the sd away from where they are converging to. Then the converging resumes. If the data are not from a process you expect to be stationary, then I can't think of a reason for using the cumulative mean and standard deviation from the very beginning of the data to the most recent point (as opposed to, for example, a rolling window, which you could use functions from the zoo package to implement). Something else to bear in mind: you are taking the averages and standard deviations of proportions. This treats each row of your data as a new observation of equal weight, even though some rows have more observations than others. The variance of the "good proportion" in each row will be inversely proportionate to its sample size. For rough and ready purposes it may be ok to ignore this, but an alternative approach would be to weight each row by its own sample size, so the rows with smaller variance get more weight in their contribution to the overall converging mean and standard deviation.
