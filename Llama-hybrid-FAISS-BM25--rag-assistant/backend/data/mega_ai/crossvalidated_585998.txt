[site]: crossvalidated
[post_id]: 585998
[parent_id]: 
[tags]: 
Shuffling data significantly decrease the performance of linear regression

I'm trying to build a simple linear regression model $y=ax+b$ using pytorch , where $y$ is the number of cells increased on Day $n$ and $x$ is the number of cells increased on Day $n-1$ . Both $x$ and $y$ range from 10000 up to 250000 but I did standardize the data before training. The time series of $x$ and $y$ are roughly periodic (as shown in the figure below). I have only 100 data points for both $x$ and $y$ and I specified the batch size as 10, the learning rate as 0.001, and the optimizer as SGD. With this setup, I found that if I set shuffle=True in DataLoader , the performance of the model is much worse. Specifically, here is the prediction values compared to the true values if shuffle=False . The model was doing a decent job, with the loss pretty close to 0. However, if I set shuffle=True (which I thought was the best practice when training data), the model was far from predictive: And the loss fluctuates a lot: I wonder the reason for this. Originally I thought shuffling should always be done when training data. Please let me know if I should provide more information. Thank you!
