[site]: datascience
[post_id]: 10417
[parent_id]: 9785
[tags]: 
Word2vec works in two models CBOW and skip-gram. Let's take CBOW model, as your question goes in the same way that predict the target word, given the surrounding words. Fundamentally, the model develops input and output weight matrices, which depends upon the input context words and output target word with the help of a hidden layer. Thus back-propagation is used to update the weights when the error difference between predicted output vector and the current output matrix. Basically speaking, predicting the target word from given context words is used as an equation to obtain the optimal weight matrix for the given data. To answer the second part, it seems a bit complex than just a linear sum. Obtain all the word vectors of context words Average them to find out the hidden layer vector h of size Nx1 Obtain the output matrix syn1 ( word2vec.c or gensim ) which is of size VxN Multiply syn1 by h , the resulting vector will be z with size Vx1 Compute the probability vector y = softmax(z) with size Vx1 , where the highest probability denotes the one-hot representation of the target word in vocabulary. V denotes size of vocabulary and N denotes size of embedding vector. Source : http://cs224d.stanford.edu/lecture_notes/LectureNotes1.pdf Update: Long short term memory models are currently doing a great work in predicting the next words. seq2seq models are explained in tensorflow tutorial . There is also a blog post about text generation.
