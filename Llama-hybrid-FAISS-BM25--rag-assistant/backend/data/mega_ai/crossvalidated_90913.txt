[site]: crossvalidated
[post_id]: 90913
[parent_id]: 90906
[tags]: 
There are several issues here: As @John Yetter notes, you have included the interaction A*B in model3 as well as the two variables A & B . If you only wanted to fit a model with the two variables, you would use: glm(f~A+B, family=binomial(link="logit")) . If you do want the interaction / when you have one included in the model, the interpretation of the main effects differs. Specifically, the main effect of A is the relationship between A and f when B is equal to 0 (and vice versa for B ). The relationship between regressing y on x , and regressing x on y is, in general, not the same. This is especially true for logistic regression (where there is a non-linear transformation in between y and x , but not between x and y ) and for multiple regression (logistic or otherwise). That is, you cannot turn around your first model f~A into a t-test A~f . You should not test the relationship between the model's predicted values and the original response value as a way to determine "how good the model is". This makes no sense. Since the predicted values are a perfect (i.e., non-stochastic) function of the predictor variables, there is no new / different information there. A more typical way to understand the quality of the model is to examine the ROC curve.
