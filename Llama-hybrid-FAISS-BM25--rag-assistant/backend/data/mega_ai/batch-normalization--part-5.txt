ment that a layer's parameters make in response to updates in previous layers, the correlation between the gradients of the loss before and after all previous layers are updated is measured, since gradients could capture the shifts from the first-order training method. If the shift introduced by the changes in previous layers is small, then the correlation between the gradients would be close to 1. The correlation between the gradients are computed for four models: a standard VGG network, a VGG network with batch normalization layers, a 25-layer deep linear network (DLN) trained with full-batch gradient descent, and a DLN network with batch normalization layers. Interestingly, it is shown that the standard VGG and DLN models both have higher correlations of gradients compared with their counterparts, indicating that the additional batch normalization layers are not reducing internal covariate shift. Vanishing/exploding gradients Even though batch norm was originally introduced to alleviate gradient vanishing or explosion problems, a deep batch norm network in fact suffers from gradient explosion at initialization time, no matter what it uses for nonlinearity. Thus, the optimization landscape is very far from smooth for a randomly initialized, deep batch norm network. More precisely, if the network has L {\displaystyle L} layers, then the gradient of the first layer weights has norm > c λ L {\displaystyle >c\lambda ^{L}} for some λ > 1 , c > 0 {\displaystyle \lambda >1,c>0} depending only on the nonlinearity. For any fixed nonlinearity, λ {\displaystyle \lambda } decreases as the batch size increases. For example, for ReLU, λ {\displaystyle \lambda } decreases to π / ( π − 1 ) ≈ 1.467 {\displaystyle \pi /(\pi -1)\approx 1.467} as the batch size tends to infinity. Practically, this means deep batch norm networks are untrainable. This is only relieved by skip connections in the fashion of residual networks. This gradient explosion on the surface contradicts the smoothness property explained in the previous section, but in fact they are consistent. The previous section studies the effect of inserting a single batch norm in a network, while the gradient explosion depends on stacking batch norms typical of modern deep neural networks. Decoupling Another possible reason for the success of batch normalization is that it decouples the length and direction of the weight vectors and thus facilitates better training. By interpreting batch norm as a reparametrization of weight space, it can be shown that the length and the direction of the weights are separated and can thus be trained separately. For a particular neural network unit with input x {\displaystyle x} and weight vector w {\displaystyle w} , denote its output as f ( w ) = E x [ ϕ ( x T w ) ] {\displaystyle f(w)=E_{x}[\phi (x^{T}w)]} , where ϕ {\displaystyle \phi } is the activation function, and denote S = E [ x x T ] {\displaystyle S=E[xx^{T}]} . Assume that E [ x ] = 0 {\displaystyle E[x]=0} , and that the spectrum of the matrix S {\displaystyle S} is bounded as 0 < μ = λ m i n ( S ) {\displaystyle 0<\mu =\lambda _{min}(S)} , L = λ m a x ( S ) < ∞ {\displaystyle L=\lambda _{max}(S)<\infty } , such that S {\displaystyle S} is symmetric positive definite. Adding batch normalization to this unit thus results in f B N ( w , γ , β ) = E x [ ϕ ( B N ( x T w ) ) ] = E x [ ϕ ( γ ( x T w − E x [ x T w ] v a r x [ x T w ] 1 / 2 ) + β ) ] {\displaystyle f_{BN}(w,\gamma ,\beta )=E_{x}[\phi (BN(x^{T}w))]=E_{x}{\bigg [}\phi {\bigg (}\gamma ({\frac {x^{T}w-E_{x}[x^{T}w]}{var_{x}[x^{T}w]^{1/2}}})+\beta {\bigg )}{\bigg ]}} , by definition. The variance term can be simplified such that v a r x [ x T w ] = w T S w {\displaystyle var_{x}[x^{T}w]=w^{T}Sw} . Assume that x {\displaystyle x} has zero mean and β {\displaystyle \beta } can be omitted, then it follows that f B N ( w , γ ) = E x [ ϕ ( γ x T w ( w T S w ) 1 / 2 ) ] {\displaystyle f_{BN}(w,\gamma )=E_{x}{\bigg [}\phi {\bigg (}\gamma {\frac