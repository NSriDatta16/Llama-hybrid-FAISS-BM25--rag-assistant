[site]: crossvalidated
[post_id]: 598553
[parent_id]: 
[tags]: 
Basic Neural network time series models

Suppose we have a time series of the following form: $$\vec{R}_n = \vec{\mu}_n(\vec{\theta})+\sqrt{\Sigma_n(\vec{\theta})}\vec{Z}_n$$ Where $Z$ is a iid standard multivariate normal. We assume the mean and covariance are predictable (some authors like D. Williams say previsible), i.e. known at time $n-1$ : $$\mu_n, \Sigma_n \in \mathcal{F}_{n-1}$$ where $\mathcal{F}_n=\sigma(\vec{R}_1, \dotsc, \vec{R}_n)$ . Specifically, we allow them to be functions of the $n-1$ previous observations of $R$ . Here $\sigma(X)$ is the sigma-algebra of the RV $X$ and $\mathcal{F}_n$ is the filtration of the $n$ observed random variables that are relevant, while $\Sigma$ is a covariance matrix (or rather a positive definite matrix), with, say, Cholesky decomposition $\sqrt{\Sigma}$ . Suppose further that, in some (perhaps loose) sense these coefficients are given by neural networks with parameters $\theta$ (either as the output or hidden states—I believe the latter is more appropriate if not mistaken). Question Have such models been studied and if so what are they called and what is the most accessible literature for such models? Personally, at first I started studying recurrent neural networks since those can handle sequential data, but most literature I found is devoted to text or natural language processing applications. Then I came across neural network Gaussian processes which sounds exactly what I’m after but I am still digging through the literature. I figured I would ask here since someone might know succinctly what to look for. I could very well try to implement this myself with specified layers, parameters, the log-likelihood, and attempting backpropagation myself, but it would be more efficient to do this in e.g. pytorch, since I’m sure this idea has already been explored. Apologize if this is well known or too broad. Essentially I am looking for neural network generalizations of classic time series models (AR, GARCH). Any classic references would be appreciated. Thank you.
