[site]: crossvalidated
[post_id]: 560232
[parent_id]: 301162
[tags]: 
Different combinations can give the same output value! Consider a simple neural network with one feature, a hidden layer with two neurons (ReLU activation), and an output neuron with an identity activation. $$ \widehat{y_i} = \widehat{b_{1,2}} + \widehat{w_{1,2}}ReLU\bigg( \widehat{b_{1,1}}+\widehat{w_{1,1}}x \bigg) + \widehat{w_{2,2}}ReLU\bigg( \widehat{b_{2,1}}+\widehat{w_{2,1}}x \bigg) $$ The following two sets of parameter estimates give the same output values of $\widehat{y_i}$ . 1 $ \widehat{b_{1,2}} = 1\\ \widehat{w_{1,2}} = 2\\ \widehat{b_{1,1}} = 3\\ \widehat{w_{1,1}} = 4\\ \widehat{w_{2,2}} = 5\\ \widehat{b_{2,1}} = 6\\ \widehat{w_{2,1}} = 7 $ 2 $ \widehat{b_{1,2}} = 1\\ \widehat{w_{1,2}} = 5\\ \widehat{b_{1,1}} = 6\\ \widehat{w_{1,1}} = 7\\ \widehat{w_{2,2}} = 2\\ \widehat{b_{2,1}} = 3\\ \widehat{w_{2,1}} = 4$ That is, neural network solutions in general are not unique.
