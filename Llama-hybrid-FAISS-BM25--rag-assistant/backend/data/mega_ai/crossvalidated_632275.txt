[site]: crossvalidated
[post_id]: 632275
[parent_id]: 
[tags]: 
Classify text by topics using SVM: Derive the upper bound for the norm of the weight vector

In the section on SVM in one book I'm reading, the authors wrote: Consider the problem of learning to classify a short text document according to its topic, say, whether the document is about sports or not. We first need to represent documents as vectors. One simple yet effective way is to use a bag-of- words representation. That is, we define a dictionary of words and set the dimension $d$ to be the number of words in the dictionary. Given a document, we represent it as a vector $\mathbf{x} \in \lbrace 0,1\rbrace^d$ , where $x_i = 1$ if the $i$ â€™th word in the dictionary appears in the document and $x_i = 0$ otherwise. Therefore, for this problem, the value of $\rho^2$ will be the maximal number of distinct words in a given document. A halfspace for this problem assigns weights to words. It is natural to assume that by assigning positive and negative weights to a few dozen words we will be able to determine whether a given document is about sports or not with reasonable accuracy. Then the authors came to a conclusion that we can upper bound the (Euclidean-)norm squared of the weight vector by 100, that is $||\mathbf{w}||^2\le 100$ . However, I can't figure out why the upper bound turns out to be $100$ . As far as I know, SVM algorithm doesn't put any constraint on any element of the weight vector so they can vary freely during training. And so even weights are assigned to only a few dozen words we still don't know its upper bound?
