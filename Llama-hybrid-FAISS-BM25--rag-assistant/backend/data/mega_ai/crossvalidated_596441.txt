[site]: crossvalidated
[post_id]: 596441
[parent_id]: 596413
[tags]: 
I will try to outline a high-level answer to your points, even though there seems to be some confusion about what PCA is doing and how you can use it in your ML framework. PCA finds the components of variability in your data as a series of orthogonal linear combinations of the input features. PCA is a dimensionality reduction technique: by itself, it doesn't pick single features (feature selection) nor assign labels to the samples. Do we use the PC1, PC2, PC3 to the supervised learning? You can use all of them (identical to using your data without PCA), or only the first, and everything in between. The question becomes: how many PCs should I retain? One might answer this by looking at the total fraction of variance explained by the first $n$ PCs, or by visualizing the elbow/scree plot of the result. Do we use the generated labels to the supervised learning? PCA does not generate labels. It generates new features as linear combinations of the inputs. You can use them as features replacing the original data, possibly reducing the size of your input and with the advantage of removing redundancy between features. Then you proceed predicting the labels using the PCs. Remember that PCA should be computed only on the training set, and the test set data should be projected using that result before feeding it to the algorithm of choice. If we use the PC1, PC2, PC3, and/or generated labels, do we use the original numerical features in supervised learning? I would say no. The point of PCA is exactly to extract those major axes of variability, so using the input features again WITH the PC introduces redundant information. Feature selection As you correctly pointed out, PCA also gives you loadings for the components. With that, you can understand which are the major features contributing to each principal component. I think this can help with the explainability of your model, and maybe with future feature selection. but PCA doesn't do that per se .
