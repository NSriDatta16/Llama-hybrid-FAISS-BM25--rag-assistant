[site]: crossvalidated
[post_id]: 478100
[parent_id]: 
[tags]: 
Exploding probability under simple hierarchical Bayesian formulation

I am wondering if someone here can clear up a point of confusion that I have when applying MCMC or an optimization method to hierarchical Bayesian problems. Let's say we have a likelihood and prior with the following form: $N(x_{i, j}|v_j, \sigma^2)N(v_j|\mu_v, \sigma_v^2)N(\mu_v|0, 1)p(\sigma_v)p(\sigma)$ Where $N$ represents a normal distribution, while $p()$ is some other unspecified distribution (e.g. half-normal). The subscript $i$ represents some repeated trial, while the subscript $j$ represents some grouping variable. For example $x_{i,j}$ is a button press time, where $i$ represents a single response for a subject $j$ . It seems to me that there are two ways to go to maximize the sum log probability here: One is that $x$ constrains the individual $v_j$ , which in turn constrains $\mu_v$ and $\sigma_v$ to their appropriate values. Another is that $\sigma_v$ collapses to 0, (all) $v_j$ collapse to $\mu_v$ , and sigma expands to cater to $x$ . In this case, the log probability of $N(v_j|\mu_v, \sigma_v^2)$ explodes since all the $v_j$ are perfectly explained by $v_j = \mu_v$ and $\sigma_v=0$ . The solution that would be typically wanted is 1, but because of the 'exploding' nature of 2 I don't see how to avoid it in this formulation (without making strong assumptions about the priors). So I am wondering if the problem definition is simply not constrained enough or if I have a conceptual misunderstanding, or something else - Thank you -
