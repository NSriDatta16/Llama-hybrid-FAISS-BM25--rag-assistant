[site]: datascience
[post_id]: 114885
[parent_id]: 114863
[tags]: 
You have not addressed how many variables there are, what type are the variables. How many are categorical? How many levels are there in those categorical variables? A very specific failure of decision trees is that with a lot of categorical variable level the tree over-trains on the categories and does not generalize well at all. It is a common kind of instability that is not talked about openly often enough in my opinion. Another thing you need to consider is that the pattern captured by the training data in the first 6-years has changed all or in part. It is an inherent possibility in any system over time. A good place to start is to figure out if your data is appropriate to a decision tree in general and a random forest in particular based on the categorical variable presence. Then make sure to do some tuning, explore the appropriate number of nodes and variables to work with and see if that improves consistency (your train my go down, but if it is more reflective of the test set it is a more robust model) And finally you can work with some diagnostics, variable importance in a good place to start, see if you can figure out the most important variables and try again. It may help stabilize your model. But before you do any of that, I would build charts and graphs of your data from the train and the test, do some tests to see if you have skew, kurtosis or heteroskedasticity. It may be there in the train or the test or both. While random forests do not demand normally distributed data, if your train is to be predictive of the test, they should be similarly patterned. Especially over a dataset as large as 8-million rows. Short of using unfit variables I think the temporal shift might be the culprit.
