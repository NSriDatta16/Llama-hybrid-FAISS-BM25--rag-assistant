[site]: datascience
[post_id]: 32760
[parent_id]: 
[tags]: 
What algorithms will stuck in the local minimum?

Algorithms like neural network are easily getting stuck in local minimum because the shape of the loss function (so there are parameters like momentum are designed to solve this type of problem). Logistic regression will always find global minimum because log-loss is a convex function (please feel free to correct me if I miss anything here). Then I am wondering for other algorithms like RandomForest, GradientBoostTree, or SVM, which one would face the problem of stuck in local minimum, which one would not? And why? Thanks!
