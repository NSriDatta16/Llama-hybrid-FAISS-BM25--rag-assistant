[site]: crossvalidated
[post_id]: 528258
[parent_id]: 
[tags]: 
Formal arguments for why an asymmetric f-divergence might be favourable to a symmetric one in analyzing importance sampling

I am reading Importance Sampling and Necessary Sample Size: an Information Theory Approach . Below is a quote from paragraph 3, section 3 of the article. While [total variation distance] and [Hellinger distance] can be shown to be distances in P(X), [Kullback-Leibler divergence] and [χ2 divergence] are not. In particular, these latter divergences fail to be symmetric, a feature that makes them appealing for the analysis of importance sampling . Indeed, the very formulation of the method is built on an asymmetric premise (the absolute continuity of P with respect to Q). Moreover, it is well acknowledged that it is desirable that the proposal has heavier tails than the target —again an asymmetric requirement. I do not understand how this claim (bolded quote) is justified by the two sentences following it. I understand that importance sampling is essentially an application of the Radon-Nikodym derivative of the target with respect to the proposal, hence absolute continuity is required for the existence of such a density. However, I do not understand how these "asymmetries" are a "feature" rather than a defect. My question is as the title reads, are there formal arguments for why an asymmetric f-divergence might be favourable to a symmetric one in analyzing importance sampling?
