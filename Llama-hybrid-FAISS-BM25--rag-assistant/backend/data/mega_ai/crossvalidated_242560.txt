[site]: crossvalidated
[post_id]: 242560
[parent_id]: 
[tags]: 
"Proof?" of Bias/Variance trade-off

I'm unsatisfied with the "proof" that there is a trade-off between variance and bias in Hastie's text (The Element of Machine Learning). I also haven't found anything on this site that speaks to my particular concern. In section 7.3, they show the following. Assume that the target $Y$ has the relationship $Y = f(X) + \varepsilon$ where $E(\varepsilon) = 0$ and $Var(\varepsilon) = \sigma_\varepsilon^2$. Since the target has error in it, they call $\sigma_\varepsilon^2$ "irreducible error". $\begin{aligned} Err(x_0) &= E\big[(Y - \hat{f}(x_0))^2 \mid X = x_0\big] \\ &= \sigma_\varepsilon^2 + \big[ E(\hat{f}(x_0) - f(x_0) \big]^2 + E\big[\hat{f}(x_0)-E(\hat{f}(x_0) \big]\\ &= Irreducible Error + Bias^2 + Variance \end{aligned}$ Fine. Obviously if Err(x) were constant, than an increase in bias would lead to a decrease in variance. But, as we change the model, who is to say that the error is constant?? Wouldn't a great model have low bias and variance whereas a horrible model could have high variance and bias? So, I see no evidence here that a more complex model must have lower bias but greater variance. Is this calculation proving anything? I would like to know if there's a rigorous, mathematical argument showing that this tradeoff is fundamental. Usually I just see intuitive arguments or case studies in the context of one specific type of analysis (such as linear regression)
