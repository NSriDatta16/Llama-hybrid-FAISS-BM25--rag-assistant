[site]: datascience
[post_id]: 114917
[parent_id]: 114886
[tags]: 
Very, very slowly. For most algorithms, adding dimensions has a non-linear impact on processing time, and once you can no longer fit all the data in memory, you should prepare for an exponential increase in processing time. Selecting your algorithm before understanding your data very rarely leads to a successful outcome and while Deep Learning can be done with massively dimensional data, it is very rarely worth the effort. The first step in any data mining process is understanding what data you have available to you. It amazes me how many people forget to do simple data correlation exercises which can often eliminate 60%-90% of the data. Algorithms that use training and testing datasets are another way to massively reduce the amount of data you need to process to determine if there is anything there worth chasing. Then apply some simple categorisation algorithms to determine if any interesting patterns exist. If they do, then you can use more complex (read expensive) algorithms to improve accuracy. You could also investigate various columnar storage approaches to massively compress the memory footprint of your data set. If a column only uses seven discrete values in your data set, then it can be represented with just three bits. In the worst case, you can apply sampling to reduce the columns and rows to something that fits in memory. If no meaningful and useful patterns are found after an arbitrary number of samples are processed, walk away from the problem saying there is nothing that can be found with the resources available.
