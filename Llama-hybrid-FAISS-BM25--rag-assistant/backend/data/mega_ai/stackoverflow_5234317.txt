[site]: stackoverflow
[post_id]: 5234317
[parent_id]: 5233769
[tags]: 
1) The R Import / Export manual should be the first port of call for questions about importing data - there are many options and what will work for your could be very specific. http://cran.r-project.org/doc/manuals/R-data.html read.table specifically has greatly improved performance if the options provided to it are used, particular colClasses , comment.char , and nrows - this is because this information has to be inferred from the data itself, which can be costly. 2) There is a specific limit for the length (total number of elements) for any vector, matrix, array, column in a data.frame, or list. This is due to a 32-bit index used under the hood, and is true for 32-bit and 64-bit R. The number is 2^31 - 1. This is the maximum number of rows for a data.frame, but it is so large you are far more likely to run out of memory for even single vectors before you start collecting several of them. See help(Memory-limits) and help(Memory) for details. A single vector of that length will take many gigabytes of memory (depends on the type and storage mode of each vector - 17.1 for numeric) so it's unlikely to be a proper limit unless you are really pushing things. If you really need to push things past the available system memory (64-bit is mandatory here) then standard database techniques as discussed in the import/export manual, or memory-mapped file options (like the ff package), are worth considering. The CRAN Task View High Performance Computing is a good resource for this end of things. Finally, if you have stacks of RAM (16Gb or more) and need 64-bit indexing it might come in a future release of R. http://www.mail-archive.com/r-help@r-project.org/msg92035.html Also, Ross Ihaka discusses some of the historical decisions and future directions for an R like language in papers and talks here: http://www.stat.auckland.ac.nz/~ihaka/?Papers_and_Talks
