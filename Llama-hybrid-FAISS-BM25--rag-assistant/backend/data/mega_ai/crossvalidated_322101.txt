[site]: crossvalidated
[post_id]: 322101
[parent_id]: 
[tags]: 
Does L2-normalization of ridge regression punish intercept? If not, how to solve its derivative?

I am new to ML. I was informed that the L2-normalization of ridge regression does not punish the intercept $\theta_{0}$. As in the cost function: $$ \nabla_{\theta}J(\theta)=\frac{1}{2}\sum_{i=1}^{m}(h_{\vec \theta}(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^{n}{\theta_{j}^{2}} $$ The L2-normalization term $\lambda\sum_{j=1}^{n}{\theta_{j}^{2}}$ only sums from $j=1$ to $n$, not from $j=0$ to $n$. I also read that: in most cases (all cases?), you're better off not regularizing $\theta_{0}$, since its unlikely to reduce overfitting and shrinks the space of representable functions which comes from the last answer of user48956 of Why is a zero-intercept linear regression model predicts better than a model with an intercept? I am confused about how to solve the derivative of the cost function, since: $$ \nabla_{\theta}J(\theta)=\frac{1}{2}（X\theta-Y）^{T}（X\theta-Y）+\lambda(\theta^{'})^{T}\theta^{'}, $$ where $\theta^{'}=\left[ \begin{matrix} \theta_{1} \\ \theta_{2} \\ ...\\ \theta_{n} \end{matrix} \right]$ , $\theta=\left[ \begin{matrix} \theta_{0} \\ \theta_{1} \\ ...\\ \theta_{n} \end{matrix} \right]$ and $X=\left[ \begin{matrix} 1 & X_{1}^{(1)} & X_{2}^{(1)} & ...& X_{n}^{(1)} \\ 1 & X_{1}^{(2)} & X_{2}^{(2)} & ...& X_{n}^{(2)} \\ ...\\ 1 & X_{1}^{(m)} & X_{2}^{(m)} & ...& X_{n}^{(m)} \end{matrix} \right]$. $\theta^{'}$ and $\theta$ are different. Hence they cannot be mixed from my point of view. And the derivative is about $\theta$，which contains $\theta^{'}$. After googling and viewing the questions on this forum, there is still no way for me to get the solution: $$ \theta=(X^TX+\lambda*I)^{-1}X^TY $$ Can anybody give me a clue? Thanks in advance for your help! However, I think there are two quick fixes to this problem: First of all, we do not add the all 1 column to $X$. Namely $X=\left[ \begin{matrix} X_{1}^{(1)} & X_{2}^{(1)} & ...& X_{n}^{(1)} \\ X_{1}^{(2)} & X_{2}^{(2)} & ...& X_{n}^{(2)} \\ ...\\ X_{1}^{(m)} & X_{2}^{(m)} & ...& X_{n}^{(m)} \end{matrix} \right]$. That is to say we do not include the intercept at all in the model:$$ y=\theta_{1}X_{1}+\theta_{2}X_{2}+...+\theta_{n}X_{n}.$$ I believe this method is adopted in the classic book Machine Learning in Action by Peter Harrington which I am currently reading. In its implementation of ridge regression (P166 and P177 if you also have the book), all the $X$ passed to ridge regression does not have the all 1 column. So no intercept is fitted at all. Secondly, the intercept is also being punished in reality. scikit's logistic regression regularizes the intercept by default. which once again comes from the last answer of user48956 of Why is a zero-intercept linear regression model predicts better than a model with an intercept? Both of the two quick fixes lead to the solution $$ \theta=(X^TX+\lambda*I)^{-1}X^TY. $$ So can the derivative of L2-normalization of ridge regression actually being solved or are just solved by quick fixes?
