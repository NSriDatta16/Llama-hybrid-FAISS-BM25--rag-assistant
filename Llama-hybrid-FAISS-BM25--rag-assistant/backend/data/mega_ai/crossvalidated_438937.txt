[site]: crossvalidated
[post_id]: 438937
[parent_id]: 438925
[tags]: 
Cross validation, as you have seen, involves randomization. Therefore, any result derived from it will have some randomness. It is always good practice to repeat cross validation a couple of times (e.g., using different RNG seeds) to see how strong this randomness is. If you have a small dataset, or a large model, your randomness will be larger than with a large dataset or a small model. And yes, if the randomness is high, then the "best" model derived from CV is a bit dubious. One way of dealing with this is the so-called "one standard error rule" , which attempts to balance this noise against the complexity of the models under consideration. If you want more stability, consider rerunning the CV multiple times and averaging the results. Nothing keeps you from using each observation multiple times in different holdouts. (Of course, this only makes sense if the rest of the holdout sample differs each time, but the randomization should ensure this.) Don't overdo it, though. Taken to extremes, this can lead to overfitting to your CV sample. Incidentally, accuracy is not a good error measure.
