[site]: crossvalidated
[post_id]: 121576
[parent_id]: 
[tags]: 
Logistic Regression sample size & bootstrapping

The data for this example can be retrieved here so that you can reproduce these estimates. It is the low birth weight dataset- http://www.umass.edu/statdata/statdata/data/ There are 59 1's and 130 0's for the outcome variable. I have a sample size of 189. And I run a logistic regression analysis and get these results: low |z|) Intercept 1.295 1.071 1.209 0.227 AGE -0.024 0.034 -0.706 0.480 FTV -0.049 0.167 -0.295 0.768 RACE Black 1.004 0.498 2.016 0.044* RACE Hispanic 0.433 0.362 1.196 0.232 LWT -0.014 0.007 -2.178 0.029* So If I wanted to know what the probability was of a black women having a low birth weight baby and she is 30 years old and her weight (in pounds) at the last menstrual period was 108 and she had 1 physician visit during the first trimester, I would calculate the probability as follows. First, $$1.295-0.024(30)-0.049(1)+1.004(1)+0.433(0)-0.014(108)=0.018.$$ Then, as a probability, $\exp(0.018)/(1+\exp(0.018))*100=50.45\%$. If the data says the probability that this person will have a low birth weight baby is 50.45%, somebody might question this and say that the sample is only 189. I only have a sample of 189 and let's say I can't get any more data, How do I convince the layperson that the results/estimates are robust? Could you do a bootstrapping perhaps? because If I understand correctly, you could resample repeatedly and randomly from the sample like 10 000 times and calculate standard errors and confidence intervals of the regression coefficents (which would make one more confident in the estimates and results). Thereafter, you could get the predicted probabilities and the 95% confidence intervals? If this is the case, how would I do the bootstrapping in R for this example?
