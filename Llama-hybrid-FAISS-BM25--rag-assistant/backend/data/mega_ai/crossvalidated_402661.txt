[site]: crossvalidated
[post_id]: 402661
[parent_id]: 402549
[tags]: 
To my knowledge, no. There are more strange things in this plot, e.g. why does bagging outperform the random forest with respect to the OOB error? It's hard to explain the observed without more information on the data, e.g. how many samples were used in training and testing? How was training and testing performed? If the model was trained and tested on only a small set of samples, the observed difference in error rate might be not significant. Further, if the problem has a rather steep learning curve and testing was performed by holding out a portion of the data while OOB error estimation was performed on the entire data-set, under-fitting might be another explanation.
