[site]: datascience
[post_id]: 121136
[parent_id]: 
[tags]: 
How chatGPT can remember previous context?

I thought I had some knowledge about NLP based on transformer, but after seeing ChatGPT, I felt like I didn't know much. There is clearly a limit to the size of text that can be processed at once with transformer. However, when conversing with ChatGPT, the length of the text becomes incredibly long, exceeding the length that ChatGPT can process at once. Nevertheless, ChatGPT seems to remember the content of previous conversations. How can the context of previous conversations, which took place a long time ago, be used in generating current sentences? Here are some hypotheses I came up with. One is to embed each sentence to correspond to a single token size and place them at the beginning of the sequence that the transformer processes, like this: . Alternatively, it may be possible to add context in a simple way, such as applying positional encoding to the transformer. However, I have not been able to find out how ChatGPT actually processes long conversations. If anyone could shed light on this for me, I would greatly appreciate it. Thank you!
