[site]: datascience
[post_id]: 103216
[parent_id]: 
[tags]: 
How does state information get transferred when predicting with LSTM

I have a typical mutivariate time series forecasting problem that I want to solve using an LSTM, with mutliple features in the input sequnce and one feature in the output sequence. If I train my LSTM as follows: # 1) Define model = Sequential() model.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True)) model.add(Dense(1)) model.compile(loss='mean_squared_error', optimizer='adam') # 2) Train model.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False) # 3) Predict yhat = model.predict(X, batch_size=batch_size) I understand that it is important to maintain the state during training, because I have a continuous sequence that has been broken up and lagged, which is discussed here . This conceptually makes sense to me because during the training process the LSTM retains information on the entire training history, but this leads to my question: In predict (part 3) of the code, when providing the network with a new unseen future sequence say sequence $X_{T+1}$ , the model makes a prediction for that individual sequence. Now when providing the model with the next unseen future sequence $X_{T+2}$ , how does it retain state over what happened in sequence $X_{T+1}$ ? Conceptually, I can undertstand if the model is retrained after every new unseen sequence, but is this the only way? Or is there some parameter in the predict function that I'm missing, or some custom walk forward approach? NOTE: I'm using a batch size of 1 I'm actually developing the model in R, but I've realised support for deep learning in R is near non-existant when compared to python so have written pseudo-like code which I'm not 100% confident about (code isn't the issue here) .
