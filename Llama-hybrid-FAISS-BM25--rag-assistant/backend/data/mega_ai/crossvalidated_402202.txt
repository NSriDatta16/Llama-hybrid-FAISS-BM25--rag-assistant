[site]: crossvalidated
[post_id]: 402202
[parent_id]: 402198
[tags]: 
You primarily have three options (particularly 2 & 3 are somewhat similar): You can come up with a good imputation method (e.g. for continuous features after some suitable transformation look at their covariance matrix and impute on that basis) and impute the unavailable data. You can train a model that implicitly handles missing values (e.g. xgboost has a default direction in each tree split, in which missing data are assigned, it may help if you have some actual missing data to make this direction be sensible). You explicitly code data as missing (and possibly artificially create missingness* and then code data as such) with a code that does not coincide with any real value, e.g. -1 for strictly positive numbers. Then you use a model that can deal with non-linearities (e.g. neural networks) to reflect that such data are to be treated differently than observed data. * This can be tricky, because your missingness process may totally mismatch what is missing in real-life. Under missing completely at random you would be fine though.
