[site]: crossvalidated
[post_id]: 399834
[parent_id]: 
[tags]: 
What's the difference between estimating on a dataset $P(X|Y)$ and $P(Y)$ vs $P(Y|X)$?

In chapter 3 of his excellent book (" Generative and discriminative classifiers: Naive Bayes and logistic regression ") , Tom Mitchell says that, when learning classifiers based on Bayes rule, one can estimate the parameters of $P(X|Y)$ and $P(Y)$ by maximum likelihood. Example: for discrete $Y$ and $X$ , estimator for $P(X=x_i|Y=y_j)$ would be the proportion in the dataset of $X$ having value $x_i$ when $Y$ has value $y_j$ . Estimator for $P(Y=y_j)$ would be the proportion of $Y$ having value $y_j$ . Thing that I don't understand is "what is the difference between 1) estimating $P(X|Y)$ and $P(Y)$ parameters as stated above, make the product and divide by $P(X)$ and 2) estimating directly the parameters of $P(Y|X)$ ? For example, estimator for $P(Y=y_j|X=x_i)$ would be the proportion of $Y$ having value $y_j$ when $X$ has value $x_i$ . The result is the same, isn't it? Am I missing something?
