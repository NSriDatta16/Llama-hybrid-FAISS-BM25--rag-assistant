[site]: crossvalidated
[post_id]: 629477
[parent_id]: 451925
[tags]: 
The obvious move would seem to be to ditch the sigmoid activation function and use the full (non-bucketed) target data. This is theoretically reasonable (activation function is analogous to a GLM link function) and should be easy to implement in code. You also will want to change the loss function to something more appropriate for a machine learning regression problem. Among other possibilities that might be more reasonable, depending on what exactly you value, mean squared error and mean absolute error are popular. Finally, you will want to change the performance metrics to regression metrics, for instance mean squared error or mean absolute error. What is not obvious is if a model that scored well in classifying observations into the bins will do well when you refit without the activation function (the software implementation might call this a linear or identity activation function). It might be that the hyperparameters you've selected are not viable for the full regression task, for instance. Therefore, I would not consider it a given that you can just change the activation, loss, and performance-assessment functions in your code, run the .py file, and expect the model to score well in terms of regression metrics, nor do I see it as a given that scoring poorly on classification metrics means the regression will do poorly. Somewhat related, I see some major downsides to bucketing the outcome and doing classification.
