[site]: datascience
[post_id]: 47679
[parent_id]: 
[tags]: 
Newton method and Vanishing Gradient

I read the article on Vanishing Gradient problem , which states that the problem can be rectified by using ReLu based activation function. Now I am not able to understand that if using ReLu based activation function solves the problem, then why there are so many research papers suggesting the use of Newton's method based optimization algorithms for deep learning instead of Gradient Descent? While reading research papers, I was having the strong impression that vanishing gradient problem was the core reason for such suggestions but now I am confused whether Newton's method is really needed if Gradient Descent can be modified to rectify all the problems faced during machine learning.
