[site]: crossvalidated
[post_id]: 260664
[parent_id]: 41704
[tags]: 
I think that this is done simply so that the feature with a larger value does not overshadow the effects of the feature with a smaller value when learning a classifier . This becomes particularly important if the feature with smaller values actually contributes to class separability .The classifiers like logistic regression would have difficulty learning the decision boundary, for example if it exists at micro level of a feature and we have other features of the order of millions .Also helps the algorithm to converge better . Therefore we don't take any chances when coding these into our algorithms. Its much easier for a classifier, to learn the contributions (weights) of features this way. Also true for K means when using euclidean norms (confusion because of scale). Some algorithms can work without normalizing also.
