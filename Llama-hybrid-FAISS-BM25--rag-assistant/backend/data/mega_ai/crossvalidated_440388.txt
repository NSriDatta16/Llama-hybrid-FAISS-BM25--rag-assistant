[site]: crossvalidated
[post_id]: 440388
[parent_id]: 440380
[tags]: 
No, the independence of estimators does not come from the sampling technique. Empirically, ensembles tend to achieve better performance when there is more diversity between models. To achieve this diversity ( not to be confused with independence ), randomized training sets can be generated (with or without replacement). Models are then trained on different samples (see e.g. Bootstrap Sampling/Bagging) and then combined e.g. by averaging. Parallel training is possible because your estimators are independent. In contrast, boosting is an example of an ensemble method based on sequential training where each estimator tries to correct the mistakes of the previous basic learner. By definition, learners are not independent.
