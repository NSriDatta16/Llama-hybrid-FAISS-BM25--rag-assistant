[site]: datascience
[post_id]: 6323
[parent_id]: 6304
[tags]: 
I would say cross validation is unnecessary here since the multiple partitioning of the data and variables is already implicit in Random Forests. But it's still a good practice to hold out a testing set that is distinct from the training set. This is mostly because you may introduce changes in your random forest to improve the performance on the test sets overall, thereby introducing the bias that the random forests are trying to overcome. So if you withheld a portion of your data and judged the final performance of the RF on that withheld set only in the predict step, then it's fine.
