[site]: crossvalidated
[post_id]: 633374
[parent_id]: 505742
[tags]: 
I myself is new to this topic so I can't be sure. But I will tell you my intuition. Regardless whether it is forward or backward mode AD, to evaluate the gradient is vector calculus that can be expressed as matrix-vector multiplications. If you have an expression for the gradient: g=A B C D e, where A,B,C,D are matrices and e is an vector, in the computer program it is common that you have a way to obtain D*e directly with no explicit storage of D in memory. Such example operation of D is like the negation of the vector, permutation of the vector elements, vector cross-product or convolutions. The core operation can be represented by much much less variables than the full expression of D as a matrix. From the product of D*e, then you can apply C,B,A consecutively (without explicit expression) to obtain g. This concept appears often in linear algebra. When you only require the product of a matrix and a vector, but not the matrix explicitly, it is way easier to implement the computation in program and have much lower memory requirement. It just happens that this concept applies to AD calculation.
