[site]: crossvalidated
[post_id]: 492900
[parent_id]: 492896
[tags]: 
For instance, would it make sense or add any benefit to train multiple RFCs on different subsets of a training data set, [...] Random forest does by default train each tree on a different subset of data. I can't see how doubling this procedure would add anything. Or, would it be sufficient to do a random train/test split on a percentage of a training data set and train a single RFC on the split data, but do so X number of times, then select the RFC model that ends up with the highest overall accuracy on the test data to use for the final model? [...] This sounds like a bad idea. One thing is $k$ -fold cross-validation , where we can overcome the problem of having small dataset when doing the split to train and test. What you propose is to cherry-pick the model that has best test set performance among the splits. This can easily lead to ending up with a result "overfitting to test set", because you'd pick the one that was trained on the most lucky train/test split, where the test set contained the most easy to classify examples among the splits. So this does not seem to make much sense as well.
