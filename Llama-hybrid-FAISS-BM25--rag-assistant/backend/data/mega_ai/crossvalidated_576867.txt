[site]: crossvalidated
[post_id]: 576867
[parent_id]: 
[tags]: 
Significance test for accuracy - normal assumption

I have a dataset with pre-defined labels. I trained two machine learning classifier methods A and B to predict the labels and calculated the accuracy for each. I varied the dataset a bit and thus got multiple experiments with multiple accuracy results. I calculated the mean and the variance for the accuracy values of the classifier methods: $m_A, m_B, v_A, v_B$ . I want to perform a significance test using p-values. For that I need to assume a distribution for the accuracy. For example if I assume that the accuracy values for method A are normal distributed and the $m_B$ is not in the range of $m_A\pm 1.92 \cdot v_A$ , then is I can reject the null hypothesis with at least 0.05 confidence that method B has the same accuracy as method A. But is my assumption that the accuracy is normal distributed correct?
