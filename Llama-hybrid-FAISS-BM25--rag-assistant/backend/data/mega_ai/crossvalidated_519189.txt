[site]: crossvalidated
[post_id]: 519189
[parent_id]: 261704
[tags]: 
Here is a more information-theoretic reason for why this is happening. Let $Y$ be the true and correct output that your network should return (the target), and let $\hat{Y}$ be the output that your network actually returns. The problem that you observe is that $\hat{Y}=K$ , where $K$ is some constant, which means that $Y$ is independent of $\hat{Y}$ . If we compute the mutual information between $Y$ and $\hat{Y}$ , we observe that: $$ I(Y;\hat{Y}) = h(Y) - h(Y|\hat{Y}) = h(Y) - h(Y) = 0 $$ Where $h(\cdot)$ is the differential entropy and $h(Y|\hat{Y})=h(Y)$ since $Y$ and $\hat{Y}$ are independent . In other words, if we assume that $Y \rightarrow \hat{Y}$ is a communications channel, then there is no information flowing between the input of the channel $Y$ and the output of the channel $\hat{Y}$ ! A more accurate channel would be the Markov chain $Y \rightarrow X \rightarrow \hat{Y}$ , where $X$ is the input to the network. However, by the data processing inequality : $$ I(Y;\hat{Y}) \leq I(Y;X) \\ I(Y;\hat{Y}) \leq I(X;\hat{Y}) $$ Which are just upper-bounds on the mutual information $I(Y;\hat{Y})$ . Our objective is to increase the mutual information $I(Y;\hat{Y})$ , which can be done by increasing a lower-bound on $I(Y;\hat{Y})$ . So, we are not interested in $I(Y;X)$ and $I(X;\hat{Y})$ at this time. Now let the mean-squared error between $Y$ and $\hat{Y}$ be: $$ \mathbb{E}\left[(Y-\hat{Y})^2\right] = \delta $$ Now, to find a lower-bound on $I(Y;\hat{Y})$ , note that: $$ \begin{align} I(Y;\hat{Y}) &= h(Y) - h(Y|\hat{Y}) \\ &= h(Y) - h(Y-\hat{Y}|\hat{Y}) \tag{1} \\ &\geq h(Y) - h(Y-\hat{Y}) \tag{2} \\ &\geq h(Y) - \frac{1}{2} \log{(2\pi e \delta)} \tag{3} \end{align} $$ Where: $(1)$ is because differential entropy is translation invariant . $(2)$ is because conditioning reduces entropy . $(3)$ is because random variable $Z=Y-\hat{Y}$ has variance $\mathbb{E}[(Z-\mathbb{E}[Z])^2]=\mathbb{E}\left[(Y-\hat{Y})^2\right] = \delta$ , and so this is the Gaussian bound on differential entropy. So: $$ I(Y;\hat{Y}) \geq h(Y) - \frac{1}{2} \log{(2\pi e \delta)} $$ If we further assume that the target $Y$ has mean $\mathbb{E}[Y]=0$ and variance $\text{Var}(Y)=\sigma^2$ , then we get the tighter lower-bound: $$ I(Y;\hat{Y}) \geq \frac{1}{2} \log{(2\pi e \sigma^2)} - \frac{1}{2} \log{(2\pi e \delta)} \geq h(Y) - \frac{1}{2} \log{(2\pi e \delta)} $$ And so we can increase the mutual information by either increasing the variance $\sigma^2$ of $Y$ or decreasing the mean squared-error $\delta$ . Unfortunately, we do not usually have the ability to adjust the variance of $Y$ , so we would need to resort to decreasing $\delta$ . Interestingly, we have just shown that minimizing the mean squared-error $\delta$ corresponds to maximizing the mutual information between $Y$ and $\hat{Y}$ , and vice-versa. At first glance, it seems like there is a contradiction here: you clearly can achieve small mean squared-error values, which means that the mutual information between $Y$ and $\hat{Y}$ is maximized. However, what you practically observe is that the mutual information between $Y$ and $\hat{Y}$ is $0$ . To resolve this contradiction, it is helpful to take a closer look at the mean squared-error between $Y$ and $\hat{Y}$ . We know that : $$ \mathbb{E}\left[(Y-\hat{Y})^2\right] = \text{Var}(\hat{Y}) + \text{Bias}(\hat{Y},Y)^2 $$ Since both of these terms are non-negative, then the variance and bias of $\hat{Y}$ each individually form lower-bounds for the mean squared-error: $$ \mathbb{E}\left[(Y-\hat{Y})^2\right] \geq \text{Var}(\hat{Y}) \\ \mathbb{E}\left[(Y-\hat{Y})^2\right] \geq \text{Bias}(\hat{Y},Y)^2 $$ And therefore the true lower-bounds on the mutual information are: $$ I(Y;\hat{Y}) \geq h(Y) - \frac{1}{2} \log{(2\pi e \cdot \text{Var}(\hat{Y}))} \geq h(Y) - \frac{1}{2} \log{(2\pi e \delta)} \\ I(Y;\hat{Y}) \geq h(Y) - \frac{1}{2} \log{(2\pi e \cdot \text{Bias}(\hat{Y},Y)^2)} \geq h(Y) - \frac{1}{2} \log{(2\pi e \delta)} $$ These new lower-bounds indicate that although your mean squared-error values were small, they were not small enough , as they are upper-bounded by the variance and bias of $\hat{Y}$ . Therefore, what we really want to do to maximize the mutual information between $Y$ and $\hat{Y}$ is to decrease the variance and bias of $\hat{Y}$ . Unfortunately, we know that there is a trade-off between the variance and bias of $\hat{Y}$ .
