[site]: datascience
[post_id]: 72691
[parent_id]: 72682
[tags]: 
The important is not the number of observation but the quality of this observations. If you have a look at toy datasets of sklearn they are way smaller than that. Random forest is a good algorithm when there is small data since it is a bagging of decision trees with bootstrap. Each decision tree is feed with a sample of data with replacement, in this way even if the data is small there are bigger chances of making a good model. In a high level, yes it seems a good way to go, but with out knowing more at the data is hard to tell. I would suggest to give it a try with a Generalized Linear Model, a support vector machine and a gradient boosting. Since your data is small you will not need much computation time for it.
