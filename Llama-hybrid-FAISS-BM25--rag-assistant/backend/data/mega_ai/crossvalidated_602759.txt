[site]: crossvalidated
[post_id]: 602759
[parent_id]: 396838
[tags]: 
No, they don't. The purpose or effect of batch normalization of a given activation in a neural network, is essentially to reduce internal covariance shift of that activation. Arguably, this "internal covariance shift" is not explained well in the 2015 Batch Norm paper. Here I think it means, mathematically, distribution of input of that activation changes per sample batch, because of nonlinearity of previous layers this input comes from. As a result, gradient updates have to reflect that distribution change when it is the wrong the to do as it does not necessarily help move weights to the correct direction of reducing loss or optimization objectivity. You already know it is beneficial to introduce scaling and shifting post-batch-normalization - to fully utilize nonliearity of network or to avoid 0 proximity in nonlinear function such as sigmoid. This scalar and shifter decouple the activation's distribution change from previous layers , thus reducing so-called covariance shift . Therefore, the activation is still "normalized" - its distribution is "fixed" thru the scaler and shifter. And although they are still subject to change as they are learned thru gradient descent as well during learning/training, you can probably see their changes are a lot slower and more steady compared to as if there's no batch-normalization and distribution depends on previous layers's nonlinearity.
