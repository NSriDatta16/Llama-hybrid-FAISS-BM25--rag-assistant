[site]: crossvalidated
[post_id]: 505857
[parent_id]: 505854
[tags]: 
I don't know if this is the best method , but one method I've used in the past is to train an Autoencoder for this task. How this works Train an Autoencoder (AE) to model your data $X$ . What you essentially want from this step is for the AE to learn the distribution of $X$ . We will use this to examine if every new sample belongs to the same distribution or not. When a new input $x$ arrives, feed it to the AE. Monitor its reconstruction loss: if it is low , you can consider that it comes from the same distribution as $X$ , because the AE was able to model it. if it is high , then the AE wasn't able to model it and you can consider that it doesn't come from the same distribution as $X$ . If the reconstruction error is low, feed it to your original estimator and generate a prediction $\hat y$ . Benefits The main benefits of this approach arise from the fact that Autoencoders can be relatively strong (i.e. high-capacity) models. Requires less assumptions than some of the previous methods you mentioned. Can be used in tougher situations (i.e. unstructured data). You can build an Autoencoder with various types of layers (e.g. fully connected, convolutional, recurrent), which makes it capable of modelling various types of input data (images, text, speech, etc.). Caveats Much more computationally expensive than your methods (requires the training of a Neural Network to work). Requires some degree of expertise with Neural Networks (what type of layers to use, number of layers, size of each layer, activations, etc.). Requires some tuning (mostly for the threshold on the reconstruction loss).
