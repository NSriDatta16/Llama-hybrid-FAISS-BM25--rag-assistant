[site]: datascience
[post_id]: 11995
[parent_id]: 10840
[tags]: 
The best way to look at this problem is to consider the extreme case: let's say you duplicate every row in your dataset. If you train a decision tree directly, there are certain hyperparameters you will want to change. For instance, in sklearn , you will want to double hyperparameters like min_samples_split or min_samples_leaf because in each node you'll have double the observations. With regards to a random forest, usually, a random forest is learn by bootstrapping with resampling=True in the entire dataset. By doubling every observation, you make each observation just as likely to be picked. Even if you use a random forest which subsamples, doubling the dataset makes no difference. Now, you are not doubling your data. You are introducing new data which might change the distribution of your data. To the extent that it does, then it will mean that whatever hyperparameters you have previously used might no longer be optimal. So you should just try. But, to answer your question, no .
