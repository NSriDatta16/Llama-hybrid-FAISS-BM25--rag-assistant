[site]: crossvalidated
[post_id]: 33967
[parent_id]: 
[tags]: 
Dynamically adjusting NN architecture: inventing the unnecessary?

I am starting out on my PhD journey, and the ultimate goal that I set before myself is developing ANNs that would monitor the environment they work in and dynamically adjust their architecture to the problem at hand. The obvious implication is temporality of data: if the data set is not continuous and doesn't change over time, why adjust at all? The big question is: with the recent rise of deep learning, is it still a relevant topic? Do FFNNs stand a chance to find themselves a niche in concept drift problems? I fear to overload the thread with too many a question, but this one isn't entirely off-topic: I am aware of RNNs, but I have limited (ok, none, or purely theoretical) experience with them; I believe dynamic architecture adaptation must be a relevant topic in the context of RNNs. The question is, has it already been answered, and will I be reinventing the wheel? P.S. Cross-posted to MetaOptimize
