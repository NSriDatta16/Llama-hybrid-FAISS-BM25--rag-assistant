[site]: datascience
[post_id]: 66944
[parent_id]: 
[tags]: 
Is it wrong to use Glorot Initialization with ReLu Activation?

I'm reading that keras' default initialization is glorot_uniform . However, all of the tutorials I see are using relu activation as the go-to for hidden layers, yet I do not see them specifying initialization for those layers as he . Would it be better for these relu layers to use he instead of glorot? As seen in OReilly's Hands-On Machine Learning with Scikit-Learn & Tensorflow : | initialization | activation | +----------------+-------------------------------+ | glorot | none, tanh, logistic, softmax | | he | relu & variants | | lecun | selu |
