[site]: datascience
[post_id]: 36138
[parent_id]: 
[tags]: 
Text representation using TFIDF .toarray() freezes my computer. Too many features to handle?

I'm new to Data Science, so hopefully this question makes sense. I have a dataset with ~50,000 rows. It consists of one column that has item category in it and one column with item description in it. I'm trying to get bigrams from the item description with the highest correlation to each category. i'm using TfidfVectorizer to do this. I compress the item descriptions into an array using this: features = tfidf.fit_transform(df.ItemDescription).toarray() In Spyder, when i even click on the features under variable explorer, the memory immediately shoots up to 99% and my computer freezes. If I try to call features at all, it will also freeze. Here is how i'm calling features later in my code after making it into an array: from sklearn.feature_selection import chi2 import numpy as np N = 2 for UNSPSC, category_id in sorted(category_to_id.items()): features_chi2 = chi2(features, labels == category_id) The array size is 50000X12000 Is that too big? Is there anything I can do to get the bigrams without my computer freezing? As a last resort, I will trim down my data. But i feel like that will reduce the accuracy of any model i try to train later. Please let me know if you need full code.
