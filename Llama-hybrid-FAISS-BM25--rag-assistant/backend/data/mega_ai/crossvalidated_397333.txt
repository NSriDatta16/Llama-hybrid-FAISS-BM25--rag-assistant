[site]: crossvalidated
[post_id]: 397333
[parent_id]: 397329
[tags]: 
I don't think the answer is wrong. The variance explained by vector -0.4 & 0.8 is the maximum. You are interpreting the results wrongly. If you read the documentation each column corresponds to 1 eigenvector. To prove your correctness I tried doing the same thing with scikit-learn PCA. from sklearn.decomposition import PCA import numpy as np p = PCA() x = [1, 2, 3, 4, 5] y = [10, 7, 6, 4, 2] x = x-np.mean(x) y = y-np.mean(y) X = np.array(list(zip(x, y))) p.fit(X) ind = 0 for eigen_vec in p.components_: print("Eigenvector : ",eigen_vec) print("Variance Explained : ",p.explained_variance_[ind]) print("Ratio of Variance Explained : ",p.explained_variance_[ind]) ind=ind+1 The output obtained was the following:- Eigenvector : [-0.46024699 0.88779092] Variance Explained : 11.662486559124249 Ratio of Variance Explained : 0.9967937230020721 Eigenvector : [-0.88779092 -0.46024699] Variance Explained : 0.03751344087575544 Ratio of Variance Explained : 0.0032062769979278143
