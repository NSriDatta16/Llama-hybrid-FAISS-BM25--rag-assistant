[site]: crossvalidated
[post_id]: 299253
[parent_id]: 298509
[tags]: 
$\newcommand{\dbeta}{\frac{\partial}{\partial \lambda} \hat\beta_\lambda}$ $\newcommand{\ddbeta}{\frac{\partial^2}{{\partial \lambda}^2} \hat\beta_\lambda}$ This answer specifically concerns the lasso (and does not hold for ridge regression.) Setup Suppose that we have $p$ covariates that we're using to model a response. Suppose that we have $n$ training data points and $m$ validation data points. Let the training input be $X_{(1)} \in \mathbb{R}^{n \times p}$ and response be $y_{(1)} \in \mathbb{R}^n$. We will use the lasso on this training data. That is, put $$\hat\beta_\lambda = \arg\min_{\beta \in \mathbb{R}^p} \|y_{(1)} - X_{(1)} \beta\|_2^2 + \lambda \|\beta\|_1, \tag{1}$$ a family of coefficients estimated from the training data. We will choose which $\hat\beta_\lambda$ to use as our estimator based on its error on a validation set, with input $X_{(2)} \in \mathbb{R}^{m \times p}$ and response $y_{(2)} \in \mathbb{R}^m$. With $$\hat\lambda = \arg\min_{\lambda \in \mathbb{R}_+} \|y_{(2)} - X_{(2)} \hat\beta_\lambda\|_2^2, \tag{2}$$ we are interested in studying the error function $e(\lambda) = \|y_{(2)} - X_{(2)} \hat\beta_\lambda\|_2^2$ which gives rise to our data-driven estimator $\hat\beta_{\hat\lambda}$. Calculation Now, we will calculate the second derivative of the objective in equation $(2)$, without making any distributional assumptions on the $X$'s or $y$'s. Using differentiation and some reorganization, we (formally) compute that \begin{align*} \frac{\partial^2}{{\partial \lambda}^2} \|y_{(2)} - X_{(2)} \hat\beta_\lambda\|_2^2 & = \frac{\partial}{\partial \lambda} \left\{ -2 y_{(2)}^T X_{(2)} \dbeta + 2 \hat\beta_\lambda^T X_{(2)}^T X_{(2)} \dbeta \right\} \\ & = -2 y_{(2)}^T X_{(2)} \ddbeta + 2 \left( \hat\beta_\lambda \right)^T X_{(2)}^T X_{(2)} \ddbeta + 2 \dbeta^T X_{(2)}^T X_{(2)}^T \dbeta \\ & = -2 \left\{ \left( y_{(2)} - X_{(2)} \hat\beta_\lambda \right)^T \ddbeta - \|X_{(2)} \dbeta\|_2^2 \right\}. \end{align*} Since $\hat\beta_\lambda$ is piecewise linear for $\lambda \not\in K$ (for $K$ being the finite set of knots in the lasso solution path), the derivative $\dbeta$ is piecewise constant and $\ddbeta$ is zero for all $\lambda \not\in K$. Therefore, $$\frac{\partial^2}{{\partial \lambda}^2} \|y_{(2)} - X_{(2)} \hat\beta_\lambda\|_2^2 = 2 \|X_{(2)} \dbeta\|_2^2,$$ a non-negative function of $\lambda$. Conclusion If we assume further that $X_{(2)}$ is drawn from some continuous distribution independent of $\{X_{(1)}, y_{(1)} \}$, the vector $X_{(2)} \dbeta \neq 0$ almost surely for $\lambda Finally, from the lasso dual, we know that $\|X_{(1)} \hat\beta_\lambda\|_2^2$ decreases monotonically as $\lambda$ increases. If we can establish that $\|X_{(2)} \hat\beta_\lambda\|_2^2$ is also monotonic, then the strong convexity of $e(\lambda)$ follows. However, this holds with some probability approaching one if $\mathcal{L} \left( X_{(1)} \right) = \mathcal{L} \left( X_{(2)} \right)$. (I'll fill in details here soon.)
