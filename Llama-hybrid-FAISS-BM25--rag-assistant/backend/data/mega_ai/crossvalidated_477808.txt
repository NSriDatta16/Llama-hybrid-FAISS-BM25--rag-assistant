[site]: crossvalidated
[post_id]: 477808
[parent_id]: 
[tags]: 
Why is the predicted variable always biased towards center?

I am working on a prediction task where the output variable is a continuous unbounded variable. The original variable is a scale on 1-10. However, I did not bound the range on the output variable. Below is the frequency distribution for the original variable. 0 1098 1 622 2 468 3 321 4 185 5 509 6 225 7 355 8 429 9 338 10 1508 I have discretized the variable on the scale 1-3. Now below is how it looks. 1 2509 2 1274 3 2275 Surprisingly, the predicted variable follows the below frequency distribution. I have tried regression forest, XGBoost, MLP and they all show the same trend. 0.0 1 1.0 162 2.0 303 3.0 138 4.0 2 Besides the fact that the model did not learn much, the output is always biased towards the center whereas the original output variable has exactly the opposite frequency distribution. I was wondering if I am missing something obvious behind this specific trend. There are 150 predictor variables and a lot of those are one-hot encoded. I have tried the original set of variables and the reduced dimension by an autoencoder. Although the performance slightly improved after autoencoder which is 30% accuracy, the trend on the output remains the same for all the algorithms and original and various projected input space. Could anyone shed some light on this behavior? Below is the confusion metric if that is of any help. Predicted 0.0 1.0 2.0 3.0 4.0 All Actual 1.0 1 67 112 50 0 230 2.0 0 29 72 31 0 132 3.0 0 66 119 57 2 244 All 1 162 303 138 2 606
