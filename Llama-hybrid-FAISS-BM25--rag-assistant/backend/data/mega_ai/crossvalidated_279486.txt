[site]: crossvalidated
[post_id]: 279486
[parent_id]: 
[tags]: 
Can Margin of Error be calculated from previous accuracy?

I'm a complete stats newbie, but I got into a discussion on the following article about polling accuracy . The author looks through the historic accuracy of UK pollsters, then finds the average prediction is 6% off the final result. From there he extrapolates that: But if polls are missing election outcomes by 5 or 6 points on average, that means the margin of error (or 95 percent confidence interval) is very large indeed. Specifically, a 6-point average error in forecasting the final margin translates to a true margin of error of plus or minus 13 to 15 percentage points, depending on how you calculate it. I went and looked into this as I wanted to check for myself and I don't see how you can calculate a Margin of Error from the actual error rates. It seems to me like this would be calculating the odds of a coin toss that has already happened and we know the results of, but the author is just a little bit more accomplished as a statistician than I am. How do you calculate the MoE from these data? Is it possible or is he just editorialising?
