[site]: crossvalidated
[post_id]: 213916
[parent_id]: 56831
[tags]: 
I think I understand your problem is one of a model building approach and how to gauge whether a difference in the $R^2$ value, before and after adjusting for a certain variable, is of notable magnitude rather than a consequence of spurious associations in the data. This calls to mind information criteria (AIC and BIC) which encourage parsimonious models by penalizing the likelihood by a number of parameters. In OLS models, the $R^2$ is related to the likelihood, so if the approach stems from this problems, there have been other developments. To address your specific question: One minimal assumption is that any subsequent covariate must not be collinear with covariates in your provisional model ($k$ taking any one of $1, 2, \ldots, p-1$). This assumption allows the inequality to be strict. Nothing further may be said in general. This is because the change $R_{k+1}^2 - R_{k}^2$ will be a function of the residuals $r_k^2$ or design $\mathbf{X}_k$ for a provisional model as well as the distribution of $x_{k+1}$. I suspect it is possible to choose sequences of either $\{x_{k+1}\}_i$ or $\{r_k\}_i$ such that $R_{k+1}^2 - R_{k}^2 \rightarrow_p 0$. To see this, consider $x_{k+1}$ an indicator function for the 1st observation, and take $y_1 \rightarrow x_{k+1} \mathbf{X}_k \beta$. Then $R_{k+1}^2 - R_{k}^2 \rightarrow_p 0$. Furthermore, conditioning on all these factors is trivial since it's equivalent to conducting the very inference you are interested in flat-out. It can be said however, that with a full rank design matrix of rank $n$, you will have $R^2=1$ exactly, but that also is trivial and does not shed any light on any of the previous covariates. If you were interested in the random process of including subsequent, unrelated vectors to a model there might be some specific derivations under a number of highly sensitive assumptions like multivariate normality, independence or orthogonality.
