[site]: crossvalidated
[post_id]: 265509
[parent_id]: 
[tags]: 
Difference between when a networks stops training because "minimum gradient reached" and when it stops training because "validation stop"

I'm using the neural networks functions in Matlab ( patternnet with trainFcn = trainlm ) to classify some data into 5 types. After running the network several times on the same datasets I realized that sometimes it stops training because "minimum gradient reached" and sometimes because "validation stop". Whenever it stops because "minimum gradient reached", the errors (mse) for training, validation and testing are very low (almost zero) and whenever it stops because "validation stop" all 3 errors are ridiculously high (anywhere from 12% to 36%). Moreover, when I increased the max_fail value for validation to 20 (default is 6), training stopped because "min gradient reached" every time, but the errors were low (almost 0) only when the number of validation fails was less than 6. Whenever validation fails exceeded 6, the errors were high even though training stopped because of "min gradient reached". I need to know why this is happening.
