[site]: crossvalidated
[post_id]: 514281
[parent_id]: 513425
[tags]: 
Can't really give you a rule of thumb along the lines that your are looking for, but I've dealt with similar challenges before. There might be a better approach than trying to figure out the interplay between corpus size, token size, and the type of embedding you're trying to create. You can take one of the pertained general embeddings for a language (see here , for embeddings that can be directly imported into a Tensorflow script) and then fine tune it to your specific corpus . This is assuming your language has such published embeddings. In my past experience, in some cases, even the fine-tuning wasn't necessary, the generic language embeddings did a good enough job of representing the documents that even fine tuning wasn't necessary. Typically fine tuning a pre-existing embedding requires less data than training a domain specific custom embedding.
