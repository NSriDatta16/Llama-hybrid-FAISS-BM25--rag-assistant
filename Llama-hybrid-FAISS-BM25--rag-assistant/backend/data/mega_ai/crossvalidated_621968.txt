[site]: crossvalidated
[post_id]: 621968
[parent_id]: 621967
[tags]: 
First, as an aside, unless these variables are well known, I would multiply x1 and x2 by 1000. This won't change the meaning of anything, but it will make interpretation easier. (You could ask "how many kilometer is it from your living room to your bedroom?" but usually you would use meters). Second, a low p value doesn't necessarily mean that the second variable improves the model much, especially if you have a large sample. P values are strongly affected by sample size. Third, maybe you know this already, but a high correlation (or, in cases with more than 2 IVs, high collinearity - they aren't the same when there are more IVs), means that the information in one variable is not much different than the information in the other. This can have weird effects on parameter estimates, standard errors, p values, and so on. It can also mean that tiny changes in the input (data) make huge changes in the output (model). In one of his books, Belsley shows a case where changing the 3rd or 4th significant figure in the data can change a parameter estimate from being significant and positive to significant and negative. What you are doing when you have two (or more) variables in a model is controlling for each. But what is x1 controlling for x2 in your case? Or x2 controlling for x1? It's the part of x1 that is not contained in x2. This is likely to be noise. Fourth, if you do PCA (which can be good) you will not be able to say which variable is adding things. If you do want to see that then, yes, I would recommend ridge regression.
