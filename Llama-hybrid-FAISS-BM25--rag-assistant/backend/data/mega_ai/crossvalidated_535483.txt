[site]: crossvalidated
[post_id]: 535483
[parent_id]: 
[tags]: 
How to improve model generalization with small dataset?

I have a fairly small dataset (45 data points (i.e populations) where I took plant information. I'm running a random forest regression on my measurements and climate information to predict a feature, but I've come into an issue. When I separate the data into test and training sets I either get a good model or a really bad model (as dictated by R square values). I've run this using both a single initial split and using K-Fold Cross Validation, using either K = 5 or 10. In either case, R square values are always great for the training data, but either pretty good or terrible for the test data. I believe this is because when the data is split into training and testing groups, one or the other sometimes becomes unrepresentative of the population data. Hyperparaterization, and feature reduction help, but it still comes down to how the data is split in that first step. What can I do?
