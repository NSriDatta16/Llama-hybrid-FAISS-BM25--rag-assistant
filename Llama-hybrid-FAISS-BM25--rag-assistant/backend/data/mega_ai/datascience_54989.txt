[site]: datascience
[post_id]: 54989
[parent_id]: 41769
[tags]: 
It depends on the model, whether your training data is representative of the testing data, and possibly on how "easy" the classification problem is. But the short answer is that (probably) no adjustment is necessary. The model is exposed to the imbalance in the training data; most models will have that baked into their probability scores, so to trust the given probabilities requires that the testing data have a similar distribution. This goes to what exactly we mean by "the probability" that the new car is blue; I think in most cases, you would want the training data to be representative of the data in your use-case, so that we really do want the probability to be aware of the imbalance. [Now, sometimes in the presence of extreme imbalance, certain models might learn better with some adjustment, in which case you'll want also some "probability calibration" post-processing, but that's generally more complex than the simple multiply-or-divide method you suggest. In logistic regression for example, there's a well-known additive adjustment to the constant term, so that turns into a multiplicative adjustment to the log-odds, which does not translate into such a nice transformation on probabilities.] Finally, some models implicitly skew their scores, so predict_proba should be taken with a fair bit of skepticism. Again, look up "probability calibration."
