[site]: crossvalidated
[post_id]: 447145
[parent_id]: 
[tags]: 
Is average standard deviation a good way to measure forecasts accuracy?

I have a database of forecasts (ranging from 1 week to 4 weeks in advance) and one of experimentally recorded values for a specific index whose value ranges from 1 to 9 (most of the time it measures 2 or 3, with values 5 and upwards becoming exponentially more rare). I guess I should compare each delta-time separately (1-week, 2-weeks, etc.). I'd like to get an idea on how accurate these forecasts tend to be. Would average standard deviation be the best way? Or would you suggest something else? Thank you Edit: given two sets of variables, A being forecasted number and B number measured, I was thinking about assessing forecast accuracy by doing something like: $$ A_1 - B_1 = |C_1|$$ $$ A_n - B_n = |C_n|$$ $$ \frac{\sum_i |C_i| }{n} = D$$ And then...I don't know. Advice welcome. Thank you
