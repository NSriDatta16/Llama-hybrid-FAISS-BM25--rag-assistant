[site]: crossvalidated
[post_id]: 370945
[parent_id]: 369827
[tags]: 
The so-called 'Bayesian p-value' does not have the same interpretation as a true p-value: remember that you do not have a formal hypothesis test so there is no real concept of a 'probability of the data under the null hypothesis'. Instead, the probability that you are monitoring is simply the probability that the coefficient is greater than zero - which is arguably easier to interpret (and more useful) than a p-value ... but that's an entirely unrelated rant. The answer is that your intuition is correct - you can simply take 1 minus the probability that the coefficient is positive to obtain the probability that the coefficient is negative, from which you could conclude (if this was probability was sufficiently small) that the coefficient most likely has a positive value. There is nothing wrong with this nor any particular reason why you could/should not (equivalently) monitor 1 minus the step function in the model. But - here comes a more relevant rant - why are you focussed on imitating a p-value at all? It makes much more sense to look at the 95% credible intervals for the coefficient posterior directly, from which you can ascertain (1) if the 95% confidence interval does not include zero (i.e. the true value is likely either positive or negative - i.e. there is a true effect) but also importantly (2) the magnitude of the effect i.e. is it very likely to be a small (and potentially meaningless) effect or is it potentially a large (and therefore important) effect. Imitating a p-value loses this valuable information.
