[site]: crossvalidated
[post_id]: 631450
[parent_id]: 
[tags]: 
Weighing hypotheses after seeing the data vs. Consistent learning

In the chapter about nonuniform learning in the book " Understanding Machine Learning , S. Ben David et al." (its free online version here ), the authors wrote: if we commit to any hypothesis before seeing the data , then we are guaranteed a rather small estimation error term $$ L_\mathcal{D}(h)\le L_S(h) + \sqrt{\frac{log(2/\epsilon)}{2m}}. $$ The statement clearly suggests that the error bound might be different if we still commit to the same hypothesis but after seeing the data. However, I don't know what the authors mean by "after seeing the data". I think that committing to a hypothesis after seeing the data would change the learning setting from non-uniform learning to consistent learning but I'm not sure it's correct or not?
