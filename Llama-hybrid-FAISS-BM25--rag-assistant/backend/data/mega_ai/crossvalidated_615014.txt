[site]: crossvalidated
[post_id]: 615014
[parent_id]: 395038
[tags]: 
"But by marginalising over , have we not lost the information that would have enabled that?" I'll try and provide insight into why the answer to this is no. If we were to consider the parameters $\theta$ as a single point, we could fit a model (for example) via maximum likelihood estimation to yield $$ p(y|\theta_{\text{ML}}) $$ Realize that we simply ignore the idea that there is randomness in $\theta$ . In a Bayesian treatment, we don't ignore this, and we instead seek to find a distribution over $\theta$ rather than a point estimate for $\theta$ . The distribution that takes into account the observed dataset we have at hand is called the "posterior" and I will denote it as $p(\theta|\pmb{\mathsf{y}})$ . Then, if we are interested in prediction, we compute the posterior predictive by marginalizing over all possible settings of $\theta$ contained in the posterior $p(\theta|\pmb{\mathsf{y}})$ : $$ p(y|\pmb{\mathsf{y}}) = \int p(y|\theta)p(\theta|\pmb{\mathsf{y}}) \text{d}\theta $$ The MLE fitted model $p(y| \theta_{\text{ML}})$ considers a fixed $\theta$ and looks at the distribution over $y$ given such a fixed setting of $\theta$ . The Bayesian posterior predictive $p(y|\pmb{\mathsf{y}})$ considers a random $\theta$ and looks at the distribution over $y$ after considering all possible $\theta$ as per the posterior. Intuitively, it should have "more randomness" or "uncertainty" and it does. Namely, it has introduced "model" or "epistemic" uncertainty -- uncertainty coming from many different settings of $\theta$ that induce a model $p(y|\theta)$ that can explain our dataset at hand. The variance of $p(y|\pmb{\mathsf{y}})$ will necessarily be greater than or equal to $p(y|\theta_{\text{ML}})$ . Maybe the (very simplified) visualization below can help Also see Pattern Recognition and Machine Learning (Chris M. Bishop) https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf . Specifically, section 1.2.5.
