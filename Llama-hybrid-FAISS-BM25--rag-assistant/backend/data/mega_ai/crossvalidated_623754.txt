[site]: crossvalidated
[post_id]: 623754
[parent_id]: 
[tags]: 
Mincer-Zarnowitz test with cointegrated time series

The Mincer-Zarnowitz test of forecast optimality regresses forecast errors $e_{t+h|t}$ on the forecasts $\hat y_{t+h|t}$ , $$ e_{t+h|t} = \gamma_0 + \gamma_1\hat y_{t+h|t} + u_t \tag{1} $$ or in another formulation, realized values $y_{t+h}$ on the forecasts $\hat y_{t+h|t}$ , $$ y_{t+h} = \delta_0 + \delta_1\hat y_{t+h|t} + u_t \tag{2} $$ (See p. 337 of Diebold "Forecasting in Economics, Business, Finance and Beyond" (2017) for details.) Suppose the realized values $y_t$ and the forecasts $y_{t+h|t}$ are each integrated. Further suppose they are cointegrated with a cointegrating vector $\beta=(1,-\beta_1)^\top$ . (For simplicity, we could assume $\beta_1=1$ .) Unless $\gamma_1=\beta_1-1$ or equivalently, $\delta_1=\beta_1$ , we get an unbalanced regression. How does this affect the distribution of the test statistic under $H_0\colon\ \delta_0=0,\ \delta_1=1$ , or equivalently $H_0\colon\ \gamma_0=0,\ \gamma_1=0$ ? Can "naive" critical values be used under such circumstances? If not, is there a way around the problem?
