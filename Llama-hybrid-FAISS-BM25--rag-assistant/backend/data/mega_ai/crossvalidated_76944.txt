[site]: crossvalidated
[post_id]: 76944
[parent_id]: 46442
[tags]: 
What you're describing would send your gram matrix to a much larger feature space than a single kernel. Consider the following. Given two valid mercer kernels, $\alpha_1K_1(x_i,x_j) + \alpha_2K_2(x_i, x_j)$ is a valid kernel for all $\alpha_1,\alpha_2\in\mathbb R^+$. Similarly $K_1(x_i,x_j)K_2(x_i,x_j)$ is also a valid kernel. These results imply that arbitrary polynomial expansion can be applied to kernels, allowing for interaction between kernels. You can get the desired result, at the normal speed of evaluating a gram matrix. See: Classes of Kernels for Machine Learning: A Statistics Perspective
