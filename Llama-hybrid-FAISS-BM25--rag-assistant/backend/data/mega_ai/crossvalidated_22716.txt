[site]: crossvalidated
[post_id]: 22716
[parent_id]: 
[tags]: 
How to deal with RAM limitations when working with big datasets in R?

I am currently playing around with the MNIST dataset ( http://yann.lecun.com/exdb/mnist/ ) in R. The training set size is 60000x748 and it seems to drain all my memory even when constructing simple models like logistic regression. My question is: how do you guys usually deal with big datasets in R? And tangent: is it feasible to break the dataset into smaller chunks, construct a model on each, then perform a weighted average on the predicted values?
