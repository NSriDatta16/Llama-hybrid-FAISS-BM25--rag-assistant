[site]: datascience
[post_id]: 54599
[parent_id]: 54594
[tags]: 
I'd say you're on the right track. If your intention were to classify rows within a group based on a strict ordering of the columns (i.e. top in m5 is definitely top, and on from there) then it would just be a matter of sorting using d.sort_values(by=["L1","m5",...]) . But it seems like you're definitely interested in applying some weights to the columns and then coming up with a weighted score for each row. I think a good option for you is to use principal components analysis (PCA) on the columns of interest and generate a score from that. PCA is a dimensionality reduction method. In a nutshell, it analyzes the covariance between a set of variables and looks for common underlying patterns (e.g. when m5 is up m3 and m2 go down,...) each pattern is a component which can usually be tied to trends in the data. You find a better explanation here . To use this on your data simply: from sklearn.decomposition import PCA # you have to decide the number of components you want to use (less than # columns) pca = PCA(n_components=2) # get all the "m" columns assuming those are the ones you care about X = d[[c for c in d.columns if 'm' in c]] # fit the PCA model to your data pca.fit(X) # apply the dimensionality reduction (predict/transform) and attach it to the data d = pd.concat([d,pca.transform(X)],axis=1) The advantage of PCA is that it provides you with a weight for each component based on how much of the variance in the variables they explain. pca.explained_variance_ratio_ You can multiply each weight with each component in the transformed data, get the weighted sum across rows and the sort by group and the weighted sum. You could apply the PCA to each group individually by creating subsets of your data ( X=d[d['L1']=='Group1'] ) but more data will yield a more reliable PCA result unless there is some remarkably pronounced difference in what you expect from each group. Hope this helps.
