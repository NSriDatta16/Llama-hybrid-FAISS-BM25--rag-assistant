[site]: crossvalidated
[post_id]: 551268
[parent_id]: 551241
[tags]: 
Here are several possible approaches, which may illustrate the difficulty in finding significance among small differences in group means, when sample sizes are as small as yours. Fictitious data. Suppose we have fictitious gamma distributed data sampled in R, with group sizes $3, 15,$ and $11,$ as you specify, and population means far enough apart to see some significant effects. [Of course, in an actual application, we would never know the distributions of the three populations for sure.] set.seed(2021) x1 = rgamma( 3, 3, .1) x2 = rgamma(15, 3, .3) x3 = rgamma(11, 3, .5) summary(x1); length(x1); sd(x1) Min. 1st Qu. Median Mean 3rd Qu. Max. 20.47 21.78 23.10 24.86 27.06 31.01 [1] 3 # sample size [1] 5.486118 # sample SD summary(x2); length(x2); sd(x2) Min. 1st Qu. Median Mean 3rd Qu. Max. 1.281 4.466 9.253 10.702 18.199 19.945 [1] 15 [1] 7.350433 summary(x3); length(x3); sd(x3) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.5353 2.0220 4.5508 4.8006 6.6338 10.7546 [1] 11 [1] 3.351256 Here is are boxplots of the three sample; narrower boxes indicate smaller sample sizes. Red X 's show sample means. [With only three observations, the boxplot for x1 is problematic.] x = c(x1,x2,x3) g = rep(1:3, c(3,15,11)) boxplot(x~g, horizontal=T, varwidth=T, col="skyblue2") points(c(mean(x1),mean(x2),mean(x3)), 1:3, pch="X", col="red") Results from a Kruskal-Wallis test. A Kruskal-Wallis test shows clearly (P-value $0.005 that at least one of the groups stochastically dominates another. [Because sample SDs differ so widely it is not strictly accurate to say that population medians differ.] kruskal.test(x ~ g) Kruskal-Wallis rank sum test data: x by g Kruskal-Wallis chi-squared = 10.696, df = 2, p-value = 0.004759 ANOVA without equal-variance assumption. Although data are not normal, we might wonder what P-value we would get from the oneway.test procedure in R, which assumes normality, but not equal group variances. The P-value is not much different than for the Kruskal-Wallis test. oneway.test(x ~ g) One-way analysis of means (not assuming equal variances) data: x and g F = 17.858, num df = 2.0000, denom df = 5.5308, p-value = 0.003863 If we want to look only at the P-value, we can use $ -notation: pv.obs = oneway.test(x ~ g)$p.val; pv.obs [1] 0.003862857 A simulated permutation test. Whether or not we trust the F-statistic of oneway.test to have an F-distribution, we might argue that the test statistic is a useful way to show how far apart sample means are, taking their variances into account. Then we might take the P-value as a reasonable metric for a permutation test. The distribution of that metric can be approximated by permuting the $3+15+11=29$ observations among the three groups. [In R this is accomplished by repeatedly scrambling the elements of g by using sample(g) .] The simulated permutation test gives P-value $0.046.$ set.seed(1106) pv = replicate(10^5, oneway.test(x~sample(g))$p.val) mean(pv Notice that while we have used computations from a normal-based test for the 'metric', the P-value of the permutation test comes from simulation, not from a normality assumption. The permutation test, which does not assume normality, barely finds significant differences at the 5% level. [It would be somewhat more common to use the F-statistic from each of the 100,000 runs as the metric of the permutation test, but in this case the degrees of freedom vary somewhat depending on the sample variances of the three permuted groups.] Note: (1) Ad hoc tests would depend on what kind of main test you choose to use to compare the three groups. In any case, you should use a method that protects against 'false discovery' upon repeated tests with the same data. For example, you might use the Bonferroni method with Welch two-sample t tests, if you choose to use oneway.test . You might use Tukey's HSD ad hoc tests if you use a standard one-way ANOVA, assuming equal variances. Etc. (2) I agree with @Dave that, with such a small amount of data, a Bayesian approach might be preferable---especially, if prior experience with such data provides clues toward a reasonable prior distribution. (3) In case there is any doubt about the validity of a permutation test using P-value as a metric, we show that when applied to normal data, it gives very nearly the same P-value as does oneway.test . set.seed(2021) y1 = rnorm(15, 70, 10) y2 = rnorm(15, 80, 10) y3 = rnorm(15, 90, 10) y=c(y1,y2,y3); gg = rep(1:3, each=15) PV.OBS = oneway.test(y~gg)$p.val; PV.OBS [1] 0.01094681 PV = replicate(10^5, oneway.test(y~sample(gg))$p.val) mean(PV
