[site]: datascience
[post_id]: 113075
[parent_id]: 113070
[tags]: 
There are multiple concepts mixed in your question. Contextual vs. non-contextual word embeddings: word2vec is a non-contextual approach to obtaining token embeddings. This means that a specific word has the same vector representation regardless of the other words in the sentence it appears. BERT, on the other hand, can be used to obtain contextual representations, because the representations of a token depend directly on the other words in the sentence. Contextual word embeddings with LSTMs. You can obtain contextual word embeddings with LSTMs. Actually, BERT has 2 predecessors that are just that. These models are ULMFit and ELMo . Both are bidirectional LSTMs. The fact that they are bidirectional is important here, otherwise, the representations would only be contextual for the words to the right of each word. Using BERT or LSTMs for classification and other tasks. Both BERT and LSTMs are suitable to perform text classification. In the case of BERT, the sentence-level representation is obtained by prefixing the sentence with the special token [CLS] and taking the representations obtained at that position as sentence representation (this is trained with the next-sentence prediction task in the BERT training procedure). In the case of LSTMs, the sentence-level representation is usually obtained either as the last output of a unidirectional LSTM or by global pooling over all the representations of a bidirectional LSTM.
