[site]: crossvalidated
[post_id]: 302466
[parent_id]: 302452
[tags]: 
That situation should be avoided. If all the neurons in one of the hidden layers are dropped, signals would not proceed towards the output neuron, and your neural network would not function as wanted. As you could see in below picture, only a part of your neurons in a layer are dropped. You normally set the dropout rates for each hidden layer. So, if you set the dropout rates below 1 , that sort of situation is avoided. Below is how dropout layer is implemented in Tensorflow and Keras . You generally set the dropout rates for all the hidden layers as the same number (0.x) as it is convenient to tune the hyperparameter. # set the dropout rate as any number between 0 and 1 dropout_rate = 0.4 # tensorflow implementation dropout = tf.nn.dropout(x, keep_prob = dropout_rate) # keras implementation dropout = keras.layers.Dropout(dropout_rate)
