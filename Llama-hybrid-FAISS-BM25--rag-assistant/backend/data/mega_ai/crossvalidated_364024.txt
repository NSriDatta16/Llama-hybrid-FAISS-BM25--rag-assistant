[site]: crossvalidated
[post_id]: 364024
[parent_id]: 363996
[tags]: 
What advantage we will get from this fine tuning? Pros: Greatly reduced training time. By using pre-trained weights, the model's first few layers are already very effective. You just need to train the final layers of your model. Improved performance . Models you usually used are pre-trained on large scale datasets (most commonly the ImageNet dataset). Because CNNs performance improves with more training data, the lower-level filters of pre-trained models are probably superior to filters trained on smaller datasets. Counter over-fitting on small datasets . CNNs need a lot of data to generalize properly, even when data augmentation techniques are applied. When trained on small datasets, their lower and mid-level filters tend to adapt specifically to the training set, leading the model to overfit. In contrast ImageNet is a very large (millions of images) and very diverse (1000 classes) dataset and filters of CNNs trained on it can extract very generic features. Using a pre-trained network is the only way I'm aware of with which you can train a CNN effectively on Cons: No guarantee that the initialization point of the weights is a good starting point; they could be stuck in a local minimum . On the other hand by training a model from scratch could lead to a better solution, which might be unobtainable by starting from the initialization point of the pre-trained model. This is relevant if both runs (initial training and fine-tuning) are done on the same dataset. Restricted architecture . The most important downside of using pre-trained models are that we are restricted to use exactly the same architecture, which might not be desirable. The good thing is that pre-trained weights are available for almost all state-of-the-art models . From the above you can see why using pre-trained models is so popular and why you should use them in almost any situation. When shouldn't you use a pre-trained model? Only in the occasions where you might want to build your own architecture. Is it needed when we use same training set but try it on different test set or is it required when we have to use different training sets and different training labels? You can use it in both cases, but in the second case you need to drop the final layers of the pre-trained model (i.e. the ones used to classify the images) with your own layers.
