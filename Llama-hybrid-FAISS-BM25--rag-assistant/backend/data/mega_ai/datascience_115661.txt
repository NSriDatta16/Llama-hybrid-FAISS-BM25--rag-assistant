[site]: datascience
[post_id]: 115661
[parent_id]: 
[tags]: 
Keras NLP TransformerDecoder MultiHeadAttention Value Error

Recently I have been working on a MIDI Music Generator using the TransformerEncoder & TransformerDecoder layers found in the Keras NLP library. There is not much info/help on these layers which is why I am here. Everything was fine until eventually needed to batch my input dataset to get around GPU memory limitations. Now I encountered this error: Exception has occurred: ValueError (note: full exception trace is shown but execution is paused at: tf__call) Exception encountered when calling layer "multi_head_attention" " f"(type MultiHeadAttention). dim -7 not in the interval [-4, 3]. for '{{node TransDecPitch/multi_head_attention/ExpandDims}} = ExpandDims[T=DT_INT32, Tdim=DT_INT32](TransDecPitch/Tile, TransDecPitch/multi_head_attention/ExpandDims/dim)' with input shapes: [?,64,64], [] and with computed input tensors: input[1] = . Call arguments received by layer "multi_head_attention" " f"(type MultiHeadAttention): • query=tf.Tensor(shape=(None, 64, 562, 149, 1007), dtype=float32) • value=tf.Tensor(shape=(None, 64, 562, 149, 1007), dtype=float32) • key=tf.Tensor(shape=(None, 64, 562, 149, 1007), dtype=float32) • attention_mask=tf.Tensor(shape=(None, 64, 64), dtype=int32) • return_attention_scores=False • training=False • use_causal_mask=False To clarify some of the numbers above: Batch Size = 64 Sequences Per Batch = 562 Sequence Length = 149 Embedding Layer Output Dim = 1007 This is the model architecture. I cannot paste the model summary because the error occurs befor reaching that line. PITCH_INPUT_NODES = len(self.unique_x_pitch) # Actual value of this is 1006 PITCH_OUTPUT_NODES = len(self.unique_y_pitch) # Actual value of this is 1006 PITCH_OUTPUT_DIM = PITCH_OUTPUT_NODES +1 # Actual value of this is 1007 PITCH_ATTENTION = 6 TRANS_DROPOUT = 0.4 opt_pitch = Adam(1e-4) input_pitch = Input(self.pitch_input_shape) x_pitch = TokenAndPositionEmbedding(PITCH_INPUT_NODES,self.seq_length,PITCH_OUTPUT_DIM, name='EmbeddingsPitch')(input_pitch) x_pitch = TransformerEncoder(PITCH_OUTPUT_NODES, PITCH_ATTENTION, TRANS_DROPOUT, name='TransEncPitch')(x_pitch) x_pitch = TransformerDecoder(PITCH_OUTPUT_NODES, PITCH_ATTENTION, TRANS_DROPOUT, name='TransDecPitch')(x_pitch) pitch_output = Dense(PITCH_OUTPUT_NODES, activation='softmax', name='OutputPitch')(x_pitch) #Pitch Output Layer self.model_pitch = Model(input_pitch, pitch_output, name="ModelPitch") self.model_pitch.compile(opt_pitch, tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=[tf.keras.metrics.SparseCategoricalAccuracy()] ) Any help would be fantastic and please let me know if any helpers need more info. Thank you
