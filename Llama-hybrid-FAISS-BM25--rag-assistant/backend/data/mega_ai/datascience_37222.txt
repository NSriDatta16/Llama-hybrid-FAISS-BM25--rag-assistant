[site]: datascience
[post_id]: 37222
[parent_id]: 37219
[tags]: 
You can take a look at this answer in Cross Validated : https://stats.stackexchange.com/a/53194/211338 It explains common techniques to visualize and compare high dimensional data. As you have 3 classes, just plot the different plots 3 times or compare them classes in "0 vs 1", "0 vs -1" and "-1 vs 1" plots, depending on the case. EDIT I don't really see why you would like to perform feature reduction, nor when. You wrote you want to use K-means on that data (60D data) and then just plot it on 2D plots to visualize the similarity of the labels. Well then, do it... You can apply K-means on your 60D data (supposing it has already been cleansed and properly analyzed), so you don't loose information. That will yield the class for each observation (0, -1 and 1) and voil√†. If you really want to perform feature reduction: If you have some knowledge of the data, try to use other technique than PCA (search for Exploratory Data Analysis). If your data is anonymous, you can then perform PCA or any other type of dimensionality reduction method, like t-SNE (which should perform better on lots of cases). As you have noticed, PCA does not always yield amazing results. 60% of information seems not enough (to me). This means 40% of the information has been lost and will not be used in your algorithm. You can try and take some more principal components. I use an 85% treshold when I have a lot of instances and 95% when the dataset is rather small.
