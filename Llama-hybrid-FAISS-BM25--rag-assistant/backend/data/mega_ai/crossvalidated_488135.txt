[site]: crossvalidated
[post_id]: 488135
[parent_id]: 
[tags]: 
Seasonailty in time series: adding seasonal lags versus detrending using Fourier Transform?

There are a number of posts on Cross-Validated about seasonality in time-series and detrending a dataset, in the context of classical time series models like AR, MA, ARIMA, etc. But my question was more of a question about intuition and practice. As an example, lets use the common airline passenger dataset from this So there are different was to deal with the presence of seasonality in a dataset. In the image above there is a clear linear trend and a seasonal trend. The linear trend makes the dataset non-stationary, so we could remove it by just applying linear regression on the time index and then differencing the linear trend from the original time series. This would leave us with something that looks very periodic. If I start with some simple AR model like: $$ y_t = \beta_0 + \beta_1*y_{t-1} + \epsilon_t \sim N(0, \sigma^2) $$ To deal with the periodicity I could. add lags to an AR or ARIMA model that correspond to the period of the seasonality. Since airline data has monthly observations, I could use a yearly or 12 month lag. In that case, the model would become: $$ y_t = \beta_0 + \beta_1*y_{t-1} + \beta_{2}*y_{t-12}+ \epsilon_t \sim N(0, \sigma^2) $$ The other approach is to just detrend the seasonality before applying the model. So I could use a Fourier transform (or any other set of basis functions) to extract out the form of the seasonal component, then difference out this seasonal component, and then run an AR model such as the original model to predict future values. Or at the least I might have a lower order lag term if some seasonality still persists? Now practically speaking I could try either approach on a training and test set and see which one performs the best. But from a mathematical or numerical standpoint, I was just trying to understand whether there was one approach that made more sense or less sense? I mean from a numerical standpoint does one approach or the other produce better theoretical results or guarantees? Most AR models are fit with simple linear regression, but complex models like ARMA, or ARIMA models are fit with Kalman filters, so I was not sure whether approach #1 or #2 worked any better from a numerical standpoint? Any thoughts would be appreciated.
