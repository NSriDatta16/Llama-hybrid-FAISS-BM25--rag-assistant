[site]: crossvalidated
[post_id]: 556035
[parent_id]: 555255
[tags]: 
Maximum-likelihood is applied mathematical optimisation --- learn the latter well This is too large a field for us to give you a comprehensive answer, but perhaps we can point you in the right direction to find the resources you need. The first thing to stress here is that, mathematically speaking, all forms of maximum likelihood involve maximising a function over an input set . For this reason, the subject falls within the general field of mathematical optimisation . For continuous distributions this optimisation is done using standard calculus methods and for discrete distributions it is done using discrete calculus (or sometimes direct optimisation methods). Now, there are some particular "tricks" that are commonly used in the context of maximum likelihood analysis. For example, most (but not all) maximum likelihood problems involve maximising a function that is a product of a large number of non-negative parts, and so we frequently start by taking logarithms and working in log-space. Nevertheless, at the end of the day this is still just mathematical optimisation applied in a particular context. If you want to get good at it in general, and be able to solve problems that do not conform neatly to standard cases, it is a good idea to give yourself a broad education in the field of mathematical optimisation. The field of mathematical optimisation is absolutely huge; you could probably fill a small library with books and papers on the topic. Nevertheless, there are some obvious places to start and some further places to progress once you have mastered the basics. Over the long-term, I recommend something like the following curriculum: Unconstrained univariate optimisation: Start by learning how to maximise a differentiable univariate function over an "unconstrained" input set (i.e., an input that can be any real number). This is done by looking at the first and second derivatives of the function. There are many introductory calculus textbooks that cover this material in detail. Unconstrained multivariate optimisation: Once you are comfortable with optimisation of univariate functions, learn how to maximise a differentiable multivariate function over an "unconstrained" input set (i.e., an input that can be any vector of real numbers). This is done by looking at the multivariate versions of the first and second derivatives of the function (called the gradient vector and Hessian matrix). Most introductory calculus textbooks will cover multivariate (or at least bivariate) calculus after they cover univariate calculus. Optimisation of composite functions: Once you are comfortable with the basics of unconstrained optimisation, learn to use the chain rules to solve optimisation problems involving composite functions (i.e., functions of functions). In particular, learn the univariate and multivariate chain rules for the first two derivatives and get used to using these to derive the first two derivatives of composite functions so that you can optimise them using standard methods. The goal here is to be able to make transformations of your input parameters and still be able to comfortably optimise the function you are working with. Constrained univariate and multivariate optimisation: Once you are comfortable with unconstrained optimisation, and optimisation of composite functions, learn how to maximise a differentiable function over an input set that is constrained by one or more non-linear equations or inequalities. This is the field of non-linear programming . There are several common techniques in this field, including transformation of input variables (creating composite functions), direct analysis using Lagrangian methods or the Kurush-Kuhn-Tucker method , and methods using "penalty functions". Practice using each of these methods on some tricky constrained optimisaton problems. Over time you will learn which of these methods is the simplest to apply to different types of constrained optimisation problems, and you will be able to derive solutions using alternative methods. Discrete optimisation: Discrete optimisation is generally treated as a seperate subject to optimisation for continuous functions, but there are some clear parallels. Discrete optimisation is usually covered in books on discrete mathematics. It requires you to learn about the difference operators (analogous to differentiation of continuous functions) and discrete calculus. This is something that is relatively easy to understand if you already have a good grounding in standard calculus methods for continuous functions. However, it is something that should be studied in its own right. Again, you should start by looking at unconstrained univariate optimisation, then unconstrained multivariate optimisation, then constrained optimisation, etc. Once you have mastered the basics you can look at some standard discrete optimisation problems in computational theory (e.g., the knapsack problem, bin-packing problem, change-making problem, etc.). If you are really keen you can also start to look at some of the theory relating to computational complexity of these problems. Numerical/simulation methods: Once you have a good grounding in the underlying theory of mathematical optimisation, and you are comfortable with both constrained and unconstrained problems involving univariate or multivariate functions, you can then examine some numerical optimisation methods. This includes dynamic programming, MCMC methods, simulated annealing, evolutionary/genetic algorithms, etc. Once you get to this point you are getting into specialist territory, but it is nice to have a rough idea of how these methods work (and ideally have the ability to program a few of them if needed). As regards the specifics of maximum-likelihood (ML) and restricted maximum-likelihood (REML), these become extremely simple to understand once you have obtained a strong underlying background in mathematical optimisation. Most general textbooks on probability and statistics will have a section on estimation that will include ML as one of the main estimation methods. Resources discussing REML are less common, but I will give you some papers here that may help. Maximum-likelihood (ML) estimation: Maximum-likelihood estimation is an applied example of mathematical optimisation, where you are maximising a joint density of some data with respect to one or more parameters. If there is one parameter then this is a univariate optisation problem and if there is more than one parameter it is a multivariate optimisation problem. It is usually (but not always) the case that the objective function of interest (the joint density) can be written as a product of non-negative parts. In particular, in the case of conditionally independent data, the objective function will be a product of density functions for each individual data point. Consequently, we usually take logarithms and maximise the "log-likelihood function". The maximisation itself proceeds using standard methods, but there are particular names for things in this context --- e.g., we call the first derivative of the log-likelihood the "score function" and the negative of the second derivative the "information function". Maximum likelihood estimation is covered in virtually all textooks that cover statistical estimation. Profile-likelihood (PL) estimation: The profile likelihood is used in some multivariate ML problems when we maximise the multivariate function one parameter at a time. One general optimisation technique that can be used in these cases is to derive the form of the MLE for a single parameter (written as a function of the data and other parameters) and then substitute this maximised parameter value back into the original likelihood function to obtain a partially maximised version of the function than no longer has that parameter. We call this partially maximised version of the likelihood function the "profile-likelihood" function. Understanding the use of this function really just requires you to be familiar with optimisation of multivariate functions using this one-at-a-time method. Profile likelihood is mentioned in some statistical textbooks in the context of finding the MLE in multivariate problems. In any case, it is really just something that arises when using a particular technique for multivariate optimisation. Restricted maximum-likelihood (REML) estimation: This is a variant of maximum-likelihood estimation that involves an attempt to estimate a parameter in a distribution while treating one or more other parameters as "nuisance parameters". REML is frequently used (and illustrated) when estimating variance components in the presence of an unknown mean. The original paper introducing the method is Bartlett (1937) and it was applied to a range of problems in Harville (1977) . In Corbeil and Searle (2012) REML was used to estimate variance components in mixed models. You can find a simple introduction to this topic in Zhang (2015)
