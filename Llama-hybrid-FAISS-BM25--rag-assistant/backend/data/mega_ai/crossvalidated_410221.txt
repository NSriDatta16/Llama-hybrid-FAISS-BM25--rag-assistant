[site]: crossvalidated
[post_id]: 410221
[parent_id]: 410118
[tags]: 
Data splitting is only reliable if you have a very large data set, but since you mentioned $n = 100,000$ in the comments as an example, you should probably be fine. However, if your data set is small, you can get very different results with different splits. In that case, consider doing nested cross-validation instead. The post you linked combines (normal, not nested) cross validation with a single random split, though. The entire procedure is as follows: Randomly divide the data set into a train and test set; Randomly divide your train set into $k$ parts; Choose your best model(s) by cross-validating on these $k$ parts: Train on $k-1$ parts; Evaluate performance on the remaining part; Repeat until all parts are used once for evaluation; Retrain the best model(s) on the entire train set (or keep the models from step 3 for e.g. a majority vote); Evaluate the performance of your best model(s) (only a handful at most) on the test set. The variance and bias estimates you obtain in step 5 are what you base your conclusions about. The split in step 1 is up to you. Many use a 80/20 split, but if your data is large enough, you may be able to get away with a smaller test set. The split in step 2 should generally be as large as you can afford in terms of computation time. 10-fold CV is a common choice. You can even run step 2-3 multiple times and average the results. This is more robust against the different results you might have obtained from different random splits in step 2. Finally, note that you should be careful with the use of the word unbiased. Cross-validation is still a form of internal validation and cannot account for the bias of this particular data set. The only way you could obtain an unbiased estimate would be through external validation (i.e. multiple data sets/studies/sources).
