more detailed audio spectrograms and greater audio resolution with the tradeoff that imperfections in the synthesis are more noticeable. 15.ai processes speech using customized deep neural networks and specialized audio synthesis algorithms. While its underlying technology could produce 10 seconds of audio in less than 10 seconds of processing time (i.e. faster-than-real-time), the user experience often involves longer waits as the servers manages thousands of simultaneous requests, sometimes taking more than a minute to deliver results. Due to its nondeterministic design, 15.ai produces variations in its speech output. 15.ai introduced the concept of emotional contextualizers, which allowed users to specify the emotional tone of generated speech through guiding phrases. The emotional contextualizer functionality utilized DeepMoji, a sentiment analysis neural network developed at the MIT Media Lab that processed emoji embeddings from 1.2 billion Twitter posts to analyze their emotional content. If an input into 15.ai contained additional context (specified by a vertical bar), the additional context following the bar would be used as the emotional contextualizer. For example, if the input was Today is a great day!|I'm very sad., the selected character would speak the sentence "Today is a great day!" in the emotion one would expect from someone saying the sentence "I'm very sad." 15.ai uses pronunciation data from Oxford Dictionaries API, Wiktionary, and CMU Pronouncing Dictionary, which uses ARPABET phonetic transcriptions. Users can input ARPABET transcriptions by enclosing phoneme strings in curly braces to correct mispronunciations. 15.ai's interface uses color-coding to indicate pronunciation certainty and also displays technical metrics, graphs, and comprehensive model analytics, which has included sentiment analysis and automatic improvements to the vocoder. The platform limits its prompt to 200 characters; users can combine multiple generations for longer speech sequences. Later versions of 15.ai introduced multi-speaker capabilities. Rather than training separate models for each voice, 15.ai uses a unified model that learned multiple voices simultaneously through speaker embeddings: numerical representations that capture each character's unique vocal characteristics. Along with the emotional context conferred by DeepMoji, this allows the deep learning model to learn shared patterns across different characters' emotional expressions and speaking styles, even when characters lack examples of certain emotions in their training data. Reception Critical reception Critics described 15.ai as easy to use and generally able to convincingly replicate character voices, with occasional mixed results. Natalie Clayton of PC Gamer wrote that SpongeBob SquarePants' voice was replicated well, but described challenges in mimicking the Narrator from the The Stanley Parable: "the algorithm simply can't capture Kevan Brighting's whimsically droll intonation." Zack Zwiezen of Kotaku reported that "[his] girlfriend was convinced it was a new voice line from GLaDOS' voice actor". Taiwanese newspaper United Daily News also highlighted 15.ai's ability to recreate GLaDOS's mechanical voice, alongside its diverse range of character voice options. Yahoo! News Taiwan reported that "GLaDOS in Portal can pronounce lines nearly perfectly", but also criticized that "there are still many imperfections, such as word limit and tone control, which are still a little weird in some words." Chris Button of Byteside called the ability to clone a voice with only 15 seconds of data "freaky," but also described the tech behind it as "impressive." Robin Lamorlette of Clubic described the technology as "devilishly fun" and wrote that Twitter and YouTube were filled with creative content from users experimenting with the tool. The platform's voice generation capabilities were regularly featured on Equestria Daily with documented updates, fan creations, and additions 