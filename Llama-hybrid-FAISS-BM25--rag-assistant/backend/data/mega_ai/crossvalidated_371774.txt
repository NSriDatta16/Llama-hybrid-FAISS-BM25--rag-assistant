[site]: crossvalidated
[post_id]: 371774
[parent_id]: 
[tags]: 
Convergence of Value Iteration in Reinforcement Learning

I am struggling to understand when the value iteration algorithm converge. Suppose that I use a discount 1. Will the algorithm always converge if I have terminal states in the MDP or it has to do on how I define the rewards? I understand that if I have terminal states in the MDP the MDP is episodic. So will end sometime. So, do I have to use a discount smaller than 1 to converge?
