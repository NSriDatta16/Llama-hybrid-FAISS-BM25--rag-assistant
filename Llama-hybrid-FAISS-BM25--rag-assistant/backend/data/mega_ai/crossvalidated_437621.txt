[site]: crossvalidated
[post_id]: 437621
[parent_id]: 
[tags]: 
Why can't my 1DCNN give the labels when I provide them to it?

I am working with Keras, and I am trying to build a 1DCNN for time-series regression using multiple sensors. However, I noticed my approach wasn't working. I decided to back up and see if the network could even give me the labels when I provide them as the only input. Namely, I am providing as inputs batches of a certain time length, and then having the network predict the last value. So, batch_size = 20, timesteps = 500, one input to the NN is an array of size (20,500,1), and the labels are passed in with size (20,1). My neural network is as follows: model = Sequential() model.add(Conv1D(100, 10, activation='relu', batch_input_shape=(batch_size, timesteps, 1))) # trainInput.shape[1] model.add(Conv1D(100, 10, activation='relu')) model.add(MaxPool1D(3)) model.add(Conv1D(100, 10, activation='relu')) model.add(Conv1D(100, 10, activation='relu')) model.add(GlobalAveragePooling1D()) model.add(layers.Dense(1)) My sensor data is coming in at 500Hz. I normalized the data by taking the standard score. The NN is not able to learn the relationship well. Again, the relationship is that the inputs and outputs come from the same set of data. The inputs are batch_size sets of time_steps length of data points, and the provided training labels are the last data point in each set. How should I alter the NN to improve performance? The training loss stalls after about 10 epochs. Would an LSTM perform better?
