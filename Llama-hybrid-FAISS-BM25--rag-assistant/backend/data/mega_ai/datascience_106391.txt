[site]: datascience
[post_id]: 106391
[parent_id]: 67781
[tags]: 
At high level, you can think of vanishing gradients in the way Chinese whispers work: Part of the original information is being lost every time it is being passed backwards to another person. In a similar way, RNN architecture "looses" part of the original information of a gradient as it is being propagated from the very last time step backwards to the very first step. Drilling down to the specifics see below: Vanishing gradients Traditional Recurrent Neural Networks (RNN) have the ability to model sequential events by propagating through time, i.e. forward and backward propagation. RNN models connect each time-step (e.g. position of a word in a sentence) using the following function defined as hidden state: $a_n = f(W_n, a_{n-1}, x_n)$ The hidden state $a_n$ carries past information by applying a linear combination over the previous step and the current input. The issue with the above is that the hidden state of every current position is a function of all previous positions . This means when you backpropagate gradients through time (see BPTT) the gradient inherently "looses" part of its "amplitude" because of the chain rule in $a_n$ : $a_n = f(W_n, a_{n-1}, x_n) = f(W_n, f(W_{n-1}, a_{n-2}, x_{n-1}), x_n)$ , since $ a_{n-1}=f(W_n, a_{n-2}, x_n)$ . In this way, the longer the input sequence is the worse long terms dependencies will be captured to to the way gradients vanish because of the chain rule in their hidden state. I hope it helps. See here my relevant post in case it may also be of help https://datascience.stackexchange.com/a/84409/102852
