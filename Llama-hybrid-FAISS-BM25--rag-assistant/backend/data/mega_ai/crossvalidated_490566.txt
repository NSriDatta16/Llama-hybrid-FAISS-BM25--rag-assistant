[site]: crossvalidated
[post_id]: 490566
[parent_id]: 490536
[tags]: 
logistic regression would blow up if the data are linearly separable, if they are not you would just learn a probability of the outcome given the input variable, the decision boundary is just a byproduct of this. Intuitively, since we are looking at the function that assigns a probability of an outcome, if the data are linearly separable, the probability wil be either 0 or 1 and there are infinitely many solutions for this problem. Also, logistic regression is based on likelihood, and likelihood, which will go to infinity for probabilities of 0 and 1 exactly (I think) SVM is a constrained optimization problem and with a hard margin and nonlinearly separable data, the constrains cannot be met so the solution does not exist, what happens depends on the optimizer you will use, it might crash, tell you the solution does not exist not converge and go forever, or not converge and stop after a fixed number of iterations
