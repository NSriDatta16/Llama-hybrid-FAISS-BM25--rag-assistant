[site]: crossvalidated
[post_id]: 82907
[parent_id]: 82878
[tags]: 
Bag of words and tf-idf are some of the basic feature representations one could start with. When used with a simple linear model which consumes those features and spits out the group(s) to which an input document belongs you will have a decent benchmark model to beat. Depening on which software you are using your mileage can vary. For example: Python appears to have a better text processing capability than R ( my opinion only, based on experience - no sources to quote). For example: There is another feature representation called the Hashing Trick - essentially you take a word ( any word) from the input document and hash-map it to a fixed sized feature vector. For example: If you think the dimensionaility of your problem would not be beyond 1024 words [in effect setting up 1024 binary/count variables]. The hashing function takes hashFunc('cat') =1023[meaning you will set the 1023rd column to 1/increment it by 1 if using a counts representation]; hashFunc('dog')=23[meaning you will set the 23rd column to 1/increment it by 1];hashFunc('cow')=957 and so on... This has the effect of dimensionaility reduction and is much more memory efficient. I think there are functions in Python which do this for you with relative easy ( search for Python hashingVectorizer) including the usual text preprocessing. Another software which uses the hashing trick is the Vowpal Wabbit Other techniques worth considering are predominently in the "how to reduce the dimensionality of your problem" domain and include things such as Singular Value Decomposition, Random Projection both applied on either the bag of words or tf-idf representation etc... More advanced techniques will invole Deep Learning (neural net)
