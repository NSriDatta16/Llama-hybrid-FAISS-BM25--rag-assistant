[site]: datascience
[post_id]: 75191
[parent_id]: 75182
[tags]: 
Usually it will not be a single feature value that is responsible for the decision of a ML model. Neural Networks, Random Forests, SVM, etc. intend to transform the input in a more beneficial feature space, where making decisions is more easier for them. As a draw back, this makes interpratability for humans more intricate. Explainability of ML methods is a whole research field. You could check out some explainability approaches. For autoencoders for example you could use Layerwise Relevancy Propagation (LRP). https://arxiv.org/pdf/1708.08296.pdf
