[site]: crossvalidated
[post_id]: 524196
[parent_id]: 201747
[tags]: 
Yes, weak learners are absolutely required for boosting to be really successful. That is because each boosting round for trees actually results in more splits and a more complicated model. This will overfit quite quick if we let it. On the other hand, if we pass a more complicated model such as a large polynomial linear regression then boosting will actually apply some regularization. Although, linear models won't be able to give you the best accuracy when boosting (typically). You mention kaggle and deep trees. There is a significant difference between standard boosting trees and the base trees used for xgboost or lightgbm (what wins Kaggle). The trees in these algorithms are heavily regularized which allows us to use deeper trees and still keep them 'weak'. In terms of param spaces to try, it depends on the algo. If you are using some basic boosted tree like scikit-learn's then you will typically be dealing with really short trees (so max depth around 1-3ish). Whereas with xgboost or lightgbm you will get good results typically going deeper to something like 8-16. Learning rates typically are lower around .01 - .1 although I have seen some go up to .5 for optimal solutions. Overall though, the huge param space search is the price we pay for state-of-the-art results. As Tim mentioned in his answer we typically leverage hyperparameter packages to speed this up like hyperopt or optuna.
