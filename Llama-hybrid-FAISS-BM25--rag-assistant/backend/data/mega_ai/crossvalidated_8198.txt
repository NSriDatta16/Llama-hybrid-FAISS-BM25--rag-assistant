[site]: crossvalidated
[post_id]: 8198
[parent_id]: 8196
[tags]: 
First, it is not always the case. There might be a composite null . Most standard tests have a simple null because in the framework of Neyman and Pearson the aim is to provide a decision rule that permits you to control the error of rejecting the null when it is true. To control this error you need to specify one distribution for the null. When you have a composite hypothesis there are many possibilities. In this case, there are two natural types of strategies, either a Bayesian one (i.e. put weights on the different null distribution) or a minimax one (where you want to construct a test that has a controlled error in the worst case. In the Bayesian setting, using the posterior, you are rapidly back to the case of a simple null. In the minimax setting, if the null is something like corre $\leq$ 0.5 it might be that the problem is equivalent to using the simple null corre = 0.5. Hence to avoid talking about minimax people directly take the simple null that is the 'extreme point' of the composite setting. In the general case it is often possible to transform the composite minimax null into a simple null... hence treating rigorously the case of a composite null is to my knowledge mostly done by going back somehow to a simple null.
