[site]: datascience
[post_id]: 110417
[parent_id]: 105013
[tags]: 
Finally, I know how to solve this. The most plain solution would be a RNN-like structure that just acts as a seq2seq model. We can use attention, bidirectional chains etc. to enhance our performance. However, this solution has the clear disadvantage that we insinuate that there is a sequential relationship between the images which is actually not really there.. as pointed out in the question, the ordering of the images in a sample does not matter at all. As a better solution, we can use a transformer model without positional encoding. As we know, attention is all you need and we can waive the RNN-like structure. Transformer models usually still make use of the positional information in common application areas such as language modeling where the ordering does matter. But in our case, ordering doesn't matter. So, we just take the positional encoding (preprocessing step in the transformer model) away and we have a reasonable architecture where our interdependencies of arbitrary sets of images are captured.
