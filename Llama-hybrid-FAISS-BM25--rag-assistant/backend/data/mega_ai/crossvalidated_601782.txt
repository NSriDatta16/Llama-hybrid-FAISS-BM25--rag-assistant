[site]: crossvalidated
[post_id]: 601782
[parent_id]: 
[tags]: 
How to rewrite DreamBooth loss in terms of $\epsilon$-prediction?

I'm trying to make the loss used in DreamBooth paper explicit, writing it in terms of the noise, as it is commonly written in the original diffusion article [ 1 ], instead of the image reconstruction loss that is written in the paper. The authors use Imagen text-to-image diffusion model, and the finetuning process is described in the paper as A conditional diffusion model $\hat{x}_\theta$ is trained using a squared error loss to denoise a variably-noised image $z_t:= \alpha_tx + \sigma_t \epsilon $ as follows $$ \mathbb{E}_{x,c,\epsilon,t} \bigl[ w_t \lVert \hat{x}_\theta(\alpha_t x + \sigma_t \epsilon,c) - x \rVert_2^2 \bigr] \tag{1}\label{db_loss}$$ where $x$ is the ground-truth image, $c$ is a conditioning vector (e.g., obtained from a text prompt), $\epsilon \sim \mathcal{N}(0,\mathbb{I})$ is a noise term and $\alpha_t,\sigma_t,w_t$ are terms that control the noise schedule and sample quality, and are functions of the diffusion process time $t \sim U([0, 1])$ But in the appendix A of the Imagen paper I can see that the model is actually trained on the variational lower bound using the $\epsilon$ -parameterization (the model is trained to predict the noise that generated $z_t$ in the forward process) We use the $\epsilon$ -prediction parameterization, defined as $\hat{x}_\theta(z_t,\lambda_t, c) = (z_t − \sigma_t \epsilon_\theta(z_t, \lambda_t, c))/\alpha_t$ , and we impose a squared error loss on $\epsilon_\theta$ in $\epsilon$ -space with t sampled according to a cosine schedule The $\epsilon$ -prediction parameterization, is described in Denoising Diffusion Probabilistic Models as $$\mathbb{E}_{x,\epsilon,c,t} \Bigl[ \frac{\beta_t^2}{2 \sigma_t^2 \alpha_t (1-\overline{\alpha}_t)}\lVert \epsilon - \epsilon_\theta(z_t,t,c) \rVert_2^2 \Bigr]$$ Question : Does the $w_t$ term of equation \eqref{db_loss} refer to the reweighting term $\frac{\beta_t^2}{2 \sigma_t^2 \alpha_t (1-\overline{\alpha}_t)}$ ? I find this terminology extremely confusing since this is very similar to the guidance weight $w$ . However, from my undestanding the guidance weight is only used during sampling. Since the reweighting term was empirically proven to yield better result when set to 1, the loss could be simplified as $$\mathbb{E}_{x,\epsilon,c,t} \Bigl[\lVert \epsilon - \epsilon_\theta(z_t,t,c) \rVert_2^2 \Bigr]$$ I did not find any details about $w_t$ in Imagen paper. DreamBooth authors observed that training using the loss \eqref{db_loss} led to overfitting and language drift , so they actually introduced a regularization term in the loss, calling it Prior-Preservation Loss We propose an autogenous class-specific prior-preserving loss to counter both the overfitting and language drift issues. In essence, our method is to supervise the model with its own generated samples, in order for it to retain the prior once the few-shot fine-tuning begins. Specifically, we generate data $x_{pr} = \hat{x}(z_{t_1}, c_{pr})$ by using the ancestral sampler on the frozen pre-trained diffusion model with random initial noise $z_{t_1} \sim \mathcal{N}(0, \mathbb{I})$ and conditioning vector $c_{pr} := \Gamma(f(”\text{a [class noun]}”))$ . The loss becomes: $$\mathbb{E}_{x,c,\varepsilon,\varepsilon',t} \bigl[ w_t \lVert \hat{x}_\theta(\alpha_t x + \sigma_t \varepsilon, c) - x \rVert_2^2 + \lambda w_{t'} \lVert \hat{x}_\theta(\alpha_{t'} x_{pr} + \sigma_{t'} \varepsilon', c_{pr}) - x_{pr} \rVert_2^2 \bigr]$$ Following the previous literature, i would use again the $\epsilon$ -parameterization and rewrite the loss as $$\mathbb{E}_{x,c,\epsilon,t}\Bigl[ \lVert \epsilon - \varepsilon_\theta(z_t,t,c) \rVert_2^2 + \lambda \lVert \epsilon' - \epsilon_{pr} (z'_{t'},t',c_{pr}) \rVert_2^2 \Bigr]$$ where $\epsilon_{pr}$ is the frozen model predicting the noise of original images, $\epsilon,\epsilon' \sim \mathcal{N}(0,\mathbb{I})$ Question : Do you find this formulation correct? Looking into HuggingFace implementation of DreamBooth, I find that this formulation is quite coherent with the code latents = vae.encode(batch["pixel_values"].to(dtype=weight_dtype)).latent_dist.sample() latents = latents * 0.18215 # Sample noise that we'll add to the latents noise = torch.randn_like(latents) bsz = latents.shape[0] # Sample a random timestep for each image timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,),device=latents.device) timesteps = timesteps.long() # Add noise to the latents according to the noise magnitude at each timestep # (this is the forward diffusion process) noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps) # Get the text embedding for conditioning encoder_hidden_states = text_encoder(batch["input_ids"])[0] # Predict the noise residual model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample if noise_scheduler.config.prediction_type == "epsilon": target = noise elif noise_scheduler.config.prediction_type == "v_prediction": target = noise_scheduler.get_velocity(latents, noise, timesteps) else: raise ValueError(f"Unknown prediction type {noise_scheduler.config.prediction_type}") if args.with_prior_preservation: model_pred, model_pred_prior = torch.chunk(model_pred, 2, dim=0) target, target_prior = torch.chunk(target, 2, dim=0) # Compute instance loss loss = F.mse_loss(model_pred.float(), target.float(), reduction="mean") # Compute prior loss prior_loss = F.mse_loss(model_pred_prior.float(), target_prior.float(), reduction="mean") # Add the prior loss to the instance loss. loss = loss + args.prior_loss_weight * prior_loss else: loss = F.mse_loss(model_pred.float(), target.float(), reduction="mean")
