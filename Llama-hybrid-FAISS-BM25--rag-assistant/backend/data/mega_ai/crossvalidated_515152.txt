[site]: crossvalidated
[post_id]: 515152
[parent_id]: 
[tags]: 
Deciding between Decoder-only or Encoder-only Transformers (BERT, GPT)

I just started learning about transformers and looked into the following 3 variants The original one from Attention Is All You Need (Encoder & Decoder) BERT (Encoder only) GPT-2 (Decoder only) How does one generally decide whether their transformer model should include encoders only, decoders only, or both encoders and decoders? As an example, if I want to train a transformer to read a sequence of images of my backyard then predict whether it will rain in an hour (2 classes "rain" or "not rain"), should this transformer model generally have only decoders?
