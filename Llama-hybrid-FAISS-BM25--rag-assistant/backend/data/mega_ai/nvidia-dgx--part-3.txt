 for massive recommender systems, generative AI, and graph analytics, offering 19.5 TB of shared memory with linear scalability for giant AI models. DGX Helios Announced May 2023, the DGX Helios supercomputer features 4 DGX GH200 systems. Each is interconnected with Nvidia Quantum-2 InfiniBand networking to supercharge data throughput for training large AI models. Helios includes 1,024 H100 GPUs. Blackwell DGX GB200 Announced March 2024, GB200 NVL72 connects 36 Grace Neoverse V2 72-core CPUs and 72 B100 GPUs in a rack-scale design. The GB200 NVL72 is a liquid-cooled, rack-scale solution that boasts a 72-GPU NVLink domain that acts as a single massive GPU. Nvidia DGX GB200 offers 13.5 TB HBM3e of shared memory with linear scalability for giant AI models, less than its predecessor DGX GH200. DGX SuperPod The DGX Superpod is a high performance turnkey supercomputer system provided by Nvidia using DGX hardware. It combines DGX compute nodes with fast storage and high bandwidth networking to provide a solution to high demand machine learning workloads. The Selene supercomputer, at the Argonne National Laboratory, is one example of a DGX SuperPod-based system. Selene, built from 280 DGX A100 nodes, ranked 5th on the TOP500 list for most powerful supercomputers at the time of its completion in June 2020, and has continued to remain high in performance. The new Hopper-based SuperPod can scale to 32 DGX H100 nodes, for a total of 256 H100 GPUs and 64 x86 CPUs. This gives the complete SuperPod 20 TB of HBM3 memory, 70.4 TB/s of bisection bandwidth, and up to 1 ExaFLOP of FP8 AI compute. These SuperPods can then be further joined to create larger supercomputers. The Eos supercomputer, designed, built, and operated by Nvidia, was constructed of 18 H100-based SuperPods, totaling 576 DGX H100 systems, 500 Quantum-2 InfiniBand switches, and 360 NVLink Switches, that allow Eos to deliver 18 EFLOPs of FP8 compute, and 9 EFLOPs of FP16 compute, making Eos the 5th fastest AI supercomputer in the world, according to TOP500 (November 2023 edition). As Nvidia does not produce any storage devices or systems, Nvidia SuperPods rely on partners to provide high performance storage. Current storage partners for Nvidia Superpods are Dell EMC, DDN, HPE, IBM, NetApp, Pavilion Data, and VAST Data. DGX Spark In March 2025, Nvidia also announced the DGX Spark (previously DIGITS), a "desktop AI Supercomputer" based on Blackwell. These machines are targeted at AI researchers and programmers and have 128 GB of integrated RAM, making it possible to train or fine-tune fairly large models ("up to 200 billion parameters" with quantization). Several partner manufacturers also offer versions of the DGX Spark. It is expected to be available in summer 2025. Accelerators Comparison of accelerators used in DGX: See also Deep Learning Super Sampling == References ==