[site]: datascience
[post_id]: 114362
[parent_id]: 114361
[tags]: 
100k apps, 300k users - quite the task. Modern ranking systems typically consist of 3 phases. Recall (Generate candidates) For each user, reduce the number of applications you could recommend them from 100k down to (roughly!) 100-500. (This number is something to test). Reduce this by building a set of rules, for example, if an application is in the top 50 trending applications - then it probably belongs in this set. Then break this trending rule out by location, device type etc.. Other examples would be user-user / user-item collaborative filtering. Get creative with these rules, this will have a big impact down the pipeline. Look at all of the user, and items historic data. Ranking Here you take the candidates that you generated (100-500), and label them. The label is: Did each user download each application in your forecasting window. So now you may have a labelled dataset with a few positive downloads, and quite a few more negative (assuming people download a handful) Next build features about each user and application. For example, download rate, trending numbers, age, gender, device, features about their interactions, etc... Feed these features and targets into a ranking model. LightGBM Ranker is very popular, and a sensible start. Post-ranking Use additional rules, that you don't want or don't know how to get your model to learn. For example, you may not want to recommend an application that is NSFW to a certain population. Or you may want to decrease the rank of an application if it is breaking a set of rules, use advertising against ToS etc etc Essentially any post-ranking analysis/rules/models you want to build. Ranking systems are often thought of as the hardest area of data science . General tips: Build a feature store so you don't have to recalculate features over and over. Get your time series k-fold train-test split correct. Get your leakage detection in place - it's not a matter of if it will happen, but when ;). Look at existing software. Note: This approach got me top 1-2% in the Kaggle H&M recommendations competition, and was used by all the top competitors.
