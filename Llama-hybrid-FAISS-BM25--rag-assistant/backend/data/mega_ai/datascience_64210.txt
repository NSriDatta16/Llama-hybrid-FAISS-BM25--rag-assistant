[site]: datascience
[post_id]: 64210
[parent_id]: 64200
[tags]: 
As far as I know, one of the first "successful" use of attention came in 2014 by Badhanau, Cho, and Bengio with their paper "Neural Machine Translation by Jointly Learning to Align and Translate" Since then, attention has been refined and improved in multiple papers. Attention is All You Need and the ever-growing use of transformers has made it even more popular. However, I think it's crucial to understand that even though it has shown incredible value, it is ultimately just a mechanism that weights inputs in an attempt to pool them together in a smarter way than max or averaging pooling.
