[site]: crossvalidated
[post_id]: 445154
[parent_id]: 317699
[tags]: 
OP's network not only exhibits large fluctuations in accuracy, but also large fluctuations in the loss. This is a classic symptom of using too large a learning rate, too small a batch size, or both. Even though OP reduced the learning rate from $10^{-2}$ to $10^{-3}$ , this smaller learning rate might not be a small enough learning rate for the problem. Finding a good configuration of neural network parameters is like lock picking, and involves a lot of trial and error; trying one thing probably isn't enough experiments to find a good configuration of model parameters. Some optimizers, like SGD, can be very picky about the learning rate used. More information: What should I do when my neural network doesn't learn? What could be the reasons that making validation loss jumping up and down?
