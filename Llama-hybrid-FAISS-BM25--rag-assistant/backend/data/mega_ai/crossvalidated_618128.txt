[site]: crossvalidated
[post_id]: 618128
[parent_id]: 
[tags]: 
How does Variational Autoencoder approximate the joint probability distribution?

I know that in Variational Inference the idea is to approximate the posterior P(z|x, y) and I know that Variational AutoEncoders (VAEs) use the idea of variational inference through neural network architecture to approximate P(z|x) in the encoding phase and P(x|z) through decoding. However, I cannot understand how VAEs are able to model the joint probability distribution and where this joint probability distribution is calculated in the encoder-decoder architecture. I am also wondering if the low dimensional embeddings created through encoding phase are the same as the latent space P(z) ?
