[site]: datascience
[post_id]: 45746
[parent_id]: 
[tags]: 
Glove supported languages

Recently I started reading more about NLP and following tutorials in Python in order to learn more about the subject. I started experimenting with words embeddings also, and I found some interesting results which I don't know how to interpret. I first used an English corpus for both training and testing and afterwards, I used the English corpus for training and a small French corpus for testing (all corpora have been annotated for the same binary classification task). In both cases, I used the pre-trained on tweets Glove embeddings. As the results in the case where I also used the French corpus improved (by almost 5%, reaching ~accuracy = 0.8), I was wondering if Glove was trained on multilingual data, as I didn't see anyone making this statement (I'm aware of the amount of data that is used) comparing to FastText, for example, where you have embeddings for different languages. Also, if Glove supports multilingual information, this would also eliminate the need for mapping different embeddings into the same embedding space (as is the case for FastText). Any clarification would be greatly appreciated.
