[site]: crossvalidated
[post_id]: 398987
[parent_id]: 265940
[tags]: 
For classification: Different from a decision tree for random forest where you can use majority of the class in the leaf code as the class for an observation, Boosting trees are totally different. In Boosting method, you have loss function as $$ l(y_i,\hat{y_i})=y_i ln(1+e^{-\hat{y_i}}) + (1-y_i) ln(1+e^{\hat{y_i}}) $$ while $y_i$ is from your training data, $\hat{y_i}$ is from your boosting trees and equal to the sum of the leaf weights. If you have 10 weak learners (10 small trees) for your boosting, then you will have 10 weights ( $w$ ) to sum over for a given X. Observations with 1's should have larger $\hat{y_i}$ . But other than this, there is no physical meaning in $\hat{y_i}$ . We want to penalize for the sum of squared $w$ 's to enforce each tree adds no substantial to the estimation, which matches the idea of each tree just serves as a weaker learner.
