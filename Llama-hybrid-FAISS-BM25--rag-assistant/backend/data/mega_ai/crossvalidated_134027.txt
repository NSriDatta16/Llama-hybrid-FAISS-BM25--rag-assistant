[site]: crossvalidated
[post_id]: 134027
[parent_id]: 
[tags]: 
marginal conditional distribution from MCMC output

I have a MCMC sampler that targets $$\mathbb{P}(U_1,U_2,...U_n \mid G(U) \leq 0)$$ where $U=(U_1,U_2,...U_n)^T$. I realize now I am more interested in estimating the conditional density $$p_k = p(u_k \mid G(U)\leq 0)$$ for all $k=1,\ldots,n$. Is there a relatively easy way to achieve this either by using the sample that I already have from $\mathbb{P}(U_1,U_2,...U_n\mid G(U) \leq 0)$ or by designing another sampler ? I gather from this paper that (1) if the conditional distributions $\mathbb{P}(U_k\mid G(U) ,U_{j} \leq 0)$, $j\neq k$ are available then it is possible to derive density estimators for $p_k$. Another possibility (2) is if a functional form of the joint density $p( u_1,u_2,...u_n)$ is available and at least one $\mathbb{P}(U_k\mid G(U) ,U_{j} \leq 0)$ is straightforward to sample, then something can be done about it. I find myself in unknown territory since neither (1) nor (2) is true in my case. Would anyone have a suggestion to tackle this problem ? Even a MCMC sampler yielding samples from $p_k$ for all $k$ would be nice, as i could try KDE afterwards to get the corresponding density. The only annoying thing is the relatively high dimension. Thanks
