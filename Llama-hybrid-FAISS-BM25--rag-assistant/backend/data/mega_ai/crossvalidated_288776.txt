[site]: crossvalidated
[post_id]: 288776
[parent_id]: 
[tags]: 
Reparameterization trick for other non-normal distributions? (Example tensorflow code?)

Can anyone point me toward tensorflow code for the reparameterization trick for non-normal distributions? (uniform, beta, poisson, negative-binomial, etc?) Both for the sampling stage and the KL loss stage? I am trying to understand and implement latent variables in tensorflow models. I roughly understand how the reparameterization trick works in principal, how it's implemented as a sampler, and included as a KL divergence in the loss. There are many, many, good examples online for the normal distribution and a few discrete distributions, but none that I can find for other distributions. Normal examples ( https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder.py ) Gumbel example ( http://blog.evjang.com/2016/11/tutorial-categorical-variational.html ) Further, has the expansion of tf.contrib.distributions made this easier recently? Many of the online examples are a few years old and write out the math fully for the sampling and loss, and I've read that they've made the sampling direction differentiable but I'm unclear on how.
