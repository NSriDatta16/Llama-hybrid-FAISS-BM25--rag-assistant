[site]: crossvalidated
[post_id]: 230193
[parent_id]: 
[tags]: 
Do extra hidden layers prevent convergence?

I have designed a simple feed-forward neural network using stochastic gradient descent. I use 22 inputs, 4 hidden layers, 1 output and am using a learning rate of 0.7 and momentum of 0.3. I have about 700 points in my training set. As I began training, I noticed that my MSE (calculated on a validation set of about 200 points) was increasing with each epoch. I then decided to reduce the number of hidden layers I was using from 4 to 3. This - for whatever reason - worked, and the MSE dropped with each epoch and the network converged. This leads me to lack confidence in my implementation of the backpropagation algorithms. Why would this occur? Do extra, unneeded hidden layers prevent convergence? Could it be my algorithm?
