[site]: crossvalidated
[post_id]: 596397
[parent_id]: 
[tags]: 
Lasso regression / SVM convergence CPU -> GPU

I have coded a simple supervised ML classification using 10-20K data points for 25 samples. Linear ML models run quickly for example naive Bayes, linear regression and SVM linear on a small multi-core CPU desktop with 8 G RAM via scikit-learn . However, non-linear models for example lasso regression and non-linear SVM via RBF and polynomial kernels "do not converge" within reasonable uptime, i.e. no more than 4 hours uptime. Three questions: is this known behaviour and is there a pattern within the data set types? alternatively are their data sets that simply do not converge for non-linear models? would it be resolved via GPU * , such as the NVIDIA machines? The rationale of the question is being able to justify that a shift onto a cloud platform will deliver the results sought. * For example, a GPU accelerated "non-linear" ML model via either: libSVM SVM RBF kernel or; thunderSVM or possibly; falcon Post question edit : the above 3 examples are external to scikit-learn .
