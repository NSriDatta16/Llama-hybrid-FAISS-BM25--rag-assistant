[site]: crossvalidated
[post_id]: 530043
[parent_id]: 529926
[tags]: 
Here are a couple thoughts to add to what has been posted so far. You might be interested in taking a look at the famous machine learning paper, Domingos, P. (2012). "A Few Useful Things to Know about Machine Learning". Communications of the ACM ( pdf ). It should contain some food for thought. Specifically, here are three relevant subsections: DATA ALONE IS NOT ENOUGH Generalization being the goal has another major consequence: data alone is not enough, no matter how much of it you have. Consider learning a Boolean function of (say) 100 variables from a million examples. There are $2^{100}$ − $10^6$ examples whose classes you don't know. How do you figure out what those classes are? In the absence of further information, there is just no way to do this that beats flipping a coin. ... FEATURE ENGINEERING IS THE KEY At the end of the day, some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used. If you have many independent features that each correlate well with the class, learning is easy. On the other hand, if the class is a very complex function of the features, you may not be able to learn it. Often, the raw data is not in a form that is amenable to learning, but you can construct features from it that are. ... MORE DATA BEATS A CLEVERER ALGORITHM Suppose you’ve constructed the best set of features you can, but the classifiers you’re getting are still not accurate enough. What can you do now? There are two main choices: design a better learning algorithm, or gather more data (more examples, and possibly more raw features, subject to the curse of dimensionality). Machine learning researchers are mainly concerned with the former, but pragmatically the quickest path to success is often to just get more data. As a rule of thumb, a dumb algorithm with lots and lots of data beats a clever one with modest amounts of it. ... The other thing I would say is that the idea that "a human needs 1-2 to reach comparable classification accuracy" is because the human is not a blank slate. A person has a wealth of experience (i.e., many prior data) and rich conceptual knowledge that can be brought to bear on learning a classification. (Sections 4 and 8 from Domingoes are related to this idea of background knowledge and knowing what to attend to.) To connect these facts to training a (deep learning or other) model, you could consider that pre-training a model sometimes helps quite a bit (although this is less done nowadays) and likewise that Bayesian models with sufficiently good priors should also perform better. Having said those things, section 9 from Domingoes implies we may be able to be sufficiently successful without those, due to the increasing volumes of data that you describe.
