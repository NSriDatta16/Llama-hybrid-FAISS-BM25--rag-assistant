[site]: datascience
[post_id]: 73304
[parent_id]: 
[tags]: 
Hyperparameter tuning results yield no improvement over spot-check

There is a balanced binary classified dataset as seen below. Things I have tried using RandomForestClassifier as chosen model: TimeSeriesSplit with n_splits =3 and 10 MinMaxScaler , MaxAbsScaler , StandardScaler , RobustScaler , QuantileTransformer , PowerTransformer Snippet of RandomizedSearchCV set-up: scorer = {..., 'precision': make_scorer(precision_score, average = 'weighted'), ..., } RandomizedSearchCV(model, grid, scoring=scorer, cv=TimeSeriesSplit(n_splits=10), refit='precision', verbose=10) However despite all these efforts, precision (the main metric I have chosen to use) are all very similar. Inital spotcheck precision results were: ~.55-.56. Hyperparameter tuning results in the same range too, no improvement at all! Other "curious" metrics like F0.5 score and recall all have no improvements too with little variation. The biggest noticeable difference so far are n_splits 3 and 10. With all else equal, the range of precision is bumped up to ~.57-.59 with n_splits=10 . Hyperparameter tuning results also within ~.57-.59! I am stumped. Does anyone have ideas on how to find out what may be the issue(s)? I can provide more information if needed. Edit 1 (grid): # Number of trees in random forest n_estimators = [x for x in range(400, 2000, 200)] # Number of features to consider at every split max_features = ['log2'] # Maximum number of levels in tree max_depth = [x for x in range(10, 110, 10)] max_depth.append(None) # Minimum number of samples required to split a node min_samples_split = [2, 5, 10] # Minimum number of samples required at each leaf node min_samples_leaf = [4] # Method of selecting samples for training each tree bootstrap = [True, False]
