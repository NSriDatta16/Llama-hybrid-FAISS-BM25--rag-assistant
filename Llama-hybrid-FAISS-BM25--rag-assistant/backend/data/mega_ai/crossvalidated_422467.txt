[site]: crossvalidated
[post_id]: 422467
[parent_id]: 
[tags]: 
how to find a test for detecting fluctuation in a time series?

I have a time series which consists of, say, 100 values (which comes from a loss function). Normally, the loss goes down over iterations. However, at some point it just starts fluctuating. I would like to have a way to detect when the loss is simply fluctuation and not actually going down. The way I am doing this currently is the following: I take my interval of 100 values, compute the median and split in half. Then I look at the proportion of value in the first interval which are bigger than the median and compute a p-value using a binomial test with H_0: p = 1/2 (with n_successes = number of values bigger than median, n = 50). Any ideas? thank you
