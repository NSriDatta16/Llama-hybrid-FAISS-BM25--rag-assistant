[site]: crossvalidated
[post_id]: 378614
[parent_id]: 376312
[tags]: 
Yann LeCun has compiled a big list of results (and the associated papers) on MNIST, which may be of interest. The best non-convolutional neural net result is by Cire≈üan, Meier, Gambardella and Schmidhuber (2010) ( arXiv ), who reported an accuracy of 99.65%. As their abstract describes, their approach was essentially brute force: Good old on-line back-propagation for plain multi-layer perceptrons yields a very low 0.35% error rate on the famous MNIST handwritten digits benchmark. All we need to achieve this best result so far are many hidden layers, many neurons per layer, numerous deformed training images, and graphics cards to greatly speed up learning. The network itself was a six layer MLP with 2500, 2000, 1500, 1000, 500, and 10 neurons per layer, and the training set was augmented with affine and elastic deformations. The only other secret ingredient was a lot of compute--the last few pages describe how they parallelized it. A year later, the same group (Meier et al., 2011) reported similar results using an ensemble of 25 one-layer neural networks (0.39% test error*). These were individually smaller (800 hidden units), but the training strategy is a bit fancier. Similar strategies with convnets do a little bit better (~0.23% test error*). Since they are universal approximations, I can't see why a suitable MLP wouldn't be able to match that though it might be very large and difficult to train. * Annoyingly very few of these papers report confidence intervals, standard errors, or anything like that, making it difficult to directly compare these results.
