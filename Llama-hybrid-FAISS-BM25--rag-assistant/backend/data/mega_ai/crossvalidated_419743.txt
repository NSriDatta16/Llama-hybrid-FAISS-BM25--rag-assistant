[site]: crossvalidated
[post_id]: 419743
[parent_id]: 419714
[tags]: 
Both approaches are equally correct. If we want a hold-out test sample probably using stratification (Option B mentioned) is probably more appropriate because it ensures that our testing population has we exactly the same distribution as our training one. If we just train our classifier repeatedly (e.g. 100x bootstraps samples) the observed variation in the proportions will be attenuated and is probably preferable as we train in more "realistic conditions"and allow for sampling variation more explicitly. We usually stratify to correct issues with our classifier's training, not because it is "more" (or "less") correct than not stratifying. For example, GLM-based routines are not strongly affected by class imbalance (see here for more information), while others (e.g. SVM-like routines) tend to be influenced more requiring some actions from our part (e.g. reweighing of training sample for the case of SVMs). CV.SE has a great thread on the matter: When is unbalanced data really a problem in Machine Learning? I would urge you to read.
