[site]: crossvalidated
[post_id]: 188303
[parent_id]: 
[tags]: 
Standard deviation of (assumed) normal distribution

Say I measure a system for $N$ periods of time $T$. In each of these time periods $T_i$, with $1 \leq i \leq N$, I get $M_i$ occurrences. I have reason to assume that the variable $M$ follows a normal distribution. I believe that the calculation of the mean of this distribution is straightforward: just take the average. I am not so sure about the standard deviation, though: At first I thought I should just do it the "regular way", by taking the square root of the sum of the squared deviations from the mean, but now I'm not sure if this is completely correct. The mean I get will obviously have some uncertainty, as I don't have the time to make an infinite number of measurements. Shouldn't this somehow be taken into account when calculating the standard deviation? After all, the standard deviation is a function of the mean, but it's not possible to know the mean of the distribution without uncertainty. So I can only know the mean of my sample, which is not the same as the mean of the distribution. What I want to know is how this difference propagates to the standard deviation.
