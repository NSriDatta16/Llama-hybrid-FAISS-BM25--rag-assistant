[site]: crossvalidated
[post_id]: 237944
[parent_id]: 
[tags]: 
How is the generator in a GAN trained?

The paper on GANs says the discriminator uses the following gradient to train: $$\nabla _{\theta_d} \frac{1}{m}\sum^{m}_{i=1} [\log{D(x^{(i)})} + \log{(1-D(G(z^{(i)})))}]$$ The $z$ values are sampled, passed through the generator to generate data samples, and then the discriminator is backpropogated using the generated data samples. Once the generator generates the data, it plays no further role in the training of the discriminator. In other words, the generator can be completely removed from the metric by having it generate data samples and then only working with the samples. I'm a bit more confused about how the generator is trained though. It uses the following gradient: $$\nabla _{\theta_g} \frac{1}{m}\sum^{m}_{i=1} [\log{(1-D(G(z^{(i)})))}]$$ In this case, the discriminator is part of the metric. It cannot be removed like the previous case. Things like least squares or log likelihood in regular discriminative models can easily be differentiated because they have a nice, close formed definition. However, I'm a bit confused about how you backpropogate when the metric depends on another neural network. Do you essentially attach the generator's outputs to the discriminator's inputs and then treat the entire thing like one giant network where the weights in the discriminator portion are constant?
