[site]: stackoverflow
[post_id]: 1085089
[parent_id]: 1085048
[tags]: 
I think that this problem is complicated, and doesn't have one best solution. You can detect exact duplication of words at the whole document level (ie someone downloads an entire essay from the web) all the way down to the phrase level. Doing this at the document level is pretty easy - the most trivial solution would take the checksum of each document submitted and compare it against a list of checksums of known documents. After that you could try to detect plagiarism of ideas, or find sentences that were copied directly then changed slightly in order to throw off software like this. To get something that works at the phrase level you might need to get more sophisticated if want any level of efficiency. For example, you could look for differences in style of writing between paragraphs, and focus your attention to paragraphs that feel "out of place" compared to the rest of a paper. There are lots of papers on this subject out there, so I suspect there is no one perfect solution yet. For example, these 2 papers give introductions to some of the general issues with this kind of software,and have plenty of references that you could dig deeper into if you'd like. http://ir.shef.ac.uk/cloughie/papers/pas_plagiarism.pdf http://proceedings.informingscience.org/InSITE2007/IISITv4p601-614Dreh383.pdf
