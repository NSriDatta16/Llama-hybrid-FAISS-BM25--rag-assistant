[site]: crossvalidated
[post_id]: 59400
[parent_id]: 59399
[tags]: 
In the machine learning world, this is called a supervised learning problem. Each of the 100 columns are called "features" which may or may not have any predictive value for the outcome. Our goal in supervised learning can be to select the smallest combination of features to create the highest amount of discrimination in the binary outcome as possible, so you get predictions that have a nice spread and easily create a decision rule where the classification accuracy is very high. There are many approaches to feature selection, SVM is one of them. There is also random forests and GLM lasso to consider. Whether a method predicts a continuous outcome or not is irrelevant because you can select a threshold to define a classification rule. In fact, iterating over the range of all possible thresholds can produce a range of specificity / sensitivity for your prediction rule. Plotting these against one another produces an ROC curve which is used to evaluate the recall of binary marker prediction rules.
