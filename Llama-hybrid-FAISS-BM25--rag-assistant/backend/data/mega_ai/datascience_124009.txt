[site]: datascience
[post_id]: 124009
[parent_id]: 
[tags]: 
XGBoost Classifier Evaluation Confusion on New Dataset Despite High Cross-Validation Scores

I have built an XGBoost classifier model with 90 features, trained on a dataset containing 760k samples. I took great care to separate the labels from the features in both the training and testing datasets (2/3 for training and 1/3 for testing) and using a random seed of 0. Additionally, I preprocessed the data by clearing, transforming, and scaling it. To account for the class imbalance in the dataset, I configured the scale_pos_weight parameter in the XGBoost model. During the training and validation process, I observed that the learning curves of the model indicate a very good fit, and when performing 5-fold and 10-fold cross-validation, the model consistently achieves over 95% accuracy(precision and recall both over 90+,confusion matrix is also perfect). However, when I use entirely new validation sets that I trust, the model's performance is very inconsistent and sometimes quite poor. This discrepancy between cross-validation results and performance on trusted new validation sets is confusing me. Could it be possible that the 760k samples I have in my dataset are not truly representative of the problem I am trying to solve? Maybe am I missing something? If so, how can I identify potential sources of bias that might explain this behavior? Is there a way to improve the model's robustness on new, unseen data despite high cross-validation scores? Any insights or suggestions would be greatly appreciated.
