[site]: datascience
[post_id]: 75522
[parent_id]: 
[tags]: 
Logistic loss increasing while training with minibatches using the adam algorithm

I am trying to write my own code to use the adam algorithm for logistic regression. I am pretty sure It is training correctly as when I run it I am able to accurately classify a bunch of toy data that was generated using the parameters from the model. However, the cost keeps going up after each epoch and I don't think that is supposed to happen. Here are the two functions where I think there could be a problem: def log_loss(mX, mY, params, penalty = 'l2', C = 1.0): #make sure y is the right shape mY = mY.reshape(-1,1) # get model parameters from dictionary w = params[list(params.keys())[0]] intercept = len(params.keys()) == 2 if intercept: b = params[list(params.keys())[1]] else: b = 0 mm = mX.shape[0] # compute model hypothesis z = mX.dot(w) + b h = sigmoid(z) # compute gradient error = h - mY dw = C * np.dot(np.transpose(mX), error) / mm if intercept: db = C * np.sum(error) / mm else: db = None # compute cost without regularization log_loss = np.multiply(y, np.log(h)) + np.multiply(1 - y, np.log(1 - h)) cost = -C * np.sum(log_loss) / mm # now add regularization if penalty == 'l2': dw += w / mm cost += np.sum(np.multiply(w, w)) / (2 * mm) elif penalty == 'l1': dw += np.divide(np.abs(w), w) cost += np.sum(np.abs(w)) / (2 * mm) if db: grads = {'dw': dw, 'db': db} else: grads = {'dw': dw} return grads, cost I am reasonably sure that the cost function implementation is correct. The function that trains the model is here: def train_model(X, y, lr = 0.01, mini_batch_size = 64, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-6, num_epochs = 1000, print_cost = True, fit_intercept = True, penalty = 'l2', C = 1): costs = [] t = 0 m, n = X.shape params = initializeParameters(n, fit_intercept) v, s = initialize_params_adam(n, fit_intercept) num_mini_batches = math.ceil(m / mini_batch_size) for i in range(num_epochs): minibatches = random_mini_batches(X, y, mini_batch_size) epoch_cost = 0 for minibatch in minibatches: (mX, mY) = minibatch grads, cost = log_loss(mX, mY, params, penalty, C) epoch_cost += cost t += 1 params, v, s = update_params_adam(params, grads, v, s, t, lr, beta1, beta2, epsilon) if print_cost: costs.append(epoch_cost / num_mini_batches) print("Cost at epoch %i: %f" %(i + 1, epoch_cost / num_mini_batches)) return params, costs When I run the full program I get this output: Cost at epoch 1: 15593.325359 Cost at epoch 2: 9042.439993 Cost at epoch 3: 9616.118761 Cost at epoch 4: 10005.095845 Cost at epoch 5: 10436.960861 Cost at epoch 6: 10914.245428 Cost at epoch 7: 11425.692792 Cost at epoch 8: 11959.885890 Cost at epoch 9: 12507.349951 Cost at epoch 10: 13060.884386 Cost at epoch 11: 13615.315131 Cost at epoch 12: 14167.092496 Cost at epoch 13: 14713.897392 Cost at epoch 14: 15254.310444 Cost at epoch 15: 15787.553480 ... and yet the model perfectly classifies the toy data. What is going on? EDIT: I just tried lower the learning rate and that definitely seemed to help, but it still goes down for a while and then slowly goes up a bit. Could this be because the learning rate is still too large When we are near the minimum?
