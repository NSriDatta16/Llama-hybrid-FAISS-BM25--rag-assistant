[site]: crossvalidated
[post_id]: 214196
[parent_id]: 
[tags]: 
Which of the 3 cases should my data matrix belong to ideally?

I found this question , and while useful, I wanted to ask something more spcific: I am trying to get a good handle/intuition for the two types of data dimensionalities (number of data samples, and the number of features of each sample), within the machine learning context. So, let us consider an input data matrix $X$, of size $m$ x $n$, ($m$ rows by $n$ columns), where $m$ is the number of features, and $n$ is the number of data samples we have. (For example, $n=10$ means we have, say, data on 10 people, while $m$ here would be the number of features we have per person, so if $m=3$, then we might have say, height, weight, and age. Thus, $X$ would be a $3$x$10$ matrix). At any rate, for any data matrix $X$, we can have three possibilities: Either there are more features than samples, so the matrix is long and thin. (ie, more rows than columns, $m$ >> $n$) Either there are more samples than features, so the matrix is short and fat. (ie, more columns than rows, $n$ >> $m$) Or lastly, we have a square matrix, where the number of features is equal to the number of samples. ($n$ = $m$) My question is, how do either of those conditions on a data matrix $X$ affect whether or not we can properly learn/predict from our data, in the machine learning context? Is one of those conditions always bad? Is one always good? Is there an ideal we just aim to strive for? Thanks.
