[site]: crossvalidated
[post_id]: 458759
[parent_id]: 458732
[tags]: 
You're correct that an auto-encoder outputs the same dimension as the input, but it goes through a smaller hidden layer. Imagine a series of layers, from input to output, each with the following number of neurons: 1000 : 500 : 100 : 500 : 1000 The 1000-dimensional input is squeezed through a 100-dimensional middle layer. Once trained, if you remove the last two layers, and only use: 1000 : 500 : 100 the 100-dimensional middle is a reduced dimensional representation, pure an encoding. You could use that as input to another learning method - another neural network or something else. Think of it like PCA in which you only use the first few PCs. Principal components regression works in this way, but uses a linear projection of the data to lower dimensions, whereas the auto-encoder uses a non-linear protection.
