[site]: stackoverflow
[post_id]: 3216440
[parent_id]: 
[tags]: 
Probabilistic Generation of Semantic Networks

I've studied some simple semantic network implementations and basic techniques for parsing natural language. However, I haven't seen many projects that try and bridge the gap between the two. For example, consider the dialog: "the man has a hat" "he has a coat" "what does he have?" => "a hat and coat" A simple semantic network, based on the grammar tree parsing of the above sentences, might look like: the_man = Entity('the man') has = Entity('has') a_hat = Entity('a hat') a_coat = Entity('a coat') Relation(the_man, has, a_hat) Relation(the_man, has, a_coat) print the_man.relations(has) => ['a hat', 'a coat'] However, this implementation assumes the prior knowledge that the text segments "the man" and "he" refer to the same network entity. How would you design a system that "learns" these relationships between segments of a semantic network? I'm used to thinking about ML/NL problems based on creating a simple training set of attribute/value pairs, and feeding it to a classification or regression algorithm, but I'm having trouble formulating this problem that way. Ultimately, it seems I would need to overlay probabilities on top of the semantic network, but that would drastically complicate an implementation. Is there any prior art along these lines? I've looked at a few libaries, like NLTK and OpenNLP, and while they have decent tools to handle symbolic logic and parse natural language, neither seems to have any kind of proabablilstic framework for converting one to the other.
