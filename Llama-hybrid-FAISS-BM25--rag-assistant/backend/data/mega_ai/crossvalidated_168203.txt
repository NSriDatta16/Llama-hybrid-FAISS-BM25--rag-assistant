[site]: crossvalidated
[post_id]: 168203
[parent_id]: 168201
[tags]: 
You could append the 15 dimensional vector to the 64 dimensional vector, obtaining a 79 dimensional vector. You can then reduce the dimensionality by projecting on eigensubspaces. In general however, there are infinitely many ways to do this, so you need to come up with a way some sort of objective criterion for evaluating the quality of your embedding. word2vec attempts to maximize the predictive ability of the embedding in skip-grams. What you could do is re-train word2vec, but using the 15 dimensional vectors as a side information in the network to predict the missing word.
