[site]: crossvalidated
[post_id]: 294418
[parent_id]: 294033
[tags]: 
There are some differences between the CART and C4.5 algorithms for building decision trees. For instance, CART uses Gini Impurity to pick features while C.4.5 uses Shannon Entropy. I don't think the differences are relevant for the answer, so I will not differentiate between those. What makes decision trees faster than you would think is: As others have said, these algorithms are 1-lookahead algorithms. They perform local optimizations. At every branch, they choose the rule which maximizes/minimizes whatever metric they use (Gini or Entropy). This means they might miss rules where using a logical operator such as and would result in a better tree. This means you should be very careful/clever when doing feature engineering. For example, say you are trying to predict how much people drink, you might want to feature engineer things like new_feature = hour > 22 & hour . Decision trees might miss such rules, or give them less importance than they should. More importantly, the metrics used by decision trees can be computed incrementally. Say that you have a feature $X_1 = \{3,1.5,2.5,2,1\}$. The decision tree does not need to compute the metric for X , then compute the metric again for X , then again for X , etc. Gini and Entropy were chosen because they can be computed incrementally. First of all, each feature is sorted, so that you have $X_1 = \{1,1.5,2,2.5,3\}$. Secondly, when you compute X , you can use the result to easily compute X . It's like doing an average. If you have an average of a sample, $\bar x$, and I give you another value $v$, you can cheaply update your average doing, $\bar x \leftarrow \frac{n\bar x+v}{n+1}$. Gini coefficient is calculated as a fraction of sums, which can be easily incrementally computed for the sample. Decision trees can be parallelized. Each node is composed of two branches which are independent. Therefore, at each branch, you have the opportunity to parallelize the tree creation. Furthermore, the feature selection itself can also be parallelized. This is what makes packages like xgboost so fast. Gradient boosting is sequential and cannot be parallelized, but the trees themselves can.
