[site]: crossvalidated
[post_id]: 420508
[parent_id]: 420502
[tags]: 
We have to be thinking about a model to answer your question so let's assume a linear model. For convenience, we'll use sums of squared deviations instead of variances; to translate for variances, divide through the sums of squares by $N - 1$ . Let $Z = (z_1, ..., z_N)$ be your data; it has sum of squared deviations $\sum_{i = 1}^N (z_i - \bar{z})^2$ . If you decide to estimate $Z$ as $\hat{Z} = \beta_0 + \beta_1 X + \beta_2Y + \varepsilon$ , then you obtain estimates $\hat{Z} = (\hat{z}_1, ..., \hat{z}_N)$ for $Z$ ; its mean is the same as $Z$ 's mean. It is a fact that the sample variance of $\hat{Z}$ is less than that of $Z$ , intuitively because we have constrained it to be on a line. Their variance is only the same if the data is exactly linear; therefore the idea is that by trying to capture $Z$ with this estimate, you are trying to capture the variation of $Z$ . So the more variance $\hat{Z}$ captures, the closer the data is to being exactly linear. The following identity holds (called the ANOVA decomposition): $$\underbrace{\sum_{i = 1}^N (z_i - \bar{z})^2}_{\text{TSS}} = \underbrace{\sum_{i=1}^N (z_i - \hat{z}_i)^2}_{\text{RSS}} + \underbrace{\sum_{i=1}^N (\hat{z}_i - \bar{z})^2}_{ESS} $$ So the total sum of squares (TSS) of $Z$ breaks up into the explained sum of squares (ESS), which is the (unnormalized) variance of the fitted data. This is the "explained variance". The residual sum of squares (RSS) is how much the real data still differs from your fitted data---the "unexplained variance". To get a proportion of explained or unexplained variance, you can divide either by TSS. The proportion of explained variance, $ESS/TSS$ is called the $R^2$ value and measures the quality of fit. The language of explained/unexplained variance isn't always useful; I really only see it with linear regression and PCA. Also, explaining as much variance as possible isn't the best idea if you want to do prediction, since this is overfitting. When you do something like ridge regression, you get a biased estimate which would "explain less variance"---the $R^2$ on the data will be worse---but the reason you do ridge regression is because the test error will usually be better. (For prediction, more important than the ANOVA decomposition is the bias-variance decomposition.)
