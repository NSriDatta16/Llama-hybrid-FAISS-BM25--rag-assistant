[site]: crossvalidated
[post_id]: 54619
[parent_id]: 54506
[tags]: 
I suggest that instead of treating the $m_i$ as missing data, you integrate them out, since this marginalization leaves the form of the data distribution unchanged (in the sense that it's still a multivariate normal distribution). Marginally of the $m_i$, $\mathrm{E}(y_i)=\mu$ and $\mathrm{Cov}\left(y_{i},y_{j}\right)=\begin{cases}2\sigma_{m}^{2}+\sigma_{y}^{2}, & i=j,\\-\sigma_{m}^{2}, & \left|i-j\right|=1,\\0, & \mathrm{otherwise.}\end{cases}$ That's a nice sparse covariance matrix right there. That leaves you with a three-parameter problem, and in fact $\mu$ can be marginalized out too if the prior is conjugate or flat, so now you're down to a 2D posterior distribution. Surely STAN can handle that -- but even if it can't, you can just plot the damn thing, integrate it numerically, and use old reliable . According to Gelman's Bayesian Data Analysis, 2$^{nd}$ ed. , equation 14.14 on page 375, for a flat prior on the mean parameter, $p(\Sigma_y|y) \propto p(\Sigma_y)|\Sigma_y|^{-1/2}|V_{\beta}|^{1/2}\exp\bigg(-\frac{1}{2}(y-X\hat{\beta})^T\Sigma_y^{-1}(y-X\hat{\beta})\bigg),$ in which $y$ is the complete data vector, $X$ is, in your case, a vector of ones, $\Sigma_y$ is the covariance matrix specified by the above expression for the covariance, (and hence $p(\Sigma_y|y) = p(\sigma^2_m,\sigma^2_y|y)$) and $\hat{\beta} = (X^T\Sigma_y^{-1}X)^{-1}X^T\Sigma_y^{-1}y$ $V_{\beta} = (X^T\Sigma_y^{-1}X)^{-1}$ Note that these above expressions make $\hat{\beta}$ and $V_{\beta}$ functions of the unknown parameter $\Sigma_y^{-1}$. As for the prior $p(\sigma^2_m,\sigma^2_y)$, a good place to start is Gelman's 2006 paper Prior distributions for variance parameters in hierarchical models .
