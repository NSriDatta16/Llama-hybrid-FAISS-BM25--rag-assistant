[site]: crossvalidated
[post_id]: 399636
[parent_id]: 399596
[tags]: 
The question reflects on a misunderstanding of the respective roles of MCMC algorithms and Bayesian inference principles, and calls for some prior investment in the basics or core of what constitutes Bayesian inference. The answer below cannot address this missing background and I cannot but recommend investing some time in an introductory Bayesian textbook like Albert or Kruschke . MCMC is a tool to simulate from a posterior distribution, not a method of (Bayesian) inference per se. This means that it requires two inputs: the distribution of the data, written as a density $f(x;\theta)$ , which also writes as the likelihood $\ell(\theta|x)$ . This function has to be specified completely, possibly as a marginal across latent variables, i.e., as an integral over these variables. In which cases the latent variables are added to the parameter to define an augmented target, of which the true target is a marginal. ( Pseudo-marginal Metropolis-Hastings techniques partake from the same principle.) the prior distribution on the parameter $\theta$ , written as a density $\pi(\theta)$ . Again, this function has to be specified completely. (Or again be expressed as a marginal.) Given those two elements, MCMC aims at simulating from the distribution with density $\pi(\theta|x)$ which is proportional to $\pi(\theta)\times \ell(\theta|x)$ . It can operate without the normalising constant. And without these elements, MCMC makes no sense. Concerning the proposal distribution, this is a technical component in some of the MCMC algorithms, namely a new value for the Markov chain is proposed from a certain distribution and accepted or rejected according to an acceptance probability, computed in such a way that the stationary distribution of the Markov chain is the posterior.
