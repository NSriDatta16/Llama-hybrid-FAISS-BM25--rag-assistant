[site]: crossvalidated
[post_id]: 584745
[parent_id]: 584742
[tags]: 
Projecting to a low number of dimensions is mostly to help visualize the information for humans (as you suggest, there's absolutely no reason higher dimensions might not be better otherwise for many cases). There are a lot of examples of representing things in higher embedding spaces, e.g. the word2vec embeddings for words (see here or here - similar ideas are in a sense used in modern transformer neural network language models) that often use about 100 to 300 dimensions, embeddings for categorical data (like capturing the properties of each shop of a retailer for which in the example 10 dimensions were used and then projected to 2D for plotting; see also the section on category embeddings in a popular recent Deep Learning book, which I think also offers a rule of thumb for how to choose the dimension ), or each row of a dataset (e.g. through a denoising autoencoder in the example I think a few 1000 dimensions were used).
