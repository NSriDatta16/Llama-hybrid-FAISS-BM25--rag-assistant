[site]: stackoverflow
[post_id]: 4806694
[parent_id]: 573863
[tags]: 
Ultimately, a standard is meant provide convenience to the developers. By being "standard", it acts as a proxy for a set of disparate requirements, so if you follow the standard: know your code will work across different technologies (browser, platforms, versions) know your code will work for different people (accessibility) client accepts your code based on that fact (project requires standards compliance) other stuff I'm not thinking of The track record of "saving us time" for web standards is pretty mixed: we'd often spend hours fighting to get validation, only to find out browsers don't support the standards. For me, getting pages to validate usually does save time. Back to the question at hand: when to use them. Here's my pragmatic take: take another look at your requirements (in terms of browsers, platforms, versions, accessibility). Compare the approach of aiming for all those vs. aiming for a standard plus whatever the standards lack. Is adding a "standard" to the list of requirements save you time, or just add another burden to completing the project?
