[site]: crossvalidated
[post_id]: 20146
[parent_id]: 
[tags]: 
Is this hypothesis test somehow "optimal"?

I have a system whose output I am analyzing. I've used a very simple hypothesis test with great results and now am curious why did I get such great results -- did I perhaps stumble on some kind of an "optimal" test without much formal statistics training? My system can be in two states. When it's in state 0 the output can is independently drawn from standard normal distribution $\mathcal{N}(0,1)$ (well, technically, very, very close approximation to it-- but it's normal enough for my purposes). When it's in state 1, the output is still independently normally-distributed with variance 1 but the means are perturbed. Thus, the output $i$ in state 1 has a mean-shifted normal distribution $\mathcal{N}(\mu_i,1)$. For large number $n$ of observations of the system in state 1, the average $\frac{1}{n}\sum_{i=1}^n\mu_i=0$. I also know that quantity $\frac{1}{n}\sum_{i=1}^n\mu_i^2=M I devised the following hypothesis test to determine whether the system is in state 0 or 1, where the null hypothesis $H_0$ corresponds to the system being in state 0, and alternate $H_1$ to state 1. First, I collect a sequence of $n$ observations $\{x_i\}_{i=1}^n$. Then I compute the following test statistic: $$S_n=\left(\frac{1}{n}\sum x_i^2\right)-1$$ I then pick a threshold $t>0$ and accept null hypothesis if $S_n This test just made sense to me and is surprisingly easy to analyze (at least surprising to me without much formal background in mathematical statistics). The mean of $S_n$ when the system is in state 0 is 0, and the variance is $2/n$. In state 1 the mean of $S_n$ is $M$ and the variance is $(4M+2)/n$. Thus, I can upper-bound both the false-positive error probability $\alpha$ and the probability that I will accept the null hypothesis in error $\beta$ for a given $n$ and $M$ using the Chebyshev's Inequality : $$\begin{array}{rcl}\alpha&\leq&\frac{2}{nt^2}\\ \beta&\leq&\frac{4M+2}{n(M-t)^2} \end{array}$$ Using the two inequalities I can pick an appropriate threshold $t$ and the number of observations $n$ I need to classify the state of the system for some particular value of $M$ given some tolerance for errors $\alpha$ and $\beta$. The math is simple and completely makes sense: for some fixed $\alpha$, as I increase $n$, my threshold $t$ decreases, and so does $\beta$. Is the reason for this beauty and simplicity that the test I devised is in some way "optimal"? If so, can the community suggest a way prove this optimality? I did find "Uniformly Most Powerful tests" in a stats book, but I am not sure if I have that (the book was quite confusing)...
