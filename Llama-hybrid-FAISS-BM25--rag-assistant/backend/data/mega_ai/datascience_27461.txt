[site]: datascience
[post_id]: 27461
[parent_id]: 
[tags]: 
Parameter tuning for machine learning algorithms

When it comes to the topic of tuning parameters, most of the time you read grid search. But if you have 6 parameters, for which you want to test 10 variants, you get to 10^6 = 1000000 runs. Which in my case would be several months of processing time. That's why I was looking for an alternative. On the Kaggle website, I found a tutorial that uses a different approach. It almost works like this: 1) Test all 6 parameter individually with the other 5 parameters as default value and plot the results 2) Change the default values for alle 6 parameters to the best value of the associated plot 3) Test all 6 parameter individually with the other 5 parameters as last best value and plot the results 4) Repeat step 2 and 3 until the results does not change anymore This approach has the advantage of requiring much fewer runs. Is this a scientifically accepted procedure? And does this approach have a name in the literature?
