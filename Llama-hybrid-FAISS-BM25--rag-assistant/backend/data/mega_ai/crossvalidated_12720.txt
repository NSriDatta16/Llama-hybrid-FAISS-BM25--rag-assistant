[site]: crossvalidated
[post_id]: 12720
[parent_id]: 12612
[tags]: 
One approach would be to model the $X$'s with a generalized linear model (GLM). Here, you would formulate $p_i$, the probability of success on the $i$'th trial as a (logistic linear) function of the recent observation history. So you're essentially fitting an autoregressive GLM where the noise is Bernoulli and the link function is logit. The setup is: $p_i = f(b + a_1 X_{i-1} + a_2 X_{i-2} + \ldots a_k X_{i-k})$, where $f(x) = \frac{1}{1+\exp(x)}$, and $X_i \sim Bernoulli(p_i)$ The parameters of the model are $\{b, a_1, \ldots a_k\}$, which can be estimated by logistic regression. (All you have to do is set up your design matrix using the relevant portion of observation history at each trial, and pass that into a logistic regression estimation function; the log-likelihood is concave so there's a unique global maximum for the parameters). If the outcomes are indeed independent then the $a_i$'s will be set to zero; positive $a_i$'s mean that subsequent $p_i$'s increase whenever a success is observed. The model doesn't provide a simple expression for the probability over the sum of the $X_i$'s, but this is easy to compute by simulation (particle filtering or MCMC) since the model has simple Markovian structure. This kind of model has been used with great success to model temporal dependencies between "spikes" of neurons in the brain, and there is an extensive literature on autoregressive point process models. See, e.g., Truccolo et al 2005 (although this paper uses a Poisson instead of a Bernoulli likelihood, but the mapping from one to the other is straightforward).
