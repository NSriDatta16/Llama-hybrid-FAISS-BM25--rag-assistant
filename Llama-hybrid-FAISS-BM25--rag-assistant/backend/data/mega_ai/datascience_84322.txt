[site]: datascience
[post_id]: 84322
[parent_id]: 84316
[tags]: 
Each BERT variant is trained with text that has been prepared differently, e.g. as the name implies, BERT uncased is trained with text where all letters are lowercase. This means that the vocabulary extraction process has also use lowercase text as input, and therefore gives as result a different vocabulary than the same vocabulary extraction process used with text in its original casing. Note that, as the vocabularies are different, each model should be used with the tokenizer it was used to train it. Using a model with a different tokenizer may lead to bad results. The choice of tokenizer, therefore, is tied to the choice of BERT model. The criteria to use one or the other (e.g. uncased vs. cased), depends on the case. For instance, for doing named entity recognition (NER) in English, it may be important to keep the original casing so that the model can more easily distinguish proper nouns.
