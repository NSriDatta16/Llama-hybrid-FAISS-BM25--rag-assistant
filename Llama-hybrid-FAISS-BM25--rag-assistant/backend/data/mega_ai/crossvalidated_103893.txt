[site]: crossvalidated
[post_id]: 103893
[parent_id]: 95250
[tags]: 
One could possible estimate the logistic regression model by minimizing the classification error, but there is usually no reason why to do so! Why do you want to do it? But, such questions have been asked here before, so I will not rewrite an answer, very good answers can be found to this question: Logistic regression: maximizing true positives - false positives Basically, minimizing misclassification error amounts to using a score function which is not a proper scoring rule, see: https://en.wikipedia.org/wiki/Scoring_rule If misclassification is minimized by some parameter vector $\beta$, it will be also minimized by many other values of $\beta$ in some vicinity of the first $\beta$. In other words, the criterion function is flat around the maximum! To see this last fact, we explore the connection with scoring rules (se wiki above). We specialize to the case with only a binary variable, possible values 0 or 1, with distribution given by a probability vector $p=[p_1, p_2]$. Let the random variable be $X$, the forecaster makes a probabilistic forecast $r=[r_1,r_2]$, a probability vector. Let $S$ be a score function. This means that if the forecaster forecasts $r$, then $X=x$ is observed, he receives the reward $S(r,x)$. This reward then has expected value $E S(r,X)=p_1 S(r,1)+p_2 S(r,2)$ and we say the scoring rule (reward) is proper if this expectation is maximized (under $p$) by forecasting $r=p$. It is strictly proper if that maximum is unique . Trying to minimize the misclassification rate corresponds to using the following score function: $$ S(r,i)=\begin{cases} 1 ~~\text{if $p_i=\max(p_1,p_2)$} \\ 0 ~~\text{otherwise} \end{cases} $$ $(i=1,2)$ Now, consider if the true $p=[0.99,0.01]$. If the forecaster then reports $r=p=[0.99, 0.01]$ then his expected reward becomes $$ E S(r,X) = p_1 S(r,1)+ p_2 S(r,2) = p_1 $$ Consider then if the forecaster report $r=s$, some other probability vector such that $s_1 > s_2$, that is $s_1 > 0.5$. Then you can calculate as above, and find the exact same expected reward! So this score function fails to reward truthful forecasting, it only depends on the event actually forecasted having been given a probability larger than one half. That is the reason use of this will not lead to very effective learning! and so should be avoided.
