[site]: crossvalidated
[post_id]: 9345
[parent_id]: 
[tags]: 
Why doesn't it seem to be standard to multiply prior scale by fraction of non-zero predictor observations?

When doing multivariate regression, it's often the case that some predictors often have many zero values - dichotomous inputs, dummy coding of polychotomous inputs, interval coding, etc. The fraction of nonzero observations for a predictor is especially decreased when interacting these variables. It seems obvious that only the observations of a predictor for which it is nonzero (after any transformations) will affect the estimation of its coefficient. When doing multivariate regression I like to use weakly informative priors (like Gaussian or Cauchy) for regularization. I've noticed that sometimes, predictors which are usually zero still take on implausible regression coefficients, like ~6 for a logistic regression model. This seems to be because a variable that is nonzero for only a handful of observations is still overcoming the prior because it's getting full credit for all of its zero observations, and thus getting a huge coefficient, which is in many cases a prior obviously overfitting, and hurts generalization performance for prediction. Multiplying the prior's scale by the fraction of observations of a predictor that are nonzero fixes this and greatly improves generalization performance, makes coefficients plausible, and seems to be common sense. So - why don't more people do this? I've tried a cursory search in common textbooks (various machine learning books, applied and theoretical Bayesian statistics), R packages on CRAN, some machine learning packages, and I don't see evidence of anyone else doing this... isn't this just common sense? Or is there something wrong with this approach that I'm missing?
