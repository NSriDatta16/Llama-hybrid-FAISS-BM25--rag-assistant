[site]: datascience
[post_id]: 52031
[parent_id]: 30555
[tags]: 
Avoid early stopping and stick with dropout. Andrew Ng does not recommend early stopping in one of his courses on orgothonalization [1] and the reason is as follows. For a typical machine learning project, we have the following chain of assumptions for our model: Fit the training set well on the cost function ↓ Fit the dev set well on the cost function ↓ Fit the test set well on the cost function ↓ Performs well in the real world And what we want are tools that can target one of these four objectives and not the others in order to keep improving our models more efficiently, and this concept is called "orthogonalization." An analogy would be the different options in photo editing apps such as brightness, contrast, and saturation adjustments, which are independent of, or "orthogonal" to one another. Examples of orthogonal optimization strategies for the four goals are listed below: Fit the training set well on the cost function (e.g. bigger neural network; Adam optimization) ↓ Fit the dev set well on the cost function (e.g. regularization; bigger training set) ↓ Fit the test set well on the cost function (e.g. bigger dev set) ↓ Performs well in the real world (e.g. change the test set; change the cost function) Because early stopping both fits the training set less well and improves the dev set performance at the same time, it is not orthogonal and Ng advises us not to use it. Reference: [1] Week 1 of Course 3: Structuring Machine Learning Projects of Coursera Deep Learning Specialization
