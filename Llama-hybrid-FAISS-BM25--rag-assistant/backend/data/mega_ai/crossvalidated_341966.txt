[site]: crossvalidated
[post_id]: 341966
[parent_id]: 341961
[tags]: 
If I understood your question correctly, you want to know which layer do the weights and biases (or lines that connect the nodes in the following figure) belong to: The answer is they belong to the layer they are going to , not the one from which they are coming from. Explanation : In a neural network: The input layer has no parameters (it just contains the network's input). We'll refer to this layer as $X$. The hidden layer has a weight matrix (we'll call it $W_h$ whose dimensions are $(number\_of\_ inputs \;\cdot \; number\_of\_neurons)$, a bias matrix ($b_h$) whose dimensions are $(number\_of\_neurons)$ and an activation function ($f_h$). The output of this layer is $y_h = f_h(W_h \cdot X + b_h)$. The weights and biases between the input and hidden layer clearly belong to this layer . The output layer has a weight matrix ($W_o$) whose dimensions are $(number\_of\_ neurons \;\cdot \; number\_of\_outputs)$, a bias matrix ($b_o$) whose dimensions are $(number\_of\_outputs)$ and an activation function ($f_o$). The output of this layer is $ \hat y = f_o(W_0 \cdot y_h + b_o)$. The weights and biases connecting the hidden with the output layer belong to this layer .
