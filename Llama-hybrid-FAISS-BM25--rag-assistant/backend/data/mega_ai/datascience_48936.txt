[site]: datascience
[post_id]: 48936
[parent_id]: 33345
[tags]: 
This is meant to be a comment, but I can't comment since I have insufficient reputation. As for the second question, intuitively speaking, instead of taking a scalar value for an action, which initially, may be highly inaccurate and noisy, taking a distribution instead would be more accurate. I'd recommend https://flyyufelix.github.io/2017/10/24/distributional-bellman.html which explains the intuitive reason for using a distribution In terms of convergence, actually, there is no guarantee of convergence. In the paper, however, explains that for distributional DQN to guarantee to converge, the gamma-contraction must be satisfied, which would be true if you measure the the distance between the distributions using wasserstein distance, but it would be impractical to try to minimize that distance, so distributional DQN uses cross entropy instead which you can find the gradients of, and perform backpropagation....etc You may be interested in "Distributional Reinforcement Learning with Quantile Regression" https://arxiv.org/pdf/1710.10044.pdf which aims to improve the original distributional DQN algorithm
