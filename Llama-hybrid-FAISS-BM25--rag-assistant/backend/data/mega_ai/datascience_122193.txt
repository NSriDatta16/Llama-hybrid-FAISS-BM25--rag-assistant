[site]: datascience
[post_id]: 122193
[parent_id]: 122191
[tags]: 
ELMo provides contextual word embeddings, but they are completely different to BERT's and GPT's in the following ways: ELMo processes its input at the character level by means of character n-gram convolutions. This allows ELMo to mitigate the out-of-vocabulary (OOV) problem, because it can represen words that is has not seen during training as long as they share the same script (Lating script, Cyrillic, etc) as the training data (see more details here ). This architecture was proposed by kim et al. (2015) , and is summarized well in one of the figures of the paper: On the other hand, BERT and GPT use subwords as inputs. At its core, ELMo uses bidirectional LSTMs to perform its processing, while BERT uses a Transformer encoder and GPT a Transformer decoder. ELMo generates word-level contextual embeddings, while BERT and GPT generate subword embeddings. The training objective of ELMo is causal [bidirectional] language modeling (i.e. predicting the next word, in both directions of the text), while BERT uses a masked language modeling objective (i.e. predicting the masked token) and GPT a causal language modeling objective (i.e. predicting the next token).
