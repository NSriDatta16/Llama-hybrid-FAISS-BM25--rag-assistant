[site]: crossvalidated
[post_id]: 614123
[parent_id]: 
[tags]: 
How to evaluate whether a treatment effect is significant given variance in measurement

I have two samples I am using to evaluate the effect of a treatment. Group $T$ contains 60 treated objects and their scores and group $C$ contains 60 control objects and their scores. The two groups are paired, so for every object in $T$ there is a corresponding object in $C$ that is identical in every way except for the treatment . Normally I would use a paired Permutation Test to evaluate whether the treatment results in a significant increase in score--randomly permuting each object in each pair between the two groups gives me a null distribution of my test statistic (average score in the group). However the catch in this case is that each of my scores also have a known distribution and I want my test to account for the variance in the scores. My gut tells me that I can still use a permutation test by first sampling scores from their distribution and then repeating the permutation test as normal (effectively getting an expected p-value over scores). However I don't think I've ever seen this done before and would love to hear if there is a better way I can go about this. Below is a concerete example to illustrate the point $$T=[X_t, Y_t,...]$$ $$C = [X_c, Y_c, ...]$$ Scores for $X_t = [0.5, 0.51, 0.48, ...]$ Scores for $X_c = [0.48, 0.46, 0.5, ...]$ Scores for $Y_t = [1, 1.2, 0.8, ...]$ Scores for $Y_c = [0.9, 0.95, 0.7, ...]$ Here we have paired objects between $T$ and $C$ which have different means associated with each object and also different levels of variance between pairs. My question is how I should be accounting for the fact that there is variance in the scores for each object in my permutation test. Everything is in-silico so assume I can get arbitrarily many samples from distributions.
