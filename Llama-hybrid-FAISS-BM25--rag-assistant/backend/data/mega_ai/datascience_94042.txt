[site]: datascience
[post_id]: 94042
[parent_id]: 94040
[tags]: 
The most general and robust way is to double the training set using the "mirror image" of each sample. Also it is sound approach from a statistical learning perspective. There can be some approaches to symmetry where one hacks the NN architecture (eg by symmetrising the network) but these are usualy very fragile and do not generalise well , while the above approach is robust and lets the network adjust itself as it needs. A similar but cheaper approach is to sort the samples before feeding them to the network. Eg for your example: If you have this input [1, 1, 1, 0, 1, 0] then the sorted related mirror input (in lexicographic order) is the [0, 1, 0, 1, 1, 1] (reverse). So you train the NN only the sorted lexicographic inputs and before testing each sample you convert it to the sorted lexicographic related input (by reversing it) if it is not already in that form. So effectively you normalise the samples (by reversing them if necessary) both before training and before testing. This is similar approach to doubling the train set, only it is cheaper (and in practice exactly accurate , while doubling the training set is only theoreticaly accurate, in practice it may have discrepancies).
