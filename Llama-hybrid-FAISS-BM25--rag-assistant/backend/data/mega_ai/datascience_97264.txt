[site]: datascience
[post_id]: 97264
[parent_id]: 97260
[tags]: 
Finding corpora for NLP research can be hit and miss, my advice would be to study the availability of adequate data when deciding about the research direction, not afterwards. Of course this completely depends on the type of requirement for the data. In case you have to create your own corpus, design the corpus collection and annotation very carefully because papers with weaknesses in the data collection can be rejected (at least in selective venues). There's no particular problem about collecting text data from the web, as long as this can be justified (for instance social media is not a good source for grammatically correct sentences ;) ). Honestly I'm not aware of any simple way to find corpora. Here are some sources: The Linguistic Data Consortium has a catalog of corpora, some free and some not. ELDA also has a catalog, it's also a semi-commercial provider. The LRE Map is a repository (also by ELDA) for people to register their research data and software. A major source of quality data are the various shared tasks which are often organized jointly with major conferences. It's very task specific though. For the rest it's often about following specific parts of the domain, for example if you find papers related to your task of interest check where the authors found their data, whether they make some data available on their webpage, etc. For phrasal verbs the PARSEME Shared Task corpora might suit your needs.
