[site]: datascience
[post_id]: 41289
[parent_id]: 38440
[tags]: 
For reference, I've come up with a slightly hacky solution to this. It only works because the second set of features $\underline{y}$ , in my use case, contains the entire first set of features $\underline{x}$ , plus one extra feature (but this hack would also work if it contained $\underline{x}$ plus multiple feature, but all of $\underline{x}$ need to be contained in $\underline{y}$ ). Before training the first model, I set all of the features contained in $\underline{y}$ but not $\underline{x}$ to a constant, while storing their real values somewhere. I then construct the relevant dmatrix and train an XGBoost model which implements binary cross-entropy manually, rather than using XGBoostClassifier (I think this stop might not be necessary, I think one can use out of the box loss functions, even when using the generic bst = xgboost.train method) When this model exits due to early stopping, I then set the features contained in $\underline{y}$ but not $\underline{x}$ to their true values, which I had stored, and then recreate the relevant dmatrix . Finally, I continue to train my already trained xgboost, object, using xgboost.train( ..., xgb_model=bst) . This continues to train the old model beyond the point where early stopping used to kick in, it now manages to continue to improve the training loss, as it now has access to more features (which do, it turns out, have an impact on the target variable, even after all $\underline{x}$ - dependence has been controlled for). So yes, a bit of a hack, but I don't have a better solution for now. If $\underline{x}$ contained any features not contained in $\underline{y}$ , this would not work, but that should never be the case in this sort of causal inference problem.
