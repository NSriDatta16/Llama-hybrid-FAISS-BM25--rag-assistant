[site]: crossvalidated
[post_id]: 583931
[parent_id]: 583930
[tags]: 
You can certainly stack your classifiers: apply your classifiers to yield classification probabilities in-sample, then train another model on the ground truth, using the two probabilities as predictors. Alternatively, you could assess the quality of your classifications, e.g., using proper scoring rules , and transform these into a weight. This is easier, and you don't need to fit a third model. As Alberto notes , you could also just create a model that uses all features, without fitting two separate models. In any case, note that unweighted combinations often outperform "optimally weighted" ones (the "forecast combination puzzle"). One explanation is that finding "optimal" weights introduces additional variance, which passes right through to your final predictions (Claeskens et al., 2016, IJF ).
