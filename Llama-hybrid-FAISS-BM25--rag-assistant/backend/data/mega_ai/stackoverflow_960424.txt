[site]: stackoverflow
[post_id]: 960424
[parent_id]: 960418
[tags]: 
... So this is the second fallacy of teleology - to attribute goal-directed behavior to things that are not goal-directed, perhaps without even thinking of the things as alive and spirit-inhabited, but only thinking, X happens in order to Y. "In order to" is mentalistic language, even though it doesn't seem to name a blatantly mental property like "fearful" or "thinks it can fly". â€” Eliezer Yudkowsky, artificial intelligence theorist concerned with self-improving AIs with stable goal systems Bertrand Meyer's homily suggests that sound reasoning about systems is grounded in knowing what concrete entities are altered by the system; the purpose of the alterations is an emergent property.
