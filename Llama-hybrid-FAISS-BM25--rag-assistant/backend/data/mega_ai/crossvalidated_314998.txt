[site]: crossvalidated
[post_id]: 314998
[parent_id]: 
[tags]: 
Is there a way to make some features more important than others in a SVM classifier?

I am using sklearn library to train the default SVM classifier on a bunch of data. My data has roughly 30 features. But I realize some of these features are "more important" than others. For example, I want the change in features 1, 2, and 3 to greatly impact the learning process, whereas, features 4 through 25 are useful information, but not as critical. If I just multiply the numbers in features 1 to 3 by say, 100, it won't make much difference because I believe the algorithm normalizes the features anyway. So my question is, how can I give a weight to the features in an SVM classifier, so that some of them influence the learning and prediction more than others? Here's the very crude version of the code I'm using: clf = SVC(probability=True, gamma=0.1, C=1)) data = np.array([1, 4, 1, 6, 2, 5, 7, 2, ... ]) target = np.array([0, 1, 0, 0, 1, ... ]) clf.fit(data, target)
