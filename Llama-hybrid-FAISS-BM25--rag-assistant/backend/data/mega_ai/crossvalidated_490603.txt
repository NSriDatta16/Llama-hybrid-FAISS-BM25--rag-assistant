[site]: crossvalidated
[post_id]: 490603
[parent_id]: 490533
[tags]: 
Yes .... The issue was standard scaling. I needed to rescale X, by subtracting the component-wise mean (mu=np.mean(X, axis=0)) and dividing by the component-wise std (sigma = np.sqrt(np.var(X, axis=0)) to recover the correct result. It should be noted that in their code the authors of sklearn.svm are careful to avoid divide by zero errors, deal with issues involving sparse/non-sparse matrices and update incremental statistics using the following reference: "The algorithm for incremental mean and std is given in Equation 1.5a,b in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. "Algorithms for computing the sample variance: Analysis and recommendations." The American Statistician 37.3 (1983): 242-247:" I did not need to be so careful for my quick sanity check. Using the following rescaling was all that was needed to correctly use the Linear SVM's hyper-plane: mu = np.mean(X, axis=0) sigma_2 = np.var(X, axis=0) sigma = np.sqrt(sigma_2) #Slightly too slavish in mimicing sklearn's calculations # Could've just calc'ed np.std directly Xscale = (X-mu)/sigma Cscale = np.dot(A,Xscale.transpose()) + b Cscale = Cscale[0,:] y_calc = (Cscale > 0).astype(np.int) clf.score(X,y_calc)
