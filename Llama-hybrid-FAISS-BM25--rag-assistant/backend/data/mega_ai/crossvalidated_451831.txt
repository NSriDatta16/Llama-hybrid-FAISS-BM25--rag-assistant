[site]: crossvalidated
[post_id]: 451831
[parent_id]: 367028
[tags]: 
If you knew the No-Free Lunch Theorem (Wolpert & Macready), you would not get so hung up on one classifier and ask why it's not the best. The NFL Theorem states essentially that "in the universe of all cost functions, there is no one best classifier." Second, classifier performance always "depends on the data." The Ugly Duckling Theorem (Watanabe) states essentially that "in the universe of all sets of features, there is no one best set of features." Cover's Theorem states that if $p>n$ , i.e., the dimensionality of the data is larger than the sample size, then a binary classification problem is always linearly separable. In light of the above, as well as Occam's Razor , there is never anything that's better than anything else, independent of the data and cost function. I have always argued that CNNs by themselves are not ensembles of classifiers for which diversity (kappa vs error) can be assessed.
