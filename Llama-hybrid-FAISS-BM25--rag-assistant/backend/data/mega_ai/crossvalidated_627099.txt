[site]: crossvalidated
[post_id]: 627099
[parent_id]: 
[tags]: 
Neural network as alternative to conventional nonlinear regression?

Can I use the strength of neural networks for data analysis? Suppose I want to find a fit to a noisy sine curve $1 - V \sin(w \theta+ \phi)$ and I want to find an estimate for parameters V, $w$ and $\phi$ . Can such a parameter estimation be done using neural networks? If it is helpful, I am aware that Pytorch can certainly do some kind of modeling of noisy sine curves, but this won't translate to parameters. For example, the following Pytorch code uses a neural network to model a noisy sine curve: import torch import torch.nn as nn import torch.optim as optim import numpy as np import matplotlib.pyplot as plt np.random.seed(0) x = np.sort(np.random.uniform(-np.pi, np.pi, 200)) V = 0.3 real = 1 - V*np.cos(x) noise = np.random.normal(0, 0.1, real.shape) y = real + noise x = x[:, np.newaxis] y = y[:, np.newaxis] x_tensor = torch.tensor(x, dtype=torch.float32).flatten().unsqueeze(1) y_tensor = torch.tensor(y, dtype=torch.float32) net = nn.Sequential( nn.Linear(in_features=1, out_features=200), nn.Tanh(), nn.Linear(in_features=200, out_features=1), ) criterion = nn.MSELoss() optimizer = optim.SGD(net.parameters(), lr=0.01) epochs = 5000 for epoch in range(epochs): output = net(x_tensor) loss = criterion(output, y_tensor) optimizer.zero_grad() loss.backward() optimizer.step() if (epoch+1) % 500 == 0: print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, loss.item())) predicted = net(x_tensor).detach().numpy() plt.plot(x, y, 'ro', label='Original data') plt.plot(x, predicted, label='Fitted line') plt.legend() plt.show() print('visiblity found with value of: ', predicted) EDIT: As suggested in the comments, I have a new attempt at this. In this I generate a dataset of X,Y values associated with Y = 1-v Cos(wX+phi), for random values of (w, phi, and v). Then I try to train a neural network that has inputs (X,Y) and provides an output of (w, phi, and v), which is written as follows: import torch import torch.nn as nn import torch.optim as optim import numpy as np import matplotlib.pyplot as plt # Parameters for generating the data n_samples = 200 n_sets = 1000 # number of different sets of parameters # Generating x values x_values = np.linspace(-np.pi, np.pi, n_samples) x_data = np.tile(x_values, (n_sets, 1)) # Repeat x_values for each set of parameters V_values = np.random.uniform(0.1, 0.5, n_sets) phi_values = np.random.uniform(-np.pi, np.pi, n_sets) w_values = np.random.uniform(1, 5, n_sets) # Generating y data based on the parameters y_data = [] targets = [] for V, phi, w in zip(V_values, phi_values, w_values): y = 1 - V * np.cos(w * x_values + phi) noise = np.random.normal(0, 0.1, y.shape) y += noise y_data.append(y) targets.append([V, phi, w]) # Convert arrays to the tensor format input_data_tensor = torch.tensor(np.concatenate((x_data, y_data), axis=1), dtype=torch.float32) params_tensor = torch.tensor(targets, dtype=torch.float32) class ParamNet(nn.Module): def __init__(self): super(ParamNet, self).__init__() self.layer1 = nn.Linear(2 * n_samples, 600) self.activ1 = nn.LeakyReLU() self.layer2 = nn.Linear(600, 600) self.activ2 = nn.LeakyReLU() self.param_layer = nn.Linear(600, 3) # Parameter output layer def forward(self, x): x = self.activ1(self.layer1(x)) x = self.activ2(self.layer2(x)) params = self.param_layer(x) return params model = ParamNet() # Defining loss function and optimizer optimizer = optim.Adam(model.parameters(), lr=0.1) loss_fn = nn.MSELoss() # Training loop epochs = 5000 for epoch in range(epochs): params = model(input_data_tensor) loss = loss_fn(params, params_tensor) optimizer.zero_grad() loss.backward() optimizer.step() if (epoch + 1) % 500 == 0: print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}") # Predicting parameters predicted = model(input_data_tensor).detach().numpy() # Evaluating model performance mse = np.mean((predicted - targets)**2) print("MSE: ", mse) V_true = np.random.uniform(0.1, 0.5) phi_true = np.random.uniform(-np.pi, np.pi) w_true = np.random.uniform(1, 5) y_new = 1 - V_true * np.cos(w_true * x_values + phi_true) noise = np.random.normal(0, 0.1, y_new.shape) # you can vary the amplitude of the noise y_new += noise # Prepare the tensor for model x_values_tensor = torch.tensor(x_values, dtype=torch.float32).repeat(1, 1) # repeat x_values for each set y_new_tensor = torch.tensor(y_new, dtype=torch.float32).unsqueeze(0) # unsqueeze to add one more dimension for batch input_new_tensor = torch.cat((x_values_tensor, y_new_tensor), dim=-1) # concatenate along the last dimension # Pass the new data through the trained model predicted_params = model(input_new_tensor).detach().numpy()[0] V_pred, phi_pred, w_pred = predicted_params y_pred = 1 - V_pred * np.cos(w_pred * x_values + phi_pred) # Plot the original and predicted functions plt.figure(figsize=(10, 6)) plt.plot(x_values, y_new, label='Original function') plt.plot(x_values, y_pred, label='Predicted function', linestyle='--') plt.xlabel('x') plt.ylabel('y') plt.legend() plt.show() It can sort of do it...but it get stuck at some local minima.
