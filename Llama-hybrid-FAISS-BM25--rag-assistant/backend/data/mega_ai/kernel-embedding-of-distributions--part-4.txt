, x ) , … , k ( x n , x ) ) T {\displaystyle \mathbf {K} _{x}=\left(k(x_{1},x),\dots ,k(x_{n},x)\right)^{T}} Properties The expectation of any function f {\displaystyle f} in the RKHS can be computed as an inner product with the kernel embedding: E [ f ( X ) ] = ⟨ f , μ X ⟩ H {\displaystyle \mathbb {E} [f(X)]=\langle f,\mu _{X}\rangle _{\mathcal {H}}} In the presence of large sample sizes, manipulations of the n × n {\displaystyle n\times n} Gram matrix may be computationally demanding. Through use of a low-rank approximation of the Gram matrix (such as the incomplete Cholesky factorization), running time and memory requirements of kernel-embedding-based learning algorithms can be drastically reduced without suffering much loss in approximation accuracy. Convergence of empirical kernel mean to the true distribution embedding If k {\displaystyle k} is defined such that f {\displaystyle f} takes values in [ 0 , 1 ] {\displaystyle [0,1]} for all f ∈ H {\displaystyle f\in {\mathcal {H}}} with ‖ f ‖ H ≤ 1 {\displaystyle \|f\|_{\mathcal {H}}\leq 1} (as is the case for the widely used radial basis function kernels), then with probability at least 1 − δ {\displaystyle 1-\delta } : ‖ μ X − μ ^ X ‖ H = sup f ∈ B ( 0 , 1 ) | E [ f ( X ) ] − 1 n ∑ i = 1 n f ( x i ) | ≤ 2 n E [ tr ⁡ K ] + log ⁡ ( 2 / δ ) 2 n {\displaystyle \|\mu _{X}-{\widehat {\mu }}_{X}\|_{\mathcal {H}}=\sup _{f\in {\mathcal {B}}(0,1)}\left|\mathbb {E} [f(X)]-{\frac {1}{n}}\sum _{i=1}^{n}f(x_{i})\right|\leq {\frac {2}{n}}\mathbb {E} \left[{\sqrt {\operatorname {tr} K}}\right]+{\sqrt {\frac {\log(2/\delta )}{2n}}}} where B ( 0 , 1 ) {\displaystyle {\mathcal {B}}(0,1)} denotes the unit ball in H {\displaystyle {\mathcal {H}}} and K = ( k i j ) {\displaystyle \mathbf {K} =(k_{ij})} is the Gram matrix with k i j = k ( x i , x j ) . {\displaystyle k_{ij}=k(x_{i},x_{j}).} The rate of convergence (in RKHS norm) of the empirical kernel embedding to its distribution counterpart is O ( n − 1 / 2 ) {\displaystyle O(n^{-1/2})} and does not depend on the dimension of X {\displaystyle X} . Statistics based on kernel embeddings thus avoid the curse of dimensionality, and though the true underlying distribution is unknown in practice, one can (with high probability) obtain an approximation within O ( n − 1 / 2 ) {\displaystyle O(n^{-1/2})} of the true kernel embedding based on a finite sample of size n {\displaystyle n} . For the embedding of conditional distributions, the empirical estimate can be seen as a weighted average of feature mappings (where the weights β i ( x ) {\displaystyle \beta _{i}(x)} depend on the value of the conditioning variable and capture the effect of the conditioning on the kernel embedding). In this case, the empirical estimate converges to the conditional distribution RKHS embedding with rate O ( n − 1 / 4 ) {\displaystyle O\left(n^{-1/4}\right)} if the regularization parameter λ {\displaystyle \lambda } is decreased as O ( n − 1 / 2 ) , {\displaystyle O\left(n^{-1/2}\right),} though faster rates of convergence may be achieved by placing additional assumptions on the joint distribution. Universal kernels Let X ⊆ R b {\displaystyle {\mathcal {X}}\subseteq \mathbb {R} ^{b}} be a compact metric space and C ( X ) {\displaystyle C({\mathcal {X}})} the set of continuous functions. The reproducing kernel k : X × X → R {\displaystyle k:{\mathcal {X}}\times {\mathcal {X}}\rightarrow \mathbb {R} } is called universal if and only if the RKHS H {\displaystyle {\mathcal {H}}} of k {\displaystyle k} is dense in C ( X ) {\displaystyle C({\mathcal {X}})} , i.e., for any g ∈ C ( X ) {\displaystyle g\in C({\mathcal {X}})} and all ε > 0 {\displaystyle \varepsilon >0} there exists an f ∈ H {\displaystyle f\in {\mathcal {H}}} such that ‖ f − g ‖ ∞ ≤ ε {\displaystyle \|f-g\|_{\infty }\leq \varepsilon } . All universal kernels defined on a compact space are characteristic kernels but the converse is not always true. Let k {\displaystyle k} be a continuous translation invariant kern