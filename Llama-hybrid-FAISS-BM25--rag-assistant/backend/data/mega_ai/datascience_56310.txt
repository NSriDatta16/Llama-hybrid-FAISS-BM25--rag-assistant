[site]: datascience
[post_id]: 56310
[parent_id]: 56304
[tags]: 
If you are using a value-based method, like Q-learning in Deep Q Networks (DQN), then the "degree" concept has little meaning to the agent, and you are effectively training an agent to learn the best discrete action out of 200 actions. Yes this could take a lot longer than learning a simple 2-action scenario, as the agent will not easily learn that the value of "Right 49%" is correlated to the value of "Right 52%" in any particular state. This is a similar problem you might face in supervised learning if you wanted to move from classifying a dog vs cat image classifier to add bounding boxes for where the creature is. You don't add the bounding box as a large number of classifications - instead you add it as a regression, mapping form the image directly to some vector of real numbers. There might be ways to address this using value-based methods, but the usual approach here would be to move to a continuous action space (maybe with -1 for 100% Right and +1 for 100% Left), and use a Policy Gradient method. There are a few different choices of Policy Gradient, but they all share the basic principle of modelling a policy function - which might be a probability distribution over action space which you need to sample from, or a specific action - converting from state to action. The most basic Policy Gradient approach, which you could use as an introduction for your problem if it is simple enough, is REINFORCE. Algorithms like A3C, DDPG, PPO are based over this idea and can cope with more challenging environments.
