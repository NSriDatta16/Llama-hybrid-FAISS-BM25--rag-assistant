[site]: datascience
[post_id]: 24341
[parent_id]: 24335
[tags]: 
Support vector data description (SVDD) finds the smallest hypersphere that contains all samples, except for some outliers. One-class SVM (OC-SVM) separates the inliers from the outliers by finding a hyperplane of maximal distance from the origin. If the kernel function has the property that $k(\mathbf{x}, \mathbf{x}) = 1 \quad \forall \mathbf{x} \in \mathbb{R}^d$ , SVDD and OC-SVM learn identical decision functions. Many common kernels have this property, such as RBF, Laplacian and $\chi^2$ . SVDD and OC-SVM are also equivalent in the case that all samples lie on a hypersphere centered at the origin, and are are linearly separable from it. See Lampert, C. H. (2009). Kernel methods in computer vision (Chapter 5) for more detailed descriptions of these models.
