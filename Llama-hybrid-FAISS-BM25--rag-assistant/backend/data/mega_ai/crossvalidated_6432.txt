[site]: crossvalidated
[post_id]: 6432
[parent_id]: 6421
[tags]: 
The introduction of the "intrinsic discrepancy" loss function and other "parameterisation free" loss functions into decision theory. It has many other "nice" properties, but I think the best one is as follows: if the best estimate of $\theta$ using the intrinsic discrepancy loss function is $\theta^{e}$, then the best estimate of any one-to-one function of $\theta$, say $g(\theta)$ is simply $g(\theta^{e})$. I think this is very cool! (e.g. best estimate of log-odds is log(p/(1-p)), best estimate of variance is square of standard deviation, etc. etc.) The catch? the intrinsic discrepancy can be quite difficult to work out! (it involves min() funcion, a likelihood ratio, and integrals!) The "counter-catch"? you can "re-arrange" the problem so that it is easier to calculate! The "counter-counter-catch"? figuring out how to "re-arrange" the problem can be difficult! Here are some references I know of which use this loss function. While I very much like the "intrinsic estimation" parts of these papers/slides, I have some reservations about the "reference prior" approach that is also described. Bayesian Hypothesis Testing:A Reference Approach Intrinsic Estimation Comparing Normal Means: New Methods for an Old Problem Integrated Objective Bayesian Estimation and Hypothesis Testing
