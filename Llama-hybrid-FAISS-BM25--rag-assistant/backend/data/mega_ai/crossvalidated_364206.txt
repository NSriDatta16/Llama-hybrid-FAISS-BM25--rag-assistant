[site]: crossvalidated
[post_id]: 364206
[parent_id]: 
[tags]: 
Balanced data, but unbalanced result

I'm fairly new to data science. I have a multi-class classification problem with 4 classes, 100K rows. The problem is that the classes are balanced but the prediction results are not. (All 4 classes have 25K observations) For example, lets say that the classes are A, B, C, and D. Currently I'm using XGBoost and the model predictions are unbalanced. The number of predicted observations are pretty much like this. [ A: 5600, B: 3900, C: 5250, D: 5250 ] I get that the features are not good enough to classify between A and B (Model prediction for class 'C' and 'D' are not bad). But I don't understand why the results are unbalanced. The model tends to predict class B as class A. I've done a lot of CV and parameter optimization but the unbalanced predictions are always there. Is there any way to improve this unbalanced results? For example, would it make sense to oversample, or use SMOTE to increase class B samples? Please help.
