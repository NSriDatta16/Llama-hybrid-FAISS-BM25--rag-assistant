[site]: crossvalidated
[post_id]: 377999
[parent_id]: 
[tags]: 
Why are Gaussian Processes valid statistical models for time series forecasting?

Duplicates disclaimer: I know about the question Time series forecasting using Gaussian Process regression but this is not a duplicate, because that question is only concerned with modifications to the covariance function, while I argue that actually the noise term has to be modified too. Question : in time series model, the usual assumption of nonparametric regression (iid data) fails, because residuals are autocorrelated. Now, Gaussian Process Regression is a form of nonparametric regression, because the underlying model is, assuming we have a iid sample $D=\{(t_i,y_i)\}_{i=1}^N$ : $$y_i = \mathcal{GP}(t_i)+\epsilon_i,\ i=1,\dots,N$$ where $\mathcal{GP}(t)$ is a Gaussian Process, with a mean function $mu(t)$ (usually assumed to be 0) and a covariance function $k(t,t')$ , while $\epsilon\sim\mathcal{N}(0,\sigma)$ . We then use Bayesian inference to compute the posterior predictive distribution, given the sample $D$ . However , in time series data, the sample is not iid. Thus: how do we justify the use of such a model? since the mean function for time series forecasting with GPs is usually assumed to be zero, when I compute a forecast sufficiently far in the future, I expect it will revert to the mean of the data. This seems a particularly poor choice, because I would like to be able (in principle) to forecast as far in the future as I want, and the model manage to get the overall time trend right, with just an increase in the prediction uncertainty (see the case below with an ARIMA model): how is this taken care of, when using GPs for time series forecasting?
