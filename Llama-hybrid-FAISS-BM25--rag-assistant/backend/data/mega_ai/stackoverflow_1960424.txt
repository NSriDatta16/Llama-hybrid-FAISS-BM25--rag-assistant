[site]: stackoverflow
[post_id]: 1960424
[parent_id]: 
[tags]: 
What is the difference between O, Ω, and Θ?

I am learning algorithm analysis. I am having trouble understanding the difference between O, Ω, and Θ. The way they're defined is as follows: f(n) = O(g(n)) means c · g(n) is an upper bound on f(n) . Thus there exists some constant c such that f(n) is always ≤ c · g(n) , for large enough n (i.e., n ≥ n0 for some constant n0 ). f(n) = Ω(g(n)) means c · g(n) is a lower bound on f(n) . Thus there exists some constant c such that f(n) is always ≥ c · g(n) , for all n ≥ n0 . f(n) = Θ(g(n)) means c1 · g(n) is an upper bound on f(n) and c2 · g(n) is a lower bound on f(n) , for all n ≥ n0 . Thus there exist constants c1 and c2 such that f(n) ≤ c1 ·g(n) and f(n) ≥ c2 ·g(n) . This means that g(n) provides a nice, tight bound on f(n) . The way I have understood this is: O(f(n)) gives worst case complexity of given function/algorithm. Ω(f(n)) gives best case complexity of given function/algorithm. Θ(f(n)) gives average case complexity of given function/algorithm. Please correct me if I am wrong. If it is the case, time complexity of each algorithm must be expressed in all three notations. But I observed that it's expressed as either O, Ω, or Θ; why not all three?
