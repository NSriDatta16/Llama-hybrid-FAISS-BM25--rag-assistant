[site]: crossvalidated
[post_id]: 229300
[parent_id]: 
[tags]: 
Normalizing data worsens the performance of CNN?

I've been using CNN for facial recognition tasks, first I train a CNN for classification, and I use the trained CNN to extract features from images and do verification (tell whether two pictures are of the same person). However I found if I subtract the mean from the data (same mean used in both training and verification), though the classification result stays basically the same, the verification result will actually drop by more than 2% on average. Is this some known fact that normalizing (image) data actually worsens CNN performance? or is it just coincidence? BTW I used batch normalization, and Euclidean distance for verification.
