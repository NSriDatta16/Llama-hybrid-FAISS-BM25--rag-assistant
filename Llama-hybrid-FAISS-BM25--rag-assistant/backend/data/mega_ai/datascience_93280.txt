[site]: datascience
[post_id]: 93280
[parent_id]: 93224
[tags]: 
Thanks to everyone who gave answer and comments. It was indeed caused by my data. Prior to this I had the same preprocessing pipeline for both models, which would be your "usual" NLP preprocessing steps (non-alphanumerical removal, lowercasing, stemming, and stop word removal). I had a hunch that both stemming and stop word removal would cause the text to lose some context and hence the benefits of BERT wouldn't be prevalent. Experimenting with another preprocess pipeline that DOES NOT do stemming and stop word removal actually proved to benefit the BERT model, as shown from the metrics below. Further reading: this article and this paper . My data DID come from the same domain as the training data for the pretrained Word2vec (i.e. tweets). On the other hand, the pretrained BERT was trained on a combination of tweets and other kinds of texts. In addition to (1), I conclude it would be much easier for Word2vec to model the texts. Further reading: this paper . Metrics Word2vec-CNN precision (macro): 0.87 recall (macro): 0.87 f1-score (macro): 0.87 accuracy (test set): 0.83 hamming loss: 0.063 BERT-CNN precision (macro): 0.91 recall (macro): 0.90 f1-score (macro): 0.90 accuracy (test set): 0.82 hamming loss: 0.051
