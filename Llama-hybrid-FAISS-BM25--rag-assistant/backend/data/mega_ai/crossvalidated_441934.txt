[site]: crossvalidated
[post_id]: 441934
[parent_id]: 439667
[tags]: 
In statistical parlance, there is usually: A loss function $\ell(\hat y, y)$ , e.g. squared loss $\frac12 (\hat y - y)^2$ , 0-1 loss $\mathbf{1}(\hat y \ne y)$ , or so on, which determines the loss due to a prediction $\hat y$ for the truth $y$ . A risk , or expected loss , function $L(\theta) = \mathbb E_{(x,y)}[ \ell( f_\theta(x), y ) ]$ , which tells you the expected loss of a predictor $f_\theta$ . In machine learning, the latter function $L(\theta)$ is also sometimes called a loss (e.g. in talking about the loss surface of a model). If you need to disambiguate, in predictive contexts I think risk might not generally be immediately understood; expected loss might, but it might also conjure associations of being e.g. the expected value of $L$ after running a randomized training procedure. Objective function might work for $L(\theta)$ , but that would typically also include any explicit regularization terms you're using, which might not be what you want. In short: I don't thing there's a totally unambiguous term that will be immediately understood. You can call it any of these things, particularly if you're not also referring to the function $\ell$ ; just make sure to clarify what you're talking about on the first usage.
