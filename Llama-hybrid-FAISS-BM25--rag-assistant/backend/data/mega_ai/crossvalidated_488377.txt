[site]: crossvalidated
[post_id]: 488377
[parent_id]: 479612
[tags]: 
As you stated, both EM and VAE are machine learning techniques/algorithms to find the latent variables z . However, despite the overall goal and even the objective function being the same, there are differences because of the complexities of the model. There are 2 issues at hand where EM (and its variants) have limitations. These are mentioned in the original VAE paper, Auto-Encoding Variational Bayes by Kingma & Welling. From section 2.1 of the paper: Very importantly, we do not make the common simplifying assumptions about the marginal or posterior probabilities. Conversely, we are here interested in a general algorithm that even works efficiently in the case of: Intractability : the case where the integral of the marginal likelihood $p_\theta(x) = \int p_\theta(z) p_\theta(x|z) ,dz$ is intractable (so we cannot evaluate or differentiate the marginal likelihood), where the true posterior density $p_\theta(z|x) = p_\theta(x|z)p_\theta(z)/p_\theta(x)$ is intractable (so the EM algorithm cannot be used), and where the required integrals for any reasonable mean-field VB algorithm are also intractable. These intractabilities are quite common and appear in cases of moderately complicated likelihood functions $p_\theta(x|z)$ , e.g. a neural network with a nonlinear hidden layer. A large dataset : we have so much data that batch optimization is too costly; we would like to make parameter updates using small minibatches or even single datapoints. Sampling-based solutions, e.g. Monte Carlo EM, would in general be too slow, since it involves a typically expensive sampling loop per datapoint.
