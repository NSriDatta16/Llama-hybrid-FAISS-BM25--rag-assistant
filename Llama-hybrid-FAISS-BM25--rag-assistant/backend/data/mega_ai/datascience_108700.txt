[site]: datascience
[post_id]: 108700
[parent_id]: 108274
[tags]: 
I would use leave one out, where the partition is on a per-job basis. Completely isolate a particular job for testing Divide your remaining data into training and validation data as you see fit. train/optimize your model with the train and validate data test on the lone, reserved job. repeat for another job, until all jobs have been tested. Do this for every job, so you'll train a new model for all the data minus 1 of your jobs, and you'll have tested each of your jobs. Then, you can use the aggregate results to approximate how your model will generalize to a new job being introduced. Your final model can be trained on all data, be a random model from the batch, an ensemble of all the models trained; it's up to you, all have their costs and benifits. If your dataset is too large for this "wasteful" strategy to be practical, you can use k-fold to achieve a similar result, but doing it with multiple jobs at a time. If that, still, remains too wasteful, you can do your 70/10/20 train/validate/test split on a per-job basis, not on an individual datum basis. As for reducing the risk of memorization, that's a feature engineering question which is highly dependent on your particular problem. A good solution might be to include certain features in your hyperparameter tuning strategy, such that you can iteratively learn which features seem useful, and which don't. If you find yourself constantly worrying about memorization, you probably don't have enough data and/or you have too many features.
