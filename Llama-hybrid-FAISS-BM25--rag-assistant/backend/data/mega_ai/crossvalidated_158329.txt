[site]: crossvalidated
[post_id]: 158329
[parent_id]: 
[tags]: 
what is 1/0 in this article?

I am reading the article with title "metric learning by collapsing classes" lately http://papers.nips.cc/paper/2947-metric-learning-by-collapsing-classes.pdf . Everything goes well until the equation (3), which I partially understand. The basic idea is to learn a distance metric (Mahalanobis distance) from the data, which is to be used in the traditional KNN methord, to better suit the specific application problem. The main point is: "our methord relies on the simple geometric inituation that, a good metric is one under which points in the same class will be close to each other simultaneously and far away from points in other classes". The author used a stochastic methord. They consider a linear projection with matrix w. Given a node i, they use $p^{A} (j|i)$ to denote the probability that the node j will select node i and use node i's label. So, the equation (3) is as follows. Here, $i\neq j$. $$ p^{A} (j |i ) = \frac{1}{z_i} e^{- d^{A}_{ij}} = \frac{ e^{-d^{A}_{ij}} }{ \sum_{k \neq i} e^{-d_{ik}^{A}}}. $$ Now, the author consider the ideal case, i.e., all the nodes in the same class are mapped into the same location, while all the nodes in different class are infinitely far away from each other. The author has the equation (4), $p_{0}(j|i)$ is approximately 1 if $y_i=y_j$, while it is approximately 0 if $y_i\neq y_j$ (Here, I do not know whether describe that symbol to be approxinmately is right or not). What I do not quite understand is the situation when $y_i=y_j$. what I get based on the equation is, $1/(0+0+...+0)$, which would be infinity, not 1. Can anyone explain to me what is going on here? Many thanks for your time and attention.
