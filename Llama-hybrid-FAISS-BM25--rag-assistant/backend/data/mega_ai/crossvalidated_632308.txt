[site]: crossvalidated
[post_id]: 632308
[parent_id]: 
[tags]: 
MLE has any competitors?

I am learning about characteristics of statistical estimators such as Efficiency and Minimum Variance. For example , on the topic of MLE, it says, here (1) it says that : " MLE is popular for a number of theoretical reasons, one such reason being that MLE is asymtoptically efficient: in the limit, a maximum likelihood estimator achieves minimum possible variance " But it does not provide any examples as possible competing estimators. I am being facetious, but right now since I am in my apartment room by myself, I could say that I am the fastest person in my room : this statement is true, but it holds little weight because I am the ONLY person in my room. If there were 2 people in the room, I could then say : Provided I am the fastest person in this room, it means I am faster than Person 1 and Person 2. In MLE, they never provide any examples to contextualize this fact. MLE achieves minimum possible variance, great ... but what compared to which other competitors? Compared to Bayesian Estimation? Compared to Random Guessing? Compared to a Static Constant? Are there some tangible examples where I can see that MLE has a lower variance to some actual, well known estimator instead of just some "arbitrary phantom estimator"? PS: I see the argument: The lowest possible variance of the parameter of any probability distribution is given by the Cramer Rao Lower Bound (CRLB). And, only the MLE can have the CRLB. Therefore, MLE has lowest possible variance amongst all estimators. But this again is like me saying: I am doing a race to see who is the fastest person in the world but I won't tell anyone the location of the race - therefore I am fastest. Footnotes: (1) https://gregorygundersen.com/blog/2019/11/28/asymptotic-normality-mle/
