[site]: crossvalidated
[post_id]: 445268
[parent_id]: 409521
[tags]: 
Misclassification costs can often be dealt with through class weights, the same way as unbalanced classes can. This means that if the misclassification cost is higher for a class, elements of such a class will be more influent when making predictions. For decision trees and random forests, this has been shown in this paper by Breiman, that I would say puts together points 1 and 2 of your question. Indeed, Weighted Random Forest uses a weighted version of the Gini Coefficient in order to make the splits. This means that the Gini Coefficient will be maximum when the weighted sum of the elements of each class is equal (normally, Gini is maximum when elements are evenly distributed within the classes) (1). At the same time, this also means that the threshold that is used when considering the majority class of a node will not be 0.5, but it will come from the ratio of the class weights. Finally, this also applies for predictions (2), as the threshold will be modified by the weights. Unfortunately, to this day I do not know any major statistical package using this method, as class weights are usually used for over/undersampling of the classes, which is much more specific to unbalanced classes. Finally, using the ROC score is always advised when your classes and/or your costs are not balanced, so that you can tweak the thresholds to balance the results of your classifier.
