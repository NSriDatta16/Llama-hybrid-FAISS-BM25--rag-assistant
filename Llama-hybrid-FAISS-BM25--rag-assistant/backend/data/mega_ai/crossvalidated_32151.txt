[site]: crossvalidated
[post_id]: 32151
[parent_id]: 32139
[tags]: 
The AUC averages the performance over the whole range of classifier scores, starting from low coverage / low false positive rate and ending at high coverage / high false positive rate. This is not always the best way to compare performance because you may have a stronger emphasis on coverage rather than precision or vise versa. Once you plot the ROC curves and/or the precision-recall curves (for the relevant R functions see, e.g., this answer ), you can compare the classifiers and select the one that provides better precision for a given high recall value (if these are your needs) or vise versa. This approach will also provide you with the cutoff for accepting the selected classifier's predictions.
