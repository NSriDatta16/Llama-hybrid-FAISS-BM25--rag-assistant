[site]: crossvalidated
[post_id]: 531653
[parent_id]: 531646
[tags]: 
No, it is not done like this. Quoting my other answer Logistic regression can be described as a linear combination $$ \eta = \beta_0 + \beta_1 X_1 + ... + \beta_k X_k $$ that is passed through the link function $g$ : $$ g(E(Y)) = \eta $$ where the link function is a logit function $$ E(Y|X,\beta) = p = \text{logit}^{-1}( \eta ) $$ As you can see, the linear predictor $\eta = \mathbf{X}\boldsymbol{\beta}$ is not equal to the conditional mean of $y$ , but you need to transform it first using the inverse of the logit link function $g^{-1}(\eta)$ . If you just ran the linear regression, you would be ignoring the fact that the transformation of the linear predictor happens. You can easily verify this yourself, run linear and logistic regression on the same data. If using linear regression would be enough, you should get the same regression parameters. As you can see from the example below, that's not the case. > lm(vs~mpg+cyl, data=mtcars) Call: lm(formula = vs ~ mpg + cyl, data = mtcars) Coefficients: (Intercept) mpg cyl 2.164638 -0.008217 -0.252454 > glm(vs~mpg+cyl, family=binomial, data=mtcars) Call: glm(formula = vs ~ mpg + cyl, family = binomial, data = mtcars) Coefficients: (Intercept) mpg cyl 15.9714 -0.1633 -2.1482 Degrees of Freedom: 31 Total (i.e. Null); 29 Residual Null Deviance: 43.86 Residual Deviance: 17.49 AIC: 23.49 Logistic regression is fitted by using an optimization algorithm that maximizes the likelihood function . The likelihood function is defined in terms of Bernoulli distribution : $$ L(\boldsymbol{\beta}|y;\mathbf{X}) = \prod_i\, g^{-1}(\mathbf{X}_i\boldsymbol{\beta})^{y_i} \, (1 - g^{-1}(\mathbf{X}_i\boldsymbol{\beta}))^{1-y_i} $$ Commonly IRLS algorithm is used for finding the maximum of this function, but you probably could ignore this fact and live a happy life without that knowledge.
