[site]: crossvalidated
[post_id]: 362867
[parent_id]: 362815
[tags]: 
A really good reference for this is the book An Introduction to Statistical Learning -- it's fairly readable for beginners, and although the solutions are implemented in R, they do a thorough explanation of the theory behind the various algorithms. It would be helpful to you here, and they give great examples. Chapter 8 focuses on tree-based methods like random forests. My explanation below will largely follow from their discussion. Entropy in this case is defined as: $D = -\sum_{k=1}^K \hat{p}_{mk} log(\hat{p}_{mk})$ where $\hat{p}_{mk}$ is the proportion of training observations in the m th region that are from the k th class. What this means is that entropy ($D$) will take on a value near zero when all of the $\hat{p}_{mk}$s are near zero or near one -- meaning that all of the observations in that node in the tree are from the same class. Entropy is one of the metrics commonly used, along with the Gini index, for evaluating the quality of a particular node split. Since the goal of the random forest classifier is to try to predict classes accurately, you want to maximally decrease entropy after each split (i.e., maximize information gained with the split). Taking a node with lots of heterogenous examples and splitting it into relatively pure nodes will maximize this. Once you understand how the proportions here work (i.e., what the p s in your equation from your comment are doing), it becomes easy to calculate the entropy.
