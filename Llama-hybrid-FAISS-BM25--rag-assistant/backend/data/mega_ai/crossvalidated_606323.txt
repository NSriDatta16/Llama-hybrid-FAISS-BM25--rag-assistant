[site]: crossvalidated
[post_id]: 606323
[parent_id]: 601159
[tags]: 
I evaluated the proposed method on the Choice of Plausible Alternatives (COPA) task . I picked this task from the SuperGLUE NLP benchmark b/c each $c \in \mathcal{Y}$ is always a few tokens long. See the evaluation notebook here . I still need to empirically evaluate calibration, and evaluate on more classification tasks. But a partial, empirical answer to the question in the title and the "Did it make sense..." question is: yes. Zero-shot accuracy on COPA train + validation is 0.92-0.95 depending on the prompt, which is comparable to text generation + post-processing. (Majority class gives 0.5 accuracy.) Edit : after some more experimentation, it seems like the issue with the prior coming from $\Pr_\theta(c)$ is not fully resolved. In other words, the average likelihood of completions which are really common—regardless of the prompt—is often too high. I hypothesized that this would be an issue in this comment . The solution I'm rolling with for now is to do something along the lines of replacing the marginal class probability in Bayes' rule. (I did a similar thing with some success in this separate work . See the section A simple fix .) We can introduce a "discount" hyperparameter which controls how much $\Pr_\theta(c)$ counts toward the average likelihood. Modify $$ \begin{equation*} \text{Pr}_\theta(c \: | \: p) \propto \text{Pr}_\theta(p \: | \: c) \text{Pr}_\theta(c) \end{equation*} $$ to $$ \begin{equation*} \log\text{Pr}_\theta(p \: | \: c) \text{Pr}_\theta(c)^\gamma = \log\text{Pr}_\theta(p \: | \: c) + \gamma\log\text{Pr}_\theta(c). \end{equation*} $$ Skipping some steps (and ignoring the unease with the ill-defined $\text{Pr}_\theta(p \: | \: c)$ term), this fudged expression is incorporated into the average likelihood as: $$ \begin{equation*} \bar{\text{Pr}}_\theta(c \: | \: p, y) = \exp \Bigg\{\frac{1}{n} \sum_{i=1}^{n} \log\text{Pr}_\theta(c_i \: | \: p, y, c_{1:i-1}) + \gamma \log\text{Pr}_{\theta}(c_i \: | \: y, c_{1:i-1}) \Bigg\}. \end{equation*} $$ Setting $\gamma = 0$ obviously applies no discount. Setting $0 applies some discount. Setting $\gamma = 1$ is equivalent to dropping $\Pr_\theta(c)$ , which should directly counteract the issue. Increasing $\gamma > 1$ further penalizes completions with higher priors, which doesn't seem sensible.
