[site]: crossvalidated
[post_id]: 256222
[parent_id]: 256218
[tags]: 
Quoting the classical Statistical Decision Theory by James O. Berger: [...] We have already stated that decision rules will be evaluated in terms of their risk functions $R(\theta, \delta)$. [...] The problem, as pointed out earlier, is that different admissible decision rules will have risks which are better for different $\theta$'s. To the rescue comes the prior $\pi(\theta)$, which supposedly reflects which $\theta$'s are the "likely" ones to occur. It seems very reasonable to "weight" $R(\theta, \delta)$ by $\pi(\theta)$ and average. Yes you can evaluate $R(\theta, \delta)$ for each $\theta$, but then you would implicitly assume that each possible value of $\theta$ is equally likely. In Bayesian scenario you pick prior $\pi(\theta)$ that reflects probabilities of observing different $\theta$'s and include such information.
