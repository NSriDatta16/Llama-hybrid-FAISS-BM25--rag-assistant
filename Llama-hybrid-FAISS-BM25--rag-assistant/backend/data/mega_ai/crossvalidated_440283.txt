[site]: crossvalidated
[post_id]: 440283
[parent_id]: 440256
[tags]: 
Context : The cross-validation method and the holdout method (train-test split) are seen as two methods to evaluate the model performance. The goal of this evaluation is to obtain an estimate of the generalization (or, test) error. Summary : If the accuracy from the cross-validation method is less than the accuracy from the holdout method, it indicates model overfitting. Explanation : When the test error is estimated by the holdout method, the data is split into the training and holdout samples. However, this split may induce a bias since there's no guarantee of a randomness within the training and test samples even if the whole dataset is considered a random sample. In order to mitigate this bias we can average the test error stemming from different test samples. This is precisely what cross-validation does - it rotates the test sample across the whole dataset and for every test sample, the remaining dataset becomes the training sample. For each split, the test error is computed after fitting the model over the corresponding training sample. The test errors from each split are averaged to obtain the average test error, or the cross-validated error. In the absence of cross-validation, it is possible that the model becomes biased by the (biased) data split. This results in overfitting . Overfitting is the result of the model memorizing the training examples (and thus capturing noise) than actually learn (or identify the true pattern/relationship) from the training examples. Only when there is no noise in the data ( which is unlikely in the real world ) and assumed model reflects the true relationship ( which is typically difficult to know without domain knowledge ), the holdout and cross-validation methods provide the same accuracy. Hope this helps !
