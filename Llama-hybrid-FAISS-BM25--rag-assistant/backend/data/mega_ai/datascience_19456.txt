[site]: datascience
[post_id]: 19456
[parent_id]: 
[tags]: 
A practical reason to use Cross-entropy as a error-function in Neural networks?

Cross-entropy tends to allow errors to change weights even when nodes saturate (which means that their derivatives are asymptotically close to 0.) Link Why is the above statement true? Figures and examples if possible.
