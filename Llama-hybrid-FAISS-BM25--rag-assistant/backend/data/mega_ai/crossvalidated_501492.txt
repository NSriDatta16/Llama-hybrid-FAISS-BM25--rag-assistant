[site]: crossvalidated
[post_id]: 501492
[parent_id]: 500988
[tags]: 
The first thing to appreciate about VAEs is that they are not just some magical deep generative model but that they are a special case of the Auto-Encoding Variational Bayes algorithm for doing variational Bayesian inference in generative models. What that means is we consider a setup with a dataset $\mathcal{D} =\{x_i\}_{i=1}^n$ where we assume the generative process for an instance $x_i$ is done in two steps: First sample a latent variable $z_i$ from some prior $p(z)$ Second sample $x_i$ from the distribution $p(x|z)$ Note that we don't observe the values of $z_i$ in our dataset so we're interested in working out for each example in our data what's the posterior distribution over its corresponding latent variable given we've seen all the data - i.e. what is $p(z|x)$ ? So in a sense then this is the main objective in VAEs - to uncover $p(z|x)$ and not exactly to model p(x), that is just a byproduct of the algorithm. To answer your first question then, we are interested in the $D_{KL}[q(z|x)||(p(z|x)]$ because what we're doing is accepting that $p(z|x)$ is intractable and we won't be able to calculate it, but we're trying to approximate it with $q(z|x)$ (which here is using a neural network) and we want them to be as similar as possible, hence why we're minimising that divergence. For your second question on why this should perform well you just need to take a look at the form of the ELBO given here: $$ \mathcal{L}(\theta,\phi) = \underbrace{\mathbb{E}_{q_\phi}[\log p_\theta(x|z)]}_{\text{Reconstruction Loss}} - \underbrace{D_{KL}[q_\phi(z|x)||p(z)]}_{\text{KL Regulariser}} $$ The first term is the expected likelihood of the data given the latent variables, it should make sense that is a sensible thing to maximise as this makes the data more likely under the model while the second term maintains the `auto-encoder' part and keeps the approximate distribution close to the prior. For your final question I would just say that I don't think it's particularly useful to think of the two network as ``mirroring'' each other. Rather they both model a different probability distribution and it so happens that we can train them jointly and effectively through this auto encoding algorithm. Comparing $q(z|x)$ and $p(z|x)$ then is measuring how good the encoder is at approximating the true posterior distribution over the latent variables, and so doesn't really say anything about the decoder. Also you repeatedly mention $q(x|z)$ but this is just not something that is part of the model since we kind of assume the structure of $p_\theta(x|z)$ and just learn that directly (it's the decoder) so we don't worry about some $q$ variational distribution.
