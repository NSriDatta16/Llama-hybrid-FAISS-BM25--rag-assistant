[site]: crossvalidated
[post_id]: 308617
[parent_id]: 
[tags]: 
Reinforcement learning in non stationary environment

Q1: Are there common or accepted methods for dealing with non stationary environment in Reinforcement learning in general? Q2: In my gridworld, I have the reward function changing when a state is visited. Every episode the rewards reset to the initial state. All I want my agent to learn is "Don't go back unless you really need to", however this makes the environment non-stationary. Can/Should this very simple rule be incorporated in the MDP model, and how? Is Q-learning the best solution for dealing with this problem? Any suggestions or available examples? Q3: I have been looking into Q-learning with experience replay as a solution to dealing with non stationary environments, as it decorrelates successive updates. Is this the correct use of the method or it is more to deal with making learning more data efficient? And I have only seen it used with value approximation. I am not sure if it is an overkill to use it for a simple discretised state space, like gridworld, or there is a different reason for this. Please feel free to answer or comment even if you can't address all questions.
