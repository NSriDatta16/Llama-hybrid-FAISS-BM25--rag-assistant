[site]: crossvalidated
[post_id]: 617467
[parent_id]: 617208
[tags]: 
In this question, the aim is to make inference for the average monthly rate $p$ of faulty items in a production line ( $p$ =number of faulty items per total). Daily count data are available over a long time period (2 years). The total number of items per day is large (thousands), and the failure rate is not very small (around 4-5%). What makes the analysis difficult? There are good reasons to doubt that daily counts can be modelled as independent. Usually, one would model daily failure counts as binomial random variables, but that would require that there are no events that systematically affect larger batches of items, such as a manufacturing machine being faulty. What could be done? Look at the daily rates directly. They cannot be treated as i.i.d., but some analysis is possible anyway, at least if the daily total production does not vary extremely. A simplistic hands-on suggestion: 1.) Analyse daily rates as a time series: calculate the autocorrelation function, and from there, the effective sample size. Also try to see if there is some systematic or seasonal trend. If you are not familiar with time series, then maybe this discussion of autocorrelation on CV can help to give some idea. 2.) If there is no trend, look at the distribution of the rates. Compare with a normal distribution e.g. using a normal qqplot. 3.) If modelling the daily rates as normal distributed is deemed ok, you can apply a test for the mean of a normal random variable, but use the effective sample size from 1.) instead of the true number of days, unless the time series did not have autocorrelation. If the data deviate grossly from normal, a transformation might help. Commonly used simple resampling methods (bootstrap) are not recommended if you detected dependence in step 1.
