[site]: crossvalidated
[post_id]: 347579
[parent_id]: 347530
[tags]: 
Jan and Cagdas give a good Bayesian reason, interpreting the regularizer as a prior. Here are some non-Bayesian ones: If your unregularized objective is convex, and you add a convex regularizer, then your total objective will still be convex. This won't be true if you multiply it, or most other methods of combining. Convex optimization is really, really nice compared to non-convex optimization; if the convex formulation works, it's nicer to do that. Sometimes it leads to a very simple closed form, as wpof mentions is the case for ridge regression. If you think of the problem you "really" want to solve as a problem with a hard constraint $$ \min_{\theta : c(\theta) \le 0} J(\theta) ,$$ then its Lagrange dual is the problem $$ \min_\theta J(\theta) + \lambda c(\theta) .$$ Though you don't have to use Lagrange duality, a lot is understood about it. As ogogmad mentioned , the representer theorem applies to the case of an additive penalty: if you want to optimize $f$ over a whole reproducing kernel Hilbert space of functions $\mathcal H$, then we know that the solution to optimization over the whole space $$ \min_{f \in \mathcal H} J(f) + \lambda \lVert f \rVert_{\mathcal H}^2 $$ lies in a simple finite-dimensional subspace for many losses $J$; I don't know if this would hold for a multiplicative regularizer (though it might). This is the underpinning of kernel SVMs. If you're doing deep learning or something non-convex anyway: additive losses give simple additive gradients. For the simple $L_2$ regularizer you gave, it becomes very simple weight decay . But even for a more complicated regularizer, say the WGAN-GP 's loss $$ \sum_{x,y} \underbrace{f_\theta(x) - f_\theta(y)}_\text{the loss} + \lambda \underbrace{\mathbb{\hat E}_{\alpha \sim \mathrm{Uniform}(0, 1)} \left( \lVert \nabla f_\theta(\alpha x + (1 - \alpha) y) \rVert - 1\right)^2}_\text{the regularizer}, $$ it's easier for backpropagation to compute gradients when it only has to consider the sum of the loss and the complicated regularizer (considering things separately), instead of having to do the product rule. Additive losses are also amenable to the popular ADMM optimization algorithm, and other "decomposition"-based algorithms. None of these are hard-and-fast rules, and indeed sometimes a multiplicative (or some other) regularizer might work better (as ogogmad points out ). (In fact, I just the other day submitted a paper about how something you could interpret as a multiplicative regularizer does better than the WGAN-GP additive one above!) But hopefully this helps explain why additive regularizers are "the default."
