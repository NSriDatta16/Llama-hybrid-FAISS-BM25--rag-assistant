[site]: datascience
[post_id]: 115992
[parent_id]: 
[tags]: 
Can I find the input that maximises the output of a Neural Network?

So I trained a 2 layer Neural Network for a regression problem that takes $D$ features $(x_1,...,x_D)$ and outputs a real value $y$ . With the model already trained (weights optimised, fixed), can I find the input $(x_1,...,x_D)$ that gives a maximum $y$ ? Particularly, I want to know if: This is possible by finding the derivative of the neural network functions composition with respect to $(x_1,...,x_D)$ , and equal that to zero. In theory, that should give me either a maximum/minimum, so I could just evaluate those values and pick the ones that gives me highest $y$ . This is possible by using some kind of gradient descent, but updating the weights in the opposite direction (this is, with a $+$ sign, gradient ascent). But here I don't see clearly how to implement it. Like, in the normal NN Gradient Descent, we update the weights W, and we use it by evaluating on the training set and computing a loss function. If I want to apply gradient ascent here, which is the loss function? and the training dataset?
