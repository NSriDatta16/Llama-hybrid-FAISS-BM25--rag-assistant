[site]: crossvalidated
[post_id]: 336844
[parent_id]: 336837
[tags]: 
I would read the thread on whether normality testing is essentially useless (which is very similar to whether you should test for any other model assumption) and consider whether you should even be doing tests for such model assumptions. Various different tests will have more or less power against certain alternatives, so it can easily happen that two tests disagree. So, you could consider reading up on how these two tests differ and whether it is plausible that the particular deviations from assumptions are more plausible than the others in your case. However, in any case, a failure to reject the null hypothesis does not demonstrate that assumptions are justified. Similarly, a rejection of the null hypothesis does not mean that a meaningful violation of assumptions is present (even if it is a correct rejection). Usually, one would ideally check on previous similar data whether the assumptions are likely to be approximately fulfilled (looking at some measure of how large deviations - if any - are, instead of just some hypothesis test). You could still do that post-hoc on the data you now have, but this kind of data-driven excercise results in there not being type I error control, confidence intervals not having their nominal coverage and so on - in case this matters to you, which may not be a major consideration for some exploratory investigation. Given the data driven decisions, it will also be important to make this process transparent to those reading your research so that they are aware that analysis decisions were taken after seeing the data.
