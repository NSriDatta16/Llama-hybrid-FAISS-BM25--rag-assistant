[site]: crossvalidated
[post_id]: 636608
[parent_id]: 636603
[tags]: 
If you know enough about the target effect size * and the magnitudes and correlations of the covariates, then you can do a formal power analysis, or use a rule of thumb based on power analysis calculations (i.e., rule #3). That's better, if you can get/are willing to make assumptions about that information . All rules of thumb about sample size and power analysis are based on informed guesses. Rule #2 is inappropriate for binary outcomes; you shouldn't use it. Rules like #1 are based on typical effect sizes for a particular field (e.g., clinical medicine). The rule that Harrell (2015) gives, which is similar to Peduzzi's, is that a reasonable number of covariates is $$ \frac{\min(\textrm{# of events}, \textrm{# of non-events})}{(10\textrm{ to } 20)} $$ Converting this to a form where you compute required total number of observations from the number of covariates ( $C$ ) and the prevalence ( $p$ ), assuming $p , and using a divisor $D$ (listed as "10 to 20" in the formula above): $$ C = \frac{p N}{D} \quad \to \quad N = \frac{D C}{p} $$ with $D=10$ , this recovers Peduzzi's rule. 500 would be the total number of observations to plan for, not the number of events. in typical studies "# of events" is the smaller value (i.e. prevalence in this formulation (where you predict number of covariates from sample size, not sample size from number of covariates), the number of non-events doesn't matter for proper inference, doing data-based selection/dropping of covariates (specifically, model selection based on the response variable — you could e.g. decide to drop covariates with relatively little variability, or a covariate that was strongly correlated with another covariate) doesn't help you Harrell (2015) chapter 4 has a lot of useful information about what your choices are when you have too many covariates and not enough observations ... if I don't take the covariates into account for calculating sample size, but I will adjust these potential confounders in the analysis, will there be a problem? I think so. However, Vittinghoff and McCulloch (2007) say: If we (somewhat subjectively) regard confidence interval coverage less than 93 percent, type I error greater than 7 percent, or relative bias greater than 15 percent as problematic, our results indicate that problems are fairly frequent with 2–4 EPV, uncommon with 5–9 EPV, and still observed with 10–16 EPV (EPV == "events per variable"; this is the same as $D$ above because the number of events $pN = DC$ ... so you might get away with (say) $D=7$ or as small as 5 ...) * Target effect size is a delicate subject; you shouldn't design a study based on the expected magnitude of the effect, but on the minimum biological/subject-area-related effect of interest, i.e. the magnitude such that effects smaller than that can be considered biologically (or whatever) unimportant ... Harrell Jr., Frank E. 2015. Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis. 2nd ed. 2015 edition. Cham: Springer. Vittinghoff, Eric, and Charles E. McCulloch. 2007. “Relaxing the Rule of Ten Events per Variable in Logistic and Cox Regression.” American Journal of Epidemiology 165 (6): 710–18. https://doi.org/10.1093/aje/kwk052 .
