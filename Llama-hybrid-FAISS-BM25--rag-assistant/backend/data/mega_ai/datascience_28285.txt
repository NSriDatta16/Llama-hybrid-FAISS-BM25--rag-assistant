[site]: datascience
[post_id]: 28285
[parent_id]: 
[tags]: 
What is the best way to deal with imbalanced data for XGBoost?

There are a lot of way to deal with class-imbalanced data like undersampling, oversampling, changing cost function etc. https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/ Here is the post talking about them all. I am working with an imbalanced multiclass classification problem and trying to solve it using XGBoost algorithm. I wanted to understand which method works best here. Since XGBoost already has a parameter called weights (which gives weight to each train record), would it be wise to directly use it instead of undersampling, oversampling, writing a cost function etc.?
