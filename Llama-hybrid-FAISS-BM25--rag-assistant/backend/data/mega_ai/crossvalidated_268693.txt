[site]: crossvalidated
[post_id]: 268693
[parent_id]: 184553
[tags]: 
The essence of the wild bootstrap is flipping signs for the regression residuals. This only works with linear regression (and you better have regression errors approximately symmetric). In all other GLM models, the residuals are fake, and flipping their signs is often insufficient. Consider logistic regression for simplicity. Your outcome $y_i$ is either 0 or 1. So unless the predicted probability is $\hat p_i=1/[1+\exp(-x_i'\hat\beta)]=0.5$, flipping the residual will produce an outcome that is out of range: if $y_i=1$ and $\hat p_i=0.7$, then the wildly-bootstrapped values are $y_i^*=1$ and $y_i^*=0.4$, the latter not being an appropriate value for the logistic regression. You can probably devise a wild-bootstrap-like routine that would produce $y_i^*=1$ with probability $\hat p_i$ and $y_i^*=0$ with probability $1-\hat p_i$; this would essentially be a parametric bootstrap procedure. I am not entirely sure it is worth much as your model may be misspecified, and the model-predicted probabilities may miss the true ones by a mile and a quarter. Moreover, generalizations to cluster wild bootstrap are absolutely weird: the wild bootstrap version would be to either retain all the exiting patterns of 0 and 1 within the cluster, or flipping all of them to the opposite -- but you need to flip them in a way that preserves the predicted probabilities somehow, and I am not sure I see how this is at all feasible. Note also that the bootstrap is an asymptotic procedure . Expecting it to produce good results for $n=4$ is naive. So I think the answer is a resounding "No".
