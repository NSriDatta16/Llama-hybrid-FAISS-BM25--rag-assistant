[site]: crossvalidated
[post_id]: 498236
[parent_id]: 
[tags]: 
In reinforcement learning, what is the correct mathematical definition of the discounted reward?

I'm seeing several different definitions of discounted reward/return as it is used in MDP based RL. I am wondering which one is correct. Let $r_t$ be the scalar-valued immediate reward at time $t$ , $R_t$ be the discounted return starting at time $t$ , then $R_t$ is, $R_t = \sum\limits_{i = 0}^T \gamma^i r_{t+1}$ $R_t = \sum\limits_{i = 0}^T \gamma^{t+i-1} r_{t+i}$ $R_t = \sum\limits_{i = 0}^T \gamma^{i} r_{t+i+1}$ where $\gamma \in (0, 1]$ (or sometimes $(0,1)$ ). Everybody basically has a different definition to the objective of RL, which is to maximize this object. For example: Carnegie Mellon University uses 1: https://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15381-s06/www/mdp.pdf McGill uses 2: https://www.cs.mcgill.ca/~dprecup/courses/AI/Lectures/ai-lecture16.pdf Stanford uses 3: http://web.stanford.edu/class/cme241/lecture_slides/rich_sutton_slides/5-6-MDPs.pdf Who is correct?
