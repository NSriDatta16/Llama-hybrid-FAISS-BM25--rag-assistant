[site]: crossvalidated
[post_id]: 562359
[parent_id]: 
[tags]: 
Does Stochastic Gradient Descent Converge on "some" Non-Convex Functions?

I am interested in better understanding the theoretical behavior (i.e. strength of convergence) of Stochastic Gradient Descent on Non-Convex Functions. We are constantly reminded that Stochastic Gradient Descent has the ability to converge for (deterministic, smooth, lipschitz continuous) Convex Functions - but usually, we also reminded that Stochastic Gradient Descent is not guaranteed to converge on Non-Convex Functions. Given that many applications in Machine Learning usually require optimizing a Non-Convex Function - several "variants" of the Stochastic Gradient Descent algorithm (e.g. ADAM, RMSprop) have been developed that literally "boost" the optimization algorithm out of the "Saddle Points" which often occur in Non-Convex Functions and result in suboptimal results. However, I found some research papers online that suggest Stochastic Gradient Descent actually might be able to display "some convergence" on Non-Convex Functions - but my knowledge in this topic is far insufficient to fully understand the claims being made in these papers . I have listed summaries of these papers below: 1) "Stochastic Gradient Descent for Nonconvex Learning without Bounded Gradient Assumptions" (Lei et al., 2019) In this paper, the authors comment that: Stochastic Gradient Descent is being heavily used on Non-Convex Functions, but the theoretical behavior of Stochastic Gradient Descent on Non-Convex Functions is not fully understood (currently only understood for Convex Functions). Currently, Stochastic Gradient Descent requires imposing a nontrivial assumption on the uniform boundedness of gradients. The authors establish a theoretical foundation for Stochastic Gradient Descent for Non-Convex Functions where the boundedness assumption can be removed without affecting convergence rates. The authors establish sufficient conditions for almost sure convergence as well as optimal convergence rates for Stochastic Gradient Descent applied to Non-Convex Functions. 2) "Stochastic Gradient Descent on Nonconvex Functions with General Noise Models" (Patel et al 2021) In this paper, the authors comment that: Although recent advancements in Stochastic Gradient Descent have been noteworthy, these advancements have nonetheless imposed certain restrictions (e.g., Convexity, Global Lipschitz Continuity etc.) on the functions being optimized. The authors prove that for general class of Non-Convex Functions, Stochastic Gradient Descent iterates either diverge to infinity or converge to a stationary point with probability one. The authors make further restrictions and prove that regardless of whether the iterates diverge or remain finite â€” the norm of the gradient function evaluated at Stochastic Gradient Descent's iterates converges to zero with probability one and in expectation; thus broadening the scope of functions to which Stochastic Gradient Descent can be applied to while maintaining rigorous guarantees of its global behavior. My Question: Based on some of these publications, have we truly been able to demonstrate that ( Stochastic) Gradient Descent has the potential to display similar global convergence properties on Non-Convex Functions , to the same extent at which it had previously displayed only on Convex Functions? Or have I completely misunderstood the results from this publications, and the conditions (and class of functions) in which the respective authors explored and demonstrated the convergence behavior of Stochastic Gradient Descent is far less "generous" compared to those pertaining to Convex Functions - and these conditions are also less likely to manifest themselves in real-world applications : And thus we still have reasons to believe that (Stochastic) Gradient Descent has more difficulties converging on Non-Convex Functions compared to Convex Functions? Thanks! References: https://arxiv.org/abs/1902.00908 https://arxiv.org/pdf/2104.00423.pdf
