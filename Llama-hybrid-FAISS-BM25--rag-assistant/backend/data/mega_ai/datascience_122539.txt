[site]: datascience
[post_id]: 122539
[parent_id]: 
[tags]: 
Can bert uncased predict text classification on foreign data?

I am trying to do the fake news/real news classification and used a pre-trained bert uncased model as transfer learning and it gave a solid 81% accuracy. But the problem is while doing sanity checks, I found my dataset has some Korean/Chinese text articles and these are some real news and it gave the trustworthy score(basically probability) as 60-70%. If Bert-uncased is only for the English language, I'm just thinking about how it processes those languages. Does anyone have any insights?
