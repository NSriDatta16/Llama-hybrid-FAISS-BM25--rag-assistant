[site]: datascience
[post_id]: 72025
[parent_id]: 
[tags]: 
What are good toy problems for testing Transformer architectures?

I am testing various variants for Transformers and Transformer architectures. But training on full language tasks is a rather time-consuming affair. What are good toy problems to test if a transformer (or alternative thereof) is working at all? I am looking for simple problems that can preferably be synthetically created and can be trained with really small setups (few layers, small embedding sizes, etc.) in a short time. Ideally, these should be problems that play to the strengths of transformers but would be hard to solve for, say, a fully connected feed-forward network. Tasks that can be applied to just an Attention Layer would be useful, too.
