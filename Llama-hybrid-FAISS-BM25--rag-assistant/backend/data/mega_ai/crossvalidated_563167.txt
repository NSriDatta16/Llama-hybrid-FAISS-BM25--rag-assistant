[site]: crossvalidated
[post_id]: 563167
[parent_id]: 
[tags]: 
Number of points a one hidden layer neural-network can interpolate

We am trying to understand the number of points that a neural network of a particular size can interpolate. I think this may be isomorphic to its degree of freedom? We are not interested in whether particular optimization methods would reach them, just if there is a theoretical bound. To be precise: take a "neural network" with one hidden layer $$ f(x) \equiv W_2 \cdot \sigma(W_1 \cdot x + b_1) + b_2 $$ Where, $f : \mathbb{R} \to \mathbb{R}$ $\sigma(\cdot) = max(0, \cdot)$ element-wise (i.e.ReLU) $W_1 \in \mathbb{R^N}$ $b_1 \in \mathbb{R^N}$ $W_2 \in \mathbb{R^N}$ $b_2 \in \mathbb{R}$ $\theta \in \{b_1, W_1, b_2, W_2\} \in \mathbb{R}^{3N+1}$ Note for a given $N$ there are $3N+1$ parameters. Question: For a fixed $N$ , what is the maximum number of points in $\mathbb{R}$ where there is always a $\theta$ which can interpolate them? For other functional forms like orthogonal polynomials, the number of points is always the number of parameters, but isn't it lower than $3N+1$ due to the collinearity of the bias?
