[site]: datascience
[post_id]: 17015
[parent_id]: 17013
[tags]: 
First you do not always need to normalize (standardize) the input vectors (feature vectors), sometimes is good, sometimes is bad. In general you scale your feature vector when the magnitude of a feature dominates the others, so the model cannot pick up the contribution of the smaller magnitude features. Read here for a detailed explanation. Second there are two general classes of machine learning problems: classification and regression. In a classification type problem the output (dependent variable) is discrete, so you do not need to normalize it. In a regression type problem scaling the output do not affect the shape of your function see here . Moreover it does not affect error functions like Mean Squared Error, i.e. the error is also scaled.
