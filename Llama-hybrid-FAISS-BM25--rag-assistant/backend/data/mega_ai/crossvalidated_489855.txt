[site]: crossvalidated
[post_id]: 489855
[parent_id]: 489832
[tags]: 
The key with binary outcomes is deciding on the causal estimand. First, you need to decide on your effect measure. The most commonly reported effect measures are the risk difference (RD; $P(Y^1=1)-P(Y^0=1)$ ), the risk ratio (RR; $P(Y^1=1)/P(Y^0=1)$ ) and the odds ratio (OR; $\frac{P(Y^1=1)}{1-P(Y^1=1)}/\frac{P(Y^0=1)}{1-P(Y^0=1)}$ ). Second, you need to decide whether you want a conditional effect or a marginal effect. A conditional effect is the effect for a given subclass of units (i.e., defined by a combination of covariate values). A marginal effect is the effect for a population. Conditional effects are useful when deciding whether to treat an individual patient. Marginal effects are useful when deciding whether to implement a rule or policy. For the RD, the marginal effect is the average of the conditional effects. In this way, the RD is collapsible. The RR and OR are not collapsible, meaning if there is any heterogeneity in individual causal effects, the average of the conditional effects is not the marginal effect (unless the marginal and conditional effects are all zero). This has led to many debates about the best choice of estimand. Conditional Effects There will certainly be effect heterogeneity in the conditional RD, which is to say that the RD of the treatment is not the same for all units. For example, if the average RD of for treatment was .4, what would happen to units who had a baseline risk (i.e., under control) of .7? Clearly their risk under treatment cannot be 1.1. If you want to estimate conditional RDs, you have to model the effect heterogeneity, which is sure to be nonlinear. This is true of the RR as well (i.e., how would an average RR of 2 apply to someone with a baseline risk of .55?). The OR does not face this problem; it is possible for the OR to be the same for all units. No matter what the baseline risk is, the expected risk under treatment is a valid number even if the OR is constant for all units. This is a reason to prefer the OR; you don't need to model effect heterogeneity, and it's possible to provide a single number that represents the conditional effect for all units. This is not possible with the RD or RR. Estimating the conditional OR is also straightforward; it is the coefficient on the treatment in a logistic regression of the outcome on the covariates and treatment, and the Wald or likelihood ratio test for the coefficient is a test of the null hypothesis of no treatment effect. An often-cited problem with the conditional OR is that its interpretation depends on the variables conditioned on in the model. The results of two studies examining the same treatment and outcome but conditioning on different covariates are not comparable to each other, even in the absence of confounding. In addition, the OR itself is challenging for people to interpret; most people tend to think about risks rather than odds, and people often misinterpret the OR as a RR (which is actually sometimes, but not often, valid). Marginal Effects A marginal effect integrates over possible effect heterogeneity in the population and cannot be said to apply to any individual unit when effect heterogeneity is present. That is, an estimated marginal RD of .2 doesn't actually mean the RD is .2 for any individual, which means you can't use a marginal effect to make clinical decisions. Even if there is no effect heterogeneity as measured by the OR, the marginal OR is not equal to the OR for an individual. When there is effect heterogeneity, the marginal effect integrates across the population, but the specific characteristics of the population (which are often unmeasured) determine the marginal effect, so different populations will generally have different marginal effects, and it's rare that a study sample is actually a simple random sample from the population to which the effect is to generalize to. (There are methods of generalizing an effect to a specific population, though, but those are beyond simply estimating an effect from that data you have.) In a randomized trial, the unadjusted contrast of the outcome under treatment and control is a marginal effect estimate. Because propensity score methods seek to emulate the conditions of a randomized trial (i.e., by removing confounding due to the measured covariates assuming strong ignorability), the unadjusted contrast of the outcome under treatment and control is a marginal effect estimate. Estimating the unadjusted marginal effect in a matched sample is as simple as regressing the outcome on just the treatment using the appropriate link function. (It's possible to estimate a covariate-adjusted marginal effect as well; this is called g-computation and involves average across model-predicted values of the outcome under treatment and control.) Now let's get into the practical aspect of this for you and your analysis. You need to decide what kind of estimand you want. Given that your treatment is COPD, which is not something about which policy can be made, it is likely you are interested in estimating a conditional effect (i.e., the effect of COPD for each unit). If you want a single number that can plausibly represent the conditional effect for all units, you want a conditional OR. If you just want to test the null hypothesis that there is no effect (i.e., what you would do with a $\chi^2$ test), it doesn't matter what method you use, but a covariate-adjusted conditional OR will have the most power to detect the effect. To get a conditional OR estimate, you need to regress the outcome on the treatment and conditioning covariates using a logistic regression model. We can do this in R with the following code: fit The coefficient on treatment is the conditional OR and can be reported as such in the results write-up, but it's important to report the specific covariates you conditioned on to provide context for interpreting the results. You may ask, if you were going to perform regression on the covariates anyway, why do matching? Matching makes it more likely that a logistic regression model on main effects will be a good fit to your data and prevents minor additional confounding due to unmodelled nonlinearities. That said, some argue that if you are going to perform logistic regression, matching is not necessary as long as work is done to ensure the regression is robust. Frank Harrell's RMS book and course explain this in detail, and his rms package provides tools to do this. The code you ran estimates a marginal effect (and without family = quasibinomial(link = "logit")) , that effect is on the RD scale. While such an analysis would be appropriate for some research questions, it doesn't seem like it would be appropriate for yours.
