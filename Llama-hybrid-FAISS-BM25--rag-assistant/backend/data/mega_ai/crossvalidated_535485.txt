[site]: crossvalidated
[post_id]: 535485
[parent_id]: 
[tags]: 
Meaning of NOT having a distribution on data

In statistics and machine learning, a common starting point is to assume some unknown distribution $\mathbb{P}$ on the cartesian product $\mathcal{X} \times \mathcal{Y}$ of input space and output space. The training data $\{x_i, y_i\}_{i = 1, \ldots , n}$ is then assumed to be i.i.d. samples from $\mathbb{P}$ . What does it mean to not have this assumption? Specifically, does it make sense to have an assumption which says that there is no distribution on $\mathcal{X} \times \mathcal{Y}$ that generated the training sample? Is learning possible in this regime? Of course, we can always assume that the training data was sampled from the empirical distribution $\frac{1}{n}\sum_{i=1}^n \delta_{(x_i, y_i)}$ , and so there always exists a distribution on $\mathcal{X} \times \mathcal{Y}$ that could have generated the sample, but this seems too trivial to be useful or interesting. The original assumption is studied under the label of "distribution free setting", and perhaps what I am asking is "Is there literally a distribution free setting studied in statistics or ML?"
