[site]: crossvalidated
[post_id]: 105463
[parent_id]: 105461
[tags]: 
Without getting into technicalities, by what you have written: Standard deviation of a variable $X$ is a measure of how "dispersed" $X$ is, i.e. on average, how far away are the individual observations of $X$ from its mean. Standard deviate of a particular observation $x_i$ of $X$ is how many "units" is $x_i$ away from the sample mean $\bar{x}$. The units are centered and scaled by the mean and standard deviation of $X$ (i.e. standardized). So suppose the standard deviate of $x_i$ is $k$, which means $x_i$ is $k$ standardized units away from the mean. This is how it works: $$ x_i=\bar{x}+k\sigma \Leftrightarrow k=\frac{x_i-\bar{x}}{\sigma} $$
