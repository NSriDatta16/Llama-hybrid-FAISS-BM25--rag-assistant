[site]: crossvalidated
[post_id]: 264759
[parent_id]: 136173
[tags]: 
For logistic regression, it's actually \begin{gather} p(x) = \frac{1}{1 + \exp(- (\sum_j W_j x_j + b) )} = \frac12 \\ 1 + \exp(- (\sum_j W_j x_j + b) ) = 2 \\ \exp(- (\sum_j W_j x_j + b) ) = 1 \\ - (\sum_j W_j x_j + b) = 0 \\ \sum_j W_j x_j + b = 0 ,\end{gather} exactly the same as for the perceptron. In general, you can try to solve for $p(x) = \frac12$ when you get a probability estimate (and the prior is bounded), or more generally if your prediction is $\mathrm{sign}(f(x))$, for $f(x) = 0$. But for nonlinear classifiers, this may be difficult to solve, and thus it's often easier to just plot the output of the classifier on a fine grid as Nick suggested.
