[site]: stackoverflow
[post_id]: 5205570
[parent_id]: 5200077
[tags]: 
AVAssetWriter and Audio This may be the same issue as mentioned in the linked post. Try commenting out these lines [writer_input markAsFinished]; [audio_writer_input markAsFinished]; [video_writer endSessionAtSourceTime: CMTimeMakeWithSeconds([[NSDate date] timeIntervalSinceDate: start_time],30)]; Edit I don't know if the way you are setting the presentation time stamp is necessarily wrong. The way I handle this is with a local variable that is set to 0 on start. Then when my delegate receives the first packet I do: if (_startTime.value == 0) { _startTime = CMSampleBufferGetPresentationTimeStamp(sampleBuffer); } and then [bufferWriter->writer startWriting]; [bufferWriter->writer startSessionAtSourceTime:_startTime]; Your code looks valid as you are calculating the time difference for each received packet. However, AVFoundation calculates this for you, and also optimizes the timestamps for placement in the interleaved container. Another thing I am unsure of is each CMSampleBufferRef for audio contains more then 1 data buffer where each data buffer has it's own PTS. I am not sure if setting the PTS automatically adjusts all the other data buffers. Where my code differs from yours is I use a single dispatch queue for both audio and video. In the callback I use (some code removed). switch (bufferWriter->writer.status) { case AVAssetWriterStatusUnknown: if (_startTime.value == 0) { _startTime = CMSampleBufferGetPresentationTimeStamp(sampleBuffer); } [bufferWriter->writer startWriting]; [bufferWriter->writer startSessionAtSourceTime:_startTime]; //Break if not ready, otherwise fall through. if (bufferWriter->writer.status != AVAssetWriterStatusWriting) { break ; } case AVAssetWriterStatusWriting: if( captureOutput == self.captureManager.audioOutput) { if( !bufferWriter->audioIn.readyForMoreMediaData) { break; } @try { if( ![bufferWriter->audioIn appendSampleBuffer:sampleBuffer] ) { [self delegateMessage:@"Audio Writing Error" withType:ERROR]; } } @catch (NSException *e) { NSLog(@"Audio Exception: %@", [e reason]); } } else if( captureOutput == self.captureManager.videoOutput ) { if( !bufferWriter->videoIn.readyForMoreMediaData) { break;; } @try { if (!frontCamera) { if( ![bufferWriter->videoIn appendSampleBuffer:sampleBuffer] ) { [self delegateMessage:@"Video Writing Error" withType:ERROR]; } } else { CMTime pt = CMSampleBufferGetPresentationTimeStamp(sampleBuffer); flipBuffer(sampleBuffer, pixelBuffer); if( ![bufferWriter->adaptor appendPixelBuffer:pixelBuffer withPresentationTime:pt] ) { [self delegateMessage:@"Video Writing Error" withType:ERROR]; } } } @catch (NSException *e) { NSLog(@"Video Exception Exception: %@", [e reason]); } } break; case AVAssetWriterStatusCompleted: return; case AVAssetWriterStatusFailed: [self delegateMessage:@"Critical Error Writing Queues" withType:ERROR]; bufferWriter->writer_failed = YES ; _broadcastError = YES; [self stopCapture] ; return; case AVAssetWriterStatusCancelled: break; default: break; }
