[site]: datascience
[post_id]: 22715
[parent_id]: 
[tags]: 
1x1 convolution

I am reading an object detection paper called YOLO (You only look once) and I have some questions about the architecture. In the CNN network, the author described that 1x1 convolution reduce feature space from preceding layers. When I see the diagram below, I am confused that he stacked 3x3x256 convolution layer follow by 1x1x256. Isn't the number of features (256 to 256) the same before and after? How is this supposed to reduce the feature space?
