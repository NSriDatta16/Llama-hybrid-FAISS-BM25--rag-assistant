[site]: crossvalidated
[post_id]: 592105
[parent_id]: 
[tags]: 
Genetic Algorithm as engine for Variational Inference?

I'm curious if anyone has used, heard of, or otherwise considered using Genetic Algorithms as an engine for Variational Inference (VI)? My understanding of VI is that it's an optimization algorithm, powered by gradient descent, which has its largest area of application as a replacement to MCMC algorithms; it frames a sampling problem as an optimization problem where the cross entropy between distributions should be minimized. I believe that the following process could be executed: Initialize an array of randomly sampled numbers (bag of samples) Compute the mean, variance, etc of the bag of samples Compute the cross entropy (or KL divergence) from the bag of samples to the target distribution. Iteratively use genetic algorithm to propose random changes to individual samples in the bag. Return bag when hyper-parameter conditions have been met (iteration count, arbitrarily small cross entropy, etc.) Tools like PyGAD can use both integer and decimal data types. By not use gradients, sampling could be used on PMFs in addition to PDFs, a constraint felt by both HMC/NUTS and gradient descent powered (traditional) variational inference, the state of the art MCMC approach. Curious on community thoughts?
