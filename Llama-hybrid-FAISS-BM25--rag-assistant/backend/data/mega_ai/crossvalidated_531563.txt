[site]: crossvalidated
[post_id]: 531563
[parent_id]: 531558
[tags]: 
A conceptual answer might be that exploitation reveals interesting areas for exploration. For example, if you try to learn to write a story by typing random words, you likely won't get far, no matter how much you try. On the other hand, once you've "exploited" and mastered how to write grammatical sentences, structure an interesting plot, use the standard character tropes -- then it's much easier to explore different styles and forms of writing. Theoretically, it's possible off-policy algorithms such as Q-learning to have an rollout policy which is totally different and unrelated from the greedy policy. However, in practice, if the states encountered in the greedy policy are very far from those encountered in the rollouts, the Q-function approximator will probably not generalize well. As for on-policy algorithms, by definition, rollouts need to be sampled from the current policy, so there is no way to improve the policy using arbitrary explorative rollouts.
