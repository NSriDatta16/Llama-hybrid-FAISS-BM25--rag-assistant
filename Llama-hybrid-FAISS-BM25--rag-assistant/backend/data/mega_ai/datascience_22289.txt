[site]: datascience
[post_id]: 22289
[parent_id]: 22199
[tags]: 
One potential approach can be iterative design of a neural network architecture such as Multi-Layer Perceptron (MLP) as described in the following post: https://stats.stackexchange.com/questions/238637/deep-neural-network-tuning-hyperparameters We can restrict ourselves to 4-8 layers with 8-128 (power of 2) neurons per layer. In addition, we can assume recommended ReLU activations with He normal weight initialization and Adam or SGD with Nesterov momentum optimizers. In order to avoid overfitting on a small dataset, it is important to add l1 or l2 regularization (weight decay) and a dropout layer (e.g. with keep probability of 0.5). We can then use cross validation with random search or bayesian optimization to choose the best architecture as described in the cross validated article above.
