[site]: crossvalidated
[post_id]: 190326
[parent_id]: 
[tags]: 
restrict splitting variable number in random forest?

Background: I have a set of ~100 features (input) that predict 25 variables (output). My input variables are integers in {1,2,3,4,5,6,7}, my output is continuous. I have ~100K data rows available. I would like to build a random forest where each tree can only split on 10 variables, that it can freely choose out of 100. But it can split as many times on that variable as it wants. In SKLearn: max_features is a randomly assigned set of 10 features, but I want the tree to choose the 10 features it deems best, out of all 100 features. max_depth and max_leaf_nodes counts multiple splits on a variable just the same as splits on different variables. I don't see any other relevant parameters. So how can I do this? Background: I am aware that trees will often pick the same features. That is exactly what I aim for. I would like to see which features are the most/least necessary for prediction, so that I can make a regression method that predicts my output almost as good on ~80 of these features instead. The ones with the lowest feature importance in the resulting forest can be safely removed. If I don't do the feature limitation as described above, the feature importances are all a bit the same, since the features are very correlated. For example, if feature A and B are both super important, but almost identical and A is a bit more important than B, then B can be left out, so with this approach, A will have a high feature importance and B will have a low feature importance in my approach. Where with plain RFs and max_features=10, both would have a very high feature importance. I'm currently limiting the number of leaf nodes and it works fine, but it does mean I'm not fully using the power of each variable, since that limits the number of binary splits. And once you do one split on a variable, the other splits on that variable should be allowed for free.
