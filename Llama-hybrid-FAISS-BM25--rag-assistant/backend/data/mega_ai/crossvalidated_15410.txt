[site]: crossvalidated
[post_id]: 15410
[parent_id]: 15406
[tags]: 
$E(X)$ explains, in the sense of defining, what the letter $\mu$ is being used to denote (assuming, modulo Wolfgang's comment, that there is something to denote). In philosophical terminology, $\mu$ is a name whereas $E(X)$ is a definite description. That is, of course, a slightly idealised distinction. Practically, the designation is, by convention, just more rigid for $E(X)$ than for $\mu$ in the sense that $\mu$ doesn't always and everywhere denote the expectation of a random variable, but $E(X)$ nearly always does. The utility of using $\mu$ is mostly in situations where you want to take advantage of this loose connection. The two classic examples are when it is a parameter, e.g. in $N(\mu,\sigma^2)$, and when you want to talk simply about an estimate, e.g. $\hat{\mu}$. In both these contexts $\mu$ is operating more like a name because there is no reason that the parameters of a distribution must to be expectations of any sort, although they often are as in the example, and there is no reason that an expectation must be the thing being estimated. The parameterisation and the estimation are the point, and a more explicit notation would obscure this. That said, there is no mathematical reason I can see not to replace $\mu$ with $E(X)$ in a distribution statement. But the pragmatic impact would be to say something like: "Notice that I have chosen to parameterise with two moments, rather than some other way". I agree that would be weird, but only because that's not the convention. This is much the same as the fact that people tend to prefer $$Y \sim Poisson(\lambda)$$ $$\log \lambda = \alpha + \beta X$$ rather than, say $$Y \sim Poisson(\exp (\alpha + \beta X))$$ Because it keeps conceptually distinct claims separate and flags what you're meant to be taking away as a reader. I hope that gives some convention-based rather than mathematical guidance about when you might want to use $\mu$ rather than $E(X)$.
