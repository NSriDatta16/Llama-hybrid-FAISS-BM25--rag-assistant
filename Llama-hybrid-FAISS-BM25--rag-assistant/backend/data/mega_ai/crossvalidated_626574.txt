[site]: crossvalidated
[post_id]: 626574
[parent_id]: 402686
[tags]: 
I disagree with the other response, because you cannot compare perplexities across different tokenizations ; the denominator is different. The same amount of log-likelihood is distributed across different numbers of units (e.g., 13 words vs 30 subwords). In that sense, your approach is correct because you're dividing by the number of words in both cases. (In natural language processing, when we say 'perplexity', we implicitly mean 'perplexity per word'. Being explicit about those last two words would make things a lot clearer.) There's a more subtle problem that prevents you from simply comparing in the way you describe. The naive approach to computing a string's probability in a subword model underestimates its probability. This is because multiple tokenizations will give you the same text . Example. If a subword vocabulary includes the tokens $\{\mathrm{t}, \mathrm{h},\mathrm{e},\mathrm{th},\mathrm{the}\}$ , then the string $\mathrm{the}$ can be generated in three ways: $\{\mathrm{t+h+e}, \mathrm{th+e},\mathrm{the}\}$ . You need to marginalize over all three. So the correct way to compute the probability of string $\boldsymbol{x}$ in the language model $r$ is to marginalize over all segmentations $\boldsymbol{s}$ which yield the string $\boldsymbol{x}$ : $$ p(\boldsymbol{x}) = \sum_{\boldsymbol{s} \in \operatorname{segmentations}(\boldsymbol{x})} \prod_{{i=1}}^{|\boldsymbol{s}|} r\left(s_{i} \mid \boldsymbol{s}_{ Each subword $s_i$ is conditioned the tokens generated before it in the sequence, $\boldsymbol{s}_{ . These issues are discussed at length in Cao and Rimell (2021 at EMNLP) .
