[site]: crossvalidated
[post_id]: 423637
[parent_id]: 
[tags]: 
Are the following is a good wrap of a comparison between all linear regression types (single, multiple and polynomial)?

I am new to machine learning and it's hard to find an instructor to help you with theory based questions. If this question does not fit to this site feel free to remove it. I am comparing the 3 types of linear regression: Single, Multiple, and polynomial. Can we say the following: Single linear regression: is when there is a single independent variable related linearly to a single dependent variable. And to train and predict values, the LinearRegression() class is used. Multiple linear regression: is when there is a multiple independent variables related linearly to a single dependent variable and we should avoid multicollinearity and dummy variable trap while coding and analyzing . To code it using the backward elimination method: a. So we check p-values and remove the higher than the significance level and keep the other; b. A new field is added to the beginning of the set to calculate b0 intercept; c. Avoid dummy variable trap; d. Afterwards, we user the same LinearRegression() class to train and predict on the final data set after removing the interdependent features. Polynomial linear regression: is when a single or multiple independent feature is/are related linearly with a parabola to a single dependent variable. We figure out that by adding a new field to the data set with a second degree (or even more for better precision) and then run the LinearRegression() class on the new data set. Is all of the above true ?
