[site]: crossvalidated
[post_id]: 447757
[parent_id]: 447747
[tags]: 
If you have discrete variables (which is generally considered bad practice with SMOTE, I guess for exactly this reason), this can happen quite easily. Suppose you have a binary variable. Then some of the synthetic data points are likely to have fractional values for that variable, and a tree can separate those samples out with two splits. Categorical variables can be dealt with ( Oversampling with categorical variables ), but if you leave in discrete numerical features you leave yourself open to this particular kind of synthetic rule. Presumably, similar but more subtle "holes" in your dataset could similarly be filled by SMOTE and then picked out by your tree model. Finally, please consider whether you actually need to balance your dataset. What is the root cause of the class imbalance problem? When is unbalanced data really a problem in Machine Learning? Class imbalance in Supervised Machine Learning
