[site]: stackoverflow
[post_id]: 4454115
[parent_id]: 4444838
[tags]: 
Quantification of test-quality is very difficult. I see code-coverage only as guidance not as test-quality metric. You can literally write test of 100% code-coverage without testing anything (e.g. no asserts are used at all). Also have a look at my blog-post where I warn against metric-pitfalls. The only sensible quantitative metric I know of and which counts for business is really reduced effort of bug-fixes in production-code. Also reduced bug-severity. Still it is very difficult to isolate that unit-tests are the only source of this success (it could also be improvement of process or communication). Generally I would focus on the qualitative approach: Do developers feel more comfortable changing code (because tests are a trustworthy safety net)? When bugs occur in production analysis really shows that it was untested code (vice versa a minor conlusion that it wouldn't have occurred if there had been unit test)
