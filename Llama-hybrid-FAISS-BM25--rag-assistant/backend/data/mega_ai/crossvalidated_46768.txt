[site]: crossvalidated
[post_id]: 46768
[parent_id]: 
[tags]: 
Confusion related to linear dynamic systems

I was reading this book Pattern Recognition and Machine Learning by Bishop. I had a confusion related to a derivation of the linear dynamical system. In LDS we assume the latent variables to be continuous. If Z denotes the latent variables and X denotes the observed variables $p(z_n|z_{n-1}) = N(z_n|Az_{n-1},\tau)$ $p(x_n|z_n) = N(x_n,Cz_n,\Sigma)$ $p(z_1) = N(z_1|u_0,V_0)$ In LDS also alpha beta forward backward message passing is used to calculated the posterior latent distribution i.e $p(z_n|X)$ $\alpha(z_n)=p(x1...xn,z_n)$ $\hat\alpha(z_n) = \alpha(z_n)/P(x1....xn)$ My first question is in the book it is given as $\hat\alpha(z_n) = N(z_n|u_n,V_n)$ How come we got the above. I mean $\hat\alpha(z_n)$ = $N(z_n|u_n,V_n))$. I mean how did we get this? My next question is related to the derivation as you can follow along the screenshots of the pages of the book attached. I didn't get what where $K_n$ came from and what Kalman filter gain is $u_n = Au_{n-1} + K_n(x_n - CAu_{n-1})$ $V_n = I - K_nC)P_(n-1)$ $c_n = N(x_n|CAu_{n-1},CP_{n-1}C^T + \Sigma$ $K_n$ is the Kalman gain matrix $P_{n-1}C^T(CP_{n-1}C^T + \Sigma) ^ {-1}$ How did we derive the above equations, I mean how come $u_n = Au_{n-1} + K_n(x_n - CAu_{n-1})$ I am just confused how the above derivation is made.
