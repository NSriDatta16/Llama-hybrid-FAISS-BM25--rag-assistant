[site]: datascience
[post_id]: 77341
[parent_id]: 
[tags]: 
Literature on selecting specific dimensions in a word embedding vector

I am aware that the different dimensions in the word embedding represents different information and algebraic operations can be performed between two embeddings for example. Can anyone point me to literature on selecting specific dimensions from a word embedding vector. I don't mean dimensionality reduction, but papers following the theory that all those dimensions are not important for all the tasks and that specific ones will be more important for a specific task. For example: Sentiment Analysis would benefit from dimension 5th to 55th and 250th to 300th instead of using the whole 300 dimensions. Let me know if this theory isn't true.
