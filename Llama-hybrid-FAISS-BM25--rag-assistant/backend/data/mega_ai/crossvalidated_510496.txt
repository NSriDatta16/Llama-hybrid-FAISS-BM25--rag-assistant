[site]: crossvalidated
[post_id]: 510496
[parent_id]: 
[tags]: 
Is gamma actually an efficient way to weigh future rewards in reinforcement learning?

Typically the discounted sum of rewards is defined as follows: G_t = Sum(gamma ** n * reward_t...) But this means that rewards are worth exponentially less with each timestep. Wouldn't it make more sense to have reward weighting looking something like this? So we maximise rewards at some t + n in the future instead of trying to maximise rewards at t + 1 and then exponentially decaying the weights thereafter? What is the intuition or reasoning behind the g ** n weighting of rewards compared to any other function of reward weighting?
