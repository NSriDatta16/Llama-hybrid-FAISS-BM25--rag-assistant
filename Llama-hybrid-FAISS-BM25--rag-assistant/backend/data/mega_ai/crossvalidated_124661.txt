[site]: crossvalidated
[post_id]: 124661
[parent_id]: 124628
[tags]: 
I do not have a text-book answer. However here are some thoughts. Boosting can be seen in direct comparison with bagging. These are two different approaches of the bias variance tradeoff dilemma. While bagging have as weak learners, some learners with low bias and high variance, by averaging the bagging ensemble decrease the variance for a little bias. Boosting on the other hand works well with different weak learners. The boosting weak learners have high bias and low variance. By building up one learner on the top of another, the boosting ensemble tries to decrease the bias, for a little variance. As a consequence, if you consider for example to use bagging and boosting with trees as weak learners, the best way to use is small/short trees with boosting and very detailed trees with bagging. This is why very often a boosting procedure uses a decision stump as weak learner, which is the shortest possible tree (a single if condition on a single dimension). This decision stump is very stable, so it has very low variance. I do not see any reason to use trees with boosting procedures. However, short trees are simple, easy to implement and easy to understand. However, I think that in order to be succesfull with a boosting procedure, your weak learner has to have low variance, has to be rigid, with very few degrees of freedom. For example I see no point to have as a weak learner a neural network. Additionally, you have to note that for some kind of boosting procedures, gradient boosting for example, Breiman found that if the weak learner is a tree, some optimization in the way how boosting works can be done. Thus we have gradient boosting trees. There is a nice exposure of boosting in the ESTL book.
