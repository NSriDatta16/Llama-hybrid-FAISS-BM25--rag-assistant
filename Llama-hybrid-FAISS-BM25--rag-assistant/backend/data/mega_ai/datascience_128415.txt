[site]: datascience
[post_id]: 128415
[parent_id]: 128360
[tags]: 
VAE objective is to maximise the ELBO: $$ \int_x p(x) \int_z q(z|x) \big\{ \log p(x|z) - \log \tfrac{q(z|x)}{p(z)} \big\} $$ the first term reconstructs the data, the second term maximises the entropy of $q(z|x)$ ("puffing out" the posteriors) and constrains latent representations $z$ to fit the prior $p(z)$ . There are few parameters to tweak, the prior $p(z)$ is typically chosen as a standard Gaussian, $q(z|x)$ learns to approximate the true posterior $p(z|x) \doteq \tfrac{p(x|z)p(z)}{\int_z p(x|z)p(z)}$ reconstruction is driven by maximising $p(x|z) =\mathcal{N}(x; dec(z), \sigma_x^2)$ The main parameter to consider is the reconstruction variance $\sigma_x^2$ . Setting this to be smaller lowers the probability of poor reconstructions, which must improve to maximise the term (if the network architecture is flexible enough to allow it). Re generation: while reconstructions should improve (for training set), if the networks are not sufficiently flexible, the distribution of $z$ may not fit $p(z)$ (e.g. consider if both encoder and decoder were linear). So sampling $p(z)$ may give $z$ 's that don't correspond to the data and generating from them gives gibberish. [Note: $\sigma_x^2$ is in the denominator of the (log) Gaussian reconstruction term, so optimising the ELBO by SGD is unchanged by multiplying through by $\sigma_x^2$ (with an adjusted learning rate), so it is common to use an MSE reconstruction term (w/o $\sigma_x^2$ ) and "weight" the KL term instead. This weight can be interpreted as $\sigma_x^2$ and reducing it has the same effect as above.]
