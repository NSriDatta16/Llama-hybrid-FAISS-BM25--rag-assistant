[site]: crossvalidated
[post_id]: 396426
[parent_id]: 395940
[tags]: 
Here is a sketch of a solution. The general strategy is to introduce the scalar sum $\theta_T=\sum\theta_i, $ then marginalize over it, then use it to turn the complicated multidimenional sum over the $\theta_{1..n}$ into a dynamic programming problem with aggressive cutoffs to prune solution space as the dynamic program runs. We know that $\theta_T$ is a sum of gamma RVs with heterogeneous parameters. Since $\beta_i>0$ for all $i$ , the variances are finite, and if $n$ is large enough, this will succumb to the CLT, so $\theta_T \rightarrow_d ~~ N(\mu_T,\sigma^2_T)$ where $\mu_T =\sum \alpha_i/\beta_i$ and $\sigma^2_T=\sum \alpha_i/\beta_i^2.$ This gives us a credible subset of $\theta_T$ over which to focus our attention on $\theta \in [\theta_{T_0},\theta_{T_1}]$ . Even if $n$ isn't particularly large, we are able to approximate $p(\theta_T)$ through numerical methods and other methods, which are contingent on the data. For each $\theta$ selected from this interval, we choose an integer $M$ so that $\theta/M$ is smallest quantity of $\theta$ to be allocated to $\theta_i$ in the partitioning of $\theta$ among the $\theta_{1..n}$ . This turns the problem into a simple dynamic programming problem in which $\sum \theta_{1..n}=\theta_T$ is equivalent to $\sum m_i=M$ where $0\le m_i \le M, m_i \in \mathbb{N}. $ The number of solutions is $\binom{M+n-1}{n-1}$ . For numbers like $M=20, n=10, \binom{29}{9} \sim 10^8$ . Fortunately, we may implement the dynamic programming with an eye to selecting the solutions for which $P(\theta_i|\alpha_i,\beta_i)>\epsilon$ which will reduce the number tremendously. This is equivalent to $\sum m_i=M$ requiring that, e.g. , $1 \le m_i \le M$ , and now the number of solutions is $\binom{(M-n)+n-1}{n-1}$ . For $M=20, n=10, \binom{19}{9}\sim 10^5,$ or 3 orders of magnitude smaller with a simple, obviously-correct requirement. The cleverness enters when you memo-ize the dynamic program in order to reuse the solutions, then use a rule for varying $\alpha_{1..n},\beta_{1..n}$ perhaps to maintain constant values for $\mu_T$ and $\sigma^2_T$ , and which will also maximize reuse of the dynamic programming solution. This constancy of $\mu_T$ and $\sigma^2_T$ may well be a within the realm of your needs. Other tricks to making the problem manageable is what I would put in a more-than-just-a-sketch answer to this question. In summary, $$ \begin{align} P(X|\alpha_{1..n},\beta_{1..n}) &= \sum_{\theta_T} P(X|\theta_T,\alpha_{1..n},\beta_{1..n})P(\theta_T| \alpha_{1..n},\beta_{1..n} )\\ &= \sum_{\theta_T} \sum_{\{\theta_{1..n}|\sum \theta_i=\theta_T\}} P(X|\alpha_{1..n},\beta_{1..n})N(\theta_T|\mu_T,\sigma^2_T). \end{align} $$ Then, $P(X|\alpha_{1..n},\beta_{1..n})= \binom{X}{x_1,x_2..x_n}\theta_1^{x_1} \cdots \theta_n^{x_1}$ need only be evaluated only for those $\theta_{1..n}$ values that are high probability in the first place. Note that the $\theta_{1..n}$ are implicitly functions of $\alpha_{1..n},\beta_{1..n}$ . Some observations Finding a dynamic programming problem in the wild is always a lot of fun. It would make for a fairly high profile research result, I should think. There are numbers $M$ and $n$ here where this approach becomes computationally very hard, even with dynamic programming. If $n$ has to be large, you may compensate by using smaller $M$ or larger $\epsilon.$ Ultimately, the problem may only be solvable if you are willing to a) use some restrictive priors, and/or b) accept a low resolution solution. The choice to marginalize over $\theta_T$ means that your use of log-likelihood will have to be term-by-term in the sum, rather than all at once, which is not so inconvenient in the grand scheme of things. Is this close to what you are looking for? Addendum A 20 line dynamic programming python script is able to enumerate $\binom{M+n-1}{n-1} M=100, ~n=20$ with a solution $4.910371215196107e+21$ in $0.18s$ . This involves counting solutions by visiting them in the dynamic programming sort of way in which redundant solutions do not need to be revisited, just accounted for. In this enumeration, $\epsilon=0$ . Visit-every-state enumeration is much slower, of course. For M=20 and n=10, the full enumeration that visits every one of the $10^8$ solutions currently takes 42s, which is where $\epsilon$ is valuable, and requirements that permit low resolution reporting of results. The python code is included here as requested. # -*- coding: utf-8 -*- """ Created on Fri Mar 8 21:38:57 2019 @author: Peter """ import numpy as np import scipy.special from datetime import datetime as dt Solutions=[] # is a list of lists of stars grouped by bars """ This is the recursive routine that uses optimal substructure -- the key design element of dynamic programming -- to *count* all of the distinct ways in which M identical stars can be located in n pidgeon holes. This is an exact enumeration of the solution to the stars-and-bars problem. This algorithm can be adapted to *visit* explicitly instead of leveraging dynamic programming and Memoization to merely *count* every solution, It is significantly slower if it visits the solutions. The advantage of visiting a solution is to eliminate solutions with a kpredictable low probability. """ """ countMe """ ########################################################## def countMe(m #The index of the pidgeon hole to start placing stars ,i #The number of stars that have to be placed ,V # ,visitEverySolution ): global Memo global Called; Called += 1 # this counts the number of times CountMe was called if i ```
