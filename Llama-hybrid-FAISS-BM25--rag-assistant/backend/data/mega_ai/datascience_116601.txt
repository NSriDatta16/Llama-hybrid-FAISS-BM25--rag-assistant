[site]: datascience
[post_id]: 116601
[parent_id]: 116567
[tags]: 
Well, "legit" is not a thing in deep learning. This is an empirical field, and we use whatever works. There are no deep learning police who are going to tell you that you are doing it wrong. I suspect you are asking whether the method will still be as effective if you only put dropout on the fully connected layers. The way to figure that out is to try both and compare how well each works. I wouldn't be surprised if it works less well to have dropout after only the fully connected layers, but it's hard to know without trying it.
