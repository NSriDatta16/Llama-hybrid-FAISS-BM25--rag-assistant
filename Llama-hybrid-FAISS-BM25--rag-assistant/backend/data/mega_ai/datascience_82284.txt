[site]: datascience
[post_id]: 82284
[parent_id]: 72590
[tags]: 
I'm assuming you do not have access to a larger machine (e.g. leveraging cloud compute) and you have to run this locally in some way. If that's the case, one suggestion I would make is to try a combination of Vaex and sklearn's SGDRegressor functionality. You can think of Vaex as providing easy manipulation of dataframes which are larger than your memory (i.e. out-of-core Dataframes). With this, you can do your feature engineering and EDA in a more straightforward way than loading chunks. When you are ready to model, you can leverage some of the models in sklearn which allow for training on batches of data at a time (e.g. SGDRegressor or SGDClassifier ). These options provide a partial_fit method for training you model sequentially on different batches from your dataset. Some links if you are interested in going down this path: https://github.com/vaexio/vaex https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html https://towardsdatascience.com/ml-impossible-train-a-1-billion-sample-model-in-20-minutes-with-vaex-and-scikit-learn-on-your-9e2968e6f385
