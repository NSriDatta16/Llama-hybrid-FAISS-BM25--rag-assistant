[site]: datascience
[post_id]: 73509
[parent_id]: 73502
[tags]: 
OLS Model: One of the assumptions behind OLS (aka linear regression) is homoskedasticity, namely: $$ Var(u| x ) = \sigma^2.$$ Recall that the linear model is defined: $$ y = X \beta + u, $$ where $u$ is the statistical error term. The error term (per OLS assumptions) need to have an expected value $E(u|x)=0$ (orthogonality condition) with variance $\sigma^2$ , so that the error is distributed $u \sim (0,\sigma^2)$ . Heteroscedasticity: In case the variance of $u$ is not "harmonic" and the assumption above is violated, we say that error terms are heteroscedastic. Heteroscedasticity does not (!) change the estimated coefficients, but it does affect the (estimated) standard errors and consequently the confidence bands. The error variance is estimated by: $$ \hat{\sigma}^2 = 1/(n-2) \sum{\hat{u}^2} .$$ The standard error (of coefficient $\beta$ ) is estimated by: $$ se(\hat{\beta}) = \hat{\sigma} / (\sum{(x_i-\bar{x})^2})^{1/2}.$$ The assumption of homoskedasticity is required in order to get proper estimates of the error variance and the ("normal", in contrast to "robust", see below) standard errors. Standard errors in turn are used to calculate confidence bands. So in case you cannot trust the estimated standard errors, you can also not rely on the confidence bands. The problem here ultimately is, that given heteroscedasticity, you cannot tell if some estimated coefficient is statistically significant or not. Significance here is defined (95% confidence) so that the confidence band of some estimated coefficient does not „cross“ zero (so is strictly positive or negative). There are different options to deal with heteroscedasticity: The most common solution is to use "robust" standard errors . There are different versions of "robust" errors (HC1, HC2, HC3). They all have in common, that they aim at getting a "robust" estimate of the error variance. Most software allows you to calculate robust SE. Find an example for R here . Another alternative would be to estimate a " feasible generalised model " (FGLS) in which you first estimate the scedastic function (to get an idea of the distribution of errors) and you try to "correct" problems in the error distribution. However, this is not something you would use very often in practice. It is more an academic excercise. Testing heteroscedasticity: Usually, you would test if there is heteroscedasticity. You can look at the " residual vs. fitted plot " to get an idea of how the error terms are distributed. However, a proper test can be done using the White or Breusch-Pagan Tests . Here is an example in R.
