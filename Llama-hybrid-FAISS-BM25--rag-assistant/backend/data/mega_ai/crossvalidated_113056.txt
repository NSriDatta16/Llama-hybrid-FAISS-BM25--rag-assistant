[site]: crossvalidated
[post_id]: 113056
[parent_id]: 
[tags]: 
Correct Evaluation of Random Forest on fixed training/test set

I have to test the performance of Random Forest on the same dataset (text classification) with about 118.000 instances of which about 1/3 is used for training and 2/3 is used for testing. The division is given, so I cannot modify either the test or the training set. I need to conduct an investigation about which parameters and/or features are best for the classification problem given a certain metric. Now, for the randomness of random forest each run I obtain a different output predictor, that will generate different results, and so the metric will vary from run to run. So my initial idea was to simply run the random forest a certain number n of times (say n=4) and take the average value of the given metric as representative of that particular configuration, in order to compare with the other possibilities. Probably a better approach would make use of a statistical test in order to establish to which degree we can assure that one model is better than another. How do you suggest to perform this considering the particular case I've mentioned above?
