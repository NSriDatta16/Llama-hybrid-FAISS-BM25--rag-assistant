[site]: crossvalidated
[post_id]: 200276
[parent_id]: 
[tags]: 
Which approach can I use to train SVMs with a hundred of thousand of examples?

I am using SVMs to learn models. Every time want to use them on a "real life" data set, I see that they take forever to run. I found that the computational complexity is O(n_samples^2 * n_features). I made an experiment and it seems to be correct: Sample size = 8780 It takes 17 secondes to train a model. Sample size = 87804 It takes 1758 secondes to train a model. However, I have a dataset with 870 000 samples. Using this formula, training with 870000 samples and 39 features will take me 50 hours... What is the recommended approach for using SVMs with so much data? Is working on a subset of data correct? If so how to select it?
