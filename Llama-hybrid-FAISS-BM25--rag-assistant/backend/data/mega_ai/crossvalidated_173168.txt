[site]: crossvalidated
[post_id]: 173168
[parent_id]: 
[tags]: 
How to measure similarity between features in different datasets, if possible?

Is it possible to measure the similarity between two different features in different datasets? It could sound like a non-sense question, but it has sense in the context of merging two almost compatible datasets. I will try to explain. The usual setup would be a bag-of-words dataset that you want to merge with another different one, with different set of words (thus, features), and possible some common ones, like length or class, among others. With the mentioned situation, we have some unique features for the first dataset, some shared ones, and some others exclusive for the second dataset. The idea is to create a new dataset with the unique features for the first, the shared ones, and the unique ones for the second, setting the appropiate new values to any convenient value or to missing values, as desired. For this to be possible I would need some feature similarity function/evaluation. It would be easy task if features were of the same length (number of values/number of instances), but this is never the case. Are there any suitable approaches to the explained problem? It would be an useful previous step to have a consistent way of transforming different length vectors (the attributes) into fixed length ones for building a dataset, when inputs are of variable length. This way, I could create a feature comparing dataset, to let data work, instead of coding a weak/fragile similarity function by hand. I think it is a difficult problem and I can not find any satisfactory approach to tackle it. Thanks in advance. Comment if you need any clarification. Edit: Weka has some built-in functions for merging datasets, but they only work for same set of attributes (identical), to add instances together, of for same set of instances, to add attributes to destination dataset. My intention is to unify and make a flexible comparator+merger for any kind of attribute, if possible.
