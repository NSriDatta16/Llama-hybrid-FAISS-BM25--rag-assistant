[site]: crossvalidated
[post_id]: 545203
[parent_id]: 
[tags]: 
Bayesian multivariate regression with common coefficients

In a hierarchical model I'm working on, I have $K$ different $N\times P$ predictor matrices, each denoted $X_k$ and $K$ length $N$ outcome vectors each denoted $y_k$ . Essentially, I have a multivariate regression problem, but I want to use one set of coefficients for all outcomes and all covariates. $$ y_k \sim N_N(X_k\beta, \Sigma_k) $$ Fitting one of these models is straightforward in a Bayesian setting. Assuming $\beta \sim N_P(m, V)$ , the posterior is closed form, because $$ \pi(\beta|X_k,y_k,\Sigma_k) = \pi(\beta) \pi(y_k | X_k,\beta,\Sigma_k) \\ \propto \exp \left( (\beta - m)'V^{-1}(\beta - m) + (y_k - X_k\beta)'\Sigma_k^{-1}(y_k - X_k\beta) \right) $$ followed by completing the square, etc, as in (eg) the wiki page on the topic of Bayesian linear regression . I'm ignoring When fitting one set of coefficients $\beta$ to the entire set $X$ of the predictor matrices $X_k$ and the entire set $Y$ of the outcome vectors $y_k$ , the posterior is of the form $$ \pi(\beta|X,Y,\Sigma_k) = \pi(\beta)\prod_{k=1}^K \pi(y_k | X_k,\beta,\Sigma_k) \\ \propto \exp \left( (\beta - m)'V^{-1}(\beta - m) + \sum_{k=1}^m(y_k - X_k\beta)'\Sigma_k^{-1}(y_k - X_k\beta) \right) $$ Is it likely this can be expressed in closed form in a similar manner? To infer the actual model I could of course use a Metropolis-Hastings step, but I'm interested if there's a known closed form or obvious solution to this, as I find MH sampling can be inefficient for these cases. Apologies if it's something obvious, I'm rather tired.
