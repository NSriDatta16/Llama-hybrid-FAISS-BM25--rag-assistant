[site]: crossvalidated
[post_id]: 465003
[parent_id]: 464985
[tags]: 
There's two reasons why you may want a validation (or holdout test set or whatever you want to call it) that you do not look at, at all, while deciding on your final model and use to evaluate the final performance. If you are instead talking about a validation set for tuning hyperparameters (and have reserved a separate test set), then no, you typically do not need such a validation set in addition. For that purpose cross-validation is generally preferable (multiple appropriate splits are more stable/reliable for making decisions such as hyperparameter tuning than a single split). Reasons for having a holdout test set: Firstly, you tuned your modeling decisions (such as hyperparameters) based on the cross-validation (obviously, more of an issue when your dataset is smallish and less of an issue when it is huge; note: large/huge is bigger than many people think). Secondly, your cross-validation may not resemble your actual prediction task (e.g. a classic case is if you are trying to predict a time series into the future, in that case 10-fold cross-validation can be extremely unreliable, while a holdout of the last month/year/whatever of your data can be a lot more meaningful). You may of course argue that that's more a question of doing cross-validation properly. On a large enough dataset with a cross-validation that closely mimicks your real-world setting, experience e.g. in Kaggle competitions suggests that your cross-validation will often provide a really good estimate for the performance on a hold-out test set. On the other hand, there are examples that even with large datasets you eventually get overfitting to a validation/test set (see e.g. https://arxiv.org/abs/1902.10811 ). As you suggest it increases credibility/removes questions if you have a hold-out test set.
