[site]: datascience
[post_id]: 53466
[parent_id]: 53427
[tags]: 
I wanted to know if there are any papers who have tried doing this (links would be really helpful). The best known paper on this topic is Visualizing and Understanding Convolutional Networks by Matthew Zeiler and Rob Fergus. But you can find many more papers and online articles that do the same. Meanwhile are there any other ways to understand what happens in each hidden layers? Unfortunately, deep neural networks are like black boxes; we have little understanding of their inner workings. Visualizing the activations of the layers is probably the best way to tell what features they extract.
