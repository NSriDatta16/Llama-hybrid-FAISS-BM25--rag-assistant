[site]: crossvalidated
[post_id]: 68180
[parent_id]: 67936
[tags]: 
I suppose it depends on how strictly you interpret the word "design". It is sometimes taken to mean completely randomized vs. randomized blocks, etc. I don't think I've seen a study that died from that. Also, as others have mentioned, I suspect "died" is too strong, but it depends on how you interpret the term. Certainly I've seen studies that were 'non-significant' (and that researchers subsequently did not try to publish as a result); under the assumption that these studies might have been 'significant' if conducted differently (according to obvious advice that I would have given), and hence been published, might qualify as "died". In light of this conception, the power issue raised by both @RobHall and @MattReichenbach is pretty straightforward, but there is more to power than sample size, and those could fall under a looser conception of "design". Here are a couple of examples: Not gathering / recording / or throwing away information I worked on a study where the researchers were interested in whether a particular trait was related to a cancer. They got mice from two lines (i.e., genetic lines, the mice were bred for certain properties) where one line was expected to have more of the trait than the other. However, the trait in question was not actually measured, even though it could have been. This situation is analogous to dichotomizing or binning a continuous variable, which reduces power. However, even if the results were 'significant', they would be less informative than if we knew the magnitude of the trait for each mouse. Another case within this same heading is not thinking about and gathering obvious covariates. Poor questionnaire design I recently worked on a study where a patient satisfaction survey was administered under two conditions. However, none of the items were reverse-scored. It appeared that most patients just went down the list and marked all 5s ( strongly agree ), possibly without even reading the items. There were some other issues, but this is pretty obvious. Oddly, the fellow in charge of conducting the study told me her attending had explicitly encouraged her not to vet the study with a statistician first, even though we are free and conveniently available for such consulting.
