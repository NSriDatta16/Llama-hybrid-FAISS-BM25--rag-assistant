[site]: datascience
[post_id]: 106239
[parent_id]: 106223
[tags]: 
In case of imbalanced datasets, we can either use stratify while splitting the data, or use Cross validation, or both. Stratifying/CV upfront helps mitigate this data leakage by developer confirmation bias. Stratify from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25) Cross validation >>> from sklearn.model_selection import cross_val_score >>> clf = svm.SVC(kernel='linear', C=1, random_state=42) >>> scores = cross_val_score(clf, X, y, cv=5) >>> scores array([0.96..., 1. , 0.96..., 0.96..., 1. ]) In this example there are 5 random splits, 5 models score. RepeatedStratifiedKFold helps. from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=42) scores = cross_val_score(model, X, y, scoring='mean_squared_error', cv=cv, n_jobs=-1) print('Mean MSE: %.4f' % mean(scores)) In this example there are 5 splits and two iteration, total of 10 models aggregated score.
