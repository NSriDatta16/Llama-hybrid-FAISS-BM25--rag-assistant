[site]: stackoverflow
[post_id]: 1941826
[parent_id]: 
[tags]: 
Why is this faster on 64 bit than 32 bit?

I've been doing some performance testing, mainly so I can understand the difference between iterators and simple for loops. As part of this I created a simple set of tests and was then totally surprised by the results. For some methods, 64 bit was nearly 10 times faster than 32 bit. What I'm looking for is some explanation for why this is happening. [The answer below states this is due to 64 bit arithmetic in a 32 bit app. Changing the longs to ints results in good performance on 32 and 64 bit systems.] Here are the 3 methods in question. private static long ForSumArray(long[] array) { var result = 0L; for (var i = 0L; i I have a simple test harness that tests this var repeat = 10000; var arrayLength = 100000; var array = new long[arrayLength]; for (var i = 0; i ForSumArray(array))); repeat = 100000; Console.WriteLine("For2: {0}", AverageRunTime(repeat, () => ForSumArray2(array))); Console.WriteLine("Iter: {0}", AverageRunTime(repeat, () => IterSumArray(array))); private static TimeSpan AverageRunTime(int count, Action method) { var stopwatch = new Stopwatch(); stopwatch.Start(); for (var i = 0; i When I run these, I get the following results: 32 bit: For: 00:00:00.0006080 For2: 00:00:00.0005694 Iter: 00:00:00.0001717 64 bit For: 00:00:00.0007421 For2: 00:00:00.0000814 Iter: 00:00:00.0000818 The things I read from this are that using LongLength is slow. If I use array.Length, performance for the first for loop is pretty good in 64 bit, but not 32 bit. The other thing I read from this is that iterating over an array is as efficient as a for loop, and the code is much cleaner and easier to read!
