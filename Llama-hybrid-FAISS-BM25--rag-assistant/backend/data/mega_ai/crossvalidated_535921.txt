[site]: crossvalidated
[post_id]: 535921
[parent_id]: 535920
[tags]: 
I am not sure if I understand you correctly. Yes, you can combine several kernels by adding and multiplying (cf. section 4.2.4 in Rasmussen/Williams, http://www.gaussianprocess.org/gpml/chapters/RW.pdf ). More precisely, if $Y(x) \sim \mathcal{GP}(m, k)$ and $Z(x) \sim \mathcal{GP}(m', k')$ (mean and kernel function) are independent Gaussian processes, then $(Y+Z)(x)\sim \mathcal{GP}(m+m', k+k')$ . For multiplication this is a little bit more involved, because the product of two Gaussian processes will not be a Gaussian process itself, but it will have mean $m m'$ and covariance $k k'$ , and there will exist a Gaussian process with those (just it will not be equal to the product, unlike the summation case). But this does not address 'loss of information' (and TBH I am not sure if I understand you there either), rather it corresponds to describing a decomposition of your process into components. I would encourage you to study the very instructive Mauna Loa example in section 5.4.3 in Rasmussen/Williams on this. Regarding weights in $\alpha Y + \beta Z$ , you can consider a constant $\alpha$ as a Gaussian process with a constant kernel (this is also supported e.g. by the sklearn GP toolbox), so yes, you can train those weights too. Again, I would point to the Mauna Loa example. (Correction: Kernel constructors in sklearn usually have a parameter for the scaling factor; however, you can also multiply with a ConstantKernel, it's equivalent. The point is that these weights behave like any other parameter of the kernel, no special treatment required in the parameter fitting) EDIT: See also here for copyable code: https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_co2.html
