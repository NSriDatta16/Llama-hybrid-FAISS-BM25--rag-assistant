[site]: crossvalidated
[post_id]: 431286
[parent_id]: 
[tags]: 
Theoretically, does deep learning defeat all other statistical methods in predictive modeling?

I am a student of statistics/machine learning, just beginning to understand a bit about different methods and predictive modeling, but this question has been bothering me for a long time. We learn all these "simple" methods (regression, decision trees, etc.), which are nice and easy to use in practice. However, I have got the impression that deep learning (which, I suppose, is usually equivalent to neural networks), is or should be vastly superior to all these simpler methods. Note that I am focusing on predictive abilities of models, not interpretation. I understand that in practice, deep learning methods can be time consuming to implement, and that trying out a few simpler methods can produce results more quickly, with the added benefit of them being understood more easily by people. However, theoretically speaking , with infinite amount of computing resources and time, is it the case that deep learning methods are always superior to simpler models (i.e. regression, trees) when it comes to predictive abilities? Or are deep learning methods susceptible to overfitting, possibly producing inferior results even with unlimited computing resources? In the long run, if computing power keeps increasing rapidly, will all predictive modeling be done by deep learning algorithms, rendering more basic statistics tools useless in predictive modeling? I suppose that this has not been mathematically proven, but I am looking for the general consensus and any articles/book chapters/posts written on this.
