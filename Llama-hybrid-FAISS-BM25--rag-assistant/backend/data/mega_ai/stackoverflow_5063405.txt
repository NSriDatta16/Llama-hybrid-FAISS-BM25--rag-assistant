[site]: stackoverflow
[post_id]: 5063405
[parent_id]: 5063178
[tags]: 
If you really want to maximize the speed, you could pre-compute a lookup table where given a byte-value you have the cardinality, but BitArray is not the most ideal structure for this, since you'd need to use reflection to pull the underlying storage out of it and operate on the integral types - see this question for a better explanation of that technique. Another, perhaps more useful technique, is to use something like the Kernighan trick , which is O(m) for an n-bit value of cardinality m. static readonly ZERO = new BitArray (0); static readonly NOT_ONE = new BitArray (1).Not (); public static int GetCardinality (this BitArray bits) { int c = 0; var tmp = new BitArray (myBitArray); for (c; tmp != ZERO; c++) tmp = tmp.And (tmp.And (NOT_ONE)); return c; } This too is a bit more cumbersome than it would be in say C, because there are no operations defined between integer types and BitArrays, ( tmp &= tmp - 1 , for example, to clear the least significant set bit, has been translated to tmp &= (tmp & ~0x1) . I have no idea if this ends up being any faster than naively iterating for the case of the BCL BitArray, but algorithmically speaking it should be superior. EDIT: cited where I discovered the Kernighan trick, with a more in-depth explanation
