[site]: crossvalidated
[post_id]: 546852
[parent_id]: 546839
[tags]: 
From practical point of view, how deep learning models are trained, deployed and improved. In the current literature, architecture usually implies a neural network architecture, i.e., choosing layer topology and activation functions. Outside neural networks, architecture makes not much sense, unless they have hierarchy and need of search. The usage of the term architecture is quite recently popularised by the field of Neural Architecture Search (NAS) . It is a form of hyper parameter optimisation but with hierarchy. Can the word model be rightly used before an architecture has been trained? In practice, one should first decide on the architecture (NAS) before defining the model fully. Defining architecture is not sufficient to have a model, while model would include the information on how network parameters are learned,i.e., learning algorithm. This division is prominently separated in NAS literature. For example, MS's NAS from Langford et. al. Are neural networks, decision trees, SVMs, etc. models generically or only after they've established fixed parameters? Again practically, they became models after establishing the fixed parameters.
