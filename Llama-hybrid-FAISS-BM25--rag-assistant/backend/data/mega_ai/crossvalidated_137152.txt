[site]: crossvalidated
[post_id]: 137152
[parent_id]: 
[tags]: 
Running all possible additive combinations of a linear model and averaging the coefficients

I have nine predictor variables and one response and when I run a linear model in R I'm getting negative coefficients and non-significant p-vales for essentially all the estimates. I've examined the predicted vs observed plot and the model seems to fit well but I'm really more interested in interpreting the coefficients but I'm not sure if I can trust the estimates and negative values don't make sense in a real world context. So I thought that maybe running all possible model combination and averaging the coefficients together might get me better estimates? Is this something that make sense to do? Call: lm(formula = sales ~ t + n + p + r + s + d + db + r + a) Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 2.867e+06 1.429e+06 2.006 0.0554 . t 1.033e+06 6.426e+04 16.070 5.09e-15 *** n 8.866e-01 3.184e+00 0.278 0.7829 p -5.045e+00 1.942e+01 -0.260 0.7971 rd 3.442e+01 2.582e+01 1.333 0.1941 s -7.799e+01 1.288e+02 -0.606 0.5500 d -9.826e+01 5.992e+01 -1.640 0.1131 db 6.301e+01 3.468e+01 1.817 0.0808 . r 1.558e+01 7.469e+01 0.209 0.8364 a -5.466e+00 2.229e+01 -0.245 0.8082 Residual standard error: 2675000 on 26 degrees of freedom Multiple R-squared: 0.9559, Adjusted R-squared: 0.9407 F-statistic: 62.65 on 9 and 26 DF, p-value: 2.421e-15 Without t: Call: lm(formula = sales ~ n + p + r + s + d + db + r + a) Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 5.265e+06 4.612e+06 1.142 0.2637 n 2.223e+01 9.390e+00 2.368 0.0253 * p 3.357e+01 6.251e+01 0.537 0.5956 rd 6.147e+01 8.359e+01 0.735 0.4685 s 5.445e+02 3.984e+02 1.366 0.1831 d 2.591e+01 1.928e+02 0.134 0.8941 db -1.120e+02 1.068e+02 -1.048 0.3039 r 4.639e+02 2.248e+02 2.064 0.0488 * a 4.808e+00 7.228e+01 0.067 0.9475 Residual standard error: 8679000 on 27 degrees of freedom Multiple R-squared: 0.5181, Adjusted R-squared: 0.3754 F-statistic: 3.629 on 8 and 27 DF, p-value: 0.005482
