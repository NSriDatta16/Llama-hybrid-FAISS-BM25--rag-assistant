[site]: crossvalidated
[post_id]: 609206
[parent_id]: 569062
[tags]: 
If you have no true positives or false positives, it means that your model only predicts negatives. Most machine learning models (such as logistic regressions and neural networks) work by making predictions about the probability of class membership. $^{\dagger}$ To get hard classifications, that predicted probability is compared to a threshold, typically $0.5$ as a software default, where cases are considered positive if their predictions are above the threshold and negative if their predictions are below the threshold. Overall, it seems like your model is making a predictions of low probabilities of being a positive case. This need not be bad behavior by the model! It might be that positive cases are always unlikely. It might be that you want to use a different threshold, perhaps something closer to (or equal to) the prior probability of the minority class (that is, the proportion of minority class members). That way, you get alerted when there is a particularly high probability of the minority class occurring, even if that probability is fairly small. This is related to the idea that I discuss here . This is particularly common when the classes are imbalanced, such as in your case where you have almost $1000$ negative cases for every positive case, since the prior probability of minority class membership is so low . However, it is not clear that you should be using a threshold of any kind, since the predicted probabilities are quite useful. Hopefully, some of the below links can convince you of this. Cross Validated : Why is accuracy not the best measure for assessing classification models? Cross Validated : Are unbalanced datasets problematic, and (how) does oversampling (purport to) help? Cross Validated : Academic reference on the drawbacks of accuracy, F1 score, sensitivity and/or specificity Cross Validated : Calculating the Brier or log score from the confusion matrix, or from accuracy, sensitivity, specificity, F1 score etc Cross Validated : Upweight minority class vs. downsample+upweight majority class? Cross Validated Meta : Profusion of threads on imbalanced data - can we merge/deem canonical any? Frank Harrell's Blog : Classification vs. Prediction Frank Harrell's Blog : Damage Caused by Classification Accuracy and Other Discontinuous Improper Accuracy Scoring Rules $^{\dagger}$ Neural networks tend to be overconfident in their predictions , so if a prediction of $p$ does not correspond to an event probability of $p$ , then it is debatable if the predictions have a probabilistic interpretation. However, the predictions are on a continuum that can be thresholded to make hard classifications (if needed). Also, because the predicted probabilities are useful, the overconfident predictions by neural networks can be seen as problematic.
