[site]: crossvalidated
[post_id]: 510474
[parent_id]: 
[tags]: 
Is it possible to appliy gradient descent if the gradient is not linear in x

I am trying to implement a SVM classifier. The problem is equivalen to finding the $\{\alpha\}$ that maximizes the following loss function: $$ \operatorname{max}\limits_{\alpha}L(\alpha)=\sum_{i=1}^{N} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y^{(i)} y^{(j)} \mathbf{x}^{(i)^{\top}} \mathbf{x}^{(j)} $$ Whose gradient is: $$\nabla L(\alpha)=(1)^{T}-\left[\begin{array}{ccc} y_{1} y_{1} x_{1}\cdot x_{1} & \cdots & y_{1} y_{N} x_{1}\cdot x_{N} \\ \vdots & \ddots & \vdots \\ y_{N} y_{1} x_{N}\cdot x_{1} & \cdots & y_{N} y_{N} x_{N}\cdot x_{N} \end{array}\right] \alpha$$ The problem is that the gradient depend on the dot product of x. How can I implement a gradient ascent given that gradient?
