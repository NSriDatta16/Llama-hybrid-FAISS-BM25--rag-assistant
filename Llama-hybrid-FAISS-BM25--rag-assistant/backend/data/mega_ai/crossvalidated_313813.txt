[site]: crossvalidated
[post_id]: 313813
[parent_id]: 
[tags]: 
How does Bishop reformulate SVM into quadratic programming?

In Pattern Recognition and Machine Learning , Bishop gives the following formulation of SVM (section 7.1): $$ \mathrm{argmax}_{w, b} \left\{ \frac{1}{||w||} \min \left[t_n(w^T \phi(x_n) + b) \right] \right\} $$ He then notes that since the objective function $$ g(w, b) = \frac{1}{||w||} \min \left[t_n(w^T \phi(x_n) + b) \right] $$ is positively homogeneous of degree 0, we have a degree of freedom in our solutions. So if $(w^*, b^*)$ is a maximum then so is $(\kappa w^*, \kappa b^*)$. Can someone give a step by step explanation how the solutions of this problem are related to those of the following quadratic programming problem? \begin{align} \mathrm{argmin}_{w,b} &\frac{1}{2}||w||^2 \\ \text{ such that } &t_n (w^T \phi(x_n) + b) \ge 1 \text{ for } n \in [1..N] \end{align} I am not convinced by Bishop's argument to take $t_{n^*(w,b)} (w^T \phi(x_{n^*(w,b)}) + b) = 1$ where $n^*(w, b)$ is the minimizer of $t_n (w^T \phi(x_n) + b)$ for a given $w$ and $b$.
