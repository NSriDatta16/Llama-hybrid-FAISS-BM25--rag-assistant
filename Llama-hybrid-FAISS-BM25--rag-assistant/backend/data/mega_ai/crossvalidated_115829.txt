[site]: crossvalidated
[post_id]: 115829
[parent_id]: 
[tags]: 
Entropy of a block of characters

I have a question about the following statement about entropy: If a source provides us with a sequence chosen from 4 symbols (say A, C, G, T), then the maximum average information per symbol is 2 bits. If the source provides blocks of 3 of these symbols, then the maximum average information is 6 bits per block. In the first case, each symbol has probability 1/4 therefore the entropy is 2 bits on average -(4*1/4*log(1/4)). The second case where we are sending blocks of 3 symbols is unclear. What is the meaning of sending block of 3 symbols? Are the symbols always different or could be the same? How do we calculate the entropy of group of symbols from a set?
