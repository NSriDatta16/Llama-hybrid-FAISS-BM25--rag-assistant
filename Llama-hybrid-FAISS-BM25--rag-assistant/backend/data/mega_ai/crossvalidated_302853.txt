[site]: crossvalidated
[post_id]: 302853
[parent_id]: 
[tags]: 
Why is the normalization necesary in Bayesian inference?

In this post it reads that: normalization can be intractable when applying Bayesâ€™ Theorem And in this answer it says that: it does not depend on the parameters since these have been integrated out. ... is nothing but a normalising constant And in this scenario the normalising constant is dropped. But it seems very necessary to work out the denominator Pr(data) of the right hand side of the Bayes' Theorem in Variational Bayes: $\Pr(\textrm{params} \mid \textrm{data}) = \frac{\Pr(\textrm{data} \mid \textrm{params}) \Pr(\textrm{params})}{\Pr(\textrm{data})}$ To be specific, in this tutorial it provides two answers but I don't get the question. That's I don't understand the problem setup section. How can I relate the left hand side P(Z|X) to what is being approximated P(X)? To be more straightforward, in what problems should I consider the normalization and then need to calculate the lower bound of the Pr(data) and when it is not necessary? And why? Could anyone please exemplify it simply.
