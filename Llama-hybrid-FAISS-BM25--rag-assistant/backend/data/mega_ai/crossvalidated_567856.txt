[site]: crossvalidated
[post_id]: 567856
[parent_id]: 
[tags]: 
What is a word embedding approach that would work for these pre-labeled documents?

My Situation: I should start off with my end goal: I want to get a distance metric between each document and all of the other documents To get there, I first need to encode these topic labels so that they can be appropriately placed in Euclidean space in relation to each other. I don't have the original text of these documents, or the "what this article is actually about" column in the example below, only labels of the topics they discuss. The labels are often repeated so there are only about 200-500 unique topic labels for 1-2 million documents. The labels follow the format: document id topic labels what this article is actually about 1 ['Tesla---Leadership', 'Elon Musk---Twitter Wars', 'SpaceX---Leadership', 'Amazon---Leadership', 'SpaceX---Launches', 'Jeff Bezos---Blue Origin'] hypothetical twitter war between Elon Musk and Jeff Bezos about their space exploration companies' launches 2 ['Amazon---Leadership', 'Jeff Bezos---Investments', 'NFL---Expansion'] hypothetical article about Jeff Bezos' personal interest in purchasing an NFL team 3 ['Tesla---Market share', 'Auto Industry---Electric vehicles', 'Ford---New models'] hypothetical article about Ford's new electric vehicle models and Tesla's share of that market segment ... ... ... Hopefully its clear from this example that each topic label has an " entity---subject " format. While the fact that a particular entity is tied to a particular subject is meaningful, the order of all the topic labels in that column is irrelevant . I need whatever encoding approach I take to not be affected if you were to shuffle the order of the topic labels in each list in that column. Tesla---Leadership should be close to Amazon---Leadership , as well as Tesla---Market share ...whereas Amazon---Leadership should be somewhat close to Jeff Bezos---Investments , but probably shouldn't be particularly close to Tesla---Market share So as far as my end goal, ideally document 1 would be closer to document 2 than document 3. In some edge cases, appropriate placement of documents can be really difficult. What I've tried: Treating each "entity--subject" topic label as one word before going into BoW or TF-IDF Pro: Keeps the relationship between that entity and that subject Con: Can't tell that Tesla---Leadership , SpaceX---Leadership , and Amazon---Leadership are all discussing the same subject, just for different companies and only places them close if they show up in enough documents together Splitting on '---' and treating each half of the topic labels as its own word (ex. SpaceX---Launches becomes SpaceX and Launches ) before going into BoW or TF-IDF Pro: Recognizes that document 1 has a lot to do with Leadership and SpaceX since those will now be in the list more than once Con: Can't tell which company's Launches the article is discussing because it doesn't keep the relationship between that entity and that subject My Question: What are some encoding methods that don't require the order of the labels to be meaningful yet can still recognize the relationship between both sides of an entity---subject label as well as the relationship between two topic labels with only one of those sides in common?
