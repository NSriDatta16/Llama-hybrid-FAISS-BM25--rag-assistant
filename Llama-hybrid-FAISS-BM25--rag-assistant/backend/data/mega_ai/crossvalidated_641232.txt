[site]: crossvalidated
[post_id]: 641232
[parent_id]: 
[tags]: 
Modern modeling approaches for tabular data with both text and numeric fields?

I am doing binary classification on tabular data. Each record in a table has one text field of variable length (typically An old-school way would be to represent text as a bag of words (eg tf-idf) and feed it into a model like logistic regression or gradient boosting alongside numeric fields. Now, today we have text embeddings. If I didn't have numeric features I would employ some BERT-like model for text classification. So here's the choice. I can still feed embedding vectors (perhaps "compressed" via PCA) into gradient boosting like a set of separate numeric features -- it works, and this approach is mentioned in catboost examples . I can cast all numeric fields as a text and feed it into some transformer model alongside proper text field (in T5's fashion ). I am not sure about this approach, since I don't have much data (~180k records -- it might be enough for fine-tuning, but feeding numeric fields seems to imply training from scratch, so that the model learns the meaning of this particular numeric layout). I can use BERT-like model and gradient boosting separately -- for text and numeric fields respectively and then combine them in some way. Here the issue is that I am not sure which features of the whole record are responsible for its label. It might be that a record belongs to a positive class, but not due to its text, in which case I don't know how to label this text for BERT training. Same for numeric features. It seems more reasonable to either use output/ intermediate representations from BERT in a downstream boosting model or train them simultaneously in such a way that the error backpropagates properly. Some other approach I don't know of. My primary concern is classification performance (inference/training time is not important). Thank you!
