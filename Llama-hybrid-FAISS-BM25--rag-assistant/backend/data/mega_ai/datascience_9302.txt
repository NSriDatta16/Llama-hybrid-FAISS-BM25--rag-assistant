[site]: datascience
[post_id]: 9302
[parent_id]: 
[tags]: 
The cross-entropy error function in neural networks

In the MNIST For ML Beginners they define cross-entropy as $$H_{y'} (y) := - \sum_{i} y_{i}' \log (y_i)$$ $y_i$ is the predicted probability value for class $i$ and $y_i'$ is the true probability for that class. Question 1 Isn't it a problem that $y_i$ (in $\log(y_i)$) could be 0? This would mean that we have a really bad classifier, of course. But think of an error in our dataset, e.g. an "obvious" 1 labeled as 3 . Would it simply crash? Does the model we chose (softmax activation at the end) basically never give the probability 0 for the correct class? Question 2 I've learned that cross-entropy is defined as $$H_{y'}(y) := - \sum_{i} ({y_i' \log(y_i) + (1-y_i') \log (1-y_i)})$$ What is correct? Do you have any textbook references for either version? How do those functions differ in their properties (as error functions for neural networks)?
