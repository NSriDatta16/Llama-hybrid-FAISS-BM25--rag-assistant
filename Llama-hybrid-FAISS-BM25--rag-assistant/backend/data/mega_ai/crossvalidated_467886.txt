[site]: crossvalidated
[post_id]: 467886
[parent_id]: 467862
[tags]: 
There's nothing "bad" about having 100% accuracy on training sample. In fact, it is common practice in deep learning to start with building a model that is able overfitt a small subset of training set before proceeding further. We are talking about overfitting when there's a discrepancy between training performance of the model, and the performance when the model is applied. This is commonly measured using cross validation, where we validate the model on different data then was used for training. There's no rule how big the difference needs to be to talk about overfitting, since this highly depends on the nature of the problem you are trying to solve. In practice, what you would do, is you'd aim at the test set performance that acceptable for you, while the discrepancy between train and test performance would suggest that there could be some possible areas of improvement. What is also worth pointing out, as @gunes mentioned in the comment, that accuracy is not the best metric and there are many issues with it .
