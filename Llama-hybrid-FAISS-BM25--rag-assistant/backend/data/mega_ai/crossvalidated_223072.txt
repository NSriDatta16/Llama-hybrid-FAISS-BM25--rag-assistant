[site]: crossvalidated
[post_id]: 223072
[parent_id]: 223068
[tags]: 
Your data is finite, correct? So, let's not assume infinity. Rather, given different information, assume that some categories or levels will be new relative to the original or previous data. That said, if we can label the data challenge you've described as "massively categorical," then some solutions do suggest themselves. You don't mention it but one of the biggest challenges to this type of analysis has been that with so many possible categorical values or levels, software doesn't exist (on the planet) able to fit all of that information into memory at one time. This is the historic problem with traditional, frequentist, statistical models and solutions -- inversion of a massive cross-products matrix. The first approach to modeling massively categorical data is hierarchical and Bayesian and introduced in a Marketing Science paper about 15 years ago by Steenburgh and Ainslie. When this paper was written, there was little understanding about possible workarounds to the memory limitations mentioned above. Titled Massively Categorical Variables: Revealing the Information in Zip Codes , an ungated copy can be found here ... http://www.people.hbs.edu/tsteenburgh/articles/Steenburgh_Ainslie_and_Engebretson_(winter_2003).pdf Their approach isn't exploratory, i.e., it presumes a pre-existing model, is quite computationally expensive and, with truly massive data (e.g., the 16 million pixels you described), is likely never to converge even on a parallel platform. A second approach is also Bayesian. David Dunson, at Duke, is probably the leading exponent of tensor approaches to modeling massive contingency tables, among other things. His papers with their attendant descriptions are much better introductions than anything I have to offer or can say. Here's one such paper as well as a link to his Google Scholar page. He's very active and prolific. https://scholar.google.com/citations?user=PxPxCv8AAAAJ&hl=en&oi=ao https://arxiv.org/pdf/1306.1598.pdf A third workaround is rooted in iterative machine-learning algorithms, is exploratory and neither presumes nor requires a pre-existing model. Variously titled, e.g., "divide and conquer" (D&C) or "bags of little jacknifes" (BLJs) algorithms, these methods, to a large degree, can be viewed as extensions of Breiman's random forest approach to CART. Breiman did his work in the late 90s on a single CPU when "big data" meant a few gigs, several thousand features and several thousand bootstrapped trees evaluated as an ensemble. Today's extensions are several: first, "massive" typically means terabytes of data containing millions, tens of millions or even hundreds of millions of features evaluated on a massively parallel, multi-core platform. Next, where Breiman wrote only about trees, any multivariate engine can and is being substituted. Finally, with this kind of computational power, millions of mini-models can be run and evaluated in a few hours. Given that, you could plug in one of the GLMs and obtain an ensemble answer to your massively categorical features. Here is an introductory, ungated copy of one review of these algorithms ... http://www.math.chalmers.se/Stat/Grundutb/GU/MSA220/S16/ReviewBigDataR.pdf I'm trying very hard to be agnostic here and avoid the "wars of religion" between Frequentists and Bayesians. Regardless, until these ML workarounds were developed, frequentists were pretty much screwed when it came to modeling truly massive data. With these workarounds, any arbitrage Bayesians enjoyed in handling massively categorical information has been eliminated.
