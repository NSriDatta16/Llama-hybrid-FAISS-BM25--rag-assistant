[site]: crossvalidated
[post_id]: 578418
[parent_id]: 
[tags]: 
Why do we write the reward function as R(s,a)?

I have been reading up on reinforcement learning, and in particular have been looking for a standard definition of the Q function. The common way it is written is: $Q(s,a)= R(s,a) + \sum_i^{|S|} p(s_i | s,a)*V^*(S_i)$ I understand intuitively what the Q function is supposed to represent, i.e. it is the state-action function that looks at the value of a policy that uses action a for the current step, and then reverts back to the optimal policy. I also understand how this equation can be used to update the Q function each time you get a reward, by looking at the temporal difference. What I don't understand is more the semantics of the above equation. Usually, the reward is considered to be based on the state transition, which is a probabilistic function of the current state and the current action. So when we write R(s,a), are we taking the expected reward? and if so, why are we not writing this explicitly? Clearly we care about the probabilistic nature of how the next state depends on the current state and current action, since we are explicitly taking the expectation of the optimal value function based on the transition probability. So why are we not putting the reward function inside the summation in the same way? Any help understanding why we write the equation like this would be greatly appreciated. EDIT: as I look at some more online resources such as this : I am realizing that it is often written inside the summation. But still, I hope there could be a productive discussion of why so many places write it the other way instead. in some cases, I have seen the definition of the Q function written simply as: $Q^*(s,a) = r(s,a) + \gamma * min_a Q^*(s',a) $ This seems to be more aligned with a non-probabilistic setting, where we are assuming that the next state is a deterministic function of the previous state and action. (or maybe we're just ignoring that probabilistic part for some reason?) Side question on this -- are there really any interesting MDPs where the next state is deterministic in the action? Other than a deterministic setting, is there any reason we would write $r(s,a)$ instead of $r(s,a,s')$ ? Are the rewards sometimes truly a function of the action and previous state alone, without considering the next state? And again, does this really capture any problems of interest?
