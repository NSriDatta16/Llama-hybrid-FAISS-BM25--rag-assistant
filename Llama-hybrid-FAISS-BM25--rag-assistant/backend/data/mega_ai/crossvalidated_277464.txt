[site]: crossvalidated
[post_id]: 277464
[parent_id]: 
[tags]: 
Maximum likelihood estimation (MLE) for a Bayesian network with binary variables

I have below Bayesian network of 4 variables A, B, C, and D. All variables are binary except D, which is real-valued. $P(A|B,C)$ is defined by the conditional probability table (CPT) on the right. Each of $P(B|D)$ and $P(C|D)$ is defined by a parametric binary classification model. I have the value of all variables for N samples, and based on this data, I want to learn the parameters of each of the classification models by MLE (maximizing the joint log-likelihood with respect to each parameter). Here is the joint log-likelihood: $\log P(A,B,C,D;\theta) = \sum_{i=1}^{N} {\log P(A_i, B_i, C_i, D_i;\theta)}$ $ = \sum_{i=1}^{N} {\log [P(A_i|B_i, C_i) P(B_i|D_i;\theta) P(C_i|D_i;\theta)]}$ using the Bayesian network decomposition rules. However, I have a confusion at this point. According to the CPT, for some of the samples, $P(A_i|B_i, C_i)=0$, which results in an undefined logarithm ($\log0$). This looks like a very basic problem to me, but I cannot figure out the solution. Do I need to use a very small number $\epsilon$ in the CPT instead of 0? Is it what people do in such a case?
