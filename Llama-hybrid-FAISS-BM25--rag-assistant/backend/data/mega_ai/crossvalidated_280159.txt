[site]: crossvalidated
[post_id]: 280159
[parent_id]: 108631
[tags]: 
Local Minima are simply Attractors for the Training Dynamic, itâ€™s the Associated Network Performance that may be good or bad. Moreover I think that typically the same performance level can be associated to multiple local minima as a result of the inference function invariance to some neurons permutation: e.g. consider a MLP with Fully Connected Topology, each Neuron Input is a Weighted Sum of all the Previous Layer Neurons Outputs so if you permute the Previous Layer Neurons and the corresponding Input Weights at each Next Layer Neuron you should land in a different point in the Parameters Space with the same Final Performance Edit I found that also Bishop in his famous "Pattern Recognition and Machine Learning" (2006) in Chap 5.1.1 "Weight Space Symmetries" refers to this phenomenon
