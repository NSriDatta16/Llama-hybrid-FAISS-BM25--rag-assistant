[site]: crossvalidated
[post_id]: 513656
[parent_id]: 
[tags]: 
Training, saving and distributing the model – what about data the transformations?

I have a question about process, and I’m a relative noob when it comes to Machine Learning. Let’s say I have dataset with X features to train a model. During development of my model I drop a few features columns, delete rows with NaN, perhaps standardize values, etc, leaving me with fewer than the original X features. I save the model with joblib (I'm using sklearn). When I give this model to another user with their own dataset with X features won’t they have to do the same data transformations (dropping data/columns/standardize etc) on the data to be able to use the model they loaded? If so, how can this be easily accomplished for the new user? It doesn’t seem possible for me to bundle code with joblib, so do I write a set of separate functions for the data transformation and send it along with the saved model? How, a standalone .py file, or a notebook (I’ve been doing most of my work in Colab this far)? What’s best practice/pragmatic here? Sorry if this is a very basic question, I see a lot about developing models, not so much about how to share models. In this case I just want to evaluate a bunch of different models (each with possibly their own sets of data transformations, though everyone started out with the same data set). I have access to the original data with the X features that everyone else started out with.
