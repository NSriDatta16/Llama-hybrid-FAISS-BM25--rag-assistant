[site]: crossvalidated
[post_id]: 577864
[parent_id]: 
[tags]: 
Strange increase in loss during training

As a project I've built a graphical neural network sandbox software, which has the feature of drawing a loss graph to show how well the network is learning during training. I'm receiving these really odd results shown below though, and I can't figure it out. The loss decreases over time telling me it is learning, however eventually it tends to reverse and increases in loss as I feed it more samples. These are all samples used in training by the way, so it's not just overfitted. The samples are also well shuffled. Is there a reason a pattern like this can occur? Or could it be caused by something wrong with my software, such as my implementation of backpropagation?
