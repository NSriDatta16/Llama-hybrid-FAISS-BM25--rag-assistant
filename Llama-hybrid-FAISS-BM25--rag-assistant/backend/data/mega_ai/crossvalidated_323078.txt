[site]: crossvalidated
[post_id]: 323078
[parent_id]: 323062
[tags]: 
Bayes' rule states $$ P(A|B) = \frac{P(B|A)P(A)}{P(B)}, $$ where $P(A|B)$ and $P(B|A)$ are conditional probabilities. For instance, $P(A|B)$ is the probability of $A$ occurring given that $B$ is observed. Bayesian inferencing is a method of inference (there are others) that applies Bayes' rule in order to update our beliefs of a certain hypothesis given observed (experimental) parameters. Technically, it involves calculating the conditional posterior distribution; that is, the conditional distribution of the random variable of interest post-observation. Often the most contentious part of the process is deciding what your priors are. In the formula above that deals with probabilities instead of distributions, $P(A)$ would be our "prior" probability; that is, the probability of $A$ occurring prior to any experimental observation. You mention wanting to inference the effect of one variable on two outcome variables. Do you have reason to believe that the two outcome variables are independent of one another?
