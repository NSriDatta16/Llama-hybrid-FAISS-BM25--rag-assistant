[site]: crossvalidated
[post_id]: 146759
[parent_id]: 145875
[tags]: 
Contrary to what you said about geometric mean shenanigans, there are actually proper scoring rules for the geometric mean. The geometric mean of a random variable $X$ is equal to $e^{E(\log X)}$. Therefore minimizing the geometric mean of a random score $S$ corresponds to minimizing the arithmetic mean of a random score $\log S$. So if $f(\hat p)$ is a standard proper scoring rule (where $f(\hat p)$ is the score you get if you predict a probability $\hat p$ and the event happens), then $g(\hat p) = \log f(\hat p)$ is a proper scoring rule for the geometric mean. Similarly, the harmonic mean of $X$ is $E(X^{-1})^{-1}$, so $g(\hat p) = -f(\hat p)^{-1}$ is a harmonic-proper scoring rule. (The negative sign is in there so the coordinate transformation is monotone increasing.) This works for any central tendency that is the arithmetic mean in a monotonically transformed space. The problem is that the median doesn't work like this. More generally, any central tendency with a nonzero breakdown point will not work, because it will be insensitive to changes in probability when $p$ is small. For instance the interquartile range won't work, because if $p Off the top of my head I can't think of any central tendencies with 0 breakdown point that can't be rewritten as a monotone transformation of the arithmetic mean, but that's probably because I don't know enough variational calculus (certainly not enough to prove I'm right). If I'm correct, however, then it would be "essentially" true that in order for the theory of proper scoring rules to work as intended, the statistical functional must be the mean. One other remark: you suggest using the RMSE as a scoring rule, but that you shouldn't do it because it coincides with the absolute error when there is one data point. This seems like it might reflect some confusion. You always evaluate a scoring rule on each individual prediction. Then if you want to summarize the scores, you can take the scores' central tendency afterwards. So predicting to optimize the RMSE is always identical to optimizing the absolute error. On the other hand, you could do something like take the square root of the mean Brier score as your summary if you wanted to have a score summary that was in "units of probability." But I think it would be more productive to simply familiarize yourself with benchmarks for the Brier score scale, since that's what you'll typically see: 0 is a perfect predictor; 0.25 means no predictive ability ($\hat p = 0.5$); 1 is a perfect anti-predictor ($\hat p = 1, p = 0$ or $\hat p = 0, p = 1$). You can also construct other benchmarks by using very simple models--for instance, if you ignore all info about the events and simply predict the base rate $p$, then your Brier score is $p(1-p)$. Or if you're predicting time series you can see how well a weighted average of the past few events does, etc.
