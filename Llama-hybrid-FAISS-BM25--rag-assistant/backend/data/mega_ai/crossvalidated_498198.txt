[site]: crossvalidated
[post_id]: 498198
[parent_id]: 444049
[tags]: 
Deducing from your question, you are considering a Bayesian linear regression model $$f(\boldsymbol{x}) = \boldsymbol{w}^{T}\boldsymbol{x}$$ with a standard Gaussian prior $\boldsymbol{w} \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{1}_{\{d \times d\}})$ . As you correctly remark, this is equivalent to the function space view where we specify a prior directly on $f$ as $$f \sim \mathcal{GP}(0, K)$$ where we have the linear kernel $K(\boldsymbol{x}, \boldsymbol{x}')=\boldsymbol{x}^{T}\boldsymbol{x}'$ . Let's think about how we produce a random draw of the function evaluated on some fixed set of points that we collect into a matrix $\boldsymbol{X}_{*} \in \mathbb{R}^{m \times d}$ . $\textit{Bayesian Linear Regression}$ : Here it is pretty straight-forward, just sample one realization $$\boldsymbol{w} \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{1}_{\{d \times d\}})$$ and we easily find the corresponding function values $$\boldsymbol{f}_{*} = \boldsymbol{X}_{*}\boldsymbol{w} \in \mathbb{R}^{m}$$ Here it is as expected, all points lie in the same hyperplane determined by $\boldsymbol{w}$ . $\textit{Function Space View}$ : This is slightly more annoying. As you correctly say, to obtain a random draw of prior function values $f_{*}$ at $\boldsymbol{X}_{*}$ , we need to sample $$f^{*} \sim \mathcal{N}(\boldsymbol{0}, K(\boldsymbol{X}_{*}, \boldsymbol{X}_{*})) \stackrel{(d)}{=}\mathcal{N}(\boldsymbol{0}, \boldsymbol{X}_{*}\boldsymbol{X}_{*}^{T})$$ The question is now: $\textit{Why should those samples lie in a hyperplane?}$ As you remarked, the covariance $\boldsymbol{C}=\boldsymbol{X}_{*}\boldsymbol{X}_{*}^{T}$ is degenerate as it is of rank $k=min(d, m)$ . Let us assume that $m>d$ , otherwise there is always a plane of dimension $d-1$ that contains the $m$ points. So we have $k=d$ . As a consequence of the low rank structure, we do not get the "full" randomness but only one that corresponds to the push-forward of a lower dimensional random variable. Let's make this low-dimensional randomness more explicit before giving the final answer. Due to positive semi-definiteness, we diagonalize as $$\boldsymbol{C} = \boldsymbol{V}\boldsymbol{\Lambda}\boldsymbol{V}^{T}$$ where $\boldsymbol{V} \in \mathbb{R}^{m \times m}$ and $\boldsymbol{\Lambda} \in \mathbb{R}^{m \times m}$ with only the first $d$ diagonal entries non-zero. Take a random variable ${\boldsymbol{\tilde{z}}} \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{1}_{\{d \times d\}})$ and pad it with $m-d$ zeros to get $\boldsymbol{z} \in \mathbb{R}^{m}$ . We can then rewrite $$f^{*}\stackrel{(d)}{=} \boldsymbol{V}\boldsymbol{\Lambda}^{\frac{1}{2}}\boldsymbol{z}$$ due to the formula $\text{cov}(\boldsymbol{A}\boldsymbol{z}) = \boldsymbol{A}\boldsymbol{A}^{T}$ and invariance of the Gaussian distribution under linear transformations. We thus see that the randomness only stems from a $d$ -dimensional random variable $\boldsymbol{z}$ . Now to the final answer, we also have due to the same facts as above that $$f_{*} \stackrel{d}{=}\boldsymbol{X}_{*}\boldsymbol{\tilde{z}}$$ We hence see that $f_{*}$ also lives in a $d$ -dimensional plane.
