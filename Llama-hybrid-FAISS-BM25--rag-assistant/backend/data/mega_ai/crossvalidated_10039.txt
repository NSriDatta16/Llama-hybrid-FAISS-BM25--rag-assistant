[site]: crossvalidated
[post_id]: 10039
[parent_id]: 9533
[tags]: 
Would the nearest-neighbor distance distribution help ? For each of the 150 observations, you have distances $d_1\ d_2$ ... to its nearest, 2nd nearest ... neighbors, and an averaged distance distribution, call it DD. A query point gives you the distribution $d_1 .. d_{150}$: compare that to DD. The metric between points is crucial, but I have no recipe. Try the fractional or near-Hamming metric $\sum |a_j - b_j|^q$ (with no outer $\frac{1}{q}$). For small $q$, say .1, this up-weights close matches in a few features, which make sense; otherwise a sum of 500 terms is just normally distributed with no contrast at all / distance whiteout. (Yes near-Hamming is not a norm, but it is a metric, satisfies the triangle inequality.) Take a look at Omercevic et al., High-dimensional feature matching: employing the concept of meaningful nearest neighbors 2007 8p, who do this: find ~ 100 nearest neighbors to a query point fit $\lambda$ to exponential background noise weight the 100 neighbors: don't understand this bit, looks ad hoc pick ~ 10 outliers as "signal". (However they're matching 128-d SIFT vectors, whose distance distribution and noise model may be very different from yours.)
