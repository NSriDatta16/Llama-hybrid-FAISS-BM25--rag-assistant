[site]: crossvalidated
[post_id]: 491484
[parent_id]: 491338
[tags]: 
Any universal approximators can do it. You need a term like $A(\beta_A+\beta_{A\times C}\times C)$ to appear, so the interaction between $A$ and $C$ suffices. $$A\times C = \frac{(A+C)^2-A^2-C^2}{2}$$ If you have an universal approximator, it can (locally) approximate the quadratic form somewhere in its formulation, giving you the interaction without explicitly multiplying $A$ and $C$ . Then, the only thing that matters is selecting a universal approximator. Neural Networks are in general universal approximator, and so are kernel machines with infinite dimensional kernel spaces (like the radial basis function, for example) too. On neural networks, if you have as inputs $A,B,C$ , then with two hidden layers and the square as the activation function you already achieves the possibility of interactions. Consider the column vector $x = [A, B, C]$ : $$\hat y = W_2\sigma (W_1 x+b_1)+b_ 2$$ $W_1 x$ passes weighted sums of the initial features, $h_1 = \sigma(W_1 x+b_1)$ square them and finally $W_2h_1+b_ 2$ makes weighted sums of the squared items.
