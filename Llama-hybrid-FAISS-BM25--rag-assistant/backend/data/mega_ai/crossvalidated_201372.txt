[site]: crossvalidated
[post_id]: 201372
[parent_id]: 
[tags]: 
The Effect of Stopword Filtering prior to Word Embedding Training

Recently I have played with the pretrained GLOVE word embedding model for Twitter http://nlp.stanford.edu/projects/glove/ I notice that common stopwords are existing in the model. That is, there is no stopword filtering before the training of the model. I wonder if stopword filtering would improve performance in terms of: higher correlation (or cos-sim) between semantically similar words less noisy sum for aggregation of set of words, because I've heard the main problem of aggregation of word embedding is poor weighing on a significant portion of noisy words in the set. Or does filtering stopwords give problems that I am not seeing?
