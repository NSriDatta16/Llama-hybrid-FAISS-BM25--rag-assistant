[site]: crossvalidated
[post_id]: 454987
[parent_id]: 
[tags]: 
Valid use of differentiable almost everywhere functions, like hinge loss, in gradient optimization/learners, like SciKit-Learn's SGDClassifier?

So, my abstract question is: is it valid (in the sense that stable convergence is roughly expected) to use functions that are differentiable almost everywhere in the practical application of optimization/learner techniques that take gradients of these functions. Or, more specifically: should I expect utilization of SciKit-Learn's Stochastic Gradient Descent Classifier (SGDClassifier) to produce reliable, valid results using its default loss function of hinge loss. Of course, this question could also apply to use of differentiable almost everywhere function within a neural network with backward-propagation. A survey of facts: (Stochastic) gradient descent theoretically requires that functions it optimizes are differentiable, convex, and Lipschitz The hinge loss function is not differentiable everywhere (not at y=1), given its definition in terms of as (()) where () = max(0,1− ) and ()= ⋅ So, theoretically , the hinge loss function cannot be used with (stochastic) gradient descent, but sub-gradient methods should work However, a gradient of the hinge loss function can be defined where the value at 1 can be forced to exist to be value to the right (or to the left) Additionally, it has been suggested that gradient descent methods are decently robust to non-convexity Lastly, it appears to me that the SGDClassifier within SciKit-Learn uses the hinge loss function by default without any notes about an underlying use of sub-gradient methods So, perhaps, I should take the performance of the default configured SGDClassifier as evidence that gradient descent methods (and more generally optimization methods utilizing gradients) are robust to functions that are differentiable almost everywhere?
