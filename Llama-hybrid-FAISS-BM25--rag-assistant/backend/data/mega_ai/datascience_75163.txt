[site]: datascience
[post_id]: 75163
[parent_id]: 75157
[tags]: 
One of the major issue is the amount of records per categories. Definitely using data augmentation is always helpful to get good accuracy but much better approach is to use transfer learning techniques with data augmentation. In your approach model is started overfitting because of that it's started having good training accuracy but bad validation accuracy. Let's have look into implementation of transfer learning. To understand more on transfer learning check this link Article Link import tensorflow.keras as keras model = keras.models.Sequential() model.add(keras.applications.ResNet50(include_top=False, weights='imagenet', input_shape=(128, 128, 3))) model.add(keras.layers.Flatten()) model.add(keras.layers.Dense(27, activation='softmax')) model.layers[0].trainable=False model.compile(keras.optimizers.Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy']) model.summary() model_obj = model.fit_generator(training_set, epochs=1, validation_data=test_set, verbose = 1) Don't forgot to pass data augmented generator as a input data into fit_generator. There is no effect if you are using data augmentation on validation/test. You can use cross validation techniques or cnn based ensemble techniques for more good results.
