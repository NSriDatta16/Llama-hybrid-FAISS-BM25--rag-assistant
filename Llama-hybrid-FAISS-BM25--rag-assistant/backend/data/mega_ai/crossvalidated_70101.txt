[site]: crossvalidated
[post_id]: 70101
[parent_id]: 
[tags]: 
Neural Networks: weight change momentum and weight decay

Momentum $\alpha$ is used to diminish the fluctuations in weight changes over consecutive iterations: $$\Delta\omega_i(t+1) = - \eta\frac{\partial E}{\partial w_i} + \alpha \Delta \omega_i(t),$$ where $E({\bf w})$ is the error function, ${\bf w}$ - the vector of weights, $\eta$ - learning rate. Weight decay $\lambda$ penalizes the weight changes: $$\Delta\omega_i(t+1) =- \eta\frac{\partial E}{\partial w_i} - \lambda\eta\omega_i$$ The question is if it makes sense to combine both tricks during the back-propagation and what effect it would have? $$\Delta\omega_i(t+1) = - \eta\frac{\partial E}{\partial w_i} + \alpha \Delta \omega_i(t) - \lambda\eta\omega_i$$
