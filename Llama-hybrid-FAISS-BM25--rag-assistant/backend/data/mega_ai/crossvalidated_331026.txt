[site]: crossvalidated
[post_id]: 331026
[parent_id]: 331024
[tags]: 
Let us start by showing that $H(\mathcal B|\mathcal A)\le H(\mathcal B)$. $\newcommand{\bs}[1]{\boldsymbol{#1}}$ First of all, let us note that it is not true that $H(\mathcal B|a_i)\le H(\mathcal B)$. While it would seem to be sensible to state that knowing what input is being injected into the channel cannot increase the uncertainty over the output, this doesn't take into account cases in which a very improbable input leads to a very mixed output. As an extreme case consider the output $\bs q=(0, 1)^T$, as produced from the input $\bs p= (0, 1)^T$ via the channel matrix $M=\begin{pmatrix}1/2 & 0\\ 1/2 & 1\end{pmatrix}$. In this case, $H(\mathcal B) = H(\bs q) = 0$ but $H(\mathcal B|a_1)=H(M^1) = 1$, where $M^i$ denotes the $i$-th column of $M$. However, taking note that $H(\mathcal B|\mathcal A)$ is defined as the average of $H(\mathcal B|a_i)$ over the possible inputs. This avereage takes into account the probability of each input actually occurring, thus removing the possibility of edge cases like the above mentioned one. Generally speaking, using same notation as above, one can write the vector of output probabilities $\bs q$ in terms of the input probabilities $\bs p$ as $$ \bs q = M \bs p = \sum_i M^i p_i.$$ This shows that the vector of output probabilities is really a (convex) mixture of the (normalized) probability vectors $M^i$, weighted by the (still probabilities) $p_i$. The convexity of the entropy now immediately implies that $$H(\mathcal B)=H(\bs q)=H\left(\sum_i M^i p_i\right)\ge \sum_i p_i H(M^i) \equiv H(\mathcal B|\mathcal A).$$ What this is saying is that taking into account that the output distribution is derived from another (known) distribution gives us (potentially) some amount of information. For the opposite direction, that is, showing $H(\mathcal A|\mathcal B)\le H(\mathcal A)$ we use similar reasoning. Note however that $M$ does not in general need to be invertible, so we cannot just write $\bs p = M^{-1}\bs q$ and apply identical reasoning as above. What we can do is define a new "inverse channel matrix" $Q$, defined via Bayesian inference as that matrix whose elements satisfy $$M_{ij} p_j = Q_{ji} q_i.$$ Note how the above leads to a matrix satisfying $\sum_j Q_{ji} = 1$, consistently with the interpretation of $Q$ itself being a channel matrix. It is now easy to show that $\bs p = Q \bs q$, so that the same convexity argument as above can be used to reach the conclusion.
