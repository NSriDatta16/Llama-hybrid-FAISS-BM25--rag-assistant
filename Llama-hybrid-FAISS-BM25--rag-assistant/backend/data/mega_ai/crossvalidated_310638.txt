[site]: crossvalidated
[post_id]: 310638
[parent_id]: 310629
[tags]: 
it doesn't seem intuitively correct that the more samples I have the less likely my model Maximum likelihood is used to find parameters of the model, so it isn't really a problem. seems to make more sense (to me) to average the likelihoods per sample. You're right if you would want to make likelihood more meaningful. But it doesn't really matter for Maximum Likelihood since it is used for finding $$\hat{\theta} = \underset{\theta}{argmax} \underset{i It also means that if just one sample that doesn't fit will crash my likelihood What do you mean by 'crashing likelihood'? If you mean that your likelihood becomes zero, this is impossible for GMM, since gaussian mixture's PDF is nonzero everywhere.
