[site]: crossvalidated
[post_id]: 136747
[parent_id]: 
[tags]: 
SVM for an unbalanced textual dataset?

I have a text classification task, currently I can classify the data with very poor precision. This are the scores: Accuracy: 0.461057692308 score: 0.461057692308 precision: 0.212574195636 recall: 0.461057692308 And this is how the dataset looks, which is about 2599 documents with 5 categories from 1 to 5: 5 1190 4 839 3 239 2 204 1 127 How can I improve the above-mentioned results? Is there something like a "kernel" that separate this biased data? What about using more negative bi-grams for balance the dataset?. Update: This is the confusion matrix: [[ 0 0 0 47 0] [ 0 0 0 36 0] [ 0 0 0 64 0] [ 0 0 0 223 0] [ 0 0 0 280 0]] I also was reading this paper : A popular approach towards solving these problems is to bias the classifier so that it pays more attention to the positive instances. This can be done, for instance, by increasing the penalty associated with misclassifying the positive class relative to the negative class. Any idea of how to do with scikit-learn?.
