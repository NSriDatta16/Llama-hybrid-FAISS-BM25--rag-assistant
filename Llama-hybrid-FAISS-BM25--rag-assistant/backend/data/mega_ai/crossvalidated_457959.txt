[site]: crossvalidated
[post_id]: 457959
[parent_id]: 440776
[tags]: 
Metrics like TPR, FPR, and AUC-ROC are defined for the binary classification problem . When you have more than two classes it is necessary to first transform the problem into a binary classification problem. The usual strategy for doing this is called one vs. all (although the nomenclature is not very standardized ) and is exactly what you intuited it should be: treat one class as the positive class and all others as the negative class. This gets you one ROC curve per class. There are actually two common strategies for combining such metrics into an average across all classes: micro averages and macro averages . The macro average simply computes the metric for each class, then averages them together. The micro average is closer to what you propose, and adds together the numbers of true positives and false positives before taking ratios. Arguably the micro approach is less misleading, but that's grossly simplifying. Read the linked answer for a more detailed discussion. The strategy you outline for drawing a combined ROC curve is basically to compute the micro averaged TPR and FPR for a series of thresholds, then draw an ROC curve from that. That certainly gets you a curve but I'm not sure I would still call it an ROC curve. As the curve goes up, the model is probably doing better in some sense, although I would worry that it would obscure the trade-offs being made under the hood. Personally, I would find such a chart difficult to interpret. I would prefer to see aggregate metrics like accuracy (which is defined for the multiclass problem.) That is how results for ImageNet are usually reported, for example. Aggregate $F_1$ scores or AUC scores computed via micro averaging might also be interesting. Too much information obscures; we want a single metric that simply represents "goodness." Such single number metrics could be used for hyper-optimization, for example. It is usually more instructive to look at misses than abstract ROC curves. For example, when an algorithm misclassifies a "housecat", does it label it a "tiger" or a "bulldozer?" Does it fail in bad light, from unusual angles, when the image is inverted or rotated, or in black-and-white? These insights more directly lead to concrete actions that can be taken to improve the algorithm or augment the training data. You mention image detection and metrics like Intersection-over-Union which suggest you are interested in the object localization problem - finding where exactly in the image the object is located. The ImageNet challenge defines a metric for that, which you can find on their website.
