[site]: datascience
[post_id]: 11336
[parent_id]: 11334
[tags]: 
Put the training data into two numpy arrays: import numpy as np # data from columns A - D Xtrain = np.array([[1, 20, 30, 1], [2, 22, 12, 33], [3, 45, 65, 77], [12, 43, 55, 65], [11, 25, 30, 1], [22, 23, 19, 31], [31, 41, 11, 70], [1, 48, 23, 60]]) # data from column E ytrain = np.array([1, 2, 3, 4, 1, 2, 3, 4]) Then train a logistic regression model: from sklearn.linear_model import LogisticRegression lr = LogisticRegression().fit(Xtrain, ytrain) Make predictions (on the training data): yhat = lr.predict(Xtrain) => results in "1, 4, 3, 4, 1, 2, 3, 4".. so it's got 7 right and 1 wrong. Calculate accuracy: from sklearn.metrics import accuracy_score accuracy_score(ytrain, yhat) => results in 87.5% accuracy To make predictions for new data, just create another numpy array containing your test data and call lr.predict on it. You might also want to look into parameter tuning to improve your score. For example the LogisticRegression class has some parameters that control regularization - tuning them with methods found in sklearn.grid_search might improve your score.
