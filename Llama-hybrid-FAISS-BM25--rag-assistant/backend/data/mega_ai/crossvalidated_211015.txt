[site]: crossvalidated
[post_id]: 211015
[parent_id]: 
[tags]: 
Interpretation of the consistency property of a loss function

I am looking for an interpretation of the consistency property of a loss function used for classification (e.g., the SVM's hinge loss: $V(t)=\max(0,1-t)$). I copy from Wikipedia: Furthermore, it can be shown that for any convex loss function $V(yf_0(\vec{x}))$, where $f_0$ is the function that minimizes this loss, if $f_0(\vec{x}) \ne 0$ and $V$ is decreasing in a neighborhood of $0$, then $f^*(\vec{x}) = \operatorname{sgn}(f_0(\vec{x}))$ where $\operatorname{sgn}$ is the sign function. Note also that $f_0(\vec{x}) \ne 0$ in practice when the loss function is differentiable at the origin. This fact confers a consistency property upon all convex loss functions; specifically, all convex loss functions will lead to consistent results with the $0-1$ loss function given the presence of infinite data. Consequently, we can bound the difference of any of these convex loss function from expected risk. Could anyone please explain why the fact that a loss function enjoys the consistency property is important?
