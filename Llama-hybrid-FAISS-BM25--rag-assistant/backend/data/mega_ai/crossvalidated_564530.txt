[site]: crossvalidated
[post_id]: 564530
[parent_id]: 564528
[tags]: 
To be completely honest, it's because everything performs poorly in more than 20 dimensions. Bayesian optimization isn't special here. Trying to optimize any function in a lot of dimensions is hard, because the volume of a high-dimensional space goes up exponentially with the number of dimensions. Consider a line segment on $[0, k]$ ; that has length $k$ . A unit square? That has area $k^2$ . And so on. So the amount of space that you have to search when you're looking for a possible solution goes up very, very, fast. I recommend looking up the term "Curse of Dimensionality" for more. This will always be true, regardless of what algorithm you use -- unless you're willing to make some strong simplifying assumptions about the shape of that function. For example, gradient descent can do quite well in high dimensions -- as long as your function is convex and differentiable. If you have a function where the gradient is 0 somewhere besides the minimum , you're screwed. And if you have a function with multiple minima, you're screwed. Bayesian optimization is exactly the same. The papers you've linked point out that if your function has an interesting structure, you can exploit this by picking good priors. Namely, you need to assume sparsity (that only a few of those dimensions are important and the rest can be ignored), or differentiability, in which case you can use gradient-enhanced Gaussian processes . But if you don't have that structure, you're screwed. As for 20 dimensions, that's a rule of thumb. There's no "threshold" or anything, but it gets hard exponentially quickly.
