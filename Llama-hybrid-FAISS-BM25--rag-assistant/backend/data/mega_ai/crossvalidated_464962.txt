[site]: crossvalidated
[post_id]: 464962
[parent_id]: 
[tags]: 
Is dropout (deep learning) even consistent?

I'm learning about dropout from these sources: https://cs231n.github.io/neural-networks-2/#reg https://arxiv.org/abs/1207.0580 At test time, the trained weights are scaled by a factor of $p$ , to replace them with their mean. According to the original paper this is kind of like an approximation to an ensemble of the $2^n$ possible dropout models that "works well in practice". It's an approximation because the network isn't a linear function of its weights. Is there any justification for this approximation? It's not even clear to me that dropout is a consistent estimator.
