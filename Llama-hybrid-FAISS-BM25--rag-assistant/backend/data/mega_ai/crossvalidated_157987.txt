[site]: crossvalidated
[post_id]: 157987
[parent_id]: 
[tags]: 
Neural network Equation question

I am looking at an example for the activation function $a_1$. Why does the equation look like $\Theta_{10}x_0 + \Theta_{11}x_1 + \Theta_{12}x_2 + \Theta_{13}x_3$ instead of $\Theta_{10}x_0 + \Theta_{11}x_1 + \Theta_{21}x_2 + \Theta_{31}x_3$? My understanding is that all input nodes will contribute to the computation of activation value of $a_1$. And hence, shouldn't $\Theta_{xy}$ be the value from $x$ to $y$? Which in this case of $\Theta_{21}$ be from Node $2$ in first layer to Node $1$ in second layer. Am I misunderstanding something? This is the url
