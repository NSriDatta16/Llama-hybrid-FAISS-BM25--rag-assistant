[site]: crossvalidated
[post_id]: 384376
[parent_id]: 384367
[tags]: 
If some data set has literally zero correlation between its variables then its correlation matrix is literally the identity matrix, which has eigenvalues of all 1's for as many columns as there are in the matrix. If all the eigenvalues are the same then doing PCA will not help you much in terms of dimension reduction, because the first principal component will not account for any more variation then the following principal components, because eigenvalues literally mean how much variation in the data is accounted for by the corresponding principal component. As the amount of correlation increases, then the first eigenvalue will slowly start to increase, and the later ones will decrease to 'compensate.' As such, PCA will be better performed on highly correlated data. Here is some R code if you'd like to see it for yourself. no.cor
