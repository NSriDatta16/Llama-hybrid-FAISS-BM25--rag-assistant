[site]: datascience
[post_id]: 13977
[parent_id]: 
[tags]: 
Earlystopping in multi-output deep learning

When working with a neural network with more than one output, what is generally advised as the best strategy for early-stopping the training process? Given that I am currently monitoring the net validation loss (validation loss from n different output neurons added together), and have noticed my training early-stopping before convergence in one or more of my losses, I am curious as to what the best practices in this situation are. Should one monitor the net validation loss, or is there any way to implement early-stopping such that m out of n (m goes from 1 to n) outputs' validation losses are monitored and training stops at convergence of all/oscillations within a certain threshold. How would one implement this without risking overtraining some output nodes while preventing others from under-training? I am a newcomer to this field, if you hadn't guessed already so please bear with me if this question is not extremely well put together!
