[site]: crossvalidated
[post_id]: 606227
[parent_id]: 606163
[tags]: 
From a purely statistical perspective, this is exactly what multiple testing correction is for. The first "design" question for you is what exactly you would like to control: Either you want to keep the probability that there is any false positive (i.e. spuriously significant test) below a certain threshold $\alpha_1$ - this leads to the so-called Bonferroni correction method, where you essentially multiply your raw p-values with the number of tests (in fact, there exists a number of variations on this, but all of them share this goal and principle) Alternatively, you could say that of your "hits" of low p-values, you'd like, in expectation, at most a certain fraction $\alpha_2$ to be false positives, i.e. randomly spuriously significant tests, so that on average a proportion of $(1-\alpha_2)$ of your significant tests results from a real signal. This leads you to the False-Discovery-Rate family of methods, which prescribe a somewhat more complex procedure to correct your raw p-values The former is evidently much more conservative than the latter, but there is no "objectively correct" choice - rather it really depends on the tradeoff between false positive and false negative rate. From your description, it seems that the FDR correction could be more suitable to your problem, but the decision ultimately rests with you.
