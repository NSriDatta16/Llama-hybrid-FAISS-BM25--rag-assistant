[site]: crossvalidated
[post_id]: 623677
[parent_id]: 
[tags]: 
Intuition behind designing DNN models

For a long time, I have been looking for a reference that could give me an intuition for designing DNN models, however, I have met with little success. I have a set of training data, I know how input and output layers should look like, but what should I add in the middle? Yes, everyone knows that RNNs are proper choice for time series, whereas Convolutions do a better job at vision. For NLP (or perhaps everything else) the go to is transformers. Add normalization for faster learning, to deal with overfit add dropout, etc, but these ideas are too broad and general. Although designing DNN models may not be very different from antique alchemy at the moment, I refuse to believe that researchers randomly put layers on top of each other until they get a good result, to publish it later on. I am not looking for a set of rules that dictate how a DNN model should be designed, it doesn't exist yet, rather, I would like to get an intuition, a feeling that tells me for example, to add a pooling max layer, instead of a pooling average layer, after a convolution layer. Grid search and any other similar method is not really feasible. Sure it can be used for hyper parameters to some extent, but nothing beyond that really. As a physicist, I have not gone through proper academic training for deep learning, so maybe that's why I am lacking this type of feeling, nevertheless, I appreciate any book, article, tutorial, etc, that can give me such intuition.
