[site]: stackoverflow
[post_id]: 5375167
[parent_id]: 
[tags]: 
Image Descriptors with SIFT/VLFEAT

I want to perform a classification task in which I map a given image of an object to one of a list of predefined constellations that object can be in (i.e. find the most probable match). In order to get descriptors of the image (on which i will run machine learning algorithms) i was suggested using SIFT with the VLFeat implementation. First of all my main question - I would like to ignore the key-point finding part of sift, and only use it for its descriptors. In the tutorial I saw that there is an option to do exactly that by calling [f,d] = vl_sift(I,'frames',fc) ; where fc specifies the key-points. My problem is that I want to explicitly specify the bounding box in which i want to calculate the descriptors around the key-point - but it seems i can only specify a scale parameter which right now is a bit cryptic to me and doesn't allow me to specify explicitly the bounding box. Is there a way to achieve this? The second question is does setting the scale manually and getting the descriptors this way make sense? ( i.e. result in a good descriptor? ). Any other suggestions regarding better ways of getting descriptors ? ( using SIFT with other implementations, or other non-SIFT descriptors ). I should mention that my object is always the only object in the image, is centered, has constant illumination, and changes by some kinds of rotations of its internal parts - And this is why I thought SIFT would work out as i understood it focuses on the orientation gradients which would change accordingly with the rotations of the object. Thanks
