[site]: crossvalidated
[post_id]: 550756
[parent_id]: 411767
[tags]: 
Zhubarb had a very nice answer. I just want to provide more details on embedding and hashing and add one common approach binning. Starting with the binning , this is a very common approach used in many fields, the key idea is many data follows 80-20 rules, that even we have a feature with many values but most of the data will concentred in few values. One simple example is nationality. There are many nations in the world, but if we want to build a statistical model using nationality, we will not use / encoding all of them (there are many reasons behind this, but generally, we may have overfitting if we use all of them). Instead we will pick top nationalities, and bin others int Others category. Note that this approach is also widely used in Deep Learning, where a word will have OOV(out of vocabulary) label when it is in Other category. This is an interesting paper to read: How Large a Vocabulary Does Text Classification Need? , In this paper, the largest vocabulary size is 60K. Embedding is a very nice idea from Deep Learning and NLP. Suppose we are building a model that vocabulary size is 60K, we do not want to do one hot embedding because the vector is very sparse and the distance between vectors are not meaningful . For example, if we encode the word cat into [0,0,....,1,0,0], a lot of space will be wasted (in real word if we use sparse vector instead of dense vector to store the data, it will still be OK, but sparse vector have its own computational challenges.). And the distance between the word "cat" and say "dog", will as same as the distance between cat and say "keyboard". Embedding uses dense vector to do the encoding, and the general idea is the distance between the dense vectors will have meanings. For example, the distance between "cat" and "dog", will be much smaller than the distance between "cat" and "keyboard". Hashing is another interesting idea, an example can be found in sklearn documentation here . The idea is we use hash functions to produce a fixed number of features. This approach will apply a hash function to the features to determine their column index in data / design matrices directly. The result is increased speed and reduced memory usage, at the expense of inspectability; the hasher does not remember what the input features looked like and has no inverse_transform method. In addition, there will be collisions if we set number of the output features small. (for example, the this trick make not be able to differentiate the word "cat" and "keyboard" as both of them mapped into same column index.)
