[site]: datascience
[post_id]: 124634
[parent_id]: 124628
[tags]: 
This doesn't look like it's overfitting at the beginning, but there are signs of overfitting towards the end of training. So I think your methodology could benefit from some different hyper-parameters to avoid the drops in accuracy and spikes in loss. However, I have no information about the task you are conducting, the type of data you have ( tabular, time-series, images? ), the type of model you are building ( deep neural network? ) and the type of problem ( binary or multi-class classification, a lot or little data, imbalanced targets? ). Point that indicate that there is no overfitting up until epoch 10 : The training and validation accuracy increase together and eventually merge around epoch 10, meaning your model is learning from your train data well and generalising well to unseen data. Points that indicate that there is overfitting after epoch 10 : There are drops in accuracy / spikes in loss for both training and validation sets, which could be explained by many factors such as noisy data, model complexity or learning rate that is too high. However, these fluctuations are very pronounced towards epochs 17, 30 and 43, which is a sign that you might be overfitting. Your model seems learning the training too well after epoch 10 (overfitting) and not generalising well to the validation set (seen by the fluctuations that are stronger in the validation set than the training set). Recommendations : Use some regularisation techniques. Again I can't be specific here as I don't know what model you are using. Decrease the learning rate, it might be too high and could be the reason why you can observe these fluctuations in accuracy and loss. Implement early stopping conditions to automatically stop after a certain amount of epochs if there is no improvement on the validation set. This could be for example "stop training if there are no improvements on the validation set for 3 epochs", and would help your model stop on time and naturally avoid overfitting. Use a test set to evaluate the model's performance on top of training and validation. This could re-affirm that you are either overfitting or not. The test set should be created from a different out-of-time sample that is not used for training. Again, these are just general recommendations as I don't have enough information about your problem.
