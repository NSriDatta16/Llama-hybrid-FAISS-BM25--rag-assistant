act of jobs displaced by AI. Long-term concerns The document closes by echoing Microsoft research director Eric Horvitz's concerns that: we could one day lose control of AI systems via the rise of superintelligences that do not act in accordance with human wishes â€“ and that such powerful systems would threaten humanity. Are such dystopic outcomes possible? If so, how might these situations arise? ... What kind of investments in research should be made to better understand and to address the possibility of the rise of a dangerous superintelligence or the occurrence of an "intelligence explosion"? Existing tools for harnessing AI, such as reinforcement learning and simple utility functions, are inadequate to solve this; therefore more research is necessary to find and validate a robust solution to the "control problem". Signatories Signatories include physicist Stephen Hawking, business magnate Elon Musk, the entrepreneurs behind DeepMind and Vicarious, Google's director of research Peter Norvig, Professor Stuart J. Russell of the University of California, Berkeley, and other AI experts, robot makers, programmers, and ethicists. The original signatory count was over 150 people, including academics from Cambridge, Oxford, Stanford, Harvard, and MIT. Notes External links Research Priorities for Robust and Beneficial Artificial Intelligence: An Open Letter