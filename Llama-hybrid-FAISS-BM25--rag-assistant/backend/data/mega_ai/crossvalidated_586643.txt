[site]: crossvalidated
[post_id]: 586643
[parent_id]: 586514
[tags]: 
As the other answers have stated, all the layers up to the last layer can be viewed as a complicated way to create features (feature map), with the last layer representing a linear model. The traditional neural network construction with sigmoid activation function corresponds nicely with logistic regression. The power of neural networks comes from the nonlinearity of the activation function, since stacking linear layers is equivalent to just one linear layer. Theoretical results are various universal approximation theorems , the first by Cybenko (1989) stating that arbitrary width neural networks with only one hidden layer can approximate any continuous function arbitrarily well. However this is an existence result and does not indicate how to actually learn these weights. More recently (2018), neural tangent kernels (NTKs) describe the learning process of gradient descent in NNs when considering the limit of infinite width. However, as a kernel method, in the unrealistic infinite-width regime this simplifies to a linear model and does not match up with empirical performance of over-parameterized neural networks ( Arona et al, 2019 ).
