[site]: crossvalidated
[post_id]: 370208
[parent_id]: 
[tags]: 
Why is gradient descent and it's variants used instead of BFGS and L-BFGS for training neural nets?

My understanding is that BFGS and L-BFGS solve the same type of optimization problems as GD and it's variants. Why is GD the go to algorithm for training neural networks?
