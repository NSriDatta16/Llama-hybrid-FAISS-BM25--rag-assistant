[site]: crossvalidated
[post_id]: 466547
[parent_id]: 466541
[tags]: 
Cross-validation calculates accuracy metrics on data that hasn't been seen by the model. It essentially breaks your training data into multiple train and test sets and reports the performance on the test portion. In that way, the CV accuracy is similar to test accuracy, not training accuracy. The performance of the model will vary based on the training and test set, so the fact that the estimated cross-validation accuracy might be 4% lower than the test accuracy is not surprising on its own. It is also not surprising that different folds in the CV results have different accuracies. The goal of CV is to get a sample of accuracies, in your case five. You can then average them to get a more stable estimate of the out of sample performance, and you can calculate the standard error of the five estimates to create a sort of confidence interval. Unless you're using the CV results to tune or change the model in some way, there is no reason to perform it and then immediately evaluate the accuracy on the test set. You'd be better off performing CV on the entire dataset to estimate test performance.
