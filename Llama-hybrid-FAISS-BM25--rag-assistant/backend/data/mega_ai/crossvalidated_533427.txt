[site]: crossvalidated
[post_id]: 533427
[parent_id]: 489429
[tags]: 
I'll give my own somewhat handwavy explanation of why this might work. I'm not an expert; this is just my reasoning. I don't have a source. Though, as with much of deep learning, there may not actually be any particularly strong theoretical reasons here; it just works well in practice. Consider an autoencoder with a single hidden layer of lower dimensionality than the input, and no activation functions. Let $W$ be the weight matrix from the input layer to the hidden layer and $V$ be the weight matrix from the hidden layer to the output layer. We want a $V$ that inverts $W$ (so that any input $x$ gets mapped to its hidden representation $Wx$ and then back to $VWx = x$ ). This is not generally possible, because $W$ maps from a higher to a lower dimensional space and is thus many-to-one. The best we can do for $V$ is the right inverse of $W$ , which will map any $x$ in the row space of $W$ back to exactly $x$ , and any $x$ not in the row space of $W$ back to the component of $x$ in the row space of $W$ -- the remaining component of $x$ is mapped to $0$ by $W$ and cannot be recovered. (For $W$ to have a right inverse it must have independent rows, but we can safely assume this.) The right inverse of $W$ is $W^T(WW^T)^{-1} = V$ . When the rows of $W$ are orthonormal, $WW^T = I$ and $V = W^T$ . So in this case, the transpose of the weights is exactly what we want. What about when the rows of $W$ are not orthonormal? Well, $W$ isn't really a fixed matrix. It is optimised during training. And if $W$ needs to be orthonormal in order for $W^T$ to invert it as well as possible, then it will become so. Note that requiring $W$ to have orthonormal rows doesn't meaningfully affect the model. Whatever $W$ is, we can form a weight matrix $W'$ with orthonormal rows via row operations on $W$ (this is the Gram-Schmidt process), which we can write as $W' = LW$ ( $L$ being the row operation matrix). This just causes the hidden representation to be transformed in a one-to-one manner (from $Wx$ to $L(Wx)$ ); it contains the same information about the inputs $x$ . To summarise, the optimal choice for $V$ is the right inverse of $W$ . This equals $W^T$ only when $W$ has orthonormal rows, but this can become true during the optimisation process. I think the other answer is correct in that this is done because (right) matrix inverses are expensive to compute. Using the transpose has the same outcome but is much cheaper.
