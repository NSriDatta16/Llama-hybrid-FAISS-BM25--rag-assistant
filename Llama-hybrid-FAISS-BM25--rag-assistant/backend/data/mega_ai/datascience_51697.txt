[site]: datascience
[post_id]: 51697
[parent_id]: 
[tags]: 
bert-as-service maximum sequence length

I installed bert-as-service ( bert-as-service github repo ) and tried encoding some sentences in Japanese on the multi_cased_L-12_H-768_A-12 model. It seems to work as I am getting vectors of length 768 per word but np.shape() shows this for each sentence: np.shape(vec_j[0]): (25, 768) np.shape(vec_j[1]): (25, 768) np.shape(vec_j[2]): (25, 768) np.shape(vec_j[3]): (25, 768) np.shape(vec_j[4]): (25, 768) type: My sentences are short so there is quite a bit of padding with 0's. Still, I am unsure why this model seems to have a maximum sequence length of 25 rather than the 512 mentioned here: Bert documentation section on tokenization "Truncate to the maximum sequence length. (You can use up to 512, but you probably want to use shorter if possible for memory and speed reasons.)"
