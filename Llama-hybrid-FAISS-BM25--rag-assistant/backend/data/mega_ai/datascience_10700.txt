[site]: datascience
[post_id]: 10700
[parent_id]: 10642
[tags]: 
Distributed representations (Glove) based on training on a large corpus are directly available from Stanford NLP group. You can use those word embeddings directly in your application (instead of using 1 hot encoded vectors and then training the network to get the embeddings). If your task is not too specialized starting with this set of embeddings will work well in practice. It will save you from training an additional $m \times V$ number of parameters where $V$ is the vocabulary size and $m$ is the dimension of the embedding space you want to project into.
