[site]: crossvalidated
[post_id]: 539819
[parent_id]: 539787
[tags]: 
There are two principal approaches: Estimate the variance $\hat{\sigma}^2$ of $\theta=p_{x1}-p_{x0}$ and assume that $\theta$ is normally distributed. Then the confidence interval is $\pm z_{1-\alpha/2} \hat{\sigma}$ , where $z_{1-\alpha/2}$ is the quantile of the standard normal distribution ( qnorm(1-alpha/2) in R), which is 1.96 for a 95% interval ( $\alpha=0.05$ ). Directly estimate a non-parametric (and possibly non-symmetrc) confidence interval with the bootstrap method. The variance for method 1. can be estimated in different ways. One is the solution based on Gauss' error propagation law suggested by @demetri-pananos . Another method would be the Jackknife, which consists in estimating the parameter $\theta_{(i)}$ $n$ times, each time with one observable left out, and compute the Jackknife Variance therefrom: \begin{align} \sigma_{JK} &= \sqrt{\frac{n-1}{n}\sum_{i=1}^n \Big(\theta_{(i)}-\theta_{(.)}\Big)^2}\\ \mbox{ with }&\quad \theta_{(.)}=\frac{1}{n}\sum_{i=1}^n\theta_{(i)} \nonumber \end{align} Method 2. is similar to the Jackknife, but the parameter is estimated several times from observables drawn with replacement . From these estimates, the confidence interval can be estimated in different ways. In comparative studies, the "Bias Corrected Accelerated Bootstrap" had the best coverage probability. See section 6 of this report for R code how to compute it and different comparative studies (section 5.2 of the same report explains the Jackknife and lists R code how to compute it).
