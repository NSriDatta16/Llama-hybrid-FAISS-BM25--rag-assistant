[site]: datascience
[post_id]: 124938
[parent_id]: 77391
[tags]: 
I worked on something similar where my model categorizes a title. My initial model had a fixed output size of 40 categories. The number of categories was supposed to be up-scalable as necessary. model_main = Sequential() model_main.add(Embedding(input_dim=30522, output_dim=100, input_length=100)) model_main.add(BatchNormalization()) model_main.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))) model_main.add(BatchNormalization()) model_main.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2))) model_main.add(BatchNormalization()) model_main.add(Dense(40, activation = 'softmax')) model_main.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=[Precision()]) Now, I trained and saved this model along with its history. hisory_main = model_main.fit(X_train, y_main_train, epochs=30, batch_size=16) pickle.dump(hisory_main.history, open(('history_main.pkl'), 'wb')) model_main.save('model_main.keras') Now, I want to add 5 more categories. I did this by creating a new model and transferred each layer except for the output layer from the stored model. As of for the output dense layer, I copied the weights and biases of the stored model for the first 40 neurons and initialized the new 5 neurons with zero weights. If you don't want the previous model, you can directly modify the last layer of the previous model directly without creating a new model. model_main = load_model('model_main.keras') new_model_main = Sequential() for layer in model_main.layers[:-1]: new_model_main.add(layer) weights, biases = model_main.layers[-1].get_weights() new_weights = np.zeros((weights.shape[0], 45)) new_weights[:, :weights.shape[1]] = weights new_biases = np.zeros((45,)) new_biases[:biases.shape[0]] = biases new_model_main.add(Dense(45, activation='softmax')) new_model_main.layers[-1].set_weights([new_weights, new_biases]) new_model_main.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=[Precision()]) For the training of this, since I wanted the previous weights to be propagated, I trained the model right where I left off by using the history file. with open('history_main.pkl', 'rb') as file: history_main = pickle.load(file) num_epochs_main = len(history_main['loss']) new_history_main = model_main.fit(X, y_main, epochs=(num_epochs_main + 2), batch_size=16, initial_epoch=num_epochs_main) for key in history_main: history_main[key] += new_history_main.history[key] In your case where you want to scale down, If you want to use the first 15 speakers weights, I believe you have to copy only the first 15 neurons from the output layer. PS: This is my first time answering to something. Pardon me if something's not right.
