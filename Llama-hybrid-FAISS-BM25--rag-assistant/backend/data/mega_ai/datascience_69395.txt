[site]: datascience
[post_id]: 69395
[parent_id]: 46807
[tags]: 
Using normal KFold cross-validation for time-series data will yield a highly optimistic error estimate since you are using data from the future to predict the past. The model just has to learn to interpolate, not to predict. Therefore you have to use time-wise CV: Furthermore, if your goal is to predict the future for a time-series you do not know the past of, you have to use leave-one-group-out, time-wise-CV to get a realistic performance estimate: Train on all time-series except for a test-set of time-series, but only using data up to a certain point in time. Validate on the test-time-series but only using data after the timepoint. I doubt you will have success with this approach since this is often a very difficult problem. For choosing the right cross-validation method you have to be clear about the goal of your method. Do you want to interpolate missing data for a couple of time-series? Use K-Fold CV Do you want to predict the future of some time-series, for which you know the past? Use time-wise CV Do you want to predict the future of some time-series, but you only know the past of some other time series? Use Leave-one-Group-out, time-wise CV I assume you are after the second point since this is the most common problem.
