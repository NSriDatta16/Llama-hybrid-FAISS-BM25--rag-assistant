[site]: crossvalidated
[post_id]: 372536
[parent_id]: 372339
[tags]: 
In the process part of the gaussian process regression name, there is a notion of continuity which is constrained by the use of a covariance kernel. The underlying assumption is that the variable is spatially auto-correlated, which means that the knowledge of the outcome at some point will give you information at the nearest locations. This is the main difference between a gaussian process and a simple gaussian variable. Regarding regression, the main obvious difference between gaussian process regression and "classic" regression techniques, is that you do not force an analytical formula for the predictor, but a covariance structure for the outcomes. Gaussian process regression is very flexible with respect to interpolation. You can make it an exact interpolator, as long as you don't have two different outcomes for the same input. You can relieve it from the exact interpolation constraint either by bounding the covariance kernel hyperparameters values, or, more usually, by adding some white noise to the covariance kernel. In your example of linear regression, you have $y = \beta^Tx + \epsilon$ with $\epsilon \sim \mathcal{N}(0,\sigma^2)$ . I think you can call it a kind of gaussian process regression as soon as you assume that $\epsilon(x)$ is a gaussian process itself, and not just some random gaussian variable (i.e. you assume that $\epsilon$ is spatially auto-correlated). In the general case of linear regression, the $\epsilon$ term is just assumed to be a white noise, and therefore you cannot call it gaussian process regression. Using both classical regression and gaussian process regression is a quite usual meta-modelling technique in the industry. The basic idea is to first make a fit by a classical technique (any analytical regression, or a deterministic computer code) which is determined by the prior knowledge of the actual process. Then, with experimental data, you can fit the discrepancy between the model and reality, assuming it follows a gaussian process. This will account for the fact that your analytical model is a simplified version of the true process, along with the measurement uncertainties. This is often referred to as bayesian calibration by gaussian processes.
