[site]: crossvalidated
[post_id]: 221473
[parent_id]: 221465
[tags]: 
The two estimators you are comparing are the method of moments estimator (1.) and the MLE (2.), see here . Both are consistent (so for large $N$, they are in a certain sense likely to be close to the true value $\exp[\mu+1/2\sigma^2]$). For the MM estimator, this is a direct consequence of the Law of large numbers, which says that $\bar X\to_pE(X_i)$. For the MLE, the continuous mapping theorem implies that $$ \exp[\hat\mu+1/2\hat\sigma^2]\to_p\exp[\mu+1/2\sigma^2],$$ as $\hat\mu\to_p\mu$ and $\hat\sigma^2\to_p\sigma^2$. The MLE is, however, not unbiased. In fact, Jensen's inequality tells us that, for $N$ small, the MLE is to be expected to be biased upwards (see also the simulation below): $\hat\mu$ and $\hat\sigma^2$ are (in the latter case, almost, but with a negligible bias for $N=100$, as the unbiased estimator divides by $N-1$) well known to be unbiased estimators of the parameters of a normal distribution $\mu$ and $\sigma^2$ (I use hats to indicate estimators). Hence, $E(\hat\mu+1/2\hat\sigma^2)\approx\mu+1/2\sigma^2$. Since the exponential is a convex function, this implies that $$E[\exp(\hat\mu+1/2\hat\sigma^2)]>\exp[E(\hat\mu+1/2\hat\sigma^2)]\approx \exp[\mu+1/2\sigma^2]$$ Try increasing $N=100$ to a larger number, which should center both distributions around the true value. See this Monte Carlo illustration for $N=1000$ in R: Created with: N truemean [1] 61.86781 > mean(mm) [1] 61.97504 > mean(mle) [1] 61.98256 We note that while both distributions are now (more or less) centered around the true value $\exp(\mu+\sigma^2/2)$, the MLE, as is often the case, is more efficient. One can indeed show explicitly that this must be so by comparing the asymptotic variances. This very nice CV answer tells us that the asymptotic variance of the MLE is $$V_t = (\sigma^2 + \sigma^4/2)\cdot \exp\left\{2(\mu + \frac 12\sigma^2)\right\},$$ while that of the MM estimator, by a direct application of the CLT applied to samples averages is that of the variance of the log-normal distribution, $$ \exp\left\{2(\mu + \frac 12\sigma^2)\right\}(\exp\{\sigma^2\}-1) $$ The second is larger than the first because $$ \exp\{\sigma^2\}>1+\sigma^2 + \sigma^4/2, $$ as $\exp(x)=\sum_{i=0}^\infty x^i/i!$ and $\sigma^2>0$. To see that the MLE is indeed biased for small $N$, I repeat the simulation for N and 50,000 replications and obtain a simulated bias as follows: We see that the MLE is indeed seriously biased for small $N$. I am a little surprised about the somewhat erratic behavior of the bias of the MM estimator as a function of $N$. The simulated bias for small $N=50$ for MM is likely caused by outliers that affect the non-logged MM estimator more heavily than the MLE. In one simulation run, the largest estimates turned out to be > tail(sort(mm)) [1] 336.7619 356.6176 369.3869 385.8879 413.1249 784.6867 > tail(sort(mle)) [1] 187.7215 205.1379 216.0167 222.8078 229.6142 259.8727
