[site]: crossvalidated
[post_id]: 321258
[parent_id]: 
[tags]: 
Does the "number of unrollings" of an RNN always have to match the length of the input sequence?

An RNN can be visualized in two ways: rolled and unrolled, as in the following picture I have seen in a few implementations the "number of unrollings" as a parameter or variable of the system, which made me wonder if the number of "unrollings" (in the picture above it should be $3$) is or not dependent on the length of the input sequence. In chapter 10.1 of the book Deep Learning (by Goodfellow et al.) it is stated What we call unfolding is the operation that maps a circuit, as in the left side of the Ô¨Ågure, to a computational graph with repeated pieces, as in the right side. The unfolded graph now has a size that depends on the sequence length . Is the number of times we unroll the recurrent connection always equal or not to the length of the input sequence? I've just come across this post , where the author asks if there is any difference between unrolling one or multiple times an RNN. The answer to that question states that the "number of unrollings" is important only during training. This seems not to be consistent with the information from the book.
