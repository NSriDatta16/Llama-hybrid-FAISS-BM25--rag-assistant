[site]: datascience
[post_id]: 10684
[parent_id]: 10683
[tags]: 
The simplest solution would be to average the predictions from "Team A vs Team B" and "Team B vs Team A" as inputs, and consider that part of your algorithm. You are already doing something similar when augmenting training data by considering both arrangements. This solution is also used by image classification models, which may output predictions averaged over more than one random patch from an input image, to get better translation invariance. Inherently the problem you face is one of dimensionality - the input features are linked by some symmetry (e.g. Team A's defensive strength is equivalent to Team B's defensive strength), but this is not encoded into the learning model. Depending on your model class, you might be able to link weights or make adjustments to the features to better encode the symmetry. But this is a lot more work than creating a simple meta-model that predicts on both input arrangements. Another simple alternative is to have a sorting algorithm apply between the features, such that which is Team A and which Team B as presented to the ML and prediction is decided by some function of the feature values (e.g. always make the team with highest value of a specific predictive feature the first team). This makes the training and prediction deterministic and avoids the issue of needing to treat the teams equivalently. In effect it breaks the symmetry, instead of "Team A" and "Team B" features you have "Team with highest X" and "Team with lowest X" features, and predict whether "Team with highest X" will win. A disadvantage of this approach is that which feature or rule you pick to sort by becomes a meta-param of your ML routine. It may help to pick a feature with a known strong influence on the result (sorting by team name is not likely to be effective).
