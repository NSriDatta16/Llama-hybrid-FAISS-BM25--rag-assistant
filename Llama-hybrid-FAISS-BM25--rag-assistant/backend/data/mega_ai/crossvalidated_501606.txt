[site]: crossvalidated
[post_id]: 501606
[parent_id]: 457889
[tags]: 
Answering my past self... One elegant solution is to use the cross-entropy with "soft-targets" as loss. This means that your targets will not be in one-hot-encodding format, but they will still sum to one. The original cross-entropy formula formula applies. The cross-entropy loss with soft targets is widely used in the knowledge-distillation field: ref .
