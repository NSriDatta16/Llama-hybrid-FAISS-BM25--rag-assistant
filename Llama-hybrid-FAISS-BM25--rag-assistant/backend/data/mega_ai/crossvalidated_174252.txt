[site]: crossvalidated
[post_id]: 174252
[parent_id]: 
[tags]: 
Performing Cross Validation to Compare Lasso and Other Regression Models in R

I have built some "regular" and robust regression models, using the standard lm function as well as rlm and lmrob. While I know that there is some discussion about using stepwise regression, I have used the stepAIC function to prune my variable set. After I've gotten a reduced set of variables using stepAIC, I've then run some robust regressions. The cvTools package allows me to use cross validation to compare the performance of my various models. I obviously would like to run lasso regression (ideally using the glmnet package) on my dataset. My question is whether or not there is an already built package/functionality that will allow me to use cross validiation to compare the lasso regression model with the other models. If there is not, then my initial thought had been to go back to first principles and manually code K-fold cross validation for lasso regression. However, I am now wondering if this is theoretically a good idea. Each time I run a fold in my manual CV, I would run cv.glmnet on the training set. Each training set would most likely result in a different lambda.min and lambda.1se. My question is: is it technically proper CV to determine the overall CV error by averaging the error on each fold given that the lambda chosen for each fold will be producing a different lasso result? Here is some sample code that I have used to create leave-one-out CV on the dataset to evaluate the lasso regression. I have computed my cross validation error on each fold using lambda.1se and labmda.min that arise for that fold. lassocv However, I get different CV results. The two results that are returned for my dataset are 21.94867 and 23.74074.
