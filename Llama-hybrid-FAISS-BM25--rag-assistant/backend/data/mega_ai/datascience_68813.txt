[site]: datascience
[post_id]: 68813
[parent_id]: 65311
[tags]: 
GAN's and traditional augmentation techniques are fundamentally different in a way: A GAN produces (and combines) patterns previously seen in a dataset, data augmentation adds patterns to the data. Well thought out data augmentation tries to add variations that could exist in the data. For instance: In arial photography rotation around the z-axis is very trivial and it could be smart to add that generously, on the other hand of the spectrum: In computer generated cartoons for instance, you'd reasonably do not expect much gaussian noise, and you might want to use that sparsely. GAN's don't add information, they mostly add the same localised patterns that your (I'm assuming) ConvNet to train also has. The benefit a GAN offers is that you can use unlabelled data to train your convolutional layers with concepts you'd might expect in the domain. Other techniques that go in that same direction, and you might want to check out, are weakly supervised learning and auto-encoding (which is imho very close to your original GAN idea).
