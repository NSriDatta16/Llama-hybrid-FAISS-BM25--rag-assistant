[site]: datascience
[post_id]: 79883
[parent_id]: 
[tags]: 
Problem of continuous training - Supervised learning

I am sure this is a most common problem, but would like to know by experts on how to tackle it. Note that, I mostly deal with textual data (NLP problems). When a supervised learning model is created, say a text classifier, and it works well on seen data then we deploy the model in production (you can think of a chatbot also). But in real time, when new type of data comes where the prediction fails, we find that a new word or new pattern is breaking the model. So we go ahead and retrain the model with new encountered data. This is where the continuous learning problem starts. Can ML/NLP veterans please suggest some alternates to solve this labor work? Following approaches have been tried and the problems also listed: We simply can't train with new data infinitely. As production systems should be self healing. We cant put the cost of a human admin constantly monitoring the project. Also, it is practically not possible to get huge domain data during model training phase. Use of advanced embeddings, and SoTA models like BERT. (Problem: The accuracy of these models is too hard to control) Synthetic data generation/data augmentatoin. (Problem : Does not work well in case of NLP problems. Refer: training-with-less-data ) Unsupervised classification (Problem: Does not work well on closed domain problems, as most unsupervised models are either statistical which give a fair value of accuracy but not decent , or are trained on public domain data) Reinforcement learning. (Problem: Real world NLP data is not labeled unlike a self driven car where the feedback is instant)
