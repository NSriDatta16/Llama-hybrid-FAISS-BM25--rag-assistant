[site]: crossvalidated
[post_id]: 553828
[parent_id]: 553769
[tags]: 
Your example model samples fine, if a little slowly (~45 minutes on my laptop), using rstanarm . So one solution is to use a Bayesian MLM/LMM: library(rstanarm) sm which recovers the expected values: mean sd 10% 50% 90% (Intercept) 0.0 0.0 0.0 0.0 0.0 xB 0.0 0.0 -0.1 0.0 0.0 b[(Intercept) g:g1] 0.0 0.0 0.0 0.0 0.0 b[(Intercept) g:g10] 0.0 0.0 0.0 0.0 0.0 b[(Intercept) g:g2] 0.0 0.0 0.0 0.0 0.0 b[(Intercept) g:g3] 0.0 0.0 0.0 0.0 0.0 b[(Intercept) g:g4] 0.0 0.0 0.0 0.0 0.0 b[(Intercept) g:g5] 0.0 0.0 0.0 0.0 0.0 b[(Intercept) g:g6] 0.0 0.0 0.0 0.0 0.0 b[(Intercept) g:g7] 0.0 0.0 0.0 0.0 0.0 b[(Intercept) g:g8] 0.0 0.0 0.0 0.0 0.0 b[(Intercept) g:g9] 0.0 0.0 0.0 0.0 0.0 sigma 1.0 0.0 1.0 1.0 1.0 This is just using the default priors, which you should tune to your specific question. My experience has been that mildly regularizing priors allow these otherwise singular models in lmer() to sample adequately.
