[site]: crossvalidated
[post_id]: 188993
[parent_id]: 
[tags]: 
Use of Random Forests for variable importance as preprocess before another analysis

the question Demonstrate the speed and accuracy of properly applied 'Random Forest' as a variable importance selection tool especially in handling very large data against alternative approaches such as support vector machines (svm). In an ideal case it would be a pre-processor followed by a second method that is less "non-parametric". I have given previous answers ( link ) that weakly answer this, but I think that a clean and complete answer is relevant for the learners in the field. motivation : There are several papers published [ 1 , 2 , 3 , 4 ] where the approach is described as "generally more accurate and incomparably faster (on massive in both dimensions datasets) than original Freidman’s MART and Breiman’s RF" and capable for tens of thousands of predictors (columns) of mixed type (categorical and numeric) and can include missing values, by tens of millions of samples (rows) when the "physics" are highly non-linear, multivariate, and noisy. In these papers the fundamental tool is the 'Random Forest' as a pre-processor. The general approach is to use the Random Forest to determine the important variables (columns) then using only those columns, operate with a much more sophisticated method. In one case there is augmentation of the columns of the data to enhance determination of variable importance. There are a number of alternative post approaches including Least-square fits, and ANOVA. I personally have found this approach to be very useful, both with high and low dimensional data. While it is published, I do not know that its utility has been demonstrated outside of a narrow part of the field, so I think many practitioners in the field do not know to use it. While it is a heuristic argument, I think the fact that the majority of winning-est algorithms on kaggle are related to parallel ensembles (rf) and series ensembles (gbm) is a decent indication of the power of these families of methods. details: There should be two data-sets. The first should be a "decent toy" like the one shown on the demos tab here , and the second should be a decent "big-boy" data set like the pre-processed version of the HIVA data. Methods should use "default" or "textbook" settings. Tuning outside of these is likely to result in less generalizable results. Measures of interest should include both cpu time, and error. ( note to moderators : I think a question like this, and its answer are important and will provide value from cv readers. I intend to provide an answer to this myself if no excellent answers are forthcoming. consider this - it is very basic, but keeps getting up-votes because folks keep finding value in it. This question is my follow-up to this meta question.)
