[site]: crossvalidated
[post_id]: 603065
[parent_id]: 
[tags]: 
How Do I Know If A Markov Chain Follows The Markov Property?

In Probability and Statistics, a Markov Chain is said to have the " Markov Property " if the next state of this Markov Chain only depends on the current state of this Markov Chain. This being said, I had the following question: Suppose I have some data about a Discrete Markov Process, such as the weather each day (e.g. Sun, Rain, Snow) over a period of days. I believe that my data has 1st Order Markov Property, i.e. the next day's weather only depends on the current weather. I go ahead and fit a 3-State Discrete Markov Chain to this data and estimate the transition probabilities corresponding to the 3 x 3 transition matrix. But how exactly do I determine if the data I have collected actually obeys the Markov Property? The only thing that comes to mind are simulation methods - that is, if I create a Markov Model and then test this model, and the state sequence transitions reasonably match the weather in the real world, I could informally conclude that my data obeys the Markov Property. However, this seems informal and I was looking for a more "mathematically rigorous" way to determine if my data follows the Markov Property. Could someone please comment on this? Note: Apparently, there also exist the "2nd Order Markov Property" in which the next state of this Markov Chain only depends on the current and the previous state of this Markov Chain. I would be curious to know how we could test to see if the same data we collected earlier might obey the "2nd Order Markov Property".
