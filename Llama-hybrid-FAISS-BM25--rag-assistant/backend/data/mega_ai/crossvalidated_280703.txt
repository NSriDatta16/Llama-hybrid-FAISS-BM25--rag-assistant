[site]: crossvalidated
[post_id]: 280703
[parent_id]: 280076
[tags]: 
So at its core this is a binary classification problem but there are many ways you can approach it. Two of which: (i'm ignoring data cleaning and other data preps) Come up with various features like word count, n-grams, tfidf scores or any other hand-made features. You then feed these into a classifier like xgboost, decision tree classifier (sklearn comes with a number of such classifers) Use a neural-net based approach. A super simple solution would be taking word2vec represenation of words, encoding them with long short term memories (lstm) and attaching a few dense layers on top. This should give you decent results with much less time invested in feature engineering. the network architecture in keras would be as simple as: embedding_layer = Embedding(nb_words+1, 300, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False) sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32') embedded_sequences = embedding_layer(sequence_input) lstm_layer = LSTM(200, dropout=0.2, recurrent_dropout=0.2) encoded_input = lstm_layer(embedded_sequences) dense_layers= Dense(200, activation='relu')(encoded_input ) dense_layers= Dropout(0.2)(dense_layers) dense_layers= BatchNormalization()(dense_layers) preds = Dense(1, activation='sigmoid')(dense_layers) model = Model(inputs=sequence_input, outputs=preds)
