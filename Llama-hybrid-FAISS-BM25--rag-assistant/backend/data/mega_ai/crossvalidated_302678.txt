[site]: crossvalidated
[post_id]: 302678
[parent_id]: 
[tags]: 
Neural Network Hidden layer has same number of units?

I've never thought deeply before but one question came out of my mind I realized that I and others commonly use each hidden layer that has the same number of units. Like if I have 2 hidden layers with each has 100 activation units. I remember that in Prof. Andrew Ng's lecture he also mentioned that make sure the number of units for each layer to be same. Why is that so? I never thought it was questionable, I just used it without any doubt. Can anyone explain it with easy word?
