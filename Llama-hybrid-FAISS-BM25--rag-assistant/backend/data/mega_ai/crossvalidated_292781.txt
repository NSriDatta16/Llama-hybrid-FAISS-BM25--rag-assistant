[site]: crossvalidated
[post_id]: 292781
[parent_id]: 
[tags]: 
Feature selection with XGBoost

XGBoost will produce different values for feature importances with different hyperparameters on the same dataset. When using XGBoost as a feature selection algorithm for a different model, should I therefore optimize the hyperparameters first? Or there are no hard and fast rules, and in practice I should try say both the default and the optimized set of hyperparameters and see what really works?
