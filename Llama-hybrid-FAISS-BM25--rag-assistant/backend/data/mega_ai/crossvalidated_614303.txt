[site]: crossvalidated
[post_id]: 614303
[parent_id]: 
[tags]: 
How can I compute the SHAP values in a high dimensional dataset?

I have a classiification problem with a dataset where the number of variables is very large, and the number of observations is small. Approximately 200 observations and 10000 variables. I am using a SVC model to make the predictions and I want to use SHAP to meassure the importance of each variable, to try and see of there is a subset of the 10000 variables that are more important for the predictions. However, when I try to use shap with the algorithm='auto' parameter, it says that "it cannot allocate a vector of size 149GB. And when I try and use other algorithms like algorith='partition' , the code simply crashes. So my question is: Â¿Is there a way to compute shap values for this kind of high dimensional datasets? I leave here a minimal code example to show my issue with some fake data: import numpy as np import pandas as pd from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.svm import SVC import shap X, y = make_classification(n_samples=200, n_features=10000, n_informative=100, scale=1, random_state=999) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25) # Build Support Vector Classifier on the train set model = SVC(C=0.5, kernel='sigmoid', gamma=0.01, probability=True) model.fit(X_train, y_train) # Compute SHAP values for the test data explainer = shap.Explainer(model.predict_proba, X_train) shap_values = explainer(X_test, max_evals=20001) # max_evals must be at least 2 * num_features + 1 # MemoryError: Unable to allocate 149. GiB for an array with shape (2000100, 10000) and data type float64 explainer = shap.Explainer(model.predict_proba, X_train, algorithm='partition') # This simply kicks me out of the python terminal.
