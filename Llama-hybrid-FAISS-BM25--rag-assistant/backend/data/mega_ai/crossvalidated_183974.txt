[site]: crossvalidated
[post_id]: 183974
[parent_id]: 183973
[tags]: 
Random forests take care of choosing subsets, that is the mtry parameter for (the number of features randomly sampled as candidates at each split). In tuneRF set the ntreeTry parameter as high as your time allows or let it be at default - otherwise you won't get statistically sound results. You don't need to worry about overfit in case of random forests, just be sure not to use the training data to evaluate model performance ( see this post ). As for your treerange parameter, I'd advise it to be well over 100, as much as your machine's performance allows. If you have so few features, I wouldn't bother at all with feature selection, unless you have performance limits. In that case try Boruta .
