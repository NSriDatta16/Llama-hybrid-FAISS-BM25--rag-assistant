[site]: crossvalidated
[post_id]: 320137
[parent_id]: 320082
[tags]: 
I don't really see what the statistical aspect of this question is, so I'll answer the optimization part. There are 2 parts to convergence: iteration cost & iteration count Pretty much every answer here is focusing on just the iteration cost and ignoring the iteration count . But both of them matter. An method that iterates in 1 nanosecond but that takes $10^{20}$ iterations to converge won't do you any good. And a method that blows up won't help either, no matter how cheap its iteration cost. Let's figure out what's going on. So: Why not use >2nd-order derivatives? Partly because (and this is true for 2nd-order too, but more on that in a bit): Higher-order methods generally only converge faster when near the optimum . On the other hand, they blow up more easily when they are farther from the optimum! (Of course, this isn't always true; e.g. a quadratic will converge in 1 step with Newton's method. But for arbitrary functions in the real world that don't have nice properties, this is generally true.) This means that when you are farther away from the optimum, you generally want a low-order (read: first-order) method. Only when you are close do you want to increase the order of the method. So why stop at 2nd order when you are near the root? Because "quadratic" convergence behavior really is "good enough"! To see why, you first have to understand what "quadratic convergence" means . Mathematically, quadratic convergence means that, if $\epsilon_k$ is your error at iteration $k$ , then the following eventually holds true for some constant $c$ : $$\lvert\epsilon_{k+1}\rvert \leq c\ \lvert\epsilon_{k}\rvert^2$$ In plain English, this means that, once you are near the optimum (important!), every extra step doubles the number of digits of accuracy . Why? It's easy to see with an example: for $c = 1$ and $\lvert\epsilon_1\rvert = 0.1$ , you have $\lvert\epsilon_2\rvert \leq 0.01$ , $\lvert\epsilon_3\rvert \leq 0.0001$ , etc. which is ridiculously fast. (It's super-exponential !) Why not stop at 1st order rather than 2nd-order? Actually, people often do this when second-order derivatives become too expensive. But linear convergence can be very slow. e.g. if you got $\epsilon_k = 0.9999999$ then you'd need maybe 10,000,000 iterations with linear convergence to get $\lvert\epsilon\rvert , but only 23 iterations with quadratic convergence. So you can see why there's a drastic difference between linear and quadratic convergence. This is not true for 2nd and 3rd-order convergence, for example (see next paragraph). At this point, if you know any computer science, you understand that with 2nd-order convergence, the problem is already solved . If you don't see why, here's why: there is nothing practical to gain from tripling the number of digits every iteration instead of doubling it—what's it going to buy you? After all, in a computer, even a double -precision number has 52 bits of precision, which is around 16 decimal digits. Maybe it will decrease the number of steps you require from 16 to 3... which sounds great, until you realize it comes at the price of having to compute third derivatives at each iteration, which is where the curse of dimensionality hits you hard. For a $6$ -dimensional problem, you just paid a factor of $6$ to gain a factor of $\approx 5$ , which is dumb. And in the real world problems have at least hundreds of dimensions (or even thousands or even millions), not merely $6$ ! So you gain a factor of maybe 20 by paying a factor of, say, 20,000... hardly a wise trade-off. But again: remember the curse of dimensionality is half the story . The other half is that you generally get worse behavior when you're far from the optimum, which generally adversely affects the number of iterations you have to do. Conclusion In a general setting, higher-order methods than 2 are a bad idea. Of course, if you can bring additional helpful assumptions to the table (e.g. perhaps your data does resemble a high-degree polynomial, or you have ways of bounding the location of the optimum, etc.), then maybe you can find that they are a good idea—but that will be a problem-specific decision, and not a general rule of thumb to live by.
