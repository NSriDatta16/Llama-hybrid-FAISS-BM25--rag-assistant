[site]: crossvalidated
[post_id]: 396201
[parent_id]: 392108
[tags]: 
Your question is not straightforward to answer because violations of i.i.d can happen in many different ways. In short, I would say that regarding your main doubt, the answers would be the following: i.i.d is invoked for estimation from samples, and violations of i.i.d can be dealt with, but this depends on the model and the query of interest. In causal inference, when you obtain an identification result, this is will be a functional of a joint distribution $P$ . For instance, assuming conditional ignorability $Y_{x} \perp \!\!\! \perp X|W$ , the distribution of the potential outcome is identified with the functional $P(y_{x})= \sum_{w}P(y|x, w)P(w)$ . Note this is an equality of population quantities. If you had the exact population values of the distribution $P$ , you would be done. But usually you have a sample of size $n$ , which can be summarized by the empirical distribution $P_{n}$ . When you assume to have an i.i.d sample of $P(y,x,w)$ , you know you can estimate $P$ consistently given large enough data, for instance, by appealing to the Glivenkoâ€“Cantelli theorem or simpler parametric results if you assume a parametric model. That's why i.i.d sample is usually assumed. Is this a general assumption that can be relaxed? What happens if we have dependence? What happens if they are NOT identically distributed? Yes this assumption can be relaxed in some ways, but usually at the expense of another (parametric) assumption. As a simple example, suppose you have time series data and a linear structural model for $Y$ , $$ Y_{t} = \tau D_{t} + \beta{Y_{t-1}} + \epsilon_{t} $$ With $|\beta| and $\epsilon_{t}$ Gaussian and $E(\epsilon_{t}|D_{t}, Y_{t-1}, ..., Y_{1})=0$ . Consider you only have one observation per $t$ . Here each of the $Y_{t}$ are both dependent and not identically distributed. Yet, $\tau$ can be consistently estimated via OLS.
