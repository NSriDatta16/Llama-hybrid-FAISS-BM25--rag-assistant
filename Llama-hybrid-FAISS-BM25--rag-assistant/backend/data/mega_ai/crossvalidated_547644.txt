[site]: crossvalidated
[post_id]: 547644
[parent_id]: 546229
[tags]: 
You could do this in a "actor-critic" style: Train a neural network $D_{\beta}(x)$ to predict $P(T_\theta(Z) using binary cross-entropy, by sampling $\hat x \sim T_\theta(Z)$ and comparing $\hat x$ with $x$ to generate labels. In other words, minimize $$-E_{x,x' \sim T_\theta}\left[\delta(x' Then simultaneously, you can define the loss on $T_\theta$ as $$E_{x \sim T_\theta}\left[(F_X(x) - D_\beta(x))^2\right]$$ Basically, we push the estimated CDF of our model toward the true CDF. We can backpropagate to minimize this loss wrt $\theta$ while holding $\beta$ fixed. Training will alternate between updating $\beta$ and updating $\theta$ . Admittedly, I kind of made this up on the spot and have no idea how practical it is.
