[site]: crossvalidated
[post_id]: 567531
[parent_id]: 
[tags]: 
Why does fixing the pretrained word embedding give lower accuracy than training from scratch?

I was doing multi-class text classification task and I built 2 models, one LSTM model that trains from scratch and other LSTM model with fixed pretrained word embedding. Fixing the pretrained word embedding gives lower accuracy than training from scratch on both the validation and training data. What is the reason behind that? Shouldn't the pretrained word give better word representation?
