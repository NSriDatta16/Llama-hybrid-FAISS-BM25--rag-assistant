[site]: crossvalidated
[post_id]: 468166
[parent_id]: 458430
[tags]: 
The simulation is indeed often used. For example, in alpha-zero style MCTS+RL algorithms, the game tree is rolled out according to the known rules at test time. (You may argue the rules of chess are too simple to qualify as a "simulation" of any real environment of course). And a recent line of research is to make physics engines whose dynamics are efficiently differentiable, so that not only can the agent make use of the known physics, it can be trained by directly backpropagating through the environment. Thirdly, a common way of training robotics policies is to compute the "optimal" solution (e.g. with an ILQR solver) and then use it as ground truth supervision. But there are good reasons why you may not want to use the simulation. And the goal is often to maximize performance in this same environment. The actual goal is either to 1. develop RL algorithms in simulation with the idea that eventually, techniques will become sample-efficient enough that they can be used in real life. 2. develop RL algorithms in simulation which will be robust enough to generalize to real life. For either of these to work, your agent can't really make use of ability to look-ahead in a sim.
