[site]: crossvalidated
[post_id]: 374326
[parent_id]: 
[tags]: 
Why compare measures of dispersion with ratios?

I'm writing a paper on a novel statistical model estimated using MCMC and am currently evaluating it using simulated data. We are comparing the performance of our model to an established model as baseline and, among other things, looking at the RMSE for the estimated model parameters. I initially computed the difference between the RMSE using the baseline and the RMSE using our model for 500 randomly generated data sets (i.e. all positive values meant our RMSE was smaller). My co-author and thesis advisor wrote in a comment on the generated plots "It would be better to compute the ratio as RMSE is a measure of dispersion. Compare to how you test whether two variances are equal by looking at the ratio of estimators [Translated from the original Swedish.]" (i.e. all values > 1 would mean our RMSE was smaller). I seem to have stumbled onto (or perhaps into) a hole in my Statistics knowledge (I'm a Computer Scientist doing a PhD in Natural Language Processing, w.r.t. Statistics my formal education is very mixed). I don't doubt he's correct but I honestly can't say I understand it myself. I've googled but the closest I've found is the F-test in Fisher's ANOVA and knowing my advisor, who've never performed an ANOVA in his life and as an avowed DeFinettian-Bayesian probably isn't a great fan of Fisher, that probably wasn't what he was referring to, even though he might be referring to the same underlying principles. I've answered the comment asking him to clarify but I don't expect an answer for quite awhile. Could someone elucidate why we want to compare measures of dispersion using ratios rather than differences, or point me to a source explaining this?
