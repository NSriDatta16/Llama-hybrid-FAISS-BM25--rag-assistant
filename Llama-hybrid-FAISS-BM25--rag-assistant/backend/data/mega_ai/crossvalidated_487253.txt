[site]: crossvalidated
[post_id]: 487253
[parent_id]: 
[tags]: 
Why don't we estimate the prior in a Naive Bayes' classifier?

I'm currently studying the textbook Introduction to Machine Learning 4e (Ethem Alpaydin) the brush up on my ML basics and had a question regarding a part w.r.t. using the Naive Bayes' classifier in multivariate analysis. More specifically, this is the part that's confusing me: Let us say $x_j$ are binary where $p_{i, j} = p(x_j = 1\ \vert\ C_i)$ . If $x_j$ are independent binary variables, we have $$p(\mathbf{x}\ \vert\ C_i) = \prod_{j = 1}^d p_{i, j}^{x_j} (1 - p_{i, j})^{(1 - x_j)}$$ This is another example of the naive Bayes' classifier where $p(x_j\ \vert\ C_i)$ are Bernoulli. The discriminant function is: $$ \begin{align} g_i(\mathbf{x}) & = \log{(p(\mathbf{x}\ \vert \ C_i))} + \log{(P(C_i))} \\ & = \sum_j \left[ x_j \log{(p_{i, j}) + (1 - x_j) \log{(1 - p_{i, j})}} \right] + \log{(P(C_i))} \end{align} $$ which is linear. The estimator for $p_{i, j}$ is: $$\hat{p}_{i, j} = \frac{\sum_t x_j^t r_i^t}{\sum_j r_i^t}$$ ( $r_i^t = 1$ if $\mathbf{x}^t \in C_i$ ). What's confusing me is, I recall in an earlier chapter about the Bayes' classifier and parametric classification that we may also use maximum likelihood estimation (MLE) to get the estimate for the prior $P(C_i)$ such that $$ \hat{P}(C_i) = \frac{\sum_t r_i^t}{N} $$ Why is it that estimation isn't made here? I thought that it was implied, but it seems to be omitted altogether.
