[site]: crossvalidated
[post_id]: 32616
[parent_id]: 
[tags]: 
How do I test for lower variability induced by systematic missings?

In my data, some individuals have missing data on the central predictor (father missed the intake assessment). Comparing the DVs' means for those with a missing/non-missing predictor yielded some sizeable effects. Now I want to find out whether the systematic missings may have led me to underestimate the size of the OLS regression coefficients. What's a good way to do this? Simply comparing the variances of the DVs in the group without missings to the group with missings is easy to do? But conceptually I want to know whether the whole sample has significantly less variability when I leave the group with missings (and significantly-lower-than-average-scores) out, not whether the two groups (with missings and without missings) have different variances. All that I found so far was about independent samples, not about subsets. Also, just to bring me up to speed: heteroscedasticity is usually used in the context of residual variance, right? What's a good term for constricted variance that would give me better luck with google?
