[site]: crossvalidated
[post_id]: 149669
[parent_id]: 149661
[tags]: 
I think it's neither. The "textbook" information criteria formulation that you cite are derived for i.i.d. data, while you have a two-way array with weird cross-dependencies: you have the same questions, and you have the same students. The issue is always there with mixed models. I am not going to try to reproduce their expressions, but Delattre et. al. (2014) paper ( DOI:10.1214/14-EJS890 ) derives the relevant contributions from the observation-level (student-by-item) and cluster-level (student) data. They, however, seem to be oblivious to the prior work by the SAMSI group on Bayesian latent variable modeling , although frankly it did not do such a great job of documenting their results. The most important one is fully documented only in somebody's presentation . It was quoted by Jim Berger in the 2007 Wald lecture at the Joint Statistical Meetings . Finally, a rather lean book on random effect and latent variable model selection edited by David Dunson appear to have closely related results, but don't derive them in the form applicable to construction of the mixed-model BIC.
