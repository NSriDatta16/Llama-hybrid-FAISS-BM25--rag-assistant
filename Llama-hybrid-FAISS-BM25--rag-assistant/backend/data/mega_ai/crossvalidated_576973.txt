[site]: crossvalidated
[post_id]: 576973
[parent_id]: 
[tags]: 
Why do we use the same parameters for the joint, marginal and conditional distributions in VAEs?

I've noticed in several resources on variational autoencoders (for example the Wikipedia article ), we use the same parameters theta ( $\theta$ ) for the prior, likelihood, posterior, etc distributions. For example the equation $p_\theta(x) = \int_z p_\theta(x|z)p_\theta(z)dz$ . Aren't $p_\theta(x)$ and $p_\theta(z)$ two different distributions, so how can we parameterize them with the same $Î¸$ params. I might be misunderstanding something about what it means to parameterize a distribution with neural nets..
