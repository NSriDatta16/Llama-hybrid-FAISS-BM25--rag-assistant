[site]: crossvalidated
[post_id]: 301600
[parent_id]: 
[tags]: 
Computing gradient w.r.t action, ReLU transfer

Foreword : I am new to machine learning and to this community, thank you in advance for your help. I am trying to implement this paper from Deepmind Continuous control for deep reinforcement learning The NN for the Critic is using 2 hidden layers taking the 'state' as input, an other hidden layer taking the 'action' as input, and then adding both parts of the network, using a ReLU transfer function before computing the output. So when we feed forward we have something in the lines of : output = ReLU(state * ws1 * ws2 + action * wa1) * wo where ws1 and ws2 are the weights for the hidden layers of the 'state', wa1 the weights for the hidden layer of the 'action', and wo the weights for the output layer. From this I need to compute the gradient of the output w.r.t the action in order to update my Policy Network. What bothers me is that ReLU function. What am I supposed to do with it when computing the derivative w.r.t the action as what is inside this function does not depend solely on the action? And a noob question while I am at it : If there was no ReLU transfer function, say the output would be calculated that way : output = (state * ws1 * ws2 + action * wa1) * wo would the derivative of the output w.r.t the action equal to wa1 * wo ?
