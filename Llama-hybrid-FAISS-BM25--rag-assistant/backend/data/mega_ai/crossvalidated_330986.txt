[site]: crossvalidated
[post_id]: 330986
[parent_id]: 
[tags]: 
Should normalization match the activation function?

I'm new to neural networks and I think I now have a good grasp of the fundamentals, but I have a question relating to normalization and activation functions. I see places that say to normalize between -1 and 1, and some that say between 0 and 1. I also see many people recommending using the ReLU activation function for performance benefits. I assume that the data should be normalized to suit the chosen activation function? i.e. if using ReLU then the data should be normalized between 0 and 1 as anything Also, because ReLU is linear, could the data be normalized beyond 0-1, and maybe 0-5? Would that be advisable? Many thanks.
