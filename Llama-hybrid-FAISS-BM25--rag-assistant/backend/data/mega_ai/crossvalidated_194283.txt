[site]: crossvalidated
[post_id]: 194283
[parent_id]: 194238
[tags]: 
Note: This post is for educational purposes. The author does not condone the usage of best subset selection for any other purpose. With that out of the way... How do we know that we don't have a different 'best' model of size i for each different fold j? You don't. If you did you'd just pick that subset and be done with it, there'd be no need for cross validation at all. The idea behind cross validation is to get an estimate of the hold out performance of a model trained on each subset size, because that's what's really important. To do so, you "fold" your data set into many different train, validate pairs, and then train and validate on each in turn. Each subset size gives you a collection of estimated validation errors, one for each fold. Each of these validation errors is a point estimate of the true hold out error, so their average should be a lower variance estimate. Now you have a single estimate of the hold out error for each subset size, so you can choose the subset size with the most desirable estimate. # For each fold for(j in 1:k){ # Fit the model with each subset of predictors on the training part of the fold best.fit=regsubsets(Salary~.,data=Hitters[folds!=j,], nvmax=19) # For each subset for(i in 1:19){ # Predict on the hold out part of the fold for that subset pred=predict(best.fit, Hitters[folds==j,],id=i) # Get the mean squared error for the model trained on the fold with the subset cv.errors[j,i]=mean((Hitters$Salary[folds==j]-pred)^2) } } The missing part of this code is taking the row means of this matrix, and using the result to choose the "best" subset size.
