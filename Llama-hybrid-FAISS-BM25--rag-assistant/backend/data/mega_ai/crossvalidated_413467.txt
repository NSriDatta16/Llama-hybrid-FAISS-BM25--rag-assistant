[site]: crossvalidated
[post_id]: 413467
[parent_id]: 
[tags]: 
Different notions of over-parameterization

While reading a paper , I came across the statement This prediction function will be parameterized by a parameter vector $\theta$ in a parameter space $\Theta$ . Often, this prediction function will be over-parameterized and two parameters $(\theta, \theta') \in \Theta^2$ that yield the same prediction function everywhere, $\forall x \in \mathscr{X}, f_\theta(x)=f_{\theta'}(x)$ , are called observationally equivalent. I thought the general notion of over-parameterization, at least within deep learning, was that of overfitting the parameters. However, the authors of the paper seem to be talking about a different idea. Are these two notions the same thing in different words or are they describing two completely different concepts?
