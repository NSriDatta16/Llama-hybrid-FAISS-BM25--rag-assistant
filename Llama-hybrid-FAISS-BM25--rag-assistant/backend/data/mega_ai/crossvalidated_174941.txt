[site]: crossvalidated
[post_id]: 174941
[parent_id]: 174920
[tags]: 
How about the ensemble method of boostrapped aggregating, also known as bragging? Using this approach you essentially create a large number of replicates of the original dataset using simple random sampling with replacement (say 10,000 bootstrapped datasets) from your original dataset. Then you implement a variable selection routine (perhaps best subsets or traditional stepwise selection methods) to select the coefficients or predictors that are significant for each of the boostrapped samples. You perform the routines for each bootstrapped samples and then look at the rates of how often the predictors are selected. Predictors that appear in say 90% or more of the sample are then used in the final mixed model. There are many other methods that could be used too, but I highlight this one as it's simple to explain and usually very easy to implement. For more information see, Breiman, Leo (1996). "Bagging predictors". Machine Learning 24 (2): 123â€“140. doi:10.1007/BF00058655.
