[site]: crossvalidated
[post_id]: 350846
[parent_id]: 350165
[tags]: 
[I]sn't the number of epochs something specific to the training phase of a neural network? You're correct -- the number of epochs is just the jargon describing how many total passes the model has made over the entire training data set. Purely in terms of mathematical terminology, an "epoch" is not really a relevant concept for the purposes of prediction, since at the time that you are obtaining predictions for some data, the model parameters are not changing. Purely in the abstract, you could, if you so desired, supply one sample at a time to the network to obtain a prediction, or all of them at once, or anything in between. Any of these configurations would yield the same results, since all parts of the model are fixed (i.e. it is not training). Why would you need to specify a number of epochs? I don't know. I've read the documentation for tf.estimator.inputs.pandas_input_fn . It's clear that this is just a convenience function used to make a function to supply data in a pandas format to TensorFlow; this is a function that makes functions. On the other hand if this function is used for obtaining predicted values, why does it have parameters for epochs and shuffling the data? Training for some number of epochs and shuffling the data for training makes sense (cf stochastic gradient descent), but when I'm obtaining predictions, I'd like to know that the order of the input matches the order of the output (i.e., don't shuffle it!) if this function is used for training (which could be appropriate given its argument set), why do the authors of this code snippet use it for prediction? if this function can be used for both training and prediction, how does TensorFlow distinguish between the two modes? I don't want parameters to update after obtaining predictions, nor do I want dropout to be applied when I am acquiring predictions. But when I'm training, I do want parameters to update, and I do want dropout to be applied (if I'm using dropout). It appears that supplying y is optional, so perhaps TensorFlow infers not to train if there is no y (since you need y to compute the loss and update parameters), but that's not at all clear from the documentation. Could it be that this function only populates tf.Tensor s in the model, and computation is deferred until a specific operation on the computational graph is called at some later stage? Absolutely. But if that's true, why do we specify a number of epochs? I would expect the number of epochs to be determined by whatever later operation I call... TL;DR - TensorFlow is poorly documented.
