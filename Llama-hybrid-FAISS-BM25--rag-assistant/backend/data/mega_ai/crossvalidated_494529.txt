[site]: crossvalidated
[post_id]: 494529
[parent_id]: 441852
[tags]: 
Just add a new embedding layer for the new words and freeze the pretrained layer and make the new layer trainable. Here is a related answer: Is it possible to freeze only certain embedding weights in the embedding layer in PyTorch?
