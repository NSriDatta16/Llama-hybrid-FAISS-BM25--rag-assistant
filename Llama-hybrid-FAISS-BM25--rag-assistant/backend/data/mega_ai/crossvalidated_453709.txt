[site]: crossvalidated
[post_id]: 453709
[parent_id]: 
[tags]: 
Where is my input when I use Linear Discriminant Analysis - LDA?

I'm trying to do picture recognition. There are 3 types of methods from OpenCV library. Eigenfaces, Fisherfaces and Local Binary Pattern Histogram. These are good, but in practice, Fisherfaces is the best becase Eigenfaces is good if you have a noisy picture and Local Binary Pattern Histogram cannot handle noice, but it can handle linear illumination differences. And Ficherfaces can handle both. To do Fisherfaces we need to have a collection $X$ where $X$ contains vector $c$ classes of pictures. This is our collection of pictures in form of vectors: $$X = X_1, X_2, X_3, ... , X_c$$ This is our picture in form of a vector: $$X_i = x_1, x_2, x_3, ... , x_n$$ Then we need to create scatter matrices. Here $N_i$ is the index of the vector $x_n$ $$S_B = \sum_{i=1}^c N_i(\mu_i - \mu)(\mu_i - \mu)^T$$ $$S_W = \sum_{i=1}^c\sum_{x_j \in X_i}(x_j - \mu_i)(x_j - \mu_i)^T$$ We can find the means from: $$\mu = \frac{1}{N} \sum_{i=1}^N x_i$$ And this is just the mean of a single class $X_i$ $$\mu_i = \frac{1}{|N_i|}\sum_{x_j \in X_i} X_j$$ To solve this optimization problem. We want to maximize this matrix $W_{opt}$ . This is the criteria. $$W_{opt} = arg max_w \frac{|W^TS_BW|}{|W^TS_WW|}$$ We can solve this optimization problem by solving the generalized eigenvalue problem: $det(A - \lambda I) = 0$ : $$S_W^{-1}S_B W = \lambda W$$ If $S_W$ is singular, we can add a small constant $\epsilon$ so $S_W$ can be invertable. That is recommended. $$ (S_W +\epsilon I)^{-1}S_B W = \lambda_i W$$ Or we can skip the $\epsilon$ and find $S_W^{-1}$ by using House Holder QR-factorization or Singular Value Decomposition. It can handle singular matrices. Copy from the lecture notes here and here and here . This is the best lecture note . Questions: Let's assume that I have my collection $X$ that contains only pictures of my cat, and I want to compare with another sample $Y$ , which is another cat, just to make sure how close $X$ are to $Y$ . $Y$ is just one sample and $X$ is just a collection. Or is $X$ a collection of different classes like $X_1$ is all pictures of my cat and $X_2$ is all pictures on my dog and so on? Or is every class in $X$ just a average picture? So how do I use this linear discriminant analysis algoritm to compare? What should I use $W_{opt}$ too? Is that a matrix? Edit: Here is my code to find $S_W$ and $S_B$ %% Our data data=load('iris'); % IRIS database X = data.Inputs'; N = size(X, 2); % Column length of the sets %% Settings S = 50; % Sets - Here you can edit C = 3; % Classes - Here you can edit %% Create ui and u ui = zeros(C, N); u = zeros(1, N); for i = 1:C for j = 1:N for k = 1:S ui(i, j) = ui(i, j) + X(k+S*(i-1), j)/S; % Sum all sets of one class into one mean. end u(1, j) = u(1, j) + ui(i, j)/C; % Sum all the means from the sets into one big mean end end %% Create Sw and Sb Sb = zeros(N,N); Sw = zeros(N,N); H = zeros(N,1); L = zeros(N,S); for i = 1:C for j = 1:N H(j) = ui(i, j) - u(1, j); end Sb = Sb + S*H*H'; % Muliply with amout of sets for k = 1:S for j = 1:N L(j, k) = X(k+S*(i-1), j) - ui(i, j); end end Sw = Sw + L*L'; end %% Print Sb and Sw Sb Sw %% Compute for hand W1 = X(1:50, :); W2 = X(51:100, :); W3 = X(101:150, :); u1 = mean(W1)'; u2 = mean(W2)'; u3 = mean(W3)'; u = mean([u1';u2';u3'])'; SB = 50*(u1 - u)*(u1-u)' + 50*(u2 - u)*(u2-u)' + 50*(u3 - u)*(u3-u)' SW = (W1' - u1)*(W1' -u1)' + (W2' - u2)*(W2' - u2)' + (W3' - u3)*(W3' - u3)' %% Compute [W, LAMBDA] = eig(inv(SW)*SB) Wopt = (W*SB*W)/(W*SW*W)
