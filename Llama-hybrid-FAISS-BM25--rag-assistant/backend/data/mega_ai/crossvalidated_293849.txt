[site]: crossvalidated
[post_id]: 293849
[parent_id]: 293830
[tags]: 
Performing k-fold cross validation can be computationally expensive for complex neural networks, but if you can do it you should. If you choose do k-fold cross validation you have to train k distinct neural networks keep track of the validation error across different networks and epochs. I suppose after you could plot how the mean and standard deviation of the error (across k-folds) evolves with the number of epochs, and determine the optimal number of epochs with which you should train with all the data to get a final network. If performing k-fold cross validation is too expensive you could use a technique called early stopping where you stop training when you performance on the validation set (just one) starts to deteriorate. I have never seen people changing the train and validation set between epochs in the same training instance like you suggested, I am not sure it is advisable to do so.
