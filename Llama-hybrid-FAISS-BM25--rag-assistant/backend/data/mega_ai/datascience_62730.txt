[site]: datascience
[post_id]: 62730
[parent_id]: 
[tags]: 
Best structure for a LSTM Bert sentence classifier

I'm interested in classifying sentences using BERT. Finetuning on a single sentence had very poor results. I'd like to add a forward and backward LSTM layer to try to improve results. I'm having trouble figuring out the best structure for an efficient implementation. My first idea is to implement it as normal, but use a tf while loop to get all the BERT embeddings, and then feed them into the output. This seems really inefficient because I'll be calculating way more embeddings than I need. The next would be to pre calculate all BERT embeddings and then feed those as input to my LSTM and classifier layer. Using this I think it's clear how to train the LSTM and classifier model, but how would I train the BERT Model as well? Is this a common method of constructing / training models? Any references? Finally the last idea was to use the 2 sentence BERT embedding. It appears it would be limited to a 3 sentence window - previous sentence, current, and next. So scaling up might not work if the model still does not perform that well. Another issue is that it's unclear, at least to me, if the pooled output of a sentence [cls] sentenceA [sep] sentenceB [sep] gives a representation of sentenceA, of both of them, or of sentenceB 'given' sentenceA. The pooled output is the representation of [cls] token, but in the paper / github there was no reference to if the [sep] representation encodes anything at all. Any help, insights, or similar work would be appreciated!
