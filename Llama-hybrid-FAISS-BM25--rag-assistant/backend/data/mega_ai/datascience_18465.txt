[site]: datascience
[post_id]: 18465
[parent_id]: 
[tags]: 
Neural networks - adjusting weights

My question would be about backpropagation and understanding the terms feedforward NN vs backpropagation. I have two questions really: If I understand correctly, even a feedforward network updates its weights (via a delta rule for example). Isn't this also backpropagation? You have some random weights, run the data through the network, then cross-validate it with the desired output, then update the rules. This is backpropagation, right? So what's the difference between FFW NN and RNN? If you can't backpropagate on the other hand, how do you update the weights in a FFW NN? I've seen NN architectures looking like this: Basically all the neurons are being fed the same data, right? OK, the weights are randomized and thus different in the beginning, but how do you make sure Temp. Value #1 will be different from Temp. Value #2, if you use the same update rule? Thank you!
