[site]: crossvalidated
[post_id]: 282753
[parent_id]: 282433
[tags]: 
How exactly does this work in practice? Say my original data is 1000 data lines. So I split it into 5 groups. Then I fit a model on each group, and test it on the other 800 data lines. Yes. In practice, you need to make sure that you split your data well: the splits need to be independent of each other and they usually should be representative for the whole data set. In particular, if you have structure in your data (e.g. data sorted wrt. the independent variable or clusters of rows that may be more similar to each other) you need to take that into account in your splitting. So, far, so good. Then what? Every time I test it, what "statistic" do I compute? This depends on your application. Cross validation is just a plan how to get test cases. What figure of merit (statistic) you calculate is independent of how exactly you get your test cases. One possible statistic would be accuracy for classification. For regression, root mean squared error RMSE is a frequent choice. Brier's score would be a mean squared error for classification. And after I do that 5 times, do I average that statistic? Or, do I do something else to measure the accuracy? Often, not the statistic but the test predictions are pooled: the underlying assumption of cross validation is that each of your 5 so-called surrogate models is (approximately) equal to "the model" trained using all cases. Therefore, you use the test predictions of the surrogate models as surrogate for test predictions of "the model". Whether or not pooling predictions is the same as averaging the statistic dependy on the figure of merit you look at (and on whether your splits have exactly the same size). E.g. the mean of 5 RMSE's is typically not the same as the RMSE of all predictions of the 5 surrogate models.
