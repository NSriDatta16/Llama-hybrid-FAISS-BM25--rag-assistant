[site]: crossvalidated
[post_id]: 56777
[parent_id]: 
[tags]: 
For bootstrapping, why does a higher subsample size lead to lower variance?

I've been working on a bootstrapping problem that's left me a little confused and wondering whether I'm doing things correctly. We have around 200 samples from a population of about 3,400, we want to bootstrap a value to estimate the total value for the 3,400 cases. Myself and a colleague both took slightly different approaches. I took the 200 samples and randomly selected 3,400 observations to create a new sample the same size as the population, then got the sum of the 3,400 values. I repeated this 10,000 and took the average and the standard deviation of all 10,000 totals. This gave me an estimate for the total value with a 95% confidence interval. My colleague did almost the exact same thing, but instead of taking a sub-sample of 3,400, his 10,000 subsamples each only had 200 in them. He got the average and standard deviation of the whole lot and multiplied it by 3,400 to get the estimate for the total. When we compared results, we found we got the exact same answer for the estimate - which is good. However, the standard deviation from his method was much bigger. From doing some research it seems like his method of resampling to the same number as the original sample is correct, but can anyone explain exactly why the difference in the standard deviation? The difference got me wondering if this is how we calculate the standard deviation at all. Should we be calculating the standard deviation of each subsample 10,000 times and estimating that the same way as the sum? Also, can anyone point to any resources/tutorials to clear things up? Thanks!
