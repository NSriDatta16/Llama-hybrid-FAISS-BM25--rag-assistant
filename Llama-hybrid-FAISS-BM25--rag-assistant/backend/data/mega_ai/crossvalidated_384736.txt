[site]: crossvalidated
[post_id]: 384736
[parent_id]: 
[tags]: 
On average of percents

I have read some pages about how to calculate the average of percents. However, still I don't understand something... Assume data points are organized in the following manner: We have two categories and each category contains some malware files. We run a virus detection program to see how many files are detected as malware. So, for SET1, there are 30 files and we have detected 25 viruses. The coverage is then 83.3%. The same is true for SET2. items detected percent set1 30 25 83.3% set2 80 40 50% Now, we want to know what is the average coverage for this program. We have two methods: 1- We can say, there are totally 110 files (30+80) and totally 65 files (25+40) are detected. So, the average coverage is 65*100/110 which is 66.6% 2- We can calculate (83.3+50)/2 which is 59%. Which one is more meaningful?
