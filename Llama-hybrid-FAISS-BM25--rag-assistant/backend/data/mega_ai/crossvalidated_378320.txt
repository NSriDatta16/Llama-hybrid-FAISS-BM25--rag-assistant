[site]: crossvalidated
[post_id]: 378320
[parent_id]: 
[tags]: 
Interpretation of learning curve (training & validation)

I know what a learning curve representative of an "ideal" overfitting looks like. However, I am not 100% sure how to interpret the learning curve shown below. Why does the model sometimes seem to "slip" ? It is a small experiment of a binary classification problem with slightly imbalanced classes (2:1 ratio) which was implemented in keras. The loss is calculated as binary crossentropy. I accounted for the imbalance by weighing one class more as described here . P.s.: Not exactly sure whether this post belongs here or rather on data science stackexchange...
