[site]: crossvalidated
[post_id]: 635373
[parent_id]: 
[tags]: 
How to validate unsupervised anomaly detection in absence of ground truth?

I am currently working on an unsupervised anomaly detection project and facing a challenge regarding the validation of the model's performance due to the absence of ground truth labels. I am using Isolation Forest, LOF and Autoencoders for anomaly detection. The dataset comprises very sparse high dimensional tabular data related to Splunk events. Ground truth labels (i.e., known anomalies) are not available in this dataset. What are effective strategies or methods for validating the performance of an unsupervised anomaly detection model in the absence of ground truth? Are there any best practices or commonly accepted metrics that can be used to gauge the reliability of the detected anomalies? I've tried xAI (mainly SHAP) techniques to gain insights on why anomalies are generated and which features contribute to anomalies and also performing statistical tests to compare the anomalies that these models output with allegedly normal data. My primary goal is to establish a credible method for assessing the performance of the anomaly detection model to ensure that it is effectively modeling the data related to normal activity, identifying true anomalies and not just outliers or noise. I appreciate any insights, references, or experiences you can share that might help in addressing this challenge. Thank you!
