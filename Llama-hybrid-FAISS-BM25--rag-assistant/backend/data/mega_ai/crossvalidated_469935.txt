[site]: crossvalidated
[post_id]: 469935
[parent_id]: 
[tags]: 
why $y$ appears in the log-likelihood of logistic regression?

I have a question. In the following text, in the $\pi$ formula why $y$ appears? In a Bayesian framework, we model the parameter Î¸ in the logistic equation as a random variable with a prior distribution $\pi_0$ . Suppose that we observe a set of independent $\{(x_i,y_i)\}_{i=1}^n$ . Define the vector $Y=(y_1,\cdots,y_n)\in \{0,1\}^n$ and let $X$ be the $n\times d$ matrix with $x_i$ as $i^{th}$ -row. We choose the prior $\pi_0$ to be a Gaussian distribution with zero mean and covariance matrix proportional to the inverse of the sample covariance matrix $\sum_X=\frac{1}{n} X^TX$ . Plugging in the formula for the prior and likelihood, we find that the posterior density is given by $\pi(\theta)=\pi(\theta|X,Y)\propto \mathrm{exp}\left\{ Y^TX\theta - \sum_{i=1}^n \log(1+e^{\theta^Tx_i})-\alpha||\sum_X^{1/2}\theta||_2^2\right\}$ It is assumed that the likelihood is logistic regression $p(y=1|x,\theta)=\frac{\exp(\theta^T x)}{1+\exp(\theta^T x)}$ , then by taking logarithm we have $\log p(y=1|x,\theta) = \theta^T x+\log (1+\theta^T x)$ . My question is that how $y$ appears in $Y^TX\theta$ , where it comes from? why it does not appear in $\log (1+\theta^T x)$ ? $\mathrm{exp}\left\{ Y^TX\theta - \sum_{i=1}^n \log(1+e^{\theta^Tx_i})\right \}$ ? https://arxiv.org/pdf/1801.02309.pdf section 4.3 page 21, second paragraph.
