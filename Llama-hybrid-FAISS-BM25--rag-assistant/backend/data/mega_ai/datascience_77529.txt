[site]: datascience
[post_id]: 77529
[parent_id]: 
[tags]: 
Effect of discounting parameter on Language Model Perplexity

The general formula for absolute discounting for calculating language model probabilities subtracts a discounting parameter d from the count of the ngram before calculating the probabilities. The perplexity of the model first decreases as d increases, and then later starts increasing again. What is the reason behind this change? Why does the perplexity decrease first and then start increasing again?
