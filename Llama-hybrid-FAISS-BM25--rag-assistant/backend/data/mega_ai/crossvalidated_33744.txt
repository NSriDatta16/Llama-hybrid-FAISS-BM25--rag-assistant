[site]: crossvalidated
[post_id]: 33744
[parent_id]: 
[tags]: 
Dealing with a partially pooled intercept in a Bayesian model

I have a multilevel logistic regression (Rasch), fit in JAGS. Specifically, $\text{logit} (P(\text{Win})) = \alpha_p + \gamma_w + \delta_{p,w}$ The prior on $\alpha_p$ is partially pooled, $\alpha_p \sim N(\mu_\alpha,\tau_\alpha)$, and the rest of the priors are normal with mean 0. The model "works", i.e., converges over multiple chains, with sensible Gelman Rhat values. Now, for interpretation, I'd like to factor out the intercept. $\text{logit} (P(\text{Win})) = \mu_0 + \alpha_p + \gamma_w + \delta_{p,w}$ If I leave the prior on $\alpha_p$ as is, the model does not converge, presumably because $\mu_0$ is not identifiable with respect to $\mu_\alpha$. The model does converge if I change the prior to $\alpha_p \sim N(0,\tau_\alpha)$. But if I do that, do I still have partial pooling in my model? That is, do the $\alpha_p$ for those $p$ who have few observations still borrow strength from the other $\alpha_p$?
