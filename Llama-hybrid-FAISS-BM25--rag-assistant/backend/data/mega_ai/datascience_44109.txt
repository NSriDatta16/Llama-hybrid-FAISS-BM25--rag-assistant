[site]: datascience
[post_id]: 44109
[parent_id]: 
[tags]: 
Disadvantages of hyperparameter tuning on a random sample of dataset

I often work with very large datasets where it would be impractical to check all relevant combinations of hyperparameters when constructing a machine learning model. I'm considering randomly sampling my dataset and then performing hyperparameter tuning using the sample. Then, I would train/test the model using the full dataset with the chosen hyperparameters. What are the disadvantages of this approach?
