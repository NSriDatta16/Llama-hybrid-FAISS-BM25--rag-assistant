[site]: datascience
[post_id]: 6230
[parent_id]: 6217
[tags]: 
The short answer is yes. Essentially you will be performing some sort of feature engineering. This means constructing a series of functions of your data, often: $\phi_j(x): \mathbb R^p \rightarrow \mathbb R \ ,\ \ j=1,...,K$ Which, strung together, define a transformed data vector $\boldsymbol \phi(x)$ of length $K$. There are a number of ways, better and worse, of doing this. You may want to look up terms like: Splines and generalised additive models. The kernel trick (how to make a model where $K\rightarrow \infty$). Feature engineering (of the manual variety, e.g. adding an $x^2$ column to your data). Deep learning, representation learning As you might guess from such a varied bag of techniques, this is a large area. It goes without saying really but care has to be taken to avoid overfitting. This paper Representation Learning: A Review and New Perspectives deals with some of the issues around what makes a particular set of features 'good', from a deep learning perspective.
