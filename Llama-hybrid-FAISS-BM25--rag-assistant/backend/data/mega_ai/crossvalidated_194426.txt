[site]: crossvalidated
[post_id]: 194426
[parent_id]: 
[tags]: 
Denoising Autoencoder not training properly

I've implemented a denoising autoencoder using TensorFlow. The code is here , there is also a command line script to launch it. The code seems to work, the cross-validation error is decreasing every iteration, but the autoencoder doesn't seem to be learning good features (I'm using MNIST). This is an example of learned features: The parameters I used are the following: --n_components 1000 --batch_size 25 --n_iter 100 --verbose 1 --learning_rate 0.01 --weight_images 0 --corr_type masking --corr_frac 0.5 --encode_valid --enc_act_func sigmoid --dec_act_func sigmoid --loss_func cross_entropy --opt momentum --momentum 0.9 --dropout 0.5 number of hidden units: 1000 batch_size: 25 epochs: 100 learning rate: 0.01 input corruption type and frac: masking 0.5 (set 50% of the pixels to zero) encoder activation function: sigmoid decoder activation function: sigmoid loss function: cross entropy optimizer: momentum, 0.9 encoder layer dropout probability: 0.5 The question is: what is a good choice of the hyperparameters for the MNIST dataset?
