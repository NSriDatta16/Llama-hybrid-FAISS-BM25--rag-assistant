[site]: crossvalidated
[post_id]: 543468
[parent_id]: 
[tags]: 
How many samples does one need to perform polynomial regression of degree $m$?

Suppose $(X_i, Y_i)$ , $i = 1,\dots, n$ are random variables such that $$X_i\sim N(0,1)$$ $$Y_i = f(X_i) + \epsilon_i$$ where the $\epsilon_i$ are i.i.d. standard Gaussian and $f(x)=\sum_{k = 0}^\infty \beta_k x^k$ is an analytic function. For each $n$ and $m , one can perform a multivariate linear regression over polynomials of degree $m$ by minimizing $$\sum_{i = 1}^n\Big(Y_i - \sum_{k = 0}^{m-1} \beta_k X_i^k \Big)^2$$ over $(\beta_0,\dots, \beta_m)\in \mathbb{R}^{m}$ and get a polynomial estimator $\hat{p}^{(m)}(x) = \sum_{k = 0}^{m-1} \hat{\beta_k} x^k.$ I am curious what is known about the behavior of, say, $\mathbb{E}[|f(Z)-\hat{p}^{(m)}(Z)|^2]$ where $Z$ is an independent Gaussian as $m,n\to \infty$ . (Or some other metric of average distance between $f$ and the estimator and $\hat{p}^{(m)}$ .) When $m=n-1$ I would expect the estimation to break down because of noise. But, as $n\to \infty$ with $m$ fixed, I would expect to get some limiting best fit polynomial of degree $m$ to $f$ . If we want to $m\to \infty$ can one find an explicit sequence $n = n(m)$ (in terms of, say, some generic property about the function $f$ ) so that the error goes to zero? In other words, how many samples should one have in order to succesfully do polynomial regression up to degree $m$ ? What happens if instead of using the basis $\{1,x,x^2,\dots\}$ one takes some other basis $\{\phi_0,\phi_1,\dots\}$ for continuous functions $\phi_i$ ?
