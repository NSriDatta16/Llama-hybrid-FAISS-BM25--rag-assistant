[site]: crossvalidated
[post_id]: 628922
[parent_id]: 
[tags]: 
Hypothesis testing for k-times-k-fold cross validation

I've done 10-times-10-fold cv over 4 datasets for 2 different machine learning classification models. So I obtain 100 performance metrics(eg. balanced accuracy) for each dataset. I would like to statistically test if the difference in performance between models is significant across all datasets. I would usually do an ANOVA with independent variables: model (2 levels) and dataset (4 levels); dependent variable being the performance metric. But I can already see there is a complication with the process that generates the data, as this cv produces not completely independent (overlapping) measures. My objective is to prove that 1 model outperforms the other across all datasets. Any idea of how could I proceed? Also references and resources would be greatly appreciated. Cheers.
