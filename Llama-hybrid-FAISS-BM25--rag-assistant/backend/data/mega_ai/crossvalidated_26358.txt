[site]: crossvalidated
[post_id]: 26358
[parent_id]: 24527
[tags]: 
I'll address your question about the amount of training data. You seem to have a 2 or 3 class classification problem, does this instance of data belong to pothole, speedbump or perhaps normal road. Given a finite number of samples, you want to have an estimate of the accuracy of your prediction algorithm. To make this estimate you need to reserve some samples that your algorithm has not been trained on and on which you can measure the accuracy of the classification algorithm you have produced, this is commonly known as the test set. The more data you use for training the less is available for testing the algorithm's accuracy. Consequently variance of the estimate of accuracy on a small test set will potentially be quite high. In general, a single partition of the data into training and test sets is unlikely to give an unbiased estimate of accuracy. This is a well trodden path so some standard approaches for producing more reliable estimates exist. Probably the two most popular are n-fold cross validation, where the whole dataset is partitioned into n disjoint training and test sets. Models are then trained on the training data and tested on the test data for each of the n partitions. Typically n is set to 10, the resulting accuracies from the n trials are then average to produce an overall accuracy figure. Cross Validation . The second method is repeated hold out, where all the data is randomly partitioned in to two parts, typically 50%/50%, models trained and then tested. Then the resulting accuracies are averaged. If you are't familiar with them already, toolkits like WEKA or R will have methods for estimating these accuracies and for building neural network and other models ready for you to just use out of the box. Once you've decided on you're approach to subsampling the time series data you might want to consider comparing performance of other modeling approaches such as random forests or Generalised Linear Models ( GLMs ) which require very little tuning in comparison to neural networks and can give very good performance. A toolkit such as WEKA will make this very easy to experiment with alternate algorithms. WEKA . I'm not sure if the other samples in your data support these observations, but one would imagine that a pothole would be characterised by a sharp negative z axis acceleration ( z-axis aligned perpendicular to the road surface )followed by a sharp positive z axis acceleration with little time inbetween. A speed bump would be the reverse, sharp +ve z axis followed by sharp -ve z-axis acceleration, with probably a slightly longer duration between the 2 events and a less steep acceleration profile. For the speed bump, the timing of the accelerations will be dependent on the profile of the bump, some have long plateaus, do you have information on the type of humps the data is from? As you suggest there may well be an accompanying deceleration (in the y-axis, orientation front to back of car ? ) in the case of the speed hump as the driver may have been aware of it and slowed accordingly. This is less likely in the case of a pot hole, but not unknown. In selecting the region of the time series to use for training you might want to combine the likelihood of the driver decelerating with the z-axis acceleration profiles for the two types of road obstruction to help you choose good regions. It seems likely that framing the correct samples consistently will significantly increase accuracy.
