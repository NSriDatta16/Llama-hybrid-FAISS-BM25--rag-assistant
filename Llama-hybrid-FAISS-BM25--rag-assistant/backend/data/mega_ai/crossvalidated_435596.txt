[site]: crossvalidated
[post_id]: 435596
[parent_id]: 435557
[tags]: 
There is a saying in computer science that the hardest computer language to learn is your second one. I believe that is true in statistics too. The most challenging probability method to learn is your second. Let’s split these problems apart into a null hypothesis solution and a Bayesian method and, instead, we will try and mentally work through what the two types of tools are trying to do. Let’s begin with the method of maximum likelihood and with the minimum variance unbiased estimator. For the binomial, the MLE is also the MVUE. I bring this up because it will create a potential issue with how we think about the Bayesian problem. If you are randomizing the sample size, then neither method you are using is appropriate because the choice of sample size is also a random process with a parameter that needs to be estimated. You are adding a gigantic problem into what you are doing called “optional stopping.” You do not want to do that. If you add randomness anywhere into a process that would not have that randomness, then you need to account for that in some way. For both cases, we are going to fix the sample size at 10 each. We are also going to imagine four parallel universes out of the infinite number possible in the parameter space of $\Theta=[0,1]$ . While the integrations will consider all possible worlds, it is impossible to visualize an infinite number of options. In universe one, $\theta=.5$ and successes equal 5; in universe two, $\theta=.41$ and successes equal 5; in universe three $\theta=.5$ and successes equal four; while in universe four, $\theta=.41$ and success equal four. In all universes, you believe that the coin is roughly fair, and on the Bayesian calculations, you assign a prior distribution of $\Pr(\theta)=B(2,2)$ . What you are looking at is not a graph of heads or tails, but of possible parameter locations. This is not about successes or failures, it is about parameter location. What you are doing is underweighting any world that is distant from .5 if you are really in one of those worlds. In all worlds, this graph is your belief system before seeing any data. In universe one, the true long-run mass function for ten draws is as follows. Five successes were observed. $\hat{\theta}_{MLE/MVUE}=.5$ Your 95% confidence interval is $[.19,.81]$ . If you would “plug-in” the parameter estimate as if true, then your mass would match the true long-run distribution below. There are two logical Bayesian point estimators based on your use of the MLE or the MVUE. The MLE minimizes the all-or-nothing loss function, which would map to the Bayesian posterior mode if the loss function were preserved. The MVUE minimizes quadratic loss. The Bayesian mapping of that loss function is the posterior mean. The split in the Bayesian result will require you to choose a loss function if you need a point estimator of $\theta$ . The Bayesian posterior mean is .5, and the posterior mode is .5. While the Frequentist confidence interval depends on the type of point estimator that you choose, the Bayesian one does not. The Bayesian interval is $[.251,.749]$ . The Bayesian density function for your beliefs is graphed below. In world two, the true long-run mass function is below. Nonetheless, because the sample is the same, the parameter estimates remain the same as do the intervals and the graphs. This is a good place to look at the Bayesian predictive distribution, which is what you were trying for above. It is the predicted number of successes based on the data and prior. It is below. The prediction isn’t considering a point estimate as valid. The prediction is the weighted average binomial distribution over the set of all possible parameter values weighted by the posterior. Note the density values on the left. The Bayesian prediction is far more agnostic about the future than is the one created from plugging in the MLE/MVUE. The Bayesian estimator is still giving weight to unlikely parameter values. It is possible that the actual value of the parameter is .6 with a 5:5 success to failure ratio. The Bayesian estimate converges to the asymptotic distribution as the sample size goes to infinity. Now let us consider the universes where we see four successes. The Frequentist point estimator is $\hat{\theta}_{MLE/MVU}=.4$ with a 95% confidence interval based on the normal approximation of $[.0964,.7036]$ . The Bayesian posterior mean is .4286, while the posterior mode is .4167. The Bayesian 95% highest density region credible interval is $[.192,.684]$ . The graph of your new beliefs is below. Note that the mode is not co-located with the MLE/MVUE. You can see that in the graph of posterior beliefs. The Bayesian predictive distribution is below. Now let’s summarize what is above. The Frequentist binomial distribution is valid only when $\frac{k}{n}=\theta$ although it will tend to be close. Assuming that the estimator is also the real value creates steep predictions. What if the actual value had been $\theta=.5$ , but there had been 9 observations. It would be impossible for the Frequentist plot to match reality. The Bayesian strength is the Bayesian weakness. If your beliefs about reality are actually stated in the prior and you have reasonable beliefs, then you will get gains in estimation because it improves precision. If your beliefs are nonsense, then your posterior will be, at least in part, based on nonsense. What if $\theta=.9$ , then it would take quite a bit of time to learn the true value of the parameter. If you saw 9 successes, your posterior beliefs would be in the graph below. The graph falls below 0 because of the smoothing method I used in geom_smooth. Sorry, it is a bit of an effort to fix it so I left it. The Bayesian prediction on the ten next values is below. The weakness of a Frequentist method is if there are zero or ten successes. For zero successes, the estimator is zero. The interval is the point containing zero. The prediction would be that every single value will be failures. It gives no weight at all to any other possible parameter value.
