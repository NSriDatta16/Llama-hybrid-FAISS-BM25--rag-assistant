[site]: crossvalidated
[post_id]: 568953
[parent_id]: 568938
[tags]: 
The generator of a GAN learns to create images that the discriminator cannot distinguish from real data. Mode collapse just means that the generator, unfortunately, creates only images that are equal to a small subset of the real data. But the generated data is still very similar to some real data. But the phenomenon you are describing cannot be explained like that: the discriminator checks the generated image for similarity with some real images, it does not check parts of the generated image for similarity with other parts of the same image. Replicating patches within the same image would not result in images that equal some of your real images, so the discriminator would flag them as fake. So unless you happen to have such images with replicating subregions in your dataset of real images, I am afraid it is probably rather a bug :(
