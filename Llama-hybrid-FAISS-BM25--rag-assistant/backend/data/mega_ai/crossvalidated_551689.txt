[site]: crossvalidated
[post_id]: 551689
[parent_id]: 551679
[tags]: 
The sparcl has the KMeansSparseCluster.permute function for choosing the wbounds , see its help page for an example how to use this. If I understand their paper well (where this is called $s$ ), the idea is the following. In principle they want to optimise the weighted k-means objective function. If the $s$ increases, the objective function will get better, but they have less sparsity. They want a good value of the objective function but still sparsity, so they've got to find a compromise. Now in order to assess the quality of a value of $s$ for finding an "optimum", they compute the solution for the given $s$ , take a logarithm of the objective function, and compare it with the average logarithm of the objective function that they get when randomly and independently permuting the values on all variables (the logarithm is chosen because it scales better over different values of $s$ ; the plain objective function is bounded by zero, and values close to zero - and their differences - don't compare well with larger values). They then select $s$ so that the difference between these two is maximised (there is also an alternative suggestion to choose the smallest $s$ that is within one standard deviation of the best value; in fact I think one could even choose two standard deviations there, as the logic comes from the gap statistic, where personal experiments led me to believe that using 2 standard errors is probably good). The rationale is that the permuted data sets are meant to give information about how log of the objective function is expected to behave if there is no meaningful clustering structure in the data. If an $s$ under investigation achieves a much better value on the original data set than on the permuted versions, this indicates that this $s$ is connected to some meaningful structure. You may ask: Why are permutations of the original data suitable for this? Indeed there may be problems with this. If the values of one variable are strongly clustered on their own, they will still be strongly clustered in the permuted version. If there are not many variables, this may still bring forth a meaningful clustering, so permutation does not necessarily remove all clustering structure. The idea is probably (this isn't discussed in much detail in the paper) that if clustering structure in the original data is supported by a number of correlated variables, the permutation will destroy that and therefore serves as a benchmark. If an $s$ does much better on the original data, it would indicate that such structure is successfully preserved with that $s$ . The authors also write: "the performance of the gap statistic of Section 3.2 for selecting the tuning parameter is mixed. This is not surprising, since tuning parameter selection in the unsupervised setting is known to be a very difficult problem. This is an area in which more work is needed." I'm not aware of anybody who has done this work though. PS: I haven't read Kondo's paper on which RSKC is based, but it seems that the L1 parameter corresponds to the $s$ above. The alpha parameter is a trimming rate, which does something different. This is discussed in the robust clustering literature and there is no automatic way to choose it. People in robust clustering will plot the objective function against various choices of alpha to see whether a certain value sticks out. A shortcut is to choose alpha as the biggest proportion of data that if observed close together would not count as "cluster" but rather as a "bunch of outliers", or in other words a little bit smaller than the smallest size (proportion) for a cluster to be meaningful, in order to avoid that complete meaningful clusters are trimmed. This has to be decided on subject matter grounds. Unfortunately, unless you choose alpha=0 , the KMeansSparseCluster.permute function won't work to choose L1 .
