[site]: crossvalidated
[post_id]: 443035
[parent_id]: 442799
[tags]: 
No, these are just rules of thumb (dictionary definition: not intended to be strictly accurate or reliable for every situation) for designing a neural network. Quoting from the neural-nets FAQ , These rules of thumb are nonsense because they ignore the number of training cases, the amount of noise in the targets, and the complexity of the function. Even if you restrict consideration to minimizing training error on data with lots of training cases and no noise, it is easy to construct counterexamples that disprove these rules of thumb. You must experiment yourself with different number of hidden layers, and number of neurons in them pertaining to your problem statement. ( A starting guide ) Take for example, autoencoders . A type of autoencoder, called undercomplete autoencoder, uses fewer hidden neurons compared to the input neurons. The sparse autoencoder include more hidden neurons than inputs, but only a small number of the hidden neurons are allowed to be active at once.
