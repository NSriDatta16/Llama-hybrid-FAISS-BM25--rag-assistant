[site]: crossvalidated
[post_id]: 559366
[parent_id]: 
[tags]: 
Understanding Dickey-Fuller Test vs. t-test

I am working to understand why it is that in an AR(1) regression, the regression coefficient is not asymptotically t-distributed. Specifically, I'm trying to understand which assumptions about a regular cross-sectional OLS are violated. I have seen a couple questions on here which try to get at this, but none which specifically go through the formulation of the simple linear regression and show where things break down. To better articulate my question, I will (1) state what I believe to be is the setup for the Dickey-Fuller test, then (2) outline the derivation for a t-distributed regression coefficient in OLS. What I am trying to reconcile is what goes wrong in (2) when doing a regression on an AR(1) process. I would really appreciate both a rigorous and intuitive explanation of this. (1) Setup for Dickey-Fuller's Test We have an AR(1) process given by $$x_t = \beta_1 + \rho x_{t-1} + \epsilon_t$$ where $\epsilon_t \sim \mathcal{N}(0,\sigma^2)$ . We want to test whether $H_0: \rho = 1$ or $H_1: \rho . Letting $\beta_2 := \rho-1$ and $\Delta x_t:= x_t-x_{t-1}$ , we have $$ \Delta x_t = \beta_1 + \beta_2 x_{t-1} + \epsilon_t$$ and this is equivalent to testing $H_0: \beta_2 = 0$ or $H_1: \beta_2 . Our OLS estimator $\hat \beta$ is in general given by $$ \hat \beta = (X^TX)^{-1}X^TY$$ and our t-statistic is given by $$T:= \frac{\hat \beta_2}{SE(\hat \beta_2)}$$ And it turns out that $T$ is not t-distributed as in a normal OLS. So we use a Dickey-Fuller table to compute critical values. This is confusing to me because I am not quite sure what goes wrong with the below formulation of the t-distributed OLS estimator. (2) Simple Linear Regression The procedure here loosely comes from Keener's Theoretical Statistics (chapter 14). In a simple linear regression, we assume a model $$y_i = \beta_1 + \beta_2(x_i - \overline{x}) + \epsilon_i$$ where the independent variables $x_1,...,x_n$ with average $\overline{x}$ are taken to be known constants , $\beta_1$ and $\beta_2$ are unknown parameters, and $\epsilon_1,...,\epsilon_n$ are i.i.d. from $\mathcal{N}(0,\sigma^2)$ . This gives a general linear model with design matrix $$ X = \begin{pmatrix} 1 & x_1-\overline{x}\\ \vdots & \vdots \\ 1 & x_n - \overline{x}\\ \end{pmatrix}$$ Note that the two columns of $X$ are orthogonal. So $$ X^TX = \begin{pmatrix} n & 0 \\ 0 & \sum_{i=1}^n (x_i-\overline{x})^2\\ \end{pmatrix} $$ and hence $$ (X^TX)^{-1} = \begin{pmatrix} 1/n & 0 \\ 0 & 1 / \sum_{i=1}^n (x_i-\overline{x})^2 \end{pmatrix} $$ and so our OLS estimator $\hat \beta$ is given by $$ \hat \beta = (X^TX)^{-1}X^TY = \begin{pmatrix} \frac{1}{n}\sum_{i=1}^n y_i\\ \sum_{i=1}^ny_i(x_i-\overline{x}) / \sum_{i=1}^n (x_i - \overline{x})^2 \end{pmatrix} $$ Moreover, we have that the variance of our estimator $\hat \beta$ is given by $$ \text{Cov}(\hat \beta) = \sigma^2 (X^TX)^{-1} = \begin{pmatrix} \sigma^2 / n & 0 \\ 0 & \sigma^2 / \sum_{i=1}^n(x_i - \overline{x})^2 \end{pmatrix} $$ To estimate $\sigma^2$ , because $\hat y_i = \hat \beta_1 + \hat \beta_2 ( x_i - \overline{x})$ , our error terms are $$ e_i = y_i - \hat \beta_1 - \hat\beta_2 (x_i - \overline{x}) $$ so our unbiased estimate of $\sigma^2$ is given by $$ S^2 = \frac{1}{n-2} \sum_{j=1}^ne_j^2 $$ Moreover, $(n-2)S^2/\sigma^2 \sim \chi^2_{n-2}$ by the definition of the chi-squared distribution. Next, our estimator $\hat \beta$ is an unbiased estimator is normal with expectation $\beta$ (because $\mathbb{E}(X^TX)^{-1}X^TY =\mathbb{E}(X^TX)^{-1}X^T(X\beta + \epsilon) = \beta$ ). To solve for the variance of our estimator, we have $$ \begin{split} \text{Var}(\hat \beta) &= \text{Var}((X^TX)^{-1}X^TY) \\ &= (X^TX)^{-1}X^T\text{Var}(Y)X(X^TX)^{-1}\\ &= (X^TX)^{-1}\sigma^2 \end{split} $$ and so $$\hat \beta \sim \mathcal{N}(\beta, (X^TX)^{-1}\sigma^2)$$ Define $M:= (X^TX)^{-1}$ for notational convenience. In particular, we have that $$ \frac{\hat \beta_j - \beta_j}{\sigma\sqrt{M_{j,j}}} \sim \mathcal{N}(0,1) $$ This variable is independent of $(n-2)S^2/\sigma^2$ because $S^2$ and $X^T\hat \beta$ are uncorrelated by design. Hence by the definition of a t-distribution we should have $$ \frac{\hat \beta_j - \beta_j}{S\sqrt{M_{j,j}}} \sim t_{n-2} $$ i.e. our estimate of the coefficient $\beta_j$ should be t-distributed. What about this process is not right for the situation in the AR(1) process?
