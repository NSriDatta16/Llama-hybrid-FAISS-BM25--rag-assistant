[site]: crossvalidated
[post_id]: 101128
[parent_id]: 101003
[tags]: 
As @Frank Harrell already mentioned, using elastic net or LASSO to perform penalized regression with all 5000 features ( p ) would be a good start for feature selection (one can't simply remove 3500 variables because they are not "statistically significant" with the dependent variable of interest). Either of these methods can be performed using the R package, glmnet . In order to take into account the relationships shared between the potential predictor variables of interest ( p = 5000), I would recommend running a random forest using the randomForest package and/or gradient boosting using the gbm package to assess the relative importance of the potential predictor variables in regards to the binary outcome. With this information, you will be much more prepared to build a more parsimonious logistic regression model.
