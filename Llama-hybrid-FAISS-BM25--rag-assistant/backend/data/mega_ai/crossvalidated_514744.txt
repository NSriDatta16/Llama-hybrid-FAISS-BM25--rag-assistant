[site]: crossvalidated
[post_id]: 514744
[parent_id]: 514733
[tags]: 
The sigmoid function $\sigma(x)=\frac{1}{1 + \exp(-x)}$ has the property $\sigma(0)=0.5$ . The sigmoid function is also monotonic increasing. Together, these facts imply that $\sigma(x) whenever $x and $\sigma(x)> 0$ whenever $x>0$ . If all you're interested in doing is classifying according to the sign of some part of the network, then you can train with the sigmoid activation and the ordinary cross-entropy loss, but predict using the value of $x$ instead of $\sigma(x)$ . In other words, just skip $\sigma$ for making predictions. Due to the connection to logistic regression, this is the same as making predictions using the logit scale instead of the probability scale.
