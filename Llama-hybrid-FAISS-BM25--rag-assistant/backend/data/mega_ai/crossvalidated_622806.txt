[site]: crossvalidated
[post_id]: 622806
[parent_id]: 
[tags]: 
How to choose the potential hyperparameters for GridSearchCV on RandomForestClassifier? Will default always be the best?

I'm fairly new to machine learning, and I know similar questions have been asked but I can't find an answer that satisfies my curiosity. I'm working on a Random Forest Classifier model in python, and I am trying to use GridSearchCV to tune the hyperparameters. At first I ran the classifier class with default params and got a score on the test data of 0.890, about equal to the OOB score. I was pretty happy with that result but wanted to go ahead with hyperparameter tuning anyway. I used GridSearchCV with the following dictionary: {'criterion':['gini', 'entropy'], 'class_weight':[None, 'balanced'], 'min_samples_split':[12, 30, 48], 'max_depth': [x for x in range(5,11)], 'min_samples_leaf': [x for x in range(3, 19, 3)]} But ultimately, the best score from this grid was only 0.851, and the best estimator used all the parameters at their extremes: min_samples_split=12, max_depth=10, min_samples_leaf=3 . So in retrospect, I'm thinking "duh" because it makes perfect sense that the best model would go to a greater depth, have the fewest samples per leaf, etc. But it leaves me confused about tuning parameters as a concept. Will this always be true that the less limited a model is, the better it performs? Did I pick reasonable parameter options? Is there a problem with using default parameters? I know it can be costly with really large data, but the df I'm working with only has ~40k entries (train and test combined), so is that okay?
