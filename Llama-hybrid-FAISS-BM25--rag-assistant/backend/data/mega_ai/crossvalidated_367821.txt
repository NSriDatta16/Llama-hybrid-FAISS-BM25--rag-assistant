[site]: crossvalidated
[post_id]: 367821
[parent_id]: 
[tags]: 
How are mean results on benchmark obtained when training neural networks?

In most neural network papers, networks are trained on a known database where state-of-the-art performance is known ("benchmark"). Whatever metric is chosen to illustrate the network's performance, often a single number is given, e.g. 95.6%. However, neural networks have some stochasticity because of their initialisation. Different runs will give different results, and the metric can vary in value much more than the significative digits provided (e.g. one run will yield 94.3%, the next 95.7%, etc). How to adress this variability when presenting results ? For the moment, I'm running X runs and averaging the score, but I didn't see any mention of such a process in most neural networks papers I read. Some papers mention cross-validation, which is sort of running X times a run, except it's more to average variability over train/test split than the random initialisation one. I feel like there is a common pratice I might be missing.
