[site]: crossvalidated
[post_id]: 224777
[parent_id]: 
[tags]: 
Are there any models that can handle out of sample features?

So I'm facing a regression problem where I have a categorical features (factor) where the levels are very commonly different between the training set and the test set. I have multiple measurements per subject, where the context where the measurement was made changes constantly and is represented by the current_context variable. I would like to measure how the context affects the measurement on a per subject basis, and try to predict new values for the response, given the context and other, fixed, predictors. Example: Training: subject_id current_context response 1 C15 55 1 D46 44 1 A416 99 ... 10^6 G426 44 10^6 G426 44 Test: subject_id current_context response 1 C15 65 1 D55 55 1 E416 101 ... 10^6 G426 44 10^6 E417 65 One solution for this would be that I could create different models (kind of like backoff in NLP) and fall back to simpler models until all the features are available. In the example I would have one model trained on all the features, and two more trained only on one feature at a time. But that's an engineering overhead I would like to avoid. Another solution as suggested is to use hierarchical modeling. This is probably impossible in my case, as I would like to have the subject_id as a random effect, which could be in the millions and that doesn't scale AFAIK. Another problem I've noticed when trying that is that because the current_context can also have hundreds of thousands of values, it leads to computational issues as well (I get the error that linpack cannot create a matrix of adequate size) So I was wondering if there are (regression) models that have this kind of robustness built in, i.e. are able to produce estimates for the dependent even if some of the features used during training are missing, or if new levels appear in factor.
