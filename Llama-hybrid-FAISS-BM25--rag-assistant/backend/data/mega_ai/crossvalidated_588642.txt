[site]: crossvalidated
[post_id]: 588642
[parent_id]: 
[tags]: 
Priors that do not become irrelevant with large sample sizes

This may be a weird question. My colleagues and I are working on a medical estimation problem, where relevant prior knowledge regarding plausible values of some physiological parameters exists. In addition, these parameters can be estimated using time series data, which often have some tens of thousands of samples. What often happens is that, due to model imperfections, MAP estimation converges towards implausible solutions (e.g., very small parameter values). The priors are essentially ignored because they become irrelevant given the large amount of available (and informative) measurements . Now I am well aware that one way (probably the preferred one) to solve this problem is to improve the time series model and try to fix its imperfections. This is proving to be really hard, however, for a number of reasons. Another (partial) remedy might be to somehow "fix" the influence of the prior such that it does not become irrelevant. This could be done, e.g., by choosing the prior's covariance as a function of the sample size or simply assigning the prior and the likelihood fixed weights when determining final parameter estimates. That got me wondering: is this "a thing"? Is there a name for doing something like this, i.e., a "fixed-influence prior"? Surely much more knowledgeable people than me have thought about this problem.
