[site]: crossvalidated
[post_id]: 579007
[parent_id]: 
[tags]: 
Is it o.k. to stack out-of-sample predictions from separate cross-validation rounds?

I am applying Lasso regression and the R function glmnet::cv.glmnet() to obtain a prediction model based on 90% of the data. I have set aside 10% as a hold-out set and obtain predicted probabilities for the hold-out set. I repeat this for each 90%-10% split of my data (10-fold cross validation). This gives me a predicted value for each observation in my data s.t. each predicted value is obtained from a model fit using training data where that observation was not included. I run a 10-fold cross-validation 20 times to obtain 20 out-of sample predicted values for each observation. Is it o.k. to plot the prediction errors corresponding to these 20 vectors (or averages and sd's of the 20 predictions for each observation) against some of the feature variables, for example age, to demonstrate the expected predictive performance of my model for new data corresponding to persons of different ages? This could reveal that predictions are more uncertain for old people. I would find this extremely useful, but have I not seen such plots in articles. Is it possible that I am ignoring some subtle issues if I combine predictions from several separate cross-validation rounds and stack them together (as described above) to form a full set of predictions, i.e. a vector of predicted values that is of same length as number of observations in data?
