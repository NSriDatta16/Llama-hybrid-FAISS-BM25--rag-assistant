[site]: datascience
[post_id]: 8467
[parent_id]: 8087
[tags]: 
First, thanks for the edits to your original question since we now know that you are applying the same transformation to all of your data. Q: Why do perceptrons perform so much better than generalized linear models for some problems? A: Because they are inherently nonlinear models, with a great deal of flexibility. The drawback is that the additional knobs require more data to correctly tune. Big picture: Less data can cause high-bias . High-bias can be overcome by more data. You have reduced your data from a 4800 feature dataset to a 38 feature dataset, so should expect to see increased bias. Neural networks require more data than models without hidden layers. Linearity vs. Nonlinearity Your artificial neural network ( perceptron ) is an inherently nonlinear model, yet you are deciding to remove features from your dataset with a linear model ( PCA ). The presence of a single hidden layer explicitly creates second order terms in your data, and there are two additional nonlinear transformations (input==>hidden and hidden==>prediction), which add sigmoidal nonlinearity at each step. The fact that the training data and the target data are both multiplied by the same matrix to achieve the reduced dimensionality really just means that your perceptron needs to learn the matrix if it wants to reconstruct nonlinear aspects of the original data. This requires more data or for you to reduce less of the dimensionality. An Experiment: I suggest trying an experiment where you perform second order polynomial feature extraction on the full dataset and then performing PCA on that enhanced dataset. See how many features you end up with while retaining 99% of the variance of the enhanced dataset. If it is larger than the dimensionality of your initial dataset then stick with the un-enhanced and un-reduced dataset. If it is betweeen the dimensionality of the original data and 38 then try training the perceptron with that data. A better idea: Rather than using the (linear) variance to determine the feature reduction of your PCA projection, try training and cross validating your model with different amounts of PCA dimensionality reduction. You will likely find that there is a sweet spot for a given set of images. For instance, an SVM on the MNIST digit data performs best when the 784 pixel features are reduced down to 50 linearly independent features using PCA. Though this is not obvious from analyzing the variance of the principal components. Other options: There are nonlinear dimensionality reduction techniques, such as isomap . You could investigate the use of nonlinear feature reduction since you are clearly loosing information with the linear PCA that you have applied. You could also look into image specific feature extraction techniques to add some nonlinearity before reducing the dimensionality. Hope this helps!
