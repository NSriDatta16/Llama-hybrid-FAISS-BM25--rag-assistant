[site]: crossvalidated
[post_id]: 585928
[parent_id]: 152897
[tags]: 
as was said in comments @ttnphns: "Dot product and projection are not quite identical." – kernel trick is utilized in computations involving the dot products (x, y). kernel itself programmically is just a core of a computer's OS, that controls & distributes "time" & "power" between CPU, RAM, Devices & etc. projection is builded Feature Map, as I understand. In ML (SVM) Kernel replaces Euclidean Inner Product, that is the measure of similarity/dissimilarity. SVM maximizes the distance between closiest points while learning the Decision Boundary - the distance that is Euclidean distance here in Euclidean space. As so as space can be multidimensional => linear models don't always help (Lnear Regression & PCA), non-linearity (e.g. with cos in geometrical space) is added in calculations to add Geometrically-Weighted-Transformations. e.g. compare Linear Kernel vs. Cosine Kernel Inner_Product : This is called cosine similarity, because Euclidean (L2) normalization projects the vectors onto the unit sphere, and their dot product is then the cosine of the angle between the points denoted by the vectors. (or mathematically & ststistically - here - for the Cosine kernel defined by the pdf f(x)=(π/4)cos(xπ/2)) Important : see here Types of SVM kernels Think of kernels as definned filters each for their own specific usecases. 1) Polynomial Kernels (used for image processing) 2) Gaussian Kernel (When there is no prior knowledge for data) 3) Gaussian Radial Basis Function(same as 2) 4) Laplace RBF Kernel ( recommend for higher training set more than million) 5) Hyperbolic Tangent Kernel (neural network based kernel) 6) Sigmoid Kernel(proxy for Neural network) 7) Anova Radial Basis Kernel (for Regression Problems) be carefull: space can be non-Euclidean or pseudo-Euclidean or etc... your choice depends on the objectives of your research task, I think P.S. So, defining either kernel (more precise here - certain kernel- trick ) to use it in algorithm - you simply define certain type of decomposition to matrices & approximation-type for the learning, But as so as this essentially leads to certain working load for CPU & RAM (as so as the learning process is rather hardware resources consuming) - they called the parameter simply "kernel". (I think, it really is as simple as just the name meaning the direction of pc-resources distribution for certain type of derivation/approximation) - some answers here are helpful to distinguish OS-kernel & ML-kernel -- terms from different fields, but I think, are meaningfull in understanding how to calculate huge matrices rapidly (async), not waiting in the queue in iterating process (don't know how operational_system cope with recursion to improve memory/speed balance) P.P.S. can see here a number of approaches where kernels are useful. P.P.P.S. by the way, analysing data in time - Weighted Geometric Mean is often used
