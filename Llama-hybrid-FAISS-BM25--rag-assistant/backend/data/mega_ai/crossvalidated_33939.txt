[site]: crossvalidated
[post_id]: 33939
[parent_id]: 24324
[tags]: 
Comparing the means is too weak: instead, compare the distributions. There is also a question concerning whether it is more desirable to compare the sizes of the residuals (as stated) or to compare the residuals themselves. Therefore, I evaluate both. To be specific about what is meant, here is some R code to compare $(x,y)$ data (given in parallel arrays x and y ) by regressing $y$ on $x$, dividing the residuals into three groups by cutting them below quantile $q_0$ and above quantile $q_1\gt q_0$, and (by means of a qq plot) comparing the distributions of $x$ values associated with those two groups. test The fifth argument to this function, abs0 , by default uses the sizes (absolute values) of the residuals to form the groups. Later we can replace that by a function that uses the residuals themselves. Residuals are used to detect many things: outliers, possible correlations with exogenous variables, goodness of fit, and homoscedasticity. Outliers, by their nature, should be few and isolated and thus are not going to play a meaningful role here. To keep this analysis simple, let's explore the last two: goodness of fit (that is, linearity of the $x$-$y$ relationship) and homoscedasticity (that is, constancy of the size of the residuals). We can do this through simulation: simulate r1], res[res.abs > r1], col="Red") plot(x,x, main="QQ Plot of X", xlab="Low residual", ylab="High residual", type="n") abline(0,1, col="Red", lwd=2) temp This code accepts arguments determining the linear model: its coefficients $y \sim \beta_0 + \beta_1 x + \beta_2 x^2$, the standard deviations of the error terms sd , the quantiles $q_0$ and $q_1$, the size function abs0 , and the number of independent trials in the simulation, n.trials . The first argument n is the amount of data to simulate in each trial. It produces a set of plots--of the $(x,y)$ data, of their residuals, and qq plots of multiple trials--to help us understand how the proposed tests work for a given model (as determined by n , the beta,s and sd ). Examples of these plots appear below. Let us now use these tools to explore some realistic combinations of nonlinearity and heteroscedasticity, using the absolute values of the residuals: n The output is a set of plots. The top row shows one simulated dataset, the second row shows a scatterplot of its residuals against $x$ (color-coded by quantile: red for large values, blue for small values, gray for any intermediate values not further used), and the third row shows the qq plots for all trials, with the qq plot for the one simulated dataset shown in black. An individual qq plot compares the $x$ values associated with high residuals to the $x$ values associated with low residuals; after many trials, a gray envelope of likely qq plots emerges. We are interested in how, and how strongly, these envelopes vary with departures from the basic linear model: strong variation implies good discrimination. The differences between the last three and the first columns make it clear this method is able to detect heteroscedasticity, but it might not be so effective in identifying a moderate nonlinearity. It could easily confuse the nonlinearity with heteroscedasticity. This is because the form of heteroscedasticity simulated here (which is common) is one where the expected sizes of the residuals trend with $x$. That trend is easy to detect. Quadratic nonlinearity, on the other hand, will create large residuals at both ends and in the middle of the range of $x$ values. This is hard to distinguish just by looking at the distributions of the affected $x$ values. Let's do the same thing, using exactly the same data , but analyzing the residuals themselves. To do this, the previous block of code was rerun after making this modification: size This variation does not detect heteroscedasticity well: see how similar the qq plots are in the first two columns. However, it does a good job of detecting nonlinearity. This is because the residuals separate the $x$'s into a middle portion and an outer portion, which will be fairly different. As shown in the rightmost column, however, heteroscedasticity can mask nonlinearities. Perhaps combining both these techniques would work. These simulations (and variations of them, which the interested reader can run at leisure) demonstrate that these techniques are not without merit. In general, however, one is far better served by examining the residuals in standard ways. For automated work, formal tests have been developed to detect the kinds of things we look for in residual plots. For instance, the Breusch-Pagan test regresses the squared residuals (rather than their absolute values) against $x$. The tests proposed in this question can be understood in the same spirit. However, by binning the data into just two groups and thereby neglecting most of the bivariate information afforded by the $(x, \hat{y}-x)$ pairs, we can expect the proposed tests to be less powerful than regression-based tests like the Breusch-Pagan .
