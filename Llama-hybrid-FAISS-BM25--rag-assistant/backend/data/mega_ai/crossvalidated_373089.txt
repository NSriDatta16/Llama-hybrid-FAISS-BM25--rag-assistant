[site]: crossvalidated
[post_id]: 373089
[parent_id]: 373067
[tags]: 
Here is a simple example to start. Suppose I have data on a process that is normally distributed with $\mu = 100$ and $\sigma = 10$ when everything is running properly. But for unknown reasons the process goes briefly 'out of control' about 5% of the time, and when that happens values have $\mu_b = 300$ and $\sigma_b = 20.$ Here is a histogram of such data: Mean and Median. If I find the descriptive statistics of all the data (in and out of control), then I get: summary(x) Min. 1st Qu. Median Mean 3rd Qu. Max. 57.71 93.72 100.50 110.01 108.00 366.95 In particular, the sample mean $\bar X = 110.01$ has been pulled up above the sample median $H = 100.50.$ So, If I want an average of values from the entire process (including the bad values), it seems best to look at $\bar X = 110.01.$ But the median 100.50 is a more robust measure of location, less influenced by the bad values. Trimmed mean. The 5% trimmed mean is somewhat of a compromise between the mean and the median. Roughly speaking, the lowest 5% and highest 5% of the data are deleted first and then the mean of the remaining 90% is computed. The result is 101.1286, mainly representative of the process when it is in control. Such a trimmed mean is not inherently biased towards higher or lower values because values have been deleted from both tails of the distribution. If the percentage of high bad values is greater, beyond a 'breakdown point', then trimming 5% from each tail may not be enough. [Loosely speaking, you might say that the median is a 50% trimmed mean, because it is is just the middle value of the sorted data (or average of the middle two).] mean(x, trim=.05) [1] 101.1286 Outliers. Here is a boxplot of the data. Notice the large number of 'outliers' towards the right. We could consider removing the far outliers. But they are all in the upper tail, so taking a mean after deleting just these high values might give a biased view of the process when it is in control. Generally speaking, it is not a good idea to remove 'outliers' from a dataset without investigating what might have caused them. Just how many high outliers could be removed without giving a false impression depends on the data, so this not a good procedure to use in general. In this particular case, it does little damage to remove just the values above 200; the mean of what's left is 99.98. But if I take the average of the values below 125, I get a perhaps sightly deceptive downward-biased value of 99.81--not extremely misleading is this dataset, but perhaps a similar deletion would be more misleading in other datasets. mean(x[x Tradeoffs. If you have uncontaminated normal data, then using either the median or the 5% trimmed mean gives a less precise estimate of the population mean than does the sample mean. In the simulation below, I find means a , 5% trimmed means t , and medians h of a million normal samples of size $n = 50$ with $\mu = 100,\, \sigma=10.$ All three are unbiased estimators of the population mean $\mu$ . It is well-known that the standard error of the sample mean is $\sigma/\sqrt{n} = 10/\sqrt{50} = 1.414.$ The standard error 1.428 of the trimmed mean is a little larger and the SE 1.749 of the median is even larger. set.seed(1022); m = 10^6 a = t = h = numeric(m) for(i in 1:m) { x = rnorm(50, 100, 10) a[i] = mean(x); t[i] = mean(x, trim=.05); h[i] = median(x) } mean(a); mean(t); mean(h) [1] 100.0018 # all unbiased: consistent with 100 [1] 100.002 [1] 100.0026 sd(a); sd(t); sd(h) [1] 1.414471 # aprx 1.414 as shown above [1] 1.427883 # slightly larger for sample trimmed mean [1] 1.749267 # largest for sample median Note: Those are a few of the basic issues. And your links mention a few more. If you would like a discussion of particular additional robust measures of location, please give names and details so someone can move on from this initial answer.
