[site]: crossvalidated
[post_id]: 73137
[parent_id]: 73125
[tags]: 
[Forewarning: this answer appeared before the OP decided to reformulate the question, so it may have lost relevance. Originally the question was about How to rank items according to their pairwise correlations ] Because matrix of pairwise correlations isn't a unidimensional array it is not quite clear what "ranking" may look like. Especially as long as you haven't worked out your idea in detail, as it seems. But you mentioned PCA as suitable for you, and that immediately made me to think of Cholesky root as potentially even more suitable alternative. Cholesky root is like a matrix of loadings left by PCA, only it is triangular. I'll explain both with an example. R, correlation matrix V1 V2 V3 V4 V1 1.0000 -.5255 -.1487 -.2790 V2 -.5255 1.0000 .2134 .2624 V3 -.1487 .2134 1.0000 .1254 V4 -.2790 .2624 .1254 1.0000 A, PCA full loading matrix I II III IV V1 -.7933 .2385 .2944 .4767 V2 .8071 -.0971 -.3198 .4867 V3 .4413 .8918 .0721 -.0683 V4 .5916 -.2130 .7771 .0261 B, Cholesky root matrix I II III IV V1 1.0000 .0000 .0000 .0000 V2 -.5255 .8508 .0000 .0000 V3 -.1487 .1589 .9760 .0000 V4 -.2790 .1361 .0638 .9485 A*A' or B*B': both restore R V1 V2 V3 V4 V1 1.0000 -.5255 -.1487 -.2790 V2 -.5255 1.0000 .2134 .2624 V3 -.1487 .2134 1.0000 .1254 V4 -.2790 .2624 .1254 1.0000 PCA's loading matrix A is the matrix of correlations between the variables and the principal components. We may say it because row sums of squares are all 1 (the diagonal of R) while matrix sum of squares is the overall variance (trace of R). Cholesky root's elements of B are correlations too, because that matrix also has these two properties. Columns of B are not principal components of A, although they are "components", in a sense. Both A and B can restore R and thus both can replace R, as its representation. B is triangular which clearly shows the fact that it captures the pairwise correlations of R sequentially, or hierarhically. Cholesky's component I correlates with all the variables and is the linear image of the first of them V1 . Component II no more shares with V1 but correlates with the last three... Finally IV is correlated only with the last, V4 . I thought such sort of "ranking" is perhaps what you seek for ?. The problem with Cholesky decomposition, though, is that - unlike PCA - it depends on the order of items in the matrix R. Well, you might sort the items is descending or ascending order of the sum of squared elements (or, if you like, sum of absolute elements, or in order of multiple correlarion coefficient - see about it below). This order reflects the how much an item is gross correlated. R, rearranged V2 V1 V4 V3 V2 1.0000 -.5255 .2624 .2134 V1 -.5255 1.0000 -.2790 -.1487 V4 .2624 -.2790 1.0000 .1254 V3 .2134 -.1487 .1254 1.0000 Column sum of squares (descending) 1.3906 1.3761 1.1624 1.0833 B I II III IV V2 1.0000 .0000 .0000 .0000 V1 -.5255 .8508 .0000 .0000 V4 .2624 -.1658 .9506 .0000 V3 .2134 -.0430 .0655 .9738 From last B matrix we see that V2 , most grossly correlated item, pawns all its correlations in I . Next grossly correlated item V1 pawns all its correlatedness, except that with V2 , in II ; and so on. Another decision could be computing Multiple correlation coefficient for every item and ranking based on its magnitude. Multiple correlation between an item and all the other items grows as the item correlates more with all of them but them correlate less with each other. The squared multiple correlation coefficients form the diagonal of the so called image covariance matrix which is $\bf S R^{-1} S - 2S + R$, where $\bf S$ is the diagonal matrix of the reciprocals of the diagonals of $\bf R^{-1}$.
