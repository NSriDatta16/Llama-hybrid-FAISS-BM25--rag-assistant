[site]: datascience
[post_id]: 124374
[parent_id]: 124365
[tags]: 
There is not all-correct or all-wrong option here, but as I see both paths present limitations: Treating a fixed window as tabularized features is simple as you pointed out but all your model is receiving is the sequential values from the timeseries. With tree-based models, all it will be able to do is come with rules like "if $y_{t-3} \geq 0$ , then something" and the classification task you are looking at can demand more sofisticated decision making than that. Boosting methods can come up with more complex logic but this nature won't really change. Computing the features mannually from a past window allows your model to make decisions based on much more complex and problem-specific logic, but in order to the model to come up with "if the standard deviation from the last 9 values is greater than the minimum divided by 3" you need to first create those specific features yourself. This approach requires that you understand the problem in a deeper way and create good features that make it easy for the model. Is with this in mind that lots of people go with neural networks for time series classification problems, the network learns how to extract good features. I would go with 1 with you have very limited time, little problem knowledge and the problem is not too critical. 2 looks more promising with you can play with the feature engineering and knows something about the problem process (or has access to someone who does). If limited time, little problem knowledge and model performance is critical, I would try some neural network approach.
