[site]: datascience
[post_id]: 106607
[parent_id]: 
[tags]: 
Handling imbalanced Feature (X) not lavbel (Y) in machine learning

I am very new to this field and have done a decent amount of research on this, but every time, I stumble upon handling the imbalanced label by using f1 score, recall, precision as metrics, and using methods like random oversampling, etc, confusion matrix, etc. What I want to know is how would we go with the imbalanced features? Here is a picture of the CRIM category by using the "binning" method from advanced Boston housing dataset Here is the code: df = pd.read_csv5("boston.csv5") binnedCRIM = pd.cut(df.CRIM.values, bins=[0,20,70,100], labels=[0 ,1 ,2]) print(binnedCRIM[binnedCRIM == 0].shape, binnedCRIM[binnedCRIM == 1].shape, binnedCRIM[binnedCRIM == 2].shape) # 0: low CRIM # 1: mid CRIM # 2: high CRIM df2.CRIM = binnedCRIM df2.head() df2.CRIM.hist() As you can see, the low class has almost all of the rows, while the mid and high classes are almost negligible as compared to the low class. How should we handle this? The number of total samples is 506, while the number of samples made up of low and high class combined is 18. How should we tackle this kind of scenario? While dropping them is the most obvious and easiest solution, I am more curious about other methods which can be applied if there are any. Thanks in advance to all those who helped!
