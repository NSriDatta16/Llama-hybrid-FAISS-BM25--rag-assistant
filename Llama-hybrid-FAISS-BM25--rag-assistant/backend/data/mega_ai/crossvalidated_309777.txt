[site]: crossvalidated
[post_id]: 309777
[parent_id]: 309729
[tags]: 
Yes. Essentially, any parameter that you can initialize (before training the neural network model) can be seen as a hyperparameter. This includes the optimizer's hyperparameters (e.g., SGD, Adam, etc.): learning rate, decay rates, step size, and batch-size; as well as model's hyperparameter (CNN): number of layers, number of units at each layer, drop out rate at each layer, L2 (or L1) regularization parameters, activation function type (ReLU, Sigmoid, Tanh), and if you are dealing with CNNs, there are extra hyperparameters such as the ones related to convolutional layer: window size, stride value, and Pooling layers. There are even more hyperparameters that you can initialize and tune. For example, take a look at this list .
