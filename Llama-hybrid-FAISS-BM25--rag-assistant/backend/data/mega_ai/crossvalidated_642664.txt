[site]: crossvalidated
[post_id]: 642664
[parent_id]: 642505
[tags]: 
EDIT I earlier claimed that the Kolmogorov-Smirnov statistic yields a proper scoring rule. That does not appear to be the case, judging from Thorarinsdottir, 2012, "Proper scoring rules and divergences - with an application to climate model ranking" : Slide 17 defines when a divergence (a measure of distance between CDFs) is n-proper . Slide 18 gives a theorem by Thorarinsdottir, Gneiting & Gissibl (2012): every proper scoring rule defines an n-proper divergence. Slide 21 finally gives the Kolmogorov-Smirnov statistic as an example of a divergence that is not n-proper. Therefore, the KS statistic does not yield a proper scoring rule. I will leave the rest of this answer up after editing, because I believe that the following aspects are still valuable: (a) the pointer to proper scoring rules as the canonical way to evaluate predictive densities, (b) an example where the KS statistic will not differentiate between different candidate predictive densities that are "arbitrarily bad", and (c) the point that a divergence or a proper scoring rule will not tell us what an "acceptable" approximation to the true density is. There are multiple aspects to this question. Let's take them one by one. Proper scoring rules First off, the standard tool to evaluate probabilistic predictions are proper scoring rules . (These do not care whether your predictions come from ML or from anywhere else, just that they are probabilistic.) In the context of a numerical prediction, people typically use the continuous ranked probability score (CRPS). A scoring rule takes a predictive density $\hat{f}$ and an actual realization $y$ to a score $s(f,y)$ . We typically evaluate this over many realizations and the corresponding densities, $$ \frac{1}{N}\sum_{i=1}^N s(\hat{f}_i,y_i). $$ A proper scoring rule is one that is minimized in expectation by the true density. (This expectation is of course what we estimate by taking this average.) Thus, it makes sense to minimize a proper scoring rule in our search for a "best" predictive density. Of course, we can formulate scoring rules in terms of cumulative predictive densities as well. So now our question is: is the Kolmogorov-Smirnov statistic a proper scoring rule? It definitely is a scoring rule, being a mapping from a predictive (cumulative) density and a realization to the reals, but is it proper? See above for an answer in the negative. An alternative proper scoring rule is the continuous ranked probability score. The KS statistic is not a very useful divergence Let's look at an example. To make it simple, we will consider degenerate discrete distributions (but the example can easily be generalized to nondiscrete and/or nondegenerate distributions). Our true density, which we want to elicit, is a point mass at zero, $$ f(k) = \begin{cases} 1,& k=0 \\ 0,& k\neq 0.\end{cases} $$ We have a family of candidate predictive densities, which are also degenerate, but each one with a point mass at some other integer, $$ \hat{f}_n(k) = \begin{cases} 1,& k=n \\ 0,& k\neq n.\end{cases} $$ What are the (expected) KS statistics and CRPS between each $\hat{f}_n$ and the true $f$ ? Well (I trust I can abuse the notation for PMFs and CMF here): $$ E\text{KS}(\hat{f}_n, f) = \begin{cases} 0,& n=0 \\ 1,& n\neq 0\end{cases} $$ and $$ E\text{CRPS}(\hat{f}_n, f) = |n|. $$ That is, the CRPS will prefer $\hat{f}_n$ over $\hat{f}_m$ if $|n| through yielding a smaller score, whereas the KS statistic will be indifferent between the two as long as neither one is correct, $n, m\neq 0$ . The KS statistic, in this sense, is "all or nothing". This does not look very useful to me. We rarely have the true density among our candidates, so it is usually more helpful to find the "closest" candidate in some sense, and the CRPS encourages that. Testing and thresholds You write : CRPS, on the contrary of KS, doesn’t have a defined threshold for what is or isn’t acceptable for its results, given a desired level of confidence. I would argue that this is mistaken. Yes, the KS statistic is always between zero and one, whereas the CRPS is unbounded. And we have a well worked out theory for the distribution of KS statistics under the null hypothesis that we indeed do have the true CDF as our predictive CDF, which allows us to translate the KS statistic to a p value. However, I dispute that this tells us much about whether a given candidate predictive CDF is acceptable . Recall that statistical significance is different from clinical or economic significance . Whether a given approximate predictive CDF is "acceptable" to you will depend on what you do with it. See that little example above: while $\hat{f}_0$ is the true CDF, $\hat{f}_1$ is probably much better than $\hat{f}_{10}$ , and $\hat{f}_2$ may be "good enough" for your use, whereas $\hat{f}_7$ may lead to large costs. Look at it the other way around: any predictive CDF that is not the true CDF will generate smaller and smaller p values in the KS test as you collect more and more observations - but does the same predictive CDF become less and less "acceptable" just because you collected more and more evidence that it is not the true CDF? This is related to the KS statistic, as a proper scoring rule, being "all or nothing" as above. I would say that to determine whether a given predictive CDF is "acceptable", you need to consider the entire decision framework in which it is estimated and used. Statistics, proper scoring rules and norms play the role of measurement devices. Your speedometer may be telling you that you are going at 30 mph, and it may be doing a good or a bad job at measuring this, but whether this speed is "acceptable" is not for the speedometer to decide without understanding the context.
