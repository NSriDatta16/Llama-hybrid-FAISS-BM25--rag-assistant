[site]: datascience
[post_id]: 94989
[parent_id]: 94975
[tags]: 
Why should I choose one over the other? You sould prepare a common validation set out of your dataset and try out each and every method on your dataset. Following are the methods that I know to handle imbalanced datasets. - Use weighted cross-entropy loss (as you mentioned) You can assign weights to your loss such that it will penalize more to the smaller classes and the less to larger classes. Many frameworks have a very easy way to do this. In Scikit-learn you can look out for class_weight parameter. For eg - random forest Here is how you can use this in Pytorch Here is how you can use this in Keras Use focal loss Originally proposed for object detection, but we can also use this for any other use case. Read more about it here Here is how you can use this in Pytorch for multi-class classification Here is how you can use this in Keras Over Sampling and Under Sampling There are so many techniques in this, check out imblearn a dedicated library just to deal with imbalanced datasets. Create a separate model for small classes If you have some classes that have very small number of instances, you can consider creating a separate classifier for these small classes (called small_classifier for eg). You can group together these small clases under a single class (called small_class for eg) so that your main classifier will classify small_class with all other big classes in the dataset. And if your main classifier encounters any instance of small_class, it will pass it to small_classifier, which will predict the actual class for the small_class instance. This technique can give you accuracy boosts are now main classifier does not need to deal with very small classes, and insted small_classifier will be looking just at these small classes.
