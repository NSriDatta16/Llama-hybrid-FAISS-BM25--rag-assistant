[site]: datascience
[post_id]: 87927
[parent_id]: 
[tags]: 
Predicting quality results from operating data

Background: I have process data (table 1) that is "batch" in the chemical engineering sense of the word. Each batch ID represents the start and end of a run. Throughout the batch, different process parameters are either adjusted or they vary as a result of the chemistry going on in the tank. At the end of a batch, a sample is taken of the product and analyzed in a lab. There is only one row per batch ID of results (table 2), while the operating data is sampled every minute. Goal: There are years worth of data for these batches and I'm trying to determine what the optimal conditions are to reached the desired quality results. I'd also like to have a machine learning model where I can input parameters that might not be exactly identical to one of the previous runs, and have a decent prediction of the product quality. In the example below, these quality results would be purity and pH (table 2). Ideally, I would have a purity of 99% and a pH of 4. What I've researched thus far: I've read a few papers that address this with statistical methods (principal component analysis or partial least squares regression). Since there are many rows of operating data for each batch ID but only one quality row, they shape the data such that each variable at each time t is a column. This creates a table where each batch can be represented by a row (table 3). I'm going to continue working through this method, but I wanted to know if this could be a machine learning application. I've been reading about neural networks (RNN, ANN, LSTMs), but I haven't been able to find an example that is similar to this. For applications of machine learning involving time series, it seems that they are meant to forecast a future value or predict a value within the time of a dataset with regression. I have multiple inputs and multiple outputs, but the examples I've seen seem to be situations where x1 and x2 affect y1 and y2 in the same plot. In my example below, var1 and var2 are both inputs and they might not necessarily be indictive of quality on their own if you looked at them separately. Also, in reality there are probably 10 or so variables that are inputs. What I'm after is more of an optimization model that looks at all of the process parameters and predicts the kind of quality I would get from running the batch with a particular set of conditions. I tried to look at this as a classification problem to join the quality data with the time series process data, but I don't think that works. Could I possibly take the quality data and turn it into ranges so that I can apply a classification solution? For example, instead of targeting a pH of 4, I can create a category where pH is between 3.5 and 4.5. Any recommendations as to what I should look into? Process data (table 1): ID time var1 var2 1 0 65 21 1 1 66 20 1 2 68 23 2 0 65 24 2 1 70 24 3 0 65 26 3 1 66 27 3 2 66 27 3 3 66 28 End of trial quality results (table 2): ID purity pH 1 98% 4 2 97% 4.5 3 99% 3.9 Batch-wise unfolding (table 3) ID t = 0, var1 t = 1, var1 t = 2, var1 t = 0, var2 t = 1, var2 t = 2, var2 purity pH 1 2 3
