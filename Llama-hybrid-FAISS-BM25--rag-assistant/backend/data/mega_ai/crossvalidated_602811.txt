[site]: crossvalidated
[post_id]: 602811
[parent_id]: 
[tags]: 
Difference between "transfer learning" and "fine-tuning"

I am currently reading the BERT paper and it splits the use of pre-trained models into two categories : Feature-based whereby you just take the embeddings for the tokens and plug it into whatever problem you want Fine-tuning where you add a couple of task-specific layers at the end of the pretrained model and then just run a couple of training epochs on that. But in section 2.3 they mention transfer learning as if it were something different from "fine-tuning". What is the difference (if any) between transfer learning and just fine-tuning as applied to LMs and downstream tasks?
