[site]: crossvalidated
[post_id]: 501795
[parent_id]: 459821
[tags]: 
Does it ever make sense to use the form in equation (2) over equation (1) given that it has twice the number of parameters? As far as I see, I have not found any arguments for doing this as stated in this question . With equation (2) you get: an infinite number of solutions with almost all neural network architectures. at worst (the binary specification and with some architectures) twice the number of parameters half of which are redundant. greater number of flops required to evaluate the loss and the gradient and greater storage requirements for your tape when using automatic differentiation. a singular Hessian of the loss function which can cause problems with some optimization methods. possibly slower convergence (more loss function and gradient evaluations are required). When we use softmax and go from 2-class classification to 3-class classification, we increase the number of parameters from $D+1$ to $3D+3$ . However, intuitively it seems that if we can do 2-class classification with D+1 parameters, we should be able to do 3-class with $2D+2$ . Is this intuition correct, and if so, is this normally done in practice? Your intuition is correct. You can do with $2D+2$ by setting one of the softmax arguments to always be zero. The reason is the sum-to-one constraint. Doing this yields the sigmoid function (logit link) as a special case. I am not too familiar with practice but it does seem that some use the overparameterized $3D+3$ specification for reason that are not clear to me. I cannot answer what is normally done as I am not too familiar with the field.
