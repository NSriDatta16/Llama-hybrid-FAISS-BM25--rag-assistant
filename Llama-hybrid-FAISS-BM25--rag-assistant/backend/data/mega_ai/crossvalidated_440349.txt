[site]: crossvalidated
[post_id]: 440349
[parent_id]: 
[tags]: 
Can we speed up matrix inversion with ML?

I am working on dynamical system reconstruction in the framework of Variational Autoencoders. In order to estimate the probability distribution of the underlying system p(Z|X) (Z = latent states, X = data which we use to infer parameters), we need to estimate mean and covariance. Therefore I combine estimations from a MLP (encode_mean, encode_cov) with calculations of a Kalman-smoother (h.blk_tridag_chol, h.blk_chol_inv). Here's a code snippet: """ cov is actually not the covariance matrix but instead a part of the Matrix used in the Kalman filter to calculate the cholesky decomposition and hence the correct covariance matrix""" mean = self.encode_mean(x) cov = self.encode_cov(x) # compute cholesky decomposition self.AA_com, BB_com, lambdaMu_com = self.calculate_cholesky_factors(mean, cov) self.the_chol = h.blk_tridag_chol(self.AA_com, BB_com) ib_com = h.blk_chol_inv(self.the_chol[0], self.the_chol[1], lambdaMu_com) self.mu_z = h.blk_chol_inv(self.the_chol[0], self.the_chol[1], ib_com) I know there are easier ways to estimate covariance and mean, but since we are working on dynamical system reconstruction, a good representation of the covariance matrix with time-dependent elements is very important (time-dependence is implemented by using batches ~x_1 ... x_250). However, the 3 last lines of the code snippet cause my algorithm to be very slow. If you are interested in the exact implementation of h.blk_chol_inv() and h.blk_tridag_chol() I can give further information, but it's basically some matrix multiplications and inversions. I am looking for a method to speed things up. I was wondering, if we could simply beforehand train two networks to learn the functions h.blk_chol_inv() and h.blk_tridag_chol(), and use these pretrained networks in the algorithm instead? Is this efficient? Can we by pretraining achieve speed ups in matrix inversion? Intuitively I would expect one forward pass through a MLP to be faster than a for-loop Matrix inversion. Any insights/thoughts appreciated. Best, DK
