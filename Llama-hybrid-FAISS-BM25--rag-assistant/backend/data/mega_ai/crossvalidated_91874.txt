[site]: crossvalidated
[post_id]: 91874
[parent_id]: 91715
[tags]: 
In some cases it seems clear that theory might work best (mice tail lengths are probably normally distributed). Tail lengths are certainly not normally distributed. Normal distributions have a nonzero probability of taking negative values; tail lengths do not. George Box's famed line , " all models are wrong, but some are useful " makes the point rather well. Cases where we might reasonably assert normality (rather than just approximate normality) are very rare indeed, almost creatures of legend, mirages occasionally almost glimpsed out of the corner of the eye. In a lot of cases there is probably no theory to describe a set of data, so you just use something that fits what you have fairly well regardless of what it was originally developed to describe? In cases where the quantities you're interested in are not especially sensitive to the choice (as long as the broad features of the distribution are consistent with what's known), then yes, you can just use something that fits fairly well. In cases where there is a greater degree of sensitivity, 'just using something that fits' isn't sufficient on its own. We might use some approach that doesn't make particular assumptions (perhaps distribution free procedures, like permutation, bootstrapping or other resampling approaches, or robust procedures). Alternatively we might quantify the sensitivity to the distributional assumption, such as via simulation (indeed I think this is generally a good idea). there seems to be the problem that maybe you should just use an empirical distribution if you really have no idea. I wouldn't describe that as a problem - basing inference on empirical distributions certainly a legitimate approach suitable for many kinds of problems (permutation/randomization and bootstrapping are two examples). does someone have a coherent way of approaching/thinking about this problem? broadly, in a lot of cases, I tend to consider questions like: 1) What do I understand* about how means (or other location-type quantities) behave for data of this form? *(whether from theory, or experience of this form of data, or expert advice, or if necessary, from the data itself, though that carries problems one must deal with) 2) What about spread (variance, IQR, etc) - how does it behave? 3) What about other distributional features (bounds, skewness, discreteness, etc) 4) What about dependence, heterogeneity of populations, tendency to occasionally very discrepant values, etc This sort of consideration might guide a choice between a normal model, a GLM, some other model or some robust or distribution-free approach (such as bootstrapping or permutation/randomization approaches, including rank-based procedures)
