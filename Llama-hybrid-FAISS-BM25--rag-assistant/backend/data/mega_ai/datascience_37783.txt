[site]: datascience
[post_id]: 37783
[parent_id]: 18968
[tags]: 
I have the same question, but I probably figured it out by reviewing the source code of Caffe. Please see source code of Caffe: line 620 & 631 of this code. It calculates derivative of each parameter by adding the derivative (of this parameter) of each input then divides it by batch size. Also, see line 137 of this code, it simply scales the derivative to 1/iter_size , just the same as the average. We can see there is NO special treatment for the Max Pooling layer when doing back propagation. As for the derivative of Max Pooling, let's see the source code of Caffe again: line 272 of this code. Obviously, only the biggest element's derivative is 1*top_diff , others' derivative is 0*top_diff .
