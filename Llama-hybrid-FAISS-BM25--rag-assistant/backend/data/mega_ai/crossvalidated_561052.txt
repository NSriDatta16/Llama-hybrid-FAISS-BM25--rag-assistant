[site]: crossvalidated
[post_id]: 561052
[parent_id]: 561049
[tags]: 
There is no single optimizer that beats all the others. If you look at the published papers, you would see different optimizes used. People often still use stochastic gradient descent, you can find a nice discussion on this on Quora . There are results suggesting that the basic SGD may generalize better ( Hardt, Recht, & Singer, 2016 ). So neither does the deep learning community prefer Adam (though it is popular given that it has proven to give decent results), not Adam is guaranteed to outperform Adam or other optimizers.
