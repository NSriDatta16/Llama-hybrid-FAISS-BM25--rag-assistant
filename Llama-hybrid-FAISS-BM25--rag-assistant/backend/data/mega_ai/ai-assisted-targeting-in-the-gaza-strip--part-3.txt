The system depends entirely on training data, and intel that human analysts had examined and deemed didn't constitute a target had been discarded, risking bias. The vice president expressed his hopes this had since been rectified. Organization The Gospel is used by the military's target administration division (or Directorate of Targets or Targeting Directorate), which was formed in 2019 in the IDF's intelligence directorate to address the air force running out of targets to bomb, and which Kohavi described as "powered by AI capabilities" and including hundreds of officers of soldiers. In addition to its wartime role, The Guardian wrote it'd helped the IDF build a database of between 30,000 and 40,000 suspected militants in recent years, and that systems such as the Gospel had played a critical role in building lists of individuals authorized to be assassinated. The Gospel was developed by Unit 8200 of the Israeli Intelligence Corps. Lavender The Guardian defined Lavender as an AI-powered database, according to six intelligence officers' testimonies given to +972 Magazine/Local Call and shared with The Guardian. The six said Lavender had played a central role in the war, rapidly processing data to identify potential junior operatives to target, at one point listing as many as 37,000 Palestinian men linked by AI to Hamas or PIJ. The details of Lavender's operation or how it comes to its conclusions are not included in accounts published by +972/Local Call, but after a sample of the list was found to have a 90% accuracy rate, the IDF approved Lavender's sweeping use for recommending targets. According to the officers, it was used alongside the Gospel, which targeted buildings and structures instead of individuals. Citing multiple sources, The Guardian wrote that in previous wars, identifying someone as a legitimate target would be discussed and then signed off by a legal adviser, and that, after 7 October, the process was dramatically accelerated, there was pressure for more targets, and to meet the demand, the IDF came to rely heavily on Lavender for a database of individuals judged to have the characteristics of a PIJ or Hamas militant. The Guardian quoted one source: "I would invest 20 seconds for each target at this stage, and do dozens of them every day. I had zero added-value as a human, apart from being a stamp of approval. It saved a lot of time." A source who justified the use of Lavender to help identify low-ranking targets said that in wartime there's no time to carefully go through the identification process with every target, and rather than invest manpower and time in a junior militant "you're willing to take the margin of error of using artificial intelligence." The IDF issued a statement that some of the claims portrayed are baseless while others reflect a flawed understanding of IDF directives and international law, and that the IDF does not use an AI system that identifies terrorist operatives or tries to predict whether a person is a terrorist. Instead, it claims information systems are merely one of the types of tools that help analysts gather and optimally analyze intelligence from various sources for the process of identifying military targets, and according to IDF directives, analysts must conduct independent examinations to verify that the targets meet the relevant definitions in accordance with international law and the additional restrictions of the IDF directives. The statement went on to say the "system" in question is not a system, nor a list of confirmed military operatives eligible to attack, only a database to cross-reference intelligence sources in order to produce up-to-date layers of information on the military operatives of terrorist organizations. Ethical and legal ramifications Experts in ethics, AI, and international humanitarian law have criticized the use of such AI systems along ethical and legal lines, arguing that they violate basic principles of international humanitarian law, such as