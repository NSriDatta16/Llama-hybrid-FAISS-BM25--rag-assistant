[site]: datascience
[post_id]: 77892
[parent_id]: 77880
[tags]: 
is it true that Label Encoding will be misinterpreted as a numeric scale by scikit-learn trees? Yes, SciKit-Learn treats it as Numeric value . Hence, it will impact the depth of Tree and result in different Tree structure . On results - Definitely, different hyperparameter tuning will be required for different methods but I am not sure about the fact that whether we will never achieve the best with Label encoding Or we may if tuned properly. It is also true that if the encoding is aligned with Labels/target, it will achieve a good result quickly. May like to read this Answer if so, are there any situations at all where arbitrary Label Encoding can be useful or does this technique has no use at all unless the variable is ordinal, and a specific labeling order is given (i.e. Ordinal Encoding is useful only when it's truly ordinal)? I doubt that it will work i.e. with Neural Network Or Linear Regression, etc. 10 will become 2 times of 5 without any such underlying relation between two values of a Feature. If it happens, it will be a coincidence or might be because of a subconscious knowledge about the Target(Target encoding) while assigning the value randomly. but now I suspect that that whole lesson is best removed altogether to avoid teaching students bad practices I think students should know how it will fail/behave in different conditions. So that they can grasp the underlying concept.
