[site]: datascience
[post_id]: 39550
[parent_id]: 39544
[tags]: 
Taking your question title literally: Is RL applicable to environments that are totally RANDOM? The answer would be no. In a totally random environment there is little to nothing that can be learned. However, you don't actually have a totally random environment. You have some quantities that fluctuate a lot in a way that you don't understand. Otherwise your environment behaves very logically - if there is demand for Y items, and you have X in stock, then you will end up with X - Y in stock if X > Y, or 0 in stock and Y - X incomplete orders otherwise. This is a very structured rule that you can definitely associate rewards with and learn. Presumably you have costs for ordering and holding stock and opportunity costs for not supplying orders. A simple variant of your situation is used as a toy example in Sutton & Barto: Reinforcement Learning, an Introduction in chapter 4, called "Jackâ€™s Car Rental" where the goal is to optimise stock at two locations with random demand occurring at either location. This toy problem is made easier by defining the distribution, and using it in a model-based way. But that is not necessary in general. Is RL really applicable to such situations? In your case, probably yes. Although you do have to assume that "totally random" is just a way of phrasing "highly variable" and not literally 0 one day, 30 million the next day and 7.5 the next day. There are going to be limits to the orders and they will follow some distribution. With very high variance then you are likely to find it hard to reach a balance point of costs for holding stock vs costs for missing orders, but in principle this is solvable and RL is a reasonable tool to attempt the solution. If it does - then what will improve the performance? Check whether any fluctuations in demand depend on any variables that you can collect and add those variables to the state. For instance, if demand has some weekly or seasonal variation, then those parts of the date should be part of your state representation. Understanding and predicting the distribution of demand, even if not the exact values, would also help with simulations and planning algorithms. The RL algorithm will over time learn the likely distribution, but it can only base its own predictions on state variables that you have let it observe. You might be able to go one better: If demand is mostly independent of the stock levels that you hold, then you can separate the problem of predicting it and use more robust supervised learning to add a "predicted demand" feature to the state. The RL will on top of this prediction learn the costs associated with just trusting this prediction vs allowing for some extra incoming orders just in case etc.
