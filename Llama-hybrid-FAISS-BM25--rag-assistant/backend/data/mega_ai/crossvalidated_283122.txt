[site]: crossvalidated
[post_id]: 283122
[parent_id]: 283115
[tags]: 
Decision tree learning produces models that are very easy to interpret. You can just trace an input's path down the tree and see exactly why it ended up in the class it landed in. That direct interpretability is actually a key strength of decision trees. Similarly, random forests , which are made of decision trees, have a similar benefit. But interpretation is less direct because there are many trees to consider for a single input. More complex, nonlinear, or highly dimensional models (like neural networks) still have rules that govern input to output relationships. Those rules are usually even deterministic, so you can trace the inputs in the same way as with decision trees. The problem arises from interpreting that information in a meaningful way. A toy example about house price prediction might help demonstrate what I mean. A decision tree can directly tell you that a house ended up in the 500K-750K bucket because it has 2 bathrooms, 2 bedrooms, between 1200 and 1400 square feet, and is newer than 5 years old. A neural net making the same prediction will directly tell you that 100 inputs to 50 tanh functions across 3 layers gave a softmax output that corresponds to 500K-750K. The latter is much more difficult to interpret, but not a lost cause . Look up model/prediction interpretation for the type of model you're interested in to see how others have approached the problem.
