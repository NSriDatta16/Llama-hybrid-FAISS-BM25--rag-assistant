[site]: crossvalidated
[post_id]: 597145
[parent_id]: 597031
[tags]: 
There are several ways how the logistic function can occur as an expression that is derived based on some underlying principle. Derivation as Bayes factor The formula occurs when one computes the Bayes factor where one assumes that the groups are normal distributed. $$\frac{P(Y = 1 | X=x)}{P(Y = 0 | X=x)} = \frac{P(Y = 1)}{P(Y = 0)} \frac{\frac{1}{\sqrt{2\pi\sigma_1^2}}\text{exp}\left(-\frac{(x-\mu_1)^2}{2\sigma_1^2}\right)}{\frac{1}{\sqrt{2\pi\sigma_0^2}}\text{exp}\left(-\frac{(x-\mu_0)^2}{2\sigma_0^2}\right)} = \text{exp}\left(a + bx + cx^2 \right)$$ where $a$ , $b$ and $c$ are functions of the prior ratio $P(Y = 1)/P(Y = 0)$ and of the means $\mu_1$ , $\mu_0$ and of the standard deviations $\sigma_1$ and $\sigma_0$ . If the standard deviations are the same then the quadratic term is zero. Derivation as natural or canonical parameter Logistic regression is often used for binary data and models the probability parameter $p$ for a Bernoulli distributed variable. For the binomial distribution the inverse of the logistic function is the logit function which is the natural parameter of the Bernoulli and binomial distribution. If we write a distribution in the form of an exponential family $$f(y|\theta) = h(y) \text{exp}\left(\eta(\theta) T(y) - A(\theta) \right)$$ Then we get for the Bernoulli distribution $$\begin{array}{rcl} \theta &=& p\\ \eta(\theta) = \eta(p)& = &\text{log}\left( \frac{p}{1-p} \right)\\ T(y)& =& y\\ A(\theta) &= &-\text{log}(1-p)\\ h(y)& = &1 \end{array}$$ $$f(y|\theta) = \text{exp}\left(\text{log}\left( \frac{p}{1-p} \right) y + \text{log}(1-p) \right) = p^y (1-p)^{1-y}$$ and the logit function $\eta = \text{log}\left( \frac{p}{1-p} \right)$ is the natural parameter. What is "natural" about the natural parameterization of an exponential family and the natural parameter space? Relation to Boltzmann distribution and maximum entropy The logistic function can originate from an exponential function for different outcomes $$p(y;x) \propto \text{exp}(f(y,x))$$ and with the normalisation using the sum $\sum_{\forall y} p(y;x)$ it becomes a logistic function $$p(y;x) = \frac{\text{exp}(f(y,x))}{\text{other stuff}+\text{exp}(f(y,x))}$$ The connection with entropy is in the exponential function $p(y;x) \propto \text{exp}(f(y,x))$ which is similar to the Boltzmann distribution . See for more: Logistic regression and maximum entropy Maximum Entropy and Multinomial Logistic Function Derivation as physical model The logistic function also occurs as the solution of a differential equation $$f^\prime(x) = f(x) \left(1-f(x)\right)$$ This occurs in growth models.
