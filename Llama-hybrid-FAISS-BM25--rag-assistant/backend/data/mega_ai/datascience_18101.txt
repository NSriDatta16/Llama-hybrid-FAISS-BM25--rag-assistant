[site]: datascience
[post_id]: 18101
[parent_id]: 
[tags]: 
Should weights on earlier layers change less than weights on later layers in a neural network

I'm trying to debug why my neural network isn't working. One of the things I've observed is that the weights between the input layer and the first hidden layer hardly change at all, whereas weights later in the network (eg. the weights between the last hidden layer and the output) change significantly. Is this to be expected or a symptom of an error in my code? I'm applying backpropagation and gradient descent to alter the weights.
