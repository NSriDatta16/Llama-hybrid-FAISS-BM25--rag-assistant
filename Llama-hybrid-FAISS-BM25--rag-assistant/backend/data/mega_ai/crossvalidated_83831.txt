[site]: crossvalidated
[post_id]: 83831
[parent_id]: 83828
[tags]: 
If scale is not an issue then any solution you probably already know is fine. It is more a matter of personal and company choice (cost/legacy issues). So it doesn't really matter if it's going be R or MATLAB, Python or Java, Weka or Rapidminer, open source tools or propriety code. However, big players like those you mention have to deal with scale. If scale is the issue then obviously you can't deploy any fancy algorithm with complexity higher than $O(n)$, like SVMs in the dual or kNN or so many others. Even more, you have to go for algorithms and implementations that are made to work in distributed environments: data are scattered across more than one machine, limited communication is allowed between machines. Obvious choices are algorithms based on Stochastic Gradient Descent, like Vowpal Wabbit (created at Yahoo! labs). You also have libraries that run on top of Hadoop (free version of Map/Reduce framework developed by Google), like Mahout. Problems and challenges in the big data environment are unlimited. For example, a common assumption is that the model $w$ you try to learn will live in a single machine, which is fine until you start having a few hundred servers reading and updating that same model in a production environment. You can look up papers and video lectures from NIPS and ICML from Yahoo!, Google, Facebook and others where they discuss similar issues they deal with and solutions they deploy (search for scalability).
