[site]: crossvalidated
[post_id]: 283430
[parent_id]: 283244
[tags]: 
First of all, you should learn to inspect your tensors at various stages to understand what is going on. It should be clear what is going on from simply looking at dense_1's output or the gradients. And you should learn and understand the purpose of batch normalization and how neural networks work and learn. Nevertheless, an explanation: What is happening in cases 1 and 2 are that the values going into your tanh operation are too large and are being compressed in the upper part of the tanh function, leading to values that are all very close together. That means gradients flowing through them will be very similar and not much learning will occur. Why doesn't batch normalization fix this? Well, it can, if you insert the batch normalization in the more common position: between the weights and the activation. The goal of batch normalization is to have a nice distribution going into these activation functions. So you should have a linear activation on dense_1 and then add whichever activation you want afterwards. If you put it there, it will essentially do that input normalization, plus some additional theoretical advantages. Another less ideal way to fix this in case 1 is to manually adjust the range of dense_1 weights based on your expected input ranges. Then you won't need batch norm or input norm.
