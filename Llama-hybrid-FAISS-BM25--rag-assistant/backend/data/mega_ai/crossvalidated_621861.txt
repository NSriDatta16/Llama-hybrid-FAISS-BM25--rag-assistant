[site]: crossvalidated
[post_id]: 621861
[parent_id]: 435318
[tags]: 
I fount both answer to this questions are not clear and coherrent. So here I would like to add another solution. In multi-head attention, the parameters are from the linear layers rather than from the scaled dot-product operation, where only multplication of the inputs are conducted. The number of heads doesn't add up to the number of attentions, because it is just reshape the input rather than doing addition calculations, as show in the code of minGPT . In the first linear layer, there are 3 linear operations for 3 inputs, V, K and Q, both the input and output has a dimenson of d_model , so the parameters are: 3x (d_model*d_model + d_model) . In the last linear layer, only one linear operation with both input and output of length d_model , so the number of parameters is d_model * d_model + d_model . So the totoal number of parameters is 4x (d_model*d_model + d_model) which matches what @Dileep Kumar Patchigolla has asked.
