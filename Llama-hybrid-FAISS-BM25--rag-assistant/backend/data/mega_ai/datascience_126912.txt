[site]: datascience
[post_id]: 126912
[parent_id]: 23343
[tags]: 
To be clear, I think the softmax approach you mention is the generic one-vs-rest approach, in which a binary classifier is trained for each class, and their predicted probabilities are softmax'ed. The averaging multiclass trees approach is used by (at least) some implementations, including scikit-learn's random forest. Gradient boosting is rather different, in that the targets each tree is trained for are different, being the gradient of the loss function (maybe with hessian information or regularization). Nevertheless, I think it's possible, and perhaps catboost uses multioutput trees targeting each component of the gradients, while xgboost and lightgbm use one-vs-rest and softmax.
