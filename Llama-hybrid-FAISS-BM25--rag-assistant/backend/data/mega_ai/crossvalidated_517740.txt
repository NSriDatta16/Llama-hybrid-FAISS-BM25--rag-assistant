[site]: crossvalidated
[post_id]: 517740
[parent_id]: 
[tags]: 
RNN provides weird output as the sample size is increased

I am currently training an RNN with an underlying binary classification problem (0 and 1). I increased the sample size drastically to see whether the performance is increased. The problem is that now, the NN returns for every single entry in my test sample 1. The out of sample accuracy is 70.73% but that's just because the test sample consists of 70.73% as 1 for the correct prediction. A NN that just tells me for any input that its class 1 seems pretty useless right? However, for a lower sample size the network returned 0's and 1's where the out of sample accuracy was around 50-60%. Is it correct to conclude that the sample size can be too large for a network to work properly? I thought that a too large sample size can only be disadvantageous in case it reduces the accuracy by increasing the noise in the dataset.
