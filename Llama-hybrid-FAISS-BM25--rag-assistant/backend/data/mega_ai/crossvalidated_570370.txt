[site]: crossvalidated
[post_id]: 570370
[parent_id]: 
[tags]: 
Logistic Regression applied to biased dataset

I have collected a binary classification dataset in a somewhat biased way: I have thousands of unlabeled samples. A small percentage of these samples belong to the positive class. I know for a fact that my regressors are strongly correlated with the target. Using this fact I can collect positive samples by picking samples whose regressors are in the top end of the values. This way I can collect enough positive samples, and randomly pick the rest to get negative samples. I don't have the luxury to label many samples, yet I need enough positive samples to get an ok model. However, I dont know the true percentage of positive samples. When fitting a logistic regression afterwards, I notice that my output is not very well calibrated (i.e. I need to tune the threshold to output a positive classification), probably because I collected the dataset in such a biased way. I suspect that if I weighed my labeled samples using a density estimate of the regressors on the whole data this could have some sort of "importance sampling" effect, but I am not sure. What can I do to improve model calibration?
