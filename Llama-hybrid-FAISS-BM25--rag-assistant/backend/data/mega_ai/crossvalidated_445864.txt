[site]: crossvalidated
[post_id]: 445864
[parent_id]: 
[tags]: 
Comparing numerical stability and computing bounds on the condition number of learned weights

I have an empirical risk minimization problem with two equivalent losses that solves it, $f_1(x; \theta_1)$ and $f_2(x ; \theta_2)$ , where $x$ is the data and $\theta$ are the model parameters (in this case, a regular matrix). We are using stochastic gradient descent to optimize $f_1$ and $f_2$ . I noticed that $f_1$ is performing better than $f_2$ , despite they being mostly equivalent (ignoring some constants). One thing I considered is that it might be due to one function being more numerically stable than the other. Empirically, I observed that $f_2$ is indeed increasing the condition number of the parameter matrix $\theta_2$ as training proceeds. I would like to give a more theoretical analysis if this is indeed true. Computing the condition number of a matrix gives me how ill-conditioned the matrix is, but how it affects the computation also depends on the problem we're tackling (if I understood correctly). Thus, I'd have to compare these two different functions $f_1$ and $f_2$ for any arbitrary matrix, right? How is that usually done? I don't know if it's that simple, but I was considering computing the condition number for an arbitrary matrix $\kappa(A) = ||A||\,\,||A^{-1}||$ , like I've seen done in this question . I tried to find an example of this comparing PCA being computed via SVD which is better numerically (which would serve as pointers), but I couldn't find anything useful. Ideally, I'd also like to compute the bounds on this condition number for $\theta_1$ and $\theta_2$ , since these problems are being solved through stochastic gradient descent and one is increasing the condition number. So, how do I compute bounds on the condition number of the parameter matrices obtained from SGD optimization? If anyone have any pointers, that would be really helpful. I'm looking for some canonical or textbook examples that will help me understand what is required to do. I did found some examples in papers, but they were hard to understand fully because sometimes they were intricate with the problem being solved at the paper. TL;DR: How do I make a numerical analysis of two solutions to a problem for arbitrary matrices? That is, how do I know an ill-conditioned matrix will behave poorly for $f_2$ , but will behave better for $f_1$ ? How can I compute the bounds on the condition number for the parameter matrices $\theta$ obtained from optimizing a loss function using SGD? I'd like to compare these bounds and see if what I observed empirically is indeed true (that $f_2$ increases the condition number of parameter matrix more than $f_1$ ). I'm asking these two questions together because they're related, however, I can ask two separate questions if someone thinks it will be better. EDIT: About question (1), one thing that occurred to me now is that maybe it's sufficient to make an analysis for a known ill-conditioned matrix and show that $f_2$ is more unstable.
