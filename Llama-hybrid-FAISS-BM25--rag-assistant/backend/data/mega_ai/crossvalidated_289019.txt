[site]: crossvalidated
[post_id]: 289019
[parent_id]: 237975
[tags]: 
3 output neurons The loss function in most applications is chosen such that it calculates a combined loss for these three neurons (e.g. cross entropy loss ). This defines the tradeoff between better matching of the target value of neuron 1 at the expense of worse matching of their respective target values of the other two neurons. You can of course define a different loss function, e.g. one that reflects that matching the target value is twice as important for neuron 1 than for the other two output neurons etc. For example, T.mean(T.pow(T-Y, 2)) . Why is this? This is the average error over your (training) sample. Usually all samples of the training sample are treated equal so matching the network outputs to the target values for sample 1 is as important as matching the target values for sample 2 etc. There are situations where some elements of the training sample are treated as more important than others, this can be modeled by adding weight terms in the loss function. In my example, shouldn't Theano backprop M, not a scalar value? This would correspond to doing backpropagation M times for a single sample where the network weights are adjusted M times, potentially in opposite directions at each of the M backpropagation operations. Plain stochastic gradient descent works like this for example. What you usually want to minimize (i.e. the original loss function) is the average over the entire training sample. You would get one weight update after calculating the loss over the entire training sample. It turns out that in practice you can get faster close to the optimum by using stochastic gradient descent. On the other hand, modern computing hardware (CPUs or GPUs) is vectorized (i.e. support running the same code on multiple values ) so minibatching allows to take into account more than one sample per upgrade at similar execution speed. when I'm using minibatches, are these libraries just backpropping scalars? They backpropagate the derivatives with respect to the average loss over the entire minibatch (which is a scalar value). If you have a network with three output neurons contributing to a single loss function, each neuron gets its share of the correction according how badly it contributed to the loss, averaged over the M rows in the minibatch.
