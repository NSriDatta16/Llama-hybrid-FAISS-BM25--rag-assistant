[site]: crossvalidated
[post_id]: 284512
[parent_id]: 
[tags]: 
DBSCAN exponentially distributed variables

I'm studying machine learning clustering algorithms by using a real example of project that the company I work for had to deliver. The goal was to cluster a group of 4920 cities according to their main characteristics (population, percentage of urban population, average citizen income, area, ...) and how the fixed broadband market is set in those cities (market share per provider, HHI, delta market share, penetration). I know that are a lot of variables involved, but I can infer these main procedures: Feature Selection Feature Transformation (in my case, log(x+1)) Normalization (standard scaler) Dimensionality reduction (PCA) Distance Metrics Selection (Euclidian) DBSCAN Pameters Selection: Episolon and min neighbors With these procedures the best results I get (after multiple variations) is about 10% noise, 2 big clusters (which contain 89% of data) and 7 to 10 small clusters (1% of data), and I think the main reason for this is that because most of my features are exponentially distributed (95% of cities have less than 30k inhabitants, 87% of cities have less than 16% fixed broadband penetration, figure bellow). Note: The features selected are (by row, column): (0, 0); (2, 0); (3, 0); (4, 0); (5, 0); (6, 0); (6, 1); (7, 1); (8, 0); (9, 0); (11, 0) I've tried to reduce episolon, but that ends up increasing the noise instead of distributing data among clusters, and the log transformation generate normally distributed variables. So I decided to take a step back and ask: What data pre processing is recommended for exponentially distributed variables to split big DBSCAN resulting clusters?
