[site]: datascience
[post_id]: 106660
[parent_id]: 106642
[tags]: 
To handle the class imbalance, I am aware of two broad categories of techniques. The first type of technique directly solves the problem by changing the data distribution itself. On the other hand, the second type of technique plays with the loss function to solve the problem. Over Sampling and Under Sampling techniques Modifying Loss functions I believe you have used the first type of technique and did not see much improvement. I recommend trying using the second type of technique to modify the loss function to encounter the imbalance in the dataset. Weighted cross-entropy loss We can assign weights to the cross-entropy loss such that it will penalize more to the smaller classes and the less to larger classes. Many frameworks have a very easy way to do this. In Scikit-learn you can look out for the class_weight parameter. For eg - random forest ( https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html ) Focal loss Originally proposed for object detection, but we can also use this for any other use case. This article ( https://amaarora.github.io/2020/06/29/FocalLoss.html ) succinctly explains the whole idea. However, currently, I do not know how to use this for sklearn models. Combining both techniques Lastly, these types of techniques are effective when the dataset is large. So,I further recommend using these techniques after SMOTE or Oversampling.
