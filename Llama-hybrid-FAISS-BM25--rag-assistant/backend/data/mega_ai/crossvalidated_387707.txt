[site]: crossvalidated
[post_id]: 387707
[parent_id]: 
[tags]: 
Question regarding posterior and prior distribution relation

I am currently reading the book Machine Learning and Pattern regocnition by Bishop. They state in (1) or (1.66) in the book (relating how to derive regularized SSE with posterior and prior distribution (MAP)) from Bayes theorem) that we have (1) $p(\mathbf{w}|\mathbf{x},\mathbf{t}, α, β) ∝ p(\mathbf{t}|\mathbf{x}, \mathbf{w}, β)p(w|α)$ , where $x,t,w $ are vectors But from Bayes theorem i have $P(model|data)∝P(data|model)P(model)$ what confuses me here is how they use different parameters in the prior and likelihood. For example in the posterior $w = model$ but in the likelihood $model = t,w,β $ Maybe i just missunderstand something and a hint in the right direction would also be appreciated.
