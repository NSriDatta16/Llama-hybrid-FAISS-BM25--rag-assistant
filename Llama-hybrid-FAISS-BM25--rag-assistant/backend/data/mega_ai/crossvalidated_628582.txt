[site]: crossvalidated
[post_id]: 628582
[parent_id]: 
[tags]: 
LLMs' latency and their usability for inference

I am trying to use a transformer decoder (LLM, for simplicity) to label a collection of texts, later to be used for training a classifier. I tried multiple 7B models, which I can save on my local machine and use for inference. Nevertheless, each tag (that is, making a query about a text) takes about 3-5 seconds. How companies deal with this problem? To my understanding, throwing money at the problem by buying an A100 would make the inference faster, but is not practical for the majority of customers who wish to use this technology. Is there other method to make the inference faster?
