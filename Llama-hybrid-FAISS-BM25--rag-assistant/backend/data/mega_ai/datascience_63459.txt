[site]: datascience
[post_id]: 63459
[parent_id]: 
[tags]: 
How to obtain the word vectors optimally

I have a list of strings as shown sent_list = ["Carrefour is in France", "Apple pie is delicious", "Amazon has just delivered", ...] My code to get word embeddings below import spacy nlp = spacy.load("en_trf_bertbaseuncased_lg") for sent in sent_list: print(nlp(sent).vector) This takes considerable time when the list is of large size (>10000). I tried disabling sentencizer within the nlp pipe but with not much improvement. How can this be optimized for shorter run time?
