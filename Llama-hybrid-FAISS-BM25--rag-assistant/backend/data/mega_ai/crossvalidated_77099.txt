[site]: crossvalidated
[post_id]: 77099
[parent_id]: 
[tags]: 
Testing equality of variances without the data, just from the variances and the mean

Assume I have a physical system in which I am sampling data from either a binomial distribution with a $p$ of 0.5, or from an alternation between a $p$ of 0.25 and 0.75. But the result is such that in this case it appears $p = 0.5$ as well. So lets say for example I do a 1000 coinflips and get 500 heads. So the $\hat{p}=0.5$. Now I want to determine the probability I am sampling from the static case rather than the alternating case by looking at the variance within the data. Because if in the first half of the coinflip series I had noticeably more heads than expected from $p=0.5$ and in the last part less, I want to be able to see that. I have a paper where they propose doing the following : cut the dataset into $M$ windows of $n=5$ coinflips. for each of these windows, determine the number of heads. calculate the variance between these datasets. Or formally: $Var[\hat{h}] = \frac{\sum_{i = 1}^{M} (h_i - \mu_i)^2}{M}$, where $h_i$ is the number of heads within a window of size $n=5$ and $\mu_i$ is the mean of the number of heads in such windows. They then do monte-carlo simulations to calculate a 99.9% confidence interval for the static case in order to be able to say if the deviation is significant. But this is the part I don't like because the data is distributed binomially and Monte-Carlo is computationally expensive. I was wondering if we could use our knowledge that under the null-hypothesis we are looking at a binomial static system instead, as such: calculate the number of windows, e.g. $M=1000/5$. calculate the theoretical variance for each window -> $np(1-p)$ with $n=5$ and $p=0.5$. So this results in a constant of $\frac{5}{4}$ for each window. The variance for the entire dataset should therefore be $Mnp(1-p)$ since variances are additive. Therefore $Var[h]_{(theory)} = \frac{5M}{4}$ from theory. I then have the theoretical variance $Var[h]_{(theory)}$ for the dataset assuming the data draws from a single binomial, and the 'measured' variance $Var[\hat{h}]$ from the actual data itself. How do I now best test if these variances are significantly different? We already know that the means are identical, if this might help. And what's very important to me is that all I am presented with in the end are these two variances, $M$ and the observed $\hat{p}$. I do not want to have to access the actual raw data anymore because the calculations are expensive. Is there a test that will help me draw the conclusion from this information? Things I found where F-test, Levene's test, Bertlett etc., but for the last two I get the impression we need to use the actual data again and the first is said to be valid only for normally distributed data.
