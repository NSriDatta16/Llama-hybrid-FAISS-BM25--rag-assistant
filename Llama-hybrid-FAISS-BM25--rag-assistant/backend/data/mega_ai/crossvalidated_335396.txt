[site]: crossvalidated
[post_id]: 335396
[parent_id]: 
[tags]: 
Why don't we use importance sampling for one step Q-learning?

Why don't we use importance sampling for 1-step Q-learning? Q-learning is off-policy which means that we generate samples with a different policy than we try to optimize. Thus it should be impossible to estimate the expectation of the return for every state-action pair for the target policy by using samples generated with the behavior policy. Here is the update rule for 1-step Q-learning: $Q(s_t,a_t) = Q(s_t,a_t) + \alpha [R_{t+1} + \gamma \max_a{Q(s_{t+1},a) - Q(s_t,a_t)}]$ Here is a link to Sutton's RL book in case you want to look something up.
