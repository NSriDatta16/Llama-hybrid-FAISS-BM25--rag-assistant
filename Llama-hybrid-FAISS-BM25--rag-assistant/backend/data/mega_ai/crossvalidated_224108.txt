[site]: crossvalidated
[post_id]: 224108
[parent_id]: 
[tags]: 
Vanishing gradient in basic 3-layer neural networks?

A 3-layer network has two layers of connections (between input and hidden layers and between hidden and output layer). Doesn't this mean that the gradient "vanishes", at least slighty, when training the connections between input and hidden layer by backpropagation, because the connections between hidden and output layer have already been adapted?
