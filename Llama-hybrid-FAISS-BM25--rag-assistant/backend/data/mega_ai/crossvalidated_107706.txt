[site]: crossvalidated
[post_id]: 107706
[parent_id]: 105811
[tags]: 
Why do you expect the oob error to be unbiased? There's (at least) 1 training case less available for the trees used in the surrogate forest compared to the "original" forest. I'd expect this to lead to a small pessimistic bias roughly comparable to leave-one-out cross-validation. There are roughly $\frac{1}{e} \approx \frac{1}{3}$ of the number of trees of the "original" forest in the surrogate forest that is actually evaluated with the left-out case. Thus, I'd expect higher variance in the prediction, which will cause further pessimistic bias. Both thoughts are closely related to the learning curve of the classifier and application/data in question: the first to the average performance as function of training sample size and the second to the variance around this average curve. All in all, I'd expect you'll at most be able to show formally that oob is an unbiased estimator of the performance of random forests containing $\frac{1}{e} \approx \frac{1}{3}$ of the number of trees of the "original" forest, and being trained on $n - 1$ cases of the original training data. Note also that Breiman uses "unbiased" for out-of-bootstrap the same way as he uses it for cross validation, where we also have a (small) pessimistic bias. Coming from an experimental field, I'm OK with saying that both are practically unbiased as the bias is usually much less of a problem than the variance (you're probably not using random forests if you have the luxury of having plenty of cases).
