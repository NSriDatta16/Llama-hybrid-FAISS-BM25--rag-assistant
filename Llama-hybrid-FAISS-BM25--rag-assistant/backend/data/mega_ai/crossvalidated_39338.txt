[site]: crossvalidated
[post_id]: 39338
[parent_id]: 39316
[tags]: 
Ultimately we'd need to see the weighting algorithm itself to answer this question; otherwise we wouldn't know if a particular input transform invalidated the weighting algorithm producing the final score. That said: First thing to consider before changing the outcomes of that weighting scheme: Is that particular use of variable weighting and limited point allocation an established practice in your particular research domain? If it is then you should be very careful changing the variable inputs directly. You could invalidate the methodology altogether. At minimum it will change the scaling of that variable and have implications for comparing your results to other results using the original methodology. If so and your intent is to remove people who didn't establish any preference across the 10 items then I'd create a new dichotomous/binary (dummy) variable identifying them and run your tests filtering on that or as a covariate. Perhaps you can retrain your weights filtering those people in and out? Addressing the fixed sum issue If it's not a common practice and instead a field experiment by you or the survey designers that you're now trying to work around you have a couple of options beyond creation of new variable identifying people who just split points evenly, per the above point. Transform within items - one example would be to recenter the range on 0, running -50 to 50, then take their absolute value or square the values. Again, such methods probably invalidate the scoring algorithm. Transform across items - take the average of all 10 scores and then transform each individual variable by re-scoring it to the difference from the mean. This becomes a measure of relative preference/sentiment/etc across the 10 items, and again could invalidate the methodology. Do #2 to create a new single measure in addition to the weighted output score, and then scale the output score by this new measure. This may or may not be a good idea depending on what the scoring algorithm itself is doing - you have to evaluate that closely. A final option of course is to revisit the scoring algorithm itself if it's not part of an accepted methodology in your domain and alter so that it best suits your needs, using some of the techniques above.
