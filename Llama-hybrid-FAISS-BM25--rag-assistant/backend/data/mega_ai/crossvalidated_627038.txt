[site]: crossvalidated
[post_id]: 627038
[parent_id]: 264904
[tags]: 
I think about it this way: Essentially Q-learning is SARSA without an explicit policy. Instead we have an implicit/induced policy: $$ \pi(s) = argmax_{a} Q(s, a) $$ which will have an optimal value given by the value of Q at that $a$ : $$ V(s) = max_{a} Q(s, a) $$ In SARSA, you can use this same expression to get a policy, but you utilize that $\pi$ for a while to improve Q before a policy refresh, in a similar alternating-optimization scheme as policy iteration (not to confuse everyone with a kinda extraneous piece of terminology/methodology). In Q-learning, changes to Q are reflected in behavior immediately, because instead of plugging $a_{k+1} = \pi(s_{k+1})$ in to $Q(s_{k+1}, a_{k+1})$ , you instead choose actions online by iterating over the action space and then use the resultant optimal quality, $max_{a} Q(s_{k+1}, a)$ , in the expression for $Q(s_k, a_k)$ . So if you do something off-policy in SARSA, the temporal difference target (the argument inside the expected value), which estimates the quality of the current state: $$ Q(s_k, a_k) = \mathbb{E}[r_{k} + \gamma \ Q(s_{k+1}, a_{k+1})] $$ will be thrown off w.r.t. $\pi$ , and you're degrading Q w.r.t. $\pi$ . But in Q-learning there is no chance for the temporal difference target on the right of: $$ Q(s_k, a_k) = \mathbb{E}[r_{k} + \gamma \ max_{a} Q(s_{k+1}, a)] $$ to diverge from the behavior we'd choose. So if we play back sub-optimal actions in Q-learning, we'll change entries/values of Q all over the place, which makes it richer and better. But in SARSA we're only really allowed to update Q along $\pi$ -optimal trajectories, lest we throw off the alternating optimization, so we can't make full use of these off-optimal experiences.
