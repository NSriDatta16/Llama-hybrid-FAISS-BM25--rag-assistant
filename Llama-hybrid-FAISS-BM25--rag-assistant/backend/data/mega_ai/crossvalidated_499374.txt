[site]: crossvalidated
[post_id]: 499374
[parent_id]: 
[tags]: 
Multi-walker MCMC

I just have a brief question regarding Markov Chain Monte Carlo with multiple walkers. I'm currently using this technique to calculate integrals and I'm not 100% on how to combine the statistics of each walker's expectation values. For example, let's say I'm integrating some function $f(x)$ with samples from a probability distribution $p(x)$ . If I sample $x$ from $p(x)$ the integral becomes a weighted sum by, $$ \int f(x) dx = \frac{1}{N}\sum_{i=1}^{N} \frac{f(x_{i})}{p(x_{i})} $$ Now, of course this will reach the exact integral with infinite samples and because $p(x)$ is normalized. In MCMC, using say Metropolis-Hastings we work with the unnormalized probability $\tilde{p}(x)$ and use a self-normalizing estimator. In MCMC, samples are generated via an accept-reject scheme to form a Markov Chain this chain comes from a single walker which "walks" around the domain collecting samples proportional to $\tilde{p}(x)$ (the unnormalized probability distribution). My question is, if I have multiple walkers collecting samples in their own chains, how exactly can I pool the statistics together to get a global value for the integral? (including an error?). Would it be as simple as saying taking a mean of all the walkers' mean values? and perhaps error propagating the standard deviations of each chain? (i.e. Euclidean norm) What I've currently implemented to calculate the expected value and variance for each chain is done via the blocking technique which allows calculation of those quantities while including the autocorrelation time of the chain itself (as the variance isn't the same as with i.i.d samples) Thank you in advance!
