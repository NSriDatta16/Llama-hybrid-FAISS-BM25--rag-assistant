[site]: crossvalidated
[post_id]: 33550
[parent_id]: 33547
[tags]: 
Suppose $X_1, X_2, \ldots, X_n$ are independent and identically distributed. This is the situation I am pretty sure you are referring to. Let their common mean be $\mu$ and their common variance be $\sigma^2$. Now the sample mean is $X_b=\sum_i X_i/n$. Linearity of expectation shows that the mean of $X_b$ is also $\mu$. The independence assumption implies the variance of $X_b$ is the sum of the variances of its terms. Each such term $X_i/n$ has variance $\sigma^2/n^2$ (because the variance of a constant times a random variable is the constant squared times the variance of the random variable). We have $n$ identically distributed such variables to sum, so each term has that same variance. As a result, we get $n \sigma^2/n^2 = \sigma^2/n$ for the variance of the sample mean. Usually we do not know $\sigma^2$ and so we must estimate it from the data. Depending on the setting, there are various ways to do this. The two most common, general-purpose estimates of $\sigma^2$ are the sample variance $s^2 = \frac{1}{n}\sum_i(X_i-X_b)^2$ and a small multiple of it, $s_u^2 = \frac{n}{n-1}s^2$ (which is an unbiased estimator of $\sigma^2$). Using either one of these in place of $\sigma^2$ in the preceding paragraph and taking the square root gives the standard error in the form of $s/\sqrt{n}$ or $s_u/\sqrt{n}$.
