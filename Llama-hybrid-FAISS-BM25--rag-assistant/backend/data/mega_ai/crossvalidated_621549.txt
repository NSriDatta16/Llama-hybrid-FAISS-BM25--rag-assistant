[site]: crossvalidated
[post_id]: 621549
[parent_id]: 
[tags]: 
How to account for 'Don't Knows' / 'Not applicable' and missing data in survey data analysis and predictions?

First of all, I understand that this topic has been repeatedly covered in Cross Validated, and I have read through the forum but was unable to find a full answer to what I am looking for. Context I have been given the results of a survey that consists of multiple questions using the Likert Scale . This survey was conducted over multiple fixed intervals (e.g. every 6 months). Questions are mostly optional, with only 1-2 mandatory questions that are filled, although this may change from interval to interval. I would like to do an analysis; to use these responses as predictors to a response variable that is binary (binned by me). As a result of this set-up, there are 3 categories of questions that I face: Questions with full responses, all of which are on the Likert Scale Questions where respondents either left the question as blank or filled in 'Not Applicable' Questions where due to survey editions, were not included in the survey and thus 'missing' So far based on what I have read, here are my assumptions : I would like the responses to Likert Scale questions to be ordinal based on question intentions I understand that category 2 and 3 are fundamentally different and should be given different values representing them - a survey response where the data for a question is 'missing' because the respondent decided it did not apply versus not even seeing the question in the first place is different I have decided to combine 'Not Applicable' responses with non-responses on a survey under the label 'NA' given the likely similar intentions of respondents when replying to these questions Lastly, I have read through the methods of imputation - taking the average, using the 'neutral' option, model-based imputation - and found it unsuitable for the purpose of this analysis for either of categories 2 and 3. I've come across this interesting answer on CV by @glen_b and was not fully fleshed out but extremely promising: ... "Don't know" implies that for whatever reason (including actually know knowing) the person didn't choose one of the ordinal responses. So one such modification is you could have some model for that process (chose "don't know" vs "chose one of the other options") and then the usual model for the cases in the second category . Such models may be somewhat similar to hurdle models or zero-inflated models. or this take by @the_scheining ...The safest way ... create an indicator variable for answered/did_not_answer . ... you retain all the information from your survey respondents. Marking N/A may be qualitatively different from filling out a score from 1 to 5. Following which, I have transformed the NA and missing responses respectively to some arbitrary number label within the ordinal scale, and adding additional columns to represent if each response has been answered (0/1) and whether it was in the survey in the first place (0/1). But I am stuck with understanding if these would flow through directly and be picked up by a model (e.g. XGBoost). Data sample Response Q1_resp (1-5 Likert Scale) Q1_resp_not_filled Q1_resp_not_in_survey 00001 3 0 0 00002 5 0 0 00003 11 (Not Appl.) 1 0 ... ... ... ... Question I would like to hear from you whether: If the above assumptions that I made, as well as the proposed method seems like an acceptable practice to you based on your past experiences? How can one model this in the context of models (e.g. XGBoost), would the model be able to differentiate natively or a separate model need to be explicitly defined for the non-responses and missing data? Will be extremely grateful if someone can point me towards the right path on this, thank you.
