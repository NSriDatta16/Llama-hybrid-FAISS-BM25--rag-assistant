[site]: crossvalidated
[post_id]: 124903
[parent_id]: 123503
[tags]: 
If I understand you correctly (please clarify otherwise), $a$ is the data, while $b$, $c$ and $d$ are fitted values from three competing multiple regression models. You'd like to compute $R^2$ for the three models. Assuming they all contain an intercept term, the square of the correlation between each set of fitted values and $a$ will give the $R^2$ for the corresponding model. If they use different numbers of predictors, the $R^2$ values aren't comparable (adding predictors should make $R^2$ higher, even if they're practically useless). If they have the same number of parameters, then given they're on the same data, higher $R^2$ will correspond to a smaller MSE (which is better in the sense that the fit is 'closer'), but choosing between models on the basis of fit alone doesn't necessarily lead to 'better' models (e.g. if you're interested in out of sample predictive accuracy, or if you're interested in actual coverage of prediction intervals, or confidence intervals for the mean, say). Even if the $R^2$'s are very high, it doesn't necessarily make the model 'good' and even if the fit is low, it doesn't necessarily make the model 'bad'. Be warned also that performing model selection (whether by $R^2$ or some other criterion) on the same data you use for inference leads to problems (standard errors tend to be too small, parameter estimates tend to be biased, confidence intervals too narrow, p-values too small, and so on).
