[site]: crossvalidated
[post_id]: 558679
[parent_id]: 558677
[tags]: 
There are several things that you could do: You could include measurement length as a feature in your model and interactions of this feature with other features. In such a case, the model would estimate parameters to "correct" for different length. This has the positive side of having explicit estimate of the effect of measurement length. By this, you could verify if the assumption that something changes with more measurements. Alternatively, you could include the inverse of measurement length as a sample weights in your model (e.g. weights parameter in R's glm , or sample_weight parameter of the fit method for logistic regression in Python's scikit-learn ). This would force the model to pay more importance to longer measurements and less to shorter, proportionally to the length. This is a pretty standard approach. One problem with this solution is that you would need to decide how would like to penalize for measurement length: should the weight be proportional to the inverse of the length or maybe it should be some non-linear transformation..? Finally, you could just ignore the rows with not enough measurements from the analysis. It's not elegant, but if they are unreliable, this could be the simplest solution to consider. Nonetheless, it might be an interesting exercise to compare results of the models with and without those observations to verify how much impact do they have and what is the nature of their impact.
