[site]: crossvalidated
[post_id]: 443359
[parent_id]: 443138
[tags]: 
If RNN need to backprop. through time to see past inputs in sequence in order to decide the output then the vanishing problem occurs. In your case RNN may not need to to use past memory of inputs in the sequence. The issues happen when RNN needs to remember inputs seen 10-20 items ago in the sequence. One of the main reasons for vanishing of learning signal is that the signal propagates multiple times through same neurons and their activation function. Activation function like sigmoid or tanh has zero derivative at extremities so it increase the chance that derivative of backprop signal becomes zero. LSTM solves partially the problem because the recurrent link does not have activation functions in its path.
