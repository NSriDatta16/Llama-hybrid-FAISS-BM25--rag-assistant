[site]: crossvalidated
[post_id]: 179863
[parent_id]: 
[tags]: 
Permutation of input features of SVM, simple logistic and KNN classifiers?

I have come across a journal article, with an impact factor higher than 2.8, in which a very strange training procedure was performed. Since I consider myself a beginner, and since the article is published in what I presume to be a good journal, I questioned my assessment that the procedure performed is indeed wrong or strange. The article tries to solve a [ single class ] classification problem for which there are less than 10 training instances. Each instance consists from 4 consecutive parts (e.g. A, B, C and D). So, the training dataset is in the format: A_1 B_1 C_1 D_1 A_2 B_2 C_2 D_2 ... A_10 B_10 C_10 D_10 The strange thing is, in order to increase the number of the training instances, the order of the parts is permuted so each of the training instances above will produce 24 training instances (i.e. 4! ). One of the instances produced from the first instance, for example, will be: C_1 D_1 A_1 B_1 This is strange and does not make sense at all to me since any feature interaction information will be lost. Am I right? If not, how would SVM, simple logistic and KNN cope with that? Another issue I noticed is that the total number of input features is more than 1000 while they have at most 240 training instance (after permuting the original training instances). I understand that it would not be a big problem for SVM, but would the other classifiers cope with such curse of dimensionality? UPDATE : this is the article in question: Source code and design conformance, design pattern detection from source code by classification approach .
