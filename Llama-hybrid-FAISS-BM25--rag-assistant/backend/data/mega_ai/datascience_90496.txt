[site]: datascience
[post_id]: 90496
[parent_id]: 
[tags]: 
Machine Learning in Practice

I worked on a machine learning project where we dealt with relatively small data sets. I noticed that the way that we tried to increase performance was basically to try out a bunch of different models with different hyperparameters, try out a bunch of different sets of features, etc. Basically, it seemed like we approached the problem fairly randomly and that we had no real theoretical basis for anything we tried. This disillusioned me a fair amount and made me think if this is what machine learning engineers do in practice. Have people found that this is fairly common? How can you work on a machine learning problem in a non-random-try-everything kind of way?
