[site]: crossvalidated
[post_id]: 429423
[parent_id]: 429384
[tags]: 
Your problem is studied in the field of Bayesian experimental design. The question that experimental design faces is, which measure should I make in order to maximize the information gain? Or put in another way, which measure of $E$ is going to allow me to discriminate the most between different values of the parameter $X$ ? A possible way to measure the information gain is the Kullback-Leibler divergence ( https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Bayesian_updating ). Its form is very close to the expression of the entropy that you used. $$ U(\boldsymbol{y}) = \int d^{n}\theta P(\boldsymbol{\theta} | \boldsymbol{y}) \textrm{log}(\frac{P(\boldsymbol{\theta} | \boldsymbol{y})}{P(\boldsymbol{\theta})})$$ which in your case can be written as $$ U(X) = \sum_{i} P(E=E_i|x)\textrm{log}(\frac{P(E=E_i|x)}{P(E=E_i)}) = P(E=A|x)\textrm{log}(\frac{P(E=A|x)}{P(E=A)}) + P(E=B|x)\textrm{log}(\frac{P(E=B|x)}{P(E=B)})$$ More explicitely, $$ U(x=a) = P(E=A|a)\textrm{log}(\frac{P(E=A|a)}{P(A)}) + P(E=B|a)\textrm{log}(\frac{P(E=B|a)}{P(B)})$$ $$ U(x=b) = P(E=A|b)\textrm{log}(\frac{P(E=A|b)}{P(A)}) + P(E=B|b)\textrm{log}(\frac{P(E=B|b)}{P(B)})$$ Substituting the values that you provide and taking into account that $P(E=A)=\alpha P(a) + \beta P(b)$ and that $P(E=B)=(1-\alpha) P(a) + (1-\beta) P(b)$ , we get $$ U(x=a) = \alpha \textrm{log}(\frac{\alpha}{\alpha P(a)+ \beta P(b)}) + (1-\alpha) \textrm{log}(\frac{1-\alpha}{(1-\alpha) P(a)+ (1-\beta) P(b)})$$ $$ U(x=a) = \beta \textrm{log}(\frac{\beta}{\alpha P(a)+ \beta P(b)}) + (1-\beta) \textrm{log}(\frac{1-\beta}{(1-\alpha) P(a)+ (1-\beta) P(b)})$$ Now, if you set $\alpha=\beta$ and take into account that $P(a)+P(b)=1$ , you will see that in fact $U(x=a)=U(x=b)=0$ , as expected. On the other hand, if $\alpha=1$ and $\beta=0$ , $U(x=a)=\textrm{log}(1/P(a))$ and $U(x=b)=\textrm{log}(1/P(b))$ so the information gain still depends on the prior probabilities.
