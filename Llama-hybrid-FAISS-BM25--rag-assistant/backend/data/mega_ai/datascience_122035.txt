[site]: datascience
[post_id]: 122035
[parent_id]: 
[tags]: 
kernel methods and parameter updates

Background information: (it might be helpful to read the first 5 pages of this: https://cs229.stanford.edu/summer2020/cs229-notes3.pdf before answering the question). I’m currently learning machine learning and one of the sources that I’m using are these lecture notes( https://cs229.stanford.edu/summer2020/cs229-notes3.pdf ). The current topic that I'm on is kernel methods. On page 5, the algorithm presented tells you how to update the values of $\beta$ . Once you have the values of $\beta$ you can just compute $\theta^{T}\phi(x)$ (as described in equation 12). Everything in this section makes sense except for one thing: the loop in the algorithm on page 5. The main question is basically when should the loop stop ? Are you supposed to keep on updating the entries of the vector $\beta$ until they seem to converge ? (Sorry if I'm asking something that may seem obvious) Thanks
