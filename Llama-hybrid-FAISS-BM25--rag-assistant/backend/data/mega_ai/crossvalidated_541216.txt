[site]: crossvalidated
[post_id]: 541216
[parent_id]: 
[tags]: 
Why in the stacking of scikit-learn the estimators are fitted on the whole training data?

In chapter 7 of "Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow", the first step of stacking method is spliting the train data into two subsets. The first subset is used to train the predictors in the first layer. Next, the first layer’s predictors are used to make predictions on the second (held-out) set and then create a new training set using these predicted values as input features. Use this new training set to train a blender： But in Stacked generalization part of scikit-learn: "During training, the estimators are fitted on the whole training data X_train. They will be used when calling predict or predict_proba", it seems that scikit-learn use all data to train the "first layer" predictors. Why scikit-learn use this setting?
