[site]: crossvalidated
[post_id]: 418038
[parent_id]: 418029
[tags]: 
Yes, that is normal. However, the differences should be relatively small. There are two main sources of randomness in neural network training: Initialization of the network weight (usually drawn from some normal or uniform distribution with parameters related to the number of hidden units in preceding/followig layers) Ordering of the training samples (they are usually randomly shuffled) Some frameworks allow you to fix the random number generator seed before starting the training, which should ensure reproducibility of the same results. I don't know if that is the case of MATLAB though.
