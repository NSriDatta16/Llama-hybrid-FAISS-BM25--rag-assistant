[site]: crossvalidated
[post_id]: 56571
[parent_id]: 55887
[tags]: 
One of the key problems with neural networks is over-fitting, which means that algorithms that try very hard to find a network that minimises some criterion based on a finite sample of data will end up with a network that works very well for that particular sample of data, but which will have poor generalisation. I am rather wary of using GAs to design neural networks for this reason, especially if they do architecture optimisation at the same time as optimising the weights. I have generally found that training networks (with regularisation) from a number (say 20) of random initial weight vectors and then forming an ensemble of all the resulting networks is generally as good an approach as any. Essentially optimisation is the root of all evil in machine learning, the more of it you do, the more likely you are to end up over-fitting the data.
