[site]: crossvalidated
[post_id]: 582587
[parent_id]: 
[tags]: 
What variable transformations are necessary to ensure coefficients have the same interpretation across regressions?

I have four regressions I performed on four disjoint sets of my data (for background on why I did this, see this question ). Happily, they all look quite similar. I'm happy to put all sets of regression coefficients in a table, but I would also like to present one composite analysis with one graph and one set of confidence intervals. It seems to me like it should be okay to average regression coefficients and the endpoints of confidence intervals (here's someone else saying they did that in another circumstance ). Here's my whole model: RelevanceContrast_brain ~ TargetType * Taught * RelevanceContrast_accuracy + (1|subject/cluster) Brain is a continuous brain measurement, TargetType and Taught are categorical variables, and accuracy is a behavioral response to a likert scale. Relevance was a third categorical variable in the experiment and the contrast refers to the subtraction of values over the levels of relevance. It seems important to me that my predictor and response variables are divided by the standard deviation to make regression coefficients comparable across datasets -- there are probably small differences in their variance due to sampling error. My first question is: must they also be mean-centered? I ask because since my predictor and response variables are both contrasts, while I have been normalizing both variables up to now, I think it's been making the effects harder for human beings to interpret. The temptation to look at a graph and see, for instance, a slope at a negative value for one level of a categorical variable and a positive value for another and to interpret that as a sign flip in the contrast is strong. I'm just not sure if mean centering is also a functional requirement of making sure the coefficients have the same meaning across the four regressions.
