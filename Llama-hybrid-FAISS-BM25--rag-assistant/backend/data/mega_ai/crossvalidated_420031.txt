[site]: crossvalidated
[post_id]: 420031
[parent_id]: 
[tags]: 
Sparsify meaningful data following a Gaussian distribution

This is following a post I made a couple of days ago on the Data Science Stack Exchange . Let's say I have 1-D data following a Gaussian distribution. I want to extract from this database the meaningful information, that is the information that lies far away from the mean. One way to do that is to apply a function (e.g. $x\to |xâˆ’\mu|$ where $\mu$ is the mean of the distribution) and sort the values I get. But I want to keep track of how meaningful the data is as I am collecting it all along this process (as I do the bottom-up approach : I gather the samples one by one and create another (smaller) database). Is there an intuitive measure of this concept ? I thought about comparing the histograms between the initial database and the one I create as I gather the samples. For example, the Wasserstein metric gives us a way to compare two probability distributions. An intuitive effect of the extraction I am making would be that the distance between the histograms does not decrease by much at first, as I only fill the error that does not contribute much to the distance. Is that a good approach ? This it what I obtain from a bit of code (at the end of the post). We clearly see that, as we do not fill the "main area" of information first, the distance do not decrease that much. Next level : If my data is made of groups of samples (still, the entire database follows a Gaussian distribution), what would be the intuitive way to select groups that have meaningful information ? I though about giving each group a weight that is the average of the variance of each sample that is in the group, and then sort the groups with the weights they have. But does this really keep track of the useful information ? Next Next level : What about a n-D data following a (multivariate) Gaussian distribution ? This is kinda theoric. If you have any questions feel free. Any article on the matter is also appreciated. Note : I have access to the whole initial database, thus I know the mean and the standard deviation (or the covariance matrix in the n-D case). I'd say I want to have a uniform distribution in the end (I want to know every information the Gaussian distribution gives me in the same manner). As I have little information far away from the mean, and a lot of information around the mean, I want to focus on what I called the meaningful information. For the Next level , I know that I will have samples in each group that will be close to the mean, so I do focus on the samples that lie far away from the mean. CODE to reproduce the picture (almost, as I do not force a random seed) import numpy as np import matplotlib.pyplot as plt from scipy.stats import wasserstein_distance # Number of samples n_samples = 5000 # Drawing samples from a standard normal distribution and computing initial histogram samples = np.random.normal(0, 1, size=n_samples) histogram_samples, histogram_edges = np.histogram(samples, density=True, bins=500) # Sorted samples sorted_samples = samples[np.argsort(np.apply_along_axis(lambda x : -np.abs(x), axis=0, arr=samples))] # Computing distances as I gather the samples WASSERSTEIN_DISTANCE_RANDOM = np.zeros(n_samples) WASSERSTEIN_DISTANCE_SORTED = np.zeros(n_samples) for collection_samples in range(n_samples) : # Compute ongoing histograms, keeping the same bins as the initial histogram current_histogram_random = np.histogram(samples[:(collection_samples+1)], density=True, bins=histogram_edges)[0] current_histogram_sorted = np.histogram(sorted_samples[:(collection_samples+1)], density=True, bins=histogram_edges)[0] WASSERSTEIN_DISTANCE_RANDOM[collection_samples] = wasserstein_distance(histogram_samples, current_histogram_random) WASSERSTEIN_DISTANCE_SORTED[collection_samples] = wasserstein_distance(histogram_samples, current_histogram_sorted) # Plot fig, ax = plt.subplots() ax.set_xlabel('Number of samples added', fontsize=15) ax.set_ylabel('Wasserstein metric', fontsize=15) ax.plot(np.arange(1, n_samples+1), WASSERSTEIN_DISTANCE_RANDOM, '-', color='red', label='Random') ax.plot(np.arange(1, n_samples+1), WASSERSTEIN_DISTANCE_SORTED, '--', color='blue', label='Sorted') ax.tick_params(size=12) ax.grid(True) ax.legend(prop={'size':15}) plt.show()
