[site]: crossvalidated
[post_id]: 613802
[parent_id]: 
[tags]: 
How bad can the naive derivative estimator be?

Let $f$ be a real valued function defined on a compact set $\mathcal X\subseteq \mathbb R^d $ (and for simplicity let's say $d=1$ for now). We don't know $f$ , but can observe it through the (possibly noisy) values $y_1,\ldots,y_n\in\mathbb R $ it takes at some uniformly sampled $x_1,\ldots,x_n \in \mathcal X$ . A standard way to estimate $f$ is to let $$\hat f := \arg\min_{f\in\mathcal F} \frac 1 n \sum_{i=1}^n (f(x_i)-y_i)^2 + \frac\lambda 2 \mathcal P(f) \tag1$$ Where $\mathcal F $ is a family of candidate functions and $\mathcal P$ some penalty. Now, assume that $f$ is smooth (i.e. differentiable many times) and that the functions in $\mathcal F $ are differentiable. If we are interested in computing $f'$ ,the pointwise derivative of $f$ , a naive way to go about it would be to simply set $$\widehat{f'} := \hat f' \tag2 $$ i.e. simply set our estimate of $f'$ as the pointwise derivative of $\hat f'$ (I call it the naive derivative estimator ). Now, it is pretty clear that $(2)$ is (in theory at least) a bad idea : depending on the noise, the sample size, and our choice of $\mathcal F$ , $\hat f$ might be a poor estimate of $f$ already, and even if it weren't, controlling $\|\hat f - f\|_{L^2} $ a priori doesn't tell us anything about $\|\hat f' - f'\|_{L^2} $ (consider a function like $x\mapsto \varepsilon \sin(Mx)$ for small $\varepsilon$ and large $M$ ). However, in the literature, there are cases where the naive derivative estimator is provably a "good" estimator : If $\hat f$ is a local polynomial estimator , then $\hat f'$ is provably consistent. See here , here , or here . If $\hat f$ is a smoothing spline , then again $\hat f'$ is provably consistent. See here or here (and references therein) Lastly, if $\hat f$ is a kernel ridge regression estimator , it can also be proved that $\hat f'$ is consistent, as shown here and here . Judging from this, it would seem that if the hypothesis space $\mathcal F$ has been reasonably chosen, $\hat f'$ would always be consistent. My question is thus : are there examples (in the literature) where the least-square estimator (1) is consistent while the naive derivative estimator (2) fails to be consistent ? In all the literature I've seen so far, $\hat f'$ is always said to have high variance, but nothing about it ever being inconsistent. Note that I purposefully didn't define "consistent" here (maybe could have used "convergent" instead ?), but think of it as convergence of $\hat f$ to $f$ in some appropriate sense (in probability, in $L^2$ , uniformly...) as $n\to\infty$ . For context, this question is motivated by this older one about the ability of Neural Networks to simultaneously learn a function and its derivative. My naive intuition is that if the hypothesis space $\mathcal F$ is "good enough" (in the sense that functions in $\mathcal F$ are differentiable and can approximate $f$ well), then the derivative estimator should also be "not too bad". The goal of this question is to check whether that intuition is true or not. (I also have an open question on this topic over at MSE.)
