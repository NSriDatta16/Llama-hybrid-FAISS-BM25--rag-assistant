[site]: datascience
[post_id]: 11222
[parent_id]: 
[tags]: 
In Neural Nets, why Use Gradient Methods as Opposed to Other Metaheuristics?

In training deep and shallow neural networks, why are gradient methods (e.g. gradient descent, Nesterov, Newton-Raphson) commonly used, as opposed to other metaheuristics? By metaheuristics I mean methods such as simulated annealing, ant colony optimization, etc., which were developed to avoid getting stuck in a local minima.
