[site]: crossvalidated
[post_id]: 266368
[parent_id]: 
[tags]: 
Deep Learning: Why does increase batch_size cause overfitting and how does one reduce it?

I used to train my model on my local machine, where the memory is only sufficient for 10 examples per batch. However, when I migrated my model to AWS and used a bigger GPU (Tesla K80), I could accomodate a batch size of 32. However, the AWS models all performed very, very poorly with a large indication of overfitting. Why does this happen? The model I am currently using is the inception-resnet-v2 model, and the problem I'm targeting is a computer vision one. One explanation I can think of is that it is probably the batch-norm process that makes it more used to the batch images. As a mitigation, I reduced the batch_norm decay moving average. Also, should I use dropout together with batch_norm? Is this practice common? My training images are around 5000, but I trained for around 60 epochs. Is this considered a lot or should I stop the training earlier?
