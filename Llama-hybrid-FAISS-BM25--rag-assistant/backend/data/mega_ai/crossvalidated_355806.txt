[site]: crossvalidated
[post_id]: 355806
[parent_id]: 355787
[tags]: 
I see what is the reason now. Thanks to @Aksakal , @amoeba, and olooney. So sklearn actually first calculate the covariance matrix of the identity matrix, which is 1/2, -1/2 -1/2, 1/2 And then, it calculates the eigenvalues and eigenvectors of the covariance matrix, which is the PCA results. I mistakenly thought that the results that I feed into the PCA data is the covariance matrix. Also, as mentioned by @olooney, because of perfect multi-colinearity, the explained variance of the second term is almost 0. The result is mostly driven by rounding errors. However, I must point out that sklearn uses covariance matrix to calculate PCA results by default, no correlation matrix.
