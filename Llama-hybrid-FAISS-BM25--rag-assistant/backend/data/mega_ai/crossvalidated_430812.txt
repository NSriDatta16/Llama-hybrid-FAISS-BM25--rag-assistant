[site]: crossvalidated
[post_id]: 430812
[parent_id]: 
[tags]: 
Why K and V are not the same in Transformer attention?

My understanding is for translation task K should be the same with V, but in Transformer K and V are generated by two different(randomly initialized) matrix $W^K, W^V$ , therefore not the same. Can any one tell me why?
