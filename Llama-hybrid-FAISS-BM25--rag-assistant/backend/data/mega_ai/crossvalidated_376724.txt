[site]: crossvalidated
[post_id]: 376724
[parent_id]: 376703
[tags]: 
Interesting question. I am not familiar with any existing solution of it, so let me share some random thoughts about it (maybe someone could give you better answer later on). Similar problem was described by Hadsell, Chopra and LeCun in Dimensionality Reduction by Learning an Invariant Mapping or by Schroff, Kalenichenko and Philbin in FaceNet: A Unified Embedding for Face Recognition and Clustering . In both cases, they trained a Simaese neural network on triplets of images, "anchor", "negative" and "positive", where "positive" was from the same class as "anchor and "negative" from different class. To classify them, they defined so-called triplet-loss , that is defined as $$ \mathcal{L}(x,y,z) = \max(0, \, d(x, y) - d(y, z) + \alpha) $$ where $d$ is a distance, in this case squared Euclidean distance $d(x,y) =\|x-y\|_2^2$ and $\alpha$ is some constant. They were aiming at enforcing margin equal to $\alpha$ between negative and positive pairs. You can check the literature on Siamese networks, maybe you'll find some inspiration or further references up there. In your case, I guess, the simplest thing you can use is something like $$ \mathcal{D}(x,y,z) = \underbrace{\max(\,d(x,y),\, d(y,z)\,)}_\text{largest of the two distances} + \beta \underbrace{| d(x,y)-d(y,z)|}_\text{how much do they differ} $$ where $\beta \ge 0$ is a constant. When the distances would be the same, you would see $| d(x,y)-d(y,z)| =0$ . For points far away from $y$ , the part $\max(\,d(x,y),\, d(y,z)\,)$ will be large. So the best case is that both distances are small and do not differ (small + 0). If both are big, this is large (large + something). If they are small, but the difference is big, it penalizes the difference (small + large). Notice that the size of the difference is relative to the distances. By definition the distances are non-negative, so their difference cannot be larger then the largest of the two distances. So setting $\beta=1$ implicitly assumes that we favor the size of distances over their difference. You can empirically set $\beta$ to something larger then one to make this more balanced if needed.
