[site]: datascience
[post_id]: 85281
[parent_id]: 85242
[tags]: 
Few factors that can cause such differences - A common issue with the default RF FE approach. It's not reliable in situations where potential predictor variables vary in their scale of measurement or their number of categories Read Correlated Features When you have correlated features, RF Tree-based FI approach will divide the importance depending upon how the Tree used the two correlated Features Permutation based method will assign negligible importance to both as skipping anyone will have no impact on the result Feature Interaction If we have two Features with Interaction, Tree based approach will catch the Interaction in successive splits and accordingly divide the importance into two Features Permutation based method will assign high importance to both the Features as dip will be high when anyone is skipped( Assuming additive Interaction) In this case, this is a question to ponder how the FI should be divided when two Features are interacting. Ideally, a third feature(e.g. X1*X2) should be created and should get all the FI . You should try - Identity correlated Features using respective tests e.g.Pearson, Crammers, Spearman etc. Use Permutation approach for all cases. Sklearn has got the option #https://scikit-learn.org/stable/modules/permutation_importance.html from sklearn.inspection import permutation_importance r = permutation_importance(model, X_val, y_val, n_repeats=30) Then if needed, look for any interaction effect (See suggested reading) Suggested reading Beware Default Random Forest Importances Discovering-interaction-effects-in-ensemble Chapter#7 - Feat Engg Interpretable-ml-book
