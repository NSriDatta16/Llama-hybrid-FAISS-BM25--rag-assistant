[site]: crossvalidated
[post_id]: 472921
[parent_id]: 
[tags]: 
Combining two sequences for text classification

I'm doing text classification on comments posted on articles/stories. The two human-labeled classes are appropriate and not appropriate (not the same as happy/angry or any "sentiment" specific mapping). A message/comment can be inappropriate if it's rude, aggressive, etc., but also if it's off-topic (ex: the article is about sports, but the comment is about politics). What I'm wanting to do is input not only the comment into the model, but also the story associated with it, so the model can learn if a comment is off-topic. How would I go about incorporating a 200 word comment AND a 1,000 word story into the same model? I'm currently experimenting with the following architectures that takes the comment as input, and outputs two units (two class classification), based on papers on the subject from the last 3-5 years: # One or more lstm layers [Text sequence as words] => [embedding] => [bi-dir lstm] => [FC] => [sortmax] CNN to lstm [Text sequence as words] => [embedding] => [conv => maxpool] => [bi-dir lstm] => [FC] => [sortmax] I'm thinking of inputting both sequences through the same embedding, but from there, not sure how to combine them - use separate conv blocks and combine the output of the lstm or something else..
