[site]: crossvalidated
[post_id]: 520446
[parent_id]: 406658
[tags]: 
The classic solution to this problem is cross-validation, aka out-of-sample testing . You are absolutely correct that you can't train and test on the same data set, because you're asking your model to predict outcomes it has already seen. You can solve this problem by testing your model on data points it has not seen. The usual methods are: Train your model, then collect more data and see how it does on the new data. Often new data is hard to come by, so this is more of an ideal than a useful approach. Split your data randomly in half - use one half as the training set, one half as the test set. Estimate the error of your model by looking at its average error on the test set, which it has never seen. Split your data into $k$ groups. First, train your model on all but group 1, and use group 1 as a test set. Then, do the same for group 2, etc, until you've performed $k$ tests. This is probably the most popular method, and is known as k-fold cross-validation . There are other CV methods too, as you can see from the wiki page - this is a common problem! Hope that helps.
