[site]: crossvalidated
[post_id]: 486928
[parent_id]: 486901
[tags]: 
These types of questions are difficult to answer but overfitting seems likely. The first error you measure seems to be on the same data you fit the model to, these errors are nearly always too optimistic to say anything about out-of-sample error. You use a highly flexible model with thousands and thousands of parameters. With such a model you can fit almost any training set perfectly. You have thousands of predictors, some set of which is very likely to perfectly separate training data by pure chance. Regularization of some sort might help. There are dozens of ways the neural network people regularize models. There might also not be anything in to learn in your data. It is still possible, even easy, to get excellent training error.
