[site]: crossvalidated
[post_id]: 388268
[parent_id]: 388124
[tags]: 
When facing imbalanced problems there are a number of approaches: resampling (as you already pointed out), reweighting the samples according to the ratio of class samples, adapting/developing an algorithm to be robust against imbalance or using the ROC curve to find the optimal threshold. There are many papers presenting some sampling schemas. One popular one is SMOTE (Synthetic Minority Over-sampling Technique). There are also implementations available to try out. In that paper you see references to previous works and problems with the different approaches. SMOTE makes some strong assumptions when generating data, and, as reported in the paper, there are cases where it actually worsens the performance, but in other cases helps quite a bit. What they show is: one needs a better strategy to oversample the minority class (just sampling with replacement does not help). If combined with undersampling of the majority class, results can be further improved. See also this other question in our forum for an example of how to make logistic regression perform well under this setting. There are a number of papers on how to adapt kNNs to cope with imbalance data sets (for example Class Confidence Weighted kNN Algorithms for Imbalanced Data Sets ), and there are specially designed algorithms for particular tasks like novely detection ( Support Vector Method for Novelty Detection ). There is a wealth of literature on the topic of imbalanced data sets (novelty/anomaly/fraud detection). But I hope these few couple of references help you to get started.
