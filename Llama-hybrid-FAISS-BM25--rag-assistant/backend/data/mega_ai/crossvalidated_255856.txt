[site]: crossvalidated
[post_id]: 255856
[parent_id]: 254779
[tags]: 
A possible solution to your question is answered here: A priori selection of SVM class weights , which suggest a weight inverse proportional to the class size. Whereas I do not know how to determine the best possible weights unless repeatedly trying out several possible combinations, I still want to answer the underlying question: How to improve the performance in this problem? Firstly , it seems that you are not determining the parameters of the classifer by crossvalidation (see " model selection " in scikit-learn). This could account for a terrible result already. Tweaking parameters is crucial. For example you could use SVC with rbf kernel and compare results. Secondly , you might want to use other classifiers before you start optimizing. Unless you have specific research interest in SVC with linear kernel, you should look at other classifiers as well. Thirdly , I recommend to look into boosting. Using boosting means repeatedly building a classifier, where those datapoints which are wrongly classified get a high weight regarding the loss function.
