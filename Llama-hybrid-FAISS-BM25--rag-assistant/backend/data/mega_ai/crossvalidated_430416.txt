[site]: crossvalidated
[post_id]: 430416
[parent_id]: 
[tags]: 
Bayesian Linear Regression, trouble with posterior. Variance equal identity

I am trying to solve the following problem. If $y | \beta \sim N(X \beta, I_n)$ and $\beta \sim N(0, g^{-1}(X^t X)^{-1})$ for $g>0$ . Find $ \pi(\beta|y)$ and show that $E(\beta|y)$ is a function of MLE of $\beta$ . My approach, I proved that to maximize the likelihood, it is equivalent to minimizing the SSE, so the OLS is equal to the MLE. I know normal is conjugate prior. So, $$\pi(\beta|y)= \frac{f(y|\beta)\pi(\beta)}{m(y)} \propto f(y|\beta)\pi(\beta) $$ $$ \pi(\beta|y) \propto \exp\{ - \frac{1}{2}(y-X\beta)^t (y-X\beta) - \frac{1}{2}\beta^t(g^{-1}(X^tX)^{-1})^{-1}\beta \} $$ $$ = \exp\{ - \frac{1}{2}( y^t y -2y^tX \beta+(1+g)\beta^t(X^tX)\beta ) \}$$ and I get stuck. I read about g-priors and I think the a posterior is $N(\frac{1}{1+g}\hat{\beta}, \frac{1}{1+g} (X^t X)^{-1})$ but I'm not sure.
