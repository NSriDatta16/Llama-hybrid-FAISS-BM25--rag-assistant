[site]: datascience
[post_id]: 97883
[parent_id]: 64583
[tags]: 
Data augumentation If the number of text data is small, text data argumentations may be applicable e.g. nlpaug . Applying text summarization, removing stopwords or punctuations would be a simple way to create variations of data. Learning Rate How to Fine-Tune BERT for Text Classification? pointed out the learning rate is the key to avoid Catastrophic Forgetting where the pre-trained knowledge is erased during learning of new knowledge. We find that a lower learning rate, such as 2e-5, is necessary to make BERT overcome the catastrophic forgetting problem. With an aggressive learn rate of 4e-4, the training set fails to converge. Probably this is the reason why the BERT paper used 5e-5, 4e-5, 3e-5, and 2e-5 for fine-tuning . We use a batch size of 32 and fine-tune for 3 epochs over the data for all GLUE tasks. For each task, we selected the best fine-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set Note that the base model pre-training itself used higher learning rate. bert-base-uncased - pretraining The model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size of 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer used is Adam with a learning rate of 1e-4 , β1= 0.9 and β2= 0.999 , a weight decay of 0.01 , learning rate warmup for 10,000 steps and linear decay of the learning rate after. Training TFBertForSequenceClassification with custom X and Y data Epochs The number of epochs would be fairly small. The original paper fine-tuning experiments indicated the amount of time/epochs required were small e.g. 3 epochs for GLUE tasks. In my personal experience, 5 is more or less sufficient for text classification tasks, although it will depends on your data. The number of epochs as well would depends on the metrics to monitor. Batch size The original paper used 32 for fine tuning but it depends on the maximum sequence length too. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding We use a batch size of 32 and fine-tune for 3 epochs over the data for all GLUE tasks. Each word is encoded into a floating point vector of size 768 and there are 12 layers for the BERT/base. If the max 512 length is used, the data may not fit into GPU memory with the batch size 32. Then reduce to 16. If the max length is 128 or 256, then 32 would be a good number. Please check the available GPU memory. Model for fine tuning You can add multiple classification layers on top of the BERT base model but the original paper indicates only one output layer to convert 768 outputs into the number of labels you have, and apparently it is the way widely used when fine-tuning is done on BERT. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding As a result, the pre-trained BERT model can be fine tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task specific architecture modifications. All the parameters in the BERT model will be fine-tuned, but you can try freezing the base model and add more classification layers on top of the BERT base model. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding For each task, we simply plug in the task specific inputs and outputs into BERT and finetune all the parameters end-to-end. Optimizer The original paper also used Adam with weight decay. Huggingface provides AdamWeightDecay (TensorFlow) or AdamW (PyTorch) . Keep using the same optimizer would be sensible although different ones can be tried. The default learning rate is set to the value used at pre-training. Hence need to set to the value for fine-tuning. Related Training TFBertForSequenceClassification with custom X and Y data Trained BERT models perform unpredictably on test set
