[site]: datascience
[post_id]: 126184
[parent_id]: 
[tags]: 
Handling categorical variables for Xgboost?

Currently there seems to be two approaches for handling categorical variables in gbdts: Xgboost as an option, but data need to be encoded properly (integers) Catboost can handle everything provided you specify the columns I tried both, and Catboost has a performance that is significantly above. I can't exclude the difference in perfromance is due to some performance in the algo. However I am wondering if it could be due to that encoding as it is the only difference I use in pipeline. Code is provided below: from sklearn.preprocessing import OrdinalEncoder cat_features = train.dtypes[X.dtypes=='object'].index.values.tolist() enc = OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-2) train[cat_features] = enc.fit_transform(train[cat_features]) test[cat_features] = enc.transform(test[cat_features]) Can someone pinpoint if there is something inherently wrong with this approach, that would prevent Xgboost to perform as well as catboost ?
