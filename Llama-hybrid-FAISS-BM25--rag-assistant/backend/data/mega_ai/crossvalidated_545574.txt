[site]: crossvalidated
[post_id]: 545574
[parent_id]: 545573
[tags]: 
I find an explanation here : ALBERT conjectures that NSP was ineffective because it’s not a difficult task when compared to masked language modeling. In a single task, it mixes both topic prediction and coherence prediction. The topic prediction part is easy to learn because it overlaps with the masked language model loss. Thus, NSP will give higher scores even when it hasn’t learned coherence prediction. And the NSP added some noise to the masked language model[ 1 ].
