[site]: datascience
[post_id]: 67454
[parent_id]: 45757
[tags]: 
In order to consider they can be used together or not, let's see it this way. Discounting is determined by the "discounting factor" or gamma symbol in the paper. This hyper-parameter always exists in the calculation of return. You can adjust your environment to have no discount at all by setting gamma=1, or you can choose to have the discount by setting gamma below 1. Adjusting this is likely to affect your learning performance. For the average reward in the figure2, the paper says "One epoch corresponds to 50000 minibatch weight updates or roughly 30 minutes of training time". During this 30 minutes, the agent will not play for just one episode, but a lot of them. Each played episode generates a return (the total reward which contains that gamma in the calculation). The average reward is calculated directly from those episodes in the same epoch. If you don't like average, you may choose max or min or any other operator. It depends on what you want to see. These two things are two knobs that you can choose to adjust independently.
