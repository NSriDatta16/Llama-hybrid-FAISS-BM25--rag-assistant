[site]: crossvalidated
[post_id]: 292400
[parent_id]: 292388
[tags]: 
My understanding is that PCA does not have a concept of "noise", as opposed to other similar methods like factor analysis or probabilistic PCA that come with a model for the data in terms of a signal + noise decomposition. PCA simply determines how well the data can be represented by projections into lower dimension subspaces, and the off-diagonal elements, as you rightly say, tell us about the (pairwise) multicollinearity. It certainly works better on data coming from a Gaussian distribution than data coming from more irregular distributions, but this isn't a requirement. I wonder what the author would say to the case where we have $n$ observations on one variable $x := (x_1, \dots, x_n)^T$ and our data matrix is $X = (x \hspace{3mm} x) \in \mathbb R^{n \times 2}$, so that $X^T X = x^T x \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}$. This ought to be the perfect setting for PCA since we can losslessly project $X$ into a 1-dimensional subspace. If anything, this is a much better setting than when $X^T X$ is diagonal because then we don't gain anything, and PCA just corresponds to deleting columns of $X$ in order of increasing variance. Could it be that there is some extra context that we are missing from this question, such as how each variable is supposed to be independent so $E(X^T X)$ is diagonal?
