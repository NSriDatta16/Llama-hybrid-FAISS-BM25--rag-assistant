[site]: crossvalidated
[post_id]: 354660
[parent_id]: 354098
[tags]: 
Your question may come from the fact that you are dealing with Odds Ratios and Probabilities which is confusing at first. Since the logistic model is a non linear transformation of $\beta^Tx$ computing the confidence intervals is not as straightforward. Background Recall that for the Logistic regression model Probability of $(Y = 1)$ : $p = \frac{e^{\alpha + \beta_1x_1 + \beta_2 x_2}}{1 + e^{ \alpha + \beta_1x_1 + \beta_2 x_2}}$ Odds of $(Y = 1)$ : $ \left( \frac{p}{1-p}\right) = e^{\alpha + \beta_1x_1 + \beta_2 x_2}$ Log Odds of $(Y = 1)$ : $ \log \left( \frac{p}{1-p}\right) = \alpha + \beta_1x_1 + \beta_2 x_2$ Consider the case where you have a one unit increase in variable $x_1$ , i.e. $x_1 + 1$ , then the new odds are $$ \text{Odds}(Y = 1) = e^{\alpha + \beta_1(x_1 + 1) + \beta_2x_2} = e^{\alpha + \beta_1 x_1 + \beta_1 + \beta_2x_2 } $$ Odds Ratio (OR) are therefore $$ \frac{\text{Odds}(x_1 + 1)}{\text{Odds}(x_1)} = \frac{e^{\alpha + \beta_1(x_1 + 1) + \beta_2x_2} }{e^{\alpha + \beta_1 x_1 + \beta_2x_2}} = e^{\beta_1} $$ Log Odds Ratio = $\beta_1$ Relative risk or (probability ratio) = $\frac{ \frac{e^{\alpha + \beta_1x_1 + \beta_1 + \beta_2 x_2}}{1 + e^{ \alpha + \beta_1x_1 + \beta_1 + \beta_2 x_2}}}{ \frac{e^{\alpha + \beta_1x_1 + \beta_2 x_2}}{1 + e^{ \alpha + \beta_1x_1 + \beta_2 x_2}}}$ Interpreting coefficients How would you interpret the coefficient value $\beta_j$ ? Assuming that everything else remains fixed: For every unit increase in $x_j$ the log-odds ratio increases by $\beta_j$ . For every unit increase in $x_j$ the odds ratio increases by $e^{\beta_j}$ . For every increase of $x_j$ from $k$ to $k + \Delta$ the odds ratio increases by $e^{\beta_j \Delta}$ If the coefficient is negative, then an increase in $x_j$ leads to a decrease in the odds ratio. Confidence intervals for a single parameter $\beta_j$ Do I just need to use $1.96âˆ—SE$ ? Or do I need to convert the SE using an approach described here? Since the parameter $\beta_j$ is estimated using Maxiumum Likelihood Estimation, MLE theory tells us that it is asymptotically normal and hence we can use the large sample Wald confidence interval to get the usual $$ \beta_j \pm z^* SE(\beta_j)$$ Which gives a confidence interval on the log-odds ratio. Using the invariance property of the MLE allows us to exponentiate to get $$ e^{\beta_j \pm z^* SE(\beta_j)}$$ which is a confidence interval on the odds ratio. Note that these intervals are for a single parameter only. If I want to understand the standard-error for both variables how would I consider that? If you include several parameters you can use the Bonferroni procedure, otherwise for all parameters you can use the confidence interval for probability estimates Bonferroni procedure for several parameters If $g$ parameters are to be estimated with family confidence coefficient of approximately $1 - \alpha$ , the joint Bonferroni confidence limits are $$ \beta_g \pm z_{(1 - \frac{\alpha}{2g})}SE(\beta_g)$$ Confidence intervals for probability estimates The logistic model outputs an estimation of the probability of observing a one and we aim to construct a frequentist interval around the true probability $p$ such that $Pr(p_{L} \leq p \leq p_{U}) = .95$ One approach called endpoint transformation does the following: Compute the upper and lower bounds of the confidence interval for the linear combination $x^T\beta$ (using the Wald CI) Apply a monotonic transformation to the endpoints $F(x^T\beta)$ to obtain the probabilities. Since $Pr(x^T\beta) = F(x^T\beta)$ is a monotonic transformation of $x^T\beta$ $$ [Pr(x^T\beta)_L \leq Pr(x^T\beta) \leq Pr(x^T\beta)_U] = [F(x^T\beta)_L \leq F(x^T\beta) \leq F(x^T\beta)_U] $$ Concretely this means computing $\beta^Tx \pm z^* SE(\beta^Tx)$ and then applying the logit transform to the result to get the lower and upper bounds: $$[\frac{e^{x^T\beta - z^* SE(x^T\beta)}}{1 + e^{x^T\beta - z^* SE(x^T\beta)}}, \frac{e^{x^T\beta + z^* SE(x^T\beta)}}{1 + e^{x^T\beta + z^* SE(x^T\beta)}},] $$ The estimated approximate variance of $x^T\beta$ can be calculated using the covariance matrix of the regression coefficients using $$ Var(x^T\beta) = x^T \Sigma x$$ The advantage of this method is that the bounds cannot be outside the range $(0,1)$ There are several other approaches as well, using the delta method, bootstrapping etc.. which each have their own assumptions, advantages and limits. Sources and info My favorite book on this topic is "Applied Linear Statistical Models" by Kutner, Neter, Li, Chapter 14 Otherwise here are a few online sources: Plotting confidence intervals for the predicted probabilities from a logistic regression https://stackoverflow.com/questions/47414842/confidence-interval-of-probability-prediction-from-logistic-regression-statsmode Edit October 2021 - New links https://fdocuments.net/reader/full/5logreg-beamer-online https://jslsoc.sitehost.iu.edu/stata/ci_computations/xulong-prvalue-23aug2005.pdf
