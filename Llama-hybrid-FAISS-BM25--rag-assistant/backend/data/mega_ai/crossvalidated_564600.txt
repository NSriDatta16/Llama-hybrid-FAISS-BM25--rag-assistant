[site]: crossvalidated
[post_id]: 564600
[parent_id]: 563763
[tags]: 
If you think about it: A Graph neural Network used for Node Classification is really similar to a regular neural network. In a regular Neural Network the activation of the last hidden layer will have the dimensions n x f where n is the batchsize and f the feature size (or the output size of that layer). In the final step we want to make a prediction for each vector within the batch. So for each [i,:] . The same applies to GNN here after the convolutions (or message passing), the activations will also be of size n x f . Here n is not the batchsize but the number of nodes in the Graph. And you want to make for some of the predictions. So scaling for GNNs is important for the same reason as it is important for other NNs. Large values can make the training unstable. Generally all gradient-based and distance-based approaches require scaling of variables.
