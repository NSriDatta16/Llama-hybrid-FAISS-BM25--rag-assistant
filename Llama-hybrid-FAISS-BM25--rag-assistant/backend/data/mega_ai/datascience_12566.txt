[site]: datascience
[post_id]: 12566
[parent_id]: 12564
[tags]: 
As far as working with data depends on one's education, expertise, goal and favorite tools, I would answer it within my narrow scope - and trying to keep your track. Framing the problem is an important starting point a lot of people neglect. Even-though it is only the beginning, this should result in first strategies to explore the data. Translate "What I want to do" to "What are the implicit information I need to have to achieve it" Given the information you need, find your way to get it (by decomposing it into tasks and sub-tasks) and the corresponding data to extract it (specific task implies specific signal(s) : structured data, pictures, movies, sounds, texts ...) Along with 1. and 2., you should have a clearer idea of the data you'll deal with and thus the tools you might use (NLP, image processing, time-series, ...) Collecting the data is now easier as it is a implied by the previous task. However, classify mentally your data in the following graph to know what to start with according to your personal trade-off: Direct data are those that can be obtained easily. Indirect are those that require some pre-process (scrapping websites, cropping images, counting number of clicks, ...) The simplicity / complexity of use depends on the data : generally speaking, structured data within arrays are easier to deal with that images. The size of the dot is the reward obtained if you achieve to work on these data, regarding your whole project Exploring and Cleaning the data : there are levels of complexity here. I usually start with standard processes to clean the data (mean/median for missing values, normalization and centering when needed, ...). Meanwhile, I start looking deeper into the data by getting histograms of values, evolution of the mean for time series, word frequency for texts, ... This is task specific but exploration is here to give you hints about your data. Once inspecting them, you should mature your cleaning process. Working on the data : As you said, here comes the fun part. You can choose your favorite tools, or start improving your skills by looking for new concepts ( as a future good data scientist ), to process your data. One reason you don't know what to start with may be that you went too fast over the previous dots - implying what you have to do is still unclear. Get back to them, write the process on a paper until you clearly identify the inputs and outputs you need. Again, generally speaking, it involves the following : Dimensionality reduction (especially for images) and feature design (one-hot encoder, floats or ints, ordinal or cardinal category, ...) Choose of your estimator / model by tuning the hyper-parameters Training with validation methods (cross-validation, Leave One Out, ...) Testing and improving your results Report the results . Not as easy as it sounds, as mentioned here . If is it for your own, having a whole project, started from scratch, is a good reward. Moreover, you may remember your scores when testing the model and how you improve it (which hyperparameters, which model, ...). If it is a well-discussed subject, you can compare to top teams in the world on well-known datasets. Finally, if it is for an employer, I would recommend to start this discussion before getting into the subject - would same time and trouble.
