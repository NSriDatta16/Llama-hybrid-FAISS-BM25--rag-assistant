[site]: datascience
[post_id]: 9485
[parent_id]: 9483
[tags]: 
It seems that XGBoost uses regression trees as base learners by default. XGBoost (or Gradient boosting in general) work by combining multiple of these base learners. Regression trees can not extrapolate the patterns in the training data, so any input above 3 or below 1 will not be predicted correctly in your case. Your model is trained to predict outputs for inputs in the interval [1,3] , an input higher than 3 will be given the same output as 3, and an input less than 1 will be given the same output as 1. Additionally, regression trees do not really see your data as a straight line as they are non-parametric models, which means they can theoretically fit any shape that is more complicated than a straight line. Roughly, a regression tree works by assigning your new input data to some of the training data points it have seen during training, and produce the output based on that. This is in contrast to parametric regressors (like linear regression ) which actually look for the best parameters of a hyperplane (straight line in your case) to fit your data. Linear regression does see your data as a straight line with a slope and an intercept. You can change the base learner of your XGBoost model to a GLM (generalized linear model) by adding "booster":"gblinear" to your model params : import pandas as pd import xgboost as xgb df = pd.DataFrame({'x':[1,2,3], 'y':[10,20,30]}) X_train = df.drop('y',axis=1) Y_train = df['y'] T_train_xgb = xgb.DMatrix(X_train, Y_train) params = {"objective": "reg:linear", "booster":"gblinear"} gbm = xgb.train(dtrain=T_train_xgb,params=params) Y_pred = gbm.predict(xgb.DMatrix(pd.DataFrame({'x':[4,5]}))) print Y_pred In general, to debug why your XGBoost model is behaving in a particular way, see the model parameters : gbm.get_dump() If your base learner is linear model, the get_dump output is : ['bias:\n4.49469\nweight:\n7.85942\n'] In your code above, since you tree base learners, the output will be : ['0:[x Tip : I actually prefer to use xgb.XGBRegressor or xgb.XGBClassifier classes, since they follow the sci-kit learn API. And because sci-kit learn has so many machine learning algorithm implementations, using XGB as an additional library does not disturb my workflow only when I use the sci-kit interface of XGBoost.
