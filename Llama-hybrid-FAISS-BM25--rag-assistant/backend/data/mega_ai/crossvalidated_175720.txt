[site]: crossvalidated
[post_id]: 175720
[parent_id]: 164071
[tags]: 
I don't know what Nadaraya-Watson smoothing is, but I can answer your second question. Then why the "kernelized" regression (like SVM or kernel-ridge-regression) on the contrary will work in high dimension (as in practice people use them successfully on high dim inputs like images)? The key difference is regularization. SVM and kernel ridge regression both impose constraints on model flexibility. The key point in the Elements of Statistical Learning discussion of kNN is that kNN and linear regression are viewed as two extremes of the bias-variance trade-off. Connecting this to SVM, note that the most common SVM with a regularization $C$ hyperparameter is use to constrain the model.
