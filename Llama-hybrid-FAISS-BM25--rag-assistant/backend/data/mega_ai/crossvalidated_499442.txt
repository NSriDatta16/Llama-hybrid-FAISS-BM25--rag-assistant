[site]: crossvalidated
[post_id]: 499442
[parent_id]: 498995
[tags]: 
The decoder generates the target sentence autoregresssively. At the very step, it has only one input, the special beginning of sentence token $\text{ }$ . Using that, you compute the probability distribution of the first word $P(\bullet | \text{ })$ . From that distribution, you take the most probable token (or several most probable ones in beam search) and you have the first token $w_1$ . Therefore, in the next step, you have two embeddings, the self-attention will attend to you can compute distribution $P(\bullet | \text{ }, w_1)$ and so on until you generate the entire target sentence.
