[site]: datascience
[post_id]: 12133
[parent_id]: 12126
[tags]: 
Albeit not wrong, Huang seems to be the only person in the world to use the term "additive hidden nodes". By this, he means that the neuron computes the sum of weighted inputs. In other words, the kind of neural network you're already used to. An RBF neuron, on the other hand, computes a distance (usually the Euclidean distance) from input to some center (which can be thought as the weights if you see them as a vector) and applies a exp(-distÂ²) function in order to obtain a Gaussian activation. Thus, RBF neurons have maximum activation when the center/weights are equal to the inputs.
