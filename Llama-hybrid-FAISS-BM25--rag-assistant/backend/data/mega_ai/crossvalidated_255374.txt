[site]: crossvalidated
[post_id]: 255374
[parent_id]: 124344
[tags]: 
My question is: Can this result be applied when using the RBF or some other non-linear kernel for the SVM? There's no gain. The result lies in the fact that instead of a coefficient vector $\beta_{P\times1}$ you have instead to estimate a $\beta_{N\times1}$, which is smaller. Consider the linear Kernel/Gram matrix decomposition, and notice $V$ is orthonormal: $$K = XX^T=UDV^TVDU^T = UD^2U^T=(UD)(DU^T)= RR^T$$ As shown, applying kernels to the SVM you are already estimating $\beta_{N\times1}$ coefficients. So the solutions are the same, and there's no gain whatsoever. Other kernels are functions of the same dot product, reproducing this result. Basically, any linear Kernel model already has this trick imbued into it. The trick only results in gains when Kernels are not explicitly used.
