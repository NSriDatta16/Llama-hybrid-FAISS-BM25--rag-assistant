[site]: crossvalidated
[post_id]: 52885
[parent_id]: 52875
[tags]: 
If your statistic actually has a t-distribution, then you can identify the parts of the $\mu$-line corresponding to insignificant t-statistics (ones that correspond to the usual interval) by evaluating $t'$ as a function over $\mu$ and seeing where $|t'| This is no different from generating the interval in the ordinary $t$ - the region of $\mu$-values that would result in acceptance of the null at significance level $\alpha$ are those regions which would be inside a $100(1-\alpha)\%$ confidence region. I mention it because it's much easier to see how to answer your question about two real roots. Strictly speaking what we're doing is taking a pivotal quantity Q, and finding an interval for the pivotal quantity and from that backing out an interval for $\mu$. It just happens that in this case a suitable pivotal quantity happens to look exactly like the test statistic in a hypothesis test ... (though the fact that the same quantity based on ML estimation and arranged to give a pivot turns up in both isn't really coincidence). As a result, we can back the interval out the the test statistic, just as you did in your first paragraph. The values of Q that result in it being inside an interval for it then correspond to values of $\mu$ inside our confidence region, even when that's complicated . [While - despite initial appearances - I didn't actually suggest it as a principle here, more generally you can use many test statistics to produce acceptance regions for parameter values (including multiple parameters); e.g you can produce acceptance regions for a pair of location shifts in the Kruskal-Wallis test. These regions are sometimes called consonance intervals (or regions ) rather than confidence intervals , though they are in many cases the same.]
