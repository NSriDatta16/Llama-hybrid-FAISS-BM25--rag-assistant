[site]: crossvalidated
[post_id]: 574511
[parent_id]: 573130
[tags]: 
The bug is not in your implementation of Bayesian linear regression but in how you sample the errors in Y . Aside : You don't cite Pattern Recognition and Machine Learning by Bishop properly. In Section 3.3 on Bayesian linear regression, Bishop assumes that the noise variance is known and that X has a (multivariate) Normal distribution. You violate both of these assumptions by sampling error terms from a mixture of uniform(-0.2, 0) and a spike at 0. This is your error distribution; it's idiosyncratic to say the least. errors = [] for x in X: errors.append(0.2 * np.random.randint(-1, 1) * np.random.rand()) You can notice the issue in your original plot as an unexpected upper bound on the spread of the observations. And here is how to sample the errors properly. alpha0 = 2.5 alpha1 = 0.8 beta = 1 n = 1000 # The predictor has uniform(0, 1) distribution. # Predictors can have any distribution actually. X = np.random.rand(n, 1) # The errors have normal distribution. error = np.random.randn(n, 1) / np.sqrt(beta) y = alpha0 + alpha1 * X + error # Equation (3.10) in Bishop The rest of your code works fine, so I only attach an updated plot of the posterior.
