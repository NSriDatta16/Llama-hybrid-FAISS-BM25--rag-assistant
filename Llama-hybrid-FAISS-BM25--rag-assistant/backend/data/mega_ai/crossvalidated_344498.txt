[site]: crossvalidated
[post_id]: 344498
[parent_id]: 
[tags]: 
Can't deep learning models now be said to be interpretable? Are nodes features?

For statistical and machine learning models, there are multiple levels of interpretability: 1) the algorithm as a whole, 2) parts of the algorithm in general 3) parts of the algorithm on particular inputs, and these three levels split into two parts each, one for training and one for function eval. The last two parts are much closer than to the first. I'm asking about #2, which usually leads to better understanding of #3). (if those are not what 'interpretability' means then what should I be thinking?) As far as interpretability goes, logistic regression is one of the easiest to interpret. Why did this instance pass the threshold? Because that instance had this particular positive feature and it has a larger coefficient in the model. It's so obvious! A neural network is the classic example of a model that is difficult to interpret. What do all those coefficients mean ? They all add up in such complicated crazy ways that it is hard to say what any particular coefficient is really doing. But with all the deep neural nets coming out, it feels like things are becoming clearer. The DL models (for say vision) seem to capture things like edges or orientation in early layers, and in later layers it seems like some nodes are actually semantic (like the proverbial 'grandmother cell' ). For example: ( from 'Learning About Deep Learning' ) This is a graphic ( of many out there ) created by hand for presentation so I am very skeptical. But it is evidence that somebody thinks that is how it works. Maybe in the past there just weren't enough layers for us to find recognizable features; the models were successful, just not easy to post-hoc analyze particular ones. But maybe the graphic is just wishful thinking. Maybe NNs are truly inscrutable. But the many graphics with their nodes labeled with pictures are also really compelling. Do DL nodes really correspond to features?
