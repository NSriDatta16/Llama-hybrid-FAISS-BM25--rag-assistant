[site]: crossvalidated
[post_id]: 325565
[parent_id]: 
[tags]: 
Understanding Calculations in LSTMs

Iâ€™m trying understand LSTMs better: When we set the LSTM units in Keras or Tensorflow as: model.add(LSTM(256)) or lstm = tf.contrib.rnn.BasicLSTMCell(256) We are actually setting the dimensions of all 4 Weights (W) in one single LSTM cell, is that right? eg. Let x have 10 dimensions and batch size = 5 or x.shape=(10, 5) where batch size is the 2nd dimension. In this forget gate example, we are setting W(f).shape = (256,266) and [h(t-1), x(t)].shape = (266,5), and the hidden layer h(t-1).shape = (256,5). If we use the add notation instead of concatenation and ignoring bias: f(t) = sigmoid(W * x(t) + U * h(t-1)) W.shape = (256,10) and U.shape = (256,256) Is my understanding correct?
