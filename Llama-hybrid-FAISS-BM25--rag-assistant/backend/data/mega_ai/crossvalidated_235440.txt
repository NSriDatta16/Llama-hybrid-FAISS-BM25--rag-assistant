[site]: crossvalidated
[post_id]: 235440
[parent_id]: 235434
[tags]: 
The likelihood function can be write as: $L(\mathbb{x}|\lambda) \propto \prod_{i=1}^N e^{\lambda}x^{x_i}$ $\propto$ means you can ignore the normalize constant at this step. Prior: $p(\lambda) \propto \frac{1}{a} \propto 1$ Therefore, the posterior distribution is: $\pi(\lambda | \mathbb{x}) \propto L(\mathbb{x}|\lambda) \times p(\lambda)$ $\pi(\lambda | \mathbb{x}) \propto e^{N\lambda}x^{\sum_{i=1}^Nx_i}$ If you are going to just write down the Bayes rule, then it is like this. But if you want to implement MCMC, then you need to consider if a non-informative prior is proper.
