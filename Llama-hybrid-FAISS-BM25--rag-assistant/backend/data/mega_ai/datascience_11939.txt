[site]: datascience
[post_id]: 11939
[parent_id]: 11919
[tags]: 
The selected answer is fantastic, but I would like to add two things: It has been observed that averaging human predictions gives better predictions than any individual prediction. This is known as the wisdom of the crowd . Now, you could argue that it is because some people have different information, so you are effectively averaging information. But no, this is true even for tasks such as guessing the number of beans in a jar. I hypothesize it has to do with some of the reasons given above about data mining models. Some techniques such as the dropout method in neural networks (where in each iteration during training you use only a chunk of your neural network) give results similar to an ensemble of neural networks. The rationale is that you are effectively forcing nodes to do the same predictor work as the other nodes, effectively creating a meta-ensemble. I am saying this to make the point that we may be able to introduce some of the advantages of ensembles in traditional models.
