[site]: datascience
[post_id]: 15387
[parent_id]: 
[tags]: 
How to deal with large training data?

Currently, I use image files and transform them into a *.npy file(saved as a numpy array) as training data. At present this training data set is nearly 3GB. Now I have more image files, so the training data set will become even larger, maybe up to 40 GB. And I am afraid that the *.npy files can not save this big of a data file. Is there another possible way to store such a large file? Since I use Keras to build a neural network model, is it acceptable to split the training data into small parts so as to train the model without having to use all the training data? Does this sound like a reasonable approach?
