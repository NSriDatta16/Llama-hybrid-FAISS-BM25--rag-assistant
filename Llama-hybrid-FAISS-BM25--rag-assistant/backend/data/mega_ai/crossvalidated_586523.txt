[site]: crossvalidated
[post_id]: 586523
[parent_id]: 586519
[tags]: 
The main problem in ML is that you always have too little data. If you would have more, you would be able to improve the performance of your model. Therefore, you want to waste as little data as possible, which comes down to getting as much samples in the training data as possible. The problem with putting all data in the training set is that you will be unable to say how good your model actually works on unseen data (the generalisation performance). That is where test (and validation) data comes into play. By taking away some of the data that you could use for training, you can create a dataset of unseen samples to estimate the generalisation performance of your model. This means that you sacrifice some of the data you could have improved your model with to tell how good your model will be. This creates a trade-off between training and evaluation: you need as much training data as possible for improving the model. However, if all data would be used for training, we will not be able to estimate its generalisation performance. you want to have as much test data as possible to get a better estimate of the generalisation performance of your model. However, if all data would be used for evaluation, our models would essentially have to make random predictions. Since it is typically more important to get a good model than estimating its performance, the training dataset will be larger than the test (and validation) dataset. Therefore, the test set would be a small (the smallest possible) subset of data that represents the data distribution well enough to obtain reasonable estimates and the rest of the data makes up the training dataset. Note that this also assumes that the dataset is large enough and is somewhat clean. E.g. if there would be a class that has only 1 sample, it would not be possible/meaningful to extract a test set, or at least you would not be able to assess the generalisation performance for this class. That is why you will normally have more training than test (or validation) data. Metrics can be compared because they are typically averaged over the number of samples.
