[site]: crossvalidated
[post_id]: 605886
[parent_id]: 
[tags]: 
How can we deal with features that are vectors in machine learning?

It is often the case that the predictors are variables that take continuous or categorical values in machine learning. However, if the predictors or some of them are vectors, how can we deal with it? Take interpolation in traditional practices for an example, we use the neighbors to predict the values at the center location $x_0$ , denoted briefly here as $f:(x_1,x_2,...,x_n) \rightarrow x_0$ . We know that the relative location/distance of $x_i,i=1,2,...$ with respect to $x_0$ is important information for an effective interpolation. Therefore, I want to introduce the spatial coordinates into the predictors for modeling, i.e., $f:[(x_1,xcoord_1,ycoord_1),(x_2,xcoord_2,ycoord_2),...,(x_n,xcoord_n,ycoord_n)] \rightarrow x_0$ . I did not see this type of models or their invariants in machine learning before. Is there any one who can give some ideas to deal with the vectors or take advantage of supportive information about each predictor?
