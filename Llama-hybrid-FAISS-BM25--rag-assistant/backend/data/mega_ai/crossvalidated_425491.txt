[site]: crossvalidated
[post_id]: 425491
[parent_id]: 405636
[tags]: 
A loop is your only option here if you have not saved your word embeddings in any other format such as a binary file. Just use a list comprehension which should be fairly quick even with 2m entries. Assuming your dictionary is named 'd' you could do the following: import numpy as np emb = np.array([list(item.values()) for item in d.values()]) Once you converted the dictionary values into a numpy array you could normalize your data using some convenient tools from sklearn such as minmax scaler: from sklearn.preprocessing import minmax_scale emb_scaled = minmax_scale(emb, feature_range=(-1, 1))
