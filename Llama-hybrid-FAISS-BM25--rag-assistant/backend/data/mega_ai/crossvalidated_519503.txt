[site]: crossvalidated
[post_id]: 519503
[parent_id]: 519501
[tags]: 
There are several forms of warm start. The basic form of warm start in which you copy all the weights from the source model to your target model. This will usually fails if the architecture is different. You can partially warm start e.g. copy embedding tables from a pre trained model and use these embedding as an input to your new model (Either trainable or not) Another approach, which doesn't require the same architecture, is to use the student teacher approach or knowledge distillation, in which you have a teacher which is the source model. And a student that try to learn from this teacher. The basic idea is to add an auxiliary task to the student in addition to his regular prediction task. In the auxiliary task the student is trying to predict the teacher answer to this problem. So you will a loss function with two terms: 1) the regular loss of your task and 2)trying to predict the answer of the teacher (the prediction of the source model for the exact same input) For more info, see here an interesting article regarding these methods: https://www.microsoft.com/en-us/research/blog/three-mysteries-in-deep-learning-ensemble-knowledge-distillation-and-self-distillation/
