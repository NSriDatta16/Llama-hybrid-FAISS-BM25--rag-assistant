[site]: crossvalidated
[post_id]: 261694
[parent_id]: 
[tags]: 
Is there any hypothetical way in which regularization (by weight decay) could benefit training (!) set performance?

I'd like to preface my question by saying that I understand that the reason for introducing a regularization term to the training criterion is to avoid overfitting, with the effect that (usually) model performance on the test set is improved, i.e. the goal being to increase the model's 'generalization' capacity. However, while exploring different hyperparameters in my neural network models, I sometimes get the impression that my network's performance on the training set can also benefit from adding regularization -- L2 regularization, in my case -- compared to not adding any penalty at all. I realize that the most likely explanation is that I just see patterns where there are none, but I wanted to know if there is any formal explanation why, hypothetically, a regularization penalty could help during the training phase as well -- or, contrary, why it is impossible for such a thing to happen. Note that by 'help during training' I don't necessarily mean 'final performance on the train data', but perhaps just 'faster convergence' to a good solution. So for example (I'm completely guessing here, my apologies), can we exclude the possibility that adding a penalty, which 'pulls' the weights towards a prior value, might help during training by allowing the model to escape a local minimum or saddle point slightly faster? As I mentioned already: I can see why a regularization penalty shouldn't (or rather: cannot) help with training set performance, but that understanding is based on my limited 'static' understanding of the model. Once I include the training process I fail to see if there would be any way that regularization could be useful in achieving faster convergence/less likelihood of training getting 'stuck'.
