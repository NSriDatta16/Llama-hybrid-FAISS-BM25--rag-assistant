[site]: crossvalidated
[post_id]: 66815
[parent_id]: 65933
[tags]: 
The Kullback-Leibler Information inequality holds for the logarithms of any two densities, irrespective of whether one of them is the "true" density describing some population. Therefore, it is equally true that $$E_1\left\{ \log f\left(x;\theta_0\right)-\log f\left(x;\theta_1\right)\right\} \leq0$$ where I have indexed by $1$ the "other" density. We should view these as likelihood functions of the $\theta$'s treated as random variables, not as densities of $X$. This inequality simply states (in totally unscientific terms) that, when you take the expectation of a concave function of "yourself" w.r.t "yourself" you obtain a value that is greater or equal than when you take the expectation of the same concave function of "yourself" w.r.t to "anybody else". This is a direct mathematical result holding under its specific premises - it is not a mathematical formulation of some intuitive truth. As for the deeper intuition regarding "true" and "not true" densities: A density describes the allocation of probabilities among the population. Assume we deal with discrete random variables, where the density actually gives the probabilities. So "average density" would mean "average probability". What kind of intuition can be attached to "average probability of the elements of a population/sample"? I don't see any - so to expect that this "average probability" should be greater when we sum-product the true density with itself compared to when we sum-product any other density with the true one, also bears no intuition. Why should in general hold that ($f(x)$ is the "true" density) $$ \sum_i \left[f(x_i)\right]^2 - \sum_i f(x_i)g(x_i) >?0$$
