[site]: crossvalidated
[post_id]: 171336
[parent_id]: 68596
[tags]: 
The Jeffreys (posterior) distribution is quite nice to do inference in a linear regression model with location-scale errors. Inference based on the Jeffreys distribution achieves very good frequentist properties: it provides confidence intervals whose coverage is close to the nominal coverage, even for very small sample sizes. Let $\Phi$ be any derivable cdf and $\phi=\Phi'$ the corresponding pdf. Consider the linear regression model $Y=X\beta+\sigma\epsilon$ with $\epsilon_i \sim_{\text{iid}} \mathrm{d}\Phi$. The Jeffreys posterior distribution is given, up to a proportionality constant, by: $$ \pi(\beta, \sigma \mid y) \propto \frac{1}{\sigma^{n+1}} \prod_{i=1}^n \phi\left(\frac{y_i-\mu_i}{\sigma}\right) =: f(\beta, \sigma \mid y) $$ where $\mu_i=X_i \beta$ is the expected value of $y_i$. The problem, given a set $A$ in the $q$-dimensional parameter space $\Theta$ (with $q=p+1$ since parameters are $\beta_1$, $\ldots$, $\beta_p$ and $\sigma$), is to evaluate $$ \int_A \pi(\beta, \sigma \mid y) \mathrm{d}\beta \mathrm{d}\sigma = \frac{\int_A f(\beta, \sigma \mid y)\mathrm{d}\beta \mathrm{d}\sigma}{\int_\Theta f(\beta, \sigma \mid y)\mathrm{d}\beta\mathrm{d}\sigma} \approx ? $$ Change of variables It is possible to transform this integral to an integral on ${[0,1]}^q$ as follows. The key point is the fact that $$ \frac{1}{\sigma^{q+1}}\prod_{i=1}^q\phi\left(\frac{y_i-\mu_i}{\sigma}\right) $$ is, up to a proportionality constant not depending on $(\beta,\sigma)$, the Jacobian of the function $$ F\colon(\beta,\sigma)\mapsto \left(\Phi\left(\frac{y_1-\mu_1}{\sigma}\right), \ldots, \Phi\left(\frac{y_q-\mu_q}{\sigma}\right)\right) \in {[0,1]}^q. $$ I have not tried to prove this point, but one can numerically check it, for a simple linear regression for example: x Thus, $$ \begin{align} \int_A f(\beta, \sigma \mid y)d\beta d\sigma & \propto \int_A \bigl|\det J_F(\mu,\sigma)\bigr| \frac{1}{\sigma^{n-q}} \prod_{i=q+1}^n \phi\left(\frac{y_i-\mu_i}{\sigma}\right) \mathrm{d}\beta\mathrm{d}\sigma \\ & = \int_{F(A)} g\bigl(F^{-1}(u_1, \ldots, u_q)\bigr)\mathrm{d}u_1\ldots\mathrm{d}u_q \end{align} $$ where $g(\beta,\sigma)=\frac{1}{\sigma^{q+1}} \prod_{i=1}^q \phi\left(\frac{y_i-\mu_i}{\sigma}\right)$. It is not difficult to get the inverse of $F$: $$ F^{-1}(u_1, \ldots, u_q) = {(\beta,\sigma)}' = {(H'H)}^{-1}H'y_{1:q} $$ where the matrix $H$ is $H=\left[ X_{1:q}, {\bigl(\Phi^{-1}(u_i)\bigr)}_{i\in(1:q)}\right]$. Note that $F^{-1}(u_1, \ldots, u_q)$ yields $\sigma 0$ has Lesbegue measure $1/2$. In fact, the Jeffreys distribution for a location-scale linear regression is the same as the fiducial distribution . The method I present is a particular case of the general method given in the paper Computational issues of generalized fiducial inference by Hannig & al.. But there is a high simplification of the general method in the case of a location-scale linear regression (we can take $K=1$ with the notations of the paper, but I will not develop this point). Algorithm The Jeffreys function below returns an approximation of the Jeffreys distribution for the linear regression model when errors follow a Student distribution with degrees of freedom df , to be set by the user. For df=Inf (default), this is the Gaussian linear regression; for df=1 this is the Cauchy linear regression. In the Gaussian case df=Inf , we can compare the results to the exact Jeffreys distribution which is known and elementary. Moreover the inference based on the Jeffreys distribution in the Gaussian case is the same as the usual least-squares inference (as we will see on examples). By default, the X matrix is the matrix of the intercept-only model y~1 . The approximation is obtained by a Riemann-like integration on ${[0,1]}^q$ using a uniform partition into hypercubes. The partition is controlled by the argument L , giving the number of centers of the hypercubes on each coordinate (hence there are $L^q$ hypercubes). #' parameters: y (sample), X (model matrix), L (number of points per coordinate) Jeffreys 1),] # outputs M 0 J 0){ # sigma>0 counter The function returns the values of $(\beta,\sigma)$ corresponding to every hypercube center in the partition of ${[0,1]}^q$. It also computes the values of the integrand evaluated at every center in the vector J , and returns the normalized vector of weights W=J/sum(J) . We will see how to deal with these outputs on some examples. First example: Gaussian sample Let's try it for an i.i.d. Gaussian sample $y_i \sim_{\text{i.i.d.}} {\cal N}(\mu, \sigma^2)$: set.seed(666) n Now we can treat Mu and Sigma as if they were weighted samples of the Jeffreys distribution, with weights W . The theoretical mean is the sample mean, and our approximation is quite good: sum(W*Mu); mean(y) ## [1] 1.109794 ## [1] 1.110175 We can get the approximate Jeffreys cdf with the ewcdf function (weighted empirical cdf) of the spatstat package, and compare with the theoretical one. Our approximation is quite perfect: ### approximate Jeffreys distribution of Âµ ### F_mu We can get confidence intervals by applying the quantile function to the weighted cdf F_mu . They are theoretically the same as the ususal confidence intervals in Gaussian linear regression, and we indeed get very close results: quantile(F_mu, c(2.5,97.5)/100) ## 2.5% 97.5% ## -0.7143989 2.9309891 confint(lm(y~1)) # theoretically the same ## 2.5 % 97.5 % ## (Intercept) -0.7121603 2.93251 The same for $\sigma$ (knowing the inverse-Gamma distribution of $\sigma^2$): F_sigma Second example: Cauchy sample Now let's try a i.i.d. Cauchy sample with sample size $n=200$. set.seed(666) n Since $n=200$ is not a small sample size, the Jeffreys means are close to the maximum-likelihood estimates: sum(W*Mu); sum(W*Sigma) ## [1] -0.01490355 ## [1] 0.9081371 MASS::fitdistr(y, "cauchy") ## location scale ## -0.01345121 0.89958785 ## ( 0.09185580) ( 0.08874509) The MASS::rlm estimates are not so close: rlmfit Jeffreys confidence intervals are close to the ML asymptotic confidence intervals: F_mu Third example : Gaussian simple linear regression Nice: f Fourth example : Cauchy simple linear regression set.seed(666) y While $n=20$ is not large, the ML estimates of the regression parameters are close to their Jeffreys means, but they are not so close for $\sigma$: X The Jeffreys confidence intervals are close the ML confidence intervals, except for $\sigma$: confint(ML) ## 2.5 % 97.5 % ## beta0 3.137719 5.2660528 ## beta1 1.905495 2.0792868 ## sigma 0.235141 0.9666069 F_Beta0 The MASS::rlm estimates of the regression parameters are rather close to their Jeffreys means too: rlmfit
