[site]: crossvalidated
[post_id]: 403655
[parent_id]: 403502
[tags]: 
I want to emphasis the use of a stacked hybrid approach (CNN + RNN) for processing long sequences : As you may know, 1D CNNs are not sensitive to the order of timesteps (not further than a local scale); of course, by stacking lots of convolution and pooling layers on top of each other, the final layers are able to observe longer sub-sequences of the original input. However, that might not be an effective approach to model long-term dependencies. Although, CNNs are very fast compared to RNNs. On the other hand, RNNs are sensitive to the order of timesteps and therefore can model the temporal dependencies very well. However, they are known to be weak at modeling very long-term dependencies, where a timestep may have a temporal dependency with the timesteps very far back in the input. Further, they are very slow when the number of timesteps is high. So, an effective approach might be to combine CNNs and RNNs in this way: first we use convolution and pooling layers to reduce the dimensionality of the input. This would give us a rather compressed representation of the original input with higher-level features. Then we can feed this shorter 1D sequence to the RNNs for further processing. So we are taking advantage of the speed of the CNNs as well as the representational capabilities of RNNs at the same time. Although, like any other method, you should experiment with this on your specific use case and dataset to find out whether it's effective or not. Here is a rough illustration of this method: -------------------------- - - - long 1D sequence - - - -------------------------- | | v ========================== = = = Conv + Pooling layers = = = ========================== | | v --------------------------- - - - Shorter representations - - (higher-level - - CNN features) - - - --------------------------- | | v =========================== = = = (stack of) RNN layers = = = =========================== | | v =============================== = = = classifier, regressor, etc. = = = ===============================
