[site]: crossvalidated
[post_id]: 503244
[parent_id]: 503184
[tags]: 
Under all three hypotheses, $X_1$ and $X_2$ are two uncorrelated random variables with $E[X_1\mid H_i]$ taking on values $0,6,-4$ according as $i=1,2,3$ while $E[X_2\mid H_i]=0$ regardless of the value of $i$ , and $\operatorname{var}(X_1\mid H_i) = \operatorname{var}(X_2\mid H_i)=2$ for all choices of $i$ . Thus, the likelihoods of the three hypotheses are $$L(H_i;x_1,x_2)=f_{X_1,X_2\mid H_i}(x_1,x_2\mid H_i) = f_i(x_1,x_2; H_i)$$ where $f_i(x_1,x_2; H_i)$ is the joint pdf/pmf of $X_1$ and $X_2$ when $H_i$ is the true hypothesis. Since the a priori probabilities of the hypotheses are equal, the Bayesian decision rule is the same as the maximum-likelihood decision rule: Upon observing $(x_1,x_2)$ , choose the hypothesis that has the maximum likelihood, that is, decide that $H_k$ is the true hypothesis where $$k = \arg\max_i L(H_i;x_1,x_2) = \arg\max_i f_i(x_1,x_2; H_i).$$ Note that in general, $X_1$ and $X_2$ cannot be assumed to be (conditionally) independent random variables and so it is not necessarily true that the decision boundaries are straight lines as the OP wants them to be. One important case when the decision boundaries are indeed straight lines occurs when $X_1$ and $X_2$ are assumed to be (conditionally) independent normal random variables with identical variances (and possibly different means) under all hypotheses. In this special case that the OP is considering, the value taken on by $X_2$ is irrelevant as far as the decision is concerned, and we are left with the problem of deciding which of three equally likely hypotheses is true given the value of $X_1$ only. A brief mental visualization of the pdfs of $\mathcal N(0,\sqrt{2}), \mathcal N(6,\sqrt{2}), \mathcal N(-4,\sqrt{2})$ look like immediately suggests that the decision regions are $$ \Gamma_1 = (-2,3], \quad \Gamma_2 = (3,\infty), \quad \Gamma_3 = (-\infty,-2],$$ and the decision boundaries are $x_1=-2$ and $x_1=3$ exactly as the OP's solution manual says. Turning to the discriminant functions that the OP knows about, note that for the specific problem (independent normally distributed observations with identical variance), the discriminant functions are the log-likelihoods and so the decision rule is still the same: Upon observing $(x_1,x_2)$ , choose the hypothesis that has the maximum log -likelihood, that is, decide that $H_k$ is the true hypothesis where $$k = \arg\max_i \ln L(H_i;x_1,x_2) = \arg\max_i \ln f_i(x_1\mid H_i) + \ln f_i(x_2; H_i).$$ Now, since $f_1(x_2; H_1) = f_2(x_2;H_2) = f_3(x_2;H_3)\sim \mathcal N(0,\sqrt{2}),$ we can ignore the $\ln f_i(x_2; H_i)$ term in the discriminant function; it is not relevant to the decision as to which of $\ln L(H_i;x_1,x_2)$ is the largest. So, we are down to deciding which of $$\ln f_1(x_1; H_1) = -\frac{x_1^2}{4}, \quad \ln f_2(x_1;H_2) = -\frac{(x_1-6)^2}{4}, \quad \ln f_3(x_1;H_3)-\frac{(x_1+4)^2}{4}$$ which can be done with a lot of formalism and tortuous mathematical calculations but I prefer the simpler visual approach that says that the three functions are identical downwards parabolas that differ only in where the apexes are (at $0$ , $6$ and $-4$ ), and so the decision boundaries are obviously $x_1=3$ and $x_1=-2$ as derived earlier (also via visualization).
