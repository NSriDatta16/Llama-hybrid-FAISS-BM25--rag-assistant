[site]: crossvalidated
[post_id]: 449967
[parent_id]: 449955
[tags]: 
In Bayesian inference we typically are interested in the conditional distribution of our parameters $\theta$ given data. In the case of regression, we condition on our predictors $X$ and outcomes $y$ . Given known Gaussian noise with known variance, this reduces to inferring the conditional distribution of regression coefficients $\beta$ given the data $X,y$ : $$ p(\beta|X,y) \propto p(y|\beta,X) p(\beta) $$ Since in linear regression $y \sim N(X\beta, \sigma^2)$ , $p(y|\beta, X)$ is the likelihood of a normal distribution. $p(\beta)$ is the prior for regression coefficients, often inverse gamma. Thus we are interested in the probability distribution over the parameters that generated $y$ given $X$ .
