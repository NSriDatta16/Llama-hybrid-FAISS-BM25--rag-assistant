[site]: stackoverflow
[post_id]: 3757769
[parent_id]: 3757165
[tags]: 
The pattern you linked to does indeed match a lot of URLs, both valid and invalid. It's not really a surprise since nearly everything in that regex is optional; as you wrote yourself, it even matches bit.ly , so it's easy to see how it would match lots of non-URL stuff. It doesn't take new Unicode domain names into account, for one (e.g., http://www.m√ºller.de ). It doesn't match valid URLs like http://msdn.microsoft.com/en-us/library/aa752574(VS.85).aspx It doesn't match relative paths (might not be necessary, though) like /cgi-bin/version.pl . It doesn't match mailto: links. It doesn't match URLs like http://1.2.3.4 . Don't even ask about IPv6 :) All in all, regular expressions are NOT the right tool to reliably match or validate URLs. This is a job for a parser. If you can live with many false positive and false negative matches, then regexes are fine. Please read Jan Goyvaerts' excellent essay on this subject: Detecting URLs in a block of text .
