[site]: datascience
[post_id]: 77410
[parent_id]: 77338
[tags]: 
Stacked LSTM is a special version of hierachical recurrent neural networks, where hard-wired memory and gating units help long-term preservation of state information. Hierarchy and recurrence have been explored in many works. One early example is the Neural Abstraction Pyramid, which introduced recurrent computation to hierarchical convolutional neural networks (a.k.a. deep learning). It incorporates partial interpretations from a larger-and-larger context via horizontal and vertical feedback loops in order to iteratively refine an interpretation. It was trained using Backpropagation Through Time to solve several computer vision tasks, such as image denoising, superresolution, and object detection. The recurrent computation is also well suited for maintaining hierarchical state information when processing image sequences. http://www.ais.uni-bonn.de/books/LNCS2766.pdf
