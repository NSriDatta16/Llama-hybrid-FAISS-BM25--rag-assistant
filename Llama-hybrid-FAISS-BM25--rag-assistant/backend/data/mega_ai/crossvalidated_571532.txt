[site]: crossvalidated
[post_id]: 571532
[parent_id]: 
[tags]: 
If I use transformers nn, I don’t need to use CTC loss and language models?

In the creation of an ASR (Automatic Speech Recognition) using transformers neural networks, we don’t need to use a CTC loss and language models, do we? I’ve seen that RNNs do need to use language models and CTC loss. By saying language models, I’m referring to models like kenlm that helps to see if the sentence has sense. But first, transformers pay attention right? It also goes through a Positional embedding, so CTC loss is useful? Why a language model if the attention mechanism already do that…
