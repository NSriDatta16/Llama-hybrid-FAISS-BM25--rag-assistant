[site]: crossvalidated
[post_id]: 340240
[parent_id]: 338904
[tags]: 
Jianlin Cheng, A Neural Network Approach to Ordinal Regression, 2007 and Niu et al., Ordinal Regression with Multiple Output CNN for Age Estimation, 2016 utilize a clever representation of the labels to measure error with cross entropy. They present the the total error as the sum of errors in predicting whether or not the "rank" of a sample $x_i$ is greater than rank $k_i$. In other words, we would generate predictions of vectors with elements $r(x_i) > k_i$, representing the prediction of the classifier for whether or not the rank of the sample is greater than each rank. This becomes a multiclass classification problem and error functions for that problem can be utilized. Total error, then, can be considered a sum of the individual binary classifier loss functions (such as cross-entropy). E.g., Predicted rank = 2 results in a predicted vector = [1, 0, 0]. Actual rank = 3 results in a label vector = [1, 1, 0]. Then calculate loss between each prediction in the vector. Another explanation of this method can be found here .
