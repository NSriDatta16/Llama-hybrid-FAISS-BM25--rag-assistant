[site]: crossvalidated
[post_id]: 153531
[parent_id]: 
[tags]: 
What is batch size in neural network?

I'm using Python Keras package for neural network. This is the link . Is batch_size equals to number of test samples? From Wikipedia we have this information: However, in other cases, evaluating the sum-gradient may require expensive evaluations of the gradients from all summand functions. When the training set is enormous and no simple formulas exist, evaluating the sums of gradients becomes very expensive, because evaluating the gradient requires evaluating all the summand functions' gradients. To economize on the computational cost at every iteration, stochastic gradient descent samples a subset of summand functions at every step. This is very effective in the case of large-scale machine learning problems. Above information is describing test data? Is this same as batch_size in keras (Number of samples per gradient update)?
