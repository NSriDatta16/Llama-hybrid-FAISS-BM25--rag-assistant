[site]: crossvalidated
[post_id]: 544812
[parent_id]: 
[tags]: 
How should one normalize activations of batches before passing them through a similarity/distance function? (like CCA, CKA, OP)

I was reading these two paper: https://arxiv.org/abs/1905.00414 https://arxiv.org/abs/2108.01661 and they mention that before computing the similarity of two layers in a neural network sim(A, B) they preprocess the matrices to be normalized. I am trying to figure out what the standard normalization process is but from reading the papers I take it's the follwing (assuming a matrices A , B of size [n, p1] , [n, p2] ): compute the average (mean) column vector by taking the average of the columns (so in pytorch A.mean(dim=1) since we want to reduce the column dimension) (deduced from paper [a]) then divide by the forbenius norm of the entire matrix (deduced from paper [b]) Quoting paper: a. Let X ∈ Rn×p1 denote a matrix of activations of p1 neu- rons for n examples, and Y ∈ Rn×p2 denote a matrix of activations of p2 neurons for the same n examples. We assume that these matrices have been preprocessed to center the columns . b. Thus for a layer with p1 neurons, we define A ∈ Rp1×n, the matrix of activations of the p1 neurons on n data points, to be that layer’s raw representation of the data. Similarly, let B ∈ Rp2×n be a matrix of the activations of p2 neurons on the same n data points. We center and normalize these representations before computing dissimilarity, per standard practice. Specifically, for a raw representation A we first subtract the mean value from each column, then divide by the Frobenius norm , to produce the normalized representation A∗, used in all our dissimilarity computations. so in pseudocode: def normalize_matrix_for_similarity(X: Tensor, dim: int =1) -> Tensor: """ Normalize matrix of size wrt to the data dimension according to the similarity preprocessing standard. Assumption is that X is of size [n, p]. Otherwise, specify which simension to normalize with dim. ref: https://stats.stackexchange.com/questions/544812/how-should-one-normalize-activations-of-batches-before-passing-them-through-a-si :param X: :return: """ from torch.linalg import norm X_star: Tensor = X - X.mean(dim=dim) / norm(X, "fro") return X_star is this right? Edit: Shouldn't we center first THEN normalize with the Frobenius norm? def __normalize_matrix_for_similarity(X: Tensor, dim: int = 1) -> Tensor: """ Normalize matrix of size wrt to the data dimension according to the similarity preprocessing standard. Assumption is that X is of size [n, d]. Otherwise, specify which simension to normalize with dim. ref: https://stats.stackexchange.com/questions/544812/how-should-one-normalize-activations-of-batches-before-passing-them-through-a-si """ from torch.linalg import norm X_centered: Tensor = (X - X.mean(dim=dim, keepdim=True)) X_star: Tensor = X_centered / norm(X_centered, "fro") return X_star After running experiments I am convinced that it's better to normalize by the centered data. The SVCCA, PWCCA, CKA, OPD all still pass the trivial test that the difference with the same matrix is small and the tolerance I used to have to decrease for OPD is not better. So OPD now is more accurate: Connected to pydev debugger (build 212.5080.64) --- Sanity check: sCCA = 1.0 when using same net twice with same input. -- Should be very very close to 1.0: sim=0.9999997615814209 (cxa_dist_type='svcca') Is it close to 1.0? True Should be very very close to 1.0: sim=1.000000238418579 (cxa_dist_type='pwcca') Is it close to 1.0? True Should be very very close to 1.0: sim=1.0 (cxa_dist_type='lincka') Is it close to 1.0? True Should be very very close to 1.0: sim=0.9999999403953552 (cxa_dist_type='opd') Is it close to 1.0? True --- Sanity check: when number of data points B is smaller than D, then it should be trivial to make similiarty 1.0 (even if nets/matrices are different) Should be very very close to 1.0: sim=1.0 (since we have many features to match the two Xw1, Yw2). Is it close to 1.0? True -- Santity: just makes sure that when low data is present sim is high and afterwards (as n->infty) sim (CCA) converges to the "true" cca value (eventually) sims=[1.0, 0.9999995827674866, 0.9836805462837219, 0.9166610240936279, 0.6240951418876648, 0.38425660133361816, 0.2672860622406006, 0.1903805136680603, 0.12058871984481812, 0.08505415916442871] Should be very very close to 1.0: sim=1.000000238418579 Is it close to 1.0? True sims=[0.2242063283920288, 0.4201244115829468, 0.6278485059738159, 0.9236520528793335, 0.9240158200263977, 0.9821705222129822, 0.9905754327774048, 0.9910891652107239, 0.9903643131256104, 0.9903094172477722, 0.9914378523826599] Also my sanity checks for CCA didn't break when using this so that is what I will use. Details here: https://github.com/moskomule/anatome/pull/11 and plots: This plot shows CCA converges to true similiarity as N increases. This plot shows CCA goes to 1.0 ("overfitting") as D increases (as more features it's easier to match data sets). ref/extras: https://www.youtube.com/watch?v=TBjdvjdS2KM https://github.com/js-d/sim_metric/issues/4 https://github.com/google/svcca
