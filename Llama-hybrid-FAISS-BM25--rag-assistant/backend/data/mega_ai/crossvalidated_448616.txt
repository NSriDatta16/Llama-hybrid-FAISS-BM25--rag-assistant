[site]: crossvalidated
[post_id]: 448616
[parent_id]: 448539
[tags]: 
The question is quite broad, but I will give some starting points: Why bother with AIC/BIC : using cross validation (CV) is (much) more computationally expensive than using AIC/BIC, except for some special cases like leave-one-out cross validation (LOOCV) for regression where it is computationally as cheap as AIC/BIC. Situations where AIC/BIC would not work: AIC/BIC are only available for models estimated using maximum likelihood estimation (MLE), and this is a relative small class of models in the context of machine learning. Connection between CV and AIC/BIC: under some assumptions, AIC is asymptotically equivalent to LOOCV while BIC is asymptotically equivalent to k-fold CV with a specific fold size that depends on the sample size. So under these assumptions, you can save a lot of computations by replacing CV with AIC/BIC. On $R^2_{adj.}$ : According to "Justification for and optimality of $R^2_{adj.}$ as a model selection criterion" , it is questionable whether $R^2_{adj.}$ can be regarded as an optimal model selection criterion. Personally, I would not use it when other alternatives like AIC, BIC or CV are available.
