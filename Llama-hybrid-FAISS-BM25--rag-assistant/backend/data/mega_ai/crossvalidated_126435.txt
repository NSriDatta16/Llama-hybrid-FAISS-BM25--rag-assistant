[site]: crossvalidated
[post_id]: 126435
[parent_id]: 126384
[tags]: 
[NB: my original answer was not entirely correct. I revised it now.] Sparse coding is a problem of representing a collection of $N$ vectors in $d$-dimensional space as linear combinations of some basis vectors, with the requirement that the weights (in linear combinations) should be sparse. Mathematically, if $\mathbf X$ is the data matrix of $N\times d$ size, then you want to find weights $\mathbf W$ and basis vectors $\mathbf T$, such that $$\mathbf X \approx \mathbf W \mathbf T$$ and $\mathbf W$ is a sparse matrix. I think this term is mostly used in neuroscience, where it is a very popular topic nowadays. It started with the by now classic 1996 Nature paper by Olshausen and Field showing that if applied to a set of natural images, then the basis vectors turn out to resemble receptive fields of the mammalian visual cortex neurons remarkably well. Crucially, for this to work, the basis should be overcomplete , i.e. the number of basis vectors is chosen to be $M \gg N$. There is no desire at all to represent the original dataset with as few basis vectors as possible (like in PCA), it is rather the opposite. For more recent computational ideas on sparse coding, see a popular 2007 paper from the Ng group. Olshausen and Field, 1996, Emergence of simple-cell receptive field properties by learning a sparse code for natural images Lee et al., 2007, Efficient sparse coding algorithms Scholarpedia article on sparse coding In sparse PCA one also wants to represent a collection of vectors as a linear combination of basis vectors (aka principal components). Here the focus, as in traditional PCA, is on choosing a small $m\ll N$ number of basis vectors that together "explain as much variance" as possible, i.e. represent the original data as well as possible. And the sparsity is enforced not on the mapping bases$\to$data, but on the mapping data$\to$bases, because the idea is to have PCs that are linear combinations of only small subsets of original features/vectors (to ease the interpretation). Mathematically, the formulation here is to find "encoder" $\mathbf A$ and "decoder" $\mathbf B$ such that $$\mathbf X \approx \mathbf X \mathbf B \mathbf A^\top,$$ subject to sparsity constraints on the decoder $\mathbf B$. Here $\mathbf X \mathbf B$ are sparse PCs (sparse because they are obtained via a sparse decoder) and $\mathbf A$ are loadings. See the seminal paper by Zou, Hastie, and Tibshirani: Zou, Hastie, and Tibshirani, 2006, Sparse Principal Component Analysis As you can see, these two problems are quite different. Sparse coding is not concerned with "decoding" aspect at all, it is only concerned with sparse encoding . Whereas sparse PCA is all about sparse decoding .
