[site]: crossvalidated
[post_id]: 343022
[parent_id]: 343021
[tags]: 
Linear regression is commonly used as a way to introduce the concept of gradient descent. QR factorization is the most common strategy. SVD and Cholesky factorization are other options. See Do we need gradient descent to find the coefficients of a linear regression model In particular, note that the equations that you have written can evince poor numerical conditioning and/or be expensive to compute. QR factorization is less susceptible to conditioning issues (but not immune) and is not too expensive. Neural networks are the most prominent example of applied use of gradient descent, but it is far from the only example. Another example of a problem that requires iterative updates is logistic regression, which does not allow for direct solutions, so typically Newton-Raphson is used. (But GD or its variants might also be used.)
