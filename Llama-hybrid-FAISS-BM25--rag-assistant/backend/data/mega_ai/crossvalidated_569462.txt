[site]: crossvalidated
[post_id]: 569462
[parent_id]: 390551
[tags]: 
It may be easier to dissect your question. You are asking 2 things at once. First, let's make clear Learning (with machine) vs. Programing : Say we have a task: assign a class (cat vs. not-cat) to pictures that have no label. We need to find a relationship between the arrangement of certain pixels to the concept 'cat'. This relationship can be a combination of 'formulas' as you said, with some parameters. Here's the kicker: if you hard program it, you need to know the formulas, and the parameters. In contrast, learning (or estimating) means we extract from the data the parameters (supervised learning) AND also the formulas if possible (deep learning). Deep learning is very flexible because it doesn't require a hypothesis (an assumed formula). Now, Deep learning vs. 'Shallow' learning (regression, for example): In a regression, there is only 1 step learning (estimation). The input is the data, the output is the 'relationship' or formula's set of parameters. And also, you need to know (or assume) the formula beforehand. In deep learning, there are multiple layers, each layer is a learning step. In the first step, the data input is 'converted' (or learned) into a synthetic intermediate output (a bit higher abstraction, loosely speaking, you look for combination of pixels instead of each single pixel). Then in each step, the input is progressively learned ( or 'transformed') into higher abstraction features. These features (which may not be comprehensible to humans) will be used in the last step of 'learning' to produce the output, which is the probability in our example. Loosely speaking, the reason we have multiple layers is that the combination of these layers will approximate the 'formula' for you, we don't need to specify any hypothesis beforehand. This is the meaning of 'learning', in ML or cognitive science: find higher abstraction patterns. In short, deep learning is a process with multiple learning steps , the last step can be a simple regression or any model. The intermediate steps just transform data input into 'higher abstraction' input. Another example of deep learning is NLP, where words are transformed progressively into vectors of numbers with increasing abstraction, such that in the end, the vectors of number can actually represent syntactic (grammar) and semantic (logic) meaning.
