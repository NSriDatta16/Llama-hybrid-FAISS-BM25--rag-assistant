[site]: datascience
[post_id]: 124969
[parent_id]: 
[tags]: 
CLIP Paper or CLAP Paper (I dont understand the loss function) - Can you help?

Can somebody help me understand this Contrastive Learning pretraining paper? This explanation comes from https://arxiv.org/pdf/2206.04769.pdf (page 2). I understand that they apply an audio encoder to the signal and a text encoder to the text (equation 1) then they map both encoder into the same size through a linear layer (equation two) and after since both learned linear projections have the same dimension, you can now compute the dot product or cosine similarity just as they are doing in equation (3) with a temperature factor “Tau”. My question is here: So far C (equation 3) is an NxN similarity matrix between the audio and text representation. I guess this matrix is symmetric right? In equation (4) they compute the average of the “audio loss” and the “text loss” and they say “l_k = 1/N sum ( log (diag (softmax C) , but I ‘m seeing that C is the same for Loss_text and Loss_audio ; why are they computing them twice? Are they the same? I’ve heard that in contrastive pretraining your loss should put every similar elements in a close space as well as dissimilar spaces far from each other, if this is considering only the diagonal matrix how is this actually happening? Well there are a lot of questions but if you can help with at least one of them that’s a huge advance for me.
