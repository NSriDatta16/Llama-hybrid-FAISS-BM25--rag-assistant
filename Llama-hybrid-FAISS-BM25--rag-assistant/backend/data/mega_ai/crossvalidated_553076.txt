[site]: crossvalidated
[post_id]: 553076
[parent_id]: 
[tags]: 
Simple Gradient Descent Project plausibility

I am currently in a numerical analysis class at my university and wanted to tackle a project applying gradient descent. Fair warning: I am new to machine learning, but my professor believed in me, so I had trouble saying no. Please bare with me and correct me wherever possible. I really would appreciate some guidance from you guys! The idea they gave me was to estimate a family of functions, specifically cubics by applying gradient descent on the loss function which will be least squares. I can start with some cubic so I know the outputs/labels. So from my understanding our hypothesis is $h(x) = \sum_{k=0}^{3}\alpha_kx^k$ ? Our loss function is $L(h(x),f(x))=(h(x_i)-f(x_i))^2$ . My professor said it would be best to use the emperical error $L(\theta) = \dfrac{1}{n}\sum_{i=0}^{n}L(h(x_i),f(x_i))$ where $\theta = [\alpha_k]_{k=0}^{3}$ and we apply the gradient descent algorithm to find $\theta^\star=\arg\min L(\theta)$ . So I believe the idea is, Have some known cubic with some gaussian noise. Initiate a cubic with random weights. Calculate the gradients of the least squares for each coefficient/weight. Apply gradient descent to $L(\theta)$ and find the optimal weights? Question 1: In general cubics are not convex. I need to explain why the method works and most of assumptions rely on having a convex function. Or am I misunderstanding something? Question 2: Why is the emperical error method a good idea here? Question/comment 3: Least squares is tripping me up, from my perspective it is no different than MSE, as it seems very similar?
