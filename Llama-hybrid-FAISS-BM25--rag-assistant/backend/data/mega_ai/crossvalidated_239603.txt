[site]: crossvalidated
[post_id]: 239603
[parent_id]: 239571
[tags]: 
The contextual embedding of a word is just the corresponding hidden state of a bi-GRU: In our model the document encoder $f$ is implemented as a bidirectional Gated Recurrent Unit (GRU) network whose hidden states form the contextual word embeddings , that is $f_i(d) = \overrightarrow{f_i}(d) \,\, ||\,\, \overleftarrow{f_i}(d)$, where $||$ denotes vector concatenation and $\overrightarrow{f_i}$ and $\overleftarrow{f_i}$ denote forward and backward contextual embeddings from the respective recurrent networks. In red is the contextual embedding of the first word:
