[site]: crossvalidated
[post_id]: 213726
[parent_id]: 
[tags]: 
Does the skipgram language model try to predict all context words at the same time?

In the skipgram language model ( Mikolov et al., 2013 ), a neural network with one hidden layer tries to predict surrounding words from current words of the corpus. After training, the hidden activation of a word is used as its vector representation. I could now construct training examples by pairing current words with one of their surrounding words each. Each example would then be a pair of two one-hot encoded vectors, i.e. with all zeros except for one element. Here are some training examples: $$ [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0] \rightarrow [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\\ [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0] \rightarrow [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\\ [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0] \rightarrow [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0] $$ Alternatively, I could sum up the surrounding words for each word. Each example would then consist of the one-hot encoded current word and a vector representing the surrounding words where some elements are one and most are zero. For the three examples above, there would be only one example in this case: $$ [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0] \rightarrow [0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0]\\ $$ Which representation makes more sense and why? I think the second one is more efficient and the only down-side I could see is that it cannot represent the same word occuring twice in the surrounding.
