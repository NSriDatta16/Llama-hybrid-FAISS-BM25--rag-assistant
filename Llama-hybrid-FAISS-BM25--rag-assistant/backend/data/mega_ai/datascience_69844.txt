[site]: datascience
[post_id]: 69844
[parent_id]: 
[tags]: 
Transformer-XL architecture

I am a bit perplex from the transformer-XL architecture that is claimed to solve the issue of context fragmantation. I probably understood it wrong but it looks like all the transformer-XL is doing, is being 2 times larger than the normal one, and that you translate at each iterations. Plus a fancy new way of doing positional encoding. Am I interpreting it wrong ?
