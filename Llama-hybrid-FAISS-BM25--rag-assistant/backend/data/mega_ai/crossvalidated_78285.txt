[site]: crossvalidated
[post_id]: 78285
[parent_id]: 
[tags]: 
Comparison of two error distributions to determine "goodness of fit"

I am a physicist who is a few years out of doing his last course in statistics, so I am hoping to get some advice when comparing some data I recently generated. The context is as follows. I have two slightly different theoretical models which I used to generate two sets of data (the numbers are of radiative strengths in multi-electron atoms for various transitions, but not exactly relevant for the mathematics). I also have a corresponding set of "accepted" or "literature" values for these transitions, and I would like to see which of my two models produces results that are closer overall/on average to the "accepted" model. My current metric of comparison is relative error for each transition (i.e. $ \delta_{rel} = \frac{|T_{i,j}-A_j|}{A_j} $ where $T_{i,j}$ is my theoretical result for transition $j$ and theoretical model $i$ and $A_j$ is the accepted result for transition $j$). Seemed like the most obvious place to start. I am now stuck as to how to get some sort of meaning from these relative errors. Currently, I have just taken the arithmetic mean of the relative errors, but my intuition tells me this isn't a rigorous or correct way about this. Would a difference of mean, t-test be appropriate? My hypothesis testing knowledge is a bit rusty, so any guidance here would be appreciated. This seems like it should be a fairly basic question, but I haven't been able to find anything reasonable through my own searches.
