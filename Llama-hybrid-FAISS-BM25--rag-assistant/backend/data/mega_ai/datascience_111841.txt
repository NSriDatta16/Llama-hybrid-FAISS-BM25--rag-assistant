[site]: datascience
[post_id]: 111841
[parent_id]: 
[tags]: 
change parameterization to eliminate weight constraints in neural networks

I am wondering if it makes sense to use a parameterization to eliminate simple weight inequalities, for example if the weights should be $w\geq 0$ , one cound train $\exp w$ over the unconstrained set instead. Also, if $\sum w_i=1$ one could parameterize $\frac{e^{w_i}}{\sum e^{w_i}}$ and optimize over the unconstrained set. While the solutions should be similar to e.g. using a corresponding constraint in tensorflow, I wonder if it makes numerically a difference, and what way is the right one. I guess the values of $\exp$ might explode and it becomes numerically instable.
