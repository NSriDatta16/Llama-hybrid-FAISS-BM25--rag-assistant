[site]: crossvalidated
[post_id]: 510652
[parent_id]: 
[tags]: 
I am curious on " how to 'grey out' certain output units in neural network depending on different observations?"

Typical code examples I have found does something like this : Feature Engineering e.g. One Hot Encoding, Label Encoding etc in pandas data frame. Splitting into X and y [as numpy array] Defining Multi Layered Perceptron Model [in Keras] Compiling MLP model [ using pre defined loss function, optimizer etc.] But all the units in output layer and input layer must be provided and the hidden layers usually will grab all the values from input units and change all output units. Can We 'GREY OUT' or 'FREEZE' certain units of input and output layers which are not applicable for certain observations? . Because, I do not want the neural network to learn from input units of the training samples that are irrelevant or not applicable(To ensure strong expandability of the learned model) I would not want the model to care more about output cells which are not applicable. I want the loss function to compare the y_pred with relevant y_true In theory I have learned about 'DropOut Layers' , 'Freezing Layers' but I have not come across anything like 'DropOut Units in Input/Output Layers'. Any help in terms of code implementation would be appreciated. Sorry for not being able to provide any reproduceable code as i am completely clueless on this issue.
