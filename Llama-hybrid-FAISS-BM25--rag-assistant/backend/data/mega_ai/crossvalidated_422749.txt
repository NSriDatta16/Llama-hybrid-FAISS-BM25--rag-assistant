[site]: crossvalidated
[post_id]: 422749
[parent_id]: 
[tags]: 
Discriminative Models with Class Priors

In discriminative models, we model $p(Y|X)$ directly while in generative models we model $p(X|Y)p(Y)$ where $X$ is the input and $Y$ is the output variable. I am confused when the parameters and priors are introduced. Let's think about logistic regression with $K$ classes. Training: We maximize $\prod_n p(y_n|x_n,\theta_k)$ to estimate the parameters $\theta_k$ where $\lbrace (x_n, y_n), n=1,...,N_k\rbrace$ is the training data of class $k$ . We do this for each class. If we don't place priors on parameters, this is maximum likelihood estimation. Prediction: We calculate $p(y_{new}=k|x_{new}, \theta)$ for each $k\in K$ , and choose the $k$ with the highest probability. Again, we don't place priors on $Y$ , but we directly calculate the posterior of the class label $p(Y|X, \theta)$ , so it is MAP estimation of class labels. Since we don't model the likelihood and priors separately (or we don't assume priors on labels), this is a discriminative model. Here are my questions: Are the explanations above (including terminology and notation) correct? If we place priors on $\theta$ , then we maximize the posterior of $\theta$ , which is $\prod_n \frac{p(y_n|x_n,\theta_k)p(\theta_k)}{p(y_n|x_n)}$ therefore it becomes MAP estimation. Is it correct? And can we ignore the denominator $p(y_n|x_n)$ ? What is the likelihood of parameters $\theta$ ? Is it $p(X|Y, \theta)$ or $p(Y|X, \theta)$ ? If the likelihood of parameters is $p(Y|X, \theta)$ and we want to place priors on class labels, how to deal with $p(Y|X, \theta)p(Y)$ ? Can we use $p(X|Y, \theta)$ instead as the likelihood? In Bishop's PRML book, likelihood for the multiclass logistic regression is written as $p(\mathbf{T}|\theta_1,...,\theta_k)=\prod_n \prod_k p(C_k|x_n)^{t_{n,k}}$ where $\mathbf{T}$ is an $NxK$ matrix of target variables with elements $t_{n,k}$ , and $\mathbf{t}_n$ is a 1-of-K coded target vector for input $x_n$ belonging to class $C_k$ . Can we say that the likelihood does not depend on the input? And how to involve class priors in this case? This can be useful for using a logistic regression or SVM in an MRF model as the unary potentials, and class priors would be the pairwise potentials. Is there a fundamental issue in my understanding? Is there any similar work for combining discriminative and generative models? Instead of the MAP estimations mentioned above, we could use the posterior predictive and resort to approximate inference to avoid overfitting caused by point estimates. Is this correct?
