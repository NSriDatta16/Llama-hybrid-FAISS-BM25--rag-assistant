[site]: stackoverflow
[post_id]: 2185584
[parent_id]: 2185469
[tags]: 
It's been a long time, but as I recall, it has no effect on the mathematics needed to implement the network (assuming you're not working with a network type that for some reason limits any part of the process to non-negative values). One of the advantages is that it makes a larger distinction between inputs, and helps amplify the learning signal. Similarly for outputs. Someone who's done this more recently probably has more to say (like about whether the 0-crossing makes a difference; I think it does). And in reality some of this depends on exactly what type of neural network you're using. I'm assuming you're talking about backprop or a variant thereof.
