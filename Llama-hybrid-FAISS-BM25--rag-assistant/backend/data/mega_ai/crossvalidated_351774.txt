[site]: crossvalidated
[post_id]: 351774
[parent_id]: 351766
[tags]: 
There is a large application of estimation of the maximum $N$ in hydrology, finance and economics where large events are of extreme concern. Usually it takes too long to see these events occur in nature and generally are estimated using various assumptions about the tails of a distribution. In the case of a lottery or any uniformly discrete distribution, the sample minimum and maximum represent all the information about the population so we end up with the MLE estimate of the parameters being the minimum and maximum of the sample itself. It turns out that unless we somehow specify our prior beliefs about the number of tickets (where tickets are consecutive) or perhaps our beliefs about how small/large we believe the numbers will be on the ticket (extremely subjective), or maybe even ask people who have seen lottery tickets about their observations of the smallest and largest tickets they have seen, it is hard to approach this problem without some Bayesian analysis. You are certainly right in saying that as our sample size increase, there is a clearly more information about the distribution of the tickets, i.e. by Law of Large Numbers we will eventually have a better estimate of the sample statistics: minimum and maximum. Let me just give you a similar example incase I didn't come across clear: Suppose there are n cable cars in San Francisco, numbered sequentially from 1 to N. You see a cable car at random; it is numbered 203. You wish to estimate N. Well we could assume the $N$ follows a geometric distribution with some mean $\bar N$ and then use our sample as our data, update our beliefs for $N$ via Bayesian analysis. See Goodman, 1952, for a discussion and references to several versions of this problem, and Jeffreys, 1961, Lee, 1989, and Jaynes, 2003, for Bayesian treatments. Let me know if I can clarify!
