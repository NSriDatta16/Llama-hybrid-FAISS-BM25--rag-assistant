[site]: datascience
[post_id]: 106493
[parent_id]: 106492
[tags]: 
I would discourage such a practice. If you use this for your outcome variable, you are making a wrong distribution assumption. You risk getting nonsense predictions like being in between a cat and a crocodile. With a categorical (multinomial) distribution, you wind up with fractional predictions, yes, but those have reasonable interpretations as class membership probabilities. You can use those probabilities to make classifications. Better yet, you can assess the probability values themselves! (1) (2) If you do this for the features, You are making up an ordering relationship (e.g., $cat ) and allowing for arithmetic when there should not be any ability to add a cat and a dog and get a crocodile. One place where this would be okay is if your software knows to interpret the integers as categories. However, all that means is that the software is smart enough to do the categorical encoding on its own. I suspect that this is more likely to be the case for $y$ than for features.
