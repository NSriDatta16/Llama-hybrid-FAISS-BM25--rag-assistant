[site]: crossvalidated
[post_id]: 614036
[parent_id]: 
[tags]: 
Is there such thing as inter-annotator agreement for a set of rankings?

I'm familiar with inter-annotator agreement (or inter-rater reliability) metrics for data that has categorical annotations, but what about a set of data samples that are ranked by several annotators? Would coefficients like Cohen's kappa still apply here? The specific context that I'm talking about is within machine learning (text generation to be precise). For a set of items, several machine learning models generate explanations about what the items are in textual form. A set of human annotators then rank the explanations made by the different models. There seems to be some subjectivity and disagreement among the annotators, but I'm wondering if there's a way to quantify that.
