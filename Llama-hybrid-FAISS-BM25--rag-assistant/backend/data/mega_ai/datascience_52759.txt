[site]: datascience
[post_id]: 52759
[parent_id]: 35801
[tags]: 
A pipeline could be part of a CI/Continuous Delivery/Continuous Deployment pipeline, or some kind of ETL work, like data load/extract pipeline.The idea is the automation of a group of processes that operate on the data science project you work on.Generally you draw a flowchart, there are such tools, you divide-n-conquer your entire work and set up automation tasks. It is like a flowing belt in manufacturing factory or amazon packaging system or oil pipeline. Normally you can walk through all steps manually, a kind of verification, then you can automate. In my understanding the distinction is based on what you process, either historical data, real-time data or operational logs. Kaggle is composed of a kernel, python environment, jupyter notebook and data science runs; sounds like a CI pipeline, all integrated and reproducible. I am not still comfortable with labelling though.
