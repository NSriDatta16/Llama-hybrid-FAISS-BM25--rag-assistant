[site]: datascience
[post_id]: 46606
[parent_id]: 
[tags]: 
XGBoost hyperparameters depend on number of samples, how can I avoid constantly retraining as I collect more samples?

I'm currently using this: XGBRegressor( learning_rate=0.2, n_estimators=100, max_depth=6, min_split_loss=0.1, min_child_weight=1, reg_alpha=2, reg_lambda=2, scale_pos_weight=1, nthread=3, subsample=0.5, colsample_bytree=0.5 ) With n samples, this works well. With 2n samples, it overfits. With n/2 samples, it underfits. I have to change the learning_rate and reg_alpha whenever I change the number of samples. Is there a systematic way to make the hyperparameters independent of the number of input samples? I could just make the hyperparameters variables dependent on the number of samples, but I'm wondering if there's a better way to do so.
