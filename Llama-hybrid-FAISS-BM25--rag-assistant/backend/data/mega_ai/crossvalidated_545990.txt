[site]: crossvalidated
[post_id]: 545990
[parent_id]: 
[tags]: 
After model selection, can I perform prediction on the test set using all my models, even the "not good ones"?

I refer to this extremely well explained post here. Link to the post The confusion arises because after you have tuned your hyperparameters, and found the best performing classifier, be it a single model, or an convoluted ensemble, you found that that hypothesis $g$ performs best during cross-validation. The idea is then also use this $g$ to predict on X_test to see how well it does on the unseen data. My question is, out of the all the models I trained, say $h_1, h_2, ...$ from various hypothesis space, or to put it simply, I trained a linear model, a tree model, and another ensemble model like Random Forest. Then eventually, I found the random forest to do best on the validation set, at last, I will use this model to predict on X_test to assess it. Why can't I use all 3 trained models to also perform on test set after the dust is settled...? Is it because it can introduce "my own bias" after I realized that maybe the ensembled model didn't get the highest score on X_test?
