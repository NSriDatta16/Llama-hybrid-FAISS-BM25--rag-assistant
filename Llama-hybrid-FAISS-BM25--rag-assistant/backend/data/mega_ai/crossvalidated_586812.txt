[site]: crossvalidated
[post_id]: 586812
[parent_id]: 586753
[tags]: 
Yes, it certainly is possible because you can over-fit the cross-validation statistic when optimising the hyper-parameters. See GC Cawley, NLC Talbot, "On over-fitting in model selection and subsequent selection bias in performance evaluation", The Journal of Machine Learning Research 11, 2079-2107 ( pdf ) [Full disclosure, this paper was written by Mrs Marsupial and I] For different samples of data, the cross-validation error as a function of the hyper-parameters will be different, due to random variations in the sampling, and have different minima: Which can give rise to models that either under- or over-fit the training data (or give good fits): The best solution is to have more data. That makes the variance of the cross-validation estimator lower and improves estimation of the hyper-parameters. This example uses the Least-Squares Support Vector Machine, for convenience, but the standard SVM does pretty much the same thing. The same thing also happens in a regression setting. I haven't tried it for SVM regression (because I don't like it very much), but it has been demonstrated for Gaussian Processes, which are likely to be a better option if the dataset isn't too large. Rekar O Mohammed, Gavin C Cawley, "Over-fitting in model selection with Gaussian process regression", International Conference on Machine Learning and Data Mining in Pattern Recognition, Pages 192-205, 2015 ( pdf )
