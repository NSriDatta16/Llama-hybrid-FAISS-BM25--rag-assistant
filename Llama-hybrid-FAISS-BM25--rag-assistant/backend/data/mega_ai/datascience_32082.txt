[site]: datascience
[post_id]: 32082
[parent_id]: 32066
[tags]: 
There is really nothing special about the backpropagation algorithm in a generative adversarial network (GAN). It is the same as that of a convolutional neural network (CNN), as CNNs are usually what the generator and discriminator of the GAN are made of. I will assume the MNIST toy example for the explanation and I will provide code to get a GAN working below. The GAN A GAN is composed of a discriminator and a generator. Each of these are held constant while training the other. We will thus alternate between training the discriminator and the generator. This is done separately. Training the discriminator is much easier so lets look at that, then we will look at training the generator as you asked in your question. Training the discriminator The discriminator has two output nodes and is used to distinguish real instances from artificial instances. To train the discriminator we will generate $m$ instances using forward passes from the generator, these are the artificial instances, their label will be $y = 0$. To generate these we simply pass a noise vector as the input to the model. We will also use $m$ instances from the real data these will have labels $y = 1$. We can then see that the discriminator is trained exactly the same way as a basic classification CNN with 2 output nodes. I will describe the training process of a CNN below. Training the generator When we train the generator we will keep the discriminator fixed, this is necessary as to not saturate our parameters and make the discriminator too strong to ever beat. So what we essentially have is a CNN (the generator) connected to another CNN (the discriminator). The connecting nodes between these two models is going to be output which will generate the desired images once trained. Do note that the generator wants the instances it will produce in this case to be classified by the discriminator as being from the real distribution, so they will have labels $y = 1$. All together this is just a CNN and the backpropagation will be computed in the exact same way. First we pass through a noise vector, it goes through the generator, some random image gets produced at its output, that then goes through the discriminator and gets classified as $artificial$. But, we are expecting the discriminator to be fooled in this case, so this is an error, it should have been labeled as $real$. We then use backpropagation to get the error contribution of each model parameter. We will then use gradient descent to update all the parameters associated with the generator. Backpropagation This is a method used to compute the contribution of each parameter on the error term. We then use gradient descent to update these parameters such that the next pass through should result in a lower loss rate. Picking the right loss function is essential for this process. For classification tasks, as is the case with a GAN, we typically choose binary cross entropy as defined by $L = - ylog(\hat{y}) - (1-y)log(1-\hat{y})$ and over $N$ instances as is typically the case for stochastic gradient descent the loss function is $L(w) = - \frac{1}{N} \sum_{n = 1}^N \big[ y_n log(\hat{y}_n) + (1-y_n)log(1-\hat{y}_n) \big] $ where $y$ is the true label and $\hat{y}$ is the predicted label. Backpropagation in deep neural networks Take a look at this answer here which describes the process of using backpropagation and gradient descent to train a single neuron perceptron, and then a multi-layered network. The only difference is we are using the binary entropy loss function here which has a different derivative with respect to $\hat{y}$. This becomes $\frac{\partial L}{\partial \hat{y}} = - \frac{1}{N} \sum_{n = 1}^N \Big[ \frac{y}{\hat{y}} - \frac{1-y}{1-\hat{y}} \Big]$ You will then backpropagate this loss through the network by using the chain rule with the activation functions you selected at each layer. Backpropagation in a CNN Please refer to this answer for extensive details and a derivation of backpropagation for a CNN. The process is very similar to that for a deep neural network. However, the CNN uses the cross-correlation function at each layer, so you need to backpropagate the loss function through the derivative of this function. That question asks about the inner working of a CNN with two outputs very much like our discriminator. Finally, in order to train the generator, just imagine the same process with more convolutional layers ahead of it. When we have the associated gradient $\nabla L(w)$ for each parameters, only apply the gradient descent algorithm to the parameters that are a part of the generator model. I am sure that through these derivations you will see that the number of intermediate nodes between layers does not affect the backpropagation algorithm. It remains the same process. You should also be convinced that for a non-human the intermediate nodes that are the generated image from the GAN, are no different from any intermediate nodes in the model. We simply train these intermediate nodes in such a way that we can perceive some meaning, such as generating instances of the MNIST dataset.
