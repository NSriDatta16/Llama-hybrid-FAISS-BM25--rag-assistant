[site]: crossvalidated
[post_id]: 357798
[parent_id]: 
[tags]: 
Rate of converge of KL-divergence to posterior

Suppose you have samples from some distribution P. You have a prior distribution Q, which represents your estimate of P, and assume for now that it's parameterized the same way as P. Upon observing samples from P you perform a Bayesian update of Q, yielding a posterior Q'. As more samples from P are observed, eventually the Kullbackâ€“Leibler divergence between Q' and P gets arbitrarily small, i.e. the KL(Q'||P) -> 0. My question is: can we write down a rate of converge as a function of the number of samples observed? This is really a duplication of this question asked by someone else, but it has no satisfactory answers so I thought I'd ask here. EDIT: I'm particularly interested in the case where P and Q are multinomial distributions.
