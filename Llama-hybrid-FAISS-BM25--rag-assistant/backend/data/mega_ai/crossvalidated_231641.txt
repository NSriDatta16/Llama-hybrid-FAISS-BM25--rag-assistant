[site]: crossvalidated
[post_id]: 231641
[parent_id]: 231171
[tags]: 
I think it's pretty plain evident that cross validation is statistically better for estimating parameters or checking stability. That's because it measures prediction error ideally over all data with equal weights. But that brings you to statistics of course. Because traditional prediction errors and model fit statistics do the job as well, if you have the right model . So in principle, if you believe in your model, you don't need to do prediction at all, you just fit your model by using all data and calculate prediction errors from statistical theory. That's what most scientists and scholars do, I believe. But if you believe that you don't have the right model, and you are just using a wrong model to mimic the data, then your question becomes relevant. As said, I would use cross validation to calibrate the parameters, but to demonstrate predictive ability, I would use a separate validation data set. Why? Because it's more convincing, more understanadable, and finally, because you very likely report the parameters calibrated from the whole data, not from any of you training subsets. Moreover, I would not just randomly pick the validation data, but I would choose it somehow conceptually different from the training data: For example, past - future, Americans - Europeans. I think that's the ultimate test of model validity. Traditional cross validation (and randomly picked validation data) just measure internal validity (terminology my own). Systematically picked validation data measure external validity which is generalizability. And science is about generalizing patterns. And needless to say, the prediction gains should be statistically significant. Otherwise, it's just optimistic reporting of noise. Of course this is a very rigorous take on model validation, and I believe most modelers actually fail one or two: cross validation, external validation or statistical significance. Because it's all too easy to overfit a model in small data and then go like 'whoah, what good predictions'. But I've also seen examples to the contrary: In cancer research, it was every day practice to first fit a random forest by cross validation, and then to demonstrate predictive ability on different patients. Because in that way they could make the doctors and biologists to believe in the result. This is my take on the issue as a practicing statistician some 5+ years into the working life.
