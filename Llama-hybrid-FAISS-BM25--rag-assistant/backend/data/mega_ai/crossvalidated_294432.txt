[site]: crossvalidated
[post_id]: 294432
[parent_id]: 294429
[tags]: 
From slide 2 here : Why use PCA? There are several uses (and abuses) for PCA. The most important use of PCA is probably in multiple regression. Suppose a response variable Y is to be regressed against a large number of covariates. Variable selection techniques are not often very effective, and there may be scientific interest in including information from most or all of the covariates. Retaining all covariates will likely lead to severe multicollinearity or non-identifiability of regression coefficients. Without remedy, standard errors will be unacceptably large, and predictions may be very inaccurate. PCA doesn't just center and rescale the individual variables. It constructs a set of orthogonal (non-collinear, uncorrelated, independent) variables. (See the visualization here .) For many model fitting algorithms, these variables are much easier to fit than "natural" (somewhat collinear, somewhat correlated, not-independent) variables. Edit To see why, let's construct some simulated data. I'm using R with the tidyverse package. library(tidyverse) set.seed(42) dataf = tibble(x = rnorm(50), y = 1 + .75*x + .5*rnorm(50)) X and Y are highly correlated, as shown in both the scatterplot and their correlation coefficient. ggplot(dataf, aes(x, y)) + geom_point() with(dataf, cor(x, y)) [1] 0.8588732 We can normalize X and Y separately by converting them to z-scores. X and Y will then both have mean 0 and standard deviation 1. dataf = dataf %>% mutate(x.norm = (x - mean(x) / sd(x)), y.norm = (y - mean(y) / sd(y))) But they're exactly as correlated as before. Centering and rescaling doesn't change correlation. As an exercise, you might show that, for any variables X and Y and constants a and b, $$Cor(X,Y) = Cor(aX + c, Y).$$ ggplot(dataf, aes(x.norm, y.norm)) + geom_point() with(dataf, cor(x.norm, y.norm)) [1] 0.8588732 Now let's extract the principal components. The plot is a scree plot. It shows the variance associated with each individual principal component. The first principal component has much more variance than the second. The calculation shows the fraction of total variance associated with each principal component; it confirms that about 93% of the total variance is captured by the first component. pc = prcomp(dataf[c('x','y')], scale. = TRUE) screeplot(pc) pc$sdev^2 %>% {./sum(.)} [1] 0.92943659 0.07056341 Let's add these to the dataframe for plotting. dataf = pc$x %>% as_tibble() %>% bind_cols(dataf, .) ggplot(dataf, aes(PC1, PC2)) + geom_point() with(dataf, cor(PC1, PC2)) [1] -4.379748e-16 The plot and correlation coefficient indicate that PC1 and PC2 are effectively uncorrelated.
