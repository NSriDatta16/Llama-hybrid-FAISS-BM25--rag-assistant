[site]: crossvalidated
[post_id]: 179279
[parent_id]: 
[tags]: 
In a neural network, how to assess the impact of different activation functions on overfitting?

I would like to see how the type of non-linearity (sigmoid, tanh, ReLU) affect overfitting behavior of a neural network, and I am not sure what's the proper way to do it. More specifically, should I also consider the convergence of each model in this case? Let's say I set the threshold for the error rate to be 0.1, and should I consider the overfitting behavior only if all the models will be able to converge at this threshold at a given set of parameters? What if different models converge at different iterations? And the corresponding overfitting may appear before or after the convergence point. What's the proper parameter to evaluate overfitting problem? For example, at which iteration the overfitting appears?
