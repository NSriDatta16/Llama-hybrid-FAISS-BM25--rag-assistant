[site]: crossvalidated
[post_id]: 358735
[parent_id]: 183659
[tags]: 
Could't figure out much from the graphs but to answer your questions- At what stages does MacQueen's update centroids that Lloyd's algorithm does not? Macqueen recalculates the center every time an iteration is completed like Lloyd's but also everytime a data point changes it's subspace (Cluster). So many more centroid calculations in Macqueen. Why does this make it more efficient? It might not be efficient always, Consider a scenario where both the clusters are easily separable. Macqueen moves slowly as calculating new centroids while Llyods would jump towards right centroids in every iteration in lesser centriod calculation. So i think efficiency is not guaranteed. What exactly does 'more efficient' even mean? (My tests show that MacQueen's and Lloyds and Hardigan's all run at the pretty much the same speed for any data). - Overall performace, like there can be a case where K-means will lead to an empty cluster. While macqueen and Hartigan algos may give different results for same datasets( if order of points is high) How does Hartigan's method even work? And how does this make it more accurate in terms of 'fixing' the wrong results that are caused by the initial random centroids? Please read the below paper- goto page 17. It takes within cluster sum of squares rather than distance as metrics to classification a point in any cluster. https://core.ac.uk/download/pdf/27210461.pdf
