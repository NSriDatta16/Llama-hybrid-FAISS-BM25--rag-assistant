[site]: crossvalidated
[post_id]: 605071
[parent_id]: 
[tags]: 
Understanding Leverage Score Sampling to get representative sample

I was reading about Leverage Score Sampling. If I am not wrong then what I know that Leverage Score Sampling help us to select representative sample. But I didn't understand how the whole process is done. I read these lecture notes but could not get the whole picture, like how Matrix Chernoff Bound helps to obtain a subspace embedding matrix by leverage scoring. Suppose, I have a million features with million rows then how does Leverage Score Sampling help to get the representative samples? Besides, chatgpt provides, Leverage scores can be calculated as the diagonal elements of the hat matrix in a linear regression analysis. The hat matrix represents the linear transformation that maps the response variables to their fitted values, and its diagonal elements are the leverage scores for each data point. Leverage scores can be calculated as the diagonal elements of the hat matrix in a linear regression analysis. The hat matrix represents the linear transformation that maps the response variables to their fitted values, and its diagonal elements are the leverage scores for each data point. The hat matrix is defined as $H = X(X'X)^{-1}X'$ , where $X$ is the design matrix for the regression model and $(X'X)^{-1}$ is the inverse of the $X'X$ matrix. The leverage score for each data point $i$ is given by $h_{ii} = H_{ii}$ . Leverage scores range from $0$ to $1$ , with values close to $1$ indicating high leverage and values close to $0$ indicating low leverage. Points with high leverage scores have the potential to have a large impact on the regression model, as they are far away from the average pattern in the data. On the other hand, points with low leverage scores are close to the average pattern and have less influence on the regression model. In general, a threshold value of $2k/n$ is commonly used, where $k$ is the number of predictors in the regression model and n is the sample size. This threshold value corresponds to Cook's distance, which is a measure of the impact that each data point has on the regression model. Points with Cook's distance greater than this threshold are considered to have high leverage and are more likely to be included in the sample. if I understand what it saying then the goal is to identify a small subset of the data that can be used to represent the entire dataset, thereby reducing the computational cost of regression analysis. Points with high leverage scores are more influential and are more likely to be selected for the sample. Question: But as it mentioned threshold value of $2k/n$ is commonly used then isn't the selection process is deterministic and everytime produce the same selection? It will be a great help if anyone redirect me any resources, papers where I can get the details with some coding explanation side along. Thanks in advance.
