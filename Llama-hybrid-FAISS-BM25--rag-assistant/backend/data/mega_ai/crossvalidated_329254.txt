[site]: crossvalidated
[post_id]: 329254
[parent_id]: 
[tags]: 
Applying deep logistic regression on sparse labels

I have a dataset of 100 000 examples. Only 1% are positive (1000 examples). I want to predict the probability that a positive event happen. To do that I have built a 4 layer DNN (Linear W*x+b -> ReLU)with a sigmoid single unit output layer. I calculate the loss using the mean_squared_error loss function and an AdamOptimizer for backprop. I observe the following during the learning: epoch 0: Predictions (0 1) Labels 0.5 0.5 0 0.5 0.5 0 .... After several iterations I can see that the learning is systematically increasing the prediction value for 0, and decrease the one for 1. It goes so far that when I have a positive example appearing it doesn't even detect it. It looks like that: Predictions (0 1) Labels 0.88977 0.11023 0 0.88977 0.11023 0 0.88977 0.11023 0 ... 0.88977 0.11023 0 0.88977 0.11023 1 0.88977 0.11023 0 It looks to me that it only learns to predict the negative example since they represent 99% of my dataset, but do not succeed at all in predicting the positive... How could I modify this architecture to be able to predict positive examples ?
