[site]: datascience
[post_id]: 65276
[parent_id]: 
[tags]: 
Why is T test reweighting on a word X word co-occurrence matrix so effective?

I am going through Stanford NLP class: http://web.stanford.edu/class/cs224u/ A task in the homework is to implement T-test reweighting on a word X word co-occurrence matrix: https://nbviewer.jupyter.org/github/cgpotts/cs224u/blob/2019-spring/hw1_wordsim.ipynb#t-test-reweighting-[2-points] $$\textbf{ttest}(X, i, j) = \frac{ P(X, i, j) - \big(P(X, i, *)P(X, *, j)\big) }{ \sqrt{(P(X, i, *)P(X, *, j))} }$$ I have 2 questions: What is the intuition behind this formula? It looks a little like PMI but I can't understand what it's doing. The T-test explanation out there seems to be unrelated to this task. It works amazingly well (when evaluated by this test ): raw matrix yield a correlation score of 0.014, PMIed matrix 0.123 and t scored matrix: 0.408979. This number seems almost too good to be true for such a simple model. Can anyone bring some intuition/experience about why that is?
