[site]: crossvalidated
[post_id]: 113207
[parent_id]: 17336
[tags]: 
Of course some math will be involved, but it's not much: Euclid would have understood it well. All you really need to know is how to add and rescale vectors. Although this goes by the name of "linear algebra" nowadays, you only need to visualize it in two dimensions. This enables us to avoid the matrix machinery of linear algebra and focus on the concepts. A Geometric Story In the first figure, $y$ is the sum of $y_{\cdot 1}$ and $\alpha x_1$ . (A vector $x_1$ scaled by a numeric factor $\alpha$ ; Greek letters $\alpha$ (alpha), $\beta$ (beta), and $\gamma$ (gamma) will refer to such numerical scale factors.) This figure actually began with the original vectors (shown as solid lines) $x_1$ and $y$ . The least-squares "match" of $y$ to $x_1$ is found by taking the multiple of $x_1$ that comes closest to $y$ in the plane of the figure. That's how $\alpha$ was found. Taking this match away from $y$ left $y_{\cdot 1}$ , the residual of $y$ with respect to $x_1$ . ( The dot " $\cdot$ " will consistently indicate which vectors have been "matched," "taken out," or "controlled for.") We can match other vectors to $x_1$ . Here is a picture where $x_2$ was matched to $x_1$ , expressing it as a multiple $\beta$ of $x_1$ plus its residual $x_{2\cdot 1}$ : (It does not matter that the plane containing $x_1$ and $x_2$ could differ from the plane containing $x_1$ and $y$ : these two figures are obtained independently of each other. All they are guaranteed to have in common is the vector $x_1$ .) Similarly, any number of vectors $x_3, x_4, \ldots$ can be matched to $x_1$ . Now consider the plane containing the two residuals $y_{\cdot 1}$ and $x_{2 \cdot 1}$ . I will orient the picture to make $x_{2\cdot 1}$ horizontal, just as I oriented the previous pictures to make $x_1$ horizontal, because this time $x_{2\cdot 1}$ will play the role of matcher: Observe that in each of the three cases, the residual is perpendicular to the match. (If it were not, we could adjust the match to get it even closer to $y$ , $x_2$ , or $y_{\cdot 1}$ .) The key idea is that by the time we get to the last figure, both vectors involved ( $x_{2\cdot 1}$ and $y_{\cdot 1}$ ) are already perpendicular to $x_1$ , by construction. Thus any subsequent adjustment to $y_{\cdot 1}$ involves changes that are all perpendicular to $x_1$ . As a result, the new match $\gamma x_{2\cdot 1}$ and the new residual $y_{\cdot 12}$ remain perpendicular to $x_1$ . (If other vectors are involved, we would proceed in the same way to match their residuals $x_{3\cdot 1}, x_{4\cdot 1}, \ldots$ to $x_2$ .) There is one more important point to make. This construction has produced a residual $y_{\cdot 12}$ which is perpendicular to both $x_1$ and $x_2$ . This means that $y_{\cdot 12}$ is also the residual in the space (three-dimensional Euclidean realm) spanned by $x_1, x_2,$ and $y$ . That is, this two-step process of matching and taking residuals must have found the location in the $x_1, x_2$ plane that is closest to $y$ . Since in this geometric description it does not matter which of $x_1$ and $x_2$ came first, we conclude that if the process had been done in the other order, starting with $x_2$ as the matcher and then using $x_1$ , the result would have been the same. (If there are additional vectors, we would continue this "take out a matcher" process until each of those vectors had had its turn to be the matcher. In every case the operations would be the same as shown here and would always occur in a plane .) Application to Multiple Regression This geometric process has a direct multiple regression interpretation, because columns of numbers act exactly like geometric vectors. They have all the properties we require of vectors (axiomatically) and therefore can be thought of and manipulated in the same way with perfect mathematical accuracy and rigor. In a multiple regression setting with variables $X_1$ , $X_2, \ldots$ , and $Y$ , the objective is to find a combination of $X_1$ and $X_2$ ( etc ) that comes closest to $Y$ . Geometrically, all such combinations of $X_1$ and $X_2$ ( etc ) correspond to points in the $X_1, X_2, \ldots$ space. Fitting multiple regression coefficients is nothing more than projecting ("matching") vectors. The geometric argument has shown that Matching can be done sequentially and The order in which matching is done does not matter. The process of "taking out" a matcher by replacing all other vectors by their residuals is often referred to as "controlling" for the matcher. As we saw in the figures, once a matcher has been controlled for, all subsequent calculations make adjustments that are perpendicular to that matcher. If you like, you may think of "controlling" as "accounting (in the least square sense) for the contribution/influence/effect/association of a matcher on all the other variables." References You can see all this in action with data and working code in the answer at https://stats.stackexchange.com/a/46508 . That answer might appeal more to people who prefer arithmetic over plane pictures. (The arithmetic to adjust the coefficients as matchers are sequentially brought in is straightforward nonetheless.) The language of matching is from Fred Mosteller and John Tukey.
