[site]: crossvalidated
[post_id]: 378008
[parent_id]: 
[tags]: 
How to handle a changing action space in Reinforcement Learning

I'm training a Reinforcement Model playing a game with self learning.(A second instance is its opponent). An agent has a set of possible action to choose from in each state. Those actions usually remain the same. Q-Learning tries than to map best actions to highest rewards. DQN tries to estimate Q values for unseen states. I have now an example, where at some time some actions can not be taken. In fact the remaining possible actions get fewer and fewer leading finally to only one possible action before the game ends. How do I handle that? Do I simply give a huge negativ reward when an action is chosen which can not be taken and let it choose again? In this way the model has to learn that those actions can not be taken in certain situations. Is there maybe a different approach which would neglect learning this?
