[site]: stackoverflow
[post_id]: 1002953
[parent_id]: 
[tags]: 
Large Sqlite database search

How is it possible to implement an efficient large Sqlite db search (more than 90000 entries)? I'm using Python and SQLObject ORM: import re ... def search1(): cr = re.compile(ur'foo') for item in Item.select(): if cr.search(item.name) or cr.search(item.skim): print item.name This function runs in more than 30 seconds. How should I make it run faster? UPD : The test: for item in Item.select(): pass ... takes almost the same time as my initial function (0:00:33.093141 to 0:00:33.322414). So the regexps eat no time. A Sqlite3 shell query: select '' from item where name like '%foo%'; runs in about a second. So the main time consumption happens due to the inefficient ORM's data retrieval from db. I guess SQLObject grabs entire rows here, while Sqlite touches only necessary fields.
