[site]: datascience
[post_id]: 13448
[parent_id]: 13446
[tags]: 
You would use a sigmoid if you want your activations to be between zero and 1 and tanh between -1 and 1, this is important for the final layer, but for a hidden layer there is not much difference between them. Here is an article on how to build a neural network in a more systematic way. The basic principle is to start with only one layer, and make it bigger and bigger until you see satisfactory results, if you don't, add another layer and start over. Oh and make sure you use batch normalization, and don't use sigmoids or tanh's, use PreLUs or ELUs: https://www.linkedin.com/pulse/keras-neural-networks-win-nvidia-titan-x-abhishek-thakur?trk=prof-post In general: Sigmoids: use for output layer which are probabilities with logloss loss function, and for gates in RNNs. Tanh: use for updating the state in RNNs (linear-like activations might make it unstable), or for an output layer with variables between -1 and 1. Rectifiers: use for activations in hidden layers of feed-forward network.
