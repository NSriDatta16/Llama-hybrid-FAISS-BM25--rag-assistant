[site]: crossvalidated
[post_id]: 555348
[parent_id]: 
[tags]: 
What is the difference between architecture selection and hyperparameter tuning in the context of neural networks

I've already seen the response to this questions here , but it doesn't really contain, what I feel, is an answer to my specific query. It seems to unclear in a lot of the literature (that I've seen) the clear distinction between what is considered model/architecture selection, and what is hyperparameter tuning. If I have a simple ConvNet I feel it's well established that things like kernal size, number of filters in a layer, stride etc can all be considered as hyperparameters. But what about things like the number of convolutional layers, the number of neurons in the FC hidden layer, the number of hidden FC layers, the number of convolutional layers that come before a batch_norm layer? The main reason I ask is because the literature (that I've read) says you should get your architecture dialled in before tuning the hyperparameters. Part of the getting the correct architecture (and sanity checking your pipeline has no obvious bugs in it) is being able to overfit a few samples of your data. If this isn't the case then something is obviously wrong (as so expertly put by Andrej Karpathy - the two major things (if we assume there are no programmer related errors) that can prevent the model being able to overfit a few samples are the information supplied by the input features (different discussion to the question I'm posing here) and the suitability of the model architecture. In this instance the line between architecture selection and hyperparameter tuning/selection seems important.
