[site]: datascience
[post_id]: 123350
[parent_id]: 123325
[tags]: 
First, the token vocabulary is extracted from the training data, usually by means of byte-pair encoding (BPE), wordpieces or unigrams. Then, regarding model definition, the first layer in LLMs is the token embeddings, which are trained with the rest of the model. Given that subword-level vocabularies mitigate the OOV problem, there are no unknown tokens, so usually there is no need to address such a problem. It is not common either to have special tokens that need to be kept unsplit. In general, it is inconvenient to modify the vocabulary of a pretrained model, because you need to retrain/finetune the model to make it able to handle the new tokens, and therefore you lose the advantages of reusing the pretrained representations; furthermore, the fine-tuning data may not be enough to deliver good representations for new tokens.
