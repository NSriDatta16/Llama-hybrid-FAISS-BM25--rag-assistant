[site]: crossvalidated
[post_id]: 206495
[parent_id]: 206483
[tags]: 
As far as know, Overfitting means the predictor performs badly on testing data while it performs well on training data (I have not heard of word like "over-training", maybe you made some typos?) Why do you train the 20 SVMs on full training set? I suppose you might train them on 20 different "bootstrapped" dataset in bagging. I believe, in general, the number of SVMs should be the more the better (In theory, the expectation of the mean does not change, but the variance will be reduced more). But you can select it using cross validation. In bagging, if the SVMs correlate together, the "averaging" is meaningless. So you might want to add more "randomness" into it. Please take a look at random forest, which use a randomly subset of features for each SVM. This will make the aggregated predictor perform better.
