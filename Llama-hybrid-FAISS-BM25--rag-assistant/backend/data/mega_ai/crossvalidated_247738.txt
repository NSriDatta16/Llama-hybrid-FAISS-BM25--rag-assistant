[site]: crossvalidated
[post_id]: 247738
[parent_id]: 
[tags]: 
How to pick two features that can linearly separate two classes as much as possible

I have a few dataset, each has about 120 features/dimensions and two classes (e.g. A and B) in it. Now I'd like to visualize the dataset from just two dimensions without doing any dimensionality reduction. The criteria for picking the two dimensions is that they should separate the two classes as much as possible. Here is what I have tried: I tried a decision tree on each dataset and use the top two features with highest importance as the two dimensions. It works for some of them. But since the cutoff chosen by tree is always orthogonal to the axis, which I thought was a bit too restricted, then I tried a second approach I trained a logistic regression binary classifier on each dataset with L1 regularization, which has the effect of feature selection . Then I order the features by their learned weights, and pick the two features at both ends (i.e. min and max) as the dimensions to visualize. Again, it works for some of the dataset, in contrast to a decision tree, the cutoff doesn't have to be orthogonal to either axis now. However, I am still not sure if the above two ways are the best one to pick dimensions for visualization. Are there better ways, please?
