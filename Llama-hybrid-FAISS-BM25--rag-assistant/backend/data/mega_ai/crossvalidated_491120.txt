[site]: crossvalidated
[post_id]: 491120
[parent_id]: 147784
[tags]: 
Pretraining is a multi-stage learning strategy that a simpler model is trained before the training of the desired complex model is performed. In your case, the pretraining with restricted Boltzmann Machines is a method of greedy layer-wise unsupervised pretraining. You train the RBM layer by layer with the previous pre-trained layers fixed. Pretraining helps both in terms of optimization and generalization. Reference: Deep Learning by Ian Goodfellow and etc.
