ines a proper metric space, satisfying non-negativity, identity of indiscernibles, symmetry, and the triangle inequality. Next, a secondary distance matrix D ~ = d ~ i , j {\displaystyle {\tilde {D}}={{\tilde {d}}_{i,j}}} is computed, where each entry measures the Euclidean distance between the distance profiles of two assets: d ~ i , j = ∑ n = 1 N ( d n , i − d n , j ) 2 {\displaystyle {\tilde {d}}{i,j}={\sqrt {\sum {n=1}^{N}(d_{n,i}-d_{n,j})^{2}}}} While d i , j {\displaystyle d_{i,j}} reflects correlation-based proximity between two assets, d ~ i , j {\displaystyle {\tilde {d}}_{i,j}} quantifies dissimilarity across the entire system, as it depends on all pairwise distances. Hierarchical clustering proceeds by identifying the pair ( i , j ) {\displaystyle (i,j)} with the smallest value of d ~ i , j {\displaystyle {\tilde {d}}_{i,j}} (for i ≠ j {\displaystyle i\neq j} ), and forming a new cluster u [ 1 ] = ( i , j ) {\displaystyle u[1]=(i,j)} . The distance between this new cluster and all remaining items is updated using a linkage criterion. One example is the single linkage (nearest neighbor) method: d ˙ i , u [ 1 ] = min j ∈ u [ 1 ] d ~ i , j {\displaystyle {\dot {d}}_{i,u[1]}=\min {j\in u[1]}{\tilde {d}}_{i,j}} The algorithm is repeated recursively: the pair with minimum distance is clustered, the matrix D ~ {\displaystyle {\tilde {D}}} is updated, and clustering continues until all items belong to a single cluster. This produces N − 1 {\displaystyle N-1} clusters and a binary tree (dendrogram) that encodes the hierarchical structure. The clustering process is recorded in a linkage matrix Y ∈ R ( N − 1 ) × 4 {\displaystyle Y\in \mathbb {R} ^{(N-1)\times 4}} , where each row ( y m , 1 , y m , 2 , y m , 3 , y m , 4 ) {\displaystyle (y_{m,1},y_{m,2},y_{m,3},y_{m,4})} represents a clustering step: y m , 1 {\displaystyle y_{m,1}} and y m , 2 {\displaystyle y_{m,2}} are the merged clusters, y m , 3 {\displaystyle y_{m,3}} is the distance between them, and y m , 4 {\displaystyle y_{m,4}} is the total number of original items included in the cluster. HRP accepts a wide range of clustering metrics and linkage criteria. For further discussion, see Rokach and Maimon (2005), Brualdi (2010), and the documentation for SciPy’s hierarchical clustering tools. Step 2: Quasi-Diagonalization At this stage, the rows and columns of the covariance matrix are reordered such that the largest values are concentrated along the diagonal. This process is referred to as a quasi-diagonalization of the covariance matrix. Unlike full diagonalization, this method does not require a change of basis. The resulting structure places similar investments near each other and dissimilar investments farther apart, facilitating block-wise portfolio construction. (See illustrative examples on the right) The procedure relies on the structure of the linkage matrix generated during hierarchical clustering. Each row in the linkage matrix Y {\displaystyle Y} represents the merger of two branches into a cluster. To obtain the ordering of original items, the final row ( y N − 1 , 1 , y N − 1 , 2 ) {\displaystyle (y_{N-1,1},y_{N-1,2})} is recursively expanded by replacing clusters with their constituents, in the order dictated by the hierarchical tree. This produces a sorted list of indices corresponding to the unclustered items. This ordering is used in the final step of the HRP algorithm to allocate capital proportionally to the hierarchical structure of asset relationships. Step 3: Recursive Bisection This final stage of the Hierarchical Risk Parity (HRP) algorithm computes portfolio weights using the quasi-diagonal covariance matrix. When the covariance matrix is diagonal, inverse-variance weighting is optimal (see Appendix 16.A.2). HRP uses this insight in both bottom-up and top-down directions: Bottom-up: estimate the variance of a cluster using inverse-variance weights. Top-down: split capital between clusters inversely proportional to their estimated variances. The