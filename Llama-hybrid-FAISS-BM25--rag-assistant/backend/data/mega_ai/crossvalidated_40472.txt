[site]: crossvalidated
[post_id]: 40472
[parent_id]: 40465
[tags]: 
It's because all of the information from the data given an assumed model is picked up by a multiple of the likelihood and that is all you need or many would argue should use in inference (tentatively taking the model as given). When that is just driven by some summary statistics, there is tremendous simplification. This is perhaps most easily seen from the Bayesion perspective by noting that posterior = prior * data model and so data model = posterior/prior. (prior/posterior is actually a relative probability - after data/before the data - that is called the relative belief ratio and is a multiple of the likelihood function) Approximate Bayesian Computation (ABC) could be a convenient way to visualize these things. A technical paper for any who might be interested http://www.utstat.utoronto.ca/mikevans/papers/surprise.pdf
