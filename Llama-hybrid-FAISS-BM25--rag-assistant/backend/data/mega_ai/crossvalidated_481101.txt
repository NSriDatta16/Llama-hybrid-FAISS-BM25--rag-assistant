[site]: crossvalidated
[post_id]: 481101
[parent_id]: 211419
[tags]: 
Here is my attempt at an explanation: A) An intuition for precision can be found in the context of measurement error. Suppose you are measuring some quantity of interest with some measurement instrument (e.g., measuring a distance with measuring tape). If you were to take several measurements of the quantity of interest with the same measurement instrument, you will likely end up with variation in the results i.e. measurement error. These errors are often well approximated by a normal distribution. The precision parameter of a normal distribution tells you how "precise" your measurements are in the sense of having larger or smaller errors. The larger the precision, the more precise your measurement, and thus the smaller your errors (and vice-versa). B) The reason that precision matrices are sometimes preferred over covariance matrices is due to analytical and computational convenience: they are simpler to work with. This is why normal distributions were classically parameterized via the precision parameter in the Bayesian context before the computer revolution when calculations were done by hand. The parameterization remains relevant today when working with very small variances as it helps to address underflow in numerical computations. The simplicity of the alternative can also be illustrated by comparing the densities of both parameterizations. Notice below how the use of $\tau = \frac{1}{\sigma^2}$ eliminates the need to divide by a parameter. In a Bayesian context (when parameters are treated as random variables) division by a parameter can make calculating posterior distributions painful. $$p_Y(y; \mu, \sigma) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{y-\mu}{\sigma})^2}$$ $$p_Y(y; \mu, \tau) = \sqrt{\frac{\tau}{2\pi}}e^{-\frac{1}{2}\tau(y - \mu)^2}$$
