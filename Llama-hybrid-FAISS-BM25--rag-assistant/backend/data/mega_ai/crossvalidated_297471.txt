[site]: crossvalidated
[post_id]: 297471
[parent_id]: 
[tags]: 
Random Forest Classifier Design

When we use random forest for classification it constructs multiple trees and finally averages the output, in the case of classification it takes a majority vote among all the trees and assigns a class. In the case of predicting with random forest, the features are again passed down each tree and a majority vote is taken at the terminal nodes. If we request for class probabilities, these probabilities are calculated by counting the proportion of each of the classes for all the trees, ex. for binary classification, $P(1|X)=\frac{\#1's}{\#1's+\#0's}$. My Question: Is it appropriate to calculate these probabilities by counting the votes and then designing a classifier, ie. computing AUC, misclassification error as a function of probability threshold, positive prediction negative prediction... The logic seems circular, where we first classify by majority vote, then we count to get probabilities and then apply a probability threshold. Can we do this?
