[site]: crossvalidated
[post_id]: 567404
[parent_id]: 567112
[tags]: 
It sounds like what you want to optimize are the "importance scores". With your metric, you are verifying if your model is able to correctly classify the samples as important vs not. In such a case, why not make the "importance" your target variable? You could use as a target -1 * importance for the negative class and +1 * importance for the positive class and treat this as a regression problem (or classification, if your algorithm allows for a fuzzy target), whereas for making hard classifications, you would use some threshold over the predicted scores. In such a case, you would be directly optimizing the scores. There wouldn't be a problem with designing custom metrics because you could just use standard loss like logistic loss (for $\pm 1$ labels), squared, or absolute error (unlike the two previous ones, absolute error is not a proper scoring rule , you probably would like to stick to the proper ones). On another hand, if you really care only about using importance weights when training and validating the results, why not just use standard weighted metrics? Weights can be introduced for standard metrics (e.g. squared error ) by replacing averaging the individual errors with a weighted average. Finally, as about the metrics themselves, accuracy is a poor metric . Accuracy doesn't care if the predicted scores are well callibrated , so it also would not be able to measure also the kind of changes that you intend to measure with your metric. In the case of standard classification, it can be the case that different metrics disagree because they measure different things, that is why the general advice is to stick to a single metric that you optimize .
