[site]: datascience
[post_id]: 43678
[parent_id]: 43677
[tags]: 
The balance between two classes in a classification is very important, as you do not want your model to overfit for a particular class. This is where you use metrics apart from accuracy to really evaluate how good your model truly is. In case you are not able to balance the dataset, there are multiple ways to work with imbalanced data. They are as follows: You could use certain techniques like SMOTE to generate more samples of the undersampled class You must split your dataset in test and train with stratification so that you have a balance in your evaluation. You can sub-sample the large class and balance the two classes, and this multiple times by taking random sub samples of the larger class. Thorough analysis of the result is very important to understand how to proceed towards the solution to the problem. Please look at f1 score, precision and recall apart from accuracy. Also read about micro/macro averaging of these metrics. There are a lot of conversations on datascience stackexchange and stackoverflow on how to work with imbalanced data for classification. Here is a link : https://stackoverflow.com/questions/40568254/machine-learning-classification-on-imbalanced-data Have fun with machine learning :)
