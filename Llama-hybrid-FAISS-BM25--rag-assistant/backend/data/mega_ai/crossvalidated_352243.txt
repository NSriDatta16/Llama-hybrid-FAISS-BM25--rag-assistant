[site]: crossvalidated
[post_id]: 352243
[parent_id]: 
[tags]: 
A better way to compare accuracy?

Hi I have an algorithm that takes a single sample, call it i and tries to predict what other samples in a cohort it is most closely related to. This cohort consist of N=11K from different tissues. So for example if I take a liver sample then it should in theory pull up samples that are all liver. In reality though, out of 100 liver samples the sample i would pull up say 99 liver samples and one brain, so the accuracy is 99%. What I like to do is iterate my algorithm with different parameter using 90% of my input set so that for example a parameter will generate something like this. df = data.frame ( type=c("brain","liver","testis","muscle","bone","adipose","blood","stomach","kidney", "bladder", "heart"), outcome = c(.89,.99,.6,.9,.9,.6,.8,.87,.88,.9,.76) ) type outcome 1 brain 0.89 2 liver 0.99 3 testis 0.60 4 muscle 0.90 5 bone 0.90 6 adipose 0.60 7 blood 0.80 8 stomach 0.87 9 kidney 0.88 10 bladder 0.90 11 heart 0.76 What I'm trying to do is evaluate how well my program is at placing samples into its correct tissue type. So for example the first row "brain" and its outcome .89 means that the algorithm was able to correctly place 100 brain samples correctly 89% of the time. Besides just comparing an average of the accuracy is there something similar like an AUC analysis? Note the algorithm is not really a classifier per se and can be something as simple as spearman correlation.
