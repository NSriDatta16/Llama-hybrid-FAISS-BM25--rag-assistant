[site]: crossvalidated
[post_id]: 330353
[parent_id]: 
[tags]: 
Loss to compare true labels to distribution?

Say I have an input tensor like this: # images = (?, 28, 28, 1) # MNIST images where the last dimension (the 1 ) is a pixel value (grayscale, so they're numbers on [0-255] for 256 total possibilties). And, accordingly the last layer of the neural network outputs something like this: # logits = (?, 28, 28, 256) where the last dimension ( 256 ) contains the "probability" (log odds/logit) for each color (so a number at index 256 would be the logit of white (255) being the correct color for that pixel). How can I write a loss function in that compares the distrubution the model predicts ( logits ) to the single correct value in images ? High-level intuitions are perfectly fine (i.e., code not needed). Optional Details. Right now, essentially, I'm forming a 2D [BatchHeightWidthChannel, Distribution (256 cols)] tensor, and checking against a 1D tensor ( images flattened). Sadly, the network always says that index zero (black) is the most probable value at test time. import tensorflow as tf images = tf.placeholder(tf.float32, shape=[None, 28, 28, 1]) # ...network goes here ... logits = tf.placeholder(tf.float32, shape=[None, 28, 28, 256]) # Softmax Cross Entropy. loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits( logits=tf.reshape(logits, (-1, 256)), # [BatchHeightWidthChannel, Distribution] labels=tf.to_int32(tf.reshape(images, shape=(-1,)) # [TruePixelValues] )) If that ^ 'looks right', that's helpful too (suggests the problem is elsewhere).
