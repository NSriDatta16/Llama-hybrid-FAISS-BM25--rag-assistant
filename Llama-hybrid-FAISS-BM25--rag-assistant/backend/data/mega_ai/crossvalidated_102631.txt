[site]: crossvalidated
[post_id]: 102631
[parent_id]: 
[tags]: 
k-fold Cross validation of ensemble learning

I am confused about how to partition the data for k-fold cross validation of ensemble learning. Assuming I have an ensemble learning framework for classification. My first layer contains the classification models, e.g. svm, decision trees. My second layer contains a voting model, which combines the predictions from the first layer and gives the final prediction. If we use 5 fold-cross validation, I am thinking of using the 5 folds as follows: 3 folds for training first layer 1 fold for training second layer 1 fold for testing Is this the correct way? Should the training data for the first and second layer be independent? I am thinking they should be independent so that the ensemble learning framework will be robust. My friend suggests the training data for the first and second layer should be the same, i.e. 4 folds for training first and second layer 1 fold for testing In this way, we will have a more accurate error of the ensemble learning framework, and the iteratively tuning of the framework will be more accurate, as it is based on a single training data. Moreover, the second layer may be bias towards the independent training data Any advices are greatly appreciated
