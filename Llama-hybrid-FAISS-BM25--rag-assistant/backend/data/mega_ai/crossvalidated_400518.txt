[site]: crossvalidated
[post_id]: 400518
[parent_id]: 
[tags]: 
Conditional independence and joint distributions in graphical models

I'm reading Deep Learning by Ian Goodfellow and Yoshua Bengio and Aaron Courville. In chapter 3 about graphical models, to reduce the model complexity, we assume that certain conditional independence assumptions hold. Specifically, in equation 3.52 on page 75 (I quote) ... suppose we have three random variables: $a$ , $b$ and $c$ . Suppose that $a$ inﬂuences the value of $b$ , and $b$ inﬂuences the value of $c$ , but that $a$ and $c$ are independent given $b$ . We can represent the probability distribution over all three variables as a product of probability distributions over two variables: $p(a,b,c) = p(a)p(b|a)p(c|b)$ I understand the last statement " $a$ and $c$ are independent given $b$ " as conditional independence, and I would write it down as, $p(a,c|b) = p(a|b)p(c|b)$ Is this right ? How should I prove the claim mentioned in the book ? I tried to use my result (of conditional independence) with the chain rule or product rule of conditional probabilities, but I could not arrive the claim (3.52 on page 75) mentioned in the book.
