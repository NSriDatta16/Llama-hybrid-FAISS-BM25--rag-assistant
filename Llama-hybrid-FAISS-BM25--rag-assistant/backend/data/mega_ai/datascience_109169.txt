[site]: datascience
[post_id]: 109169
[parent_id]: 
[tags]: 
The separate of K and V is redundant in transformer?

imho, I think the separate of K and V is redundant in transformer, as they are basically the same regardless in encoder self-attention, or decoder self-attention, or even the encoder-decoder attention. Can anyone counter-argue with me, based on the mathematical computation where K, V, Q are done in the training, to disprove my claim? I would further argue that the preservation of Q, K, V in transformer is just a gimmick to match with the status quo of information retrieval, which adds little sense and brings much confusion to the learners of transformer.
