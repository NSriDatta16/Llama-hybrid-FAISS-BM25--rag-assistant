[site]: datascience
[post_id]: 17944
[parent_id]: 17942
[tags]: 
In short PCA, returns an orthogonal set of basis features that best represent the variance in the data. Intuitively, imagine you want to identify whether we are talking about a dog or a cat. Your features are: size, weight, color, fur type, etc... but you also have features like weather, owner name, etc... It should be evident that the first set of features obviously explains the variance between a dog and a cat much better than the second set. Thus, you should only consider those and completely disregard the second set. This is what PCA does however, it adds the extra restriction that all features must be orthogonal. It then assigns a metric to each component based on the amount of variance that feature explains. You can then sort the components and extract the top $n$ components (feature reduction).
