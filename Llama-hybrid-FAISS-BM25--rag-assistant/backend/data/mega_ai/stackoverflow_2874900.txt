[site]: stackoverflow
[post_id]: 2874900
[parent_id]: 2874570
[tags]: 
Maybe threading would help. But first you could try something simpler. Make two copies of your program, with one reading the first 32768 files, and the other the second half. Run both programs at the same time. Does this take less than 14 hours? If not, then adding threads is probably useless. Defragmenting, as roygiv suggests above, might help. Added : The 14 hours is obviously wrong, since this is almost 1 second per file. Alejandro's comment above says that with an Solid State Drive, the time is only 0.1 ms per file, totaling 6.5 s. Which seems fast to me. So I'm guessing that Alejandro has to repeat this about 7000 times, each time with a different slice of data from the 65536 files. If so, two further suggestions are: Write a program to cat the files to a new file. You probably have enough space on you SSD to do this, as your other SO question indicats 32 GB of data, and the SSD is likely several times that. Then each run uses just this single huge file, which removes 65535 open's and close's. And, instead of just concatenation, while creating the huge file you could 'reverse the rows and columns' or 'stripe the data', providing locality. Further addition : You've probably already considered this, with your phrase "writing of the read data into a single file".
