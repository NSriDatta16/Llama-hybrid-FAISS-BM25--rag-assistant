[site]: crossvalidated
[post_id]: 511129
[parent_id]: 510953
[tags]: 
Yes, your idea is right. When unrolled in time, the RNN looks like this I am going out . ↓ ↓ ↓ ↓ ↓ █ → █ → █ → █ → █ → □ where █ is the recurrently applied function and □ is the classifier. The so-called back-propagation in time is nothing but pretending that the RNN is a very deep feed-forward network with the same layer repeated many times. Because the parameters in al the █ layers are the same, the gradient gets summed up.
