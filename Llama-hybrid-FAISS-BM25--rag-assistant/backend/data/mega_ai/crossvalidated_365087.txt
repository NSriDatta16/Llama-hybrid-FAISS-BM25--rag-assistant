[site]: crossvalidated
[post_id]: 365087
[parent_id]: 
[tags]: 
Policy gradient derivation confusion

In the derivation of the policy gradient, I am confused about why the sum of rewards, $r$, is constant with respect to $\theta$ where $\theta$ is the weights of the neural network used to determine the policy. I believe that the second line should have a bracket around both $\pi$ and $r$ because $r$ is dependent on the sequence of actions $a$, and the sequence of $a$ is obviously dependent on the policy $\pi$ which is ultimately dependent on $\theta$. so $\nabla_\theta r \neq 0 $
