[site]: crossvalidated
[post_id]: 618540
[parent_id]: 618530
[tags]: 
That's the point of embeddings! Using the time-series example from my answer to your other question, say that you want to create a weekly Fourier feature using the $\sin$ function as $\sin\big(\tfrac{2\pi t}{7}\big)$ where $t$ is the time index for days. The feature when plotted looks like below. As you can see, I marked every 7th day with a red triangle and each of them has the same value of the "embedding" feature. That is the point, the feature represents 7th day with some numerical value. This enables us to use the feature in a regression model, where the numerical value would represent the day of the week. The same in positional embeddings in the transformer model, the embeddings map the position of the word in the sentence to the embeddings space. Assuming that we are interested in "every seventh word" in a sentence (as in the time-series example above), we can use the embeddings to record the information. This enables you to model periodic trends in the data. this just circulates the 2Pi. so in the sense of trying to assign a meaningful values for positional embeddings I dont realize how does it make sense. its not confined to sth like (i/max_sequence_length*pi/denominator[0]) , that way if we used only cosine, the last words were assigned to -1 and first words assigned to 1. If you did it like this, your model would be able to only work with texts of length smaller or equal to max_sequence_length , while we want it to work with texts of arbitrary length. Also, how would it differ from just using i as a feature? The point of positional embeddings is to encode cycles in the data.
