[site]: crossvalidated
[post_id]: 269975
[parent_id]: 
[tags]: 
Sampling density of statistical functional versus posterior density

Let's suppose we collect a dataset $\mathcal{X}$, say $\mathcal{X} = (x_1,\ldots,x_N)$, i.i.d. samples from the R.V. $X$ with unknown probability density $p(x)$. Our goal is to estimate an expectation (i.e. a statistical functional): $$ Y = \Bbb{E}_{p(x)}[f(x)] = \int f(x)p(x)dx $$ To do this, a standard parametric method will suggest that $p = p(x;\theta)$ where $\theta\in\Theta$. First suppose we choose an estimator $\hat{\theta}$ by some means. We then build an estimator for $Y$ by computing the expectation in terms of the parametric density $p(x;\hat{\theta})$: $$ \hat{Y} = \Bbb{E}_{p(x;\hat{\theta})}[f(x)] $$ Because $\hat{Y}$ is constructed using the dataset $\mathcal{X}$, it is a random variable and will have a sampling density $p(\hat{Y})$ Alternately, we can take the Bayesian approach and treat $Y$ as a random variable from the get-go, forming the posterior density $p(Y\vert \mathcal{X})$. My question is, under what conditions is the sampling density of $\hat{Y}$ the same as the the posterior density of $Y$ given $\mathcal{X}$? I am intrigued by this paragraph on the Wikipedia page : In Bayesian inference, when the sampling distribution of a statistic is available, one can consider replacing the final outcome of such procedures, specifically the conditional distributions of any unknown quantities given the sample data, by the conditional distributions of any unknown quantities given selected sample statistics. Such a procedure would involve the sampling distribution of the statistics. The results would be identical provided the statistics chosen are jointly sufficient statistics. Can anyone expand on this connection? Is there any quantitative bound on the distance (in terms of KL divergence, say) between $p(\hat{Y})$ and $p(Y\vert\mathcal{X})$? I assume the answer depends on the prior on $Y$?
