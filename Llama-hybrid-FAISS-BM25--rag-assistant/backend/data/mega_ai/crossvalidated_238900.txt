[site]: crossvalidated
[post_id]: 238900
[parent_id]: 233265
[tags]: 
After speaking with a distinguished researcher in the field of machine learning, I came away with the following: If the individual classifiers are working with extremely unbalanced data, the resulting probability estimates are probably suspect regardless of what one does with them. If some technique is used to account for these unbalanced data, subsequent multiplication of the individual probabilities is valid so long as I'm absolutely sure of each event's independence. Some other approaches to resolve my data imbalance were offered, most of which are covered or at least touched on in a recent survey paper (He et al. 2009). I found that random oversampling partially resolved my issues with the classifiers, and that targeted oversampling did even better. The general sense I got from the researcher, however, is that multiplying the probabilities was perfectly fine if the probability estimates themselves were good estimates, but that those probabilities were suspect when dealing with unbalanced data sets, and especially when different classes from different classifiers exhibited different forms of imbalance. The full citation of He's survey paper is given here: He, H., & Garcia, E. A. (2009). Learning from imbalanced data. IEEE Transactions on knowledge and data engineering, 21(9), 1263-1284.
