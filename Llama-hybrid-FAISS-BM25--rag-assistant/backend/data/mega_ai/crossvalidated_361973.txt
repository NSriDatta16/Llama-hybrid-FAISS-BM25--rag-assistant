[site]: crossvalidated
[post_id]: 361973
[parent_id]: 361969
[tags]: 
In order to prevent overfitting, EarlyStopping should monitor a validation metric. Because your loss function is the mse , by monitoring val_loss you are essentially monitoring the validation Mean Squared Error . If you think that mae is a better metric for your task, you should monitor val_mae instead. Why monitor a validation metric when performing early stopping? Early stopping, is mostly intended to combat overfitting in your model. Overfitting is a phenomenon, commonly occurring in Machine Learning, where a model performs worse on a validation/test set than the training set. Take a look at the image below: The red and blue lines depict the model's loss, during training, on the training/validation sets respectively. As you can see, while the training loss continues to drop , the validation loss initially plateaus and then starts to raise . This happens because after some point the model learns to memorize the training data, while forgetting how to effectively differentiate between the classes. When we reach this point, even though we think the model is improving (indicated by all the training set metrics), actually has begun to diverge from its goal (to perform well on unseen data). Early stopping is a method of combating this. By terminating the model, before it has completed its training we might get a better performance on unseen data. This works by monitoring a validation metric and terminating the model when this metric stops dropping.
