a\}}}-{\frac {\sum W(X)X\in \{X\in D|X(A)=a,X(Y)=+\}}{\sum W(X)X\in \{X\in D|X(A)=a\}}}} It can be shown that after reweighting this weighted discrimination is 0. Inprocessing Another approach is to correct the bias at training time. This can be done by adding constraints to the optimization objective of the algorithm. These constraints force the algorithm to improve fairness, by keeping the same rates of certain measures for the protected group and the rest of individuals. For example, we can add to the objective of the algorithm the condition that the false positive rate is the same for individuals in the protected group and the ones outside the protected group. The main measures used in this approach are false positive rate, false negative rate, and overall misclassification rate. It is possible to add just one or several of these constraints to the objective of the algorithm. Note that the equality of false negative rates implies the equality of true positive rates so this implies the equality of opportunity. After adding the restrictions to the problem it may turn intractable, so a relaxation on them may be needed. Adversarial debiasing We train two classifiers at the same time through some gradient-based method (f.e.: gradient descent). The first one, the predictor tries to accomplish the task of predicting Y {\textstyle Y} , the target variable, given X {\textstyle X} , the input, by modifying its weights W {\textstyle W} to minimize some loss function L P ( y ^ , y ) {\textstyle L_{P}({\hat {y}},y)} . The second one, the adversary tries to accomplish the task of predicting A {\textstyle A} , the sensitive variable, given Y ^ {\textstyle {\hat {Y}}} by modifying its weights U {\textstyle U} to minimize some loss function L A ( a ^ , a ) {\textstyle L_{A}({\hat {a}},a)} . An important point here is that, to propagate correctly, Y ^ {\textstyle {\hat {Y}}} above must refer to the raw output of the classifier, not the discrete prediction; for example, with an artificial neural network and a classification problem, Y ^ {\textstyle {\hat {Y}}} could refer to the output of the softmax layer. Then we update U {\textstyle U} to minimize L A {\textstyle L_{A}} at each training step according to the gradient ∇ U L A {\textstyle \nabla _{U}L_{A}} and we modify W {\textstyle W} according to the expression: ∇ W L P − p r o j ∇ W L A ∇ W L P − α ∇ W L A {\displaystyle \nabla _{W}L_{P}-proj_{\nabla _{W}L_{A}}\nabla _{W}L_{P}-\alpha \nabla _{W}L_{A}} where α \alpha is a tunable hyperparameter that can vary at each time step. The intuitive idea is that we want the predictor to try to minimize L P {\textstyle L_{P}} (therefore the term ∇ W L P {\textstyle \nabla _{W}L_{P}} ) while, at the same time, maximize L A {\textstyle L_{A}} (therefore the term − α ∇ W L A {\textstyle -\alpha \nabla _{W}L_{A}} ), so that the adversary fails at predicting the sensitive variable from Y ^ {\textstyle {\hat {Y}}} . The term − p r o j ∇ W L A ∇ W L P {\textstyle -proj_{\nabla _{W}L_{A}}\nabla _{W}L_{P}} prevents the predictor from moving in a direction that helps the adversary decrease its loss function. It can be shown that training a predictor classification model with this algorithm improves demographic parity with respect to training it without the adversary. Postprocessing The final method tries to correct the results of a classifier to achieve fairness. In this method, we have a classifier that returns a score for each individual and we need to do a binary prediction for them. High scores are likely to get a positive outcome, while low scores are likely to get a negative one, but we can adjust the threshold to determine when to answer yes as desired. Note that variations in the threshold value affect the trade-off between the rates for true positives and true negatives. If the score function is fair in the sense that it is independent of the protected attribute, then any choice of the threshold will also be fair, but classifiers of this type tend 