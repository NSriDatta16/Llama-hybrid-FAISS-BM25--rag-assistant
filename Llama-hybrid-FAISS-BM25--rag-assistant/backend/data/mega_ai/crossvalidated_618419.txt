[site]: crossvalidated
[post_id]: 618419
[parent_id]: 
[tags]: 
How to identify outliers and drop rows in train splits of all folds, when using StratifiedKFold in GridSearchCV?

For predicting whether a subject has liver disease or not, I'm using StratifiedKFold CV in GridSearch for AdaBoost and RandomForest Classsifiers. For Outlier anlaysis, I've identified all feature outliers and extracted their row indices on full dataset but decided against dropping it since there might be some data leakage due to this strategy. My next strategy was using log normalization to reduce the skewness, but the results are still sub-optimal. When maximized for precision, I'm onl touching the maximum of mean_average_score_ of 0.78 with AdaBost and 0.76 with RandomForest. Need a new strategy for dropping outliers in 2 distinct sub-groups: Has liver disease Healthy How do I perform outlier detection/drop only on the training split, of each Stratified fold of the data? Kaggle Liver Patients Dataset
