[site]: crossvalidated
[post_id]: 228704
[parent_id]: 
[tags]: 
How does one Initialize Neural Networks as suggested by Saxe et al using Orthogonal matrices and a gain factor?

I was reading Bengio, Goodfellow and Courville deep learning book and on chapter 8 (Optimization chapter) they mention that Saxe et al have a initialization based on orthogonal matrices and a gain factor $g$ that depends on the non-linearity. The chapter doesn't actually say how to do this initialization. To address this issue I tried reading the paper but it seems to be a bit beyond my level of (maths) sophistication. Does anyone understand what the initialization that they are referring to is done? For example the question's that would be nice to know are: How does one choose orthogonal matrices? Just any K orthogonal matrices for any weight matrices? How is $g$ chosen depending on the non-linearity? I probably should have mentioned but Iam planning to use it with python/tensorflow if possible. 3 Exact solutions to the nonlinear dynamics of learning in deep linear neural networks, Andrew M. Saxe, James L. McClelland, Surya Ganguli
