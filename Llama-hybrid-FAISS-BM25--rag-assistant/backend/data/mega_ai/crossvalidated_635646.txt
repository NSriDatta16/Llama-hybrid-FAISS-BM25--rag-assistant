[site]: crossvalidated
[post_id]: 635646
[parent_id]: 635642
[tags]: 
Linear regression is also existent outside machine learning where it requires much less data. Why does Machine Learning need a lot of data while one can do statistical inference with a small set of data? It is largely a matter of bias-variance trade-off. Kaggle problems often have large datasets that help to reduce variance and allow less biased more flexible models like tree models. On the other hand, too much variation/flexibility is neither good (leads to overfitting). If ensemble methods like random forest do better than a plain decision tree, then it is because there is too much flexibility and it needs to be tamed. Why do 'we' still use linear models? You still get people that try linear models because that is how Kaggle works. People with all sorts of backgrounds try to give it a go at some problem. If they end up at the bottom of the ranking then it just means that the problem requires a flexible model like a tree model: it doesn't make linear regression useless for other problems. Linear regression works very well when the true/best model is close to the solution space of a linear model. Also related: Why do we even bother running regression models? where it is suggested to fit a plethora of variable models instead of a single one that makes sense (because we have enough data anyway). For small problems a linear model might also be more easily interpretable. Such a situation is highlighted in the question Why would you perform transformations over polynomial regression? . You may tackle a bunch of data with all sorts of variable models as in the image below: But a simple model is sometimes easier to interpret, like this:
