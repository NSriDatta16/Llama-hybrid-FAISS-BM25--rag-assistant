[site]: crossvalidated
[post_id]: 346764
[parent_id]: 
[tags]: 
Is Cross Validation useless unless the Hypotheses are nested?

If I generate many random models (without considering the data at all) in a regression setting simply by randomly assigning coefficient values and then evaluating these models over the dataset with an error metric and choosing the best model based on this error metric would I still run into overfitting? Eventually we will end up with the OLS solution (see comments). If this is the case how is Cross Validation different than this procedure? For example in a Ridge or Lasso regression setting I am still generating a bunch of models (indexed by $\lambda$) and evaluating them on unseen data segment and choosing the best one. It seems to me that CV works well with standard regularization methods like Ridge and Lasso is because the tried models are somewhat nested (i.e. Ridge is ordered by Rademacher complexity). Hence the Structural Risk Minimization principle kicks in. Otherwise CV looks like a dead end effort. If we use cross validation to compare bunch of unrelated models we will end up with the random model generation scenario that I described above. Under the Structural Risk Minimization framework, for example in SVM, one bounds the error and reduces the model complexity. So how does CV actually achieve the same effect when applied in conjunction with regularization methods? What to do when compared models are not nested?
