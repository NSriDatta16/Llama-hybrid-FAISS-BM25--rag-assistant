[site]: crossvalidated
[post_id]: 519327
[parent_id]: 
[tags]: 
Measuring uncertainty with bayesian neural network

One of the ways to measure epistemic uncertainty, is using bayesian inference in neural networks. The idea is to learn the posterior over the weights $P(\phi|X)$ which describe the probability distribution over the weights. And then sample from this distribution, run the NN using this sample and calculate the mean and the variance of these results of the NN for all the samples. The variance should represent the uncertainty. We can assume using variational inference to approximate the posterior. I have two questions regarding that: I would expect that uncertainty will be dependent on the specific input i.e. for some parts in the feature space where we lack of data, the network will output high uncertainty and for other parts of the feature space, low uncertainty. If we only learn the posterior over the weights, will the network be able to differentiate between the different inputs? In some cases where the weights are connected directly to the input like categorical feature embedding, i guess it would work fine, but would it work in the general case? One way to solve it is learning the posterior over the output of the network layers instead of learning the posterior over the weights. This way it includes also the input. But in this case my question is: does the mathematical basis of the bayes theorem and variational inference still holds? Should something be changed in the theory\notations if we decide to learn the posterior over the network output layer.
