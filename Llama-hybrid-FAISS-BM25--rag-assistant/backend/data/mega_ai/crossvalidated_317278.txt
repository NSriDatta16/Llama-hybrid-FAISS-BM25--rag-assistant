[site]: crossvalidated
[post_id]: 317278
[parent_id]: 161189
[tags]: 
Learning to classify odd numbers and even numbers is a difficult problem. A simple pattern keeps repeating infinitely. 2,4,6,8..... 1,3,5,7..... Nonlinear activation functions like sin(x) and cos(x) behave similarly. Therefore, if you change your neurons to implement sin and cos instead of popular activation functions like tanh or relu, I guess you can solve this problem fairly easily using a single neuron. Linear transformations always precede nonlinear transformations. Therefore a single neuron will end up learning sin(ax+b) which for the right combination of a & b will output 0's and 1's alternatively in the desired frequency we want which in this case is 1. I have never tried sin or cos in my neural networks before. So, apologies if it ends up being a very bad idea.
