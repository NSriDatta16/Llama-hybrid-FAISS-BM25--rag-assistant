[site]: crossvalidated
[post_id]: 450899
[parent_id]: 450865
[tags]: 
Notice that you can multiply both sides by $p(D)$ : $$p(H|D)p(D) = p(D|H)p(H)$$ This directly corresponds to the graphical proof of Bayes' Theorem - if all your variables are independent, you get the same slice of the pie (total probability) no matter in which order you make the cuts (conditioning on one variable). In other words, $p(D|H)$ is the space of events supported (with various probabilities) by the hypothesi s, and the entire RHS just normalizes that fraction relative to how much of the space of all hypotheses under consideration is occupied by the hypothesis that can generate your data . Similarly, $p(H|D)p(D)$ is the space of all considered hypotheses supported by the data , normalized to the probability of observing that data . Calling $p(D)$ 'evidence' is therefore a bit of a mental shortcut; more accurately, it would be ' the probability of seeing the evidence ', but that's a bit of a mouthful. $D$ would be a better match for the term - it's the set of events that supports or refutes the hypothesis , which is, more or less, the intuitive, real-world definition of evidence. For completeness' sake - please note the 'all's in the bolded parts - the formulation of the theorem you've used, while correct, can only be naively applied in a binary comparison. Otherwise, you need to sum/integrate over all the components - that's what had been holding large-scale Bayesian inference back for a long time.
