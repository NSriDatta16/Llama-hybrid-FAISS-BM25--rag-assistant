[site]: crossvalidated
[post_id]: 103208
[parent_id]: 103205
[tags]: 
As I'm sure you are aware, ARMA is an acronym for AutoRegressive Moving Average (Stochastic Process). More fully, we use ARMA(p, q) where p is the order of the autoregressive portion and q the order of the moving average portion. Both of the statements are saying the same thing: if you form a linear combination of AR processes an ARMA process of some order results. This is a consequence of an inversion theorem in Time Series analysis which states that all stationary ARMA processes have a representation as a (possibly infinite) stationary AR process. The ARMA model expresses $X_t$ as the sum of polynomials in the lagged values of X_t and the lagged values of $\epsilon_t$ (the i.i.d. noise terms). So an AR(1) model looks like $X_t = \theta X_{t-1} + \epsilon_t$. The single lagged value of the series is $X_{t-1} = \theta X_{t-2} + \epsilon_{t-1}$. Now, let $0 $Y_t = \phi\theta X_{t-1} + (1-\phi)\theta X_{t-2} + \phi \epsilon_t + (1-\phi) \epsilon_{t-2}$. Some mild algebra (basically arranging things to make a $Y_{t-1}$ appear on the right hand side) puts this in the form $Y_t - \epsilon_t = (\xi_1 Y_{t-1} + \xi_2 Y_{t-2}) + (\phi \epsilon_{t-1} + \phi^2 \epsilon_{t-2})$ Note that this is the sum of a lagged polynomial in $Y_t$ and a lagged polynomial in the noise vector, $\epsilon$. This is an ARMA process.
