[site]: crossvalidated
[post_id]: 416244
[parent_id]: 
[tags]: 
Quadratic Approximation of the binary logistic regression

I am using https://web.stanford.edu/~hastie/Papers/glmnet.pdf package to solve my optimization problem for the Binary Logistic Regression. On page 10 it is stated that the quadratic approximation of the log likelihood function is used in the algorithm which is understandable due to speed. I am trying to get from eq.14 to eq.15 in this paper using the Taylor expansion however got stuck. Below is what I have achieved so far: argmin { $\frac{1}{n}$ $\sum_{i=1}^n $ -( $\bf x_i^T$ $\bf{\beta}$ ) $y_i$ $+ log(1+exp($ ( $\bf x_i^T$ $\bf{\beta}$ )) $+{\lambda}||{\beta}||_1$ } $...(1)$ $\frac{-\partial\ell}{\partial\beta}$ = $-\mathbf x_i^T y_i+\pi_i \mathbf x_i^T$ $\frac{-\partial^2\ell}{\partial\beta^2}$ = $-\mathbf x_i^T \pi_i (1-\pi_i) \mathbf x_i^T$ I have ahieved the 1st and 2nd derivative but when am trying to get to the quadrative approximation of $(1)$ am not getting the same as in eq.15 of the above mentioned paper. Can someone help me please?
