[site]: crossvalidated
[post_id]: 118366
[parent_id]: 118360
[tags]: 
It sounds like it would be useful to think a little bit more carefully over why you want to include the random slopes $\beta_i$. I can think of two good reasons, and one bad reason. Good reason 1. You are interested in the average heterogeneity in the time coefficient. Then $\sigma_\beta$ gives you just that--and you can get asymptotic confidence intervals, etc, if you so desire. Good reason 2. You desire valid inference (standard errors or confidence intervals) on $\beta$ or $\alpha$ and you suspect temporal correlation ($Cov(\epsilon_{it_1}, \epsilon_{it_2}) \neq 0$) of a form that can be modeled with a random slope. You point out that if $E(\epsilon)=0$, then $E(\hat \beta) = \beta$, but your standard errors on $\beta$ will be invalid if there's unmodeled correlation in your data. Bad reason 1. You want a way to sorta, kinda estimate the individual slopes $\beta_i$ but don't want to pay the cost of that many degrees of freedom. The last motivation is probably a bad reason because the random effect model has that $\beta_i \perp \epsilon_i$ by construction--so leads to an uninterpretable mess if there is any sort of confounding, and by the very nature of caring about specific values of $\beta_i$ suggests that you think that some units might be special, hence confounded with $\epsilon_i$. For example, your coefficients will no longer maintain the interpretation of unit changes in the average of $y$ per unit changes in $x$, with all else held constant. As far as the bias/variance tradeoff goes, you'd only accept that tradeoff if it leads to a smaller mean square error in estimating $\beta_i$, and it's not obvious to me that it always does with random effects models under confounding. The amount of shrinkage under random-effects is data-dependent, so perhaps you end up shrinking too much, or not enough. If you want lower MSE, you probably should just use ridge regression and tune with cross-validation. You might also play around with some simulations under your assumed data-generating model to see how the different models behave.
