n and Bayes A connection between regularization theory and Bayesian theory can only be achieved in the case of finite dimensional RKHS. Under this assumption, regularization theory and Bayesian theory are connected through Gaussian process prediction. In the finite dimensional case, every RKHS can be described in terms of a feature map Φ : X → R p {\displaystyle \Phi :{\mathcal {X}}\rightarrow \mathbb {R} ^{p}} such that k ( x , x ′ ) = ∑ i = 1 p Φ i ( x ) Φ i ( x ′ ) . {\displaystyle k(\mathbf {x} ,\mathbf {x} ')=\sum _{i=1}^{p}\Phi ^{i}(\mathbf {x} )\Phi ^{i}(\mathbf {x} ').} Functions in the RKHS with kernel K {\displaystyle \mathbf {K} } can then be written as f w ( x ) = ∑ i = 1 p w i Φ i ( x ) = ⟨ w , Φ ( x ) ⟩ , {\displaystyle f_{\mathbf {w} }(\mathbf {x} )=\sum _{i=1}^{p}\mathbf {w} ^{i}\Phi ^{i}(\mathbf {x} )=\langle \mathbf {w} ,\Phi (\mathbf {x} )\rangle ,} and we also have that ‖ f w ‖ k = ‖ w ‖ . {\displaystyle \|f_{\mathbf {w} }\|_{k}=\|\mathbf {w} \|.} We can now build a Gaussian process by assuming w = [ w 1 , … , w p ] ⊤ {\displaystyle \mathbf {w} =[w^{1},\ldots ,w^{p}]^{\top }} to be distributed according to a multivariate Gaussian distribution with zero mean and identity covariance matrix, w ∼ N ( 0 , I ) ∝ exp ⁡ ( − ‖ w ‖ 2 ) . {\displaystyle \mathbf {w} \sim {\mathcal {N}}(0,\mathbf {I} )\propto \exp(-\|\mathbf {w} \|^{2}).} If we assume a Gaussian likelihood we have P ( Y | X , f ) = N ( f ( X ) , σ 2 I ) ∝ exp ⁡ ( − 1 σ 2 ‖ f w ( X ) − Y ‖ 2 ) , {\displaystyle P(\mathbf {Y} |\mathbf {X} ,f)={\mathcal {N}}(f(\mathbf {X} ),\sigma ^{2}\mathbf {I} )\propto \exp \left(-{\frac {1}{\sigma ^{2}}}\|f_{\mathbf {w} }(\mathbf {X} )-\mathbf {Y} \|^{2}\right),} where f w ( X ) = ( ⟨ w , Φ ( x 1 ) ⟩ , … , ⟨ w , Φ ( x n ⟩ ) {\displaystyle f_{\mathbf {w} }(\mathbf {X} )=(\langle \mathbf {w} ,\Phi (\mathbf {x} _{1})\rangle ,\ldots ,\langle \mathbf {w} ,\Phi (\mathbf {x} _{n}\rangle )} . The resulting posterior distribution is then given by P ( f | X , Y ) ∝ exp ⁡ ( − 1 σ 2 ‖ f w ( X ) − Y ‖ n 2 + ‖ w ‖ 2 ) {\displaystyle P(f|\mathbf {X} ,\mathbf {Y} )\propto \exp \left(-{\frac {1}{\sigma ^{2}}}\|f_{\mathbf {w} }(\mathbf {X} )-\mathbf {Y} \|_{n}^{2}+\|\mathbf {w} \|^{2}\right)} We can see that a maximum posterior (MAP) estimate is equivalent to the minimization problem defining Tikhonov regularization, where in the Bayesian case the regularization parameter is related to the noise variance. From a philosophical perspective, the loss function in a regularization setting plays a different role than the likelihood function in the Bayesian setting. Whereas the loss function measures the error that is incurred when predicting f ( x ) {\displaystyle f(\mathbf {x} )} in place of y {\displaystyle y} , the likelihood function measures how likely the observations are from the model that was assumed to be true in the generative process. From a mathematical perspective, however, the formulations of the regularization and Bayesian frameworks make the loss function and the likelihood function to have the same mathematical role of promoting the inference of functions f {\displaystyle f} that approximate the labels y {\displaystyle y} as much as possible. See also Regularized least squares Bayesian linear regression Bayesian interpretation of Tikhonov regularization == References ==