[site]: crossvalidated
[post_id]: 488577
[parent_id]: 488537
[tags]: 
Yes, you can repeat the cross validations (both or any of them, as necessary). Repeated cross validation improves the estimate of generalization error in a specific way: it allows you to easily measure and reduce random uncertainty due to model instability (unstable predictions). I find it more convenient to think of the nested cross valiation a bit differently : Outer validation: aim is to measure generalization error of a fully tuned, useable model. Repeated CV is a good candidate for this. Training of a fully tuned model: As usual, this self-tuning training function is a black box to the outer validation. The training procedure can internally use further (inner) cross validations, which of course can be repeated as well. IMHO the advantages of this point of view are It is immediately clear that the details of the validations can be choosen independently for the outer vs. inner validation procedures. You can adapt/organize the inner validation procedure as suitable for the type of model you tune. It will automatically lead to a correctly nested setup. Coding this way guards against indexing errors that leak outer test data into the tuning part. The final model can be trained as usual: by calling the self-tuning training function on the whole data set.
