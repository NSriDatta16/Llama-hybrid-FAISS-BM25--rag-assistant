[site]: crossvalidated
[post_id]: 192109
[parent_id]: 192088
[tags]: 
This is a sequential Monte Carlo problem where one considers a sequence of posteriors $\pi_1,\pi_2,\ldots,\pi_T$ that are "reasonably" close from one another for the simulations from $\pi_t$ to be interesting proposals for $\pi_{t+1}$. The solution you propose is not valid as such because of the reliance on this proportion $r$ that is actually equal to zero when running the sequential Monte Carlo scheme. (Or, in other words, there is no prior ratio for the marginal distributions of $y_{1:t}$ and of $y_{1:t+1}$.) A very similar resolution is sequential importance sampling where the current sample from $\pi_t$ $\theta_1^t,...,\theta_G^t$ is reweighted by $$\{\pi_{t+1}/\pi_t\}(\theta_g^t)$$ to become a sample from $\pi_t$. In the case of a state-space model where $$\pi_{t+1}(\theta)\propto\pi_t(\theta) f(y_{t+1}|\theta,y_{1:t})$$ the importance weight reduces to $$\{\pi_{t+1}/\pi_t\}(\theta)\propto f(y_{t+1}|\theta,y_{1:t})$$ While valid per se (in the importance sampling sense that the expectation of the weighted average produces the correct expectation under $\pi_{t+1}$), this na√Øve type of sequential importance sampling is dominated by more advanced particle filters where the sample itself is modified before being reweighted. This avoids in particular the degeneracy phenomenon, namely that the weights$$\prod_{j=1}^h f(y_{t+j}|\theta,y_{1:(t+j-1)})$$degenerate to zero almost surely. I suggest you read the Wikipedia entry on particle filters , as it is quite detailed and definitely rigorous, given that one of the leaders in the field, Pierre Del Moral, has contributed to its improvement.
