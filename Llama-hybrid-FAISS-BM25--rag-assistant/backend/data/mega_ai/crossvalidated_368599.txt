[site]: crossvalidated
[post_id]: 368599
[parent_id]: 310940
[tags]: 
It can help because you have to update weights, and most optimization schema based on backpropagation through gradient descent impose a single step size for all weights in a layer. If you have a single output node, then probably it won't help much. If you have more then it might make sense to normalize outputs. You can do it within the neural network architecture, then you get outputs in the correct scale anyways, but the neural network sees them normalized.
