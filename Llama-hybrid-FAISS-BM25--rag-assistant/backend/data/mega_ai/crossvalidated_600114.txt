[site]: crossvalidated
[post_id]: 600114
[parent_id]: 419916
[tags]: 
Firstly, there's artificial examples. E.g. a 95% CI that contains the whole parameter space 95% of the time and the is just the interval from 11.23234576 to 11.2323457 5% of the time is obviously a valid 95% CI (i.e. contains the true value at least 95% of the time), but also completely useless. Nobody uses that kind of thing in practice, so let's ignore these types of examples. Secondly, there's the situation where you use something like sample mean $\pm$ 1.96 $\times$ standard error. I.e. you are effectively using flat priors. If you truly knew absolutely nothing about the problem at hand, I guess that might be a reasonable prior belief and okay. In practice, we are hardly ever in that situation. We often have some idea of the variability of the measurement, as well as location (e.g. we have some idea by how much someone's blood pressure can change without treatment, we have some idea how much of an effect a new blood pressure drug could possibly have etc.). If one ignores that prior information, then extreme estimates and confidence intervals created using flat priors are much more likely to not contain the true value than the ones that are more consistent with the prior information. In practice, I believe the issue is usually from the second case. This is a real issue e.g. for a drug company that has a series of drugs coming out of pre-clinical research and there's not that much to distinguish them (all are considered sort of promising and showed some promising results in in-vitro and animal experiments). Now, these drugs are each being tested in small proof-of-concept studies (especially so, if these are powered for e.g. 80% power at the 10% one-sided significance level for very optimistic assumed effect sizes). If about 30 to 50% of the new drugs being tried really have a meaningfully large effect (with some obvious bounds on that), then confidence intervals containing very large effect sizes are more likely to lie above the true effect than those that are less "optimistic". If you then need to make decisions (proceed or not) and for determining the size of the next studies (and likelihood of success), using the confidence intervals as if they were 95% credible intervals is likely being much too optimistic. That's why in practice people will take much more Bayesian approaches (see e.g. what one company does and another ).
