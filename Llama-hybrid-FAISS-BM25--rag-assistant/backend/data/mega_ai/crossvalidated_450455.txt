[site]: crossvalidated
[post_id]: 450455
[parent_id]: 
[tags]: 
Termination Condition for AdaBoost.R2

I can't quite wrap my head around the termination condition of AdaBoost.R2 as defined by Drucker in this paper . On page 2 of the paper he states to "repeat the following while the average loss* $\bar{L}$ defined below is less than .5". Also he later (page 3) states: "Convergence to zero training error depends on obtaining a learning machine such that $\bar{L} " Why is that the case? Additonal Question: In this paper Dietterich mentions that "A necessary and sufficient condition for an ensemble of classifiers to be more accurate than any of its individual members is if the classifiers are accurate and diverse". Is there an analogous condition for ensembles of regressors? And if yes, does this relate to my original question? Edit: * The average loss $\bar{L} = \sum_{i=1}^N L_ip_i$ is the weighted average loss over the $N$ samples in the original training set, yielded by the current "candidate" predictor that is to be added in the ensemble. As with regular AdaBoost "candidate" predictors are trained on separate training sets. $p_i$ , which is just the normalized weight of a sample, denotes the probability of sample $i$ being picked form the original training set to be used in the predictors training set. $L_i$ denotes the loss the predictor yields on the $i^{th}$ sample in the training set. There are three proposed loss functions in the paper (although any loss function with an image in $[0,1]$ would do): $$ L_i = \frac{|y_i - y^p(x_i)|}{D} \quad \text{(linear)} \\ L_i = \frac{|y_i - y^p(x_i)|^2}{D} \quad \text{(square law)} \\ L_i = 1-exp \left( \frac{-|y_i - y^p(x_i)|}{D } \right) \quad \text{(exponential)} \; , \\ $$ where $y^p$ is the hypothesis of the predictor and $D$ denotes the maximum error yielded of all samples training set.
