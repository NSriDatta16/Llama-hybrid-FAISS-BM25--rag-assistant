[site]: datascience
[post_id]: 44045
[parent_id]: 44042
[tags]: 
Scaling your input variables in Linear Regression has both advantages and disadvantages. By not scaling you let the parameters, i.e. weights, to have significance by themselves. If the weight(featureA) = 50 it means that an increase of 1 in feature A will increase the target by 50. Nonetheless, the features may have completely different natural ranges (number of bedrooms vs square feet), which may slow the training speed of algorithms based on gradient descend. If you decide to scale , you may have an increase in speed but you loose the intrinsic interpretability of the parameters. Nonetheless, you can now compare relative feature importances, since they will be forced to a common range of values. Since your target is a Binary Classification I advise you using the Logistic Regression, which is still a linear algorithm but it ends with a sigmoid function which forces the outputs to be between y âˆˆ [0, 1] . $$ sigmoid(x) = \frac{\mathrm{1} }{\mathrm{1} + e^{-x} } $$ Similarly to your MLR, Logistic Regression allows for interpretability in the features' weights.
