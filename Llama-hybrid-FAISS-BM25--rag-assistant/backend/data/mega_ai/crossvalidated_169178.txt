[site]: crossvalidated
[post_id]: 169178
[parent_id]: 169156
[tags]: 
Ok, so let's analyze the example of the child clustering its toys. Imagine the child has only 3 toys: a blue soccer ball a blue freesbe a green cube (ok maybe it's not the most fun toy you can imagine) Let's do the following initial hypothesis regarding how a toy can be made: Possible colors are: red, green, blue Possible shapes are: circle, square, triangle Now we can have have (num_colors * num_shapes) = 3 * 3 = 9 possible clusters. The boy would cluster the toys as follows: CLUSTER A) contains the blue ball and the blue freesbe, because thay have the same color and shape CLUSTER B) contains the super-funny green cube Using only these 2 dimensions (color, shape) we have 2 non-empty clusters: so in this first case 7/9 ~ 77% of our space is empty. Now let's increase the number of dimensions the child has to consider. We do also the following hypothesis regarding how a toy can be made: Size of the toy can vary between few centimeters to 1 meter, in step of ten centimeters: 0-10cm, 11-20cm, ..., 91cm-1m Weight of the toy can vary in a similar manner up to 1 kilogram, with steps of 100grams: 0-100g, 101-200g, ..., 901g-1kg. If we want to cluster our toys NOW, we have (num_colors * num_shapes * num_sizes * num_weights) = 3 * 3 * 10 * 10= 900 possible clusters. The boy would cluster the toys as follows: CLUSTER A) contains the blue soccer ball because is blue and heavy CLUSTER B) contains the blue freesbe because is blue and light CLUSTER C) contains the super-funny green cube Using the current 4 dimensions (shape, color, size, weigth) only 3 clusters are non empty: so in this case 897/900 ~ 99.7% of the space is empty. This is an example of what you find on Wikipedia ( https://en.wikipedia.org/wiki/Curse_of_dimensionality ): ...when the dimensionality increases, the volume of the space increases so fast that the available data become sparse. Edit: I'm not sure i could really explain to a child why distance sometimes goes wrong in high-dimensional spaces, but let's try to proceed with our example of the child and his toys. Consider only the 2 first features {color, shape} everyone agrees that the blue ball is more similar to the blue freesbe than to the green cube. Now let's add other 98 features {say: size, weight, day_of_production_of_the_toy, material, softness, day_in_which_the_toy_was_bought_by_daddy, price etc}: well, to me would be increasingly more difficult to judge which toy is similar to which. So: A large number of features can be irrelevant in a certain comparison of similarity, leading to a corruption of the signal-to-noise ratio. In high dimensions, all examples "look-alike". If you listen to me, a good lecture is "A Few Useful Things to Know about Machine Learning" ( http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf ), paragraph 6 in particular presents this kind of reasoning. Hope this helps!
