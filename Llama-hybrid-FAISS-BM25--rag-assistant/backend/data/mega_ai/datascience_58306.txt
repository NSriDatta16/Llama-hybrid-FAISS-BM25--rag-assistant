[site]: datascience
[post_id]: 58306
[parent_id]: 58026
[tags]: 
As far as I can understand, you know the derivation and just want to get why we use error signal whenever we talk about backpropagation. You said correctly that it is helpful for calculation of weights and biases and that is exactly the main reason why it is standardised. Here is a neat derivation that uses the error signal: The process of backpropagating the error signal can iterate all the way back to the input layer by successively projecting $\delta_k$ (error signal for output layer) back through $w_{jk}$ , then through the activation function for the hidden layer via $g'_j$ to give the error signal $\delta_j$ , and so on. This backpropagation concept is central to training neural networks with more than one layer. Also, it is useful to save us from computing the same values again and again. If we do not standardize this concept, we would be calculating the same values repetitively and as we go deeper into the network, calculating the gradients will become more cumbersome. Using the concept of error signal to calculate the weight gradients at any layer $l$ in an arbitrarily-deep neural network, we simply need to calculate the backpropagated error signal that reaches that layer $\delta_l$ and weight it by the feed-forward signal $a_{l-1}$ feeding into that layer. Hence, using the error signal is an integral part of the backpropagation technique and forms the basis of it's derivation!
