[site]: crossvalidated
[post_id]: 582673
[parent_id]: 339894
[tags]: 
A possible reasoning is that residual connections reduce the feature space that a network searches for, as mentioned here : A neural network without residual parts explores more of the feature space. This makes it more vulnerable to perturbations that cause it to leave the manifold, and necessitates extra training data to recover. Due to lower sensitivity to perturbations, the network can tend to have smaller loss values, leading to smaller gradients and hence prevent gradient explosion.
