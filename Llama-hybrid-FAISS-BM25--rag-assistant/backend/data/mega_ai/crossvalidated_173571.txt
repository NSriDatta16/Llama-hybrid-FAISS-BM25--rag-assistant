[site]: crossvalidated
[post_id]: 173571
[parent_id]: 173568
[tags]: 
That is very strange advice, I am forced to wonder who in the world advanced it. The correct way to fit a logistic regression leaves the zeros and ones alone, and determines the parameters that minimize the log likelihood function: $$ f(\beta) = \sum_i y_i \log(p_i) + (1 - y_i) \log(1 - p_i) $$ Where $p_i$ is shorthand for $$ p_i = \frac{e^{\beta \cdot x_i}}{1 + e^{\beta \cdot x_i} }$$ The exponents are vector dot products and $p_i$ is a function of the parameter vector $\beta$. The $y_i$s in this expression are either $0$ or $1$, and it's pleasant to notice that this causes each term to be equal to either $$ \log(p_i) $$ or $$ \log(1 - p_i) $$ Generally, yes, this expression is minimized using a method called iteratively re-weighted least squares, which is itself derived from Newton's classical method for minimizing non-linear functions. R's glm function does exactly this. No response replacement in sight.
