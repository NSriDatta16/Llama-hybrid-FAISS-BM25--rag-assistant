 k ⟩ {\displaystyle \langle I,g^{-1}t_{k}\rangle } is equal to zero everywhere except some subset of G 0 {\displaystyle G_{0}} . This subset is called support of ⟨ I , g − 1 t k ⟩ {\displaystyle \langle I,g^{-1}t_{k}\rangle } and denoted as supp ⁡ ( ⟨ I , g − 1 t k ⟩ ) {\displaystyle \operatorname {supp} (\langle I,g^{-1}t_{k}\rangle )} . It can be proven that if for a transformation g ′ {\displaystyle g^{\prime }} , support set will also lie within g ′ G 0 {\displaystyle g^{\prime }G_{0}} , then signature of I {\displaystyle I} is invariant with respect to g ′ {\displaystyle g^{\prime }} . This theorem determines the range of transformations for which invariance is guaranteed to hold. One can see that the smaller is supp ⁡ ( ⟨ I , g − 1 t k ⟩ ) {\displaystyle \operatorname {supp} (\langle I,g^{-1}t_{k}\rangle )} , the larger is the range of transformations for which invariance is guaranteed to hold. It means that for a group that is only locally compact, not all templates would work equally well anymore. Preferable templates are those with a reasonably small supp ⁡ ( ⟨ g I , t k ⟩ ) {\displaystyle \operatorname {supp} (\langle gI,t_{k}\rangle )} for a generic image. This property is called localization: templates are sensitive only to images within a small range of transformations. Although minimizing supp ⁡ ( ⟨ g I , t k ⟩ ) {\displaystyle \operatorname {supp} (\langle gI,t_{k}\rangle )} is not absolutely necessary for the system to work, it improves approximation of invariance. Requiring localization simultaneously for translation and scale yields a very specific kind of templates: Gabor functions. The desirability of custom templates for non-compact group is in conflict with the principle of learning invariant representations. However, for certain kinds of regularly encountered image transformations, templates might be the result of evolutionary adaptations. Neurobiological data suggests that there is Gabor-like tuning in the first layer of visual cortex. The optimality of Gabor templates for translations and scales is a possible explanation of this phenomenon. Non-group transformations Many interesting transformations of images do not form groups. For instance, transformations of images associated with 3D rotation of corresponding 3D object do not form a group, because it is impossible to define an inverse transformation (two objects may looks the same from one angle but different from another angle). However, approximate invariance is still achievable even for non-group transformations, if localization condition for templates holds and transformation can be locally linearized. As it was said in the previous section, for specific case of translations and scaling, localization condition can be satisfied by use of generic Gabor templates. However, for general case (non-group) transformation, localization condition can be satisfied only for specific class of objects. More specifically, in order to satisfy the condition, templates must be similar to the objects one would like to recognize. For instance, if one would like to build a system to recognize 3D rotated faces, one need to use other 3D rotated faces as templates. This may explain the existence of such specialized modules in the brain as one responsible for face recognition. Even with custom templates, a noise-like encoding of images and templates is necessary for localization. It can be naturally achieved if the non-group transformation is processed on any layer other than the first in hierarchical recognition architecture. Hierarchical architectures The previous section suggests one motivation for hierarchical image recognition architectures. However, they have other benefits as well. Firstly, hierarchical architectures best accomplish the goal of ‘parsing’ a complex visual scene with many objects consisting of many parts, whose relative position may greatly vary. In this case, different elements of the system must react to different objects and parts. In hierarchica