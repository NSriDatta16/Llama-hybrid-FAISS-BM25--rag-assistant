[site]: crossvalidated
[post_id]: 448126
[parent_id]: 363558
[tags]: 
Sampling a language model is a good way of understanding what the model has learnt. You're right there's no point randomly sampling(with equal probability of each word at each time step t), then why have you even trained your model!? We rather sample based on the probability distribution of $\hat{y^{t}}$ . Eg: say probability of $\hat{y_{i}^{t}}=0.16$ (where $i^{th}$ index in probability distribution of our prediction for time step t) means that we select word at $i^{th}$ index with the probability 0.16. You do that in numpy using np.random.choice . Also if you select the word with the highest probability you'll end up with the same sentence given the same initial word. This is a great link to understand novel sampling.
