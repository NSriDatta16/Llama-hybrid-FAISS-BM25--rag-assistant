[site]: datascience
[post_id]: 103547
[parent_id]: 103541
[tags]: 
From what I understand, you are looking for a term-weighing scheme. I'd try to address this using TF-IDF which is the most popular term weighing scheme today. Here's a brief recap of TF-IDF from wiki: In information retrieval, tf–idf, TF*IDF, or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.[1] It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes today As your use-case is different from a standard document search, I'd try a modified ranking method as listed below: Find all the common words between A and B , let's say there are n such words. Calculate TF-IDF score for all common words individually for A and B and add them to have n scores. You will need to use whole of your text corpus for this. Define a parameter k (1,2,3 etc), which will act as cut-off to the number of common words used in ranking. Take the sum total of TF-IDF of the top k common words and apply thresholding on it to build a binary 'relevant', 'irrelevant' classification.
