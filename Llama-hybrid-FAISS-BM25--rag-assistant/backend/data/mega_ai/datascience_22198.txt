[site]: datascience
[post_id]: 22198
[parent_id]: 
[tags]: 
fitting classifier object of type 'int' has no len()

We have LDA topic modeling whose purpose is to generate a number of topics given a set of documents. So each document can belong to various topics. Also, we can evaluate the model we have created. one of the approaches is using classification method like SVM. My goal is to evaluate the created model. I am faced with two kinds of code for making the LDA model. Approach 1: # generate LDA model id2word = corpora.Dictionary(texts) # Creates the Bag of Word corpus. mm = [id2word.doc2bow(text) for text in texts] # Trains the LDA models. lda = ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=10, update_every=1, chunksize=10000, passes=1,gamma_threshold=0.00, minimum_probability=0.00) this way I can not use Fit_transform Approach 2: tf_vectorizer = CountVectorizer(max_features=n_features, stop_words='english') tf = tf_vectorizer.fit_transform(data_samples) lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5, learning_method='online', learning_offset=50., random_state=0) lda_x=lda.fit_transform(tf) in the first approach, there is no fit_transform method for LDA model, I do not know why as I don't understand the difference between them. Anyway, I need to pass that LDA model I have created with the first approach to SVM(the reason I put these two approach here is that I know with the second approach there is no error probably because of fit_transform but for some reason I couldn't use that. Final code: import os from gensim.models import ldamodel from nltk.tokenize import RegexpTokenizer from nltk.stem.porter import PorterStemmer from gensim import corpora, models from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer from sklearn.pipeline import Pipeline from sklearn.multiclass import OneVsRestClassifier from sklearn.svm import LinearSVC tokenizer = RegexpTokenizer(r'\w+') # create English stop words list en_stop = {'a'} # Create p_stemmer of class PorterStemmer lines=[] p_stemmer = PorterStemmer() lisOfFiles=[x[2] for x in os.walk("data")] fullPath = [x[0] for x in os.walk("data")] for j in lisOfFiles[2]: with open(os.path.join(fullPath[2],j)) as f: a=f.read() lines.append(a) for j in lisOfFiles[3]: with open(os.path.join(fullPath[3],j)) as f: a=f.read() lines.append(a) for j in lisOfFiles[4]: with open(os.path.join(fullPath[4],j)) as f: a=f.read() lines.append(a) # compile sample documents into a list doc_set = lines # list for tokenized documents in loop texts = [] # loop through document list for i in doc_set: # clean and tokenize document string raw = i.lower() tokens = tokenizer.tokenize(raw) # remove stop words from tokens stopped_tokens = [i for i in tokens if not i in en_stop] # stem tokens stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens] # add tokens to list texts.append(stemmed_tokens) # generate LDA model id2word = corpora.Dictionary(texts) # Creates the Bag of Word corpus. mm = [id2word.doc2bow(text) for text in texts] # Trains the LDA models. lda = ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=10, update_every=1, chunksize=10000, passes=1,gamma_threshold=0.00, minimum_probability=0.00) # Assigns the topics to the documents in corpus dictionary = corpora.Dictionary(texts) # convert tokenized documents into a document-term matrix corpus = [dictionary.doc2bow(text) for text in texts] #creating the labels lda_corpus = lda[mm] label_y=[] for i in lda_corpus: new_y = [] for l in i: sorted_labels = sorted(i, key=lambda z: z[0], reverse=True) if l[1] > 0.005: new_y.append(l[0]) label_y.append(new_y) classifier = Pipeline([ ('vectorizer', CountVectorizer(max_df=2,min_df=1)), ('tfidf', TfidfTransformer()), ('clf', OneVsRestClassifier(LinearSVC()))]) classifier.fit(lda, label_y) As you see in my code I have used the first approach for some reasons, but in the last line, it raises an error(object of type int has no len()). It seems it can not accept LDA created in this way (I was thinking because in this way I didn't use fit_transform) how can I fix this error with my code? Stack trace: /home/saria/tfwithpython3.6/bin/python /home/saria/PycharmProjects/TfidfLDA/test4.py Using TensorFlow backend. Traceback (most recent call last): File "/home/saria/PycharmProjects/TfidfLDA/test4.py", line 92, in classifier.fit(lda, label_y) File "/home/saria/tfwithpython3.6/lib/python3.5/site-packages/sklearn/pipeline.py", line 268, in fit Xt, fit_params = self._fit(X, y, **fit_params) File "/home/saria/tfwithpython3.6/lib/python3.5/site-packages/sklearn/pipeline.py", line 234, in _fit Xt = transform.fit_transform(Xt, y, **fit_params_steps[name]) File "/home/saria/tfwithpython3.6/lib/python3.5/site-packages/sklearn/feature_extraction/text.py", line 839, in fit_transform self.fixed_vocabulary_) File "/home/saria/tfwithpython3.6/lib/python3.5/site-packages/sklearn/feature_extraction/text.py", line 760, in _count_vocab for doc in raw_documents: File "/home/saria/tfwithpython3.6/lib/python3.5/site-packages/gensim/models/ldamodel.py", line 1054, in __getitem__ return self.get_document_topics(bow, eps, self.minimum_phi_value, self.per_word_topics) File "/home/saria/tfwithpython3.6/lib/python3.5/site-packages/gensim/models/ldamodel.py", line 922, in get_document_topics gamma, phis = self.inference([bow], collect_sstats=per_word_topics) File "/home/saria/tfwithpython3.6/lib/python3.5/site-packages/gensim/models/ldamodel.py", line 429, in inference if len(doc) > 0 and not isinstance(doc[0][0], six.integer_types + (np.integer,)): TypeError: object of type 'int' has no len() Process finished with exit code 1
