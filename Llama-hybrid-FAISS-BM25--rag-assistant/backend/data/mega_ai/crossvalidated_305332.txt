[site]: crossvalidated
[post_id]: 305332
[parent_id]: 305262
[tags]: 
In general, the exactly normalization of data isn't super important in neural networks as long as the inputs are at some reasonable scale. As Alex mentioned, with images, normalization to 0 and 1 happens to be very convenient. The fact that normalization doesn't matter much is only made stronger by use of batch-normalization, which is a function/layer frequently used in neural networks which renormalizes the activations halfway through the network to zero mean and unit variance. And the authors of the paper you linked did use batch normalization, which means however the data was normalized before, it was renormalized a bunch of times inside the network anyway. Furthermore, reading their code, which is on github, they actually did preprocess the data two ways -- with zero mean unit variance normalization, and also with min 0 max 1 normalization. They didn't explain why they chose one preprocessed dataset over the other, but I suspect they either just arbitrarily to use min 0 max 1 normalization, or some preliminary hyperparameter searches showed that one worked better than the other for whatever reason.
