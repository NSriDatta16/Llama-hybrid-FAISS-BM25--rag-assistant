[site]: crossvalidated
[post_id]: 51287
[parent_id]: 
[tags]: 
SVM classification step on embedded system with RBF kernel

I am about to implement the classification step of a trained SVM model. I would like to ask, how the actual classification step is carried out (assuming I would like to port that step to some low-level language)? From my trained Matlab SVM model I have: Support vectors (n * #features) the bias (1x1) alpha (n * 1) shift (1 x n) scaleFactor (1 x n) sigma for rbf (1x1) Given a new sample (1 x #features) I would carry out the classification step as follows: Scale and shift each feature in sample: sample = scaleFactor * (sample + shift) Calculate the kernel mapping with an RBF with kernel = exp(-1/(2*sigma^2) * ||x-y_i||^2) where x is my new sample and y every single support vector (?) Now I am puzzled: Is every distance between x and y_i multiplied by the appropriate alpha? Are all these values summed and then the bias added followed by a simple sign() ? So: sign(sum(exp(-1/(2*sigma^2) * ||x-y_i||^2) * alpha_i) + bias) Would that be correct? If so, to save memory on runtime - is there a way to divide the kernel computation in a way that not all support vectors have to be stored in memory?
