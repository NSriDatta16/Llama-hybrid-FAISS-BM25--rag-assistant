[site]: crossvalidated
[post_id]: 603576
[parent_id]: 
[tags]: 
A better linear model has less precision(relative to the worse model) at a larger threshold

I trained two models using the same algorithm - logistic regression ( LogisticRegression(max_iter=180, C=1.05) for ~27 features and ~330K observations). I used the same features for both models, except six additional feature for the second model. When I use the threshold 0.5, the second model has better precision than the first model(which has fewer features), but when I set the threshold to 0.6 then first model is better than the second. I use k-fold with 11 folds for validation. How is it possible? And how can I achieve better precision with the second model? UPD I checked the precision recall AUC for both models. AUC for the first model = 0.56821 , for the second = 0.56814 . So, according to PR AUC I got a worse model. I also checked feature importance using this code: kbest = SelectKBest(f_classif, k=1) kbest.fit(df[additional_x_columns], df['target']) print(kbest.pvalues_) And got this: [6.56951599e-01 4.17353178e-40 0.00000000e+00 0.00000000e+00 2.21697597e-93 9.36746822e-03] So, most of them are significant. How can I get worse model with these features? UPD2
