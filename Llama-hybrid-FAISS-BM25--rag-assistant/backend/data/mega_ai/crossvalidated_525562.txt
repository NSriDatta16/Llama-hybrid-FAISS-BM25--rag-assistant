[site]: crossvalidated
[post_id]: 525562
[parent_id]: 
[tags]: 
Why do Bayesian neural networks result in intractable distributions?

I am trying to read and understand this paper . They are introducing a method inspired by Assumed Density Filtering (ADF) to train a Bayesian neural network. In section 3, where they are explaining their backpropagation model and comparing it to the normal backpropagation, they say, "PBP also has two phases equivalent to the ones of BP. In the first phase, the input data is propagated forward through the network. However, since the weights are now random, the activations produced in each layer are also random and result in (intractable) distributions .". What has been confusing me is that why the multiplication of a constant by an initial distribution of a weight results in an intractable distribution? what do they mean by the intractability of these distributions? I appreciate any help. I am new to the field of BNNs and have been having a difficult time adjusting myself to the methods introduced in related papers.
