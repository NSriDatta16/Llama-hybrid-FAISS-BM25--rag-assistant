[site]: datascience
[post_id]: 64086
[parent_id]: 64042
[tags]: 
It can mean both, depending on the context. In order to improve prediction, a Network might need either more parameters or more depth, but these features improve models in different ways. For example, it's true that a Network with too few parameters won't be able to learn much; there is a minimal amount of parameters required simply to transform and represent signals in a sufficiently sofisticated way. On the other side, depth is fundamental in order to make your model generate more complex abstractions that will make it superior to other ML algorithms. In other words: the number of parameters overall tells you how many things your network can learn. Its depth tells you how complex and sofisticated these things can be. Deep Neural Network complexity is a multifaceted thing, it can't be represented on a single dimension.
