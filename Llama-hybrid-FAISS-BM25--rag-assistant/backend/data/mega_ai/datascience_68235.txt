[site]: datascience
[post_id]: 68235
[parent_id]: 
[tags]: 
Conv-2 CNN architecture - CIFAR-10

I have a CNN architecture for CIFAR-10 dataset which is as follows: Convolutions: 64, 64, pool Fully Connected Layers: 256, 256, 10 Batch size: 60 Optimizer: Adam(2e-4) Loss: Categorical Cross-Entropy When I train this model, training and testing accuracy along with loss has a very jittery behavior and does not converge properly. Is the defined architecture correct? Should I have a maxpooling layer after every convolution layer? The Conv-2 architecture is an attempted implementation of the following paper. The following table is taken from The Lottery Ticket Hypothesis By: Jonathan Frankle, Michael Carbin Thanks!
