[site]: crossvalidated
[post_id]: 178191
[parent_id]: 178184
[tags]: 
They say they increased the size of the training set by a factor of 2048. Does this mean they trained on a total of 2024 X 1.2 millions images? Yes, in the paper: The first form of data augmentation consists of generating image translations and horizontal reflections. We do this by extracting random 224x224 patches (and their horizontal reflections) from the 256x256 images and training our network on these extracted patches They are generating those extra patches 'on the fly' from the original images, said here: In our implementation, the transformed images are generated in Python code on the CPU while the GPU is training on the previous batch of images. So these data augmentation schemes are, in effect, computationally free. What do they mean they extracted ﬁve 224 × 224 patches (corner, center and horizontal)? And why does it result in ten patches in total? The original images have size 256x256, so they are getting a patch by cropping the original picture on the upper left corner with a size 224x224. Same thing for upper right, lower left, lower right and center. So that's making 5 patches. And for each of those patch they are mirroring the picture, so they get 5 more patches. Total 10, and then they take the average prediction.
