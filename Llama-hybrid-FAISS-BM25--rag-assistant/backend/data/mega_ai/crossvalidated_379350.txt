[site]: crossvalidated
[post_id]: 379350
[parent_id]: 379325
[tags]: 
In probability theory (as this has nothing specific to machine learning or statistics) Bayes' formula is based on the joint distribution of the pair of random variables $(D,\theta)$ , $p(D,\theta)$ , which can be expressed either as $$p(D,\theta)=p(D)\times p(\theta|D)$$ or as $$p(D,\theta)=p(\theta)\times p(D|\theta)$$ [with the confusion notation of using the same $p(\cdot)$ everywhere!] where $p(D)$ denotes the marginal density of $D$ [integrates to one in $D$ ] also called marginal likelihood or evidence $p(\theta)$ denotes the marginal density of $\theta$ [integrates to one in $\theta$ ] also called prior $p(D|\theta)$ denotes the conditional density of $D$ given $\theta$ [integrates to one in $D$ ] also called likelihood and often denoted $\ell(\theta)$ $p(\theta|D)$ denotes the conditional density of $\theta$ given $D$ [integrates to one in $\theta$ ] also called posterior
