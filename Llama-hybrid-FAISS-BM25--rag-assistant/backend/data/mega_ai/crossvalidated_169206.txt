[site]: crossvalidated
[post_id]: 169206
[parent_id]: 168498
[tags]: 
To use your notation let's say we want a mixture of $R$ distribution and we have $N$ data points. I will always use $i$ to iterate through mixtures and $j$ to iterate through data points. My understanding of the typical discrete mixture model, has: 1) A Multinomial Distribution with parameter $w$ from which $y = (y_1, \dots, y_N)$ will be drawn, with $y_j \in \{1, \dots, R\}$. 2) A collection of distributions (you mentioned Gamma), $f_i(x) = P(x | y_j = i, \theta_i)$, where $\theta_i$ is the parameter governing the $i^{th}$ component. We would use our data to estimate $w, y, \theta$, using something like $MLE$. To make this model Bayesian, we would add prior distributions to both $w$ (often a Dirichlet distribution) and $\theta$ (often a conjugate prior to whatever family of distributions you have chosen for the pixture components). We would then use $MAP$ (often using numerical optimization), or Bayesian Inference (often using MCMC, especially if we did not use conjugate priors) to estimate parameters.
