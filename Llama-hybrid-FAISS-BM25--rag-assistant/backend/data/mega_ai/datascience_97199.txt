[site]: datascience
[post_id]: 97199
[parent_id]: 45803
[tags]: 
In my experience, one-hot encoding helps the model to distinguish differences in data where small changes would have large outcomes in the result. Say, if it's pixel data of an image, a small change in one pixel would have little or no effect on what the image represents, so the model can easily differentiate that data without one-hot encoding it. The final layer's activation function should be chosen based on the kind of data you're trying to model. Use no activation (a.k.a. linear) and MSE loss if you just want it to predict numbers with no restriction. For example, if the model needs to be able to predict the number 1000, then let it use linear activation in the final layer. Refrain from using ReLU here if you need the model to also be able to output negative numbers. Use softmax activation with categorical cross-entropy loss if the model is intended to make a prediction on what the best "choice" is for something. This almost always takes the form of classification. Use sigmoid activation with binary cross-entropy loss if the model is to answer a bunch of "yes or no" questions about the data. For example, a model takes text as input and outputs multiple numbers between 0 and 1, and perhaps the first number indicates whether or not the text has a positive connotation, and maybe the second number indicates whether or not the text uses a wide range of vocabulary. This is what we call binary classification, and we're completely allowed to have multiple "classes" for it. The main difference between sigmoid and softmax is that softmax normalizes the sum of the output whereas sigmoid does not. So softmax makes the output behave like a probability distribution, and sigmoid makes each number in the output be independent of one another, but still behaving as though it's a probability of "yes" for that specific part of the label. Having said that, use binary classification (sigmoid activation and binary cross-entropy loss) if the labels in your data set are vectors consisting of only zeros and ones but are not restricted to being one-hot vectors. For a final note, these rules apply to almost any neural network architecture, including LSTM/GRU and convolutional neural networks.
