[site]: datascience
[post_id]: 54346
[parent_id]: 
[tags]: 
Data-generating probability distribution, probability distribution of a dataset, in ML

In Goodfellow I, Bengio Y, Courville A. Deep learning. MIT press; 2016 Nov 10. http://thuvien.thanglong.edu.vn:8081/dspace/bitstream/DHTL_123456789/4227/1/10.4-1.pdf p. 102 (for example), it is said that with Unsupervised Learning, one usually wants to ''learn the entire probability distribution that generated a dataset'', $p(\vec{x})$ . My question is that I would like to have a better interpretation/understanding of the concept, i.e., can we say that $p(\vec{x})$ , for a given an example (represented as a vector) $\vec{x}$ , in the dataset, means the ''probability to find this example Ã  priori '' in the wild... or something like that? E.g. if it is an image of a cat of breed X, e.g., $\vec{x_i}$ = ''vectorized image of this cat of breed X'' does $p(\vec{x_i})$ means the probability (we estimate given our limited dataset of course) to obtain this image of this cat of breed X, if we draw a sample from this dataset - and even if we want to generalize - the prob to obtain this cat if we sample the TEST SET. ? There is a similar question here, but the answer is far from answering anything about the question: What does it mean for the training data to be generated by a probability distribution over datasets
