[site]: datascience
[post_id]: 47494
[parent_id]: 
[tags]: 
Precision decreases with each epoch in CNN

I've built the following CNN that is used to classify a binary classification set (something a like cats or dogs): self.opt = SGD(lr=0.0001) self.model.add(Conv2D(filters=16, kernel_size=3, input_shape=(100, 100, 3), padding='same')) self.model.add(BatchNormalization()) self.model.add(Activation(self.activation)) self.model.add(MaxPooling2D(pool_size=2)) self.model.add(Conv2D(filters=32, kernel_size=3, activation=self.activation, padding='same')) self.model.add(MaxPooling2D(pool_size=2)) self.model.add(Conv2D(filters=64, kernel_size=3, activation=self.activation, padding='same')) self.model.add(MaxPooling2D(pool_size=2)) self.model.add(Conv2D(filters=128, kernel_size=3, activation=self.activation, padding='same')) self.model.add(MaxPooling2D(pool_size=2)) self.model.add(Dropout(0.5)) self.model.add(Flatten()) self.model.add(Dense(150)) self.model.add(Activation(self.activation)) self.model.add(Dropout(0.5)) self.model.add(BatchNormalization()) self.model.add(Dense(1, activation='sigmoid')) self.model.summary() self.model.compile(loss='binary_crossentropy', optimizer=self.opt, metrics=[self.precision]) Sadly, my training seems to be doing very badly as the precision is decreasing with each epoch: What are some common issues when this behaviour is noticed? Is the most likely cause that my training data is biased for one class? Right now I have 25% class 1 and 75% class 2. How much does the size of the pictures influence the performance? As of now, all pictures are size 100x100. Does increasing the size lets say to 400x400 make the CNN more capable of detecting the features? Additional code: Loading the Images: def convert_image_to_array(files,relpath): images_as_array=[] len_files = len(files) i = 0 print("---ConvImg2Arr---") print("---STARTING---") for file in files: # Convert to Numpy Array images_as_array.append(img_to_array(load_img(relpath+file, target_size=(soll_img_shape, soll_img_shape)))/255) if i == int(len_files*0.2): print("20% done") if i == int(len_files*0.5): print("50% done") if i == int(len_files*0.8): print("80% done") i +=1 print("---DONE---") return images_as_array from sklearn.model_selection import train_test_split from keras.preprocessing import image X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) cnn = nn.NeuralNetwork() cnn.compile() ##### Load Images to train, test and validate x_train = np.array(convert_image_to_array(X_train,"images/processed/")) x_test = np.array(convert_image_to_array(X_test,"images/processed/")) Also the prediction metric: def precision(self, y_true, y_pred): '''Calculates the precision, a metric for multi-label classification of how many selected items are relevant. ''' true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1))) precision = true_positives / (predicted_positives + K.epsilon()) return (precision)
