[site]: datascience
[post_id]: 122254
[parent_id]: 
[tags]: 
How to implement a custom loss with a non-mathematical operation (simulation) that backpropagates with PyTorch?

I am writing a Neural Network, which output is not used directly for the loss-function, but rather as the input for a simulation model. After the simulation ran, I am using the simulated_value and the real_value (target) to define pretty much a MSE-Loss. In pseudo code my loss looks something like this: def simulation_loss(sim_params=nn_model_output, batch=batch): sim_value = RUN_SIMULATION(sim_params) sim_value.requires_grad = True real_value = batch['target'] loss = nn.MSE(input=sim_value, target=real_value) return loss It seems, that because I am cutting off the original gradient from nn_model_output , NN-model does not converge. How can I run this sort of loss-function including a simulation model? Do I need to define a custom torch.autograd.Function with a backwards call? And if so, how should I pass the gradient, if there is no derivative from the simulation model possible? Or should I treat the simulation as an activation-function?
