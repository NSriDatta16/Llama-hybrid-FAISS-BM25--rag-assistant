[site]: crossvalidated
[post_id]: 287619
[parent_id]: 287494
[tags]: 
You should not consider Cronbach's alpha along with exploratory Factor analysis. Alpha is from the domain of reliability , specifically, one of measures of item-item homogeneity (interchangeability) within a scale; or more specifically about alpha - how much sufficient a scale is supplied with items. Factor analysis is done to uncover latent traits and to validate a scale which pretend to be based on one factor. See reliability vs validity views. According to classic reliability viewpoint, questionnaire items (which scores are summed or averaged to give the overall scale score) are seen as the repeated measurements of something one and same. Consider just one respondent and a scale comprised of two items. His responses are modelled as: Item1 = T + e' and Item2 = T + e'' T stands for the "true score" of the scale. We do not claim that the true score is the value of some latent trait of factor, though we might. There could be a mixture of factors actually behind the true score; the question what is behind it is beyond the scope of reliability. Suffice to think there is something "true" that the questionnaire measures. The fact that T is the same symbol in both equations says that it is the same value in both, characterizing our concrete respondent's posession (or view) of what is being measured (we'll assume for simplicity that both items have equal so called "difficulty level"). The fact that T is capital letter serves to convey to us that within the respondent level it is a fixed quantity, not a random value which might vary. e stands for "measurement error". It could be conceptualized as momentary fault occuring by chance while the respondent is making an answer. Items do differ by wording, however that respondent's reaction to an item peculiarity is considered by the model exactly like his unexpected sneeze to put out the pen one point away. It is single random variable within the respondent (the letter is lowercase) with two happened realizations, values e' and e'' . Like most noise, they are assumed to come from normal distribution with zero mean and some variance. That variance is the measure of an individual item's unreliability. We'll assume for simplicity that variances are equal for both items, though they could differ. Now to the essence of the question - why reliability (in the form of Cronbach's at least) may grow just simply as we add more items (as was shown in the answer by @T.E.G.). Because T (on respondent level) is constant and items are repeated measurement attempts of the same thing, varying only by the unbiased (symmetric) noise e , the more attempts (items) 1,2,3,... we use the closer the sum e'+e''+e'''+... is to zero, their expectation, and so the closer the mean of items to T . Cancelling out of noise effect on the mean as tries accumulate. If there were no error terms (no "sneezing" effects, so to speak) then all correlations would be complete (r=1) and alpha'd be 1. It is just due to the error assumed independent from item to item that mean r come below 1 - since the simplest reliability model declares no other source of variation besides true score T and "sneezing" error e . With only 2 items, the sneeze shifts won't probably cancel each other but with many items they will. Thus, alpha grows as errors are smaller (hence correlations are bigger) and/or correlating items are more plenty . Classic Factor analysis model is a bit more complex than the classic scale reliability model. Let us have just one-factor model: Item1 = F + S1 + e' and Item2 = F + S2 + e'' F is the latent common factor which our scale (consisting of items) aims to measure. S1 and S2 are the specific factors belonging to item1 and item2, respectively. e is the random error term like the one described earlier. Specific factors are not known or measured but they are real, because it is realistic to suppose that each item contains something common with other items (and what makes them to correlate) and something individual (what hinders them to correlate completely). That individual parts S1 and S2 are systematic (i.e. fixed on a given respondent level), like factor F is, and they are two different values. Thus the "true score" of the scale T1=F+S1 and T2=F+S2 is not same magnitude in different items. Even though as you add more and more items (with individual S 's) error term sum e'+e''+e'''+... will converge towards zero, it won't help stabilize on the unbiased F (our aim) as something what the scale (sum or mean of items) reflects, because we don't know S1+S2+S3... sum of values within a respondent. Adding items to a scale (construct) does not necessarily make the scale more valid as a measure of the latent factor it was intended to measure, in spite that the scale tends to grow in reliability to measure "whatever it measures", something one. Reliability testing, to repeat, assumes that items are equivalent and their scores are just randompy noised, so adding the sufficient number of items (few, if possible, they are to correlate better, then) recovers true, unbiased score of the scale for every respondent. Factor analysis do not pose that items are equivalent, they contain true factor inside plus item-specific biases away from it, along with random noise; so summing up items (even quite well correlated due to the same factor) does not guarantee the unbiased estimate of the factor value for every respondent. Moreover, in factor analysis as it is computationally we actually do not consider measurement errors e separately at all. That term is seen immersed into the term called "unique factor" U : U1=S1+e' and U2=S2+e'' , which complements the common F in the model . Factor extraction usually will aim to maximize F variance, thus minimize on the overall U s' variances. That implies that error e s' variances are also constrained, "preselected". That makes Factor analysis barely comparable with Reliability analysis. There, however, exist Alpha factor analysis which is more kindred to Reliability analysis with its Cronbach.
