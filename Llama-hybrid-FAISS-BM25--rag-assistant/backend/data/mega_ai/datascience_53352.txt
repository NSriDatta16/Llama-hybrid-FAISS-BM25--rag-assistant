[site]: datascience
[post_id]: 53352
[parent_id]: 
[tags]: 
Residual Neural Network with 2D input data in tensorflow

I have a simple feedforward network to approximate a function $f(x,y)$ . I'm interested in adding skip-connections to my network (i.e. ResNet). In 1D, it's fairly straightforward. Here's a code snippet for a single block: def create_model(X, weights, biases, num_layers, num_hidden): # first layer W = weights[0] b = biases[0] H = tf.nn.tanh(tf.add(tf.matmul(X, W), b)) # interior hidden layers for l in range(1,num_layers-2): W = weights[l] b = biases[l] H = tf.tanh(tf.add(tf.matmul(H, W), b)) # residual connection W = weights[num_layers-2] b = biases[num_layers-2] H = tf.add(tf.tanh(tf.add(tf.matmul(H, W), b)), X) # output (activation) layer, linear W = weights[-1] b = biases[-1] Y = tf.add(tf.matmul(H, W), b) return Y The above code snippet runs due to broadcasting. In particular, we have $$ \sigma(\sigma(\mathbf{x}W_{1} + b_{1})W_{2} + b_{2}) + \mathbf{x} $$ for a block with two hidden layers and a skip connection. Let $n$ be the number of neurons and $W_{1} \in \mathbb{R}^{1\times n}$ , $W_{2} \in \mathbb{R}^{n\times n}$ , $b_{1}, b_{2} \in \mathbb{R}^{1 \times n}$ . Suppose $\mathbf{x} \in \mathbb{R}^{m \times 1}$ (so $m$ is the length of the input X ). We have the first term $\sigma(\sigma(\mathbf{x}W_{1} + b_{1})W_{2} + b_{2})$ is in $\mathbb{R}^{m \times n}$ , and tensorflow allows broadcasting so that $\mathbf{x}$ is added to each column of the first term. Is this how the skip connection is supposed to work? I'm interested in extending this to two-dimensional functions, again only with a single block for now: # initialize weights def initialize_NN(num_layers, num_hidden): weights = [] biases = [] # first layer, based on input data W = init_weights([2, num_hidden], 'xavier', xavier_params=(1, num_hidden)) b = init_weights([1, num_hidden], 'zeros') weights.append(W) biases.append(b) # interior hidden layers for l in range(1,num_layers-1): W = init_weights([num_hidden, num_hidden], 'xavier', xavier_params=(num_hidden, num_hidden)) b = init_weights([1, num_hidden], 'zeros') weights.append(W) biases.append(b) # output (activation) layer W = init_weights([num_hidden, 1], 'xavier', xavier_params=(num_hidden, 1)) b = init_weights([1, 1], 'zeros') weights.append(W) biases.append(b) return weights, biases # setup model def create_model(X, Y, weights, biases, num_layers, num_hidden): DATA = tf.concat((X, Y), axis=1) # first layer W = weights[0] b = biases[0] U = tf.nn.tanh(tf.add(tf.matmul(DATA, W), b)) # interior hidden layers for l in range(1,num_layers-2): W = weights[l] b = biases[l] U = tf.tanh(tf.add(tf.matmul(U, W), b)) # residual connection W = weights[num_layers-2] b = biases[num_layers-2] U = tf.add(tf.tanh(tf.add(tf.matmul(U, W), b)), DATA) # output (activation) layer, linear W = weights[-1] b = biases[-1] U = tf.add(tf.matmul(U, W), b) return U Of course, I get an error with the shapes now since DATA is two-dimensional. The error I get is ValueError: Dimensions must be equal, but are 100 and 2 for 'Add_3' (op: 'Add') with input shapes: [?,100], [?,2]. in this line: U = tf.add(tf.tanh(tf.add(tf.matmul(U, W), b)), DATA) I've read that I should multiply by some appropriate transformation $W_{s}$ , but I'm not sure how to determine this mapping. If I tried something like U = tf.add(tf.tanh(tf.add(tf.matmul(U, W), b)), tf.matmul(DATA, tf.identity(weights[0])))) then it certainly fixes the shape issue, but I'm not sure if this is correct. Can someone explain to me or suggest a good tutorial how I can rectify this problem?
