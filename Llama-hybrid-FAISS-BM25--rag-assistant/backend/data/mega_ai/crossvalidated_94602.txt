[site]: crossvalidated
[post_id]: 94602
[parent_id]: 94596
[tags]: 
In order of increasing on-line complexity: Linear SVM has prediction complexity $O(d)$ with $d$ the number of input dimensions since it is just a single inner product. NN complexity is related to the architecture, but will surely be above that of linear SVM. Prediction complexity of kernel SVM depends on the choice of kernel and is typically proportional to the number of support vectors. For most kernels, including polynomial and RBF, this is $O(n_{SV} d)$ where $n_{SV}$ is the number of support vectors. An approximation exists for SVMs with an RBF kernel that reduces the complexity to $O(d^2)$ . For computer vision applications, additive kernels are often used because they yield very fast prediction speed (independent of the number of SVs). Since you are doing a computer vision application, I recommend using either the RBF approximation or additive kernels as they are very fast in evaluation and among the state-of-the-art in terms of accuracy. Neural networks that can rival the performance of SVMs using these kernels in CV applications are typically quite large, and thus slower.
