[site]: crossvalidated
[post_id]: 244060
[parent_id]: 243120
[tags]: 
I would frame the problem this way: $p(M_j|y_{1:T}) = \frac{\big(\int{p(y_{1:T}|\theta,M_j)p(\theta|M_j)d\theta}\big)p(M_j)}{\sum_{l=1}^2\big(\int{p(y_{1:T}|\theta,M_l)p(\theta|M_l)d\theta} \big) p(M_l)}$ where $\theta=(\theta_1,\dots,\theta_d)$ is the vector of random parameters on which your models depend. Actually, $d$ may be different for different models, but since you have only two models, I'll keep the notation simple. If you have $d_1\neq d_2$, then in the following just consider $d=\max{(d_1,d_2)}$. From the above formula, it's obvious that you need two "ingredients": you need prior probabilities for the models. You assumed that $p(M_1)=p(M_2)=0.5$, so that's already taken care of, but it's good to remember that your results are conditional on this assumption. you need the marginal likelihoods for each model, or equivalently the Bayes factors. That is to say, for each model $M_j$ you need to integrate the likelihood $p(y_{1:T}|\theta,M_j)$ with respect to $p(\theta|M_j)$, which is the prior distribution of $\theta$ for model $M_j$. Now, in a discrete case, where $\theta$ can assume only one possible value under model $M_j$, this step is easy. For example, this is often the case for the model derived under the null, in Bayesian hypothesis testing. But in the continuous case, this becomes a bit more complicated. If $p(y_{1:T}|\theta,M_j)$ and $p(\theta|M_j)$ are a conjugate pair, then this is easy, thus I assume they aren't. I can think of 3 options: if $T\gg d$, then the likelihood will be extremely peaked around the MLE of $\theta$. In this case, the Laplace approximation should work well. Another large sample approximation is the one based on the BIC. However, I don't know a lot about it: I only used it for linear regression models. Since you're talking about forecasts, I guess you're doing time series modeling. I know that BIC is used also for time series modeling. I would at least cross-check with another method. since you have an explicit expression for $p(\theta|M_j)$, you can sample from it. This means that you can compute $\int{p(y_{1:T}|\theta,M_j)p(\theta|M_j)d\theta}$ by Monte Carlo integration ( unless of course you chose a Cauchy prior). Since likelihood functions are nasty beasts, usually this will converge slowly, because most of the MC samples $\theta_i$ will correspond to very small likelihood values. Importance sampling will improve upon that. if $d$ is small (say, less than 9) numerical integration may work very well. Sparse grid Gaussian quadrature or adaptive Gaussian quadrature will do the trick. If $d$ is very small (for example, less than 4 or 5), even tensor grid Gaussian quadrature could work.
