[site]: stackoverflow
[post_id]: 4681598
[parent_id]: 4681526
[tags]: 
There are many things to consider in desigining a "better" hash table. One of the reasons that the custom approaches you tried were slower or no better than the .NET Dictionary is that very often the performance of a hash table is very dependant on: The data being hashed The performance of the hash function The load factor of the table The number of collisions vs non-collisions The algorithm for collision resolution The amount of data in the table and how it's stored (by pointer/reference or directly within the buckets) The access patterns to the data The number of insertions/deletions vs retrievals The need for resizing in a closed hashing/open addressing implementation and many other factors... With so many things to tweak and tune, it is difficult, without a significant amount of effort to come up with a general high performance (time and speed) hash table. That is why, if you are going to try to create a custom hash table instead of one built into a standard library (such as .NET) be ready to spend countless hours and be aware that your finely tuned implementation may be only tuned for the specific type and amount of data you are hashing. Therefore, no, the .NET Dictionary is not the ultimate hash table for any specific purpose. But, given the frequency of dictionary use, I am sure that the Microsoft BCL (Base Class Library) team performed a huge amount of profiling to choose the approach they chose for the general case.
