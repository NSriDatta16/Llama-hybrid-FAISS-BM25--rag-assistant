[site]: crossvalidated
[post_id]: 358284
[parent_id]: 261227
[tags]: 
I was wondering the same; here are my ideas: I suppose that if the outputs share some hidden patterns, then training can benefit from simultaneously learning the regression for all the outputs. It would be interesting to try an architecture where you build a neural network for each output, but all the neural networks share some layers (the first half layers for example). Then you could train each neural network at the same time: inside the learning loop, each neural network is trained one step (with one batch) sequentially. This would be similar to knowledge transfer, but with the difference that in knowledge transfer each neural network is fully trained before reusing some part of it to train another neural network. I bet someone has thought about this before, but I have no reference to it.
