[site]: datascience
[post_id]: 93660
[parent_id]: 93368
[tags]: 
The activation functions are used in hidden layers and output layer. The output layer will usually have sigmoid or softmax activation while hidden layers usually use ReLU. ReLU is available in Orange at least (there is a dropdown which you can explore but ReLU works best mostly, read this ). Also, check this link for looking at neural networks in Orange.
