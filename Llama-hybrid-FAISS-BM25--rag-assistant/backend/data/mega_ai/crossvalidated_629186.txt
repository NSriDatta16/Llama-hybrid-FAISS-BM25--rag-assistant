[site]: crossvalidated
[post_id]: 629186
[parent_id]: 
[tags]: 
Finding a condition that maximizes the conditional correlation between two random variables

Letâ€™s say I have $n$ independent and identically distributed samples that is, $(X_1,Y_1)$ , $(X_2,Y_2)$ , ... $(X_n,Y_n)$ which is my dataset. I want to find a condition on $X$ such that the conditional correlation coefficient (say Pearson's correlation) is maximized. Formally, I want to find a function $f: \mathbb{R}^m \rightarrow \{0,1\}$ such that $corr(X,Y|f(X)=1)$ is maximized, where $X$ and $Y$ are two random variables, and $X$ takes values in $\mathbb{R}^m$ . One thing possible is to assume a parametrized form for $f$ , say a simple linear model or possibly a neural network, then somehow obtain an expression for the correlation using the samples in terms of the parameters of $f$ and then optimize for those parameters using something like gradient descent. The problem is how do I obtain the parametric expression for the conditional correlation? Is there a simpler way to approach this problem?
