[site]: crossvalidated
[post_id]: 477645
[parent_id]: 477601
[tags]: 
Feature selection is a statistical search problem. Several strategies have been developed over the years: Sequential forward search - find the most discriminative single feature, build a classifier from that, find the next feature that performs best (on a training set ) in combination with the earlier selected single feature, add the most suited 3'rd feature, and so forth; Sequential backward search - build a classifier from all $n$ features, try removing each feature, and build each possible classifier from $n-1$ features, choose the best performing classifier (on a training set ) - and its feature subset, and iterate again by removing the least important feature from that classifier, and so forth; Branch-and-bound search - an algorithmic approach to narrow in on the optimal feature subset, taking some variation into account Floating search - switch between sequential forward search and sequential backward search, according to a specialized algorithmic scheme; Markov Chain Monte Carlo approaches - construct a Markov chain where features are added and removed from the classifier in the current Markov state (Metropolis Hastings algorithm); The 'sequential' methods are not guaranteed to provide the optimal (sub)set of features because overfitting to noisy features often occurs. This situation is called Peaking . Floating search and MCMC approaches use heuristics and stochastics to perform a directed search for the optimal subset of feature variables. It seems like you are doing sequential forward search , as appears from your question. You can start off by the sequential methods and switch to the more advanced approaches if there are clear signs of a performance peak, as estimated from an independent test set.
