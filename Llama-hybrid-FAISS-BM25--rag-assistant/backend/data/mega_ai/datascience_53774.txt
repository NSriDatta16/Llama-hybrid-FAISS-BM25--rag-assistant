[site]: datascience
[post_id]: 53774
[parent_id]: 53395
[tags]: 
I got a response from one of the authors: A bit more intuitively, think of the neural network being asked to produce a scaled variant of the q-function, namely $P(s,a) = h(Q(s,a))$ The Bellman equation reads: $$Q(s,a) = R + \gamma * Q(s', a')$$ and by applying function $h$ to both sides you get, equivalently, $$P(s,a) = h(Q(s,a)) $$ $$= h ( R + \gamma * Q(s', a') ) $$ $$= h ( R + \gamma * h^{-1}( P (s', a') ) )$$ So hopefully this makes it clear that the target for the network's output at (s,a) should be $$h ( R + \gamma * h^{-1} ( P (s', a') ) )$$ where $P (s', a')$ is the same network's output at the next state. From my original post: "these Q is a still big source of "exploding rewards" threat ... these destination Q are free to grow as large as they wish." With the author's help I realised that they are actually not "entirely free to grow" in the way I though originally: We should not forget that these "destination scores" were downscaled themselves - as the yellow-quote formula tells, they will be $h(P(s',a'))$ themselves. So we need to "unpack" them before we can add the currently observed reward. Thus, we won't have exploding rewards in the long run, and we also don't have to truncate any reward-ratios by using the old-school reward clipping ...That's a very clever way to do it! :) The yellow-quote formula tells us to: quickly restore (un-shrink) the downscaled destination $h(P)$ , apply discount and reward, then downscale it back. It now can be the "downscaled destination", for any states earlier in time. Stepping back in time, we begin from 1) and process earlier state in the same fashion. The goal is to make the network learn to predict these "downscaled destinations", and it's a lot nicer - because these destination scores are smaller due to being downscaled. There will be no huge gradient entering the network, since the differences are a lot smaller. P.S. Don't forget that $h^{-1}$ is NOT $\frac{1}{h}$ Instead, google for what an inverse function is.
