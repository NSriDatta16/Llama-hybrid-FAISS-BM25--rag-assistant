[site]: crossvalidated
[post_id]: 473835
[parent_id]: 370179
[tags]: 
As @today pointed out, loss value doesn't have to be 0 when the solution is optimal, it is enough that it is minimal. One thing I would like to add is why one would prefer binary crossentropy over MSE . Normally, the activation function of the last layer is sigmoid, which can lead to loss saturation ("plateau"). This saturation could prevent gradient-based learning algorithms from making progress. In order to avoid it, it is then good to have a log in the objective function to undo the exp in sigmoid , and this is why binary crossentropy is preferred (because it uses log , unlike MSE). I have read this in the Deep Learning Book , but I now can't find where exactly (I think chapter 8).
