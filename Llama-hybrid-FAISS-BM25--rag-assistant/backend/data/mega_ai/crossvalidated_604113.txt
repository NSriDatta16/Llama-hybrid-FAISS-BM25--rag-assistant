[site]: crossvalidated
[post_id]: 604113
[parent_id]: 
[tags]: 
Weights Update - Ensemble Models

I must identify if a data point is an outlier or not in a dataset (we don't have labels). I have different unsupervised models to identify the outlier. Then, I normalize the outlier score and I combine them via a weight average. According to the fact that I don't have information about their accuracy I use the same weight for each models. Now, suppose that I have a small fraction of the dataset with also the label. How can I update the weights according to the new information? This is what I think that could work, but I don't have reference to say that for sure. Imagine that I have 4 models. The probability of the model I assume that is the weight, so 0.25. $$ P(Model) = 0.25 $$ Then, the likelihood: $$ P(Outlier | Model) $$ that are the normalized outlier score. And so, the posterior is: $$ P(Model | Outlier) = P(Outlier | Model) P(Model)$$ My question is: can I sum all the different posterior probability for each observation and then normalize w.r.t. the sum of the other models? I give you an example in python of my thinking: import numpy as np def bayesian_update(anomaly, weight, prob): #inizialization vector posterior = np.zeros(len(anomaly)) for i in range(len(anomaly)): #if the labled indicate an anomaly if anomaly[i] == 1: posterior[i] = prob[i] * weight #if the labled indicate a non anomaly else: posterior[i] = (1-prob[i]) * weight return posterior np.random.seed(0) n_observations = 100 n_models = 4 # models_probs = np.random.rand(n_observations, n_models) #a way to indicate that the first model (colomn) is better anomaly = np.where(models_probs[:, 0] > 0.5, 1, 0) posterior_sum = np.zeros(n_models) for i in range(n_models): posterior_sum[i] = np.sum(bayesian_update(anomaly, 0.25, models_probs[:, i])) new_weight = posterior_sum/np.sum(posterior_sum) print(new_weight) ```
