[site]: stackoverflow
[post_id]: 4072216
[parent_id]: 4071258
[tags]: 
I'm going to assume you are using WebSockets or Socket.IO since you are implementing a game where latency matters (and you tagged it as such). I would think the server should probably measure and keep track of this for each client. You probably want to implement some sort of ping action that the server can request of the client. As soon as the client receives the request, it sends back a response to the server. The server then divides by 2 and updates the latency for that client. You probably want the server to do this periodically with each client and probably average the last several so that you don't get strange behavior from sudden but temporary spikes. Then, when there is a message from one client that needs to be sent (or broadcast) to another client, the server can add client1's latency to client2's latency and communicate this as the latency offset to client2 as part of the message. client2 will then know that the event on client1 happened that many milliseconds ago. An additional reason to do this on the server is that some browser Javascript timestamps are inaccurate: http://ejohn.org/blog/accuracy-of-javascript-time/ . I suspect node.js timestamps are just as accurate (or more so) than V8 (which is one of the few accurate ones).
