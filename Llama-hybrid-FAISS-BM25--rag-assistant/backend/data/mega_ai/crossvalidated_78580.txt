[site]: crossvalidated
[post_id]: 78580
[parent_id]: 78532
[tags]: 
the point is to pick a value of the calibration parameter that gives optimum model correlation In that case, the question is about model optimization, which is related to, but usually not considered the same, as data dredging. Fortunately, there are "recipes" that you can follow to avoid the problems associated with the dredging. The big problem with both model selection/optimization and data dredging is that people forget to validate their final model with independent test data. The problem is not primarily that lots of models are tested. IMHO you can try around as much as you like, as long as the final model comes with an independent test. Incidentally, the difference between independent test results and "internal"/optimization results gives you an indication of how much overfitting you have. Thus, in oder to avoid the data dredging/optimization problems, you need to do two levels of testing. The inner test data is used for model selection/optimization, and once that is finished, you do the outer testing with data that did in no way contribute to the model. This includes that splitting occurs at the uppermost level of the data hierarchy and the set-aside test data does not contribute to data-driven pre-processing (centering, scaling, PCA projection, ...) Keywords to look for would be nested/double validation. A situation that occurs frequently in this context is that aggressive optimization overfits, and you end up with many models that seem perfect. In that situation the optimization/selection will not know which model to choose as they all seem similarly perfect. If then you observe a huge drop in performance estimate with the independent (outer) error estimation, you may be better off not optimizing at all but instead using your expert knowledge about the problem to fix the parameter in question. I write this here as your network radius in m probably has a "physical" interpretation, and you may know what order of magnitude would be sensible. You should not be afraid of using this expert knowledge to fix the parameter and thus avoid all problems of data dredging! In any case, I'd point out that the choice of the radius parameter is a "benign" problem, as you observe a rather broad plateau from 500 - 1000 m. Within this interval, nothing happens, which also means that you do not need to worry whether 605 m is better than 607 m. This is an important information, because for future data you may fix the radius, and argue that with that you avoid all possible data dredging/optimization problems and that this is sensible because you found this wide plateau in the previous study. Note however, that looking for the maximum now and finding the plateau, you cannot any longer argue that you fixed the parameter beforehand! As you did this, you need to do the independent validation now. Note that in calibration with models with low complexity and many independent data points (classical calibration as in @Eupraxis1981's answer), usually lack of fit is responsible for the error. That is, if at all, the model is underfit, but for sure not overfit. In that situation you may get away with resubstitution error (i.e. use the goodness of fit with your training data as approximation for generalization error).
