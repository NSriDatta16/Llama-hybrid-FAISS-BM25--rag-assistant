[site]: crossvalidated
[post_id]: 596296
[parent_id]: 596109
[tags]: 
There is no theory that two regressions $\operatorname{E}(Y|X) = g^{-1}\left(\beta_0 + \beta_1x_1\right)$ and $\operatorname{E}(Y|X) = g^{-1}\left(\beta_0 + \beta_1x_1 + \beta_2x_2\right)$ should make the same/similar predictions when you plug in $x_2 = 0$ . This would be the case only if the estimates for $\beta_0$ and $\beta_1$ are the same/close. The fact that your model is a logistic (and not, say, linear) regression and that you have more than two variables is irrelevant. It might help to review how we interpret regression coefficients: $\beta_0$ and $\beta_1$ have a different meaning in the two models, so we can't expect them to have the same value in general. The interpretation is easier in the case of linear regression, so let's assume the link function $g$ is the identity. For illustration purposes, I'll also assume that the variable $X_1$ is gender ( $x_1 = 1$ indicates males) and the variable $X_2$ is IQ standardized so that $x_2 = 0$ corresponds to the average IQ in the study population. In the linear regression $\operatorname{E}(Y|X) = \beta_0 + \beta_1x_1$ , the intercept $\beta_0$ is the expected value of Y for females and the coefficient $\beta_1$ is the expected difference in Y between males and females. In the linear regression $\operatorname{E}(Y|X) = \beta_0 + \beta_1x_1 + \beta_2x_2$ , the intercept $\beta_0$ is the expected Y for females of average IQ and the coefficient $\beta_1$ is the expected difference in Y between males and females of average IQ. So the meaning of $\beta_0$ and $\beta_1$ is different and generally we cannot expect the two models to make the same predictions, most obviously in the case when $X_2$ is predictive of the outcome $Y$ . (To make the example more concrete, consider what the difference between the two regressions might be if $Y$ is test scores vs if $Y$ is height.)
