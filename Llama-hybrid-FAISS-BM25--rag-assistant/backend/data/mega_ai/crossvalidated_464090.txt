[site]: crossvalidated
[post_id]: 464090
[parent_id]: 
[tags]: 
Combine ReLU with TanH is a good idea?

I have a CNN implementation for the Generator of a GAN, internally, the architecture is using ReLU for non-linearities, but at the output, the paper of the architecture specifies Tanh must be used. The paper doesn't specify if ReLU must be used on the internal layers or not. But I'm concerned that the Relu is basically dropping out the negative side and when it reaches Tanh, it is lowering the quality of the Data, maybe I'm wrong, but can't find online anything concrete, specific to this question. This is the paper: https://arxiv.org/pdf/1908.03826.pdf Thanks in advance! PD: what I mean with "lowering the quality" is that, with Relu, the network adapts internally to work with values > 0.0, but when the flow reaches the output, it must suddenly adapt to a new range -1.0 - 1.0, my insight is that this can be disruptive, maybe I'm wrong PD2: As an extra, the discriminator receives these normalized images in the range -1.0, 1.0 but it uses LeakyReLU for the convolutions, is that something viable?
