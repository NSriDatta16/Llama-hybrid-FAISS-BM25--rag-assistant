[site]: crossvalidated
[post_id]: 580121
[parent_id]: 579980
[tags]: 
As far as I know the problem of train/validate/test has received more attention in the "classic" ML realm than in the time series realm. Some thoughts: You almost always need to do "time series cross validation" to get reliable error estimates for time series. Problem: most of the time you don't have enough data to do a proper backtest for validation & test on different data slices. You will easily backtest overfit without realising it. Marco Lopez de Prado published good stuff about the topic (but mostly specific to the finance domain). Your third option will choose a good model, but the error estimate on the probably small test most likely will be unrealistic. Depending on your data and models you might get better results the other way around. Hypertuning parameters often can be done with relatively few data from a small validation set and then you can estimate the resulting error with a full backtest. From my experience you definitely don't think too complicated. These issues are very real. If a correct validation scheme exists, it is probably impractical. You have to make trade-offs and these depend on your data, models and your tuning needs.
