[site]: datascience
[post_id]: 79672
[parent_id]: 79660
[tags]: 
No, you can definitely search for k-NN with more than 2-dimension data. Here is an example based on sklearn : X = [[0, 0, 0], [3, 3, 3], [1, 2, 3]] from sklearn.neighbors import NearestNeighbors neigh = NearestNeighbors(n_neighbors=2) neigh.fit(X) print( neigh.kneighbors([[2,2,2]]) ) PCA is used to reduce the input dimensionality but this is not mandatory before searching k nearest neighbors (it is often used in tutorials so the data could be visualized on a 2-d plot). One thing to know/understand about k-NN is that if you plan to use it for classification, it will handle features with a lot of information the same way as the features with no information (if you normalize them). PCA could be used to handle this problem (but this is not the only way and would not always work, but I think this is another question :) ).
