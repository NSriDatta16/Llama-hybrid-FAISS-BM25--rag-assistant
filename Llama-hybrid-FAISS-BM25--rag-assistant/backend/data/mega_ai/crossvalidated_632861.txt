[site]: crossvalidated
[post_id]: 632861
[parent_id]: 
[tags]: 
Loan Data: Bucket recoveries 1-D array

Some context: When someone defaults on their loan, we keep track of the recoveries as a percentage of the exposure (loan amount), we have a limited time T (legally) to collect recoveries, those recoveries are accumulated through time, usually they only go up, but sometimes they go down as well in some specific cases. Recoveries should be between 0 and 1, although there are outliers. Using historical data on closed cases (meaning we are passed the limited time T, I am tasked to get estimates called factors, those factors are calculated as follows: We then calculate those factors for every case (loan) we have, and average them, we use them to infer recoveries with this formula: The second formula gives us a way to infer the recoveries at T given the recoveries at t and the calculated averaged factor, we use this formula to infer the recoveries at T for open cases (we are still in the legal process, so we still expect recoveries). My problem: if the recoveries go down, which happens quite often, and the recoveries at t were close to one, we get a very high factor value, which skews the average and leads to absurd values for the recoveries at T. I illustrated the problem in the following XL screenshot. The table below containing the inferred values that don't make any sense. Of course this is just an example and I have a lot more data to work with, but I'm forced into this methodology, and if it doesn't work, I must find a way to bucket my recoveries before computing the average multiplier in order to get more realistic inferred values. One naive way is to split between "high" and "low" recoveries, above and below the average recoveries at t for all our loans, and treat those groups separately with different average factors, do a t-test to check if the means between groups are significantly different statistically and go with it, but it isn't granular enough, so you must keep splitting, and it does seem a bit arbitrary to me. I tried K means, but I end up with clusters with one observation (outliers) which makes sense since I am using squared Euclidian distance and my data is dirty and I am not allowed to do anything to it. If I were to do it by "eye", I would put the values close to 1 in a specific bucket (but where should I cut it?), the values close to 0 in a specific bucket as well, and the values inbetween in a third one. But again, I am guessing at this point.
