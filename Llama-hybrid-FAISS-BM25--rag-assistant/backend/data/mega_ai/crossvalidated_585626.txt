[site]: crossvalidated
[post_id]: 585626
[parent_id]: 
[tags]: 
Bayesian (Stan) advantage for estimating small variance components in a multilevel model

Imagine you have some data with one continuous predictor x, a categorical predictor A, and some dependent variable y. You have some amount (enough) data for several subjects S across all conditions. You fit a model: y ~ x * A + (x*A|S) using lme4 and brms (using Stan) in R. The lme4 model returns the following error " boundary (singular) fit: see help('isSingular') " meaning the model had problems estimating very small variance components. The Bayesian model will not usually have problems fitting the same model and Gelman suggests keeping 'random effects' with small variances in our models so that we can say that they have a small variance. My question is: What is it specifically about Stan that allows it to fit models with small variance components with fewer problems than lme4? I'm not saying estimating these components is not more difficult or does not cause problems, I just mean that small variance components are not generally considered problematic for a Bayesian model fit using Stan. Is it: The Hamiltonian Monte Carlo moves around in such a way that boundaries (e.g. 0) don't cause as much of a problem? Priors somehow help the posterior estimate these small values? Random sampling itself offers an advantage over trying to find the single 'best' value? Some combination of the three? Something else?
