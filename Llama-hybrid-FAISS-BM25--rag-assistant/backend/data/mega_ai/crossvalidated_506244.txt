[site]: crossvalidated
[post_id]: 506244
[parent_id]: 506236
[tags]: 
You're evidently trying to check the calibration of the medical score against real obsrvations, which is a good idea. There are a couple of problems with the specifics of your approach, however. First, by using the average predicted and average actual survival over all your patients, you're ignoring all the patient-specific information. (I'm assuming that the medical-score prediction values differ among patients.) Second, even if you were to compare the overall averages, the t-test isn't usually the best test for comparing proportions, as the assumption of a normal distribution of values is unlikely to hold. (That said, if your survival probabilities are on the order of 50-75%, the error in using a t-test might not be so big; the problems are largest near the extremes of probability.) There are a few ways you could try to improve your efforts. One classic measure of the quality of a probabilistic model that takes into account individual cases is the Brier score , which is similar to the mean-square error in linear regression. Here, you could calculate for each patient the squared difference between predicted survival probability and actual survival (1 for alive, 0 for dead), then take the average squared difference over all patients, at each of 1 year and 5 years. You will have to be careful, however, if some of your patients are still alive but haven't yet reached either of the 1-year of 5-year levels. It's easy to make a mistake and code a patient as "alive" at 5 years even if she's only been followed out to 3 years. A small Brier score indicates a good model. As a further step, you could plot the medical-score predictions against the observed survival to get a calibration curve. With 190 patients, you might put them in order of predicted survival probabilities and then break them down into 10 groups of increasing predicted-survival order. Then do the averages that you proposed in your question within each of those 10 groups and plot the predicted against the observed survival probabilities for the 10 groups. Do that for each of 1-year and 5-year survival. If the model is well calibrated, a line through the points on the plot should be close to the line of identity. Such a plot can illustrate particular probability-prediction regions in which the model has problems. These tasks are implemented, for example, in the val.prob() function in the R rms package , along with other ways to evaluate probabilistic model predictions against observed events. See this answer for more details.
