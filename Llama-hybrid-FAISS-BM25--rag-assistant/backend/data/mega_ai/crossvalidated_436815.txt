[site]: crossvalidated
[post_id]: 436815
[parent_id]: 436802
[tags]: 
In batch training, applications may have slight differences but you sum (or average) the individual gradients calculated for each sample $X_i$ . The $x_i$ in the equation for $\delta_j$ denotes the $i$ -th entry of the layer's input vector. It's not associated with any specific batch sample. In batch training, you sum the update amounts (or the gradients) calculated for each sample in the batch i.e. $\Delta w_{ij}=\sum_m{\Delta w_{ij}(X_m)}$
