[site]: stackoverflow
[post_id]: 5135277
[parent_id]: 5135155
[tags]: 
This question is a great place to start: Detecting 'stealth' web-crawlers Original post: This would take a bit to engineer a solution. I can think of three things to look for right off the bat: One, the user agent. If the spider is google or bing or anything else it will identify it's self. Two, if the spider is malicious, it will most likely emulate the headers of a normal browser. Finger print it, if it's IE. Use JavaScript to check for an active X object. Three, take note of what it's accessing and how regularly. If the content takes the average human X amount of seconds to view, then you can use that as a place to start when trying to determine if it's humanly possible to consume the data that fast. This is tricky, you'll most likely have to rely on cookies. An IP can be shared by multiple users.
