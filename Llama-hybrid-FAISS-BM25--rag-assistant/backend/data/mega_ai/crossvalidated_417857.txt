[site]: crossvalidated
[post_id]: 417857
[parent_id]: 417408
[tags]: 
1) Multi-head attention can be evaluated in parallel. It could simplify multi GPUs implementation. Single head attention could link it as you said, however, authors of the paper noticed that application of the multi-head attention is 'beneficial'. It's a black-box statement. 2) Feed-forward layers are always adding a "space" for a preparing mixture of information from previous layers. Quote: "Another way of describing this is as two convolutions with kernel size 1.", you can interpret convolution with kernel 1 as a layer for preparing a linear combination of all channels.
