[site]: crossvalidated
[post_id]: 585065
[parent_id]: 
[tags]: 
Independence in Graphical model of $p(h_{1:T}|v_{1:T})$ of an HMM

I am studying Hidden Markov Models and I'm trying to understand the following exercise: Consider Hidden Markov Model with hidden states $h_{1:T} = \{h_1,...,h_T\}$ and observed states $v_{1:T}=\{v_1,...,v_T\}$ . When the sequence of outcomes $v_{1:T}$ is observed, it induces the distribution on the hidden states $p_v(h_{1:T}) = p(h_{1:T}|v_{1:T})$ . Question 1: What is the graphical model of this distribution? Question 2: Based on the graphical model, is $h_1$ independent of $h_T$ in this distribution? Question 3: Is is possible to sample efficiently from $p_v(h_{1:T})$ ? My answers are: Answer to Question 1: We have that $p(h_1,...,h_n|v_{1:n})$ forms a first order Markov chain. The simplest way to show this is to notice that the undirected graph for the hidden Markov model is the same as the DAG but with the arrows removed as there are no colliders in the DAG. Moreover, conditioning corresponds to removing nodes from an undirected graph. This leaves us with a chain that connects the $h_i$ . By graph separation, we see that $p(h_1,...,h_n|v_{1:n})$ forms a first-rder Markov chain so that e.g. $h_{1:t-1}$ is independent of $h_{t+1:n}$ given $h_t$ (past independent from the future given the present). Answer to question 2: We can see from the Markov model that $h_1$ has a path leading to $h_T$ which is not blocked, hence they are not independent. Answer to question 3: It is possible to efficiently sample from $p(h_{1:T}|v_{1:T})$ using an MCMC method like Gibbs sampling, since $p_v(h_{1:T})$ is a Markov chain. Is my understanding correct with my answers?
