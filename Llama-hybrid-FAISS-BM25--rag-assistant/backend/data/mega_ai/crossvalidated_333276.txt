[site]: crossvalidated
[post_id]: 333276
[parent_id]: 333269
[tags]: 
People refer to this as "feature engineering". It often helps. Under certain conditions, neural nets can approximate any continuous function . These conditions don't typically hold with modern nets (deep, rather than wide, and using ReLU-family activatons), but still neural nets can approximate quite a lot. But that doesn't guarantee that neural nets can do so efficiently. Say the true model is $$ y = X\beta + \epsilon $$ You don't observe $X$, but you observe $\mathbf{Z}$, where $X = f(\mathbf{Z})$, and $f$ is really complicated. With infinite data, you can approximate $f$ pretty well using a neural net. If you know $f$, or know it to some approximation, your neural net has to do less work in figuring out the optimal combination of $\mathbf{Z}$ to get $X$. In other words, a neural net (with a single layer for simplicity) is $$ y = a(\mathbf{Z}\Gamma)\beta + \epsilon $$ Most of the work goes into finding $\Gamma$. If you already know it, your process is more efficient, and you'll get better results.
