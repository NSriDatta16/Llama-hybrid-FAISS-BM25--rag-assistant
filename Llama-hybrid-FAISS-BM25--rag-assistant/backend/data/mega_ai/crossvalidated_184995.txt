[site]: crossvalidated
[post_id]: 184995
[parent_id]: 184602
[tags]: 
What you're asking is the difference between voting (using classifications) and averaging (using probabilities), and as far as things go, it's up to preference and/or performance. Try both and see what works best, and if there is not a significant difference, pick one that you feel better about. I'll try to explain the feeling better part now. Voting might sound a bit un-natural as you've said since it ignores if the probability is 0.1 or 0.49 - however take this example. Gradient boosted tree (GBT) models are really prone to overfitting. That's why all the parameters need to be tweaked carefully and why a small change leads to a massively better generalized performance (check kaggle for reference). In this case, overfitting might mean that the GBT model will be very sure for an instance with $p_1 = 0.85$. However, two different, simpler models will not be that sure and will output $p_2 = 0.39$ and $p_3 = 0.32$ respectively. The average is $p_{avg}=0.52$ and the sample is classified according to the GBT model, even though 2/3 of your models did not agree with this. In case that you used voting, the classification would obviously be different. So, what's the conclusion here? If some of your classifiers are prone to overfitting hard and providing large probability errors for missclassified samples (which you can test for with various loss measures) but performs well otherwise, perhaps you should use voting for the error to be fixed by other classifiers. If the probability errors for the missclassified samples are within reasonable bounds, then you might be better off using probabilities, since there is no information loss and that is usually a good thing. Overall , I doubt there is an absolutely correct way to do this, and it definitely differs from task to task and ensemble to ensemble.
