[site]: crossvalidated
[post_id]: 560997
[parent_id]: 
[tags]: 
Confidence interval over mean or over all values?

Say I have a dataset generated by running my AI on a boardgame. The data set consists of two "main" values: Actual gained reward from running the AIs optimal policy and the reward my AI expected to get from running its policy. More precisely: Every time my AI has trained for 1 million "training" steps, I evaluate its policy by letting it run 1 million steps following its optimal policy. Each step generates the two data points; What it expected to get from following its policy for this step and what it got from following its policy for this step. I wish to show, that the more moves there are in a game, the bigger the difference between the two is. Meaning that the more moves there are in a game, the more this specific AI algorithm tends to overestimate. My original plan was to take the average of the steps from a single run and use the standard deviation of between these steps and their average to calculate the confidence interval. My question is; Should I instead calculate the average of multiple runs and then use the standard deviation of this to compute the confidence interval of the mean? What is the key difference between these two? My train of thought; If the confidence interval shows where, on average, my AI estimates the reward and where, on average, the reward usually lies; If I take enough steps such that these two are separated I get the average difference between estimation and the actual reward. Meaning I get the average overestimation.
