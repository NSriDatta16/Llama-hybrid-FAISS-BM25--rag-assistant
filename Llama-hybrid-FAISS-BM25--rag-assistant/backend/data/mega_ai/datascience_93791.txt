[site]: datascience
[post_id]: 93791
[parent_id]: 
[tags]: 
DQN CartPole-v1 neural network doesn't optimize

I'm doing my first dnq algorithm, I'm trying to build a dnq agent, and neural network from scratch, but it seems that neural network doesn't optimize, I did 2 hidden layers, with ReLU, and the output layer with linear activation function, but often the weights diverge to NaN, or when the loss function decrease, the total_reward/steps doesn't increase, I don't know why, I debugged a lot, the code is below the logic should be like this code that I found https://github.com/abhavk/dqn_from_scratch/blob/master/cartpole.py which works fine, I don't know where I'm wronging... I didn't use replay memory for this, I think it should be work without at least Or in colab you can try this (neural network that doesn't work... with other code in internet) import random import gym import numpy as np from collections import deque import numpy as np import pickle as pk class NeuralNetwork: def __init__(self, n_neurons: np.array, path: str = None): """ args: n_neurons: np.array number of neurons per layer path: path to weights and biases dumped """ self.n_neurons = n_neurons if path == None: self.biases = [np.random.uniform(low=-0.5, high=0.5, size=(layer)) for layer in n_neurons[1:]] self.weights = [np.random.uniform(low=-0.5, high=0.5, size=(n_neurons[i+1], n_neurons[i])) for i in range(len(n_neurons) - 1)] else: with open(path, "rb") as file: self.weights, self.biases = pk.load(file) def predict(self, input: np.ndarray) -> np.ndarray: """ Args: input: np.ndarray 1dim array that rapresent inputs Return: prediction: np.ndarray 1dim """ for weights, biases, activation_function in zip(self.weights, self.biases, self.activation_functions): input = activation_function(weights.dot(input) + biases) return input def forward_propagate(self, input: np.array): """ Args: input: np.ndarray 1dim array that rapresent inputs Return: predict values a, z """ z = [input] a = [input] for weights, biases, activation_function in zip(self.weights, self.biases, self.activation_functions): z_ = weights.dot(input) + biases z.append(z_) input = activation_function(z_) a.append(input) return z, a def backpropagate(self, z: list, a: list, target_output: np.array, learning_rate: float): print(self.biases) if any([any([any(np.isnan(weights)) for weights in weights]) for weights in self.weights]) or any([any(np.isnan(biases)) for biases in self.biases]): print('nan weights or biases') raise Exception('NNNNNNOOOOOONNNNNNNNNEEEEEEEEEEEEE') """ Args: z: list[np.ndarray] a: list[np.ndarray] target_output: np.array learning_rate: float """ # compute last layer gradients gradients_a = self.cost_function_derivative(a[-1], target_output) gradients_z = gradients_a * self.activation_functions_derivative[-1](z[-1]) # update weights and biases, and compute gradients for hidden layers for weights, biases, z, a_, activation_function_derivative in reversed(list(zip(self.weights[1:], self.biases[1:], z[1:-1], a[1:-1], self.activation_functions_derivative[:-1]))): weights -= learning_rate * gradients_z.reshape(-1, 1).dot(a_.reshape(1, -1)) biases -= learning_rate * gradients_z gradients_a = weights.T.dot(gradients_z) gradients_z = gradients_a * activation_function_derivative(z) #update first weights and biases self.weights[0] -= learning_rate * gradients_z.reshape(-1, 1).dot(a[0].reshape(1, -1)) self.biases[0] -= learning_rate * gradients_z def save(self, path: str): with open(path, "wb") as file: pk.dump((self.weights, self.biases), file) class CustomNeuralNetwork(NeuralNetwork): def __init__(self, n_neurons: np.array, path: str): """ args: n_neurons: np.array number of neurons per layer path: path to weights and biases dumped """ super().__init__(n_neurons, path) def identity(x): return x def identity_derivative(x): return np.ones(len(x)) def ReLU(x): return np.multiply(x,(x>0)) def ReLU_derivative(x): return (x>0)*1 def sum_square_error(predicted, target): return np.sum((predicted - target)**2) / 2 def sum_square_error_derivative(predicted, target): return predicted - target """ Define in object functions: activation_functions: iter[len(n_neurons)-1] dtype:function(np.array)->np.array activation_functions_derivative: iter[len(n_neurons)-1] dtype:function(np.array)->np.array cost_function_derivative: function(predicted: np.array, target: np.array)->np.array partial derivatives of cost_function respect to predicted values """ self.activation_functions = [ReLU, ReLU, identity] self.activation_functions_derivative = [ReLU_derivative, ReLU_derivative, identity_derivative] self.cost_function = sum_square_error self.cost_function_derivative = sum_square_error_derivative ENV_NAME = "CartPole-v1" GAMMA = 0.95 LEARNING_RATE = 0.1 MEMORY_SIZE = 1000000 BATCH_SIZE = 20 EXPLORATION_MAX = 1.0 EXPLORATION_MIN = 0.01 EXPLORATION_DECAY = 0.995 class DQNSolver: def __init__(self, observation_space, action_space): self.exploration_rate = EXPLORATION_MAX self.action_space = action_space self.memory = deque(maxlen=MEMORY_SIZE) self.nn = CustomNeuralNetwork([observation_space, 24, 24, action_space], None) #self.model = Sequential() #self.model.add(Dense(5, input_shape=(observation_space,), activation="relu")) #self.model.add(Dense(24, activation="relu")) #self.model.add(Dense(self.action_space, activation="linear")) #self.model.compile(loss="mse", optimizer=Adam(lr=LEARNING_RATE)) def remember(self, state, action, reward, next_state, done): self.memory.append((state, action, reward, next_state, done)) def act(self, state): if np.random.rand() source: https://github.com/LorenzoTinfena/deep-q-learning-itt-final-project/tree/main/src/core And this work with keras neural network, why?? is really Adam optimizer?? import random import gym import numpy as np from collections import deque from keras.models import Sequential from keras.layers import Dense from keras.optimizers import Adam #from scores.score_logger import ScoreLogger ENV_NAME = "CartPole-v1" GAMMA = 0.95 LEARNING_RATE = 0.001 MEMORY_SIZE = 1000000 BATCH_SIZE = 20 EXPLORATION_MAX = 1.0 EXPLORATION_MIN = 0.01 EXPLORATION_DECAY = 0.995 class DQNSolver: def __init__(self, observation_space, action_space): self.exploration_rate = EXPLORATION_MAX self.action_space = action_space self.memory = deque(maxlen=MEMORY_SIZE) self.model = Sequential() self.model.add(Dense(24, input_shape=(observation_space,), activation="relu")) self.model.add(Dense(24, activation="relu")) self.model.add(Dense(self.action_space, activation="linear")) self.model.compile(loss="mse", optimizer=Adam(lr=LEARNING_RATE)) def remember(self, state, action, reward, next_state, done): self.memory.append((state, action, reward, next_state, done)) def act(self, state): if np.random.rand() ```
