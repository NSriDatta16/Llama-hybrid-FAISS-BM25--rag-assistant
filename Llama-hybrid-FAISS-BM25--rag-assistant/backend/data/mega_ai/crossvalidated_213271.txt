[site]: crossvalidated
[post_id]: 213271
[parent_id]: 
[tags]: 
Why is optimisation solved with gradient descent rather than with an analytical solution?

I'm trying to understand why, when trying to minimise an objective function, gradient descent is often used, rather than setting the gradient of the error to zero, and solving it analytically. In school, I was taught that in order to minimise a function, you take it's first derivative with respect to the parameters, and then set this to zero. Solving this equation then yields one or more solutions (local minima), and you can try each one out to find which is the global minimum. However, when learning about neural networks, I have seen that training is carried out by using gradient descent (or back-propagation in the case of multi-layer networks). Here, the gradient of the error function is taken, but instead of setting this to zero, you just move the parameters a small amount in the direction of this gradient. I don't understand why the analytical approach isn't used, given that it has the obvious strength that the solution is perfect, and guarantees returning the global minimum, rather than potentially a local minimum as with gradient descent. My intuitive reason why the analytical approach isn't used is that it becomes very hard to solve an equation with all these parameters by hand, but, perhaps naively, I would have thought that there were automated ways to solve these kind of complex equations these days? Wouldn't the effort in enabling this be worth it, considering the better results, compared to the effort in getting gradient descent to work only to get inferior results? Thank you :)
