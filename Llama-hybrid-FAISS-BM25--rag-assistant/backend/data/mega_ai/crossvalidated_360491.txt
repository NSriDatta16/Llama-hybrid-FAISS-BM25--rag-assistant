[site]: crossvalidated
[post_id]: 360491
[parent_id]: 357449
[tags]: 
The two regimes of behavior probably reflect a saddle point, or a region of the parameter space that is just very shallow. This image gives a nice conceptual illustration. Adaptive learning rates and momentum allow the optimizer to escape a region with a shallow gradient or saddle point. Quickly adapting to changing curvature is the goal of methods like adam , but some recent research has found that the gains due to Adam are marginal (" The Marginal Value of Adaptive Gradient Methods in Machine Learning" by Ashia C. Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, Benjamin Recht). This can also happen if you’re reducing the learning rate at the particular epoch where the drop happens. But since you don’t mention that you’re specifically doing this, it’s probably due to a saddle point.
