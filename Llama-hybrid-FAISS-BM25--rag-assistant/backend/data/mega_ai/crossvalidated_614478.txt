[site]: crossvalidated
[post_id]: 614478
[parent_id]: 
[tags]: 
Replacing a fully connected layer with a 1x1 convolution vs with fxf convolution

Suppose an input of shape (width x height x channel_num) = (10 x 10 x 15) is obtained from previous convolutional layers, and this input is about to be inserted into a fully connected (fc) layer of K=100. How can this fc layer be replaced completely? Method 1: One way is to replace this fc layer with a convolution of kernel size equal to that of the input kernel size, and the number of output channels equal to K of the fc layer. In this case replace with a convolution of kernel size f=10, stride s=1, padding p=0, output channel number = 100. This would give an output of shape (1 x 1 x 100), which is somewhat like the expected 1d vector of an fc layer of length 100. Number of learnable parameters= #weights + #biases = 10x10x15x100 + 100 = 150100 Method 2: However, how can the fc be replaced by 1x1 convolutions, instead of the fxf convolution? If we directly convolve the input of shape (10 x 10 x 15) with some (1 x 1 x 15) filter and we use 100 such filters with s=1, p=0, then the output is of shape (10 x 10 x 100), instead of a shape that can be flattened to a 1d length 100 vector like (1 x 1 x 100). A workaround is reshaping the input like this: (10 x 10 x 15) --> (1 x 1 x 1500), then convolving with 100 (1 x 1 x 1500) filters. This gives an output of (1 x 1 x 100). Number of learnable parameters = 1x1x1500x100 + 100 = 150100 It appears both methods give the same number of learnable parameters. The number of learnable parameters in the normal fc = 10x10x15x100 + 100 = 150100 which matches the number in the two conversions. Are these calculations correct? The 10x10x15 part stems from the thinking of flattening of the multi-dimension input into a single vector of length 1500. Is this intuition appropriate? Are both methods correct? Is there any difference? Which one do large CNNs use?
