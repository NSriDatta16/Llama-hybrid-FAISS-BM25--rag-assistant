[site]: crossvalidated
[post_id]: 524297
[parent_id]: 
[tags]: 
Why would a cross-entropy approach negative infinity?

I'm studying Deep Learning by Ian Goodfellow. In section 6.2.1.1 it says For real-valued output variables, if the model can control the density of the output distribution (for example, by learning the variance parameter of a Gaussian output distribution) then it becomes possible to assign extremely high density to the correct training set outputs, resulting in cross-entropy approaching negative infinity . But the cross-entropy is defined as $$−\mathbb{E}_{x∼P_{data}}\log P_{model}(y|x)$$ or in the empirical form $$−\frac{1}{m}\sum_{i=1}^m \log P_{model}(y|x)$$ As we can see it is bounded below by $−\log 1=0$ . Then why the book says that it would approach negative infinity? Any thought would help!
