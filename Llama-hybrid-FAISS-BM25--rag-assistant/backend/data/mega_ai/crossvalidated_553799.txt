[site]: crossvalidated
[post_id]: 553799
[parent_id]: 553791
[tags]: 
If your training and test sets differed substantially in terms of average observations, as your question suggests, then perhaps there is a problem with your train/test split. The problem potentially posed by a difference in average observations between the training and test sets is that the two sets might not represent the same underlying population. That puts many modeling assumptions into question. You certainly don't want your model to depend heavily on a particular single choice of train/test split. Another approach is to build the model first on the entire data set. You then evaluate the performance of the modeling method by repeating all modeling steps on multiple bootstrap samples of the data, using the entire data set as the test set for each model. That provides estimates of optimism bias in the modeling method. Frank Harrell estimates that this approach is superior unless you have 20,000 or more observations . depending on the signal/noise ratio in your data.
