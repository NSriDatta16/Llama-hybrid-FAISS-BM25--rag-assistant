[site]: crossvalidated
[post_id]: 641550
[parent_id]: 235020
[tags]: 
In my personal opinion.... Thoughts: Random forest looks at nonlinear interactions of variables, but logistic regression does not, unless you explicitly code it. Random forest does bootstrap resampling so it is more resistant to outliers than least-squares fits. You could have ridge or lasso for you logistic regression, but I did not see that indicated. There is a fun library called "Boruta" that has some of the niggling parts of this reasonably engaged. It is sometimes possible to make linear regression as powerful as a random forest by computing the RF variable importances, then associating those with the linear fit as variable weights. Opinion: I prefer to use the "Z-scores of mean decrease accuracy measure" for variable importance instead of Gini importance. There are cases where the Random Forest has been much more useful for this than logistic regression.
