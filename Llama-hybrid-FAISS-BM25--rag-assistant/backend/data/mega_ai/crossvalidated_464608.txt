[site]: crossvalidated
[post_id]: 464608
[parent_id]: 
[tags]: 
Are the factors in a time series factor model independent over time?

Consider the time series extension to the standard factor model: $$X_t = \Lambda F_t + e_t, \qquad t = 1, 2, \ldots, T$$ where $X_t$ represents \the vector of observations at time $t$ , $F_t$ represents the vector of latent factors at time $t$ , $\Lambda$ is a factor loading matrix, and $e_t$ is a random vector of idiosyncratic errors. We assume that the factors and errors are uncorrelated and Gaussian, more specifically $F_t \sim \mathcal N (0, \operatorname{Id})$ and $e_t \sim \mathcal N (0, \Sigma)$ . Also $\Sigma$ is assumed to be diagonal. I am interested in writing out the so-called complete likelihood $$p(X_{1:T}, F_{1:T} \:|\: \Lambda, \Sigma)$$ My research seems to suggest that it should be expressed in the following way \begin{align} p(X_{1:T}, F_{1:T} \:|\: \Lambda, \Sigma) &= p(X_{1:T} \:|\: F_{1:T}, \Lambda, \Sigma) p(F_{1:T} \:|\: \Lambda, \Sigma ) \\ &= \prod_{t = 1}^T p(X_t \:|\: F_t) p(F_t) \end{align} for example see equation 3.19 here (Variational Approach to Factor Analysis and Related Models). This particular formulation seems to suggest some things that I have not been able to find sources to back up (hence the reason for this post). It seems to suggest the following: The factors are independent over time, i.e. $F_s \perp F_t$ for all $s \neq t$ The observations are conditionally independent over time given the factors, i.e. given $F_t$ , $X_t \perp X_s$ for all $s \neq t$ I feel like these are assumptions that have to be made because how else would you write the complete likelihood if all you know is the information I stated in the first paragraph of this post? That reasoning is unsatisfactory for me, however. My questions are Are the factors assumed to be independent over time? Are the observations assumed to be conditionally independent over time? Any help/clarifications would be appreciated.
