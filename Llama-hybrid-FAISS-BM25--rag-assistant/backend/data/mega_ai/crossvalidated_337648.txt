[site]: crossvalidated
[post_id]: 337648
[parent_id]: 226555
[tags]: 
Yes, averaging models is a good approach! This is a consequence of the convexity of most loss functions (log-loss, MSE...). Recalling Jensen's inequality for a convex function $f$ : $$f(\lambda x + (1-\lambda) y) \leq \lambda f(x) + (1-\lambda)f(y)$$ Basically, the error of the average of two predictions will always be lower than the average of these errors. So the error cannot be bigger than the biggest error of your two models. Now the big improvement usually comes from the fact that if two (say regression) models disagree on some points, one by lower values, the other one by larger values, then the average becomes really close to the actual value! Usually, model averaging performs even better when the produced predictions have a low correlation and similar performance as the situation where model disagree are quite common.
