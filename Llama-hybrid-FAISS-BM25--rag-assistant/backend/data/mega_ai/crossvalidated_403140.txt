[site]: crossvalidated
[post_id]: 403140
[parent_id]: 402700
[tags]: 
TL;DR: Epochs are (unfortunately) the standard way to talk about the length of neural network training, mainly because we use them as a natural checkpoint for checking the model performance on a validation set. "Epochs" are indeed very deceiving unit used to measure the length of the training. Using "number of updates" would make more sense because it is independent of the actual training set being used, and could be in theory tuned via cross validation. If you use on-line data augmentation, counting epochs starts making even less sense, because there is no finite dataset that one could pass, there are just new unseen samples coming at every iteration. On the other hand, one has to check the validation loss regularly. Doing it after every iteration would introduce a major overhead; doing it every 100 (1 000, 10 000?) iterations is just arbitrary. Doing it after looking at each training sample once makes some sense, e.g. should give a good estimate how the model improved since the last check, suppressing potential effects of "training with more informative samples since the last time". In this sense, using epochs sounds reasonable. Since the validation set is used for early stopping, and early stopping is used as the golden standard for checking when to terminate the training, epochs are the standard unit for training length.
