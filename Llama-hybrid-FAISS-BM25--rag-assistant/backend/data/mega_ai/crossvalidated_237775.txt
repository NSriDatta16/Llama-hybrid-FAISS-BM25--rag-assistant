[site]: crossvalidated
[post_id]: 237775
[parent_id]: 237763
[tags]: 
Two thoughts. RF are often split until purity. This often means that there are many terminal nodes, each with a single observation. The final splits leading up to these nodes may not generalize very well because there are so few observations to work with at that depth of the tree. So you may get more generalizable trees with lower out-of-sample variance if you increase the minimum node size from 1 to something like 10 or more (depending on how much data you have -- this is another hyper-parameter, so you might profitably tune it). This also has the property of yielding consistent probability estimates, which can be desirable in some contexts. Increasing the number of trees will reduce the variance of the estimator. This is an obvious consequence of one of the CLTs -- each tree is a binomial trial, and the prediction of the forest is the average of many binomial trials. Moreover, the trees are iid in the sense that they are all fit on different re-samplings of the data and different random subsets of features. So you have iid binomial trails (which have finite variance because each trial is 0 or 1, i.e. has finite cardinality). This can make the predictions less volatile because the trees only have to explain chunks of your data, instead of each observation. So four times as many trials will cut the standard error of the mean in half. There is extended discussion of some of these RF properties in Elements of Statistical Learning . The consistency property is discussed in Malley JD, Kruppa J, Dasgupta A, Malley KG, Ziegler A. Probability Machines: Consistent Probability Estimation Using Nonparametric Learning Machines . Methods of Information in Medicine. 2012;51(1):74-81. doi:10.3414/ME00-01-0052. Finally, as a general observation, the best regularizer is more data, and better features usually beat a cleverer algorithm.
