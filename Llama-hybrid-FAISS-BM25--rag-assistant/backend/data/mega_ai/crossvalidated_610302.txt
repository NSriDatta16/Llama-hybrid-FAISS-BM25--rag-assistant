[site]: crossvalidated
[post_id]: 610302
[parent_id]: 
[tags]: 
Derivation of the Pitman estimator

I am having trouble with a step of a proof in the book Statistical Estimation Asymptotic Theory by Ibragimov and Has'minskii. Lemma 2.1: Let $T=T(X_1, ..., X_n)$ be an arbitrary statistic with $\mathbb{E}_\theta|T| . Then $$ \mathbb{E}_\theta[T | X_2 - X_1, ..., X_n - X_1] = \int_{R^k}T(X_1 + \theta - u, ..., X_n+\theta-u)\frac{\Pi_1^nf(X_j-u)}{\int_{R^k}f(X_j - v)dv}du. $$ Following is the snapshot from the book: The proof begins with: Denote by $I$ the right hand side of the equation above. $I$ is a function of $X_2-X_1$ , ..., $X_n-X_1$ only. Therefore it is sufficient to prove that for any statistic $z$ of the form $z(X_2-X_1, ..., X_n-X_1)$ the equality $\mathbb{E}_\theta[zI] = \mathbb{E}[zT]$ is valid. I cannot see the jump between the final equality, $\mathbb{E}_\theta[zI] = \mathbb{E}[zT]$ , and the main statement of the lemma. How does one imply the other? Edit: Adding definition of $\bf W$ The other missing definition is that $\bf W$ satisfies ${\bf W}(u;v) = w(u-v)$ $w(u)$ is defined and nonnegative, $w(0)=0$ ,and continuous at $u=0$ but is not identically $0$ $w(u) = w(-u)$ the sets $\{ u:w(u) are convex for all $c>0$
