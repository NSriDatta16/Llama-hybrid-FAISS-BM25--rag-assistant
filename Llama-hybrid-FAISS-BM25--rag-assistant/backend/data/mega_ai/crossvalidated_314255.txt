[site]: crossvalidated
[post_id]: 314255
[parent_id]: 314228
[tags]: 
Huh this is an interesting problem. The cost function for a one-class SVM is : \begin{align*} \text{min}_{w, \rho, \xi_i}&\frac{1}{2}||w||^2 + \frac{1}{\nu}\sum_i\xi_i-\rho \\ s.t. w^{T}\Phi(x_i) &\geq \rho - \xi_i \\ \xi_i \geq 0 \end{align*} (apologies for formatting...) From this we can see that we're trying to find a function $f(x|w) = w^Tx$ that is bigger than $\rho$ for all the training points (the slack variables $\xi_i$ prevent this from causing no solution if no function exists). $\nu$ controls how we trade-off the norm of the vector we find ($||w||^2$) and the amount of slack we allow. I think this gives an answer to your answer for your question (2), namely that it doesn't matter. Nothing about the problem formally changes if you have one of your training points as the origin, in particular because you assume the feature generating function $\Phi()$. If you assume that function is the identity then you see that $w^T\mathbf{0} \ngeq \rho$ and therefore this must be taken care of by the slack variables. With respect to your question (1), look at the projection of your points onto the hyperplanes you've drawn. H2 and H3 are the same line except for an offset (approximately), which would fall under the question of what $\rho$ the training procedure returns. H1, if you look at the projections of the points onto the hyperplane clearly has less distance between projected origin and the closest projected point, which prevents it from being your answer. The $\nu$ parameter is actually not involved, since your points are perfectly separable (incidentally this also exposes a problem with the one-class SVM if you use a linear kernel, namely that it can only give you a half-space at best. It really can only be useful with the kernel trick, I would say) It was an interesting question! Thanks :)
