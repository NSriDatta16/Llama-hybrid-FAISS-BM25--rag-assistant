[site]: datascience
[post_id]: 96812
[parent_id]: 
[tags]: 
How do we bring Pareto optimality into the realm of Machine Learning?

I have a multi-objective optimisation problem with a large number of objectives (more than 10) which is generally the case in most-real life problems. The use of traditional GAs such as NSGA-II or SPEA-II fails in this scenario because of 'the curse of dimensionality' as discussed in Deb et al. (2005) The same paper suggests the use of PCA for dimensionality reduction within evolutionary computing algorithms. I want to know if there is another way to obtain 'pareto-optimality' ? Can we look at this MOO problem as an inverse ill-posed curve fitting problem and solve it using multi-task learning? I say this because MTL is popular because of 'Regularization'. If not, what would be a feasible approach?
