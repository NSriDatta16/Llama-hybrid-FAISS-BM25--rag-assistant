[site]: crossvalidated
[post_id]: 248737
[parent_id]: 248188
[tags]: 
It seems they used mean absolute values of correlations (not mean correlations) between their models and their observations. They calculated those only if they had significant regressions, and when not, they omitted them. The regressions and correlations were from standardized expected sighting of each animal species use of highway underpass wildlife crossings for the various classifiers of each wildlife crossing's physical characteristics, How long, how high etc. Then they ranked each significant crossing classifier for influence on the mean absolute values of correlations to show inter-species variation, and showed that in Table 3. As an illustration of what this means the authors offered "For example, we found that underpass distance from the east gate (positive correlation) was the most significant underpass attribute affecting black bear performance ratios, whereas underpass length (negative correlation) was the most significant attribute affecting elk performance ratios (Table 3)." The rankings themselves are highest to lowest of strongest to weakest (absolute) correlation. That implies, that after neglecting the sign of the correlations in order to rank them, once they did so, they promptly referred to the sign of that ranking in order to interpret the ranking itself. Now as to the regressions, they "used a family of simple curvilinear and polynomial regression curves to optimize the fit between species-performance ratios and each underpass attribute (Jandel Scientific 1994)." So they did a model for each attribute which is where they are getting absolute correlations to rank for each attribute. The performance ratios (of crossings) in the paper were obtained by dividing observed crossing frequencies by the expected crossing frequencies. As multiplying a $y$-axis variable by a constant, i.e., $1/E(y)$, has no effect on the correlations sought, it is unclear why this is being done. Then, they used an unusual and unusually complicated method to select best regression models, which seems to be done to get a best magnitude correlation, and not to be best models, particularly. The plethora of models in the appendix section represents a trial and error approach to modelling, as contrasts to a more organized data exploration based on statistical characterization of data transformations. The trial and error method is less efficient than organized model exploration by at least an order of magnitude of the number of trial models. The various models were likely default models included in the software used . My personal experience with the trial and error approach has suggested to me that even when thousands of model equations are tried in all categories poly, trig, power, etc., one does not find an optimal regression, and the trial and approach is more useful for finding how not to consider the data than how best to treat it. Now since their methodology is not presented over well, please allow me to paraphrase. They performed a variant of ANOVA called step-wise reduction of number of parameters, but remember, they are only using each trait by itself, so their models look like $BearCrossing_{No}=c_0+c_1 length+c_2 length^2+c_3 length^3$, then they test if the total model is significant (ANOVA F-test, $p If yes, they generate progressively smaller models $BearCrossing_{No}=c_0+c_1 length+c_2 length^2$, $BearCrossing_{No}=c_0+c_1 length$ Then, they ask for those equations with a significant highest order term (e.g., $c_3$), which remaining equation has the highest overall F-statistic? Now, there are doing this for all sorts of variations on the theme for example they also test $Ln(BearCrossing_{No})=c_0+c_1 length+c_2 length^2+c_3 length^3$, in the same fashion, all the while looking for the best F-statistic. So, they are doing approximately 100 regressions generated from the 41 different models in the appendix all looking for the best F-statistic with a significant partial probability of the highest order parameter in that equation. All that just for one animal and one crossing parameter. They repeat that process for all the assorted crossing parameters for bears. Then they are doing all that over and over again for every single animal and every single crossing parameter. I would not have done that is perhaps not too impolite a statement. Supplementary OP Question "What would you have done?" For an example of how an entirely different but statistically somewhat similar problem has been handled navigate here and left click download . First of all, I would examine the data to determine what type of variables I have, testing and retesting my assumptions as I go. For example, in the paper last referenced above, I discovered that all of the variables could be considered proportional type, which meant that I could take logarithms and work with power function expressions to the exclusion of others. Finding appropriate variable transformations for homoscedasticity/normality is important, especially to reduce skewness, followed by outlier testing and reduction. In the contrary case, one should use only non-parametric statistics, for example, Spearman rank correlation and not Pearson correlation. I would certainly combine parameters in each expression to see which is best for predicting the target. To determine that I would use adjusted $R^2$ or I might use BIC as a model selection criterion if I can use maximum likelihood for the appropriate residual type of the transformed data (transformed for best homoscedasticity/normality if I can get it). Now, since there are multiple $y$-axis variables (the animals here), I would consider using MANOVA on the lot of them rather than multiple ANOVA classifications. I would consider doing Kruskal-Wallis on the whole data to get a global significance of what I am doing, before proceeding to look at individual comparisons for significance. Note , I do not use step-wise regression, I test all variable combinations, as the next paragraph explains. I would test VIF, and explore any high collinearity between crossing parameters to determine non-linear co-dependence, unusual collinearity stabilizing parameter combinations and other explanations for why the data behaves as it does. That is, as a first guess, and mind you, only a roughed out estimate of what I would do. This is data mining of a sort, so one is obliged to assume nothing and test everything.
