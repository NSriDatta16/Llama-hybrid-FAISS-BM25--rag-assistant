[site]: crossvalidated
[post_id]: 454115
[parent_id]: 295397
[tags]: 
I'd like to explain the difference visually and in detail(comments in code) and in a very easy approach. Let's first check the Conv2D in TensorFlow . c1 = [[0, 0, 1, 0, 2], [1, 0, 2, 0, 1], [1, 0, 2, 2, 0], [2, 0, 0, 2, 0], [2, 1, 2, 2, 0]] c2 = [[2, 1, 2, 1, 1], [2, 1, 2, 0, 1], [0, 2, 1, 0, 1], [1, 2, 2, 2, 2], [0, 1, 2, 0, 1]] c3 = [[2, 1, 1, 2, 0], [1, 0, 0, 1, 0], [0, 1, 0, 0, 0], [1, 0, 2, 1, 0], [2, 2, 1, 1, 1]] data = tf.transpose(tf.constant([[c1, c2, c3]], dtype=tf.float32), (0, 2, 3, 1)) # we transfer [batch, in_channels, in_height, in_width] to [batch, in_height, in_width, in_channels] # where batch = 1, in_channels = 3 (c1, c2, c3 or x[:, :, 0], x[:, :, 1], x[:, :, 2] in the gif), in_height and in_width are all 5(the sizes of the blue matrices without padding) f2c1 = [[0, 1, -1], [0, -1, 0], [0, -1, 1]] f2c2 = [[-1, 0, 0], [1, -1, 0], [1, -1, 0]] f2c3 = [[-1, 1, -1], [0, -1, -1], [1, 0, 0]] filters = tf.transpose(tf.constant([[f2c1, f2c2, f2c3]], dtype=tf.float32), (2, 3, 1, 0)) # transfer the [out_channels, in_channels, filter_height, filter_width] to [filter_height, filter_width, in_channels, out_channels] # out_channels is 1(in the gif it is 2 since here we only use one filter W1), in_channels is 3 because data has three channels(c1, c2, c3), filter_height and filter_width are all 3(the sizes of the filter W1) # f2c1, f2c2, f2c3 are the w1[:, :, 0], w1[:, :, 1] and w1[:, :, 2] in the gif output = tf.squeeze(tf.nn.conv2d(data, filters, strides=2, padding=[[0, 0], [1, 1], [1, 1], [0, 0]])) # this is just the o[:,:,1] in the gif # And the Conv1D is a special case of Conv2D as stated in this paragraph from the TensorFlow doc of Conv1D . Internally, this op reshapes the input tensors and invokes tf.nn.conv2d. For example, if data_format does not start with "NC", a tensor of shape [batch, in_width, in_channels] is reshaped to [batch, 1, in_width, in_channels], and the filter is reshaped to [1, filter_width, in_channels, out_channels]. The result is then reshaped back to [batch, out_width, out_channels] (where out_width is a function of the stride and padding as in conv2d) and returned to the caller. Let's see how we can transfer Conv1D to a Conv2D problem. Since Conv1D is usually used in NLP scenarios, we can illustrate that in the below NLP problem. cat = [0.7, 0.4, 0.5] sitting = [0.2, -0.1, 0.1] there = [-0.5, 0.4, 0.1] dog = [0.6, 0.3, 0.5] resting = [0.3, -0.1, 0.2] here = [-0.5, 0.4, 0.1] sentence = tf.constant([[cat, sitting, there, dog, resting, here]] # sentence[:,:,0] is equivalent to x[:,:,0] or c1 in the first example and the same for sentence[:,:,1] and sentence[:,:,2] data = tf.reshape(sentence), (1, 1, 6, 3)) # we reshape [batch, in_width, in_channels] to [batch, 1, in_width, in_channels] according to the quote above # each dimension in the embedding is a channel(three in_channels) f3c1 = [0.6, 0.2] # equivalent to f2c1 in the first code snippet or w1[:,:,0] in the gif f3c2 = [0.4, -0.1] # equivalent to f2c2 in the first code snippet or w1[:,:,1] in the gif f3c3 = [0.5, 0.2] # equivalent to f2c3 in the first code snippet or w1[:,:,2] in the gif # filters = tf.constant([[f3c1, f3c2, f3c3]]) # [out_channels, in_channels, filter_width]: [1, 3, 2] # here we also have only one filter and also three channels in it. Please compare these three with the three channels in W1 for the Conv2D in the gif filter1D = tf.transpose(tf.constant([[f3c1, f3c2, f3c3]]), (2, 1, 0)) # shape: [2, 3, 1] for the conv1d example filters = tf.reshape(filter1D, (1, 2, 3, 1)) # this should be expand_dim actually # transpose [out_channels, in_channels, filter_width] to [filter_width, in_channels, out_channels]] and then reshape the result to [1, filter_width, in_channels, out_channels] as we described in the text snippet from Tensorflow doc of conv1doutput output = tf.squeeze(tf.nn.conv2d(data, filters, strides=(1, 1, 2, 1), padding="VALID")) # the numbers for strides are for [batch, 1, in_width, in_channels] of the data input # Let's do that using Conv1D(also in TensorFlow): output = tf.squeeze(tf.nn.conv1d(sentence, filter1D, stride=2, padding="VALID")) # # here stride defaults to be for the in_width We can see that the 2D in Conv2D means each channel in the input and filter is 2 dimensional(as we see in the gif example) and 1D in Conv1D means each channel in the input and filter is 1 dimensional(as we see in the cat and dog NLP example).
