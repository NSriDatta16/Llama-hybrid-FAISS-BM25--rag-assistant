[site]: crossvalidated
[post_id]: 465720
[parent_id]: 
[tags]: 
Regression loss function to yield high correlation

I am using a neural network to predict a target. Currently, my loss is the mean squared error. I am not interested in the absolute values of my predictions, thus I evaluate the predictions using the Pearson correlation coefficient instead of the MSE. I have the intuition that I can improve the Pearson by optimizing it more directly. Additionally, I found in this answer that "Maximizing correlation is useful when the output is highly noisy", which is definitely the case for my data. Is there something similar to the siamese neural network/Triplett loss approach which is as far as I know only used for classification/distinction? Is it reasonable to just use the Pearson correlation as a loss function? I could not find much about this in the literature, apart from some comments on this site .
