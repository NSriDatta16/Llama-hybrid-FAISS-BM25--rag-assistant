[site]: crossvalidated
[post_id]: 307859
[parent_id]: 305619
[tags]: 
It is true that any function can be approximated arbitrarily close both by something that counts as a neural network and something that counts as a polynomial. First of all, keep in mind that this is true for a lot of constructs. You could approximate any function by combining sines and cosines (Fourier transforms), or simply by adding a lot of "rectangles" (not really a precise definition, but I hope you get the point). Second, much like Yoni's answer, whenever you are training a network, or fitting a regression with a lot of powers, the number of neurons, or the number of powers, are fixed. Then you apply some algorithm, maybe gradient descent or something, and find the best parameters with that. The parameters are the weights in a network, and the coefficients for a large polynomial. The maximum power you take in a polynomial, or the number of neurons used, are called the hyperparameters. In practice, you'll try a couple of those. You can make a case that a parameter is a parameter, sure, but that is not how this is done in practice. The point though, with machine learning, you don't really want a function that fits through your data perfectly. That wouldn't be too hard to achieve actually. You want something that fits well, but also probably works for points that you haven't seen yet. See this picture for example, taken from the documentation for scikit-learn . A line is too simple, but the best approximation is not on the right, it's in the middle, allthough the function on the right fits best. The function on the right would make some pretty weird (and probably suboptimal) predictions for new data points, especially if they fall near the wiggly bits on the left. The ultimate reason for neural networks with a couple of parameters working so well, is that they can fit something but not really overfit it. This also has a lot to do with the way they are trained, with some form of stochastic gradient descent.
