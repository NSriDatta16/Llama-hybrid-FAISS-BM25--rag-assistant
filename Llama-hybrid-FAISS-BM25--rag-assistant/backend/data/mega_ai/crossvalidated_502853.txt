[site]: crossvalidated
[post_id]: 502853
[parent_id]: 502849
[tags]: 
The idea of regularization is related to the Bayesian idea of shrinkage: you are biasing your model towards a constant fit (hence the penalty on non-zero coefficients). The regularization factor defines the "price" your model must pay to use that coefficient (i.e., make non-zero). Therefore, if you have noisy data, the noise terms will tend to not give much reduction in RMS or whatever loss metric you have, so the model will think its better off not including them (to save on the "price" of using that coefficient). That is how regularization reduces overfitting -- in logistic or any other linear model.
