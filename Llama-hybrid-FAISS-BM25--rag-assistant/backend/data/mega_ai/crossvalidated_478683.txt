[site]: crossvalidated
[post_id]: 478683
[parent_id]: 
[tags]: 
Why do people use tanh more often than ReLU in vanilla recurrent neural networks?

For instance, the default activation function of tf.keras.layers.SimpleRNN is tanh. My doubt is because tanh activation functions may also cause (like sigmoids) the vanishing gradient problem.
