[site]: crossvalidated
[post_id]: 534851
[parent_id]: 
[tags]: 
How to understand maximum likelihood estimation from an objective Bayesian paradigm?

I am trying to understand maximum likelihood estimation from an objective Bayesian/Jaynesian paradigm. My current understanding is that: There is a parametric family of functions f(x; theta) indexed by a tensor theta. And there is a mapping from the set of tensors to the set of propositions model(theta) that maps a tensor theta to a proposition which we refer to as a “model”. The probability(X = x | model(theta)), i.e. probability that X takes on the value x given that the proposition model(theta) is true, is equal to f(x; theta) for any tensor theta. The dataset is a set of propositions: X = x_1, X = x_2, ..., X = x_n. probability(model(theta) | dataset) is proportional to probability(dataset | model(theta)) * probability(model(theta)). Assuming a flat prior, i.e. probability(model(theta)) = alpha for any tensor theta, we can ignore the probability(model(theta)) term. probability(dataset | model(theta)) = probability(X = x_1 | model(theta)) * ... * probability(X = x_n | model(theta)) = f(x_1; theta) * ... * f(x_n; theta) The goal of maximum likelihood estimation is to find the value of theta which maximises the probability(model(theta) | dataset), i.e. to find the most plausible “model” in the range of model(theta) given the dataset, by maximising the above expression. Is this a correct understanding of maximum likelihood estimation from a Jaynesian viewpoint? Thank you
