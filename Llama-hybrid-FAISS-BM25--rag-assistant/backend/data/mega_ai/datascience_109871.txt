[site]: datascience
[post_id]: 109871
[parent_id]: 109867
[tags]: 
First of all, welcome to the community and secondly, BIG ALARM! Unsupervised and Classification are two fundamentally different keywords in Machine Learning. As you asked for unsupervised classification, I strongly recommend to make yourself familiar with these concepts to avoid falling into doing things without knowing its background which may result in not so meaningful results. Third, I assume two scenarios bellow and answer your question: You Have Those Labels i.e. climbing , idling , etc. Then you have a classification problem and the classification itself has nothing to do with being unsupervised. You can use unsupervised algorithms for feature engineering but the classification itself is supervised . In this case you have options which are OK with high dimensional data e.g. SVMs, Random Forests and ANNs. Write the basic architecture (in Python, Sci-kit Learn gives you proper tools to start) and do cross-validation and monitor your model. At this step it is too broad to discuss it. You Do Not Have Those Labels and you want to cluster data to different bunches of similar activities according to your variables. That is Unsupervised and yes, you can do that but you do not know out-of-the-box, which one is climbing and which one is idling at the beginning. In Case of 2 , you may cluster your data and check a few samples from each cluster and if you can manually label, label them according to different activities and assume the rest of cluster represents the same activities. Still your problem is unsupervised so soon or late you come to the question of how many clusters should I assume in my data? which is another issue. UPDATE So the problem is he second case. This is the solution: Firstly try EDA to understand your data. Dimensionality reduction is a good way to reduce the number of features to less but more informative one. I suggest starting with visualisation with UMAP or t-SNE and see how your data looks like (intuitively) Try dimensionality reduction algorithms and intuitively see the data. It gives you a rough idea of which algorithm to use. I suggest simple PCA, kernel PCA, LLE and UMAP as mentioned before Depending on the distribution of data and clusters you choose the proper algorithm. In case of well-separated gaussian clusters a simple k-means would do the job. DBSCAN is used when density of clusters are different. Spectral clustering is a computationally expensive but pretty nonlinear approach to clustering problem too Clustering has no evaluation You need to manually check your clusters and see if they make sense. This is not to get rid of answering! this is a fact that I see being mistaken frequently. There is no evaluation for clustering except of manually label some data and see if they are in right cluster! PS: KNN is a supervised algorithm and needs label. You mentioned in comments so I thought it is good to correct it here too. Good Luck!
