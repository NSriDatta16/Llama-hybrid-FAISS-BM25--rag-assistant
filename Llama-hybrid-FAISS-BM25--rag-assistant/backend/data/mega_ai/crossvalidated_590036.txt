[site]: crossvalidated
[post_id]: 590036
[parent_id]: 576360
[tags]: 
Going by the more general definition of latent space, then yes, since the latent values/activations of a neural network are also random variables, but these are values we don't observe directly unlike the input/output pairs. While those values don't have a formal characterization as a distribution, they can be considered as an unnormalized distribution. Therefore, training your network can be considered as implicitly optimizing the factorization of $P(y,x)$ with $p(y,x|l)$ where $l$ is a set of latent values/activations.
