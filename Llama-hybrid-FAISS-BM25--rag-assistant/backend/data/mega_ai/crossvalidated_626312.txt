[site]: crossvalidated
[post_id]: 626312
[parent_id]: 
[tags]: 
Do uncalibrated "probability" predictions satisfy Kolmogorov's axioms?

Let's say we have some binary variable of interest and fit a model to predict the probability of the two classes, say a logistic regression or a "classification" neural network. This model gives us predictions in the interval $[0, 1]$ . Must these predictions satisfy the Kolmogorov probability axioms, even if these predictions lack calibration? We definitely get that the values are non-negative, and I think having an upper bound $1$ gives us unit measure (but I am not as confident about this). For $\sigma$ -additivity, I have no idea.
