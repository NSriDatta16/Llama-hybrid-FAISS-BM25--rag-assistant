[site]: crossvalidated
[post_id]: 79493
[parent_id]: 79406
[tags]: 
It is not necessarily the case that you will always find significant differences as you increase sample size, but it becomes more and more likely. As several people have pointed out, truly identical samples may not result in a significant difference. What it does do is make very, very small differences much more likely to be detected - differences that we, in the real world, can't really act on in any meaningful fashion. For example, if I told you that the average IQ of one group was 100.0001 and the other group 100.0002, would you really be able to treat the second group as "smarter" (given all the caveats around IQ as a measure of intelligence)? I'll use an example from my own work: I was simulating an intervention in a hospital to help prevent patients from developing a particular disease. My data set was a number of simulated hospitals with treatment, and a number of hospitals without treatment. The difference between them was statistically significant, and strongly so. This was entirely because the "No Treatment" hospitals had a few examples with slightly more infections. But in most meaningful ways, the two arms were identical. They had the same median number of cases, the same minimum, the same 75th percentile, and 95th percentile and even 99th percentile number of cases. The significance was entirely driven by a few edge cases at the extreme end of the distributionâ€¦and a large sample size. The effect of the treatment was, in the real world, utterly undetectable and meaningless. But because I had a large sample size, it was statistically significant. If I had wanted it to be more so, I could have gone to dinner and let the simulation run longer, but that wouldn't have made the intervention any more effective.
