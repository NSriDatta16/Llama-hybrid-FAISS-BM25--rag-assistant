[site]: crossvalidated
[post_id]: 368532
[parent_id]: 
[tags]: 
What happens if weight sharing is avoided in convolutional neural networks?

I understand that one of the advantages of convolutional layers over dense layers is weight sharing. Assuming that memory consumption is not a constraint, would a CNN work better if a different kernel is trained for each patch of input? In other words, if instead of training a single kernel for all input patches, I train a different kernel for each patch, should I expect to get better or worse results?
