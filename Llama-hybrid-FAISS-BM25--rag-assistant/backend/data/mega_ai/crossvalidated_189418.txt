[site]: crossvalidated
[post_id]: 189418
[parent_id]: 
[tags]: 
How to find associations between Likert-type scale and numeric measurements?

I’m trying to statistically analyze possible associations between two sets of data. One comprises of Likert-type scale data which measured the opinion of a team member regarding other members' attitudes during a collaborative project. The other set are numeric measurements that were extracted from activity logs. 16 students worked in teams of 4 members to develop software together. They used tools capable of tracking different activities (e.g., number of modifications in files, number of messages sent to other team members via chat). I have extracted the activity logs of a 4 months period for each student. Students peer assessed each other member of their teams twice during the project. They assessed their peers by responding to Likert questionnaire items such as ‘how communicative was the student A during this phase of the project?’ — Not at all communicative; Slightly communicative; Somewhat communicative; Very communicative; Extremely communicative. My question is: how do I investigate possible associations between the two sets described above? My assumption is that some metrics, e.g. the number of messages sent via chat, will positively correlate with the respective peer assessment score obtained by a student regarding the quality of their communication with others, i.e. the higher the number of messages sent, the better the score of a student will be. From the first dataset, I have around 6 aspects that were peer assessed. I mentioned communication, but we also asked students to evaluate other characteristics such as leadership, technical knowledge etc. From the second dataset, I have around 20 different metrics. All of them are numbers based on the number of times a student performed each activity within a given period of time during the project. I’m hoping that this analysis will reveal some unexpected associations between (A) characteristics that were peer assessed and (B) counts that were extracted from the activity logs. Another related question is: each student was assessed by three other team members. I noticed some variation in the responses, which may be explained due to different opinions, but also due to social desirability bias, central tendency bias etc. Is it safe to summarize that in order to generate a single score for each student? If yes, what is the proper way to aggregate these assessments? Median? I have found different methods to do that, but this seems to be a controversial issue. Assuming that I solved the aggregation problem, I’d have to choose a proper statistical test in order to find and analyse possible associations between the peer assessment aggregated scores and the metrics I have extracted. My goal is to recommend a set of metrics that a teacher may pay attention to when assessing different aspects of teamwork and individual performance. What do you suggest? Which statistical tests should I perform to find and measure the strength of those associations? Thanks in advance for any help.
