[site]: datascience
[post_id]: 65291
[parent_id]: 
[tags]: 
How does backpropagation work with averaging layers?

I'm studying Word2Vec algorithm, and so far i understood that, in the case of input context bigger than 1 (so multiple words) we have our hidden layer that performs averaging between the inputs (as explained here: Word2Vec CBOW ) I didn't understand how does backpropagation work in this case. When i start backpropagating from the softmax output layer and i reach for the hidden layer in my process, am i coping the weight over all the " same position " inputs? To make my question clearer, if i have 2 context words coded as one hot vectors and i call the first weight of the first word as W11 and the first weight of the second word as W21 , when i'm backpropagating, those two weights will be updated with se same value, right?
