[site]: crossvalidated
[post_id]: 179979
[parent_id]: 179863
[tags]: 
There is lots of bad analysis in papers, it should not happen, but it is very common, so don't consider everything published right. However, it's also a good idea to assume that other people knows what they are doing. You are right that permuting features is weird. There is no reason to do that. Features are features for a reason, you cannot permute 'number of eyes' with 'weight' or anything like this. Just no. Permutation can be used. You might permute target labels as way of significance testing or you might permute order of your samples if it matters for your training algorithm (e.g SGD). There is a way creating additional data, sometimes small amount of noise is added, or you are imputing new in a space between other samples as in SMOTE or ROSE, however, that will not help you if you don't have enough data. It might help your model to behave nicely, but it will not give you more training data in a sense that you can get more information out of your training set. Yes, feature interaction is lost and those algorithms will not cope with that, because it just doesn't make sense. However, since you are probably using linear models, you are not really looking at feature interactions anyway, e.g. you cannot see XOR interaction. They don't have 240 instances they have 10!!! Whatever you do with your data, it's part of your model/training algorithm. So you have 10 training instances and youa re trying to learn some structure from your 10 instances, if you create synthetic data, they wills till be created only based on information from your 10 training data. There is big difference between 'not enough data' and 'course of dimensionality.' They often appear simultaneously, but they bring different problems. IF you don't have enough data, regardless of they dimensionality, you cannot do a proper holdout, therefore you cannot really check if you are overfitting. You might have model with good performance just by chance. It is therefore important to have model with as little things to tune as possible, because you don't have independent set for parameters tuning and every tuning is just opportunity for a bias. This is why you would stick to very simple (possibly linear) models, because you don't have to tune them as much, or at all. Course of dimensionality is a different problem. You might have billion pictures of size bilion*bilion pixels. It's a huge dimensionality problem, but you can have 300 milion images as your hold out testing set, and you can therefore do any crazy modelling schemes you want without a fear of overfitting, peeking, double dipping, biasing... With high dimensional space, relative distance between your samples is approximately same, therefore KNN will fail, other approaches will depends on your data, but now you can use kernells and hidden neurons. In high dimensional space with small sample sizes all linear classifiers works approximately the same. You might choose a classifier based on other desired properties, for example GPX if you want to have probabilistic prediction, SVM if you don't want to wait ages for GPC and so on.
