[site]: crossvalidated
[post_id]: 592601
[parent_id]: 592535
[tags]: 
Of course you can use predictive models on individuals. They wouldn't be very useful if you couldn't. Your statistics professor's position is so bizarre that I wonder if maybe something got lost in communication. For example, if we were to take your reporting of his comments at face value, we would have to conclude that he had never heard of prediction intervals, which seems unlikely for a professor of statistics. Formally, a statistical model model predicts a probability distribution for the random variable $Y$ that it is modeling. For example, an ordinary least squares model predicts: $$Y_i \sim \text{Normal}(\alpha + \beta x_i, \sigma),$$ where $\sim$ means "is distributed as". The $\alpha$ , $\beta$ , and $\sigma$ are all parameters of the model; finding them is what we mean by "fitting" the model. So, it's baked into the definition of the model that we can't predict an individual case with certainty. When people talk about the model "predicting" $\hat{y}_i$ for a case, they usually mean that $\hat{y}_i$ is the expected value of the predicted distribution. Depending on the circumstances that may or may not be a useful summary of the model's predictions, but it's not the whole story. For example, if you care about individual variation, you might compute a 95% prediction interval as $\hat{y}_i \pm 2\sigma$ , which gives an idea of the range of actual events you might commonly encounter. Note, by the way, that this prediction interval will generally be much larger than a "confidence interval" for $\hat{y}_i$ . The confidence interval tells you about the uncertainty in the estimate of the expectation value, which is something that is only relevant to populations. Now, machine learning practitioners often elide all of this and just talk about "the prediction" of the model. Depending on what those are trying to do, this might be a reasonable approach, or it might not. Sometimes you need a system that is going to take its best guess and move on; it all depends on what you are trying to accomplish. I happen to think that ML models should make probabilistic predictions more often than they do, so perhaps some criticism of data science is due there, but a blanket dismissal dramatically overstates the case. Most of your professor's other criticisms seem like red herrings to me. Obviously a model fit to data from a very limited population is only useful within that population. This comes as a surprise to nobody. And I can't even figure out what he's trying to say with his bridge analogy. One thing I think we can all agree on is this: it's good to give some thoughts to what your model's predictions actually mean and what their limitations are. It's fair to be skeptical of boiling a prediction down to a single number, as long as you recognize that sometimes that is the right solution for certain problems. The best way to use a statistical model is to think about what question you are trying to model, and consider all of the information returned by the model and how it might be used to answer that question. That last part demands judgement, and developing and exercising good judgement is where data scientists and statisticians alike earn their keep.
