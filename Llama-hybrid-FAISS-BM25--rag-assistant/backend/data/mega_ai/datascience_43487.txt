[site]: datascience
[post_id]: 43487
[parent_id]: 43485
[tags]: 
I'm pretty sure that your feature importances are 0 because your classifier isn't doing any classifying. From the code, it looks like you're training only on positive examples, and giving the fit function a label vector that consists entirely of 1s. The classifier has no information; the decision rule is just "when given an example, predict 1". There's no way to measure which features are most strongly associated with the label because they're all equally associated - there's only one label, so there's no way to associate the features with anything else. Is there a reason you're not using the negative examples? It seems like you have the dataframe available. When you ran naive bayes and logistic regression, did you also give those models only the positive examples?
