[site]: datascience
[post_id]: 20347
[parent_id]: 20170
[tags]: 
CG does not converge to the minimum as well as BFGS If I may add an answer here to my own question too, credits given to a good friend who volunteered to look at my code. He's not on Data Science stackexchange, and didn't feel the need to create an account just to post the answer up, so he's passed this chance to post over to me. I would also reference @Neil Slater, as there is chance his analysis on the numerical stability issue could account for this. So the main premise behind my solution is: We know that the cost function is convex, meaning it has no locals, and only a global minimum. Since the prediction using parameters trained with BFGS is better than those trained using CG, this implies that BFGS converged closer to the minimum than CG did. Whether or not BFGS converged to the global minimum, we can't say for sure, but we can definitely say that it is closer than CG is. So if we take the parameters that were trained using CG, and pass them through the optimization routine using BFGS, we should see that these parameters get further optimized, as BFGS brings everything closer to the minimum. This should improve the prediction accuracy and bring it closer to the one obtained using plain BFGS training. Here below is code that verifies this, variable names follow the same as in the question: # Copy the old array over, else only a reference is copied, and the # original vector gets modified theta_all_optimized_bfgs_from_cg = np.copy(theta_all_optimized_cg) for k in range(y.min(),y.max()+1): grdtruth = np.where(y==k, 1,0) results = minimize(compute_cost_regularized,theta_all_optimized_bfgs_from_cg[k-1,:], args = (X_bias,grdtruth,0.1), method = "BFGS", jac = compute_gradient_regularized, options={"disp":True}) # optimized parameters are accessible through the x attribute theta_optimized = results.x # Assign thetheta_optimized vector to the appropriate row in the # theta_all matrix theta_all_optimized_bfgs_from_cg[k-1,:] = theta_optimized During execution of the loop, only one of the iterations produced a message that showed a non-zero number of optimization routine iterations, meaning that further optimization was performed: Optimization terminated successfully. Current function value: 0.078457 Iterations: 453 Function evaluations: 455 Gradient evaluations: 455 And the results were improved: In[19]: predict_one_vs_all(X_bias, theta_all_optimized_bfgs_from_cg) Out[19]: 96.439999999999998 By further training the parameters, which were initially obtained from CG, through an additional BFGS run, we have further optimized them to give a prediction accuracy of 96.44% which is very close to the 96.48% that was obtained by directly using just only BFGS! I updated my notebook with this explanation. Of course this raises more questions, such as why CG did not work as well as BFGS did on this cost function, but I guess those are questions meant for another post.
