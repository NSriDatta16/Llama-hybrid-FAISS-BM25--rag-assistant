[site]: crossvalidated
[post_id]: 85613
[parent_id]: 85537
[tags]: 
The original question appears ill-posed, regarding part 1). For arbitrary regressor $X$ the stability condition may or may not be satisfied. The "hint" provided could imply a regressor $X=1,2,...$ which is not random, and for which obviously the stability condition is not satisfied, since the average moment "matrix" is a 2nd-degree polynomial in $n$ which does not converge. If indeed this is the regressor implied, then regarding speed of convergence we have \begin{align*} n^p(b-\beta) &= n^p\left(\frac 1n X'X\right)^{-1} \frac 1n X'\epsilon\\ &= \frac{6n^p}{(n+1)(2n+1)} \cdot \left(\frac 1n\sum_{i=1}^ni\epsilon_i\right)\\ &=\frac{6n^{p+1/2}}{(n+1)(2n+1)} \cdot \left(\frac 1{\sqrt n}\sum_{i=1}^n(i/n)\epsilon_i\right)\end{align*} Now the last term converges to a zero-mean normal (see for example Hamilton's "Time-Series Analysis", the chapter about Deterministic Time Trends), while in order for the first to converge to a finite non-zero constant we need $p+1/2 = 2$ (to match orders of magnitude of numerator and denominator). So $p=3/2$ and so the speed of convergence is $n^{3/2} = n\sqrt n$. Part 2) can be worked analogously. ADDENDUM The sequence $\{(i/n)\epsilon_i\}$ is a martingale difference (m.d.s) because a) $E[(i/n)\epsilon_i] = 0 $ and b) $E[(i/n) \epsilon_i\mid \epsilon_{i-1}, \epsilon_{i-2},... ] = 0$ In order then for $\frac 1{\sqrt n}\sum_{i=1}^n(i/n)\epsilon_i$ to obey the Central Limit Theorem we need more over the following conditions (denote $\sigma^2$ the variance of $\epsilon$ and $v_i$ the variance of $(i/n)\epsilon_i$): c) $v_i=E[(i/n)\epsilon_i]^2>0$ for all $i$ d) $\frac 1n \sum_{i=1}^nv_i \rightarrow v>0$ e) $E|(i/n)\epsilon_i|^r 2$ and all $i$ f) $\frac 1n \sum_{i=1}^n[(i/n)\epsilon_i]^2 \rightarrow v$ Condition c) is obviously satisfied. For condition e) assume that the 4th moment of $\epsilon$ exists and it is finite (usually an innocuous assumption related to the real world phenomenon under study), set $r=4$ and the condition is satisfied. For condition d) we have $$\frac 1n \sum_{i=1}^nv_i = \frac 1n \sum_{i=1}^nE[(i/n)\epsilon_i]^2 = \frac {\sigma^2}{n^3} \sum_{i=1}^ni^2 = \frac {\sigma^2}{n^3}\frac{1}{6}n(n+1)(2n+1) \rightarrow \frac {\sigma^2}{3}$$ so condition d) is satisfied. I 'll leave condition f) unproven, it needs a roundabout way -but it holds, as demonstrated in the reference I have provided. Behind these technical requirements the intuition is the usual one associated with the CLT: the variance does not explode, neither does it tend to zero (although this intuition is just a step in understanding - there exists the generalized CLT for processes with infinite variance).
