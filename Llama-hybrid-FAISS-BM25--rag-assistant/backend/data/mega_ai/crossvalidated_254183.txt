[site]: crossvalidated
[post_id]: 254183
[parent_id]: 253333
[tags]: 
$\blacksquare$1.Whether this(the method of SDE solving kernel problem) is extensible to the reducible nonstationary kernels? Let me restate your question. The method your pointed out in [Hartikainen&Särkkä] is to regard the covariance(OR its spectral form) as a solution to stochastic differential equation. Your question is that if $k(t,t')$ is not a stationary kernel, whether it can still be expressed as a solution to SDE. The answer is NO in general. Because kernels can be regarded as inner products on the state space families indexed by the time variable $t\in T$, if you have a nonstationary kernel i.e. $k(t,t')\not\propto |t-t'|$ then you are saying that the families of inner products vary with time. So the best possibility for general kernels is that you get a family of SDEs, whose coefficients varying with time variable $t$, that at each fixed time $t_0$ you get a SDE with $Spectral_{k}(t,t')$ as solution at this specific time point. But I guess for special family of kernels $\color{red}{see\, my\, update}$, say the kernel is reducible in sense of [Genton], this result can be extended with coefficient of SDEs varying with the connecting function $\phi$ you pointed out in your OP. $\blacksquare$2.Are there any benefits if you have a reducible kernel? First this is not a "reducible kernel" as I usually used in a representation morphism, it is "stationary reducible kernel" defined as the author in [Genton] said: We say that a nonstationary kernel $K(x, z)$ is stationary reducible if there exist a bijective deformation $\phi$ such that: $K(x, z) = K^{*}_{S}(\phi(x) − \phi(z))$ The importance of studying such a kind of kernel traces back to [Sampson&Guttorp], which use a thin-plate spline to recover the missing dimension used to recover stationary-ness. The main idea is that lower dimension nonstationary kernel must be a projection of stationary kernel in higher dimension. There are two basic drawbacks of this idea in practice, one drawback is scale-sensitive; the other is the difficulty of separating the noise and mean. There are earlier motivations of this idea from mathematics but [Sampson&Guttorp] is the first literature that brings this idea to statisticians. Reference [Hartikainen&Särkkä]Hartikainen, Jouni, and Simo Särkkä. "Kalman filtering and smoothing solutions to temporal Gaussian process regression models." 2010 IEEE International Workshop on Machine Learning for Signal Processing. IEEE, 2010. [Genton]Genton, Marc G. "Classes of kernels for machine learning: a statistics perspective." Journal of machine learning research 2.Dec (2001): 299-312. [Karlin&Taylor]Karlin, Samuel, and Howard E. Taylor. A second course in stochastic processes. Elsevier, 1981. [Sampson&Guttorp]Sampson, Paul D., and Peter Guttorp. "Nonparametric estimation of nonstationary spatial covariance structure." Journal of the American Statistical Association 87.417 (1992): 108-119. Update My knowledge on nonstationary kernels is mainly from spatial modeling and group representation theory instead of "machine learning", so here is my best knowledge but possibly a bit different from what you want. One highly cited "geological" paper that explained the mathematical difficulty of deriving such a family of SDE indexed by time variable is [Neuman]. Nonstationary covariance function has a special problem of differentiability when we try to obtain a solution family the corresponding SDE family in classical way(It is easily understood because you cannot expect the spectral function to be regular if the underlying manifold defined by the kernel process is not smooth). To visualize this point you can imagine that a stochastic process without smooth path. One well-known example that the covariance kernel is not only nonstationary but also nonsmooth is given by setting up a Dirichlet process as prior in $M(\mathcal{X})$, the space of a measures on a Polish sample space $\cal{X}$, whose path is almost nowhere smooth. If the (posterior) covariance is determined by a Dirichlet process, say in a Bayesian nonparametric setting like [Gelfand et.al], then the resulting covariance kernel is not smooth let alone stationary. This example will correspond to a local velocity field which is also not smooth. Hence it is expected that such a covariance kernel is nonstationary. If the resulting $K(t,t')$ is not smooth, then the same differentiability problem arise when we try to figure out the connecting function $\phi$ or $\phi_t$(varying with $t$). It is a practice (Disclaimer: I do not know whether it is common or not in statistical community!) that sometimes we want to assume sort of "local differentiability" for $K$ or $\phi$ in order to solve the SDE for a fixed $t_0$. Moreover if you assume some smoothness then you are actually assuming some sort of (local) invertibility, you cannot assume non-invertible smooth kernels by implicit function theorem . So your comment to @Dougal seems a bit confusing to me. One method of overcoming this is to define weak derivatives and try to solve it as PDE by regarding the time variable as a new variate; another method is aforementioned by [Sampson&Guttorp] and later by Perrin as we discussed above. As for simple example, I'm afraid that there is no a simple/non-pathological example using Gaussian process since its path is smoothly differentiable everywhere. One example for "special family" I mentioned in my answer to Question 1 above is given in [Solin&Särkkä]. A question for you: what is your motivation of asking such a problem from viewpoint of "machine learning"? Thanks, this is a good discussion! Update Reference [Paciorek&Schervish] Paciorek, Christopher J., and Mark J. Schervish. "Spatial modelling using a new class of nonstationary covariance functions." Environmetrics 17.5 (2006): 483-506. [Neuman]Neuman, Shlomo P. "Eulerian‐Lagrangian theory of transport in space‐time nonstationary velocity fields: Exact nonlocal formalism by conditional moments and weak approximation." Water Resources Research 29.3 (1993): 633-645. [Solin&Särkkä]Solin, Arno, and Simo Särkkä. "Explicit Link Between Periodic Covariance Functions and State Space Models." AISTATS. 2014.
