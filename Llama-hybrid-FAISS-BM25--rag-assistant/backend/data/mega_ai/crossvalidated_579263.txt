[site]: crossvalidated
[post_id]: 579263
[parent_id]: 579262
[tags]: 
"I can trust the standard errors of my coefficients and their accompanying p-values" .... well, yes, at least in the limit as $n\to\infty$ , as long as it's only the distributional assumption that was an issue (heteroskedasticity is a very common issue when you have strong skewness, for example, and that problem doesn't go away with large sample size; similarly issues like serial correlation and other dependencies). You don't necessarily know at any specific finite $n$ that the normal approximation will be suitably close for your purposes unless you know the (conditional) population distribution. [If you do have a pretty decent idea of that distribution you can simulate to assess properties like type I error rates, distributions of p-values under $H_0$ , etc under any estimator you happen to wish to use.] Note also that approximate correctness of significance levels (and hence, of p-values) is not the only consideration in a test; that is generally it's not just the accuracy of the type I error rate that's an issue for hypothesis testers, but also relative power. Specifically, if your sample was large because you want to pick up small effects, you don't want to waste power on potentially inefficient tests if you have a suitable distributional assumption that would let you avoid such a loss of power from what you might get with a better model. If so, how do I know I have a large enough sample? You don't, in general; note in particular that the assumptions relate to the population you're supposedly getting a random sample from, not the specific details that happen to occur in the sample. However, you may have some notion of the behavior of your response variable (typical skewness, tendency to heavy tails etc) from outside the sample - other data, theoretical considerations, expert knowledge etc. Alternatively you might choose to split off a largish chunk of your data at random to make such determinations about the behaviour of the estimators you're using (perhaps via bootstrapping, for example, if you have decent sample size). There are other ways to get close to your desired type I error rate, $\alpha$ . If you're testing the overall regression (or just testing the slope in a simple regression), then you can do a permutation test. More generally in large samples there's bootstrap tests (e.g. ones based on residuals or ones based on resampling rows), but you can have some issues to deal with to maintain the control on type I error quite as well; it's not always quite as simple as a simple permutation test. Or maybe the better question is, what's best practice if my goal is inference and I want to conduct a proper hypothesis test on the validity of predictors What hypothesis test is testing "validity"? (To be honest I'm not even clear what "valid" means in this sentence) The tests I thought you were asking about would typically be tests of whether coefficients differed from zero (or some other specific hypothesized value); predictors may be "valid" in typical senses of the word even if the relationship is extremely weak (but perhaps not very informative) and my residuals are highly abnormal. "non-normal" would be the more usual expression in this context. Beware looking at the marginal distribution of residuals without being pretty confident that the other aspects of the model are very close to correct (e.g. suitability of a linear model in the regression context; homoskedasticity; independence). Again, keep in mind that the "correctness" of significance levels / p-values is based on an assumption about the population rather than the particular idiosyncracies of a single sample. But not only that; for type I error rates and p-values the assumption about the population is relevant to the (likely counterfactual) situation when $H_0$ is true . That is, very often $H_0$ is going to be false (whether or not you manage to reject it), and so in that case the sample may not always be particularly relevant to the assessment of the assumption (e.g. if it were that case that the conditional distribution of the response became more skew when H0 was false, perhaps increasing with effect size, the skewness in the sample might mislead you about type I error rates, which are only under $H_0$ ). That all said, in largish samples (many hundreds say and plenty of observations per parameter, and no very high leverage values), a moderate amount of skewness or discreteness etc in residuals would not concern me much (if there were no other issues). Typically if I undertake a regression (say), I'd have a good idea of the likely level of skewness and other expected behavior before I saw any data and have made suitable plans for my analysis rather than be in the dangerous position of trying to work out what to do after I discover a potential issue (which has the serious problem of choosing what you do based on what you see in the data you're doing it with; if you worry about $\alpha$ and $\beta$ in your tests and bias in your estimates and standard errors, the potential impact of such data-based strategies should concern you).
