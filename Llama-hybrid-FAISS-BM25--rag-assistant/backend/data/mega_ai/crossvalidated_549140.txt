[site]: crossvalidated
[post_id]: 549140
[parent_id]: 
[tags]: 
Is it a valid technique to exclude features based on occuring very rarely? Or would it be better to do feature selection on subsets?

I have a large sparse binary dataframe with codes from health data. The train columns are one-hot-encoded ICD Codes, OPS Codes, ATC Codes, and other covariates such as age etc. (scaled to be between 0, 1) My y values are binary: has a stationary hospital case in the following year for each insurant The training dataset has 162153 datapoints, the test dataset has 69495 datapoints. The code occurance is extremely sparse, 11861 out of the total 26315 codes (across all code types) occur 3 times or less in the entire train set. I want to use logistic regression with lasso for selecting the top features. However, having all codes in the dataframe, this takes a long time to run. Would it be a valid approach to discard all codes which occur 3 times or less? It makes sense because they wouldnt be very predictive overall, and could be predictive due to randomness only if they occur so rarely. However, I couldn't find any information on that technique online. My other idea would be to use logistic regression with lasso feature selection for each code type (ATC, ICD) individually, then put the selected codes into one dataframe and do the code selection again. Would this be a good approach? (I thought about using PCA, however it is not applicable in this project since I need to provide a list of selected Codes used in the model which are important for prediction. The goal is still to predict whether someone will be in the hospital, but they want to know which codes are relevant as a side information)
