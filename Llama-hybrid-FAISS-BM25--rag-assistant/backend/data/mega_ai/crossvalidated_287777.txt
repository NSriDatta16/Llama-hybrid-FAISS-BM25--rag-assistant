[site]: crossvalidated
[post_id]: 287777
[parent_id]: 
[tags]: 
CART vs. Random Forests: How do random forests prevent greediness?

as I've been learning about tree-based classification algorithms, I've learned the single tree algorithms are 'greedy', meaning that they'll minimize entropy at every stage. This means that sometimes these algorithms will fail to minimize total entropy because they are only looking one step ahead. Here's an example: In this case, the picture above represents the 'ideal split'. That is, every square is homogeneous. However, a decision tree algorithm might make the following split: This is because the algorithm will split the data in half along the x axis to minimize initial information gain, without considering future information gain. Now, my question is as follows: I'm trying to understand the random forest algorithm, and I was wondering if someone could explain how the algorithm would function in this situation. How would a random forest be able to find this 'optimal split'.
