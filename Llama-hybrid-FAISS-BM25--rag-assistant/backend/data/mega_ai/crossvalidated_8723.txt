[site]: crossvalidated
[post_id]: 8723
[parent_id]: 6920
[tags]: 
You can always just perform gradient descent on the sum of squares cost $E$ wrt the parameters of your model $W$. Just take the gradient of it but don't go for the closed form solution but only for the search direction instead. Let $E(i; W)$ be the cost of the i'th training sample given the parameters $W$. Your update for the j'th parameter is then $$W_{j} \leftarrow W_j + \alpha \frac{\partial{E(i; W)}}{\partial{W_j}}$$ where $\alpha$ is a step rate, which you should pick via cross validation or good measure. This is very efficient and the way neural networks are typically trained. You can process even lots of samples in parallel (say, a 100 or so) efficiently. Of course more sophisticated optimization algorithms (momentum, conjugate gradient, ...) can be applied.
