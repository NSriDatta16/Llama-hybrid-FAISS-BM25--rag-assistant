[site]: crossvalidated
[post_id]: 594122
[parent_id]: 
[tags]: 
Why was Bayes better on exponential model rather than log-linear model?

I wanted to train a Bayesian version of this model which we can consider to be this log-linear form. $$\ln \text{PSI} = \alpha \text{Time} + \beta.$$ Here are the priors I guessed for $\alpha$ and $\beta$ . $$\alpha \sim \mathcal{N}(-0.04, 1)$$ $$\beta \sim \mathcal{N}(0, 1)$$ Here are samples from the posterior of these parameters. What I noticed was that $\alpha$ is roughly centered on zero, and had a mean of 0.028. The original data plot (along with an OLS fit) strongly suggests a (small) negative slope. Since the sample size is small, it isn't altogether unreasonable combined with my choice of priors that non-negative slopes were not ruled out. It just made me think twice about how to choose priors. What I could not readily explain is when I instead trained the model $$\text{PSI} = \exp \left(\alpha \text{Time} + \beta\right)$$ I got much more precise estimates of the parameters, and their sampled averages were closer to the OLS solution. Clearly the difference is due to the change in variables. My guess is it has something to do with the derivatives with respect to the parameters being larger in the exponential model. But I have not made this reasoning explicit or complete on how it affects the training via the no-U-turn sampler (NUTS) . Why is the exponential model so much more precise and accurate when given the same priors and data than the log-linear model? Here is a Python gist for the code that trains the exponential form. Here is a scatter plot of PSI against time elapsed since pressurizing. And here is a plot of the residuals of $\text{PSI} = \exp(-0.04 \text{Time} + 2.86)$ , suggesting some trend in the errors. Here is a plot of the residuals against fitted:
