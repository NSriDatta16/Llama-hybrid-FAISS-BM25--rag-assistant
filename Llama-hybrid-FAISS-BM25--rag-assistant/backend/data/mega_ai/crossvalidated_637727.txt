[site]: crossvalidated
[post_id]: 637727
[parent_id]: 477627
[tags]: 
I think it's useful to understand Autoencoders by first considering a linear autoencoder (ie a single linear layer for each of encoding and decoding networks). Minimising the reconstruction error using n bottleneck nodes (with input of N>n) then corresponds to projecting to the subspace spanned by the first n principal components (see dimensionality reduction section of PCA Wiki ). So if you are familiar with principal components analysis this should give you an insight on how and when they are useful. regular autoencoders allow to identify nonlinear relationships too so eg if your typical data is 2 dimensional of the form $(x, x^2)$ + noise then the autoencoder can extract the 'x' from both inputs in a single bottleneck node and then reconstruct $(x,x^2)$
