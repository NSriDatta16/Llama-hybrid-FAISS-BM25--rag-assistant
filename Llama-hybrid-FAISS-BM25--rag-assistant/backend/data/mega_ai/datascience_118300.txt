[site]: datascience
[post_id]: 118300
[parent_id]: 118121
[tags]: 
I am wondering if this is because I am using a general XLSR model without fine-tuning it for emotion recognition. That might be still true but your approach contains a fundamental error you should eliminate first. You are using the class Wav2Vec2FeatureExtractor to extract the features from an audio file, but this class is not a neural network. It is a preprocessor that pads and normalizes the floating point time series from librosa. As stated by the documentation the normalization makes sure that the array has: zero mean and unit variance These features, will therefore only makes sense for the model that was trained with it. When you trained an SVM with it, you actually compared if an SVM can beat wav2vec2 and not if wav2vec2 is better than MFCC! To get the actual embeddings from the wav2vec2 model, you can use the following code: import librosa import torch from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model input_audio, sample_rate = librosa.load("/content/bla.wav", sr=16000) model_name = "facebook/wav2vec2-large-xlsr-53" feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name) model = Wav2Vec2Model.from_pretrained(model_name) i= feature_extractor(input_audio, return_tensors="pt", sampling_rate=sample_rate) with torch.no_grad(): o= model(i.input_values) print(o.keys()) print(o.last_hidden_state.shape) print(o.extract_features.shape) Output: odict_keys(['last_hidden_state', 'extract_features']) torch.Size([1, 1676, 1024]) torch.Size([1, 1676, 512]) Please refer to this StackOverflow post for the difference between last_hidden_state and extract_features . As you can see, the features are multi-dimensional for my file ([bacth_size, seq_len, hidden_size]), which means you probably want to apply some pooling (e.g. mean). P.S.: Another point that comes to my mind when I look at your question, is if those embeddings are actually meaningful by themselves. For the pure BERT, we know that the sentence embeddings: I'm not sure what these vectors are, since BERT does not generate meaningful sentence vectors. I assume you will probably need to fine-tune wav2vec2 a bit. You can use huggingfaces Wav2Vec2ForSequenceClassification class for that.
