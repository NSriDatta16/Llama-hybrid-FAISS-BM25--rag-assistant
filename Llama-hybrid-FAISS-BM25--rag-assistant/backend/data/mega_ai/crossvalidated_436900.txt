[site]: crossvalidated
[post_id]: 436900
[parent_id]: 436878
[tags]: 
Indeed, there are many claims that large batch sizes do just as well, and just as many claims on the other side. Basically, it doesn't look like there's any consensus on the matter yet. For either side you can find as much theoretical justification as you'd like. (Obviously these aren't "actual" mathematical contradictions, since different authors start from different assumptions.) These two facts seems to contradict one another, and maybe it's because of my major or something, and it's hard for me to just "accept" this fact purely on the test results. I like to have theoretical justification as much as anyone, but neural networks is one place where the theory hasn't really caught up, and when the empirical evidence contradicts what theory suggests, it's probably a better idea to go with the evidence. Priya Goyal et al. "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour" https://arxiv.org/abs/1706.02677 Elad Hoffer et al. "Train longer, generalize better: closing the generalization gap in large batch training of neural networks" https://arxiv.org/abs/1705.08741 Christopher J. Shallue et al. "Measuring the Effects of Data Parallelism on Neural Network Training." https://arxiv.org/abs/1811.03600 Nitish Shirish Keskar et al. "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima" https://arxiv.org/abs/1609.04836 Tao Lin et al. "Don't Use Large Mini-Batches, Use Local SGD" https://arxiv.org/abs/1808.07217 Dominic Masters and Carlo Luschi. "Revisiting Small Batch Training for Deep Neural Networks." https://arxiv.org/abs/1804.07612
