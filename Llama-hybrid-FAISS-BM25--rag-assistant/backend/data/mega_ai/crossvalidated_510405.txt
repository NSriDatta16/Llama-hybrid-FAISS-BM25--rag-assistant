[site]: crossvalidated
[post_id]: 510405
[parent_id]: 243000
[tags]: 
I found this page in a search, because I had the same question, but I think I have figured out what's going on. First, a demonstration of the problem: import numpy as np import statsmodels.api as sm import matplotlib.pyplot as plt x = np.arange(1000.,1030.,1.) y = 0.5*x X = sm.add_constant(x) plt.plot(x, y,'x') plt.show() mod_ols = sm.OLS(y, X) res_ols = mod_ols.fit() print(res_ols.summary()) Notice the very high condition number of 1.19e+05. This is because we're fitting a line to the points and then projecting the line all the way back to the origin (x=0) to find the y-intercept. That y-intercept will be very sensitive to small movements in the data points. The condition number takes into account high sensitivity in either fitted parameter to the input data, hence the high condition number when all of the data are far to one side of x=0. To solve this, we simply center the x-values: x -= np.average(x) X = sm.add_constant(x) plt.plot(x, y,'x') plt.show() The condition number is now greatly reduced to only 8.66. Notice that the fitted slope and calculated R**2 etc. are unchanged. My conclusion: in the case of regression against a single variable, don't worry about the condition number UNLESS you care about the sensitivity of your y-intercept to the input data. If you do, then center the x-values first.
