[site]: crossvalidated
[post_id]: 615339
[parent_id]: 
[tags]: 
Trade-offs when building VAR models

I am trying to build my first VAR model, consisting of three time series, for forecasting and have gotten quite far. I have made all the tests and comparing models indcluding different lags, different adjustments, inclusion of dummy variables, and different orders of differencing the variables. But I really do not know how to value the different measures of strength or validity that have been suggested to me. They seem sometimes to point in different directions. For example, after choosing a model we can check for Granger causality and we can look for residual correlation (autocorrelation). While plotting and playing around with the parameters I have noticed the following: Two of the series have strongly correlated errors and are likely to be cointegrated given a Johansens measure. For this I intend to look further into a VECM model. However, the first difference for one of the series shows apparent heteroskedasticity. I have tried to get rid of this using different levels of lambda in a BoxCox transformation of this first difference, and also using ordinary normalization (using the function normalize() from the BETS package in R). This has been unsuccesful in two ways: 1. The transformed series looks awkward - the heteroskedasticity is gone but the left half is everywhere above zero and the right half everywhere below zero using the recommended lambda, and no other value of lambda yields any better results, and 2. When I use this transformed variable then the Granger causality, that was earlier very clear between the two series, has now completely disappeared (p-value went from 0,004 to 0,7). At the same time, the residual correlation that was earlier 0,5 is now completely gone. This seems intuitive given that the granger causality has disappeared. However, what measure should I prioritize here? Is Granger causality more important than reducing the residual correlation? Another issue is that when I include a time dummy for one series (the third one) where there is an apparent structural break in levels, this clearly removes all the residual correlation between this variable and my main variable. But (!) only when I use the original, non-differenced variables. When I use the differenced series, the dummy doesnt reduced the correlation as much. So which method would you prefer here? A VAR model with or without differenced variables, where the non-differenced model including the dummy yields the lowest residual correlation of all models, or the model with first-differenced variables, simply because of following the recommendation to use differenced variables? What measures of validity of a VAR model for forecasting is most important? How would you rank the different measures? And now I only mean the measures that are important in model selection, and not the forecasting evaluation measures such as RMSE, MAPE, etc.? Any input is appreciated! Edit: More concisely, what is not clear to me is what measures to trust the most when building a VAR model. Is the existence of Granger causality more important than the non-existence of residual correlation? I ask because I noticed that, after Box-Cox transforming one of the variables, residual correlation was reduced but so was Granger causality.
