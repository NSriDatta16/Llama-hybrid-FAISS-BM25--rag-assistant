[site]: crossvalidated
[post_id]: 335848
[parent_id]: 335840
[tags]: 
Like any transformation, the SMOTE process has to take place in the INNER folds. That is, you need to apply SMOTE in the pipeline you want to test with a hyperparameter grid. I'm not as familiar with R as I am with python pipelines, but essentially you would have the a scheme such as: pipe = Impute -> SMOTE -> RandomForest clf = GridSearchCV(pipe, grid, inner_cv) nested_scores = cross_val_score(clf, outer_cv) So you can see that you have a clean hold out in the outer cv, but also a hold out from SMOTE in each grid search. Your grid search will be overly optimistic, but it at least prevents your oversampled cases from contaminating your grid searched test folds. That's the real danger with oversampling. Your outer hold out sets will give you your realistic, although slightly pessimistic, scores. In my experience, Random Forests are somewhat resilient to imbalance, but it would depend on the dataset I suppose. With an extreme imbalance like you have I can see the need for oversampling. Have you also tried a svm with a weighted class score? That eliminates the need for oversampling and I find that they do a comparable job to Random Forests and XGBoost.
