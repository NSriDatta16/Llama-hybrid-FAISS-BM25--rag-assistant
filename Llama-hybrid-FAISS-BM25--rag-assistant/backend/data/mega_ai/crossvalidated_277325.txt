[site]: crossvalidated
[post_id]: 277325
[parent_id]: 277310
[tags]: 
First of all, you should keep labelling your data by hand --maybe a rough labelling first-- to train your neural network. As you have, 500*4 = 2000 features in the input space (4 variables per profile , and each profile has 500 points right?)! So 70 examples to train on is very little to learn something in such high-dimensional space...(and this holds for linear methods too, you will need a good regularization anyway). Even if the neural networks might give you better final results after a bit of improvements, you will most likely gain better insights by trying to get good results with SVM let's say. And after all, if you manage to get good results with SVM and good insights in the data, then it may benefit for your NN approach. So my main advise will be to reduce the dimensionality "by hands" before doing the classification (I say "by hands" to discard neural networks). Namely try to get features from your profiles that will help you classify them as scarp or no scarp : (here just random examples to foster ideas): mean eastward length of all bouts overall eastward length variance of the bouts ratio of number of eastward over flat etc... And all the above for each sub-classes in the profile (all the above collapses the spatial information you have, a bit like in your dataset 2, keep this in mind!). Then you will be able to see which feature helped. Other than that you might use data-driven dimensionality reduction like PCA , or if you want to keep the spatial information of the profile you could think of other methods too (convolution with specific kernels?). But all this will make the subsequent classification faster, lighter and more interpretable . Finally it will surely give you ideas on how to improve the neural network (e.g. which architecture to use!).
