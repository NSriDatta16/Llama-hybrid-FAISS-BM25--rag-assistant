[site]: crossvalidated
[post_id]: 82508
[parent_id]: 82503
[tags]: 
I suppose that, if you were going to do this with $k$ going to infinity, you would obtain the linear regression model you obtain by doing ordinary linear regression with the full sample. Just notice that the average of $k$ structurally equal linear models is again a structurally equal linear model, simply with the parameters averaged (use distributive law). But I didn't do the math and I'm not completely sure. And here is why it isn't as attractive to do the "random"-thing with linear models as it is with decision trees: A large decision tree created from a large sample is very likely to overfit the data, and the random forest method fights this effect by relying on a vote of many small trees. Linear regression on the other hand, is a model that is not very prone to overfitting and thus isn't hurt by training it on the complete sample in the beginning. And even if you have many regressor variables, you can apply other techniques, such as regularization, to combat overfitting.
