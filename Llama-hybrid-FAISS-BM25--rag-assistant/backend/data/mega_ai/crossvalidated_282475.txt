[site]: crossvalidated
[post_id]: 282475
[parent_id]: 
[tags]: 
EM algorithm maximize which objective function

In Wikipedia EM algorithm section of Gaussian Mixture examples , there are two likelihood functions: incomplete-data likelihood function $L(\theta;\mathbf{x})$ complete-data likelihood function $L(\theta; \mathbf{x},\mathbf{z})$ which do we optimize when performing EM algorithm, why? I was told by the tutor we optimize formula 2 as formula 1 won't give us the right clustering results. I don't fully agree with what he said. Sure we are optimize formula 2, I agree on that. But the reason is that we are not able to set derivative to zero directly for formula 1 according to Andrew Ng's notes (last paragraph on page 1) . However, there's a way to get us out of the difficulty, using Jensen's inequality (page 3) , that is, applying Jensen's inequality to formula 1. Calculations on formula 1 shows that formula 2 is indeed an intermediate step of formula 1. So formula 1 also can provide us the same results/right clustering as formula 2 as well. This is my understanding of the relations between formulas 1 and 2, please correct me if I've missed anything.
