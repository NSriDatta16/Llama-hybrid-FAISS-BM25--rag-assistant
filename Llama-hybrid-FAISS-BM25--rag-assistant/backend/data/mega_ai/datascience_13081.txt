[site]: datascience
[post_id]: 13081
[parent_id]: 5357
[tags]: 
Scalable Machine Learning Solutions for Big Data: I will add my $.02 because there is a key area that seems not to have been addressed in all of the previous posts - machine learning on big data ! For big data, scalability is key, and R is insufficient. Further, languages like Python and R are only useful for interfacing with scalable solutions which are usually written in other languages. I make this distinction not because I want to disparage those using them, but only because its so important for members of the data science community to understand what truly scalable machine learning solutions look like. I do most of my work with big data on distributed memory clusters . That is, I don't just use one 16 core machine (4 quad core processors on a single motherboard sharing the memory of that motherboard), I use a small cluster of 64 16 core machines. The requirements are very different for these distributed memory clusters than for shared memory environments and big data machine learning requires scalable solutions within distributed memory environments in many cases. We also use C and C++ everywhere within a proprietary database product. All of our high level stuff is handled in C++ and MPI, but the low level stuff that touches the data is all longs and C style character arrays to keep the product very very fast. The convenience of std strings are simply not worth the computational cost. There not many C++ libraries available which offer distributed, scalable machine learning capabilities - MLPACK . However, there are other scalable solutions with APIs: Apache Spark has a scalable machine learning library called MLib that you can interface with. Also Tensorflow now has distributed tensorflow and has a C++ api . Hope this helps!
