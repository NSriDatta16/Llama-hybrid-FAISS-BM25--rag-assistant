[site]: crossvalidated
[post_id]: 348182
[parent_id]: 318132
[tags]: 
I would say no, because the ideal batch size is not determined by the size of the dataset but by computational efficiency and learning efficiency. The most efficient use of data to learn (based on number of elementary calculations) would be to use batch sizes of 1 with an appropriately low learning rate. The information from each sample is very noisy, but used with maximum efficiency. The purpose of minibatches is to take advantage of the parallelism available in the hardware being used. If you compare the run time for different batch sizes, you will find that very small batch sizes are very slow compared with ones that are a bit bigger, but large batch sizes offer no additional speed (and learn more slowly). So a good choice is to pick a batch that takes significantly under twice the time to run as a batch half the size, but is not much slower per sample than a large batch. With GPUs the same principles apply, but the ideal batch size will depend on how much memory the model uses. Here too you can empirically determine the time taken per sample for a wide range of batch sizes and pick a size that is a bit smaller than the one with maximum samples per second (this is determined by the ratio of the size of the GPU memory to the size of the model). See Deep Learning by Goodfellow et al section 8.1.3
