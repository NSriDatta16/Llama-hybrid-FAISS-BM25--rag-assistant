[site]: datascience
[post_id]: 123304
[parent_id]: 
[tags]: 
Handling Missing values in data

I have a Machine Learning task wherein I have to predict a target variable using an input. However, my dataset has a considerable percentage of missing values. What is the best approach to handle it? On running the following script, I calculate the percentage of missing values in each input feature. # Calculate the percentage of null val# Calculate the percentage of null values column-wise null_percentage = (df.isnull().sum() / len(df)) * 100 sorted_null_percentage = null_percentage.sort_values(ascending=False) pd.set_option('display.max_rows', None) print(sorted_null_percentage.values) pd.reset_option('display.max_rows') And the percentages are as follows:- [9.99116433e+01 9.99116433e+01 9.93788378e+01 9.93788378e+01 9.93788378e+01 9.93788378e+01 9.14593223e+01 8.79589399e+01 7.71387651e+01 7.66249457e+01 7.07246898e+01 7.07246898e+01 6.74590502e+01 6.74590502e+01 6.74590502e+01 6.74590502e+01 6.74590502e+01 6.74590502e+01 6.74590502e+01 6.74590502e+01 6.74590502e+01 6.74590502e+01 6.60857853e+01 6.56559071e+01 6.37467704e+01 6.20709048e+01 6.20709048e+01 5.54769354e+01 5.37712415e+01 5.26348044e+01 5.26348044e+01 5.24136348e+01 4.78674941e+01 4.78674941e+01 4.78674941e+01 4.78674941e+01 4.78674941e+01 3.52821894e+01 3.37223174e+01 3.15739325e+01 2.07080013e+01 2.07080013e+01 2.07080013e+01 2.07080013e+01 2.07080013e+01 2.07080013e+01 2.07080013e+01 1.80212916e+01 1.67438159e+01 1.67438159e+01 1.55705758e+01 1.24338654e+01 1.24338654e+01 7.70811689e+00 7.70811689e+00 5.47079480e+00 5.46558628e+00 4.99510302e+00 4.99510302e+00 4.81371267e+00 3.28626920e+00 6.60295546e-01 2.68831228e-01 3.36141242e-02 1.38593872e-02 1.38593872e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
