[site]: crossvalidated
[post_id]: 172839
[parent_id]: 21152
[tags]: 
Some time ago I had to justify a RF model-fit to some chemists in my company. I spent quite time trying different visualization techniques. During the process, I accidentally also came up with some new techniques which I put into an R package ( forestFloor ) specifically for random forest visualizations. The classical approach are partial dependence plots supported by: Rminer (data-based sensitivity analysis is reinvented partial dependence), or partialPlot in randomForest package. I find the partial dependence package iceBOX as an elegant way to discover interactions. Have not used edarf package , but seems to have some fine visualizations dedicated for RF. The ggRandomForest package also contain a large set of useful visualizations. Currently forestFloor supports randomForest objects(support for other RF implementions is on its way). Also feature contributions can be computed for gradient boosted trees, as these trees after training are not much different from random forest trees. So forestFloor could support XGBoost in future. Partial dependence plots are completely model invariant. All packages have in common to visualize the geometrical mapping structure of a model from feature space to target space. A sine curve y = sin(x) would be a mapping from x to y and can be plotted in 2D. To plot a RF mapping directly would often require too many dimensions. Instead the overall mapping structure can be projected, sliced or decomposed, such that the entire mapping structure is boiled down into a sequence of 2D marginal plots. If your RF model only has captured main effects and no interactions between variables, classic visualizations methods will do just fine. Then you can simplify your model structure like this $y = F(X) \approx f_1(x_1) + f_2(x_2) + ... + f_d(x_d)$. Then each partial function by each variable can be visualized just as the sine curve. If your RF model has captured sizable interactions, then it is more problematic. 3D slices of the structure can visualize interactions between two features and the output. The problem is to know which combination of features to visualize, ( iceBOX does address this issue). Also it is not easy to tell if other latent interactions still are not accounted for. In this paper , I used an very early version of forestFloor to explain what actual biochemical relationship a very small RF model had captured. And in this paper we thoroughly describe visualizations of feature contributions, Forest Floor Visualizations of Random Forests . I have pasted the simulated example from forestFloor package, where I show how to uncover a simulated hidden function $y = {x_1}^2 + sin(x_2\pi) + 2 * x_3 * x_4 + $ noise #1 - Regression example: set.seed(1234) library(forestFloor) library(randomForest) #simulate data y = x1^2+sin(x2*pi)+x3*x4 + noise obs = 5000 #how many observations/samples vars = 6 #how many variables/features #create 6 normal distr. uncorr. variables X = data.frame(replicate(vars,rnorm(obs))) #create target by hidden function Y = with(X, X1^2 + sin(X2*pi) + 2 * X3 * X4 + 0.5 * rnorm(obs)) #grow a forest rfo = randomForest( X, #features, data.frame or matrix. Recommended to name columns. Y, #targets, vector of integers or floats keep.inbag = TRUE, # mandatory, importance = TRUE, # recommended, else ordering by giniImpurity (unstable) sampsize = 1500 , # optional, reduce tree sizes to compute faster ntree = if(interactive()) 500 else 50 #speedup CRAN testing ) #compute forestFloor object, often only 5-10% time of growing forest ff = forestFloor( rf.fit = rfo, # mandatory X = X, # mandatory calc_np = FALSE, # TRUE or FALSE both works, makes no difference binary_reg = FALSE # takes no effect here when rfo$type="regression" ) #plot partial functions of most important variables first plot(ff, # forestFloor object plot_seq = 1:6, # optional sequence of features to plot orderByImportance=TRUE # if TRUE index sequence by importance, else by X column ) #Non interacting features are well displayed, whereas X3 and X4 are not #by applying color gradient, interactions reveal themself #also a k-nearest neighbor fit is applied to evaluate goodness-of-fit Col=fcol(ff,3,orderByImportance=FALSE) #create color gradient see help(fcol) plot(ff,col=Col,plot_GOF=TRUE) #feature contributions of X3 and X4 are well explained in the context of X3 and X4 # as GOF R^2>.8 show3d(ff,3:4,col=Col,plot_GOF=TRUE,orderByImportance=FALSE) Lastly the code for partial dependence plots coded by A.Liaw described by J.Friedman. Which do fine for main effects. par(mfrow=c(2,3)) for(i in 1:6) partialPlot(rfo,X,x.var=names(X)[i])
