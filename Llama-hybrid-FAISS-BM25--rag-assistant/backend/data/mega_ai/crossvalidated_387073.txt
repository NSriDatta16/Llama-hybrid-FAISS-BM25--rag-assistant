[site]: crossvalidated
[post_id]: 387073
[parent_id]: 386075
[tags]: 
What the Amazon story shows is that it is very hard to avoid the bias . I doubt that Amazon hired dumb people for this problem, or that they were lacking skills, or that they didn't have enough data, or that they didn't have enough AWS credits to train a better model. The problem was that the complicated machine learning algorithms are very good at learning patterns in the data, gender bias is exactly that kind of pattern. There was bias in the data, as the recruiters (consciously or not), favored male candidates. I'm not saying in here that Amazon is a company that discriminates job candidates, I'm sure they have thousands of anti-discriminatory policies and also hire pretty good recruiters. The problem with this kind of bias and prejudice is that exists no matter how hard you try to fight it. There are tons of psychology experiments showing that people may declare not to be biased (e.g. racist), but still make biased actions, without even realizing it. But answering your question, to have algorithm that is not biased, you would need to start with data that is free of this kind of bias. Machine learning algorithms learn to recognize and repeat the patterns they see in the data, so if your data records biased decisions, the algorithm will likely learn and amplify those bias. Second thing is managing the data. If you want to prohibit your algorithm from learning to make biased decisions, you should remove all the information that would help if to discriminate between groups of interest (gender in here). This does not mean removing only the information about gender, but also all the information that could lead to identifying gender, and this could be lots of things. There are obvious ones like name and photo, but also indirect ones, e.g. maternal leave in resume, but also education (what if someone went to girls-only school?), or even job history (say that recruiters in your company are not biased, but what if every other recruiter before was biased, so the work history reflects all those biased decisions?), etc. As you can see, identifying those issues may be pretty complicated (another reason why Amazon may have failed). As about questions 2. and 3., there is no easy answers and I do not feel competent enough to try answering them in detail. There is tons of literature on both prejudice and bias in society, and about algorithmic bias. This is always complicated and there is, unfortunately, no simple recipes for this. Companies, like Google, hire experts whose role is identifying and preventing this kind of bias in algorithms.
