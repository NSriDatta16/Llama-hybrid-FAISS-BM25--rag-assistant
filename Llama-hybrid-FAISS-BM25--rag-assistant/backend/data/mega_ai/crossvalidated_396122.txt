[site]: crossvalidated
[post_id]: 396122
[parent_id]: 396118
[tags]: 
What you simulate is (the arima.sim part) $$ Y_t=\phi_1Y_{t-1}+\phi_2Y_{t-2}+\phi_3Y_{t-3}+\varepsilon_t $$ and, for m , $X_t=\mu+Y_t$ , or $$ X_t-m=\phi_1(X_{t-1}-m)+\phi_2(X_{t-2}-m)+\phi_3(X_{t-3}-m)+\varepsilon_t $$ or $$ X_t=m\left(1-\sum_{j=1}^3\phi_j\right)+\phi_1X_{t-1}+\phi_2X_{t-2}+\phi_3X_{t-3}+\varepsilon_t $$ where $m\left(1-\sum_{j=1}^3\phi_j\right)=5(1-0.9)=0.5$ , which is actually estimated pretty well, with an average of 0.5377802 . This is therefore not the same as simulating $$ X_t=m+\phi_1X_{t-1}+\phi_2X_{t-2}+\phi_3X_{t-3}+\varepsilon_t $$ The issue is related to the fact that the expected value of an AR(p) process $$ Y_t=c+\phi_1Y_{t-1}+\phi_2Y_{t-2}+\ldots+\phi_pY_{t-p}+\varepsilon_t $$ is given by (see e.g. here ) $$ \mu=\frac{c}{1-\sum_{j=1}^p\phi_j} $$ When taking m even larger, computations may fail entirely, as mentioned by ?ar.ols . Taking an AR(1) for simplicity, we get > n x m The "culprit" in ar.ols seems to be in the lines where the regressor matrix is generated: # example data n Output: > XX [,1] [,2] [1,] 99.00 49491.75 [2,] 49491.75 24741893.87 > rank [1] 1 Effectively, the sum of squares of the regressors (the 2,2-element of XX ) seems to be too large relative to n (the 1,1-element plus 1) to ensure, according to ar.ols , sufficiently high numerical precision of the $(X'X)^{-1}$ matrix in the OLS estimate. Interestingly, other routines report a rank of 2 for this matrix, corresponding to what we know to apply in this example: > library(Matrix) > rankMatrix(XX) [1] 2
