[site]: crossvalidated
[post_id]: 601861
[parent_id]: 601853
[tags]: 
Yes, in the decoder, the queries come from the decoder's hidden states, and the keys and values come from the encoder's hidden states. This does not necessarily mean that $K=V$ . It would be true only if the dot-product attention were applied directly. However, Transformers use multi-head attention with a linear transformation from the states to the queries, keys, and values. This means that keys and values are different linear projections of the same hidden states . A usual interpretation of the linear projections is extracting relevant information for kyes and values.
