[site]: crossvalidated
[post_id]: 285673
[parent_id]: 30406
[tags]: 
I work in the field of credit scoring, where what here is being presented as a strange case is the norm. We use logistic regression, and convert both categorical and continuous variables into weights of evidence (WOEs), that are then used as the predictors in the regression. A lot of time is spent grouping the categorical variables, and discretising (binning/classing) the continuous variables. The weight of evidence is a simple calculation. It is the log of the odds for the class, less the log of odds for the population: WOE = ln(Good(Class)/Bad(Class)) - ln(Good(ALL)/Bad(ALL)) This is the standard transformation methodology for almost all credit scoring models built using logistic regression. You can use the same numbers in a piecewise approach. The beauty of it is that you will always know whether the coefficients being assigned to each WOE make sense. Negative coefficients are contrary to the patterns within the data, and usually result from multicollinearity; and coefficients over 1.0 indicate overcompensation. Most coefficients will come out somewhere between zero and one.
