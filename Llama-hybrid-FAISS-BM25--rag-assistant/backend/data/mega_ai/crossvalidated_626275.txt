[site]: crossvalidated
[post_id]: 626275
[parent_id]: 626271
[tags]: 
A lot of literature regarding multi-collinearity being problematic refer to either a) difficulty in interpreting coefficients (arguably of less importance in a model that's primarily for prediction), b) difficulty in finding a maximum likelihood estimate for model parameters in a traditional statistical model (e.g. complete or quasi-complete separation in logistic regression - which is a problem without regularization, but some other model types deal with it more "gracefully" e.g. random forest, gradient boosted decision trees), c) inefficiency (the more highly similar multiple features are, the more you wonder whether there's a lower dimensional representation that captures almost the same information and then some issues go away, such as in gradient boosted decision trees the regularizing effect of sub-sampling columns is reduced if you have many almost identical columns), or d) overfitting (too many features and too little training data = potential to have some randomly associated with the target variable by chance = overfitting risk, if not dealt with via a robust process). One can debate how much any of these truly matter for a model that is solely for prediction (e.g. explainability/interpretability may still be rather important). However, beware some proposed solutions that are known to be problematic (e.g. pre-screening each feature vs. the target in isolation aka "univariate feature selection").
