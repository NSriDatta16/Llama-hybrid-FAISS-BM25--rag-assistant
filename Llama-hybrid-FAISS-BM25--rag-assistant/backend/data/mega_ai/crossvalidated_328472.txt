[site]: crossvalidated
[post_id]: 328472
[parent_id]: 
[tags]: 
Odds ratio from logistic regression SPSS output different from what I calculate by hand + why should type of coding matter for odds ratio?

I'm having a couple logistic regression-related questions that I can't for the life of me figure out the answer to on my own. Disclaimer that I'm not super stats saavy, so thanks in advance for bearing with me. The model I'd ultimately like to be able to interpret involves two categorical predictors each with 2 levels, Article Type and Order, as well as their interaction. My DV is whether or not people help. I ran my model with deviation coding (-1, 1) so that I could interpret the main effects. My first issue arose when I noticed that the odds ratio listed for article type, at 1.52, seemed surprisingly low given what I knew about proportion who helped in each condition and decided to calculate it by hand for comparison. Here are the crosstabs: This is the whole sample. I calculated the odds ratio (odds of helping in experimental vs. control condition) to be (239/91)/(44/38)= 2.27. After some googling I discovered that having other predictors in the model can affect a given predictor's odds ratio, so I tried running the model with article type as the only predictor. With deviation coding, I now got 1.51 (so very close to what I got before, but still very different from what I calculated by hand), but rerunning it with indicator coding (0, 1), I got 2.27--the same odds ratio I'd calculated by hand. I'm baffled. My main questions are: (1) Why is the odds ratio so off when I use deviation coding in the model with the interaction? My understanding from past googling/first-hand experience is that the use of deviation coding shows you the equivalent of main effects, and I've compared the odds ratio from an analogous analysis to one calculated by hand and came up with roughly the same number. So I'm at a loss here. (2) Relatedly, how can I run the regression with two predictors and an interaction and get an odds ratio that more closely corresponds to what I calculate by hand? (I know indicator coding isn't the answer, since that just shows simple effects in the presence of an interaction (plus I tried it despite knowing that and sure enough it didn't solve the problem).) (3) Perhaps even more baffling to me than the first issue, why does switching from deviation coding to indicator coding when running the model with only one predictor change the odds ratio so drastically (or at all??). Isn't it just comparing the odds in one condition versus the other in either scenario? What does coding type have to do with that? (4) Relatedly, why is the odds ratio in the single predictor model consistent with what I calculate by hand when I use indicator coding, but not when I use deviation coding? There's nothing obviously weird about my data, like missing cases, that would help explain this (as far as I can tell), and the number of cases that shows up in the output is the same regardless of which type of coding I use. Any help would be so appreciated. I'm in the process of writing up results for my dissertation and this issue has slowed my progress to a halt.
