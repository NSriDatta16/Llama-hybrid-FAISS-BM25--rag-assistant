[site]: crossvalidated
[post_id]: 412206
[parent_id]: 
[tags]: 
Is the Keras Embedding layer dependent on the target label?

I learned how to 'use' the Keras Embedding layer, but I am not able to find any more specific information about the actual behavior and training process of this layer. For now, I understand that the Keras Embedding layer maps distinct categorical features to n-dimensional vectors, which allows us to find, for example, how similar two features are. What I do not understand is how these vectors in the embedding layer are trained. Here is an explanation where there is information that these vectors are not computed with any operation, but working only as a lookup table, but I always thought that they are somehow "trained" to find similarities between distinct features. If they are trained, are they trained from target labels, or from the order in which they appear (similar to GloVe, word2vec, etc.) or from both? I have the following example of two pairs of rows in a dataset. y is the model target label and X are the features encoded to integers to be used in the embedding layer: #pair 1 dataset_y_row1 = [1] dataset_y_row2 = [0] dataset_X_row1 = [3,5,8,45,2] dataset_X_row2 = [3,5,8,45,2] #pair 2 dataset_y_row3 = [1] dataset_y_row4 = [1] dataset_X_row3 = [3,5,8,45,2] dataset_X_row4 = [3,5,45,8,2] My questions are the following: Will the embedding layer see any difference between rows 1 and 2 (i.e. is it 'target-label-sensitive')? Will the embedding layer see any difference between rows 3 and 4 (i.e. is it sensitive to order of features like word2vec, GloVe, etc.)?
