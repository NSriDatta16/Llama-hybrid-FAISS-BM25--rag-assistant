[site]: crossvalidated
[post_id]: 198777
[parent_id]: 198776
[tags]: 
The correlation coefficient $\rho$ is intrinsically related to the linear regression coefficient, or the least squares estimated "slope", which is often called $\beta$. The formula relating these values is: $$\beta = \rho \frac{\mbox{var} \left(Y \right)}{ \mbox{var} \left(X\right)} $$ So, if $X$ and $Y$ are centered and scaled, so that their variances are 1, (this means they are transformed into unitless quantities) then the regression slope is the correlation coefficient. The least squares regression coefficient is often interpreted as a "first order trend" meaning that if the data take some nonlinear form, the slope gives you a "best fitting" line to that curve. This is intuitively useful, because only if a trend is highly curvilinear does the slope fails to show much strong relationship with the data. Any perfectly symmetrical trend about the mean of the $Y$ will have a slope equal to 0. Another interpretation of the least squares slope is an average derivative: that is, whatever the actual trend is between $X$ and $Y$ (imaging some meandering, wiggling trend), the first order trend or the least squares regression slope is what you get taking a weighted average of the instantaneous trend at each point. I spoke a lot about $\beta$ when you asked about $\rho$. The relationship serves to show they are similar, and indeed inferring about one is equivalent to inferring about the other. I prefer the slope as a data summary measure. I look at the fact that it incorporates the units of $X$ and $Y$ as a very good thing, because it means the summary is contextually appropriate. For instance, I want to know how much I'd expect blood pressure to change with an extra 10mg / day dose of a hypertension medication, not simply the correlation between them.
