[site]: crossvalidated
[post_id]: 23128
[parent_id]: 
[tags]: 
Solving for regression parameters in closed-form vs gradient descent

In Andrew Ng's machine learning course , he introduces linear regression and logistic regression, and shows how to fit the model parameters using gradient descent and Newton's method. I know gradient descent can be useful in some applications of machine learning (e.g., backpropogation), but in the more general case is there any reason why you wouldn't solve for the parameters in closed form-- i.e., by taking the derivative of the cost function and solving via Calculus? What is the advantage of using an iterative algorithm like gradient descent over a closed-form solution in general, when one is available?
