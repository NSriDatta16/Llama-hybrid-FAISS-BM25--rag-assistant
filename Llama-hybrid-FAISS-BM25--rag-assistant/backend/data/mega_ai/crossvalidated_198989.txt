[site]: crossvalidated
[post_id]: 198989
[parent_id]: 
[tags]: 
Why does pre-training help with deep learning?

I'm trying to understand why pre-training of a deep neural network improves classification performance. By initialising the network weights from a pre-training stage, and then training on data for classification, the performance is usually better than if the weights were initialised randomly. But why wouldn't random initialisation eventually find this same minimum of the loss function? The loss function uses the same data in both, and both approaches should converge to the same global minimum, right? If anybody has any intuition about this then it would be helpful, thanks!
