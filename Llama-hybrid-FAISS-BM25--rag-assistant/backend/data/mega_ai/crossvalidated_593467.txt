[site]: crossvalidated
[post_id]: 593467
[parent_id]: 593418
[tags]: 
There was an extended discussion in the comments that surfaced some misconceptions, so I'll address them in the answer as it’d probability answer the question. For the explanatory and predictor variables, I have tried all the mechanisms of linear regression, including the interaction models (from full to partial). However, I do not think that the behavior of the system is linear, per se (given the graphs I get). As a result, I would like to use the Neural Networks for model fitting. You didn't exhaust the possibilities of extending the linear regression model. You could add polynomial features or other data transmissions, to model non-linear relations. Moreover, there are many other machine learning models between linear regression and neural networks, so jumping straight to neural networks seems a bit fast. While comparing various linear regression models with full and partial interactions, we found that the indicators of performances R^2, Adj - R^2, AIC, BIC, log-Likelihood, F-statistic and corresponding p-Value were not behaving the same way. For example, the least AIC did not have the largest R^2. So what? It's not surprising to see different metrics giving different results because they measure different things. If all the metrics gave the same results all the time, there wouldn't be any reason to have more than one metric. It has nothing to do with the relationship being non-linear. It's not clear at all why you want to use the $F$ -test here. It is not a measure of goodness of fit, but a hypothesis test. As noticed in the comment by Dave, it tests the hypothesis if your model is better than the intercept-only model, i.e. predicting the mean for all the cases. I don't see how testing if a neural network behaves better than predicting a constant made any sense. Sure, it's a good benchmark to make sure that your predictions are not complete rubbish, but you can verify that by comparing some metrics for predicting mean vs another model on a held-out test set to verify that. Finally, as I noticed in the comment it's not clear how would we define the degrees of freedom for neural networks (or many other machine learning models). I linked to a thread that discusses that and provides some papers digging deeper into the subject. For example, neural networks can have more parameters than there are samples in data, so the $n-p$ in $F$ -test would get negative (invalid), and the number of parameters is not even a good measure of the degrees of freedom, as they interact with each other. So it doesn't seem that you need the $F$ -test, it wouldn't make much sense in here, and there is no meaningful way to calculate it. From what you described, it is not even necessarily true that there is something “wrong” with your predictions, or that it can't be solved with linear regression.
