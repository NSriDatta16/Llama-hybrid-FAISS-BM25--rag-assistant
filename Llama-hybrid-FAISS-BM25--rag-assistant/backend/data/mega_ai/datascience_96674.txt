[site]: datascience
[post_id]: 96674
[parent_id]: 96447
[tags]: 
I suggest you use the state of the art for this kind of problems: a BERT -based approach. This kind of approach is well documented and very accessible, given the large amount of examples available online. The approach consists of taking a pre-trained neural network model from the BERT family (Transformer encoders normally trained on a masked language model task over a large dataset), and fine-tuning it on your data. This would allow you to profit from: BERT's subword vocabulary, which will avoid having out-of-vocabulary words, because it decomposes words into smaller fragments. The power of transfer learning, which would mitigate the situations where you don't have a lot of data. State of the art performance. You can check at paperswithcode that BERT-based approaches are frequent top-performers in standard text classification benchmarks. One of the most well documented and maintained python libraries for this is Hugginface Transformers . You can have a look at some examples on how to do text classification on a custom dataset here . About the computational resources to train the system, you may use your own GPUs if you have, you may also train on CPU (which may be feasible given your short sequence lengths), or you may use Google Colab , which is very handy if you don't need a lot of training time.
