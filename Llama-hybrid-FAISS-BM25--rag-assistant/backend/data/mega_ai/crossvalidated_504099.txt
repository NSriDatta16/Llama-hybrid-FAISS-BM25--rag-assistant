[site]: crossvalidated
[post_id]: 504099
[parent_id]: 503790
[tags]: 
This is less of an answer and more of an extended comment. There is no way to generate additional data in a reliable way in order to meet the (very liberal) 1-in-10 EPV rule, which is what I think you are working off. Here are some of my thoughts: Penalized methods are not what you want if your goal is inference. You lose the ability to do inference since the models are by design biased. New techniques for doing inference from these models exist (I think they go by the name of post selection inference or selective inference ). I'm not intimately familiar with them and so won't comment on if they are good or bad. I will leave that to you to decide. If you have good prior information, you might consider a Bayesian analysis. The prior information acts as regularization (indeed the LASSO and Ridge regression have Bayesian interpretations). That might be more work to do right than you are willing to commit. The bootstrap is a good idea, but might prove problematic in some cases. The problem with too many predictors is that you risk complete or quasi-complete separation of the data. If that happens you get incredibly large estimates of the standard error even from bootstrapping because the coefficients can be enormous. You can try and get around that by using Firth's logistic regression.
