[site]: datascience
[post_id]: 38394
[parent_id]: 37171
[tags]: 
After some more research, I believe this can be solved with a straightforward application of encoder-decoder networks. In this tutorial, we can simply replace sampled_token_index and sampled_char with actual_token_index and actual_char , computed according. And of course in our case it's actual_word . To summarize we divide our training set into input/output pairs, where output examples begin with a token and end with , and train a sequence to sequence model on these pairs, as described in the tutorial. Then at inference time we feed the token to the model to predict the next word. Then after we receive the actual next word, we feed the actual (observed) output so far into the model, and so on. The other answers had some interesting ideas, but unfortunately they didn't really address how to deal with variable length inputs and outputs. I believe seq2seq models based on RNNs are the best way to address this.
