[site]: crossvalidated
[post_id]: 464257
[parent_id]: 464170
[tags]: 
The following viewpoint may help your intuition: Let some there be some data distributed according to a quadratic curve: $$y \sim \mathcal{N}(\mu = a+bx+cx^2, \sigma^2 = 10^{-3})$$ For instance with $x \sim \mathcal{U}(0,1)$ and $a=0.2$ , $b=0$ and $c=1$ . Then a linear curve and a polynomial curve will have very different coefficients for the linear term. set.seed(1) x Correlation The reason is that the variables/regressors $x$ and $x^2$ correlate. The coefficient estimates computed with a linear regression are not a simple correlation (perpendicular projection onto each regressor seperately ): $$\hat{\beta} \neq \alpha = \mathbf{X^t} y$$ (this would give coefficients $\alpha_1$ and $\alpha_2$ in the image below, and these coordinates/coefficients/correlations do not change when you add or remove other regressors) Using the correlation/projection $\mathbf{X^t}y$ is wrong, because if there is a correlation between the vectors in $\mathbf{X}$ , then there will be an overlap between some vectors. This part that overlaps will be redundant and added too much. The predicted value $\hat{y} = \alpha \mathbf{X}$ would be too large. For this reason there is a correction with a term $(\mathbf{X^t}\mathbf{X})^{-1}$ that accounts for the overlap/correlation between the regressors. This might be clear in the image below which stems from this question: Intuition behind $(X^TX)^{-1}$ in closed form of w in Linear Regression Intuitive view So the regressors $x$ and $x^2$ both correlate with the data $y$ and they both will be able to express the variation in the dependent data. But when we use them together then we are not gonna add them according to their single independent effects (according to correlation with $y$ ) because that would be too much. If we use both $x$ and $x^2$ in the regression then obviously the coefficient for the linear term $x$ should be very small since this is the same in the true relation. However, when we are not using the quadratic term $x^2$ in the regression (or otherwise add a bias to the coefficient for the quadratic term), then the coefficient for $x$ which correlates somewhat with $x^2$ will partly take correct this (take over) and... the value of the estimate for the coefficient of the linear term will change. See also: regression with multiple independent variables vs multiple regressions with one independent variable Why is the intercept in multiple regression changing when including/excluding regressors? Why and how does adding an interaction term affects the confidence interval of a main effect? Why is the intercept changing in a logistic regression when all predictors are standardized? Intuition behind $(X^TX)^{-1}$ in closed form of w in Linear Regression Does adding more variables into a multivariable regression change coefficients of existing variables? Estimating $b_1 x_1+b_2 x_2$ instead of $b_1 x_1+b_2 x_2+b_3x_3$ Why do regression coefficients change when excluding variables? Does the order of explanatory variables matter when calculating their regression coefficients?
