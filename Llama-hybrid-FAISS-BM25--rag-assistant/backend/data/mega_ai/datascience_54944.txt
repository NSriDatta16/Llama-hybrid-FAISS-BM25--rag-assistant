[site]: datascience
[post_id]: 54944
[parent_id]: 54877
[tags]: 
Short Answer - Yes. Adam-bias-corrected is a good example with all benefits. Long answer- RMSProp by Hinton & Adadelta ( with corrected units by Hessian approximatin), both methods do same what you have asked for ie doensn't work on single learning rate, rather have different learning rates and also they don't need a selection of global learning rate ( which is the case with original Adadelta). Both of them work in similar fashion with their own pros and cons. Adgrad is the first one which was introduced with adaptive learning rates for different paramters. while RMSprop has exponentially decaying average which stores this decaying average of past squared gradients vt. mt below is the past gradients' average which brings us to Adam - bias-correct method. ABove, since mt and vt are initialised as vectors of zeros, they are biased towards zero, expecially during the initial time steps or when the decay rates are small, hence Adam - bias -correct which is works with corrected values as follow Hope that helps.
