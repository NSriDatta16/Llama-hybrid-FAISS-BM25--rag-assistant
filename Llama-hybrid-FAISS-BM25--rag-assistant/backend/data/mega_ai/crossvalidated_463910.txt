[site]: crossvalidated
[post_id]: 463910
[parent_id]: 
[tags]: 
Softmax backpropagation

I know there are similar questions out there, but none of the answers really helped me. I'm working on an own neural network implementation and I want to implement the softmax activation function. I'm using backpropagation as the learning algorithm to train the network. I learned that the softmax doesn't have a vector, but a matrix as its derivative. However I'm not sure how to apply that knowledge to the algorithm. To calculate the error signal of an output neuron, the formula I use looks like this: $$\delta_j=\varphi'(net_j)\cdot(o_j-t_j)$$ Where $\delta$ is the error signal, $\varphi$ is the activation function, $net$ is the netinput, $o$ is the output and $t$ is the target value. This works fine for all activation functions with a vector derivative, but since the softmax' derivative is a $n\times n$ matrix I don't know how to interpret it. As far as I understood everything right so far, that would mean that for every neuron there are $n$ different derivatives. How do I use them to calculate the error signals? I hope I formulated my problem in an understandable way, if not feel free to correct me as english is not my native language. I hope y'all can help me. Thank you!
