[site]: datascience
[post_id]: 117570
[parent_id]: 
[tags]: 
Python dataset normalization for convolutional autoencoder

I have a csv files which contain pixel neighboorhood information. Here an example of the dataset: 0 0 1.875223e+01 1 0 1.875223e+01 2 0 2.637685e+01 3 0 2.637685e+01 4 0 2.637685e+01 5 0 2.637685e+01 6 0 2.637685e+01 7 0 2.637685e+01 8 0 2.637685e+01 9 0 1.875223e+01 I would like to know if it is a good idea to apply a normalization on this dataset before training the convolutional autoencoder ? Normalize the value data scaler = StandardScaler() values_scaled = scaler.fit_transform(values.reshape(-1, 1)) My second question, should I use MSE or binary_crossentropy as loss function?
