[site]: crossvalidated
[post_id]: 297749
[parent_id]: 
[tags]: 
How meaningful is the connection between MLE and cross entropy in deep learning?

I understand that given a set of $m$ independent observations $\mathbb{O}=\{\mathbf{o}^{(1)}, . . . , \mathbf{o}^{(m)}\}$ the Maximum Likelihood Estimator (or, equivalently, the MAP with flat/uniform prior) that identifies the parameters $\mathbf{θ}$ that produce the model distribution $p_{model}\left(\,\cdot\, ; \mathbf{θ}\right)$ that best matches those observations will be $$\mathbf{θ}_{ML}(\mathbb{O})= p_{model}\left(\mathbb{O}; \mathbf{θ}\right) = \underset{\mathbf{θ}}{\arg\max}‎‎\prod_{i=1}^{m} p_{model}\left(\mathbf{o}^{(i)}; \mathbf{θ}\right)$$ or, more conveniently $$\mathbf{θ}_{ML}(\mathbb{O})= \underset{\mathbf{θ}}{\arg\min}\sum_{i=1}^{m} -\log p_{model}\left(\mathbf{o}^{(i)}; \mathbf{θ}\right)$$ and see the role that $\mathbf{θ}_{ML}$ can play in defining a loss function for multi-class deep neural networks, in which $\mathbf{θ}$ corresponds to the the network's trainable parameters (e.g., $\mathbf{θ} = \{\mathbf{W}, \mathbf{b}\} )$ and the observations are the pairs of input activations $\mathbf{x}$ and corresponding correct class labels $y \in [1, k]$, $\mathbf{o}^{(i)}$ = {$\mathbf{x}^{(i)}, y^{(i)}$}, by taking $$p_{model}\left(\mathbf{o}^{(i)}; \mathbf{θ}\right) \equiv p_{model}\left(y^{(i)} | \mathbf{x}^{(i)}; \mathbf{θ}\right)$$ What I don't understand is how this relates to the so called "cross entropy" of the (vectorized) correct output, $\mathbf{y}^{(i)}$, and the corresponding output activations of the network, $\mathbf{a}(\mathbf{x}^{(i)}; \mathbf{θ})$ $$H(\mathbf{o}^{(i)}; \mathbf{θ}) = -\mathbf{y}^{(i)}\cdot \mathbf{log}\,\mathbf{a}(\mathbf{x}^{(i)}; \mathbf{θ})‎$$ that is used in practice when measuring error/loss during training. There are several related issues: Activations "as probabilities" One of the steps in establishing the relationship between MLE and cross entropy is to use the output activations "as if" they are probabilities. But it's not clear to me that they are, or at least that they $all$ are. In calculating training error — specifically, in calling it a "cross entropy loss" — it is assumed that (after normalizing activations to sum to 1) $$p_{model}\left(\mathbf{o}^{(i)}; \mathbf{θ}\right) \equiv a_{y^{(i)}}(\mathbf{x}^{(i)}; \mathbf{θ})\tag{1}\label{1}‎‎$$ or $$\log p_{model}\left(\mathbf{o}^{(i)}; \mathbf{θ}\right) = \log a_{y^{(i)}}(\mathbf{x}^{(i)}; \mathbf{θ})‎‎$$ so that we can write $$-\log p_{model}\left(\mathbf{o}^{(i)}; \mathbf{θ}\right) = -\mathbf{y}^{(i)}\cdot \mathbf{log}\,\mathbf{a}(\mathbf{x}^{(i)}; \mathbf{θ})‎\tag{3}\label{3}$$ and thus $$\mathbf{θ}_{ML}(\mathbb{O})=\underset{\mathbf{θ}}{\arg\min}\sum_{i=1}^{m} H(\mathbf{o}^{(i)}; \mathbf{θ})$$ But while this certainly makes $a_{y^{(i)}}(\mathbf{x}^{(i)}; \mathbf{θ}_{ML})$ a probability (to the extent that anything is), it places no restrictions on the other activations. Can the $\mathbf{a}_{y^{(i)}}(\mathbf{x}^{(i)}; \mathbf{θ}_{ML})$ really be said to be PMFs in that case? Is there anything that makes the $a_{y^{(i)}}(\mathbf{x}^{(i)}; \mathbf{θ}_{ML})$ not in fact probabilities (and merely "like" them)? Limitation to categorization The crucial step above in equating MLE with cross-entropy relies entirely on the "one-hot" structure of $\mathbf{y}^{(i)}$ that characterizes a (single-label) multi-class learning problem. Any other structure for the $\mathbf{y}^{(i)}$ would make it impossible to get from $\eqref{1}$ to $\eqref{3}$. Is the equation of MLE and cross-entropy minimization limited to cases where the $\mathbf{y}^{(i)}$ are "one-hot"? Different training and prediction probabilities During prediction, it is almost always the case that $$p_{model}\left(y^{(i)} | \mathbf{x}^{(i)}; \mathbf{θ}\right) \equiv P\left(\underset{j\in[1,k]}{\arg\max}\,a_j(\mathbf{x}^{(i)}; \mathbf{θ}) = y^{(i)}\right)\tag{2}\label{2}$$ which results in correct prediction probabilities that are different from the probabilities learned during training unless it is reliably the case that $$a_{y^{(i)}}(\mathbf{x}^{(i)}; \mathbf{θ}_{ML}) = P\left(\underset{j\in[1,k]}{\arg\max}\,a_j(\mathbf{x}^{(i)}; \mathbf{θ}_{ML}) = y^{(i)}\right)$$ Is this ever reliably the case? Is it likely at least approximately true? Or is there some other argument that justifies this equation of the value of the learned activation at the label position with the probability that the maximum value of learned activations occurs there? Entropy and information theory Even assuming that the above concerns are addressed and the activations are valid PMFs (or can meaningfully be treated as such), so that the role played by cross entropy in computing $\mathbf{θ}_{ML}$ is unproblematic, it's not clear to me why it is helpful or meaningful to talk about the entropy of the $\mathbf{a}(\mathbf{x}^{(i)}; \mathbf{θ}_{ML})$, since Shanon entropy applies to a specific kind of encoding , which is not the one being used in training the network. What role does information theoretic entropy play in interpreting the cost function, as opposed to simply providing a tool (in the form of cross entropy) for computing one (that corresponds to MLE)?
