[site]: datascience
[post_id]: 63430
[parent_id]: 
[tags]: 
Can feature representation acquired by a same model but trained on different corpus be used on the same classification model?

For example, if I wanna do document classification with doc2vec embeddings, first I train the training set to get doc2vec embeddings, and fit the embeddings to a classification model; later on when I want to classify a new doc, do I have to use the previous doc2vec model to infer vectors to use on the classification model, or I can train a new doc2vec model to get vectors, since the dimension is the same? From my point of view, I think the same doc2vec model has to be used, since even if we can get vectors of the same dimension via a newly trained doc2vec model, they are not on the same feature space, here the doc2vec model would be a feature function and we has to use the same function to get features in a consistent feature space. But doc2vec, or word2vec is based on distributed representation, so even if a new doc2vec model is trained on new documents, theoretically sentences with same semantics tend to be represented the same regardless of the model, so I'm confused. Any suggestions or corrections would be appreciated!
