[site]: crossvalidated
[post_id]: 504801
[parent_id]: 
[tags]: 
Bayesian Regression- Expectation Maximization

In Bayesian regression, we have $y_i=x_i^{T}w+\epsilon_i$ where $w \sim \mathcal{N}(0,\alpha)$ and $\epsilon_i \sim \mathcal{N}(0,\frac{1}{\beta})$ . Inference of $\alpha$ and $\beta$ is done by maximizing the likelihood(or marginal likelihood) given the data. This paper (Appendix A.1) explains that maximization can be done by expectation-maximization(EM). My questions is that why do we need EM to do the maximization. I understand that EM can be used by treating the $w$ as the hidden variable. However, $w$ can be integrated out and we can apply gradient descent(GD). Why don't we use GD?
