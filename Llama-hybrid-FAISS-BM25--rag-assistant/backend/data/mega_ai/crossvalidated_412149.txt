[site]: crossvalidated
[post_id]: 412149
[parent_id]: 410440
[tags]: 
Neural nets have thousand or millions of parameters so you're solving an optimization problem in a very high dimensional space. Thus the likelihood that you will exactly solve the minimization problem at hand is extremely small. When we say that the network's performance "has converged," we don't generally mean, in the context of neural networks, that the exactly optimal weights have been found. Rather, we mean that the network parameters are in some small neighborhood around a locally optimal solution. So the reason the norm of the gradient is still decreasing is that even though, for all intents and purposes, you should think of the network as having arrived at a local optimum, the parameters are still very slowly tending to that exact point.
