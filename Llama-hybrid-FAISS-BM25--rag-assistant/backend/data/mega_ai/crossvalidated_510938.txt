[site]: crossvalidated
[post_id]: 510938
[parent_id]: 510933
[tags]: 
Yes, your intuition is correct. In " Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift ", Sergey Ioffe and Christian Szegedy write It has been long known (LeCun et al., 1998b; Wiesler & Ney, 2011) that the network training converges faster if its inputs are whitened – i.e., linearly transformed to have zero means and unit variances, and decorrelated. So you might expect to find that the network trains faster if you de-correlate the inputs in addition to applying zero mean and unit variances. PCA will improve the convergence even if the dimension is not reduced, because the effect of correlation is not present. The following citations provides more detail. LeCun, Y., Bottou, L., Orr, G., and Muller, K. " Efficient backprop. " In Orr, G. and K., Muller (eds.), Neural Networks: Tricks of the trade . Springer, 1998b. Wiesler, Simon and Ney, Hermann. " A convergence analysis of log-linear training. " In Shawe-Taylor, J., Zemel, R.S., Bartlett, P., Pereira, F.C.N., and Weinberger, K.Q. (eds.), Advances in Neural Information Processing Systems 24, pp. 657–665, Granada, Spain, December 2011
