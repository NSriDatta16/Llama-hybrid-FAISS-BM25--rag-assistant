[site]: crossvalidated
[post_id]: 57250
[parent_id]: 57230
[tags]: 
What sort of SVM do you use?. If you are using $\nu$-SVM, then you can find the answer in this paper . The idea is the following: "v is an upper bound on the fraction of margin errors, a lower bound on the fraction of support vectors, and both quantities approach v asymptotically". Additionally, this number cannot exceed the quantity 2*lmin/l, where l is the total number of SVs and lmin is the minimum between the number of positive and negative SVs (labels +/-1). Notice that this means that for inbalanced problems, you will have to work with lower values of v, so you will tend to overfit data. Why? Because it forces you to use an eventually very small value of nu, that is, you allow very small number of errors during training, what leads to overfitting. Oversampling is a way to dodge this problem. This intuition is also valid for C-SVM, though the meaning of the C parameter is different from that of the $\nu$ parameter. Still, it also acts as a parameter controlling the amount of regularization. The idea is that C weights a term which can be understood as the number of errors your classifier does on the training data. If a class has a much higher density of samples, then the optimization algorithm tends to reduce the error by pushing the separating hyperplane to the minority class. The idea is that you decrease the total error by reducing the mistakes on the big class. Overall looks better, but as a result you have a higher error rate on the smaller class. Consider the extreme case when you have a ratio 97%-3%. More details about the problem and techniques to overcome it here . Hope this helps.
