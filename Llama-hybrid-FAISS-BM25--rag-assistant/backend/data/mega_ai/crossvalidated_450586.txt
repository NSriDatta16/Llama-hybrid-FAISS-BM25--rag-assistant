[site]: crossvalidated
[post_id]: 450586
[parent_id]: 
[tags]: 
Controlling details in images generated by Generative Adversarial Networks (GANs)

With conditional GANs it is possible to generate images of a certain class of objects. And moreover with current text-to-image methods it seems to be possible to control certain details of the generated images as well, as i.e. this research shows . In my case I analyze a data set of labeled car images where the labels contain ca. 20-30 features such as brand, color, shape, type, etc. Currently I have two questions concerning this problem: Is it possible to train a GAN to generate realistic images with a resolution of less than 256 by 256 pixels of cars? And will the GAN generalize to unseen feature vectors? Generally speaking: Given a feature vector of around 20-30 categorical variables controlling details of the image. Is it possible with state-of-the-art methods to build a GAN-generator that takes a feature vector as input and generates an image with the details specified in the feature vector?
