[site]: datascience
[post_id]: 54041
[parent_id]: 
[tags]: 
Stationary time series for clustering algorithms

I have a set of time series data that I would like to feed into a clustering algorithm (like k-means, using dynamic time warping as the distance function). After standardizing the data with mean 0 and variance 1, the k-means classifier generated a batch of centroids that seemed to fit the data pretty well. The only question I have is whether the data should be stationary. Models such as ARIMA require for the data to be stationary due to the nature of it. However, the data I want to cluster is mortgage rates as a function of time, which could be subject to seasonal trends, which could be useful when clustering other future time series data. The question is: do clustering algorithms for time series data generally require for the data to be stationary?
