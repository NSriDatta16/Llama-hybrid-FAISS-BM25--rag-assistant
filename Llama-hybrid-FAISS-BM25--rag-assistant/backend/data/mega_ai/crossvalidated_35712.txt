[site]: crossvalidated
[post_id]: 35712
[parent_id]: 35708
[tags]: 
Multicolinearity is not generally a problem for SVMs. Ridge regression is often used where multicolinearity is an issue, as the regularisation term resolves the invertibility issue by adding a ridge. The SVM uses the same regularisation term as ridge regression does, but with the hinge loss in place of the squared error. Ridge regression has a link with PCA as explained by Tamino , essentially it penalises principal components with large eigenvalues less than components with low eigenvalues, so it is a bit like having a soft selection of the most important PCs, rather than a hard (binary) selection. The important thing is to make sure that the regularisation parameter, C, and any kernel parameters are tuned correctly and carefully, preferably by minimising an appropriate cross-validation based model selection criterion. This really is the key to getting good results with an SVM (and kernel methods in general).
