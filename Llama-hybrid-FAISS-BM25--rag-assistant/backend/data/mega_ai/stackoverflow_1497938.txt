[site]: stackoverflow
[post_id]: 1497938
[parent_id]: 1467898
[tags]: 
Speaking of lower bounds on compute time : Let's analyze my algo above : for each row (key,score,id) : create or fetch a list of top scores for the row's key if len( this list ) minimum score in list replace minimum of list with current row update minimum of all lists if needed Let N be the N in top-N Let R be the number of rows in your data set Let K be the number of distinct keys What assumptions can we make ? R * sizeof( row ) > RAM or at least it's big enough that we don't want to load it all, use a hash to group by key, and sort each bin. For the same reason we don't sort the whole stuff. Kragen likes hashtables, so K * sizeof(per-key state) Kragen is not sorting, so K*N (note : A If the data has a random distribution, then after a small number of rows, the majority of rows will be rejected by the per-key minimum condition, the cost is 1 comparison per row. So the cost per row is 1 hash lookup + 1 comparison + epsilon * (list insertion + (N+1) comparisons for the minimum) If the scores have a random distribution (say between 0 and 1) and the conditions above hold, both epsilons will be very small. Experimental proof : The 27 million rows dataset above produces 5933 insertions into the top-N lists. All other rows are rejected by a simple key lookup and comparison. epsilon = 0.0001 So roughly, the cost is 1 lookup + coparison per row, which takes a few nanoseconds. On current hardware, there is no way this is not going to be negligible versus IO cost and especially parsing costs.
