[site]: stackoverflow
[post_id]: 3626723
[parent_id]: 3492302
[tags]: 
The problem with solutions based on sums of numbers is they don't take into account the cost of storing and working with numbers with large exponents... in practice, for it to work for very large n, a big numbers library would be used. We can analyse the space utilisation for these algorithms. We can analyse the time and space complexity of sdcvvc and Dimitris Andreou's algorithms. Storage: l_j = ceil (log_2 (sum_{i=1}^n i^j)) l_j > log_2 n^j (assuming n >= 0, k >= 0) l_j > j log_2 n \in \Omega(j log n) l_j So l_j \in \Theta(j log n) Total storage used: \sum_{j=1}^k l_j \in \Theta(k^2 log n) Space used: assuming that computing a^j takes ceil(log_2 j) time, total time: t = k ceil(\sum_i=1^n log_2 (i)) = k ceil(log_2 (\prod_i=1^n (i))) t > k log_2 (n^n + O(n^(n-1))) t > k log_2 (n^n) = kn log_2 (n) \in \Omega(kn log n) t Total time used: \Theta(kn log n) If this time and space is satisfactory, you can use a simple recursive algorithm. Let b!i be the ith entry in the bag, n the number of numbers before removals, and k the number of removals. In Haskell syntax... let -- O(1) isInRange low high v = (v >= low) && (v Storage used: O(k) for list, O(log(n)) for stack: O(k + log(n)) This algorithm is more intuitive, has the same time complexity, and uses less space.
