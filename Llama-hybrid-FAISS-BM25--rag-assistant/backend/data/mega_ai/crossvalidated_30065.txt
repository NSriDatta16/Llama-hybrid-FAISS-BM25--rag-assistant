[site]: crossvalidated
[post_id]: 30065
[parent_id]: 30042
[tags]: 
I will just try to explain my opinion that appeared to be shared by most of my friends. I have the following concerns about NN that are not about SVM at all: In a classic NN, the amount of parameters is enormously high. Let's say you have the vectors of the length 100 you want to classify into two classes. One hidden layer of the same size as an input layer will lead you to more then 100000 free parameters. Just imagine how badly you can overfit (how easy is it to fall to local minimum in such a space), and how many training points you will need to prevent that (and how much time will you need to train then). Usually you have to be a real expert to chose the topology at a glance. That means that if you want to get good results you should perform lots of experiments. That's why it's easier to use SVM and tell, that you couldn't get similar results with NN. Usually NN results are not reproducible. Even if you run your NN training twice, you will probably get different results due to the randomness of a learning algorithm. Usually you have no interpretation of the results at all. That is a small concern, but anyway. That doesn't mean that you should not use NN, you should just use it carefully. For example, Convolutional NN can be extremely good for image processing, other Deep NN proved to be good for other problems as well. Hope it will help.
