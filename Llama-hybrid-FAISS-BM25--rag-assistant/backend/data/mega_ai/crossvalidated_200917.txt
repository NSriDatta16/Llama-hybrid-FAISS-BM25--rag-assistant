[site]: crossvalidated
[post_id]: 200917
[parent_id]: 200905
[tags]: 
I'm not Stata user and won't interpret the specific output you show, the so more that you gave only results, not the data to analyze it. Instead, I'll offer few lines about the relationship between the types of analyses, just to guide you. Multiple Correspondence analysis (which is Correspondence analysis of 3+ dim. contingency table and with optional computation of individuals' "object scores"), MCA, is an optimal scaling dimensionality reduction / mapping technique for all the input variables being nominal . It is actually a particular case of, and becomes equivalent to Categorical Principal Component analysis (CatPCA) when the latter uses multiple nominal quantification for all the input variables. If all the variables are dichotomous then MCA is equivalent to CatPCA using any type of quantification - because a variable with just 2 categories can be quantified no otherwise than one way - linearly. And this way MCA/CatPCA becomes almost equivalent to usual linear PCA . The equivalencies pertain to eigenvalues and object scores (= component scores). MCA does not or normally should not output component-variable loadings because MCA uses "multiple nominal quantification" which is the treating of every value (category) of a variable as a separate variable (a dummy). What will correspond in MCA to the variable "loadings" of PCA is the coordinates of centroids of the categories of the variables. But in your case - because all the variables are dichotomous and so each variable is fully represented by one nonredundanrt category (one dummy) - you may use and interpret results of just PCA with its loadings, in place of MCA with its centroids coordinates. To repeate: since all variables binary, you actually don't need MCA, usual PCA suffice$^1$ $^2$. By word inertia the scale is meant in literature: either squared (eigenvalues and their sum) or nonsquared (singular values). To know more about MCA or simple (two-way) CA please read a good text. $^1$ You may expect just minor differences, primarily due to MCA being iterative and PCA not. If the convergence is not complete, as often, these ignorable differences show up. $^2$ When we are considering to use PCA with binary data it is worth thinking about doing or not doing variable centering before the analysis. Typically, PCA is done on centered or standardized data (= done based on covariances or correlations, respectively). Not centering the data can produce very different PCA results, but they can make more sense in some studies with binary categorical data. For example, in text analytics PCA is frequently done based on cosines which implies data normalization without centering.
