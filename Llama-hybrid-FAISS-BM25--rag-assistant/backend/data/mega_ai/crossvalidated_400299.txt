[site]: crossvalidated
[post_id]: 400299
[parent_id]: 
[tags]: 
Adaboost Training Error and It's Trend

The Adaboost M1 algorithm is as follows: $\mathbf{Input}$ : sequence of m examples $ $ with the labels $y_i \in Y = \{1,...,k\}$ weak learning algorithm WeakLearn integer T specifying the number of iterations $\mathbf{Initialize}$ : $D_i(i) = \frac{1}{m}$ for all i $\mathbf{Do}$ t = 1,2,...,T: 1. Call WeakLearn, providing it with distribution $D_t$ 2. Get back a hypothesis $h_t : X \rightarrow Y$ 3. Calculate the error of $\displaystyle h_t: \epsilon_t = \sum_{i:h_t(x_i)\neq y_i} D_i(i)$ If $\epsilon_t > 0.5$ , then set $T = t - 1$ and abort loop. 4. $\displaystyle \beta_t = \frac{\epsilon_t}{1-\epsilon_t}$ 5. Update distribution $D_t$ as $\displaystyle D_{t+1} = \frac{D_t(i)}{Z_t} \times \begin{cases} \beta_t, & \text{if $h_t(x_i) = y_i$} \\ 1, & \text{otherwise} \end{cases} $ where $Z_t$ is a normalization constant (chosen so that $D_{t+1}$ will be a distribution) Someone explained to me that the error, $\epsilon_t$ would decrease as the algorithm iterates. However, this does not make sense to me. I am using an SVM as a WeakClassifier and training on samples drawn from $X$ , based on each samples weights. This makes SVM a weak classifier. Now assume that $\epsilon_t$ is 0.23. This means that $\beta_t$ is 0.29. To update the distribution, I multiply the correctly classified sample's weights by $\beta_t$ and leave the incorrectly classified samples weights unchanged. Now note the fact that I have to normalize the distribution, the distribution at this stage must be less than a sum of 1.0 (because we only decrease some weights and leave others unchanged). Now, when I have to normalize this new distribution with $Z_t$ , which is the sum of $D_t$ and so less than 1.0. Hence, I increase all the weights in the distribution. However, since it is a distribution only the relative weights matter. However, when I compute the weighted error $\epsilon_t$ , then it won't be less than the previous iteration necessarily - simply because all the weights have increased in absolute terms. Since the weight increases, I end up ending the loop very early sometimes. Does the above analysis make sense or am I missing an important detail?
