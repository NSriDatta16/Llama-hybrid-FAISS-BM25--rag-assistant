[site]: crossvalidated
[post_id]: 515532
[parent_id]: 502750
[tags]: 
Ooh, I like this question. Think about it from the perspective of model capacity. When you don’t use ignore_index to ignore the padding token, the model must predict every token you’ve provided it, including the padding tokens at the end. In essence, you’ve changed the sequence you’re trying to model. It’s no longer W1 W2 W3 ... Wn. Instead, it’s W1 W2 W3 ... Wn PAD PAD ... PAD. A parametric model like most used in NLP has a fixed capacity. Without ignore_index , you’re spending some of your (figurative) capacity budget on learning that PAD is followed by PAD, which is followed by PAD, and so on. This could have been spent on modeling the important data instead.
