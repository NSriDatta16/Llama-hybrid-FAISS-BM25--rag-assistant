[site]: crossvalidated
[post_id]: 627580
[parent_id]: 
[tags]: 
Neural Network ReLU majority of weights small

When I view a histogram of my weights it is very much centred at 0, with the overwhelming majority being very small. I want to ensure I do not have a vanishing gradient problem. I must preface this by saying that my target data is normalised to [0,1] & my input data is standardised. With ReLU I understand that weights being small isn't necessarily bad as it can be viewed as "not enough evidence to fire" the signal. Can this be a sign to simplify my network? I have 4 hidden layers with 64 neurons in each, I am using dropout too. My training and validation losses are quite good so should I be worried at all? Will the distribution of weights normally look like this deeper into the network - I am tempted to reduce the complexity significantly. Edit: I know ReLU is meant to rectify this, but since the gradient of the activation is just $1$ when its input is positive and $0$ when the input is negative surely it is not hard to get dead weights which don't update at all? $\frac{\delta L_i}{\delta w_{pq}} = \sigma ' (a_p) * \gamma$ where $\gamma$ are other terms. If our pre activation is negative the entire gradient update is 0. So is this something I should expect for my network to converge quickly?
