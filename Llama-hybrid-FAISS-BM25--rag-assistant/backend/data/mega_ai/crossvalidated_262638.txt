[site]: crossvalidated
[post_id]: 262638
[parent_id]: 261209
[tags]: 
In my mind it's not enought to compare the centered and scaled scores given by all your individual models (each trying to predict if an observation belong to a given class). As you figured out there would still be an issue due to the different type of distribution (in part the skewness) of your different scores. I think a better idea would be to train a multiple classes classification model on top of the $p$ scores (say you have $p$ classes) you get. To do this you may for instance use logistic regression, Random Forest or Gradient Boosting Machine (GBM). Also, since this approach would consist in stacking models, be careful about the overfitting. The simplest way I would do this : Split your data in 3 equal parts (A, B and C) Train your $p$ single class classifiers on A and compute predictions on B and C Train your "on top" model on the predictions made for B, get the predictions of this "on top" model on the dataset C On C, you then can compute the accuracy of your model (test accuracy, free of overfitting) Remark 1 : if you use a Random Forest for the "on top" model, you may split your data in only 2 parts, and use the OOB (out of bag) scores given by your Random Forest to assess the test accuracy of your final model. Remark 2 : The kind of problem you are facing is common in the field of neural network since for a $p$ multi-class problem, a neural network usually output $p$ single scores (each corresponding to 1 class classification problem) Best
