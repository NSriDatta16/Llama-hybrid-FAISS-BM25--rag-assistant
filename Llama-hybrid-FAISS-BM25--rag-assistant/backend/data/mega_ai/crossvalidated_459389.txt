[site]: crossvalidated
[post_id]: 459389
[parent_id]: 384556
[tags]: 
Have you tried using AIC (Akaike Information criterion) or BIC (Bayesian Information Criterion)? It's possible to implement AIC or BIC to work with hmmlearn. Here is my implementation for GaussianHMM for covariance_type='diag'. If the covariance_type changes then the number of parameters will have to be adjusted for covars_. You can extend it to GMMHMM if you know the number for components of the GMM. import hmmlearn as hmm import numpy as np def bic_general(likelihood_fn, k, X): """likelihood_fn: Function. Should take as input X and give out the log likelihood of the data under the fitted model. k - int. Number of parameters in the model. The parameter that we are trying to optimize. For HMM it is number of states. For GMM the number of components. X - array. Data that been fitted upon. """ bic = np.log(len(X))*k - 2*likelihood_fn(X) return bic def bic_hmmlearn(X): lowest_bic = np.infty bic = [] n_states_range = range(1,7) for n_components in n_states_range: hmm_curr = hmm.GaussianHMM(n_components=n_components, covariance_type='diag') hmm_curr.fit(X) # Calculate number of free parameters # free_parameters = for_means + for_covars + for_transmat + for_startprob # for_means & for_covars = n_features*n_components n_features = hmm_curr.n_features free_parameters = 2*(n_components*n_features) + n_components*(n_components-1) + (n_components-1) bic_curr = bic_general(hmm_curr.score, free_parameters, X) bic.append(bic_curr) if bic_curr Similar question. Number of parameters in Markov model
