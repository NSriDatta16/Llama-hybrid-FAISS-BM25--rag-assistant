[site]: crossvalidated
[post_id]: 321490
[parent_id]: 321470
[tags]: 
The term "model" is generally used in one of two slightly different ways. It can refer to a statistical model . Formally, this is a set of probability distributions. A "learning algorithm" tries to find a distribution from this set which matches the data best. There are different types of statistical models. Linear models and most neural networks correspond to parametric models . Here, each element of the set is identified by a vector of parameters. $$\mathcal{P} = \{ p_\theta : \theta \in \Theta \}$$ A linear regression model could be written as $$\mathcal{P} = \{ \mathcal{N}(y; \mathbf{a}^\top \mathbf{x} + b, 1) : \mathbf{a} \in \mathbb{R}^n, b \in \mathbb{R}\}.$$ In neural networks, the network's architecture defines the set of possible distributions, and the weights and biases identify distributions from that set. Other types of statistical models are graphical models , in which the family of distributions is determined by a graph, or non-parametric models , in which the number of parameters is not fixed. The other use of "model" in machine learning is to refer to a particular instance of a statistical model. I.e., instead of $\mathcal{P}$, "model" can refer to a particular $p_\theta$ where $\theta$ might be the parameter vector found by a learning algorithm. It is also common to use terms to describe a combination of models and algorithms. For example, the term "linear regression" is generally understood to mean maximum likelihood estimation in the linear model. Another example would be variational autoencoders , which refers to variational inference in certain neural network based generative models.
