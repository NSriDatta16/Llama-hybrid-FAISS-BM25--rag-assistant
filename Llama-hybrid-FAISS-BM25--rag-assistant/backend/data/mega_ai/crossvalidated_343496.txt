[site]: crossvalidated
[post_id]: 343496
[parent_id]: 
[tags]: 
Why does Cross-Validation in neural networks make sense?

I have 2 similar questions about Cross-Validation in neural networks: If I use the Cross-Validation technique to get a better evaluation of the network's performance. Let's say I use 10-fold Cross-Validation, and on each CV iteration I train the network and then test it on the left-out fold. in the end, the weights in each of those 10 neural networks will be different since it was trained on different data. That means that I compare the performance of 10 different neural networks. So how can I reveal something from all of this? In the case when using CV to grid search for the ideal hyper-parameters (HP). what is the difference between testing each new HP on the same test set or on a new fold? We might get a biased test set either way. The first test set I use for all the HP might be unrepresentative, or the current fold I use to test a specific HP might give me great result only by chance.
