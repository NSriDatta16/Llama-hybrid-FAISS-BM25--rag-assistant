[site]: crossvalidated
[post_id]: 492794
[parent_id]: 
[tags]: 
Least Square Cross Validation for Density Estimation with Histograms

In a 1981 paper by Rudemo an easy to compute expression for the integrated squared error of a histogram relative to the true distribution is derived (Eq. 2.8 of the paper and the last equation in this post). The idea is to choose the bin sizes of a histogram in order minimize the least square error between the true density $p(x)$ and the histogram density $\hat{p}_{\{\Delta_k\}}(x)$ computed with bin sizes $\{\Delta_k\}$ and $k \in [1, K]$ (in the simplest case the bins are all equal and $\Delta_k = \Delta$ ). So we have $$\mathcal{L}(\{\Delta_k\}) = \int_{-\infty}^{\infty} \left[ \hat{p}_{\{\Delta_k\}}(x) - p(x)\right]^2 \mathrm{d}x$$ This is the integrated square error and it can be expanded in 3 terms $$\int_{-\infty}^{\infty} \left[ \hat{p}_{\{\Delta_k\}}(x)\right]^2 \mathrm{d}x - 2 \int_{-\infty}^{\infty} \left[ \hat{p}_{\{\Delta_k\}}(x) p(x)\right] \mathrm{d}x + \int_{-\infty}^{\infty} \left[ p(x)\right]^2 \mathrm{d}x $$ We can ignore the last term since it is independent of $\{\Delta_k\}$ . The first term can be computed exactly because we know $\hat{p}_{\{\Delta_k\}}(x)$ . The second term can be thought of as the average "height" of the histogram under the true distribution. Rudemo proposes to approximate this expectation through a leave-one-out estimator, so that $$\int_{-\infty}^{\infty} \left[ \hat{p}_{\{\Delta_k\}}(x) p(x)\right]\mathrm{d}x = \frac{1}{N}\sum_{i=1}^N \hat{p}_{\{\Delta_k\}, -i}(x_i)$$ where N is the number of observations and $\hat{p}_{\{\Delta_k\}, -i}(x)$ represents the estimated density (histogram "height") at point $x_i$ computed by excluding point $x_i$ from the density estimation. The cost function to minimize can then be written as $$\begin{aligned} \mathcal{L}(\{\Delta_k\}) &= \int_{-\infty}^{\infty} \left[ \hat{p}_{\{\Delta_k\}}(x)\right]^2 - \frac{2}{N}\sum_{i=1}^N \hat{p}_{\{\Delta_k\}, -i}(x_i) \\ &= \frac{2}{N(N-1)} \sum_{k=1}^K \frac{n_k}{\Delta_k} - \frac{N+1}{N^2(N-1)}\sum_{k=1}^K \frac{n^2_k}{\Delta_k} \end{aligned}$$ where $n_k$ is the number of observations in bin $\Delta_k$ . Is anyone aware of how to derive this last result? I have found this theorem in a number of places but none of them provides a proof. If it's of any use, the equivalent result for generic kernels can be found in a paper by Scott, see Eq. 3.8 of this paper , and its derivation is available in a number of other sources e.g. Sivlerman's book on density estimation (page 49).
