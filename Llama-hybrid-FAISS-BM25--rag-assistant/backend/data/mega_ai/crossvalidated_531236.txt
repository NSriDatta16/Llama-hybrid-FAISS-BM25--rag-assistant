[site]: crossvalidated
[post_id]: 531236
[parent_id]: 
[tags]: 
Zero Hidden layers + ReLu-Activation + Gradient Descent = Tobit?

when I train a neural network with zero hidden neurons and ReLu activation functions using gradient descent, can it be expected that the resulting weights are approximatly equal to those of a tobit model where the dependent variable is only to be allowed zero or greater than zero? Thanks!
