[site]: datascience
[post_id]: 19090
[parent_id]: 19062
[tags]: 
Suppose we want to fit a function $f(x)$. We can either try to learn a neural network model $F(\cdot)$ so that $F(x) \approx f(x)$. Or, in the residual network approach, we try to learn a neural network model $R(\cdot)$ so that $x+R(x) \approx f(x)$. Why is the latter easier to learn? There's no fundamental reason why it should necessarily be so, in general. It depends on the specific data set. However, one possible intuition is that we might expect that in some settings, $f(\cdot)$ might be approximately linear: i.e., as a first-order approximation, $f(x) \approx x$ might be a reasonable first-order approximation. Then we want to model the error term: i.e., suppose $f(x)=x+r(x)$; then we want to model $r(x)$. In some settings, the error term $r(x)$ might be simpler to model or smaller than $f(x)$. In that case, a residual network architecture might work better. If you like, you can think of this as being akin to a Taylor-series approximation. The Taylor series for $f(x)$ is $$f(x) = c_0 + c_1 x + c_2 x^2 + \cdots$$ Suppose $x$ is small (i.e., $|x| \ll 1$). Then $f(x) = c_0$ is a zero-th order approximation; $f(x) = c_0 + c_1 x$ is a first-order approximation; and so on. At each step, we expect that the residual/error term is probably smaller than the approximation. You can think of a residual network as learning a Taylor series for $f(x)$, in the special case where $c_0 = 0$ and $c_1 = 1$. There are other more sophisticated explanations / intuitions, but hopefully this gives one possible perspective.
