[site]: datascience
[post_id]: 5865
[parent_id]: 5863
[tags]: 
Your reference of Bishop is not entirely accurate. What he states in the paper you linked is It should be noted that the standard sum-of-squares error, introduced here from a heuristic viewpoint, can be derived from the principle of maximum likelihood on the assumption that the noise on the target data has a Gaussian distribution [references cited]. Even when this assumption is not satisfied, however, the sum-of-squares error function remains of great practical importance. The important point with regard to your question is that there is no inherent assumption that there is Gaussian noise when training a Multilayer Perceptron (MLP). Therefore, for an MLP, the sum-of-squares error function is not derived from the principle of maximum likelihood. For example, consider training an MLP to learn the XOR function. There are four pairs of inputs with corresponding outputs but there is no noise in the data. Yet the sum-of-squares error is still applicable. The relevance of using sum-of-squares for neural networks (and many other situations) is that the error function is differentiable and since the errors are squared, it can be used to reduce or minimize the magnitudes of both positive and negative errors.
