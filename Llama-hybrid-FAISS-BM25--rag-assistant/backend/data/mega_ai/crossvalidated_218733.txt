[site]: crossvalidated
[post_id]: 218733
[parent_id]: 217995
[tags]: 
There is a result from 1936 by Eckart and Young ( https://ccrma.stanford.edu/~dattorro/eckart%26young.1936.pdf ), which states the following $\sum_1^r d_k u_k v_k^T = arg min_{\hat{X} \epsilon M(r)} ||X-\hat{X}||_F^2$ where M(r) is the set of rank-r matrices, which basically means first r components of SVD of X gives the best low-rank matrix approximation of X and best is defined in terms of the squared Frobenius norm - the sum of squared elements of a matrix. This is a general result for matrices and at first sight has nothing to do with data sets or dimensionality reduction. However if you don't think of $X$ as a matrix but rather think of the columns of the matrix $X$ representing vectors of data points then $\hat{X}$ is the approximation with the minimum representation error in terms of squared error differences.
