[site]: datascience
[post_id]: 116612
[parent_id]: 116610
[tags]: 
Regarding first: Usually dropout is only active during the training phase of your model (Exceptions to this include models providing simple uncertainty quantification). Thus, during inference of new unseen inputs the whole model is used, essentially resulting in a superposition of all previously trained sub-models, which were created using dropout. I think this what they mean by averaging. Regarding second: You need to consider the given context. Given a model with n neurons, which have the binary trait of being either enabled or disabled by dropout, results in 2^n possible sub-models.
