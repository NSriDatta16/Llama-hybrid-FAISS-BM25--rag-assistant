[site]: datascience
[post_id]: 67219
[parent_id]: 67206
[tags]: 
In self-attention, it is not the decoder attending the encoder, but the layer attends itself, i.e., the queries and values are the same. In practice, this is usually done in the multi-head setup. You can view that as every head focusing on collecting different kinds of information from the hidden states. In multi-headed attention with $H$ heads, you first linearly project the states in $H$ query vectors, $H$ key vectors, and $H$ value vectors, apply the attention, concatenate the resulting context vectors and project them back into the same dimension.
