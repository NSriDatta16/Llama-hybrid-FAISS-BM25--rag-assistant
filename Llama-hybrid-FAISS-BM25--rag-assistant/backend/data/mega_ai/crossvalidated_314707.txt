[site]: crossvalidated
[post_id]: 314707
[parent_id]: 
[tags]: 
Time series- log returns statistics, test normality

I downloaded data from yahoo to study all the statistics of the time series. I then calculated the returns by using: $Ln(\frac{S_t}{S_{t}-1})$ using the adj close price in the following excel file: Then I calculated the daily mean using the =AVERAGE function, Variance using the =VAR function, Volatility, skewedness and kurtosis and I plot an histogram. To check if the time series comes from a Normal Distribution. To check if it in fact comes from a normal distribution I can test it using different statistic tests such as: • The Kolmogorov-Smirnov test (K-S) • Shapiro-Wilk (S-W) • D’Agostino-Pearson omnibus test or • Jarque-Bera test Or I also saw to follow a normal distribution my data should have a skweness= 0 and kurtosis=3. But because in my case my skewness is = -0.56 and kurtosis is= 8.7 I can find the Standard Deviation error for both and by definition: If the absolute value of the skewness for the data is between +/- twice the standard error this indicates that the data is symmetric, and therefore normal. Similarly if the absolute value of the kurtosis for the data is between +/- twice the standard error this is also an indication that the data is normal. I am confused, is it enough to show the above conditions regarding the skweness and kurtosis to show that my data follows a Normal distribution or should I use the above statistic tests? And which one is better when we are dealing with log returns? Another question that I have is I know that $r_t = N=(\mu,\sigma)$, we assume that $r_t$ is: $r_t=\mu +\sigma\epsilon_t$ Normally distributed and independent with mean µ, and (stdev) σ. $\epsilon-> N=(0,1)$ and $\sigma\epsilon_t=> N(0,\sigma)$ In my understanding the $r_t$ are the erros of the time series, and I found that this time series is called a constant return model but I am not sure if this is correct, can anyone clarify this? So, this means that the model is then given by: Unconditional 1st moment mean: $E[r_t]=\mu$ Unconditional 2ns moment variance $Var[r_t]=\sigma^2$ The 3rd and 4th moments are: Skweness: $\frac{E[r_t-\mu^3]}{\sigma^3}$ Kurtosis: $\frac{E[r_t-\mu^4]}{\sigma^4}$ The model is then: $Ln(S_t)=Ln(S_{t-1})+\mu+\sigma\epsilon_t$ which is a randon walk. Is this correct? I am a bit confused regarding the $r_t$ which I believe that represents the errors of this time series and the model given by: $Ln(S_t)=Ln(S_{t-1})+\mu+\sigma\epsilon_t$. Also, I am not sure if the mean and variance uses the unconditional formula. Can anyone help me on this? Thanks
