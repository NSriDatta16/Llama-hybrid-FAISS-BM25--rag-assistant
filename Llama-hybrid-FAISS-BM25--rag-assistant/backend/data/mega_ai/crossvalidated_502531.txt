[site]: crossvalidated
[post_id]: 502531
[parent_id]: 
[tags]: 
Elementary explanation of Gaussian Processes

I'm learning of Gaussian Processes (GP) in the context of Bayesian Optimization (BO), where a surrogate function is learned to approximate the reward signal then a (hopefully) global optima is selected. Because GP are the backbone of BO, I'd like to get a better feel for what it's all about. Thus far, I've heard two explanations: GPs are distributions over functions. GPs are spline regression on steroids (instead of a supplied number of nodes, it's assumed an infinite number or at least one per observed point.) These definitions don't seem to agree with one another; however, there's one thing they agree on: The kernel choice and hyperparameters are the most important elements of the GP. Here's how I currently understand GPs: A GP receives some input data and initializes a "need" for several normal distributions (one over each observation.) the GP randomly samples distribution parameters for each observation The kernel, which largely regulates the cohesion of adjacent observation's distributions, evaluates the goodness of fit. This process repeats over and over until it converges on a posterior (of posteriors over each observation). Could someone comment on how far this is off from reality? (If quite far, please provide an explanation.)
