[site]: crossvalidated
[post_id]: 175424
[parent_id]: 175318
[tags]: 
Hyperparameter search uses a very wide variety of optimization methods. In my opinion, Nelder-Mead is often a poor choice because hyperparameter response surfaces are usually not smooth with tons of local optima. In my experience, Nelder-Mead almost always gets stuck in poor local optima (in the context of optimizing hyperparameters for machine learning methods, so ymmv). Common methods include: Nelder-Mead particle swarm optimization genetic algorithms harmonic search racing algorithms EGO Bayesian optimization ... Many of the metaheuristic methods mentioned above are offered in Optunity , while several other packages offer Bayesian optimization (e.g. Hyperopt , SMAC and BayesOpt ).
