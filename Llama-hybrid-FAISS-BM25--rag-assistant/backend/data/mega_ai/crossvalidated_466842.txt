[site]: crossvalidated
[post_id]: 466842
[parent_id]: 455713
[tags]: 
You are right that doing a comparison in the time/waveform domain is susceptible to large differences based on things that don't impact the speech much, like minor time shifts or phase differences. You can do a little bit better by operating in the Time-Frequency domain and discard phase, using a time resolution of around 25 ms. Typical representations are Melspectrogram and Gammatone spectrograms, usually log-scaled. However this is still far from a compact/efficient representation of the things important to human perception of audio or speech. So there exists many proposals for Perceptual Audio metrics, and in the latter years a lot of work on differentiable ones that can be used as a loss function in Deep Neural Networks. If you do a litterature search for combinations of "perceptual loss", "audio", "differentiable", "generative", "text to speech" you should find tens of relevant papers. A very recent paper (January 2020) that also has a clean open-source code (using Tensorflow), is A Differentiable Perceptual Audio Metric Learned from Just Noticeable Differences by Pranay Manocha et al. The code can be found here: https://github.com/pranaymanocha/PerceptualAudio
