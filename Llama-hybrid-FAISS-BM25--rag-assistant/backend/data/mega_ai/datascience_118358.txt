[site]: datascience
[post_id]: 118358
[parent_id]: 
[tags]: 
How much data and computation power do I need to train a machine translation model using Transformer architecture?

I am working right now on creating a dataset to use in creating a machine translation model to translate between two dialects. I have two questions that I am trying to find an answer for: How much data is enough? how many pairs of sentences would be sufficient to have a decent translation model that can prove the concept? How much computation power would I need if I am going to use the transformer architecture? Would GPUs on Google colab be sufficient?
