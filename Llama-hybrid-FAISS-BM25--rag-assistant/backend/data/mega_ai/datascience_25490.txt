[site]: datascience
[post_id]: 25490
[parent_id]: 25399
[tags]: 
I am not very sure what do you mean as input. In Supervised Learning the learning signal comes from the difference between true response and model's response ("teacher's supervision"). In Reinforcement Learning the learning signal comes from the reward which might come delayed, sometimes not at all in that particular trial (but in another yes) etc. Deep-Q learning which basically is Q-learning with function approximation is a Model-free RL. It means that at the end you want your system to learn a mapping between stimulus and response. Think of it as a reflex elicited by a stimulus. The reward cannot be the input to your system as it is your learning signal. If you are referring to the experience replay, as I mentioned to you, the reward sometimes doesn't come "on time". So we need to decorrelate states,actions and sequences and that's why we don't update the network at every single time step. Instead we prefer to build a buffer with experience and sample from that. So as you stated, you want to learn to avoid enemies not locations and for this if you sample experience from the buffer the network's training will be more "intuitive".
