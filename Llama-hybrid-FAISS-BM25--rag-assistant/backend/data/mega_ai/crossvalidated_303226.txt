[site]: crossvalidated
[post_id]: 303226
[parent_id]: 81778
[tags]: 
While I agree with the existing answers, I think one nuance hasn't been stressed enough and I will try to illustrate it with an example. If you increase $n$, the power of your test rises which means that you are better able to detect real effects. If your $n$ is very large, you will be able to detect very small effects which are practically irrelevant. Although not of practical relevance, those tiny effects that you can detect with very large $n$ are still real though. They are not statistical flukes as they are sometimes mistakenly called. A large $n$ doesn't give you an increased risk of finding small effects that do not exist. If such was the case, we would need to warn against studies with large $n$. The detected small effects are real, but unimportant for reasons related to practical applications. Let's suppose you measure the height of one million male babies, all exactly one year old since their birth and none of which were prematurely born. One week later, you measure the same million babies again. Usually, I would advice to use two-sided tests, but here we can exclude that the babies have shrunk I would say: $H_0$ The babies have not grown $H_a$ The babies have grown You can, for each baby, subtract both measures and see how much it has grown. This gives you one column with a million measures. A t-test can show if it's mean is significantly larger than 0. The enormous sample size of 1000000 and the paired setup increase the power of the test. According to the WHO the median baby grows from 75.7cm to 87.8cm between his first and second birthday. Assuming linearity within that year, babies would grow on average $0.23cm$ during the week that our observations span. With the enormous $n$, I am quite confident we could find a highly statistically significant result that the growth was not zero, a very low p-value. That growth of the average baby is real, not a statistical fluke stemming from our large sample size. But does that mean this result has practical relevance? No, as a clothing manufacturer I still wouldn't produce separate onesies for 53 week olds than for 52 week olds for two reasons: 0.23cm is not enough of a difference to warrant another intermediate size of clothing. Those 0.23cm are for the average baby anyway, not for each individual baby. Individual shoppers are used buying smaller or larger sizes than indicated for the age. Because the variance of baby height is considerable and doesn't depend in any way on our study, some parents don't have the choice but to do this. This variance of baby height is not to be confused with the variance of the estimator of the average baby height, that one decreases with $n$, hence the power of the test. In this example, the scale on which the test compares measures is intuitive and the effect size can and should be measured with a confidence interval on that scale. In some scenarios that cannot be done, for example when comparing the effect sizes of multiple tests on different scales. Standardized effect size measures like Cohen's d can then be used. Simulation in R: I cannot simulate what it would look like to measure the same babies twice, but I can reasonably approximate a test in which a million 52 week olds were measured and another million other babies which are all 53 weeks old were measured for comparison. This test is not paired and has lower power than the paired one. If I can show that the practically irrelevant $0.23cm$ are statistically significant with this test, they will be significant with the other as well. Height is normally distributed and from the WHO data, I can roughly deduce the following distributions: 52 week olds: $\mathcal{N}(\mu=75.735,\sigma=(80.35 - 71.10)/4=2.313)$ 53 week olds: $\mathcal{N}(\mu=75.995,\sigma=(80.79 - 71.25)/4=2,385)$ I did this using the thumb rule that there are 4 standard deviations $\sigma$ between the $0.025^{th}$ and the $0.975^{th}$ quantile. > t.test(rnorm(1e+6,75.995,2.385), rnorm(1e+6,75.735,2.3125), alternative = "two.sided", mu = 0, paired = FALSE, var.equal = FALSE, conf.level = 0.999) Welch Two Sample t-test data: rnorm(1e+06, 75.995, 2.385) and rnorm(1e+06, 75.735, 2.3125) t = 79.403, df = 1998100, p-value I even used the Welch test that doesn't assume equal variances (they are very close to equal) and used a two sided test (babies will not shrink). Both of these circumstances reduce the power of the test. And yet the statistical significance of the test is exceptional with $p-value Regarding the effect size we can say that 999 out of 1000 confidence intervals would contain the true height gain of the average baby and our confidence interval is $[ 0.253, 0.275 ]$ cm which is a practically irrelevant although real quantity.
