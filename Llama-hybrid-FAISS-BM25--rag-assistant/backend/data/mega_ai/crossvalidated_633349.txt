[site]: crossvalidated
[post_id]: 633349
[parent_id]: 
[tags]: 
What is the best way to measure sparsity of a neural network?

I (computer scientist) was discussing with a friend (mathematician) about what is the best way to measure the sparsity of a neural network. Consider a very simple model with two layers: $\hat{y}=\sigma\left( B \cdot ReLU(A\cdot \mathbf{x})\right)$ , where $ReLU$ is the rectified linear unit activation and $\sigma$ would be the softmax. This is a toy example I have for myself and the way I measure sparseness is to count the number of weights that $- \epsilon . The absolutely right way would be to take the L-0 norm. But when training a neural network with 32bit floats you never have 0s in your weights, unless you initialize them in that way. My friend told me about proximal gradient methods to measure sparsity in this case. I am not familiar with them and I do not understand how it could be applied to measure sparsity. If someone could point me to a paper or anything I would be extremely thankful.
