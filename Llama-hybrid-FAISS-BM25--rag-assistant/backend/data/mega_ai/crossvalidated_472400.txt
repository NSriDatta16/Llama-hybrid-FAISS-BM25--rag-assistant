[site]: crossvalidated
[post_id]: 472400
[parent_id]: 
[tags]: 
Accounting for errors in measurement instead of measurement error?

I have a question regarding whether or not a certain type of statistical model exists. What I need to model is an error of measurement, and not measurement error in the sense of what I've generally come across in my learning of statistics. If this situation is indeed a simple case of measurement error, and model typically used to deal with measurement error is appropriate, please correct me, as I really need to understand this content (PhD student in social science). I have a data-set consisting of acoustic measurements of speech taken from digital recordings. In this data-set, certain measurements are incorrect. There are 3 dependent variables, and for all of them, some values may not be representative of the reality they are supposed to measure. This is because the measurements are made by a computer algorithm that is not perfect, and as such a certain percentage of the values in all three dependent variables will be wrong: let's be conservative and say 5% of measurements are outside the range of a possibly correct value given knowledge of general acoustics and known properties of human speech. The only option I see right now is to remove the multivariate outliers outside a certain Mahalanobis distance from the center of the space. This could be used in conjunction with expert knowledge of the phenomenon to make a cutoff at the reasonable value. I would then treat values outside the space as missing data and use a Bayesian imputation method to account for the cells I've deleted. What I'd like to know is if there is a type of model I could run that would automatically detect likely measurement errors and reduce their weighting and/or mitigate their influence on the model. Any suggestions of models or concepts to look into would be greatly appreciated, especially in the form of academic papers or books.
