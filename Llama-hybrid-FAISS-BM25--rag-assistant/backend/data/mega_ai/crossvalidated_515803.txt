[site]: crossvalidated
[post_id]: 515803
[parent_id]: 
[tags]: 
What is the intuition behind the expected value in orginal GAN papers objective function?

I know how Generative Adversarial Network(GAN) works but it became a daunting task to grasp the non mathematical intuition behind the expected value in the objective function $L_D = - \left[\; \mathbb{E}_{x \sim p_{data}}[log \; D(x)] + \; \mathbb{E}_{z \sim p_{z}}[log \; (1 - D(G(z)))] \right]$ After searching I found this post where the author explains the mathematical reasoning for the expected value. I understood most of what he had written but got stuck at this, Finally, D(x) represents the probability with which D thinks that x belongs to pdata. By writing the cross-entropy formula we get: $H(y, D(x)) = \mathbb{E}_y[-log \; D(x)] = \frac{1}{N} \sum_{i=1}^{N}{ \; y_i \; log(D(x_i))}$ Is $D(x)$ here considered as a probability distribution? What is the intuition behind the expected value in it? I am aware that we use expected value to find the mean of a distribution. I am guessing when we pass real images to discriminator we get different mean of distribution when comparing to the fake images. This difference in mean difference is significant for GAN learning!? In my knowledge expected value is written as $E(X) = \sum_{i=1}^{k} x_i p_i$ then how are we deriving $\mathbb{E}_y[-log \; D(x)]$ from $\frac{1}{N} \sum_{i=1}^{N}{ \; y_i \; log(D(x_i))}$
