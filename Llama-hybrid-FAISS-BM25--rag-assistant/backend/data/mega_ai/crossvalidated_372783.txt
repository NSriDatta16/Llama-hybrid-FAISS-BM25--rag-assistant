[site]: crossvalidated
[post_id]: 372783
[parent_id]: 
[tags]: 
Using model from one data set to predict results for another data set

I'm not certain how to phrase this question: I have a dataset of ~45000 execution times of two sets of data. Approximately 35000 of these execution times is ran in one environment, and the remaining ~10000 are ran in a very different environment. The goal of the execution is the same, The first dataset has execution times ranging from 1-300 seconds, and the second set has a theoretically unbounded, but practical range of 1-7000 seconds. Both datasets perform the same task, but in slightly different ways, over entirely different inputs. I have a predictive model for the first dataset, which is accurate on new data which belongs to the same set, e.g., I am able to use the model from set 1 to predict the execution time of new executions running in the same environment. However, when I try to predict the execution times of data from set B, on the environment from set A - I get suspicious results. e.g., the predicted times are bound between 0 and 300. I've tried both a linear regression and random forest regression. The random forest regression resulted in predictions from 0-300 and the linear regression resulted in non-sensical predictions (e.g., ranging from -4000 to 4000). What are some other approaches I can try - I cannot get a better range of response variables from the first set - as that environment is bound to a 300 second execution time.
