[site]: crossvalidated
[post_id]: 448761
[parent_id]: 448757
[tags]: 
Feature engineering is about data and is the process of finding/creating features that might improve your model's performance. You sometimes engineer new features from raw data you have, use the existing ones and perform univariate/multivariate transformations. So, by engineering more features your linear regression model may for example become $y=w_1x_1+w_2x_2+b$ , where $x_1,x_2$ are your engineered features. Hyperparameter optimization (HPO) is related to the model you have, not the data. Many models have hyper-parameters to be tuned. In simple linear regression without regularization we don't have any. The $w$ coefficient is a parameter of the model, as well as $b$ . They're not hyperparameters. A hyperparameter can be the regularization coeffcieint, $\lambda$ in your regularized linear regression model.
