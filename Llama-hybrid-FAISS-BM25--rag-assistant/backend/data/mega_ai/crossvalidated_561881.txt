[site]: crossvalidated
[post_id]: 561881
[parent_id]: 561874
[tags]: 
OK so I have figured out what is happening by examining the case of positive predictive value which also had deviations in outcome depending on whether the total PPV was calculated vs averaging all users PPVs. Im using SKlearn. Organization of the confusion matrix is as follows: [[TN, FP] [FN ,TP]] CM for user 1 [[29 6] [ 1 14]] ppv: 0.7 (14/(14+6)) CM for user 2 [[26 9] [ 0 15]] ppv: 0.625 (15/(15+9)) Therefore the PPV achieved by averaging the two computed PPVs (0.7+0.625)/2 = 0.6625 However the PPV computed from the data frame containing both users 1 and 2 is below [[55 15] [ 1 29]] ppv: 0.659091 Computing the entire PPV from the data frame rather than from averaging individual users PPVs gives 0.659091 rather than 0.6625 This might seem like a trivial difference but as more users are added I find the deviation (particularly for PPV) worsens. I think the correct procedure is to compute the individual users PPVs and then average them to give an overall PPV. However that is my gut instinct. If anyone can explain the maths behind this I would be grateful.
