[site]: datascience
[post_id]: 109953
[parent_id]: 
[tags]: 
How does bert produce variable output shape?

Suppose if I provide a list of sentences: ['I like python', 'I am learning python', # longest sentence of length 4 tokens 'Python is simple'] Bert will produce an output of (3 * 4+2 * 768). Because there were 3 sentences, 4 max tokens, 768 hidden states. Suppose if I provide another list of sentences: ['I like python', 'I am learning python', 'Python is simple', 'Python is fun to learn' # 5 tokens ] The new embedding output would be (4 * 5+2 * 768). I understand that dim[0] becomes 4 because there is now 4 sentences instead. This is achieved by increasing the rows of the tensor(batch size) during tensor computation. I also understand that dim[1] becomes 5+2 because the max number of token is number 5 and there is [CLS] and [SEP] tokens at the start and end. I also understand that there is a padding mechanism that accepts up to a max_position_embeddings=512 for bert model. What I want to ask is: during computation, does bert pad all the values after 5th element with zeros and process with computation using a input of (4 * 512) (4 sentences, 512 max tokens). then after computation from the output of (4 * 512 * 768), the tensor is trimmed to output: (4 * 5+2 * 768). if the above assumptions is true, isn't it a huge waste of resources, since majority of the 512 tokens are not attention-required. I read about the attention_mask matrix that tells the model which are the tokens needed for computation, but I don't understand how does attention_mask achieve this; when the architecture of the model is initialised with N dimensional inputs, how does attention_mask help during computation to ignore/avoid the computation of the attention-masked elements? which part of the bert model explicitly restrict the output to (4 * 5+2 * 768)?
