[site]: crossvalidated
[post_id]: 553557
[parent_id]: 553545
[tags]: 
In general, latent variables are some variables we can't directly measure. That is, we cannot directly measure an 8th graders' level of math knowledge, simply because we cant directly connect to their brains and perform this measurement (unlike physical attributes, for example, which we can directly measure). So we provide them with a set of math-related tests - 2 geometry tests, 2 algebra tests, 2 more of word problems. We would like to assume that this set of quizzes properly represents the various aspects of math knowledge for this level (this gets very deep into the area of psychometrics). Now, assume we have the various scores for each student - we would like to find out the structure of our latent variable, which is math knowledge of 8th graders. There are many methods for this kind of dimensionality reduction, I will relate to the 2 I think are most important: PCA : principal component analysis takes a data of $p$ columns and returns... $p$ columns, each is a linear combination of the original ones. The new variables are promised to be orthogonal to each other, and sorted by proportion of explained variance. Given a threshold (lets say 98% explained variance), we can find what is $k$ for which now columns $1...k$ explain at least 98% of the variance. The upside: very intuitive, easy to implement, orthogonality. The downside: new variables aren't promised to be interpretable. Factor Analysis : Unlike PCA, the FA requires us to provide in advance $k$ , the number of required factors ( $k , there's a rule of thumb which I can't find right now), then it constructs the appropriate linear combinations. Output variable might or might not be orthogonal to each other (depends on the rotation method selected), but they are almost bound to be interpretable. Moreover, if we construct 2 new variables or 3 new variables, the output variables would differ. If we get back to the collection of math tests, requiring a single factor would probably yield a "general math ability". Requiring 2 factors would yield variables that can be interpreted as "quantitive ability" and "geometry ability", requiring 3 might yield "algebra", "geometry" and "problem-solving" or "math ability", "visual perception" and "reading comprehension". These are just examples. The upside: interpretability, small number of factors. The downside: algorithm is more complex, new variables aren't promised to be orthogonal (which might not be a downside for some). In this context, I think you should run some exploratory factor analysis on your data to find out if there's anything interesting there. I do think than $n=100$ is not a very large dataset. You have namedropped clustering - this is not a method for finding latent variables (at least not in the common meaning) but rather for finding whether the $n$ samples could be grouped to some $g$ groups.
