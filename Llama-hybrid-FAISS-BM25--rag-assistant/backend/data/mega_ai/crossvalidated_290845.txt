[site]: crossvalidated
[post_id]: 290845
[parent_id]: 
[tags]: 
How to remove features in logistic regression model, yet still retain accuracy?

I have a dataset with 75 features that I plan to use to create a logistic regression model. However, I want to reduce this number of features as my computer can barely handle this large dataset. Accuracy wise, are there any negative effects of having too many features in a logistic regression model? Using dummy variables after one hot encoding also drastically increases the number of columns, does this too have a negative effect on accuracy? In regards to Pearson, I am thinking of testing it on all of my continuous features and when two continuous features have a correlation of above .7, I will remove one of the features at random from the training set. I won't remove outliers, because outliers will reduce the Pearson correlation coefficient and therefore make me keep both features, which makes sense because both features impact the logistic regression model in different ways because of the presence of outliers. Is my understanding sound? Finally, is there a good way to test correlation between features that are not continuous, specifically features that I one hot encode using dummy variables? Spearman's tests ordinal variables so I'm not sure it would work on one hot encoded features.
