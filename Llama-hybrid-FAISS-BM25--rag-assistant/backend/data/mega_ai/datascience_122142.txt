[site]: datascience
[post_id]: 122142
[parent_id]: 
[tags]: 
Is sequence length and hidden size is the same in Transformer

I'm confused about sequence length and hidden size if they are the same in Transformer. I think they are different but not sure. class Embeddings(nn.Module): """Construct the embeddings from patch, position embeddings. """ def __init__(self, config, img_size, in_channels=3): super(Embeddings, self).__init__() self.hybrid = None self.config = config img_size = _pair(img_size) b_size=16 if config.patches.get("grid") is not None: # ResNet grid_size = config.patches["grid"] patch_size = (img_size[0] // b_size // grid_size[0], img_size[1] // b_size // grid_size[1]) patch_size_real = (patch_size[0] * b_size, patch_size[1] * b_size) n_patches = (img_size[0] // patch_size_real[0]) * (img_size[1] // patch_size_real[1]) self.hybrid = True else: patch_size = _pair(config.patches["size"]) n_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1]) self.hybrid = False if self.hybrid: self.hybrid_model = ResNetV2(block_units=config.resnet.num_layers, width_factor=config.resnet.width_factor) in_channels = self.hybrid_model.width * 16 self.patch_embeddings = Conv2d(in_channels=in_channels, out_channels=config.hidden_size, kernel_size=patch_size, stride=patch_size) print("config.hidden_size",config.hidden_size) print("n_patches",n_patches) self.position_embeddings = nn.Parameter(torch.zeros(1, n_patches, config.hidden_size)) self.dropout = Dropout(config.transformer["dropout_rate"]) def forward(self, x): if self.hybrid: x, features = self.hybrid_model(x) else: features = None x = self.patch_embeddings(x) # (B, hidden. n_patches^(1/2), n_patches^(1/2)) x = x.flatten(2) x = x.transpose(-1, -2) # (B, n_patches, hidden) embeddings = x + self.position_embeddings embeddings = self.dropout(embeddings) return embeddings, features
