[site]: datascience
[post_id]: 118370
[parent_id]: 118359
[tags]: 
The problem with your approach is catastrophic forgetting. If you train only on the samples not properly classified in the previous epoch, then, after training the following epoch, your model will perform poorly on the "easy data" because it overfitted the difficult one. Also, the behavior of the model at the beginning of an epoch is much different than at the end, so, in order to achieve your proposed approach properly, you would need to perform inference again on the whole training data used at that epoch. This would certainly be very costly. What you propose vaguely resembles a technique called "boosting", but instead of using a single model, boosting trains different models sequentially. The first model is trained on all data. The second model is trained focusing on the data where the first model performed badly (the data are given "weights" and the weights increase or decrease from one model to the next depending on the correctness of the predictions of the previous model). And so on. However, with boosting the first model is never touched again after training on all data, so it retains its classification power. Also, boosting is normally used with "weak" classifiers (i.e. not deep neural networks), although not necessarily.
