[site]: crossvalidated
[post_id]: 298414
[parent_id]: 
[tags]: 
Why take the derivative when using slsqp algoritm?

I'm looking at the documentation for scipy's optimize minimize module as seen here . The problem supposes a function as well as two constraints. Objective Funtion: f(x, y) = 2 x y + 2 x - x^2 - 2 y^2 subject to an equality and an inequality constraints defined as: x^3 − y = 0 y - 1 ≥ 0 From here the documentation proceeds to take these steps: def func(x, sign=1.0): ... """ Objective function """ ... return sign*(2*x[0]*x[1] + 2*x[0] - x[0]**2 - 2*x[1]**2) >>> def func_deriv(x, sign=1.0): ... """ Derivative of objective function """ ... dfdx0 = sign*(-2*x[0] + 2*x[1] + 2) ... dfdx1 = sign*(2*x[0] - 4*x[1]) ... return np.array([ dfdx0, dfdx1 ]) Note that since minimize only minimizes functions, the sign parameter is introduced to multiply the objective function (and its derivative) by -1 in order to perform a maximization. My question is: Why are they taking the derivative at all? Scipy is a module of python, which is a programming language afterall, why do we need to provide that for the module; it seems uncharacteristic of a language that is usually very intuitive and elegant. Here are some of my guesses thus far: If you were optimizing by hand, then you could imagine they wanted to set the derivative to zero and solve for x, but that does not seem to be what they are doing, so maybe this is not the reason. Are they just showing that step for robustness/clarity? Or perhaps the derivative is needed because they intend to maximize the function using the minimize module, which requires a bit of craftiness?
