[site]: crossvalidated
[post_id]: 372342
[parent_id]: 336818
[tags]: 
I think one-hot encoding is just special representation of word. It is unnecessary. If you get a image word representation $v$ from last fc layer of VGG network, and $W$ is hidden layer of embedding, the dot product $v\cdot W$ will give you the embedding code.
