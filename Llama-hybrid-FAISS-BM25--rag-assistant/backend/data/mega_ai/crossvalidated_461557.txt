[site]: crossvalidated
[post_id]: 461557
[parent_id]: 
[tags]: 
Weight Clipping range with WGAN and relation with other factors

For a while, my code with WGAN has failed to generate quality images through a complicated multi-class database. My issue has been with implementation and not code. Recently I read the WGAN-GP paper which points out that the clipping of weights essentially restricts the learning patterns of the discriminator, thus over-simplifying things. I looked at my loss values and saw that for a shallower model-- The losses hit a peak very fast(50th epoch) and then the losses oppose required trends. With a deeper models, they show right trends but the values are terrible. Generator loss is positive for a very very long time and from what I understand, that is not good. My informed guess told me that weight clipping broadening was the way to go. The original implementations for both MNIST and CIFAR-10 used -0.001 to +0.001. When I increased them to -0.1 -> +0.1 and -1 -> +1 I found an improved performance with losses, if not with image quality, still basically glorified noise. But I was looking to see if there was an explicit tie in between my model, learning rate or some other hyper-param with respect to Lipchitz condition and weight clipping. Is there an ideal set of values I might be able to calculate?
