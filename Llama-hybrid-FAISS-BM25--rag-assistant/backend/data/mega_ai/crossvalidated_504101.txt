[site]: crossvalidated
[post_id]: 504101
[parent_id]: 486406
[tags]: 
I'll go through some of your questions: Why can we use this reconstruction loss to judge the pixel-wise differences between the input x and the decoded latent sample z? Implicitly, we assume that pixel values are Normally distributed with uniform diagonal covariance. I mean, x gets encoded and specifies some latent distribution over z's. Now we sample from it to get a specific z. But this z does not have to be meant to be the latent code for the input, doesn't it? I think you are mixing two generative strategies here: VAEs and GANs (generative adversarial networks). Most (not all) GANs don't try to replicate inputs, instead they simply try to create realistic looking images to fool the discriminator. Since in GANs we do not have a mapping from image to latent code, there is no correspondence (BiGANs/ALI, for example, has this). VAEs come from a complete probabilistic point of view. In VAEs we want to retrieve the posterior of the distribution that generates the images. $$p(z|x) = \frac{p(x|z)p(z)}{p(x)}$$ This is very expensive (and actually intractable) due to $p(x)$ , and we instead approximate it by $q(z|x)$ , an amortized model of the actual posterior (this the VAE encoder). So the decoded version of z does not have to show the same image content? Let's say we are using face images. The input could be male. The sampled z could be producing a female person? So using MSE between these two seems to be wrong? As I explained above, ideally, the reconstructed image should look like the input as best as well as the latent code can afford it.
