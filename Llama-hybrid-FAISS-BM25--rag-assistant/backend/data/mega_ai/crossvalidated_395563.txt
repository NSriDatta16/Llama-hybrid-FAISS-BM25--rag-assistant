[site]: crossvalidated
[post_id]: 395563
[parent_id]: 
[tags]: 
What's the term used when identical feature vectors map to different target variables?

Context: Fitting a Machine Learning Algorithm on a labeled dataset. For a feature vector [a,b,c] and a labeled output/target variable, what's the term used when identical feature vectors map to two (or more) different target variables? I've been using 'coincident feature target collision' but I'm sure there's a better, more formal, community accepted term for this. :) import pandas as pd data = {'a':[0,0,0,0,0], 'b':[0,0,1,1,1], 'c':[0,1,0,1,0], 'target':[0,1,0,1,1]} df = pd.DataFrame(data) print(df.head()) a b c target 0 0 0 0 0 1 0 0 1 1 2 0 1 0 0 In the machine learning context, we observe that for this scenario the machine learning algorithm will not be able to properly fit a model that predicts the target variable. Update: @whuber thanks for the comment. Looking around a bit I see you also made a comment on this somewhat related question: reducible vs irreducible error . Also from this article the term 'irreducible error' is called out https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/ . "The irreducible error cannot be reduced regardless of what algorithm is used" That article also gives this definition "Variance is the amount that the estimate of the target function will change if different training data was used." So using the general term 'variability' for this particular issue which I'm clumsily calling 'coincident feature target collision' would be ambiguous at best and highly confusing at worst. I feel like it's part of the 'irreducible error'. Another quote from the article: It is the error introduced from the chosen framing of the problem and may be caused by factors like unknown variables that influence the mapping of the input variables to the output variable.
