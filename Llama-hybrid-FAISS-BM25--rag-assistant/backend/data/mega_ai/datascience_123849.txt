[site]: datascience
[post_id]: 123849
[parent_id]: 
[tags]: 
How to add multiple embeddings (layers) to LSTM layer

The similar question was asked before here https://stackoverflow.com/questions/52627739/how-to-merge-numerical-and-embedding-sequential-models-to-treat-categories-in-rn/52629902#comment136040845_52629902 , but I didn't understand a clear answer. I will use the authorâ€™s @GRS materials to explain. I'm trying to solve the problem of classifying sequences of categorical features. For this task, the vector of the last hidden state from the LSTM layer is very often used. For each categorical variable of the sequence I define learnable layer nn.Embedding. The question is which approach is better and more correct: create a separate lstm layer for each feature - categorical embedding layer (as in the picture), or combine all embeddings and transfer them (like one tensor) into a single lstm layer? The first approach I never see, but I don't understand, why it is not popular. Thank you very much in advance
