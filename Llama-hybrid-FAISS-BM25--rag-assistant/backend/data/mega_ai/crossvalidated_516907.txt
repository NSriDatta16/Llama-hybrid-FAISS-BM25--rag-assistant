[site]: crossvalidated
[post_id]: 516907
[parent_id]: 516901
[tags]: 
AutoEncoder is not a classifier, but you can use it as a layer before your classification layers. The reason to use AutoEncoder is to get a better representation of your input, you can think of it as a dimensionality reduction technique like PCA (but a nonlinear one). In order to build an end to end classifier with AutoEncoder you can do the following: Build the following neural network: Given the input is X, pass it via several layers and output a dense vector D. D will be your dense input representation. Given D you now have two tasks: Classification task: Take D as an input and pass it via several layers with sigmoid as final activation to get the classification output Y*. The loss for this task will be a simple cross entropy loss with the label and the output Y*. AutoEncoder auxiliary task: Take D as an input and try to reconstruct the input X. In order to do it, pass D via several layers and output a dense vector X*, with the same size as X. The loss for this task will be the average square distance between X and X*. The final loss for your network will be a weighted average between the losses of these two tasks. Backpropagation is used for both tasks. The model validation doesn't change. You still can calculate AUC for your test set using the label Y and the output Y*. One way to implement it will be the following: Take X as an input, pass it via several fully connected layers and output a latent vector D Take D, pass it via additional fully connected layers and output a dense vector K, with the size of |X| + 1. The last element of the vector K will be your logits. pass it via sigmoid to get the probability Y*. The first |X| elements of vector K will be the reconstruction of X. Create a loss function L which is a weighted average of two losses. The first loss is a simple cross entropy loss between the label Y and the prediction Y*. The second loss is the euclidean distance between X and X* Finally minimize L via gradient descent optimizer
