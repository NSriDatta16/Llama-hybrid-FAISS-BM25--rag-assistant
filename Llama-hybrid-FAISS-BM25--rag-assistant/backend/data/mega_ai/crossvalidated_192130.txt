[site]: crossvalidated
[post_id]: 192130
[parent_id]: 
[tags]: 
Off-diagonal elements of a correlation matrix after removing the first principal component

I have some data with more variables than observations , that I'd like to subject to a principal components analysis. For didactic reasons (to give an intuition for factor retention criteria under parallel analysis), I am here interested in the distribution of the residual correlations. Let this be my data (sampled from real data): data (sorry about the clunky input). Now, let's look at the initial correlation and then the residual correlations. (I'm doing this here with psych::principal for the sake of convenience, but I've also calculated the residuals by hand with prcomp , with same results). cor Warning in cor.smooth(r): Matrix was not positive definite, smoothing was #> done #> In factor.scores, the correlation matrix is singular, an approximation is used #> Warning in cor.smooth(r): Matrix was not positive definite, smoothing was #> done # Calculate residuals === cor Warning: Removed 18 rows containing missing values (geom_path). The above plots shows the distribution of correlation coefficients (denoted as PC 0), as well as the residual correlations from the first 8 principal component, all in one frequency polygon plot. The smoother plot uses the full dataset, the rough one just the above sample (can't share the full dataset). I get why the distribution becomes more leptopkurtic (aka: steep) as more components are extracted; that's the whole point of PCA (and this illustration): the remaining correlation matrices approximate a singular matrix with all zeros (and 1s on the diagonal). I also understand why the original correlations are asymmetric -- given my kind of data ( Q-sorts ), that frequently happens. What I don't understand is why the asymmetry seems to disappear completely after the first PC is extracted . Shouldn't the asymmetry dissipate slowly as more PCs are extracted? My questions are: Is this to be expected? Is this in the logic of PCA, or a computational artefact? Is this approach meaningful to illustrate parallel analysis / why you shouldn't overretain components? Also, as a bonus , I'd be really curious what the expected distribution of correlation coefficients from random data would be, but that's another question .
