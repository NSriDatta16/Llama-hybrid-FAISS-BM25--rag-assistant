[site]: crossvalidated
[post_id]: 172628
[parent_id]: 171676
[tags]: 
If new data is expensive/time-consuming to generate, surrogate-model based optimization may be good approach. That is: fit a suitable, data-driven model to your data optimize the surrogate model to derive a new candidate solution evaluate candidate solution (that is, measure true purity) update model with the new data you just obtained repeat 2.-4. until stopping criterion is reached (budget depleted, target purity attained) What defines a suitable model depends on your problem. For something non-linear, neural networks (as noted by usÎµr11852), support vector machines or Gaussian Process Regression (aka Kriging) might work. On the other hand, the computational effort for training a model will be adversely affected if your data-set is rather large. Kriging provides the advantage of giving an uncertainty estimate of its prediction. This allows to compute the Expected Improvement of a candidate solution. While this does not exactly "integrate these 2 steps" (modeling+optimization) EI provides the means to elegantly balance Exploitation vs. Exploration in a surrogate-model based optimization framework. I suggest reading Engineering Design via Surrogate modeling, Forrester et al. (2008) . Since you already have pre-existing data, you can probably skip Chapter 1 (sampling plans), and focus on Chapter 2 and 3. Regarding your constraints: If the constraints themselves are inexpensive to calculate on-the-fly, just respect them in the above step 2. (as you already suggested in your comment). If constraints themselves are expensive to evaluate, you may consider to replace them with a model, too. Similar to EI, you can use the uncertainty estimate of a Kriging model to compute the probability of feasibility .
