[site]: crossvalidated
[post_id]: 199012
[parent_id]: 198989
[tags]: 
This question has been studied extensively in the literature. Pre-training finds a good spot of weights in the error surface. Intuitively, were finding a good set of weights to compressing the input data in the pre-training phase. Ideally, this compressed representation is good for solving generic tasks. For the record, no one pre-trains deep neural networks anymore. They simply drop some connections and train the entire network with backpropagation.
