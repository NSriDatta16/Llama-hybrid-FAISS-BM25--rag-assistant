[site]: stackoverflow
[post_id]: 4935078
[parent_id]: 4934203
[tags]: 
Sort the distances to the 10 centres; they could be 1 5 6 ... — one near, others far 1 1 1 5 6 ... — 3 near, others far ... lots of possibilities. You could combine the 10 distances to a single number, e.g. 1 - (nearest / average) ** p, but that's throwing away information. (Different powers p makes the hills around the centres steeper or flatter.) If your centres are really Gaussian hills though, take a look at Multivariate kernel density estimation . Added: There are zillions of functions that go smoothly between 0 and 1, but that doesn't make them probabilities of something. "Probability" means either that chance, likelihood, is involved, as in probability of rain; or that you're trying to impress somebody. Added again: scholar.google.com "(single|1) nearest neighbor classifier" gets > 300 hits; "k nearest neighbor classifier" gets almost 3000. It seems to me (non-expert) that, out of 10 different ways of mapping k-NN distances to labels, each one might be better than the 9 others — for some data, with some error measure. Anyway, you could try asking stats.stackexchange.com ,
