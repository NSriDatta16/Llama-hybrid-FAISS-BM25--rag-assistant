[site]: datascience
[post_id]: 81178
[parent_id]: 81171
[tags]: 
One approach would be to simply average everything, as proposed in the FedAvg preprint . Some very recent preprints suggest only relaying the learned parameters back to the central server and keeping local batch normalization (BN) statistics separate, as proposed by the SiloBN preprint . The authors of this paper claim that: Keeping BN statistics local permits the federated training of a model robust to the heterogeneity of the different centers, as the local statistics ensure that the intermediate activations are centered to a similar value across centers. To paraphrase them; they distinguish BN statistics as encodings for local domain information whereas the learned parameters are to be domain-invariant. I believe their aggregation method for the relayed information is simply averaging. Here's their first figure for reference:
