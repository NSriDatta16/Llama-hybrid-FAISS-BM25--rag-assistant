[site]: crossvalidated
[post_id]: 530265
[parent_id]: 309642
[tags]: 
The relationship between softmax confidence and uncertainty is more complicated than a lot of work makes it sound. Firstly, there are two separate issues that often get conflated. Callibration - Does 90% softmax confidence mean it is correct 90% of the time? This is evaluated over the training distribution. We are interested in the absolute confidence values. Uncertainty - Does softmax confidence reduce when the network doesn't know something? This is evaluated by comparing softmax confidence on the training distribution to some other data (often called out-of-distribution, OOD). If over the training distribution softmax confidence is in the range 92-100%, on OOD data it should be relative confidence values. Callibration. Deep neural networks typically output very high softmax confidence for any input (say >95%), and are known to be poorly calibrated. As far as I know this is fairly uncontroversial. The classic reference: 'On Calibration of Modern Neural Networks' by Guo et al. . Uncertainty. This issue is less clear cut. There are well-known ways to make softmax confidence fail, such as magnifying an input , or creating adversarial examples . Softmax confidence also conflates two different sources of uncertainty (aleatoric & epistemic). These counterexamples have drawn a lot of attention, leading to claims (made with varying strength) that softmax confidence $\neq$ uncertainty. What's sometimes forgotten in light of these failure modes, is that naively interpreting softmax confidence as uncertainty actually performs pretty well on many uncertainty tasks . Moreover, a lot of methods that claim to 'capture uncertainty' generally don't beat softmax confidence by all that much. The paper, 'Understanding Softmax Confidence and Uncertainty' by Pearce et al. , investigates why softmax confidence performs reasonably in these uncertainty benchmarks, describing two properties of unmodified neural networks that, in certain situations, seem to help softmax confidence $\approx$ uncertainty.
