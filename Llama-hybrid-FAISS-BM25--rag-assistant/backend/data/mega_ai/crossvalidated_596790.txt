[site]: crossvalidated
[post_id]: 596790
[parent_id]: 595344
[tags]: 
Reading the original Friedman paper this question comes from clarified it. The classifiers are trained on a training set $T$ and the distributions of the $\hat{G}$ are conditional on $T$ . For the 0-1 loss, the risk or average error for a classifier (where the average is over $Y$ given any $x_0$ point) is: $$ \textrm{Err}(x_0) = E(\mathcal{L}(\hat{G}(x_0), Y)) = P(Y \neq \hat{G}(x_0)|X = x_0) $$ When $\hat{G}(x_0) = 1$ , $\textrm{Err}(x_0) = P(Y \neq 1 | X = x_0) = 1 - f(x_0)$ . Similarly, when $\hat{G}(x_0) = 0$ , $\textrm{Err}(x_0) = P(Y \neq 0 | X = x_0) = f(x_0)$ . Thus, $$ \textrm{Err}(x_0) = f(x_0)\mathbf{1}(\hat{G}(x_0) = 0) + (1-f(x_0))\mathbf{1}(\hat{G}(x_0) = 1) $$ So for every $x_0$ , the only two possible values of the error are either $f(x_0)$ or $1 - f(x_0)$ . The Bayes classifier achieves the Bayes error, the lowest possible error among classifiers. This means the Bayes error is: $$ P(Y \neq G(x_0) | X = x_0) = \min(f(x_0), 1 - f(x_0)) $$ When $\hat{G}(x_0)$ corresponds to the Bayes classifier, it achieves the Bayes error. When the two do not coincide, the error of the classifier $\hat{G}(x_0)$ differs from the Bayes error and instead achieves error $\max(f(x_0), 1 - f(x_0))$ . Thus, in this case, the difference between the errors is given by: $$ \textrm{Err}(x_0) - \textrm{Err}(x_0)=\max(f(x_0), 1 - f(x_0)) - \min(f(x_0), 1 - f(x_0)) = |1 - 2f(x_0)| $$ Thus, $$ \textrm{Err}(x_0) = |1 - 2f(x_0)|\mathbf{1}(\hat{G}(x_0) \neq G(x_0)) + P(Y \neq G(x_0)|X = x_0) $$ Each classifier is implicitly conditioned on a training set $T$ that is used to train the classifier. Averaging over $T$ (which only appears with terms including $\hat{G}$ ), we get: $$ \textrm{Err}(x_0) = |1 - 2f(x_0)|P(\hat{G}(x_0) \neq G(x_0)) + P(Y \neq G(x_0)|X = x_0) $$
