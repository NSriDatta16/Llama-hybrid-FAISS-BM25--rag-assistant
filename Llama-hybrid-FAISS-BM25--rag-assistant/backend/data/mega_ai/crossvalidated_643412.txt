[site]: crossvalidated
[post_id]: 643412
[parent_id]: 
[tags]: 
Scholkopf single class linear SVM equation: why ρ substracted to 1/2 ||w||² is the same as maximizing the distance

In the one class linear SVM, the equation is : $\min_{w, \rho} \frac{1}{2} \|w\|^2- \rho + C\sum_{i=1}^{n} \xi_i$ subject to: $\begin{align*} & w \cdot x_i \geq \rho - \xi_i, \\ & \xi_i \geq 0, \\ & i = 1, 2, ..., n. \end{align*}$ The first constraint suggests that, when x is a support vector, we have $w \cdot x = \rho$ The distance to a support vector x is $\frac{w \cdot x}{\|w\|}$ , hence $\frac{\rho}{\|w\|}$ Maximizing the distance would be minimizing $\frac{\|w\|}{\rho}$ . How does this become minimizing $\frac{1}{2} \|w\|^2 -\rho$ ?
