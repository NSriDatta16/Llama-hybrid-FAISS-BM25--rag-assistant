[site]: crossvalidated
[post_id]: 602313
[parent_id]: 581257
[tags]: 
There are no issues with using the double ML strategy provided that your classifier provides probabilistic outputs, i.e. it estimates the conditional response function. However in finite samples, it is possible the strategy produces results outside the natural bounds of the problem. For example, the average treatment effect with a binary outcome must fall within [-1,1]. This happens because double ML supplies a bias correction term to a plug-in estimate of your target estimand, and this correction term does not necessarily respect the constraints of the problem in a finite sample. However, it practically almost always does, so I would not be afraid to use this estimation methodology - plus, the theory still works out in this case. An alternative approach you could try is TMLE. The two methodologies are quite similar in their flexible use of ML estimators for nuisance functions and the ability to use double ML'S idea of cross-fitting (called CV-TMLE in this literature). However, TMLE has an advantage in this case in that it does infact respect problem constraints in finite samples. It achieves this by providing a correction in an earlier step. We still use a plug-in estimator for our target estimand, but now we provide the correction to the quantity being plugged-in. The relevant quantity being plugged-in in this case would be the log-odds of the outcome under treatment or control. Since we provide the correction on the log-odds scale, when we convert back to the probability scale when we plug-in, we respect the bounds of the problem.
