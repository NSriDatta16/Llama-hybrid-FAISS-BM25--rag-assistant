[site]: crossvalidated
[post_id]: 524593
[parent_id]: 
[tags]: 
Why is information theory studied separately from probability theory?

As I study information theory, I find myself increasingly perplexed by both the subject's relative obscurity and the fact that it's not studied together with probability theory. It seems to me that the fundamental "idea" behind information theory can be summarized thus: Given some (discrete) random variable X, we may ask ourselves what the "average probability" of the outcomes of X are. To do this properly, we need to take a geometric mean. Unfortunately, geometric means are obnoxious - conveniently for us, though, the logarithm of the geometric mean is the arithmetic mean of the logarithm. Viewed this way, information theory really is "just" probability theory, except we've taken logarithms to turn multiplication into addition. So, why aren't these studied together? Why is information theory generally only the purview of electrical engineering departments, rather than statistics departments? Why isn't entropy ever discussed as the logarithm of the "average probability," rather than the average of the logarithms?
