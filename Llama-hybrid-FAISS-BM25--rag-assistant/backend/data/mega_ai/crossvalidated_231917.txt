[site]: crossvalidated
[post_id]: 231917
[parent_id]: 
[tags]: 
Is this the right interpretation of relative entropy for the Bayesian approach?

Suppose I know the true distribution, $P^*(x)$ and I have approximated the true distribution with $\tilde{P}(x|D)$, which is the predictive posterior density. Does the relative entropy of $\tilde{P}(x|D)$ with respect to $P^*(x)$ become: $$D_{KL}(P^{*}||\tilde{P})=\int_{-\infty}^{\infty}P^{*}(x)\,\log\frac{P^*(x)}{\tilde{P}(x|D)} dx$$ Also, does a smaller relative entropy in this case also imply a smaller distance between my hypothesis and the true distribution?
