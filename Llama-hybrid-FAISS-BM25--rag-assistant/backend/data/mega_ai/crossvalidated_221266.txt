[site]: crossvalidated
[post_id]: 221266
[parent_id]: 
[tags]: 
Understanding the meaning of $n = 1$ in the Bernoulli distribution

I am not very familiar with the Bernoulli distribution and am seeking to understand when to use it rather than other, more common binomial distributions in machine learning models. I began by reading on Wikipedia, which gives the following definition: The Bernoulli distribution is a special case of the two-point distribution, for which the two possible outcomes need not be 0 and 1. It is also a special case of the binomial distribution; the Bernoulli distribution is a binomial distribution where n=1. It then proceeds to get more technical, but I don't see it clearly referring back to that statement later in the article. I'm just looking for a basic, high-level understanding but what confuses me about the definition above is that it says "where n = 1". Which "n" is this referring to? It was not defined in the Wikipedia summary, but lacking another definition I usually take n to refer to sample size. Is that saying the Bernoulli distribution is for when you have a sample size of 1? That doesn't make any sense to me and seems to conflict with some empirical examples I've seen of the Bernoulli distribution being using in Generalized Boosting Models, thus I'm confused.
