[site]: crossvalidated
[post_id]: 550054
[parent_id]: 
[tags]: 
Estimating model performance using samples from test set

Question: Would sampling (with or without replacement??) from the test set of data, computing mean squared error (MSE) for each sample, and then computing the average and standard deviations of those MSEs be a better measure of model performance than just computing MSE once for the whole test set? Is this even a valid strategy, or am I violating some rule I am unaware of? Edit & Bonus: The procedure I describe in the question appears to be bootstrapping as shown by ML Mastery and the Wiki Page . A related question --> If I have some input $X \in \mathbb{R}^{\text{samples} \times \text{past timesteps} \times \text{features}}$ and output $Y \in \mathbb{R}^{\text{samples} \times \text{future timesteps} \times \text{targets}}$ , can I perform the procedure in the question? Note, the time series windows $X$ and $Y$ are overlapping. To understand this data format, assume that samples=2, features=1, and targets=1 while past_timesteps=3, and future_timesteps=1. To give this physical meaning, one might say that three previous days are used to predict some value for the next day. X = [[0 1 2], [1 2 3] Y = [[3], [4]] t-2 t-1 t X[0] = [0 1 2] X[1] = [1 2 3] t+1 Y[0] = [3] Y[1] = [4] Context: I have split a dataset into three sets: training, validation, and testing set. I have one independent variable $x$ and one dependent variable $y$ . Once my model $\hat{f}(x)$ has been hyperparameter tuned using the training and validation set via a cross-validation strategy, I must evaluate a final trained model on the test set. I could use each value of $x$ in the test set to make a prediction $\hat{y}$ and compute MSE using the actual value $y$ also from that test set; HOWEVER , if I do this I only get ONE MSE for my model's generalization error! Alternatively, I could use the technique I propose in the question. For example, if I randomly sample with (or without??) replacement $n$ items from the test set, compute the MSE for these items, and then repeat that process $K$ times, I could get an average MSE denoted by $\overline{MSE}$ . The sampled items corresponding to the independent variable is denoted $x_n$ while the corresponding dependent variable (NOT the prediction) is denoted $y_n$ . The prediction for $x_n$ is $\hat{y}_n = \hat{f}(x_n)$ $$ \begin{align} \overline{MSE} &= \frac{1}{K} \sum_{i=1}^K{MSE(\hat{f}(x_n), y_n)^{(i)}}\\ &= \frac{1}{K} \sum_{i=1}^K{MSE(\hat{y}_n, y_n)^{(i)}} \end{align} $$ Below is some code that illustrates the different approaches. import numpy as np def mse(y_hat, y): """Mean squared error. :param y_hat: Vector of predictions. :param y: Vector of targets. :return: Scalar for MSE. """ return np.mean(np.square(y_hat - y)) def model(x, tuned_hyperparameter=0.2): """Arbitrary model. This happens to be linear model for simplicity, but in reality I use a neural net. :param x: Vector or scalar input. :param tuned_hyperparameter: Value obtained from some form of cross validation. :return: Vector or scalar prediction(s) based on x. """ return tuned_hyperparameter*(0.5*x + 3) # Data x_test = np.random.normal(size=(10,)) y_test = np.random.normal(size=(10,)) ################################## # APPROACH 1: Scalar Evaluation ################################## single_mse = mse(model(x_test), y_test) ################################## # APPROACH 2: Sampling Evaluation ################################## mse_list = [] K = 5 n = 2 # size of random sample from test set with_replacement = False # replacement strategy is False for now # For a number of arbitrary iterations K for i in range(K): # Sampling ix_of_samples = np.random.choice(x_test.shape[0], n, replace=with_replacement) x_n = x_test[ix_of_samples] y_n = y_test[ix_of_samples] # Compute the i^th MSE ith_mse = mse(model(x_n), y_n) # Append to the mse list mse_list.append(ith_mse) # Compute mean and std of MSE list over K iterations mse_mean = np.mean(mse_list) mse_std = np.std(mse_list) # Results print('\nScalar MSE:', single_mse) print('\nFrom MSE List --> ', mse_mean, '+/-', mse_std) ```
