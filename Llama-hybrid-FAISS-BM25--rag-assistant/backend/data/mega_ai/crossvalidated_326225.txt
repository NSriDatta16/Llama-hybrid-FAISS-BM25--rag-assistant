[site]: crossvalidated
[post_id]: 326225
[parent_id]: 
[tags]: 
Predicted probabilities for binary labels by Randon Forest and XGBoost

Task Binary classification for an imbalanced dataset with 70 positive samples and 700 negative samples Data Oversample positive samples to match the number of negatives. Each sample has 10 features. Model By cross-validation, the tree-based methods gave better performance than Logistic regression and two-layered dense neural networks. The methods in use are the XGBoost api for Sklearn and the RandomForestClassifier in sklearn. Training result Normalized confusion matrix for testing data by XGBoost: $$\quad\quad true\quad false$$ $$true\quad 0.71\quad 0.29$$ $$false\quad 0.33\quad 0.67$$ Normalized confusion matrix for testing data by Random Forest: $$\quad\quad true\quad false$$ $$true\quad 0.77\quad 0.23$$ $$false\quad 0.27\quad 0.73$$ The hyperparameter settings PARAM_CONFIG = { 'random_forest':{ 'n_estimators':80, 'criterion':'entropy', 'min_samples_leaf':20, 'max_depth':3 }, 'xgboost':{ 'max_depth':3, 'learning_rate':0.0001, 'objective':'binary:logistic', 'scale_pos_weight': 1, 'n_estimators': 50, 'subsample': 0.8 } } Predicted probability on independent dataset The question Now comes to my problem, the model performances from training are very close for both methods. But when I looked into the predicted probabilities, XGBoost gives always marginal probabilities, which is not the case for random forest. XGBoost and Random Forest gave the same prediction accuracy but XGBoost is less confident for all samples. What could go wrong here? or How should one explain this?
