[site]: datascience
[post_id]: 10773
[parent_id]: 
[tags]: 
How does SelectKBest work?

I am looking at this tutorial: https://www.dataquest.io/mission/75/improving-your-submission At section 8, finding the best features, it shows the following code. import numpy as np from sklearn.feature_selection import SelectKBest, f_classif predictors = ["Pclass", "Sex", "Age", "SibSp", "Parch", "Fare", "Embarked", "FamilySize", "Title", "FamilyId"] # Perform feature selection selector = SelectKBest(f_classif, k=5) selector.fit(titanic[predictors], titanic["Survived"]) # Get the raw p-values for each feature, and transform from p-values into scores scores = -np.log10(selector.pvalues_) # Plot the scores. See how "Pclass", "Sex", "Title", and "Fare" are the best? plt.bar(range(len(predictors)), scores) plt.xticks(range(len(predictors)), predictors, rotation='vertical') plt.show() What is k=5 doing, since it is never used (the graph still lists all of the features, whether I use k=1 or k="all")? How does it determine the best features, are they independent of the method one wants to use (whether logistic regression, random forests, or whatever)?
