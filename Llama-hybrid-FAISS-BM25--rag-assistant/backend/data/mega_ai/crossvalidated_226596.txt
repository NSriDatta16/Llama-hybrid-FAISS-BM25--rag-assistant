[site]: crossvalidated
[post_id]: 226596
[parent_id]: 226592
[tags]: 
The mathematical difference is simple (and you probably got that already). A mixture distribution has a density which is a weighted sum of other probability densities (often from the same class) whereas a convolution is a sum of random variables. The intuition for a mixture can be illustrated (in line with your example) as follows: Let's say you have $k$ sensors each of which draws an independent measurement $X_i\sim f_i$ (for $i=1,\ldots,k$). Furthermore, let's say that you are only observing the measurement $W$ of one of these sensors $s$, i.e. $W=X_s$ by choosing the sensor s randomly (from $1,\ldots,k$) using a discrete uniform distribution. Then, the density of $W$ given that $s$ is known corresponds to $f_s$. Now, as $s$ is not known, we can consider all possible values for s and we obtain for the density a mixture distribution $$f_W(x) = P(s=1)\cdot f_1(x) + \ldots + P(s=k)\cdot f_k(x)=\frac{1}{k}\sum_{i=1}^k f_i(x)$$ In the sensor example you would have a convolution if you would take all measurements (assuming them to be independent) and sum them up,i.e., $W=X_1+\ldots+X_k$. This may happen as part of averaging the sensor measurements. Then, the resulting density is $$f_W(x) = f_{X_1+\ldots+X_k}(x) = (f_1 * f_2 * \ldots*f_k)(x)\ ,$$ where $*$ denotes the convolution operation. Side note: For the actual averaging procedure, we would have to divide the sum by the number of sensors. That is $W=k^{-1}(X_1+\ldots+X_k)$ and $$f_W(x) = f_{k^{-1}(X_1+\ldots+X_k)}(x) = k^{-1}(f_1 * f_2 * \ldots*f_k)(k\cdot x)\ .$$
