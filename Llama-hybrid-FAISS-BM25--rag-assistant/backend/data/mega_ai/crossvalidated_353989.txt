[site]: crossvalidated
[post_id]: 353989
[parent_id]: 353984
[tags]: 
Given that OLS minimizes the MSE of the residuals amongst all unbiased linear estimators (by the Gauss-Markov theorem) , and that a weighted average of unbiased linear estimators (e.g., the estimated linear functions from each of your $k$ folds) is itself an unbiased linear estimator, it must be that OLS applied to the entire data set will outperform the weighted average of the $k$ linear regressions unless, by chance, the two give identical results. As to overfitting - linear models are not prone to overfitting in the same way that, for example, Gradient Boosting Machines are. The enforcement of linearity sees to that. If you have a very small number of outliers that pull your OLS regression line well away from where it should be, your approach may slightly - only slightly - ameliorate the damage, but there are far superior approaches to dealing with that problem in the context of a very small number of outliers, e.g., robust linear regression, or simply plotting the data, identifying, and then removing the outliers (assuming that they are indeed not representative of the data generating process whose parameters you are interested in estimating.)
