[site]: crossvalidated
[post_id]: 519280
[parent_id]: 519278
[tags]: 
It might help to read the introductory chapter(s) of e.g. MIT's Deep Learning Book . They explain the mathematical background of deep learning briefly and, more importantly, introduce the reader to the style of notation commonly found in papers on deep learning. The three notations you mentioned: $x \sim G(z, \,c)$ : $x$ is a random variable distributed as probability distribution $G$ with parameters $z$ and $c$ . $c' \sim P(c \, |\, x)$ is also used, which means $c'$ is distributed as $P$ with parameters $c$ conditional on $x$ ; $c'$ (read: $c$ prime): I think they are recycling notation here and use $c'$ to mean 'alternative' $c$ ; $D_{KL}(P \, || \, Q)$ : The Kullback-Leibler divergence from $Q$ to $P$ . This is a measure of 'distance' going from one distribution to another. In the paper, the distance from the predictions made by the model to the true values it is the objective being minimized. Ian J. Goodfellow, Yoshua Bengio and Aaron Courville (2016). Deep Learning. MIT Press, 2016.
