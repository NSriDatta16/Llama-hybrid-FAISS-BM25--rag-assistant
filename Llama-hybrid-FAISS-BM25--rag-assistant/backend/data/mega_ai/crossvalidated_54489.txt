[site]: crossvalidated
[post_id]: 54489
[parent_id]: 45820
[tags]: 
The idea behind the MVA inequalities is simple: PCA is equivalent to estimate the correlation matrix of the variables. You are trying to guess $p\frac{p-1}{2}$ (symetric matrix) coefficients from $np$ data. (That is why you should have n>>p.) The equivalence can be seen this way: each PCA step is an optimization problem. We are trying to find wich direction express the most variance. ie: $$ max( a_{i}^{T} * \Sigma * a_{i} ) $$ Where $\sigma$ is the covariance matrix. under the constraints: $$ a_{i}^{T} * a_{i} = 1 $$ (normalization) $$ a_{i}^{T} * a_{j} = 0 $$ (for $j The solution of these problems are clearly eigenvectors of $\Sigma $ associated to their eigenvalues. I have to admit that I don't remember the exact formulation, but eigenvenctors depends on the coefficients of $\sigma$. Modulo normalisation of the variables, covariance matrix and correlation matrix are the same thing. Taking n = p is more or less equivalent to guess a value with only two datas... it's not reliable. There's no rules of thumbs, just keep in mind that PCA is more or less the same thing as guessing a value from $2\frac{n}{p}$ values.
