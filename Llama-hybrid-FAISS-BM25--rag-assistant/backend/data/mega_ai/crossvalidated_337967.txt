[site]: crossvalidated
[post_id]: 337967
[parent_id]: 
[tags]: 
L2 regularization, averaged over number of examples?

I have seen in different sources that, when adding L2 regularization to the objective function of our task, we need to average this regularization term by the number of examples. However, I have also seen it without this averaging. I know it is just a matter of scaling, but I would like to know if there is any general accepted convention. Source: https://sebastianraschka.com/faq/docs/regularized-logistic-regression-performance.html And in the machine learning course of Andrew Ng I've seen: Source: https://www.youtube.com/watch?v=KvtGD37Rm5I I have seen more repeatedly the first formula, but I want to know if there is any logic behind this confusion. Thanks for any help
