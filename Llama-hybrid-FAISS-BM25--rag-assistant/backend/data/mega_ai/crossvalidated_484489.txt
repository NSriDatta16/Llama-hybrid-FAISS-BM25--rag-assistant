[site]: crossvalidated
[post_id]: 484489
[parent_id]: 
[tags]: 
Autoencoder Incorrect Output/Predict - Model Built and Trained. *Please Assist*

Background Let me preface, I am new to python and machine learning. I have been tasked with creating an autoencoder to reduce dimensionality on a made-up dataset (proof of concept). I am working in google colab. I also have the anaconda version of py 3.x.x and use Jupiter notebook with it. I have assembled the autoencoder and it trains and runs, however, when it is run and I get the outputs from my layers, I have columns of zeros in my layer outputs and sometimes my model outputs. I believe having the columns of zeros is incorrect. I thought I resolved the issue by using .predict from my model layers then I receive non-zero values. However, my latent layer (feature extraction) shows values above 2 from the .predict. Which I also believe indicates a problem. I have looked over the code dozens of times, but I see no issue. As a note my weights are not tied. Questions Is having a column of zeros in decoder/encoder layers or latent layer an immediate sign of an issue? Should I be looking at the outputs or the .predictions for feature extraction? Where do I have an issue or is it working correctly? Data Link: https://docs.google.com/spreadsheets/d/1cGOrdm7IvnpjBi8Zy0nVJ9m4gckF_TzNRlL3ronnzuA/edit?usp=sharing import pandas as pd import numpy as np import pickle import matplotlib.pyplot as plt from scipy import stats import tensorflow as tf from tensorflow import constant import datetime, os import seaborn as sns from pylab import rcParams from sklearn.model_selection import train_test_split from sklearn.preprocessing import MinMaxScaler from tensorflow.keras.models import Model, load_model, Sequential from tensorflow.keras.layers import Input, Dense from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard from tensorflow.keras import regularizers from tensorflow.keras.utils import plot_model import re %matplotlib inline %reload_ext tensorboard sns.set(style='whitegrid', palette='muted', font_scale=1.5) rcParams['figure.figsize'] = 14, 8 #Setup Random Seed RANDOM_SEED = 13 df = pd.read_csv("autoencoder_ts1_nolabel_Reworked.csv") X = df.iloc[:, :8].values y = df.iloc[:, :8].values # Copy initial "df" data frame to "data" data frame. data = df # Split data 80% Training / 20% Testing -> picked at random using RANDOM_SEED value from the Pre-amble. X_train, X_test,y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=RANDOM_SEED) #Large range of values from each variable (column) require a scaled version of the dataset for loss to converge. scaler = MinMaxScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Confirm Intended Train Shape (5034 * .80) = 4027 Rows X_train.shape # Determine the Number of items in the # Shape: [7] -> 8 -> 4 -> 2 -> 4 -> 8 #number of neurons in each hidden layer. progression above shows desired # of neurons. input_size = 8 hidden_size = 4 code_size = 2 #Model declaration using Sequential and Dense Layers autoencoder = tf.keras.models.Sequential() ip = autoencoder.add(Dense(input_size, input_shape = (8,), activation = 'relu',name = 'input_layer')) #Input - Dense layer with input shape encode = autoencoder.add(Dense(hidden_size,activation ='relu', name = 'encode_layer')) # Encoder - Dense layer from 8 neurons to 4 code = autoencoder.add(Dense(code_size, activation = 'relu', name = 'code_layer')) # Latent/code layer reducing neurons from 4 to 2 decode = autoencoder.add(Dense(hidden_size, activation = 'relu', name = 'decode_layer')) # Decode layer - expanding from 2 to 4 neurons op =autoencoder.add(Dense(input_size,activation = 'sigmoid', name = 'output_layer')) # Ouput layer - expands from 4 to 8 neurons. Should recreate input #Sub-models - Used to capture features/outputs from each layer in the autoencoder input_layer_model = tf.keras.Model(inputs=autoencoder.input,outputs=autoencoder.get_layer('input_layer').output) input_layer_output = input_layer_model(X_test) encode_layer_model = tf.keras.Model(inputs=autoencoder.input,outputs=autoencoder.get_layer('encode_layer').output) encode_output = encode_layer_model(X_test) code_layer_model = tf.keras.Model(inputs=autoencoder.input,outputs=autoencoder.get_layer('code_layer').output) code_output = code_layer_model(X_test) decode_layer_model = tf.keras.Model(inputs=autoencoder.input,outputs=autoencoder.get_layer('decode_layer').output) decode_output = decode_layer_model(X_test) output_layer_model = tf.keras.Model(inputs=autoencoder.input,outputs=autoencoder.get_layer('output_layer').output) output_layer_output = output_layer_model(X_test) # The number of times to cycle through the data. epoch = 128 # The number of records (samples) to group together batch_size = 256 # Compile Neural Network Model # - optimizer: method for changing attributes to minimize loss # - loss: how the model's performance will be judged # - metrics: how the loss will be judged autoencoder.compile(optimizer='adam', loss='mse', metrics=['accuracy']) logdir = os.path.join("logs", datetime.datetime.now().strftime("%Y%m%d-%H%M%S")) tensorboard_callback = TensorBoard(logdir, histogram_freq=1) #Train the model autoencoder.fit(x=X_train,y=X_train,epochs=epoch, shuffle = True, callbacks=[tensorboard_callback]) autoencoder.summary() Weights = autoencoder.get_weights() encode_output_prediction = encode_layer_model.predict(X_test) df_encode_output_prediction = pd.DataFrame(encode_output_prediction) code_layer_output = code_layer_model.predict(X_test) df_code_layer_output = pd.DataFrame(code_layer_output) decode_output_prediction = decode_layer_model.predict(X_test) df_decode_output_prediction = pd.DataFrame(decode_output_prediction) output_prediction = autoencoder.predict(X_test) df_output_prediction = pd.DataFrame(output_prediction) df_X_TEST = pd.DataFrame(X_test) output_layer_model.predict(X_test) plot_model(autoencoder, to_file='autoencoder_plot.png', show_shapes=True, show_layer_names=True)
