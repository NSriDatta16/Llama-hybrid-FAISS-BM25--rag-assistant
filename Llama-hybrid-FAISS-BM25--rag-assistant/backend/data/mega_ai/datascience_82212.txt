[site]: datascience
[post_id]: 82212
[parent_id]: 
[tags]: 
Should I normalise image pixel wise for pretrained VGG16 model

My goal is to use pre-trained VGG16 to compute the feature vectors excluding the top layer. I want to compute embedding(no training involved) per image one by one rather than feeding batches to the network as this network is just to compute embedding and not classification. In batch training I understand the importance of batch normalization but for a single image should I normalize it pixel-wise? All I can think is maybe it's useful to reduce the importance of illumination in an image. Am I missing something?
