[site]: datascience
[post_id]: 67419
[parent_id]: 67366
[tags]: 
Hi David and welcome to the community! I think that the [0] is for accessing the array of actions as it is in double brackets. After this you need the action index to update appropriately. The Q network would have as many outputs as the actions available (in your case 2). Then you want to update the weights of the network in the output layer that is responsible for estimating the Q that corresponds to the action selected . So it seems that you have received a $r(s,action)$ and thus you assign it as target in order to use it for the MSE( $Q(s,action)$ , $r(s,action)$ ) between estimation and real reward. The weights of the other outputs cannot be updated by using the collected $r(s,action)$ as the Q-learning update equation would be wrong so the target remains the same as the prediction (resulting a MSE=0). Think that each of the network's outputs is responsible for estimating the return by performing a particular action, at a particular state. For example the first output is responsible for estimating the return (given input state) for action=left ( $Q(s,left)$ ) and the other for action=right ( $Q(s,right)$ ). You train each network head (output layer) with MSE between real reward and estimation. Each time the reward that is sampled from the environment would be the result of a selected action. Thus you update only the corresponding head by assigning as target the reward sample.
