[site]: crossvalidated
[post_id]: 568161
[parent_id]: 
[tags]: 
Suggestion regarding usage of pre-trained BERT

I have recently been working with the pre-trained BERT. It produces quite good results on supervised tasks with just a bit fine tuning. But now I wonder if I want to perform some unsupervised task on a custom dataset like amazon reviews or maybe a news dataset. Would it be wise to further pre-train BERT or use the already available version? I know that pretraining is performed in two parts i.e. MLM and NSP. While this type of custom data might be suitable for MLM, I'm not sure if it'd be suitable for NSP as it, as far as I know, requires a "story type" text for Next Sentence Prediction. Any views/insights in this regard would be appreciated.
