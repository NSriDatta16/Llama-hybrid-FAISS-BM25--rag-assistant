[site]: crossvalidated
[post_id]: 487363
[parent_id]: 
[tags]: 
Using variance components of a mixed model to obtain Std. Error of a coef from an OLS model (in R)

In the famous hsb dataset (students nested in schools; DV: math ), suppose we fit 3 models: Ignoring the level 2 clustering ( sch.id ). Below, I ran a simple model called ols1 that uses a binary cluster-level predictor ( sector ) in this model. Ignoring the level 1 data and just using the average of each cluster as DV. Below, I ran a simple model called ols2 that uses a binary cluster-level predictor ( sector ) in this model. Fitting a mixed-effects model that takes sch.id as a random-effect and uses sector as a predictor ( m1 ). From m1 alone, it is easy to find the Std. Error of sector coefficient in ols1 model because: sigma(ols1)^2 (* sigma^2 in blue ) almost equals 6.68 ( $\tau_{00}$ ) + 39.15 ( $\sigma_{e}^2$ ) (i.e., variance components) from m1 . where $n$ is the average cluster sample size, $J$ is the number of clusters, and $\sigma_{sector}$ is the variance of the sector predictor. Question: How can we obtain Std. Error of sector coefficient in ols2 using variance components in m1 ? library(lme4) library(tidyverse) hsb % group_by(sch.id) %>% mutate(math_ave = mean(math)) %>% slice(1) ols1
