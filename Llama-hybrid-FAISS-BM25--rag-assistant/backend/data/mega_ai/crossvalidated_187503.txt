[site]: crossvalidated
[post_id]: 187503
[parent_id]: 
[tags]: 
Problem with Newton--Raphson Implementation of Inverse-Linked Gamma GLM in R

Recently, I've been trying to implement functions in R that use Newton--Raphson to find the MLE of parameters for various GLMs. My focus has (thus far) been on data with responses $y$ that are distributed along members of the exponential family with sufficient statistic $T(y) = y$. For clarity, I've been following the notation where the probability density function of $y$ parameterized by $\eta$ follows: $$f(y; \eta) = b(y) \: \exp \left\{ \eta y - a(\eta) \right\}$$ Glossing over the math, I've come to understand that the gradient and Hessian of the log-likelihood $\ell$ can be expressed as follows: $$\nabla_\theta \ell(\theta) = \sum_{i = 1}^{m} \left[ y^{(i)} - a^\prime(\theta^Tx^{(i)}) \right] x^{(i)} $$ $$H_\theta(\ell) = - \sum_{i=1}^{m} x^{(i)}x^{(i)T} a^{\prime\prime}(\theta^Tx^{(i)})$$ for some vector of parameters $\theta \in \mathbb{R}^{p}$ over a training set $\left\{\left(x^{(i)}, y^{(i)}\right)\right\}_{i=1}^{m}$. For exponentially distributed responses, I found that: $$a^\prime(\theta^T x) = -(\theta^Tx)^{-1} \quad \text{and} \quad a^{\prime\prime}(\theta^T x) = (\theta^T x)^{-2}$$ The output of my function, however, does not agree with the estimates from the built-in glm function in R . I've been calling the function as follows: glm(y ~ x1 + x2, family = Gamma(link = "inverse")) The data was generated with: x1 = runif(100) x2 = runif(100) y = rexp(100, -(x1 + x2)) And finally, my function is: my.glm = function(x.mat, y.mat, first.func, second.func, iters) { nobs = nrow(x.mat) x.mat = cbind(rep(1.0, nobs), x.mat) theta.mat = matrix(rep(0.5, ncol(x.mat)), nrow = ncol(x.mat)) x.dat.list = vector("list", length = nobs) for(ob in 1:nobs) { x.dat = matrix(as.numeric(x.mat[ob, ]), ncol = 1) x.dat.list[[ob]] = x.dat %*% t(x.dat) } for(k in 1:iters) { hess.sum = 0 grad.sum = 0 for(ob in 1:nobs) { x.dat = matrix(as.numeric(x.mat[ob, ]), ncol = 1) eta.val = (t(x.dat) %*% theta.mat)[1,1] hess.sum = hess.sum - second.func(eta.val) * x.dat.list[[ob]] grad.sum = grad.sum + (y.mat[ob, 1] - first.func(eta.val)) * x.dat } theta.mat = theta.mat - ginv(hess.sum) %*% grad.sum } return(theta.mat) } And I've set: first.func = function(x){-1/x} second.func = function(x){1/x^2} I tried it with linear regression, logistic regression, and poisson regression using the appropriate functions, and the outputs seem to agree with those of glm . At this point, I have no idea how to go about diagnosing the problem. Does anyone have any insights on this? It would be much appreciated. :)
