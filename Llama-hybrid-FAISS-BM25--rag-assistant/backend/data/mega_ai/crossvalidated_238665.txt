[site]: crossvalidated
[post_id]: 238665
[parent_id]: 237961
[tags]: 
Cross-validation is a tool to estimate the variance of your performance metric due to randomness in the data (and maybe in the learning algorithm if it not deterministic). So if you use only one split, e.g. 80% train + 20% test and report your performance metric from this single experiment there are good chances that anyone trying to reproduce your experiment using exactly the same parameters will find a different performance figure (sometimes very different). Unless of course you provide the same exact split which is meaningless. To come back to your question I think you should definitely use CV to report your performance (e.g. do a 10 folds CV and report the mean and standard deviation of the performance metric). Now for tuning your algorithm you may use a much smaller validation set sampled from the training set (make sure it is not included in the test set). If you're afraid that you won't find the best hyperparameters using a small set then you're probably overfitting your algorithm to the specifics of the dataset. If you can't find a configuration using a small sample that gives a reasonable performance among all folds then the algorithm is probably not very useful in practice. Also keep in mind some algorithms are simply too slow / don't scale well in some configurations. This is also a part of practical model selection. Since you mention SVMs, of course most implementations will be slow when trying to find parameters for non-linear kernels by grid search. Grid search has exponential complexity, so use it with very few parameters. Also keep in mind that most libraries provide sensible default parameters (or at least you set one parameter and there are heuristics to set the others).
