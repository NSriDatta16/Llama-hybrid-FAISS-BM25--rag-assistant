[site]: crossvalidated
[post_id]: 291626
[parent_id]: 291622
[tags]: 
The term "regularization" covers a very wide variety of methods. For the purpose of this answer, I am going to narrow in to mean "penalized optimization", i.e. adding an $L_1$ or $L_2$ penalty to your optimization problem. If that's the case, then the answer is a definitive "Yes! Well kinda". The reason for this is that adding an $L_1$ or $L_2$ penalty to the likelihood function leads to exactly the same mathematical function as adding either a Laplace or Gaussian a prior to a likelihood to get the posterior distribution (elevator pitch: prior distribution describes uncertainty of parameters before seeing data, posterior distribution describes uncertainty of parameters after seeing data), which leads to Bayesian statistics 101. Bayesian statistics is very popular and performed all the time with the goal of inference of estimated effects. That was the "Yes!" part. The "Well kinda" is that optimizing your posterior distribution is done and is called "Maximum A Posterior" (MAP) estimation. But most Bayesian don't use MAP estimation, they sample from the posterior distribution using MCMC algorithms! This has several advantages, one which being that it tends to have less downward bias in the variance components. For the sake of brevity, I have tried not to go into details about Bayesian statistics, but if this interests you, that's the place to start looking.
