[site]: crossvalidated
[post_id]: 205633
[parent_id]: 204976
[tags]: 
You want to know $$ p(c \mid l) $$ where $c$ is observed, but $l$ is a noisy realization of an unobserved $l^*$ which is generated by $c$. If you could observe $l^*$ you would and you'd condition on it, but you don't so you can't. A simple graph consistent with this measurement error problem is $$ c \rightarrow l^* \rightarrow l $$ You assume that $l^*$ has the same possible values as $l$, so I'll assume that too, though it's not essential. And you even know $p(l^* \mid l)$, at least for the value of $l$ you care about; that's your tuple. (Whether you got that directly or had to infer it with extra distributional assumptions is determined by whether you have a classical or Berkson measurement error setup, but we'll ignore all that). Moreover you also assume you know how to get a distribution of $c$ given $l^*$, perhaps because you know $p(c)$ and $p(l^* \mid c)$ and can apply Bayes theorem. With all this in hand, the conditional probability you want is $$ p(c \mid l) = \sum_{l^*} p(c \mid l^*, l)\, p(l^* \mid l) = \sum_{l^*} p(c \mid l^*)\, p(l^* \mid l) $$ where the first step is the rule of total probability and the second step comes from the structure of the graph: $c$ is independent of $l$ conditional on $l^*$. The result is, I think, pretty intuitive: we should average over all the $l^*$ involving possible routes between $c$ and $l$, weighting by what we know about their relationships to $l^*$. If there are more variables involved then the result might be different and perhaps much messier. In particular, the conditional independence need not hold. Nevertheless, writing down the graph and cranking through the rules of probability will get you where you need to go.
