[site]: crossvalidated
[post_id]: 533658
[parent_id]: 532786
[tags]: 
One type of model to consider could be LSTM neural networks with a survival loss function . This could be quite a good way when there is some complex pattern on multiple inputs (i.e. one for which it is hard for humans to engineer good features) that indicates an upcoming failure and if you've observed a decent number of all failure types. For the different time resolutions of different sensors, there's multi-resolution LSTMs that are intended for such data. If the differentce in resolution is not too extreme, you could also treat the lower resolution sensors as missing unless measured. A purely machine-learning based approach may be less than optimal, if you have a very low number of failures or certain types of failures have not been observed, yet (or only very rarely). In that case human knowledge may have to play a major role in any solutions, although perhaps an anonmaly detection approach (e.g. predict with fbprophet and see whether sensor readings lie outside prediction intervals, or train a [e.g. denoising] autoencoder on certain time snippets from normal operation and see whether the hidden representation deviates a lot from the representations for normal) could be conceivable or could at least provide good features for simpler models for predicting upcoming failure times (but that might also flag if things look better than ever seen before...).
