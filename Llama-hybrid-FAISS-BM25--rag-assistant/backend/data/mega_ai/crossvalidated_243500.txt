[site]: crossvalidated
[post_id]: 243500
[parent_id]: 243486
[tags]: 
The cutoff is ultimately something you have to choose using domain knowledge as well as the costs of getting it wrong. You also might have a case where your decision space isn't the same as the event you're trying to predict. For instance, if you predict the probability of a person having a given disease is 50% perform another, possibly more invasive test. Of course, that's just an example, but the idea is that what you do with the probabilities is up to you. There is no cutoff chosen by the logistic regression and it doesn't even have to correspond to "declare this instance a 0 or a 1". And even if it does, if the cost of calling a 1 a 0 is ten times that of calling a 0 a 1, you would be better off calling 1's even if you thought there were only a 20% (or lower chance) of it being 1. This idea comes up in what is called "cost sensitive learning". Also, since logistic regression is just giving you the log-odds ratio (or equivalently the probability) if you're just looking for a measure of how well it does on a problem, you might not even care what label an individual class gets. You could look at loss functions based only on the predicted conditional class probabilities, like log loss or a Brier score. The AUC curve is a tool to look at the tradeoff of sensitivity and specificity if you were to set the cutoff at different points. As that other answer says, if you want to optimize for getting the most true positives subject to an upper bound on false negatives you'd do one thing, whereas if you wanted as few false negatives as possible with a lower limit on the number of true positives you'd do another. At the end of the day, there's no rule, and it comes down to your understanding of the problem and reasons for using the method. Edit : As @GeoMatt22 wisely suggested, the "other answer" I'm referring to that he originally mentioned is: Obtaining predicted values (Y=1 or 0) from a logistic regression model fit
