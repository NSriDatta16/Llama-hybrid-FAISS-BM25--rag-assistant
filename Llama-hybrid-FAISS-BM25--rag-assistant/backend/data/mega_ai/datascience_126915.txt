[site]: datascience
[post_id]: 126915
[parent_id]: 
[tags]: 
What are the differences between contextual embeddings of Bidirectional-LSTM and Transformer?

A Transformer, like Roberta, can generate contextual embeddings using the encoder part, similar to a Bidirectional-LSTM that concatenates hidden states. What are the differences between them ? Are there any advantages of Transformer contextual embeddings over Bidirectional-LSTM? Could someone please explain?
