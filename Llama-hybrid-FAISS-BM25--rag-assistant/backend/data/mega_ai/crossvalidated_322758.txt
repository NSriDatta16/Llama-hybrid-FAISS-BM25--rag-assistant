[site]: crossvalidated
[post_id]: 322758
[parent_id]: 322756
[tags]: 
Regularization is one way to deal with potential overfitting. As such, it does not depend on the data type of the explanotory variables. If you have e.g. 10'000 observations and 2000 dummy variables, regularization may be indicated. Many of XGBoost's parameters deal with overfitting, e.g. max_depth (depth of the trees) min_split_loss (smallest impurity gain to do a split) min_child_weight (related to minimum size of child to do a split) colsample_bytree and colsample_bylevel (column subsampling) lambda and alpha (L2 and L1 regularization strength) Their choice will affect each other. So it could as well be that setting a low depth and high split loss might make it unnecessary to impose L1/L2 regularization. Careful tuning of all parameters is important though (and quite fun).
