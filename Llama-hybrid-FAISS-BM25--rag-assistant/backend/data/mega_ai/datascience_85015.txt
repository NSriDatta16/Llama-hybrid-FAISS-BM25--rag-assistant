[site]: datascience
[post_id]: 85015
[parent_id]: 85008
[tags]: 
Impact of imbalanced datasets First I would say that imbalanced dataset impact depends on the type of model you are using. For instance: Gaussian Naive Bayes should not be that much impacted if you have a certain amount of data for each class that is enough to approximate your gaussian distributions. (and that your data are normally distributed) Neural Networks learning is using the error of your predictions to update its model, so having an imbalanced dataset would lead to imbalanced learning (for example 70% of the weights/biais have been updated according to class 'Good'). You don't want this happening a priori . How to deal with imbalanced dataset ? There might be other approaches, but you can do at least: Use an algorithm that is not much impacted by imbalanced datasets Some algorithms have a class_weight parameter. You can use it to penalize more the minority class during the learning process so the model is forced to pay more attention to the minority class observations. See this post for more details: How does the class_weight parameter in scikit-learn work? Resample your dataset so it becomes balanced. That could be through undersampling the majority class or oversampling the minority one. Further reading How to Deal with Imbalanced Data 8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset Hope it helps.
