[site]: crossvalidated
[post_id]: 389730
[parent_id]: 389241
[tags]: 
Actor-critic algorithm Deep learning can solve the problem of high observation dimensional data it only handles discrete and low-dimensional action-spaces. This algorithm relies on finding the action that maximizes the action-value function, which in continuous spaces requires solving a complex optimization problem. Obvious solutions like action-space discretization lead to the explosion of the numbers of discrete actions. One of the solutions is used actor-critic approach. It consists of two components: actor adjust parameters of the stochastic policy () by stochastic gradient ascent. Deterministic Policy Gradient The majority of model-free algorithms are based generalized policy iteration: iterating policy evaluation with policy improvement. Policy evaluation estimates the action-value function $Q_{\omega} = {\nabla}_{\theta}log_{\theta}\pi_{\theta}(a|s)^T{\omega}$ In continuous action space, greedy policy becomes problematic as it requires maximization every step. Instead, the better alternative is to move in the direction of the gradient. Specifically for each visited state the policy parametrs ${\theta}^{k+1}$ are updated in proportion to the gradient ${\nabla}_{\theta}Q^{\mu}(s,{\mu}_{\theta}(s)$ A deterministic policy gradient theorem that states that the actor is updated by applying the chain rule to do extend return from to the expected return from the start distribution $J$ We use Neural network as a function approximator and allows to learn in large state and action spaces online. The main challenge when using the neural network in reinforcement learning is that most optimization algorithms assume that the samples are independently and identically distributed. To address these issues the replay buffer, finitesized cache, is used Also I suggest you to go through this lecture
