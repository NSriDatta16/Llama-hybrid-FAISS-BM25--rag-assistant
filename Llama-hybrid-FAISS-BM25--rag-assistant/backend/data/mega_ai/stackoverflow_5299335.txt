[site]: stackoverflow
[post_id]: 5299335
[parent_id]: 
[tags]: 
Reduce memory imprint when Java application reads gigantic file in chunks

I am creating an application to upload data to a server. The data will be pretty huge, up to 60-70gb. I am using java since I need it to run in any browser. My approach is something like this: InputStream s = new FileInputStream(file); byte[] chunk = new byte[20000000]; s.read(chunk); s.close(); client.postToServer(chunk); For the moment it uses a large amount of memory, steadily climbs to about 1gb, and when the garbage collector hits it is VERY obvious, a 5-6 second gap between chunks. Is there any way to improve the performance of this and keep the memory footprint to a decent level? EDIT: This is not my real code. There is alot of other things I do like calculating CRC, validating against InputStream.read return value, etcetera.
