[site]: datascience
[post_id]: 92504
[parent_id]: 
[tags]: 
Elman RNN with keras

I have to perform multi-step multivariate forecasting of time series, using keras. I found an example where LSTM is used. I could modify that example replacing LSTM with SimpleRNN. Now I would like to use Elman RNN, which is different from SimpleRNN, as far as I know. How could I implement Elman RNN using keras? I try to be more explicit, illustrating in detail what I would like to do. I have three sequences, two of them representing the input, one the output: in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90]) in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95]) out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))]) The sequences are used to fill the data structures X and y such that the following loop: for i in range(len(X)): print(X[i], y[i]) gives: [[10 15] [20 25] [30 35]] [65 85] [[20 25] [30 35] [40 45]] [ 85 105] ... This means that I associate the values of X related to 3 time steps, to the values of y related to 2 time steps. n_steps_in, n_steps_out = 3, 2 The Elman model I am trying is: model = Sequential() model.add(SimpleRNN(100, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features))) model.add(TimeDistributed(Dense(n_steps_out, activation='relu'))) model.summary() model.compile(optimizer='adam', loss='mse') # fit model model.fit(X, y, epochs=200, batch_size=n_steps_in, verbose=0) where n_features = X.shape[2] # i.e., 2 Now, when I test the prediction with x_input = array([[70, 75], [80, 85], [90, 95]]) x_input = x_input.reshape((1, n_steps_in, n_features)) yhat = model.predict(x_input, verbose=0) print(yhat) I get results like: [[[195.89265 224.1713 ] [162.78471 194.95282] [139.1635 161.6026 ]]] but also like: [[[207.80466 0. ] [189.1255 0. ] [184.61163 0. ]]] or like: [[[ 0. 240.88077] [ 0. 218.17479] [ 0. 205.80429]]] I would like to understand why sometimes there are 0. values in yhat (i.e., the predicted y) and why yhat has 6 values (3 pairs) instead of 2 (1 pair).
