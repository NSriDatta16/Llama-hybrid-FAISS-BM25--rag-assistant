[site]: crossvalidated
[post_id]: 164986
[parent_id]: 34587
[tags]: 
Seems to me that the original question and the responses ignore the values of calibration sources for temperature. The data seems to come from a quality assurance check of a measurement device. I note that the temperature is reported to the nearest degree. Consider sensor 1 is the in situ probe, and sensor2 is the calibrated QA probe, calibrated, for example, with an ice bath temperature (O degC), and a boiling water temperature (100 degC). The ice bath and boiling water baths are the "calibration standards", by definition. sensor 2 is the "transfer standard". Someone has to decide what precision & accuracy is needed for the in situ sensor1 measurements, the calibration & QA procedures to use, the model, how much data, and threshold difference to use to determine if there is sufficient agreement between the two datasets to pass or fail a QA check. If it passes, then the temperature of the in situ sensor1 is treated as acceptable or valid, if not, the probe is recalibrated, repaired or replaced and calibrated, as necessary to meet the needs of the process. So does the process that probe A measure require temperature data to be accurate and precise to within 10 degrees? 1 degree? 0.01 degree? The calibration and quality assurance requirements of the process will have a direct impact on the level of effort and complexity in determining what calibration & QA procedures to use. Otherwise, the PCA approach is an interesting one, to account for the error inherent in both the in situ sensor1 and transfer standard's sensor2 measurements. But how would this take advantage of the values of the known calibration standards of 0 degC and 100 degC.
