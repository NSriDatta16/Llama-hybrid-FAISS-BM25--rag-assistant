[site]: crossvalidated
[post_id]: 320565
[parent_id]: 
[tags]: 
RandomForest in R: Bad performance even on training set

I'm trying to run a RandomForest for a regression. However, it does not perform well even on the training set, not to mention the test set. I'm now wondering whether this is caused by a bad quality of the input data, or if I still can improve something in my algorithm? Here is my model: n=430 2 continuous input variables 1 categorical input variable 1 continuous output variable Formula (caret package) : control Output (results for "fit" from cross-validation on training set): Random Forest 324 samples 3 predictor No pre-processing Resampling: Cross-Validated (10 fold, repeated 3 times) Summary of sample sizes: 292, 292, 292, 291, 292, 291, ... Resampling results across tuning parameters: mtry RMSE Rsquared 2 4983092 0.5596401 3 5128162 0.5452369 RMSE was used to select the optimal model using the smallest value. The final value used for the model was mtry = 2. Changing nodesize, ntree and mtry does not alter the results very much. Is it thus a problem of data quality, or are there some other possibilities to improve the model which I have overlooked, e.g. through data normalization? To my understanding, it should at least be possible to overfit the model a bit, so that I'd get at least better results for the training set ...
