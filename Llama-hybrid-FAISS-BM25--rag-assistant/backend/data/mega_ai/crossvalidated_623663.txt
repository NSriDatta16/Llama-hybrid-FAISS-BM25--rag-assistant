[site]: crossvalidated
[post_id]: 623663
[parent_id]: 387445
[tags]: 
In the first plot below we can see some data points we are trying to classify into two groups on the left. We also have the function $p_{w,b}(x) = 1/(1+e^{-(wx+b)})$ for the values of $w$ and $b$ which minimize logistic loss. Note: minimizing the logistic loss is the same as maximizing the likelihood that $p_{w,b}(x)$ generated the data in the sample. On the right is a contour plot of the logistic loss as a function of the parameters $w$ and $b$ . Here $w$ is on the horizontal axis and $b$ is on the vertical axis. Each point in this parameter space would give us a different logistic regression curve on the left. As you can see, this loss function is smooth, convex, and has a unique local minimum! A good candidate for gradient descent (or in this case, more advanced convex optimization algorithms) to minimize the loss function. Here is the same situation but using a contour plot of $$\frac{\textrm{# incorrectly classified}}{\textrm{# of data points}}$$ Note that I am using a decision rule of predicting class $1$ if the model probability is greater than $0.5$ . It is vitally important to understand that this decision step is distinct from the statistical modeling, and that a threshold of $0.5$ is not always appropriate. See this excellent answer for more details. This is still a function of the same parameters $w, b$ . I zoomed out some in this graph relative to the other graph to show more interesting features. Firstly note that this is a piecewise constant function! The fraction of samples which are incorrectly classified doesnâ€™t change when you vary the parameters unless you pass over one of the sharp lines between different regions. So the gradient of this function is zero everywhere it is defined! It is not a good candidate for minimization using gradient descent. Minimizing a generic piecewise constant function is going to have to be, essentially, trial and error. Finally note that logistic regression does not minimize the fraction misclassified ! In fact, it doesn't even really make sense to talk about that without a decision threshold already in mind. To summarize: When we are fitting our model we want the model we get to accurately model the probability as a function of the features. We fit this by maximizing likelihood, which is equivalent to minimizing logistic loss. This minimization function is smooth as a function of the parameters, and so can be minimized using numerical analysis techniques. We cannot discuss model accuracy until we have also implemented a decision rule: how do we take these probabilities and convert them into predictions? This is not part of modeling. The decision we make will depend on our use case (Is the decision expensive or cheap? What are the costs of misclassification both ways?). If we did attempt to "fit" the model to maximize accuracy we would run into a problem: the accuracy is a piecewise constant function of the parameters, so it is not amenable to minimization using any technique other than systematic search / trial and error. Lastly: if all you are going to care about is classification accuracy, why even bother having a probability function at all? Just search for the cutoff value which gives you the best accuracy! In the multivariate case, just look for the hyperplane which best separates the data. You might be more interested in a SVM if this is your goal.
