[site]: crossvalidated
[post_id]: 364177
[parent_id]: 363797
[tags]: 
Does anyone have any thoughts on the validity of this method? The main issue I can see is that the linear regression proposes to split the sample space in two (the two "sides" of a hyperplane), whereas the nearest neighbors will split it in regions (maybe a lot), depending on how the different classes are sampled on the sample space. The worst case scenario I can see is the following. Your data shows a distribution that KNN can handle well, but that a linear regression will perform miserably at, like a chessboard on the following picture (the code is at the bottom of the post). Though this is a caricature, many other datasets will exhibit the same behavior. A linear regression would discard both features if you select your features based on the p-values : Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 1.538955 0.041424 37.151 If you decide to weight the features based on the weights of the linear regression, note that the weights may be order of magnitudes different, though there is no reason for them to be ! This would be equivalent to project your data on a single axe, on which the KNN could not learn anything relevant (think about the colors of the dots if you look at them from a single dimension). Therefore, I would not use such an approach to calibrate the weights of a KNN (or select the attributes). Edit You can increase any goodness of fit measure adding variables showing a linear dependency with respect to the target. In this case, the goodness of fit would not be trivial any longer, but you would lose the information that comes from the first two variables. On weighted KNN However, there is a possibility to change the distance function, from an Euclidean distance to a distance where the dimensions have different weights. This approach is long to tune because you have to evaluate the model each time you change the weight of one attribute. If $p$ is your number of features and you want to replace a feature $x$ by $x/2$ and $x\times2$, you have $3^p$ models to evaluate (with a naive strategy). Note that you may have a significant improvement if you treat the features independently (not every combination of them), running only $3\times p$ models. Other improvements for KNN J. Wang, P. Neskovic, and L. N. Cooper, “Improving nearest neighbor rule with a simple adaptive distance measure,” Pattern Recognition Letters, vol. 28, no. 2, pp. 207–213, Jan. 2007. "Random KNN feature selection - a fast and stable alternative to Random Forests" Shengqiao Li, E James Harner and Donald A Adjeroh, Bioinformatics 2011 12:450 Code N 0)+1 } x
