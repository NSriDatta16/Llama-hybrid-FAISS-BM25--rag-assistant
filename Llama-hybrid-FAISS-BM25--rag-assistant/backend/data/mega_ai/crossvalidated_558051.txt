[site]: crossvalidated
[post_id]: 558051
[parent_id]: 558034
[tags]: 
What you write might come close in some circumstances but can't be counted on in general. For example, putting multiple imputation aside for a moment, an average of hazard-ratio confidence intervals from Cox survival regression models among bootstrapped samples from a complete data set will tend to be very poorly behaved. For multiple imputation, Section 2.3 of Stef van Buuren's Flexible Imputation of Missing Data explains that Rubin's Rules take not only within-imputation and between-imputation variances into account but also a further variance due to a finite number of imputations. The variance of an averaged statistic $\bar Q$ among $m$ imputations thus has three sources: The total variance [of $\bar Q$ ] stems from three sources: $\bar U$ , the variance caused by the fact that we are taking a sample rather than observing the entire population. This is the conventional statistical measure of variability; $B$ , the extra variance caused by the fact that there are missing values in the sample; $B/m$ , the extra simulation variance caused by the fact that $\bar Q$ itself is estimated for finite $m$ . The addition of the latter term is critical to make multiple imputation work at low values of $m$ . What you write seems to be most closely related to the variance contributed by $\bar U$ , although it might include some contribution from $B$ insofar as the mean-value estimates change among imputation sets and thus shift the CI. It doesn't seem, however, to include the extra variance due to a finite value of $m$ . If you had a large number $m$ of imputations that might not be a big problem. So it's safest to follow Rubin's Rules and stick with the pooled SE.
