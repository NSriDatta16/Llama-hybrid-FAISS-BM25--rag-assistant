[site]: datascience
[post_id]: 67204
[parent_id]: 
[tags]: 
Does it make sense to train multiple classifiers for multiple partitions of data?

Let's say I have a dataset consisting of 100 features and a binary target variable. On exploring the data, I see that Feature 10 which is binary, seems to split the data in an 80:20 ratio with respect to the target variable. Does it then make sense to partition the data into "Feature 10 is 0" and "Feature 10 is 1" and try to build separate classifiers for both cases? Or is this what models like Random Forests do under the hood?
