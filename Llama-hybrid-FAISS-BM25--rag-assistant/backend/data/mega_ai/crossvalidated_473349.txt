[site]: crossvalidated
[post_id]: 473349
[parent_id]: 473321
[tags]: 
Per Wikipedia, on the concept known as homogeneity of variance , as opposed to heteroscedasticity, is defined as, to quote: In statistics, a sequence (or a vector) of random variables is homoscedastic if all its random variables have the same finite variance. where lack of homogeneity of error variance is not uncommon in linear regression where the magnitude of the error are often proportional to the size of the explanatory variable(s) themselves. Wikipedia comments, for example: ...if richer consumers' whims affect their spending more in absolute dollars, we might have ${Var(\epsilon _{i})=x_{i}\sigma ^{2}}$ rising with income... As such, to correct for heteroscedasticity, one may try regressing the fitted first stage OLS residues, as squared values, against the explanatory variable. The inverse of this fitted of variance expectation as a function of x is employed in a second stage weighted Least-Squares analysis. Note, ignoring the problem is not advised as per Wikipedia again: Assuming a variable is homoscedastic when in reality it is heteroscedastic results in unbiased but inefficient point estimates and in biased estimates of standard errors, and may result in overestimating the goodness of fit as measured by the Pearson coefficient...Homoscedasticity is not required for the coefficient estimates to be unbiased, consistent, and asymptotically normal, but it is required for OLS to be efficient.[3] It is also required for the standard errors of the estimates to be unbiased and consistent, so it is required for accurate hypothesis testing, e.g. for a t-test of whether a coefficient is significantly different from zero. A test for lack of homoscedasticity includes the Breusch–Pagan test , which is, unfortunately, sensitive to departures from normality and also small sample sizes. An alternate recommended test is the generalized Breusch–Pagan test.
