[site]: crossvalidated
[post_id]: 258130
[parent_id]: 24249
[tags]: 
How to limit impact of a predictor when doing multiple regression? One straightforward way to do this is simply doing a Bayesian regression. For instance you say: I think lm() results give too much weight to certain predictors This reveals you have a prior belief that those predictors shouldn't have much influence. With a Bayesian model you can explicitly put a stronger prior on some of your parameters, and that will accomplish what you want. It's worth noticing that Ridge and LASSO can be interpreted as bayesian regressions with a Gaussian and Laplace priors, respectively. Let me provide you a simple example on how to do this using the arm R package : library(arm) set.seed(10) n lm(formula = y ~ x1 + x2) #> coef.est coef.se #> (Intercept) 1.02 0.10 #> x1 2.00 0.10 #> x2 2.86 0.10 #> --- #> n = 100, k = 3 #> residual sd = 0.97, R-Squared = 0.92 # restricting the influnce of x2 M2 bayesglm(formula = y ~ x1 + x2, prior.scale = c(Inf, 0.03), prior.df = Inf) #> coef.est coef.se #> (Intercept) 0.93 0.13 #> x1 1.95 0.14 #> x2 2.01 0.11 #> --- #> n = 100, k = 3 #> residual deviance = 159.5, null deviance = 1146.1 (difference = 986.6) #> overdispersion parameter = 1.6 #> residual sd is sqrt(overdispersion) = 1.28 The first model M1 is the simple lm . On the second model we we are shrinking x2 to zero with a tighter prior. If you want more flexible ways to specify your model, you should check Stan and R packages around it.
