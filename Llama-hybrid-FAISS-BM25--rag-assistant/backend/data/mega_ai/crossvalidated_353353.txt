[site]: crossvalidated
[post_id]: 353353
[parent_id]: 46368
[tags]: 
If you are from scikit-learn background, this answer might be helpful. k-fold cross-validation is used to split the data into k partitions, the estimator is then trained on k-1 partitions and then tested on the kth partition. Like this, choosing which partition should be the kth partition, there are k possibilities. Therefore you get k results of all k possibilities of your estimator. these are computationally expensive methods, but if you are going to try different estimators you can try these three for doing the hyperparameter tuning along with CV: i. GridSearchCV - an exhaustive list of all possible P and C for the hyperparameters for all the estimators. In the end gives the best hyperparameters using the mean of that particular estimator CV's mean. ii. RandomizedSearchCV - Does not do all the P and C of hyperparameters, but on a randomized approach, gives the closest possible accurate estimator saving more on computation. iii. BayesSearchCV - Not part of scikit-learn but does Bayesian optimization for doing a randomized search and fit results. tl:dr : CV is just used to avoid high bias and high variance for you r estimator because of the data you are passing. Hope it was helpful.
