[site]: datascience
[post_id]: 53803
[parent_id]: 9262
[tags]: 
Consider the three following samples from a distribution(s). values1 = np.asarray([1.3,1.3,1.2]) values2 = np.asarray([1.0,1.1,1.1]) values3 = np.array([1.8,0.7,1.7]) Clearly, values1 and values2 are closer, so we expect the measure of surprise or entropy, to be lower when compared to values3. from scipy.stats import entropy print("\nIndividual Entropy\n") print(entropy(values1)) print(entropy(values2)) print(entropy(values3)) print("\nPairwise Kullback Leibler divergence\n") print(entropy(values1, qk=values2)) print(entropy(values1, qk=values3)) print(entropy(values2, qk=values3)) We see the following output: Individual Entropy 1.097913446793334 1.0976250611902076 1.0278436769863724 # We see this makes sense because the values between values1 and values3 and values 2 and values 3 are simply more drastic in change than values1 to values 2. This is my validation to understanding KL-D and the packages that can be leveraged for it.
