[site]: crossvalidated
[post_id]: 114259
[parent_id]: 
[tags]: 
Does the average of the square roots of random variables mean anything?

I recently made a plot for work that used a signed square-root scale on the $y$ axis, for visual clarity. The $y$ observations are impulse response functions (IRF) of vector autoregressions computed on separate data sets. Since one of the goals of the project was to come up for group means, I ended up drawing horizontal lines at the means... of the square roots of $y$. It turned out that the average $\sqrt{y}$ told a slightly better "story" (to the client) than the average $y$, so my boss asked me whether average of the square roots of a variable was a valid measurement in its own right. I told him I didn't think so, and that it was really just to make it easier to see everything. He was okay with this, but I wonder if I was wrong. Does $\frac{1}{N}\sum_{i=1}^N \sqrt{\mathrm{IRF}_i}$ mean anything with respect to the underlying VARs, or the data more broadly? I'm also aware that IRF's are asymptotically normal, but they are computed on samples with just 25 time periods so I'd be leery of appealing to normality here.
