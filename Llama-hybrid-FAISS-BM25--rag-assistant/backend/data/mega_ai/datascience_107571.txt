[site]: datascience
[post_id]: 107571
[parent_id]: 107569
[tags]: 
Binary Logistic Regression has a simple form $$ y = \frac{1}{1+e^{-Xw}} $$ where $X$ is your $objects \times features$ matrix and $w$ is a learnt vector of model weights. Now, recall sizes of all vectors (for simplicity I omit sigmoid activation, since it does not affect matrices sizes): $$ y_{n \times 1} = X_{n \times m} w_{m \times 1} $$ the resulting vector $y$ (after sigmoid activation) contains numbers between 0 and 1, interpreted as probability of an objects being of class "1" (the closer the number to 1 the higher the probability). Now, if you want to predict the probability of an object to be of one of 3 classes (as in Iris dataset), your vector $w$ should be a matrix $W$ : $$ y_{n \times c} = X_{n \times m} w_{m \times c} $$ Now, every $y_i$ will contain $c$ numbers between 0 and 1 (after applying some nonlinearity, e.g. sigmoid again) - likelihoods of the $i$ 'th object being of $c$ 'th class.
