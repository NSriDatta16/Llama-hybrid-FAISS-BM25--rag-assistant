[site]: crossvalidated
[post_id]: 311555
[parent_id]: 
[tags]: 
Appropriate forecasting model for a huge amount of variables (updated at different, but constant intervals)?

I'm looking for suggestions on how to approach a forecasting problem for a very large (several thousand) amount of variables. Suppose there is around 1000 variables (time series). Each is updated regularly (say monthly), but at different times - for example $X_{512}$ is updated on the 17th every month, $X_{778}$ is updated on the first Monday of each month etc. We have gathered about 60 data points for every variable (5 years). The variables are highly correlated with each other. Of these 1000 variables, let's say we designate about a 100 as variables of interest. At any given point, we are insterested in forecasting a particular one of those 'variables of interest', and only one time period (of that particular variable) ahead. (It's not a priori known when and which one variable will need to be forecast). One suggested idea to deal with this problem is to use a 'correlation clustering' algorithm in order to separate the 1000 variables into more manageable subsets and then deal with only the relevant subset once it's decided which variable has to be studied. Which other methods could be used for such a problem? What general concept/technique/literature should I study in order to understand and select an appropriate model for this problem?
