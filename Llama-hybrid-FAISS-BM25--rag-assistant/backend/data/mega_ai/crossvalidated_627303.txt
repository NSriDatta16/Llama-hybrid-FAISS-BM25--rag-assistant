[site]: crossvalidated
[post_id]: 627303
[parent_id]: 
[tags]: 
ROC curve analysis for when having a training/validation/test split

I have a dataset I split in training/validation/testing data for a binary classification model. The data is used as following: Training data: for training the model (model weights, etc.) Validation data: for model selection Test data: to report the final results The validation data is used both for early stopping, as well as ROC curve analysis to select the optimal threshold to binarize my probabilities. Final metrics are reported on the test set, using the threshold selected on the validation set. It has come to my attention that since the threshold is a hyperparameter, it is conceptually wrong to select it using the validation set , but rather it should be selected using the training set . As far as I know, however, I've always seen the threshold being selected using the validation set when using training/validation/test split. So, is this conceptually incorrect? Is there some literature reference to support each choice? Thanks
