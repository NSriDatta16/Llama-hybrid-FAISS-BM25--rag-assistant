[site]: datascience
[post_id]: 82221
[parent_id]: 82213
[tags]: 
I support your "proof is in the pudding" sentiment. Some of those hyperparameters are not that extreme, in my experience. Boosted trees very often perform best with weak individual learners; your max_depth is right in line with what I'm used to seeing as best. The score regularization penalties (alpha, lambda) don't play as important a role in my experience, but I'm used to seeing optimal parameters chosen in the high double-digits. Your subsampling and column subsetting rates also seem reasonable, if on the lower end of what I've generally seen as being optimal. Your gamma is quite high, but that doesn't mean something is wrong; perhaps if you shrink the max depth a bit you could relax the gamma regularization, but I don't think that's in any way necessary . One possible explanation for this situation: your data is relatively linear and without interactions, so that xgboost doesn't get its main benefits. And your data is noisy enough that, lacking those nonlinear trends, xgboost ends up fitting to noise readily unless you strongly regularize it.
