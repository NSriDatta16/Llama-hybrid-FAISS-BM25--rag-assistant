[site]: crossvalidated
[post_id]: 299523
[parent_id]: 299302
[tags]: 
This image, which occurs in other forms at many places ( a b c d ) on this website may help you to get a better intuitive understanding. The image is a demonstration of relationship RSS of training and test data as function of the order of the fit curve and the order of the true relationship. Your question relates to this over-/under-fitting training data The linear function, polynomial function, and spline function are actually nested models. The space of all polynomial functions includes all linear functions, and the space of all spline functions contains all polynomial functions (if they are the same order). Therefore you will always have $RSS_{linear} \geq RSS_{polynomial} \geq RSS_{spline}$ for the training data due to over-fitting. This is the blue curve going down. test data The case for the test data is more difficult because it is not monotonically decreasing with increasing model complexity. In the case of over-fitting the fitting model has more degrees of freedom than the true relationship. These degrees of freedom will reduce the RSS in the training data due to the fitting of the error terms. However these error terms will be different in the test data and therefore those extra degrees of freedom (which make the fitted function, more similar to the error terms, but also more different from the true relationship) actually decrease the accuracy of the fitted model. Variations in the effect of over-/under-fitting Notice the following effects in the images. If there are more points (from top to bottom 40, 160, 640 points) than the over-fitting will be less strong The decrease of RSS for training data (blue curves) is less strong The increase of RSS for test data (red curves) is less strong There is a certain point at which the RSS for test data is at a minimum. Before this point you have under-fitting, past this point you have over-fitting. This point is where the true relationship and fitted curve are most resembling (note: not entirely, in the case of the image the 3th order actually performs better than the 4th order in modeling the 4th order true relationship. This may be because the additional fourth order term might be correlating too strongly with the error terms in the training data and not result in a better fit of the test data) Also note that the test RSS is always higher than the training RSS*. The linear curve will also over-fit the linear relationship (the error terms in the training data may have some linear relationship, which is different from the true relationship, causing the fitted model to be not exactly the same as the true relationship, and while the fitted model optimizes the RSS in the training data, this optimization is detrimental in the test data) case of linear and cubic relationships So in the linear true relationship the linear function is doing just fine. the polynomial function is over-fitting and the spline even more. And with the cubic true relationship the linear function is underfitting the polynomial is just fine for order 3 (and underfitting for order 2 and overfitting for order >3, so the question is a bit ambiguous) the spline is over-fitting (Although the comparison is difficult. For instance a spline of order 1 or 2 might do reasonable in fitting a cubic relationship. In the above six images the spline also does not really fit on the x-axis since the spline is not just a function varying in order but also the number of pieces) sum of squares of residuals in relation to sum of squares of errors Without over-/under-fitting the RSS will be close to the sum of squares of the error terms. For the training data it will be lower (because of slight fitting of the error terms that is always present) and for the testing data it will be higher (because of the fit not matching exactly the true relationship, due to these error terms in the test data). With over-fitting the RSS will be lower for training data but higher for test data. With under-fitting the RSS will be higher for both training data and test data. problematic question In the case of your specific question it is difficult to give exact answer because of the ambiguity. The polynomial function can have different orders. The spline is not defined exactly it is difficult to say, in the cubic relationship case, if the under-fitting of the linear function is worse than the over-fitting of the spline the cubic function might be be very close to a linear function if the coefficient for the higher order terms are small. as stated above: In theory, a first order function might be better at fitting a third order true relationship than a polynomial function of order 2 or 3. This has to do with the error terms correlating with the 2nd and 3th order terms and possibly creating a larger deviation from the true relationship rather than a better approximation of the true relationship. The reduction in degrees of freedom reduces the quality of the fit and may be worse than what the resemblance of the fitting function to the the true relationship can recover. related question Comparing RSS from linear and higher power models in a training and test data The answer to that question explains very nicely the change of RSS in training data as function of polynomial order. (the title in that question also mentions test data aside from training data, but that test data part does not occur in the text of the question and answer, which is more about the confusion between RSS and RSE) Note that over-fitting reduces RSS in training data, but increases RSS in test data. *I just encountered a question where it is not the case that Also note that the test RSS is always higher than the training RSS It is the question Is it ok to have low validation loss from the first epoch? where a neural network is fitted. In that case the x-axis of the plot is not something like polynomial order, but instead the epoch, the number of steps taken (like gradient descent) to train the model. In that case the first point in the graph, 0 epoch, and relates to zero training. Without any training it can be possible that the model performance is higher for the validation data than the training data. In addition I said 'always', but that is practically speaking. In theory it is possible to fit some model on some training data, and it will have better performance on validation data than on the training data. But, the probability will be small.
