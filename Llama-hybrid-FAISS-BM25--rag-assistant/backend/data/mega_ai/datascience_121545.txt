[site]: datascience
[post_id]: 121545
[parent_id]: 121542
[tags]: 
Well, yes and no. This is a position-wise feed-forward network. $x \in \mathbb{R}^{n\times d_{model}}$ , where $n$ is the sequence length. When we apply the matrix multiplication and bias additions, we do so for each individual position. Therefore, the actual multiplication is a vector of dimensionality $1\times d_{model}$ by the $W_1$ matrix. In actual implementation terms, we obtain the result for the $n$ vectors with a single matrix multiplication. The bias is a single vector that is broadcasted in the addition operation. Broadcasting was introduced by numpy and then adopted by deep learning frameworks. It is defined as: The term broadcasting describes how NumPy treats arrays with different shapes during arithmetic operations. Subject to certain constraints, the smaller array is “broadcast” across the larger array so that they have compatible shapes. Broadcasting provides a means of vectorizing array operations so that looping occurs in C instead of Python. It does this without making needless copies of data and usually leads to efficient algorithm implementations. In this case, broadcasting is equivalent to repeating the bias vector $n$ times. This does not mean that the actual vector is $n \times d_{ff}$ , but that we apply the addition to each of the $n$ positions.
