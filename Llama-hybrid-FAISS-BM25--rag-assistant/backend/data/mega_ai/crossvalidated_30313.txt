[site]: crossvalidated
[post_id]: 30313
[parent_id]: 
[tags]: 
Improving recall in a neural network

I built a neural net in Octave to predict if products are a match or not using a number of features. The net has 8 input features, 1 hidden layer with 8 nodes and outputs either match or no match. I employ feature regualarization and use fmuncg to perform gradient decend. I trained it with 30,000 examples with +/-10% of those training sets being positve matches. Here is my problem: The precision on the net is excellent, around 92%. Recall is very low at less than 10%. Playing around lambda in the regularization is not helping much, nor is performing more iterations in fmuncg . I dont want to balance the training set because that would reduce my data a lot. Is there anything else that I can do? This is my cost function: J = (-1/m)*sum(sum(y1.*log(r)+(1-y1).*log(1-r)))+ (lambda/(2*m))*((sum(sum(nTheta1.*nTheta1))+sum(sum(nTheta2.*nTheta2)))); Is there a way that I can change it, to penalise false negatives more?
