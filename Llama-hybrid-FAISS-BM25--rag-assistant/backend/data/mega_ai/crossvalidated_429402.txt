[site]: crossvalidated
[post_id]: 429402
[parent_id]: 335408
[tags]: 
The issue with statistical model assumptions in general is that they are never fulfilled precisely, so we more or less always do analyses in which assumptions are violated. The important question is whether assumptions are violated in ways that will cause misleading results , which is not exactly the question that misspecification tests such as Kolmogorov-Smirnov address (although not totally unrelated either). The Central Limit Theorem implies that many analyses based on methods derived from a normal distribution assumption are approximately valid for non-normal error distributions. However, if you were to analyse your data using standard linear regression, there are problems that the CLT cannot amend: Your dependent variable (y) has a fixed value range, and chances are that observations, at least in some regions of the explanatory variables (X, "X-space"), are not strongly concentrated in a safe distance from the limits of the value range. In this case a linear regression can give you predictions outside the value range for realistic values of X, which don't make sense. If your regression coefficients are not very close to zero, in some parts of X-space the values for y will be close to the maximum, in others close to the minimum, with potentially strong "local" skewness (possibly in both directions, but maybe far stronger in one direction than in the other). I'd expect that the overall regression slopes will produce some kind of compromise between areas in which y-values are rather central and symmetric and linearity may be locally fine, and areas in which there is skewness and linearity cannot hold because of closeness to the borders of the value range, so chances are regression parameters will be too low in absolute value for fitting the "good central" area while attempting to fit the more extreme areas linearly where this doesn't work. Homoscedasticity (equal variances) will also be violated because y-values have more space to vary in the central areas, i.e., where they are mostly in the middle of your ordinal scale. This means that different observations have different information content for fitting the regression and should be reweighted, which is ignored by standard linear regression (although the previous item may stop reweighting from helping here). The same issue can be caused by local skewness. Note by the way that the y-histogram and the qq-plot don't help much diagnosing normality, because the normal assumption in regression requires normality to hold in every area of X-space whereas these plots ignore the location in X-space. In your situation it seems (from your other plots) that any illusion of "near normality" is caused by putting together quite non-normal distributions from different regions in X-space with some kind of non-normality (e.g., skewness in different directions) that cancel each other out to some extent. Overall this is a bunch of good reasons to rather go for ordinal logistic regression as already recommended, rather than to use a standard linear (standard linear can be OKish if your regression slopes are low enough in absolute value that y-predictions are safely away from the borders of the y-value range in all realistic areas of X-space). PS: Combining scales resulting in scales that have a bigger value range as you suggest in your item (a) could actually help with making borders of value ranges matter less in standard regression. It depends on the data how well that will work and of course the scales still need to be meaningful (which depends on the subject matter).
