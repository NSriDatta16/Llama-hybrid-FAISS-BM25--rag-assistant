[site]: crossvalidated
[post_id]: 411897
[parent_id]: 
[tags]: 
Large scale SVM classification problem

Problem I am now working on a sentiment analysis task where the largest dataset involves 36 million custom reviews and associated sentiment (positive or negative). The feature extraction process is completed after some efforts. Individual review is encoded into a dense 300 dimension vector and this will serve as a feature for classification. The SVM seems to be a good choice for classification. However, the runtime is endliess even on very powerful cloud computing facility. Therefore, there are three questions If I insist on working with SVM, what trick could I do to improve the runtime. By the way, I am using sklearn.svm.SVC with linear kernel. Is it okay to reduce the number of training samples in practice? I once took statistical learning theory course, it told me that the error rate grows with $O(\frac{1}{\sqrt{n}})$ , so it seems it is okay to reduce to 10% or even 1% of the original dataset? If SVM is hopeless in terms of runtime. Are there other classifiers that could do the job with good speed. Further, is there some results that pinpoint the runtime of different sklearn based algorithms? Thank you, any input is appreciated.
