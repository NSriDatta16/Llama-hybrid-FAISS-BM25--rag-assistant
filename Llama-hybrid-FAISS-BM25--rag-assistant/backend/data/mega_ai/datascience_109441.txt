[site]: datascience
[post_id]: 109441
[parent_id]: 109431
[tags]: 
One of the reasons deep neural networks have taken off is because GPU hardware has improved. The computations needed for practical deep networks are often highly parallelizable. Thus, I think one indicator of "deep" in addition to the standard criteria of ">= 2 layers" is that the network is structured so that many computations can be done in parallel. Conversely, practical networks which are not highly parallelizable are often not "deep".
