[site]: crossvalidated
[post_id]: 104098
[parent_id]: 
[tags]: 
Stock Returns Covariance Prediction - Number of Principal Components

I am working on the following problem. Given N days of stock returns, I compute the covariance matrix for stocks. I then use Probabilistic PCA to "shrink" the covariance matrix. I am trying different number of principal components: 1, 2, etc. I then compute the log likelihood of observing the next-day's returns under the multivariate normal distribution using the "shrinked" covariance matrix (inverse of it to be precise). Just to make sure, the matrix is non-singular, as in PPCA the "remaining" principal components are not zeroed out, but instead are assumed to be uncorrelated. I run this process over some period of time, to compute the total log-likelihood of the next-day's returns prediction under this method (every day re-estimating the covariance matrix, computing next-day's returns log-likelihood, and adding it up). Just to make sure, the computed log-likelihood is always out-of-sample. Now, I would expect that the highest log-likelihood would be for a small number of principal components. However, what I find is that the highest log-likelihood is in fact for the model with 1 principal component, gradually decreasing as more principal components are used. Now, this could mean two things: 1) The best model is the one using 1 principal component (aka CAPM) 2) I need to adjust the computed log-likelihood to make it properly Bayesian. I know that if I would be fitting the model in-sample, then I would need to actually estimate the posterior for the model parameters (including the number of principal components), and then integrate it out. However, because I am calculating log-likelihood out-of-sample, it's in fact something like cross-validation. And so I don't really see the problem with this approach. Are there any problems I might have missed?
