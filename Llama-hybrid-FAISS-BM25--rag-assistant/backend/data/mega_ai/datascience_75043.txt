[site]: datascience
[post_id]: 75043
[parent_id]: 
[tags]: 
Why is my LSTM is working best with batch size of 2 and no hidden layers?

I am building an LSTM for price prediction using Keras . I am using Bayesian optimization to find the right hyperparameters. With every test I make, Bayesian optimization is always finding that the best batch_size is 2 from a possible range of [2, 4, 8, 32, 64] , and always better results with no hidden layers. I have 5 features and ~1280 samples for the test I am trying. Why is this the case? Is the lack of hidden layers due to the fact that I do not have many inputs and samples? Also, literature always seems to suggest that a batch size of 2 is very low and a middle ground for the optimal batch_size is 32. How can I interpret this result?
