[site]: crossvalidated
[post_id]: 298303
[parent_id]: 
[tags]: 
What does "introspection" mean in the context of neural networks?

I have seen papers in which the authors present a modification to some architecture, and claim that it enables "introspective reasoning capabilities" in the network. An example can be seen in the abstract from this (or if you want some details, section D + figure 1 ), where the authors claim that expressing the model as a variational auto-encoder to reconstruct optical flow results in introspective reasoning. What does this mean, and why would any architecture have this effect? Edit: In the case of encoder/decoder architectures, then I think that if the decoder's aim is to reconstruct the intial input $x$ of the encoder, then comparing this reconstruction to $x$ gives some information about the accuracy of the encoder's output $y$. Alhtough, this only works if $x$ can be expressed as some function $f(y)$. Is that correct?
