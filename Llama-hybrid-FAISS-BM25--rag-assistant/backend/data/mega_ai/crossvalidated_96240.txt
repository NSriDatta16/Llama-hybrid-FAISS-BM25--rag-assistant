[site]: crossvalidated
[post_id]: 96240
[parent_id]: 96225
[tags]: 
A neural network can in principle deal with this. Actually, I believe they are among the best models for this task. The question is whether it is modeled correctly. Say you are looking at a regression problem and minimize the sum of squares, i.e. $$L(\theta) = \sum_i (\hat{y}_i - y_i)^2.$$ Here, $L$ is the loss function we minimize with respect to the parameters $\theta$ of our neural net $f$, which we use to find an approximation $\hat{y}_i = f(x_i; \theta)$ of $y_i$. What will this loss function result in for ambiguous data like $(x_1, y_1), (x_1, y_2)$ with $y_1 \neq y_2$? It will make the function predict $f$ predict the mean of both. This is a property which not only holds for neural nets, but also for linear regression, random forests, gradient boosting machines etc--basically every model that is trained with a squared error. It makes now sense to investigate where the squared error comes from, so that we can adapt it. I have explained elsewhere that the squared error stems from the log-likelihood of a Gaussian assumption: $p(y|x) = \mathcal{N}(f(x; \theta), \sqrt{1 \over 2})$. Gaussians are uni modal, which means that this assumption is the core error in the model. If you have ambiguous outputs, you need an output model with many modes. The most commonly used one is mixture density networks , which assume that the output $p(y|x)$ is actually a mixture of Gaussians, e.g. $$p(y|x) = \sum_j \pi_j(x) \mathcal{N}(y|\mu_j(x), \Sigma_j(x)).$$ Here, $\mu_j(x), \Sigma_j(x)$ and $\pi_j(x)$ are all distinct output units of the neural nets. Training is done via differentiating the log-likelihood and back-propagation. There are many other ways, though: This idea is applicable also to GBMs and RFs. A completely different strategy would be to estimate a complicated joint likelihood $p(x, y)$ which allows conditioning on $x$, yielding a complex $p(y|x)$. Efficient inference/estimation will be an issue here. A quite different example is certain Bayesian approaches which give rise to multimodal output distributions as well. Efficient inference/estimation is a problem here as well.
