[site]: crossvalidated
[post_id]: 240532
[parent_id]: 239426
[tags]: 
For deciding how to combine the results of two perceptrons, it might pay to consider what are the convergence guarantees of a single perceptron (at least in the classical version). Given a linearly separable set, a perceptron is only guaranteed to output some separating hyperplane. E.g., each of the two hyperplanes in the following diagram (taken from the Wikipedia entry) is valid: In particular, a perceptron is not guaranteed to find the hyperplane which is equi-distanced to the points it separates - its convergence makes no such considerations (as opposed to the result of an SVM , for example). FWIW, based on this, I don't think your proposed solution displays some properties that a combination of SVMs should intuitively have. For example, logically, if $n \gg m$, we would expect that effectively only the first SVM should affect the results of the combined SVM. This is not necessarily the case. Consider the following dataset consisting of four equi-sized clusters of points. with the red points indicating 0, and the black points indicating 1. Say you train the second perceptron on a small number of points $m$. The chance that all these points are taken from the extreme groups $A$ and $D$ is $\left( \frac{1}{2} \right)^m$ independent from the size of the sets or their distance from each other . If this is the case, note that the separating hyperplane could be anywhere between A and D . Playing around with the distances of the clusters, this could affect every combined classification, even with the weighted scheme you suggest. Note that for high-dimension problems, there are several chances of this type of problem occurring. Because of this, I'd suggest conservatively combining two perceptrons as simply the output of the perceptron trained on the majority of points - it at least avoids this problem.
