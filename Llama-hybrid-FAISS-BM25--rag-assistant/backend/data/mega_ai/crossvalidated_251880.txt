[site]: crossvalidated
[post_id]: 251880
[parent_id]: 251279
[tags]: 
{1} explains why the output of discriminator network $D$ converges to $\frac{1}{2}$: For $G$ fixed, the optimal discriminator $D$ is $D^*_G(\mathbb{x}) = \frac{p_\text{data}(\mathbb{x})}{p_\text{data}(\mathbb{x}) + p_g(\mathbb{x})}$. Therefore, if you have $p_g=p_\text{data}$, meaning that the neural network $G$ has learned the true distribution, then $D^*_G(\mathbb{x})=\frac{1}{2}$. {1} gives some proof of that claim, but intuitively you can consider Algorithm 1's weight update strategy: Sample minibatch of $m$ noise samples $\{ \mathbb{z}^{(1)}, \dots, \mathbb{z}^{(m)} \}$ from noise prior $p_g(\mathbb{z})$. Sample minibatch of $m$ examples $\{ \mathbb{x}^{(1)}, \dots, \mathbb{x}^{(m)} \}$ from data generating distribution $p_\text{data}(\mathbb{x})$. Update the discriminator by ascending its stochastic gradient: $$ \nabla_{\theta_d} \frac{1}{m} \sum_{i=1}^m \left[ \log D\left(\mathbb{x}^{(i)}\right) + \log \left(1-D\left(G\left({z}^{(i)}\right)\right)\right) \right]. $$ When $\mathbb{x}^{(i)}$ is undistinguishable from $\mathbb{z}^{(i)}$, $D$ simply cuts in the middle, i.e. $\mathbb{x}^{(i)} = \mathbb{z}^{(i)} = \frac{1}{2}$. References: {1} Goodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. "Generative adversarial nets." In Advances in Neural Information Processing Systems, pp. 2672-2680. 2014. https://arxiv.org/abs/1406.2661v1
