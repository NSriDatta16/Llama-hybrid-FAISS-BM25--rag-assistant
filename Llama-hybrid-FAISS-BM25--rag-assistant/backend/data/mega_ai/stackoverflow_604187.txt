[site]: stackoverflow
[post_id]: 604187
[parent_id]: 604030
[tags]: 
Irreverently I would point you at Parkinson's Law of Data . However, for each table in a database I try to get an idea of the average record size (especially when dealing with variable length fields like varchars) and then multiply that by the number of records you expect to have added over a year. Then I add them all together, and round up to the most significant digit, and double the result. That leaves plenty of room for overhead and growth. round_up_to_one_sig_digit(sum(average_table_row_size * num_rows_in_one_year)) * 2 A similar approach works with the network capacity, but you will run into some peculiarity of humans and netwoks. They don't all log in at average intervals (so you get peaks during the day/evening and valleys in the early morning. You also don't want to exceed 80% on your network capacity or performance just plain tanks with collisions, etc.)
