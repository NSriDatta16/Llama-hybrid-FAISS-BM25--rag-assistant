[site]: datascience
[post_id]: 69812
[parent_id]: 37021
[tags]: 
Dropout helps improving performance of a machine learning model for the following reasons: Making Network Simpler: It makes the network simpler hence, prevents over fitting. Better than Using a Single Simple Network: It is better than manually re-designing a simpler network because once you have designed a particular architecture, you cannot change it until the whole training process ends i.e, for all the epochs the network architecture is fixed. But in dropout the network is being simpler in various ways in each epoch. Say for 1000 epochs you are actually trying with 1000 types of simpler network's variations. Learn in Many Ways: The input and output of the network don't get changed, the only thing is changing is the mapping between them. So just imagine that, the network is learning the same thing in various different ways. So just like this network, for us the humans- whenever we think of the same problem in different ways we automatically learn to generalize it and our overall knowledge and understanding also improves and the similar thing occurs to the network. As during the dropout process in each epoch randomly some weights (connection from a neuron to another neuron of next layer) are getting cut, hence, we are forcing the network to learn using the existing connections that are still available and thus the network is learning how to analyze the same problem from different perspectives.
