[site]: crossvalidated
[post_id]: 565196
[parent_id]: 
[tags]: 
Why are residual connections needed in transformer architectures?

Residual connections are often motivated by the fact that very deep neural networks tend to "forget" some features of their input data-set samples during training. This problem is circumvented by summing the input x to the result of a typical feed-forward computation in the following way: $$ \mathcal F(x) + x = \left[ W_2 \sigma( W_1 \mathbf{x} + b_1 ) + b_2 \right] + \mathbf{x}.$$ This was schematically represented in [ 1 ] as: On the other hand, it is also well known that transformer architectures have some residual networks, as the following picture elaborates: Question: Residual connections are motivated in the context of very deep network architectures, but attention blocks perform very little computations compared to the networks that were outperformed in [ 1 ]; so, what is the motivation for the presence of shortcut connections in the attention-blocks of transformer architectures ?
