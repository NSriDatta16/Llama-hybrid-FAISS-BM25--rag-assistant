[site]: crossvalidated
[post_id]: 332170
[parent_id]: 332143
[tags]: 
The universal approximation theorem is mainly a proof that for every continuous mapping there exists a neural network of the described structure with a weight configuration that approximates that mapping to an arbitrary accuracy. It does not give any proof that this weight configuration can be learned via traditional learning methods, and it relies on the fact there are enough units in each layer, but you don't really know what is "enough". For these reasons, UAT has very little practical use. Deep networks have multitude of benefits over shallow ones: Hierarchical features: Deep learning methods aim at learning of feature hierarchies with features from higher-levels of the hierarchy formed by the composition of lower level features. Automatically learning features at multiple levels of abstraction allows a system to learn complex functions mapping the input to the output directly from data, without depending completely on human crafted features. [1] Distributed representations: In addition to depth of architecture, we have found that another ingredient is crucial: distributed representations. (...) most non-parametric learning algorithms suffer from the so-called curse of dimensionality. (...) That curse occurs when the only way a learning algorithm generalizes to a new case x is by exploiting only a raw notion of similarity (...) between the cases. This is typically done by the learner looking in its training examples for cases that are close to x (...). Imagine trying to approximate a function by many small linear or constant pieces. We need at least one example for each piece. We can figure out what each piece should look like by looking mostly at the examples in the neighborhood of each piece. If the target function has a lot of variations, we'll need correspondingly many training examples. In dimension d (...), the number of variations may grow exponentially with d, hence the number of required examples. However, (...) we may still obtain good results when we are trying to discriminate between two highly complicated regions (manifolds), e.g. associated with two classes of objects. Even though each manifold may have many variations, they might be separable by a smooth (maybe even linear) decision surface. That is the situation where local non-parametric algorithms work well. (...) Distributed representations are transformations of the data that compactly capture many different factors of variations present in the data. Because many examples can inform us about each of these factors, and because each factor may tell us something about examples that are very far from the training examples, it is possible to generalize non-locally, and escape the curse of dimensionality. [1] This can be translated into pictures: A non-distributed representation (learned by a shallow network) has to assign an output to every piece of the input space (represented by colored hypercubes). However, the number of pieces (and thus number of training points needed to learn this representation) grows exponentially with the dimensionality: On the other hand, distributed representations do not try describe completely every piece of the input space. Instead, they partition the space by isolating simple concepts which can be later merged to provide complex information. See below how K hyperplanes split the space into 2$^K$ regions: (Images from [1] ) For more insight about distributed representations, I also recommend this thread at Quora: Deep Learning: What is meant by a distributed representation? In theory, deep networks can emulate shallow networks: Let us consider a shallower architecture and its deeper counterpart that adds more layers onto it. There exists a solution by construction to the deeper model: the added layers are identity mapping, and the other layers are copied from the learned shallower model. The existence of this constructed solution indicates that a deeper model should produce no higher training error than its shallower counterpart. [2] Note that this is also rather a theoretical result; as the cited paper states, empirically deep networks (w/o residual connections) experience "performance degradation". [1]: http://www.iro.umontreal.ca/~bengioy/yoshua_en/research.html [2]: Deep Residual Learning for Image Recognition (He et al., 2015)
