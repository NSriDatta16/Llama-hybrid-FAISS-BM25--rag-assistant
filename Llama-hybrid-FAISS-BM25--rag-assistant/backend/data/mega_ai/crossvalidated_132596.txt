[site]: crossvalidated
[post_id]: 132596
[parent_id]: 
[tags]: 
AdaBoost over blackbox weak classifier

Can I somehow implement AdaBoost procedure over a weak classifier from another library? For example over SVM from libsvm, or over some neural network. The idea of AdaBoost is that current weights of each sample influence learning step of gradient process. I have implemented AdaBoost over cascade correlation network, and liked the effect. Now I want to try other methods, but cant modify their code, and in some methods there is just no gradient process. So my question is: how can technically weights of training samples can influence learning process? The only idea I have is to form a new train set, in which samples with greater weights occur several times proportionally to current weights, but is looks weird. Are there other ideas? Thanks! UPD: some svm implementations like in python's scikit-learn support weighted train set
