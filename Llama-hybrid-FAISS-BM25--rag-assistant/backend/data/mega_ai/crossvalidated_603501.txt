[site]: crossvalidated
[post_id]: 603501
[parent_id]: 602221
[tags]: 
In the code you provided, Keras is using a multi-output architecture for your neural network, with two branches each having their own output and loss function. The common part of the network is shared between the two branches, and the gradients from the two branches are backpropagated separately to update the weights in the common part of the network. The gradients from the two branches are computed separately using the respective loss functions, and are then used to update the weights in the common part of the network. Keras uses the optimizer specified in the compile function to update the weights. In this case, the optimizer is 'adam'. In terms of the gradients computation, each branch has its own gradients computed and they are not summed up. The gradients are used to update the weights on the common part of the network, the gradients are computed based on the respective losses. The gradients are passed to the optimizer which updates the weights by minimizing the total loss. The optimizer is responsible for updating the weights based on the gradients it receives. Therefore, the optimizer will update the weights based on the gradients computed by the two branches separately. So in summary, the losses are computed separately for each branch, and the gradients from each branch are used to update the weights in the common part of the network separately, but the optimizer takes the total loss which is the sum of the two losses and uses the gradients to optimize the model.
