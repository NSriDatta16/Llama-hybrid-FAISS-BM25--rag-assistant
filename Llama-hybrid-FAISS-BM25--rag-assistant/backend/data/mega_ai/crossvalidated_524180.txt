[site]: crossvalidated
[post_id]: 524180
[parent_id]: 
[tags]: 
Repeating the testing/training split while performing cross-validation on rf model

I'm fitting random forest regressions on my data, and using 10 K-fold cross-validation to evaluate model performance. While re-runing the cross-validation, I noticed that the results differed between each run, sometimes by a lot. So, I decided to repeat the cross-validation calculation 20 times, creating a for-loop, and then summarising the results afterwards. Just to illustrate, I'm doing something like this: for (i in 1:20) { trainIndex So, essentially, each re-run I am splitting my dataset into training and testing sets again. This results in a lot of variance, when ploting the resuls of each run together they vary from ~ 0.2 to 0.6 R.squared . If I don't do this within the loop (i.e. if I split into training/testing before the loop), then the results of the 20 runs are very similar. Which way is the right way to go about this?
