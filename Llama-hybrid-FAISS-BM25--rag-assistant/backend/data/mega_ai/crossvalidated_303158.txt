[site]: crossvalidated
[post_id]: 303158
[parent_id]: 303150
[tags]: 
If you are assigning a treatment, it is not a "quasi-experimental" study as you say in the header. 1) The paper which you cite has a subtle claim: statistical testing of differences in baseline covariates is bad practice. Historically, RCT analyses were initiated with a litany of t-tests and chi-square tests, and provided none of these were significant, the authors claimed that "randomization was good" or "balanced". This is just untrue and misleading, especially with deliberate variable omission which occurs frequently in practice. Regrettably, it's true that large imbalances in baseline covariates between randomization arms will lead to biased or inefficient analyses. The solution they propose is simply to adjust (or perhaps block-randomize) for strongly prognostic variables. 2) There is no widely accepted solution to this problem. The approach most often used is what we'd call an intent-to-treat analysis. Use the data from the non-compliant participants up until the time they leave the study (either with censoring or including them as imbalanced clusters in a mixed model). Some sensitivity analyses can be done to guess at what their trajectory would have been had they stayed in the study. Last-observation-carried-forward (LOCF) is never recommended. You can use multiple imputation via chained equations if there is no growth component (time-covariate interactions) in the model, or you can use some Martingale processes like a simple random walk with Markov Chain to forecast outcomes. Another type of sensitivity analysis is worst-observation-carried-forward (WOCF) where you use the worst observed or unobserved outcome (within an individual or the whole panel) in those who drop out. Presumably drop-out is higher in the treatment group because the treatment is more invasive and different from standard of care, so this can be seen as a conservative approach; however you must verify this. 3) I will just say that you should read this: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4458010/ . Community randomized studies are important and widely popular for evaluating policies and interventions. 4) Again, for any evaluation, analyze "defiers" with intent-to-treat. If I randomize a set of cancer patients to chemotherapy, but 50% of them tolerate the treatment so poorly they refuse to show up for infusions, I can't say the treatment was effective in 100% of the 50% who showed up, it was only 50% effective. 5) With community randomized studies it can be difficult with perfectly nested effects due to site. The best way to handle this variability per the above referenced article is to adjust for important site based predictors of the outcome, where available. This increases the generalizability of the results. While adjusting for site directly is tempting and would normally be done in other circumstances, with few sites (typically the case) perfectly nested within treatment, there can be some forms of bias that occur.
