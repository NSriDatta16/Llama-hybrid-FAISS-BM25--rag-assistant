[site]: datascience
[post_id]: 67157
[parent_id]: 67154
[tags]: 
That pattern is common in neural networks training. Train performance is an estimate of bias, and validation performance is an estimate of variance. Initially both go down. Bias continues to go down, but variance goes up. That is the classic bias-variance tradeoff. However, in neural networks variance will start to go down again. This is called the “double descent curve”. It is not known why neural networks training show this “double descent curve.” One idea is that this pattern might be a concatenation of the two curves. One curve might be from optimization and the other curve might be from sampling. The paper " A Modern Take on the Bias-Variance Tradeoff " goes into more detail about it.
