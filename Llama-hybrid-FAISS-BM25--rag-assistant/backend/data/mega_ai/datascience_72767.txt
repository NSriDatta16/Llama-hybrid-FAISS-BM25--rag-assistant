[site]: datascience
[post_id]: 72767
[parent_id]: 72719
[tags]: 
You could either do it in TensorFlow or not, IMHO. One way is to use pretrained embeddings, or some pretrained model such as BERT to generate a representation of y You can also do it with a TensorFlow model. For example, you could feed each piece of text (processed as a sequence of tokens) into an Autoencoder, take the compressed representation of your data, and later run some clustering techniques such as k-Means on that. You could either use Conv or RNN layers for the Encoder and the Decoder. TensorFlow model can work either with pretrained and trained-on-scratch embeddings. You con create your own by putting an Embedding() layer at the input of your Neural Network. A very fast and effective alternative is to train a doc2vec model. gensim library offers built-in functions for that. PS: I think that TensorFlow 2.x makes things much easier compared to the 1.x, that was not at all simple and super verbose. Pretty much any Keras example you can find around can be ported to TF2 effortlessly.
