[site]: crossvalidated
[post_id]: 279525
[parent_id]: 
[tags]: 
How do neural networks model interaction terms?

If I have data generated from an underlying process like this one: $y = a + b x_1 + c x_2 + d x_1 x_2 + noise$ How would neural networks represent the interaction term between $x_1$ and $x_2$? Is there a special type of unit that can output (a linear combination of) the interactions between its inputs? Or does a network have to learn to approximate multiplication just to express the $x_1 x_2$ interaction term? I know we could simply include $x_1 x_2$ as an additional input (i.e., basis expansion), but I'm trying to understand if there is any way to avoid the exponential blow up in inputs with the dimensionality of $x$, and instead automate the learning of interactions.
