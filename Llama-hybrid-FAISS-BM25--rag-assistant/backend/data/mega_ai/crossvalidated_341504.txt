[site]: crossvalidated
[post_id]: 341504
[parent_id]: 341312
[tags]: 
Analysts trained in machine learning, whose main goal is optimizing computational efficiency when processing massive data, ignore any inherent variance and structure in data by treating each observation as iid (for a recent statement of this see Sirignano, et al., Deep Learning for Mortgage Risk , https://arxiv.org/pdf/1607.02470.pdf ). Sampling using the iid rule would consist of random draws from the data to create train and test groups by time in this example. A statistician would argue that the ML method destroys structure and variance which can only be explained, preserved and recovered by partitioning train and test on, in this example, subjects . Sirignano, et al., explicitly compare the predictive accuracy of deep learning NNs with a baseline logistic regression and conclude that, using the iid rule, logistic regression is grossly inaccurate in comparison to NNs. But is this a fair comparison? One could argue that a comparison based on iid sampling leaves LR with one hand tied behind its back. I'm not aware of papers which use the statistician's rule and does the reverse comparison. In other words, how does the predictive accuracy of the two methods (LR and NNs) compare when the inherent structure and variance in the data is preserved? These issues bring up one of the biggest limitations of NNs: the requirement to convert multilevel categorical fields into 0,1 dummy variables. To take an extreme example residential US zip codes are an ~36,000 level massively categorical feature. Workarounds have been proposed in the literature enabling statistical, structure preserving modeling that facilitate estimating how even such a massively categorical feature explains variance in relative importance terms. On the other hand NNs would require converting this massive feature into ~36,000 dummy variables that would not only create a profusion of useless features (inevitably slowing down convergence) but also confusing the summary of results -- who cares about a specific zip code? Here's a theoretical graphic of performance vs amount of data for which I've lost the reference: There are some CV threads with related discussions, e.g.: Reference showing that only deep learning algorithms benefit from using huge datasets Fitting multilevel categorical variables with neural nets
