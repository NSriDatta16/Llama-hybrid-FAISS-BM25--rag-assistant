[site]: crossvalidated
[post_id]: 246524
[parent_id]: 246481
[tags]: 
In complement to peuhp's excellent answer , I want to add that the only debate I am aware of is whether or not hypothesis testing should be part of the Bayesian paradigm. This debate has been going on for decades and is not new. The arguments against producing a definitive answer to the question "is the parameter $\theta$ within a subset $\Theta_0$ of the parameter space?" or to the question "is model $\mathscr{M}_1$ the model behind the given data?" are many and, in my opinion, compelling enough to be considered. For instance, in a recent paper, as pointed out by peuhp , we argue that model choice and hypothesis testing can be conducted via an embedding mixture model that can be estimated, the relevance of each model or hypothesis for the data at hand being translated by the posterior distribution on the weights of the mixture, which can be seen as an "estimation". The traditional Bayesian procedure for testing hypotheses is to return a definitive answer based on the posterior probability of the said hypothesis or model. This is formally validated by a decision-theory argument using Neyman-Pearson's $0-1$ loss function, which penalises all wrong decisions with the same loss. Given the complexity of the model choice and hypothesis testing settings, I find this loss function much too rudimentary to be compelling. After reading Kruschke's paper , it seems to me that he opposes an approach based on HPD regions to the use of a Bayes factor, which sounds like the Bayesian counterpart of the frequentist opposition between Neymann-Pearson testing procedures and inverting confidence intervals.
