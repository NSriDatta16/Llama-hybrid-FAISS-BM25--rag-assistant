[site]: crossvalidated
[post_id]: 270240
[parent_id]: 269964
[tags]: 
There is a vast literature on methods for handling class imbalance. The Caret library you are using has methods for dealing with class imbalance, documented here . It looks like you are already using the SMOTE method as specified by the sampling parameter in your code snippet currently, which is a very reasonable choice. You are unlikely to get an improvement using other methods. I think you'll find in practice that using the whole dataset as-is will give you the best results. You're more likely to get an improvement using an alternative ML algorithm or implementation. SMOTE and related methods are more for use with classical predictors such as least squares or logistic regression, with flexible methods such as boosting they rarely help. In terms of alternative algorithms, I would recommend xgboost , which is closely related to AdaBag but often superior in practice. If you post diagnostic plots (such as from the classifierplots package) we may be able to provide more specific advice. EDIT: looking at the diagnostic plots, nothing stands out to me as a huge problem. The calibration curve is poor, but that's typical when using boosting methods. AUC values in the high 0.6 to low 0.8 is typical for real-world problems where noise in the label is involved. Is your label expected to be noisy? It's only for pattern recognition type problems where you can really expect to get higher AUC values.
