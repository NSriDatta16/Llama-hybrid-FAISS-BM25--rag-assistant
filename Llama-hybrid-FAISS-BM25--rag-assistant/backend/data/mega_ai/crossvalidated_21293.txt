[site]: crossvalidated
[post_id]: 21293
[parent_id]: 21291
[tags]: 
The answers depend on whether you are using linear SVM or kernel SVM. With linear SVM, you are only using the features you give it, and it does not take into account interactions. With Kernel SVM, basically you are using a lot of different features, depending on which kernel you chose. If there is a separating hyperplane, i.e., if $sign(\sum_{i=1}^{K}\beta_i(x)-\beta_0)$ determines the class where $\beta_i,i \in \{1,2,...K\}$ are the features, then you can have complete fitting of the training data. Usually, you do not specify the features, but give a kernel $K$ that is related to the features as $K(x_1,x_2) = \sum_{i=1}^K \beta_i(x_1) \beta_i(x_2)$. Look up reproducing Kernel Hilbert spaces.
