[site]: datascience
[post_id]: 122420
[parent_id]: 
[tags]: 
Add parameter efficient training in retrieval natural language processing

I am studying some paper with retrieval augmented generation and parameter efficient work on NLP task, ex: adapter, prefix tuning, prompt ... I wonder that can I get better performance with parameter-efficient training and than using retrieval augmented generation in some specific task ? For example: dialogue system. I know that with retrieval augmented generation, we don't need to train the pre-trained model and also get much more correct answer with some predefined document. And parameter-efficient work can use much more efficient way to fine-tune the language model. If I want to first do some parameter-efficient tuning and doing some application with some retrieval document. It can work, right ? Note: I think that I will use some mixture of expert to do parameter-efficient tuning, make sure that the language model not only work on one task, but more than one task.
