[site]: datascience
[post_id]: 67843
[parent_id]: 
[tags]: 
Interpreting Adaboost model results

I'm trying to get a better grasp of model interpretability using many different kinds of models for a binary classification problem. Quick note: By interpretability in this case, what I mean is understanding which features have the largest effect on the target. (Not necessarily understanding how the model works under the hood, although I do understand this fairly well for many models.) For Logistic Regression , I know that you can inspect the model coefficients. Assuming that the features were normalized before training the model, the magnitude of each coefficient is proportional to its importance, and also shows the direction of the influence. (Correct?) For Adaboost (in Python's scikit-learn package), you have access to model.feature_importances() which shows the importance of the feature, but not the direction. By using partial dependence plots, you can also get the direction of the influence. The main question: Am I correct in assuming that using feature importance with partial dependence plots for Adaboost gives a fairly good view of how to understand which features influence the target, similar to using model coefficients for Logistic Regression? Is there a better way that people usually do this? Another quick note: I know that for right now I'm leaving aside the question of model accuracy. I'm assuming in this case that we want to build a model for the sake of interpreting relationships, not necessarily for predictive power.
