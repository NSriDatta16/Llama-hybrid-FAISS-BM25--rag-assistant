[site]: datascience
[post_id]: 124256
[parent_id]: 
[tags]: 
How to improve GPT2 tokenizer trained from scratch?

I trained a GPT2 Tokenizer on Hindi dataset of size 170 MB from scratch and saved it as new_tokenizer. When I tried the new_tokenizer on a Hindi sentence मेरा नाम विनय है The number of tokens generated are 12 which are as follows ['म', 'े', 'र', 'ा', ' न', 'ा', 'म', ' व', 'ि', 'न', 'य', ' ह', 'ै'] I experimented with vocab size = 300, 500, 1000, 5000. In none of the cases new_tokenizer was able to capture complete words. How can I improve my tokenizer? from transformers import GPT2TokenizerFast # validate the changes text = "मेरा नाम विनय है" new_tokenizer = GPT2TokenizerFast.from_pretrained("new_tokenizer_gpt2") encoded_tokens = new_tokenizer.encode(text) print(encoded_tokens) print([new_tokenizer.decode(input_id) for input_id in encoded_tokens]) ```
