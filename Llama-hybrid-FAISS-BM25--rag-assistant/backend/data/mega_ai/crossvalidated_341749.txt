[site]: crossvalidated
[post_id]: 341749
[parent_id]: 222585
[tags]: 
Posting a late reply, since there is a very simple answer which has not been mentioned yet. what are the prices we need to pay by using different "proxy loss functions", such as hinge loss and logistic loss? When you replace the non-convex 0-1 loss function by a convex surrogate (e.g hinge-loss), you are actually now solving a different problem than the one you intended to solve (which is to minimize the number of classification mistakes). So you gain computational tractability (the problem becomes convex, meaning you can solve it efficiently using tools of convex optimization), but in the general case there is actually no way to relate the error of the classifier that minimizes a "proxy" loss and the error of the classifier that minimizes the 0-1 loss . If what you truly cared about was minimizing the number of misclassifications, I argue that this really is a big price to pay. I should mention that this statement is worst-case , in the sense that it holds for any distribution $\mathcal D$. For some "nice" distributions, there are exceptions to this rule. The key example is of data distributions that have large margins w.r.t the decision boundary - see Theorem 15.4 in Shalev-Shwartz, Shai, and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.
