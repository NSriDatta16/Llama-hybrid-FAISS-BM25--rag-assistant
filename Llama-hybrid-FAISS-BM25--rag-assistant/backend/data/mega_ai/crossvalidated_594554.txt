[site]: crossvalidated
[post_id]: 594554
[parent_id]: 
[tags]: 
Understanding what defines a Bayes optimal classifier in classification tasks

Say we are given a data-distribution $D$ over $\mathcal{X} \times \mathcal{Y}$ , where $\mathcal{X}$ is our input/feature space and $\mathcal{Y}$ the set of (discrete) labels. Hence, $D$ is a joint-distribution over datapoints $x \in \mathcal{X}$ and their associated labels $y \in \mathcal{Y}$ . We denote the associated random variables $X$ and $Y$ . In machine learning, we often want to find a "good" predictor/classifier $f$ which associates to a given datapoint $x$ a label $y$ . To define what we mean by "good", one often uses the $0-1$ -loss $\ell(y,f(x))$ , which is $0$ if $y = f(x)$ and $1$ otherwise. Then, one wants to choose a function $f$ , which minimize the expected loss over the data-distribution, also called the (true) risk defined as $R(f) = \mathbb{E}_{(x,y)\sim D} \ell(y,f(x))$ . Now, it is easy to show that the optimal $f^*$ , minimizing the true risk, is $f^*(x):=\text{argmax}_{y \in \mathcal{Y}} \mathbb{P}(Y=y|x)$ . $f^*$ is called the Bayes (optimal) classifer. My question now is simple: Is $f^*$ called Bayes classifier, because it $i)$ associated the most likely class to $x$ , conditioned on $x$ , or $ii)$ because it is minimizes the risk. Some thoughts / what I have researched so far to answer this question: It is easy to see that depending on how one defines the risk, the optimal classifier may not be the most likely class, by e.g., associating different penalties to different class-predictions $\hat{y}:=f(x)$ . Hence, $i)$ and $ii)$ are not equivalent. Regarding $ii)$ : I have delved a bit into statistical decision theory of which I am by no means an expert in and hence, hope for some help in this community. Works on decision theory define the Bayes decision rule , as the rule which minimzes the Bayes risk . This is usually defined w.r.t. some parameters $\theta \in \Theta$ , which we want to estimated based on data $x \in \mathcal{X}$ using a decision rule $\delta(x)$ and we assume we have a prior distribution $\pi$ over $\theta$ . Then, the Bayes risk (with respect to some loss $\ell$ ) can be written as $$ r(\delta) = \int_\Theta \int_\mathcal{X} \ell(\theta, \delta(x)) p(x|\theta) dx \pi(\theta) d\theta $$ Now, if we associate $\theta$ with $y$ , and $\delta$ with $f$ we can rewrite $r(\delta)$ : $$ r(f) = \int_\mathcal{Y} \int_\mathcal{X} \ell(y, f(x)) p(x|y) dx \pi(y) dy \\ = \int_\mathcal{Y} \int_\mathcal{X} \ell(y, f(x)) p(x,y) dx dy \\ = \int_{\mathcal{X} \times \mathcal{Y}} \ell(y, f(x)) p(x,y) dx dy \\ = \mathbb{E}_{(x,y)\sim D} \ell(y,f(x)) = R(f) $$ The second line follows from the fact if the prior $\pi(y)$ is actually the true marginal distribution of $y$ , i.e. $\pi(y)=\int_\mathcal{X} p(x,y) dx$ . I write $\int_\mathcal{Y}$ out of simplicity, but if $\mathcal{Y}$ is discrete, I mean the sum (as in the above classification scenario). The third line follows from interchanging the integrals and assuming that the conditions of the Fubini-Tonelli theorem are satisfied. The fourth line and hence, the equivalence with the true risk in classification follows by definition. Therefore, $f$ minimizing the true risk $R(f)$ is equivalent to a decision rule $\delta$ minimzing the Bayes risk $r(\delta)$ with the specific prior and hence, is a Bayes optimal decision rule and hence, we call it a Bayes optimal classifier. Is this derivation / argument true? This would mean, if I choose a different loss, the Bayes optimal classifier may not be the most likely class. However, if $i)$ would be true, than the Bayes classifer would just not be an optimal classifier anymore. Regarding $i)$ . Here, an argumentation can maybe be developed based on saying that the Bayes classifier is some sort of posterior probability over the classes given the data. Related posts, which my post is partly based on but which do not directly answer my question, are Mathematical definition of Bayes Optimality and Different definitions of Bayes risk Edit: I found this work which is the first and only ML reference I found clearly defining (and not just stating) the Bayes classifier and which supports hypothesis $ii)$ , i.e. they define the Bayes classifier as the classifier minimizing the expected loss and not necessarily the one choosing the most likely class.
