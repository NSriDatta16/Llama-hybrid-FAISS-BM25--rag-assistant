[site]: crossvalidated
[post_id]: 161096
[parent_id]: 
[tags]: 
What does the equation $h^k = \sigma(x * W^K + b^k)$ mean in the context of convolutional neural networks (CNNs)?

I was reading a paper on CNN for auto-encoders and in section 3 they had the following section: For a mono-channel input $x$ the latent representation of the k-th feature map is given by $$ h^k = \sigma(x * W^k + b^k )$$ where the bias is broadcasted to the whole map, $\sigma$ is an activation function (we used the scaled hyperbolic tangent in all our experiments), and $*$ denotes the 2D convolution. A single bias per laten map is used, as we want each filter to specialize on features of the whole input (one bias per pixel would introduce too many degrees of freedom). but I wasn't sure if I understood that equation properly. One of the things that is confusing me in particular is the index $k$. When they write $h^k$, is $h^k \in \mathbb{R}$, i.e. is $h^k$ a single number or a vector (or a matrix)? The reason I am not sure is because the equation they have there is doing the convolution and the convolution outputs vectors (or in fact, functions). Therefore, it didn't quite make sense to me what was going on, specially because they mention that $*$ was the 2D convolution. Also, is what is $W^k$ indexing? A row of $W$ and column of $W$? Is $b^k$ a vector or a single number? Also, I don't think I even know if $x$ is a vector or a matrix, or an image, that wasn't clear either. Does someone know what is going on? So as a list/summary: What are the dimensions of the mathematical objects in question? $x$, $W$, $W^k$, $x * W^k$, $b^k$, $b$, $h$, $h^k$ (hope I didn't miss any). What does the convolution return in this context? The usual convolution conv2 that matlab implements? $(x * k)[i,j] = \sum_n \sum_m x[m,n]k[i -m, j -n]$? What is going on with the indexing $k$?
