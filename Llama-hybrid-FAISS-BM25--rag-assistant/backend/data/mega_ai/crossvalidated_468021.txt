[site]: crossvalidated
[post_id]: 468021
[parent_id]: 467975
[tags]: 
What you call "non bootstrapping" is actually a parametric bootstrapping . That's what @whuber is doing in the answer/post that prompted your question. Here's a semi-formal definition of a bootstrap from Babu, G. J., and Rao, C. R. (2004). Goodness-of-fit tests when parameters are estimated . Sankhya, 66, no. 1, 63-74.: The idea's that you estimate the parameters of the distribution, then generate sample from the distribution with estimated parameters. Now you can study sampling distribution of parameters. That's what you called "Monte Carlo null." This presentation explains the method in good detail. The application was Kolmogorov-Smirnov goodness-of-fit test. It's a very popular GoF for distribution fitting. The problem is that one needs to know the true distribution, i.e. its parameters are not estimated from data. When parameters are estimated, then the test statistics can be generated by parametric bootstrapping. The author of the presentation has detailed papers on the subject, they're quite well written. For instance, KS test application is explained in this one: Babu, G. J., and Rao, C. R. (2004). Goodness-of-fit tests when parameters are estimated. Sankhy¯a, 66, no. 1, 63-74. Here's another paper on parametric bootstrapping used in connections with Bayesian inference to generate the posterior distributions: Efron B. Bayesian inference and the parametric bootstrap. Ann Appl Stat. 2012;6(4):1971‐1997. doi:10.1214/12-AOAS571 url: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3703677/ It's just another application of the same idea, which you describe as "Monte Carlo null." There's any number of papers on this subject, you may pick any that you consider authoritative.
