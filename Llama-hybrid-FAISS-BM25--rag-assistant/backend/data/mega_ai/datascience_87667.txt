[site]: datascience
[post_id]: 87667
[parent_id]: 49313
[tags]: 
In order to avoid overfitting to the finetuning data (which is what the authors of ULMFiT focus on when they refer to catastrophic forgetting), these measures are normally taken for BERT: Multiple BERT models are trained (random restarts), and the one with the best validation performance is chosen. The bias compensation in the Adam optimizer is omitted, leading to the so-called "BERTAdam". This turned to be one of the main sources of BERT finetuning instability, as shown in On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines and in Revisiting Few-sample BERT Fine-tuning , so it's better to stick to Adam or AdamW with bias compensation. Only around 3 finetuning epochs were used. This turned to be needed only because of the omission of the bias compensation (see previous bullet and the referred articles).
