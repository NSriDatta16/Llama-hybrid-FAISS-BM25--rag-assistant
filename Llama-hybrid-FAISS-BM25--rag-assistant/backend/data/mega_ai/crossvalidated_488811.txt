[site]: crossvalidated
[post_id]: 488811
[parent_id]: 488804
[tags]: 
I don't believe it's possible in general to do this with ordinary least squares, since OLS is at heart a trick to calculate $\hat \beta$ in $E[y] = \bf{X}\hat\beta$ using matrix division. It can be done more generally, though. I think the tricky bit is figuring out exactly what you mean by each predictor having an additive or multiplicative effect on the response . For example, with two predictors, do you mean: $$ y = (\beta_0 \times \beta_2 x_2) + \beta_1 x_1 ?\\ y = (\beta_0 + \beta_1 x_1) \times \beta_2 x_2 ?\\ y = \beta_0 + (\beta_1 x_1 \times \beta_2 x_2) ? \\ $$ ...and there's probably others as well. Of these, the first (multiplication before applying the additive effects) is the simplest to estimate, as it has fewer high-order multiplicative terms, and is more likely to correspond to the model you intended. Unfortunately, even this isn't simple to estimate, since the predictions with $\beta_0 = 2, \beta_2 = 2$ → $y = (2 \times 2 \times x_2) + \beta_1 x_1$ are the same as those when $\beta_0 = 1, \beta_2 = 4$ → $y = (1 \times 4 \times x_2) + \beta_1 x_1$ . The best way around this is to use a Bayesian estimation tool like Stan to set reasonable priors on your model parameters (for example that the multiplicative effect, $\beta_2$ , should be close to 1), and find the best parameter estimates that are consistent with those priors.
