[site]: crossvalidated
[post_id]: 191233
[parent_id]: 191199
[tags]: 
For your first question Why is it important to take correlation into account and what can go wrong? It depends on the model you use. Let say you want to do some clustering on a dataset that has some correlated features. If you use K-means with the Euclidian distance, the clusters will be spherical around their center. K-means can be viewed as a special case of fitting Gaussian distributions to your data; the center of each cluster is the mean, and, more importantly, since the cluster is spherical, there is an assumption that the features are not correlated. If the features are correlated, K-means clustering loses meaning. So if you fit a Gaussian with a diagonal covariance matrix to a your data, you might indeed accept or reject your hypothesis wrongly. In the case of the Outlier example you cite, the 95% interval of the Gaussian would not consider the circled data point as an outlier, while it "obviously is". In such a setting, using PCA to decorrelate your data into principal components helps. The principal components are the dimensions that maximize variance, and are orthogonal, there should not be any correlation between dimensions, which should solve the correlation problem. There are a lot of visualizations on how PCA works, they might be helpful if you have trouble understanding what it does. Second question I want to determine the borders of the islands, but I know that there is a correlational structure inside each island (...) How should I deal with this problem? It is unclear to me what your data is, as your "sample of rows of numbers" is not cristal clear, and I do not see a clear link between estimating Gaussian distributions and HMM. From your question, I understand that you assume your data comes from multiple Gaussian distributions, and you want to approximate them. If your model accounts for correlation and tries to learn it from data instead of posing too strict assumptions, you will probably be fine (as long as there is sufficient data). For this I would go for a Gaussian Mixture Model and, indeed, learn the covariances matrices. There is always the problem of over/underfitting, but no method will give you the true model behind no data. The more assumption you make, the less data you need, but you lead the risk of those assumptions being false. On the other end, making no assumptions and trying to learn everything is nice, but requires more data to learn something meaningful. If you can make educated/domain-specific assumptions, put those into the model but do not rely on picking a model without understanding the assumption it makes. Response to comments: How will PCA behave if I the covariance matrix is sparse? A sparse covariance matrix, mostly composed of 0, means most features are not correlated. In that case, the principal components will mostly be the original features vectors, and some of the original vectors will be decomposed. How would you regularise the covariance matrix? If you want the covariance matrix to be sparse, I guess you could add the L1|L2 norm of the covariance matrix to the cost function that is minimized by the model you choose. In the case of GMM, it would be added to the log-likelihood. I could not find much background in a relatively short search, other than this paper .
