[site]: crossvalidated
[post_id]: 213148
[parent_id]: 93569
[tags]: 
As Stefan Wagner notes, the decision boundary for a logistic classifier is linear. (The classifier needs the inputs to be linearly separable.) I wanted to expand on the math for this in case it's not obvious. The decision boundary is the set of x such that $${1 \over {1 + e^{-{\theta \cdot x}}}} = 0.5$$ A little bit of algebra shows that this is equivalent to $${1 = e^{-{\theta \cdot x}}}$$ and, taking the natural log of both sides, $$0 = -\theta \cdot x = -\sum\limits_{i=0}^{n} \theta_i x_i$$ so the decision boundary is linear. The reason the decision boundary for a neural network is not linear is because there are two layers of sigmoid functions in the neural network: one in each of the output nodes plus an additional sigmoid function to combine and threshold the results of each output node.
