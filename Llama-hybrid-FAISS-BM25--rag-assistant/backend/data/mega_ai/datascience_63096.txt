[site]: datascience
[post_id]: 63096
[parent_id]: 63036
[tags]: 
You are mixing two different concepts in the same question: One hot encoding : approach to encode $n$ discrete tokens by having an $n$ -dimensional vectors with all 0's except one 1. This can be used to encode the tokens them selves in networks with discrete inputs, but only if $n$ is not very large, as the amount of memory needed is very large. Transformers (and most other NLP neural models) use embeddings, not one-hot encoding. With embeddings, you have a table with $n$ entries, each of them being a vector of dimensionality $e$ . In order to represent token $k$ , you select the $k$ th entry in the embedding table. The embeddings are trained with the rest of the network in the task. Positional encoding : in recurrent networks like LSTMs and GRUs, the network processes the input sequentially, token after token. The hidden state at position $t+1$ depends on the hidden state from position $t$ . This way, the network has a means to identify the relative positions of each token by accumulating information. However, in the Transformer, there is no built-in notion of the sequence of tokens. Positional encodings are the way to solve this issue: you keep a separate embedding table with vectors. Instead of using the token to index the table, you use the position of the token. This way, the positional embedding table is much smaller than the token embedding table, normally containing a few hundred entries. For each token in the sequence, the input to the first attention layer is computed by adding up the token embedding entry and the positional embedding entry. Positional embeddings can either be trained with the rest of the network (just like token embeddings) or pre-computed by the sinusoidal formula from (Vaswani et al., 2017) ; having pre-computed positional embeddings leads to less trainable parameters with no loss in the resulting quality. Therefore, there is no advantage of anyone over the other, as they are used for orthogonal purposes.
