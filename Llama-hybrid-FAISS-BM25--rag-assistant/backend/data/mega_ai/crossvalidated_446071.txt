[site]: crossvalidated
[post_id]: 446071
[parent_id]: 445565
[tags]: 
when challenged, just say "the problem is highly non-convex and optimization is hard" However handwavy that may sound, that is actually very precise description of the problem. When facing such a problem, you have two options: Try hard to come up with some convex relaxation to (at least a part of) the problem where you can guarantee desirable properties like convergence and optimality, or go ahead with the tools you have and see how far it gets you. In the case of deep learning, simple tools like SGD got us surprisingly far. Is that a bad research? I don't think so, for two reasons: Deep learning research has produced large number of experimental results in a particular type of high-dimensional non-convex optimization, and these experimental observations inspire research of theoretical foundations, such as [1]. This is not a unique to DLâ€”I dare to say in most science fields experimental observations come first and later theories are proposed that explain these observations. Deep learning research has produced large number of practical applications, pushing forward the state-of-the-art of tasks that computers can do. Solving actual problems is one of the most important goals of research. Problems like driving cars and diagnosing diseases are by no means easy problems, and researchers very much care about whether the solutions are good. However, "good" is measured in different terms than in other fields. If a solution works in actual real-life scenario and solves an actual problem, it is only fair to consider it good. Finally, to answer your question in the title, Is it fair to say that most of the 'success' of deep learning comes from the fact we do not need to optimize exactly? It is wrong. The most success of DL comes from having vast amounts of data, vast computational power, and simple but very general models and methods. The fact that we can get good solutions with simple SGD is rather an implication of these conditions. [1]: Choromanska, Anna, et al. "The loss surfaces of multilayer networks." Artificial intelligence and statistics. 2015.
