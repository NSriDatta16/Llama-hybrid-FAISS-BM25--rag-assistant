[site]: crossvalidated
[post_id]: 515758
[parent_id]: 
[tags]: 
Information loss

Let's imagine we have some binary random variable $Y$ and some other continuous variable $X\in\mathbb{R}$ and we have some sample of size $n$ . Suppose we want to determine the relationship between $Y$ and $X$ in some sense. Given that $Y$ is binary, we first round all values of $X$ in some neighbourhood to a single value. This gives us $\bar{X}$ . For example, $\{2.2,2.7,2.9,3.8\}$ would become $\{2.0,2.5,2.5,3.5\}$ if we chose to round down to the nearest half integer. Then for each distinct $\bar{X}$ we could define the $\bar{Y}$ , the average $Y$ in that neighbourhood. We could then use, for example, a spline to flexibly model the relationship between $\bar{Y}$ and $\bar{X}$ . Is there some reasonable way to measure the information loss of the approximate relationship compared to the original relationship? I would assume the information lost would be at least a function of the extent of the aggregation. I was thinking an approach would be to evaluate upper and lower bounds of the entropy i.e. the entropy with no rounding (lower bound) and with everything rounded to a single value (upper bound). Then, for a given choice of rounding, one could check where the entropy sits in that range.
