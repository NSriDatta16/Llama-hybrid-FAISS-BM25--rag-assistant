[site]: datascience
[post_id]: 14923
[parent_id]: 14874
[tags]: 
A few assumptions I'm making: I believe you're mapping your input set of integers to another integer, which means your output is discrete, so I think you're using [multinomial logistic regression] ( https://en.wikipedia.org/wiki/Multinomial_logistic_regression ). You're treating your discrete inputs (integers) as continuous--i.e., you're not using a one-hot-encoding for each integer. Your underlying function is something to the like of if $\forall j=2,..,10$ if $x \in (x_{j-1}, x_j)$ then $y = c_j$, correct? If the above three are true, then it's clear why Logistic won't work and why the Decision Tree will. The Logistic will approximate the relationship between your input and output with a $continuous$ linear relationship, which is not exactly what you have here because you have a discontinuity. It's worth mentioning that Decision Trees will perform better at these points, which means the more discontinuities you have the worse your performance will be. But after a quick simulation, the magnitudes you quoted are too large to be coming from this. So I'm guessing you're using $binary$ logistic regression, which is just the wrong choice of model (since it's mapping everything to a single class instead of many classes). So, it sounds like you're just using the wrong model. Below is a quick demo in R. # Loading packages require(nnet) require(rpart) set.seed(024) buildData = x_j[i-1] & x
