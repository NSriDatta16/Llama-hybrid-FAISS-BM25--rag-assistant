[site]: crossvalidated
[post_id]: 231606
[parent_id]: 
[tags]: 
Prioritizing A/B tests by using the prior data

Let's say that I am an active user of the Tinder app and in addition to the standard features, I've got access to the A/B testing of my primary picture (the one that appears first when users stumble on my profile). My objective is maximizing the number of "swipe right" events, so I try various pictures, but I test them first to make sure that they wouldn't harm the "swipe right rate". I am using Bayesian approach to assess the test pictures' performance as soon as possible. I'm preserving the history of my test results which looks like: So far so good. But I can only run the single test at a time, let's say this is the limitation of this hypothetical Tinder Testing feature. At the same time I have a lot of pictures I would like to try, so I need to somehow prioritize them. Here is the idea : Compare each picture with the basiline one and detect what areas have been changed. For example in one of the new pictures I have a new haircut, so "hairs" have changed, another one features my new car, so "environment" has changed and so on. For each area select the tests results from the history and build the beta-distribution of the "swipe right rate" of those tests. Compare the calculated beta distributions by using one of the available methods, let's say compare the graphs to roughly estimate which area can provide better results. Arrange the "tests backlog" accordingly. I do not have the strong math/stat background, so I understand that that idea might be pretty silly, but I would like to know what community thinks about that. In general what methods could be used to prioritize the a/b tests based on the prior data?
