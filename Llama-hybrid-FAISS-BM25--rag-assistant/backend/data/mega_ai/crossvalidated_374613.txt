[site]: crossvalidated
[post_id]: 374613
[parent_id]: 374603
[tags]: 
Frequently faced issues while clustering are, 1. full distance matrix, 2. Memory, 3. runtime. In hierarchical clustering, we create hierarchical decomposition of the given set of data in two ways such as from bottom to the top or top to down. On the basis how we create hierarchical decomposition we divide this method into two approaches one is agglomerative approach and other is the divisive approach. Hence, hierarchical clustering is the iterative process. Hierarchical clustering builds clusters within clusters and does not require a pre-specified number of clusters like k means. If you run hclust from R for a bigger dataset, it will demand higher RAM. In k-means clustering, we try to identify the best way to divide the data into k sets simultaneously. A good approach is to take k items from the data set as initial cluster representatives, assign all items to the cluster whose representative is closest, and then calculate the cluster mean as a new representative; until it converges (all clusters stay the same). Apart from that following are the pros and cons of each of techniques: KMEANS Advantages • Easy to implement • With a large number of variables, K-Means may be computationally faster than hierarchical clustering (if K is small). • k-Means may produce tigher clusters than hierarchical clustering • An instance can change cluster (move to another cluster) when the centroids are recomputed. Disadvantages • Difficult to predict the number of clusters (K-Value) • Initial seeds have a strong impact on the final results • The order of the data has an impact on the final results • Sensitive to scale: rescaling your datasets (normalization or standardizaHon) will completely change results. While this itself is not bad, not realizing that you have to spend extra attention on to scaling your data might be bad. Hierarchical Advantages • Hierarchical clustering outputs a hierarchy, ie a structure that is more informative than the unstructured set of flat clusters returned by k-means. Therefore, it is easier to decide on the number of clusters by looking at the dendrogram • Easy to implement Disadvantages • It is not possible to undo the previous step: once the instances have been assigned to a cluster, they can no longer be moved around. • Time complexity: not suitable for large datasets • Initial seeds have a strong impact on the final results • The order of the data has an impact on the final results • Very sensitive to outliers From all consideration, in my opinion KMeans should be preferred choice.
