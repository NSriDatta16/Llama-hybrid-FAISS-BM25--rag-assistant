[site]: datascience
[post_id]: 29552
[parent_id]: 29529
[tags]: 
If you want to model the unique meaning of commonly occurring n-grams, often times called collocations , the best solution is to train a new embedding space to model those specific semantics. Aggregating existing embeddings will only capture part of the collocation meaning. If you do not have very much data, you can do fine-tuning/transfer learning. Fine-tuning/transfer learning takes an existing model architecture and weights, then does additionally training with more data. In this case, take Google's Wikipedia model and train it with your collocations. An example of fine-tuning for word2vec in Keras can be found here .
