[site]: crossvalidated
[post_id]: 532501
[parent_id]: 
[tags]: 
how to check overfitting in my ML/ DL model

My data contains 2 classes the negative one contains 292 samples the positive one contains 192. I trained my model with 5 ML/DL models: data = data.drop(['filename'],axis=1)#Encoding the Labels genre_list = data.iloc[:, -1] encoder = LabelEncoder() y = encoder.fit_transform(genre_list)#Scaling the Feature columns X = data.iloc[:, :-1].values scaler = StandardScaler() X = scaler.fit_transform(np.array(data.iloc[:, :-1], dtype = float)) #Dividing data into training and Testing set X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size=0.22) Our models are: #GLM linear_model.LogisticRegressionCV(), #Navies Bayes naive_bayes.GaussianNB(), #Nearest Neighbor neighbors.KNeighborsClassifier(), #SVM svm.SVC(probability=True) ] and then the training of the models: MLA_columns = [] MLA_compare = pd.DataFrame(columns = MLA_columns) row_index = 0 for alg in MLA: predicted = alg.fit(X_train, y_train).predict(X_test) fp, tp, th = roc_curve(y_test, predicted) MLA_name = alg.__class__.__name__ MLA_compare.loc[row_index,'MLA Name'] = MLA_name MLA_compare.loc[row_index, 'MLA Train Accuracy'] = round(alg.score(X_train, y_train), 4) MLA_compare.loc[row_index, 'MLA Test Accuracy'] = round(alg.score(X_test, y_test), 4) MLA_compare.loc[row_index, 'MLA Precission'] = precision_score(y_test, predicted) MLA_compare.loc[row_index, 'MLA Recall'] = recall_score(y_test, predicted) MLA_compare.loc[row_index, 'MLA AUC'] = auc(fp, tp) row_index+=1 MLA_compare.sort_values(by = ['MLA Test Accuracy'], ascending = False, inplace = True) MLA_compare Their evaluations metrics are: Their confusion matrix: The auc/roc curbe: model = Sequential() model.add(layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],))) model.add(layers.Dense(128, activation='relu')) model.add(layers.Dense(64, activation='relu')) model.add(layers.Dense(10, activation='softmax')) model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy','mse', 'mae','mape', 'cosine_proximity']) classifier = model.fit(X_train, y_train, validation_split=0.33, epochs=20, batch_size=128, verbose=0) pred_train= model.predict(X_train) scores = model.evaluate(X_train, y_train, verbose=0) print('Accuracy on training data: {} \n Error on training data: {}'.format(scores[1], 1 - scores[1])) pred_test= model.predict(X_test) scores2 = model.evaluate(X_test, y_test, verbose=0) print('Accuracy on test data: {} \n Error on test data: {}'.format(scores2[1], 1 - scores2[1])) Accuracy on training data: 0.9792899489402771 Error on training data: 0.0207100510597229 Accuracy on test data: 0.9383561611175537 Error on test data: 0.06164383888244629 Do I have overfitting in my models? and which model is the best?
