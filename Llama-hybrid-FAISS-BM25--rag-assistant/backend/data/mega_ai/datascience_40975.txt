[site]: datascience
[post_id]: 40975
[parent_id]: 40972
[tags]: 
Provided each network implements the XOR function approximately, but within reasonable error bounds, then this is NEAT behaving as designed. Neural networks allow for multiple equivalent solutions. Even using a fixed architecture and stochastic gradient descent to learn a simple function, there is a good chance of ending up with very different weights each time. With NEAT, it also explores alternative architectures, and the search includes many calls to random number generators in order to make decisions. The main deterministic step in NEAT is the selection process - comparing population members with each other to rank them in terms of fitness. However, as multiple designs of neural network can all solve the XOR problem roughly equally well, it is unlikely this will find the same one each time. If you need to exactly reproduce an experiment in NEAT, you can achieve that by setting the RNG seed. I think this could just be a call to random.seed() for the library you are using, but some libraries may have other RNG instances that need setting up too.
