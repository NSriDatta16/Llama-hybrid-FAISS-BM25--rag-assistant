[site]: datascience
[post_id]: 27771
[parent_id]: 16620
[tags]: 
The output dimension of a convolution in deep learning depends on multiple factors the size of the filter (aka kernel) the padding (whether you add zeros or not around your image and how many) the numbers of filter that you use the stride The simplest dependency is that on the numbers of filters N . It gives you the numbers of feature maps that your output has. For the input that may be the RGB channels i.e. 3, for the output this number can be chosen freely. The next factor is the zero-padding. If you use a filter size of (3,3) and "valid" padding i.e. adding NO zeros around the image you end up with an output of dimension. (100, 100, 3) -> (98, 98, N) Because you use a stride of 1. If you move the filter across the image at the end of the picture in each direction the filter will hit the border after 98 steps. However, if you use "SAME" padding you compensate for the filter size -in case of a filter size of (3,3) that would correspond to one line of zeros around the image- you will end up with: (100, 100, 3) -> (100, 100, N) With a stride of 2 for example you shift the position of the filter by two pixels. Therefore, you get (100, 100, 3) -> (50, 50, N)
