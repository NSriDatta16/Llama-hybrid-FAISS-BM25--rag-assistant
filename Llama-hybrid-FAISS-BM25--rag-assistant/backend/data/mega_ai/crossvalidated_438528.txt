[site]: crossvalidated
[post_id]: 438528
[parent_id]: 438496
[tags]: 
They are not necessarily contradictory, since hidden representation in CNNs has also spatial dimension which decreases with depth. In a CNN, the amount of information about the input sample in a hidden layer $n$ (let's call it $I(h_n)$ ) is size of the feature map times number of features: $$I(h_n) = \mathrm{width}\times\mathrm{height}\times{\#features}.$$ Here we mean width and height of the feature map. Usually, number of features is doubled whenever the resolution is halved, so $$I(h_{n+1})= \frac{\mathrm{width}} 2 \times\frac{\mathrm{height}} 2 \times{2\,\#features}$$ which is $\frac 1 2 I(h_n)$ . Number of features increased, but the amount of information has halved, forcing the network to compress the information. In this sense, both types of network behave similarily.
