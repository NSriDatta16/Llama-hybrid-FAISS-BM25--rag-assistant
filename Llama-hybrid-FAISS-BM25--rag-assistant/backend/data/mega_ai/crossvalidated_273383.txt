[site]: crossvalidated
[post_id]: 273383
[parent_id]: 
[tags]: 
Audio classification with neural networks

I would like to classify bird species from audio recordings. To do this, I am calculating a number of features in a frequency spectrum: I take certain block size (say, 2048 samples) and compute mel spectrum coefficients, spectral rolloff, LSF coefficients, spectral flux etc. - they make together a very good description of the sound. A bird call typically lasts a few seconds, meaning it consists (with 16 kHz sampling) of easily 30000 samples. Can you provide some hints on what architecture I could use to tackle the problem? I was thinking of the following: Calculate features for the 2048 block size. Calculate next block with half overlap (1024). Repeat the process until I have captured 2 seconds of features. If in a single block I get 50 features, then the complete vector would have roughly 1500 features (because of the 30k samples in a typical call). The 1500 features will constitute my input to the ANN. It covers 2 seconds of a recording. With half of overlap I would then take next 2 seconds and repeat the process. I was then thinking of ANN with 1-2 hidden layers, likely much smaller than the input vector. What do you think? As an alternative I am considering making a spectrogram and working with CNNs.
