[site]: datascience
[post_id]: 122711
[parent_id]: 122710
[tags]: 
Dropout is a regularization technique used to prevent overfitting by dropping some ratio of units of the neural network and on the other hand BatchNorm is used to normalize the units of each batch. The ordering is the problem. Why I do not have enough concrete explanation as to why placing dropout and batchNorm together is a problem, it is a common observation that the losses tend to go up when these two are placed side by side. It's generally a rule of thumb to make use of batchNorm instead of dropout in Convoluted layers and mostly before the activation function. Dropout on the other hand is preferably used in dense layers . There is some disharmony created when these two are placed together. So my best guess is to stick with batch Norm
