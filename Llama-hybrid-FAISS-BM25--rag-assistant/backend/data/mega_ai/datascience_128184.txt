[site]: datascience
[post_id]: 128184
[parent_id]: 
[tags]: 
Weighting training instances by time in machine learning models

I am training a neural network based on data whose relevance I think diminishes based on how far each instance is in the past. I've had a look and one way to do this it seems is to 'weight' training instances according to recency, using some kind of exponential decay function. My question is how to account for the fact that, once the model is trained on such a weighted dataset: i) Testing instances will not be all equidistant (in time) from the training dataset. Say for instance I trained on time-weighted data from 2012-2017 and used 2018-2019 as testing, instances in 2018 will be 'closer' to the training set than those in 2019...will there not be an inherent 'decay' in the model's predictive capabilities as we make predictions further and further away from the training data? ii) Relatedly, what is the practice when it comes to updating a trained model? Thinking about my example, is there a procedure to decide long I would let a model trained using 2012-2017 data make predictions before deciding to retrain or update? I tried searching online, some others have asked related questions on this forum but I don't think a solution has been found.
