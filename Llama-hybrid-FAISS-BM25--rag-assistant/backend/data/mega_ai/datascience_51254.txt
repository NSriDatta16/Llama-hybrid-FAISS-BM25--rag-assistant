[site]: datascience
[post_id]: 51254
[parent_id]: 31248
[tags]: 
This is simple case of overfitting. To improve accuracy, I would suggest to do the following changes: Since your 'x' variable are sentences, you can try to use Sequential model with one Embedding Layer and one LSTM layer: from tensorflow.keras.layers import Dense, Embedding, LSTM from tensorflow.keras.models import Sequential model = Sequential() model.add(Embedding(max_features, 32)) model.add(LSTM(32) Add the last layer as Dense Layer, and the loss function as binary_crossentropy: model.add(Dense(1, activation='sigmoid')) model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy']) Fit the model and store the history variable to find the optimal epochs: history = model.fit(x_train, y_train, epochs=20, batch_size=128, validation_split=0.2) Plot the following 2 curves: i) Validation loss vs epochs ii) Validation accuracy vs epochs # Plotting Results import matplotlib.pyplot as plt acc = history.history['accuracy'] val_acc = history.history['val_accuracy'] loss = history.history['loss'] val_loss = history.history['val_loss'] epochs = range(1, len(acc) + 1) plt.plot(epochs, acc, 'b', label='Training acc') plt.plot(epochs, val_acc, 'g', label='Validation acc') plt.xlabel('Epochs') plt.ylabel('Accuracy') plt.title('Training and validation accuracy') plt.legend() fig = plt.figure() fig.savefig('acc.png') plt.plot(epochs, loss, 'b', label='Training loss') plt.plot(epochs, val_loss, 'g', label='Validation loss') plt.xlabel('Epochs') plt.ylabel('Loss') plt.title('Training and validation loss') plt.legend() plt.show() After which you will get a graph something like these: For this specific case, we see that the optimal epochs is occurring at 12, So we need to again train with 12 epochs and test on test data. model.fit(x_train, y_train, epochs=12, batch_size=128) model.evaluate(x_test, y_test)
