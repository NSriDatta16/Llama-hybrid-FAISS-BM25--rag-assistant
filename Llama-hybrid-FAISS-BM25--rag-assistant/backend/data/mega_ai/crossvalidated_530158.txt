[site]: crossvalidated
[post_id]: 530158
[parent_id]: 
[tags]: 
Why accuracy is divided by the number of classes?

I am doing simple image classification using CNN. My accuracy always equals one divided by the number of classes. For example, for one class it is 100%, for two classes it is 50%, for three it is 33%, for four 25% and so on. Could you please help me with this issue? In what conditions something like this happens? The inputs to the network are 240*64 tensors. I have tried normalizing them but it didn't help. Here is the network I am using. This network is working well for MNIST data but not for my data: class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(1, 10, kernel_size=1) self.conv2 = nn.Conv2d(10, 20, kernel_size=5) self.conv2_drop = nn.Dropout2d() self.fc1 = nn.Linear(20*14*58, 50) self.fc2 = nn.Linear(50, 4) def forward(self, x): x = F.relu(F.max_pool2d(self.conv1(x), 2)) x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2)) x = x.view(-1, 20*14*58) x = F.relu(self.fc1(x)) x = F.dropout(x, training=self.training) x = self.fc2(x) return F.log_softmax(x) And here is the training procedure: network.train() pred=0 correct=0 for batch_idx, (data, target) in enumerate(train_loader): optimizer.zero_grad() output = network(data.double()) loss = F.nll_loss(output, target) top_p, top_class = output.topk(1, dim=1) pred1 = top_class.flatten().long() loss.backward() optimizer.step() pred = output.data.max(1, keepdim=True)[1] correct += pred.eq(target.data.view_as(pred)).sum() target = np.round(target.detach()) y_pred.extend(pred.tolist()) y_true.extend(target.tolist()) CF = confusion_matrix(y_true, y_pred) #print( skm.classification_report(y_true,y_pred)) if batch_idx % log_interval == 0: print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item())) train_losses.append(loss.item()) train_counter.append( (batch_idx*64) + ((epoch-1)*len(train_loader.dataset))) ```
