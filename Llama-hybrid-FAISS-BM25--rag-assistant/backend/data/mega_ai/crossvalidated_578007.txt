[site]: crossvalidated
[post_id]: 578007
[parent_id]: 
[tags]: 
Is deep double descent important in practical contemporary CNNs?

Deep double descent is an empirically observed phenomenon that happens with contemporary neural networks. Its essence is that often, increasing the model complexity first leads to the test loss decreasing at first, then increasing, and then it starts decreasing again and becomes better and better. According to Understanding “Deep Double Descent” by Evan Hubinger double descent occurs not just as a function of model size, but also as a function of training time and dataset size, and since double descent can happen as a function of dataset size, more data can lead to worse test performance! Is deep double descent important in contemporary CNNs? If I am solving some computer vision task with a modern CNN-based model, does it actually make sense to continue training it more and more while it overfits harder and harder and wait for deep double descent to happen? Also, does it make sense to discard a part of the training dataset to cause the model to overcome the interpolation threshold? Do people do this?
