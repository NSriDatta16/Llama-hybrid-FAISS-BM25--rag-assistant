[site]: datascience
[post_id]: 30743
[parent_id]: 30742
[tags]: 
Unless you normalize the MSE in scenario 1 (or denormalize the MSE in scenario 2), comparing two MSE with two different scales is irrelevant . You can have data with values varying from 10 to 30 millions, centered then normalized to -1/+1. Suppose you have a MSE of 1000 in the first case, and 0.1 in the second, you will easily see that the second MSE is way more impacting than the first one after (de)normalization. That said, if you want to retrieve the target from scenario 2, you need to apply the reverse operations to what has been done to get the "normalized" target. Assuming for instance that you centered / reduced the target. $$ Z = \frac{Y-\bar{Y}}{\bar{Y}} $$ $$ Z = \alpha X_1 + \beta X_2 + \gamma X_3 $$ Where $Y$ is your initial target, $\bar{Y}$ its average, $Z$ your normalized target and $X_i$ your predictors. When you apply your model and get a prediction, say $\hat{z}$, you can calculate its corresponding value $\hat{y}$ after applying the reverse transformations to your normalization. $$ \hat{z} = \alpha x_1 + \beta x_2 + \gamma x_3 $$ $$ \frac{\hat{y}-\bar{y}}{\bar{y}} = \alpha x_1 + \beta x_2 + \gamma x_3 $$ $$ \hat{y} = \bar{y} *(\alpha x_1 + \beta x_2 + \gamma x_3 ) + \bar{y} $$ Now in this particular case, as Ankit Seth pointed out, you need not normalizing your target. The linear model would have adjusted its coefficients automatically, because you have been using linear operands. However, if you proceed to non linear operations for target normalization, the same logic should apply. Say for instance that your model predicts the logarithm of your target. $$ log(Y) = \alpha X_1 + \beta X_2 + \gamma X_3 $$ The reverse operation would be to apply its reciprocal function, e.g. exponential. $$ Y = exp(\alpha X_1 + \beta X_2 + \gamma X_3) $$
