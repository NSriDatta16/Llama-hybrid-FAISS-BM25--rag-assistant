[site]: datascience
[post_id]: 126213
[parent_id]: 126187
[tags]: 
I don't understand if we should combine the causal mask with the padding mask from the encoder output or if we should just apply the padding mask (since the VALUES are coming from the encoder, and we should have full access over the whole encoder's input) For a translation scenario, here are what the masks should be: Encoder mask: don't attend to or irrelevant tokens. Typically, the encoder can have access to the full sequence. An edge case where you would mask out future tokens could be: You need to simulate a scenario where your input comes in a streaming fashion and make a prediction before knowing if the stream ends. Decoder mask: don't attend to tokens that "don't exist" yet. In practice, when you decode, you typically do it one token at a time, so you don't know yet about the future tokens. But that's only for cases where your decoder aims to generate a next token. Let's say you have a textual entailment task where you need to provide the relationship between input A and B. If you model your task as a next token generation (i.e. entailment/no entailment), you might want to give your decoder full context over the input as it would act as "another" encoder Cross-attention mask: Similarly to the previous two, it should mask input that the model "shouldn't have access to". So for a translation scenario, it would typically have access to the entire input and the output generated so far. So, it should be a combination of the causal and padding mask. Well-written question, by the way.
