[site]: datascience
[post_id]: 122888
[parent_id]: 
[tags]: 
Gradients of lower layers of NN when gradient of an upper layer is 0?

Say we have a neural network with an input layer, a hidden layer and an output layer. Say the gradients with respect to the weights and biases of the output layer are all 0. Then, by backpropagation and the chain rule, does this necessarily mean that the gradients with respect to the weights and biases of the hidden layer are 0 too?
