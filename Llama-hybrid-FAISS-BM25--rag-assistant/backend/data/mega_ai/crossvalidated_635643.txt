[site]: crossvalidated
[post_id]: 635643
[parent_id]: 635642
[tags]: 
For most problems it is easy to beat a tree model with a regression model. That's because tree models allow for all possible interactions among predictors and these are seldom needed. Interactions are notoriously hard to estimate, being double or triple differences etc., and much higher sample sizes are needed to estimate double differences (e.g., 4x greater sample sizes than needed to estimate simple differences = main effects, in an ideal case). Deficiencies in tree performance are often invisible to the user who does not assess absolute predictive accuracy of the tree or tree ensemble method. Once you get used to plotting unbiased estimates of smooth calibration trees you'll see what I mean. An enlightening paper is this which showed that tree methods require more than 10x higher sample sizes than regression, in order to be reliable. For single tree it's much higher than that. Trees make poor use of available data and single trees can't handle continuous predictors correctly. More here . In my experience random forests set the record for horrendous calibration accuracy (and overfitting).
