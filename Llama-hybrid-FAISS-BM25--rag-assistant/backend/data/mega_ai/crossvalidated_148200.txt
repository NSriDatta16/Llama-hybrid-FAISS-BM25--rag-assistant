[site]: crossvalidated
[post_id]: 148200
[parent_id]: 
[tags]: 
Batch Gradient Descent - Parallelization

Here is the scenario that I have been thinking for sometime: You have a large dataset over which you want to run a regression analysis, using the classic gradient descent method to minimize the error rate. The classic way suggests that for each iteration, you process the entire dataset and calculate the gradients. But, if this concept can be parallelized over a set of machines with each machine processing a randomly selected batch of data on its own, than this can basically meet the optimization level that is achieved by, lets say, stochastic gradient descent. Finally, once each machine has finished its own processing, we can collect all the weights calculated and basically average them.. Is my hypothesis theoretically correct?
