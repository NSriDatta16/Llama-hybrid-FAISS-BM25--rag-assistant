[site]: crossvalidated
[post_id]: 642174
[parent_id]: 428986
[tags]: 
It is correct that typical variational inference methods model the surrogate distribution $q_{\phi}(z \vert x)$ as a continuous latent space. A challenge of the following approach is dealing with posterior collapse. Provided with a trained powerful neural network decoder, i.e. universal function approximator, only a subset of the latent variables $z_{i} \in z$ will be exploited for decoding. This causes the variational distribution to replicate the prior, and results in difficulties for the decoding network to leverage the information contained within the latent space. In particular, Vector Quantised Variational AutoEncoder ( VQ-VAE ) was a method developed to deal with the challenges of posterior collapse by leveraging instead a discrete latent representation. In more detail, the posterior and prior distributions are categorical. The data points are selected from these distributions using an index latent embedding table $e \in \mathbb{R}^{K \times D}$ where $K$ is the size of the discrete latent space (K-way categorical) and $D$ the dimensionality of each latent embedding vector $e_{i} \in \mathbb{R}^{D}, i \in 1,2,\ldots,K$ . The posterior categorical distribution is defined as $q(z = k \vert x) = \begin{cases} 1 &\text{if } \text{for k} \text{=} \text{argmin}_{j} \Vert z_e(x) - e_j \Vert_2 \end{cases}$ $q(z = k \vert x) = \begin{cases} 0 &\text{} \text{otherwise} \end{cases}$ Additionally, a follow-up publication VQ-VAE2 to the original was published for large scale image generation, improving the coherence and fidelity of image generation. The following excerpts are taken from my book on variational inference. Learn more on the topic by visiting https://www.thevariationalbook.com/
