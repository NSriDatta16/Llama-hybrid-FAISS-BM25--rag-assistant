[site]: datascience
[post_id]: 24200
[parent_id]: 8213
[tags]: 
As a complement to Jérémie Clos' and AN6U5's answers, there are at least two methods helping to cope with a large number of classes: use a hierarchy, like hierarchical softmax . Instead of having a flat list of categories, one builds a tree of them, then on each node predicts if the correct category is on the left or on the right branch. do not classify directly, but first learn an embedding into a lower-dimensional space, where instances of the same class should have close representations. A famous example for this is FaceNet (the use case is face recognition) : they embed the image of a face into a 128 dimensional byte vector. The algorithm to learn this embedding is triplet loss (I've heard of magnet loss as well). Then when presented with a new request, compute its representation (a small vector), and look for the closest vectors in the trainset. This is similar to kNN, or to label embedding if each instance belongs to several classes: "Our method consists in embedding high-dimensional sparse labels onto a lower-dimensional dense sphere of unit-normed vectors, and treating the classification problem as a cosine proximity regression problem on this sphere." The work of http://manikvarma.org/index.html might be relevant to your problem as well. Alos of potential use: http://jmlr.org/papers/volume15/gupta14a/gupta14a.pdf .
