[site]: crossvalidated
[post_id]: 184171
[parent_id]: 183935
[tags]: 
As a general rule of thumb, it's better to use a single estimator that incorporates all the rather than the average of several estimators that uses subsets of the data (side note: bagging could be thought of as an alternative, but that's really a special case). Here's a common example of why you would want pool all your data rather than just using the average of stratified estimators, which I believe applies to your case (although not the only reason). Many estimators are biased but consistent; I'm fairly certain your case is an example of that. Suppose we can then write the following about the expected value: $\mathbb{E}[\hat \theta] = \theta g(n) $, with $|g(n) - 1| > |g(n+1)-1|$ and $\lim_{n \rightarrow \infty}g(n) = 1$ (Note: there's cases of consistent estimators that don't meet this criteria, but there are plenty more that do. And I do believe that your estimator fits this criteria). Suppose then we have $\hat \theta_1$ and $\hat \theta_2$, where these are estimators used from cutting our data in half and separately estimating $\theta$ from each data set. Meanwhile, $\hat \theta_.$ is the estimator created from using all the data. Then we get that $\mathbb{E}[ (\hat \theta_1 + \hat \theta_2)/2] = (\theta g(n/2) + \theta g(n/2) )/2 =\theta g(n/2)$ While on the other hand $\mathbb{E}[ (\hat \theta_.) ] = \theta g(n)$, Since $|1 - g(n)|$ this implies that $\hat \theta_.$ is less biased than $(\hat \theta_1 + \hat \theta_2)/2$ More generally, many estimators have nice asymptotic properties. By cutting up your full sample into smaller samples and averaging across, you may be reducing the asymptotic effect.
