[site]: stackoverflow
[post_id]: 3141559
[parent_id]: 
[tags]: 
Bleeding edge vs field tested technology. How will you strike a balance

I have been pondering about this for some time. How do you pick a technology ( am not talking about Java vs .Net vs PHP) when you are planning for a new project /maintaining an existing project in an organization. Arguments for picking the latest technology It might overcome some of the limitations of the existing technology ( Think No SQL vs RDBMS when it comes to scalability). Sometimes latest technology is backward compatible and only get to gain the new features without breaking the old functionality It will give better user experience (May be HTML 5 for videos, just a thought) Will cut down development time/cost and make maintenance of the code base relatively easy Arguments for picking field tested technology/against picking a bleeding edge technology It has not stood the test of time. There can be unforeseen problems. convoluted solutions might lead to more problems during maintenance phase and the application might become a white elephant Standards might not yet be in place. Standards might change and significant rework might be needed to make the project adhere to standards.Choosing the field tested technology will save these efforts The new technology might not be supported by the organization. Supporting a new (or for that matter a different technology) would require additional resources It might be hard to get qualified resources with bleeding edge technology From a developer perspective, I do not see a reason not to get hands dirty with some new technology (in your spare time) but he/she might be limited to open source/free ware/developer editions From am organization perspective, it looks like its a double edged sword. Sit too long in a "field tested" technology and good people might move away (not to mention that there will always be people who prefer familiar technology who refuse to update their knowledge). Try an unconventional approach and you risk overrunning the budged/time not to mention the unforeseen risks TL;DR Bottom line. When do you consider a technology mature enough so that it can be adopted by an organization ?
