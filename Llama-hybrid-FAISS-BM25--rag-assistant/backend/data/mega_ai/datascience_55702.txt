[site]: datascience
[post_id]: 55702
[parent_id]: 55699
[tags]: 
I would not start with (manual) feature selection. Use Lasso instead to "automatically" shrink/select features (this is Logit with shrinking of features basically). Logit (or Logit with Lasso as here) is for binary cases, but you can also do "Multinominal Logit" (option multi_class='multinomial' in sklearn), which is for more than two classes. Usually you use sklearn in Python for such things. Also see the examples in the sklearn docs. Make sure you have a test and trainings set. Also make sure that you do not use data from your test set for training. Train on the train set only and use the test set to see how your model performs on data NOT seen during training. It is not clear what you mean when you say "move to production". This depends on your problem. You simply need to make predictions here, but the implementation is of course contingent on the environment. It is okay to play around with data. However, if you really want to go for serious data science, you should have a look at the methods behind all this magic. I recommend " Introduction to Statistical Learning ". It is a really good book with many code examples and it is not very technical. Note that there is no silver bullet. Lasso or Logit may be okay, but other methods may be better. This really depends on the problem/data. Here is a little sample code for Lasso: # Split test/train from sklearn.model_selection import train_test_split xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.20, random_state=7) from sklearn.metrics import roc_auc_score from sklearn.metrics import confusion_matrix from sklearn.linear_model import Lasso from sklearn.linear_model import LassoCV # Perform lasso CV to get the best parameter alpha for regulation lasso = Lasso(max_iter=10000) lassocv = LassoCV(alphas=None, cv=10, max_iter=10000) lassocv.fit(xtrain, ytrain.values.ravel()) # Fit lasso using the best alpha lasso.set_params(alpha=lassocv.alpha_) lasso.fit(xtrain, ytrain) # Look at results (coefficients) la1 = pd.Series(abs(lasso.coef_), name="lasso") la2 = pd.Series(X.columns, name="names") dflasso = pd.concat([la2,la1], axis=1) dflasso = dflasso.sort_values(by=['lasso'], ascending=False) print(dflasso) # Look at AUC print("AUC Lasso: %.3f" %roc_auc_score(ytest.values, lasso.predict(xtest))) # Predict probs lasspreds0 = lasso.predict(xtest) # Classes lasspreds = np.round(lasspreds0) # Confusion matrix tnlog, fplog, fnlog, tplog = confusion_matrix(ytest, lasspreds).ravel() #y_true, y_pred print("True negative: %s, False positive: %s, False negative: %s, True positive %s" %(tnlog, fplog, fnlog, tplog)) print("Share false %.2f" %(((fplog+fnlog)/(fplog+fnlog+tplog+tnlog)))) # Look at probs print("Min. prob. of belonging to class 0: %.3f" %lasspreds0.min()) print("Max. prob. of belonging to class 0: %.3f" %lasspreds0.max()) EDIT: Please note that the sklearn Lasso as described above does not do a logistic regression, which means predictions can be smaller zero or larger one. To use Lasso with Logit (ensuring predictions are zero or one), one can use LogisticRegression : from sklearn.linear_model import LogisticRegression from sklearn.datasets import load_iris X, y = load_iris(return_X_y=True) log = LogisticRegression(penalty='l1', solver='liblinear') log.fit(X, y)
