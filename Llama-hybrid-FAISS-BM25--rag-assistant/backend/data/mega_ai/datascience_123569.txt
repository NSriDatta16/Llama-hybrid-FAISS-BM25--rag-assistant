[site]: datascience
[post_id]: 123569
[parent_id]: 
[tags]: 
LSTM output capped at a maximum

I am using a LSTM built using to forecast a single-value (solar irradiance) by using weather data as my input. When predicting my validation test, I get a weird results as it looks like all my predictions are capped at a maximum value: Here is some details about what I did which might help flag any mistake I am doing: Target : I am forecasting a 20-minute horizon at 1-minute resolution Data : Test data is for a whole year at a 1-minute resolution (target and features) Features : I created 20 lagged features for each original feature (lag is a minute long each time) Preprocessing : MinMaxScaler to bound values between 0 and 1 Reshape to fit LSTM Model : LSTM (Long-Short Term Memory), see details below: def lstm_model(): # Define layers model = Sequential() model.add(LSTM(units=30, kernel_regularizer=l2(0.01), # add some L2 to discourage learning complex patterns by penalising loss function return_sequences=False, # return single GHI value, not the sequence of values activation='tanh', recurrent_activation='sigmoid', use_bias=True, input_shape=(X_train.shape[1], X_train.shape[2]))) model.add(Dropout(rate=0.10)) model.add(Dense(1)) # Define optimiser optimiser = Adam(learning_rate=0.001) model.compile(optimizer=optimiser, loss='mean_squared_error', metrics=['mean_absolute_error', root_mean_squared_error]) return model Training : Early stopping with patience=3 Batch size 32 Trained on 2014 data, validated on 2015 data Predictions : preds in the plot are computed using y_pred = model.predict(X_test) for now (will compute 20-minute horizons once I solve this issue). Any ideas what could be causing this? I thought about potentially adding the target as a lag feature as well as it will have it in its history when making future predictions. Thanks for the help! Edit 1: Added the train data to the same graph.
