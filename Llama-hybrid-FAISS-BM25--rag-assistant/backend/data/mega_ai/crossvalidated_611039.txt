[site]: crossvalidated
[post_id]: 611039
[parent_id]: 
[tags]: 
Could someone help me interpret the data that I have gathered thus far?

I am trying to train a SVM model for my statistical learning course. The problem is a binary image classification problem (wildfire, nowildfire). This is the rigorous amount of testing that I have done so far: Initially I started by training the model using the library which was rather easy. Then I toyed around some more with the results and once I was sure that RBF for Kernal function and scale for gamma value would be best I moved on to my own manual implementation. Now the manual implementations are as follows: SMO was terrible in terms of efficiency and scalability whereas SGD also didn't seem scalable (although it performed well, for a larger dataset it was somewhat slow) so I landed on MBGD which is also the consensus of most of the Statistical Learning and Data Analytics literature. However, what I do not understand is why MBGD performed slightly worse with 2368 images (fourth last row, mini-batch size = 32) than it did with 480 images (last three rows, mini-batch sizes = 32, 16, 8), in terms of both best score and perhaps even test accuracy considering the difference in size. My other big problem so far has been efficiency for which I implemented an early stopping criteria based on the difference between two successive weights and biases. I also made the learning rate dynamically decrease with every successive Epoch/iteration so that I do not overfit the data whilst keeping the learning rate large enough to converge in a reasonable amount of time. So I guess my question is: can anyone help me understand how to improve my model and what mistakes I am currently making?
