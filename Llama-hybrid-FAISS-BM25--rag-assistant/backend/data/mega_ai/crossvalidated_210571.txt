[site]: crossvalidated
[post_id]: 210571
[parent_id]: 210565
[tags]: 
tl;dr: It's (sort of) a typo and essentially means "find the values that minimize this equation", not "take the min of the single number output by this equation." It probably would have been clearer if the authors wrote $\underset{b_u, b_i}{\arg \min}$. This paper is about building a recommendation system and Section 2.1 describes a baseline model that doesn't take any information about the item or user into account. The following variables were defined previously. $u$ is an index over users $i$ is an index over items $r_{u,i}$ is the rating that user $u$ gives the $i$th item $\mathcal{K} = (u, i \mid r_{u,i} \textrm{is known})$ (i.e., the k nowledge or training data) $\mu$ is the average rating over all items $b_i$ is the "baseline" or average rating for the $i$th item, across users, relative to $\mu$ $b_u$ is the "baseline" or average rating given by user $u$, relative to $\mu$ The section in question concerns estimating $b_u$ and $b_i$, which is set up as the least squares problem: $$ \underset{b*}{\min} = \sum_{(u,i) \in \mathcal{K}} \bigg(r_{u,i} - \mu - b_u - b_i\bigg)^2 + \lambda_1 \bigg(\sum_u b_{u}^2 + \sum_i b_{i}^2\bigg)$$ From context, it seems like we want to find the $b_u$ and $b_i$ values that minimize this equation. The first part of this equation is the actual least-squares part: note that if we got $\mu$, $b_u$ and $b_i$ exactly right, it would become zero; in other words, we would predict the user's rating perfectly. The second part is a regularization term that penalizes extreme values of $b_u$ and $b_i$.
