[site]: datascience
[post_id]: 66594
[parent_id]: 
[tags]: 
Activation function between LSTM layers

I'm aware the LSTM cell uses both sigmoid and tanh activation functions internally, however when creating a stacked LSTM architecture does it make sense to pass their outputs through an activation function (e.g. ReLU)? So do we prefer this: model = LSTM(100, activation="relu", return_sequences=True, input_shape(timesteps, n_features)) model = LSTM(50, activation="relu", return_sequences=True)(model) ... over this? model = LSTM(100, return_sequences=True, input_shape(timesteps, n_features)) model = LSTM(50, return_sequences=True)(model) ... From my empirical results when creating an LSTM-autoencoder I've found them to be quite similar.
