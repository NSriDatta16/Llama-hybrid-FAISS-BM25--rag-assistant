[site]: crossvalidated
[post_id]: 140205
[parent_id]: 77570
[tags]: 
One class svm are hard. It is not only the $\nu$ parameter, but also the parameters in the kernel, if you are using anything else than a linear kernel. So, the only way I know is if you have the true class for both your class of interest and the "other" class. Then you can ajust both $\gamma$ (in the RBF kernel) and $\nu$ using some metric of classifier success, either the accuracy or the just the accuracy of your class of interest (false positive rate or flase negative rate, specificity or sensitivity - it depends on whether you call your class of interest as positive or negative). In my limited experience in this, the one class SVM is almost always worse than using a standard classifier if you look at accuracy (I dont remember if I tested for false positive rate). So, if you have both the interesting and the uninteresting classes, you should try the standard classifier as proposed by @Hanan Shteingart. The problem is that if you dont have enough or representative examples of the uninteresting class. In this case you should go into the direction of open set recognition. I know at least this paper http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6365193 (sorry behind pay wall). This paper http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1512050 may also be relevant (given the title) but I have not read it.
