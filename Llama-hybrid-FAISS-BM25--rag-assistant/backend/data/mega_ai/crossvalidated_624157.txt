[site]: crossvalidated
[post_id]: 624157
[parent_id]: 
[tags]: 
Understanding a beta-variational autoencoder

I'm working on a beta-variational autoencoder using car images from the Vehicle Color Recognition Dataset . At this point, I'm just exploring different architectures and values for beta. (If you're interested, the notebook is here , but it is very much a work in progress, a bit sloppy at this point, and will certainly be changing as I explore beta-VAEs.) Here is an example of some generated images with an encoding layers of size 16, 3 convolutional layers, and a beta values of 1e-5, 1e-3, and 1e-1. And another set with encoding size 256, 3 convolutional layers, and the same values for beta. In both cases, starting at the third row, it looks like posterior mode collapse and the resulting model is not capturing any details. These images appear more like an overall average of all images. Does this make sense? The second row looks like more reasonable images, albeit quite blurry and not quite the quality I was hoping for. Any suggestions on how to improve these images? Row 1 is a complete mystery to me. While the images are certainly car-like, they are very cartoony or hallucinogenic. Any ideas what is going on here? I appreciate any comments and suggestions.
