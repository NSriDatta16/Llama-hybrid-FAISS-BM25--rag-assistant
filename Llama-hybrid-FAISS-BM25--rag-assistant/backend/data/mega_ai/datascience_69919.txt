[site]: datascience
[post_id]: 69919
[parent_id]: 69917
[tags]: 
In simple terms, the learning rate affects how big a step you update your parameters to move towards the minimum point in your loss function. If your step is too big, you might miss the minimum point on your loss function. If your step is too small, it might take too long to converge to the minimum point. Ways to deal with the above problems are the use of momentum which is a weighted average of the previous learning rate and the current learning rate or use a decaying learning rate.
