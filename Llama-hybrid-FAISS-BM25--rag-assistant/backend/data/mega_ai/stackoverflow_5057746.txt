[site]: stackoverflow
[post_id]: 5057746
[parent_id]: 5057649
[tags]: 
Yes. One could further precise that bots and crawlers are a secondary phenomenon where software attempts to mimic human behaviour. So that your high level distinction still stands. The huge difference is that web sites have a double role provide information present information Whereas for web services, there is no concept of presentation. You will find this same distinction in their respective expression languages: whereas HTML cumulates both information tagging and presentation directives, xml is only about information identification, organisation, transformation and organisation. Historically XML followed in the wake of HTML when people figured out that there were better ways to access information exposed by web sites than just ripping their not well formed html pages more or less aping humans; whilst at the same time everybody knew that neither CORBA nor RPC could fill the need for B2B communication because of their unability to be routed through a WAN. Hence then, SOAP, all the OASIS standards and only later REST services, still preferred for devices too 'light' to accommodate full fledged SOAP stacks.
