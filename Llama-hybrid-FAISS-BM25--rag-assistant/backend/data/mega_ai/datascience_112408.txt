[site]: datascience
[post_id]: 112408
[parent_id]: 112406
[tags]: 
The most obvious reason is that a gradient of the norm of 1 is expected to be at learning_rate * 1 away from the loss function minimum. That is averaged of course. Proper gradient near the minimum approaches to 0 due to cancellation of the first order term in Taylor expansion of Loss function around the minimum. So at least in theory, a step based on a Loss gradient rather than normalised gradient can converge towards the minimum.
