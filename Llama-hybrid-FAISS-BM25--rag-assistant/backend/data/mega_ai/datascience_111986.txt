[site]: datascience
[post_id]: 111986
[parent_id]: 111951
[tags]: 
In my experience the most valuable data that Anne could have in addition is identified by asking validation questions "do we model the right data?" (as opposed to verification as in calculating some metrics: "does the model predict the existing data right"): Are there factors/sub-domains of the data or model space/scenarios for application use that are not yet covered? E.g., say, Anne trains some medical classifier and her training data was recorded at hospital X. Now hospitals Y and Z offer to join the project - this would be very valuable since Anne can now look at inter-hospital variation. This recommendation boils down to putting together a list of potential influencing factors (including confounders) and trying to identify whether the new data offers additional coverage here. An entirely different scenarios that looks at new data points within the factors already covered but also related to the question is deciding for which (existing) data points to acquire labels. I have that when setting up regression models where many samples are measured (and all are to be predicted), but only a small number of the measured cases can be sent for (expensive, time-consuming) reference analysis. Kennard-Stone-algorithm or initially k-means can be used here for regression. This strategy is related to the Bayesian uncertainty in the other answer. For classification, there is the additional question, whether the application requires more typical cases or more (deliberately) cases that are close to the class boundary - they may be acquired in a dedicated fashion. (This is again related to my first thought)
