[site]: crossvalidated
[post_id]: 517946
[parent_id]: 
[tags]: 
Systematic error in time series simulations(?)

Statistical models of time series tend to underestimate points above average and overestimate points below average, due to regression to the mean. If points above and below average are treated separately, the error associated with the two subsets is systematic, not random. Also, the intensity of the error seems proportional to deviation from the mean itself. I would thus assume it would be possible to compensate for it; however I have not seen this done in any of the commonly used algorithms. Why? Many thanks
