[site]: crossvalidated
[post_id]: 598546
[parent_id]: 
[tags]: 
Fisher information matrix for logistic regression using the logit link

A question on a problem sheet I am working on asks us to find the Fisher information matrix (FIM) for the following setup: Suppose that $Y_i \sim \textrm{Bin}(r_i, \pi_i)$ for $i = 1, 2, \dots , n$ , all independent, where the $r_i$ are known, $\ln({\pi_i/(1 − \pi_i)}) = \beta_0 + \beta_1 x_i$ and $x_i$ is a known covariate. We would like to find the FIM. When finding the FIM for a model using the identity link, it is as simple as finding derivatives of the log-likelihood, here it seems more challenging. At first I wrote $\pi_i =\frac{\exp(\beta_0 + \beta_1 x_i)}{1+\exp(\beta_0 + \beta_1 x_i)}$ and attempted to differentiate the log-likelihood with respect to the parameters, but that seems to be incorrect. The solution sheet says that the following solves the problem: "We know that $\mu_i = \pi_i r_i$ and $\eta_i=\ln(\pi_i/(1 − \pi_i)) =\ln(\mu_i /(r_i - \mu_i))$ So that $$\frac{d \eta_i}{d \mu_i}= \frac{r_i}{\mu_i(r_i- \mu_i)} = \frac{1}{r_i \pi_i(1-\pi_i)} $$ Since $\operatorname{Var}(Y_i)=r_i \pi_i(1-\pi_i)$ the Fisher information matrix is given by: $$V= \begin{pmatrix}\sum_{i=1}^nr_i \pi_i(1-\pi_i) & \sum_{i=1}^nx_ir_i \pi_i(1-\pi_i)\\ \sum_{i=1}^nx_ir_i \pi_i(1-\pi_i) & \sum_{i=1}^nx_i^2r_i \pi_i(1-\pi_i) \end{pmatrix}.$$ I am very unsure how this solution was achieved; can anyone break this down a bit more?
