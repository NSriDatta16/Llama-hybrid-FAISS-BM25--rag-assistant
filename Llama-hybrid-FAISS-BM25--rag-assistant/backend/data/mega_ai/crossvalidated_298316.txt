[site]: crossvalidated
[post_id]: 298316
[parent_id]: 
[tags]: 
Combining a Keras classifier with an XGBoost classifier to achieve better F1 Score

I've been working on a particular binary classification problem for some time now, and have discovered the two best classifiers among many models to be a Keras Conv1D net and a XGBoost model. As it turns out, my Keras model tends to achieve much higher recall on the minority class, and the XGB model tends to achieve much higher precision on the minority class. What is the best way to utilize both classifiers' prediction capabilities, e.g. to sum the predictions or combine their predictive capabilities? Below are classification reports on the test set. XGBoost model: precision recall f1-score support 0.0 0.95 0.98 0.96 5365 1.0 0.74 0.58 0.65 657 Keras model: precision recall f1-score support 0.0 0.98 0.89 0.93 5365 1.0 0.49 0.81 0.61 657 This is an interesting guide, but many of the code examples are very old, and about much more intensive ensembling than simply two classifiers. Ideally, I'd like to take the predictions output by both, and do something to combine(?) them to make those predictions more accurate.
