[site]: datascience
[post_id]: 117848
[parent_id]: 85486
[tags]: 
In original Transformer the decoder cannot attend to previous token while encoder can attend to all tokens. Also, output of encoder is same number of tokens as input while output of decoder is just one token. For GPT/BERT, this is the main differentiator that leads to calling them decoder-only vs encoder-only models. There is no cross attention involved in either of them. BERT generates same number of tokens as input that can be fed to linear layer and uses masked language modeling so this is strictly encoder only model. GPT generates one token at a time just like decoder of transformer and has causal language modeling so it is strictly decoder only model. For completeness, there are indeed architectures with only decoder but using masked language modeling but they show less of zero shot perf. There are also encoder-decoder architectures like T5 which again doesnâ€™t do as good as decoder-only architectures like GPT3 unless you train on supervised datasets as well (example T0 generated from T5). Reference: https://arxiv.org/abs/2204.05832
