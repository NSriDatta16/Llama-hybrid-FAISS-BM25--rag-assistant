[site]: crossvalidated
[post_id]: 453404
[parent_id]: 453397
[tags]: 
Actually, if you are interested in classification, you do not need to get rid of collinear variables. These will influence your coefficients, but not your predictions. You can just leave them in. A random forest would be one of the most logical candidates for a "good", as in, a model that predicts well. Random forests are robust and require little tuning. They also work fine with collinearity, and will accept categorical features without any modifications. No need to think about interactions much either. All in all, probably the most obvious choice without additional information. Now clearly, you rule these out, but it is unclear why. If you are looking to fit some kind of more classical model with coefficients, a first choice would be to go for a logistic regression. Then, you would have to consider adding interactions or nonlinearities to your features if they play a large role. This would mean that A is recognizable from a linear combination of the first and the second feature, and C and B are separable with a linear combination of the first and the fifth feature. Which exact linear combination that is depends on where the clusters are. You could create these linear combinations and base your model only on these two combinations. This would simplify the model, and if the separation is indeed very strong on these axes, might not hurt the performance of the resulting models very much. It is very unlikely to improve the performance of your models, but it's possibly in theory. Again, if you are using a random forest or a logistic regression or any other classification algorithm, those algorithms will learn this information without any help.
