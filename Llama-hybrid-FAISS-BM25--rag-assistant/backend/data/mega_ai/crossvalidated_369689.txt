[site]: crossvalidated
[post_id]: 369689
[parent_id]: 
[tags]: 
Clarification about RNN/LSTM Sequence Models with Word Vector inputs

Say that we are trying to train a language model with an RNN/LSTM i.e. the inputs are words in a sentence and the outputs are the same words shifted by one such that for each input word the output is the next word. Also let's say that instead of using one-hot encoded inputs we are using word vectors. My question is that even if the inputs are word vectors, are the output labels still one-hot encoded vectors? If so, has anyone ever tried making the output labels word vectors also?
