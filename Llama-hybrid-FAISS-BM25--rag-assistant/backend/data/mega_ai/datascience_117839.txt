[site]: datascience
[post_id]: 117839
[parent_id]: 
[tags]: 
Why does softmax perform well on MNIST but poorly on EMNIST letters?

I am learning about softmax regression using Dive into Deep Learning . I have a very basic question on why softmax performs well on one dataset and poorly on another. I tried modifying the results from ยง4.5 concise implementation of softmax regression on the MNIST dataset and got decent performance. (Fig. 1) However, the model did very poorly for the EMNIST letters dataset. (Fig. 2) Here is the data and model that I used: # the model class softmaxRegression(d2l.Classifier): def __init__(self,num_outputs,lr): super().__init__() self.save_hyperparameters() self.softmax = nn.Sequential(nn.Flatten(),nn.LazyLinear(num_outputs)) def forward(self,X): return self.softmax(X) def loss(self,Y_hat,Y): return functional.cross_entropy(Y_hat,Y) # data for EMNIST letters class EMNIST(d2l.DataModule): def __init__(self,batch_size=64,resize=(28, 28)): super().__init__() self.save_hyperparameters() trans = transforms.Compose([transforms.Resize(resize), transforms.ToTensor(), lambda t: torch.transpose(t,1,2)]) self.train = torchvision.datasets.EMNIST( root=self.root, split = "letters", train = True, transform = trans, download=True) self.val = torchvision.datasets.EMNIST( root=self.root, split = "letters", train = False, transform = trans, download=True) def get_dataloader(self, train): data = self.train if train else self.val return torch.utils.data.DataLoader(data,self.batch_size) Also # data for MNIST class MNIST(d2l.DataModule): def __init__(self,batch_size=64,resize=(28, 28)): super().__init__() self.save_hyperparameters() trans = transforms.Compose([transforms.Resize(resize), transforms.ToTensor()]) self.train = torchvision.datasets.MNIST( root=self.root, train = True, transform = trans, download=True) self.val = torchvision.datasets.MNIST( root=self.root, train = False, transform = trans, download=True) def get_dataloader(self, train): data = self.train if train else self.val return torch.utils.data.DataLoader(data,self.batch_size) My Question. Why did this happen? The two datasets look similar enough that the models ought to perform well in both of them. Fig 1: Softmax for MNIST Fig 2: Softmax for EMNIST Letters
