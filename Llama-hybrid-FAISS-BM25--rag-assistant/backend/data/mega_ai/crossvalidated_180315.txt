[site]: crossvalidated
[post_id]: 180315
[parent_id]: 180303
[tags]: 
So, Tot_Paid_Amt is zero-heavy and/or concentrated around small values less than ~500. Otherwise, it is sparsely populated with a few values ranging out to ~500k, correct? To me, Tot_Paid_Amt sounds like and is behaving like information from insurance claims data where there are lots of small claims and a few huge ones. In instances like these, the mean would be a very biased and unreliable measure of location. You would be much better off using a more robust measure of location such as the median. In terms of distributional assumptions and assignments -- given that the histogram appears to be heavy tailed or extreme valued -- estimating a tail index for your data would be a more rigorous method for classification than just "guessing" Weibull. In this PDF, Cosima Shalizi presents some rigorous approaches to evaluating a tail index ... http://www.stat.cmu.edu/~cshalizi/2010-10-18-Meetup.pdf as well as here ... https://www.cs.purdue.edu/homes/agebreme/Networks/papers/clauset-powerLaw-siamRev05.pdf However, there are less rigorous, "quick and dirty" proxies for doing this based on OLS regression that Shalizi disapproves of but that may be "good enough" for your purposes, whatever they are ... http://www.eco.uc3m.es/temp/jbes.2009.06157.pdf Having obtained a tail exponent, there is a nice discussion of what they mean as well as assignment rules in the Examples section on this Wiki page ... https://en.wikipedia.org/wiki/Tweedie_distribution How would this be helpful in an applied context such as yours? As noted, any effort at predicting average claims amounts, stoploss thresholds -- whatever -- rooted in classic, textbook approaches such as OLS regression will be hugely biased and unreliable. While standard actuarial workarounds would prescribe deleting outliers (based on a data-driven cutoff) to obtain a more "representative" distribution enabling mean-based estimators, this has the serious drawback of needlessly throwing away useful information -- as the tail exponent indicates. Just one of the challenges with using this approach is that, having thrown away the outliers, how do you add them back in to predicted values based on that trimmed distribution to obtain the "right" predicted totals (or summed claims amounts)? In my opinion, robust predictive modeling methods such as quantile regression should provide a "good" (or less biased), tractable solution to these challenges. The default is typically to estimate the median quantile but a grid search across a range or set of possible quantiles would pin down a better value in terms of a bias-variance tradeoff. Given the positive skew, you can safely assume quantiles in a range less than the median will be most likely to minimize some loss function. If none of these suggestions work for you, please elaborate on how you would like to use Tot_Paid_Amt.
