 is expanded to the smallest APR covering any instance x 2 {\displaystyle x_{2}} in a new positive bag B 2 {\displaystyle B_{2}} . This process is repeated until the APR covers at least one instance from each positive bag. Then, each instance x i {\displaystyle x_{i}} contained in the APR is given a "relevance", corresponding to how many negative points it excludes from the APR if removed. The algorithm then selects candidate representative instances in order of decreasing relevance, until no instance contained in a negative bag is also contained in the APR. The algorithm repeats these growth and representative selection steps until convergence, where APR size at each iteration is taken to be only along candidate representatives. After the first phase, the APR is thought to tightly contain only the representative attributes. The second phase expands this tight APR as follows: a Gaussian distribution is centered at each attribute and a looser APR is drawn such that positive instances will fall outside the tight APR with fixed probability. Though iterated discrimination techniques work well with the standard assumption, they do not generalize well to other MI assumptions. Diverse Density In its simplest form, Diverse Density (DD) assumes a single representative instance t ∗ {\displaystyle t^{*}} as the concept. This representative instance must be "dense" in that it is much closer to instances from positive bags than from negative bags, as well as "diverse" in that it is close to at least one instance from each positive bag. Let B + = { B i + } 1 m {\displaystyle {\mathcal {B}}^{+}=\{B_{i}^{+}\}_{1}^{m}} be the set of positively labeled bags and let B − = { B i − } 1 n {\displaystyle {\mathcal {B}}^{-}=\{B_{i}^{-}\}_{1}^{n}} be the set of negatively labeled bags, then the best candidate for the representative instance is given by t ^ = arg ⁡ max t D D ( t ) {\displaystyle {\hat {t}}=\arg \max _{t}DD(t)} , where the diverse density D D ( t ) = P r ( t | B + , B − ) = arg ⁡ max t ∏ i = 1 m P r ( t | B i + ) ∏ i = 1 n P r ( t | B i − ) {\displaystyle DD(t)=Pr\left(t|{\mathcal {B}}^{+},{\mathcal {B}}^{-}\right)=\arg \max _{t}\prod _{i=1}^{m}Pr\left(t|B_{i}^{+}\right)\prod _{i=1}^{n}Pr\left(t|B_{i}^{-}\right)} under the assumption that bags are independently distributed given the concept t ∗ {\displaystyle t^{*}} . Letting B i j {\displaystyle B_{ij}} denote the jth instance of bag i, the noisy-or model gives: P r ( t | B i + ) = 1 − ∏ j ( 1 − P r ( t | B i j + ) ) {\displaystyle Pr(t|B_{i}^{+})=1-\prod _{j}\left(1-Pr\left(t|B_{ij}^{+}\right)\right)} P r ( t | B i − ) = ∏ j ( 1 − P r ( t | B i j − ) ) {\displaystyle Pr(t|B_{i}^{-})=\prod _{j}\left(1-Pr\left(t|B_{ij}^{-}\right)\right)} P ( t | B i j ) {\displaystyle P(t|B_{ij})} is taken to be the scaled distance P ( t | B i j ) ∝ exp ⁡ ( − ∑ k s k 2 ( x k − ( B i j ) k ) 2 ) {\displaystyle P(t|B_{ij})\propto \exp \left(-\sum _{k}s_{k}^{2}\left(x_{k}-(B_{ij})_{k}\right)^{2}\right)} where s = ( s k ) {\displaystyle s=(s_{k})} is the scaling vector. This way, if every positive bag has an instance close to t {\displaystyle t} , then P r ( t | B i + ) {\displaystyle Pr(t|B_{i}^{+})} will be high for each i {\displaystyle i} , but if any negative bag B i − {\displaystyle B_{i}^{-}} has an instance close to t {\displaystyle t} , P r ( t | B i − ) {\displaystyle Pr(t|B_{i}^{-})} will be low. Hence, D D ( t ) {\displaystyle DD(t)} is high only if every positive bag has an instance close to t {\displaystyle t} and no negative bags have an instance close to t {\displaystyle t} . The candidate concept t ^ {\displaystyle {\hat {t}}} can be obtained through gradient methods. Classification of new bags can then be done by evaluating proximity to t ^ {\displaystyle {\hat {t}}} . Though Diverse Density was originally proposed by Maron et al. in 1998, more recent MIL algorithms use the DD framework, such as EM-DD in 2001 and DD-SVM in 2004, and MILES in 2006 A number of single-instance algorithms have