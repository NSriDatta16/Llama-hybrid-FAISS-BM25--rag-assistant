[site]: datascience
[post_id]: 32146
[parent_id]: 32143
[tags]: 
These are my thoughts: This seems to me a more reasonable approach than just making up values, but I have no theoretical justification for this. Actually, there is not much theoretical justification for classical imputation. I think your method makes sense when just very few of the predictors have missing values. For instance, if there is just one predictor that has missing values, you can build a model to imput those missing values. However, as you said, things scale very badly. Moreover, apart from computational issues, if there are lots of missing values in every predictor your data is not very good, and therefore any predictive model won't be very good anyway. This is done sometimes, but I don't think there's any library in R that implements it, you'll have to code it yourself (I don't think it is difficult). If you do it and your final model is a logistic regression, I don't recommend to fit the missing values with another linear model, as you will suffer from a collinearity problem. One other thing that is typically done is that, for every predictor with missing values, create a binary predictor that is $0$ if the other predictor's value is missing and $1$ otherwise.
