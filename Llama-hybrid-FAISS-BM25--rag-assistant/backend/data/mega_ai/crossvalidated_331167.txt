[site]: crossvalidated
[post_id]: 331167
[parent_id]: 
[tags]: 
Iteratively Compute Mean and Variance on Dataset of Images

Not sure where is it better to ask this question because it is kind of between implementation details and conceptual question. Probably it should be moved somewhere else. I need to normalize images before feeding them into deep learning model. I've decided to compute mean and std for the whole dataset and then subtract and divide these values for each of samples during training process. But I can't read all the images into memory and use something like np.mean to compute "average image". Therefore, I've decided to write a function which will iterate though folder with images and compute required stats iteratively instead. I am using the following implementation: from pathlib import Path from keras.preprocessing.image import load_img, img_to_array def compute_mean_and_std(target_size, *folders): global_mean = None global_variance = np.ones(target_size) for folder in folders: for n, filename in enumerate(Path(folder).glob('*.jpg'), 1): x = img_to_array(load_img(filename, target_size)) if global_mean is None: global_mean = x else: global_variance = ( (n - 2)*global_variance/(n - 1) + (1./n)*(x - global_mean)**2) global_mean = (1./n)*(x + (n - 1)*global_mean) return global_mean, np.sqrt(global_variance) But my question is not about implementation per se, but more about the "correctness" of this idea. First of all, do you think it makes sense to compute "global" stats of the dataset using method provided in the code above? Second, do you think it makes sense (in general) to compute these "featurewise" stats for a specific target size on the whole dataset (excluding test data), or better to normalize each sample separately? I know there is such thing in Keras library like featurewise_mean_normalization which takes all samples from dataset and computes the "global" mean. But I am not sure if that implementation gives same results as the one provided above.
