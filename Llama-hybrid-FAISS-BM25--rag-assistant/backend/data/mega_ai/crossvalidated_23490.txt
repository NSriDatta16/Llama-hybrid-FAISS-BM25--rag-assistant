[site]: crossvalidated
[post_id]: 23490
[parent_id]: 
[tags]: 
Why do naive Bayesian classifiers perform so well?

Naive Bayes classifiers are a popular choice for classification problems. There are many reasons for this, including: "Zeitgeist" - widespread awareness after the success of spam filters about ten years ago Easy to write The classifier model is fast to build The model can be modified with new training data without having to rebuild the model However, they are 'naive' - i.e. they assume the features are independent - this contrasts with other classifiers such as Maximum Entropy classifiers (which are slow to compute). The independence assumption cannot usually be assumed, and in many (most?) cases, including the spam filter example, it is simply wrong. So why does the Naive Bayes Classifier still perform very well in such applications, even when the features are not independent of each other?
