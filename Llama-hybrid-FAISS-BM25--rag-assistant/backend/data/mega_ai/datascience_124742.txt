[site]: datascience
[post_id]: 124742
[parent_id]: 
[tags]: 
Fine-tuning Hugging Face’s Llama Model with Unlabelled Data from PDFs from niche domain

I’m unsure about the next steps. Specifically, I have the following questions: How can I prepare my unlabelled data for the fine-tuning process? What’s the best way to fine-tune the Llama model with my specific dataset? Are there any specific considerations or best practices I should be aware of when fine-tuning a model with unlabelled data? I have a large amount of raw text data scraped from thousands of PDFs in a specific domain. I want to use this unlabelled data to fine-tune the Llama model from Hugging Face’s Transformers library. Here’s what I’ve done so far: I’ve successfully scraped the text from the PDFs and have it stored in a suitable format.
