[site]: crossvalidated
[post_id]: 449195
[parent_id]: 
[tags]: 
Universal approximation theorem on limited precision arithmetic

The universal approximation theorem of a single-layer neural network holds for all activation functions which are bounded and nonconstant, so it's clear that a limited-precision function can be used. However, does the theorem still hold when the multiplication and summation operations are not on reals, but with limited precision arithmetic? That is, when the whole thing runs on a physical computer? This has practical considerations, like whether we can expect a physical implementation of a neural net to hold the same universality as its theoretical counterpart.
