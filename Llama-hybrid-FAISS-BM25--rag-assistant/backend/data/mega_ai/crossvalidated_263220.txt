[site]: crossvalidated
[post_id]: 263220
[parent_id]: 263066
[tags]: 
It almost sounds like you're (re)inventing DropOut or DropConnect. These are usually applied to units embedded in a deeper network, but the idea is pretty much the same. Some units' outputs (or input connections) are randomly set to zero during each training presentation. The weights are then adjusted at test time to account for the fact that fewer units/connections were present during testing. This process does something akin to model averaging or a very extreme form of bagging, and is thought to prevent "co-adaptation" between units within a layer. If you had highly correlated input features, I suppose applying it to a single perceptron could work as well. This paper by Baldi and Sawdoski has some pointers into the DropOut literature and, as a special case, looks at applying it to a single linear unit in section 2.1. This approach is a little different from regularization. Regularization adds a penalty term which is some function of the model's weights. For example, $\ell_2$ regularization uses the sum of the squared weights, which encourages the model to reduce very large weights more. $\ell_1$ uses their summed absolute values, which applies an equal penalty to all weights (i.e., moving a weight from 101 to 100 reduces the penalty as much as moving from 1 to 0). In practice, this tends to drive small weights to/towards zero. However, if you have two features that are highly correlated, it tends to select one and drive the other towards zero. If you want a sparse model, this may be a good thing. However, if you want an "OR"-like operation in your model, so that learns to response to either A or B, despite having both A and B present in most of the training data, DropOut might be a better fit.
