[site]: crossvalidated
[post_id]: 385370
[parent_id]: 
[tags]: 
Estimated Marginal Likelihood in Variational Autoencoder

In Auto-Encoding Variational Bayes Appendix D, the author proposed an accurate marginal likelihood estimator when the dimensionality of latent space is low ( $$p_{\mathbf{\theta}}(\mathbf{x}^{(i)}) \simeq \bigg( \frac{1}{L}\sum^{L}_{l=1} \frac{q(\mathbf{z}^{(l)})}{p_{\mathbf{\theta}}(\mathbf{z}) p_{\mathbf{\theta}}(\mathbf{x}^{(i)}|\mathbf{z}^{(l)})}\bigg)^{-1}$$ where $$\mathbf{z} \sim p_{\mathbf{\theta}}(\mathbf{z}|\mathbf{x}^{(i)})$$ However, when the latent dimensionality is low , why can't we directly estimate the likelihood as: $$ p_{\theta}(\mathbf{x}^{(i)}) \simeq \frac{1}{L}\sum_{l=1}^L p_{\theta}(\mathbf{x}^{(i)}|\mathbf{z}^{(l)}), \mathbf{z} \sim p(\mathbf{z}) $$ ? Why do we want to sample the latent variables from posterior probability $p_{\theta}(\mathbf{z}|\mathbf{x})$ instead of the prior probability $p_{\theta}(\mathbf{z})$ ?
