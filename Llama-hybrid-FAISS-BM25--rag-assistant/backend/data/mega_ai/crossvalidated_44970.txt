[site]: crossvalidated
[post_id]: 44970
[parent_id]: 44942
[tags]: 
K-means minimizes the total sum of squared deviations. Don't think of it in terms of distances . Yes, you assign objects by their distance. But that distance (squaredEuclidean) is just the sum of 1d squared deviations. So what you really are optimizing is the sum of squared 1d deviations. The key part however is the mean, and here it is more obvious that this method optimizes based on single dimensions. The arithmetic mean is the minimum least squares estimation, again. So you really should plot the sum of squared deviations , not the mean of euclidean distances. Yes: euclidean distances and squared euclidean distances are monotone - for a single object. Example in one dimension: Objects at 0 and 2, mean at 0.5: average distance: (0.5 + 1.5) / 2 = 1 squared deviations: 0.25 + 2.25 = 2.5 Object at 0 and 2, mean at 1: average distance: (1 + 1) / 2 = 1 squared deviations: 1 + 1 = 2 So the second example is only better wrt. squared deviations, not with linear deviations. In particular, an optimum for the linear deviations isn't necessarily optimal for the squared deviations. Above example highlights that one shouldn't just plug in arbitrary distances into k-means. It technically is not distance based. It is based on squared deviations in each single dimension, which happen to sum up to the squared euclidean distance, which is monotone with the regular euclidean distance, so it appears as if we are assigning every object to the nearest cluster center, although what we are actually doing is minimize the sum of squared 1d deviations.
