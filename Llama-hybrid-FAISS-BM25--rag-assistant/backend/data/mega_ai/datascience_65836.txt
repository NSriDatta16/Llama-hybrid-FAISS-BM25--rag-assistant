[site]: datascience
[post_id]: 65836
[parent_id]: 
[tags]: 
How curvature information in second order optimization methods helps

It is said that second order optimization methods in neural networks work better than first order because they contain information about rate of change of gradient or the curvature. This information helps to choose a better step size for moving forward on the error surface. It is not clear as to how the rate of change of gradient is controlling the step size and leading to better optimization. For simplicity, consider only one weight update iteration.
