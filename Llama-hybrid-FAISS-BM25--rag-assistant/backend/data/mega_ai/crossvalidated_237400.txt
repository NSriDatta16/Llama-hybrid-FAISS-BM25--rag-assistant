[site]: crossvalidated
[post_id]: 237400
[parent_id]: 
[tags]: 
The reason that the larger gradient flowing through an ReLU neuron can cause it to die

In this link about different neuron types, there is an introduction on the disadvantage of ReLU, (-) Unfortunately, ReLU units can be fragile during training and can "die". For example, a large gradient flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again. If this happens, then the gradient flowing through the unit will forever be zero from that point on. That is, the ReLU units can irreversibly die during training since they can get knocked off the data manifold. For example, you may find that as much as 40% of your network can be "dead" (i.e. neurons that never activate across the entire training dataset) if the learning rate is set too high. With a proper setting of the learning rate this is less frequently an issue. I am not clear why large gradient flowing through a ReLU neuron could cause the neuron to die. Please see the following ReLU picture for reference.
