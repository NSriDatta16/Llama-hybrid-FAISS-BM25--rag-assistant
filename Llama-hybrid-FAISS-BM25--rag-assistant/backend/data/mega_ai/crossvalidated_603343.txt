[site]: crossvalidated
[post_id]: 603343
[parent_id]: 603340
[tags]: 
This is really talking about conditional independence, with conditioning on the hypothesis. So if the null hypothesis is correct, then the probability the test is significant should be $\alpha$ . If you then collect new data and test the new data, the probability the second test is significant should also be $\alpha$ , and the conditional independence means the probability of both tests being significant is $\alpha^2$ . Similarly, if the alternative point hypothesis is correct, and the test has power $1-\beta$ , i.e. that is the probability of a significant result, then the probability of two significant results is $(1-\beta)^2$ for the same reason. But if you do not know which hypothesis is correct then you do not get this independence. If in a quasi-Bayesian approach you think there is a probability $p$ that the null hypothesis is correct and a $1-p$ probability that the alternative point hypothesis is correct, then the probability the first test is significant is $p\alpha + (1-p)(1-\beta)$ , and that both are significant is $p\alpha^2 + (1-p)(1-\beta)^2$ which is not the square of the previous expression when $0 . This is the concern you express in your question. You also need to consider what you would do if the first test was significant and the second not, which happens with probability $\alpha(1-\alpha)$ or $\beta(1-\beta)$ depending on which hypothesis is correct.
