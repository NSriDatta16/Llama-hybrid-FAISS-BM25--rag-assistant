[site]: crossvalidated
[post_id]: 359085
[parent_id]: 315272
[tags]: 
Let's call word2vec vector model $W$ & glove $G$. Now, an embedding is just a vector and $W$ is a vector space. These two embeddings are in different vector spaces. You need to either align the 2 vector spaces like in this paper by Mikolov . The idea is that even though vector spaces are almost isomorphic, they are mostly at an angle and you need to multiply one space, say $G$ with a rotation matrix to align it with $W$. You could do this by initializing a random matrix and performing gradient descent to minimize reconstruction error. or combine them by doing dimensionality reduction like in this paper by Conceptnet by doing something like PCA as answered by @Jakub. From the paper Our goal is to learn a projection from k dimensions to k â€² dimensions that removes the redundancy that comes from concatenating multiple sources of embeddings. Suggestions Since your primary problem is handling out-of-vocabulary(OOV) words, try using FastText pretrained embeddings for handling OOV which keeps embeddings for subwords (char n-grams) too; one can use those to build up a word. ($v_{dies} = v_{die} + v_{ies}$) Just use Conceptnet Numberbatch embeddings which itself combine glove & word2vec like you are trying to do
