[site]: crossvalidated
[post_id]: 172681
[parent_id]: 
[tags]: 
Model averaging for negative gradient boosting?

I'm using negative gradient boosting for Cox regression. After finding out the optimum lambda value by evaluating the empirical risk of 10-fold cross validation, I want to determine the final model for prediction. However, there seems to be two ways to do that: First, I can average all the coefficients at the optimum lambda from all ten cross-validations. Second, I can boost one more time with all the data using the optimum lambda. Since these two methods all give me good results, which one should I use? Or which one is more accurate theoretically?
