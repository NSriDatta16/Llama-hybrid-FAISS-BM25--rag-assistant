[site]: crossvalidated
[post_id]: 385869
[parent_id]: 385862
[tags]: 
The probability of any sample, $\mathbb{P}_\theta(X=x)$ , is equal to zero and yet one sample is realised by drawing from a probability distribution. Probability is therefore the wrong tool for evaluating a sample and the likelihood it occurs. The statistical likelihood, as defined by Fisher (1912), is based on the limiting argument of the probability of observing the sample $x$ within an interval of length $\delta$ when $\delta$ goes to zero (quoting from Aldrich, 1997) : $\qquad\qquad\qquad$ when renormalising this probability by $\delta$ . The term of likelihood function is only introduced in Fisher (1921) and of maximum likelihood in Fisher (1922). Although he went under the denomination of "most probable value", and used a principle of inverse probability (Bayesian inference) with a flat prior, Carl Friedrich Gau√ü had already derived in 1809 a maximum likelihood estimator for the variance parameter of a Normal distribution. Hald (1999) mentions several other occurrences of maximum likelihood estimators before Fisher's 1912 paper, which set the general principle. A later justification of the maximum likelihood approach is that, since the renormalised log-likelihood of a sample $(x_1,\ldots,x_n)$ $$\frac{1}{n} \sum_{i=1}^n \log f_\theta(x_i)$$ converges to [Law of Large Numbers] $$\mathbb{E}[\log f_\theta(X)]=\int \log f_\theta(x)\,f_0(x)\,\text{d}x$$ (where $f_0$ denotes the true density of the iid sample), maximising the likelihood [as a function of $\theta$ ] is asymptotically equivalent to minimising [in $\theta$ ] the Kullback-Leibler divergence $$\int \log \dfrac{f_0(x)}{f_\theta(x)}\, f_0(x)\,\text{d}x=\underbrace{\int \log f_0(x)\,f_0(x)\,\text{d}x}_{\text{constant}\\\text{in }\theta}-\int \log f_\theta(x)\,f_0(x)\,\text{d}x$$ between the true distribution of the iid sample and the family of distributions represented by the $f_\theta$ 's.
