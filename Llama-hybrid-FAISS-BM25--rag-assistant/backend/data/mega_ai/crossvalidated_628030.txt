[site]: crossvalidated
[post_id]: 628030
[parent_id]: 
[tags]: 
What to do after Cross-Validation?

My question is a follow-up of posts such as this . To summarise, I am given say 1000 data points. I want to fit say Random Forest and optimise among $n$ choices of hyperparameters using k-fold cross-validation. I have trained $n \times k$ models and can select the best choice among the $n$ hyperparameters sets (having $k$ surrogate models per set of hyperparameters). So far so good. Now that I have selected near-optimal hyperparameters, I can re-train the model on my whole dataset. There is no validation set though, I have to wait for a real data point coming from the wild and be confident my model is the best possible. The question is: how do I know my model is the "best possible"? Could it not be that I have overfitted when training on the whole dataset, and would not have been better to retain one (assuming there was a way to select) of the $k$ surrogate models using the best set of hyperparameters? Two questions then: Is there a rigorous justification for the procedure sketched above? Is one needed at all, and the argument "more data -> better prediction when facing a new datapoint" is obvious? Thanks
