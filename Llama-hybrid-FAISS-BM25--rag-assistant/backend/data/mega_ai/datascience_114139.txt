[site]: datascience
[post_id]: 114139
[parent_id]: 114071
[tags]: 
It is best practice to split the data into train and test datasets. Make modeling choices only on the train data set. Evaluate the usefulness of those choices on the test dataset. Traditional NLP extraction techniques follow the same logic because they often have modeling choices. One example is the number of topics in non-negative Matrix Factorization (NMF). It is best practice to choose the number of topics on the training dataset, and then evaluate the quality of those topics on the test dataset. The same logic holds true when estimating a statistic and then making modeling choices on that statistic. Tfâ€“idf (term frequency-inverse document frequency) is a common example. It is best practice to estimate tf-idf on the training set only because later modeling choices are made (or not made) based on tf-idf statistics.
