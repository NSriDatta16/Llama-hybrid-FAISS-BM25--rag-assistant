[site]: crossvalidated
[post_id]: 272411
[parent_id]: 
[tags]: 
Why is empirical risk minimization prone to overfitting?

According to Chapter 8 of the book Deep Learning, "..empirical risk minimization is prone to overfitting. models with high capacity can simply memorize the training se." My question why is it so? Models with high capacity can also memorise the training set when we have the true distribution and reduce the true cost function.
