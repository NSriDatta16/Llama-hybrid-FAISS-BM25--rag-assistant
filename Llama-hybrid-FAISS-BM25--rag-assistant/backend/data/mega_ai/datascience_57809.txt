[site]: datascience
[post_id]: 57809
[parent_id]: 
[tags]: 
Batch Normalization and Dropout together causing incorrect segmentation results

So, I've been running a test to see how well a number of networks can perform road segmentation on a particular customer's dataset. I am testing UNET, RDRCNN, and Tiramisu against each other. UNET reliably trains, however RDRCNN and Tiramisu cannot. Immediately during training, the validation loss and accuracy plateau. After looking at the results, it looks as if the network is guessing a single pixel value; leading to inferences that are just a square of all one color. I have tried turning the learning rate down, using a simple optimizer like SGD, and making sure I am loading the data with the same exact loader. I have also tried using a dice/jaccard loss to account for class imbalance. None of these have worked. Has anyone ever experienced this before? Any help would be greatly appreciated. Thank you!
