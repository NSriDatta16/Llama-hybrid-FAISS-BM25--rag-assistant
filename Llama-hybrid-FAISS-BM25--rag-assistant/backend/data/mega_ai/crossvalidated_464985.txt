[site]: crossvalidated
[post_id]: 464985
[parent_id]: 
[tags]: 
Do I need a validation set?

Do I need a validation set if I am using cross validation and grid search for parameter tuning? Similar to this question - what I understood is that it helps to prevent overfitting but it's not necessary. I've read dozens of other posts on stack exchange, but now I'm starting to see conflicting answers and I am very confused... In train-validation-test, the validation set is used to tweak parameters. But in my training set, I am using 10 fold cross validation with grid search (simultaneously) for finding the parameters of my random forest model. I'm not going to change the parameters, because I trust GridSearch already found the best ones. I have performance metrics (sen, spe, PPV, NPV) and they seem good enough so I have no reason to suspect the model is bad. So what's the point of using a validation set other than "proper" science? Note: My dataset is 600 samples, and split 70-30% (train-test for now). I will apply my model to the test set just once report my final performance metrics. I don't want a validation set since my dataset is already super tiny.
