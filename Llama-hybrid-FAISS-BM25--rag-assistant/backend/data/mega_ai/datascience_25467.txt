[site]: datascience
[post_id]: 25467
[parent_id]: 
[tags]: 
Adding feature leads to worse results

I have a dataset with 20 variables and ~50K observations, I created several new features using those 20 variables. I compare the results of a GBM model (using python xgboost and light GBM) and I found that it doesn't matter what are the hyper-parameters of the model the 'thinner' version leads to better results (AUC) even that all 20 original variables are included in the wider version. when I compare the same using Lasso model - the wider version is better (~1% higher) as expected. I guess it can be related to to the randomness in the GBM but I was surprised to see that GBM doesn't fix it along the way. Any explanation of the phenomena will be appreciated.
