[site]: crossvalidated
[post_id]: 318576
[parent_id]: 318550
[tags]: 
In the first case (maximum likelihood estimation), you take into account the uncertainty around the estimated parameter value in the inference you make. In the second case (model selection), if you then naively use the selected model, as if it were the model you intended all along, you are ignoring the uncertainty around the model. A rough equivalent to naive forward model selection would be, if in the first case would be if you picked the parameter value identified by maximum likelihood for some parameter and then decided that you had always known that this is the exact true value of that parameter (i.e. no uncertainty around it). E.g. you want to estimate the mean $\mu$ of a normal distribution and there is also the variance parameter $\sigma^2$. So, you estimate these parameters, then declare that you have always known that $\sigma^2=\hat{\sigma}^2$ and then you re-estimate $\hat{\mu}$ for a fixed known $\sigma^2=\hat{\sigma}^2$. An approximate equivalent to maximum likelihood estimation within a single model when it comes to multiple models is for example AIC model averaging across the candidate models (or at least those that come close to minimizing the AIC). More on this topic can be found e.g. in the book by Burnham and Anderson ("Model Selection and Inference: A Practical Information-Theoretic Approach").
