[site]: crossvalidated
[post_id]: 541116
[parent_id]: 541112
[tags]: 
A big one is generalisability. Good prediction requires that the conditional distribution of $Y$ given the features $X$ is the same in future use as it was in the training set. If the training set were a simple random sample from the population of future use this would be guaranteed by sampling, without any causal assumptions, but that's relatively unusual. Causation provides another way to warrant an expectation that the conditional distribution won't change too much. Note that this is weaker than assuming $X$ causes $Y$ -- we want to assume that $Y$ is associated with $X$ for stable causal reasons, so that the distribution of $Y$ given $X$ is going to stay (roughly) the same. Two contrasting examples: US car insurance companies use the association between credit score and insurance claim risk to target insurance marketing (at least they did, ten years ago). This makes sense; while high credit score does not cause low insurance risk, it's reasonable to believe they are associated for stable causal reasons. That is, if you drew a causal DAG and worked out the implied conditional independence graph, I expect it would imply an association between credit score and insurance risk. There's a dataset in the UCI machine learning archive of North American mushrooms from the genera Lepiota and Agaricus , with visual features and edibility. You can get very good prediction not just in the training set but in an independent test set. However, there's no stable causal relationship between the features and edibility. Once you go outside North American Lepiotus and Agaricus the model will happily classify as edible Amanita phalloides and Amanita virosa , the 'death cap' and 'destroying angel' mushroooms.
