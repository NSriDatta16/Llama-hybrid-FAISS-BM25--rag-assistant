[site]: crossvalidated
[post_id]: 206971
[parent_id]: 206944
[tags]: 
In practice, a reinforcement learning algorithm is considered to converge when the learning curve gets flat and no longer increases. However, other elements should be taken into account since it depends on your use case and your setup. In theory, Q-Learning has been proven to converge towards the optimal solution. However, in this section of (Sutton and Barto, 1998), since the exploration parameter $\varepsilon$ parameter is not gradually increased, Q-Learning converges in a premature fashion (before reaching the optimal policy). To my experience, it is not always obvious to make the $\varepsilon$ and the learning rate $\alpha$ decrease in a way that ensures convergence and most of the time, there is some tuning involved here (when moving these parameters, your Q-Learning curve will stabilize in different levels). Finally, don't forget that Q-Learning has been propose in 1989 by Watkins, which is a little bit outdated. It is well suited when you learn about reinforcement learning but not that much when implementing real learning agents. I would recommend exploring more state-of-the-art techniques.
