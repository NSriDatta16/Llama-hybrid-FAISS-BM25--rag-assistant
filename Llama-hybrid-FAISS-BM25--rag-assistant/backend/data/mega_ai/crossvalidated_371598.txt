[site]: crossvalidated
[post_id]: 371598
[parent_id]: 371564
[tags]: 
It might help you to study a specific example. The authors denote an entire layer of activations via $\phi(x)$ , for any input image $I$ . So imagine we have a layer with three neurons, whose activations we denote via: $(\phi_1(x),\phi_2(x),\phi_3(x))$ . The claim that some earlier papers made is that certain neurons respond to certain features. As an example, you could figure out which images maximally activate the first neuron, by computing (really: sampling images) the maximum value attained by that neuron over a set of test images: $\mbox{argmax}_{I\in\mathcal{I}}\phi_1(x)$ . Then you could find images which achieve similar activation strength to the maximum, and make claims about similarities between those images (e.g. diagonal strokes). To be clear, the above claim is empirically correct: certain neurons are more sensitive to particular features. Before proceeding, note that $\phi_1(x) = \langle \phi(x),e_1\rangle$ , e.g. the right side is just a dot product between the full vector of activations and the first unit vector, which pops out the first neuron's activation. The linked paper argues however, that it is incorrect to conclude that neural networks disentangle images into features across individual neurons. Because, if certain neurons respond more toward a given feature, one might think that the network tries to be efficient and not do the same encoding work twice across multiple neurons. Note that part of the reason for this is using dropout, which would make it more difficult for single neurons to focus on a specific feature. To prove this, the authors instead consider optimizing $ \mbox{argmax}_{I\in\mathcal{I}} \langle \phi(x),v\rangle=\mbox{argmax}_{I\in\mathcal{I}}(\phi_1(x)v_1+\phi_2(x)v_2+\phi_3(x)v_3)$ , where $v$ is a random vector. So now instead of focusing on a particular neuron, we're focusing on an arbitrary linear combination. They show that images that give high activations for this are also semantically related. This implies that there doesn't seem to be a preferred basis for disentangling semantics .
