[site]: datascience
[post_id]: 14516
[parent_id]: 13660
[tags]: 
The IDF part of TF-IDF gives less weight to a word if it occurs in a large fraction of the documents in your corpus. However, this doesn't necessarily mean that the word is unimportant for distinguishing your two classes. A word which is common in your corpus, but which also occurs substantially more often in one class than the other, could very well be quite valuable in distinguishing the classes. This can especially be true if your set is not balanced between the two classes. For example: Suppose that 95% of negative reviews contained the term "boring", while only 5% of the positive reviews contained "boring". If this occurred in a balanced set of negative and positive reviews, that means that half the total reviews would contain "boring", and this might be one of the terms whose weight is more strongly suppressed by IDF, despite the fact that it's obviously a valuable term for distinguishing the classes. If instead 90% of the reviews are negative (with "boring" again occurring in 95% of negative reviews and 5% of positive ones), then a full 86% of your documents contained the term, and its weight might be highly suppressed by IDF despite its obvious importance. In essence, the problem is that TF-IDF is agnostic to the class labels in your training set. One alternative to TF-IDF for text classification that doesn't have this flaw is Bi-Normal Separation (BNS), see for example this paper: Forman, G. BNS Feature Scaling: An Improved Representation over TF-IDF for SVM Text Classification. Hewlett-Packard Labs Tech Report HPL-2007-32R1, 2007. (link to PDF)
