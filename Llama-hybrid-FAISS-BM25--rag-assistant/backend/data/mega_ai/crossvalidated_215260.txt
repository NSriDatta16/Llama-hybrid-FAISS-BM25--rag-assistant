[site]: crossvalidated
[post_id]: 215260
[parent_id]: 
[tags]: 
Scikit-learn's SGDClassifier code question

I have a question regarding the code of function SGDClassifier , from library scikit-learn , which implements linear classification using the stochastic gradient descent (SGD) algorithm. It concerns its application for the Support Vector Machine (SVM) model - i.e. SVM is parametrized by a hinge loss function and a L2 penalty function. The SVM decision function is: $$ d(x) = sign\left(f(x)\right) = sign\left(w^Tx\right) $$ The optimal solution $w^*$ is found by solving the following regularized problem ($[x]_+ \equiv \max[0,x]$): $$ \min_w \left(\sum_{i=1}^{n}{\left[1-y_i\,w^Tx_i\right]_+}+\lambda\|w\|^2\right) $$ From [Bottou, 2012] , page 3, I recall the SGD algorithm's main update equation for the SVM: $$ w \leftarrow w \ - \ \eta_t \left(\lambda w - y_tx_t\mathbf{1}_{\{y_tw^Tx_t However, I am having trouble finding this same equation in SGDClassifier . The code of SGDClassifier is based on the function _plain_sgd() written in Cython: for an optimal learning rate ( 2 ), a hinge loss $-$ see definition of dloss in the code from line 108 on; dloss for a hinge loss function (SVM) is in line 160 $-$ and a L2 penalty ( 2 ), and ignoring class and sample weights, I have the impression the value for the update of $w$ is computed in line 639 and is equal to -eta * dloss : $$ w \leftarrow w \ - \ \eta_ty_t\mathbf{1}_{\{y_tw^Tx_t There is a last computation in line 656: w.add(x_data_ptr, x_ind_ptr, xnnz, update) But I do not understand what this does. Is there anything I have missed on _plain_sgd code ? Has anybody a clue on where these differences come from? Especially given that scikit-learn claims that they follow Bottou's approach: The implementation of SGD is influenced by the Stochastic Gradient SVM of LÃ©on Bottou.
