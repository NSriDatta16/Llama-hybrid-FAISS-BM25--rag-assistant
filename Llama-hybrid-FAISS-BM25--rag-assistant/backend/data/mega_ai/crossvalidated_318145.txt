[site]: crossvalidated
[post_id]: 318145
[parent_id]: 318138
[tags]: 
Neural networks are universal function approximators. To approximate any function, you need to be able to model it in a non-linear way. If you consider Neural Network with just input-output layer, then it is just linear approximation (i.e. visually equal to drawing a straight line to divide the example-categories). Hidden layers allow introducing non-linearities to function. E.g. think about Taylor series. You need to keep adding polynomials to approximate the function. You can draw an analogy (although weak) between adding the polynomials and adding the hidden layers in the neural network. The role of each hidden layer cannot be easily known beforehand. Having too many hidden layers will make the Neural network to overfit the function ("high variance"). Having not enough hidden layers will make the Neural network to underfit the function ("high bias").
