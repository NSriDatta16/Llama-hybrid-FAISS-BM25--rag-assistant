[site]: crossvalidated
[post_id]: 250010
[parent_id]: 
[tags]: 
Intuition of KernelPCA

I'm dealing currently with kernels and kernel PCA. For this purpose I've been reading a few papers on these topics. In this context I've been reading the paper "Kernel Principal Component Analysis" by Sch√∂lkopf et. al. While reading the paper certain questions emerged: Performing the kernel PCA, we get a kernel matrix K. From the 'standard' PCA we get Eigenvalues and Eigenvectors. How do we get the Eigenvectors and Eigenvalues from the kernel matrix? The dual eigenvalue problem is: $m \cdot \lambda \cdot \alpha = K \cdot \alpha$ where m is the number of data points, alpha are some coefficients and K is our kernel matrix. How are the coefficients alpha determined? What do they stand for? Did I understand correctly that the PCA itself is performed implicitly in the feature space? having the Eigenvalues and Eigenvectors: What is the intuition of them in feature space? What do they mean if we use e.g. a polynomial kernel of degree 2 and obtain the top k-eigenvalues? What is the geometric/visual intuition of the obtained Eigenvalues in feature space? How can we reconstruct from the determined Eigenvalues and Eigenvectors the original dataset? In how far can the Eigenvalues and Eigenvectors be compared to those in the 'standard'/linear PCA?
