[site]: crossvalidated
[post_id]: 260232
[parent_id]: 
[tags]: 
Multicollinearity and perfect separation in logistic regression: what should I do?

I have a dataset composed of 61 variables a qualitative one y =(0 or 1) and 60 other quantitative variables and 40000 observations. I want to do logistic regression, Lda, svm, rpart of the model y~. When I use the vif function of package car it shows multicollinearity. The following is a part of the output: > vif(glm(y~., data=mydat,family=binomial)) >> n1 direc1 n2 direc2 n3 >> 2.25 1.87 3.28 2.35 3.27 >> direc3 P1 P2 P3 P4 >> 2.49 158.87 190.24 143.28 13.74 >> P5 P6 P7 P8 P9 >> 119.93 212.23 616.43 146.59 169.48 Should I keep only n1 ,direc1, n2 ,direc2 , n3 ,direc3 in the model? Or there is another solution to the problem ? In fact, I found this in some internet pages: "perfect separation is related to collinearity." I want to address both problems. I read that collinearity between variables gives wrong coefficient estimates in a logistic regression model for example. And perfect separation gives wrong coefficients estimates also. Really I am searching the best model prediction for a month but I did not find the good one till now because of these problems! To solve multicolinearity problem I did a PCA, but I did not know how to use the results obtained in regression. In fact, the image below did not show a separation between the two classes (0 and 1). How can I interpret it? which command in R allows me to get the new variables to use in regressions? thanks a lot in advance for any help
