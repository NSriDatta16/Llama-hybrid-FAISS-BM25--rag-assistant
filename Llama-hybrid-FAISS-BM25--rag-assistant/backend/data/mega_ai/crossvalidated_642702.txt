[site]: crossvalidated
[post_id]: 642702
[parent_id]: 
[tags]: 
Performing classification by Gaussian classifier versus logistic regression. When to choose the more indirect regression method?

Say we measure a variable dependent on a dichotomic class $X|_\text{class}$ where the distributions conditional on that class are $$X|_\text{class =1} \sim N(\mu_1,\sigma^2)$$ $$X|_\text{class =2} \sim N(\mu_2,\sigma^2)$$ With this situation we can apply a logistic model, as explained in: How to prove that LDA has a similar form to Logistic Regression for the binary classification example? Deduce the logistic regression formula The question here is about which method would create the lowest mean squared error for the classification boundary We could fit the normal distributions based on the two populations and use that to compute a classification boundary. We could perform a logistic regression and use that to compute a classification boundary. These methods do not give the same result as demonstrated with the code below (fitting normal distributions is better, with lower deviations from the ideal boundary, than fitting a logistic regression model). Question: Apparently, logistic regression is not always the best performer. Are there systems/methods to decide on the use of logistic regression instead of straightforward fitting of normal distributions? set.seed(1) sample = function() { n = 30 x = rnorm(n,0) y = rnorm(n,1) xy = c(x,y) z = rep(c(0,1), each = n) mod = glm(z ~xy, family = binomial()) ### boundary based on average between group means boundary1 = mean(c(x,y)) ### boundary from logistic model boundary2 = -mod $coefficients[1]/mod$ coefficients[2] return(c(boundary1,boundary2)) } b = replicate(10^3, sample()) plot(b[1,],b[2,]) mean((b[1,]-0.5)^2) ### error from straightforward classification = 0.0166966 mean((b[2,]-0.5)^2) ### error from logistic regression = 0.01713042
