[site]: datascience
[post_id]: 117591
[parent_id]: 117583
[tags]: 
In the Transformer architecture, the positional embedding is added to the data embedding vectors so, if you had just the embedded data and kept the ordering between data vectors, you could just pass it through the normal transformer:
