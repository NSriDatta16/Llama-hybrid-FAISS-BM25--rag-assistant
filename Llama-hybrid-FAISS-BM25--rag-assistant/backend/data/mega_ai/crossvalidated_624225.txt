[site]: crossvalidated
[post_id]: 624225
[parent_id]: 624214
[tags]: 
The typical way to do this is with the variance inflation factor . The coefficient standard errors depend on four factors. Sample size Residual variance Variance of the feature Variance inflation factor The interpretation of the variance inflation factor is that it inflates the standard error of a coefficient (so the square root of the variance of the sampling distribution) beyond where it would be for independent features. The other three factors will always be present, but if there is no multicollinearity, then the variance inflation factor will be $1$ : no variance inflation. Variance inflation factor for feature $j$ is calculated by running a linear regression that uses every other feature to predict that feature $j$ of interest. Then calculate the $R^2$ of this regression, sometimes denoted as $R^2_j$ . The variance inflation factor is then $\frac{1}{1-R^2_j}$ . A higher degree of feature predictiveness of this feature of interest means a higher $R^2_j$ and a higher variance inflation factor. By calculating pairwise correlations between features, you are getting a lower bound on the variance inflation factor, since additional features cannot lower the $R^2_j$ . That is, if you have correlation $r$ between two features, you know the variance inflation factor for each feature to be at least as high as $\frac{1}{1-r^2}$ . Some people give variance inflation factors of $5$ or $10$ as the thresholds to regard multicollinearity as problematic. This might serve as a decent rule of thumb, but it does hide a lot of nuance about why multicollinearity might be a problem. For instance, if the variance is inflated by a factor of $10$ , but the sample size is so huge that standard errors are still going to be minuscule, this seemingly high variance inflation factor might not be such a big problem. Further, if what ultimately want to do is predict the outcome accurately instead of draw inferences about particular regression parameters, the inflation of standard errors is much less important. Validated predictive performance is what would matter most. Note that this applies to OLS linear regression only. If you move to other models and estimation techniques (e.g., logistic regression and other GLMs, ridge regression), the techniques will differ. Generalized linear models do have generalized variance inflation factors, and the usual variance inflation factor lacks the literal interpretation that it has for OLS linear regression.
