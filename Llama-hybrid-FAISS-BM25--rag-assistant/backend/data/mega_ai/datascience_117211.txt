[site]: datascience
[post_id]: 117211
[parent_id]: 117189
[tags]: 
Underfitting and high bias are closely related concepts in machine learning. Underfitting refers to a model that is not complex enough to capture the underlying trend in the data. It is not able to capture the patterns in the data well and as a result, it performs poorly on both the training and the test sets. On the other hand, high bias refers to the tendency of a model to consistently make the same types of errors, regardless of the input data. A model with high bias pays little attention to the training data and oversimplifies the model, leading to poor performance on the training and test sets. So, underfitting is often caused by a model with high bias, which means that it is oversimplifying the problem and is not able to capture the complexity of the data. Overfitting, on the other hand, refers to a model that is too complex and fits the noise in the training data, rather than the underlying trend. It performs well on the training set, but poorly on the test set. High variance is often a cause of overfitting, as it refers to the sensitivity of the model to small fluctuations in the training data. A model with high variance pays too much attention to the training data and ends up learning the noise in the data, rather than the underlying trend. Therefore, overfitting is often caused by a model with high variance, which means that it is too sensitive to the noise in the training data and is not able to generalize well to unseen data. In short, underfitting is usually caused by high bias, which leads to oversimplification of the model and poor performance on both the training and the test sets. On the other hand, overfitting is usually caused by high variance, which leads to a model that is too sensitive to the noise in the training data and performs poorly on the test set.
