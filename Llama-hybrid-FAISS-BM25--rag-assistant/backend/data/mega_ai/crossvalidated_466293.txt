[site]: crossvalidated
[post_id]: 466293
[parent_id]: 383310
[tags]: 
I will expand upon @Bloc97 's answer about the difference between $L1$ and $L2$ constraints, in order to show why $L1$ may drive some weights to zero. In the case of $L2$ regularization, the gradient of a single weight is given by $$ \delta w = u - 2pw$$ where $u$ is the input from the previous layer being multiplied by weight $w$ , and $p$ is parameter weighting the $L2$ penalty. Without loss of generalization, assume that $u>0$ and $w>0$ . Then the sign of $\delta w$ is given by $$ sign(\delta w) = sign(\frac{u}{2p} -w)$$ showing that $L2$ regularization will drive $w$ to grow bigger if $w$ drops below $\frac{u}{2p}$ . On the other hand, in the case of $L1$ regularization, the gradient of a single weight is given by $$ \delta w = u - p$$ so the sign of $\delta w$ is given by $$ sign(\delta w) = sign(u-p)$$ showing that $L1$ regularization will drive $w$ to grow smaller when the input $u$ is smaller than the $L1$ regularization parameter $p$ . Effectively, $p$ is functioning as a threshold such that, whenever $u$ is less than $p$ , $L1$ regularization will push the weight to grow smaller, and whenever $u$ is greater than $p$ , $L1$ regularization will push the weight to grow larger. The above is a local linear approximation of a nonlinear system: $u$ is actually an average over, for example, all the samples in a batch, and $u$ also changes with with each update. Nevertheless, it gives an intuitive understanding of how $L1$ regularization tries to drive some weights to zero (given large enough $p$ ).
