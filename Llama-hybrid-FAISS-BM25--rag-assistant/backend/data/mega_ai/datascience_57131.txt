[site]: datascience
[post_id]: 57131
[parent_id]: 31773
[tags]: 
There are some papers studying uncertainty in deep learning models using dropout. For instance take a look at Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning and Uncertainty quantification using Bayesian neural networks in classification: Application to biomedical image segmentation As far as I understood, enabling dropouts while predicting allows running a kind of Monte Carlo Simulations, hence you can obtain the mean of these simulations. For a classification task, these simulations are followed by estimating the epistemic and aleatoric of a model by making use of the discrete nature of the output 2 . However, it is not very clear to me how this works in case of regression. But one idea that comes to my mind is that we can estimate the confidence interval using the mean and standard deviation of the predicted values in the Monte Carlo runs using \begin{equation} \text{confidence interval} = \mu \pm t \times \frac{\sigma}{\sqrt{T}} \end{equation} where $\mu$ and $\sigma$ are the mean and standard deviation obtained from the Monte Carlo runs, $t$ is derived from t-distribution table and using the degrees of freedom and $T$ is the number of Monte Carlo simulations. I am not sure if this is a good measure of uncertainty and I would like to get some feedback on this as I am working on a similar issue.
