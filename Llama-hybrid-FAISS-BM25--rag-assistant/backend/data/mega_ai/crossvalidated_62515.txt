[site]: crossvalidated
[post_id]: 62515
[parent_id]: 62492
[tags]: 
Both models will give the exact same results. Why? The Likelihood principle . RJags is an R package that uses the software JAGS to conduct Bayesian inference, and any fully Bayesian procedure, one where inference proceeds from the posterior distribution, will satisfy the Likelihood principle. Essentially, the Likelihood principle states that if two likelihood functions are proportional to each other, then the same inference about the parameters should be obtained from the two likelihood functions. In your example we are inferring the probability of a coin landing heads up, $p$, from $n$ independent tosses, $X_1,...,X_n$, of that coin. Prior to tossing the coin, you assume that any value of $p$ in the interval $[0,1]$ is equally likely. Thus the prior distribution for the parameter $p$ is $\pi (p)=1$. Suppose we observe $k$ coin tosses where the coin lands heads up, where $0\leq k\leq n$. In the case of the model using the binomial distribution, the likelihood function is $$ l(p|X_1,...,X_n)= {n \choose k}p^k (1-p)^{n-k} $$ For the Bernoulli model, the likelihood function is $$ l_\star (p|X_1,...,X_n)=p^k(1-p)^{n-k} $$ We have observed the data, so both $n$ and $k$ are fixed values and therefore ${n \choose k}$ is just a constant, and $l(p|X_1,...,X_n) \propto l_\star(p|X_1,...,X_n)$, bearing in mind $l$ and $l_\star$ are functions of $p$. Once we have our samples from the posterior distribution from RJags, we will make the same conclusion, aside from any error due to having a finite sample from a Markov Chain that has hopefully converged. Also, if you are familiar with sufficient statistics, you could note that $k=\sum_{i=1}^n{X_i}$ is a sufficient statistic for $p$ in both models (assuming $n$ fixed).
