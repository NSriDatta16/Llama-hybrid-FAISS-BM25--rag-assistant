[site]: crossvalidated
[post_id]: 263158
[parent_id]: 
[tags]: 
Deriving Constraints in the dual form of SVM

In a regular soft-margin SVM we want to find $\min_{w,b, \xi} \frac{1}{2}||w||^2+C\sum_{i=1}^{\ell}\xi_i$ subject to $y_i((w,x_i),+b) \geq 1 - \xi_i$ and $\xi_i \geq 0$ Which we convert to the Lagrangian $L(w, b, \alpha, \beta) = \frac{1}{2}||w||^2+C\sum_{i=1}^{\ell}\xi_i - \sum_{i=1}^{\ell}\alpha_i[y_i((w,x_i)+b)-1+\xi_i] - \sum_{i=1}^{\ell}\beta_i \xi_i$ To find the minimum with respect to $w,b,\xi$ we find where the gradient is the 0 vector, giving $$ \begin{aligned} \frac{\partial L}{\partial w} & = w - \sum_{i=1}^{\ell}\alpha_i y_i x_i = 0 \\ & \equiv w - \sum_{i=1}^{\ell}\alpha_i y_i x_i = 0 \\ & \equiv w = \sum_{i=1}^{\ell}\alpha_i y_i x_i \\ \end{aligned}$$ $$\begin{aligned} \frac{\partial L}{\partial b} & = \sum_{i=1}^{\ell} -\alpha_i y_i = 0 \\ & \equiv \sum_{i=1}^{\ell} \alpha_i y_i = 0 \\ \end{aligned}$$ $$ \begin{aligned} \frac{\partial L}{\partial \xi} & = \sum_{i=1}^{\ell}C-\sum_{i=1}^{\ell}\alpha_i-\sum_{i=1}^{\ell}\beta_i = 0\\ & \equiv \sum_{i=1}^{\ell}C = \sum_{i=1}^{\ell}\alpha_i+\beta_i \\ \end{aligned}$$ My question is this.... In every text I look at, (such as pg.8 of these teaching notes or implied in pg.20 of these course notes ) the last equation ( $\frac{\partial L}{\partial \xi}$ ) results in $C=\alpha_i + \beta_i$ without the summation. Can anybody explain to me how this leap is made?
