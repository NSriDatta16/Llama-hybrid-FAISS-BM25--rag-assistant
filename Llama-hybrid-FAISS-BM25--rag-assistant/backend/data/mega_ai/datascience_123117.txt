[site]: datascience
[post_id]: 123117
[parent_id]: 
[tags]: 
What information is lost due to normalization?

Normalization is usually seen as a good thing for data preprocessing before training a model. But I am wondering if there are some information in the data that might be lost during this process. The analogy is convolutional neural networks vs feedforward neural network. Using a regular feedforward neural network for image classification requires collapsing an image into a single column vector. This causes a loss of spatial information in images. I am wondering if normalization used in data preprocessing (e.g., StandardScaler) also causes some information loss and should not be used in certain situations.
