[site]: crossvalidated
[post_id]: 35209
[parent_id]: 
[tags]: 
Why bother with low-rank approximations?

If you have a matrix with $n$ rows and $m$ columns, you can use SVD or other methods to calculate a low-rank approximation of the given matrix. However, the low-rank approximation will still have $n$ rows and $m$ columns. How can low-rank-approximations be useful for machine learning and natural language processing, given that you are left with the same number of features?
