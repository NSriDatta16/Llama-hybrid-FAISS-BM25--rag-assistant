[site]: stackoverflow
[post_id]: 1793713
[parent_id]: 1793575
[tags]: 
Yes, is reasonable. For a system that finely tunned and runing optimally can deliver 3.4 mil rows in about 12 minutes, this is exactly the expected result... None the less, some places to look for to improve performance: Does the table fit in the buffer pool? Ie. do you have enough RAM to store your entire database? If no, then you're going to hit the disk for IO. Page life expectancy counter is a good indicator. How fast is your disk I/O subsystem? Are we talking about a 5000 RPM second hand IDE drive or a RamSAN-500? What is the throughput reported by sqliosim ? How about perf counters, Avg. Disk Queue Length, Avg. Disk Sec/Transfer on the physical disks? Is it different for Reads vs. Writes? How fragmented is the table? A scan performance is affected first and foremost by read-ahead efficiency and read-ahead size is determined by hobt fragment size. Perhaps you need to optimize the ETL of the table, follow the FastTrack methodology. any contention going on? Have you measured lock wait times? Perhaps snasphot isolation can aleviate the problem. Can the client receive the 3.4 mil rows in time? Does the server block on client buffers availability? Again, wait stats can indicate this. Another good place to start is to follow the Wait and Queues methodology.
