[site]: crossvalidated
[post_id]: 55483
[parent_id]: 55457
[tags]: 
Depends on what you consider to be the "Occam's razor"; the original formulation is an unclear theological mumbo-jumbo, so it flourished into a bunch of (often incompatible) interpretations. Vapnik criticizes the ultranaive version saying more less that a model with lower number of fitted parameters is better because too much parameters imply overfitting, i.e. something in the melody of the Runge's paradox . It is of course false in machine learning because the "greedyness of fitting" there is not constrained by the number parameters but (via some heuristic) by the model accuracy on the future data. But does it mean that ML training is introducing plurality without necessity? I would personally say no, mainly due to the second part -- ML models are usually better than hand-razored classical regressions, so this extra complexity pays off. Even if it can be reduced by a human to a simpler theory, this almost always come for a price of extra assumptions, so it is not a fair comparison.
