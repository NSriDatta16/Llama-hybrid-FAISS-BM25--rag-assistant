[site]: datascience
[post_id]: 112461
[parent_id]: 51325
[tags]: 
The problem is with the optimizer. You used Adam optimizer which is mainly for large neural networks. What you need is simple stochastic gradient descent SGD. import torch from torch import nn from torch.optim import SGD, Adam from torch.autograd import Variable import numpy as np X,y = np.loadtxt("stackexchange.csv", delimiter=",", unpack=True) class Linear_Reg(nn.Module): def __init__(self): super(Linear_Reg, self).__init__() self.linear = nn.Linear(1,1) def forward(self, x): y_pred = self.linear(x) return y_pred net = Linear_Reg() Xt = Variable(torch.Tensor(X)) yt = Variable(torch.Tensor(y)) Xt = Xt.view(-1,1) criterion = nn.MSELoss() optimizer = SGD(net.parameters(), lr=0.001) EPOCHS = 100 for epoch in range(EPOCHS): optimizer.zero_grad() pred_y = net(Xt) loss = criterion(pred_y, yt) loss.backward() optimizer.step() print('Eopch: {}, \t\t loss: {}'.format(epoch, loss.data.item())) xxt = torch.arange(5,23) with torch.no_grad(): a = net(xxt.reshape(-1,1).float()) plt.scatter(X, y, s=30, c='r', marker='x', linewidths=1) plt.plot(xxt.data.numpy(),a.data.numpy(), label='Linear regression (Gradient descent)')
