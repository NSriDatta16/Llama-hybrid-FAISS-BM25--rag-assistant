[site]: datascience
[post_id]: 14197
[parent_id]: 14184
[tags]: 
Before deciding on the model, I would recommend to re-formulate the dataset to best suit your problem. You could approach this problem as follows: Since the output you're trying to predict is validity of the observation, keep "validity" = True/False, or, 1/0 as the target variable. One of the parameters is a categorical variable "species", and I'm expecting this to have a high cardinality. Since there are approximately 8.7 million species on earth, if you used this variable in a model it could possibly expand into 8.7 million individual columns (in on-hot encoding form). Even a conservative estimate of 100,000 species makes it nonviable to be used as it is. So you need a way to convert this species information to a fewer features. One approach you could try out is to create geographical clusters for each species (using only valid marked records), then identify the nearest center and max/avg./quartile measures of distance from their cluster center for each species. Do this for each quarter of the year separately to account for seasonal changes. Next, add this information back to the main dataset to indicate for each record - all the geographical centers of that species cluster. In the next step, for each record find the nearest cluster center and calculate this particular observation's distance from its cluster center. Then calculate the ratio of its distance from cluster center vs. max distance and vs. avg distance from that cluster's center. Use this metric instead of the geospatial coordinates and species identifier. Another approach could be to add additional features such as the climate of each location and average historic temperature at that location during the time of the year when the observation was taken. This is because some animals may migrate north/south based on the seasons and so if a species' location was found valid in the summer, it may be impossible to find it in the same location in winter due to it being unable to survive the cold weather. If you combine this with #3 above, it would enrich the observations significantly. After doing this extensive hard work, you should do some exploratory analysis and plot subsets of this data to better understand it. By visualizing the data, sometimes we're able to figure out best course of action more quickly than without visualizing the data. Next, you may explore different machine-learning algorithms to fit a model to this refined data. I would recommend trying out other algorithms such as logistic regression, SVM, ridge-regression, random forests and gradient boosting machines in addition to neural networks and then select the best performing one. Most machine learning suites/frameworks implement these, so it should not be difficult to find out how to apply these to your dataset. Neural networks are fine to try out, but as with all algorithms you need to be careful about the usual pitfalls such as: Avoid over-fitting the model to training data: to avoid this use regularization and keep cross checking accuracy with an independent held-out validation set. Use cross-validation (10-fold) and repeat several times to get good estimates of the model's performance metrics on new data. Since the data is highly class imbalanced (many valid records but few invalid records by proportion), use a performance metric other than simple true positive accuracy. Try using F1 score, precision (of identifying invalid records), Kappa metric, etc. Due to high class imbalance, it would help if you either over-sampled the minority class (invalid) or under-sampled the majority class (valid), or did both together. This will improve the model's ability to classify mode precisely. Adjust the hyper-parameters such as learning rate and hidden layers/no. of units for best model performance.
