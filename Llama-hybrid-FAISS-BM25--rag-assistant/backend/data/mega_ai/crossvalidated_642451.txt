[site]: crossvalidated
[post_id]: 642451
[parent_id]: 
[tags]: 
Regression models as goal metrics for experiments

What are the known issues or pitfalls associated with scoring regression/ML models on observations involved in a randomized experiment (a/b test, etc.)? In a randomized experiment, for most metrics (those directly observed), the difference in per-branch averages provides an unbiased estimate of the average treatment effect (ATE). We can conduct a t-test to compare the observations in the control group against those in the treatment. We can also use regression models for covariate adjustment: $$ Y^{obs}_i=\alpha+\tau W_i+\beta X_i+\epsilon_i $$ where $\tau$ is the ATE and $W_i$ represents assignment. But what if $Y^{obs}_i$ is not directly observed but is instead the output of a model (possibly trained on a subset of $X$ )? For example, suppose I had a model to predict future revenue. Under what conditions can we say that an experiment has demonstrated evidence of a causal relationship between treatment assignment and future revenue? Naively, it's simple to test for statistically significant differences in model predictions between the branches, but what conditions must be satisfied for that to be evidence of a causal impact on expected revenue (not just a causal impact on the model predictions)? As an example, suppose our "model" is "future revenue = 0.1 * num_visits". If the treatment branch changes the functional relationship between num_visits and future revenue such that the true factor is 0.05 (perhaps by causing the number of visits to double and the conversion rate to halve) then the model will be wrong on that treatment branch and we cannot use inferences from it.
