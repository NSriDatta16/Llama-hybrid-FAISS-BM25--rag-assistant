[site]: crossvalidated
[post_id]: 342046
[parent_id]: 
[tags]: 
Explaining dimensionality reduction using SVD (without reference to PCA)

I have seen dimensionality reduction mentioned as one of the practical usages of SVD. However, the explanation for me has always been Let me find the directions in which the variance of the data is maximum The derivation for PCA follows The principle axis are the eigenvectors of covariance matrix ( $AA^T$ ) Ohhh wait that is precisely what SVD is if we are finding $U$ in $ U*S*V^T$ I see we can use SVD to reduce dimensionality of data But people seem to understand SVD in some capacity which makes them see dimensionality reduction directly as a consequence of SVD. Which I don't quite understand. Can somebody help me understand how without knowing that the eigenvectors of covariance matrix are the directions of principal axis, you get to see dimensionality reduction as something you can do from SVD?
