[site]: crossvalidated
[post_id]: 617263
[parent_id]: 
[tags]: 
Statistical assessment of block size for bootstrapped distribution fitting

I have a set of intensities from unordered independent events (with no date or timestamps), many of which constitute extremes, and I want to generate an extreme value distribution. The only information I have other than the intensities is the average number of events that occur per year. My approach is to generate a set of $N$ GEV distributions using max pool statistics on resampled data as a form of bootstrapping / Monte Carlo simulation. For example, I randomize the events, then generate blocks of $L$ years (using the average events per year) from which I can calculate block maximums and fit a GEV. The resampling and GEV fitting is performed $N$ times so that confidence intervals can be generated for extreme quantiles. One challenge when scaling this approach (I have this sort of data over many locations and analysis is high throughput) is that a larger block size $L$ produces better results in certain areas. For example, sometimes the GEV intensities explode at low return frequencies (an unlikely/impossible scenario). To address this, I perform the bootstrapping described (of $N$ simulations) above at each location for hyperparameter $L = 10, 20, 30, 40, 50, 60, 70, 80, 90, 100$ . I have done some investigation and found the Cramer von Mises test to be sufficient for determining if $L$ is suitable for a single GEV produced from a single resampled dataset. In other words, when I fail to reject $H_0$ of the Cramer von Mises test that "the sample data comes from the fitted GEV distribution," I find that the GEV quantiles are not spurious. Now I want to scale the test for all $N$ GEVs/resampled datasets. How can I use $N$ Cramer von Mises tests (for an overall type-I error rate $\alpha=0.05$ ) to determine if a block size $L$ is suitable? Is it appropriate to use a multiple test correction (like Bonferroni by using $\frac{\alpha}{N}$ )? I am not sure if this setup constitutes familywise comparison. Is it appropriate to use the average p-value from $N$ tests, or construct a 95% CI for the p-value? Maybe I can determine if the block size is suitable by checking if less than ${\alpha}*N$ tests reject the $H_0$ ?
