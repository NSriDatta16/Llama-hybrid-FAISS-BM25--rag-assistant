[site]: datascience
[post_id]: 53763
[parent_id]: 53759
[tags]: 
Try below code, here I have taken other data. pca = PCA(n_components = None) first you give here none to check how much each feature is contributing. # Importing the libraries import numpy as np import matplotlib.pyplot as plt import pandas as pd # Importing the dataset dataset = pd.read_csv('Wine.csv') X = dataset.iloc[:, 0:13].values y = dataset.iloc[:, 13].values # Splitting the dataset into the Training set and Test set from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0) # Feature Scaling from sklearn.preprocessing import StandardScaler sc = StandardScaler() X_train = sc.fit_transform(X_train) X_test = sc.transform(X_test) # Applying PCA from sklearn.decomposition import PCA pca = PCA(n_components = None) X_train = pca.fit_transform(X_train) X_test = pca.transform(X_test) explained_variance = pca.explained_variance_ratio_ print explained_variance Output: array([0.36884109, 0.19318394, 0.10752862, 0.07421996, 0.06245904, 0.04909 , 0.04117287, 0.02495984, 0.02308855, 0.01864124, 0.01731766, 0.01252785, 0.00696933]) In the above output you can see the contribution in decreasing order. If you choose n =2 , then 57% of variance you are covering, etc. After reducing dimension, you can choose algorithm for this.
