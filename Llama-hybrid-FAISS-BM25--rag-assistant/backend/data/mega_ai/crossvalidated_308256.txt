[site]: crossvalidated
[post_id]: 308256
[parent_id]: 307249
[tags]: 
The key thing to consider is the interpretation of each model (more so, in fact, than the proportional odds assumption, which is essentially just a constraint one puts on the parameters in the model). Or, more specifically, "which" logit is being modeled (I'm assuming by "cumulative probability" you refer to the "cumulative logit" models). Let's keep things simple and assume you have an outcome, $Y$, with 3 levels. Now, as with logistic regression, you are modeling this outcome using the logit function. $$g(t) = log\left(\frac{t}{1-t}\right)$$ The choice you are making between different ordinal models comes down to a choice of $t$. In the cumulative logit model, you are choosing $t=P(Y\leq i)$ for $i \in \{1,2\}$ (I hope you understand why we will always have $i-1$ logits for any categorical outcome of $i$ levels). So,: $$g(P(Y\leq i)) = log\left(\frac{P(Y\leq i)}{1-P(Y\leq i)}\right)=log\left(\frac{P(Y\leq i)}{P(Y\gt i)}\right)$$ So, for any given level of $Y$, you are modeling the log-odds of $Y$ falling into or below that level versus falling into a higher level. To show it more explicitly: $$ i=1: log\left(\frac{P(Y=1)}{P(Y=2)+ P(Y=3)}\right) $$ $$ i=2: log\left(\frac{P(Y=1) + P(Y=2)}{P(Y=3)}\right) $$ Notice, too, that regardless of the level, the entire response scale is being used to inform the logits. In a regression model using this logit, we are saying that the relationship between the predictors and the outcome is linear on the scale of the log cumulative odds of the response categories . The proportional odds assumption, in fact, is simply a constraint that forces the regression lines across categories to be parallel. This also means, for example, that a positive regression coefficient in this model is associated with a decrease in the outcome; i.e. that the log-odds of being in a lower ordered category has increased relative to the log-odds of being in a higher ordered category. Now, compare this to the adjacent-category model. This model conceptualizes the denominator of the logit a little differently (i.e. it parameterizes the quantity "$1-t$" a bit more stringently): $$g(P(Y=i)) = log\left(\frac{P(Y = i)}{P(Y = i+1)}\right)$$ That is, we are looking at the log-odds of $Y$ being at a certain level versus being at the next higher level (in addition, it can be shown that this is simply a linear transformation of the baseline-category model, where we specify a 'reference' level and compare each level of the outcome against that reference level). This gives us: $$ i=1: log\left(\frac{P(Y=1)}{P(Y=2)}\right) $$ $$ i=2: log\left(\frac{P(Y=2)}{P(Y=3)}\right) $$ Compare this to what we see with the cumulative logit model. For $i=1$, we've removed a term from the denominator, while for $i=2$, we've removed a term from the numerator. More specifically, each logit is no longer informed by the entire response scale. It is only interested in contrasting adjacent categories of the response (thus the name!). In a regression context, the relationship between the predictors and the outcome is linear on the scale of the log adjacent odds of the response categories .A positive coefficient using this model is interpreted as an increase in the log-odds of being at one level of the response versus being at the next level (though, as I alluded to earlier with my comment about the baseline-category model, this can be easily reparameterized to refer to any pairwise comparisons of outcome levels). I've bolded and italicized the points about linearity for a reason. In a logistic regression model, we are saying that the predictors are linearly related to the log-odds of the outcome being a 1 versus being a 0 (thus allowing us, after a transformation, to make inferences about the probability of the outcome being a 1, which is the intuitive interpretation we are generally seeking). But when we have multiple levels of the outcome, we need to make a decision about which log-odds to use, and thus which probability we are making inferences about. In other words, when deciding between these two models, you have to consider which probability is actually of interest (which depends on exactly what your outcome is and how the different levels of it relate to each other). One reason the cumulative logit model is so popular is that the interpretation is fairly intuitive (especially with the proportional odds assumption imposed): how is the outcome being "shifted" into lower categories of the response based on covariate values, taking into account the entire distribution of the outcome? The adjacent-category model, on the other hand, is interested in sequential log-odds pairs, not cumulative ones. This may be of interest; for example, it may be more powerful for comparing two specific levels in the middle of the distribution, e.g. if you had a 5-level outcome and were particularly interested in the difference between levels 2 and 3, or 3 and 4; since the cumulative model won't get directly at that comparison, the adjacent-category model may be more useful. Now, as for continuation/stopping ratio (also called 'stage') models, I am not as familiar with these as I am with the above, so I will refrain from going too deep into them. However, from my understanding, these models are specifically used in cases where an individual can only 'continue' on to higher levels of the outcome if they have already passed through lower levels of the outcome; e.g., if you are modeling education on an ordinal scale based on the type of degree received, an individual with a master's degree has to have already received a bachelor's degree, thus they 'passed through' a lower level of the scale before reaching the higher level. The primary difference between these types of models and the above (the cumulative logit and adjacent category logit) are that these are modeling conditional log-odds; i.e. the log-odds of being at a certain level given having been at other levels.
