[site]: crossvalidated
[post_id]: 559693
[parent_id]: 
[tags]: 
Statistical comparison of multiple methods on a single data set

I'm trying to optimize and evaluate multiple machine learning methods on a single data set. I know that the "usual" procedure with k-fold cross-validation works as follows: Step: Split your data into a training set and a testing set Step: To select the best hyperparameters for each method, use the training set and k-fold cross-validation Step: Retrain your methods using, first, the best hyperparamters from the previous step and second, the training set Step: Calculate the performance metric (e.g., accuracy) on the test set using the model from step 3. However, when using such a procedure we end up with having only one observation per method , and thus, I can't calculate any statistical differences in performance. That is why I came up with the following procedure to optimize and evaluate my methods: Step: Split your data into a training set and a testing set Step: To select the best hyperparameters for each method, use the training set and k-fold cross-validation Step: For each method, use the k models from cross-validation with the best hyperparamter setting from the previous step and estimate their scores on the independent test set. Thus, we end up having k observations per method (compared to having only 1 observation per method in the previous procedure). Question: Is this a valid approach to optimize and evaluate my methods? Question: If this is a valid approach to compare methods which statistical test can I used to look for significant differences in performance?
