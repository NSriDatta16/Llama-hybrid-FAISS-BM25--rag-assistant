[site]: crossvalidated
[post_id]: 554124
[parent_id]: 
[tags]: 
systematically determine which model is better on a specific dataset

In general, there is no silver bullet to choose a better machine learning model for all datasets. However, I'm wondering, what if we fix a dataset including all the train/test set ahead of time. Given supervised model $f_1, \cdots, f_n$ , a labeled regression dataset $X, y$ , the dataset is nicely split into training and testing(no out of distribution situation) and using RMSE as comparing metric. Is there any systematic way to determine which model will perform better, without actually fitting all $n$ models? And nice properties of $f_i$ can be assumed. (E.g. we fix all the hyper-parameter ahead of time). Let the $f_i$ be a CART(decision tree) and their max_depth is fixed with $i$ .
