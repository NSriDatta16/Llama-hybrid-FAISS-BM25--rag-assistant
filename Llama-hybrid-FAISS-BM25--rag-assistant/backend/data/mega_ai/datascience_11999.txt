[site]: datascience
[post_id]: 11999
[parent_id]: 11880
[tags]: 
Possibly see my answer regarding the unreasonable effectiveness of ensembles, and the tradeoffs on explanation versus prediction. Minimum Message Length (MML, Wallace 2005) gives a formal definition of explanation in terms of data compression, and motivates the expectation that explanations generally fit without overfitting, and good explanations generate good, generalizable predictions. But it also touches on the formal theory why ensembles will predict better -- a result going back to (Solomonoff 1964) on optimal prediction and intrinsic to fully Bayesian approaches: integrate over the posterior distribution, don't just pick the mean, median, or mode.
