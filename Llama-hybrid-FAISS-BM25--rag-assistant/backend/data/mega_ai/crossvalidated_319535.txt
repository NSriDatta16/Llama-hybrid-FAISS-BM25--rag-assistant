[site]: crossvalidated
[post_id]: 319535
[parent_id]: 
[tags]: 
Combining perceptrons

Has anyone ever tried to build a Multi-Layer Perceptron Neural Network without the sigmoid function? Let me explain better: We know that a perceptron is a binary classifier that assign the test point to the class 0 if the dot product with the learned weight is negative, and to the class $1$ if the scalar producut is zero or positive. Imagine now that I build a layered network, like a MLP, but without the sigmoid function for activating the neurons at each step. In this way the output of each neuron is just the dot product of the input at the previous layer. Has this a name? What's in your opinion the best way to test this? Which framework/software/library would you use to test this?
