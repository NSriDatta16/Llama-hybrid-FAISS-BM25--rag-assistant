[site]: crossvalidated
[post_id]: 87013
[parent_id]: 86991
[tags]: 
The intercept term is absolutely not immune to shrinkage. The general "shrinkage" (i.e. regularization) formulation puts the regularization term in the loss function, e.g.: $RSS(\beta) = \|y_i - X_i \beta \|^2$ $RegularizedLoss(\beta) = RSS(\beta) - \lambda f(\beta)$ Where $f(\beta)$ is usually related to a lebesgue norm, and $\lambda$ is a scalar that controls how much weight we put on the shrinkage term. By putting the shrinkage term in the loss function like this, it has an effect on all the coefficients in the model. I suspect that your question arises from a confusion about notation in which the $\beta$ (in $P(\beta)$) is a vector of all the coefficients, inclusive of $\beta_0$. Your linear model would probably be better written as $y = X \beta + \epsilon$ where $X$ is the "design matrix," by which I mean it is your data with a column of $1's$ appended to the left hand side (to take the intercept). Now, I can't speak to regularization for neural networks. It's possible that for neural networks you want to avoid shrinkage of the bias term or otherwise design the regularized loss function differently from the formulation I described above. I just don't know. But I strongly suspect that the weights and bias terms are regularized together.
