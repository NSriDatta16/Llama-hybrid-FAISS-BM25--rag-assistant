[site]: crossvalidated
[post_id]: 339849
[parent_id]: 
[tags]: 
Working with integrals and matrices- beginner question

In the book Analysis of Incomplete Multivariate Data (Schafer, 1997), the writer tells about the following model: $Y$ is our data with missing entries, divided to $Y_{mis}$ and $Y_{obs}$, the missing and observable entry set. $R$ is an indicator variable matrix indicating for each entry of $Y$ if it's missing or not. $Y$ and $R$ depend on hidden variables: $\theta$ is data generating hidden parameter(s), and $\xi$ is the missing data parameter(s). Can someone explain the integrals? Specifically, I want to know: What are the rules used between equalities? Intuitive reasoning about differentiating on non-trivial variable, such as $Y_{mis}$? How should I think about it? 2.3 The observed-data likelihood and posterior 2.3.1 Observed-data likelihood Following arguments given by Rubin (1976) and Little and Rubin (1987), it can be shown that under ignorability, we do not need to consider the model for $R$ nor the nuisance parameters $ξ$ when making likelihood-based or Bayesian inferences about $θ$. Because the observed data truly consist not only of Y obs, but also of R, the probability distribution of the observed data is actually given by \begin{align} P(R, Y_{obs}|θ, ξ) &= \int P(R, Y| θ, ξ) dY_{mis} \\ &= \int P(R|Y, ξ)P(Y|θ) dY_{mis}, \tag{2.3} \end{align} where the integral is understood to mean summation for distributions that are discrete.
