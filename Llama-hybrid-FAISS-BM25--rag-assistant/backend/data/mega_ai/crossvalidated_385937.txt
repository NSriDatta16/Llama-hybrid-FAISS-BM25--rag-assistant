[site]: crossvalidated
[post_id]: 385937
[parent_id]: 350090
[tags]: 
It might help to use some more specific notation because the Bayesian update will depend on what model you're using. As an example, say you had a linear regression model where you regressed $y_i$ on $\mathbf{x}_i$ variables. The coefficients/parameters for this model could be called $\theta$ . If you had a batch of $n$ data points, you might write your likelihood as $$ p(\mathbf{y}_{1:n} \mid \mathbf{X}_{1:n}, \theta) = \prod_{i=1}^n p(y_i \mid \mathbf{x}_i, \theta). $$ Your model would be complete as soon as you chose some prior distribution $\pi(\theta)$ . Bayes' rule states $$ \pi(\theta \mid \mathbf{y}_{1:n}, \mathbf{x}_{1:n}, \theta) \propto p(\mathbf{y}_{1:n} \mid \mathbf{X}_{1:n}, \theta)\pi(\theta). $$ If you got another row of data ( $y_{n+1}, \mathbf{x}_{n+1}$ ), then you could update your posterior using $$ \pi(\theta \mid \mathbf{y}_{1:n+1}, \mathbf{x}_{1:n+1}, \theta) = p(y_{n+1} \mid \mathbf{x}_{n+1}, \theta)\pi(\theta \mid \mathbf{y}_{1:n}, \mathbf{x}_{1:n}, \theta). $$ So the old posterior distribution takes the place of the prior distribution when you update sequentially. As I said before, the Bayesian update will depend on what model you're using. A different model would have different conditional independence structure. However, many other models will resemble the last expression in that the old posterior is used as a prior, and it's being multiplied by some marginal "likelihood."
