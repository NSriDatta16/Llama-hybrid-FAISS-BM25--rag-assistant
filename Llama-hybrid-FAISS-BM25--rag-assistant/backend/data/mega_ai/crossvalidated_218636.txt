[site]: crossvalidated
[post_id]: 218636
[parent_id]: 218619
[tags]: 
If you are going to use the sample covariance matrix and you don't have enough samples, your covariance is ill conditioned, won't be invertible and you are going to get VERY POOR estimation results. There are many ways to practically deal with this. Here are the main strategies I would use: 1.There is a whole field of research that aims to regularize this problem and compute a better estimate of the covariance matrix when you do not have enough samples. The most common thing I see statistics loving people using, is either using the pseudo inverse covariance matrix or using a form of shrinkage. Shrinkage is extremely common . If you are looking for something more recent, I suggest this paper . 2.An alternative strategy, more related to the machine learning world, is to use the strategy adopted by Random Forests called Bagging . In Bagging, you randomly select a small subset of the features, so, you will need a lot less samples for your covariance matrix to be properly conditioned. Then you can use an ensemble of such well conditioned classifiers and get a better result. As a rule of thumb, you should use about x10 times the number of samples as there are dimensions in your data. You don't only want your covariance to be well conditioned, you also want it to be accurate.
