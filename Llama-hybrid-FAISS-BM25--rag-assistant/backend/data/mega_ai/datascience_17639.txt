[site]: datascience
[post_id]: 17639
[parent_id]: 16817
[tags]: 
Text mining is rather a tricky field of machine learning application, since all you've got is "unstructured and semi structured data" and the preprocessing and feature extraction step matters a lot. The text mining handbook is a priceless reference in this area of research. but to be specific about your case, I can suggest two answers: as noted, preprocessing step plays a very important role here. in text mining it is likely to get trapped inside the curse of dimensionality, since you probably have say around 1000 documents but more than 15000 unique words in a dataset. techniques such as stemming and lemmatizing, static and dynamic stopword and punctuation erasing all aim solving this issue. So preprocessing and feature extraction is not an option. it is a MUST Naive Bayes model is a linear classifier. Even though it is a very popular algorithm in text classification, there are still risks of rising such problems as yours. the main reason could be that your word-space matrix is highly sparse. you must have paid attention to the fact that in calculating the posterior probability of belonging to a class, Naive Bayes, naively multiplies all single probabilities of P(y|x_i) . and if there is at least one zero probability, your final answer would be zero, no matter what other inverse observation probabilities are. If you have implemented the algorithm yourself, try already-constructed tools in MATLAB, Python sci-kit learn library, or data mining softwares like KNIME and RapidMiner. they have delicately handled such practical issues in implementing Naive Bayes algorithm.
