[site]: crossvalidated
[post_id]: 568169
[parent_id]: 568155
[tags]: 
David Silver has chosen not to include the action taken at time $t$ because the history is being referenced so that the agent can choose $A_t$ . The observation $O_t$ is available because the agent has just arrived at time step $t$ and received the observation. It needs to make a decision on what to do next. Histories at other stages of the process exist in principle. E.g. just after taking the action, but before receiving the immediate reward and next state is a valid point to construct a history of the trajectory so far. However, it is a less useful history when considering what inputs you have available for a policy function.
