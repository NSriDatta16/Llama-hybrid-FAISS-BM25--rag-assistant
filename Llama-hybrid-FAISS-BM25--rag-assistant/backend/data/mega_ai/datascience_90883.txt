[site]: datascience
[post_id]: 90883
[parent_id]: 90871
[tags]: 
Usually, the norm to asking any question on Stack Overflow or any other sister websites is that one is supposed to ask one question only unless they are very similar which doesn't seem to be in this case. To answer your questions, $X^TX$ is called Sample Covariance (or Correlation) Matrix where $X$ is the data matrix of dimensionality $(m,d)$ . So, the resultant matrix has a dimensionality of $(d,d)$ where $d$ is the dimensionality of the feature space. And as you said, this matrix is made to go an eigendecomposition to get $w\cap w^{-1}$ where $\cap$ is a diagonal matrix of eigenvalues, arranged in decreasing order and $w$ is the normalized eigenvectors stacked according to the corresponding eigenvalues. The reason one might want to choose some $k$ number of dimensions is to reduce dimensionality. Reduced dimensionality provides multiple benefits - reducing space complexity, faster computations etc. The problem arises when you specifically talk about time series . PCA, ICA does not take into account temporal dependence which might cause data to have suboptimal forecast. There are different ways to tackle this issue, one might want to use Forecastable Component Analysis , autoencoders etc to ensure they are reaping the benefits of not only gaining the benefits that PCA provides but also ensuring that the problems with PCA's are avoided. To answer the second question, I'm NOT sure of the reason but one may want to drop the first principal because it is in the direction of maximum variance, i.e. it varies most in this direction.
