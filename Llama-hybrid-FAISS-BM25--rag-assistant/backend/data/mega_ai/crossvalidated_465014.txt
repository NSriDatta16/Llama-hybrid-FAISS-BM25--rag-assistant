[site]: crossvalidated
[post_id]: 465014
[parent_id]: 464735
[tags]: 
The standard fast solution is really averaging the embeddings. The problem of averaging is that function words that appear in most sentences cause that the longer the sentence is, the more it converges to non-informative average. Therefore removing stop words is necessary in this case to get reasonable performance. Methods based on spectral matrix decomposition might also be an interesting alternative. Nevertheless, I would say that nowadays, representing a sentence using static word embeddings is a poor man's solution that comes into play when speed or memory requirements really matter. When you represent a sentence using static word embeddings, the main weakness is that embeddings are not informed about the context in which the words are used. This is the reason why contextual word representations such as ELMo or BERT has been developed and why they are so successful. They provide an answer that you probably don't want to hear: build a large recurrent or Transformer network and pre-train it as a language model. This is currently the best way how to represent a sentence when you start with word embeddings.
