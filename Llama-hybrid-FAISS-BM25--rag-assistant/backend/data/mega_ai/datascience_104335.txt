[site]: datascience
[post_id]: 104335
[parent_id]: 103927
[tags]: 
If you are unhappy with using your training-validation split set for evaluating your model, here are a few additional ways to compare your performance: Metric tracking. This is often used when data is abundant (for example - MSMarco uses MRR to evaluate the quality of their embeddings). You can find that here: https://microsoft.github.io/msmarco/ Another good metric is mean RBO (Rank biased overlap). Eyeball checks using a few queries. This is most helpful when you are looking to build something for the first time and you want to sense-check it across your top X queries. If your data has overfit - you will see very poor performance on search queries outside of your data distribution. Embedding projector for you to evaluate your embeddings and their nearest neighbors such that about the data bias. The embedding projector will require a good dimensionality reduction algorithm and should have a good clustering algorithm to help you detect these biases. I have helped co-author a few Python packages to help out some of these issues: For comparing search performances: https://github.com/RelevanceAI/search_comparator (Releasing a few packages in the next few days that will help with this as well - will update this comment when I do!)
