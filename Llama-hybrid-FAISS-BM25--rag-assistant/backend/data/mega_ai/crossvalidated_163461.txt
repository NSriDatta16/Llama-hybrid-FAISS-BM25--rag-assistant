[site]: crossvalidated
[post_id]: 163461
[parent_id]: 163414
[tags]: 
The term ``calibration'', as applied to survey weights, appears to have been coined by Deville and Sarndal (1992) . They put an umbrella on a bunch of different procedures that used the known population totals: $$ \sum_{i \in \mathcal{U}} Y_i = T_i $$ where $Y_i$ is a vector of characteristics known for every unit in the population $\mathcal{U}$. For general human populations, these would be census data on demographic characteristics, like age, gender, race/ethnicity, education, geography (regions, states, provinces), and may be income. For establishment populations, these variables typically have to do with establishment size and income. For list samples -- whatever it is that you have attached to your sample. Deville and Sarndal (1992) discussed how to go from design weights (inverse probabilities of selection), $d_i, i\in \mathcal{S}$ where $\mathcal{S}$ is the sample drawn from $\mathcal{U}$, to calibrated weights $w_i$ such that $$ \sum_{i \in \mathcal{S}} w_i y_i = T_i $$ i.e., the sample agrees with the population on these variables. They did this by optimizing a distance function $$ F(w_i,d_i) \ge 0 \to \min, \quad F(r_i,r_i) = 0, \mbox{ subject to } \sum_{i \in \mathcal{S}} w_i y_i = T_i $$ Typically, as is often the case in statistics, bringing in additional information improves variances asymptotically, although may throw things off and introduce weird small sample biases. Deville and Sarndal (1992) quantified these asymptotic efficiency gains, which was their central contribution to the literature. In regards of using auxiliary data, survey statistics stands as a pretty unique branch. Bayesian folks utilize auxiliary data in their priors. The i.i.d. frequentists / likelihoodists usually don't have much of a way to incorporate auxiliary information, it seems, as all information must be contained in the likelihood. There is, however, a branch of empirical likelihood estimation where auxiliary information is being used to generate and/or aggregate estimating equations; in fact the empirical likelihood objective functions is one of the objective function cases considered by Deville and Sarndal (1992). (Econometricians should sniff, quite properly, and point out that they have known the ways to calibrate statistical models via generalized method of moments for more than 30 years by now, since Hansen (1982) . A quadratic loss is another naturally interesting case in Deville and Sarndal (1992); while it is the easiest to compute, it can give rise to negative weights that are usually considered weird.) Another use of the term `` calibration '' in statistics that I heard of is the reverse regression, in which you have inaccurate measurements of the variable of interest and want to recover the value of the predictor (the running example I was given by my marathon runner of a stats professor was measuring the distance of the course by biking it and counting the revolutions of the bike wheels, vs. more accurate GPS measurements -- that was in the late 1990s era before smarthphones and handheld GPS devices.) You calibrate your bike on an established 1km course, and then try to bike around to get 42 that much. There may be yet other uses. I am not sure dumping them all in one entry is particularly wise though. You indicated factor analysis as one potential user of that term, but I am not particularly well aware of how it is used there.
