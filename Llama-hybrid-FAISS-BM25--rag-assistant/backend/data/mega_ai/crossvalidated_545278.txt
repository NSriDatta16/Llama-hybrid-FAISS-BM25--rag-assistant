[site]: crossvalidated
[post_id]: 545278
[parent_id]: 545273
[tags]: 
Compared to the others, my answer is focused on understanding how you use ROC and AUC in Data Science cases. If you need the mathematical / statistical part, my answer won't help you. Basically, ROC curve shows false positive (FP) RATE and true positive (TP) RATE for each threshold of the model (score you decided as being the limit between classification '1' and '0'). So at the start, if your threshold is 1 (max possible score for your model), you classify everything as 0 and then there's 0% FP and 0% TP. If threshold is 0 (min possible score for your model), everything is classified as 1 and so your TP and FP rates are 100%. Using a threshold strictly between 0 and 1, you'll have FP and TP rates between 0% and 100%. Since this curve is representing Rates obtained at each possible threshold, if you print ROC Curve for your test set, it's totally independant from the training set. It only shows how much FP and TP you have, compared to the maximum you can have in the set. Let's take an easy example : You have a test set with 100 '0' and 10 '1'. Having found 5 of the 10 '1', but misclassifying 30 '0' as '1' to achieve that, you obtain for your curve x = FP_Rate = 30/100 = 0.3 y = TP Rate = 5/10 = 0.5 Imagine now your dataset is balanced and you have 50 '0' and 50 '1'. If you still find half of the ones (25 '1') misclassifying 30% of your zeros (15 '0'), you'll still find x=0.3 ; y=0.5 for your curve. The only matter with ROC Curve is the percentage of FP compared to the percentage of TP, wether the model is balanced or not. --- Edit after comment question : This depends how you use AUC (Area under ROC curve, what you might call ROC metric). AUC measures the performance of 1 model on 1 set. So if you apply it on Train, it'll measure how your model (built on Train) performs on Train (you often do it to compare AUC_Train and AUC_Test and see if you overfit). AUC has nothing to do with how your model is built, it just evaluates the result of 1 model applied on 1 certain set. Wether the set is Train or Test, when you calculate AUC, it's just "The set in which you test your model performance". So this makes no difference. Also, if you want a probabilistic way to understand AUC : If you have a 0.8 AUC, it means that if you take one random '1' row and one random '0' row and apply your trained model on them, the probability of having a higher score for your '1' row than for your '0' is 0.8 You then understand how AUC=0.5 means the model is a random classifier.
