[site]: crossvalidated
[post_id]: 627103
[parent_id]: 
[tags]: 
What does it mean when the Neural Network frequently has poor weight initialization?

I've been training a neural network to learn simple functions like addition. When I train this neural network from scratch, about half the time the neural network gets great accuracy, and the other half of the time the neural network fails to learn anything and the loss remains flat. This seems like the training gets stuck in some local minima way too frequently. What does this entail? The architecture is bad? Optimizer is bad?
