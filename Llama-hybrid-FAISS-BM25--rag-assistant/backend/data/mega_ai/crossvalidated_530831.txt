[site]: crossvalidated
[post_id]: 530831
[parent_id]: 
[tags]: 
A question about integer quantization during the forward pass

I am reading the a paper by Tailor, Fernandez-Marques, and Lane called Degree-quant: quantization-aware training for graph neural networks . In the paper, it is written: For integer QAT [quantization-aware training], the quantization of a tensor $x$ during the forward pass is often implemented as: $$x_q = \min\left(q_{\max}, \max\left(q_{\min}, \lfloor x/s+z \rfloor \right) \right)$$ where $q_{\min}$ and $q_{\max}$ are the minimum and maximum representable values at a given bit-width and signedness $s$ is the scaling factor making $x$ span the $[q_{\min}, q_{\max}]$ range, and $z$ is the zero-point, which allows for the real value 0 to be representable in $x_q$ . Can someone please explain what the formula does? I mean a detailed explanation about the variables that are explained in the where part after the formula. I am new to the field so any related reference suggestion is also appreciated.
