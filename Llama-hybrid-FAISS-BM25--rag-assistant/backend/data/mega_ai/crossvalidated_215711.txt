[site]: crossvalidated
[post_id]: 215711
[parent_id]: 215696
[tags]: 
Some algorithms use a hyperplane (i.e. linear function) to separate the data. A prominent example is logistic regression. Others use a hyperplane to separate the data after a nonlinear transformation (e.g. neural networks and support vector machines with nonlinear kernels). In this case, the decision boundary is nonlinear in the original data space, but linear in the feature space into which the data are mapped. In the case of SVMs, the kernel formulation defines this mapping implicitly. Other algorithms use multiple splitting hyperplanes in local regions of data space (e.g. decision trees). In this case, the decision boundary is piecewise linear (but nonlinear overall). However, other algorithms have nonlinear decision boundaries, and are not formulated in terms of hyperplanes. A prominent example is k nearest neighbors classification. Ensemble classifiers (e.g. produced by boosting or bagging other classifiers) are generally nonlinear.
