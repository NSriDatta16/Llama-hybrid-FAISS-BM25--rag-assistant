[site]: crossvalidated
[post_id]: 629739
[parent_id]: 
[tags]: 
Do you use the final validation accuracy or best validation accuracy during training to fine-tune hyperparameters?

I am currently training a neural network on the EMNIST dataset for an assignment. We are asked to select the best combinations of hyperparameters (inclusion probability for the dropout layer and penalty coefficient for L1/L2 regularisation) based on the performance of a model on the validation set. If using validation accuracy (instead of validation error), would you assess the performance of a particular model based on the best validation accuracy throughout the entire training process, or based on the final validation accuracy (once you are done training the model)? For example, if the validation accuracy reaches a maximum after 50 training epochs, but you train for 100 epochs, causing the validation accuracy to fall again, would you use the validation accuracy at 50 or 100 epochs to assess the performance of the given model? Many thanks.
