[site]: crossvalidated
[post_id]: 339137
[parent_id]: 
[tags]: 
Reinforcement Learning: does sarsa still converge when policy changes during each episode

I use n-step Sarsa/sometimes Sarsa($\lambda$) to solve a maze problem. After experimenting a bit with different epsilon schedules I found out that the agent learns faster when I change the epsilon during an episode based on the number of steps already taken and the mean length of the last 10 episodes. Low number of steps/beginning of episode => Low epsilon High number of steps/end of episode => High epsilon This works far better than just an epsilon decay over time from episode to episode. Does the theory allow this? I think yes because all states are still visited regularly.
