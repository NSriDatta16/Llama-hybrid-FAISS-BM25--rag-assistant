[site]: datascience
[post_id]: 102437
[parent_id]: 
[tags]: 
Where do Q vectors come from in Attention-based Sequence-to-Sequence Transformers?

I'm taking a course on Attention-based NLP but I'm not understanding the calculation and application of Attention, based on the use of Q, K, and V vectors. My understanding is that the K and V vectors are derived from the encoder input and the Q vector is derived from the decoder input. This makes sense to me in the context of training , where the entire input sequence is presented to the encoder and the entire output sequence is presented to the decoder. What does not make sense, however, is how this applies in the context of inference . In that case, it would seem like there is no input to the decoder, so where does the Q vector come from?
