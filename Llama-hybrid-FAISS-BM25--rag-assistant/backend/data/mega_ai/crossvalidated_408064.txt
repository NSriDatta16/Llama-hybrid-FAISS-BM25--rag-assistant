[site]: crossvalidated
[post_id]: 408064
[parent_id]: 
[tags]: 
Performance Imbalance Dataset Decision Tree

I have a imbalance dataset for a classification task, with the minority class accounting for about 21% of the total. When I use a decision tree based model for prediction, let's say a classification tree or a Random Forest (with scikit-learn), I get a global accuracy of 76%. The detection rate of the minority class (also called sensitivity), is only about 43%. So, to get an higher detection rate for the minority class, I penalize it more heavily. In sklearn, one can do it with the parameter class_weight={'0':1, '1':penal_value} (here, 0 being the majority class, and 1 the minority). Of course penal_value is >= 1. For example, when I set it to 2, I get a 71 global accuracy, but a 55 minority class detection rate. When I set it to 3.76 (= to 81% / 21%, the rate of the majority on the minority), I get 64 global accuracy, and 60 detection rate. When I set it to 5, ... Well, you understand the procedure. What I whant is a measure, that tell what is a "good" value for the penalization. Because it seems that, at the beginning, so with penal_value between 1 and 3.76, it's in some way interesting to lose some global accuracy, because we gain enough sensitivity. It's worth the trade-off. But past this point, 3.76, it's "not worth" it. The loose in global accuracy "is not compensated" by the gain in sensitivity. How could I put in a more "objective" way/measure?
