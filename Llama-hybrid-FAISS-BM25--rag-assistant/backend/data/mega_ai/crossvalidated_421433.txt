[site]: crossvalidated
[post_id]: 421433
[parent_id]: 421393
[tags]: 
I'll take a more intuitive stab at answering your question, Ben. Imagine we have a population of employees for whom we want to understand the relationship between their current salary (dependent variable) and years of work experience (independent variable). To study this relationship, we design a study whereby we will randomly select n = 100 employees from the target population and we will record their current salary and years of experience. Before we collect the data , we postulate the model which we believes best describes the relationship of interest: $ Y = \beta_0 + \beta_1 X + \epsilon $ , where $Y$ stands for current salary and $X$ stands for years of work experience. What does this model really mean? It means that, if we were to randomly select an employee from our target population, we would expect their current salary to be approximately a linear function of years of experience (e.g., the salary will be expected to be smaller for someone with fewer years of experience, but higher for someone with more years of experience). The $\epsilon$ term quantifies how good this approximation is: if $\epsilon$ is 0, the approximation is perfect; if $\epsilon \neq 0$ , the approximatiom is subject to some error. The magnitude of $\epsilon$ tells us by how much we are off when making this approximation. Note that, before we collect the data, $\epsilon$ is an unknown, random term. (The randomness comes from having randomly selected an employee from the target population.) When we assume that $\epsilon$ comes from a normal distribution with mean 0 and variance $\sigma^2$ , we're essentially saying the following: We believe that the variable years of experience has the largest effect on salary and that this effect is linear (e.g., as the years of experience go up, salary tends to go up). While there are potentially other variables which may affect salary , their individual influences are likely small - these combined influences are captured by the error term $\epsilon$ . After collecting the data , we can use these data to estimate the values of $\beta_0$ , $\beta_1$ and $\sigma^2$ . This would enable us to compute the residual term for an employee in our sample. That residual term $ \hat{\epsilon} $ estimates the $\epsilon$ term but, unlike the unknown $\epsilon$ , it is fully known. In other words, the residual term is our best guess from the data of the amount by which our linear approximation of salary by years of experience is off for that particular employee. Our assumption that $\epsilon$ comes from a normal distribution with mean 0 and variance $\sigma^2$ helps us derive the sampling distributions of the regression coefficient estimators, which describe how consistent the values of these estimators would be from sample to sample if we were to repeatedly draw random samples of n = 100 employees from the target population. Under this assumption, these sampling distributions would also be normal so we could use them as a basis for constructing confidence intervals for $\beta_0$ and $\beta_1$ . The width of these intervals will depend on the magnitude of $\sigma^2$ - the larger the value of $\sigma^2$ , the larger the width of these confidence intervals. In other words, higher values of $\sigma^2$ lead to increased uncertainty regarding the values of $\beta_0$ and $\beta_1$ . If we were to plot salary versus years of experience for the sample of n = 100 employees in our (fictitious) study, we would get clues about the value of $\sigma^2$ . Just looking at the scatter of the observations in this plot about the fitted regression line would tells us what we need to know: a low degree of scatter would suggest a small value for $\sigma^2$ ; a high degree of scatter would suggest a large value for $\sigma^2$ . Addendum The plot below shows (approximations to the) sampling distributions of the slope estimator in the simple linear regression model: $Y = \beta_0 + \beta_1*X + \epsilon$ under three different assumptions: $\epsilon$ comes from a $Normal(0, \sigma^2)$ where $\sigma = 0.5$ ; $\epsilon$ comes from a $Normal(0, \sigma^2)$ where $\sigma = 1.0$ ; $\epsilon$ comes from a $Normal(0, \sigma^2)$ where $\sigma = 1.5$ . These sampling distributions were obtained by drawing a large number of samples (i.e., 1000 samples) from this model under each of the three scenarios, with each sample having a size of n = 100. (In effect, the values of $X$ were fixed to be the same in each sample - just some regularly spaced numbers between 0 and 1, and only the values of $Y$ were random.) For this model, let's assume that we know the actual value of the true slope $\beta_1$ to be 2 (while $\beta_1 = 1$ ). As expected, the sampling distributions of the slope estimator of $\beta_1$ are all centered at 2 (the value of $\beta_1$ ) but their spreads differ: the higher the value of $\sigma$ , the larger the spread. Here is what a sample of n = 100 (X,Y) values corresponding to each of the three scenario would look like: In this second plot, the true regression line (i.e., the line with an intercept $\beta_0 = 1$ and a slope $\beta_1=2$ ) is shown as a dotted line having magenta colour. The estimated regression line obtained from the sample data is shown as a solid line having blue colour. The left panel in the second plot corresponds to $\sigma = 0.5$ , the middle panel corresponds to $\sigma = 1.0$ and the right panel corresponds to $\sigma = 1.5$ . If you think of $\sigma$ as quantifying the amount of noise in the data and the true regression line as quantifying the signal you are trying to learn something about, you will note that it's "easier" to extract the signal when you have little noise in the data and it's "harder" to extract the signal when you have a lot of noise in the data. That is because the noise has a nasty habit of obscuring the signal. In other words, the more noise there is in the data, the more uncertainty there will be about the signal itself. The sampling distribution of the slope estimator captures this interplay between noise and signal.
