 ) {\displaystyle L_{Grad-CAM++}^{C}(x,y)=\sum _{k}w_{k}^{C}A_{k}(x,y)} in which the coefficient w k C {\displaystyle w_{k}^{C}} is defined as: w k C = ∑ i , j α k C ( i , j ) × R e L U ( ∂ y C ∂ A k ( i , j ) ) {\displaystyle w_{k}^{C}=\sum _{i,j}\alpha _{k}^{C}(i,j)\times ReLU({\frac {\partial y^{C}}{\partial A_{k}(i,j)}})} with A k ( x , y ) {\displaystyle A^{k}(x,y)} , the activation of node k in the target layer of the model at position (x,y); y C {\displaystyle y^{C}} the logit score for class C, and α k C ( i , j ) {\displaystyle \alpha _{k}^{C}(i,j)} being: α k C ( i , j ) = ∂ 2 y C ∂ ( A k ( i , j ) ) 2 2 × ∂ 2 y C ∂ ( A k ( i , j ) ) 2 + ∑ a , b A k ( a , b ) × ∂ 3 y C ∂ ( A k ( i , j ) ) 3 {\displaystyle \alpha _{k}^{C}(i,j)={\frac {\frac {\partial ^{2}y^{C}}{\partial (A_{k}(i,j))^{2}}}{2\times {\frac {\partial ^{2}y^{C}}{\partial (A_{k}(i,j))^{2}}}+\sum _{a,b}A_{k}(a,b)\times {\frac {\partial ^{3}y^{C}}{\partial (A_{k}(i,j))^{3}}}}}} While addressing some Grad-CAM problems, Grad-CAM++ method still relies on gradients, and it only improves the underlying math. It is, however, still based on the idea of assigning a direct and valid relationship between gradient and importance. Notation: (a,b) indexes all pixel positions in the feature‐map, exactly like (i,j) does, but for the summation in the denominator. Score-CAM Score-CAM is a gradient-free CAM technique, thus redefining the original Grad-CAM and Grad-CAM++ working principles. It uses the model confidence scores instead of gradients. Score-CAM performs the following operations: Extracts the feature maps A k {\displaystyle A_{k}} of the final convolutional layers, as in the original Grad-CAM; Upsamples each activation map A k {\displaystyle A_{k}} to the same input image dimensions, defining a mask M k {\displaystyle M_{k}} and each mask is normalized; Multiplies the original input image by the mask, defining a masked image X k ′ = M k ⊙ X {\displaystyle X'_{k}=M_{k}\odot X} ( ⊙ {\displaystyle \odot } is the element-wise multiplication); Gets a confidence score (a softmax probability is the output value after the softmax operation; the logit is the value before the softmax) for the masked images X k ′ {\displaystyle X'_{k}} , by feeding it into the CNN (either the soft-max probability or the raw logit can be used; both yield similar results in practice); Considers that confidence score as the weight w k c {\displaystyle w_{k}^{c}} for the feature map A k {\displaystyle A_{k}} . These operations allow to replace the gradient calculations with the actual model outputs, building more accurate heatmaps. Mathematically, the localization map is defined as: L S c o r e − C A M C ( x , y ) = R e L U ( ∑ k w k c A k ( x , y ) ) {\displaystyle L_{Score-CAM}^{C}(x,y)=ReLU(\sum _{k}w_{k}^{c}A_{k}(x,y))} and the coefficient w k C {\displaystyle w_{k}^{C}} s: w k C = s o f t m a x k ( y C ( X k ′ ) ) = e x p ( y C ( X k ′ ) ) ∑ m e x p ( y C ( X m ′ ) ) {\displaystyle w_{k}^{C}=softmax_{k}(y^{C}(X'_{k}))={\frac {exp(y^{C}(X'_{k}))}{\sum _{m}exp(y^{C}(X'_{m}))}}} where A k ( x , y ) {\displaystyle A_{k}(x,y)} is the activation of channel k at location (x,y), y C ( X ) {\displaystyle y^{C}(X)} is the logit for class C for an input X and M k {\displaystyle M_{k}} is the mask, defined as: M k ( x , y ) = U ( A k ) ( x , y ) − m i n x , y U ( A m ) m a x x , y U ( A k ) − m i n x , y U ( A k ) {\displaystyle M_{k}(x,y)={\frac {U(A_{k})(x,y)-min_{x,y}U(A_{m})}{max_{x,y}U(A_{k})-min_{x,y}U(A_{k})}}} with U {\displaystyle U} as the upsampling operation. Since the process of score calculation is repeated for every channel, Score-CAM is slow with respect to gradient-based methods. Moreover, it focuses on regions highlighted by individual feature maps, ignoring the context of the full image, reducing interpretability in complex scenes. LayerCAM LayerCAM enhances backwards class-specific gradients using both intermediate and final convolutional layers. Combining information across layers al