[site]: datascience
[post_id]: 17454
[parent_id]: 13304
[tags]: 
Using frequentist hypothesis testing in this manner (using the p-value to determine pass/fail) has serious drawbacks even in bug-free software. My suggestion is to use some form of Bayesian A-B testing. There is much nerd debate about this (frequentist vs. Bayesian A/B testing). First, some references. Then we'll have a look at your AA test with the online calculator. On using p-values as pass/fail criteria: Limitations of p-values (blog regarding epidemiological studies) Too Big to Fail: Large Samples and the p-Value Problem (research article) Statistics Done Wrong (very accessible book) On Bayesian A/B testing: LinkedIn's use of Bayesian A/B testing (conference proceedings) Bayesian/Frequentist nerd debate example (blog) Bayesian Methods for Hackers (book, online notebook) Peak Conversion Online Calculator In your case the frequentist and Bayesian approaches agree-- your "A" is not equal to your other "A". It appears that there are one or more currently unseen variable(s) you need to take into account. If it was a question of insufficient data, the two peaks would not be so nicely separated. I'd be interested in hearing other possible explanations.
