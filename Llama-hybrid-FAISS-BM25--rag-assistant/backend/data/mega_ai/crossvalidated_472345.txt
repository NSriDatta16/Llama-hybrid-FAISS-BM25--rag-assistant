[site]: crossvalidated
[post_id]: 472345
[parent_id]: 472342
[tags]: 
The dimension of word embeddings is the dimension of the vector space they live in, not the dimension of the tensor which is 1. Therefore, it is common in mathematical jargon (just an overloaded term). I don't think you will have much trouble to disambiguate based on the context. There are clues everywhere. For matrices, for example, it is frequent to read "A matrix of dimension $n \times n$ " which makes it pretty unambiguous.
