[site]: datascience
[post_id]: 69226
[parent_id]: 
[tags]: 
Practical limitations of machine learning

Working on some applied machine learning problems, I've started to encouter some practical difficulties. Those difficulties relate to - but are not limited to - convergence of the learning process, stability trough recalibration, explainability, stability of the explainability trough recalibration. Those problems are a bit difficult to handle, notably because: They are not really dealt with in introductory theoretical books. (However, there are some advanced resources to deal with them individually.) They are mostly specific to advanced ML models, compared to more standard approaches. You only get aware of them once you stumble upon them by doing some very specific tests. I would be interested in some serious sources on the main practical problems related to machine learning (not necessarily how to deal with them). Both as a checklist of pitfalls to avoid and as an authoritative argument if I have to formulate some concerns about any future ML solution. Can you help me with any sources?
