[site]: crossvalidated
[post_id]: 437357
[parent_id]: 435609
[tags]: 
You can't directly compare the estimated coefficients since the units of the response variable are not the same in both models. See, a logistic regression will estimate a binomial probability of observing the event you modelled. So a number between 0 and one is ultimately estimated. Note also that the estimate is not linearly related to the covariates you estimate in the model. So the effect depends on the initial value. Now, in a Negative binomial regression, the outcome is a count, a number between zero and infinity, and again, the covariates are not linearly related to the outcome, due to the link function. But it doesn't mean you cannot compare both models. It just takes a little more effort. I would plot the partial effects of the covariate of interest (and relevant interactions) to each response variables and build my rationale from there. Update My idea is to help you investigate the variables effects onto the different response variables by exposing the model's mechanics. For this we will need an example. In the following code I generate the independent variables x and z . Then I generate the linear dependent portion of a logistic regression (log odds) response_binary and the negative binomial counts and theta using this SO answer. suppressMessages(library(tidyverse)) suppressMessages(library(Hmisc)) suppressMessages(library(glmmTMB)) suppressMessages(library(broom)) suppressMessages(library(MASS)) suppressMessages(library(modelr)) N % mutate(logodds = 1.2*x + 0.2*z - 1.2*x*z + rnorm(N, 0, 0.1) , mu = 1.2*x + 2*x^2 + z + rnorm(N, 0, 0.1) ) %>% mutate( prob = bin_link(logodds), neg_par = exp(mu), response_binary = rbinom(n=N, size=1, prob=prob), theta = sample(c(5,8,10, 15), replace = T, size=N), response_count = rnbinom(n = N, size = theta, mu=neg_par) ) The resulting distribution of response_count is in the following histogram. df %>% ggplot(aes(response_count)) + geom_histogram(color='black', fill='skyblue') + labs(title='Count variable distribution') #> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. And the scatter plots between x and response_count with z colours indicate an idea of what the model should be capable of identifying. df %>% ggplot(aes(x = x, y = response_count, color=z)) + geom_point() + labs(title='relation between x, z and the count response') And to explore the relation between x , z and the response_binary I turn to a lowess plot visualization taken from chapter 12 in Frank Harrel's Regression Modeling Strategies . df %>% ggplot(aes(x = x, y = response_binary, group=z, color=z)) + histSpikeg(response_binary~x*z, lowess=T, data=df) + labs(title='Estimated lowess for the relation between x, z and the\nproportion/probability of the binary response') Now to explore the relationship between x + z when it comes to the response_count and the response_binary I suggest you inspect the models partial effect plots, since the coefficients can be directly compared. First we build two simple models. A negative binomial model nb_md for the response_count variable, and a logistic regression logi_md for the response_binary . nb_md #> Call: #> glm.nb(formula = response_count ~ x * z, data = df, link = "log", #> init.theta = 8.292271032) #> #> Deviance Residuals: #> Min 1Q Median 3Q Max #> -3.1667 -0.7803 -0.1371 0.5643 2.7400 #> #> Coefficients: #> Estimate Std. Error z value Pr(>|z|) #> (Intercept) -0.36614 0.08754 -4.183 2.88e-05 *** #> x 3.37600 0.12373 27.285 zTRUE 0.98352 0.13262 7.416 1.21e-13 *** #> x:zTRUE -0.08763 0.19660 -0.446 0.656 #> --- #> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #> #> (Dispersion parameter for Negative Binomial(8.2923) family taken to be 1) #> #> Null deviance: 2448.32 on 499 degrees of freedom #> Residual deviance: 548.59 on 496 degrees of freedom #> AIC: 2455.9 #> #> Number of Fisher Scoring iterations: 1 #> #> #> Theta: 8.29 #> Std. Err.: 1.24 #> #> 2 x log-likelihood: -2445.94 logi_md #> Call: #> glm(formula = response_binary ~ x + z, family = "binomial", data = df) #> #> Deviance Residuals: #> Min 1Q Median 3Q Max #> -1.639 -1.248 0.829 1.031 1.451 #> #> Coefficients: #> Estimate Std. Error z value Pr(>|z|) #> (Intercept) 0.03917 0.19276 0.203 0.838968 #> x 1.00490 0.33105 3.035 0.002401 ** #> zTRUE -0.67308 0.20188 -3.334 0.000856 *** #> --- #> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #> #> (Dispersion parameter for binomial family taken to be 1) #> #> Null deviance: 679.64 on 499 degrees of freedom #> Residual deviance: 659.00 on 497 degrees of freedom #> AIC: 665 #> #> Number of Fisher Scoring iterations: 4 The models could be better built, in order to better capture the non-linearity we can see in the exploration plots. However I build the partial effects plot and plot the predictions together with the standard error. From these we can have an idea of how the variable X can have a different effect over response_count and response_binary . Here are some observations we can make: the effect of variable x greatly increases for larger values of x when it comes to understanding it's effect on response_count , being greater when z == T the effect of variable x on response_binary tends to be the same, regardless of z in lower ranges. But it tends to be much larger with z == F if x is large. partial_effects_frame % tidyr::crossing(z=c(T,F)) bind_rows( augment(nb_md, newdata= partial_effects_frame, type.predict='response') %>% mutate(model='nb_md'), augment(logi_md, newdata= partial_effects_frame, type.predict='response') %>% mutate(model='logi_md') ) %>% ggplot(aes(x, .fitted, group=z)) + geom_ribbon(aes(ymin= .fitted - .se.fit, ymax= .fitted + .se.fit), alpha=0.2) + geom_line(aes(group=z, color=z)) + facet_wrap(~model, scales = 'free', ncol=1) + labs(title = 'Partial effects of the variable x interacted with z') Now this is just a simple example of what I meant with a little more effort. Changing base values, exploring the plots and everything, working with tables might help you study the effect on different response variables. Created on 2019-11-22 by the reprex package (v0.3.0)
