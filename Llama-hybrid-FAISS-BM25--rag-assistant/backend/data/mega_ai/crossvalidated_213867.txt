[site]: crossvalidated
[post_id]: 213867
[parent_id]: 
[tags]: 
Neural Networks back propagation

I have gone through neural networks and have understood the derivation for back propagation almost perfectly (finally!). However, I had a small doubt. We are updating all the weights simultaneously, so what is the guarantee that they lead to a smaller cost? If the weights are updated one by one, it would definitely lead to a lower cost and it would be similar to linear regression. But if you update all the weights simultaneously, might we not cross the minima? Also, do we update the biases like we update the weights after each forward propagation and back propagation of each test case?
