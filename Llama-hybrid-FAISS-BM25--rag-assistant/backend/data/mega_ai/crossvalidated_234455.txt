[site]: crossvalidated
[post_id]: 234455
[parent_id]: 234365
[tags]: 
As mentioned by @horaceT in his comment, it is impossible to know beforehand whether dropping the low importance features will increase or decrease model performance. The good news is that you will get the answer by conducting an experiment where you will drop the low importance features one by one (or all at once) and measure cross-validation performance. If the dropped features contain redundant information, the performance of the model should stay the same. If the dropped features contain non-redundant and objective-wise relevant information with regard to the kept features, the performance of the model is likely to decrease because you have rather high sample/feature ratio. In case of low sample/feature ratio, dropping those low importance non-redundant features could, however, potentially increase the performance of the model, because of the curse of dimensionality, which states that when the number of features becomes large, the number of training samples required for a good estimator grows exponentially. As you drop features, the complexity of the predictive model decreases, which eases the interpretation of the model, leads to better generalization due to reduced overfitting and lowers training and classification time. By the way, it is not F1 score that XGBoost is using to assess feature importances. F1 score measures test's accuracy and varies between [0, 1]. XGBoost assess the feature importances with a f(eature)score, which tells the number of occurence of features in the ensemble. If you have for example 100 features but the list of feature importances contains only 75 features, the remaining 25 features never got selected in the trees.
