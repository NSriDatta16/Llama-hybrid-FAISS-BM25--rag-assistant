[site]: crossvalidated
[post_id]: 117439
[parent_id]: 
[tags]: 
GMM estimation of linear regression with intercept restriction

Say I have a time series regression as follows: $$y_t = a_i + \beta_i x_t + \varepsilon_t^i \ \ ; \ \ t = 1, 2, \cdots, T \ \ \text{for each } i$$ Now say I impose the following restriction on the intercept, $a_i$: $$a_i = \beta_i[\lambda - E(x)]$$ where $\lambda$ is a constant and $E(\cdot)$ denotes the expectation. How can I use GMM to write down a set of moment conditions that I can use to estimate this model and test the restriction on $a_i$? Attempt: I know how to do this if there was no restrictions on $a_i$. Let $b$ denote the vector of parameters, i.e., $b = [a \ \ \beta]'$. Then we know from GMM theory that $Var(\widehat{b}) = \frac{1}{T}d^{-1}Sd^{-1 \prime}$ where $d = \frac{\partial g_T(b)}{\partial b'}$ and $g_T(b)$ denotes the sample moment conditions, i.e., $$g_T(b) = \begin{bmatrix} E_T(y_t - a_i - \beta_i x_t) \\ E_T(y_t - a_i - \beta_i x_t)x_t \end{bmatrix} = E_T\left(\begin{bmatrix}\varepsilon_t \\ x_t \varepsilon_t \end{bmatrix}\right)$$ where $E_T(\cdot) = \frac{1}{T}\sum_{t=1}^{T} (\cdot)$. $S$ is given by: $$S = \sum_{j=-\infty}^{\infty}\begin{bmatrix} E(\varepsilon_t \varepsilon_{t-j}') & E(\varepsilon_t \varepsilon_{t-j}' x_{t-j}) \\ E(x_t \varepsilon_t \varepsilon_{t-j}') & E(x_t \varepsilon_t \varepsilon_{t-j}' x_{t-j}) \end{bmatrix}$$ which can be simplified further assuming the errors are uncorrelated and not heteroskedastic. Now once there are restrictions on $a_i$, how do I proceed?
