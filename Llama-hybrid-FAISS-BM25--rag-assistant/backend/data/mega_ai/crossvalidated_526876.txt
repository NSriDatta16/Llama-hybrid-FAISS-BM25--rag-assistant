[site]: crossvalidated
[post_id]: 526876
[parent_id]: 
[tags]: 
Moment Matching for a Laplace Distribution

I have this derivative It belongs to this paper . In the paper, they are trying to model a lightweight bayesian deep neural network by having the distributions on only the activation functions. They use Assumed Density Filtering as a deterministic variation inference method to approximate the distribution of activations. Each activation layer is denoted by Z(i). They choose a gaussian distribution to approximate the posterior at each step. To this end, they try to minimize KL Divergence (KLD). A method of minimization of KLD is moment matching. First and second moments of two distributions, here p and q must be equal. I am trying to assume q as a laplace distribution instead of a gaussian and derive the mentioned derivation. Until now, what I have understood is that the range must be from minus infinity to zero and then from zero to minus infinity. And the rest of steps would be the same. Is this a correct starting point? are there any other changes that will occur when changing the distribution? I appreciate your guidance.
