[site]: crossvalidated
[post_id]: 257389
[parent_id]: 
[tags]: 
Is there an accepted method to determine an approximate dimension for manifold learning

Apologies for the rather vague title, I had difficulty explaining the question without making the title obnoxiously long. The manifold hypothesis suggests that natural data exists on or close to a lower dimensional manifold. There are plenty of real-world and theoretical examples of cases where this is shown to pan out, and I'm not questioning that. How, though, does one determine what the dimension of that manifold might be? In PCA, one can specify how much variance to retain and use the eigenspace that accounts for that much variance, but more complex methods non-linear methods don't generally allow for such convenient rules. A na√Øve approach might be laid out treating the lower dimension as a hyperparameter: Assume I have data $\left( (\mathbf{x_1,y_1}) \ldots (\mathbf{x_n,y_n})) \right)$ with $x_i \in \mathbb{R}^p$. Split data into a standard train/test split Determine some function $f: \mathbb{R}^p \rightarrow \mathbb{R}^d$ that maps the data to a lower dimensional space Perform your favorite predictive algorithm on the new low-dimensional $x^* \in \mathbb{R}^d$ to find a $\hat{y}$ Measure $\mathcal{L}(y,\hat{y})$ Repeat, dropping d by some prescribed amount Plot $d$ against $\mathcal{L}$ and hope for some insight (maybe a clear kink, or some such). But that method seems so unsatisfactory. Surely someone has come up with a better way, or has given a reason why there is no better way. I guess it could also be framed as a regularization problem, but that doesn't avoid the use of a hyperparameter. This is strongly related to the solution in ( What is the manifold assumption in semi-supervised learning? ) but I thought it sufficiently different to warrant being its own question.
