[site]: crossvalidated
[post_id]: 581198
[parent_id]: 581142
[tags]: 
First, if you have $N=4$ binary variables $A, B, C, D$ , the total number of possible outcomes is not $4^2$ but $2^4$ . Next, in Step 3 you state: $$ P(A, B, C, D) = P(A)\;P(B)\;P(C)\;P(D), $$ which would be true if your four random variables were independent. Note, that in this case, your probabilistic graph is just a set of four isolated nodes without any edges between them. If independence is indeed the case, conditioning doesn't change anything, e.g.: $$ P(D = d^0|B = b^1) = P(D=d^0), $$ which is the very meaning of "independence": the outcome of $D$ does not depend on the outcome of $B$ , so conditioning on $B$ doesn't change anything. If, however, you cannot presume the above independence, you need to use the formula you stated. E.g. $$ P(D = d^0|B = b^1) = \frac{P(B=b^1\cap D=d^0)}{P(B=b^1)}. $$ And you can approximate $P(B=b^1\cap D=d^0)$ by dividing the number of events for which both $B=b^1$ and $D=d^0$ holds, let's denote it by $\#ev(B=b^1\cap D=d^0)$ , by the total number of events , let's call it $E$ , not by the number of possible outcomes ( $2^4$ ). (E.g. if you toss a coin, you have only two possible outcomes but you can have thousands of events.) I.e.: $$ \begin{align} P(D = d^0|B = b^1) &= \frac{P(B=b^1\cap D=d^0)}{P(B=b^1)}\\ &\approx \frac{\frac{\#ev(B=b^1\cap D=d^0)}{E}}{\frac{\#ev(B=b^1)}{E}}\\ &= \frac{\#ev(B=b^1\cap D=d^0)}{\#ev(B=b^1)}.\\ \end{align} $$ Obtaining a probabilistic graph (usually the Bayesian network ), is, in general, not trivial. In particular, note that they are usually not unique. You might want to consult libraries that have been developed for this purpose, e.g. bnlearn . But this also only works if you don't have cycles or confounders.
