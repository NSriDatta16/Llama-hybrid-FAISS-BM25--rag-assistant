[site]: datascience
[post_id]: 11811
[parent_id]: 11665
[tags]: 
This looks like almost an ideal use case for a Recurrent Neural Network (RNN) model. I say almost, as your output variable is 2 separate class vectors, that part is slightly fiddly. The main advantage of using a RNN is that the model would be built on the assumption that s1_var1 is of the same type as s2_var1 etc. Whilst with other models you could flatten the time series into a 10 x 20 = 200 long vector, and the output likewise into a 50 long vector, and treat the whole problem as a single step supervised learning, such a model would not include the same assumption. For your problem, the RNN would have 10 inputs (corresponding to the 5 classes of each of 2 variables) and 10 outputs. You would train it by presenting your input sequence one sample at a time in the correct order 1 to 20, then continue running the network 5 more steps, comparing the outputs against expected. When predicting, you do similar, but then just read off the outputs as predictions, one at a time. There are a few different Python libraries that support RNN models, and there are different internal choices (such as using LSTM layer versus GRU) which you might want to explore. I am currently learning the Keras framework, which supports some options for RNNs. Probably the example code which learns to predict an exponentially-decaying sine wave would be a good place to start trying to understand the Keras model - although that's a regression, it should give you some insight into how RNNs can be built for this kind of series-based data. An alternative to RNN here might be a hidden markov model - if you have some insight into the internal state of the system, that could be a better option.
