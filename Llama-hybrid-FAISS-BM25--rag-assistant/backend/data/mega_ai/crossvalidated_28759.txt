[site]: crossvalidated
[post_id]: 28759
[parent_id]: 28727
[tags]: 
It sounds like you're trying to do a calibration study to show the non-superiority of the new test. As you know, inverted hypothesis tests are complex. Since updating A to give you A' takes time and energy, wouldn't you be okay with a rule which says something like, "If A' doesn't provide better discrimination, then maintain A", i.e. a superiority test in which you could look at the AUC and even confidence intervals for it. It also sounds like you have multiple metrics of model performance though the problem description is not clear. As I understand it, you're predicting various levels of complexity in the network, e.g. 10% means you're looking to correctly predict perhaps 10% of the edges of a network graph... or is it 10% sensitivity? If you have several different outcomes, you should think carefully about how you would like to weight each outcome based on its importance for desired application. Perhaps you're more interested in the 30% network... or 90% network. Or you could just take a flat average of all the predictive performances (as measured by the AUC).
