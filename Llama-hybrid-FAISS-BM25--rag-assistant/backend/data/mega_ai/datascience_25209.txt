[site]: datascience
[post_id]: 25209
[parent_id]: 
[tags]: 
Why are policy gradient methods preferred over value function approximation in continuous action domains?

In value-function approximation, in particular, in deep Q-learning, I understand that we first predict the Q values for each action. However, when there are many actions, this task is not easy. But in policy iteration we also have to output a softmax vector related to each action. So I don't understand how this can be used to work with continuous action space. Why are policy gradient methods preferred over value function approximation in continuous action domains?
