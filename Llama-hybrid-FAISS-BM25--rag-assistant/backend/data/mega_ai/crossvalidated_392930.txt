[site]: crossvalidated
[post_id]: 392930
[parent_id]: 
[tags]: 
Time series cross validation

Reading through the scikit learn documentation on time series cross validation ( https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-of-time-series-data ): Time series data is characterised by the correlation between observations that are near in time (autocorrelation). However, classical cross-validation techniques such as KFold and ShuffleSplit assume the samples are independent and identically distributed, and would result in unreasonable correlation between training and testing instances (yielding poor estimates of generalisation error) on time series data. Therefore, it is very important to evaluate our model for time series data on the “future” observations least like those that are used to train the model. To achieve this, one solution is provided by TimeSeriesSplit. Informally, that makes sense to me. However, is anyone aware of a formal proof (i.e., book or journal article) that shows the statement is correct?
