[site]: crossvalidated
[post_id]: 446734
[parent_id]: 446455
[tags]: 
Random Forest is a classification/regression algorithm. It can be used as a "feature selection" method in the sense that -once it has been trained for classification- it provides some Feature Importances based on the information that was gained when making splits on each variable. So technically yes, you can train your Random Forest on the full data and then retrain it only on the important variables. Given the interaction between the features, however, it would be best to do so in a stepwise fashion, removing only one feature at the time. Also, it is important to be wary of RF feature importances , as they can be quite misleading. In particular, they can be strongly affected by correlated features, and they have a strong bias towards numerical or high cardinality features.
