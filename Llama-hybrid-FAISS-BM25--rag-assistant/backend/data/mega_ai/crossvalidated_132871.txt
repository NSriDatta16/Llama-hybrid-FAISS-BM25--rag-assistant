[site]: crossvalidated
[post_id]: 132871
[parent_id]: 
[tags]: 
How does machine learning handle the vast number of words in NLP application

Disclaimer: I'm no expert in the field, this question could be axiomatically flawed. When programming a very basic regression based machine learning algorithm one uses a number of variables; it seems to me that when applying this to natural language processing the number of variables would be so large that the training set would be unreasonable. I've heard that a good rule of thumb for training set size is 5 - 15 times the number of variables - even with a relatively simple application that's many thousands of words - many millions of 2 word 'n-grams'. Overall question: How would one apply machine learning when one of the inputs is a body of text?
