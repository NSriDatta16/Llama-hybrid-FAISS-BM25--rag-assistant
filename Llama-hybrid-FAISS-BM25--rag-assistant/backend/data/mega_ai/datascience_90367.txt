[site]: datascience
[post_id]: 90367
[parent_id]: 67781
[tags]: 
A good way to understand and intuitively comprehend the concept of vanishing gradients and exploding gradient would be through manually solve through a backpropagation. Since, Feed Forward Neural Network is simplest of all and Mostly sigmoid function and Tanh suffers from vanishing gradient . It would be wise to build a MLP with at least one hidden layer and compute the change in parameter values after forward pass , error calculation and backward pass to update weights and biases initialized randomly. https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ Likewise, RNN are mostly suffering from exploding gradient you could apply same method. It may seems far fetched to go to this trouble for understanding concepts , but it is worth your time.
