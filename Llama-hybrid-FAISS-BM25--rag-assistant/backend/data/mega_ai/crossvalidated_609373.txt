[site]: crossvalidated
[post_id]: 609373
[parent_id]: 609166
[tags]: 
Likelihood is a slippery concept. The likelihood function, $L(t | w)$ , expresses how probable the data $t_n$ are in relation to the model function $y(x,w)$ . The uncertainty in the empirical measurements enters the modeling through the misfits $ε_n$ ; one misfit for each data point. We introduce a misfit probability distribution, usually a Gaussian. The values of the misfits have no specific meaning. But the introduction of the misfit probability distribution has changed our state of knowledge in a rather fundamental way. As data scientists, we prefer models with small misfits and look suspiciously at large outliers. The misfits $ε_n$ are now subjected to an overarching probability distribution $p(ε_n)$ , which “connects” the previously unrelated individual misfit values by their probabilities. Conceptually, this metamorphosis is a big step. The likelihood value is the product of the $p(ε_n)$ . The likelihood function is the generalization of the likelihood value. The likelihood function is the product of the misfit probability densities but with the dependency on the coefficient $w$ of the model function $y(x,w)$ taken into account. Technically, the likelihood function is not a normalized probability distribution over $w$ . It is a factor in Bayes' theorem, and does not necessarily integrate to unity. $$\int L(t | w) dw \ne 1.$$ However, it is a normalized probability distribution when integrated over the data values $t$ $$\int L(t | w) dt = 1.$$ Which makes it confusing. The above texts are excerpts from my new tutorial book "What is your model?, a Bayesian tutorial". It is a short self-study book on Bayesian data analysis. One can read 80% for free on Amazon under the Look Inside feature , thereby avoiding bad buys.
