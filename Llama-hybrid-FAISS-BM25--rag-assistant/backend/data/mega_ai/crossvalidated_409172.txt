[site]: crossvalidated
[post_id]: 409172
[parent_id]: 271198
[tags]: 
I think CNNs are often talked about as putting squares on top of bigger squares with the "neural network" aspect hidden. They're definitely neural networks and can be drawn out. Apply the filter to the upper left 2x2 array. Apply the filter to the upper right 2x2 array. Apply the filter to the bottom left 2x2 array. Apply the filter to the bottom right 2x2 array. Here is the entire layer, with the 3x3 input image mapping to four neurons for the four positions in the image where convolution occurs. You can draw those four neurons in a square array if you plan to do additional convolutional layers. That doesn't make so much sense with a 2x2 output, but you're probably working with images that are bigger than 3x3. I think that it's a useful exercise to draw out a simple example like this. Another useful exercise is to predict how many parameters there will be in a layer of 3x3 filtering (10 filters) over a 5x5 image. (The answer is 100: 9 for each of the ten filters, plus one bias term per filter, but bias terms typically get ignored when we draw out the "web" of a neural network. (However, bias terms are important!) Now try it for 3x4 filtering (20 filters) over a 7x5 image!) To compare with a fully-connected layer, the fully-connected layer would have each pixel linked to each of the four neurons in the next layer, and each of those $36$ connections would have its own color to indicate the connection having its own weight, rather than sharing weights as is done in a convolutional layer.
