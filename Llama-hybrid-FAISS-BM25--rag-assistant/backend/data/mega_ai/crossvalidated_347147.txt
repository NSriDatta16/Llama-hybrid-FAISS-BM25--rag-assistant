[site]: crossvalidated
[post_id]: 347147
[parent_id]: 27228
[tags]: 
There are two main ways to do this, but first I’ll challenge the idea that you want to pick only one. Most likely, an ensemble model of the three separate models will achieve the best performance of all. The main, perhaps best, way to do it is to use the model to obtain confidence intervals around the evaluation metric. This is commonly done via bootstrapping ( or Poisson bootstrap ). The other way is to use a statistical test. Every test makes different assumptions, and these are often used to compare a value or sample taken from a distribution rather than a single point evaluation. Many of these statistical tests formally require independence, which you usually don’t have when comparing multiple results of the same model or multiple models over time series data. With time series prediction specifically, you should be doing backtesting with cross-validation and evaluating train and test error at each time ( example ). When you do this, I doubt your models will all perform so similarly that you need a statistical test to differentiate; most likely, you’ll see large differences. Note also that historical evaluation metrics (comparing actuals to forecast) alone are insufficient for prediction evaluation. Given two predictions that fit known historical data perfectly but one also matches prior beliefs about the future and the other clearly violates (e.g., if one vanishes to zero but you have reason to believe that can’t happen), you’ll prefer the prediction that better matches your prior.
