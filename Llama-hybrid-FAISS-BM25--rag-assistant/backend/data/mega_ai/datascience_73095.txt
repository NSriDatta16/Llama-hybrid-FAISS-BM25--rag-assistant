[site]: datascience
[post_id]: 73095
[parent_id]: 37355
[tags]: 
Just from the probabilistic side of things: When the classes are not mutually exclusive, the events $x^j=C_i$ are not disjoint, so in general (if every example gets some label), $$1 = P(x^j=C_0 \vee \dotsb \vee x^j=C_t) \lneq \sum_i P(x^j=C_i).$$ That is, the answer to the value can be larger than 1 but can a probability be larger 1? is "the value (of the sum) is not a probability, it's a sum of probabilities." Is a non-mutual classification really a common case? Can somebody may be linking some cases (paper?) were they needed non-mutual classification? This is commonly known as "multi-label classification." Examples include topics/genres/themes, where a given item may include more than one. I'll defer to the question you linked to provide other links. Note that the probability being at least 1 above relied on every item receiving at least one label. In cases where that's not the case, the sum may be less than 1. And the outputs of a neural network (when individually sigmoid-activated) may sum to less than 1 even when every item receives a label, from imperfect calibration. Finally, I'd just like to say that the notation $x^j=C_0,\ x^j\neq C_i\setminus C_0$ seems off to me; using $\in$ and $\notin$ seem better.
