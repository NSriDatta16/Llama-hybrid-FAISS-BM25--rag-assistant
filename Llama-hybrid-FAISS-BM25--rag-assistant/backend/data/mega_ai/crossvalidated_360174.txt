[site]: crossvalidated
[post_id]: 360174
[parent_id]: 359785
[tags]: 
First suggestion is the coefficients you have used are very large. They correspond to odds ratios of 55 and 20. Unless you have specific reasons for doing this, $\ln(4)$ and $\ln(3)$ are probably more appropriate arbitrary choices. The problem with your data simulation is you are only specifying the predicted probability when $x_1$ and $x_2$ are zero. This is easy to verify. Run your logistic regression model, and apply plogis() (inverse logit function) to the intercept coefficient. You will arrive at .25. The issue is the transformation from logits to probabilities is non-linear. In your example, $$.25 = (1+e^{-\bar{z}})^{-1} \neq \mathrm{E}\big[(1+e^{-z})^{-1}\big]$$ as you observed. I might approach your example problem using the latent variable approach. In this model, we assume: \begin{equation} y^* = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon \quad\mathrm{and}\quad y = \begin{cases} 1, & \mathrm{if}\ y^* > 0\\ 0, & \mathrm{otherwise} \end{cases} \end{equation} where $\epsilon \sim L(0, 1)$ , $x_1\sim N(0,1)$ , and $x_2\sim N(0,1)$ keeping with your approach. With this approach, we can readily manipulate $\mathrm{E}[y]$ by changing the intercept. So we can re-write the equation as: \begin{equation} y = \begin{cases} 1, & \mathrm{if}\ \beta_1 x_1 + \beta_2 x_2 + \epsilon > -\beta_0\\ 0, & \mathrm{otherwise} \end{cases} \end{equation} Since the equation is additive on the logit scale, we can assume the quantity, $\beta_1 x_1 + \beta_2 x_2 + \epsilon$ , converges to normality. The variance of such a distribution would be: $\beta_1^2 \times 1^2 + \beta_2^2 \times 1^2 + \pi^2/3$ , $\pi^2/3$ is the variance of $L(0, 1)$ . If we want the proportion of 1's to be 25%, we can set $-\beta_0$ to the 75th percentile of a normal distribution with the variance above. The mean of this distribution would be: $\beta_1\times \bar{x}_1 + \beta_2 \times \bar{x}_2$ . In our example, that would be 0. So $-\beta_0$ would be the 75% percentile of $N(0, \beta_1^2 \times 1^2 + \beta_2^2 \times 1^2 + \pi^2/3)$ . Following my own recommendation, I use $\ln(4)$ and $\ln(3)$ as $\beta_1$ and $\beta_2$ . In R syntax, this would be: set.seed(666) b0 Here's a more complicated example to see if it still works: \begin{equation} y = \begin{cases} 1, & \mathrm{if}\ x_1 + 1.5 \times x_2 + .25 \times x_3 + \epsilon > -\beta_0\\ 0, & \mathrm{otherwise} \end{cases} \end{equation} where $x_1\sim N(1, 1)$ , $x_2 \sim N(0, 1)$ and $x_3 \sim \mathrm{Bern}(.5)$ . In this example, I want the mean of $y$ to be 10%. So $-\beta_0$ would be the 90% percentile of $N(1 + .25 \times .5, 1 + 1.5^2 + .25 \times .5 ^ 2 + \pi^2/3)$ : b0 I hope this helps. I have tested it a couple of times and it appears to do a decent job. This is one approach I attempted that worked not so badly. EDIT: The normal approximation requires that there are many additive features with none having an inordinate influence on the latent variable relative to others. If you simulate a large number of variables, then it works relatively well. set.seed(12345) mean(replicate(10000, { X
