[site]: crossvalidated
[post_id]: 487911
[parent_id]: 486608
[tags]: 
Intro Yes this is correct. Since you asked for a complete answer, I'll start by setting up notation and establishing preliminaries. Notation It sounds like your goal is to understand the relationship between driving at night and crashing a car. Let's denote the binary dependent variable of whether a car crash occurred as $y = \{0,1\}$ , and the binary independent variable of driving at night as $x= \{0,1\}$ . Furthermore, we'll denote the probability $P[y|x] = p(x)$ . We'll estimate $p(x)$ using a logistic regression As your sources note the motivation of logistic regression is a linear model for the log-odds: $$ \log\left[\frac{p(x)}{1-p(x)}\right] = \alpha + \beta x $$ Since $x$ has only two levels, we can make this notation a little simpler by defining $\beta_0 = \alpha$ (the log-odds of a crash during the day) and $\beta_1 = \alpha + \beta$ (the log-odds of a crash at night). It will also help if we define the logit function : $$ \text{logit}(z) = \log\left[\frac{z}{1-z}\right] $$ Which allows us to easily write: $$ p(x) = \begin{cases} \text{logit}^{-1}(\beta_0) & x=0\\ \text{logit}^{-1}(\beta_1) & x=1\\ \end{cases} $$ Posteriors and Priors In the Bayesian methodology this model would be fit to the datapoints $(x_1,y_1),...,(x_n,y_n)$ by looking at the posterior distribution : $$ P[\beta_0,\beta_1|x_1,...,x_n,y_1,...,y_n] = \prod\limits_{i=1}^n p(x_i)^{y_i} (1-p(x_i))^{1-y_i} P[\beta_0,\beta_1] $$ Where $P[\beta_0,\beta_]$ is the prior distribution over the parameters, typically assumed to have prior independence in the parameters: $$ P[\beta_0,\beta_1] = P[\beta_0] P[\beta_1] $$ Answer The paper has provided you the 95% quantiles, mean, and standard deviation of the posterior distribution of the value $\text{logit}(p(1)) = \beta_1$ . Say the mean here is $m_1$ and the standard deviation is $s_1$ . A standard result in Bayesian analysis is that, with sufficiently many datapoints, the posterior distribution is approximately normal (the Laplace approximation). Thus $m_1$ and $s_1$ are sufficient to characterize the posterior distribution (approximately), and it is the normal distribution $N(m_1,s_1)$ . In general, variance is standard deviation squared, so an alternate parameterization of their posterior/your prior would be the normal distribution $N(m_1,s_1^2)$ , which is what you have here: $\beta_k = (1.12,.04)$ PS Note that the variance of the prior equaling $.04 = .02^2$ is not unique to log-odds. For any distribution, variance equals standard deviation squared (this is just the definition of standard deviation). Your source 3 is actually providing a proof of the Laplace Approximation, ie. the fact that the previous posterior is approximately normal. In general you will want to also perform a sensitivity analysis on your choice of prior. $N(1.12,.04)$ is very tight around a rather large value of $m_1$ (it implies that the probability of crashing at night is like ~75%). It would be smart to re-run your analysis with multiple priors with increasing variances, to see what happens to your results when you loosen up your prior confidence.
