[site]: datascience
[post_id]: 103511
[parent_id]: 
[tags]: 
data.iterrows() + plt.imread() alternative, really exhausting

Currently, I'm loading my images for the training model with TensorFlow, like this: for index, datum in data.iterrows(): sat_name = directory_path+datum['sat_image_path'] sat_img = plt.imread(sat_name) sat = cv.resize(sat_img,(SIZE,SIZE)) tf_data['sat'].append(sat) print("Progress: ",(100*index)/data.size,"%") clear_output(wait=True) I know this is not the best way to do it. But this is what I got. This data is DataFrame , swhich holds all the image-ids and their relative paths. And this seems, pretty slow, it takes like 50 minutes in Colab, to load 12k images. And when feeding to my model, I do something like this: unet.fit( np.array(tf_data['sat']), ... Can anyone suggest to me a better way to do it, more faster and elegant and surely updated way?
