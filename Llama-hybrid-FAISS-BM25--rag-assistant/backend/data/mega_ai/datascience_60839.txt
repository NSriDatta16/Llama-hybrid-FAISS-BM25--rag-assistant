[site]: datascience
[post_id]: 60839
[parent_id]: 60827
[tags]: 
Concistency of any algorithm in machine learning or statistics rather means that assuming you train on an infinite amount of data that your algorithm will converge to the true value of your estimate. Meaning that if you feed infinite datasets into the algorithm your error will converge to 0 Now regarding 1NN: 1NN has an important property: When feeding infinite training examples the error rate of the 1NN converges to a limit of twice the Bayes error. Now if you take the definition above and match the 1NN's property you'll see the reason why it's inconsistent. See his paper for a proof of the property: http://cseweb.ucsd.edu/~elkan/151/nearestn.pdf
