[site]: datascience
[post_id]: 94242
[parent_id]: 94232
[tags]: 
The goal of activation functions is to introduce non-linearity in the neural network, so it can learn beyond linear relations between input and outputs. As stated in your image, each neuron output is a linear combination of inputs + a bias, and then all this is sent into a non-linear function (activation function). This allow the network to learn complex relations between input ans outputs. Most used activation function is ReLU, which is not a probability as it can return a value greater than 1.
