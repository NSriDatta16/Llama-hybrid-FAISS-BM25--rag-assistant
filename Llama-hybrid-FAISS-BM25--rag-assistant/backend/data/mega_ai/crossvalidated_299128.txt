[site]: crossvalidated
[post_id]: 299128
[parent_id]: 298131
[tags]: 
Let say the you have a data set D and hypothesis class H. You can use D to find the subset of H that is consistent with it (assume prefect consistency fro simplicity). The will enable you to learn P(h|D), the likelihood of hypothesis h given the data. Of course, you can also use P(h), the probability of the hypothesis. There you can use prior assumptions on properties of the data, MDL, and so on. Now let's consider that you simulate some data S. You will make some assumptions on the data in order to decide how to simulate it. If you could make these assumptions formal and clear, you could just add them into P(h). Since your simulation will be a result of these assumptions and D, consistency with S, P(h|S), shouldn't give you more information over P(h) and P(h|D). This argument shouldn't be taken as a justification of never using simulated data. In computer vision it is very common to generate samples using rotation, splitting and other operations on the original image. We know that if there is a cat in a picture we can consider a slightly rotated picture to have a cat too. However, since we don't have clear and formal ways to characterize picture with cats, we do benefit from the simulated data. In the field of NLP and sentence classification, it is usually hard to define manipulations of the data that will respect the concept of interest. If you can find such manipulations, you might benefit since this is another area in which it is hard to define well the hypothesis prior.
