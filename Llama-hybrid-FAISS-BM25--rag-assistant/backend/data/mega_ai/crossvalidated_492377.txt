[site]: crossvalidated
[post_id]: 492377
[parent_id]: 
[tags]: 
FInding stationary points of Logistic regression with cross-entropy loss

Let's say that I want to find the stationary points of the Cross-Entropy Loss function when using a logistic regression The 1 D logistc function is given by : \begin{equation}\label{eq2} \begin{split} \sigma(wx) = \frac{1}{1+\exp{(-wx)}} \end{split} \end{equation} and the cross entropy loss is given by : \begin{equation}\label{eq3} \begin{split} \textbf{L}(wx) = -y \log{(\sigma(wx))} - (1-y) \log{(1-\sigma(wx))} \end{split} \end{equation} When I simplify and differentiate and equal to 0, I find the following: \begin{equation}\label{eq7} \begin{split} \frac{d\textbf{L}}{dw} &= (1-y)x - \frac{xe^{-wx} }{1+e^{-wx}} = 0\\ (x-xy)*(1+e^{-wx}) &=xe^{-wx} \\ (1-y)(1+e^{-wx}) &= e^{-wx}\\ 1 +e^{-wx} -y- ye^{-wx} &= e^{-wx}\\ 1-y - ye^{-wx}& = 0\\ 1-y & = ye^{-wx}\\ \frac{(1-y)}{y} &= e^{-wx}\\ w &= - \frac{\log(\frac{(1-y)}{y})}{x} \end{split} \end{equation} However, this is very weird and strongly feels very wrong: First x cannot be equal to 0 Second, y cannot be equal to 0 Third, y cannot be equal to 1 1 and 0 are the only values that y takes in a cross-entropy loss, based on my knowledge. I am not sure where I left the right track. I know that cross-entropy loss means there are 2 loss (one for each value of y) but I am not sure if that plays in the steps and if yes, how? Could you please help me? Thanks in advance
