[site]: datascience
[post_id]: 33639
[parent_id]: 
[tags]: 
Terminology - cross-validation, testing and validation set for classification task

Confusion1) If k=10 then does this mean that 90% is for training and 10% for testing? So always we have k% for testing? Confusion2) In the following code I have used 10-fold cross-validation for training a Support Vector Machine (SVM). In general a data set will be split into (a) Training set, meas(trainIdx,:) (b) Testing set, meas(testIdx,:) c) Validation set. In the cross-validation approach I am building the SVM learner by training and validating inside the loop. Based on my understanding, the validation data must be completely different from the training and testing. But, in many online resources it is said that after cross-validation, one must re-train on the entire data set which in this example would be the meas(:,1:end) . If so, then the learned model svmModel inside the cross-validation is lost. Have I misunderstood completely wrong? Can somebody please show what is the next step in the classification once the cross-validation is over?
