[site]: crossvalidated
[post_id]: 470772
[parent_id]: 9311
[tags]: 
KS test and KL divergence test both are used to find the difference between two distributions KS test is statistical-based and KL divergence is information theory-based But the one major diff between KL and KS test, and why KL is more popular in machine learning is because the formulation for KL divergence is differentiable. And for solving optimization problems in machine learning we need a function to be differentiable. In the context of machine learning, KL_dist(P||Q) is often called the information gain achieved if Q is used instead of P links: https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test
