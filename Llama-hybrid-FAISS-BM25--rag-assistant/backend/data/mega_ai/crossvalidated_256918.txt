[site]: crossvalidated
[post_id]: 256918
[parent_id]: 
[tags]: 
R: gam() with smoothing splines (10-fold cross-validation, multiple predictors)

Good day, I have some difficulty writing up proper code where regression is applied to some data. I am following the book Introduction to Statistical Learning & Prediction by Trevor Hastie & Robert Tibshirani, but I can not quite follow their way of backfitting as how they call it. The issue is that I want to fit a gam() with smoothing splines to my own data, I have 20 predictors and 1 response variable (y). There is no GAM comparison present, only one model will be applied to the data. In the book, when they use gam() they specify degrees of freedom only for the quantitative variables in their code (they dummify the qualitative ones -- however R does this automatically "under-the-hood" for factors right? ) : gam.m3 = gam( wage ∼ s( year , 4) + s( age , 5) + education , data = Wage) I am not sure how they computed the specific values in the book, because they are talking about effective degrees of freedom found by using a tuning parameter (λ) selected by cross-validation. They go through it quite fast so there isn't a lot of code that I can check, let alone one for multiple predictors (example above is from p.293) Trial and error is getting me little, I think I understand what is needed but I can't funnel this in to R (or aRgh at the moment). In order to run the gam() correctly I need the effective degrees of freedom (obtained from optimal λ using CV), with added smoothing splines, s(), in its function. A smoothing spline is basically a natural cubic spline at every unique level of x(i). The tuning parameter λ controls the roughness of the smoothing spline, and hence the effective degrees of freedom which I have to add in the gam(), if I recall correctly . Playing around with the package (mcgv), or adding the argument method = "GCV.Cp" to the gam() function didn't help much. I know the smooth.splines() function has a built-in CV argument, but given that there are more predictors than one I'd like to include it in the gam() function. Can anyone help me out on how to write up (pass gam() arguments) and find an optimal value for λ, in order to fit my GAM to the data with the found df? tl;dr Help writing up and finding the effective degrees of freedom obtained from 10-CV (λ) for GAM (within gam() if possible or mcgv package) Thanks in advance. My code below is what I did for logistic regression with LASSO penalty for informative purposes on how I did my 10-CVs (if applicable). nFolds = 10 CVfolds = sample( rep( 1:nFolds , length=n ) , n ) cv_errors_lasso = matrix( -9999 , 1 , nFolds ) nFolds_inner = 10 for( fold in 1:nFolds ) { xtrain = x[ CVfolds!=fold , ] # Train is everything that isn't the fold itself ytrain = y[ CVfolds!=fold ] xtest = x[ CVfolds==fold , ] # Test is the current fold (k) being used ytest = y[ CVfolds==fold ] cv_lasso = cv.glmnet( xtrain , ytrain , alpha = 1 , nfolds = nFolds_inner , family = c( "binomial" ) ) cv_lasso_train = glmnet( xtrain , ytrain , alpha = 1 , lambda = cv_lasso$lambda.min ) predict_lasso_train = predict( cv_lasso_train , xtest ) cv_errors_lasso[ fold ] = sqrt( apply( ( ytest - predict_lasso_train )^2 , 2 , mean ) ) print( fold ) }
