[site]: datascience
[post_id]: 123382
[parent_id]: 87906
[tags]: 
I agree with Noe's third explanation "It is to make the positional encoding relatively smaller. This means the original meaning in the embedding vector wonâ€™t be lost when we add them together.". I find a reference book ( Dive into Deep Learning-Section 11.7.4 ) mentions this as "In the following Transformer encoder implementation, we stack num_blks instances of the above TransformerEncoderBlock classes. Since we use the fixed positional encoding whose values are always between -1 and 1 , we multiply values of the learnable input embeddings by the square root of the embedding dimension to rescale before summing up the input embedding and the positional encoding."
