[site]: crossvalidated
[post_id]: 478858
[parent_id]: 478835
[tags]: 
When you ask about "the t-statistic", I think about the concrete quantity $$\frac {{\bar {X}}-\mu }{S/{\sqrt {n}}}$$ To actually calculate this quantity, we have to specify $\mu$ . This is typically chosen in reference to some given null hypothesis. So to me it seems awkward to try to disentangle "the statistic" from the null hypothesis to which it is implicitly linked by $\mu$ . Setting $\mu$ to 0, for example, which you're implicitly doing when you type t.test(rnorm(10))$statistic into R, is implicitly related to the hypothesis test $H_0: \mu = 0$ . Where I think of Student's t-distribution as useful is as a parametric form for fitting to data. At the end of the day, it's just another symmetric, bell-shaped distribution. It just has fatter tails than a Gaussian. So it can be used to model things for which you'd like to preserve that symmetry and bell-shape, but give the extreme outcomes more probability mass than a Gaussian does. I know it's used in finance to model asset-returns ( link 1 , link 2 ) for example, but I can't speak to how successful or useful these kinds of models are because I don't use them myself. I'd suspect them to be of particular use to hierarchical modelers who have some prior knowledge that points to fat tails. Gelman briefly discusses using the t instead of the the Gaussian in fat-tail situations in section 17.2 of Bayesian Data Analysis.
