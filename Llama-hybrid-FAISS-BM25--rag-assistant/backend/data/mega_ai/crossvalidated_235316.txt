[site]: crossvalidated
[post_id]: 235316
[parent_id]: 93705
[tags]: 
Google has published Inception-v3 . It's a Neural Network (NN) for image classification algorithm (telling a cat from a dog). In the paper they talk about the current state of image classification For example, GoogleNet employed only 5 million parameters, which represented a 12x reduction with respect to its predecessor AlexNet, which used 60 million parameters.Furthermore, VGGNet employed about 3x more parameters than AlexNet and that is basically why we call NN for black boxes. If I train an image classification model - with 10 million parameters - and hand it to you. What can you do with it? You can certainly run it and classify images. It will work great! But you can not answer any of the following questions by studying all the weights, biases and network structure. Can this network tell a Husky from a Poodle? Which objects are easy to classify for the algorithm, which are difficult? Which part of a dog is the most important for being able to classify it correctly? The tail or the foot? If I photoshop a cats head on a dog, what happens, and why? You can maybe answer the questions by just running the NN and see the result (black box), but you have no change of understanding why it is behaving the way it does in edge cases.
