[site]: datascience
[post_id]: 55779
[parent_id]: 55774
[tags]: 
The interpretation of the output depends not only on the architecture of the network, but also on the final-layer activation functions and the training procedure. Most importantly, training a neural net requires you to choose a loss function, which describes how far off the predictions in the final layer are from ground truth. If you can specify a sensible loss function, then you've implicitly defined how the output layer is to be interpreted. Offhand, I can't think of a classification problem where it would be helpful to have more output neurons than labels (but maybe someone else is more creative!) One kind-of similar case are the advantage actor-critic networks commonly used in reinforcement learning. The ultimate goal of a reinforcement-learning agent is to choose an action from a set of $n$ possible actions, so traditionally we might try a network with $n$ outputs. Actor critic methods actually have $n+1$ outputs. The first $n$ choose an action, and the "extra" neuron tries to estimate the value of the chosen action.
