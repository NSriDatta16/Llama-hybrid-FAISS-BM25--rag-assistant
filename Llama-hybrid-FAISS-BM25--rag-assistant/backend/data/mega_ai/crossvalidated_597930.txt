[site]: crossvalidated
[post_id]: 597930
[parent_id]: 596862
[tags]: 
As @Kuku points out, this pattern in the fitted value stems from a feature in the raw test scores: there is a quartet of high performing students with average test scores well above the rest of the class. A similar thing occurs at the bottom; however, the two students with the lowest scores don't perform much worse than the majority, so we don't see a big "gap" in the fitted values. It's easiest to see these patterns by making our own residuals plots in order to highlight points of interest. The R code to reproduce the figures is attached at the end. p13, p56, p57 & p60 are the students with the highest scores. They do quite a bit better than the rest of the class. We see this clearly if we plot the fitted scores against the raw scores. Importantly, the model predicts the average score of over/under-achievers well but it under-predicts the within-student variation in test scores. Take for example student p60: the model predicts about the same score on all three tests (79, 78.5, 78.4) while in reality this student did particular well on the control test (91, 78.5, 77.4). This is a property of the model itself, so it might be relevant to understand. What are the fitted values $\widehat{y}_{j,t}$ for student $j$ on test $t$ ? $$ \begin{aligned} \widehat{y}_{j,\text{control}} &= \widehat{\alpha}_j + \widehat{\beta}_0 \\ \widehat{y}_{j,\text{testA}} &= \widehat{\alpha}_j + \widehat{\beta}_0 + \widehat{\beta}_A \\ \widehat{y}_{j,\text{testB}} &= \widehat{\alpha}_j + \widehat{\beta}_0 + \widehat{\beta}_B \end{aligned} $$ where $\alpha_j$ is the student's random effect, $\beta_0$ is the expected score on the control test and $\beta_A$ and $\beta_B$ are the expected difference in score (relative to the control) on tests A and B. The hats indicate that these are estimates of the model parameters. Here are the estimates for $\beta_0,\beta_A$ and $\beta_B$ : tidy(a, "fixed") #> effect term estimate std.error statistic #> 1 fixed (Intercept) 62.2 0.797 78.1 #> 2 fixed typetestA -0.672 0.708 -0.949 #> 3 fixed typetestB -0.516 0.708 -0.728 What we learn is that the predicted within-student variability is at most 0.672 (that's the difference between the control and A tests). And it is the same for every student! In practice there is more variability in the students' performance but the differences (between tests within a student) are not consistently in the same direction. That's why the improvement of test A and B compared to the control is only -0.7 and -0.5 points. (And this may be a good thing since the "improvement" is actually a decrease of about half a point on average.) Created on 2022-12-04 with reprex v2.0.2 library("broom.mixed") library("lme4") library("tidyverse") a % mutate( across(ID, as.character), # Standardize the residuals .std.resid = .resid / sigma(a) ) df_1a %>% filter( .fitted >= 70 ) %>% select( ID, SCORE, .fitted ) IDs_70plus = 70) %>% pull(ID) IDs_55minus % pull(ID) df_1a % mutate( label = if_else(ID %in% c(IDs_70plus, IDs_55minus), ID, "o"), color = case_when( ID %in% IDs_70plus ~ "a", ID %in% IDs_55minus ~ "b", TRUE ~ "c" ) ) make_plot % ggplot( aes(.fitted, {{ y }}) ) + geom_smooth( color = "#D9D9D9", se = FALSE ) + geom_text( aes( label = label, color = color ) ) + labs( x = "Fitted values", y = title.y, title = title ) + guides( color = "none" ) } make_plot( SCORE, "Test scores", "Fitted values against students' test scores" ) make_plot( sqrt(abs(.std.resid)), "sqrt( |Std. residuals| )", "Fitted values against standardized residuals" )
