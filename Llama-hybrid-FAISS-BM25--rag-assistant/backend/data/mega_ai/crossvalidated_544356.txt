[site]: crossvalidated
[post_id]: 544356
[parent_id]: 
[tags]: 
Removing duplicate training vectors?

As an extension to this question , for ML problems where it makes sense to remove duplicates (ie: identical data & target variables) from your distribution, in which scenarios would it (if at all) also make sense to de-duplicate the corresponding feature vectors before learning? For example: lets say we are classifying images of standalone digits. Since the goal is to learn the shape concept vs usage frequency of numbers, we desire a balanced yet unique dataset for each digit. After feature engineering (be it manual or deep learned), syntactically different images of conceptually the same digit may end up encoded with identical feature vectors. In the spirit of data balancing, since some techniques may randomly duplicate the minority classes while others may fiddle with their vectors to synthesize new data-points, is it wrong then to de-duplicate these vectors as well?
