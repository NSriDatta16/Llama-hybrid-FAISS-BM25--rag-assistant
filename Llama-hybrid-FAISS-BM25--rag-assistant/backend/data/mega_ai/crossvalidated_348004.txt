[site]: crossvalidated
[post_id]: 348004
[parent_id]: 348003
[tags]: 
This is described in the second chapter of the Feature Selection with the Boruta Package paper by Kursa and Rudnicki: Boruta algorithm is a wrapper built around the random forest classification algorithm [...] It is an ensemble method in which classification is performed by voting of multiple unbiased weak classifiers â€” decision trees. These trees are independently developed on different bagging samples of the training set. The importance measure of an attribute is obtained as the loss of accuracy of classification caused by the random permutation of attribute values between objects. It is computed separately for all trees in the forest which use a given attribute for classification. Then the average and standard deviation of the accuracy loss are computed. Alternatively, the $Z$ score computed by dividing the average loss by its standard deviation can be used as the importance measure. Unfortunately the $Z$ score is not directly related to the statistical significance of the feature importance returned by the random forest algorithm, since its distribution is not $N(0, 1)$ (Rudnicki, Kierczak, Koronacki, and Komorowski 2006). Nevertheless, in Boruta we use $Z$ score as the importance measure since it takes into account the fluctuations of the mean accuracy loss among trees in the forest.
