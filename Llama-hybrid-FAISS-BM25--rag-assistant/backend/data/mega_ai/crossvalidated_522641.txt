[site]: crossvalidated
[post_id]: 522641
[parent_id]: 522364
[tags]: 
When the treatment effect $\tau$ is constant across the confounders $X$ , i.e., the true outcome model can be written as $$ E[Y|X]=\beta_0 + \tau A + \beta_1 X_1 + \beta_2 X_2 + \dots $$ where $A$ is the treatment, the bias in a naive (unadjusted) estimate of $\tau$ is $$ E[\hat{\tau}] - \tau = \beta_1 (\bar{X_1}^1 - \bar{X_1}^0) + \beta_2 (\bar{X_2}^1 - \bar{X_2}^0) + \dots $$ where $\bar{X_1}^a$ is the mean of $X_1$ in treatment group $a$ . When you include $X_1$ in a regression of the outcome on the treatment, the bias due to $X_1$ is removed. This is the point of regression adjustment for estimating treatment effects in observational studies with fully observed confounders. The bias can also be eliminated by ensuring that $\bar{X_1}^1 - \bar{X_1}^0 = 0$ , e.g., by using matching or weighting. The point is that when the outcome model includes more than just linear terms, regression on the covariate main effects will not be sufficient. For example, if the true outcome model is $$ E[Y|X]=\beta_0 + \tau A + \beta_1 X_1 + \beta_2 X_1^2 + \dots $$ including just $X_1$ in the outcome regression will not eliminate the bias due to $X_1$ unless $\bar{X_1^2}^1 - \bar{X_1^2}^0 = 0$ . This is a motivation behind using matching or weighting to first remove covariate distribution differences in the treatment groups prior to running a regression. In the case above, the mean and variance of $X_1$ must be the same between the two groups for an unadjusted treatment effect to be unbiased, and the variance of $X_1$ must be the same between the two groups for a regression of the outcome on the treatment and $X_1$ to be unbiased. If we included $X_1$ and $X_1^2$ in the regression model, the bias would be removed, but because we never know the true form of the outcome model, we don't know exactly which terms would be required to remove the bias. So, when the outcome model is nonlinear, we can either adjust for the nonlinearities by including additional terms (e.g., $X_1^2$ ) in the outcome model or by ensuring that the corresponding moments of the covariate distribution (e.g., the variance of $X_1$ ) are balanced between the treatment groups. With many covariates and when the outcome model is arbitrarily complicated, it can be easier to remove differences between the covariate distributions by matching or weighting rather than by including all the terms in a regression model (though using machine learning methods in the outcome model can attempt to handle this as well). Including just the covariate main effect will be sufficient to remove bias when: The treatment effect is constant across the confounders The outcome model is linear in the covariates OR the distributions of covariates between the treatment groups differ only in the means It is impossible to prove the outcome model is linear in the covariates, so it is important for the distributions of the covariates to be the same between the treatment groups, which is why we use matching and weighting before or instead of regression adjustment.
