[site]: crossvalidated
[post_id]: 627279
[parent_id]: 
[tags]: 
Mean squared error (MSE) vs Least squares error (LSE)

From my understanding the only difference between MSE and LSE is that with MSE you divide the sum of squared errors by the total number of values to get an average rather than just using the sum. This means that when LSE is used as a loss function, the loss will scale up with the batch size. Isn't that pretty undesirable? Here is an example using batch size 1 vs batch size 128. Say we have a classification model with an output between 0-1 and it is really bad. The target value is 1 but it predicted a 0. Here the LSE = 1. Now the same situation with a batch of 128. (128 predictions of 0 and 128 target values of 1) The LSE = 128! This higher batch size scenario is using the same model, so it isn't 128 times worse, but the loss is 128 times higher. It seems like it would especially be a problem if your loss was made up of multiple different losses, each with different weighting, and one of them used LSE (cycle GAN is an example). You could find the right weighting with a given batch size, but then if you ever want to try another batch size wouldn't you have to recalculate the weighting? Whereas MSE you wouldn't? How could this ever be desirable? Why would you want to use LSE instead of MSE when training a neural network? (LSGAN for example) Edit: I am now way more confused after looking at several different pytorch implementations that use least squares and seeing they all use a mean, which is just MSE. Looking at the equation in the LSGAN paper I don't see anything about taking the mean, but in code implementations I do. def LSGAN_D(real, fake): return (torch.mean((real - 1)**2) + torch.mean(fake**2)) def LSGAN_G(fake): return torch.mean((fake - 1)**2) Isn't this literally just MSE?...
