[site]: crossvalidated
[post_id]: 636814
[parent_id]: 
[tags]: 
A/B Test from two distributions with unknown means

I will run a experiment to check if a given change, represent some significance in user interactions. I have two sets, sampled from two groups, one called control which counts the number of interactions from 30 sampled users in control group, and I have treatment which counts the number of interactions from 30 sampled users with a test feature. They are given in the Python code below import numpy as np control = np.array([1, 0, 1, 3, 2, 1, 0, 1, 3, 2, 1, 0, 1, 3, 2, 1, 0, 1, 3, 2, 1, 0, 1, 3, 2, 1, 0, 1, 3, 2]) treatment = np.array([0, 1, 3, 2, 1, 2, 1, 3, 2, 1, 0, 2, 3, 2, 1, 0, 2, 3, 2, 1, 0, 2, 3, 2, 1, 0, 2, 3, 2, 4]) For this test i will use the following parameters Practical Significant Boundary: $d_{min} = 0.05$ Significance level $\alpha = 0.05$ d_min = 0.05 alpha = 0.05 I want to check if in average there is no usage difference between the two groups. Then I state the null hypothesis as $H_0: \mu_t == \mu_c \implies d = 0$ where $\mu$ is the average of control group (c) or treatment group (t) and $d$ the difference $d = \mu_t - \mu_c$ . mu_c = control.mean() mu_t = treatment.mean() d = mu_t - mu_c By the characteristics of the data, I will use the t-test for evaluation. I will assume the variance is the same between both groups, that will give me the following computation to test statistics. n_c = len(control) # Number of samples in control n_t = len(treatment) # Number of samples in treatment df = n_c - 1 + n_t - 1 # Degree of freedom # squared sum of errors from control group SS_c = np.sum((control - mu_c)**2) # squared sum of errors from treatment group SS_t = np.sum((treatment - mu_t)**2) # Pooled standard error S_pool = np.sqrt((SS_c + SS_t)/df) # Test statistics for t-distribution TS = d / (S_pool * np.sqrt(1/n_c + 1/n_t)) Now I will verify if the test confirms or rejects the null hypothesis. For that I compute the t_alpha with 58 degrees of freedom, import scipy.stats as stats t_alpha = stats.t.ppf(1-alpha/2, df) TS > t_alpha or TS Since TS is out of the boundaries of t_alpha we can reject the null hypothesis . Now I will verify if the result is practically significant, that is if my confidence interval including the standard error intersects somehow the significance region. For that I need to estimate the margin error ( $m$ ), then compute the confidence interval from $d \pm m$ m = t_alpha * S_pool ci = np.array([d - m, d + m]) Just for reminder d is the difference between the averages from two groups (control and treatment). To visualize we can build the given plot. import matplotlib.pyplot as plt fig = plt.figure(figsize=(10, 3)) ax = fig.add_subplot(111) ax.vlines(0, -1, 1) ax.vlines([-d_min, d_min], -1, 1, linestyles='dashed', label="Significant boundary"); ax.plot(d, 0,'b.', label="Center Confidence Interval"); ax.hlines(0, *ci, 'b', label="Confidence Interval"); ax.set_title("Significance"); plt.legend(); Since the confidence interval overlaps with the significant boundary (totally), even cross the $0$ , I cannot assume that this test has practical significance, hence I would reject it. The problem is : I was not expecting an interval SO large. Probably I just have used a wrong formula for margin error, but it seems correct. Any help to go forward in this is welcome.
