[site]: stackoverflow
[post_id]: 1495424
[parent_id]: 1495363
[tags]: 
As Paul said a lot of robots.txt interpreters are not too bright and might not interpret wild-cards in the path as you intend to use them. That said, some crawlers try to skip dynamic pages on their own, worrying they might get caught in infinite loops on links with varying urls. I am assuming you are asking this question because you face a courageous crawler who is trying hard to access those dynamic paths. If you have issues with specific crawlers, you can try to investigate specifically how that crawler works by searching its robots.txt capacity and specifying a specific robots.txt section for it. If you generally just want to disallow such access to your dynamic pages, you might want to rethink your robots.txt design. More often than not, dynamic parameter handling "pages" are under a specific directory or a specific set of directories. This is why it is normally very simple to simply Disallow: /cgi-bin or /app and be done with it. In your case you seem to have mapped the root to an area that handles parameters. You might want to reverse the logic of robots.txt and say something like: User-agent: * Allow: /index.html Allow: /offices Allow: /static Disallow: / This way your Allow list will override your Disallow list by adding specifically what crawlers should index. Note not all crawlers are created equal and you may want to refine that robots.txt at a later time adding a specific section for any crawler that still misbehaves.
