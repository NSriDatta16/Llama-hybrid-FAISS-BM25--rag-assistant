[site]: crossvalidated
[post_id]: 566973
[parent_id]: 566529
[tags]: 
I am assuming (without going to deal with python functions) that your post is in fact asking: Is covariance matrix $\bf\hat R$ reproduced by Factor analysis of a sample covariance matrix $\bf R$ a better (closer) estimate of the population (" t rue") covariance matrix $\bf R_t$ than $\bf R$ is? That is, is $\bf ||R_t-\hat R||$ smaller than $\bf ||R_t-R||$ , where $\bf R$ is a random realization of $\bf R_t$ for some sample size $n$ ? My answer would be yes . I've conducted (in SPSS) a series of simulations, each one randomly creating some population factor loading p x m matrix $\bf A_t$ (it is what you call W); $p$ variables = 50 and $m$ factors = 5. According to the factor theorem ( footnote 1 ), this yields us the corresponding population covariance matrix $\bf R_t = A_tA_t^{'}+\text{diag}(u_t{^2})$ , $\bf u_t{^2}$ being the p -length vector of some uniquenesses, also randomly generated. So, this $\bf R_t$ is the m common factor covariance matrix. For each such population $\bf R_t$ , I generated 1000 sample covariance matrices $\bf R$ under normal population assumption and the sample size, say, $n$ =200 observations. I did not actually generated the data; rather, I used Wishart distribution to get $\bf R$ s (you did equivalently by actually generating the data sample according to the factor model and then computing covariance matrix of your variables). On each $\bf R$ , I performed Factor analysis (like you did it on your sample data), by Principal Axis factoring method with initial communalities estimated as the images (this is the usual way), extracting precisely $m$ factors. The obtained loadings $\bf A$ then yielded the reproduced covariance matrix $\bf \hat R = AA^{'}$ with the final communalities on the diagonal. But we need the full, nonreduced matrix, so the diagonal was then replaced by the diagonal of $\bf R$ . (Thus, diagonals of $\bf R$ and $\bf \hat R$ are equal.) Departures $\bf ||R_t-\hat R||$ as well as $\bf ||R_t-R||$ were recorded for each sample, and their difference $\bf ||R_t-\hat R|| - ||R_t-R||$ computed. This difference averaged over the 1000 samples was negative, - and so was observed in every simulation of population factor structure $\bf R_t$ . Moreover, the difference was negative in all individual samples. We may conclude that Factor analysis of a sample covariance matrix (or sample data) yields an estimate of the population (true) covariances which is (always) a more accurate estimate than the sample covariance matrix itself. Of course, provided you are extracting in your FA the true number of factors $m$ (but who will tell you the true $m$ for certain, in practice, when all what you have is just one sample of data sized $n$ , and n is not very large?) [Note: the difference $\bf ||R_t-\hat R|| - ||R_t-R||$ is non zero only for off-diagonal covariances; since diagonals of $\bf R$ and $\bf \hat R$ are equal - see above.] I've conducted the alike simulation series also with Principal Component analysis in place of FA, to extract factors and get the $\bf \hat R$ . The difference $\bf ||R_t-\hat R|| - ||R_t-R||$ now was positive, not in every sample though, but in the great majority of samples; and positive overall (averaged). We may say that PCA of a sample covariance matrix (or sample data) yields an estimate of the population (true) covariances which is (most of the time) a less accurate estimate than the sample covariance matrix itself. But PCA can approach FA as a latent structure discoverer under certain conditions. More comparison of FA and PCA with an extensive simultion study can be found in this big thread , particularly in my answer .
