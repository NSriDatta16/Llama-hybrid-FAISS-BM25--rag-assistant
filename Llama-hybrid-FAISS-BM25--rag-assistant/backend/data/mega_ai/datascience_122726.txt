[site]: datascience
[post_id]: 122726
[parent_id]: 
[tags]: 
Transformers for time series - what is the role of the encoder?

I am trying to become more familiar with transformers by applying them to simple sequences like uni-variate time series. For now I am reading the papers. One of the important parts of transformers seems to be that attention mechanism compares every point in the prior series to every other prior point. That makes sense. What makes less sense is why one needs a decoder, in general? For example, here is the architecture of the Informer model ( https://arxiv.org/abs/2012.07436 ) Is the encoder-decoder needed here because the model is trained to be general enough to work for many different time-series? Thus input into the encoder is needed to bias the model to work for a specific type of time series during the application? What if I wanted to train a model of only one type of time series? Could I not remove the encoder part and aim to get all the knowledge of the time series to be implanted into the decoder weights via training? Thanks
