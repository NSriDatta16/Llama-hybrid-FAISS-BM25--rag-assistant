[site]: crossvalidated
[post_id]: 52418
[parent_id]: 52395
[tags]: 
One can proof that in the limit of infinite data, both estimates converge. Let us consider the case of regression, where you assume that the target data is generated from a smooth function width additive Gaussian noise. Then you have for the likelihood of your training data, $$p(D|\mathbf{w}) = \prod_{n} p(t_{n}|\mathbf{x_{n}},\mathbf{w}) = \prod_{n}\exp \left(\frac{\beta}{2} \left[t_{n}- y(\mathbf{x_{n}},\mathbf{w}) \right]^{2}\right)/Z_{D}(\beta)$$ where $\mathbf{w}$ is a vector containing all parameters which characterize your algorithm and $Z_{D}(\beta)$ is a normalization constant. If you maximize the log-likelihood if this expression you get the ML estimate. Now, you add a prior on the parameters which acts as a regularizer and helps you avoid overfitting by controlling the complexity of your classifier. Concretely, in the case it is natural to assume that your parameters are Gaussian distributed, $$p(\mathbf{w}) = \exp \left( -\frac{\alpha ||\mathbf{w}||^{2}}{2}\right)/Z_{W}(\alpha)$$ MAP is defined as $\arg\max_{w} p(\mathbf{w}|D)$. Using Bayes' theorem, $$p(\mathbf{w}|D) = p(D|\mathbf{w})p(\mathbf{w})$$ If you substitute the above expressions and take logarithms you end up with (the $Z$'s do not depend on $\mathbf{w}$), $$\arg\min_{w} \sum_{n}\frac{\beta}{2} \left[t_{n}- y(\mathbf{x_{n}},\mathbf{w}) \right]^{2} + \frac{\alpha}{2}\sum_{i}w_{i}^{2}$$ which is nothing more as ridge regression. The more data you add, the bigger the first term will be in comparison to the second, i.e. the closer to the ML estimate. A very similar derivation can be done for the case of classification. If you are interested on Machine Learning, I would recommend you to get a copy of Bishop's book.
