[site]: crossvalidated
[post_id]: 389840
[parent_id]: 372612
[tags]: 
In the typical bayesian setting, when we have some predictors $X = \{x_i\}$ (in this case, uncalibrated predictions), labels $R = \{r_i\}$ (in this case, the binary result) and model parameters $\theta$ , we can define a posterior over the parameters as: $$P(\theta | R, X) \propto P(R|X,\theta)P(\theta)$$ Where $P(\theta)$ is some simple prior over the parameters. Usually, we model $R|X,\theta \sim D(X,\theta)$ for some distribution $D(X,\theta)$ . A popular choice is $D = \mathcal{N}(\mu = f(X;\theta), \sigma^2)$ , or for binary tasks, $D = \text{Bernoulli}(p = f(X;\theta))$ . However, neither of these choices would be valid in our case: we want our model to define a probability distribution over calibrated predictions $Y$ , not the results $R$ . Suppose we define a distribution $Y \sim \text{Beta}(\alpha, \beta = f(X;\theta))$ -- I chose the beta distribution as its support is the unit interval, although other choices can be made. Then we can write $$D = \text{Bernoulli}(p \sim Y) = \text{Bernoulli}(p \sim \text{Beta}(\alpha, \beta = f(X;\theta)))$$ Computing $P(R|X,\theta)$ now requires integrating out $Y$ in the general case, but it just so happens that in this case we can show $D = \text{Bernoulli}(p = \frac{\alpha}{\alpha + \beta})$ . Note that the MLE version of this model -- $\theta^* = \text{argmax}_\theta P(R|X,\theta)$ -- is equivalent to training the model to regress $R$ from $X$ , as mentioned. Yet by specifying the beta-bernoulli distribution, we can perform full bayesian inference. Now that we have defined both the prior and likelihood terms, all which remains is to sample from the posterior (which can be done via MCMC, variational inference, or other bayesian methods depending on the exact form of $f$ ), and then once we have samples of $\theta$ , to sample from $Y$ in order to obtain the predictive distribution.
