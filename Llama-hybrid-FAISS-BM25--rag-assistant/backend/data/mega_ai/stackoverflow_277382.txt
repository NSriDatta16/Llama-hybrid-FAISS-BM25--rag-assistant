[site]: stackoverflow
[post_id]: 277382
[parent_id]: 
[tags]: 
What kind-of stats does your company collect to define code / software product quality

Most programming houses / managers i know of can only define quality in terms of the no of bugs made / resolved in retrospect. However most good programmers can innately sense quality once they start meddling with the code.(right?) Has any programming houses that you know of, successfully translated this information into metrics that organizations can measure and track to ensure quality? I ask since i very often hear rantings from dis-gruntled managers who just cannot put their finger on what quality really is. But some organizations like HoneyWell i hear has lots of numbers to track programmer performance, all of which translates to numbers and can be ticked off during appraisals. Hence my question to the community at large to bring out the stats they know of. Suggestions about tools that can do a good job of measuring messy codes will help too.
