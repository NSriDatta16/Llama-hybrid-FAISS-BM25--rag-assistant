[site]: crossvalidated
[post_id]: 124798
[parent_id]: 
[tags]: 
Query on "Deep neural networks for object detection"

I was trying to follow the paper "Deep neural networks for object detection" at https://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf . I need bit of help to understand the mask and the design of the last two layers. These are my questions 1) What is the dimension of the output of the network (or may be 'a' network per mask) ? 2) If you see Fig1, top row, second block from left, it seems that each neuron output is a single binary mask (0 or 1) which represents if a box covers the intended object. I am sure there is some confusion in this representation as it does not fit. Or does it ? 3) So lets say the output of the network is X dimensional. I.e, there are X neurons at the last layer. That X must be equal to some p times q. Now the input image is say m*n. With some labelled boxes. As I understand, the labelled boxes are re-scaled to the ratio of m/p and n/q. So now I have one or more re-scaled labelled boxes of varying size in my training set. Now, say there is a 24*24 object mask, which essentially has 576 binary numbers in it. So this set of binary numbers, are they obtained from my p*q output ? by sliding the 576 window every time ? And for every 576 window the L2 error is calculated by comparing with one or more labelled box ? If the understanding till there is right, then my next question is 3a) The labelled box(es) will have dimensions which will never match the 24*24 window. The dimension will depend on the size of the object. So, how is the ground truth mask matched (dimension wise) with the object mask at every slided window position? 4.The intention here is to find the best set of bounding box(es) for a class of object and merge. How is that derived from the output of network and from the regression? What is actually meant by regression here - what is my "y" and what is my "X" ? Generally for regression problems the target is a number. What is that target or "y" ? If the target is "a set of 0s and 1s" and we are minimizing an aggregated error over that target, then I understand the output will be trained to produce 1s (within its p*q area) for the pixels covering the object (re-scaled) and 0s elsewhere. Is this is right understanding ? If this understanding is correct, the next question is 5) So do they first run a ImageNet (referred as [14] in their paper) type network to determine the class of the object - and then run separate trainings for each class for localization ? (because obviously otherwise, if there are multiple classes in the same image then the regression for one class will fail as there will be multiple labelled boxes which are not that type of object whose mask is being trained for)
