[site]: crossvalidated
[post_id]: 357309
[parent_id]: 357187
[tags]: 
This is actually the way Random Forest is designed to work, so what you are seeing is not too surprising if you know a few things about the algorithm. It helps to think in terms of the bias variance tradeoff. As a reminder, at least roughly: $$ \text{Test Error} = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error} $$ A random forest is an average of decision trees. The individual trees are decorated from one-another by clever subsampling schemes: bootstrap sampling of the training data, and split level subsampling of the features. Since any the trees are identically distributed (each individual possibility for a fit tree is just as likely to show up as the 67th one in a forest as the 154th one) the bias of the average of all the trees is the same as for a single tree. So the averaging keeps the bias of the model the same. On the other hand, since the trees are not perfectly correlated, the variance of the average of the ensemble is less than that of a single tree. This is how random forest works, it attacks the variance part of the error decomposition. But for this to work out, you need the bias of a single tree to be low. Having multiple trees will not make it any lower, so your only chance to keep the bias low is in the complexity of the component trees. Thus, in many cases, the best strategy for building a forest is just to build low bias, i.e. very deep, trees, and let the averaging attack the incurred variance. It is not always the optimal strategy to grow the trees as deep as possible, it's conceivable that averaging does not overcome the suffered variance, but it's not particularly surprising when it does work out that way.
