[site]: crossvalidated
[post_id]: 369674
[parent_id]: 171986
[tags]: 
I just want to add, after three years the question was raised, that there is a paper submitted to ICLR 2019: LEARNING WITH RANDOM LEARNING RATES https://openreview.net/pdf?id=S1fcnoR9K7 The main idea of the paper is: We present the All Learning Rates At Once (Alrao) optimization method for neural networks: each unit or feature in the network gets its own learning rate sampled from a random distribution spanning several orders of magnitude. This comes at practically no computational cost. Perhaps surprisingly, stochastic gradient descent (SGD) with Alrao performs close to SGD with an optimally tuned learning rate, for various architectures and problems.
