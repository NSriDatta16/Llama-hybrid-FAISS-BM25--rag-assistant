[site]: crossvalidated
[post_id]: 345871
[parent_id]: 
[tags]: 
Are the precision gains from stratified sampling a free lunch?

I'm following the discussion in Field Experiments by Gerber and Green, Chapter 3 as well as these resources: http://ocw.jhsph.edu/courses/StatMethodsForSampleSurveys/PDFs/Lecture4.pdf http://home.iitk.ac.in/~shalab/sampling/chapter4-sampling-stratified-sampling.pdf Gerber and Green claim that under reasonable assumptions, blocking improves precision: By making a small design change, we greatly improve the precision with which we estimate the [Average Treatment Effect]. (Ch. 3) I wrote an R script to explore this: total_sample Which returns [1] "stratified" [1] 0.05 [1] 0.004873 [1] "normal" [1] 0.05 [1] 0.006892 In this script, the stratification adds no information (we split the sample 50/50, and the proportion outcome we are measuring is the same between both groups). However, the standard error for the proportion is lower under stratified sampling. This seems like a free lunch! If this is true, it seems to me that researchers could generate completely random "blocks" for their study and show radically improved "precision" without adding any additional information to the study. How can this be the case?
