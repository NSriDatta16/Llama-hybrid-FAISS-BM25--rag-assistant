[site]: datascience
[post_id]: 78226
[parent_id]: 78222
[tags]: 
Found out what I was doing wrong. Yes, I built the autoencoder wrong. I didn't think about how I need to flatten the tensor before passing it into a dense layer. Important step but easy to forget... Here is my new model summary in case someone else needs some guidance: Model: "model_9" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_8 (InputLayer) [(None, 256, 256, 3)] 0 _________________________________________________________________ conv2d_14 (Conv2D) (None, 254, 254, 4) 112 _________________________________________________________________ conv2d_15 (Conv2D) (None, 252, 252, 16) 592 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 126, 126, 16) 0 _________________________________________________________________ flatten_4 (Flatten) (None, 254016) 0 _________________________________________________________________ dense_16 (Dense) (None, 16) 4064272 _________________________________________________________________ latent (Dense) (None, 8) 136 _________________________________________________________________ dense_17 (Dense) (None, 16) 144 _________________________________________________________________ reshape_4 (Reshape) (None, 256, 256, 3) 0 _________________________________________________________________ conv2d_transpose_11 (Conv2DT (None, 258, 258, 16) 448 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 129, 129, 16) 0 _________________________________________________________________ conv2d_transpose_12 (Conv2DT (None, 131, 131, 3) 435 ================================================================= Total params: 4,066,139 Trainable params: 4,066,139 Non-trainable params: 0 _________________________________________________________________
