[site]: crossvalidated
[post_id]: 388077
[parent_id]: 388026
[tags]: 
The "Bayesian gold standard" is to "regularize" predictions by computing the posterior predictive distribution $P(y|x,D) = \int p(y|x,w) p(w|D) dw$ , where $D$ is our dataset and $w$ is the weights in our network. This integral is intractable in most cases. We propose to do this by approximating an equally weighted geometric mean of the predictions of an exponential number of learned models that share parameters The "exponential number of models" is simply all the possible dropout masks which could randomly have been selected. The geometric mean refers to the fact that at test time, you downscale the weights, creating some sort of average of all models.
