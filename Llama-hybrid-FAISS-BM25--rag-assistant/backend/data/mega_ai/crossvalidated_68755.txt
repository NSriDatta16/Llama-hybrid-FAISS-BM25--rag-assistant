[site]: crossvalidated
[post_id]: 68755
[parent_id]: 68751
[tags]: 
First of all you are confusing two things: Classification task - where you are trying to model a mapping $\phi(X) \rightarrow \{ 1,..,K \}$. Example of such mapping would be building a model to find the main topic of the document from the set of $K$ predefined topics. Ranking - where you are building some query similarity measure $s$, and for given query $q$ return the list of objects sorted according to your function $s$. In machine learning community, your problem is called learning to rank , and quality measures that you are refering (like f1-measure) are not applicable for such problems (directly, as those are metrics of classification). For ranking problems you should consider using one of the special measures, like for example: DCG and NDCG; Mean reciprocal rank; Kendall's tau Mean average precision (MAP); In all cases you will need some kind of test set, where you provide system with some kind of "gold standard" - you need some set of your data with already selected "best" answers, so the system can be compared to it. Once you create such a set, with "expected answers" you can split it to the training/test set using K-folds and run one of the above metrics. K-fold cross folding is also a method for hyperparameters selection using restricted set of labeled data, so it is completely independent from used metrics, you can use it to optimize some parameters of your model. If this is a case, you split your data into two parts: training/testing, then run K-fold on training so it is internally splited into training (for actual training) and validation (for "testing" the hyperparameter choice), and once you select the best parameters - you test it on the "test" set. From your post I deduce, that your method actually does not require learning (it is designed by hand), so you can skip the "training set" and use only a "test set" (and "validation set" if you want to optimize some parameter).
