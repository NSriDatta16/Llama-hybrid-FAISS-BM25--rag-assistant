[site]: crossvalidated
[post_id]: 442194
[parent_id]: 442118
[tags]: 
You've a typo in the formulation. It's actually the following (Page 166): $$y=f(x;\theta,w)=\phi(x;\theta)^Tw$$ It means that neural nets learn a function of the data, where it also can be represented as a linear function of a nonlinear transformation of the data. $w$ are the weights of the last layer (output layer) of the feedforward neural network, such that when multiplied with the hidden layers' outputs it gives $f$ . So, by learning $\phi$ and $w$ , we learn $f$ . If there was only $w$ , it would mean no hidden layers, and the output would equal to $x^Tw$ , i.e. a linear model. Instead, we use hidden layer(s) and transform our data to $\phi(x)$ parameterised by $\theta$ , i.e. $\phi(x;\theta)$ , and then multiply the transformed features with output layer's weights.
