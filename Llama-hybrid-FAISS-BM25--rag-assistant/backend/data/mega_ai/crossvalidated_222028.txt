[site]: crossvalidated
[post_id]: 222028
[parent_id]: 222021
[tags]: 
Your looking for partially observable Markov decision processes (POMDP) . A POMDP is a formalization of a probabilistic (state transition is probabilistic, and depends on prior latent state and chosen action), state-discrete (latent state, actions, observation variable are discrete), time-discrete, sequential (there are multiple decisions in sequence), decision problem (the problem is to choose an action to maximize a reward). Parameter learning Most of the literature about POMDPs is about acting/planning. When trying to learn parameters for a POMDP, you have to distinguish between many different cases: Do you only have observational data (traces of actions, observations, immediate reward)? If you can interact with the environment during learning (like in Bayes active learning), you should look into (partially observable) model-free reinforcement learning. There are various approaches for learning POMDP parameters depending on what is given in advance. If you can interact during learning, then [1] assumes only a given reward function. You can also try searching literature in the field of neuroscience for reinforcement learning models, as your application seems to be closely related. [1] Bayes-adaptive POMDPs, Ross, Stephane and Chaib-draa, Brahim and Pineau, Joelle
