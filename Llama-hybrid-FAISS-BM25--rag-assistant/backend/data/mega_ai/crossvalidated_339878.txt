[site]: crossvalidated
[post_id]: 339878
[parent_id]: 
[tags]: 
Higher Test Scores but Higher Variance?

I am tuning hyper-parameters using 5-fold cross-validated grid search for various multiclass classifiers, and I keep running into the same issue that I can't quite wrap my head around. The hyper-parameter configurations achieving the highest "test score" often have extremely low training error - suggesting overfitting. The table above shows my results for tuning a decision tree, the test score of the configuration shown in the top row is slightly higher, but the train score is much higher. Obviously in this case the test scores are so close together that I can use either, but sometimes the difference is larger. Plotting learning curves for each of the two configurations shown above makes the difference in variance very noticeable: I'm having a hard time understanding why this is happening, and also reasoning which classifier is "better". If lower variance is the goal, should I be choosing the classifier configuration that produces the best test score, where the difference in train/test score is within a limit? P.S. The same behaviour is shown for the results of an SVM - very high C values massively overfit (1.0 train scores) but tend to have better test scores than the classifiers with lower C values.
