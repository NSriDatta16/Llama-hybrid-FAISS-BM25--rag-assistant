[site]: crossvalidated
[post_id]: 101569
[parent_id]: 101565
[tags]: 
Such a model may be estimated by ordinary least squares, in similar fashion to an ordinary AR, which may also be estimated by OLS (see the second paragraph here ); it works for this model works for exactly the same reasons. In this case, you construct response and predictors as follows: $Y = \{Y_{p+1},Y_{p+2},\ldots,Y_T\}$ $X_1 = \{Y^{h_1}_{p},Y^{h_1}_{p+1},\ldots,Y^{h_1}_{T-1}\}$ $X_2 = \{Y^{h_2}_{p},Y^{h_2}_{p+1},\ldots,Y^{h_2}_{T-1}\}$ $X_3 = \{Y^{h_3}_{p},Y^{h_3}_{p+1},\ldots,Y^{h_3}_{T-1}\}$ And then regress $Y$ on $(X_1,X_2,X_3)$ This conditions on the first $p$ $Y$-values, just as with OLS estimation of an $AR(p)$. A useful way to look at it is to decompose the likelihood for $Y_{1:T}$ as $f(Y_{T}|Y_{1:T-1})\cdot f(Y_{T-1}|Y_{1:T-2})\cdot f(Y_{T-2}|Y_{1:T-3}) \cdot \ldots \cdot f(Y_{p+1}|Y_{1:p})\cdot f(Y_{1:p})$. OLS estimation corresponds to optimizing an approximation to the likelihood based on all but the final term. As $n\to\infty$, the relative impact on the likelihood of the first $p$ terms becomes less and important - the likelihood is dominated by the last $n-p$ terms (one can formalize this line of argument somewhat).
