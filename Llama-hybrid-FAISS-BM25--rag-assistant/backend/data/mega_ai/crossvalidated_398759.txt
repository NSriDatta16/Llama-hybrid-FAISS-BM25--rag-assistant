[site]: crossvalidated
[post_id]: 398759
[parent_id]: 398752
[tags]: 
One obvious solution is to use the time series of features (e.g. numbers by e.g. 64 frequency bands) as an input to a LSTM neural network (or similar), which should naturally deal with varying sequence lengths. Alternatively, decide on a maximum length and pad with silence, if a clip is shorter. Have a look at what Google did for Audioset with the VGGish model they published ( https://github.com/tensorflow/models/blob/master/research/audioset/README.md ).
