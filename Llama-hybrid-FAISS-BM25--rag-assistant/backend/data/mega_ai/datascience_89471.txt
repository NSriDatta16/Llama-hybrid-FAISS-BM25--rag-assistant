[site]: datascience
[post_id]: 89471
[parent_id]: 89093
[tags]: 
Approaches 1 & 2 1. Noise If we adopt a frequentist view, we can consider the attributes of a movie to have an underlying probability distribution (or, if you'd rather be Bayesian, you can choose to believe they come from a certain distribution). For a single attribute, its values are a sequence of random variables drawn from the same distribution. We can model our data for one attribute by considering a data model of the general form $Y = X + \epsilon$ , where $Y$ is a random variable representing one measurement, $X$ is a random variable coming from the "true" underlying distribution and $\epsilon$ is a noise term. For some variables, the noise term is negligible (e.g. the runtime of a movie is actually not noisy at all, it's a real, objective, fixed value). For others, noise may follow a normal distribution, or another distribution with mean 0. In that case, over many measurements, the law of large numbers dictates that the mean of your $Y$ s will converge to the expectation of $X$ . Here comes the first warning: if you have noisy data and few of them, your data might not mean much. Finally, you might be so unlucky that you have noise $\epsilon$ coming from a distribution with a mean which is not $0$ , in which case you are likely to have systematically biased data. In both approaches, you partition your observations into groups/classes (in 1 based on the values of one attribute, in 2 on the values of all other attributes). This can lead to class imbalance, in which case you might end up with some classes with many observations (and presumably little noise) and some classes with few members (maybe only one!) and a possibly a lot of noise. Furthermore, if you look at variation only, and not correlation, then you might end up with high variance (even comparatively, if one attribute is very noisy perhaps due to previously mentioned class imbalance) and low correlation, which would erroneously lead you to deduce that an attribute has good explanatory power. 2. Multicollinearity Approach 1 opens you up to the problem of collinearity: two predictor variables might be strongly linked, in which case it's not easy to say which one is really a valuable predictor and which one can be considered just an epiphenomenon. For instance, assume fantasy as a genre is very popular, but people dislike overly long movies. Fantasy movies would then be very long and have a large revenue, but it would be wrong to conclude that the long runtime is responsible for their large revenue. Even worse: while this example used two attributes which you probably have access to, a hidden, unobserved (i.e. latent) attribute might actually be causally responsible for the phenomenon and for the correlation between the two observed attributes. In approach 2, you're doing something akin to statistical matching, so you at least stand a better chance of avoiding multicollinearity and confounding variables. Personal recommendation I would personally start with some exploratory data analysis (EDA). Plot your data! Use heatmaps, small multiples, violin plots, the works, anything that might give you insight into its structure. Next, start small, go big if needed. Try measures of "linear" correlation, first Pearson correlation. For categorical variables, you can use Spearman rank correlation or Kendall rank correlation. You can also use measures of rank correlation (e.g. those mentioned previously) to estimate non-linear correlation. You can also use SHAP values, Q-Q plots, partial plots. You can then move on to maybe a simple decision tree (or random forest), or a linear regression if you encode your categorical variables. If that gives you good accuracy, then you can examine what your model has learned. You can use stepwise linear regression, where you either add or subtract attributes one by one and examine the change in predictive power. You can also try dimensionality reduction, for instance do PCA or ICA then look at which attributes carry a lot of weight in the first few PCA/ICA components. You can also try other types of matrix factorization. If accuracy is poor with simple models, you can try more complex models, such as neural networks, then use interpretability models designed for those, such as LIME, which builds a locally accurate linear surrogate of a deep network.
