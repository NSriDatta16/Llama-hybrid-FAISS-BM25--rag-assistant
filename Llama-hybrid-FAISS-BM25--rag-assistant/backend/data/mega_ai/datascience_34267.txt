[site]: datascience
[post_id]: 34267
[parent_id]: 
[tags]: 
Choosing between prepadding and postpadding for variable length sequence data in a CNN

Convolutional Neural Networks (CNN) assume the same sized data. Often sequence data is variable length (e.g., natural language). Thus sequence data is padded to make sure all data is all the same length. Prepadding is adding zeros to the beginning of shorter sequences. Postpadding is adding zeros to the ending of shorter sequences. What is the empirical reason to choose prepadding or postpadding?
