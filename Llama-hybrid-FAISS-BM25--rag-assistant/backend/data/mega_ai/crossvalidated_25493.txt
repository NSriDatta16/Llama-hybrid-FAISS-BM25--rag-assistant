[site]: crossvalidated
[post_id]: 25493
[parent_id]: 23761
[tags]: 
In my answer, I provide many links to background material to save space here. I'm going to write my answer taking the info in the links as given. I think a Bayesian approach is a natural fit to this problem, especially since you seek to convince only yourself. It's a bit convoluted to use confidence intervals to answer the question you really care about, to wit, how plausible is it that $Q_{7} Likelihood function Let $f_k$ be the observed frequency of integer outcome $k$ in your sample and let $N$ be the sample size. The likelihood function is proportional to the multinomial distribution . It has the form $L(z_{0},...z_{8};f_{0},...f_{8})=\prod_{i=0}^{8}{z_{i}}^{Nf_{i}}$. Prior distribution The Dirichlet distribution is the natural choice for the prior distribution because it is the conjugate prior for the multinomial likelihood. It has the form $p(z_{0},...z_{8};\alpha_{0},...,\alpha_{8})\propto\prod_{i=0}^{8}{z_{i}}^{\alpha_{i}-1}$ This prior has nine hyperparameters (the $\alpha_i$ values), and they're a bit of a pain to deal with. In this “large sample” context, any reasonable choice of hyperparameter values will have negligible influence on the result, but still, I think it's worth devoting a bit of effort to selecting sensible values. Here's how I recommend setting the hyperparameters. First, note that under this distribution $\mathrm{E}(z_{i})=\frac{\alpha_{i}}{\sum_{i=0}^{8}\alpha_{i}}$. Next, note that the simplest maximum entropy distribution over the naturals is the geometric distribution . So set $\alpha_{i+1}=r\alpha_{i}=r^{i}\alpha_{0},\,0 $\alpha_{0}=A\left(\frac{1-r}{1-r^{9}}\right).$ Then $\mathrm{E}(z_{i})=r^{i}\left(\frac{1-r}{1-r^{9}}\right)$, so the distribution of the $z_{i}$ values is centered on a (truncated) geometric distribution. Furthermore, $\mathrm{Var}\left(z_{i}\right)\propto\frac{1}{(A+1)}$, so the value of $A$ controls the dispersion around this expectation but has no effect on the expectation itself. This specification reduces the number of hyperparameters from the nine $\alpha_{i}$ values to just $r$ and $A$. I'll defer discussion of specific values of $r$ and $A$ for now. Posterior probability of the proposition of interest The posterior distribution of the $z_{i}$ values is the following Dirichlet distribution: $p(z_{0},...z_{8}|f_{0},...,f_{8})\propto\prod_{i=0}^{8}{z_{i}}^{\alpha_{i}+Nf_{i}-1}.$ Let $\mathbb{Y}=\left\{ z_{0},...z_{8}|Q_7 $\Pr(Q_7 This integral is intractible, but you can compute the probability of interest numerically using the following Monte Carlo algorithm. For $j$ from $1$ to $J$, Sample a set of $z_i$ values from their posterior distribution. Use the sampled values to compute $y_j=I(Q_{7} Then $\Pr(Q_7 The accuracy of the Monte Carlo approximation goes as $\sqrt{J}$: $J=10^4$ will get you at least two decimal places of accuracy 19 times out of 20, $J=10^6$ will get you at least three decimal places of accuracy 19 times out of 20, etc. And if your posterior probability of interest isn't close to 0 or 1, just sample more data, rinse, and repeat. Prior hyperparameters, part two The exponent of $z_i$ in the expression for the posterior density is $\alpha_i + Nf_i - 1 = Ar^{i}\left(\frac{1-r}{1-r^{9}}\right) +Nf_i - 1 = A\mathrm{E}(z_i) +Nf_i - 1$ It can be seen that the hyperparameter $A$ plays the same role in the prior distribution as $N$ plays in the likelihood -- it's a kind of "prior sample size". To ensure that the prior has a negligible influence on the conclusion, just pick a value of $A$ such that $A\ll N$; for example, $A = 1$. To set $r$, note that you can calculate the prior probability of the proposition $Q_7
