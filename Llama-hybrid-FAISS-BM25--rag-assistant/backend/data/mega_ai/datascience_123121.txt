[site]: datascience
[post_id]: 123121
[parent_id]: 123112
[tags]: 
You can train a character-level masked language model. Like BERT , but with the following traits: The token vocabulary of the model would be the characters that are allowed in the words (e.g. lowercase Latin script letters) plus special tokens like [MASK] . The input sequence would be a sequence of characters forming a word. Some characters would be replaced by a special token [MASK] . The expected output would be the same as the input sequence but without any masked tokens. The loss would be the categorical cross-entropy between the actual model output and the expected output.
