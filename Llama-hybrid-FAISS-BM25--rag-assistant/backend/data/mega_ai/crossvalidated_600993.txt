[site]: crossvalidated
[post_id]: 600993
[parent_id]: 600395
[tags]: 
See also link in @jbowman comment, method 5. Are you willing to entertain a Bayesian approach? If so, you could specify a uniform prior on the five-dimensional surface $\sum\limits_{x=1}^{6} p_x = 1$ , sample from the posterior distribution, and identify the $100 (1-\alpha)$ % region with the highest posterior density. Algorithm sketch loop: Gibbs sample parameter vector $\theta_i$ from prior surface. Compute likelihood $w_i \propto \mathcal{L}(\theta_i | X)$ . Accumulate sums weight $\sum w_i$ weighted vectors $\sum w_i \theta_i$ weighted squared vectors $\sum w_i \theta_i \theta_i^\textrm{T}$ output: multivariate posterior sample mean and variance library(dplyr) # Set the probabilities for each number (pretend this is unknown in real life) probs % count(value) %>% rename(x=n) %>% pull(x) print(x) ## [1] 8 18 32 22 11 9 theta = rep(1 / 6, 6) # starting point for Gibbs sampler sum_wt mu is the posterior mean, sigma is the posterior covariance matrix. Note off diagonal entries of sigma are negative. This is expected. A positive error in one element of mu is offset by negative errors in the other elements of mu .
