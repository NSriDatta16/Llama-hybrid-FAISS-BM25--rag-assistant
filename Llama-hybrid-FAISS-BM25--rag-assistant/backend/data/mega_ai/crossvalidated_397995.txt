[site]: crossvalidated
[post_id]: 397995
[parent_id]: 397793
[tags]: 
...Answering my own question for the sake of having an answer, with all thanks due to @whuber. The key question is "what is it about matrix Z that makes it a good demo data set/why go to all the trouble they did to make Z ?" Short answer: it has "structure" and it is of appropriate scale. Here's a tour to give a fuller answer: First, X as defined in the example is of course a draw from a normal distribution and does not have an interesting structure. By "interesting structure" I mean it would give a scree plot resembling something you'd like to see in a real data set. Here is the scree plot created via plot(prcomp(X)) : Notice that the explained variance falls off very slowly, and there is no elbow evident. Further, the amount of variance explained is very low. This is on the y-axis. Unfortunately the built-in scree plot gives absolute values for the variance explained. To see the percent variance explained, do summary(prcomp(X)) . In this case the PCs are only explaining about 4% each. So X does not give a good or realistic demonstration of PCA. In the example it appears that the desired structure comes from the construction of the toeplitz matrix. It's easy to see what this is via the example in ?toeplitz (look at the diagonals): > x toeplitz (x) [,1] [,2] [,3] [,4] [,5] [1,] 1 2 3 4 5 [2,] 2 1 2 3 4 [3,] 3 2 1 2 3 [4,] 4 3 2 1 2 [5,] 5 4 3 2 1 One can see there is some structure to this matrix. Do let's do the same steps as we did for X using S , the toeplitz matrix created in ?prcomp . Here's the scree plot which looks like it came from a real data set, in that it drops off quickly. And inspection of summary(prcomp(S)) shows that three PCs explains > 95% of the variation. So this is not a bad demonstration. Perhaps the author of ?prcomp could have stopped here. However, they went ahead and found the root of S , C , via Cholesky decomposition , and then post-multiplied X with it to give Z ( Z = X %*% C ). In the comments they point out that cov(Z) ~= S . That's true (they prove it), I guess it's important because it shows the structure of S is carried through. For instance, plot(prcomp(X %*% S)) gives something visually similar to plot(prcomp(S)) though the vertical scale is quite different (the percent variance explained is roughly similar though). The only reason I can think of that the authors go all the way to Z in their demo is "noise". The scree plots and summaries of prcomp(S or C or Z) are similar. The difference however is in the range of the scores (note that range on class(prcomp) is the range of the scores, which roughly parallel the range of the raw data): > range(prcomp(S)) [1] -1.577132 1.577132 > range(prcomp(C)) [1] -1.7389697 0.7116142 > range(prcomp(Z)) [1] -11.28021 13.54936 So I guess that a data set like Z is less noisy and more desirable as a demonstration if all other factors are equal (specifically, the trend in variance explained is similar). If this is applicable to real data sets, I don't know what it is called (and in the realm of real data sets we usually have the data we have, and that's what must be analyzed, there is no other choice).
