[site]: crossvalidated
[post_id]: 340420
[parent_id]: 
[tags]: 
Orthogonality in the proof of the representer theorem

I'm following these lecture notes on the representer theorem in the context of an SVM. In terms of the notation there, let $f^*$ be the element in the reproducing kernel Hilbert space that is a solution to the regularized optimization problem given data $(x_i, y_i)$, $i=1\ldots n$. The notes proceed as follows for a given $f$ (not necessarily the solution) Assume $f = f_s + f_\bot$ where $f_s$ lives in the subspace spanned by the data vectors, so that $f_s = \sum_{i=1}^n a_i k(x_i , .)$ and (as far as I can tell) $f_\bot = f-f_s$, i.e. it is just everything else. Then, in terms of evaluating the terms that arise in the loss function we have: $$ f(x_j) = \langle f, k(x_j,.) \rangle = \langle f_s , k(x_j, .) \rangle + \langle f_\bot , k(x_j, .) \rangle $$ I get that the first term works out to $f_s(x_j) = \sum_{i=1}^n a_i k(x_i, x_j)$, but it is not clear to me that the second term $f_\bot(x_j)$ is necessarily $0$. What makes that the case? If you say it is "by construction in the definition of $f = f_s + f_\bot$", then my question is, given a decomposition $f = f_s + f_\bot$ such that $f_\bot(x_i)=0$ for all of the data vectors, how do I know that $f_s = f-f_\bot$ can be expressed as a linear combination of the just kernels for the data vectors?
