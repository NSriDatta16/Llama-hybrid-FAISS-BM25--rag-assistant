[site]: datascience
[post_id]: 117290
[parent_id]: 117289
[tags]: 
train a separate model ... for each combination of applicable labels You are getting at model performance. You don't have to do anything, but if you train different kinds of models, that is, models drawn from distinct hypothesis sets, you may observe various performance levels. It sounds like you have an existing model that outputs five raw scores. And then you incorporate "extra" knowledge, which the model is ignorant of, to zero out some scores before scaling them into probabilities and choosing argmax maximum likelihood. Training many models is certainly one possible refinement, but augmenting the width of the input training vector is another. One obvious augmentation is to tack on five indicator variables, saying that each one-hot encoded label is {allowed, disallowed}. Logistic regression, decision tree, and other modeling techniques will learn from such hints. But perhaps the business constraint comes from something simpler, like stage within the customer journey. If you have labels like {prospect, initial customer, repeat customer} which induce {allowed, disallowed} over the labels, consider one-hot encoding those instead, to reduce the model complexity. No matter which approach you choose, or how much of the SME's business rules you make available during training, you can always post-filter, there's nothing wrong with that. And simple filters can be implemented very quickly, without the cost of retraining a model to learn new business rules. tl;dr: If you can expose Bayesian priors at training time, do so, expose as much signal within the noise as feasible.
