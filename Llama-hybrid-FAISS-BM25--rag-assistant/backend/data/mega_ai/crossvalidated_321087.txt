[site]: crossvalidated
[post_id]: 321087
[parent_id]: 
[tags]: 
High validation accuracy without scaling paramters when using dropout

I was training a CNN network on German traffic sign classifier data. The architecture was- 3 convolutional layers with intermediate max pooling concatenated outputs of layer 2 and layer 3 to feed to the input layer of the fully connected classifier. A final layer giving the softmax probabilities of the classes. Used ReLu as the activation function. Dropout for regularization only for the input layer of fully connected(FC) network with a keep_prob of 0.5 I did not scale the outputs of the FC input layer neither at the training time nor at the testing time. And while testing used a keep_prob of 1.0. I achieved an accuracy of 99 on validation set and 94 on the testing set. Now, my question is- How did I achieve this much accuracy without scaling when using dropout?
