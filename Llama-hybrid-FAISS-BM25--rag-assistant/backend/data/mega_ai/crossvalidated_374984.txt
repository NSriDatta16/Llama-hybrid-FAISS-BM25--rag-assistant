[site]: crossvalidated
[post_id]: 374984
[parent_id]: 374944
[tags]: 
Infact the opposite, if you are normalizing with zero mean and unit variance, such normalization will help in eliminating vanishing gradient problem. There is no reason to doubt that normalization would cause such problems. Using batch normalization along with ReLu activation function will solve your vanishing gradient problems. Edit : If you are normalizing in the range [0,1] for a neural network, the neurons would saturate and the training process would not move along. To avoid this, you could either normalize in range [0.1,0.9] or ideally, in range [-0.5,0.5]. The results would not differ a lot as long as you avoid saturation.
