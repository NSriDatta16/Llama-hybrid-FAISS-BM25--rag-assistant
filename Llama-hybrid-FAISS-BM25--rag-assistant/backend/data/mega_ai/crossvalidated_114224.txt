[site]: crossvalidated
[post_id]: 114224
[parent_id]: 114168
[tags]: 
So using 10-fold CV will split the data into 10 different sets of roughly the same size. The model is fit on 90% and the remaining 10% is used to estimate accuracy. This process continues "round robin" 9 more times. The accuracy is the average of the 10 holdouts for each tuning value. For example: > set.seed(1) > train_control output There is a sub-object that has the hold-out estimates of accuracy for each fold, and averaging these gets you the reported value (the ones shown here are for the optimal value of Cp): > output$resample Accuracy Kappa Resample 1 0.8666667 0.8 Fold01 2 0.9333333 0.9 Fold02 3 1.0000000 1.0 Fold03 4 0.9333333 0.9 Fold04 5 0.9333333 0.9 Fold05 6 0.8000000 0.7 Fold06 7 1.0000000 1.0 Fold07 8 1.0000000 1.0 Fold08 9 0.9333333 0.9 Fold09 10 0.9333333 0.9 Fold10 > mean(output$resample$Accuracy) [1] 0.9333333 > getTrainPerf(output) TrainAccuracy TrainKappa method 1 0.9333333 0.9 rpart2 There is little value in getting predictions on the 90% each time. Since the same data is used to build the model, the predictions can be extremely optimistic (which is the motivation for using cross-validation in the first place). If you want to see where the estimates in output$resample were created: > ## For the model associated with optimal Cp value, here is the predictions on the > ## first fold that was held-out > first_holdout head(first_holdout) pred obs rowIndex maxdepth Resample 1 setosa setosa 6 2 Fold01 2 setosa setosa 27 2 Fold01 3 setosa setosa 31 2 Fold01 4 setosa setosa 36 2 Fold01 5 setosa setosa 45 2 Fold01 6 versicolor versicolor 67 2 Fold01 If we get the accuracy for this set: > postResample(first_holdout$pred, first_holdout$obs) Accuracy Kappa 0.7666667 0.6500000 Max
