[site]: crossvalidated
[post_id]: 463648
[parent_id]: 
[tags]: 
SVM Regularization Term(C) = (1/lambda) vs 1/(2*lambda)

The way I learned SVM Cost Function from Andrew ng Coursera was: $$ J = \sum_{i=1}^my*Cost_1(z) + (1-y)*Cost_0(z) + \frac{\lambda}{2}*\sum_{j=1}^n\theta^2$$ which further simplifies with parameter $ C = \frac{1}{\lambda}$ : $$ J = C*\sum_{i=1}^my*Cost_1(z) + (1-y)*Cost_0(z) + \frac{1}{2}||\theta||^2$$ where $$ z = \theta^TX \space\space\space \text{(weights * input)} $$ $$ Cost_0(z) = max(0,1+z) $$ $$ Cost_1(z) = max(0,1-z) $$ I then encountered a similar form of equation on wikipedia: $${\displaystyle \left[{\frac {1}{n}}\sum _{i=1}^{n}\max \left(0,1-y_{i}({\vec {w}}\cdot {\vec {x}}_{i}-b)\right)\right]+\lambda \lVert {\vec {w}}\rVert ^{2},}$$ Acknowledging the notation differences, My only problem with this definition is in the regularization term, where it is missing $\frac{1}{2}$ . Because of this, it seems to suggest me that $C = \frac{1}{2*\lambda}$ , if I try to make sense of the Wikipedia definition out of what I learned from Andrew Ng Course. I understand that 1/2 is just a scale and most likely not affect the decision made by SVM - I may be thinking wrong - but I still want to understand the reason anyone would go with the later equation. Thank you.
