[site]: crossvalidated
[post_id]: 385419
[parent_id]: 233677
[tags]: 
I believe it is actually pretty clear what you mean by the term input lags, but I will state explicitly. When doing a regression problem with an LSTM, a input signal $ \mathbf{x} \in \mathbb{R}^{n \times t \times c_1 } $ is used to predict another signal $ \mathbf{y} \in \mathbb{R}^{n \times t \times c_2} $ . For simplicity I consider $ c_1 = 1, c_2 = 1 $ , and I will take one time series $ x[t] $ , so it is possible to talk about in discrete time series terms. An input delay is then the choice of $ \tau \in \mathbb{Z}^{0+} $ , which will transform the signal like $ x_{delayed}[t] = x[t - \tau] $ , and for $ t , we define $ x[t] = 0 $ , so the signal is zero-padded at the beginning, but this is merely a choice of signal processing. With similiar reasoning it is possible to define the concept of output lag too. In case of LSTMs input lags is typically less concern than output lags in my experience. This could be checked by considering the problem of training an LSTM to predict the delayed version of itself. Consider the LSTM model equations, $$ f_t = \sigma_g(W_{f} x_t + U_{f} h_{t-1} + b_f) \\ i_t = \sigma_g(W_{i} x_t + U_{i} h_{t-1} + b_i) \\ o_t = \sigma_g(W_{o} x_t + U_{o} h_{t-1} + b_o) \\ c_t = f_t \circ c_{t-1} + i_t \circ \sigma_c(W_{c} x_t + U_{c} h_{t-1} + b_c) \\ h_t = o_t \circ \sigma_h(c_t) $$ The goal is then for the algorithm to learn $ h_t = x_{t-\tau} $ . We cannot explicitly learn that, but it is possible to learn weights such that $ c_t = f(x_{t-\tau})$ . Considering a mapping of $h_2 = x_1$ , we could set $ o_2 = 1 , f_2 = 1, i_1 = 1, i_2 = 0, U_c = 0$ , and $ W_c, b_c $ could be chosen to constrain the input values in the approximately linear regime of the sigmoid, so $ h_2 = \sigma_h ( \sigma_c ( x_1 )) \approx x_1 $ . An additional regression layer might help to scale back the values from the linear regime of the sigmoid to the original scale. So in terms of the model equations, the parameters exist to circumvent the mapping, learnability is a more involved question to answer, depending on the actual optimisation used. For output lags, this doesn't eliminate the need however, because an LSTM is a causal model. BLSTM as mentioned above, is acausal, so it might be used to circumvent this problem, however this comes at the cost of sacrificing causality of your model, i.e. real-time signal processing becomes unfeasible.
