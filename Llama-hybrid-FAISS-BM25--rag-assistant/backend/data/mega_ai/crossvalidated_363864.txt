[site]: crossvalidated
[post_id]: 363864
[parent_id]: 363855
[tags]: 
You can do that, but you might have a hard time getting useful results. If there is a strong separation between "negative" and "positive" examples and you have a lot of data, then you might do OK with an ensemble-of-trees model (like random forest or gradient boosted trees) or a neural network. But with a more traditional linear model I wouldn't expect good accuracy, unless you know of specific features in the data that help distinguish the two. One alternative is to build a model in two stages. First, you classify records into "positive" and "negative", then you make a value prediction for the records you predicted positive. You can, of course, do this by building a positive/negative classifier and a positive-only regression model separately, and feeding the output from one into the other. But you won't be accurately capturing the uncertainty inherent in that procedure. Another possibility is to return to your idea of recoding all negative values to zero, and use what is called a "zero-inflated" model . There is a lot of information on those models here on Cross Validated and elsewhere, so I won't cover them here.
