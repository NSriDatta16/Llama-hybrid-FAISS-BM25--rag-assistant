[site]: crossvalidated
[post_id]: 398248
[parent_id]: 398243
[tags]: 
I was wondering if there is any other new variables that i could create to improve the accuracy I used a decision tree as it is fast to build and gives me indication whether a newly created variable is useful or not." Instead of just considering adding features, you can also consider removing features as decision trees are very prone to overfitting -- it would be helpful to know the gap between training and validation accuracy. For removing features, you can try sequential feature selection. This is very simple to implement, and one off-the-shelf implementation for Python can be found in mlextend package: http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/ I am not sure how many data points you have, but random forests are usually better with dealing with larger number of features in terms of preventing overfitting (the main issue with using a decision tree classifier)
