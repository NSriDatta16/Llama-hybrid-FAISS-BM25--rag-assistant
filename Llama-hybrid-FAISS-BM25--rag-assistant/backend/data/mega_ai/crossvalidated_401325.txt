[site]: crossvalidated
[post_id]: 401325
[parent_id]: 256211
[tags]: 
No. You can use different algorithms with same loss function, for example you can minimize squared errors using linear regression, neural network, random forest etc. Each of those algorithms would give different results, will achieve them in different ways and will differ in performance. So nonetheless that they used same loss function, you will get different results. More formally, usually we think of machine learning in terms of finding best set of parameters $\theta$ of some function $f$ that approximates your target variable $y$ by minimizing the loss function $J$ $$ \underset{\theta}{\operatorname{arg\,min}}\; J(y, f(X; \theta)) $$ So there are two main components of machine learning model: The function $f$ that approximates $y$ . You can have many different functions, think of linear regression, regression tree, neural network, $k$ -nearest neighbors regression, they all will produce very different kinds of functions. The loss function $J$ . It (only!) tells us how bad is our approximation of $y$ . Additionally, there is also the algorithm for minimizing the loss function, it is the procedure that we apply to find the optimal parameters. In many cases it is just a matter of choosing an optimizer that we use, but in other cases th algorithm defines the model (e.g. $k$ NN)
