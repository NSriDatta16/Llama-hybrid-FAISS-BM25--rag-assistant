[site]: crossvalidated
[post_id]: 135851
[parent_id]: 135717
[tags]: 
There are perhaps two separate issues: (1) Logistic models are not classification models - they are probability based and inherently different. In that sense accuracy is a poor measure for such models and skewed outcomes data are handled differently. Specifically the model outputs a probability rather than a class and you can change accuracy by changing the probability cutoff you use to classify outcomes. Many stats programs will assume a 50% probability cut-off, but that is arbitrary and you should decide on your own probability cut point depending on need. (2) Is a decision forest a random forest? Is so there are many ways of dealing with skewed data - this is easily a book chapter or more. Max Kuhn does a reasonable job covering this in his book Applied Prediction Modelling . The code is freely downloadable from CRAN and I think the book's website. So difficult to provide a complete answer. The short answer is that this is most often dealt with by undersampling the majority class when building the random forest. The Balanced Random Forest is a simple approach to undersampling, but there are many possible approaches. The linked paper also has summary of alternative approaches.
