[site]: datascience
[post_id]: 117286
[parent_id]: 97138
[tags]: 
There is another technique that works with non linear relations. Is called T-distributed Stochastic Neighbor Embedding - tSNE( https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html ) The author (Laurens van de Maaten) wrote a blog some time ago that I found very useful https://lvdmaaten.github.io/tsne/ t-SNE is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. with different initializations we can get different results. Answering your questions I know that PCA used to find linear correlation. But what can we learn from that example ? PCA finds linear correlation. Many features have a monotonic transformation, so there is a linear relation. Those who are not, can not be reduced with PCA. That figure is telling you that all that PCA finds, is what is in the first feature. Which makes sense given how you have created the variables. Can we use the PCA results and use only the first component of the PCA to train-predict our model? I would not advise on that. If your model captures nonlinear dependencies, it will loose performance. I would suggest you make the experiment so things are clearer for you. Does the result show here are valid (correct for further processing)? Seems valid. You are applying PCA. You are getting PCA.
