[site]: crossvalidated
[post_id]: 293876
[parent_id]: 293867
[tags]: 
For multiclass problems and SVM, you have to run all possible pairwise comparisons of 2-class problems. So, for example, during the class 2 to class 4 comparison use +1 for objects in class 2 and -1 for objects in class 4. (there would be no comparison needed for class 4 vs class 2). Let $\Omega (\omega=1,2,\ldots,\Omega)$ be the number of classes. Assume classes $l$ and $m$ are the two classes being compared, the decision function for test object $\bf{x}$ for this comparison is \begin{equation} D_{lm}({\bf x})=\sum_{i=1}^n \alpha_i y_i {\bf x}_i^T{\bf x} + b_{lm}, \end{equation} where $y_i$ and ${\bf x}_i$ are for all the training objects. Next, run all comparisons of classes $l$ and $m$ $(l \begin{equation} D_{l}({\bf x})=\underset{m \neq l, m=1}{\sum^\Omega} {\rm sign}(D_{lm}({\bf x})). \end{equation} For test object $\bf x$, there will be a value of $D_{l}$ for each class, and the greatest one is the class that object $\bf x$ is assigned to, which in functional form looks like \begin{equation} D_{l}({\bf x}\rightsquigarrow \omega)=\underset{l=1,2,\ldots,\Omega}{\arg \max}\{ D_l({\bf x}) \}. \end{equation} During run-time when comparing class $l$ vs $m$, you just don't use the data for the other classes. The $D_{lm}(\bf x)$ is like the predicted value of the test object. A common approach is to assign $\bf x$ to class $l$ if $D_{lm} 0$. However, you simply only need a $D_{lm}(\bf x)$ for each test object for all possible class comparisons, i.e., $D_{12}(\bf x)$, $D_{13}(\bf x)$, $D_{14}(\bf x)$, $D_{23}(\bf x)$, $D_{24}(\bf x)$, $D_{34}(\bf x)$ -- and you only use training and test objects from true classes $l$ and $m$ when obtaining each $D_{lm}(\bf x)$ - because the trained SVM can only make predictions for the two classes of data it was trained with. Then, using all the $D_{lm}(\bf x)$ values, calculate $D_{l}(\bf x)$ for each class $(l=1,2,3,4)$, and the greatest value among $D_{1}(\bf x)$, $D_{2}(\bf x)$ , $D_{3}(\bf x)$ and $D_{4}(\bf x)$ is the predicted class. These decision functions used for SVMLS are the same as those used in L2SVM. SVMs offer many advantages over other classifiers, for example, they maximize generalization ability, avoid local maxima, and are robust to outliers. However, their disadvantages are that they do not extend easily to multiclass problems, can require long training times when quadratic programming is used, and are sensitive to model parameters the same way artificial neural networks are sensitive to the number of hidden layers and number of nodes at each hidden layer. I actually don't like SVMs, and would never use them first. Rather, I would use linear regression [inputs $y_i=-1,+1$, with $D_{lm}({\bf x})=\beta_0 + \mathbf{x}^T\boldsymbol{\beta}$], kNN, NBC, LDA to first assess linear separability, then SVM and ANNs if the classes were not linear separable.
