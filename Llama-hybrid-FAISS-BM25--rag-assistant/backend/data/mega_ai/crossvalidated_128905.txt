[site]: crossvalidated
[post_id]: 128905
[parent_id]: 127575
[tags]: 
Okay a few things. naive Bayes per se is not necessarily known for its good performance. The main reasons why naive Bayes is still used nowadays is that it can outperform "more powerful" alternatives if the sample sizes are small [1] Another thing why naive Bayes is still popular is that it is very, very fast to train (also very compatible to on-line learning) [1] Pedro Domingos and Michael Pazzani. On the optimality of the simple bayesian classifier under zero-one loss. Machine learning, 29(2-3):103â€“130, 1997. Which performance metric you use is up to your task. It may be accuracy, ROC auc, F1, precision, recall ... An auc of 0.57 to 0.61 would suggest that your classifier is only slightly better than random guessing. That's unfortunate... But it might also have sth to do with your extreme imbalance in your training data. Have you tried 10-fold stratified cross-validation for evaluation on the training set for feature selection, dimensionality reduction, and hyperparameter optimization? I.e, precision, recall, F1. Or confusion matrices would also be interesting to see what's going on. Btw. if you are interested, I have some resources on naive Bayes with further references that could be helpful: Naive Bayes and Text Classification I - Introduction and Theory And here a music-lyrics classifier that is build upon a naive Bayes model: https://github.com/rasbt/musicmood
