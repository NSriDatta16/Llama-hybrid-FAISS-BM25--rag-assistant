[site]: crossvalidated
[post_id]: 609728
[parent_id]: 609683
[tags]: 
I find it easier to think about this model using the latent-variable representation. For simplicity, let's assume that your model has only one independent variable (GPA), and that the dependent variable is ordinal with 5 categories (instead of 11). Suppose that every student has a score, $Z_i$ , that is on a continuous scale. The model equation is $$ Z_i = \underbrace{\beta x_i}_{\eta_i} + \epsilon_i $$ where $x_i$ is the student's GPA, and $\epsilon_i \sim \mathrm{Logistic}(0,1)$ . We thus have $Z_i \sim \mathrm{Logistic}(\eta_i,1)$ , where the linear predictor $\eta_i$ is the location. Here are the density functions for the scores of two students, $Z_1$ and $Z_2$ : You don't get to observe the value of $Z_i$ directly (it's a latent variable). Instead, imagine that you have cutpoints $\zeta_1 along the horizontal axis, and you only know which group $Z_i$ falls into. This is your independent variable, $Y_i$ . Specifically, you observe: $$ Y_i = \begin{cases} 1 & \text{if } Z_i The plots below show that $P(Y_1=2)>P(Y_2=2)$ , as $Z_1$ is likelier than $Z_2$ to fall between these two cutpoints. Notice that some cutpoints are closer together, so those categories will not be observed as often. We have: \begin{align*} P(Y=2) &= P(\zeta_1 where $\sigma$ is the CDF of the $\mathrm{Logistic}(0,1)$ distribution, i.e. the inverse of the logit function. Notice that we have $$ P(Y>k)=\sigma(\eta-\zeta_k) \Leftrightarrow \mathrm{logit}(P(Y>k))=\eta - \zeta_k\,. $$ A one-unit increase in GPA changes the linear predictor from $\eta$ to $\eta+\beta$ , so the log-odds of a student falling in a category higher than $k$ (for any $k$ ) increase by $\beta$ . It's also helpful to interpret the parameters by comparing individual cases, e.g., the probability of each value of $Y$ for a student who is average in every predictor, versus one who the same but with a GPA that is one point higher.
