[site]: crossvalidated
[post_id]: 197353
[parent_id]: 197261
[tags]: 
To address one of your problems, which is performance estimation, I suggest you perform the hold-out procedure multiple times (with different splits) to get an idea of the variance of the performance estimation. You can use the average over all iterations as your final performance estimate. Even better would be to use cross-validation and to repeat it multiple times (5 folds may be reasonable in your case). It is not clearly stated in the question, but I assume you do not actually perform any model selection (that is, you train only a single model on your train data). In case you are actually also performing model selection, you have to keep an additional validation set which is used to select the best hyper-parameter configuration. In this case I would suggest you to use a nested cross-validation protocol for performance estimation and a cross-validation for model selection (see this post for a description of the procedure). Again, I would suggest to repeat the procedure multiple times.
