[site]: datascience
[post_id]: 51691
[parent_id]: 51677
[tags]: 
We let $$a=\operatorname{Softmax}(z)$$ that is $$a_i = \frac{e^{z_i}}{\sum_{j=1}^N e^{z_j}}.$$ $a$ is indeed a function of $z$ and we want to differentiate $a$ with respect to $z$ . The interesting thing is we are able to express this final outcome as an expression of $a$ in an elegant fashion. If you look at the section of "Derivative of Softmax Function" in your link , using quotient rule: We can see that if $i=m$ , \begin{align}\frac{\partial a_i}{\partial z_m} &=\left(\frac{e^{z_i}}{\sum_{j=1}^N e^{z_j}}\right)\left(\frac{\sum_{j=1}^Ne^{z^j}-e^{z_m}}{\sum_{j=1}^N e^{z_j}} \right)=a_i(1-a_m)\end{align} if $i\ne m$ , \begin{align}\frac{\partial a_i}{\partial z_m} &=-\left(\frac{e^{z_m}}{\sum_{j=1}^N e^{z_j}}\right)\left(\frac{e^{z_i}}{\sum_{j=1}^N e^{z_j}} \right)=-a_m(a_i)\end{align} If you want to evaluate things in terms of $z$ , you can still use the middle term though using the formula $$\frac{da}{dz}=ae^T\circ (I-ea^T)$$ is more elegant.
