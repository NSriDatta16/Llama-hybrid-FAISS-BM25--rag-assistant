[site]: datascience
[post_id]: 122895
[parent_id]: 
[tags]: 
Gradients of lower layers of a CNN when gradient of an upper layer is 0?

Say we have a convolutional neural network with an input layer, 3 convolutional layers and an output layer. Say the gradients with respect to the weights and biases of the third convolutional layer are all 0. Then, by backpropagation and the chain rule, does this necessarily mean that the gradients with respect to the weights and biases of the first two convolutional layers are 0 too?
