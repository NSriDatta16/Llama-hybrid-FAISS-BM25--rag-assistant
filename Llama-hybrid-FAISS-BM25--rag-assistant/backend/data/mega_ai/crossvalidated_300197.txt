[site]: crossvalidated
[post_id]: 300197
[parent_id]: 
[tags]: 
Overfitting on the loss graph, but not the accuracy graph

I am looking at learning curves ( CNN for text classification , which is based on this paper ) and trying to play with regularization to prevent overfitting. This model uses L2 regularization and dropout. What is interesting is that by looking at the accuracy graph I cannot really tell which model is the best. On the other hand the loss graph shows some differences. See pictures below. Here are my question: should we always look at the loss curves to check for overfitting? the accuracy graph is not very precise because accuracy is discrete and a lot of information gets lost when we compute it?
