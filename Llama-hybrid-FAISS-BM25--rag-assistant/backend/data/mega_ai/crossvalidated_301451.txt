[site]: crossvalidated
[post_id]: 301451
[parent_id]: 301442
[tags]: 
The problem with spurious variables in Random Forest is that each tree selects only a random subset of features , and if their number gets high, so if most of them are bogus (which is most likely the case here, since most of higher n-grams will occur only a bunch of times), some trees won't learn anything useful. Random Forests are used because single decision tree is highly likely to overfit, and averaging predictions in decreases variance compared to using a single estimator. It doesn't seem likely that your single decision trees are overfitting, since they are using only a tiny random subset of huge, mostly irrelevant feature space. You didn't say anything about the trees - did you try using more of them and simultaneously limiting their depth? Scikit-learn's Random Forests have a couple of parameters that you can tune. Another question would be if you really actually need decision trees - I for example would at least try using logistic regression (with Lasso/ElasticNet) for such problem, as these methods naturally fit such sparse problems - they consider all the features, and do feature selection themselves.
