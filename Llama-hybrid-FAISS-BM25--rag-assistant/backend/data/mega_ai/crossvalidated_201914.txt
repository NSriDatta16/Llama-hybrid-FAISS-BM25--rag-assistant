[site]: crossvalidated
[post_id]: 201914
[parent_id]: 
[tags]: 
Gradients of cross-entropy error in neural network

Neural network with a single hidden layer of logistic units being used for a multi–class classification problem: \begin{align} h &= \sigma (W^{(1)} x+b^{(1)}) \\[5pt] \hat y &= {\rm softmax}(W^{(2)}h + b^{(2)}) \end{align} and trained using the cross–entropy error: $$ C(y,\hat y) = -\sum_i y_i \log \hat y_i $$ I need to find the gradients of the error with respect to the parameters in the first layer, i.e., the layer closest to the input. The output target $y$ is a one-hot representation. Was given this additional info: $$ \frac{\partial C}{\partial z} = y - \hat y $$ where $$ z = W^{(2)}h + b^{(2)} $$
