[site]: crossvalidated
[post_id]: 45015
[parent_id]: 44999
[tags]: 
This is really two questions: (1) What is the definition of a p-value? Answer: Definition 2—the probability under the null hypothesis of getting a value of the test statistic greater than or equal to that observed. (As @whuber pointed out, it needs some qualification: in the case of a composite null hypothesis the probability involved is the maximum probability over every point null in that set; the probability of what's sometimes called the proximal null hypothesis.) (2) Should a test statistic strictly increase with decreasing probability under the null hypothesis ? I have tried to answer this in responses to your previous post. (Answer: not always.) Hope someone can explain more clearly if needed. At least note here that many commonly used test statistics don't. You have ... (a) test statistics ordered by the probability under the null: Fisher's Exact Test, as Greg Snow notes, & the test for a binomial parameter given by Zag. (b) test statistics ordered by likelihood ratio (sometimes but not always giving the same ordering as (a)): my binomial goodness-of-fit test example. (c) test statistics chosen for maximum power against specified alternatives (sometimes but not always giving the same ordering as (a) and/or (b), as I think RobertF was getting at): 'The Emperor's new tests', Perlman & Wu (1999), together with the comments & rejoinder, is very interesting (though difficult). If you read the paper by Christensen that Zag linked to, you will see that in the first example he writes "With only this information, one must use the density itself to determine which data values seem weird and which do not". The clear implication is that with more information you needn't necessarily use the density itself to determine which data values seem weird and which do not. In response to @whuber's comment ... The likelihood ratio test is in fact a good example of Defn 2's being used. The p-value in this case is just the probability (under the null) of the the likelihood ratio's being larger or equal to that observed. As an elementary example, you can test two hypotheses for the probability of success in a Bernoulli trial : $$H_0: \theta = 0.55$$ $$H_1: \theta = 0.35$$ Nine independent trials give $t$ successes: $$\newcommand{\pr}{\mathrm{Pr}}\begin{array}{cccc} t & \pr(t|H_0) & \pr(t|H_1) & \frac{\pr(t|H_1)}{\pr(t|H_0)}=x\\ 0 & 0.00076 & 0.02071 & 27.372\\ 1 & 0.00832 & 0.10037 & 12.060\\ 2 & 0.04069 & 0.21619 & 5.3128\\ 3 & 0.11605 & 0.27162 & 2.3406\\ 4 & 0.21276 & 0.21939 & 1.0312\\ 5 & 0.26004 & 0.11813 & 0.4543\\ 6 & 0.21188 & 0.04241 & 0.2001\\ 7 & 0.11099 & 0.00979 & 0.0882\\ 8 & 0.03391 & 0.00132 & 0.0389\\ 9 & 0.00461 & 0.00008 & 0.0171 \end{array}$$ The likelihood ratio $x$ is your test statistic. Using Defn 1 to get a p-value, you have to add up all the probabilities (under the null) for less (or equally) probabable values of $x$ than that observed. So, observing $t = 2$, you'd add up those for $2$, $8$, $1$, $9$, & $0$ successes to give $0.04069 + 0.03391 + 0.00832 + 0.00461 + 0.00076 = 0.08829$ Using Defn 2, you add up all the probabilities (under the null) for values of $x$ larger than (or equal to) that observed. So, observing $t = 2$, $x$ is larger for $0$ & $1$ successes so you add their probability under the null to that of $2$ successes to give a p-value of $0.04069 + 0.00832 + 0.00076 = 0.04977$. It's clear that the latter procedure is the likelihood ratio test as usually understood, & that defined by the Neyman–Pearson lemma.
