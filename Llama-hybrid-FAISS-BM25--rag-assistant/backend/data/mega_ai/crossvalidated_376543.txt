[site]: crossvalidated
[post_id]: 376543
[parent_id]: 373850
[tags]: 
I'm not an expert I'll try to answer your questions. :) 1) I believe it can happen, as redundant units are very common in neural networks. In another paper referenced by the transformer paper , it addresses this issue by adding a regularization term to the loss $p=\mid\mid AA^T-I\mid\mid$ , which penalizes redundancy in matrix A. 2) It should be the full vector, since the dimension of the weight matrix is $d_{model}\times d_k$ . Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality. Also if we split the vector before attention, the computational cost will be reduced by a factor h.
