[site]: datascience
[post_id]: 117949
[parent_id]: 
[tags]: 
Requirements for variable length output in transformer

I have been working on modifying the transformer from the article The Annotated Transformer . One of the features I would like to include is the ability to pass a sequence of fixed length, and receive an output sequence of a shorter length, which is possible per this reference . In my case, I am using a sequence of 10 randomly generated integers 0-9 for the input (just like the article) and trying to return a sequence of five 2s (this is the simplest attempt to get an output of a shorter length I could think of). The start of the sequence is denoted as 1, and the end of the sequence is not defined. I am successfully able to send the encoder the "source" batch tensor, and the decoder the "target" batch tensor consisting of only 5 columns in the batch tensor. The transformer will train on this data, but it returns a sequence of length equal to the input. What are the requirements of the transformer network to output a sequence of length that is not equal to the length of the input sequence? Thanks in advance for any assistance
