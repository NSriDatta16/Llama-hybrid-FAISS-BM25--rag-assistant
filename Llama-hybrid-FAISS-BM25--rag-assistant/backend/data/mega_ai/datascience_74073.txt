[site]: datascience
[post_id]: 74073
[parent_id]: 73990
[tags]: 
Like a Russian Matryoshka doll, there can be many layers of data partitioning and model selection you can use with a dataset. For a "simple" approach, you can first partition your data into train and test sets. Within your training set, you can use cross validation to choose the best hyperparameters for each model. Once you choose your best hyperparameters, you can see how well the model with those hyperparameters performs on the test set. You can repeat this process to build models with many approaches, such as a linear model, random forest, SVM, etc. The model that performs best on the test set will be your "best" model to deploy. To be clear: you do NOT tune on the test set. This would lead to overfitting. You would simply use the test set to compare a few different models. However, the more you look at the test set, the more risk of overfitting.
