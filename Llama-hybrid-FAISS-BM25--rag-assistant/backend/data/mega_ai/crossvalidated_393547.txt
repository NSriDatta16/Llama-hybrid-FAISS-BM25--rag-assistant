[site]: crossvalidated
[post_id]: 393547
[parent_id]: 
[tags]: 
How to interpret the Matthews Correlation Coefficient (MCC) for an imbalanced data set

I am trying to assess the performance of a machine learning model that has been passed down. The XGBoost model was trained on data that had a class imbalance of 84% majority class (label 0: 117,409 samples) and 16% minority class (label 1: 22,233) without oversampling the minority class or undersampling the majority class. I am more interested in the minority class. I found this paper: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5456046/ that simulates predictions using 3 methods to check various metrics' robustness against imbalanced data. Accuracy and f1 score were affected by class imbalance, whereas AUC and MCC were robust (yielded consistent values) across various ratios of class imbalance in the data. When looking up Matthews Correlation Coefficient (MCC), most of the answers I have found are either in relation to how to calculate the MCC or what the values of the MCC mean, such as from Wikipedia, https://en.wikipedia.org/wiki/Matthews_correlation_coefficient : +1 represents a perfect prediction 0 is no better than random prediction âˆ’1 indicates total disagreement between prediction and observation However, I am looking for more clarity with regards to interpretation of Matthews Correlation Coefficient, especially with regard to imbalanced data. I have the predicted probabilities from the model, so I have chosen different classification thresholds and then calculated the MCC for the predicted labels at each probability threshold. The MCC varies based on classification threshold between 0.04 to 0.3. The MCC increases from about 0.04 at the lowest classification threshold to about 0.3 at a probability threshold of 0.2 for classification, and then MCC decreases with increasing probability threshold. For now let's just look at the MCC of 0.3 at a 20% probability threshold. Which of these would be correct in interpreting the MCC (without considering the data imbalance)? Since the MCC is calculated overall from considering all of the predictions: "the classifier/model (at 20% classification threshold) is able to classify about 30% of the data correctly." "the classifier/model has about 30% predictive power." If none are correct, please clarify by adding the correct interpretation. Next I am looking for clarification about how to interpret the MCC in relation to the imbalanced data. Do I need to adjust my interpretation of how well the model is able to classify the data? Since the class imbalance is 16% minority class, random chance would likely predict the minority class about 16% of the time. Would one need to correct for this and say that the model is correct in its predictions 30% (MCC) - 16% (class imbalance) = 14% of the time? I don't think this is correct, but I need more clarity around this. (The thought process here is if I calculated accuracy, and got a value of say, 90%, I would interpret that as actually only being 6% better than random chance.) Regardless of opinion whether 84%/16% is considered imbalanced or not, I'm more interested in the conceptual understanding, so would still appreciate insight and comments regarding imbalanced data sets, whether that's for a 95%/5% imbalance or a 84%/16% imbalance. Thanks in advance for any insights!
