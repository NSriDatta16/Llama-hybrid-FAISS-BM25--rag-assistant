[site]: datascience
[post_id]: 41897
[parent_id]: 41892
[tags]: 
Firstly, LSTM is perfect for time series as a sequence model. You can do classification with LSTM. Let's say you have 3 classes in your labels. You want the probabilities of each class as predictions. What you can do is to have a usual neural network combined with the LSTM at the end. The last layer will have 3 nodes/neuron which you will use 'softmax' activation function for the classification task. Here I share my Keras code for my multclassification task as an example: nn = Sequential() nn.add(LSTM(120, batch_input_shape=(64,100,18), return_sequences=True)) nn.add(LSTM(80)) nn.add(Dropout(0.2)) nn.add(Dense(100, activation='relu')) nn.add(Dropout(0.3)) nn.add(Dense(40, activation='relu')) nn.add(Dropout(0.2)) nn.add(Dense(10, activation='relu')) nn.add(Dense(3, activation = 'softmax')) opt = Adam(lr = 0.001, decay = 1e-5) nn.compile(loss='categorical_crossentropy', optimizer= 'adam') We have 2 LSTM layers, and 4 layers of DNN, yet you could just use the last layer of DNN to get an output. If you want binary classification, just change the last layer with: nn.add(Dense(1, activation = 'sigmoid')) And you will have if the probability of the class 1 occurring at the output. If you do not know Dropout, do not get scared, it is some strategy for dealing with overfitting. If you have any further questions, I will be around. Please do not hesitate to ask.
