[site]: crossvalidated
[post_id]: 632970
[parent_id]: 
[tags]: 
Using MCMC-derived posterior to design variational approximation function

I am trying to fit a hierarchical model that estimates the covariance of some parameters, using the probabilistic programming language pyro. In simulation experiments, I saw that the MCMC generates good posterior samples that estimate the ground truth covariance well. However, to scale up the model to high-dimensional data, I was trying to design a variational approximation (guide) to implement stochastic variational inference (SVI). Standard mean-field approximations do not yield correct estimates, so I think that they are not complex enough to describe the posterior. In order to understand which latent variables interactions to include in the approximation, would it make sense to look for these interactions in the posterior samples generated with MCMC and generalizing those to the high-D settings? In other words, let's say that I look into the MCMC posterior samples and observe that there is an approximately linear relationship between variables $A$ and $B$ , I could impose this same relationship in the variational approximation. I have never heard of this approach, as usually I knew that one tends to use SVI to get a fast approximate fit and then MCMC to refine, whereas this would be the reverse process.
