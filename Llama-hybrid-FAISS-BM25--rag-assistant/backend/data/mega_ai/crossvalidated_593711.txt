[site]: crossvalidated
[post_id]: 593711
[parent_id]: 
[tags]: 
Should you adjust predicted probabilites when evaluating feature calibration when you have a prediction problem

I am testing different ways to estimate the calibration of of machine learning models (binary classifiers, both during training and once it is in production). The problem I have come across is how to handle when you have a general prediction error (in production). For example: I have a model predicting probabilities. My average prediction is 50% but the observerd outcome is 60%. If you look at feature 1 it looks very well calibrated besides the fact that the model is a factor 1.2 off in its predictions. Feature 2 seems to be worse calibrated. However if I use some sort of metric that measures how much error I have (for exaple R2) many will say the the error is the same on both features. I could multiply all my predicted probabilites with 1.2 to adjust for the general error. If I do this the metrics will clearly be able to differentiate calibration level. The assumption then is that the general error is at the same level for each bucket when bucketing a certain feature. Is it ok to make this assumption? Is there some other way to approach the problem? EDIT: What I mean with lookin at calibration for a certain feature whether or not it was included in the model. Imagine you have a model trying to predict the probability that a customer wants to purchase a certain product. Included in the model is gender, social status, income, number of recent purchases (as an example). You have a bunch of different other potential features that you did not include for example, age, height, star sign. You could then bin predictions on age, 10-20, 20-30... in each bucket you will have a predicted probability and an observed probability. If the predicted probability in these bucket differs a lot from the observed probabilities you could conclude that there is some information in age that you were missing and that for some reason did not come through when you trained your model.
