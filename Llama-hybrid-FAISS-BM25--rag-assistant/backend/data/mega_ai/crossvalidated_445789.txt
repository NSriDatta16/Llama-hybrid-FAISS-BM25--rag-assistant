[site]: crossvalidated
[post_id]: 445789
[parent_id]: 444829
[tags]: 
I think you are somehow mixing independence and conditional independence. The idea is not that $X_{n+1}$ is independent from $X_0 ... X_{n-1}$ , but that it is conditionally independent given $X_n$ . This simply means that, once you know the value of $X_n$ , the previous values $X_0...X_{n-1}$ are irrelevant for the distribution of $X_{n+1}$ . Taking your example, let us assume that $X_n = 5$ . Now, when we want to predict the value of $X_{n+1}$ , knowing the values of $X_0...X_{n-1},Z_0...Z_{n-1}$ is completely irrelevant. Nothing changes in your prediction if $X_{n-1}=4$ or $X_{n-1}=5$ , once you know that $X_n = 5$ . Now: $$\mathbb{P}(X_{n+1}=x_{n+1}|X_0=x_0,...,X_n=x_n)$$ $$\mathbb{P}( \sum_0^{n+1}Z_i =x_{n+1}|X_0=x_0,...,X_n=x_n)$$ $$\mathbb{P}( X_n + Z_{n+1} =x_{n+1}|X_0=x_0,...,X_n=x_n)$$ $$\mathbb{P}( Z_{n+1} =x_{n+1}-x_n|X_0=x_0,...,X_n=x_n)$$ $$\mathbb{P}( Z_{n+1} =x_{n+1}-x_n|X_n=x_n)$$ $$\mathbb{P}( X_{n+1} =x_{n+1}|X_n=x_n)$$ We were able to remove the conditioning on all $X_i$ for i smaller than $n$ because our probability is only a function of $Z_{n+1}$ (which is independent from them as a consequence of its independence from all $Z_i$ ), and of $x_n$ which is a given value. This gives you a Markov Chain with infinite states, where every state has a transition probability of $p$ to the next state (+1) and $1-p$ to itself. The same reasoning applies to $\Pi_n$ . This will instead only have two states ${0,1}$ . State $0$ is an attractor, so it has transition probability $p=1$ to itself, while from state $1$ has probability $p$ of transition to itself, and probability $1-p$ of going to state $0$ . Now, (3) is NOT a Markov chain. $$\mathbb{P}(X_{n+1}=x_{n+1}|X_0=x_0...X_n=x_n)$$ $$\mathbb{P}(X_n + S_n+Z_{n+1}|X_0=x_0 ... X_n=x_n)$$ Here, however, we have no way of knowing the value of $S_n$ just by $X_n$ . Knowing for example that $X_n=10$ would simply mean that $\sum_0^nS_i=10$ , but we cannot infer the value of $S_n$ without peeking at the previous "history". Therefore, the markovian property does not hold. (4), on the other hand, is a Markov Chain (I will use $(x,y)$ notation since now $X$ is multivariate). $$\mathbb{P}(X_{n+1}=(x_{n+1},y_{n+1})|X_0=(x_{0},y_{0})...X_n=(x_{n},y_{n}))$$ $$\mathbb{P}((S_{n+1},\sum_0^{n+1}S_i)=(x_{n+1},y_{n+1})|X_0=(x_{0},y_{0})...X_n=(x_{n},y_{n}))$$ $$\mathbb{P}((S_{n}+Z_{n+1},\sum_0^{n}S_i+S_{n}+Z_{n+1})=(x_{n+1},y_{n+1})|X_0=(x_{0},y_{0})...X_n=(x_{n},y_{n}))$$ $$\mathbb{P}((x_{n}+Z_{n+1},y_n+x_{n}+Z_{n+1})=(x_{n+1},y_{n+1})|X_0=(x_{0},y_{0})...X_n=(x_{n},y_{n}))$$ Again, this is only a function of $Z_{n+1}$ (which is independent of previous $S_i$ ) and of $(x_n,y_n)$ , and we can therefore remove conditioning on previous values as they are redundant. $$\mathbb{P}((x_{n}+Z_{n+1},y_n+x_{n}+Z_{n+1})=(x_{n+1},y_{n+1})|X_n=(x_{n},y_{n}))$$ $$\mathbb{P}(X_{n+1}=(x_{n+1},y_{n+1})|X_n=(x_{n},y_{n}))$$ Which makes it Markovian. I hope the logical passages are clear enough, otherwise let me know!
