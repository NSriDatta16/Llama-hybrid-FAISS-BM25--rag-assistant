[site]: crossvalidated
[post_id]: 415628
[parent_id]: 
[tags]: 
LeNet5 "symmetry breaking" step, is (was) it important?

Reading through Yann LeCun's original paper on LeNet5 , I have come across something that I haven't seen before in convolutional neural network architectures. (Maybe that's just because I'm late to the party.) Table I on page 8 shows that the connections between layers S2 and C3 are deliberately incomplete. The authors state that this incomplete connection serves two purposes: 1) To reduce the size of the tensor being passed between these layers (obviously a more pressing concern in 1998 than today), and 2) To "break symmetry" in the model, forcing different and complimentary connections between the features in these two layers. I have looked at several web pages where people implement what they call "LeNet5", and none of them seem to bother with this symmetry-breaking step. It could be a bit challenging to implement with standard CNN software such as Keras, freezing only some of the weights and allowing others to vary. None of the more modern CNN visual processing architectures I have examined (AlexNet, Inception, etc.) seem to bother with symmetry-breaking either. Symmetry-breaking vaguely resembles a regularization technique, at least to me. Once we get to using dropout or batch normalization, are we effectively accomplishing the same goal as symmetry-breaking by different means?
