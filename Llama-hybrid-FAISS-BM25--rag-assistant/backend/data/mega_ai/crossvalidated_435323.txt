[site]: crossvalidated
[post_id]: 435323
[parent_id]: 
[tags]: 
Why does logistic regression with a logarithmic cost function converge to the optimal classification?

Why does logistic regression with a logarithmic cost function converge to the optimum for the classification problem (i.e. minimum number of mislabeled training samples)? Put differently, why is the optimum for the probabilities $h_\Theta(x) = P(y = 1|x;\Theta)$ equivalent to the optimum for the classification problem? My line of thought For the terminology and formulas, see Andrew Ng coursera course on machine learning (relevant slides: https://github.com/vkosuri/CourseraMachineLearning/blob/master/home/week-3/lectures/pdf/Lecture6.pdf ). If we use the convex cost function $J(\Theta) = -1/m * \Sigma_{i=1}^m ( y^{(i)} log h_\Theta(x^{(i)}) + (1 - y^{(i)}) log (1 - h_\Theta(x^{(i)})) )$ , we exponentially penalize higher differences between $h_\Theta(x)$ and $y$ . Thus this logistic regression optimizes the parameters $\Theta$ for the probabilities $h_\Theta(x) = P(y = 1|x;\Theta)$ . But for the (in this example binary) classification problem, we use a threshold of $P=0.5$ to decide whether a sample is classified as positive or negative. So the classification problem does not care whether $P=0.4$ or $P=0$ , however the cost function makes an exponential distinction. Thus I do not understand why logistic regression with a logarithmic cost function also converges to the optimum for the classification problem . For example, $\Theta$ that yields many samples with $h_\Theta(x)=0.4$ and $y=1$ , has lower cost (due to the exponential distinction) compared to $\Theta'$ that yields $(0.6,1)$ for most of those samples and $(0.1,1)$ for only few of those samples. So $J(\Theta')$ is higher, but $\Theta'$ classifies much better in terms of amount of false predictions over the training set. Given one bad optimization step, logistic regression should not be able to converge to the optimal classification!?
