[site]: crossvalidated
[post_id]: 574899
[parent_id]: 574786
[tags]: 
Is there a reason why we need to apply Jensen's Inequality here? Can't we just sample approximate the term like it is? There are two possibilities for what you’re asking here. I’ll address both. Why do we need Jensen’s inequality? To ensure that this is in fact a bound. If the optimization objective weren’t a bound, then there wouldn’t be much point in optimizing it. Speaking loosely, think of lifting a handful of sand. If it’s not a lower bound, sand slips through the gaps between your fingers. Why do we need to bound the likelihood (using Jensen’s inequality) instead of optimizing it directly by a Monte Carlo method? Well, we assume that the joint probability factors as $p(x,z)=p(z)p(x\mid z)$ . That posterior is the (computationally) hard part. In general, we can’t assume that we can normalize it. But maybe we can sample from it? In fact, what you’re proposing is common! MCMC is often used to approximate the posterior. But if there’s a large amount of data or a complex model, that becomes very slow. That’s when people switch to VI.
