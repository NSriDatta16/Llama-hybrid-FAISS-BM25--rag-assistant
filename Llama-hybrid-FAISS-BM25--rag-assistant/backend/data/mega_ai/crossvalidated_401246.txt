[site]: crossvalidated
[post_id]: 401246
[parent_id]: 401084
[tags]: 
Yes, and it's easy to see why if we look at it the right way. Forget about the M-step; in fact, forget the fact that we're inside an iterative algorithm at all. Forget about the past and that you ever had other estimates for $\vec{\mu}$ or $\mathbb{\Sigma}$ . And forget that there are other classes, and just focus on one class. And forget that the weights can be interpreted as $p(m|\vec{x}_i;\theta)$ and just think of them as sample weights, origin unknown, called $w_i$ . For convenience let's also assume that these weights are normalized, e.g. $\sum w_i = 1$ . What remains is a sample of $n$ (instead of $T$ so I can reserve $T$ for "transpose") observations $\vec{x}_i$ with weights $w_i$ which we believe came from a multivariate distribution $\mathcal{N}(\vec{\mu}, \mathbb{\Sigma})$ . What is the procedure for estimating $\vec{\mu}$ and $\mathbb{\Sigma}$ ? Well, it's to first calculate the mean, subtract it off, then average over the outer products: $$ \vec{\mu}= {1 \over {n}}\sum_{i=1}^n w_i \vec{x}_i \tag{1} $$ $$ \mathbb{\Sigma} = \frac{1}{n} \sum_{i=1}^n w_i ( \vec{x}_i - \vec{\mu} )(\vec{x}_i - \vec{\mu})^T \tag{2} $$ This is the ML estimate . Note that the $\vec{\mu}$ in (2) is exactly the same as the $\vec{\mu}$ in (1) - this is a closed form solution, not an iterative algorithm! Any other estimate for $\vec{\mu}$ and $\mathbb{\Sigma}$ will have lower likelihood, by definition. Now we can lift our self imposed amnesia and recall that we are using E-M to numerically approximate the maximum likelihood of a GMM. For that one particular M step, we held all other parameters constant and maximized the likelihood with respect to just $\vec{\mu}$ and $\mathbb{\Sigma}$ . Because we used the closed form solution of (1) and (2), we didn't take a "step" towards that maximum - we jumped straight to the maximum within that subspace. In particular we can guarantee that likelihood either increased or stayed the same. We need this property because it is necessary for the convergence of the E-M algorithm as a whole. Using the closed form ML estimate for the multivariate distribution is by easiest way to prove it, and any other procedure for updating $\vec{\mu}$ and $\mathbb{\Sigma}$ - such as using the $\mu$ left over from the previous step - would require a separate proof. One good resource on GMM and the EM algorithm I used was this Stanford Lecture by Andrew Ng . I've linked to the part of the lecture where he shows this update step in particular but the whole lecture is worth watching.
