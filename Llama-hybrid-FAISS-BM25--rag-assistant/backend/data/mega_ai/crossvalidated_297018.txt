[site]: crossvalidated
[post_id]: 297018
[parent_id]: 
[tags]: 
Dirichlet Process vs. Mixture Models with Many Mixtures

The Dirichlet Process prior is a Bayesian non-parametric prior to model your data as coming from an infinite mixture of distributions. Since your data is finite, only a finite number of these mixture components will be populated. How is this practically different from a finite mixture model where the number of mixture components is equal to your data? If you put a prior on the mixture weights of this finite mixture, you will learn their posterior. Undoubtedly many of these weights will be tiny and their corresponding mixture will be effectively unpopulated. A sampler for the finite mixture will be easier to write and won't have the same "mixture label swapping" problem that the the Dirichlet Process will have. The Dirichlet Process must have some other advantage - what am I missing?
