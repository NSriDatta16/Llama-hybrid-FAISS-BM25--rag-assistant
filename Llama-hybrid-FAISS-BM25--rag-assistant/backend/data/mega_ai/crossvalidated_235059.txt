[site]: crossvalidated
[post_id]: 235059
[parent_id]: 
[tags]: 
How do I "remove" the effect of a predictor which is highly correlated in a time series?

I apologize in advance for any errors in terminology. Please let me know if you require further information. I have time-series data in which a probe measures the concentration of oxygen in solution, over time. The probe also measures the temperature of the solution at each time point. In this case, oxygen is being produced by a particular chemical reaction. It is also possible for us to probe a "null" experimental control by not inducing the reaction in the solution. The problem is that the oxygen sensitivity of the probe also depends on the temperature of the solution, and during the experiment, temperature also changed with time. What I have found is that the oxygen concentration is highly correlated with the change in temperature, but I what I would like is instead to find the "true" change in oxygen concentration, independent of the effects of temperature. How would I go about this? EDIT: There appears to be a strong linear relationship between temperature and oxygen concentration. I have considered doing a two-sample t-test between a reactionless control solution and a solution with the reaction. Assuming that both treatments are kept at the same temperature the entire time, would this method be appropriate for determining whether there is a statistically significant difference in oxygen concentration due to the reaction?
