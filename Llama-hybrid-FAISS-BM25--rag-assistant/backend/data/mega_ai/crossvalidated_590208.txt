[site]: crossvalidated
[post_id]: 590208
[parent_id]: 590204
[tags]: 
Because traditional deep learning methods do not take into account a crucial property of the adjacency matrix representation: node permutation equivariance. One axiom of deep learning is that inductive biases tend to help. This was the case with convolutions in computer vision, recurrence in timeseries, trigonometric bases in implicit neural representations. So it made sense to incorporate this into the architecture of Graph Neural Networks, in the hopes that it would help with performance and/or efficiency. Some of these operations can also be directly derived from group theory (see Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges, 10.48550/arXiv.2104.13478 and Gauge Equivariant Convolutional Networks and the Icosahedral CNN, 10.48550/arXiv.1902.04615 for examples).
