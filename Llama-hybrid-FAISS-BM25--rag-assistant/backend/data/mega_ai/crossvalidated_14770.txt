[site]: crossvalidated
[post_id]: 14770
[parent_id]: 
[tags]: 
MCMC sampling of decision tree space vs. random forest

A random forest is a collection of decision trees formed by randomly selecting only certain features to build each tree with (and sometimes bagging the training data). Apparently they learn and generalize well. Has anybody done MCMC sampling of the decision tree space or compared them to random forests? I know it might be computationally more expensive to run the MCMC and save all the sampled trees, but I am interested in the theoretical features of this model, not the computational costs. What I mean is something like this: Construct a random decision tree (It would probably perform horribly) Compute likelihood of the tree with something like $P(Tree|Data) \propto P(Data|Tree)$, or perhaps add a $P_{prior}(Tree)$ term. Choose a random step to change the tree and select based on the likelihood $P(Tree|Data)$. Every N steps, save a copy of the current tree Go back to 3 for some large N*M times Use the collection of M saved trees to do prediction Would this give a similar performance to Random Forests? Note that here we are not throwing away good data or features at any step unlike random forests.
