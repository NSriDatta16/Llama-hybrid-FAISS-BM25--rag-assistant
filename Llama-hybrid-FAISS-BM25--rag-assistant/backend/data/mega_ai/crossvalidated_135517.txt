[site]: crossvalidated
[post_id]: 135517
[parent_id]: 135476
[tags]: 
There is no shortage of optimizers in R. I will let a more knowledgeable R user recommend the most appropriate one(s). A cheap solution is to iteratively reweight your data, treating $r_i^{(k)}=\exp\bigl[-\alpha^{(k)} y_i\hat{g}^{(k-1)}(x_i)\bigr] $ as a fixed weight within iteration $k$ (I took a liberty to move $\alpha$ from the denominator to the numerator), where $\hat{g}^{(k-1)}=x^T\hat\beta^{(k-1)}$ is the result of the weighted least squares from the previous iteration. So Set iteration counter $k=0$, $\alpha^{(k)}=0$, run OLS to get $\hat\beta^{(0)} \equiv \hat\beta_{\rm WLS}$ Update $k \leftarrow k+1$, $\alpha^{(k)}=\alpha$, $r_i^{(k)}=\exp\bigl[-\alpha^{(k)} y_i\hat{g}^{(k-1)}(x_i)\bigr] $ Run weighted least squares $\sum_{i=1}^n w_i r_i (y_i - x'\beta)^2 \to \min_\beta$ Iterate 2--4 a few times until the coefficients and/or the weights $r_i$ stabilize. As an option to improve stability a little bit, you can devise a trajectory for $\alpha^{(k)}$, so that for the first say 10 iterations, $\alpha^{(k)}=k/10 \cdot \alpha$ for your selected ultimate $\alpha$, and then you stick $\alpha^{(k)}=\alpha$ for the remaining $k=11,12,\ldots$ Given how quick regression is, I won't be surprised to see that this converges faster than the full non-linear optimization. The standard errors, however, will be too small, as they don't account for uncertainty in $r_i$ (due to uncertainty in $\hat\beta$ that the weights $r_i$ utilize internally). You can get the right standard errors from the sandwich variance formula. Finding the "right" value of $\alpha$ must come from the same (substantive) considerations which dictate the need for asymmetric treatment of the regression error. Given that you did not explain what these are, I am not sure how to advise on that.
