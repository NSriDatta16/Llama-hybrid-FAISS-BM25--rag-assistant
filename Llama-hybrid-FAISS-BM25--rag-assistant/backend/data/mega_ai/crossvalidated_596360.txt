[site]: crossvalidated
[post_id]: 596360
[parent_id]: 596346
[tags]: 
If you use K-fold cross validation (CV) for hyper parameter tuning, you should train a single model on the entire training set with the best found hyper-parameters and test on the test set. If you use K-fold CV for performance evaluation (like in sklearn cross_val_score), then you don't need to split your dataset into train/test. The performance reported in each fold will be a test performance. People usually average them or get all the predictions and then evaluate the entire dataset. This is usually done to assess performance when the dataset is small and there isn't a single model output for this case, nor the aim is to have it.
