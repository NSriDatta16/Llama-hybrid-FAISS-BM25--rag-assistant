[site]: crossvalidated
[post_id]: 309115
[parent_id]: 285495
[tags]: 
Two suggestions to improve the model. Use average of word2vec to represent the item sequence may lose too much information. It would be worth trying to add an encoding layer to first get the encoded vector of each history item, then concatenate them and put into a dense layer and use that as the representation of history item sequence (you can choose a dimension say 200, 20 is too small). Enlarge the size of the LSTM. It would be better to first make sure the LSTM can overfit on the input data. From what you have described, the model may even not have learnt well even on the training set.
