[site]: crossvalidated
[post_id]: 520269
[parent_id]: 
[tags]: 
How do we conclude that a statistic is sufficient but not minimal sufficient?

I want to show that the statistic $\left(\sum_{i = 1}^n Y_i, \sum_{i = 1}^n Y_i^2 \right)$ is sufficient for $\mu$ but not minimal sufficient where $(Y_1, \dots, Y_n)$ is a random sample from $N(\mu, \mu)$ for $\mu > 0$ . I have already shown that the statistic $\sum_{i = 1}^n Y_i^2$ is minimal sufficient for $\mu$ where $(Y_1, \dots, Y_n)$ is a random sample from $N(\mu, \mu)$ for $\mu > 0$ . I began by calculating the joint density for $N(\mu, \sigma^2)$ : $$L(\mu, \sigma; \mathbf{y}) = \dfrac{1}{(2 \pi \sigma^2)^{n/2}} \exp{\left\{ -\dfrac{1}{2 \sigma^2} \sum_{i = 1}^n (y_i - \mu)^2 \right\}}$$ Switching to $N(\mu, \mu)$ , I calculated the likelihood ratio to be $$\dfrac{L(\mu; \mathbf{y_1})}{L(\mu; \mathbf{y_2})} = \exp{\left\{ \dfrac{1}{2 \mu} \left( \sum_{i = 1}^n y_{2i}^2 - \sum_{i = 1}^n y_{1i}^2 \right) + \left( \sum_{i = 1}^n y_{1i} - \sum_{i = 1}^n y_{2i} \right) \right\}}$$ This likelihood ratio does not depend on the parameter $\mu$ when $\sum_{i = 1}^n y_{2i}^2 = \sum_{i = 1}^n y_{1i}^2$ : $$\exp{\left\{ \dfrac{1}{2 \mu} \left( \sum_{i = 1}^n y_{2i}^2 - \sum_{i = 1}^n y_{1i}^2 \right) + \left( \sum_{i = 1}^n y_{1i} - \sum_{i = 1}^n y_{2i} \right) \right\}} = \exp{\left\{ \left( \sum_{i = 1}^n y_{1i} - \sum_{i = 1}^n y_{2i} \right) \right\}}$$ So, assuming I am understanding this correctly, if $\sum_{i = 1}^n Y_i^2$ was the only test statistic, then we could say that it is minimal sufficient. Now, to account for $\sum_{i = 1}^n Y_i$ , if we use $\sum_{i = 1}^n y_{2i} = \sum_{i = 1}^n y_{1i}$ , then we get $$\exp{\left\{ 0 \right\}} = 1$$ So, clearly, nothing changed between our calculations for the test statistic $\sum_{i = 1}^n Y_i^2$ and our calculation for the test statistic $\left(\sum_{i = 1}^n Y_i, \sum_{i = 1}^n Y_i^2 \right)$ – the likelihood ratio does not depend on the parameter $\mu$ . What exactly are we supposed to do to show that $\left(\sum_{i = 1}^n Y_i, \sum_{i = 1}^n Y_i^2 \right)$ is sufficient but not minimal sufficient? What exactly is the argument that we're supposed to present? I want to use the Fisher–Neyman factorization theorem , of the form $L(\mu; \mathbf{y}) = g(T(\mathbf{y}), \mu) \times h(\mathbf{y})$ , to factor $\exp{\left\{ \dfrac{1}{2 \mu} \left( \sum_{i = 1}^n y_{2i}^2 - \sum_{i = 1}^n y_{1i}^2 \right) + \left( \sum_{i = 1}^n y_{1i} - \sum_{i = 1}^n y_{2i} \right) \right\}}$ and show that the statistic $\left(\sum_{i = 1}^n Y_i, \sum_{i = 1}^n Y_i^2 \right)$ is sufficient for $\mu$ . So we immediately know that we have $T(\mathbf{Y}) = \left(\sum_{i = 1}^n Y_i, \sum_{i = 1}^n Y_i^2 \right)$ . And since $\dfrac{1}{2 \mu} \left( \sum_{i = 1}^n y_{2i}^2 - \sum_{i = 1}^n y_{1i}^2 \right)$ has the parameter $\mu$ , and $\left( \sum_{i = 1}^n y_{1i} - \sum_{i = 1}^n y_{2i} \right)$ has the data for $\sum_{i = 1}^n Y_i$ , I would say that we require $g(T(\mathbf{y}), \mu) = \dfrac{1}{2 \mu} \left( \sum_{i = 1}^n y_{2i}^2 - \sum_{i = 1}^n y_{1i}^2 \right) + \left( \sum_{i = 1}^n y_{1i} - \sum_{i = 1}^n y_{2i} \right)$ , and so we require that $h(\mathbf{y}) = 1$ . This way, we get the Fisher-Neyman factorization $$L(\mu; \mathbf{y}) = \left( \dfrac{1}{2 \mu} \left( \sum_{i = 1}^n y_{2i}^2 - \sum_{i = 1}^n y_{1i}^2 \right) + \left( \sum_{i = 1}^n y_{1i} - \sum_{i = 1}^n y_{2i} \right) \right) \times 1 = \left( \dfrac{1}{2 \mu} \left( \sum_{i = 1}^n y_{2i}^2 - \sum_{i = 1}^n y_{1i}^2 \right) + \left( \sum_{i = 1}^n y_{1i} - \sum_{i = 1}^n y_{2i} \right) \right)$$ And so this shows that $\left(\sum_{i = 1}^n Y_i, \sum_{i = 1}^n Y_i^2 \right)$ is a sufficient statistic.
