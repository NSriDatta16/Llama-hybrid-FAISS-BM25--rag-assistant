[site]: crossvalidated
[post_id]: 261692
[parent_id]: 
[tags]: 
Gradient descent for logistic regression partial derivative doubt

I'm a software engineer, and I have just started a Udacity's nanodegree of deep learning. I have also worked my way through Stanford professor Andrew Ng's online course on machine learning and now I'm comparing. I have a big doubt about Gradient Descent with sigmoid function because on Andrew Ng's course it is different from the one I see on Udacity's nanodegree. From Andrew Ng's course, gradient descent is ( First formula ): But, from Udacity's nanodegree is ( Second formula ): Note : first picture is from this video , and second picture is for this other video . But in this CS229 course notes from Andrew Ng's, on page 18, I have found the demonstration from Andrew Ng's gradient ascent formula. I only add it here as a demostration: Note : the formula above is for Gradient Ascent. I'm not sure if I have understood everything, but in this derivative I see have the derivative from f function disappears (f function is the sigmoid function). But in Udacity's nanodegree they continue using the sigmoid's derivative in their gradient descent. The difference between first formula and second formula is the derivative term. Are the two formula equivalents?
