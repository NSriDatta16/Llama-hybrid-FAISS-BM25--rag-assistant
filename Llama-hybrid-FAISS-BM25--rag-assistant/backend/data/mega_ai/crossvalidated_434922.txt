[site]: crossvalidated
[post_id]: 434922
[parent_id]: 
[tags]: 
Source of vanishing/exploding gradients in RNN

Problem I am trying to understand the source of vanishing/exploding gradients in vanilla RNN. The update rule of vanilla RNN is $$ \begin{aligned} &\mathbf{a}^{\left }=\tanh(\mathbf{W}_ {aa}\mathbf{a}^{\left }+\mathbf{W}_ {ax}\mathbf{x}^{\left }+\mathbf{b}_ a)\\ &\mathbf{y}^{\left }=g(\mathbf{W}_ y\mathbf{a}^{\left }+\mathbf{b}_ y) \end{aligned} $$ To my understanding, only $\mathbf{W}_{aa}$ and $\mathbf{W}_y$ will cause the issues since they all require $\nabla_{\mathbf{a}^{\left }}f$ , where $\mathbf{a}^{\left }$ is in turn dependent on previous steps, which result in multiplicative terms $(\mathbf{W}_{aa}^{T})^k$ and $(\mathbf{W}_{y}^{T})^k$ and thereby vanishing/exploding gradients. However, since $\nabla_{\mathbf{x}^{\left }}f$ does not depend on previous steps. So $\mathbf{W}_{ax}$ will not cause the problem. Is my understanding correct? Thank you in advance!
