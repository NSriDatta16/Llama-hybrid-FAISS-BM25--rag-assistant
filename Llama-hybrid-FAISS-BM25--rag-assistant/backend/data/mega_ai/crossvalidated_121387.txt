[site]: crossvalidated
[post_id]: 121387
[parent_id]: 
[tags]: 
Revert minmax normalization to original value

I'm training a neural network. Normalization of inputs and outputs (training data) is carried out using min and max to a scale of [0-1]. I'm applying backpropagation learning algorithm. I need to get the error offset. i.e. error = actual output $-$ output How do I scale my output [0-1] back to actual real values such as in zero to thousands range?
