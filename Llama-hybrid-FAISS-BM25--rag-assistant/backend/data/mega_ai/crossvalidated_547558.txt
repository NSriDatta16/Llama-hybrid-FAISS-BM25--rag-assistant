[site]: crossvalidated
[post_id]: 547558
[parent_id]: 547551
[tags]: 
You probably want to look up techniques that people use for fairness and/or (differential) privacy. There, people use various techniques that try to e.g. change internal representations of neural networks so that they no longer - even indirectly - contain certain information. One such approach is to use an adverserial model (compare the generative adverserial approach as a classic example of adverserial training), which in your case would aim to classify the author of the text (multi-class classification). At the same time your "main model" would try to build good embeddings/representations as judged by some other metric, while being penalized for constructing them in such as way that allows the adversery to classify the author.
