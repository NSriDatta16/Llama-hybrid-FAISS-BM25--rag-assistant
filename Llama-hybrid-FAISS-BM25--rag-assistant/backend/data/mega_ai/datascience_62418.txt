[site]: datascience
[post_id]: 62418
[parent_id]: 62409
[tags]: 
One of the main strengths of DL methods is that they can work from raw data, and often perform better this way than traditional methods with carefully crafted features. So it's indeed very tempting to consider traditional feature engineering as obsolete, since it requires more work and often results in lower performance. However one should be careful before discarding feature engineering in this way: First, as scientists we should be wary of the dynamic nature of technological trends. For example very few ML experts would have bet on neural nets 15-20 years ago as the next big thing. We should take stock of the evolution of ML methods, not blindly adopt the latest technology. DL methods are computationally expensive and usually require a large amount of data. There are still plenty of applications/problems where more lightweight traditional methods are a better fit. DL methods are by nature less open to interpreting their results. Interpretability/explainability is already an important issue and is likely to become even more important as applications of ML meet real-life problems: ethical issues (what if a ML system is racist?), legal issues (why did a ML system make a bad decision and who is responsible?). By contrast, some statistical methods such as decision trees offer a very clear explanation of their decisions. In some cases leaving feature engineering to DL is suboptimal. There have been a few results (in NLP as far as I'm aware) showing that on some specific problems carefully crafted features performs better than DL. I don't know if these are significant or just exceptions to the rule. Subjective interpretation: there might be a risk of "design laziness", i.e. counting on DL to do the job instead of properly understanding and structuring the problem.
