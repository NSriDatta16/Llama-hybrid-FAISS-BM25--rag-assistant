[site]: crossvalidated
[post_id]: 90989
[parent_id]: 89678
[tags]: 
I don't know how helpful a general answer will be. You're asking how to do something difficult; good answers will probably depend on the discipline and will probably be long and nuanced. :) As far as organization goes, you're already using git, so next you should start using a makefile to execute the analysis. The makefile lays out how different files depend on each other (i.e., which statistics are derived from which code) and when you call make , everything that needs to be updated will. Now, that doesn't help with the exploratory part. For EDA I use (mostly) R in emacs via ESS. You need need need a REPL for EDA. My workflow is to play with plots, estimates, etc. in ESS (in an exploratory.R type file), decide what I want to keep, then recode it so that it can be batch-executed by make. Re: git, I don't know how you're using it, but I use a single repository for each project (usually a single paper) and rebase the hell out of my codebase to keep a clean history; i.e. I use $ git merge meandering-branch --squash $ git add -p somefile $ git rebase -i master $ git reset HEAD --hard way more than when I started with git, and way more than I'd recommend a beginner. If you're not familiar with all of those commands and options, you may want to learn more git. The biggest thing that's helped me is to be disciplined about making logically distinct commits; i.e. every commit should contain all of the changes that you might want to undo all at once in the future (and no more or less). As far as actually exploring the data, I've found these books helpful and interesting, and they deal specifically with large datasets (at least in parts): The Graphics of Large Datasets , edited by Unwin, Theus, and Hofmann. via springerlink if you have access, otherwise individual chapters are probably available by googling. The handbook of data visualization , edited by Chen, HÃ¤rdle, and Unwin. also via springerlink Data Analysis by Huber (2011)..
