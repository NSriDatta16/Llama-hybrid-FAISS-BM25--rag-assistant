[site]: datascience
[post_id]: 25692
[parent_id]: 25680
[tags]: 
A simple approach could be the following: suppose $i \in \{0,1\}^d$ is the vector you want to predict which of the $0$ entries could be $1$ and $j \in J$ the rest of the feature vectors. Take the $k$ nearest neighbors, under some suitable distance ( Jaccard , Hamming , Manhattan distance ). For each $0$ entry the probabilities could be the percentage of the $k$ nearest neighbors that have $1$ in the corresponding entry. This problem has been extensively study in the collaborative filtering community. The best known example being the Netflix Prize . This blog post provides a nice explanation of this approarch for binary data. Another, more involved, approach is matrix completion , in particular check this reference . If you are into deep learning check this .
