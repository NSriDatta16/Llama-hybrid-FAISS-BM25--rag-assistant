[site]: crossvalidated
[post_id]: 419988
[parent_id]: 
[tags]: 
Bayesian online changepoint detection (modeling assumptions in recursive derivation)

I am reading Bayesian Online Changepoint Detection ( https://arxiv.org/pdf/0710.3742.pdf ), and I do not understand one step in the derivation of Equation $3$ . For completeness, this is my derivation: $$ \require{cancel} \begin{align} p(r_t, \mathbf{x}_{1:t}) &= \sum_{r_{t-1}} p(r_t, r_{t-1}, \mathbf{x}_{1:t}) \\ &= \sum_{r_{t-1}} p(r_t, x_t \mid r_{t-1}, \mathbf{x}_{1:t-1}) p(r_{t-1}, \mathbf{x}_{1:t-1}) \\ &= \sum_{r_{t-1}} p(x_t \mid r_t, r_{t-1}, \mathbf{x}_{1:t-1}) p(r_t \mid r_{t-1}, \mathbf{x}_{1:t-1}) p(r_{t-1}, \mathbf{x}_{1:t-1}) \\ &= \sum_{r_{t-1}} p(x_t \mid r_{t-1}, \mathbf{x}_{1:t-1}) p(r_t \mid r_{t-1}) p(r_{t-1}, \mathbf{x}_{1:t-1}) \end{align} $$ The only way this works if is two independence assumptions are made: $p(r_t \mid r_{t-1}, \mathbf{x}_{1:t-1}) = p(r_t \mid r_{t-1})$ $p(x_t \mid r_t, r_{t-1}, \mathbf{x}_{1:t-1}) = p(x_t \mid r_{t-1}, \mathbf{x}_{1:t-1})$ The first assumption makes sense. $r_t$ is conditionally independent from the data if we know $r_{t-1}$ . In other words, nothing about the data tells us about whether or not a changepoint will or will not occur. This is just prior knowledge we need to encode into our model. What I don't understand is the second assumption. Why isn't it $$ p(x_t \mid r_t, r_{t-1}, \mathbf{x}_{1:t-1}) = p(x_t \mid r_t, \mathbf{x}_{1:t-1})\tag{$\star$} $$ (Condition on $r_t$ rather than $r_{t-1}$ .) I have seen a couple resources write Equation $\star$ when explaining BOCD, but the paper is pretty consistent in writing this predictive distribution as conditioned on the previous $r_{t-1}$ .
