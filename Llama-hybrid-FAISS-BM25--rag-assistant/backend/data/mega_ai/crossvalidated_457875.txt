[site]: crossvalidated
[post_id]: 457875
[parent_id]: 
[tags]: 
Getting unreasonably high R2 when creating % change lag variables using linear regression

I have a time series dataset. More specifically, there are 1000 "objects" in my dataset and I have 100 years worth of monthly observations for each of them. The variable I am observing will be called x. I am trying to predict x(t+12) using current and lagged versions of x. Since x is non-stationary, I am predicting the percent change: $$ \frac{x(t+12)-x(t)}{x(t)}$$ Now the issue comes in when I try creating features to use as my independent variables. Depending on how I define my % change for my features, I can get an insane R2. Here are my two version: 1) $$ \frac{x(t)-x(t-12)}{x(t-12)}$$ 2) $$ \frac{x(t-12)-x(t)} { x(t)}$$ version 1 gives me terrible results. r2 of 0 version 2 gives me a very large r2. Far beyond what my domain experts would expect to be possible. I have plotted both of these and version 2 seems to match the variable I'm trying to predict pretty well. Version 1 doesn't seem to match at all. I have also run the Augmented Dickey-Fuller test on both versions of the features as well as the target to confirm they are no longer unit-root. Finally, While I'm not sure if this helps, I've tried randomly turning a % of the input variables to 0. In doing that, there is a pretty high chance of getting a high R2. I've also tried generating totally random input variable and trying to use that as a predictor and I have never gotten an R2 above 0. What might make these two versions of %change so different and what might I be doing wrong?
