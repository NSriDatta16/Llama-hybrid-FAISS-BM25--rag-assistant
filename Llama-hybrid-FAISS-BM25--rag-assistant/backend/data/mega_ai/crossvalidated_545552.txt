[site]: crossvalidated
[post_id]: 545552
[parent_id]: 371171
[tags]: 
One of the problems with "Expected Risk Mimimization" is that the distribution, $p(y×³|x_i;\phi)$ , is unknown, and thus it is not clear how to minimize it. You could theoretically argue, that we can try to find this distribution, during the minimization process, but as you said, in this case it will not approximate the true distribution, because the model incentive is minimize the loss and not finding the probability distribution. For example, It can end up with putting the mass probability on the a single point that minimizes the loss, instead of finding the full distribution. If all we care about is minimizing the loss (as opposed to approximating the true distribution), we could minimize the loss directly e.g. minimize the MSE. i.e. $\min_{\theta} \sum_i MSE(y_i, y^{'}_i)$ Moreover, this is just theoretically. Practically, true distribution is intractable and cannot be computed. This is also true for the maximum likelihood optimization. Usually in machine learning, in order to solve the intractability problem, we add assumptions regarding the output distribution. We tend to pick "easy to work with" distribution and not complex one. For example distributions that are easily described by finite numbers like $\mu, \sigma$ . A popular choice for such distribution is the normal distribution. i.e. we assume that the the output is normally distributed with the label as mean plus some variance around it. It can be proved that, assuming normal distribution, maximizing the log likelihood is equivalent to minimizing the MSE. So in the practical setup, in which you assume normal distribution, both tasks are the same. i.e. in one shot, we are minimizing the loss and maximizing the likelihood. See more details here
