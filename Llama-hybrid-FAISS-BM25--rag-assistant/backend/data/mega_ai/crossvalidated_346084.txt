[site]: crossvalidated
[post_id]: 346084
[parent_id]: 
[tags]: 
waiting for two events with given average and stddev

Imagine that I am working in a fire station. For the truck to leave, I need all my firefighters. They receive their alarm at the same time. Their time to come to the station can be modelled as a random normal variable having the average $\mu_i$ and standard deviation $\sigma_i$. Is it possible from those values to determine the average waiting time for the truck to leave and its standard deviation? Edit on 15/05: I tried to compute it the following (wrong) way: I can compute the probability that each variable is greater than another one $P(A-B>0) = 1-CDF(0;\mu_A - \mu_b; \sqrt{\sigma_A ^2 + \sigma_B ^2})$. Next step I do is to compute this for every possible pair. I then compute the probability that a given variable is the maximum value $P(max = A) = P(A-B>0) \times P(A-C>0) \times ...$. From there, I thought that I could determine the average time to wait as $P(max = A) \times \mu_A + P(max = B) \times B + ...$. This is wrong because if all random variable are equals, the sum of all $P(max = .)$ is not 1. I don't know what is wrong exactly, but I get the feeling that I may have to take into account the fact that two variable can be equal to each other. But in such case, I don't have a real intuition on how this should be taken into account.
