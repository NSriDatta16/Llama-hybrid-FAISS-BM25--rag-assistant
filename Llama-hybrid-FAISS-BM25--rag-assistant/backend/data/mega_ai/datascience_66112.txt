[site]: datascience
[post_id]: 66112
[parent_id]: 
[tags]: 
Compute loss gradient w.r.t. inputs of Bayesian Neural Network using Pyro

Suppose I inferred the parameters of all the posterior distributions for a BNN using Pyro. In particular, the implementation uses the HiddenLayer class: class BNN(nn.Module): def __init__(self, dataset_name, input_shape, data_format, device): super(BNN, self).__init__() self.net = torch_net(dataset_name=dataset_name, input_shape=input_shape, data_format=data_format) self.n_hidden = 512 self.n_classes = 10 self.device = device def model(self, inputs, labels=None, kl_factor=1.0): size = inputs.size(0) flat_inputs = inputs.view(-1, 784) a1_mean = torch.zeros(784, self.n_hidden) a1_scale = torch.ones(784, self.n_hidden) a1_dropout = torch.tensor(0.25) a2_mean = torch.zeros(self.n_hidden + 1, self.n_classes) a2_scale = torch.ones(self.n_hidden + 1, self.n_hidden) a2_dropout = torch.tensor(1.0) a3_mean = torch.zeros(self.n_hidden + 1, self.n_classes) a3_scale = torch.ones(self.n_hidden + 1, self.n_hidden) a3_dropout = torch.tensor(1.0) a4_mean = torch.zeros(self.n_hidden + 1, self.n_classes) a4_scale = torch.ones(self.n_hidden + 1, self.n_classes) with pyro.plate('data', size=size): h1 = pyro.sample('h1', bnn.HiddenLayer(flat_inputs, a1_mean, a1_dropout*a1_scale, non_linearity=nnf.leaky_relu, KL_factor=kl_factor)) h2 = pyro.sample('h2', bnn.HiddenLayer(h1, a2_mean, a2_dropout*a2_scale, non_linearity=nnf.leaky_relu, KL_factor=kl_factor)) h3 = pyro.sample('h3', bnn.HiddenLayer(h2, a3_mean, a3_dropout*a3_scale, non_linearity=nnf.leaky_relu, KL_factor=kl_factor)) logits = pyro.sample('logits', bnn.HiddenLayer(h3, a4_mean, a4_scale, non_linearity=lambda x: nnf.log_softmax(x, dim=-1), KL_factor=kl_factor, include_hidden_bias=False)) labels = nnf.one_hot(labels) if labels is not None else None cond_model = pyro.sample('label', OneHotCategorical(logits=logits), obs=labels) return cond_model def guide(self, inputs, labels=None, kl_factor=1.0): size = inputs.size(0) flat_inputs = inputs.view(-1, 784) a1_mean = pyro.param('a1_mean', 0.01 * torch.randn(784, self.n_hidden)).to(self.device) a1_scale = pyro.param('a1_scale', 0.1 * torch.ones(784, self.n_hidden), constraint=constraints.greater_than(0.01)).to(self.device) a1_dropout = pyro.param('a1_dropout', torch.tensor(0.25), constraint=constraints.interval(0.1, 1.0)).to(self.device) a2_mean = pyro.param('a2_mean', 0.01 * torch.randn(self.n_hidden + 1, self.n_hidden)).to(self.device) a2_scale = pyro.param('a2_scale', 0.1 * torch.ones(self.n_hidden + 1, self.n_hidden), constraint=constraints.greater_than(0.01)).to(self.device) a2_dropout = pyro.param('a2_dropout', torch.tensor(1.0), constraint=constraints.interval(0.1, 1.0)).to(self.device) a3_mean = pyro.param('a3_mean', 0.01 * torch.randn(self.n_hidden + 1, self.n_hidden)).to(self.device) a3_scale = pyro.param('a3_scale', 0.1 * torch.ones(self.n_hidden + 1, self.n_hidden), constraint=constraints.greater_than(0.01)).to(self.device) a3_dropout = pyro.param('a3_dropout', torch.tensor(1.0), constraint=constraints.interval(0.1, 1.0)).to(self.device) a4_mean = pyro.param('a4_mean', 0.01 * torch.randn(self.n_hidden + 1, self.n_classes)).to(self.device) a4_scale = pyro.param('a4_scale', 0.1 * torch.ones(self.n_hidden + 1, self.n_classes), constraint=constraints.greater_than(0.01)).to(self.device) with pyro.plate('data', size=size): h1 = pyro.sample('h1', bnn.HiddenLayer(flat_inputs, a1_mean, a1_dropout*a1_scale, non_linearity=nnf.leaky_relu, KL_factor=kl_factor)) h2 = pyro.sample('h2', bnn.HiddenLayer(h1, a2_mean, a2_dropout*a2_scale, non_linearity=nnf.leaky_relu, KL_factor=kl_factor)) h3 = pyro.sample('h3', bnn.HiddenLayer(h2, a3_mean, a3_dropout*a3_scale, non_linearity=nnf.leaky_relu, KL_factor=kl_factor)) logits = pyro.sample('logits', bnn.HiddenLayer(h3, a4_mean, a4_scale, non_linearity=lambda x: nnf.log_softmax(x, dim=-1), KL_factor=kl_factor, include_hidden_bias=False)) I need to compute the gradient of the loss function w.r.t. the some input $x$ for a posterior sample of this BNN: $$\nabla_x \ell(BNN(w,x),y)$$ $w\sim \text{posterior}$ . My idea was to get this posterior sample as a torch.nn.Module object. Should I define a new model with the updated parameters or is there any automatic way of doing this? After doing this, it is still unclear to me how I should use pytorch autograd to compute the gradient w.r.t. the inputs instead of the weights.
