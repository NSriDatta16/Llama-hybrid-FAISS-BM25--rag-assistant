[site]: crossvalidated
[post_id]: 345994
[parent_id]: 345737
[tags]: 
Regularization is a method for including prior information into a model. This will seem straightforward from the Bayesian perspective but is easy to see outside from the perspective as well. For example, the $L_2$ penalty + standarization of covariates in Ridge Regression is essentially using the prior information that we don't believe that estimation should be entirely dominated by a small number of predictors. Similarly, the $L_1$ penalty can be seen as "betting on sparseness of the solution" (side note: this doesn't make sense from the traditional Bayesian perspective but that's another story...). A key point here is that regularization isn't always helpful. Rather, regularizing toward what should probably be true is very helpful, but regularizing in the wrong direction is clearly bad. Now, when it comes to deep neural nets, the interpretability of this models makes regularization a little more difficult. For example, if we're trying to identify cats, in advance we know that "pointy ears" is an important feature. If we were using some like logistic regression with an $L_2$ penalty and we had an indicator variable "pointy ears" in our dataset, we could just reduce the penalty on the pointy ears variable (or better yet, penalize towards a positive value rather than 0) and then our model would need less data for accurate predictions. But now suppose our data is images of cats fed into a deep neural networks. If "pointy ears" is, in fact, very helpful for identifying cats, maybe we would like to reduce the penalty to give this more predictive power. But we have no idea where in the network this will be represented! We can still introduce penalties so that some small part of the system doesn't dominate the whole network, but outside of that, it's hard to introduce regularization in a meaningful way. In summary, it's extremely difficult to incorporate prior information into a system we don't understand.
