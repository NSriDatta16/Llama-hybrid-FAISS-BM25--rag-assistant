[site]: crossvalidated
[post_id]: 108973
[parent_id]: 108964
[tags]: 
One difficulty in answering the question is that you didn't mention what is the nature of the data set that you are actually using the classifier for. Franck's answer is excellent, but he assumes you are using it to find documents. The response may be a bit different if your application is medical research / clinical trials or evaluating the performance of your SVM for a stock or futures trading system, etc. In your case the support values for H & R are very different and this has implications for the bias inherent in metrics such as Precision, Recall and F1 that you are using. Depending on the application, it may be preferable to use unbiased metrics which adjust for the differences in support. For example, the unbiased version of Precision is Markedness, defined as: Markedness = Precision + NPV - 1 = TP/(TP+FP) +FN/(FN+TP) = 0.72 for your example. The other unbiased metric is Informedness = TPR - FPR = distance from random on the ROC chart, which comes out to be 0.22 in your case. The geometric mean of Markedness & Informedness is the Matthews correlation coefficient = 0.40 for your example. Whether or not the low value of Sensitivity (Recall) for class R is a problem depends on the associated "cost" of this error in your particular case.
