 × E ( 2 x − 1 ) {\displaystyle 0.18215\times E(2x-1)} , with shape ( 4 , 64 , 64 ) {\displaystyle (4,64,64)} , where 0.18215 is a hyperparameter, which the original authors picked to roughly whiten the encoded vector to roughly unit variance. Conversely, given a latent tensor y {\displaystyle y} , the decoded image is ( D ( y / 0.18125 ) + 1 ) / 2 {\displaystyle (D(y/0.18125)+1)/2} , then clipped to the range [ 0 , 1 ] {\displaystyle [0,1]} . In the implemented version, the encoder is a convolutional neural network (CNN) with a single self-attention mechanism near the end. It takes a tensor of shape ( 3 , H , W ) {\displaystyle (3,H,W)} and outputs a tensor of shape ( 8 , H / 8 , W / 8 ) {\displaystyle (8,H/8,W/8)} , being the concatenation of the predicted mean and variance of the latent vector, each of shape ( 4 , H / 8 , W / 8 ) {\displaystyle (4,H/8,W/8)} . The variance is used in training, but after training, usually only the mean is taken, with the variance discarded. The decoder is also a CNN with a single self-attention mechanism near the end. It takes a tensor of shape ( 4 , H / 8 , W / 8 ) {\displaystyle (4,H/8,W/8)} and outputs a tensor of shape ( 3 , H , W ) {\displaystyle (3,H,W)} . U-Net The U-Net backbone takes the following kinds of inputs: A latent image array, produced by the VAE encoder. It has dimensions ( channel , width , height ) {\displaystyle ({\text{channel}},{\text{width}},{\text{height}})} . Typically, ( channel , width , height ) = ( 4 , 64 , 64 ) {\displaystyle ({\text{channel}},{\text{width}},{\text{height}})=(4,64,64)} . A timestep-embedding vector, which tells the backbone how much noise there is in the image. For example, an embedding of timestep t = 0 {\displaystyle t=0} would indicate that the input image is already noiseless, while t = 100 {\displaystyle t=100} would mean there is much noise. A modality-embedding vector sequence, which indicates to the backbone about additional conditions for denoising. For example, in text-to-image generation, the text is divided into a sequence of tokens, then encoded by a text encoder, such as a CLIP encoder, before feeding into the backbone. As another example, an input image can be processed by a Vision Transformer into a sequence of vectors, which can then be used to condition the backbone for tasks such as generating an image in the same style. Each run through the U-Net backbone produces a predicted noise vector. This noise vector is scaled down and subtracted away from the latent image array, resulting in a slightly less noisy latent image. The denoising is repeated according to a denoising schedule ("noise schedule"), and the output of the last step is processed by the VAE decoder into a finished image. Similar to the standard U-Net, the U-Net backbone used in the SD 1.5 is essentially composed of down-scaling layers followed by up-scaling layers. However, the U-Net backbone has additional modules to allow for it to handle the embedding. As an illustration, we describe a single down-scaling layer in the backbone: The latent array and the time-embedding are processed by a ResBlock: The latent array is processed by a convolutional layer. The time-embedding vector is processed by a one-layered feedforward network, then added to the previous array (broadcast over all pixels). This is processed by another convolutional layer, then another time-embedding. The latent array and the embedding vector sequence are processed by a SpatialTransformer, which is essentially a standard pre-LN Transformer decoder without causal masking. In the cross-attentional blocks, the latent array itself serves as the query sequence, one query-vector per pixel. For example, if, at this layer in the U-Net, the latent array has dimensions ( 128 , 32 , 32 ) {\displaystyle (128,32,32)} , then the query sequence has 1024 {\displaystyle 1024} vectors, each of which has 128 {\displaystyle 128} dimensions. The embedding vector sequence serves as both the key sequence and as the value