[site]: crossvalidated
[post_id]: 212656
[parent_id]: 
[tags]: 
Leverages and effect of leverage points

I just got some question about the hat matrix in linear models. My first question is: Why in a balanced one-way layout $(n_1=...=n_c=n_0)$, all leverages $h_{ii}$ have the same value $\frac{1}{n_0}$? I know that $h_{ii}$ is the $(i,i)$ entry in the hat matrix $H=X(X^TX)^{-1}X^T$. But I couldn't see any relationship between this expression and the result. My second question is: when discussing the leverage points in a general linear model, we know that $$0\leq h_{ii}\leq1$$ $$\sum_{i=1}^n h_{ii}=p$$ where $p$ is the number of parameters to be estimated, and $$Var(\hat {Y_i})=\sigma^2h_{ii}=\frac{\sigma^2}{\frac{1}{h_{ii}}}$$ Then it says $\frac{1}{h_{ii}}$ is roughly the number of observations needed to estimate $\hat{Y_i}$. Why is that? My third question is: Continuing with my second question, it then says, if $h_{ii}$ is very close to 1, then variance of the $i$th residual very lose to zero (already know that $Var(E_i)=\sigma^2(1-h_{ii})$) so $Y_i-\sum_{j=1}^p\hat{\beta_j}x_{ij}\simeq0$. Isn't this expression for the $i$th residual? Why we can conclude this from 0 variance of residual? I mean, can't $E_i$ be constant so that the variance is also zero? My last question is: Continuing with the above, it then conclude that $\hat{Y_i}\simeq Y_i$ and hence almost one degree of freedom needs to be used to just fit this one observation. Could anyone explain this result to me? Isn't $\hat{Y_i}=Y_i$ exactly what we want? Since I think it means we estimate the $i$th observation perfectly.
