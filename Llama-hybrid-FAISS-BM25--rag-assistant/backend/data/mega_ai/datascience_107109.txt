[site]: datascience
[post_id]: 107109
[parent_id]: 32151
[tags]: 
I have read about SVM ...I know that it produces decision plane SVM does not explicitly produce a decision plane; it is not a parametric method. The decision plane implied by the fitted SVM can be visualized in two or three dimensions, but the plane merely results from the class labels and weights of the training observations. If the decision plane would have been estimated directly, each variable would obtain an estimated weight (and perhaps a non-linear transformation), but this would be a parametric approach. With SVMs, each training observation obtains an estimated weight, making it a non-parametric method. What I want to understand in simple terms is that: Suppose I have a similarity matrix of all training examples specifying amount of similarity between any (all) two examples in training sample. How can I make a classifier or cluster based only on this information? To make a classification, we compute the similarity of a new observation $x_0$ to all training observations (note that $x_0$ can actually be new, or just resampled from the training data). This gives us a weight for each of the training observations' class labels; we basically weigh and sum the training observations' class labels, to obtain the predicted class for $x_0$ . For an SVM with linear kernel, this looks like: $$f(\mathbf{x}_0) = \mathrm{sgn}\left( \sum_{i=1}^n \alpha_i y_i \mathbf{x}_i \cdot \mathbf{x}_0 + b \right)$$ Note that we can see $b$ as an intercept (estimated based on training observations), and $\alpha$ as weights of the training observations ( $1, \dots, n$ ). $\mathbf{x}_i$ (a vector of predictor variable values) and $y_i$ (taking a value of -1 or 1) are simply observed in the training sample, and $\mathbf{x}_0$ are the observed predictor variable values for the new observation.
