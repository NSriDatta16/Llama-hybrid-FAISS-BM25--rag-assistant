[site]: crossvalidated
[post_id]: 15659
[parent_id]: 15651
[tags]: 
Imagine that we didn't model anything and we didn't have any information on $x$ to use in our predictions. What would our guess of $y$ be? If we performed the following regression: $$ \begin{align*} y_i = \beta_0 + \epsilon_i \end{align*}$$ then $\hat{\beta}_0 = \bar{y}$. We can write $$ \begin{align*} y_i = \hat{y}_i + e_i, \end{align*}$$ where $e$ is the residual. Now, subtract $\bar{y}$ from both sides: $$ \begin{align*} y_i - \bar{y} = \hat{y}_i - \bar{y} + e_i. \end{align*}$$ Let's square both sides and sum over all $i$: $$ \begin{align*} \sum{\left(y_i - \bar{y}\right)^2} = \sum{\left(\hat{y}_i - \bar{y}\right)^2} + \sum{e_i^2} \end{align*}$$ (we used the fact that $\sum{\left(\hat{y}_i - \bar{y}\right)e_i} = 0$). } We create a statistic called $R^2$: $$ \begin{align*} R^2 = \frac{\sum{\left(\hat{y}_i - \bar{y}\right)^2}}{\sum{\left(y_i - \bar{y}\right)^2}} = 1 - \frac{\sum{e_i^2}}{\sum{\left(y_i - \bar{y}\right)^2}}. \end{align*}$$ This is the ratio of the variation predicted by our model to the variation not predicted by the mean alone, the simplest possible model of our outcome. Of the variation to be predicted, what fraction is predicted by your model? Alternatively, it is 1 minus the variation that remains relative to the total variation. Since OLS minimizes the sum of squared residuals, for any given data $y$, you can view OLS as choosing coefficients to maximize the $R^2$. Because our models include an intercept, the regression line goes through the point of means ($\bar{x}, \bar{y}$) and thus they predict no less than the mean alone. Using the derivation above and the fact that our regression includes an intercept term, $R^2$ is bounded between 0 and 1. Additionally, $R^2$ is equal to the sample correlation squared $\left(r_{x,y} \text{ for univariate regression and }r_{\hat{y},y} \text{ generally}\right)$. $R^2$ is the most common goodness-of-fit measure for regression, but it needs to come with a warning siren. All that $R^2$ tells you is the proportion of the overall variation that is predicted by your model. Note that I avoid the typical "is explained by your model" phrasing that people use. This implies causality and $R^2$ does not measure that. You can have a perfect model: $y$ is just its mean plus some error---that's the definition of $y$---but, the $R^2$ is 0. Your model is exactly right, but it has a small $R^2$ simply because you can't predict the variation of $y$ no matter what information (variables) you use. On the other hand, you can have a really high $R^2$, but have a completely wrong model when two things are correlated for some other reason. A time series example is if I regress my weight from ages 0 to 27 on the weight of someone else from ages 0 to 27. We'll get a high $R^2$ simply because our weights were trending up for a while as we grew and relatively plateaued later on; my weighted was not caused by or related to in any way the weight of the other person ( see this question on spurious causation ).
