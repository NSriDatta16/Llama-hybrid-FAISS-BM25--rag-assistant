[site]: crossvalidated
[post_id]: 124424
[parent_id]: 
[tags]: 
Algorithm to Rank "Risk Factors" on Individual Level from Logit Regression Scoring

I have a logit regression which predicts the probability of attrition. The equation is roughly: $$\log(\text{odds})=\text{logit}(P)=\ln(\frac{P}{1-P})=a+bX=\beta_0+\beta_{1...n}X_{1...n}+\delta_{1..n}Z_{1...n}+\epsilon$$ where beta naught (zero) is the intercept beta 1 to n and X 1 to n represent n continuous predictors of attrition delta 1 to n and Z 1 to n represent n dichotomous predictors of attrition epsilon is the error term I fit the model on a large sample of individuals from my database. I use the fitted model to score new groups of individuals, rank them, then within each individual I apply an algorithm to identify the top 2 "risk factors" for each individual. Risk factors are predictors in X, Z which contribute most highly to the individual's risk of attrition. The algorithm I've been using is roughly: Take the difference between the value of a predictor (a given X or Z) for an individual Subtract the mean of the group from the individuals value, i.e. for an individual i , group g , and predictor X n , I have a difference = X ni - X ng = X dni I'm sure I could notate this better, forgive me Multiply the difference (X dni ) with the coefficient of that predictor, i.e. X dni * B n = R ni Rank all of the values from #3 within each individual for each predictor (X 1...n , Z 1...n ), so if we have 10 X's and 10 Z's, then there are 20 ranked values R ni for each individual Select the top 2 values from #4 (i.e. top 2 of R i ) for each individual as their top risk factors then launch a campaign to discuss the top 2 risk factors for each individual with them, in hopes of reducing their probability of attrition. Example : B n = -.1, i.e. it reduces probability of attrition (P) and thus is a "good" thing X ni = .5 X ng = .4 Thus this individual has a higher value on a "good thing", which is good, and thus she is better than average on this predictor of attrition. .5 - .4 = .1 .1 * -.1 = -.01 On the other hand, if X ni = .5, X ng = .4, but B n = .1, then the individual is worse than average (because the sign of B is positive) and R ni = .01. This seems reasonable, because the risk factor where the individual is better than average is ranked -.01, which is lower than the one where they are worse than average (R ni = .01), thus if we were only picking 1 of this 2 values as a "risk factor" we'd correctly select the higher ranking (.01) factor where they are worse than average. We scale this up to include all X and Z, 1...n. My question is basically "is this a valid approach or is there a way to improve the algorithm?" A problem I've been having seems to revolve around the binary predictors Z, though theoretically it could probably occur with the continuous predictors X as well. Here's what happens: A person is above average ("good") on the values of X, there's a Z n which reduces the probability of attrition slightly, i.e. Dn = -.002 the value of Z ni = 1, which is "good" for individual i and reduces their probability of attrition, i.e. 1 - X ng = 1 - .3 = .7 and .7 * -.002 = -.0014 however, R ni = -.0014 is still selected as a top risk factor even though reduces P, because the other values of R for individual i are bigger reducers of risk Example : If X's are things like age, income, and time spent reading, and Z n is an indicator for if an individual has a PhD then intuitively, having a PhD should not be considered a risk factor for individuals who have PhD's, but if the person is sufficiently above average on the factors in X, then having a PhD may reduce the probability of attrition the least (or 2nd to least, since we select 2 risk factors) and thus it gets reported as a risk factor even though that doesn't seem to make much real-world sense. Currently, I've "patched" the algorithm in my R code using conditional statements that say things (in the code's language) like "if a person has a PhD and PhD is ranked in the top 2 then delete it". However, I wonder if there's a fundamentally better approach to the algorithm, so that I change the model specification in the future then I don't have to worry about catching these weird cases. Are these types of problems fundamentally a scaling issue? Is this problem limited to dichotomous variables? What's a better approach? Thanks in advance; I know it's a long question.
