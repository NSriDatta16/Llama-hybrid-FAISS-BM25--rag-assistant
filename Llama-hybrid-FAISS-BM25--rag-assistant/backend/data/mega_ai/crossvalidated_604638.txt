[site]: crossvalidated
[post_id]: 604638
[parent_id]: 125058
[tags]: 
I agree with the accepted answer that it would not be appropriate to utilize the finite population correction. However, I think Rubin's potential outcome framework explains why this would be inappropriate much, much more persuasively than Cochran. Presumably, by comparing, say worker satisfaction under the old boss and the new boss, the intent is to answer a causal question. What's the average treatment effect of replacing the old boss with the new boss? Rubin says that the best way to answer a question like that is to think about it as a randomized experiment. At a specific point in time, $a_1$ , there are two possibilities and two corresponding potential outcomes for each person (unit). One possibility is that the old boss is retained. Then satisfaction is measured at point $a_2$ . For each unit, denote this measure as $c_i$ . The other possibility is the new boss is hired. Then satisfaction is measured at point $t_2$ . Denote this measure as $t_i$ . We want to compare $$ \frac{1}{N} \sum^N_{i=1} t_i - \frac{1}{N} \sum^N_{i=1} c_i $$ The problem is that even in a randomized experiment, we can't do that because for each unit, we always only observe either $t_i$ or $c_i$ . Thus, questions of causal inference, and this extends beyond randomized experiments, are really questions about missing data. In a randomized experiment, we have guarantees that difference between the sample mean for the treatment and control group will give us an unbiased estimate of the average treatment effect. In your example, we're missing the whole $c_i$ vector, so it's more difficult. The key point here though is that even if you every one responds to your survey, you're missing half of the data. There are ways to try to sort of get around this (propensity scores, etc.) so here we'll just assume that you do sort of get around it. Now you might think we can just go ahead and apply the finite population correction to our estimate of the variance of the difference in means as long as we account for the fact that the real population is $N = N_t + N_c$ . However, returning to our grounding example of a randomized experiment, we see that assignment is random but not independent (assuming fixed sizes for the treatment and control group). The last unit assigned can be perfectly predicted by the assignment of the previous units. That means we have a non-zero covariance term. Letting $W_i$ represent a binary for inclusion (1, 0), we have $$ Var(\frac{1}{N_t} \sum^N_{i=1} W_i t_i - \frac{1}{N_c} \sum^N_{i=1} (1 - W_i) c_i) = \\ Var(\frac{1}{N_t} \sum^N_{i=1} W_i t_i) + Var(\frac{1}{N_c} \sum^N_{i=1} (1 - W_i) c_i) - 2 \cdot Cov(\frac{1}{N_t} \sum^N_{i=1} W_i t_i, \frac{1}{N_c} \sum^N_{i=1} (1 - W_i)c_i) $$ The problem is that because we never observe $t_i$ and $c_i$ together, it turns out, we can't estimate the covariance. That forces us to take a conservative approach that approximates dropping the finite population correction. For more details see my response here . For a good introduction to this perspective, see Causal Inference by Rubin and Imbens. A recent article by Abadie, Athey, Imbens, Wooldridge that exploits this perspective, "Sampling-based vs. Design-based Uncertainty in Regression Analysis", recently appeared in Econometrica. Here's a link to the pre-print .
