[site]: crossvalidated
[post_id]: 620664
[parent_id]: 620643
[tags]: 
One possible Bayesian approach is a model with a flat prior according to \begin{align*} X_i | (a, b) &\sim \mathcal{N}(a, 1), \\ Y_i | (a, b) &\sim \mathcal{N}(b, 1), \\ \pi(a, b) &= \mathbf{1}_{a We find that the posterior distribution is then the truncated normal distribution \begin{align*} (a, b) | (X, Y) \sim \mathcal{TN}\left( \begin{pmatrix} \bar{X} \\ \bar{Y} \end{pmatrix}, \begin{pmatrix} n^{-1} & 0 \\ 0 & m^{-1}\end{pmatrix}; a where the $a part indicates the truncation. Now, our job is to find $\mathbf{E}((a,b) | a under these parameters. Using ideas similar to those discussed here: Sample from a multivariate gaussian distribution given a linear constraint on samples and here: If a multivariate Gaussian distribution is truncated what will be the new distribution? , we find that for a multivariate Gaussian $W \sim \mathcal{N}_p(\mu, \Sigma)$ and constants $c\in \mathbf{R}^p, \kappa \in \mathbf{R}$ , we have that $$ \mathbf{E}(W | c^{\mathrm{T}}W > \kappa) = \mu + \frac{\psi(\kappa - c^{\mathrm{T}} \mu)}{c^{\mathrm{T}} \Sigma c} \Sigma c, $$ where $\psi(x) = \mathbf{E}(Z | Z > x)$ for a standard normal $Z$ . We can write $\phi(x) = \phi(x) / (1 - \Phi(x))$ where $\phi$ and $\Phi$ are the standard normal pdf and cdf respectively. In our case, the model parameters are $\mu = (\bar{X}, \bar{Y})$ and $\Sigma = (\begin{smallmatrix} n^{-1} & 0 \\ 0 & m^{-1}\end{smallmatrix})$ , while the constraints are $c = (-1, 1)$ and $\kappa = 0$ , so we can find the Bayes estimator by computing the posterior mean \begin{align*} \begin{pmatrix} \hat{a}_\pi \\ \hat{b}_\pi \end{pmatrix} = \mathbf{E}_\pi\left[\begin{pmatrix} a \\ b \end{pmatrix} \middle| X, Y \right] = \begin{pmatrix} \bar{X} - \frac{m}{n+m} \cdot \psi(\bar{X} - \bar{Y}) \\ \bar{Y} + \frac{n}{n+m} \cdot \psi(\bar{X} - \bar{Y}). \end{pmatrix} \end{align*} If $\bar{X} \ll \bar{Y}$ , then $\psi(\bar{X} - \bar{Y})$ is very small, so $(\hat{a}_\pi, \hat{b}_\pi) \approx (\bar{X}, \bar{Y})$ . On the other hand if $\bar{X} \gg \bar{Y}$ , then $\psi(\bar{X} - \bar{Y}) \approx \bar{X} - \bar{Y}$ , and so $$ \begin{pmatrix} \hat{a}_\pi \\ \hat{b}_\pi \end{pmatrix} \approx \begin{pmatrix} \frac{1}{n+m} (\sum_{i=1}^n X_i + \sum_{i=1}^m Y_i) \\ \frac{1}{n+m} (\sum_{i=1}^n X_i + \sum_{i=1}^m Y_i)\end{pmatrix}, $$ both of which make intuitive sense and mimic the behaviour of the maximum likelihood estimator.
