[site]: crossvalidated
[post_id]: 236249
[parent_id]: 
[tags]: 
Reinforcement learning actions and decisions

What RL does actually update in order to reach always a desired output? Is it true that the inputs (or the Agent actions) have to be re-updated whenever an RL reaches a bad output (or decision) based on a reward system?
