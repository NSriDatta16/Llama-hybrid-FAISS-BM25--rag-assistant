[site]: crossvalidated
[post_id]: 503213
[parent_id]: 
[tags]: 
frequentist vs Bayesian approaches to Gaussian Processes?

I've been reading this blog post , which has been tremendously helpful in understanding Gaussian Processes (GP.) The author has used the terms "prior", "posterior", and "95% confidence interval" throughout; this made it difficult for me to tell if he was approaching GP from a Frequentist or Bayesian perspective. In equation 11, he defines the log marginal likelihood then uses scipy to optimize the hyperparameters, L and Sigma. He doesn't explicitly call it "MLE" but the concept seems analogous enough for me to consider his approach Frequentist. (Is this valid?) And if so, how might a Bayesian approach GP? My currently thinking is: Rather than optimize the log marginal likelihood for point estimates of L and Sigma, sample hyperparameter values from the joint distribution. (Is this valid?) Lastly, the author makes some decisions that made the distinction between paradigms a little blurry for me. Namely, early in the article he discusses the prior, sets arbitrary values for L and Sigma, generates some fake X input data, then uses the Kernel(X,X) to generate a multivariate gaussian (w/ one dimension per data point) and samples from this multivariate gaussian 3 times to show what this "prior over functions might look like". And this has caused me a decent amount of confusion as it sounds quite Bayesian. Could anyone offer some clarification? Edit: Wow, there is no agreement on this topic, see extended conversation.
