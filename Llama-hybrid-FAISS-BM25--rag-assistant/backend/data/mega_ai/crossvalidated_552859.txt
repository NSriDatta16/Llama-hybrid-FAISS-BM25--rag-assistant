[site]: crossvalidated
[post_id]: 552859
[parent_id]: 
[tags]: 
Proof of upper bound on the leave-one-out-cross-validation error of linear SVM

I have to show that for a two-class SVM classifier trained on a linearly separable dataset $(x_i, y_i)_{i =1}^n$ the following upper bound on the leave-one-out-cross-validation error holds true: $$ L_1OCV = \frac{1}{n} \sum_{i = 1}^n [y_i \ne f_i(x_i)] \le \frac{|SV|}{n}, $$ where for all $i = 1, \dots, n$ $f_i(x_i)$ is the SVM classifier fitted on the entire data without the observation $(x_i, y_i)$ and $|SV|$ is the number of support vectors of the SVM classifier fitted on the entire data. I have no idea about how to start. Has someone an idea?
