[site]: crossvalidated
[post_id]: 420741
[parent_id]: 420705
[tags]: 
I think your problem setup is a little troublesome. Let's take a step back and write down a model. Bayes' Rule says $$ p(\theta \vert y) \propto p(y \vert \theta) p(\theta)$$ Equivalently $$\log(p(\theta \vert y)) \propto \log(p(y \vert \theta)) + \log( p(\theta))$$ I'm going to compute things on the log scale because it's easier. Let's say I have some data that I think comes from a bivariate normal with known covariance matrix (assume it is the identity for simplicity). Our job is to estimate the mean of the bivariate normal from data. Our likelihood is then $$ y \vert \mu \sim \mathcal{N}(\mu, \Sigma) $$ Now we need a prior for mu. Let's just use the one you gave $$ \mu \sim \mathcal{N}\left(\left[\begin{array}{l}{1} \\ {1}\end{array}\right],\left[\begin{array}{cc}{1} & {.5} \\ {.5} & {1}\end{array}\right]\right) $$ Now, we need to evaluate the posterior along a 2d grid of possible values of $\mu$ . To compute the posterior probability (well, not really, just something that is proportional to the probability) we compute $$\log(p(y \vert \mu)) + \log( p(\mu))$$ But the important part is that we need to compute this over a grid. That grid will be the elements of $\mu$ . Let's turn to python to do this import numpy as np from scipy.stats import multivariate_normal import matplotlib.pyplot as plt #Data Generating Parameters true_mean = [-1.5, -1.75] true_Sigma = np.eye(2) #Generate data from our data generating process data = multivariate_normal.rvs(mean = true_mean, cov = true_Sigma, size = 100) #Our prior prior = multivariate_normal(mean = np.ones(2), cov = np.array([[1,0.5], [0.5,1]])) #Generate a grid on which to evaluate the posterior grid = np.linspace(-2,2,26) mu_1, mu_2 = np.meshgrid(grid,grid) #Reshape the grid for ease of computation MU = np.stack((mu_1, mu_2), axis = -1).reshape(-1,1,2) #Instantiate the loglikelihood loglik = prior.logpdf(MU) fig,ax = plt.subplots(dpi = 120) ax.axes.set_aspect('equal') plt.contourf(mu_1, mu_2,loglik.reshape(mu_1.shape)) plt.title('Prior') plt.show() LL = [] for m in MU: #Evualuate the likelihood of the data over the grid LL.append(multivariate_normal.logpdf(data, mean = m[0], cov = np.eye(2))) LL = np.array(LL) #Add the log likelihood to the log prior loglik+=LL.sum(axis = 1) fig,ax = plt.subplots(dpi = 120) ax.axes.set_aspect('equal') plt.contourf(mu_1, mu_2,loglik.reshape(mu_1.shape)) plt.title('Posterior') plt.show() This code produces the following As you can see, the prior looks like your prior distribution (as it should). After seeing data, the posterior has shifted the density towards the true mean of the data generating process. So it isn't enough to just evaluate bayes rule as you do in your samples. You have to work with densities, which can get tricky. And where does MCMC come in? Is that just a way to figure out where in the distributions I should be sampling from? As you can imagine, the densities become very difficult to deal with with a modest amount of parameters. MCMC and HMC (Hamiltonian Monte Carlo) are ways to generating markov chains with limiting distributions equal to our posterior. There is a lot to say about MCMC and HMC, so I suggest you create another question if you are really interested.
