[site]: crossvalidated
[post_id]: 432423
[parent_id]: 431579
[tags]: 
We need some more notation. Suppose the random variable $X$ has a distribution from some family $f(x; \theta)$ parametrized by $\theta \in \Theta$ . Suppose that $T=T(X)$ is a sufficient statistic (for $\theta$ .) Then by the factorization theorem we have $$ f(x; \theta)= h(x) g(T(x); \theta) $$ where $h$ is a function not depending on $\theta$ . Now, using the result from Can the Fisher factorization theorem be understood as a product of densities? , this can be interpreted as a factorization of the distribution of $X$ , and we can use this to simulate from the distribution of $X$ by first simulating $T$ and then simulating from the distribution of $X \mid T=t$ . So after having observed $T(x)=t$ , we can simulate surrogate data having the same distribution as $X$ by simulating from the above conditional distribution, which by sufficency do not depend on $\theta$ . This is a way of giving intuitive meaning to sufficiency; knowing only $T(X)=t$ we can recreate by simulation surrogate data having the same distribution as $X$ . There are other ways to get intuitive meaning to $T$ having the same information content as $X$ , via its use in inference. Without going into details The mle (maximum likelihood estimator) of $\theta$ is a function of $T$ (or if nonunique, can be chosen in such way) given a prior for $\theta$ , the bayesian posterior will be a function of $T$ and there are many more general results of this sort. The two above should be easy exercises.
