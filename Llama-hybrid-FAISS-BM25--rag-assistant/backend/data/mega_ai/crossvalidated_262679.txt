[site]: crossvalidated
[post_id]: 262679
[parent_id]: 
[tags]: 
What is the most beginner-friendly book for information geometry?

Question: What is the most beginner-friendly book for information geometry? The book: Amari and Nagaoka, Methods of Information Geometry , is often mentioned as a reference for information geometry. However, Amari has also written several other books about the subject, at least two of which also seem like they are oriented towards beginners: Amari, Differential Geometric Methods in Statistics . Amari, Information Geometry and its Applications . These two books by different authors also seem targeted towards beginners: Arwini, Dodson, Information Geometry . Murray, Differential Geometry and Statistics . In case Amari is one of those geniuses who has so much to say about their ideas that they can't possibly explain it concisely/simply/straightforwardly, perhaps it might be better to start with something written secondhand by another author. I also have access to these other books, which seem like they are more advanced monographs, but I am not really certain, so I am mentioning them here anyway: Cencov, Statistical Decision Rules and Optimal Inference . Kass, Vos, Geometrical Foundations of Asymptotic Inference . This thesis (later published as a book ) also seems relevant: Lebanon, Riemannian Geometry and Statistical Machine Learning . What I often do is read a lot of books and get only a little out of each (because I don't spend any time thinking about any of them or doing any of the problems). However, this time, instead of reading eight books, which would be exhausting anyway, I want to focus on one and only one but get a lot out of it. Thus any informed suggestions or recommendations would be useful/helpful. Note: Also there is the issue of there being other applications to statistics of differential geometry than just the field of information geometry itself, per se, although I am not knowledgeable about this distinction at all. All of the above books seem to reference Riemannian metrics and exponential families of random variables in some way, so I assume they are about information geometry, but if that is not the case for one of them, that would make for a simple and easy elimination criterion.
