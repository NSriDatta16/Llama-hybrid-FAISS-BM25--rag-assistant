[site]: datascience
[post_id]: 29810
[parent_id]: 29800
[tags]: 
I have the following solutions: If you have abundant data you can shuffle them and make validation and training data. After that, your neural network should exploit generalization techniques not to overfit the training data. By doing so, you may have relatively acceptable performance which works in noisy situations. The other technique is evaluating Bayes error. This does not have any relation to the neural nets. It just tries to investigate in the feature space of the problem what percentage of your data is misleading, having same input patterns with contradictory labels. Another approach can be using an existing model for validating the data-set.
