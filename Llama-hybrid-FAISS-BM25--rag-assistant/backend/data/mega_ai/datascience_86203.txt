[site]: datascience
[post_id]: 86203
[parent_id]: 86198
[tags]: 
You don't say what the amount of available data is and if you use a test set. When you are up to prediction, always use a test set (some randomly chosen part of the data, say 20-30% NOT used for model training) to test your model predictions. With sklearn: import numpy as np from sklearn.model_selection import train_test_split X, y = np.arange(10).reshape((5, 2)), range(5) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123) Predict the test data based on your model and compare the predictions to the TRUE values in the test set (mean absolute error or mean squared error for instance): from sklearn.metrics import mean_absolute_error mean_absolute_error(y_true, y_pred) So you get an idea of how well your model works. Also note that linear regression can deliver weak predictions if you have high correlation in your $x$ variables or if there is non-linearity in your $x$ variables. In this case you could try to use random forest (or related methods), generalised additive models (GAM), lasso/ridge regression etc. (a lot of things can matter wrt good model choice). However, the takeaway is that linear regression can perform very poorly in some cases (compared to other methods).
