[site]: crossvalidated
[post_id]: 627708
[parent_id]: 
[tags]: 
Help with Difference-in-Difference Experiment Simulation

I came across this article/blog post by Spotify R&D in which they talk about a statistical method called Difference-In-Difference (DID) that they use in the context of A/B testing and how they are solve some issues related to DID in presence of autocorrelation. I had never heard about DID, but I though that it would be cool to follow along and try to replicate what they are talking about. However it seems I can't get it right. I am stuck where they say We examined this problem by creating artificial time series with positive autocorrelation, fitting DID models, and analyzing the significance test results. Importantly, we added no causal effect to the “treatment” time series in these simulations (half of the simulated time series units were labeled as treatment). If the standard errors were correctly estimated at the significance level we used (alpha=0.05), we’d expect to detect significance at a 5% rate (false positive rate), despite no added treatment effect. However, due to the autocorrelation in our data, the false positive rate was over 30% (Figure 2), over five times too high! Here below you can find the code snippet I used to generate multiple random time-series with some degree of autocorrelation and which share a common trend. The idea was to pick a random $\lambda_t$ for each timestamp $t$ in order to determine a general trand shared by all units, and a generic autocorrelation factor $\alpha$ to control autocorrelation. Then I'd compute each $y_t$ as $$ y_t = y_{t-1} \cdot \alpha + \lambda_t \cdot (1 - \alpha) + \epsilon $$ import numpy as np import pandas as pd import scipy.stats as sts def build_exp_data(n_units=100, n_timestamps=12, ac =.8): """ Build a matrix of size (n, m) with m time-series of size n. Ensure that the time-series have some degree of autocorrelation (ac), and share a common trend. """ # unit id unit_mx = np.tile(np.arange(n_units), (n_timestamps, 1)) # ts timestamp_mx = np.tile(np.arange(n_timestamps), (n_units, 1)).T # pick a random lambda for each possible timestamp # (this lambda controls the trend) lambda_t = sts.norm(0, 1).rvs(n_timestamps) lambda_t_mx = np.tile(lambda_t, (n_units, 1)).T # random errors epsilons = sts.norm(0, .3).rvs((n_timestamps, n_units)) # first half of time-steps is post=0, rest is post=1 is_post_mx = np.zeros((n_timestamps, n_units)) is_post_mx[n_timestamps//2:, :] = 1 # first half of units is treatment=0, rest is treatment=1 is_treatment_mx = np.zeros((n_timestamps, n_units)) is_treatment_mx[:, n_units//2:] = 1 # product post x treatment post_x_treatment_mx = is_post_mx * is_treatment_mx # compute ys artificially y_mx = np.zeros((n_timestamps, n_units)) # y0 is just the trend + error y_mx[0, :] = lambda_t_mx[0, :] + epsilons[0, :] # rest is a % of previous value + % of trend + error for t in range(1, n_timestamps): y_mx[t, :] = y_mx[t-1, :] * ac + lambda_t_mx[t, :] * (1 - ac) + epsilons[t, :] stacked_data = np.column_stack([ mx.flatten(order='F') for mx in [unit_mx, timestamp_mx, y_mx, is_post_mx, is_treatment_mx, post_x_treatment_mx] ]) did_df = pd.DataFrame( stacked_data, columns=['unit', 'ts', 'Y', 'is_post', 'is_treatment', 'post_x_treatment'], ) return did_df The result is a dataset that looks like this: Also, if we plot the time-series and the autocorrelation we get something like this: Once I generated the data, I tried to fit a Linear Regressor to this data like so: import statsmodels.formula.api as smf did_df = build_exp_data() model = smf.ols('Y ~ is_post + is_treatment + post_x_treatment', data=did_df) model_fit = model.fit() At this point I thought about how to determine the significance, since I could look at two p-values: the general one (the p-value of the F-statistic) or the specific p-value for the post_x_treatment parameter of the model. What I understood is that if I repeat this experiment many times, 30% of the times I should get a p-value above .05. For example with this snippet, the value printed in each iteration should tend to .3: ps = np.array([]) for _ in range(1_000): did_df = build_exp_data() model = smf.ols('Y ~ is_post + is_treatment + post_x_treatment', data=did_df) model_fit = model.fit() p = model_fit.pvalues.iloc[-1] ps = np.append(ps, p) print((ps > .05).mean(), end='\r') However, I get something around .8-.9. Can anyone help me with this. Where am I making a mistake?
