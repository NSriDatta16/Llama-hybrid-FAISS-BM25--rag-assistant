[site]: crossvalidated
[post_id]: 363193
[parent_id]: 363190
[tags]: 
The purpose of activation functions is mainly to add non-linearity to the network, which otherwise would be only a linear model. A convolutional layer by itself is linear exactly like the fully connected layer. In fact if you visualize each pixel of the input and output images as a node, then you would obtain a fully connected layer with a lot less edges. Or, in other words, the input values get multiplied by coefficients. Following a complex logic, but nothing more. Here I am talking about the ReLu that in general it is used after each convolutional layer, but there are other uses of activation functions in CNNs. For example, if you are performing binary classification, then you would need a softmax to regularize your output.
