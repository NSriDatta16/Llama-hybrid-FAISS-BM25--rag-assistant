[site]: crossvalidated
[post_id]: 448452
[parent_id]: 
[tags]: 
Convergence time for LSTM and Vanilla feed-forward NN training/validation errors

While learning myself, I am doing a simple example of traffic forecasting with LSTM, comparing with vanilla feedforward NN (FFNN). I observed the following When I have a large number of training examples, the mean absolute error (mae) for LSTM and FFNN both coverage (to zero) pretty quickly and almost is a similar number of epochs (for both training and validation sets). When I have a small number of training samples, then LSTM's mae is reducing very quickly (in terms of epochs) while that of FFNN is taking a bit longer time to converge to 0. I can, of course, deduce that LSTM is quicker in learning even if it is fed with a smaller number of samples. However, I cannot understand the underlying reason or theory behind it.
