rent appropriate kernels for each, or when the appropriate kernel is unknown. If there are two kernels, for example, with feature maps A {\displaystyle A} and B {\displaystyle B} that lie in corresponding reproducing kernel Hilbert spaces H A , H B {\displaystyle {\mathcal {H_{A}}},{\mathcal {H_{B}}}} , then a larger space, H D {\displaystyle {\mathcal {H_{D}}}} , can be created as the sum of two spaces: H D : f = h + h ′ ; h ∈ H A , h ′ ∈ H B {\displaystyle {\mathcal {H_{D}}}:f=h+h';h\in {\mathcal {H_{A}}},h'\in {\mathcal {H_{B}}}} assuming linear independence in A {\displaystyle A} and B {\displaystyle B} . In this case the ℓ 2 , 1 {\displaystyle \ell _{2,1}} -norm is again the sum of norms: ‖ f ‖ H D , 1 = ‖ h ‖ H A + ‖ h ′ ‖ H B {\displaystyle \left\|f\right\|_{{\mathcal {H_{D}}},1}=\left\|h\right\|_{\mathcal {H_{A}}}+\left\|h'\right\|_{\mathcal {H_{B}}}} Thus, by choosing a matrix regularization function as this type of norm, it is possible to find a solution that is sparse in terms of which kernels are used, but dense in the coefficient of each used kernel. Multiple kernel learning can also be used as a form of nonlinear variable selection, or as a model aggregation technique (e.g. by taking the sum of squared norms and relaxing sparsity constraints). For example, each kernel can be taken to be the Gaussian kernel with a different width. See also Regularization (mathematics) == References ==