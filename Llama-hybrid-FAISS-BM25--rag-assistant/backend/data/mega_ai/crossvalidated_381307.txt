[site]: crossvalidated
[post_id]: 381307
[parent_id]: 381179
[tags]: 
You can think of an autoencoder as a form of dimension-reduction, similar to PCA. But, unlike in PCA, all dimensions are of equal importance. The number of neurons in your hidden layer are the number of dimensions you are reducing to. The interesting metric for an autoencoder is MSE ( h2o.mse(m) where m is your model), and the lower the better. Generally, the more hidden neurons you use, the lower the MSE will be, but it will be diminishing returns, and the curve can be a bit noisy, too. (That is my answer to your Q2: the other fields are "informational", but only MSE is of interest.) For purposes of anomaly detection... if you have just a few features, but little correlation between them, I'd be tempted to use the same number of hidden neurons as features. If you have thousands of features, but most of them are correlated, or carrying little information (e.g. each column is zero or NA for 90+% of your samples), then I'd be thinking to try something like the square root of the number of columns. Another approach is to use PCA ( h2o.prcomp() ) on your data, and see how many dimensions are needed to capture, say, 99% of the variance, and use that as the number of hidden neurons. If the data is noisy, consider using a bit of dropout, or l1/l2 regularization, and increasing the number of neurons to compensate. Remember that you can have more hidden neurons than input columns (which is another way it differs from PCA)! You can also experiment with more than one hidden layer. By the way, you were asking for rules of thumb, ways to avoid hyperparameter grid searches. But, in a real project, I would do a grid search, doubling and halving my rule-of-thumbed hidden neuron count. E.g. if 1000 input columns, most of which are sparse, I'd maybe try 20, 40 and 80. I'd be looking for an MSE (and a scoring history chart) that is basically the same even after doubling the number of neurons, so I'd feel comfortable going with my original guess. If using l1/l2 regularization, I'd also put them in the grid search, at plus and minus an order of magnitude. E.g. trying 1e-4 , 1e-5 , 1e-6 for each.
