[site]: crossvalidated
[post_id]: 405127
[parent_id]: 405048
[tags]: 
A fundamental problem with your approach, as you have now elaborated it in one of your comments, is that there is no way to calculate a reliable adjusted- $R^2$ when you have used the outcome to select predictors in a standard stepwise regression. The problem with best-subset or stepwise regressions is not (just) about multiple hypothesis tests. For standard OLS regression with a pre-specified model, minimizing mean-square error and maximizing $R^2$ are obviously equivalent. The adjusted $R^2$ uses the degrees of freedom spent, the effective number of fitted predictors, to account for the improvement in $R^2$ inherent in adding additional predictors to a model. But if the choice of predictors to add was itself based on their relations to outcome, in unpenalized regression you have used up more effective degrees of freedom than is simply expressed by the number of predictors. Statistical Learning with Sparsity puts the case quite clearly early on (pp. 17-18): Suppose we have $p$ predictors, and fit a linear regression model using only a subset of $k$ of these predictors. Then if these $k$ predictors were chosen without regard to the response variable, the fitting procedure “spends” $k$ degrees of freedom. ... However if the $k$ predictors were chosen using knowledge of the response variable, for example to yield the smallest training error among all subsets of size $k$ , then we would expect that the fitting procedure spends more than $k$ degrees of freedom. ... Similarly, a forward-stepwise procedure in which we sequentially add the predictor that most decreases the training error is adaptive, and we would expect that the resulting model uses more than $k$ degrees of freedom after $k$ steps. For these reasons and in general, one cannot simply count as degrees of freedom the number of nonzero coefficients in the fitted model. So your hope to use an adjusted- $R^2$ to find a best stepwise model does not have a reliable theoretical foundation. There is no way in standard stepwise regression to adjust the degrees of freedom for the use of the outcome to select the predictors. So there is no way to obtain a reliable adjusted $R^2$ for model selection with standard stepwise approaches. The next sentence shows a way out of the problem: However, it turns out that for the lasso, one can count degrees of freedom by the number of nonzero coefficients ... The penalization of coefficients in lasso effectively makes up for the use of the outcome to select the predictors. Furthermore, as lasso is based on a defined optimization procedure rather than a highly data-dependent algorithm, a wide range of theoretical supports for the method have been developed that now extend to use of lasso for inference (Chapter 6 of the book). There are some other problems with your proposed implementations. For example, the standard approach to use bootstrapping to estimate and correct for bias, the optimism bootstrap , uses the reverse of your approach. The idea is that the relationship of the data sample at hand to the population of interest is akin to that of the bootstrap samples to the data sample at hand. So to see how well your model based on your data set might generalize to the full population of interest, you don't develop a model on the full data set and then test it on bootstrap samples, as you propose. Rather, after you have developed a model on the full data set, you develop models from multiple bootstrap samples while testing each of them on the full original sample. The difference between the average of the results for the bootstrap-based models and that for the full model provides an estimate of bias for the modeling procedure , which is then used to correct the model developed on the full data set so that it lessens the bias when applied to the population of interest. The major issue, however, is that there is now a firm theoretical basis for using lasso even for inference, while there is none for unpenalized stepwise regression.
