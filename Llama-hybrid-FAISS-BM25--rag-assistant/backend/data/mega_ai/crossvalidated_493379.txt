[site]: crossvalidated
[post_id]: 493379
[parent_id]: 
[tags]: 
Minimization of the loss function in soft-margin SVM

According to Wikipedia , the goal of the soft-margin SVM is to minize the hinge loss function: $$\left[\frac{1}{n} \sum_{i=1}^{n} \max \left(0,1-y_{i}\left(\vec{w} \cdot \vec{x}_{i}-b\right)\right)\right]+\lambda\|\vec{w}\|^{2}$$ Could you tell me more why we add $\lambda$ ? What is its effect on the minimization?
