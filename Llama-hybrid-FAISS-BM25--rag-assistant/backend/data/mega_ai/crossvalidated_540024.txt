[site]: crossvalidated
[post_id]: 540024
[parent_id]: 539287
[tags]: 
Notwithstanding @Frank Harrels's comments, I think it's useful to think about this in terms of the intercept or bias terms in the logistic regression model, rather than in terms of thresholds. I'm not totally confident in this approach, but it should hopefully be useful! Let's imagine you only have one predictor, $x$ . For convenience, let $x$ be centred to have a mean of $0$ in the training data. The model is then $P(y_i = 1) = \text{logit}^{-1}(\alpha + \beta x_i)$ , the intercept $\alpha$ is the log odds of a positive outcome when $x = 0$ (the average value). Given a set of predictors values $x_1, x_2, \dots, x_N$ , and the parameters $\alpha$ and $\beta$ , the predicted prevalence is just the average of the predicted probabilities (I think), $\frac{\sum_{i=1}^N \text{logit}^{-1}(\alpha + \beta x_i)}{N}$ . Now, if prevalence is 15% in your training context, and 30% in your test context, there are a few possible explanations. The first is that the distribution of the predictors - just $x$ here, but multiple things in reality - differ between the contexts, and this difference explains the difference in the total number of positive cases. If this is the case, your model can be used without modification in the test context. The second is that the distribution of the predictors hasn't changed, but some additional factors not captured by your model have. This could be handled heuristically by adjusting the value of $\alpha$ until the average predicted probability of a positive outcome matches the prevalence you expect for the test data (30%). The third, and most likely explanation is a mixture of the above: some things captured by your model have changed, and some things not not captured have changed as well. I think, but I can't say for certain, that this could handled in the same way, by adjusting the value of $\alpha$ until the mean predicted probability matches the expected prevalence. Now, none of this will help if the relationship between your predictors and the outcome differ between the training and test contexts, but there's not much we can do about that. Update For my own L&D, I had a go at simulating this, and it seems to work as described. Code is here: https://gist.github.com/EoinTravers/656ac7b77a5cfa966c706888185afcd5
