[site]: crossvalidated
[post_id]: 194910
[parent_id]: 181440
[tags]: 
I do not have any solution for you, but if I were you I would look for inspiration on how to think about the problem in Item Response Theory . Among IRT models, one of the simplest ones is Rasch model for binary data. It is used for modeling student responses for test items. Assume that your data consists of $i = 1,...,n$ students that answered $j = 1,...,k$ test items $X_{ij}$ marked $1$ for correct answer and $0$ for incorrect. The data can be modeled using the following model $$ P(X_{ij} = 1) = \frac{\exp(\theta_i - \beta_j)}{1-\exp(\theta_i - \beta_j)} $$ where $\theta_j$ is students "ability" and $\beta_i$ is items "difficulty". As far as I understand, your data is different, but some insights from IRT can be generalized to your problem. You have $k$ questions that (possibly) differ in how difficult they are. Among $n$ persons that answered the questions you can possibly find such persons that were confident about both easy and hard questions -- unfortunately you don't know if they are confident because they are right or if they are overconfident. Similar with the persons who are unconfident about everything. Since, as you say, the questions are frames in such way that it is unlikely that anyone knows the exact answer, then you assume that people ranking high on confidence would be rather overconfident than right. If is it so, then you should probably assign greater weights to answers by people with average ratings of confidence. As I said, I don't have the ready answer and if you find this idea convincing, then you should think about ways of adapting it to your needs. Rasch model was made for different data and different purpose but the take-away message is to think of your problem in terms of two factors for questions' difficulty and people trait confidence.
