[site]: crossvalidated
[post_id]: 124439
[parent_id]: 122570
[tags]: 
How can I quantify how much more mental health varies between these two populations than poverty does? That has a straightforward, technical answer that I fear might be rather unsatisfactory in practice. Shannon entropy is a measure of the unpredictability of a message's content. (Consider "message" in this context to be an event drawn from a distribution.) The most intuitive, realistic example is that the entropy of a random message can expresses the number of binary digits needed, on average, to encode a message. Say you have a fair coin; half the time you must tell me it landed heads up, half the time tails. The entropy of each flip is one bit: One yes-no item is all it takes to convey the outcome. Now say you have a double-headed coin, and I know you flip it once each morning. It has an entropy of zero: I know that the coin will always come up heads, it's entirely certain. (A simple example, but the important point is that entropy gauges uncertainty on a common scale.) Since Shannon entropy can be applied to probability distributions, it's one tool to compare uncertainty, if you have distributions to compare. For example, in Bayesian terms you're looking to compare the uncertainty in the posteriors for mental health and addiction. If you assumed that each person was drawn from a categorical distribution with a Dirichlet prior, you'd end up with a Dirichlet posterior for each group and question. The R package entropy can help you calculate entropies over these posteriors: > entropy.Dirichlet(c(.55, .2, .1, .15) * 500, 1/2) [1] 1.167259 > entropy.Dirichlet(c(.75, .1, .1, .05) * 50000, 1/2) [1] 0.826111 > entropy.Dirichlet(c(.35, .4, .15) * 500, 1/2) [1] 1.026848 > entropy.Dirichlet(c(.30, .45, .25) * 50000, 1/2) [1] 1.067096 (The above are expressed in nats, which use the natural logarithm rather than $\log_2$; one nat $\approx 1.44$ bits.) This gives a suggestion of what tool exists to perform the mathematical task you're asking about, but I haven't any guidance on how to interpret these in a public health study. More, I gather that its practical use depends on how well-exposed your audience is to the concept. I did find this , which applies entropy to seismic interpretation, and outlines a few boons and pitfalls of using it as an uncertainty measure (p. 135): "Entropy is a better measure of uncertainty when pdfs have multiple modes[.]" "Entropy captures non-linear codependence whereas the covariance captures only linear dependence." "Entropy measures can be estimated from nonparametric pfds, are invariant to [...] coordinate transforms[.]" "A pitfall [...] is that entropy does not depend on the actual values of the variables but only on the pdfs." Perhaps these will help you determine if entropy is of value in this case, and make an argument to that effect, if you choose to apply it.
