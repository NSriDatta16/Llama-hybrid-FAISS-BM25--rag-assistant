[site]: crossvalidated
[post_id]: 144344
[parent_id]: 64010
[tags]: 
As already explained in several other answers and in comments, this question was based on at least three confusions: Function anova() uses sequential (also called type I) sum of squares (SS) decomposition that depends on the order of predictors. A decomposition corresponding to the regression coefficients and $t$-tests for their significance, is type III SS, that you can obtain with Anova() function from car package. Even if you use type III SS decomposition, then partial $R^2$ for each predictor are not going to be equal to the squared standardized coefficients $\beta_\mathrm{std}$. The ratios of these values for two different predictors will also be different. Both values are measures of effect size (or importance), but they are different , non-equivalent, measures. They might qualitatively agree most of the times, but they do not have to. What you called partial R squared is not partial R squared. Partial $R^2$ is defined as $\text{SS}_\text{effect}/(\text{SS}_\text{effect}+\text{SS}_\text{error})$. In contrast, $\text{SS}_\text{effect}/\text{SS}_\text{total}$ can be called "eta squared" (borrowing a term from ANOVA), or squared semipartial correlation, or perhaps semipartial $R^2$ (in both formulas $\text{SS}_\text{effect}$ is understood in the type III way). This terminology is not very standard. It is yet another possible measure of importance. After these confusions are clarified, the question remains as to what are the most appropriate measures of predictor effect size, or importance. In R, there is a package relaimpo that provides several measures of relative importance. library(car) library(relaimpo) mod Using the same Anscombe dataset as in your question, this yields the following metrics: Relative importance metrics: lmg last first betasq pratt genizi car income 0.47702843 0.4968187 0.44565951 0.9453764 0.64908857 0.47690056 0.55375085 young 0.14069003 0.1727782 0.09702319 0.1777135 0.13131006 0.13751552 0.13572338 urban 0.07191039 0.0629027 0.06933945 0.1188235 -0.09076978 0.07521276 0.00015460 Some of these metrics have already been discussed: betasq are squared standardized coefficients, the same values as you obtained with lm() . first is squared correlation between each predictor and response. This is equal to $\text{SS}_\text{effect}/\text{SS}_\text{total}$ when $\text{SS}_\text{effect}$ is type I SS when this predictor is first in the model. The value for 'income' (0.446) matches your computation based on anova() output. Other values don't match. last is an increase in $R^2$ when this predictor is added last into the model. This is $\text{SS}_\text{effect}/\text{SS}_\text{total}$ when $\text{SS}_\text{effect}$ is type III SS; above I called it "semipartial $R^2$". The value for 'urban' (0.063) matches your computation based on anova() output. Other values don't match. Note that the package does not currently provide partial $R^2$ as such (but, according to the author, it might be added in the future [personal communication]). Anyway, it is not difficult to compute by other means. There are four further metrics in relaimpo -- and one more (fifth) is available if the package relaimpo is manually installed: CRAN version excludes this metric due to a potential conflict with its author who, crazy as it sounds, has a US patent on his method. I am running R online and don't have access to it, so if anybody can manually install relaimpo , please add this additional metric to my output above for completeness. Two metrics are pratt that can be negative (bad) and genizi that is pretty obscure. Two interesting approaches are lmg and car . The first is an average of $\text{SS}_\text{effect}/\text{SS}_\text{total}$ over all possible permutations of predictors (here $\text{SS}_\text{effect}$ is type I). It comes from a 1980 book by Lindeman & Merenda & Gold. The second is introduced in (Zuber & Strimmer, 2011) and has many appealing theoretical properties; it is squared standardized coefficients after predictors have been first standardized and then whitened with ZCA/Mahalanobis transformation (i.e. whitened while minimizing reconstruction error). Note that the ratio of the contribution of 'young' to 'urban' is around $2:1$ with lmg (this matches more or less what we see with standardized coefficients and semipartial correlations), but it's $878:1$ with car . The reason for this huge difference is not clear to me. Bibliography: References on relative importance on Ulrike Grömping 's website -- she is the author of relaimpo . Grömping, U. (2006). Relative Importance for Linear Regression in R: The Package relaimpo . Journal of Statistical Software 17, Issue 1. Grömping, U. (2007). Estimators of Relative Importance in Linear Regression Based on Variance Decomposition . The American Statistician 61, 139-147. Zuber, V. and Strimmer, K. (2010). High-dimensional regression and variable selection using CAR scores . Statistical Applications in Genetics and Molecular Biology 10.1 (2011): 1-27. Grömping, U. (2015). Variable importance in regression models . Wiley Interdisciplinary Reviews: Computational Statistics, 7(2), 137-152. (behind pay wall)
