[site]: crossvalidated
[post_id]: 56522
[parent_id]: 56516
[tags]: 
In a machine learning context (i.e. "kernel methods"), the key requirement for a kernel is that it must be symmetric and positive-definite, that is, if $K$ is a kernel matrix, then for any (column) vector $x$ of the appropriate length, $x^{T}Kx$ must be a positive real number. This restriction is in place mostly due to requirements of optimization processes that operate downstream on this matrix. To answer your question, certain basic operations preserve positive-definiteness, and some don't. You can use the definition of positive-definiteness above to decide whether an operation preserves that property or not. The product operation does not always preserve positive definiteness (it does preserve it in the case where their product is commutative). The square operation however does preserve it. For a real positive number $r$, $rK$ is positive definite, and the sum of any two positive definite matrices is also positive definite. So in your examples, $K_3$ and $K_4$ may or may not be proper kernel matrices (to use your terminology) and $K_5$ is definitely a proper kernel matrix.
