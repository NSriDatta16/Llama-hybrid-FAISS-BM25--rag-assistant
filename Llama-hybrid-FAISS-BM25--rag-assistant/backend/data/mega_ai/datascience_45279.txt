[site]: datascience
[post_id]: 45279
[parent_id]: 
[tags]: 
How should multiclass classifier performance be measured when one type of error is preferred over another?

Sorry if this question has been asked before--I am having trouble searching this topic since I'm not sure of my wording. Say you have a classification problem where there are more than two labels which are discrete but roughly correspond to an increase in some quality--call these labels A, B, and C. Also say in this problem it would be preferrable to over-estimate that quality, rather than to underestimate. Is there a type of metric that captures this skew and penalizes a predicted A on an actual B more than it penalizes a predicted C on an actual B? Or is this preference better handled in a different part of data science methodology?
