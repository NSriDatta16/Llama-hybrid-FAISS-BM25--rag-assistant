[site]: crossvalidated
[post_id]: 539547
[parent_id]: 539545
[tags]: 
Think of the bias as "if I repeat my experiment many times and apply my estimator each time, how much does the average estimation result differ from the true result?" For example, the sample variance $$ \hat{\sigma}_Y^2 = \frac{1}{N}\sum_{i=1}^N (Y_i-\bar{Y})^2 = \frac{N-1}{N} \sigma_Y^2 $$ is a biased estimator of the variance of a distribution, which means that on average over many repeated experiments it will under-estimate the true variance $\sigma_Y^2$ . This does not mean that it will under-estimate it every single time . To assess the biasedness or unbiasedness of the estimator, one will usually make some assumptions about $P(x, \theta)$ . E.g., the derivation of the biasedness of the sample variance assumes independent and identically distributed (i.i.d.) measurements. The magnitude of the bias can in general depend on many factors such as the sample size $N$ and the true parameter vector $\theta$ . To put it another way, the bias of an estimator is something that is usually analyzed theoretically under specific assumptions. Therefore, one can assume full knowledge of the true parameters $\theta$ for the purpose of the derivation. In the above case of the sample variance, the bias is easy to correct (just multiply the resulting estimate by $\frac{N}{N-1}$ ), but in general this may not be possible, e.g., if the bias depends on the true parameter vector $\theta$ . As a sidenote, in many cases it may be desirable to use a biased estimator because, e.g., the mean squared error of a biased estimator may be lower than the mean squared error of an unbiased estimator. See, e.g., the bias-variance trade-off . One classical example is the James-Stein estimator , which is biased but has a lower MSE than ordinary least squares, which is unbiased.
