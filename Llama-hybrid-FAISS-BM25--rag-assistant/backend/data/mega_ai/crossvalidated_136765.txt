[site]: crossvalidated
[post_id]: 136765
[parent_id]: 136742
[tags]: 
The Bayes risk is the frequentist risk averaged over the parameter space against to the prior distribution $\lambda$. The notion turns a function of $\theta$, $R(\theta,\delta)$, into a positive number, $R(\delta,\lambda)$, and hence allows for a total ordering of estimators $\delta$, hence for the definition of the Bayes estimator$$\delta^\lambda=\arg\min_\delta R(\delta,\lambda)$$ The link with the conditional Bayesian approach is that, thanks to Fubini's theorem, the Bayes estimator can also be derived by minimising for every $x$ the posterior expected loss$$\varrho(\delta(x),\lambda)=\mathbb{E}[L(\theta,\delta(x))|X=x]$$ As noted by @guy, this quantity also has frequentist motivations, one of them being that, in regular problems, the minimax risk equals the maximin risk in the following way:$$\min_\delta\max_\theta R(\theta,\delta)=\max_\lambda\min_\delta R(\delta,\lambda)$$expressing minimax estimators as "worst" Bayes estimators.
