[site]: datascience
[post_id]: 42008
[parent_id]: 
[tags]: 
Multiple classification algorithms are predicting always exactly with the same scores. Is that normal? If not, what should I suspect?

I have been working on a multilabel classification problem. I am using Python machine learning libraries to implement the classification algorithms. For the cross-validation, I am using repeated K-Fold cross-validation. I have experimented with SVM, Logistic Regression, Random Forest, Decision tree, K-Neighbour, and Naive Bayes and using Binary Relevance, Classifier Chain, and Label Powerset transformation methods for all of them. I noticed, for Classifier Chain, SVM, Logistic Regression, Random Forest, and K-Neighbors are always achieving same subset accuracies and hamming losses. For Label Powerset, SVM, Logistic Regression, and Random Fores are achieving the same scores. However, for the binary relevance, all scores are different. No matter, what random seed I use or how many repetitions of cross-validations I run, they always end up scoring the same. I was wondering if this is a normal occurrence or they are suffering from class imbalance or overfitting or anything of that sort problems? P.S. I am a beginner. Sorry for bothering with a naive question.
