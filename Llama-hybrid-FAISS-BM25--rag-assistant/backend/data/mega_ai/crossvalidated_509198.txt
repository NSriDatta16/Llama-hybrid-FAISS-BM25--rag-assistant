[site]: crossvalidated
[post_id]: 509198
[parent_id]: 
[tags]: 
How can I ensure that a neural network doesn't map small values to zero?

I'm currently trying to train an autoencoder that reconstructs magnitude spectrograms. Here is an example of a magnitude spectrogram: And its reconstruction after passing it through the trained autoencoder: From the reconstruction, it seems that values that are small, less than $-30 \text{dB}$ , are mapped to even smaller values in the reconstruction, and values that are large, more than $-30 \text{dB}$ , are mapped to even larger values in the reconstruction. The input signal is first standardized (zero mean and unit variance), and the magnitude spectrogram is then computed. The magnitude spectrogram itself is also limited to the range $[0,1]$ as it is passed through the network and then re-scaled back to its original values to help in training. If necessary, I can post the architecture of the autoencoder and the code I am using to train it.
