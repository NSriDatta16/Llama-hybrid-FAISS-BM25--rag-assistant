[site]: crossvalidated
[post_id]: 229676
[parent_id]: 
[tags]: 
Formulation of states for this RL problem and other questions.

Consider a gridworld 100 $\times$ 100 with a starting position, $S$, on the lower left corner and a goal position, $G$, somewhere at the center. Both $S$ and $G$ are fixed and does not move. Furthermore, subgoals exist one at a time. For example, '1' appears in a random position. If '1' is reached, then '2' appears and so on. The succeeding numbers appear randomly but will have a distance closer and closer to the final goal $G$. The transition probability is not available and will not be learned, so model free learning will be used. Agent can move in 4 cardinal directions. I have many serious questions: What could be a nice state space representation for this? The fact that it is 100 $\times$ 100 means that I am avoiding the usual $(x,y)$ 'coordinate' position or the cardinality of the state space will be so big. My idea would be to choose the 'relative' position between the agent and the goal. Something like: $(x,y)$, $x = \{ -1, 0, 1\}$. $-1$ if the goal is to the left of the agent, 1 if the goal is to the right, and 0 if the goal is at the same $x$ coordinate as the agent. $y = \{ -1,0,1\}$, $-1$ if the goal is below, $1$ if above the agent. 0 if they the same $y-$coordinate. So maybe the state space formulation can follow this form found here: Model free reinforcement learning with subgoals: how to reinforce learning with only one reward? $(x,y,k)$ where $k$ is the number subgoal attained? Is this a tractable problem? I mean, I understand that this may have been poorly written, but I want to share my intuition. The number of subgoals that I have here might not be known apriori. But they are definitely going nearer and nearer to the goal position. Is there a way to write the states to include the subgoal positions without giving multiple rewards? I thought about giving multiple rewards to the agent after getting each subgoal, but that would be 'cheating'. I believe it is standard in RL that the reward is given at the end of the task. I would some your valued insights into this problem. Feel free to send comments or more questions if there are any.
