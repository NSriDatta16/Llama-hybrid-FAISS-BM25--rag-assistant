[site]: datascience
[post_id]: 122195
[parent_id]: 122194
[tags]: 
In the case of word2vec and GloVe , the out-of-vocabulary (OOV) problem is usually addressed by simply ignoring the OOV word. That is, act as if the word were not present in the text since the beginning. There is not much of an alternative, because they are simply tables that associate a word with a vector. If some word is not on the table, then you may try to find a similar word by other means (e.g. a synonym dictionary) and use its vector instead, but normally it is not worth it and the most cost-effective solution is to simply drop the word. For FastText the issue is different because it works with character n-grams: a word embedding is simply the sum of its n-gram vectors plus the vector of the word itself. Therefore, if the n-grams of the OOV word are in the embedding, FastText would still be able to give you a representation for it.
