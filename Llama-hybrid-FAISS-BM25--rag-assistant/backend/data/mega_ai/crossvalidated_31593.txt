[site]: crossvalidated
[post_id]: 31593
[parent_id]: 
[tags]: 
How do I compare the accuracy of two measurement devices when one of them is the reference?

I'm currently trying to compare the measurement accuracy of two devices. However, I am using one of the devices as a "gold standard" to say something about the accuracy of the other device. The quantity I am trying to measure is the distance (depth) to a point in a scene (image). I am using a laser scanner which gives me $(x,y,z)$ coordinates for a point and I am using optical geometry to give me $(x,y,z)$ coordinates for the same point. My base (or reference) measurement is the laser scanner measurement. I am using this as a gold standard. I now have a corresponding measurement from the optical geometry based measurement system. I have corresponding measurements for 1000's of points. I average the error in the difference between these and get a mean error. This error, however, neglects the fact that the laser scanner too is a measurement system and has its own error. It comes with an accuracy of $\pm5\,mm$ for an object at $10\,m$ range. How do I incorporate this into an accuracy metric for the optical geometry based system?
