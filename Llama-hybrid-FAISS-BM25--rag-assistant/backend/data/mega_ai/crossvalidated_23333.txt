[site]: crossvalidated
[post_id]: 23333
[parent_id]: 23331
[tags]: 
Consider a finite set of m records. If you use all the records as a training set you could perfectly fit all the points with the following polynomial: y = a0 + a1*X+a2*X^2 + ... + an*X^m Now if you have some new record, not used in training set and values of an input vector X are different from any vector X, used in training set, what can you tell about the accuracy of prediction y? I suggest you to go over an example where you have 1 or 2-dimensional input vector X (in order to visualize the overfitting polynomial) and check how big is the prediction error for some pair (X, y) which X values are just a little different from the values from the training set. I don't know if this explanation is theoretic enough, but hopefully it helps. I tried to explain the problem on regression model as I consider it more intuitively understandable than others (SVM, Neural Networks...). When you build a model, you should split the data into at least training set and test set (some split the data into training, evaluation, and cross validation set). Usually 70% of data is used for training set and 30% for evaluation and then, when you build the model, you have to check the training error and test error. If both errors are big, it means your model is too simple (the model has high bias). On the other hand if your training error is very small but there is a big difference between training and test error, it means your model is too complex (the model has high variance). The best way to choose the right compromise is to plot training and test errors for models of various complexity and then choose the one where the test error is minimal (see the picture below).
