[site]: crossvalidated
[post_id]: 466914
[parent_id]: 465196
[tags]: 
About the first two bullets in your question: yes, KS test statistic can be used as a distance metric between different empiric distributions, and yes, it ranges between 0 and 1, where 0 occurs if the two ECDFs are indentical, and 1 if the two samples are completely distinct (the least value of one sample is higher than the maximum value of the other), the more the two samples are "mixed" and then hard to distinguish, the lesser KS test statistic will be. However, KS test statistic is not a very sensible metric: it only uses the maximum difference between ECDFs, without considering their distance in the rest of their domain, this is the same reason for which KS test is so conservative: ref 1 ref 2 . If you are willing to consider other options, there is plenty of choice! The alternatives Strictly related metrics to KS test statistic are those of Cram√©r-von Mises test and Anderson-Darling's . They both consider the whole difference function between the ECDFs. In particular, the latter is generally better regarded. All these three metrics depend on the order of the observations, and not on their distance, hence, the result is invariant to monotonic transformations of the data. Another class of distances between probability distributions includes the already cited and very popular KL divergence (or, more properly, Jensen-Shannon divergence), L1 and L2 distance, Hellinger distance . All these metrics require an estimate of the two probability distribution functions, the most simple being histograms, (over the same bins, necessarely). In that case L1 distance is directly related to histogram intersection similarity which has lately known some fortune in data science, for what I've heard. Anyway, JS divergence is the most theoretically founded among all these, from a probability perspective. All these metrics are computed over the difference between PDFs, so they do not take into consideration distance between data points (of course density estimation can take distance between points into consideration in many ways, like with tails of gaussian kernel for instance). But my favorite of all between-distributions distances is earth mover distance , which is, in the univariate numeric case, the whole area between two ECDFs. EMD doesn't require estimation of PDFs, and considers not only the order of observations, but also their distance, meaning that it is very sensitive, as long as your observations lies on a sensible metric space. EMD is also very intuitive in its definition, and widely used. Of course, if you don't trust the metric space of your data, you can go for another option. All these options are "holistic" distance metrics between distributions, in the sense Single Malt meant. Of course you can consider narrower metrics like the simple difference between the arithmetic means of the two samples. That makes a lot of sense for some applications, however, in this answer I have covered more general distances that consider, in one way or another, the whole shape of the two distributions.
