[site]: crossvalidated
[post_id]: 300163
[parent_id]: 
[tags]: 
compare classification models with t-test (or Wilcoxon)

I am curious whether the statistical test (parametric/non-parametric) are appropriate tools to compare classification models. For example, consider a data set with 10 features - 6 essential and 4 optional. I build three models (for the sake of simplicity, consider a linear logistic regression function with two outcome classes: 0 and 1): model I - model with only 6 (essential) feature model II - model with all 10 (essential+optional) features model II - model with 4 (optional) features I split the data set into train/test sets, train three models on the training set and assess their performance on the test set. Each split results in three metrics (for example, accuracy: m1 , m2 , m3 ) for each model respectively. Then I repeat the process n times (n=1000) where each run corresponds to a random train/test split, but all models are trained and tested on the same data . The result is a matrix of size n x 3 . QUESTION Is it meaningful to apply (paired) t-test (or non-parametric) to assess whether three models are significantly different? Of course the t-test should be adjusted to the fact that runs are correlated. NB One can of course use a standard method for models selection (for example, LASSO, or similar), however, I am interested not in 'the best model' selected from 10 features, but rather to compare the aforementioned models.
