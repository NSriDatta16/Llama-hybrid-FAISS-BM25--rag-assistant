[site]: crossvalidated
[post_id]: 475218
[parent_id]: 
[tags]: 
Variable length memory / information flow in Transformers

My understanding is that RNNs, LTSMs and GRUNs can theoretically "remember" and "use" information in an input sequence spanning arbitrarily long distances, and one does not need to specify in any way the max. separation or distance between symbols in the input sequence that we may want the network to consider. Do transformers ( paper ) have the same ability? From my high-level understanding of transformers, they don't seem to have any recurrent information flow that would allow them to consider arbitrarily old inputs or outputs when decoding new inputs. Or am I wrong? And if so, where in the following schematic from the original paper would the network capture that recurrent dependency? (i.e where in the circuit is information from an arbitrarily old past re-used?)
