[site]: crossvalidated
[post_id]: 236703
[parent_id]: 236700
[tags]: 
If your regression is like e.g. $y=\beta_0 + \beta_1 x + \epsilon$ where $\epsilon$ has the usual assumptions, then you can partition on $x$ because it is known, but $y$ is a random quantity (because of the error term $\epsilon$) so if you partition on $y$ it could be that sometimes a subject changes class because of randomness (i.e. the realisation of the random $\epsilon$). The model $y=\beta_0 + \beta_1 x + \epsilon$ tells you that $y$ is a random quentity, it est, for a given value of $x$ you know the distribution of $y$: $y|_x \sim N(\beta_0 + \beta_1 x,\Sigma)$, so $y$ can change from one experiment to another (because $y$ 'is a distribution, not a value'). If you partition on a value that can change from one experiment to another, obviously your partition also becomes random. An example of the consequences can be seen when analysing the Hosmer-Lemeshow test for a logistic regression model, this is a $\chi^2$-test but if you look at the degrees of freedom of the Hosmer-Lemeshow test you will see that it is unusual. This is because for computing the test statistic you define 10 groups based on the predicted (and thus random) probabilities of a logistic regression model. Other examples can be found in Greene, Econometric Analysis, where the author analyses the consequences of choice based sampling for logistic regression.
