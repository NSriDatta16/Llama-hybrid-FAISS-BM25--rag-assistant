[site]: crossvalidated
[post_id]: 362178
[parent_id]: 
[tags]: 
Does it make sense to use a dropout layer in a neural network for a regression to predict an absolute Error?

I am working on a regression problem where I try to predict an Error with a NN with as little calculation steps as possible. Currently I have an input layer consisting of 21 Neurons and a Dense Output layer consisting of 4 Neurons. Would it make sense to use a Dropout Layer to prevent Overfitting? I read that using dropout layers makes sense in general but for this particular problem it seems counterintuitive, since I am creating an Error every time I ignore a certain Neuron so the other neurons would be trained to "overcompensate" this error.
