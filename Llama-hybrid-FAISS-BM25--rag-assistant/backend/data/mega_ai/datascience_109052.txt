[site]: datascience
[post_id]: 109052
[parent_id]: 
[tags]: 
Feature selection with "overly important" features

I am very new to machine learning modeling, but I encountered a feature selection problem that I hope can get your insights on: For example, I have A,B,C,D as my independent variables and y as my dependent variable. The end user is more interested in C & D's impact on y since A and B are factors that the user don't have much power to change. But in the modeling, we see that A and B have very large feature importance in predicting y, while C and D have low prediction power. In this case, should I train the model only based on C&D or I should train the model based on ABCDï¼Ÿ or is there any feature engineering I should do?
