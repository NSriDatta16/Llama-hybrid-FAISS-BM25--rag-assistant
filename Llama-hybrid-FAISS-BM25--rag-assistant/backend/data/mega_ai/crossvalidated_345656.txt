[site]: crossvalidated
[post_id]: 345656
[parent_id]: 
[tags]: 
Test accuracy much higher than training accuracy

I trained a fully connected neural network with with five hidden layers of size $2024$ each. I used the Adam optimizer with a learning rate of $1e-4$ and a drop out rate of $0.4$. Batch size was $1000$. After about $24h$ of training I saw that the test accuracy is much higher than the training accuracy. How is that possible and how can I interpret this result? EDIT: I trained the network for the MNIST data set and rescaled test and training data by $2.0*(IMAGES/255.0-0.5)$ Therefore I assume, that both training and test set are equally distributed.
