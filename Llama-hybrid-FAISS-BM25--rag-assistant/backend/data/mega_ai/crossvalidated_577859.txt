[site]: crossvalidated
[post_id]: 577859
[parent_id]: 
[tags]: 
In Bayesian hierarchical models, what is the difference between an Empirical Bayesian approach to parametrising priors vs using flat hyperpriors

Say I have a simple hierarchical model, where: $y_{g,i} = \beta_g x_{g,i} + e_{g,i}$ where $g$ represents the group, $i$ represents the individual within the group, and $e$ is the error. So the variable $x$ has different effects for the different groups. I want to approach this in a Bayesian way, so I make the following assumption: $\beta_g \sim f(\mu, \sigma)$ , where $f$ is some distribution with mean $\mu$ and standard deviation $\sigma$ . As far as I can tell, I have two options when it comes to parametrising the priors $\mu$ and $\sigma$ : I could further assume that the priors themselves have some (probably flat) distribution (i.e. use uninformative hyperpriors); or I could use the data to obtain point estimates for the average and standard deviation of the group effects of $x$ , and assign these as the values of $\mu$ and $\sigma$ respectively (i.e. empirical Bayes). As far as I can tell, from a philosophical perspective the first approach is preferable, because it lets the Bayesian model 'learn' the distributions of $\mu$ and $\sigma$ from the dataset and doesn't involve looking at the data twice, which I think is generally inadvisable. But in practice: Will these approaches lead to different results for our estimates of $\beta_g$ ? Based on my experience, they will. But why? And will one approach tend to lead to more aggressive pooling/shrinkage than the other? Is there ever a a priori reason to prefer the empirical Bayes approach (other than maybe a computational one)? If not, why would we ever want to do empirical Bayes when we can just use uninformative hyperpriors and let the Bayesian method figure things out on its own?
