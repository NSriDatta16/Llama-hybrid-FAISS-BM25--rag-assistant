[site]: crossvalidated
[post_id]: 486413
[parent_id]: 
[tags]: 
Does model need normalized data?

With a very limited time to look at the model's architecture, how does one decide whether or not an arbitrary model need normalized input data? There are tons of ML libraries out there and most of the time the end-user only has a vague idea about what the algorithm is doing on the inside. For example, suppose I only the know the naive k-means algorithm this deep: Specify number of clusters K. Initialize centroids by first shuffling the dataset and then randomly selecting K data points for the centroids without replacement. Keep iterating until there is no change to the centroids. i.e assignment of data points to clusters isnâ€™t changing. Compute the sum of the squared distance between data points and all centroids. Assign each data point to the closest cluster (centroid). Compute the centroids for the clusters by taking the average of the all data points that belong to each cluster. source Can I still say whether or not this algorithm needs whitened data with reasonable confidence? What about K-Means? DBSCAN? SVM? Is there some general flag that makes a new user go "aha! so it does need normalization!" ?
