[site]: crossvalidated
[post_id]: 397891
[parent_id]: 353406
[tags]: 
As a simplified abstraction, you can think of XGBoost as a special kind of logistic regression, where the "features" are boolean indicators of membership in the terminal nodes of trees. I elaborate on this more in How does gradient boosting calculate probability estimates? If you adopt this perspective of a boolean feature matrix given by some decision tree, then you need to figure out how to set the weights $w$ (i.e. the coefficient vector). In the section " Model Complexity ," the author writes Here $w$ is the vector of scores on leaves... The score measures the weight of the leaf. See the diagram in the " Tree Ensemble " section; the author labels the number below the leaf as the "score." The score is also defined more precisely in the paragraph preceding your expression for $\Omega(f)$ : We need to define the complexity of the tree $\Omega(f)$ . In order to do so, let us first refine the definition of the tree $f(x)$ as $$f_t(x)=w_{q(x)}, w \in R^T, q:R^d \to {1,2,\dots,T}.$$ Here $w$ is the vector of scores on leaves, $q$ is a function assigning each data point to the corresponding leaf, and $T$ is the number of leaves. What this expression is saying is that $q$ is a partitioning function of $R^d$ , and $w$ is the weight associated with each partition. Partitioning $R^d$ can be done with coordinate-aligned splits, and coordinate-aligned splits are decision trees. The meaning of $w$ is that it is a "weight" chosen so that the loss of the ensemble with the new tree is lower than the loss of the ensemble without the new tree. This is described in " The Structure Score " section of the documentation. The score for a leaf $j$ is given by $$ w_j^* = \frac{G_j}{H_j + \lambda} $$ where $G_j$ and $H_j$ are the sums of functions of the partial derivatives of the loss function wrt the prediction for tree $t-1$ for the samples in the $j$ th leaf. (See " Additive Training " for details.)
