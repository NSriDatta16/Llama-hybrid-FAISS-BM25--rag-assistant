[site]: crossvalidated
[post_id]: 283468
[parent_id]: 283458
[tags]: 
Perhaps, this is not going to be a complete answer as your question is very broad. Let me emphasize some aspects: Numerical stability. If you have all inputs in range zero to one, the float number representation is let's say more predictable. Interpretation. If we consider e.g. linear or logistic regression, we can interpret the parameters as influence of being best in a specific input parameter. Robust methods. E.g. the penalization of coefficients in ridge regression can be fair. Without normalization, you would overuse the columns with a wide range and ignore those with a small range. Note that the normalization is typically carried out column-wise, i.e. each column has its own normalization. This causes the difference. Imagine that one column's values range from 0 to 1e6 while the other's from 1 to 1e-6. Without normalization, the k-means will use just the first column as the second has no influence. After normalization of both to 0 to 1, the impact of the second column will come.
