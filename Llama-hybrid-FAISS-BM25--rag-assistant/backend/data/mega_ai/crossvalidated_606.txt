[site]: crossvalidated
[post_id]: 606
[parent_id]: 490
[tags]: 
A very popular approach is penalized logistic regression, in which one maximizes the sum of the log-likelihood and a penalization term consisting of the L1-norm ("lasso"), L2-norm ("ridge"), a combination of the two ("elastic"), or a penalty associated to groups of variables ("group lasso"). This approach has several advantages: It has strong theoretical properties, e.g., see this paper by Candes & Plan and close connections to compressed sensing; It has accessible expositions, e.g., in Elements of Statistical Learning by Friedman-Hastie-Tibshirani (available online); It has readily available software to fit models. R has the glmnet package which is very fast and works well with pretty large datasets. Python has scikit-learn , which includes L1- and L2-penalized logistic regression; It works very well in practice, as shown in many application papers in image recognition, signal processing, biometrics, and finance.
