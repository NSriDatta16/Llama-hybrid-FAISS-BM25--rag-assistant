[site]: crossvalidated
[post_id]: 351710
[parent_id]: 351705
[tags]: 
I've never trained a GAN, so this answer is incomplete. However, the mixup paper, " mixup : Beyond Empirical Risk Minimization " (Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, David Lopez-Paz) claims that mixup stabilizes the training of GANs. Mixup works by training a model using an augmented data set which is composed of random, convex combinations of the original data. For random pairs of indices $i,j$ , mixup makes new data points and the loss is computed as a convex combination of the two labels. These indices refer to the samples comprising the data, so if your data set is 100 images, then the indices are 1, 2, 3, ..., 100. $$ \begin{align} \lambda &\sim \text{Beta}(\alpha, \alpha) \\ \tilde{x} &= \lambda x_i + (1-\lambda)x_j \\ \mathcal{L}(\tilde{x}|\lambda,y_i,y_j, f) &= \lambda L(f(\tilde{x}), y_i) + (1-\lambda)L(f(\tilde{x}),y_j) \end{align} $$ where $f(\cdot)$ is the model prediction and $L$ is a standard loss function (e.g. cross-entropy loss) and $\alpha$ is a tuning hyperparameter. This helps training by enforcing smoother interpolation between training data, instead of abrupt shifts. There are probably more tricks that could help, this just happens to be one that I learned "by accident" in the course of doing some other research.
