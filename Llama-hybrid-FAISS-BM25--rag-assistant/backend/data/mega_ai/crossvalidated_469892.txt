[site]: crossvalidated
[post_id]: 469892
[parent_id]: 
[tags]: 
Deep learning's auto extraction of representation

I am following up on a paper that demonstrates using deep learning (CNN) for classification. Specifically, their approach transformed the spatial data into fixed-length segments appropriate for CNN requirements. In their work, the channel's dimension consists of 3-kinematics of min_speed, avg_speed, max_speed . Based on the outcome of their work, the model achieves 80+ on the accuracy, and well on other metrics. Out of curiosity, I took the task to replicate their work and was successful. However, on a further check, I decided to take a look at the distribution of these kinematics computed per class before feeding to the CNN, in a boxplot. It turns out that a lot of data points fall as outliers. So I begin to doubt. Are we feeding the network with instances full of outliers despite this interesting result? How does feeding a deep learning algorithm with data points full of outliers affect itÂ´s performance? Should we expect the network to learn representation from what seems to be outliers in the first place? The figure below shows a boxplot of the kinematics in the channel dimension of instances.
