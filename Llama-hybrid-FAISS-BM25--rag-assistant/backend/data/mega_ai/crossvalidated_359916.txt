[site]: crossvalidated
[post_id]: 359916
[parent_id]: 
[tags]: 
In the expectation step, why do we sometimes assign the data to a component (i.e. complete the data) instead of calculating the expected value?

Let $Y|X$ be a mixture distribution conditional on covariates $X$, with distribution function $Y(x; \sigma, \psi, \phi) = \alpha Y_1(x; \psi) + (1 - \alpha) Y_2(x; \phi)$, for the averaging parameter $\alpha = \alpha(x; \sigma)$. One application of the expectation maximisation (EM) algorithm is to find the maximum likelihood estimate (MLE) of $\boldsymbol{\theta} = (\sigma, \psi, \phi)$ given observed data $(x_1, y_1), (x_2, y_2), \dots, (x_N, y_N)$. The tutorial (pg 6,7) by Sean Borman derives the steps to do this. Essentially, we posit a latent, unobserved random variable $Z \sim \operatorname{Bern}(\alpha)$, such that when $Z = 0$ (resp. $Z = 1$), variates $y$ of $Y$ are drawn from the distribution $Y_1$ (resp. $Y_2$). Then, we can find $\boldsymbol{\theta}$ by iteratively producing better new estimates $\boldsymbol{\theta}_\text{new}$ from old estimates $\boldsymbol{\theta}_\text{old}$ using the following procedure (reproduced with some minor alterations from this answer ): E step : Calculate function $Q$, the expected log-likelihood $Q(\boldsymbol{\theta}\mid\boldsymbol{\theta}_\text{old})=\sum_Z p(Z\mid X, Y,\boldsymbol{\theta}_\text{old})\log p(X,Z|\boldsymbol{\theta})$ M step : $\boldsymbol{\theta}_\text{new}=\max_\boldsymbol{\theta} Q(\boldsymbol{\theta}\mid\boldsymbol{\theta}_\text{old})$ At this point it is clear to me that the distribution of $Z$, conditional on the observed data and old parameters is used to calculate the function $Q$ in the E-step. My confusion arises when $p(Z\mid X,\boldsymbol{\theta}_\text{old})$ in the expectation step is replaced with realised values of $Z$, in a process referred to as "completing the data". Some examples: In this answer , it is stated: "On the contrary, using the EM algorithm, we first "assign" each sample to a component ( E step )..." The first example given on page 2 of the original EM paper ( Dempster, Laird and Rubin ) seemingly estimates a completion of the data in the E-step, then uses these realised values (instead of the distribution of the latent variable) in the M-step. In this post ( on baseball statistics ), baseball players are assigned into two clusters "A" and "B" in the E-step. So far I have not been able to find a justification on why we are allowed to do this. The best so far is from the first page of the introductory Nature paper , which states: "The expectation maximization algorithm is a refinement on this basic idea.". My interpretation of this statement is that the basic idea is the data completion step and the refinement is the computation of the expected value, as stated above in the E-step, using the distribution of $Z$. My two questions are: When is it possible to use the data completions instead of the expected value in the E-step? Why would you use data completions?
