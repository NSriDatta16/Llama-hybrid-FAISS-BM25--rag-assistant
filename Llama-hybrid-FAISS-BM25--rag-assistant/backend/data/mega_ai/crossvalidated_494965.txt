[site]: crossvalidated
[post_id]: 494965
[parent_id]: 140061
[tags]: 
I think the approach to only encode the ordinal labels as class 1 is represented as [0 0 0 0 ...] class 2 is represented as [1 0 0 0 ...] class 3 is represented as [1 1 0 0 ...] and use binary cross-entropy as the loss function is suboptimal. As mentioned in the comments, it might happen that the predicted vector is for example [1 0 1 0 ...]. This is undesirable for making predictions. The paper Rank-consistent ordinal regression for neural networks describes how to restrict the neural network to make rank-consistent predictions. You have to make sure that the last layer shares its weights, but should have different biases. You can implement this in Tensorflow by adding the following as the last part of the network (credits for https://stackoverflow.com/questions/59656313/how-to-share-weights-and-not-biases-in-keras-dense-layers ): class BiasLayer(tf.keras.layers.Layer): def __init__(self, units, *args, **kwargs): super(BiasLayer, self).__init__(*args, **kwargs) self.bias = self.add_weight('bias', shape=[units], initializer='zeros', trainable=True) def call(self, x): return x + self.bias # Add the following as the output of the Sequential model model.add(keras.layers.Dense(1, use_bias=False)) model.add(BiasLayer(4)) model.add(keras.layers.Activation("sigmoid")) Note that the number of ordinal classes here is 5, hence the $K-1$ biases. I tested the difference in performance on actual data, and the predictive accuracy improved substantially. Hope this helps.
