[site]: crossvalidated
[post_id]: 584460
[parent_id]: 
[tags]: 
Is it circular reasoning to compute the ELBO using MCMC?

Let's say we have a posterior distribution $q(\theta) = p(\theta \mid D, \mathcal{M})$ over parameters $\theta$ given data $D$ and a model $\mathcal{M}$ . As is often the case, computing $q$ is hard, because of its normalization constant (aka partition function , or model evidence ) in the denominator of $q$ $$ Z = p(D \mid \mathcal{M}) = \int{ p(D, \theta \mid \mathcal{M}) d\theta} $$ where one needs to integrate over potentially huge parameter spaces. However, we can rewrite $\ln{Z}$ using the Helmholtz free energy (like done here ): $$ \ln{Z} = -F_H = \underbrace{\int{ q(\theta) \ln{p(D \mid \theta, \mathcal{M}) d\theta}}}_{\text{ELBO for accuracy}} - \underbrace{\int{ q(\theta) \ln{\frac{q(\theta)}{p(\theta \mid \mathcal{M})}} d\theta}}_{\text{KL-divergence for complexity}} $$ Now, using MCMC allows me to draw samples $\hat{\theta} \sim q$ from the posterior while only being able to actually compute the joint $p(D, \theta \mid \mathcal{M})$ . I get that I cannot compute the model evidence using MCMC, because of the KL-divergence, which would require me to be able to compute $q$ . But I should be able to compute the other term (which is often called ELBO , I think) without problems using a Monte Carlo estimator: $$ \text{ELBO} \approx \frac{1}{S} \sum_{i=1}^S{\ln{p(D \mid \hat{\theta}_i, \mathcal{M})}} $$ where the $\hat{\theta}_i \sim q(\theta)$ are $S$ samples drawn e.g. using the Metropolis-Hastings algorithm. On one hand, I'm afraid this is circular reasoning, because we used just that (log-)likelihood to draw the samples in the first place. On the other hand, in the BIC we also get an parameter value $\hat{\theta}_{ML}$ from maximizing the function we try to estimate. So is this something one can do? And if yes, would it make sense to improve the BIC by using the MCMC estimator for the ELBO instead of the maximum log-likelihood?
