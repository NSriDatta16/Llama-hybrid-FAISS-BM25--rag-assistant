[site]: crossvalidated
[post_id]: 94065
[parent_id]: 94060
[tags]: 
A common way of evaluating classification models is the F-score , where $P$ is precision and $R$ is recall: $2\frac{PR}{P+R}$. Precision is the quotient of predicted true positive prediction and all predicted positive values; recall is the quotient of true positive values and all actual positive values. In more intuitive terms, precision is the percentage of your positive predictions that were correct; recall is the percentage of positive conditions that you correctly identified. A very intuitive explanation is given in this video from Andrew Ng's machine learning course. (Highly recommended!) In the Coursera version of that video, a quiz suggests a reasonable approach to setting the logistic regression threshold for prediction: Calculate the F-score on a cross-validation set, and choose the threshold that maximizes F. (Note that an error-free, perfect classifier would have $F = 1$.) I would advise seeing if any threshold could outperform the all-positive model, in terms of F-score. Meaning, if guessing that everyone has cancer yields a higher F, then the model needs improvement. This could mean using polynomials of the features to a draw a non-linear decision boundary, or considering another method all together.
