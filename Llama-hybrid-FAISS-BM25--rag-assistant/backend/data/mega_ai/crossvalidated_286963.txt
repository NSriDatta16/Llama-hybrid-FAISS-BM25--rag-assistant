[site]: crossvalidated
[post_id]: 286963
[parent_id]: 272792
[tags]: 
Why then do we need to compare two forecasts directly, given we've already established which is one is superior via RMSE etc? Isn't our interest in establishing which one replicates values closest to the actual figures? Let Diebold himself answer this for you ( Diebold, 2015 , second paragraph in the Introduction): The need for formal tests for comparing predictive accuracy is surely obvious. We’ve all seen hundreds of predictive horse races, with one or the other declared the “winner” (usually the new horse in the stable), but with no consideration given to the statistical significance of the victory. Such predictive comparisons are incomplete and hence unsatisfying. That is, in any particular realization, one or the other horse must emerge victorious, but one wants to know whether the victory is statistically significant. That is, one wants to know whether a victory “in sample” was merely good luck, or truly indicative of a difference “in population.” (emphasis mine) Secondly how would one go about interpreting the following Diebold Mariano test I've conducted. Would the below be correct? Strictly speaking, you never accept a hypothesis (be it null or alternative), you can only fail to reject. Also note that Forecast 1 is less accurate than forecast 2 is the same as Forecast 2 is more accurate than Forecast 1 (of course you did not mean that). Other than that, I do not know what the particular software means by >prog (with a spelling error) or , but it indeed looks as if they refer to outcomes of two one-sided $t$-tests. You should be able to figure out which forecast is better by looking at the loss values, then you will know clearly what forecasts these test result correspond to. References: Diebold, Francis X. "Comparing predictive accuracy, twenty years later: A personal perspective on the use and abuse of Diebold–Mariano tests." Journal of Business & Economic Statistics 33.1 (2015): 1-1. (Ungated version here .)
