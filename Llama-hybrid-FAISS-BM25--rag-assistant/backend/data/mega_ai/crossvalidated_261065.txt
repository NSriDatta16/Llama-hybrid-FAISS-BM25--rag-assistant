[site]: crossvalidated
[post_id]: 261065
[parent_id]: 261030
[tags]: 
First of all: Always include the intercept (i.e. "fitting the intercept") in your model unless you do not have a very good reason to belive that your intercept is 0 (e.g. because you centered your prediction target y and your predictors X ). Otherwise your model will be uninterpretable and unfit for prediction . You are correct that the intercept is not penalized by lambda in ridge regression by default. Therefore, at very large levels of lambda, all predictors will be close to null and predictions will essentially reflect the intercept, which will essentially be the mean of your training sample. At lower levels of lambda, the other parameters will be larger and eventually outweigh the influence the intercept on your predictions. Now, the strong negative correlation for intercept='T' is a result of your leave-one-out cross-validation (LOOCV): As described above, each left-out y will be predicted to be approx. at the mean of the current training sample. Large left-out y 's in your sample will always be larger than the training mean and therefore under-estimated. Vice-versa, low values of y will always be over-estimated, since they would drag the mean down â€” thus the negative correlation. Your cross-validation results basically represent distance of each data-point to the global mean. Just try varying the size of foldcount (e.g. foldcount = 5 ). The closer the foldcount gets to the sample-size the larger the negative correlation.
