[site]: crossvalidated
[post_id]: 466573
[parent_id]: 
[tags]: 
Variational autoencoders: Computational vs. analytical intractability of KL divergence

I am currently trying to understand the ideas behind variational autoencoders. Specifically, I am a trying to understand why the KL divergence between the approximate posterior $q(z | x)$ and true posterior $p(z | x)$ is merely computationally intractable rather than analytically intractable. In this blog post , the author states that the posterior is computationally intractable given that it relies on computing the following integral: $p(x) = \int p(x |z) \cdot p(z)\ dz,$ which according to him "requires exponential time to compute." The author of this post states something similar, while adding that, given enough time, one could estimate $p(x)$ by Monte Carlo sampling $p(x) \approx\frac{1}{m} \sum_m^M p(x | z^{(m)}).$ This I interpret as "If we had enough time, we could simply sample a huge number of $z$ 's and look at the corresponding $x$ 's, which would enable us to make a decent estimate of $p(x)$ ." What puzzles me is how one would even begin to compute the above integral or to perform Monte Carlo sampling. It seems to me that one would need to know $p(x | z)$ in advance to be able to do this, and that the issue therefore has nothing to do with "exponential time" but with an intractable analytical problem. Am I missing something?
