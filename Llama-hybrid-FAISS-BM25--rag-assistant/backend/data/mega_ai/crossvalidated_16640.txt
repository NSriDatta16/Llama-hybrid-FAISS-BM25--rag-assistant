[site]: crossvalidated
[post_id]: 16640
[parent_id]: 16635
[tags]: 
Many statistical algorithms take as input a set of vectors $x_1,\ldots,x_n\in\mathcal{R}^d$, and possibly some additional input such as a response variable corresponding to each input vector. We say such an algorithm can be "kernelized" if the algorithm can be rewritten to produce the same result when it's given only the Gram matrix of the vectors $x_1,\ldots,x_n$ as input, and not the vectors themselves. The Gram matrix, which is also known as the "kernel matrix", is defined as follows: $$K_{ij}=x_i^Tx_j.$$ That is, the ($i,j$)th entry in the matrix $K$ is the dot product of $x_i$ and $x_j$. Now, we can also change the effect of the algorithm by using a different inner product, such as $K_{ij}=x_i^T M x_j$ for some symmetric positive definite M. Furthermore, suppose we have some "inputs" that aren't easily representable as elements of $\mathcal{R}^d$, for some $d$. (An example of this might be a classification problem on time series, but the time series are all of different lengths.) We can still apply a kernelized algorithm to these inputs, as long as we can define an inner product on the inputs. This is what was meant by 'we never need to explicitly represent feature vectors.' The time series example is possible, but a bit exotic. More commonly, people still do represent their inputs as vectors in $\mathcal{R}^d$, but they replace the dot product with other inner products. These inner products are computed by a "kernel function". There are whole catalogs of kernel functions to choose from. A bunch are given on this blog posting . And one can play with all the different inner products one wants, without ever changing the algorithm itself --- the algorithm always runs on a kernel matrix, however it was computed.
