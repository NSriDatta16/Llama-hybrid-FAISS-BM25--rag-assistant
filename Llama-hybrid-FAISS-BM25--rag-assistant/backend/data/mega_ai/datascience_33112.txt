[site]: datascience
[post_id]: 33112
[parent_id]: 33110
[tags]: 
One important factor to take into account is how you use the numerical representation of words / embeddings from either TF-IDF or Word2Vec to then compute sentiments. Without knowing how you do this, it is difficult to give a concrete answer. Also, which task are you working on, what does a result of 90% mean? Regardless of how you compute TF-IDF (there are several definitions - shown below), it is essentially assigning a numerical value to a word, thus creating a mappng of sorts. Word2Vec technically created an embedding , as it maps individual words into a vector space. Images taken from here The final TF_IDF is simply the multiplication of the term frequency and the inverse document frequency. I won't go into details as to how the vectors in Word2Vec are computed, but they also define a way to assign a numerical vector (an embedding) to a single word. In essence, both of these are saying how important a word is, in the context of your documents (your corpus), with Word2Vec also having the interpretability of comparison between word vectors. For example, doing this with the associated vectors of the words actually works really well: King - Man + Woman = Queen Perhaps the sentiments you are computing, based on either of these methods , does something similar to taking an average over many words appearing in a sentence, and you end up with a normalised and similar results. TF-IDF takes a more intuitive approach, looking at how many times a word appears in general, in how many of the documents does it appear and how many times. Word2Vec instead looks at which words often appear together (there is a related quote that normally gets brought up here: "You can judge a person by the company they keep" ). So the intuition behind each on is slightly different, but you have numerical values for each word. Perhaps a closer comparison of TF-IDF would be to look at Doc2Vec. There is also the GLoVE embedding model, which scores well in many NLP tasks - on the same level as Word2Vec embeddings.
