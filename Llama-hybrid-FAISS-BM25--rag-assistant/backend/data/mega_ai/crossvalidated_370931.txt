[site]: crossvalidated
[post_id]: 370931
[parent_id]: 
[tags]: 
GLMM optimiser test - optimx.L-BFGS-B doesn't converge, but the rest do

I am running GLMM using lme4 in R for the first time. I have a complex model (with three main effects and four interactions), as well as a random intercept of participant (as my experiment used a repeated measures design). Even with this basic random effects structure, I have been having issues with convergence. The model works with the bobyqa optimiser, but I am concerned it may be changing the data and giving me unreliable estimates. As suggested by the authors of lme4 (on "?converge"), I have checked all possible optimisers. This is what it says re. convergence: try all available optimizers (e.g. several different implementations of BOBYQA and Nelder-Mead, L-BFGS-B from optim, nlminb, ...) via the allFit() function, see ‘5.’ in the examples. While this will of course be slow for large fits, we consider it the gold standard; if all optimizers converge to values that are practically equivalent, then we would consider the convergence warnings to be false positives. These are the results I am getting: bobyqa TRUE Nelder_Mead TRUE nlminbw TRUE optimx.L-BFGS-B FALSE nloptwrap.NLOPT_LN_NELDERMEAD TRUE nloptwrap.NLOPT_LN_BOBYQA TRUE So all but one of the models are fitting. The estimates from these are very similar to each other. See the example below. Is this okay or is this problematic? If this is okay, should I report the results with the bobyqa optimiser? If this is problematic, what analysis would you recommend? ANOVAs are sub-optimal as the data is binary (correct/incorrect), and logistic regression is sub-optimal as the design is repeated measures. Thank you for your help. Hopefully I've explained the problem in enough detail, but I'm happy to provide more information if necessary. Thanks.
