[site]: crossvalidated
[post_id]: 584408
[parent_id]: 
[tags]: 
Pix2Pix facede dataset, prevent "gray" in dataset to be predicted

I'm trying to build from scratch the pix2pix architecture, the one on this paper . As they did, I'm using the facade dataset, and this is one of their result: I'm particularly interested in the last one, where they have used cGAN+L1, specifically because the dataset is filled with images like that one, where on the borders they have some "black" padding for some reason, and in my test that is causing to have that black part also on the predictions: How do you think that they managed to avoid this? As of now, the Gen loss is the sum of the crossentropy and the reconstruction, and in their paper the have a $\lambda$ to scale down the rec loss, so maybe is that, or maybe it's something else that I'm missing, any idea?
