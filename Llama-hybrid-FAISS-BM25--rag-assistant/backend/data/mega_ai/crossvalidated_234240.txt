[site]: crossvalidated
[post_id]: 234240
[parent_id]: 
[tags]: 
Censored logit transform for (ad hoc) exploratory data analysis

In my work I commonly have to analyze binary composition data, expressed as a fraction $f\in[0,1]$. The data $f[x]$ is spatially distributed ($x\in\mathbb{R}^n$, $n=1,2,3$), and typically comes in the form of large gridded arrays. Due to the particularities of the data and application, visualization and analysis tasks are most often done in special-purpose software that has limited statistical capabilities. However several Gaussian-oriented statistics are always available, such as moments and bivariate (linear least squares and/or gaussian-process ) regression. Consequently, I tend to leverage these when doing exploratory data analysis. The context is commonly "live" consulting with clients, where an on-the-fly aproach is a must. Given these constraints, a techique I commonly use is what might be called a censored logit transform: $$z=\max(\alpha,\min(z_0,1-\alpha))$$ where $$z_0=\log\left(\frac{f}{1-f}\right)$$ and typically $\alpha\approx 5-6$. The idea is essentially to assume (for convenience) that $f$ has a logit-normal distribution , to enable use of the Gaussian-oriented toolkit available in the limited software environment. Question: I am curious if this technique is known in the larger statistical community, and/or what issues I should be aware of when applying it. I would also be interested in any suggestions for practical* alternatives. (*For extended analysis, I can of course use more general techniques, if the effort to transfer the data to a stronger analysis platform is justified. This question focuses on the initial exploratory phase, where the platform is constrained.) This question is not about any particular data set or analyses I am currently working on. However, I can provide more information about the types of data and analyses I use with this technique, if that would be helpful. Update: As noted in the comment, for compositional data $f=A/(A+B)$ it would generally be preferred to base analyses on the raw counts ($A$ and $B$) rather than the fractional composition $f$, which makes sense given my description above. However I neglected to mention an important aspect of my data $f$: The corresponding component-mass data ("counts") is commonly unavailable. In one scenario, the data $f=A/(A+B)$ is derived from a proxy measurement $\hat{A}=g[A]+\epsilon$, where $g$ is some (possibly nonlinear) indicator of $A$ abundance and $\epsilon$ is measurement noise. In this case there may be no a-priori parametric form for the indicator $g[A,\Theta]$, or good constraint on the noise distribution $p[\epsilon]$ (although usually it is assumed to be unbiased, $\langle\epsilon\rangle=0$). In the other common scenario, $f$ was acquired as legacy data. So while it almost certainly came from proxy measurements (for $A$ and/or $B$), this raw data is not available for analysis. I would appreciate any answers which can address either of these scenarios (i.e. partial counts $A$ available, and no counts, just $f$). I am also still interested in answers for the "full counts" case ($A$ and $B$ both available) which are practical for cases where the platform has no built-in support for things like Gamma/Beta distributions or Logistic regression, but only Gaussian-oriented statistics.
