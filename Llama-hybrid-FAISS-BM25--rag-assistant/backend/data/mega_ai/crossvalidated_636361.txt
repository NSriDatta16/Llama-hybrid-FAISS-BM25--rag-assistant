[site]: crossvalidated
[post_id]: 636361
[parent_id]: 636272
[tags]: 
Consider the so-called kernel shift functions $k(.,\,x)$ and $k(.,\,x_i)$ for $i=1$ to $N$ . These functions are defined on $\mathbb{R}^d$ . Let $Y(u)$ a centered Gaussian Process with index $u \in \mathbb{R}^d$ and with covariance kernel $k$ . let us assume for simplicity that $k(X,\,X)$ is invertible. The condition of the OP, say (1) , is equivalent to any of the two following (2) The conditional variance $\text{Var}[Y(x) \,\vert\, Y(X) ]$ is zero i.e. $Y(x)$ can be perfectly predicted from $Y(X) := [Y(x_i)]_i$ . (3) The kernel shift $k(.,\,x)$ is a linear combination of the $N$ kernel shifts $k(.,\,x_i)$ . The equivalence of (1) and (2) is easily deduced from the expression of the so-called Kriging variance. $$ \text{Var}[Y(x) \,\vert\, Y(X) ] = k(x, \, x) - k(x,\,X) \, k(X, \, X)^{-1} \, k(X, \, x). $$ The equivalence of (1) and (3) holds because the unknown coefficients of the linear combination in (3) come by solving a linear system with matrix $k(X, \,X)$ . These equivalent conditions obviously depend much on the chosen Kernel and can be further detailed for some specific choices of the kernel as in the OP. However when the kernel is positive definite, the conditions never hold when $x$ is not in the set $\{x_i\}_i$ because otherwise the square matrix $k(X^\star, \, X^\star)$ would fail to be positive definite where $X^\star$ is the matrix adding a new row $x$ to $X$ . If $N$ is large the condition holds approximately for all $x$ provided that $X$ becomes dense is some suitable meaning. To be more precise, consider the (semi-) distance $d_k(u,\,u')$ related to the kernel as defined by $$ d^2_k(u,\, u') := k(u,\,u) + k(u',\,u') -2 k(u, \, u'), \qquad u,\, u'\in \mathbb{R}^d. $$ Note that $d_k(u,\,u') = \| k(.,\,u) - k(.,\,u')\|_{\mathcal{K}}$ where $\| . \|_{\mathcal{K}}$ is the norm in the Reproducing Kernel Hilbert Space (RKHS) $\mathcal{K}$ of the kernel $k$ . Also $d_k(u,\,u') = \| Y(u) - Y(u')\|_{L^2}$ . The distance of $x$ to the design set $X$ is $d_K(x,\, X) := \min_{1 \leq i \leq N} d_k(x,\, x_i)$ . If this distance tends to zero for $N \to \infty$ then we have $\lim_{N \to \infty} \text{Var}[Y(x) \,\vert\, Y(X) ] = 0$ . Indeed the Kriging variance is $\| Y(x) - \mathbb{E}[Y(x) \vert Y(X) ]\|_{L^2}^2$ hence is smaller than $\| Y(x) - Y(x_i)\|_{L^2}^2$ for all $i$ .
