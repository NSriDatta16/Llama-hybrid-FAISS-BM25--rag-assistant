[site]: crossvalidated
[post_id]: 500364
[parent_id]: 500344
[tags]: 
For me, I think a helpful way to reason about this is to step back and think about polynomials. I'll make a little example with a cubic polynomial, since the truth in your picture appears to be cubic. Let the true data-generating process be $$ y=\theta x^3, \text{ where } \theta=1. $$ I'll add some $N(0,1)$ noise and plot this in the picture below for some $x$ values in $[-2,2]$ . Now let's pretend I fit my Bayesian model and compute $p(\theta|y)$ . Let's say I draw twice from this distribution and get $\theta^{(1)}=.75$ and $\theta^{(2)}=1.25$ . To experiment, let's see what the cubic curves look like with these coefficients. Below they're plotted in red and blue. If you squint and close one eye, both fit the data decently. But what if I zoom out to $[-4,4]?$ Here you can see the curves are really starting to behave differently, and this gets at the heart of why you can think about the tails getting wider for your scenario. Since we haven't observed data at points $ or $>2$ , the model doesn't "know" which parameter values are best. Additionally, the posterior predictive distribution in practice is computed by first sampling (using your notation) $\tilde\theta\sim p(\theta|X)$ and then sampling $p(y^*|x^*,\tilde\theta)$ . So if we repeat this process many many times, we're going to get wider intervals for the posterior predictive distribution just because lots of different $\theta$ values have been sampled. The cubic curve is very sensitive to the $\theta$ draws as we move away from the origin. But near the origin, many values of $\theta$ are roughly compatible with the observed data. That was all informal and heuristic, but hopefully it helps. cheers
