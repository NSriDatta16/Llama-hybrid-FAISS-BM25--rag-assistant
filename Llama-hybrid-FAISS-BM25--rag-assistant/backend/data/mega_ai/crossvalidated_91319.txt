[site]: crossvalidated
[post_id]: 91319
[parent_id]: 91290
[tags]: 
The approach you describe for using HMMs for classification is really only applicable to settings where you have independent sequences you want to classify. For example, if I was classifying the sentiment of sentences as positive or negative, I could build an HMM for each as you've described. See the related answer I gave here . Notice how this rests on the assumption I can break sequences up into the meaningful chunks to be classified before I compare posteriors. This doesn't seem to be the case for you problem since you effectively have one large length $T$ time series. Here's what I would try. You mentioned over at reddit that you were hesitant to assign a single state for each class. Have you tried this? It may not work as poorly as you think. The estimation problem is also significantly easier in this case. Estimating the transition probabilities is easy, you just count essentially. Furthermore you can just fit the emission probabilities for each state based on you're observed data and corresponding class, ignoring the temporal aspects. If you are convinced this is a bad idea, or find it performs poorly but still want to stick with generative models, you could use something like a Hierarchical HMM. For example, you could let the states in the top-level represent the classes and then allow the lower level HMMs to model the temporal variation within classes. You could also use one big HMM to achieve something similar. If you have $K$ classes, allocate $N$ states for each class (so $N \times K$ states altogether) of the form $s_{ki}$, $k = 1, \ldots, K$, $i=1,\ldots N$. During training you would need to force the HMM to only assign positive probability to transitioning to a state at time $t$ where $k$ matches the label at time $t$. I might have worded this a little awkwardly so I hope it is clear what I mean. Obviously you can generalize this to having different numbers of states per class. There are probably other types of Dynamic Bayesian Networks you could use as well. Kevin Murphy's thesis is an excellent reference. He also discusses converting HHMMs to HMMs. Lastly, you could switch to a discriminative model like a Conditional Random Field. A discriminative model will allow you to easily incorporate more complex features and more directly addresses the problem at hand (estimating conditional densities). This is probably what I'd try first.
