[site]: crossvalidated
[post_id]: 445535
[parent_id]: 445532
[tags]: 
A few thoughts: The intercept is the log-odds of a positive class membership when all categorical predictors are at their reference levels and continuous predictors are at values of 0. An intercept of -3 represents odds of 0.05 and a probability of 4.7% at those conditions. Predictor variables can have any reasonable values. The important trick is making sure that their values are linearly related to the log-odds of class membership. As a probability of 0 means infinitely negative log-odds, a simple logistic regression model will not be able to handle actual probabilities of 0 adequately. It's possible to have perfect separation in a logistic model, with some combination of predictor values exactly distinguishing cases with probabilities of 0 and 1. In that case penalization of the model coefficients is one way to proceed. Whether you are penalizing to deal with perfect separation or for other reasons, note that penalization seeks deliberately to trade off increased bias for decreased variance. So if you are using a penalized model you should expect some bias.
