[site]: crossvalidated
[post_id]: 486229
[parent_id]: 373850
[tags]: 
My question is what stops the network from learning the same weights or linear combination for each of these heads i.e. basically making the multiple head bit redundant. Can that happen? Not stopping or preventing it but the different attention heads is calculating attention for different subparts of the Query and Key vectors, standard setting with 512 dimensions is 8 heads doing 64 dimensions each. Two sets of different 64 dimensions being redundant over all tokens is highly unlikely, although proven by some papers some attention heads, if to many, doesn't contribute much to the overall result. The multi-head is more about parallelization of the processing than improving the result. I also wonder if we actually use the full input vector for each of the heads. Yes. Full input sequence, all 512 tokens, but different vector dimension group for each attention head.
