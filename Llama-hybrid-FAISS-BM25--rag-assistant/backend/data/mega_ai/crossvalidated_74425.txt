[site]: crossvalidated
[post_id]: 74425
[parent_id]: 74402
[tags]: 
The short answer, in three parts, is a) no, you can't do a paired test, as has been pointed out in comments, b) yes, you can do an unpaired test, and c) that 10% of non-respondents to the second test may be important. Let us consider a simplistic hierarchical model of response, where there is an individual-level characteristic $\theta_i$ which has some distribution $f(\theta)$ and a test-specific response $y_{ij}$ for tests $j \in {1,2}$ that depends upon the individual-level characteristic through its distribution $p_j(y_{ij} | \theta_i)$. If we know $i$ for each $y_{ij}$ we can obviously do the paired test, and the difference between $y_{i1}$ and $y_{i2}$ obviously are not influenced by the differences between the $\theta_i$, since it is for the same $i$. If, on the other hand, we don't know the individual $i$, we are faced with draws from two distributions $p^*_j(y_{ij}) = \int_\Theta p_j(y_{ij} | \theta_i) f(\theta_i)\text{d}\theta_i$. The scores $y_{ij}$ are still independent across $i$ and, under the null, independent across $j$ as well. The distribution itself no longer varies across $i$. Under alternative hypotheses, the distributions $p_j$ will still differ across $j$, it's just that they are population-level distributions rather than individual-level distributions. Consequently, we can still perform an (unpaired) test for differences between $j$, but it's going to be less powerful than if you could get rid of the extra variability introduced by not knowing the individuals. It's just a matter of what you can condition on; more conditioning reduces variability and thereby increases power. Personally, I'd use the unpaired version of the Wilcoxon, as you can't lose much relative to the unpaired version of the $t$-test and you might gain a lot. See this question for a little more information. Of greater concern is that missing 10% of the original sample. You'd really like to understand the missing data mechanism, if any. Consider the possibility that the 10 who dropped out were among the poorest performers on the original test, and that the amount of improvement was strongly negatively related to how well an individual performed on the first test (i.e., poor performers improved a lot more on average than good performers). That, combined with regression-to-the-mean effects, means you'd likely be missing data on some of your largest gains, thus a) weakening your ability to detect a significant difference, and b) biasing your estimate(s) of how much improvement there was downwards. OTOH, under the null hypotheses, we expect to see a gain, because we are including the low scorers in sample 1 but removing 10 likely low scorers from sample 2. So there's an upwards bias there too. Which effect dominates isn't clear, but what is clear is that your test and associated estimates would almost certainly be biased. For example, if I simulate from the simple model above assuming $\theta_i \sim \text{N}(0,1)$ and $y_{ij} \sim \text{N}(\theta_i,1)$, and drop the $y_{i2}$ for which $y_{i1}$ was in the 10 lowest values, the expected value of $y_{i2} \approx 0.136$ while that of $y_{i1} = 0$. 0.136 is about 0.96 standard deviations above 0, relative to the std. dev. of the difference between the means of $y_{i1}$ and $y_{i2}$, which would obviously have a huge impact on your type I and type II error probabilities. e2
