[site]: crossvalidated
[post_id]: 635748
[parent_id]: 134282
[tags]: 
[This is just to mention how matrix ${ U }$ in the SVD of a data matrix ${ X }$ (also determined by eigenvectors of ${ X X ^{T} }$ ) gives the principal directions in PCA.] Consider data ${ x _1, \ldots, x _n \in \mathbb{R} ^{d} }.$ Goal is to find, for every ${ k ,}$ an affine subspace ${ \mathscr{A} _k }$ of dimension ${ k }$ minimising the mean squared loss/error $${ \frac{1}{n} \sum _{i=1} ^{n} \text{dist}( x _i, \mathscr{A} _k) ^2 }.$$ Best 0 dimensional fit : We want a ${ p \in \mathbb{R} ^{d} }$ such that the loss ${ \frac{1}{n} \sum _{i=1} ^{n} \lVert x _i - p \rVert ^2 }$ is minimised. Note loss $${\begin{align*} \frac{1}{n} \sum _{i=1} ^{n} \lVert x _i - p \rVert ^2 &= \frac{1}{n} \sum _{i=1} ^{n} \lVert (x _i - q) - (p - q) \rVert ^2 \\ &= \frac{1}{n} \sum _{i=1} ^{n} \lVert x _i - q \rVert ^2 - \frac{1}{n} \sum _{i=1} ^{n} 2 (p-q) ^{T} (x _i - q) + \lVert p - q \rVert ^2 .\end{align*} }$$ Setting ${ q = \overline{x} = \frac{1}{n} \sum _{i=1} ^{n} x _i }$ gives the inequality $${ \begin{align*} \frac{1}{n} \sum _{i=1} ^{n} \lVert x _i - p \rVert ^2 &= \frac{1}{n} \sum _{i=1} ^{n} \lVert x _i - \overline{x} \rVert ^2 + \lVert p - \overline{x} \rVert ^2 \\ &\geq \frac{1}{n} \sum _{i=1} ^{n} \lVert x _i - \overline{x} \rVert ^2 , \end{align*} }$$ with equality holding when ${ p = \overline{x} }.$ Hence the loss ${ \frac{1}{n} \sum _{i=1} ^{n} \lVert x _i - p \rVert ^2 }$ is minimised when ${ p = \overline{x} = \frac{1}{n} \sum _{i=1} ^{n} x _i }.$ Best k dimensional fit : We want orthonormal vectors ${ [\hat{w} _1, \ldots, \hat{w} _k ] }$ in ${ \mathbb{R} ^d }$ so that ${ \overline{x} + \text{span}(\hat{w} _1, \ldots, \hat{w} _k) }$ is minimiser of the loss $${ \frac{1}{n} \sum _{i=1} ^{n} \text{dist}( x _i, \mathscr{A} _k) ^2 }$$ taken over all ${ k}$ dimensional affine subspaces ${ \mathscr{A} _{k} = \overline{x} + \text{span}(w _1, \ldots, w _k) }.$ Firstly, for any orthonormal vectors ${ W = [w _1, \ldots, w _k] }$ in ${ \mathbb{R} ^d }$ we have $${\begin{align*} & \text{dist}(x _i, \overline{x} + \text{span}(w _1, \ldots, w _k)) ^2 \\ &= \text{dist}(x _i - \overline{x}, \text{span}(w _1, \ldots, w _k)) ^2 \\ &= \lVert x _i - \overline{x} \rVert ^2 - \sum _{j=1} ^{k} ( w _j ^{T} (x _i - \overline{x}) ) ^2 \\ &= \lVert x _i - \overline{x} \rVert ^2 - \sum _{j=1} ^{k} w _j ^{T} \left( (x _i - \overline{x}) (x _i - \overline{x}) ^{T} \right) w _j \end{align*} }$$ and hence the loss $${ \begin{align*} &\frac{1}{n} \sum _{i=1} ^{n} \text{dist}(x _i, \overline{x} + \text{span}(w _1, \ldots, w _k)) ^2 \\ &= \frac{1}{n} \sum _{i=1} ^{n} \lVert x _i - \overline{x} \rVert ^2 - \sum _{j=1} ^{k} w _j ^{T} \left( {\color{red}{\frac{1}{n} \sum _{i=1} ^{n} (x _i - \overline{x}) (x _i - \overline{x}) ^{T}}} \right) w _j. \end{align*} }$$ Writing ${ X = [x _1 - \overline{x}, \ldots, x _n - \overline{x}] \in \mathbb{R} ^{d \times n} }$ we have the matrix $${ \frac{1}{n} \sum _{i=1} ^{n} (x _i - \overline{x}) (x _i - \overline{x}) ^{T} = \frac{1}{n} X X ^{T} }.$$ The problem of minimising the loss is reduced to the problem of finding orthonormal ${ [w _1, \ldots, w _k] \in \mathbb{R} ^{d \times k} }$ which maximise $${ \sum _{j=1} ^{k} w _j ^{T} \left( \frac{1}{n} X X ^{T} \right) w _j .}$$ Writing the SVD of ${ X }$ as ${ X = U \Sigma V ^{T} },$ we have $${ \begin{align*} \frac{1}{n} X X ^{T} &= \frac{1}{n} U \Sigma \Sigma ^T U ^{T} \\ &=\frac{1}{n} U \underbrace{\begin{pmatrix} \sigma _1 ^{2} & & & \\ &\ddots & & \\ & &\sigma _r ^2 & \\ & & & \end{pmatrix}} _{d \times d} U ^{T} \\ &= \frac{1}{n} U \Sigma _d ^2 U ^{T} . \end{align*} }$$ Now for any orthonormal ${ [w _1, \ldots, w _k] },$ we have $${ \begin{align*} &\sum _{j=1} ^{k} w _j ^{T} \left( \frac{1}{n} X X ^{T} \right) w _j \\ &= \frac{1}{n} \sum _{j=1} ^{k} w _j ^{T} U \Sigma _d ^2 U ^{T} w _j \\ &= \frac{1}{n} \sum _{j=1} ^{k} \tilde{w} _j ^{T} \Sigma _d ^2 \tilde{w} _j \end{align*} }$$ where orthonormal set ${ [\tilde{w} _1, \ldots, \tilde{w} _k] = U ^{T} [w _1, \ldots, w _k] }.$ For ${ k \leq r }$ we have $${ \frac{1}{n} \sum _{j=1} ^{k} \tilde{w} _j ^{T} \Sigma _d ^2 \tilde{w} _j \leq \frac{1}{n} (\sigma _1 ^2 + \ldots + \sigma _k ^2) },$$ with equality attained for eg when $${ [\tilde{w} _1, \ldots, \tilde{w} _k] = [e _1, \ldots, e _k] }$$ that is ${ U ^{T} [w _1, \ldots, w _k] = [ e _1, \ldots, e _k] }.$ So the required orthonormal set is $${ [w _1, \ldots, w _k] = [U _1, \ldots, U _k ] \text{ for } k \leq r }.$$ Finally, for ${ k \leq r },$ the set ${ \overline{x} + \text{span}(U _1, \ldots, U _k) }$ is the best fit ${ k }$ dimensional affine subspace for the data. The corresponding minimum loss is $${ \begin{align*} &\frac{1}{n} \sum _{i=1} ^{n} \text{dist}(x _i, \overline{x} + \text{span}(U _1, \ldots, U _k)) ^2 \\ &= \frac{1}{n} \sum _{i=1} ^{n} \lVert x _i - \overline{x} \rVert ^2 - \frac{1}{n} (\sigma _1 ^2 + \ldots + \sigma _k ^2 ) \\ &= \frac{1}{n} \text{tr}(X ^{T} X) - \frac{1}{n} ( \sigma _1 ^2 + \ldots + \sigma _k ^2) \\ &= \frac{1}{n} (\sigma _{k+1} ^2 + \ldots + \sigma _r ^2). \end{align*} }$$ One can also consider the best rank ${ k }$ approximation of ${ X = [x _1 - \overline{x}, \ldots, x _{n} - \overline{x}] \in \mathbb{R} ^{d \times n} }$ to get the same result.
