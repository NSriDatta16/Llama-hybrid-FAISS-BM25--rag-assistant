[site]: datascience
[post_id]: 123325
[parent_id]: 
[tags]: 
how do we adapt LLM token embeddings with custom vocab

Hi im just getting started with understanding transformer based models and I am not able to find how the token embeddings are arrived at?. there are multiple tokenization approaches and multiple vocabularies/documents llms are trained on. so my question is whether each llm also trains its own token embeddings? how do those pre trained embeddings work for transfer learning or fine tuning, on custom data sets where some OOV words may be present or we have some special unique tokens we want to keep whole and not have tokenizer do subword tokens?
