[site]: crossvalidated
[post_id]: 266008
[parent_id]: 265994
[tags]: 
It depends on the multiple imputation software/packages you used. If you for example used the 'mice' package in R, missing values for ordinal variables are set to be calculated with the polr function. This function tries to fit a proportional odds logistic regression, which is way more computer intensive when compared to the standard for scale variables (for which I assume you mean continuous/"numeric" variables), which is predictive mean matching (pmm in short, a really fast way of imputing a random value sampled from a small group of similar individuals from your data as the one with the missing value). I expect most imputation software to have similar functions in order to use the appropriate model for the appropriate type of data. That said, sometimes you can opt to handle a certain variable as if its distribution is less 'time-consuming'. For example, replacement values for some >2 level categorical variables can also be estimated by pmm. This will give you realistic values, as they are drawn from the observed data. Do note that this strategy makes the replacement values become more dependent on the combinations available in your data, which might unjustly lower the variability of the imputed values. Using a proportional odds or multinomial logistic regression is time consuming, but might result in a less data-affirming set of replacement values. Concluding, the trade-off between using fast algorithms to estimate missing values and the slower ones, is that in the end, the slower ones might (note the use of 'might' here) be better able to maintain the uncertainty inherent in replacing missing values with estimates.
