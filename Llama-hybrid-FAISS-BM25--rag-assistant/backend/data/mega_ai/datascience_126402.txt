[site]: datascience
[post_id]: 126402
[parent_id]: 126401
[tags]: 
I think you might approach this problem a little differently and optimise the process on the level of the optimizer, as well. Last year a paper was published with a new optimizer that automatically set all its hyperparams during the training. You might consider using it there to enhance the entire training procedure. Here you can find a paper: Bernstein, J., Mingard, C., Huang, K., Azizan, N., & Yue, Y. (2023). Automatic Gradient Descent: Deep Learning without Hyperparameters. arXiv preprint arXiv:2304.05187. And even a GitHub repo with PyTorch implementation. I'm aware that it might not fully answer your question, but I think you will find it helpful after all.
