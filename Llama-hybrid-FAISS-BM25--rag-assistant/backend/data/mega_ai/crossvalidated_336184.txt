[site]: crossvalidated
[post_id]: 336184
[parent_id]: 335859
[tags]: 
To complement the above answers, I would like to give my opinion purely from an intuitive standpoint. " No Free Lunch (NFL) " theorem states that no algorithm is better than another, on average across all possible data distributions. However, since real-world datasets are limited to few specific distributions, some models can be tuned well to show high performance in real-world scenarios. Gradient boosting provides an ensemble of weak learners . If we assume that the dataset is best modeled using a mixture of distributions , then each of these weak-learners could be learning only to perform well in certain distributions. Hence, when you take the average of all such weak learners, it has a very high overall performance across the whole dataset in contrast to a heavily overfitted single model which can perform well only in very few distributions or part of the dataset ( aka Wisdom of Crowd ). In short the single model attempt to generalize and lose the prediction power, whereas multiple models specialize and improve by the consensus. This is purely my opinion, either take it or leave it :)
