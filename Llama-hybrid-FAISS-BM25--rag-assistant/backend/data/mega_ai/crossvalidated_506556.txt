[site]: crossvalidated
[post_id]: 506556
[parent_id]: 477565
[tags]: 
First off, +1 to Robert's answer . In particular, he makes a great point that you should not bin your data into "negative"/"neutral"/"positive" categories, because that just needlessly loses too much information. For instance, if you bin $[-10,-4]$ as "negative", $[-3,3]$ as "neutral" and $[4,10]$ as "positive", then this binning treats someone with a score of $-10$ exactly the same as someone with a score of $-4$ - but presumably the first respondent was quite a bit more negative than the second one. So we have lost information. Don't do it. As Robert writes, with 21 ordered categories, you can likely enough use Ordinary Least Squares (OLS) rather than ordered logistic regression, because the difference between the two approaches will be very small for such a large number of categories. And in any case, this difference in models will be completely dominated by the fact that your measurements only imperfectly measure the underlying construct you are really interested in. I would not put too much emphasis on the normality of residuals, whether assessed graphically or by formal tests. This is nice to have, but regression parameters are quite robust to departures from normality. I would rather run diagnostic plots of residuals against your predictors: if there is a pattern in such plots, it suggests some unmodeled nonlinearities. (Note that such diagnostics will bias your p-values, so if these are what you are interested in, then don't overdo it.) If you are really interested in deciding between OLS and ordered logistic regression, and if you have enough data, then consider cross-validating both approaches and seeing which one yields lower out-of-bag mean squared errors. If they are within one standard error, then go with the simpler model, which here is the OLS one.
