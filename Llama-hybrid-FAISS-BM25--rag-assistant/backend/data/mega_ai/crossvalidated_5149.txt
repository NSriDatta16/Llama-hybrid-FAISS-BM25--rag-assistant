[site]: crossvalidated
[post_id]: 5149
[parent_id]: 
[tags]: 
Determining trend significance in a time series

I have some time series data and want to test for the existence of and estimate the parameters of a linear trend in a dependent variable w.r.t. time, i.e. time is my independent variable. The time points cannot be considered IID under the null of no trend. Specifically, the error terms for points sampled near each other in time are positively correlated. Error terms for samples obtained at sufficiently different times can be considered IID for all practical purposes. I do not have a well-specified model of how the error terms are correlated for points close to each other in time. All I know from domain knowledge is that they are positively correlated to some degree or another. Other than this issue, I believe the assumptions of ordinarly least squares linear regression (homoskedasticity, linearity, normally distributed error terms) are met. Modulo the correlated error term issue, OLS would solve my problem. I am a complete novice at dealing with time series data. Is there any "standard" way to proceed in these circumstances?
