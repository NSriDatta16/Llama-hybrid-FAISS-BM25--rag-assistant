[site]: crossvalidated
[post_id]: 680
[parent_id]: 645
[tags]: 
If you have large data sets - ones that make Excel or Notepad load slowly, then a database is a good way to go. Postgres is open-source and very well-made, and it's easy to connect with JMP, SPSS and other programs. You may want to sample in this case. You don't have to normalize the data in the database. Otherwise, CSV is sharing-friendly. Consider Apache Hive if you have 100M+ rows. In terms of analysis, here are some starting points: Describe one variable: Histogram Summary statistics (mean, range, standard deviation, min, max, etc) Are there outliers? (greater than 1.5x inter-quartile range) What sort of distribution does it follow? (normal, etc) Describe relationship between variables: Scatter Plot Correlation Outliers? check out Mahalanobis distance Mosaic plot for categorical Contingency table for categorical Predict a real number (like price): regression OLS regression or machine learning regression techniques when the technique used to predict is understandable by humans, this is called modeling. For example, a neural network can make predictions, but is generally not understandable. You can use regression to find Key Performance Indicators too. Predict class membership or probability of class membership (like passed/failed): classification logistic regression or machine learning techniques, such as SVM Put observations into "natural" groups: clustering Generally one finds "similar" observations by calculating the distance between them. Put attributes into "natural" groups: factoring And other matrix operations such as PCA, NMF Quantifying Risk = Standard Deviation, or proportion of times that "bad things" happen x how bad they are Likelihood of a successfully completed iteration given x number of story points = Logistic Regression Good luck!
