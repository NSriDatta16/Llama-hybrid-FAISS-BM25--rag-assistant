[site]: crossvalidated
[post_id]: 314228
[parent_id]: 
[tags]: 
One-class SVM: "training set vs. origin" logic

first of all, I apologize about asking this question again since a similar one was posted recently; I had to repost it since I still don't understand the answer and I had no other way to interact with those who answered. That said, I can't fully get how the "training set vs. origin" logic can work in One-Class SVM, even though I think I pretty much understood binary SVM. 1) I can see how it might work with linear separation, but I can't get how it would work using Kernel functions. Let's take this binary SVM animation as a reference. Imagine that we have the following training set in R^2 (training set -> red; origin -> blue). If we apply a Kernel function similar to the one used in the animation above, we would obtain something similar to the next figure (it is in R^3, the X axis would point towards you; sorry for the poor representation!) I guess that H3 would be the best hyperplane to choose, but I think that the one who maximizes the margin between the two classes would be H2 (which is still pretty good). My question is: what does prevent the algorithm to choose H1 as the decision boundary, since it would be very large (and therefore bad) while still having a decent margin between the classes? How would the $\nu$ parameter be "graphically" involved in this decision? 2) What would happend if the origin (0, ..., 0) was already INTO the training set? Thank you in advance!
