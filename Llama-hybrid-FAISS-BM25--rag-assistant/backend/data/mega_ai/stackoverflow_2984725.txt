[site]: stackoverflow
[post_id]: 2984725
[parent_id]: 2984459
[tags]: 
Also look at your training method. Multicores help you work FASTER. Working smarter is good too. If you are just using backpropagation you are going to take a long time to converge. At a minimum use something like resilient propagation, I think that might be in Neuroph. Or look at Scaled Conjugate Gradient or Levenberg Marquardt. Encog does both of these. Encog can also use your GPU to even further speed things using OpenCL. Speeding up iterations is good. But doing MORE with a training iteration is often even better. And doing BOTH is the best of all. How independent are your neural networks? Honestly, I am the main Encog programmer and I would love to see you switch. BUT, if you are under a time crunch and need to stay Neuroph and those nets ARE truly independent, then you might be able to spawn multiple threads and have several Neuroph training loops going at once. Over all of your cores. Assuming there is nothing in Neuroph that is going to goof up when there are several instance of their trainer going at once. I don't know Neuroph well enough to say how re-entrant it is. Also I agree, your research sounds really interesting.
