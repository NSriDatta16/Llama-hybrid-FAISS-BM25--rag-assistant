[site]: crossvalidated
[post_id]: 534449
[parent_id]: 
[tags]: 
Why is the posterior of a neural network gaussian process equal to the posterior of a neural network in the limit of infinite width layers?

Its noted in this paper, Deep Neural Networks as Gaussian Processes , that "a single-layer fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP), in the limit of infinite network width". I'm wondering, if that's the case, why so? And, how to prove it? And, under which conditions is this true for a particular model $f$ ? My thoughts so far have been, if I get the kernel of a neural network $f_\theta$ , I have $$k(x,y) = cov(f_\theta(x), f_\theta(y)) = E_\theta[f_\theta(x)f_\theta(y)^T] = \Phi(x)\Phi(y)^T$$ for some $\Phi$ . If I then did some learning process over the linear learning problem $w\Phi(x)^T + \lambda ||w||_2$ , and the equivalence claim above is true, then the posterior over $w$ in the linear problem would lead to the same model as the posterior over $\theta$ in $f_\theta$ . This seems somewhat surprising to me? My intuition says that for some $f_\theta$ s, some information in the original posterior would be lost when you take the covariance, so why does it hold for the covariance function of a neural network? Thanks in advance. If there are other resources I can read up on this topic to understand it better, would be curious to learn more too!
