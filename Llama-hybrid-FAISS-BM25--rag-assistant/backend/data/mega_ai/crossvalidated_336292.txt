[site]: crossvalidated
[post_id]: 336292
[parent_id]: 
[tags]: 
How do sparsity and cardinality of features influence relevance in a random forest classifier?

I am looking at relevance of my dataset features using the random forest implementation from Scikit learn. I get some results which are making sense, but some others don't. Some features have very sparse data. Only a few examples have 1 (about 1%), 99% have 0 as value. These features are extremely relevant in the classification but they are evaluated as unrelevant by the RF (close to 0% relevance). Some features have a very high cardinality (large amount of different values) and I have the same issue as sparse features, they are considered irrelevant. More generally, in what way sparsity and cardinality of features influence their relevance in a RF algorythm? how can it be addressed?
