[site]: stackoverflow
[post_id]: 2899622
[parent_id]: 2872184
[tags]: 
Are my performance concerns with the many columns approach valid? Honestly, I don't think performance is really the key issue here. If you have 2M rows with 100 columns and those columns never change then SQL Server / MySQL / etc. will actually do just fine. It might suck up a lot of unnecessary space, but the DB will probably perform adequately well. What the DB won't do is accept any changes very readily and I think that's the central concern. Trying to add a column into a central table with 2M rows is basically going to be a nightmare. You can "farm out" the separate fields to separate sub-tables, but that doesn't necessarily solve the problem, just delays it. EAV is also kind of a nightmare as you now take your 2M rows and convert them into 20M rows. Did I overlook something and there is a much better way to do it in an RDBMS? As long as you're willing to make the usual RDBMS sacrifices, then you'll probably find MongoDB to be much faster, easier and flexible for basic CRUD. Is MongoDb on Windows stable enough? I can't speak to that. The forums definitely have people running on Windows. But 2M records is honestly small potatoes for most people that are using it. Does my approach with different behaviour wrappers make sense? Are you suggesting the following? All of the product data lives in one collection . Each product is flagged with a type, so when accessed by the code an appropriate "wrapper class" is loaded. If so, then I think that's the way to go. That way you can implement business logic (books must have ISBNs) while still making storage really simple.
