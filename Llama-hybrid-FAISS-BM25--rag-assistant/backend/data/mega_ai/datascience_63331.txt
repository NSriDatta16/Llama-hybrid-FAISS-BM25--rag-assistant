[site]: datascience
[post_id]: 63331
[parent_id]: 63324
[tags]: 
from keras.preprocessing.text import Tokenizer from keras.preprocessing.sequence import pad_sequences from keras.models import Sequential from keras.layers import LSTM, Embedding, GlobalAveragePooling1D, Dense from keras import backend as K import numpy as np words = ['kennedy','smith','kitchen','caterpillar'] label = [1,1,0,0] tokens = Tokenizer(30, char_level=True) tokens.fit_on_texts(words) X = tokens.texts_to_sequences(words) X = pad_sequences(X) K.clear_session() model = Sequential() model.add(Embedding(30, 100, mask_zero=True, input_shape=(None,))) model.add(LSTM(100)) model.add(Dense(1, activation='sigmoid')) model.compile(loss='binary_crossentropy', optimizer='adam') model.summary() So this is a very simple implementation built with keras. So the model is very simple it is only to turn sequence of letters into tokens (e.g. transform A->1, B->3, etc. This is automatically handled by keras tokenizer), put embedding on top of these tokens to get your text as sequence of vectors and then feed the sequence of input with LSTM, and simply apply classification on top of the final LSTM output.
