[site]: datascience
[post_id]: 126949
[parent_id]: 118913
[tags]: 
If you understand decision trees and the intuition behind the decision tree then the random forest is simple. To understand the decision tree, we can intuitively consider what we want it to do. We want to find the best possible way for us to split up our data according to the input features. The way of deciding what is best is the loss function, entropy/gini or any other loss function you want. The model is built directly from the data itself, and the model is the data itself. A random forest is an extension of decision trees by creating a bunch of them with different data features. A major difference between the intuition behind a tree and a linear model is that a decision tree does not have "parameters". The tree model performs direct manipulations on the data itself. Whereas the linear models need to create and find the optimal parameters. Thus the need to perform back propagation to "learn" the parameters.
