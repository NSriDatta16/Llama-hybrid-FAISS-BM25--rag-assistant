[site]: datascience
[post_id]: 33203
[parent_id]: 33201
[tags]: 
Up to my knowledge, no rigorous studies exist to give you an answer. Also because the robustness of a model depends on it's ability to handle the overfit: a neural network without any regularization will be much less robust w.r.t one with regularization. And both will be less robust w.r.t. a Bayesian Neural Network. So the Bayesian one will require much less samples in the dataset in order to give reliable results w.r.t. the non regularized Neural Network. The robustness will also depend on the number of parameters on which your model is built on: a Neural Network with 50 Million of parameters need more data w.r.t. one with just one Million parameters. So the better way to choose how many features to keep is to check if you are able to build a reliable model or not: start training your model with all the features; if the model gives you a good performance keeping the overfit under control, than you are ok with that. Otherwise you start pruning features.
