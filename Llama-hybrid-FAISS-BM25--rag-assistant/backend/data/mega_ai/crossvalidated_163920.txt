[site]: crossvalidated
[post_id]: 163920
[parent_id]: 
[tags]: 
Relation between changing the prior and the effect of an additional data point

E. T. Janes writes the following in "Probability Theory: The Logic of Science": A useful rule of a thumb is that changing the prior probability $p(\alpha | I)$ for a parameter by one power of $\alpha$ has in general about the same affect on our final conclusions as does having one more data point. This is because the likelihood function generally has a relative width $1/\sqrt{n}$, and one more power of $\alpha$ merely adds an extra small slope in the neighbourhood of the maximum, thus shifting the maximum slightly. Generally, if we have effectively $n$ independent observations, then the fractional error in an estimate that was inevitable in any event is $1/\sqrt{n}$, approximately, while the fractional change in the estimate due to one more power of $\alpha$ in the prior is about $1/n$. I am struggling with understanding of this paragraph even though it seems quite important. It highlights that often the choice of a prior doesn't change the computations that much and hence de-emphasises debates about the appropriate choice of a prior. Yet I am struggling with the reasoning he outlines behind such proposition and I need some elaboration.
