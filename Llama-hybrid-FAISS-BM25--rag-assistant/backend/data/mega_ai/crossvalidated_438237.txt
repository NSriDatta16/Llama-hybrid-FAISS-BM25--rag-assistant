[site]: crossvalidated
[post_id]: 438237
[parent_id]: 438231
[tags]: 
Reinforcement learning endeavors to make self-teaching agents which can solve some problem. An example is Google's AlphaGo and AlphaGoZero agents which can teach themselves how to play Chess, Go and Shogi better than any human. The trick here is that in reinforcement learning, the goal is to maximize some reward . We don't tell the agent what the optimal solution is; in the case of chess and Go, it's usually too hard to say which play is the optimal play. So we don't even know how to maximize the reward; figuring out an approximation to the best play is left up to the agent. This is in contrast to supervised learning contexts where we know the right answer ahead of time, and the model has to learn to match the right answer . Transfer learning is usually framed as a special case of supervised learning. For example, we might have an image classifier trained on millions of examples for a specific set of classes, but we want to apply it to a different set of classes where data is much more scarce. It can be more effective to only partially retrain the model instead of retraining the whole thing (because the new data is too scarce to effectively train the image classifier). This is still a supervised learning problem, because we're giving labels to the model and telling it to match the labels, but it's different from the standard supervised learning problem because we're adapting the model to the new domain (the different set of classes). This hot question on AI.SE seems related; https://ai.stackexchange.com/questions/16741/how-can-an-ai-train-itself-if-no-one-is-telling-it-if-its-answer-is-correct-or-w
