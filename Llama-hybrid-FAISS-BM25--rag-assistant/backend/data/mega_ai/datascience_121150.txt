[site]: datascience
[post_id]: 121150
[parent_id]: 121147
[tags]: 
Do you have a single class and you're trying to predict whether or not the input is an instance of it? In this case, you're doing binary classification with logistic regression, though you only need 1 output: your model would predict the probability that the input belongs to the class, and will vary between 0 and 1. If the output of the model is >= 0.5, then the input is predicted to belong to the class, otherwise no. If you have two or more classes, then you need two or more outputs (i.e. your y[]s) and you would want to do softmax regression where your model predicts the probability of each class and then you take the predicted class with the highest probability. For visualizing, you want to do dimensionality reduction. There are several ways to do this and it's a large subject in itself, but probably the easiest way to start is to project your 14 dimensions down to 2 and plot them. Scikit-learn has a PCA class which makes this straightforward: pca = PCA(n_components=2) X_projected = pca.fit_transform(X_in) To check for overfitting or underfitting, you generally want to separate out some percentage of your data for testing purposes only, not used for training/fitting the model. If I understand correctly, you have already done this with 30% of your data? Then you check the ability of your model to predict the class of the test data it hasn't seen. Overfitting models will perform much better on training data vs. test data. hth.
