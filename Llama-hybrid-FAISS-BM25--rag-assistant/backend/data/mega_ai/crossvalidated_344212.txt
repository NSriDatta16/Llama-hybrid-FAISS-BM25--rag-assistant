[site]: crossvalidated
[post_id]: 344212
[parent_id]: 
[tags]: 
Second derivative test for machine learning algorithms

I have a question on second derivative test for most "modern" machine learning algorithms. I learned that in calculus but never seen it in real applications. Most machine learning algorithms optimizations seems to be trying to get the parameters that make derivative to $0$, without further tests to see if it is a max / min or saddle point. Why? Is that because many objective function is not twice differentiable? I know saddle point is a problem in neural network optimization, but even for simple case, say ridge regression / logistic regression, we also do not do the second derivative test, why? Is that because we know the objective function is convex?
