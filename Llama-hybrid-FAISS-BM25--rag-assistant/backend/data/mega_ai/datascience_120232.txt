[site]: datascience
[post_id]: 120232
[parent_id]: 120227
[tags]: 
It depends on the context imho: where does the list of words/phrases come from? Did some expert compile it thinking about every word carefully, or is it just intended as some rough indication of the actual context to be tagged? Anyway from your description the output must use these particular phrases as columns, so it wouldn't make sense to use embeddings like BERT since the output would not really match the phrases themselves. I don't see any point in a supervised model: what would be the target variable? All the phrases?? This would probably result in a very complex method for a very simple problem. It might be disappointing but sometimes the simple answer is the appropriate one: direct search for these phrases, possibly after lemmatization to cover lexicographic variants.
