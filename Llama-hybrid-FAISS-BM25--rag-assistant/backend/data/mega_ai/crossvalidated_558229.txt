[site]: crossvalidated
[post_id]: 558229
[parent_id]: 
[tags]: 
Is it possible in deep learning to train on a subset of training set in order to find the best hyper-parameters?

In classic machine learning, it is not uncommon to do a search for hyper-parameters by training different configurations on a small subset of training set. Usually, for each set of hyper-parameters, a k-fold cross validation is done over a small subset of training set. However, in deep learning, models are usually very hungry of data. So, my question is that do you think is it still possible to use the same strategy in deep learning? What is your experience?
