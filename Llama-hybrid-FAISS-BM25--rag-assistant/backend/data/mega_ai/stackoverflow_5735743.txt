[site]: stackoverflow
[post_id]: 5735743
[parent_id]: 
[tags]: 
Merge sorted files efficiently

I need to merge about 30 gzip-ed text files, each about 10-15GB compressed, each containing multi-line records, each sorted by the same key. The files reside on an NFS share, I have access to them from several nodes, and each node has its own /tmp filesystem. What would be the fastest way to go about it? Some possible solutions: A. Leave it all to sort -m . To do that, I need to pass every input file through awk / sed / grep to collapse each record into a line and extract a key that would be understood by sort . So I would get something like sort -m -k [...] B. Look into python's heapq.merge . C. Write my own C code to do this. I could merge the files in small batches, make an OMP thread for each input file, one for the output, and one actually doing the merging in RAM, etc. Options for all of the above: D. Merge a few files at a time, in a tournament. E. Use several nodes for this, copying intermediate results in between the nodes. What would you recommend? I don't have much experience about secondary storage efficiency, and as such, I find it hard to estimate how either of these would perform.
