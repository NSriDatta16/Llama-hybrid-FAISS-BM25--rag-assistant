[site]: crossvalidated
[post_id]: 501091
[parent_id]: 500900
[tags]: 
As you seem to recognise already, ordinal logistic regression is an extension of simple logistic regression, so the interpretation is quite similar. Ordinal logistic regression draws multiple logistic curves (or hypersurfaces in a multi-variate case as yours) through your data, one per each successive dichotomy, with all curves having identical shape, except in the intercept term . Let's simplify your problem to only one, continuous predictor variable, say, accessPNcont . Your output would look something like this (the numbers are purely fictional and serve only for illustration purposes): Call: polr(formula = overallPN ~ accessPNcont, data = pn[pn$provider == "A", ], Hess = T) Coefficients: Value Std. Error t value accessPNcont 0.9377 0.9111 -1.0292 Intercepts: Value Std. Error t value Dissatisfied|Neutral -1.4037 0.6436 2.1812 Neutral|Satisfied -3.6533 0.7400 4.9367 This can be graphically represented as in the figure below: You have two curves, identical in shape, only that the blue one is shifted to the right. The red one represents the probability of the overallPN being at least "Neutral", while the blue one represents the probability of the overallPN being at least "Satisfied" (which is in this case, the same as being exactly "Satisfied"). One way of saying it could be: A unit change in accessPNcont increases the odds of overallPN achieving the leap into a higher category by a factor of $e^{0.9377} = 2.5541$ . This odds ratio, $2.5541$ is constant for all categories. This is the essence of proportional odds . Now, for your real data, you have six ordinal predictor variables, which you encode as factors (something I wouldn't do). You can interpret all the variables in the same way. Since you use factors, you probably would't say "unit change" but use something like "category switch". For example: A change in response regarding access from "dissatisfied" to "neutral" decreases the odds of switching into a higher "overall" category by a factor of $e^{-0.9377} = 0.3915$ . This is very similar to what you say in your comment, only somewhat clearer for my taste. Now, let me offer you unsolicited advice: When you use factors in regression, R will automatically convert them into unrelated binary dummy variables. For ordinal variables this is not quite correct. You cannot be at the same time "neutral" and "satisfied". Being "satisfied" implies being already more than "neutral". Normally you'd expect the coefficient for "satisfied" to be larger than for "neutral", but in your model this is not always the case. See for example "fairly": Being "neutral" towards it (as opposed to "dissatisfied") increases the odds of "overall" by $\exp(1.0702)$ , but being "satisfied" (again, as opposed to "dissatisfied"), only by $\exp(0.5618)$ . This non-monotonous behaviour is likely to require an explanation and probably does not correctly reflect the real process in the population. To avoid such problems, you could either: use numerical, instead of ordinal predictors (say 0, 1, and 2 for "dissatisfied", "neutral", and "satisfied"), or manually encode each predictor in two binary dummy variables, with the meaning "at least neutral" (i.e. "neutral or satisfied") and "satisfied". A "dissatisfied" response would be encoded as $(0, 0)$ , a "neutral" as $(1, 0)$ , and a "satisfied" as $(1, 1)$ . The first approach has the advantage of keeping the number of predictors low, reducing the danger of overfitting and being more likely to find significant predictors. The disadvantage is that it implicitly assumes the same "step size" between "dissatisfied" and "neutral" as between "neutral" and "satisfied". For the second approach it's the other way round.
