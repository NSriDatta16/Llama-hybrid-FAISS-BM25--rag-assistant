[site]: crossvalidated
[post_id]: 534263
[parent_id]: 164048
[tags]: 
Despite the legitimate warnings that this approach might fail in some cases, this should not discourage you from trying it out! Breimann reports an example ( Breimann 2001 ) that selecting features by variable importance from a random forest and plugging them into logistic regresission outperformed variable selections specifically tailored for logistic regression, and others report similar observations, e.g., with using Boruta as a preprocessing variable selection step for logistic regression. As both Random Forest variable importance computation and Boruta feature selection are readily available in R or other software and thus can be tested without much effort, this is something that should be given a try. If you have enough data, you can even validate the approach by doing both steps on different fractions of the data.
