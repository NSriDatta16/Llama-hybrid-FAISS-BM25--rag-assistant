[site]: datascience
[post_id]: 120685
[parent_id]: 
[tags]: 
Interpretation of evaluation metrics for an imbalanced dataset

I am currently dealing with a classification problem for a massively imbalanced dataset. More specifically, it is a fraud detection dataset with around 290k rows of data, with distribution of 99.8% for class 0 (non-frauds) and 0.17% for class 1 (frauds). I have been using XGBoost, Random Forest and LightBGM as my predictive models. I have also tried running the models differently by tuning class weights and resampling the dataset to bring it on a balanced scale. Moreover, I used f1-score, ROC-AUC score and a Precision-Recall curve as my main metrics, as it seems that other metrics are not representative of the result on an imbalanced dataset. However, I seem to still be overfitting massively on my training data. In all scenarios, the f1-score, ROC-AUC score and the AP from Precision-Recall Curve of my training set are either 1.0 or 0.999, whereas those of the testing set are roughly around 0.85. I wanted to ask whether this is a normal occurrence for an imbalanced dataset, and if not, is there any other method for me to fix it. I would appreciate any response, and thank you all very much!
