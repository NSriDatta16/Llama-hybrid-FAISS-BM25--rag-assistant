[site]: datascience
[post_id]: 56835
[parent_id]: 56833
[tags]: 
After training your model, use xgb_feature_importances_ to see the impact the features had on the training. Note that there are 3 types of how importance is calculated for the features (weight is the default type) : weight : The number of times a feature is used to split the data across all trees. cover : The number of times a feature is used to split the data across all trees weighted by the number of training data points that go through those splits. gain : The average training loss reduction gained when using a feature for splitting. Here's an example : #Available importance_types = [‘weight’, ‘gain’, ‘cover’, ‘total_gain’, ‘total_cover’] f = 'gain' xgb.get_booster().get_score(importance_type= f)
