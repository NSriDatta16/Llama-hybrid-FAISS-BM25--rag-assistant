[site]: crossvalidated
[post_id]: 319370
[parent_id]: 319363
[tags]: 
I think there might be some confusion here... Cross-validation lets you choose from a family of models by estimating the prediction error for each one of them. One then takes the model whose estimated error is minimal. Say you have a family of models $f_\alpha$ indexed by a hyperparameter $\alpha$. Cross-validation provides a function $CV(\hat{f},\alpha)$, where $\hat{f}$ denotes the models trained with CV, for fixed $\alpha$. You then pick $\hat{\alpha} \in \operatorname{argmin} CV(\hat{f},\alpha)$ and retrain the model $f_\alpha$ with all the data. This gives you $\hat{f}_\alpha$ with some specific set of parameters. In logistic regression these parameters are your betas, and the hyperparameter $\alpha$ is the regularization coefficient for standard Tykhonov regularization, and it is typically $C$ or $\lambda$, ($C=1 / \lambda$). For a good explanation of model selection see Hastie, Tibshirani, Friedman "The Elements of statistical learning" , Chapter 7.
