[site]: crossvalidated
[post_id]: 159637
[parent_id]: 159589
[tags]: 
This question can be interpreted in a way that makes it interesting and not entirely trivial. Given something $X$ that looks like a random variable, to what extent is it possible to assign probabilities to its values (or shift existing probabilities around) in such a way that its variance equals some prespecified number $r$? The answer is that all possible values $r\ge 0$ are allowable, up to a limit determined by the range of $X$. The potential interest in such an analysis lies in the idea of changing a probability measure, while keeping a random variable fixed, in order to achieve a particular end. Although this application is simple, it displays some of the ideas underlying the Girsanov theorem , a result fundamental in mathematical finance. Let's restate this question in a rigorous, unambiguous fashion. Suppose $$X:(\Omega, \mathfrak{S}) \to \mathbb{R}$$ is a measurable function defined on a measure space $\Omega$ with sigma-algebra $\mathfrak{S}$. For a given real number $r \gt 0$, when is it possible to find a probability measure $\mathbb{P}$ on this space for which $\text{Var}(X) = r$? I believe the answer is that this is possible when $\sup(X) - \inf(X) \gt 2\sqrt{r}$. (Equality can hold if the supremum and infimum are both attained: that is, they actually are the maximum and minimum of $X$.) When either $\sup(X)=\infty$ or $\inf(X)=-\infty$, this condition imposes no limit on $r$, and then all non-negative values of the variance are possible. The proof is by construction. Let's begin with a simple version of it, to take care of the details and pin down the basic idea, and then move on to the actual construction. Let $x$ be in the image of $X$: this means there is an $\omega_x\in\Omega$ for which $X(\omega_x) = x$. Define the set function $\mathbb{P}:\mathfrak{S}\to [0,1]$ to be the indicator of $\omega_x$: that is, $\mathbb{P}(A) = 0$ if $\omega_x\notin A$ and $\mathbb{P}(A) = 1$ when $\omega_x\in A$. Since $\mathbb{P}(\Omega)=1$, obviously $\mathbb P$ satisfies the first two axioms of probability . It is necessary to show it satisfies the third; namely, that it is sigma-additive. But this is almost as obvious: whenever $\{E_i, i=1, 2, \ldots\}$ is a finite or countably infinite set of mutually exclusive events, then either none of them contain $\omega_x$--in which case $\mathbb{P}(E_i)=0$ for all $i$--or exactly one of them contains $\omega_x$, in which case $\mathbb{P}(E_j)=1$ for some particular $j$ and otherwise $\mathbb{P}(E_i)=0$ for all $i\ne j$. In either case $$\mathbb{P}\left(\cup_i E_i\right) = \sum_i \mathbb{P}(E_i)$$ because both sides are either both $0$ or both $1$. Since $\mathbb{P}$ concentrates all the probability on $\omega_x$, the distribution of $X$ is concentrated on $x$ and $X$ must have zero variance. Let $x_1 \le x_2$ be two values in the range of $X$; that is, $X(\omega_1) = x_1$ and $X(\omega_2) = x_2$. In a manner similar to the previous step, define a measure $\mathbb{P}$ to be a weighted average of the indicators of $\omega_1$ and $\omega_2$. Use non-negative weights $1-p$ and $p$ for $p$ to be determined. Just as before, we find that $\mathbb{P}$--being a convex combination of the indicator measures discussed in (1)--is a probability measure. The distribution of $X$ with respect to this measure is a Bernoulli$(p)$ distribution that has been scaled by $x_2-x_1$ and shifted by $-x_1$. Because the variance of a Bernoulli$(p)$ distribution is $p(1-p)$, the variance of $X$ must be $(x_2-x_1)^2p(1-p)$. An immediate consequence of (2) is that any $r$ for which there exist $x_1 \le x_2$ in the range of $X$ and $0 \le p \lt 1$ for which $$r = (x_2-x_1)^2p(1-p)$$ can be the variance of $X$. Since $0 \le p(1-p) \le 1/4$, this implies $$2\sqrt{r} = \sqrt{4 r} \le \sqrt{\frac{r}{p(1-p)}} = \sqrt{(x_2-x_1)^2} = x_2-x_1 \le \sup(X)-\inf(X),$$ with equality holding if and only if $X$ has a maximum and minimum. Conversely, if $r$ exceeds this bound of $(\sup(X)-\inf(X))^2/4$, then no solution is possible, since we already know that the variance of any bounded random variable cannot exceed one-quarter the square of its range.
