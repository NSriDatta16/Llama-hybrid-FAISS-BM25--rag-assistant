[site]: crossvalidated
[post_id]: 606216
[parent_id]: 606197
[tags]: 
It is actually the same cross-entropy as with current language models; only the notation is different. It is well illustrated in Figure 1 of the paper: Function $f$ already returns the $i$ -th index of the final softmax. So, calling $f(w_t, w_{t-1}, \ldots, w_{t-n+1};\theta)$ actually means: take the embeddings of words $w_{t-1}, \ldots, w_{t-n+1}$ (denoted as function $C$ ), apply the non-linear layer and the softmax layer and take the predicted probability corresponding to $w_t$ . The loss function can be therefore written as $\sum_t \log P (w_t|w_{t-1}, \ldots, w_{t-n+1})$ , which is the log-likelihood. Today's convention is to use negative log-likelihood.
