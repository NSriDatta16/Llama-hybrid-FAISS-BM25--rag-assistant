[site]: crossvalidated
[post_id]: 638717
[parent_id]: 
[tags]: 
What is a scoring rule for binary classification that is not dependent on the "difficulty" of classification?

Consider a model that predicts the probability of some binary event $Y$ (potentially given some features $X$ ). Denote the estimated probability of $Y$ occurring as $\hat{p}$ . One possible choice for a (proper) scoring rule to evaluate $\hat{p}$ is the logistic scoring rule: $$-\left(Y\log\left(\hat{p}\right) + (1 - Y)\log\left(1 - \hat{p}\right)\right)$$ The issue I have is that the value of the expected score when making an optimal decision is dependent on the true $P(Y) = p$ . Let's say $p = .9$ , then the optimal $\hat{p} = .9$ , which minimizes the expected score, which in this case is about .325. Now, let's say $p = .5$ . In this case, optimal $\hat{p} = .5$ , leading to an expected score of .693, more than double the optimal score in the previous case. I have a Bayesian regression model, which I first train on a few hundred data points. I then use the resulting posteriors to generate $\hat{p}$ s for the next few hundred data points (call this Dataset A). Then, I train the model on both the original training set AND Dataset A, and generate $\hat{p}$ s for another few hundred data points (call this Dataset B). All 3 datasets follow each other in time. The average logistic loss is much lower on Dataset A than on Dataset B. The na√Øve interpretation of this is that the model is somehow "better" at estimating probabilities on Dataset A than on Dataset B, and that somehow the model did not learn anything and/or there was such a distribution shift between A and B, that the model "became worse." Yet what I believe is happening is that a lot of the points in Dataset A were simply easier to predict than in Dataset B, i.e., the true $p$ s were more often closer to 0 or 1 in Dataset A, and more often closer to .5 in Dataset B. Thus the average logistic loss is higher in Dataset B, not because the model became a worse model, but because the problem became harder. How to reconcile this? I would like some metric that tells me how "good" my model is at estimating these probabilities, without being dependent on the underlying distribution of true probabilities. Does one exist?
