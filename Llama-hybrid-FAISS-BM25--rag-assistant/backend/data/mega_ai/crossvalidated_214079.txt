[site]: crossvalidated
[post_id]: 214079
[parent_id]: 213411
[tags]: 
As others have noted, this can be modeled linearly with an interaction. You're interacting two dummies, and there's nothing non-linear about this. Given the model: $$ wt = \alpha +b_1age+b_2gender+b_3age*gender+\epsilon $$ The 'gender' marginal effect is the partial derivative: $$\frac{\partial wt}{\partial gender} = b_2 + b_3age$$ See how if gender and age can only take values of 0 or 1, we're essentially only looking at a difference in means for four different groups? That is, we only have the four different combinations we can plug into the above equations: (1) $gender = 0$ and $age=0$, (2) $gender = 1$ and $age = 1$, (3) $gender = 0$ and $age = 1$, and (4) $gender = 1$ and $age = 0$. Thus, your specific example is equivalent to a comparison between four group means. It might also be helpful to see this discussion to understand how the above is equivalent to ANOVA with two interacted nominal variables. As another way to restate the fact that with your specific example, (again, because there are only four possible combinations of age and gender) we could also specify a model like the following, without an explicit interaction term: $$ wt = \alpha + b_1young.male + b_2old.male + b_3young.female + \epsilon $$ Where $old.female$ is omitted as your reference category, and for example, the coefficient $b_1$ will be a difference in means between $old.female$ and $young.male$. Where the intercept $\alpha$ will also be equal to average $wt$ within $old.female$ (again, the reference category). Try it out with your own data. With a linear model with an interaction, an ANOVA with an interaction, or using dummies for each of the groups with no interaction, you'll get the same results. Pretty cool, huh? A statistics book might discuss each of these methods in a diferent chapter$\dots$ but all roads lead to Rome. Really, seeing how this works with your own data is one of the best ways to learn. The above examples are thus an overly complicated way to get at this conclusion (that we're really just comparing four group means), but for learning about how interactions work, I think this is a helpful excercise. There are other very good posts on CV about interacting a continuous variable with a nominal variable, or interacting two continutous variables. Even though your question has been edited to specify non-parametric tests, I think it's helpful to think through your problem from a more conventional (i.e., parametric) approach, because most non-parametric approaches to hypothesis testing have the same logic but generally with fewer assumptions about specific distributions. But the question asked specifically for a non-parametric approach, which might be more appropriate, for example, if we didn't want to make certain assumptions about the normality of $wt$. An appropriate non-parametric test would be Dunn's test . This test is similar to the Wilcoxon-Mann-Whitney rank-sum test but with more than two categories. Other permutation tests might also be appropriate if you had a specific difference in means you were testing, for example, $old.men$ vs. $young.women$. Whether or not you use R, the 'coin' package documenation provides a good summary of different non-parametric tests, and under what circumstances these tests might be appropriate. Short aside on "significant" interactions Sometimes, you'll see statments like, "the interaction between $x_1$ and $x_2$ was statistically significant." Such statements are not necessarily wrong, but they are misleading. Usually, when an author writes this, they are saying that the coefficient on the interaction term was statistically signficant. But this is an unconditional effect in a conditional model. A more accurate report would say that "$x_1$ was statistically signficant over 'some values' of $x_2$," where all other covariates were held constant at some reasonable value, like a mean, median or mode. But once more, if we only have two covariates that can only take values of 0 or 1, that means that we're essentially looking at four group means. Worked Example Let's compare results from the interaction model with results from Dunn's test. First, let's generate some data where (a) men weigh more than women, (b) younger men weight less than older men, and (c) there is no difference between younger and older women. set.seed(405) old.men Estimate the interaction model and get predicted $wt$ from marginal effect (w/ 'effects' package). See here for why we don't want to interpret the unconditional effects in a model like this. Instead, we want to interpret marginal effects. The model does a decent job of detecting the differences we imposed when we generated our example data. mod Need to calculate a standard error or confidence interval for your marginal effect? The 'effects' package referenced above can do this for you, but better yet, Aiken and West (1991) give you the formulas, even for much more complicated interaction models. Their tables are conveniently printed here , along with very good commentary by Matt Golder. Now to implement Dunn's test. #install.packages("dunn.test") dunn.test(data$wt, data$cat, method="bh") Kruskal-Wallis chi-squared = 65.9549, df = 3, p-value = 0 Comparison of x by group (Benjamini-Hochberg) Col Mean-| Row Mean | old.men young.me young.wo ---------+--------------------------------- young.me | 3.662802 | 0.0002* | young.wo | 7.185657 3.522855 | 0.0000* 0.0003* | old.wome | 6.705346 3.042544 -0.480310 | 0.0000* 0.0014* 0.3155 The p-value on the Kruskal-Wallis chi-squared test result suggests that at least one of our groups 'comes from a different population.' For the group-by-group comparisons, the top number is Dunn's z-test statistic, and the bottom number is a p-value, which has been adjusted for multiple comparisons. As our example data were rather artificial, it's unsurprising that we have so many small p-values. But note the bottom-right comparison between younger and older women. The test correctly supports the null hypothesis that there is no difference between these two groups. So, both the interaction model and Dunn's test lead us to similar conclusions. In all of the examples given above, we are somehow comparing group means. And, while there are certainly more strait-forward approaches to comparing group means, I have tried to illustrate how comparing group means can also be understood as an interaction or "2D effect," with some model specifications, specifically with nominal interactions. I think that understanding this is helpful for understanding more complicated models with interaction effects. I'm going to link to this artice once more, just because I think it should be required reading for anyone working with interactions (there's a reason this article has been cited over 3k times$\dots$). UPDATE: Given other answers, this answer has been updated to dispute the idea that this requires any form of non-linear modeling, or that -- given OP's specific example of two binary covariates, i.e., four groups -- that there must be a sign change to asesses this non-parametrically. If age were continuous, for instance, there would be other ways to approach this problem, but that was not the example given by OP.
