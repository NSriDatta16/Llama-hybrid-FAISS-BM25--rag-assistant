[site]: crossvalidated
[post_id]: 571051
[parent_id]: 571030
[tags]: 
Most of the time in linear models, the intercept is not meant to be interpreted, instead it is to get rid of potentially biased estimates. If we omit the intercept then we force our regression line to past through the origin meaning our slope coefficients - which are the ones we actually care about - will likely biased. Further, if we don't include the intercept we can have potentially negative $R^2$ values and the equality $TSS=ESS+RSS$ may not hold. There are a few situations when forcing the line to go through the origin is reasonable (the CAPM model is a typical example), although these are pretty much all due to some underlying theory stating that it should pass through the origin. That being said, there are some occasions where the intercept does have a reasonable interpretation, but these are just accidental when the regressor could reasonably be 0. Additionally, I believe the reason for many textbooks not going into too much detail about the 'centred' form could be because a similar form can be recovered from partitioned regression using the Frisch-Waugh method. Hopefully, this is the sort of thing you were looking for when talking about centering the variables. Using matrix notation can make things easier to show, although a more scalar approach can be recovered quickly either way. Suppose we have a linear model $y=X\beta +\epsilon$ . If we define the column vector $\iota =(1,1,\dots , 1)'$ and a vector space $V=\text{span}(\iota)$ then the projection matrix onto the space orthogonal to $V$ is given by $N=I_n-\iota (\iota' \iota)^{-1} \iota'$ . The matrix $N$ has the property that it gives deviations from the mean of a given vector. For example, $$Ny=y-\iota (\iota'\iota)^{-1} \iota'y=y-\iota [n^{-1}\sum y]=y-\iota \bar{y}$$ Further, for any constant vector $c=(c,\dots,c)'$ it has the property of annihilating this vector, so $Nc=0$ . Lastly, since we assume $\mathbb{E}[\epsilon]=0$ , we have that $N\epsilon =\epsilon$ . Using this matrix, we can extract all variation in the model due to the constant to obtain the model $$Ny=NX\beta +N\epsilon=NX\beta+\epsilon$$ . The new model provides the OLS estimator $$b=(X'NX)^{-1}X'Ny$$ . To make it explicit, if $X$ is a matrix of a single regressor our model $Ny=NX\beta +\epsilon$ becomes $$y_i-\bar{y}=\beta_1 (x_i-\bar{x})+\epsilon$$ where the $\beta_1$ is equal to the one from the original model since $$b=(X'NX)^{-1}X'Ny=([NX]'NX)^{-1}[NX]'Ny=\frac{\sum (x_i-\bar{x})(y_i-\bar{y})}{\sum (x_i-\bar{x})^2}$$ . Now we see that the interpretation of $\beta_1$ can loosely be given as the deviation in $y$ from its mean due to a deviation in $x$ from its mean. Further, our t-test in this new model is exactly the same as in the original model since the variance of $y_i-\bar{y}$ is equal to that of $y_i$ (the error variance has not changed from $\sigma^2$ ). This method is likely why some textbooks don't go into detail (or even cover) variables which have been centered; the OLS estimates of the slope coefficients themselves already represent a measure of how deviations from the average of $x$ correlates with deviations of $y$ about its mean and the intercept is often best left not interpreted. I hope this is the sort of thing you were looking for.
