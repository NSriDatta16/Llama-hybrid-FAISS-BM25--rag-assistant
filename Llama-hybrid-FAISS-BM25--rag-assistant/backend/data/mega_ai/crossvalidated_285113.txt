[site]: crossvalidated
[post_id]: 285113
[parent_id]: 133656
[tags]: 
Logically speaking, the drawbacks of K-means are : needs linear separability of the clusters need to specify the number of clusters Algorithmics : Loyds procedure does not converge to the true global maximum even with a good initialization when there are many points or dimensions But K-means is better than we usually think. I've become quite enthusiastic about it after testing it against other clustering methods (spectral, density...) and LDA in real life text classification of one million texts : K-means had far better accuracy than LDA for example (88% vs 59%). Some other clustering methods were good, but K-means was close to the top... and more affordable in terms of complexity. I've never read about a clustering method that is universally better on a wide range of problems. Not saying K-means is universally better either, just that there is no universal clustering superhero as far as I know. Many articles, many methods, not a true revolution (in my personal limited experience of testing some of them). The main reason why the logical drawbacks of K-means are often only apparent is that clustering points in a 2D plane is something you rarely do in machine learning. Many things from geometric intuition that is true in 2D, 3D... are irrelevant in rather high dimension or abstract vector spaces (like bag of words, vector of variables...) Linear separability : You rarely have to deal with circular clusters in real life data. It's even better to assume they do not exist in these cases. Allowing your algorithm to search for them would allow it to find odd circular clusters in the noise. The linear assumption in K-means makes it often more robust. Number of clusters : There is often no true ideal number of clusters that you wish to see. For text classification for example, there may be 100 categories, 105, 110... it's all rather subjective. Specifying the number of clusters becomes equivalent to specifying a global granularity. All clustering methods need a granularity specification anyway. Global maximum : I think it's a true issue. The true abstract K-means that would consist in finding the global minimum for the S.O.D is fundamentally NP-Hard. Only Lloyd is affordable and it is... very imperfect. We have really seen that being close to the real minimum (thanks to replications) clearly improved the quality of the results. Replication of the K-means is an improvement but not a perfect solution. For a big dataset you would need $10^{\text{a lot}}$ replications to have a small chance to find the true minimum. Others methods like "finish it with greedy search" (proposed in Matlab) are astronomically costly in big datasets. But all clustering algorithms have such limitations. For example in Spectral clustering: you can't find the true eigenvectors, only approximations. For the same computation time, a quite optimized LDA library did less good than our home-made (not perfectly optimized) K-means. Since then, I think a bit differently.
