[site]: crossvalidated
[post_id]: 279689
[parent_id]: 
[tags]: 
Predicting a fair coin flip outcome with logistic regression

I am trying to better understand the results of logistic regression models and I wanted to apply a logistic regression model on a trivial "fair" coin flip simulation example. I want to simulate a number of coin tosses and fit a logistic regression model with whether we had at least one heads as the outcome and the number of coin tosses as the only predictor. In this scenario, I know that a logistic regression model is not very useful, but I am just trying to understand how to interpret things and apply the model to real-life examples better. Here is a simple explanation of what the simulation does: There are 10000 examples (rows) Each example has a random number of coin tosses (between 0 and 30, uniform distribution) Each coin toss has a 0.5 chance of getting a heads For each example, result = 1 if at least one toss resulted in a heads, result = 0 otherwise We end up with a 10000 by 2 dataframe with two columns: nb_toss and result My simulation uses the following code: import numpy as np import pandas as pd # Set random seed to always get the same results np.random.seed = 2 examples = [str(x) for x in range(10000)] results = [] nb_tosses = [] # For each of my 10000 example rows... for e in examples: # Assign a random (uniform distribution) number of tosses to each example, from 0 to 30 nb_toss = int(np.random.uniform(0,30, size=1)) nb_tosses.append(nb_toss) result = 0 for toss in range(nb_toss): # Each coin toss has a 0.5 chance of success (binomial random variable) # If at least one coin results in a success, assign 1 to result, otherwise assign 0 result = np.random.binomial(1, 0.5, 1)[0] if result == 1: break results.append(result) # Build a dataframe with the number of tosses and the result (1 or 0) for each row toss_df = pd.DataFrame({"nb_toss" : nb_tosses, "result" : results}) So as you can see the coin is completely fair (50% probability of heads). Now I fit my logistic regression model: from patsy import dmatrices import statsmodels.api as sm # Build my X (10000 rows, cols: intercept = 1, nb_toss) # and y (result) y, X = dmatrices("result ~ nb_toss", data=toss_df) # Fit a logistic regression model that predicts toss result given number of coin tosses logit_mod = sm.Logit(y, X) logit_res = logit_mod.fit(disp=0) # Print out the summary logit_res.summary() Logit Regression Results Dep. Variable: result No. Observations: 10000 Model: Logit Df Residuals: 9998 Method: MLE Df Model: 1 Date: Mon, 15 May 2017 Pseudo R-squ.: 0.6503 Time: 13:47:43 Log-Likelihood: -849.40 converged: True LL-Null: -2429.0 LLR p-value: 0.000 coef std err z P>|z| [95.0% Conf. Int.] Intercept -1.5767 0.102 -15.490 0.000 -1.776 -1.377 nb_toss 1.1099 0.046 24.204 0.000 1.020 1.200 I am particularly interested in the interpretation of the pseudo R-squared. Also, I find a predicted probability of 17% for 0 coin tosses, which is obviously impossible. Is there a way to adjust the model to take this into account? I am finally trying to understand the following: say I have a dataset where the result of the coin toss has an influence on another binary outcome y. If my example gets at least one heads, then the probability of y = 1 is 80%. Otherwise, the probability of y = 1 is 40%. If I am unaware of these true probabilities, how well can I fit a logistic regression model that estimates the effect of getting at least a heads or tails, or the effect of the number of coin tosses on this binary outcome?
