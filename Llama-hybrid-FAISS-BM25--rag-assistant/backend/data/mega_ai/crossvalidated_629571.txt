[site]: crossvalidated
[post_id]: 629571
[parent_id]: 
[tags]: 
neural network model doesn't learn if feature and target are swapped

I'm currently training a neural network using var1, var2, and var3 as input features, with var0 as the target variable. The initial training showed promising results, as the predicted_var0 closely matched var0, creating a nearly 1:1 plot. However, it is challenging when I attempted to use var1, var2, and var0 as features with var3 as the target variable, treating is as an inversion problem. In this scenario, the training loss decreased, which is a good sign. But the predicted_var3 is noisy, like a uniform distribution. I have taken several steps to address this issue, including data scaling, introducing bounds constraints on var3, fine-tuning the learning rate and adjusting the number of layers in the network, and experimenting with different optimizers. Unfortunately, none of these attempts led to an improvement in the results. AutoML gave similar outcomes. I am looking for an explanation for this problem and would appreciate any insights or guidance on how to resolve it.
