[site]: crossvalidated
[post_id]: 35485
[parent_id]: 28909
[tags]: 
Coming at this from a different angle: In PCA, you're approximating the covariance matrix by a $k$-rank approximation (that is, you only keep the top $k$ principal components). If you want to picture this, the covariance vectors are being projected orthogonally down into a lower dimensional linear subspace. Since you've only got 100 data points, the sample covariance necessarily lies on a subspace of dimension $\leq$ 100 (actually as ttnphns proves, 99). Of course, the point is to keep the large PCs and throw out the small ones to avoid fitting noise. You said 6 accounts for 96% of the variance, so that sounds good. Another technique would be to do cross validation and figure out how high $k$ gets before error on the hold-out data increases.
