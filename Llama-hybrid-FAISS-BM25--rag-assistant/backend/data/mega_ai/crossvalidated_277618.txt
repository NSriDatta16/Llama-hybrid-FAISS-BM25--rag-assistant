[site]: crossvalidated
[post_id]: 277618
[parent_id]: 
[tags]: 
Neural network failing because of Infinity

I have different activation functions in my network. I have noticed my networks failing (producing NaN ). The reasoning behind this is: I have a large layers with average weights at start, so some neurons get large values as input Softplus activation function outputs Infinity from ~800 Sinusoid/Softsign/Bent identity produces NaN as output to (-)Infinity How can I stop this from happening? Shoud I put limits on inputs (e.g. Max(10e15, input) . Also, the derivative of the complementary log-log activation function also returns Infinity rather quickly (even though the output of the function is limited), causing the backpropagation algorithm to fail. I solved this by returning 0 if x > 800. And last of all, I have nodes with the Absolute activation function. When these nodes are selfconnected, their activations will infinitely keep getting larger -> after a while they will output Infinity as well.
