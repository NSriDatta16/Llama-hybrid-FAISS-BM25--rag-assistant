[site]: crossvalidated
[post_id]: 212411
[parent_id]: 209013
[tags]: 
In the paper you read a total of 12 parameters can be trained in S1 layer meant the number of output planes in the pooling layer, not the number of parameters in the weight matrix. Normally, what we train within a neural network model are the parameters in the weight matrix. We don't train parameters in input planes or output planes. So students who wrote the paper didn't express themselves clearly, which made you confused about what a pooling layer really is. There are no trainable parameters in a max-pooling layer. In the forward pass , it pass maximum value within each rectangle to the next layer. In the backward pass , it propagate error in the next layer to the place where the max value is taken, because that's where the error comes from. For example, in forward pass , you have a image rectangle: 1 2 3 4 and you would get: 4 in the next layer. And in backward pass , you have error: -0.1 then you propagate the error back to where you get it: 0 0 0 -0.1 because the take the number 4 from that location in the forward pass.
