[site]: crossvalidated
[post_id]: 159193
[parent_id]: 
[tags]: 
Weight shrinking in linear regression by L2 regularization

Quoting Prof. Bengio from his Deep Learning text ( http://www.iro.umontreal.ca/~bengioy/dlbook/regularization.html ), $ w = (X^{T}X + \alpha I)^{-1}X^{T}y $ We can see L2 regularization causes the learning algorithm to “perceive” the input X as having higher variance, which makes it shrink the weights on features whose covariance with the output target is low compared to this added variance After spending an hour, I can't understand how to approach the proof of this. Can anybody help me get an intuition for this?
