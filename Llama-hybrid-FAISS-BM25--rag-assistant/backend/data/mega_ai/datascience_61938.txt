[site]: datascience
[post_id]: 61938
[parent_id]: 
[tags]: 
How to create a seq2seq without specifying a fixed decoder length?

Based on the model presented in this answer: def create_seq2seq(features_num,latent_dim,decoder_length): ## encoder_inputs = Input(shape=(None, features_num)) encoded = LSTM(latent_dim, return_state=False ,return_sequences=True)(encoder_inputs) encoded = LSTM(latent_dim, return_state=False ,return_sequences=True)(encoded) encoded = LSTM(latent_dim, return_state=False ,return_sequences=True)(encoded) encoded = LSTM(latent_dim, return_state=True)(encoded) encoder = Model (input=encoder_inputs, output=encoded) ## encoder_outputs, state_h, state_c = encoder(encoder_inputs) encoder_states = [state_h, state_c] decoder_inputs=Input(shape=(1, features_num)) decoder_lstm_1 = LSTM(latent_dim, return_sequences=True, return_state=True) decoder_lstm_2 = LSTM(latent_dim, return_sequences=True, return_state=True) decoder_lstm_3 = LSTM(latent_dim, return_sequences=True, return_state=True) decoder_lstm_4 = LSTM(latent_dim, return_sequences=True, return_state=True) decoder_dense = Dense(features_num) all_outputs = [] inputs = decoder_inputs states_1=encoder_states # Placeholder values: states_2=states_1; states_3=states_1; states_4=states_1 ### for _ in range(1): # Run the decoder on the first timestep outputs_1, state_h_1, state_c_1 = decoder_lstm_1(inputs, initial_state=states_1) outputs_2, state_h_2, state_c_2 = decoder_lstm_2(outputs_1) outputs_3, state_h_3, state_c_3 = decoder_lstm_3(outputs_2) outputs_4, state_h_4, state_c_4 = decoder_lstm_4(outputs_3) # Store the current prediction (we will concatenate all predictions later) outputs = decoder_dense(outputs_4) all_outputs.append(outputs) # Reinject the outputs as inputs for the next loop iteration # as well as update the states inputs = outputs states_1 = [state_h_1, state_c_1] states_2 = [state_h_2, state_c_2] states_3 = [state_h_3, state_c_3] states_4 = [state_h_4, state_c_4] for _ in range(decoder_length): # Run the decoder on each timestep outputs_1, state_h_1, state_c_1 = decoder_lstm_1(inputs, initial_state=states_1) outputs_2, state_h_2, state_c_2 = decoder_lstm_2(outputs_1, initial_state=states_2) outputs_3, state_h_3, state_c_3 = decoder_lstm_3(outputs_2, initial_state=states_3) outputs_4, state_h_4, state_c_4 = decoder_lstm_4(outputs_3, initial_state=states_4) # Store the current prediction (we will concatenate all predictions later) outputs = decoder_dense(outputs_4) all_outputs.append(outputs) # Reinject the outputs as inputs for the next loop iteration # as well as update the states inputs = outputs states_1 = [state_h_1, state_c_1] states_2 = [state_h_2, state_c_2] states_3 = [state_h_3, state_c_3] states_4 = [state_h_4, state_c_4] # Concatenate all predictions decoder_outputs = Lambda(lambda x: K.concatenate(x, axis=1))(all_outputs) model = Model([encoder_inputs, decoder_inputs], decoder_outputs) #model = load_model('pre_model.h5') print(model.summary() return (model) It is possible to create a Keras seq2seq model where the prediction of each decoding step is fed as input for the next step. But while the model defined by this code is unlimited in the length of its input sequence, it is limited to a pre-defined output sequence length. This may be fine for the toy problem demonstrating the use of this code, that is, just demonstrating the ability to predict the continuation of a given sine-like sequence, but a length agnostic solution would be preferable for an NLP problem I am trying to tackle. How can I define such a model in Keras? NB: This question was previously asked in Stack Overflow
