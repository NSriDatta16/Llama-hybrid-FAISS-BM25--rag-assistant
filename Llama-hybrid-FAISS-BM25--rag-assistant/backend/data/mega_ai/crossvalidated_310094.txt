[site]: crossvalidated
[post_id]: 310094
[parent_id]: 
[tags]: 
Is BIC useful for a multimodal distribution?

Given a dataset $D$ and a model $M$ with parameters $\theta$, the Bayesian Information Criterion can be used to approximate the model's marginal likelihood $\int p(D|\theta,M)p(\theta|M) d\,\theta$. The BIC has the form: $$BIC = 2\log p(D|\hat{\theta},M) + |M|\log n,$$ where $\hat{\theta}$ is the MLE for $\theta$, $n$ is the number of observations in $D$, and $|M|$ is the number of parameters $M$ estimates. Kevin Murphy's book Machine Learning: A Probabilistic Approach derives the BIC. His derivation requires the Laplace approximation for posterior densities, in which a posterior density is approximated by a multivariate Gaussian centered at the posterior mode. Question This seems to suggest that, if $p(D|\theta,M)p(\theta|M)$ is multimodal, the Laplace approximation will not be accurate, and BIC will not be good for approximating the marginal likelihood. So then BIC would not be useful for model selection. Is that the case? I am aware that some people (Andrew Gelman, notably) do not view BIC as a useful quantity at all. So my question could be rephrased as: If BIC is useful for approximating the marginal likelihoods of unimodal posteriors, then is it also useful for also approximating the marginal likelihoods of multimodal posteriors?
