[site]: crossvalidated
[post_id]: 229227
[parent_id]: 182944
[tags]: 
Checking with the standard reference on weights ( https://stats.idre.ucla.edu/other/mult-pkg/faq/what-types-of-weights-do-sas-stata-and-spss-support/ ), I think that you have a legit case for analytic weights based on the underlying sample size. That is to say, you can average the QA scores with weights proportional to the number of QAs by a given rater (if you believe in variances inversely proportional to the sample size, or precision which is the inverse of variance), or to the square root of that number (if you believe in standard deviations of the sample mean). Another approach could be to test the hypothesis that one rater is better than the other. Rather than forming a frequentist test for proportions, you may be better off with a Bayesian approach with a prior that the two raters have the same proportions, and evaluating the posteriors with the answers they provided. I will modify the numbers in your example, so that it works like a "hit" vs "no hit". So let's say the first guy did 200 QAs with 150 hits and 50 no hits at 75% rate, and the second guy, 5 QAs with 4 hits and 1 no hit. Denoting the probability of a hit by the $i$ -th rater as $\theta_i$ , let's start with a uniform prior: $$ p(\theta_1) = U[0,1], p(\theta_2) = U[0,1], \theta_1 \perp \theta_2 $$ For this model, $$ {\rm Pr\,} [\theta_1 which is the area of a triangle above the diagonal -- no big deal, we don't know anything about the raters. Let us now invoke conjugacy of binomial with the beta distribution (using the fact that the prior was $U[0,1] \equiv B(1,1)$ ). Thus with $k_i$ successes in $n_i$ trials, the posterior for each $\theta_i$ is $B(k_i+1, n_i-k_i+1)$ : $$ p(\theta_1|\mbox{QA data}) = B(161,41), \quad p(\theta_2 | \mbox{QA data}) = B(5,2) $$ Now, with the data at hand, we can evaluate $$ {\rm Pr\,} [\theta_1 $$ = \int_{0}^{1} \int_0^{\theta_2} \frac{x^{k_1}(1-x)^{n_1-k_1}}{B(k_1+1,n_1-k_1+1)} {\rm d}x \, {\rm d} p(\theta_2|\mbox{QA data}) = $$ $$ = \int_{0}^{1} I_{\theta_2}(k_1+1,n_1-k_1+1) \, {\rm d} p(\theta_2|\mbox{QA data}) $$ $$ = \int_{0}^{1} I_{x}(k_1+1,n_1-k_1+1) \frac{x^{k_2+1} (1-x)^{n_2-k_2+1}}{B(k_2+1,n_2-k_2+1)} \, {\rm d} x $$ where $I_x(a,b)$ is the incomplete Beta function . This would have to be integrated numerically: ## k1 So there is very, very weak evidence that rater 1 is better than rater 2, so I would reluctantly put the first guy over the second guy. If we go up to 8 successes out of 10 trials for rater 2, though, the conclusion is reversed, albeit the evidence is still weak: ## k1
