[site]: crossvalidated
[post_id]: 438496
[parent_id]: 
[tags]: 
Why is the number of filter used in higher layers more than lower layers in CNN?

Note that the number of filters grows as we climb up the CNN toward the output layer (it is initially 64, then 128, then 256): it makes sense for it to grow, since the number of low-level features is often fairly low (e.g., small circles, horizontal lines), but there are many different ways to combine them into higher-level features. / Géron, Aurélien. Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (Kindle Locations 10320-10322). O'Reilly Media. Kindle Edition. The above says about the filter count of CNN with regard to its position(Lower or Higher). As for the hidden layers, it used to be common to size them to form a pyramid, with fewer and fewer neurons at each layer — the rationale being that many low-level features can coalesce into far fewer high-level features. / Géron, Aurélien. Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (Kindle Locations 6928-6930). O'Reilly Media. Kindle Edition. And the above says about the neuron count of DNN with regard to is position(Lower or Higher). It seems like the above two statements are in disagreement. So what is correct? Lower-level features are more or higher-level feature are more?
