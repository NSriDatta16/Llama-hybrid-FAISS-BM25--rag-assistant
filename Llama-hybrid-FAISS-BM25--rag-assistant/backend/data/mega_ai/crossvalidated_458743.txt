[site]: crossvalidated
[post_id]: 458743
[parent_id]: 458727
[tags]: 
Agreed with Karolis' answer in "there are no hard boundaries". In addition, It's the same architecture of course. However, although we don't have a hard threshold on the number of layers for a neural network to be deep, in DL, we're more interested neural networks with large number of layers, instead of 1 or 2. Typically , yes. See the wikipedia page for example: Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning. But, this doesn't mean it'll be always. A lot of new architectures have been developing lately, e.g. Graph Neural Networks that learns over arbitrary graph structures. This is an extension of neural-nets to graphs, but also significantly differ from the fully connected neural nets we're accustomed to. But, not all of these new architectures have to fit in under ANN topic and we might need to extend the definition in the near future. Not sure what you've asked.
