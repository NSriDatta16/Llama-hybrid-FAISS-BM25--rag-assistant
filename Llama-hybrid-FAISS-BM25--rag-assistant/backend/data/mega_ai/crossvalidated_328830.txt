[site]: crossvalidated
[post_id]: 328830
[parent_id]: 
[tags]: 
Why do CNN's use ReLU over Sigmoid function?

I am trying to map my basic understanding of MLP's to CNN's. Why does a CNN sacrifice all negative inputs with the ReLU over the sigmoid. Is it because: The sigmoid has a range of between zero and 1 which is worse for CNN's than 0 to infinity. Is it to do with the fact the function only has one horizontal asymptote? The negative inputs are meaningless? Or to speed up computation? Implied here on this question. Any help is a appreciated.
