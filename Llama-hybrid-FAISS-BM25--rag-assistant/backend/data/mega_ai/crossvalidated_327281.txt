[site]: crossvalidated
[post_id]: 327281
[parent_id]: 326711
[tags]: 
The specific part that you quote is dealing with the scenario when your network is not trained to predict the value of $y$ (which is the most common use-case of neural networks these days), but it predicts the parameters of the distribution of $y$. Let's say that we know that $p(y|x)$ is distributed normally, but we don't know parameters of the Gaussian. However, we know they depend on $x$ (if $x$ is age, maybe $y$ has higher variance in some age groups). So instead of letting $y=f(x,\theta)$, we let $\omega=f(x, \theta)$ and $y\sim \mathcal{N}(\omega)$. As stated in the book, you can train such model using the maximum likelihood principle. The trained model gives you the parameters of the distribution that allow you evaluating $p(y)$. To answer your question: what exactly is the relationship between the probability distribution and the network or the network's parameters? What is the relationship between $\omega$ and the network or the network's parameters ($\theta$)? Network parameters $\theta$ parametrize the network function $f(x; \theta)$. Output of this function, $f(x;\theta)=\omega$, are the parameters of the distribution of $y$. In the example case I gave above, it is the mean and the variance of the normal distribution.
