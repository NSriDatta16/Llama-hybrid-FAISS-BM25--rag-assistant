[site]: crossvalidated
[post_id]: 460716
[parent_id]: 459770
[tags]: 
What is shown in the learning curves presented is not unprecedented within the context of boosting. It is widely observed that (especially in AdaBoost) the test error might decrease even after the training error is zero. See Schapire et al. (1998) Boosting the Margin: New Explanation for the Effectiveness of Voting Methods for details; the general idea is that maximizing the margin can improve the generalization error of a classifier even after the training error reaches zero. By " margin " we mean the distance between the sample point and the decision boundary learned by the classifier; we usually associate it with SVM ( Support Vector Machines ) but it is relevant for boosting too. Now, focusing again on the learning curve: over-fitting is split in two main situations: A. where both the training and test losses are decreasing, but the training loss is decreasing faster than the test loss and B. where the training loss is decreasing, but the test loss is increasing. The later (B) is clearly the problematic one for all classifiers. The former (A) is actually called optimism. Optimism is usually defined as the mean training error minus the mean validation error. Optimism of a model usually decreases with an increasing number of events per variable; van der Ploeg et al. (2014) Modern modelling techniques are data hungry: a simulation study for predicting dichotomous endpoints is an excellent and highly readable reference. Strictly speaking optimism is bad, but not the end of the world; it is more a problem with NN cause it suggests memorisation which in turn suggest issues with generalisation. That is because especially with a very large NN the capacity of it is sufficient for memorising the entire data set; Arpit et al. (2017) A Closer Look at Memorization in Deep Networks has more information on this. The same can happen with Gradient Boosting machines I suppose but I have not seen any references on the matter. To recap, I think this model is mostly OK and does not over-fit massively, it just does not get enough "bang for its data buck" after some point! It appears to get rather optimistic after about 1000 iterations so it is worth exploring how to regularise it a bit more; for example, the subsample is set to 1 so it means we always used the whole training set when growing trees, maybe something smaller (0.80?) is more appropriate. And a final note, the calibration plots looks good; no obvious S-shape and more or less looks monotonic. If not used already, using isotonic regression or a even a simple sigmoid on top of this classifier for some further probability calibration might help further both in terms of Brier score as well as with ranking measurements like AUC-ROC; it's no free lunch but it might help.
