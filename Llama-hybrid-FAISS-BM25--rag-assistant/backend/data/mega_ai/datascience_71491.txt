[site]: datascience
[post_id]: 71491
[parent_id]: 45028
[tags]: 
You are correct that the number of parameters depends on the number of features and it of the number of observations. You are seeing this error because you and the computer do not agree on what your input data mean. Sklearn assumes your data to be $n\times p$ , where each row represents an observation, and each column represents a variable. I think you’re doing the transpose if that, $p \times n$ , where each column represents an observation. As you have more and more observations, you have more and more columns, telling sklearn that there are additional features. If you transpose your predictor data, the functions should behave as expected. This isn’t unique to logistic regression. You would have this happen with any model. Say you trained a k-NN on 80 observations of 6 features. Then you test on 20 observations of those 6 features. Python should come back like, “You gave me 80 features for training and now only 20 for testing. What gives?”
