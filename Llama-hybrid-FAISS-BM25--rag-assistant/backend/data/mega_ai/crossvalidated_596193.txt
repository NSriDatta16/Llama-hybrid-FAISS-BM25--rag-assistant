[site]: crossvalidated
[post_id]: 596193
[parent_id]: 
[tags]: 
What is "one" in leave-one-out cross validation

Lets say I have $x_{ij} \sim Bernoulli(p_j)$ and $d_j = \sum_{i=1}^n x_{ij} \sim Binomial(n, p_j)$ I could do binomial logistic regression by regressing the logit of $p_j$ s on some predictors. If I was then interested in doing some cross validation like LOOCV, what is the difference in leaving out one $x_{ij}$ , computing the $d_j$ s, and fitting, vs leaving out one of the $d_j$ s? Maybe more concretely: Consider data library(tidyverse) set.seed(123) df % mutate(x = rbinom(10*12,1,prob=plogis(-2 + 0.3*p))) df_grouped % group_by(p) %>% summarize(d = sum(x), n=length(x)) I can then either do glm(x ~ p, data=df, family="binomial") or glm(cbind(d,n-d) ~ p, data=df_grouped, family="binomial") Now if I am interested in doing leave one out cross validation what is the difference in leaving out a row in df , fitting the model, and computing statistics (repeat for all rows) VS leaving out a row in df_grouped , fitting the model, and computing statistics?
