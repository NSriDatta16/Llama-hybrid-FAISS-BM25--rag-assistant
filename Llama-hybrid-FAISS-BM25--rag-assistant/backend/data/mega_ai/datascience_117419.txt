[site]: datascience
[post_id]: 117419
[parent_id]: 117404
[tags]: 
If the paragraphs containing plenty comma separated values are easy to detect, I wouldn't search for a very complex algorithm: After separating each paragraph from each chapter, look for the ones that have higher relative stats of commas, slashes, and/or return chariots. You can compare them easily to other paragraphs. Normally, topic paragraphs should have a much higher quantity of commas than other ones. Then, you can extract content between separators using the split function. Here is an example: s = "element1,element2,element3,element4" # Split the string on the ',' character elements = s.split(',') print(elements) If it is more complex than that, you can use doc2vec to classify according to targets [normal, topic], but you have to train with many samples, so that the model can differentiate normal and topic paragraphs correctly. doc2vec_embs = Doc2VecEmbeddings() x_train_tokens = doc2vec_embs.build_vocab(documents=x_train) doc2vec_embs.train(x_train_tokens) x_train_t = doc2vec_embs.encode(documents=x_train) x_test_t = doc2vec_embs.encode(documents=x_test) from sklearn.linear_model import LogisticRegression model = LogisticRegression(solver='newton-cg', max_iter=1000) model.fit(x_train_t, y_train) y_pred = model.predict(x_test_t) from sklearn.metrics import accuracy_score from sklearn.metrics import classification_report print('Accuracy:%.2f%%' % (accuracy_score(y_test, y_pred)*100)) print('Classification Report:') print(classification_report(y_test, y_pred)) Source
