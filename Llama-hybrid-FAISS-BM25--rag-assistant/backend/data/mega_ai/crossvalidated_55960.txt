[site]: crossvalidated
[post_id]: 55960
[parent_id]: 55887
[tags]: 
Whenever you deal with huge amounts of data and you want to solve a supervised learning task with a feed-forward neural network, solutions based on backpropagation are much more feasible. The reason for this is, that for a complex neural network, the number of free parameters is very high. One industry project I am currently working on involves a feed-forward neural network with about 1000 inputs, two hidden layers @ 384 neurons each and 60 outputs. This leads to 1000*384 + 384*384 + 384*60 = 554496 weight parameters which are to be optimized. Using a GA approach here would be terribly slow.
