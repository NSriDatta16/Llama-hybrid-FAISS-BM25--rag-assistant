[site]: datascience
[post_id]: 58737
[parent_id]: 
[tags]: 
Machine learning for missing data in time series

We have two time series columns - column A is the reference column ( source of truth) and column B is a ''cousin'' of column A, in the sense that it exhibits ( or should exhibit) the same patterns, evolution, rates of changes etc, as column A. However, at certain periods column B values for wtv reason start exhibiting abnormal values which we are comfortable to classify as outliers. We are happy to remove these value as outliers. Now given the unequal timeseries ( post outlier removal of column B) would like to explore machine learning techniques that would impute data in column B, using column A as the reference or source of truth, effectivly replicating the behavior of column A to impute missing data in column B. I've researched many models to use, regression, LTSM etc. hoping for some expertise that would help shed light on what technique would best work for the problem statement at hand.
