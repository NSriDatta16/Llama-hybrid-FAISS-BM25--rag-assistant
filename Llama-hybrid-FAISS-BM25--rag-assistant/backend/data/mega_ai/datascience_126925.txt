[site]: datascience
[post_id]: 126925
[parent_id]: 
[tags]: 
Intuition Behind Xavier Initialization

I am trying to understand the intuition behind why xavier works. So far I have pieced togther that $Var(Z)=n_{in}*Var(X)*Var(W)$ and so if we want the variance to mitigate the variance diminishing during the forward pass we want $$Var(Z)\approx Var(x)$$ then $$Var(X)=n_{in}*Var(X)*Var(W)$$ so then if I simplify down: $$1=n_{in}*Var(W)$$ and so then $$Var(W)=1/n_{in}$$ I understand that this roughly handles the variance from the forward portion of the neural network but where does the $1/n_{out}$ come from in the Xavier initialization formula what is a rough equation for it would be $$Var(Z)=n_{out}*Var(A)*Var(W)$$ and we want Var(Z) going backwards to maintin the variance of Var(A) as much as possible so $$Var(Z)\approx Var(A)$$ so then to simplify $$1=n_{out}*Var(W)$$ $$1/n_{out}=Var(W)$$ So Xavier Initialization comes out to be the $$sqrt(2/(n_{in}+n_{out}))$$
