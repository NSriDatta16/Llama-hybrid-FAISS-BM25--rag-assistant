[site]: stackoverflow
[post_id]: 2893342
[parent_id]: 2893241
[tags]: 
First of all, thanks for providing so much information about your network! Here are a few pointers that should give you a clearer picture. You need to normalize your inputs. If one node sees a mean value of 100,000 and another just 0.5, you won't see an equal impact from the two inputs. Which is why you'll need to normalize them. Only 5 hidden neurons for 10 input nodes? I remember reading somewhere that you need at least double the number of inputs; try 20+ hidden neurons. This will provide your neural network model the capability to develop a more complex model. However, too many neurons and your network will just memorize the training data set. Resilient backpropagation is fine. Just remember that there are other training algorithms out there like Levenberg-Marquardt. How many training sets do you have? Neural networks usually need a large dataset to be good at making useful predictions. Consider adding a momentum factor to your weight-training algorithm to speed things up if you haven't done so already. Online training tends to be better for making generalized predictions than batch training. The former updates weights after running every training set through the network, while the latter updates the network after passing every data set through. It's your call. Is your data discrete or continuous? Neural networks tend to do a better job with 0 s and 1 s than continuous functions. If it is the former, I'd recommend using the sigmoid activation function. A combination of tanh and linear activation functions for the hidden and output layers tend to do a good job with continuously-varying data. Do you need another hidden layer? It may help if your network is dealing with complex input-output surface mapping.
