[site]: datascience
[post_id]: 64658
[parent_id]: 
[tags]: 
R train(method="naive_bayes") and naiveBayes() very different performance

I am an R novice and having some difficulty. I was hoping R would be a good (flexible, easy) way to do machine learning of textual data. A few years ago, I wrote a naive Bayesian classifier (from scratch) in Perl and I'm trying to replicate it in R (in order to extend it using all the cool toys R enables). My Perl script uses LOOCV and achieves an overall hit rate of 69.4%. Using equivalent pre-processing steps in R with naiveBayes() from the e1071 package with an 80-20 split I get hit rate of 60.3%. I can imagine this discrepancy is due to different crossvalidation methodologies. Now, I am trying to find a way to use LOOCV to do an apples-to-apples comparison between my perl script and R. I'm using train() (from package "caret") with method="naive_bayes" (from a package called "naivebayes"). The code is below. No matter what I do, the results are horrible (chance-level, even worse than a no information model that exploits the baserates). Clearly either I'm doing something wrong or there is a bug; my prior p=0.999 that it's the former. I'm hoping one of you can give me a clue. My code is below. The other aspect of performance is speed. Perl is interpreted like R and runs the LOOCV on a sample of 655 in a few seconds (so perhaps 163 N-1 analyses/second). R runs a single naive bayesian analysis very quickly, but train() takes the better part of an hour to run. This code works well: classifier $class) pred class) This is an example of one way of using train that doesn't work well: train_control
