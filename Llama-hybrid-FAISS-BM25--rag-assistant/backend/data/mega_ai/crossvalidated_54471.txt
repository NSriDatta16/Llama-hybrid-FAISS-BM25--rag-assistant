[site]: crossvalidated
[post_id]: 54471
[parent_id]: 54464
[tags]: 
My recollection from grad school was an emphasis on always differencing I(1) processes. The main reason for differencing I(1) processes is that the standard errors are wrong (OLS assumes there is no serial correlation), which means t-statistics are wrong and statistical tests could give you the wrong answers. The parameter estimates are not wrong, it is only the standard errors. I'm more skeptical of the "always difference" approach now. That's not to say I would recommend regressing an I(1) process on another I(1) process. However, I would rather rely on a statistical technique to ensure the overall time series process is stationary. For instance, estimating a vector autoregression (VAR) with two I(1) variables can be done with the differenced variables or in levels. When they are both strongly I(1), strong in the sense that the coefficient of an AR(1) model in levels is close to 1 and highly significant, and there is no cointegration, then the residuals should be stationary and they will produce similar forecasts. However, it is possible that some of the variables exhibit some modest amount of mean-reversion or together are cointegrated, in which case a VAR in levels or error correction model (ECM) would produce better forecasts because the VAR in differences would not pick up the mean-reversion effect (univariately or jointly). In my experience, the ECM is more helpful for hypothesis testing, but that is not a focus for me. I might estimate the ECM, imposing some cointegrating vectors, and then convert it to a VAR for forecasting or creating impulse responses.
