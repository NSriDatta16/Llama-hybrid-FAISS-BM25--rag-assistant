[site]: crossvalidated
[post_id]: 63694
[parent_id]: 63658
[tags]: 
Creating new (composite) features as functions of existing terms can capture interactions between the variables not readily apparent which could otherwise be missed growing the trees. Also, since each tree in a random forest is built from a different sampled subset of the available variables, adding composite variables to your dataset effectively increases the probability that one of the variables used to build the composite variable will have an opportunity to contribute information to a tree in the forest. For example, if you have a dataset comprised of the variables W, X, Y, Z and you build a random forest classifying Z ~ W + X + Y, the random forest algorithm will build a few trees that leave out the Y variable, such as Z ~ W + X. If you built a composite variable P=X/Y and built your forest classifying Z ~ W + X + Y + P, then some trees that leave out Y will still include P, so Y's information still contributes to the model. This effectively adds some additional weight to the variables used to build the composite variables, since they are more likely to contribute to models.
