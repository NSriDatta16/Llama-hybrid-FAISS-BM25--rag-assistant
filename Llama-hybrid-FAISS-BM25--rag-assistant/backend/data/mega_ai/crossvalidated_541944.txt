[site]: crossvalidated
[post_id]: 541944
[parent_id]: 
[tags]: 
Bayesian posterior after dropping some information

Problem setup I have a data set $\mathcal D = \{(x_1, y_1), \dotsc, (x_n, y_n)\}$ and I would like to estimate parameter $\theta$ in the Bayesian way. I know that the Bayesian network for this model is $$Y \leftarrow \theta \to X$$ and the distribution $P(X\mid \theta)$ is known and easy to calculate. On the other hand, $P(Y\mid \theta)$ is intractable. Hence, it is impossible to model "the proper" Bayesian posterior $$P(\theta \mid \mathcal D) \propto P(\mathcal D \mid \theta ) P(\theta),$$ but it is easy to marginalize out variable $Y$ obtaining a posterior $$P(\theta\mid \mathcal D_X) \propto P(\mathcal D_X \mid \theta ) P(\theta),$$ where $\mathcal D_X = \{x_1, \dotsc, x_n\}$ . This corresponds to replacing the original Bayesian network with a simpler model $\theta \to X$ . The question I wonder if marginalizing out $Y$ is a proper thing to do in this case, as I am ignoring some known information. Is replacing the original (complete, but intractable) with a simpler one common in Bayesian statistics? Can I still call $P(\theta\mid \mathcal D_X)$ the posterior? I would be grateful for any opinions. (Or literature suggestions, so I can read more on this topic, if this question is trivial).
