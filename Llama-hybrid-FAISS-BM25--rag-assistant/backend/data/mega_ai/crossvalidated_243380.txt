[site]: crossvalidated
[post_id]: 243380
[parent_id]: 231208
[tags]: 
I'm not sure I understood the question correctly, but if you don't have state information available then nothing prevents you to use just Q(a) instead of Q(s, a), i.e. assume that Q(s,a) doesn't depend on s. Q-learning still works in this case. Alternatively, if you have initial conditions available, you can use it as a state, keeping s the same for all steps of a trajectory, but varying it between trajectories. Maybe it can become more clear that all of above works if you think about Q-Learning with function approximation. Q(s,a) is an estimate of the discounted future return given current state and action. You're essentially solving a regression problem: at each step features are $(s_t, a_t)$ pairs (converted to vectors), and target variable is $r_{t+1} + \gamma Q(s_{t+1}, a_{t+1})$. Not using $s_t$ or using fixed $s_t$ means that different features are used, nothing more; you're free to change features as you want.
