[site]: crossvalidated
[post_id]: 284799
[parent_id]: 
[tags]: 
Neural network and information redundancy

Suppose I have a set of data described by the variables A and B. The variable A by itself is not useful or not informative enough for a classification task. If I transform A, it describes the data better. Now if we suppose that I apply 2 different transformations to the variable A (e.g., derivative and squaring function), I obtain new representations A' and A'' that can be interpreted differently. On the other hand, if I transform A and multiply it by B, I get another type of representative data that adds interpretability to the model. Is it generally considered to apply these transformations/combinations to create some sort of "new" variables to train neural networks, or does it just adds information redundancy that we should avoid? This question is from a non-expert.
