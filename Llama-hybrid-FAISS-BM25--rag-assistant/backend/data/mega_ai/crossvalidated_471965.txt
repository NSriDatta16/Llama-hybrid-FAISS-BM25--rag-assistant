[site]: crossvalidated
[post_id]: 471965
[parent_id]: 471962
[tags]: 
In a linear model, that is what $R^2$ does. The trouble is that it’s hard to say if $R^2$ is low or high, too! $R^2=0.30$ might be stellar for one study, while $R^2=0.90$ might be pitiful for another. The reason that I say this is for a linear model is because of how the total sum of squares decomposes into the sum of squares of the regression and the sum of squares of the residuals (related to MSE). With nonlinear models, there is a third term that the traditional $R^2$ equation does not consider. In linear models, that third term is always $0$ . I posted something about this last year. I’ll be back with a link. EDIT Look at the math in this post of mine where I decompose the total sum of squares for both linear and nonlinear models: Neural Net Regression SSE Loss . (That I’m discussing neural networks in particular is not so important.)
