[site]: crossvalidated
[post_id]: 401825
[parent_id]: 401751
[tags]: 
R/GLM and statsmodels.GLM have different ways of handling "perfect separation" (which is what is happening when fitted probabilities are 0 or 1). In Statsmodels, a fitted probability of 0 or 1 creates Inf values on the logit scale, which propagates through all the other calculations, generally giving NaN values for everything. There are many ways of dealing with perfect separation. One option is to manually drop variables until the situation resolves. There are also some automated approaches. Statsmodels has elastic net penalized logistic regression (using fit_regularized instead of fit ). But this will give you point estimates without standard errors. The statsmodels master has conditional logistic regression. I don't think Statsmodels has Firth's method. Edited: reading your question again, it looks like the optimization converged but the Hessian was not invertible. This is a related but less severe version of what I described above. But the same advice still applies.
