[site]: crossvalidated
[post_id]: 358338
[parent_id]: 358311
[tags]: 
Let me answer the question in a meme way. Why (always) deep learning? All neural nets do is linear regression (x*w+b) with some non-linearity around the (intermediate) response. Lets talk machine learning, better yet, optimization. The obvious general class of problems you are referring to is function approximation (not regression per se). So, why not to use method that was developed to do exactly this: writing programs . In theory, yes, you can use 'artificial intelligence' methods to create programs and one of them, given enough data and time, can theoretically be FizzBuzz. Or a programm that computes prime numbers (and that program could be theoretically the same when written by a human). -- No deep learning here --. Learning from data Well, can we learn from data? Yes, we can. But first we need to understand data and to engineer some features . Because only one numeric feature is not expressive enough... (for now). Some code incoming: library(tidyverse) theData 0) & (a5 > 0) ~ 'FizzBuzz', a3 > 0 ~ 'Fizz', a5 > 0 ~ 'Buzz', TRUE ~ 'Number')) %>% mutate(cl = factor(cl)) Now we have a numerical feature a (numbers) and a3 and a5 te help with the decision ... ... tree . (╯°□°)╯︵ ┻━┻ again not deep learning here. But a stacked model: first level is DT and scond level is (using Viola-Jones-Cascades or simple filter on the Number response) a plain old linear regression with the solution $y=a$. The DT first: treeModel THAT IS CRAZY ! A simple decision tree learned FizzBuzz! But did it? Apply some test data: testData 0) & (a5 > 0) ~ 'FizzBuzz', a3 > 0 ~ 'Fizz', a5 > 0 ~ 'Buzz', TRUE ~ 'Number')) predictions Perfect on test set for numbers 200 to 300! Well, the second layer is easy: lmModel The error estimating the number is testData %>% mutate(pred = predict(treeModel, ., type = 'class')) %>% filter(pred == 'Number') %>% mutate(apred = predict(lmModel, .), error = a - apred) %>% pull(error) %>% summary() Min. 1st Qu. Median Mean 3rd Qu. Max. 5.684e-14 5.684e-14 5.684e-14 5.684e-14 5.684e-14 5.684e-1 ... well very close to 0. Tada! We learned FizzBuzz from data! Derp learning Probably you can do the same stuff with deep learning. You can also do stacked models with LSTM layers and convolution (ya know, because of modulo 3 and 5), and with a huge amount of data you may have a chance to generalize some patterns... yeah... no. So hope this answer helps to clarify that yes it is possible. And no, you don't need deep learning to do the job. And now, from a single feature a even deep learning will not be able to learn FizzBuzz. As for prime numbers... if you compute/engineer as many features as there are prime numbers, you can learn them from data, too. ¯_(ツ)_/¯
