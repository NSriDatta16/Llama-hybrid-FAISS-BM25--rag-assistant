[site]: crossvalidated
[post_id]: 118651
[parent_id]: 
[tags]: 
Definition of Parameters

I imagine this either extremely simple or extremely complex. I am trying to understand the interpretation of the term 'parameter'. A couple of quick online searches deliver an intuitive understanding of parameter, i.e. where a statistical model is a joint probability distribution over the variables of interest, parametric models denote a subset of such models (in the form of a set of probability distributions), and parameters are therefore unique finite-dimensional points that index a particular distribution. All fine. I can see why means, variances, covariances, correlations, and any other such moments are parameters. What I can't see is why numerous other features, such as regression coefficients, etc. (i.e. the Betas we try to estimate with a linear model) are called parameters. Is is simply because things like regression coefficients are functions of parameters in the first sense? And when we estimate beta coefficients, we are really trying to estimate the ratio between parameters, or are the beta coefficients parameters in themselves? (I assume it must be the latter, the OLS estimator, for example, is just a function of the sample that estimates a number called Beta; coverage of the OLS seems to imply that the OLS ratio of cov to var just happens to be a great estimator of that number, rather than of the cov to var ratio in the population...)
