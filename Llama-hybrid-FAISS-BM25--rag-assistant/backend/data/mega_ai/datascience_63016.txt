[site]: datascience
[post_id]: 63016
[parent_id]: 61639
[tags]: 
K-nearest_neighbors wiki As the size of training data set approaches infinity, the one nearest neighbour classifier guarantees an error rate of no worse than twice the Bayes error rate (the minimum achievable error rate given the distribution of the data). I don't like that they didn't give the error rate for both in the KNN they gave the average error rate. Which makes me think what if the accuracy of the training data was 95% accurate for the training data but the test accuracy was so bad it ended up being 18% error on average for both. Maybe they want you to think 30-20=10 10*2=20 20>18
