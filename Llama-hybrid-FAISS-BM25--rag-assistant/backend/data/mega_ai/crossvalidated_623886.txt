[site]: crossvalidated
[post_id]: 623886
[parent_id]: 
[tags]: 
Measuring perplexity over a limited domain in an LLM

Are there papers/a literature on measuring perplexity in using a Large Language Model such as ChatGPT/Flan over a limited domain? I want to prompt an LLM to do movie recommendations/next job prediction and such, and these models are often compared using loglikelihood/perplexity. There is a lot of work doing recommendations and measuring AUROC or ranking metrics like NDCG, but nothing reports loglikelihood over test set or preplexity. The challenge here would be to measure perplexity over a limited domain and over multiple tokens (not simply next token).
