[site]: crossvalidated
[post_id]: 147762
[parent_id]: 147758
[tags]: 
Random Forests are stochastic by nature, so it is normal that the results slightly change from run to run. However, the differences should be very small, especially for measures like Variable Importance (either Gini or mean decrease in accuracy) that are stabilized by being averaged over all trees. First I was thinking that it had something to do with the randomized number of variables used in each tree, so I increased ntree The ntree parameter does not control the size of the subset of variables that are randomly tried at each split. The mtry parameter does. I suspect that there is an issue with your ntree and mtry parameters. If you try only a few variables at each split, and your ntree is not high enough, some variables won't be given a chance to play a role in each tree. This could lead to some instability in the variable importance measures. This is especially true if you have a low mtry , and a lot of important predictors. But there might be other reasons. You should perform a grid search to tune ntree and mtry based on the OOB error rate first. Or, based on a decent number of trees (to ensure convergence), e.g., 500, you can use the tuneRF function to optimize mtry . Then, once you have your best model, you can use its output (e.g., variable importance measures).
