[site]: crossvalidated
[post_id]: 425136
[parent_id]: 
[tags]: 
How to perform feature normalization for training regression CNNs with datasets with different distributions?

I'm teaching a 3D convolutional neural network to learn different functions that map a 3D scalar field into another one. It is essentially a regression problem. The distribution of input datasets does not change and essentially will get closer to a normal distribution. On the other hand, the outputs eventually may vary significantly in terms of distribution and range of possible min and max values. I'm training different networks to learn different output sets. Which brings some questions: What would be the best approach for feature normalization in this case? Should I consider one type of normalization for inputs and a different one for outputs? Make sense to use any approach to fit the output scalar fields to a normal distribution? How do I achieve this? Please consider the following histograms. Blue represents a common distribution for inputs and green two possible outputs, trained separately at different networks. Min-max normalization: Standard-scale: When using min-max normalization, I'm getting values in a [0,1] range, but poorly distributed. Using a standard scale, min and max values are at unknown ranges but better distributed.
