[site]: crossvalidated
[post_id]: 139677
[parent_id]: 
[tags]: 
Could you use Randomized Optimization in order to find weights for linear regression?

Let's say you are doing linear regression. We are trying to fit $w^Tx=y$. One way to do that is by utilizing gradient descent to minimize this function: $J(w) = \frac1{2n} \sum_{i=1}^{n} (w^Tx-y)^2$. (Context: slight variation from Andrew Ng's Coursera Machine Learning course). Now, my question is, can you say $J(w)$ is our fitness function, and then apply, for instance, simulated annealing (SA), randomized hill climbing (RHC), a genetic algorithm (GA), or PBIL to it? (easy enough to take negatives or switch minima to maxima, depending on the implementation) Does it matter that the values I want for $w$ are going to be real-valued, or will that cause these optimization methods to not work well? Edit/update for clarification: I know gradient descent will (unless I have a special example indeed!) give me real numbers (any number of digits after the decimal) rather than integer solutions. My question was whether with randomized optimization, would your precision be limited? Based on the answer given by jolvi, some versions of GA/PBIL could limit said answers (exactly my concern). I'm concluding that it doesn't matter for SA or RHC. Additionally: I did want to ask if Mutual Information Maximizing Input Clustering (MIMIC) also caused limited precision, but I wasn't sure if this was a well-known algorithm. I found Wikipedia pages for everything else but this algorithm.
