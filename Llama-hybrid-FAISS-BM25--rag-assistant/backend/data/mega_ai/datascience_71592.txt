[site]: datascience
[post_id]: 71592
[parent_id]: 
[tags]: 
Effect of embedding dimension in performance of embedding algorithms

Consider a simple Matrix Factorization or any embedding algorithm. It is obvious that a very small embedding dimension does not contain enough information so the algorithm will have low performance and enlarging the dimension will improve the performance. However, as I tried various algorithms, enlarging the dimension more than a threshold decreases the performance of the algorithm. Why does this happen? I know using larger embedding while you can get the same result with smaller ones does not make sense, however, I am just curious to find out more about that.
