[site]: crossvalidated
[post_id]: 570091
[parent_id]: 569504
[tags]: 
Christopher Bishop called his classic book Pattern Recognition and Machine Learning because what machine learning algorithms do is find patterns in the data and identify those patterns at prediction time. It proved to work very well as an automated solution for solving many different practical problems. However, if you think about it, why if you collected a bunch of samples, and trained an algorithm on them, does it allow you to generalize the results to the samples you didn't see? To do this, you need to assume that your training data is a sample that comes from a broader population , and shares common characteristics with the population, so you can extrapolate your results to the rest of the population. Probability theory and statistics provide formal theoretical frameworks for doing exactly this. As stated in the other answers, not every model predicts probabilities or expected values, not every loss directly translates to a likelihood function, but on theoretical grounds, we think of all those models using probability theory language because it allows us to connect the samples with the population. Most statistical models were designed in probabilistic terms from the beginning. For other algorithms that were not, on theoretical grounds we will still use probabilistic language to reason about them.
