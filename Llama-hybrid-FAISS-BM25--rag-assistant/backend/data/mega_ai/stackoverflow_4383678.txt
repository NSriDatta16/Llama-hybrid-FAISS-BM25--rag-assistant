[site]: stackoverflow
[post_id]: 4383678
[parent_id]: 4382703
[tags]: 
No. As you mention, rendering to a texture is the way to achieve that functionality. If you take a look at a block diagram of a GPU pipeline, you'll see that the blending stage - which is what combines fragment shader output with the framebuffer - is separate from the fragment shader and is fixed-function. I'm not a GPU designer - so I can only speculate the reason for this. Presumably it is to keep framebuffer access fast and insulate the fragment shader stage from the frame buffer so that it can be better parallelised. There are probably also issues regarding multi-sampling, and so on. (Not to mention that fixed-function blending is "good enough" in most cases.)
