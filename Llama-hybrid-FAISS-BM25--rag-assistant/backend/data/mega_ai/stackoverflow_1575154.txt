[site]: stackoverflow
[post_id]: 1575154
[parent_id]: 1538235
[tags]: 
I made a little critters that lived in this little world. They had a neural network brain which received some inputs from the world and the output was a vector for movement among other actions. Their brains were the "genes". The program started with a random population of critters with random brains. The inputs and output neurons were static but what was in between was not. The environment contained food and dangers. Food increased energy and when you have enough energy, you can mate. The dangers would reduce energy and if energy was 0, they died. Eventually the creatures evolved to move around the world and find food and avoid the dangers. I then decided to do a little experiment. I gave the creature brains an output neuron called "mouth" and an input neuron called "ear". Started over and was surprised to find that they evolved to maximize the space and each respective creature would stay in its respective part (food was placed randomly). They learned to cooperate with each other and not get in each others way. There were always the exceptions. Then i tried something interesting. I dead creatures would become food. Try to guess what happened! Two types of creatures evolved, ones that attacked like in swarms, and ones that were high avoidance. So what is the lesson here? Communication means cooperation. As soon as you introduce an element where hurting another means you gain something, then cooperation is destroyed. I wonder how this reflects on the system of free markets and capitalism. I mean, if businesses can hurt their competition and get away with it , then its clear they will do everything in their power to hurt the competition. Edit: I wrote it in C++ using no frameworks. Wrote my own neural net and GA code. Eric, thank you for saying it is plausible. People usually don't believe in the powers of GA (although the limitations are obvious) until they played with it. GA is simple but not simplistic. For the doubters, neural nets have been proven to be able to simulate any function if they have more than one layer. GA is a pretty simple way to navigate a solution space finding local and potentially global minimum. Combine GA with neural nets and you have a pretty good way to find functions that find approximate solutions for generic problems. Because we are using neural nets, then we are optimizing the function for some inputs, not some inputs to a function as others are using GA Here is the demo code for the survival example: http://www.mempko.com/darcs/neural/demos/eaters/ Build instructions: Install darcs, libboost, liballegro, gcc, cmake, make darcs clone --lazy http://www.mempko.com/darcs/neural/ cd neural cmake . make cd demos/eaters ./eaters
