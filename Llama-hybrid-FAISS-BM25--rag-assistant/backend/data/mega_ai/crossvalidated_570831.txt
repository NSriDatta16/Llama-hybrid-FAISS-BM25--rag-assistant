[site]: crossvalidated
[post_id]: 570831
[parent_id]: 570817
[tags]: 
If you consult the source code of those two implementations, you will see that PCA is used for two different things in the R and in the sklearn implementation. R Here, the idea is to take the original data $D$ and reduce its dimension with PCA before tSNE will be applied. The default dimension is initial_dims = 50 . This is some preprocessing of your original data, which is done because, for data of dimensions much higher than 50, tSNE could struggle. So the advantage is reduced memory and improved speed, the disadvantage is that you might lose lots of vital information when replacing your original data $D$ with the PCA projection $D_{PCA}$ . sklearn In sklearn, the original data $D$ will not be projected to some PCA affine subspace before tSNE is applied. Rather, PCA is used for something else: Let's say we want a tSNE projection in two dimensions. Then tSNE works by iteratively creating a sequence $(D_2^{(n)})_{n=1}^\infty$ of 2D images which should converge to an image $D^{(\infty)}_2$ that minimizes some tSNE loss function $L(D, D_2^{(n)})$ . Slightly abusing notations, we have: $$ D_2^{(\infty)} := argmin_{D_2^{(n)}} \{L(D, D_2^{(n)})\}. $$ The tSNE loss function is cleverly designed such that the final tSNE images look as nice as they do. Now, this iteration $(D_2^{(n)})_{n=1}^\infty$ of 2D datasets must be initialized, and this is often done by simply creating a random initialization. But with sklearn, you have the option of initializing to the 2D PCA projection of $D$ . And that is how PCA is used in sklearn, and now it is also clear that there is no extra configuration parameter for the PCA dimension, because, if you use PCA with sklearn, the PCA dimension will always be the dimension of your tSNE image, given by n_components . The advantage is that this might be a better initialization than just random and will improve the convergence speed and result of tSNE. The disadvantage is that you cannot use as large and high-dimensional data compared to when working with a projected original dataset as Rtsne does. Note: Note, that both implementations, R and sklearn, use this iterative process of minimizing $L(D, D_2^{(n)})$ , but in Rtsne the first argument $D_{PCA}$ is the PCA projection to, by default, 50 dimensions, while in sklearn, the first argument is always the original dataset $D$ : Rtsne: loss function: $L(D_{PCA}, D_2^{(n)})$ sklearn: loss function: $L(D, D_2^{(n)})$ .
