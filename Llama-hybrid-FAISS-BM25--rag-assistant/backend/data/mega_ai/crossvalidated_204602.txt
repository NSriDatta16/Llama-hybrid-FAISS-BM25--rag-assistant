[site]: crossvalidated
[post_id]: 204602
[parent_id]: 204530
[tags]: 
In the paper, we actually demonstrate the precision fallacy in multiple ways. The one you're asking about --- the first in the paper --- The example is meant to demonstrate that a simplistic "CI = precision" is wrong. This is not to say that any competent frequentist, Bayesian, or likelihoodist would be confused by this. Here's another way to see what's going on: If we were just told the CIs, we would still not be able to combine the information in the samples together; we would need to know $N$ , and from that we could decompose the CIs into the $\bar{x}$ and $s^2$ , and thus combine the two samples properly. The reason we have to do this is that the information in the CI is marginal over the nuisance parameter. We must take into account that both samples contain information about the same nuisance parameter. This involves computing both $s^2$ values, combining them to get an overall estimate of $\sigma^2$ , then computing a new CI. As for other demonstrations of the precision fallacy, see the multiple CIs in the Welch (1939) section (the submarine), one of which includes the "trivial" CI mentioned by @dsaxton above. In this example, the optimal CI does not track the width of the likelihood, and there are several other examples of CIs that do not either. The fact that CIs --- even "good" CIs can be empty, "falsely" indicating infinite precision The answer to the conundrum is that "precision", at least in the way CI advocates think about it (a post-experimental assessment of how "close" an estimate is to an parameter) is simply not a characteristic that confidence intervals have in general, and they were not meant to. Particular confidence procedures might ... or not. See also the discussion here: http://andrewgelman.com/2011/08/25/why_it_doesnt_m/#comment-61591
