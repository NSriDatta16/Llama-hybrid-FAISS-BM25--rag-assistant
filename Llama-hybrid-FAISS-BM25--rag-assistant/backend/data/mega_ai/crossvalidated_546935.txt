[site]: crossvalidated
[post_id]: 546935
[parent_id]: 546912
[tags]: 
Your data is too small, so that some of the glm models trained on different folds fail to converge. With random synthetic data, I obtained the following results for $5$ -fold CV, we need to define our summaryFunction and use returnResamp = 'all' in the trainControl , in order to obtain the confusion matrices for the models on the folds: cfm $pred, data$ obs)) print(cm) cm$byClass } specifications It will print the confusion matrix for each fold #for the 1st fold... #Confusion Matrix and Statistics # # # no yes # no 2 3 # yes 3 2 # # Accuracy : 0.4 # 95% CI : (0.1216, 0.7376) # No Information Rate : 0.5 # P-Value [Acc > NIR] : 0.8281 # # Kappa : -0.2 # # Mcnemar's Test P-Value : 1.0000 # # Sensitivity : 0.4 # Specificity : 0.4 # Pos Pred Value : 0.4 # Neg Pred Value : 0.4 # Prevalence : 0.5 # Detection Rate : 0.2 # Detection Prevalence : 0.5 # Balanced Accuracy : 0.4 # # 'Positive' Class : no # #for the 2nd fold... #Confusion Matrix and Statistics # ... The metrics can be obtained for each fold in the following way: model1$resample # Sensitivity Specificity Pos Pred Value Neg Pred Value Precision Recall F1 Prevalence Detection Rate Detection Prevalence #1 0.6274510 0.3469388 0.5000000 0.4722222 0.5000000 0.6274510 0.5565217 0.51 0.32 0.64 #2 0.5490196 0.4081633 0.4912281 0.4651163 0.4912281 0.5490196 0.5185185 0.51 0.28 0.57 #3 0.5686275 0.4285714 0.5087719 0.4883721 0.5087719 0.5686275 0.5370370 0.51 0.29 0.57 #4 0.5098039 0.2857143 0.4262295 0.3589744 0.4262295 0.5098039 0.4642857 0.51 0.26 0.61 #5 0.7843137 0.2653061 0.5263158 0.5416667 0.5263158 0.7843137 0.6299213 0.51 0.40 0.76 With summaryFunction = twoClassSummary we obtain the following result for each fold: # ROC Sens Spec parameter Resample #1 0.4649860 0.6274510 0.3469388 none Fold1 #2 0.4697879 0.5490196 0.4081633 none Fold2 #3 0.4597839 0.5686275 0.4285714 none Fold3 #4 0.4457783 0.5098039 0.2857143 none Fold4 #5 0.4709884 0.7843137 0.2653061 none Fold5 Also, glm does not have any tunable hyper-parameter for caret it seems, you can use glmnet and tune the $\mathbb{L}_1$ and $\mathbb{L}_2$ regularization hyper-parameters of the elastic net with CV and obtain the best model w.r.t. the average CV score on the folds.
