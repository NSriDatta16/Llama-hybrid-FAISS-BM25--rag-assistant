[site]: datascience
[post_id]: 128409
[parent_id]: 
[tags]: 
When applying softmax over multiple dimensions of a tensor, does the order in which those dimensions are matter?

Lets say i have a tensor of order 256, dimensions indexed from 0 to 255. Lets say i am writing a function implementing the softmax operation because i am a newbie and i want to understand the fundamental principles of machine learning. Lets say the softmax is to be applied to dimensions 2 51 and 139. Will the final result stay the same, regardless of whether i do softmax(2,51,139) or softmax(51,2,139) or softmax(139,51,2)? Disregard the obvious problem of floating point operations not being really commutative.
