[site]: crossvalidated
[post_id]: 614700
[parent_id]: 
[tags]: 
Training computationally cheap models for feature selection and then extending to more flexible models

To what extent can I apply lessons learnt about the model performance on a simple "cheap" model (e.g. Naive Bayes, LDA) to a more flexible model (i.e. ensemble of trees, neural networks)? For example: How irresponsible is it to evaluate preprocessing strategies (e.g. should I use Fourier transform? PCA? Which signal segment length? Which robust standardization method? Which hyper-parameters etc.) using a simple, cheap model and then assume the same principles will also hold for flexible models? On the one hand this seems like a practical solution where you could rapidly prototype in a computationally cheap way. The "information" the data contains after preprocessing would, after all, be the same for a simple and a more sophisticated model? On the other hand, I can recognize that a more flexible/expensive model might be able to transform the input data in a more intricate way. As a result, you might make a mistake in assuming that the optimal pre-processing procedure for the simple model is also near-optimal for the expensive model.
