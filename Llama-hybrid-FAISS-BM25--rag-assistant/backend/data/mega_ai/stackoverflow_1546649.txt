[site]: stackoverflow
[post_id]: 1546649
[parent_id]: 1546381
[tags]: 
The way computers store numbers can be compared to an odometer in a car. If the odometer has 4 digits, it stores the number 33 as "0033". If someone asks you what your mileage is, you aren't going to say "zero thousand zero hundred and thirty three". By default, Java doesn't either. (Although you can tell it to.) Then wouldn't storing a small number in a large bit space waste a lot of bits? Well, not really. Suppose you had 11000100 in memory somewhere. How is the computer supposed to know whether this means 11000100, or 11000 followed by 100, or 1 followed by 1000 followed by 100, and so on? Well, actually the computer is just following the program it is given (remember that a Java program is created partly by you and partly by the people who design Java). If you can create a viable system for saving bits, you can make the computer do it. However, keep in mind that there's a tradeoff in terms of processor usage and programming difficulty. Since a typical computer can work with bytes much more quickly than it can with say, 7-bit or variable-bit numbers, storing ASCII codes in bytes is a very common choice for storing text. But let me return to your question. Then wouldn't storing a small number in a large bit space waste a lot of bits? Mathematically speaking, no. A branch of mathematics called Information Theory tells us that the number of bits which are absolutely necessary depends on the possibilities you want to encode and how likely each of them is. Let's suppose you have only a four letter alphabet (A, B, C, D), and use two-bit numbers (00, 01, 10, 11 respectively) to represent it. If each of these letters is equally likely, then the minimum number of bits required per letter (on average) is 2. In other words, there are no wasted bits even though A is 00 and B is 01. On the other hand, if you use ASCII and encode A, B, C, D as the following 7-bit numbers: A: 1000001 B: 1000010 C: 1000011 D: 1000100 then you are "wasting" 5 bits per letter (even though you're not "storing small numbers in a large bit space"). These sorts of considerations are important when designing compression algorithms, and not so important for everday applications. It's certainly important to understand bits and bytes if you wish to learn C.
