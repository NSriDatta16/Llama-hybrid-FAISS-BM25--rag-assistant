[site]: datascience
[post_id]: 100655
[parent_id]: 100649
[tags]: 
In order to evaluate a model (see how well it works) you usually set aside one part of the data (often randomly chosen) which is NOT used for model training (so you split the data in train and test data). You use the train data to train some model. Models are usually trained (or estimated) based on optimization of some function (the "loss"). In linear regression for instance, you minimize the sum of squared residuals. In logistic regression you optimize a maximum-likelihood function. In order to get some feedback on how well your (now trained) model works, you can obtain different "metrics". In regression for instance, you can look at the mean absolute error (MAE) or the mean squared error (MSE). In classification you will usually check how well your model works in making a classification. So you get some "classification error" (how well does the model work) which is not the same as looking at a loss function. Since most models will be able to make more or less okay predictions on data the model has seen during training, using the training data to evaluate some model is not a good idea! To get an idea of how well your model works on "new" data not seen by the model during training, you use the test set to make a prediction and to obtain the classification error. Instead (or on top of) using a test set to check your model, you can also use cross-validation. See " Introduction to Statistical Learning " (Ch. 5.1 for more details).
