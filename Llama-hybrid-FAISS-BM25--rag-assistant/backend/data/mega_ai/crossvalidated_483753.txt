[site]: crossvalidated
[post_id]: 483753
[parent_id]: 469799
[tags]: 
The overfitting nature of logistic regression is related to the curse of dimensionality in way that I would characterize as inversed curse, and not what your source refers to as asymptotic nature . It's a consequence of Manhattan distance being resistant to the curse of dimensionality. I could also say that it drives the loss to zero because it can . You can lookup a highly cited paper "On the Surprising Behavior of Distance Metrics in High Dimensional Space" by Aggarwal et al, here https://bib.dbvis.de/uploadedFiles/155.pdf They study different distance metrics and found that Manhattan distance is the most robust in high dimenional problems for the purpose of classification. Other metrics such as Euclidian distance can't tell the points apart. Now, all sigmoid fuctions have a linear term in Taylor approximation, see this one for example: Hence, the predictor $y(X\beta)\sim X\beta$ , which is very similar to a Manhattan distance $L_1$ . The log loss function is also linear around any point of choosing $\ln (x+e)=\ln x + \ln (1+e/x)\approx e/x$ . Therefore, the predictors in logistic regressions even after applying the loss function are going to be separating points in high dimensions very robustly, and will have no trouble driving the loss function to zero. This is in contrast to OLS regression where the setup is such that Euclidian distance is used to separate points. This distance is never linear by construction, it's exactly quadratic. As I already wrote Euclidian distance doesn't work well in high dimensional problems. You can see now that asymptotic nature has nothing to do with logit's tendency to overfit. Also, what your source means by that concept is the following: when $|X\beta|\to\infty$ then we have the predictor $y(X\beta)$ tend to either 0 or 1. Hence, the "asymptotic" characterization. The loss at the edges is infinitely large.
