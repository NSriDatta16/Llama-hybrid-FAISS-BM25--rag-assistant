[site]: crossvalidated
[post_id]: 305583
[parent_id]: 
[tags]: 
Are there realistic/relevant use-cases for one way ANOVA?

I have trouble imagining realistic research questions that can be answered by a one way ANOVA. Two or more ways with interaction terms and perhaps mixed factors (within and between subjects), sure you will need to do an ANOVA among others, but one way? I speak about one-way ANOVAs with more than two groups. If there are only two groups, that's a straightforward t-test or Wilcoxon variation. A way to put my question is this: Are there really research questions where all you want to know is whether at least any one of those groups is different from any other group, but where you don't care which ones they are? Another way to put my question is to state that the heavy lifting is not done by the ANOVA but by the planned contrasts or exhaustive post-hoc tests you do along with it. But do those really rely on the ANOVA? Being variants of t-tests, they only require i.i.d. data from interval variables that needs to be normally distributed unless $n$ is large enough. No ANOVA needed so far. Since you will be doing more than one comparison, you should control your type I error rate. If you're doing all pairwise comparisons and control for the FWER, the consensus seems to be that you don't need the ANOVA (and that you should do a power analysis instead of only taking 30 sample points per group for the CLT). Yet in this frequent scenario, I often see the ANOVA done anyway. Is this just a historic relic? I think this addition can be harmful as it reduces the power of the overall procedure by requiring the ANOVA must be significant before doing the post-hoc tests (discussed in more detail below). Then there are the planned contrasts where you don't compare all pairs but perhaps only some pre-selected ones and/or some linear combinations of groups (are those two treatments on average better than the average of the other three treatments etc.). It is alleged for example by (Howard Seltman, p.325) that those require to first reject the null hypothesis of the ANOVA: The same kind of argument applies to looking at your planned comparisons without first “screening” with the overall p-value of the ANOVA. Screening protects your Type 1 experiment-wise error rate, while lack of screening raises it. The scare-quotes around "screening" seem telling to me. Sure, if you add one more hurdle, your type I error cannot go up for it. All other things being equal, it can at most stay the same (the tests are completely redundant) or decrease (the tests measure something slightly different). But if your only concern was type I error rate, why not just decrease the $\alpha$ level? That seems certainly a cleaner solution than to add some other test that doesn't really address your research question. (I also don't agree with the implicit logic that type I error rate should always be your foremost concern, at least not unless you also do a power analysis.) Am I overlooking something? Is the condition of the significant ANOVA result perhaps only instated because for pre-selected contrasts, no correction for multiple comparisons is done at all? If yes, how do we know that this is not a case where we should correct for FWER? (Family size would be determined by the number of pre-selected contrasts, if we really select them upfront) How would we know that instead, the prerequisite ANOVA is the correct way to deal with multiple pre-selected contrasts for each of which we leave $\alpha$ at the nominal level? Edit : And since some of the more complex ANOVA designs can be equivalent to a one-way design with more groups (that cover all the combinations among the two- or more way ANOVA), perhaps my question is even more general and applies to many other ANOVAs. I don't want to overstate my case though. I'm not sure which ones can be equivalent to a one-way design and which ones cannot.
