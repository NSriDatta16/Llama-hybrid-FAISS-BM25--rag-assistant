[site]: crossvalidated
[post_id]: 232636
[parent_id]: 232356
[tags]: 
As @amoeba noticed, we have frequentist definition of probability and frequentist statistics . All the sources that I have seen until now say that frequentist inference is based on the frequentist definition of probability, i.e. understanding it as limit in proportion given infinite number random draws (as already noticed by @fcop and @Aksakal quoting Kolmogorov) $$ P(A) = \lim_{n\to\infty} \frac{n_A}{n} $$ So basically, there is a notion of some population that we can repeatably sample from. The same idea is used in frequentist inference. I went through some classic papers, e.g. by Jerzy Neyman , to track the theoretical foundations of frequentist statistics. In the 1937 Neyman wrote ( ia ) The statistician is concerned with a population, $\pi$, which for some reason or other cannot be studied exhaustively. It is only possible to draw a sample from this population which may be studied in detail and used to form an opinion as to the values of certain constants describing the properties of the population $\pi$. For example, it may be desired to calculate approximately the mean of a certain character possessed by the individuals forming the population $\pi$, etc. ( ib ) Alternatively, the statistician may be concerned with certain experiments which, if repeated under apparently identical conditions, yield varying results. Such experiments are called random experiments [...] In both cases described, the problem with which the statistician is faced is the problem of estimation. This problem consists in determining what arithmetical operations should be performed on the observational data in order to obtain a result, to be called an estimate, which presumably does not differ very much from the true value of the numerical character, either of the population $\pi$, as in ( ia ), or of the random experiments, as in ( ib ). [...] In ( ia ) we speak of a statistician drawing a sample from the population studied. In another paper (Neyman, 1977), he notices that the evidence provided in the data need to be verified by observing the repeated nature of the studied phenomenon: Ordinarily, the 'verification', or 'validation' of a guessed model consists in deducing some of its frequentist consequences in situations not previously studied empirically, and then in performing appropriate experiments to see whether their results are consistent with predictions. Very generally, the first attempt at verification is negative: the observed frequencies of the various outcomes of the experiment disagree with the model. However, on some lucky occasions there is a reasonable agreement and one feels the satisfaction of having 'understood' the phenomenon, at least in some general way. Later on, invariably, new empirical findings appear, indicating the inadequacy of the original model and demanding its abandonment or modification. And this is the history of science! and in yet another paper Neyman and Pearson (1933) write about random samples drawn from fixed population In common statistical practice, when the observed facts are described as "samples," and the hypotheses concern the "populations", for which the samples have been drawn, the characters of the samples, or as we shall term them criteria, which have been used for testing hypotheses, appear often to be fixed by happy intuition. Frequentist statistics in this context formalize the scientific reasoning where evidence are gathered, then new samples are drawn to verify the initial findings and as we accumulate more evidence our state of knowledge crystallizes. Again, as described by Neyman (1977), the process takes the following steps ( i ) Empirical establishment of apparently stable long-run relative frequencies (or 'frequencies' for short) of events judged interesting, as they develop in nature. ( ii ) Guessing and then verifying the 'chance mechanism', the repeated operation of which produces the observed frequencies. This is a problem of 'frequentist probability theory'. Occasionally, this step is labeled 'model building'. Naturally, the guessed chance mechanism is hypothetical. ( iii ) Using the hypothetical chance mechanism of the phenomenon studied to deduce rules of adjusting our actions (or 'decisions') to the observations so as to ensure the highest 'measure' of 'success'. [...] the deduction of the 'rules of adjusting our actions' is a problem of mathematics, specifically of mathematical statistics. Frequentists plan their research having in mind the random nature of data and the idea of repeated draws from fixed population, they design their methods based on it, and use it to verify their results (Neyman and Pearson, 1933), Without hoping to know whether each separate hypothesis is true or false, we may search for rules to govern our behavior with regard to them, in following which we insure that, in the long run of experience, we shall not be too often wrong. This is connected to repeated sampling principle (Cox and Hinkley, 1974): (ii) Strong repeated sampling principle According to the strong repeated sampling principle, statistical procedures are to be assessed by their behaviour in hypothetical repetitions under the same conditions. This has two facets. Measures of uncertainty are to be interpreted as hypothetical frequencies in long run repetitions; criteria of optimality are to be formulated in terms of sensitive behaviour in hypothetical repetitions. The argument for this is that it ensures a physical meaning for the quantities that we calculate and that it ensures a close relation between the analysis we make and the underlying model which is regarded as representing the "true" state of affairs. (iii) Weak repeated sampling principle The weak version of the repeated sampling principle requires that we should not follow procedures which for some possible parameter values would give, in hypothetical repetitions, misleading conclusions most of the time. As contrast, when using maximum likelihood we are concerned with the sample that we have , and in Bayesian case we make inference based on the sample and our priors and as new data appears we can perform Bayesian updating. In both cases the idea of repeated sampling is not crucial. Frequentists rely only on the data they have (as noticed by @WBT ), but keeping in mind that it is something random and it is to be thought as a part of process of repeated sampling from the population (recall, for example, how confidence intervals are defined). In frequentist case the idea of repeated sampling enables us to quantify the uncertainty (in statistics) and enables us to interpret real-life events in terms of probability . As a side note, notice that neither Neyman (Lehmann, 1988), nor Pearson (Mayo, 1992) were as pure frequentists as we could imagine they were. For example, Neyman (1977) proposes using Empirical Bayesian and Maximum Likelihood for point estimation. On another hand (Mayo, 1992), in Pearson's (1955) response to Fisher (and elsewhere in his work) is that for scientific contexts Pearson rejects both the low long-run error probability rationale [...] So it seems that it is hard to find pure frequentists even among the founding fathers. Neyman, J, and Pearson, E.S. (1933). On the Problem of the Most Efficient Tests of Statistical Hypotheses. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences. 231 (694–706): 289–337. Neyman, J. (1937). Outline of a Theory of Statistical Estimation Based on the Classical Theory of Probability. Phil. Trans. R. Soc. Lond. A. 236: 333–380. Neyman, J. (1977). Frequentist probability and frequentist statistics. Synthese, 36(1), 97-131. Mayo, D. G. (1992). Did Pearson reject the Neyman-Pearson philosophy of statistics? Synthese, 90(2), 233-262. Cox, D. R. and Hinkley, D. V. (1974). Theoretical Statistics. Chapman and Hall. Lehmann, E. (1988). Jerzy Neyman, 1894 - 1981. Technical Report No. 155. Department of Statistics, University of Califomia.
