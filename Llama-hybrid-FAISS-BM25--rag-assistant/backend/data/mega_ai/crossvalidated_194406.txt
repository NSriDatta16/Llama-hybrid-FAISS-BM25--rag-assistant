[site]: crossvalidated
[post_id]: 194406
[parent_id]: 99373
[tags]: 
I have experimented with the following methods of combining predictions, with varying degrees of success: Take an average of the predictions. For regression models, you can take the average of the predictions themselves. For classification models, you can take the average of the class probabilities. Similar to the above, but take a weighted average. You could determine the weights through linear regression, as you suggested in your question. I don't think you'd need the intercept term though. Again, similar to (1), but use non-linear methods to determine the optimal weights. For example, you could train a neural net, random forest or some other statistical learning algorithm by feeding it the predictions of your ensemble's constituent models. For classification problems, combine the predictions in a voting arrangement. Depending on your problem, you could choose your final prediction as the prediction that received the majority of the votes or the most votes. For some binary classification problems, it may be appropriate to demand consensus, depending on the consequences of misclassification. Whatever method you choose, you should ensure that it is appropriately cross-validated. In some instances, it would be very easy to overfit, especially using (3) above. There are some R packages that are built for combining predictions. caretEnsemble is fantastic for combining models tuned with the caret package. I understand that H20 and SuperLearner are built with ensembling in mind, though I've not used these packages extensively.
