[site]: crossvalidated
[post_id]: 319119
[parent_id]: 318934
[tags]: 
This paragraph gives a very high-level overview of backpropagation. Backpropagation is a mathematical technique that relates the change of a weight to a change in the outcome. A good discussion is this chapter. As you can see, a whole chapter has been devoted to explaining it. At the heart of backpropagation is an expression for the partial derivative $\partial C / \partial w $ of the cost function $C$ with respect to any weight $w$ (or bias $b$) in the network. The expression tells us how quickly the cost changes when we change the weights and biases. And while the expression is somewhat complex, it also has a beauty to it, with each element having a natural, intuitive interpretation. So the answer is that you adjust all weights and biases simultaneously by the amount calculated using backpropagation.
