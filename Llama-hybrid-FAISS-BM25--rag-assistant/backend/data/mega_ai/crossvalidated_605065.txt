[site]: crossvalidated
[post_id]: 605065
[parent_id]: 605062
[tags]: 
It's not. Consider the problem of estimating a random variable $Y$ using another random variable $X$ . We don't estimate random variables, but things about random variables. What would it mean to “estimate the random variable”? If I told you that I “estimated that the length of life is 70 years” what would it mean? Everybody would live exactly 70 years? It's maximum, minimum, average, mode, or median..? The statement would be quite meaningless. Also, why would you “estimate” a random variable with a single value when you can estimate its distribution (e.g. with empirical distribution or kernel density)? The best estimator of $Y$ by a function of $X$ is the conditional expectation $E[Y|X]$ . [...] $E[Y]$ or $E[Y|X]$ are not estimators, but properties of random variables. The estimator is a function of a sample, the expected value is a property of a random variable. However, $E[Y|X]$ seldom has a nice closed expression. [...] So we use linear regression techniques such as the least-squared solution to get an estimate from a random sample. One has nothing to do with the other. To find $E[Y]$ you need to solve an integral. $E[Y]$ applies to mathematical objects (random variables). Linear regression needs data, if I asked you to tell me what is the expected value given that you know the probability density function of the variable, you wouldn't be able to calculate linear regression on the given function. There wouldn't be a closed-form solution either. The opposite is also true, you cannot calculate the expected value from the data, but you can use some estimator to approximate it.
