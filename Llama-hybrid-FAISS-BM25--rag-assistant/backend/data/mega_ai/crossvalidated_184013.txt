[site]: crossvalidated
[post_id]: 184013
[parent_id]: 184010
[tags]: 
With random forests? No. They don't look at linear combinations of features, so this isn't a problem, and actually if you're doing this one-hot feature representation where, say, you don't include brown hair (because it's equivalent to not-red, not-blond, not-black), then individual trees will have a hard time asking about brown hair because they need to have three randomly included features to do that. If you include a brown hair feature, it can. Many random forest packages, though definitely not all, allow "real" categorical inputs, and so you don't have to do the one-hot representation. With SVMs? Generally no. A distance-based kernel, e.g. the RBF kernel, definitely does not have this issue. Linear or polynomial kernels or so on do use linear combinations in that way, and so if you don't regularize, you can run into the same multicollinearity problems as you do with OLS. But you should almost never not regularize an SVM; the standard $L_2$ regularization is equivalent to doing ridge regression, which handles most of the issues with multicollinearity, as noted in this thread linked to by @DJohnson above.
