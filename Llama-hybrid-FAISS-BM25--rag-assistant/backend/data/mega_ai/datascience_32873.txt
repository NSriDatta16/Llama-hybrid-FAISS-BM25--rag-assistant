[site]: datascience
[post_id]: 32873
[parent_id]: 
[tags]: 
Prioritized Replay, what does Importance Sampling really do?

I can't understand the purpose of importance-sampling weights (IS) in Prioritized Replay (page 5) . A transition is more likely to be sampled from experience replay the larger its "cost" is. My understanding is that 'IS' helps with smoothely abandoning the use of prioritized replay after we've trained for long enough. But what do we use instead, uniform sampling? I guess I can't realize how each component in such a coefficient is affecting the outcome. Could someone explain it in words? $$w_i = \left( \frac{1}{N}\cdot \frac{1}{P(i)} \right) ^\beta$$ It's then used to dampen the gradient, which we try to get from transitions. Where: $w_i$ is "IS" N is the size of Experience Replay buffer P(i) is the chance to select transition $i$ , depending on "how fat its cost is". $\beta$ starts from 0.4 and is dragged closer and closer to 1 with each new epoch. Is my understanding of these parameters also correct? Edit Sometime after the answer was accepted I found an additional source, a video which might be helpful for beginners - MC Simmulations: 3.5 Importance Sampling Edit As @avejidah said in the comment to his answer " $1/N$ is used to average the samples by the probability that they will be sampled" . To realise why it's important, assume $\beta$ is fixed to 1, we have 4 samples, each has $P(i)$ as follows: 0.1 0.2 0.3 0.4 That is, first entry has 10% of being chosen, second is 20% etc. Now, inverting them, we get: 10 5 3.333 2.5 Averaging via $1/N$ (which in our case is $1/4$ ) we get: 2.5 1.25 0.8325 0.625 ...which would add up to '5.21' As we can see they are much closer to zero than the simply inverted versions ( $10, 5, 3.333, 2.5$ ). This means the gradient for our network won't be magnified as much, resulting in a lot less variance as we train our network. So, without this $\frac{1}{N}$ were we lucky to select the least likely sample ( $0.1$ ), the gradient would be scaled 10 times. It would be even worse with smaller values, say $0.00001$ chance, if our experience replay has many thousands entries, which is quite usual. In other words, $\frac{1}{N}$ is just to make your hyperparameters (such as learning-rate) not require adjustment, when there you change the size of your experience replay buffer.
