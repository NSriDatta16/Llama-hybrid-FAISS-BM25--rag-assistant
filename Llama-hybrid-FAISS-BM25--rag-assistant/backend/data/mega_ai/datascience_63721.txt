[site]: datascience
[post_id]: 63721
[parent_id]: 
[tags]: 
What is the approx minimum size of dataset required to build 90% correct model?

I am working with a financial dataset size which is around 3000. I have attempted the supervised-learning regression techniques and not able to go beyond 70% accuracy. Features: 10 Data size:3700 Models attempted: Decision Trees, Random forest, Lasso Regression, Ridge regression, Linear regression I am of the opinion that dataset size is too small to expect any good results beyond 65%. It's obvious because machine learning algorithms are data-hungry in nature. However, In a generic sense, Is there a lower-bound on the dataset size that has been found to achieve 90% accuracy? Such a theory would also help me to gather data until I reach that point and then do some productive work. Any help is appreciated.
