[site]: datascience
[post_id]: 121628
[parent_id]: 92389
[tags]: 
The number of hidden layers does not have a direct affect on the performance of the activation function. However, with more hidden layers, the probability for the vanishing gradient meaning that the gradient takes on very small values making it hard for the neural network to learn properly. The sigmoid function is know to have a higher risk for the vanishing gradient in larger networks, which is why, ReLU or Leaky ReLU are preferred in larger architectures. So the effect is more indirect.
