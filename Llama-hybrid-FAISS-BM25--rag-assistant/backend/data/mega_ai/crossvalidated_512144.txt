[site]: crossvalidated
[post_id]: 512144
[parent_id]: 512139
[tags]: 
I would not use lasso. The state of the art in machine learning are ensemble/boosting methods (lightgbm, RF, HistGradientClassifier (and Regressor but also works with trees)), especially in kaggle competitions, thus if it is possible I would always try to use a method relying on trees, even if it is no ensemble. We can do so in your case. The rpart package from r is able to handle regression trees also known as CART with a target as yours (I suppose you do not want to discretize your target outcome). Although this tend a little to overfit, as it has no aggregated trees which can make a decision on the final outcome, it would still be better than Lasso imho. If you still want to use lasso, I can look for a notebook with elastic_net in my repo, which contains a fluent intervall between lasso and ridge. To have a first look into rpart look here: https://www.geeksforgeeks.org/decision-tree-for-regression-in-r-programming/ You are also cordially invited to use a notebook from me: The keywords are method="anova" thus rpart know its time for a regression not a classifier task. The notebook should still function, if not come back to me. This notebook also includes a 10-fold cv, as you can see with multifolds If you are feeling fine with ML in R. You should change to mlr3 the pendant to scikit in R. #'repeated K_Fold Model check splits $class)) accuracy overall["Accuracy"] %>% as.numeric() scores If you have any questions doesn't matter the subject, feel free to write me here or send me a message! I'm always eager to help. *Update do not forget to install the packages rpart , rpart.plot , caret , magrittr and e1071 I believe is also a good option
