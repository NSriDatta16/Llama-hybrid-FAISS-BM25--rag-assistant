[site]: crossvalidated
[post_id]: 11588
[parent_id]: 11562
[tags]: 
Overview Usually when I think of multiple raters assessing multiple objects, I think of "bias" as a mean difference in expected rating of a particular judge from the mean of a hypothetical population of judges. This is a rather statistical definition of bias, which does not necessarily correspond to everyday definitions of bias, which would presumably also include the notion of failure to impartially apply relevant standards. Basic ideas Bearing in mind that there is probably an established literature on this, these are the ideas that came to my mind: Compare mean rating of each judge is a given judge harsher or more lenient on average? Compare standard deviation or variance of each judge is the judge differentiating to the extent that is expected or in ways consistent with other judges? For each judge, correlate that judge's ratings with the mean of all other judges, and use the correlation as an index of the validity of that judge's ratings is the judge identifying quality in the same way as other judges? Build a model predicting ratings for contestant i by judge j and record the residuals ; large absolute residuals could be excluded from some overall rating. The model could be as simple as an ANOVA predicting response for contestant i by judge j using just the main effects (no interaction effects). is a judge responding in an uncharacteristic manner for a particular contestant? The mean approach is what I think of as bias. The residuals approach will capture what you are interested in. Basic implementation in R I hacked this out in a few minutes, so hopefully there aren't any bugs (but use at your own risk). # Import data x # Mean: Judge's Mean rating - i.e., bias round(tapply(x$rating, x$judge, function(X) mean(X)), 1) 1 2 3 4 5 89.6 83.1 80.2 90.6 64.4 This shows that judge 5 is harsh and perhaps also that judge 1 and 4 maybe too lenient. > # SD: Judge's SD rating i.e., excessive or insufficient variability in ratings round(tapply(x$rating, x$judge, function(X) sd(X)), 1) 1 2 3 4 5 5.6 10.8 6.1 3.6 22.8 This shows that judge 5 is vastly more variable, but equally the other judges vary in their variability quite a lot also. > # Correlation judgecor Judge 1 and 5 are less consistent with the other judges. > # Residuals fit This shows the largest five absolute residuals in ratings after taking out mean contestant and mean rater effects. It shows clearly that the rating by judge 5 on contestant 4 was an extreme outlier, relative to the other residuals.
