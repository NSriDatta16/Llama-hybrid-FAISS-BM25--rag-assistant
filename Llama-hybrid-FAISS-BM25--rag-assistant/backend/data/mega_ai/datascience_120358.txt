[site]: datascience
[post_id]: 120358
[parent_id]: 
[tags]: 
How many parameters does the vanilla Transformer have?

The original Transformer paper ( Vaswani et al; 2017 NeurIPS ) describes the model architecture and the hyperparameters in quite some detail, but it misses to provide the exact (or even rough) model size in terms of parameters (model weights). I could not find a source with a definite answer on this. Table 3 also mentions a base and a big model, but for none of them model size is given. How many parameters does a base or a big Transformer model, according to the original implementation by Vaswani et al., have?
