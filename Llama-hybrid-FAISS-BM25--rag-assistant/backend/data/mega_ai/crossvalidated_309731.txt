[site]: crossvalidated
[post_id]: 309731
[parent_id]: 309565
[tags]: 
It might be overwhelming to start off, so I will try to give you some advice and some references that hopefully can get you started. A simple approach is to treat each row as a text document with three words. The "keyword_group" and the "keyword" columns need to be represented as vectors. The simplest way to do that is to convert your document into a (sparse) vector, where each column is the count of a certain "keyword" or "keyword_group" (so the number of columns will be equal to size of the vocabulary). There are other ways of doing word embeddings , but this is the easiest. If you are familiar with python, here is a good reference of how to do that. The "is_source_file" is a binary variable, so you can easily represent it as 0 or 1 and add it as a new feature. At that point, you can use any classifier. Here is an example where a lot of them are used. It's a famous dataset for which the task is to to classify documents into 20 categories. You only have two, but the problem is similar. Finally, as others have mentioned, you can add more words to you documents taking combinations of those.
