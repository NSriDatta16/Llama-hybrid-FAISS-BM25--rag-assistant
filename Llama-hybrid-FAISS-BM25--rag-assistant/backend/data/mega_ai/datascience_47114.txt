[site]: datascience
[post_id]: 47114
[parent_id]: 47094
[tags]: 
First of all, I have noticed that you have used sigmoid activation function for your LSTM Dense Layer and in your ANN you used relu , maybe, MAYBE, this can be a reason for your lower performance. That is could be happening because sigmoid functions suffer from two problems: - Saturation of gradients : sigmoid functions have tail distributions, meaning that they saturate in this 'flat' regions practically diminishing the gradient to zero and affecting backpropagation/training process. - Sigmoid outputs are not zero-centered : This is an issue due the gradient calculation during backpropagation. Either having all enters positivo or negative will add a 'zigzag' effect difficulting the training process. My comments above are taken from this excellent tutorial that you should read: http://cs231n.github.io/neural-networks-1/ I tried to summarize it but they did a master work and I think you should read it. Second, we must consider other factors such as: are you analyzing your train/test/val losses? Maybe your LSTM networks just takes longer to train and reach its minimum. You need to work a little more on these parameters before taking any conclusions. Plot a graph showing your training and validation loss so we can assess if your model is underfitting. Lastly, I have a question for you: Why should your LSTM network perform better than a simple ANN? Although LSTM + Embeddings are powerful techniques that have gained attention in a lot of fields, essentially NLP, that will be not every task that they beat classical approaches. I myself have tried with different data sets, and depending on the application, simple ML algorithms such as SVM would easily beat the more complex ones, including sentiment analysis. So to conclude, try these things and let us know your results. Also, if anyone disagrees with my answer, I would like to discuss it. I hope it helps.
