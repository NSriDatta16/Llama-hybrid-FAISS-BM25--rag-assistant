[site]: crossvalidated
[post_id]: 456072
[parent_id]: 
[tags]: 
Bayes Factor, Likelihood Ratio, and p-values

I am interested in "simple" changepoint detection algorithms. I originally was using very simples approaches that consist of making t-test calculations and calculate a p-value (similar to what is described in this paper: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4967025/ , Fig. 3). I then read more on p values and found that they might not be the best approach. First they cannot favor one hypothesis and second they remain quite independent on the sample size (in contrast to the other two factors that I discuss below). So I "discovered" bayes factor, which have the advantage of directly comparing two hypothesis. But more recently, I "found" that the bayes factors were not that different from likelihood ratio (in case of uniform priors). For instance, this post http://www.claudiobellei.com/2017/01/25/changepoint-bayesian/ and its counterpart ( http://www.claudiobellei.com/2016/11/15/changepoint-frequentist/ ) give basically the same results. So my questions: Q1. Is the affirmation (regarding p-values): First they cannot favor one hypothesis and second they remain quite independent on the sample size (in contrast to the other two factors that I discuss below) correct ? Q2. Is the affirmation: in case of non informative priors, calculating a Bayes factor or likelihood ratio is equivalen correct ? The difference is that, for Bayes factors, you perform an integration on all possible values of your parameters. Btw, is there a simple argument to understand this equivalence ? Many thanks for your answers !!!
