[site]: datascience
[post_id]: 113359
[parent_id]: 
[tags]: 
why there is no preprocessing step for training BERT?

I would like to train a BERT model from scratch. I read the paper as well as a few online material. It seems there is no preprocessing involved. e.g. removing punctuation, stopwords ... I wonder why is it like that and would that improve them model if I do so ?
