[site]: crossvalidated
[post_id]: 317995
[parent_id]: 
[tags]: 
Wilcoxon signed rank test correct for subject accuracy?

I'm conducting a test where subjects have to select targets on a touch screen display. I.e. 100% means user hits all targets on the first try, 25% means user needs 3 tries on average before they hit a target. The data is not normally distributed so we eventually wanted to use Wilcoxon signed rank to compare subject accuracy across different input methods with whom the targets were selected. When evaluating these results I'm now noticing that the data is in certain cases very skewed towards 100%. For example, out of 20 subjects, one case of an input method with 'easy targets' looks like this (R vector): > [1] 1.0000000 1.0000000 1.0000000 0.9375000 0.9677419 0.9677419 1.0000000 1.0000000 1.0000000 0.9375000 1.0000000 1.0000000 0.9375000 1.0000000 0.9677419 0.9375000 0.9677419 1.0000000 1.0000000 1.0000000 I.e. 12 out of 20 targets had a 100% accuracy on target selection. This means a lot of ties in the data. It looks like the continuity assumption of the Wilcoxon signed rank test is not fulfilled. Could this still be OK if the underlying 'ground truth' distribution is continuous and I just caught an unlucky sample? The samples from the data above are gathered by letting subjects select ~30 targets and then just compute the overall accuracy. I doubt anyone would have a 100% accuracy if we gathered thousands of samples per subject, in which case I think there would be a much more smooth distribution of values anywhere from high 95% up close to 100%. If it's not OK, what are my alternatives? If it's OK, how do I proceed? Fuzz the data? Ignore the tie samples?
