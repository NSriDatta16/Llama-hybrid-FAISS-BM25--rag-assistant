[site]: datascience
[post_id]: 65751
[parent_id]: 
[tags]: 
Why replay memory store old states and action rather than Q-value (Deep Q-learning)

Here is the algorithm use in Google's DeepMind Atari paper The replay memory D store transition (old_state, action performed, reward, new_state) The old_state and the performed action a are needed to compute the Q-value of this action in this state. But since we already compute the Q-value of action a in state old_state in order to choose a as the best action, why don't we simply store directly the Q-value ?
