[site]: crossvalidated
[post_id]: 432356
[parent_id]: 432351
[tags]: 
I think it is related to the very reason deep models generalize by over-parameterization. There is a hypothesis that this generalization happens because bigger networks contain more "lottery-ticket" sub-networks during initialization that are close to being "correct" for the objective and backpropagation is good with preserving and utilizing them . If it is true and training process is deterministic, then we can define an r.v. of scores(function of initialized weights which are r.v.). This r.v.'s distribution will depend on how many "good" sub-networks in our network can appear (and how probable they are). If we know that only spatially related pixels matter in images, then we can use this intuition and limit overall space of sub-networks by ignoring weights that are less probable to be a part of a good solution and therefore increasing a chance of "good" sub-network to appear. That is what CNN's do IMO. Also as you've mentioned CNN's contain much less connections (because they ignore less necessary ones) and therefore can be bigger/deeper than their fully-connected counterparts.
