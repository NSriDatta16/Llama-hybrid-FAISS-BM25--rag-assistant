[site]: crossvalidated
[post_id]: 614588
[parent_id]: 
[tags]: 
On the reliability of validation loss as a metric

I have the following plot of the training and validation loss from a deep neural network. The signature U-curve of the validation loss can be noticed. I want to use the validation loss as the metric for saving the weights of my model and I am currently using the weights with the lowest possible validation loss. However, a question came to me, suppose I list down the top 5 weights with the following losses: Epoch 120: 1.62 Epoch 150: 1.65 Epoch 190: 1.68 Epoch 230: 1.73 Epoch 270: 1.78 As you can see, their losses are very near each other while the best validation loss occurs the earliest epoch. Particularly, how can I quantify which is better than Top 1 and 2 when their losses varies only by less than 0.03? I am using data augmentation and I think that the generalizability would be better when trained for longer epochs, however the validation loss does not select Top 3 for example when its difference between the Top 1 is quite small. Any suggestions will do, thank you.
