[site]: crossvalidated
[post_id]: 451244
[parent_id]: 451010
[tags]: 
By linear projections, one projection per attention head later used in the encoder-decoder attention. To be consistent with the notation in the paper , it would better to say that in the encoder-decoder attention $K = V$ which are the final states of the encoder and $Q$ are decoder states from a particular layer. They are used as input of the $\text{MultiHead}$ (unnumbered equation on top of page 5) function where they are projected for individual heads. So in the individual head computation according to Equation 1: it is indeed true $K$ and $V$ are linear projections of the final encoder states (outputs of the top encoder layer).
