[site]: crossvalidated
[post_id]: 490999
[parent_id]: 490992
[tags]: 
Mean absolute error is defined as an average of the absolute differences between observed values $y_i$ and the predictions for them $\hat y_i$ $$ \frac{\sum_{i=1}^n |y_i - \hat y_i|}{n} $$ So it is not a percentage of anything. My guess is that they mean that the error is measured in percentage point units, because their target is the relative runtime (p. 3), i.e. by how many percentage points does the runtime differ from the reference runtime. In such case, you can say that the machine learning model makes predictions that are off by some amount of relative runtime units, hence the percentage points. But I must say, that I agree with Stephan Kolassa, that it is rather strange that they discuss things like standard deviations or normal distributions while discussing MAE, so I'd take the discussion with a grain of salt.
