[site]: crossvalidated
[post_id]: 355646
[parent_id]: 
[tags]: 
Bayesian inference of a coin's bias when we don't directly observe the flips

Consider a coin with bias $p$. We generate a random sample $x_1, \dots, x_n \sim \text{Bernoulli}(p)$, but we do not observe results of these coin tosses . Instead, for each $x_i$, we observe a set of features $y_{i1}, \dots, y_{im}$ about the flip, e.g. the height of the toss, the coin's rotational velocity, etc. Then we feed the observed feature vector $\mathbf{y}_i$ into a black box predictor (perhaps a logistic regression or tree-based model trained on past observations). The model yields $\hat{p}(\mathbf{y}_i) \in [0,1]$, which we interpret as the probability that $x_i = 1$. I am trying to figure out how to use $\hat{p}(\mathbf{y}_1), \dots, \hat{p}(\mathbf{y}_n)$ to obtain a posterior distribution on $p$. My initial idea is to run the model $\hat{p}$ on a test dataset, and obtain the empirical distributions $P(\hat{p} \mid x = 0)$ and $P(\hat{p} \mid x = 1)$. We then have all the likelihood functions for the hierarchical model (ignoring the $\mathbf{y}_i$'s) and can generate MCMC samples of $p$. Is there anything wrong with this approach? Is there a better way? Can we gain anything by making assumptions about the model $\hat{p}$?
