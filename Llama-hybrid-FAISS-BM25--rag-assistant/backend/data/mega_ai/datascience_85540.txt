[site]: datascience
[post_id]: 85540
[parent_id]: 85532
[tags]: 
This is totally normal and reflects a fundamental phenomenon in data science: overfitting. When the validation loss stops decreasing, while the training loss continues to decrease, your model starts overfitting. This means that the model starts sticking too much to the training set and looses its generalization power. As an example, the model might learn the noise present in the training set as if it was a relevant feature. When training your model, you should monitor the validation loss and stop the training when the validation loss ceases decreasing significantly. It is also the validation loss that you should monitor while tuning hyperparameters or comparing different preprocessing strategies.
