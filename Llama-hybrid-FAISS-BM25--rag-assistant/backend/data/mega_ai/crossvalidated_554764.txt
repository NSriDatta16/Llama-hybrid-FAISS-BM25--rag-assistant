[site]: crossvalidated
[post_id]: 554764
[parent_id]: 
[tags]: 
Is backpropagation a fancy way of saying "calculate gradient by taking partial derivative w.r.t. all x's"

I understanding how gradient is calculated in the usual context--it is just taking partial derivative w.r.t. each element of the X vector. Say a function $f$ has $n$ independent variables, denoted by $x_1$ ... $x_n$ , the gradient of $f$ is: $$ \nabla f = \left $$ However, when it comes to the gradient of the loss function of a neural network, a new term, backpropagation, emerges. My confusion is I am not sure if backpropagation makes the above calculation different or is it just another name of it. Let me use the following simple neural network as an example (courtesy of this very informative video )ï¼š This NN has two nodes, four weights (denoted by $w_1$ ... $w_4$ ) and three biases (denoted by $b_1$ ... $b_3$ ). So the loss function is defined like this: $L(w_1, w_2, w_3, w_4, b_1, b_2, b_3)$ = [a loss function, which can be extremely lengthy but differentiable] Without knowing the exact meaning of backpropagation, I would say its gradient is just: $$ \nabla L = \left $$ and the calculation of the partial derivative w.r.t. $w_1$ , $w_2$ ... $b_2$ , $b_3$ is no more than repetitive application of chain rules, etc--we don't need anything more than good old calculus to make it work. With gradient calculated, we can use gradient descent to minimize the loss function (well, the convexity/local minima is another big issue, let's ignore it for the purpose of this particular question). So it begs the question: what is the role of "backpropagation" in the calculation? Does backpropagation make the calculation easier (i.e., backpropagation is faster but the resultant gradient will be exactly the same as the traditional partial derivative approach) or is it just a name of this calculation?
