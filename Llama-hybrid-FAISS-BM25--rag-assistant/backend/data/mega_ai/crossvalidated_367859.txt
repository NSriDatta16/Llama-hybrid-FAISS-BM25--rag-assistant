[site]: crossvalidated
[post_id]: 367859
[parent_id]: 350707
[tags]: 
I think your preprocessing steps are fine, there isn't too much you can do there. I think that 5 iterations is way too small because LDA usually takes a really long time to converge, even when using the Variational Bayes algorithm. Whenever I use LDA, I give it at least a 100 iterations. Another suggestion is to keep track of how the LDA model's perplexity changes over time while training, and decide on the number of iterations accordingly. I would also suggest trying to not reduce the number of dimensions to 5000, depending on the use case, it may or may not be a good idea. You can try running LDA without any dimensionality reduction on the vectorizer, and see what you get. Be careful about the max_df and min_df parameters in sklearn's TfidfVectorizer. When you use a float between 0 and 1, it corresponds to the fraction of documents. When you use an integer, it corresponds to the absolute count of documents.
