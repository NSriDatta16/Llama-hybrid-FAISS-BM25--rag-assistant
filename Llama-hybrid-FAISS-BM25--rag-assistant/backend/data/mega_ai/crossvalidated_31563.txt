[site]: crossvalidated
[post_id]: 31563
[parent_id]: 31513
[tags]: 
Not sure if there'll be any other "ranty" responses, but heres mine. Cross Validation is in no way "new". Additionally, Cross Validation is not used when analytic solutions are found. For example you don't use cross validation to estimate the betas, you use OLS or IRLS or some other "optimal" solution. What I see as a glaringly obvious gap in the quote is no reference to any notion of actually checking the "best" models to see if they make sense. Generally, a good model makes sense on some intuitive level. It seems like the claim is that CV is a silver bullet to all prediction problems. There is also no talk off setting up at the higher level of model structure - do we use SVM , Regression Trees , Boosting , Bagging , OLS , GLMS , GLMNS . Do we regularise variables? If so how? Do we group variables together? Do we want robustness to sparsity? Do we have outliers? Should we model the data as a whole or in pieces? There are too many approaches to be decided on the basis of CV . And another important aspect is what computer systems are available? How is the data stored and processed? Is there missingness - how do we account for this? And here is the big one: do we have sufficiently good data to make good predictions? Are there known variables that we don't have in our data set? Is our data representative of whatever it is we're trying to predict? Cross Validation is a useful tool, but hardly revolutionary. I think the main reason people like is that it seems like a "math free" way of doing statistics. But there are many areas of CV which are not theoretically resolved - such as the size of the folds, the numbers of splits (how many times do we divide the data up into $K$ groups?), should the division be random or systematic (eg remove a state or province per fold or just some random 5%)? When does it matter? How do we measure performance? How do we account for the fact that the error rates across different folds are correlated as they are based on the same $K-2$ folds of data. Additionally, I personally haven't seen a comparison of the trade off between computer intensive CV and less expensive methods such as REML or Variational Bayes . What do we get in exchange for spending the addiional computing time? Also seems like CV is more valuable in the "small $n$" and "big $p$" cases than the "big $n$ small $p$" one as in "big $n$ small $p$" case the out of sample error is very nearly equal to the in sample error.
