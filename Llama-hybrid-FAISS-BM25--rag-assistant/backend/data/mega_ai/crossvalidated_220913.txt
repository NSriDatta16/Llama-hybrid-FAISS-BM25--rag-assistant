[site]: crossvalidated
[post_id]: 220913
[parent_id]: 
[tags]: 
What is the purpose of including negative samples in a training set?

I am new to machine learning and advanced statistics (anything beyond first-year college statistics), and I have been exploring the effectiveness of various classifiers on modeling different hand gestures based on some IMU data I've gathered. I have been told by multiple colleagues that I should train my model on 'positive' and 'negative' data sets in order to generate a more accurate model for my data set, but this seems a little counter-intuitive to me. Q: How does including random noise in a training set improve the accuracy of a model? It seems to me that random noise would be counter-productive by introducing bias to the estimator, but on the other hand I have a feeling that this has something do to with reducing the chance of overfitting a model on a set of data.
