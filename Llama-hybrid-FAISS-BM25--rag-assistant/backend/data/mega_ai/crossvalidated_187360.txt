[site]: crossvalidated
[post_id]: 187360
[parent_id]: 185616
[tags]: 
In boosting, weak or or unstable classifiers are used as base learners. This is the case because the aim is to generate decision boundaries that are considerably different. Then, a good base learner is one that is highly biased, in other words, the output remains basically the same even when the training parameters for the base learners are changed slightly. In neural networks, dropout is a regularization technique that can be compared to training ensembles. The difference is that the ensembling is done in the latent space (neurons exist or not) thus decreasing the generalization error. "Each training example can thus be viewed as providing gradients for a different, randomly sampled architecture, so that the final neural network efficiently represents a huge ensemble of neural networks, with good generalization capability" - quoting from here . There are two such techniques: in dropout neurons are dropped (meaning the neurons exist or not with a certain probability) while in dropconnect the weights are dropped. Now, to answer your question, I believe that neural networks (or perceptrons) are not used as base learners in a boosting setup since they are slower to train (just takes too much time) and the learners are not as weak, although they could be setup to be more unstable. So, it's not worth the effort. There might have been research on this topic, however it's a pity that ideas that don't work well are not usually successfully published. We need more research covering pathways that don't lead anywhere, aka "don't bother trying this". EDIT: I had a bit more though on this and if you are interested in ensembles of large networks, then you might be referring to methods of combining the outputs of multiple such networks. Most people average or use majority voting depending on the task - this might not be optimal. I believe it should be possible to change the weights for each network's output according to the error on a particular record. The less correlated the outputs, the better your ensembling rule.
