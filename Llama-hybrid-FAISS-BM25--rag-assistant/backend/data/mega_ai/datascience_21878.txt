[site]: datascience
[post_id]: 21878
[parent_id]: 21877
[tags]: 
GridSearchCV lets you combine an estimator with a grid search preamble to tune hyper-parameters. The method picks the optimal parameter from the grid search and uses it with the estimator selected by the user. GridSearchCV inherits the methods from the classifier, so yes, you can use the .score, .predict, etc.. methods directly through the GridSearchCV interface. If you wish to extract the best hyper-parameters identified by the grid search you can use .best_params_ and this will return the best hyper-parameter. You can then pass this hyper-parameter to your estimator separately. Using .predict directly will yield the same results as getting the best hyper-parameter through .best_param_ and then using it in your model. By understanding the underlining workings of grid search we can see why this is the case. Grid Search This technique is used to find the optimal parameters to use with an algorithm. This is NOT the weights or the model, those are learned using the data. This is obviously quite confusing so I will distinguish between these parameters, by calling one hyper-parameters. Hyper-parameters are like the k in k-Nearest Neighbors (k-NN). k-NN requires the user to select which neighbor to consider when calculating the distance. The algorithm then tunes a parameter, a threshold, to see if a novel example falls within the learned distribution, this is done with the data. How do we choose k? Some people simply go with recommendations based on past studies of the data type. Others use grid search. This method will be able to best determine which k is the optimal to use for your data. How does it work? First you need to build a grid. This is essentially a set of possible values your hyper-parameter can take. For our case we can use $[1, 2, 3, ..., 10]$. Then you will train your k-NN model for each value in the grid. First you would do 1-NN, then 2-NN, and so on. For each iteration you will get a performance score which will tell you how well your algorithm performed using that value for the hyper-parameter. After you have gone through the entire grid you will select the value that gave the best performance. This goes against the principles of not using test data!! You would be absolutely right. That is the reason grid search is often mixed with cross-validation. So that we keep the test data completely separate until we are truly satisfied with our results and are ready to test. $n$-fold cross-validation takes a training set and separates it into $n$ parts. It then trains on $n-1$ folds and tests on the fold which was left out. For each value in the grid, the algorithm will be retrained $n$ times, for each fold being left out. Then the performance across each fold is averaged and that is the achieved performance for that hyper-parameter value. The selected hyper-parameter value is the one which achieves the highest average performance across the n-folds. Once you are satisfied with your algorithm, then you can test it on the testing set. If you go straight to the testing set then you are risking overfitting.
