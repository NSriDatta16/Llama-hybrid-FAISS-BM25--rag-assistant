[site]: crossvalidated
[post_id]: 237085
[parent_id]: 
[tags]: 
How to correctly compute $\rho$ in reinforcement learning with importance sampling?

This question regards off-policy reinforcement learning using importance sampling. From the paper Off-Policy Temporal Difference Learning with Function Approximation , page 3: $\rho_t = \frac{\pi(s_t,a_t)}{b(s_t,a_t)}$ where $\rho_t$ is the importance sampling ratio for time $t$, which will get multiplied by the $\alpha$ step factor / learning rate during parameter updating. From what I understand, $\pi(s_t,a_t)$ is the probability of selecting action $a_t$ in state $s_t$ according to the target policy $\pi$. Assuming that $\pi$ is the greedy policy (and here I'm probably wrong), wouldn't this value always be 0 (exploration action) or 1 (greedy action)? Meaning that the algorithm would never learn when performing exploratory actions? It seems I don't quite understand how to compute $\rho$ by not understanding what exactly is being computed by that ratio. How are $\pi(s_t,a_t)$ and $b(s_t,a_t)$ (and consequently $\rho$) really computed?
