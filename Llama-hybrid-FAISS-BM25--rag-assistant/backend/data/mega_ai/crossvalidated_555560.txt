[site]: crossvalidated
[post_id]: 555560
[parent_id]: 278716
[tags]: 
Out of the three options, let's first talk about option 2 and option 3 first, then option 1. Option 2: Panel expert judgement No. Never trust subject-matter experts on their judgement about anything quantitative. This applies to even ML experts on their topic-of-interest ML models' performance metrics. In fact, the more the experts know about the subject, the more biased they are about that subject. In addition, experts tend to be over-confident on their ability to avoid biases, which makes it even worse. That's why almost all companies insist on A/B test before fully deploying models. Listen to how Gary King explains a version of it: https://youtu.be/rBv39pK1iEs?t=594 Option 3: Accept that it's unsupervised and look for other tasks for the competition Let's assume that we accept the fact that we do not have any label data and we are doing some sort of clustering. Although there are some metrics that define how well documents cluster based on the words in them, it boils down to having an algorithm that optimizes whatever metrics you pick directly. So at the end the competition becomes who's designing a better EM algorithm for the chosen metric, which is pretty lame. So yes, probably it's best to move on to other datasets. Option 1: Make it supervised by creating labels This is the only option assuming we do not want to give up on the dataset. One option is to leave the title, and/or first paragraph, and/or last paragraph out, and extract some keywords (e.g. simply TF-IDF) from them as document tags, and make it a tag prediction task. The another option is to think about some external attributes of the documents that you can harvest with a reasonable amount of time and effort. This most likely depends on the kind of documents you are handling. For example, if the documents are financial reports of public companies, you can check the price change for their stocks after the date of the report release. If it's academic papers, you can try to extract their number of citations via Google scholar and have a task to predict citation at 90 day after publication. It heavily depends on context and creativity.
