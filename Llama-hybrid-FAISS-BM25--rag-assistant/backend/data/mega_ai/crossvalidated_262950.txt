[site]: crossvalidated
[post_id]: 262950
[parent_id]: 
[tags]: 
Posterior using two different types of data

My student made the interesting problem and asked me how to solve it. I believe the Bayesian approach is promising, and got a kind of solution. But, I'm not confident of my solution because it's too complicated considering the simpleness of the problem. There are two questions: Is this problem equivalent some famous one? Could you please suggest how to solve it? The problem my student made Setting There are two boxes. Each box contains $N^{c}$ coins. The coins are labeled with numbers $1,2,3,\cdots,N^{C}$. The probabilities of getting heads or tails are NOT necessarily 0.5 (distorted cons). Also, $p^{A,i}$ is NOT necessarily $p^{B,i}$, where $p^{\eta,i}$ is the probability of getting head when tossing $i$-th coin in the box $\eta \in \{A,B\}$. Data 1 Alice tossed each coin in both of the box A and B with $N^{\eta,i}$ times, where $N^{\eta,i}$ is how many times she tossed the $i$-th coin in the box $\eta$. She recored how many times head or tail had been obtained. So, the observed data is like $D_1=\{N^{A,1}_{head}, N^{A,1}_{tail},N^{A,2}_{head}, N^{A,2}_{tail},\cdots,N^{B,1}_{head}, N^{B,1}_{tail},N^{B,2}_{head}, N^{B,2}_{tail},\cdots\}$. (Of course, $N^{\eta,i}_{head}+ N^{\eta,i}_{tail} = N^{\eta,i}$) Data 2 Then, Bob selected randomly the box A or box B and tossed all of coins inside the selected box. He recorded head or tail of each coin and passed the result to Alice as the data 2. So, the data 2 is like $D_2=\{x^{1}, x^{2}, \cdots,x^{N_c}\}$, where $x^{i}$ is head or tail of $i$-th coin. problem Using the two kind of data, Alice is supposed to infer the posterior probability that Bob chose the box A. Formally, she calculates $P(Box=A|D_1, D_2)$. My answer probability of each coin Using the data $D_1$ the probability of getting the head of each coin can be calculated as below. By the bayese theorem, the probability $p^{\eta, i}$, by which one gets the head of the $i$-th coin in the box $\eta \in \{A, B\}$ is $Beta(\alpha=1+N^{A,i}_{head}, \beta=1+N^{A,i}_{tail})$, where $Beta(\alpha, \beta)$ is the beta distribution . Likelihood of $D_2$ With Monte Carlo simulation, you can calculate the likelihood $P(D_2|\eta, p^{\eta, i})$. Posterior With MCMC, you can calculate $P(\eta|D_1, D_2) = P(\eta|D_1, p^{\eta, i})$ with a setting of that the likelihood is set as $P(D_2|\eta, p^{\eta, i})$ calculated at the above step, and the prior $P(A) = P(B) = 0.5$.
