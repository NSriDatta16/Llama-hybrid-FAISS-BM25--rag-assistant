[site]: datascience
[post_id]: 36866
[parent_id]: 33098
[tags]: 
A potential approach: Build the most accurate model possible, using most of the data: parameter tuning using cross-validation. Accuracy should be tested on out-of-sample data. Alternatively use the default xgboost parameters and proceed to step 2 . Assess feature importance using the tuned parameters: build multiple models on random subsets of data (using the same set of tuned parameters from (1) in each model) and extract feature importance. The more aggressive you are in bagging, the more models you will need to build. That is: smaller subsamples -> more models. Combine / aggregate feature importance measures from models to obtain the most important features.
