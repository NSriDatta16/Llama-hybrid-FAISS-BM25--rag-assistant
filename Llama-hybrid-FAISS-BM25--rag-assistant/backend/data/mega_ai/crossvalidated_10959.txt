[site]: crossvalidated
[post_id]: 10959
[parent_id]: 10958
[tags]: 
No it should not. e.g. for logistic regression, which appears to be the case here, it may be that the greater p-value (I'm assuming you mean the right kind of p-value here, like from a likelihood ratio test) comes from increasing the odds for (correctly predicted) observations that are already relatively extreme (e.g. all observations that had predicted probability 0.8 become 0.9 and all those that had 0.2 become 0.1), while the ones that were only just on the right side of the 50% threshold are now just on the other side. As a result, the extremities are now predicted with more confidence, but there are more misclassifications. In general, good fit does not guarantee good prediction (or the other way around) - even though that's the way most 'scientific' publications work these days :-( I would advise you to look into a more evolved technique like LASSO or elastic net for variable selection... It will also easily allow you to optimize for some predictive measure like missclassification. In R, try glmnet.
