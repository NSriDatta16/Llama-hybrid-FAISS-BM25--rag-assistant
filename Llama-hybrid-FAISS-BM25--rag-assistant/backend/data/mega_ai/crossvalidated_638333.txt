[site]: crossvalidated
[post_id]: 638333
[parent_id]: 
[tags]: 
Locality sensitive hashing (LSH) with word embeddings and cosine similarity

I would like to ask about the methodology of LSH algorithm with Word Embeddings and Cosine Similarity to identify similar documents. First, I tokenize my sentences to create a list of tokens. Then, I apply a pre-trained GloVe model to generate embeddings per token. If one sentence has 5 words, then I will have 5 *(1,300) embedding vectors, where 300 its the vector size of embeddings. To narrow this down for each sentence I average the embeddings by taking the mean() value. Thus the (5,300) vector becomes (1,300) embedding vector. Having the embeddings per sentence I apply sklearn.manifold.LocallyLinearEmbedding to find non-linear PCA components of the embeddings. Then I input the reduced-size embeddings to a Gaussian Random Projector . The random projections is the starting point of this methodology. From those random projections I keep only the positive values (>0) and then I create the hash buckets . Each bucket is a string representation like '1001', '0001', etc. Each document belongs to a single bucket. Now comes the tricky part. Every time a new document come I re-apply the steps above to assign it into a hash-bucket. The documents inside that bucket are considered its neighbors. I want to compute the cosine_similarity() of those neighbors (between the new document and the existing documents); and if the similarity is >0.9 then the new document is a duplicate of the document that achieved this similarity. I compute the cosine_similarity using the low-dimensional word embeddings (result of LocallyLinearEmbedding algorithm). However, I am not sure if this is the correct input to compare the two documents. If not, what else could be the proper input to generate meaningful cosine-similarities between a new document and its neighbors inside the same hash bucket?
