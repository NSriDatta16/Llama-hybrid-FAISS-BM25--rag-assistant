[site]: crossvalidated
[post_id]: 534790
[parent_id]: 
[tags]: 
Evaluating a multi-step forecasting model?

The literature is a bit confusing for me on this one, from what I understand, a great deal of papers evaluate multi-step forecasting models on a single forecasting horizon on the hold out set. It seems a bit silly to me to evaluate the whole model on a single forecasting horizon. But say I'm interested in evaluating a model on a 24 step forecasting horizon. My current way of doing it is training and hyperparameter tuning on a training set and then rolling window forecasting on the test set. My main question is, is this a valid way to evaluate my model? If not could somebody point me to a paper with a different evaluation scheme? A side question would be about a proper forecasting metric or more precisely, say you've chosen a metric for evaluating a single forecast horizon (MAE in my case, percentage errors are problematic for me since I have a large number of zero values in the time series), and then calculating the mean of the chosen metric over all forecast horizons. I've seen that approach used in a few places, alongside other MAE statistics, but I'm struggling to find a consensus in the literature. I know this question might be unfocused, but these two questions come in the same package and it would be nice to have them both answered in the same place.
