[site]: crossvalidated
[post_id]: 184194
[parent_id]: 184192
[tags]: 
or what is the general approach when the number of one class outrun the number of the other class in response variables? In general, most classifiers work by minimizing a cost function which can be adapted to take into account the imbalance. For instance, in SVMs the empirical risk to be includes a term L(y(x), t), which stands for the cost of classifying x as y(x) when the true label is t. If we have a problem where misclassifying A as B is different from misclassifying B into A (which is sometimes the case in imbalanced data), then we can specify those costs in L. (It's just we sometimes forget that this is possible because, by default, SVMs and other classifiers employ the 0-1 loss which gives the same penalty for misclassifying classes). In the case of SVMs, logistic regression and many other classifiers, this imbalanced loss function can be handled equivalently by re-weighting the data (i.e. using the "default" loss function but assigning weights to the different samples). Some known implementations of such algorithms allow specifying those weights as input. Now quick feedforward to random forests, according to this in R you can use the classwt parameter to set class weights in a random forest classifier.
