[site]: crossvalidated
[post_id]: 67788
[parent_id]: 67755
[tags]: 
What classifier are you working with? I would definetly try out a random forest model for your particular problem. It is an ensemble method that consists of a large amount of orthogonal trees. This is achieved by taking a bootstrap sample of your data and then using this to grow a clasification tree. This is done a predefined number of times and then the prediction is then the aggregation of the individual trees predictions, in this case it could be a majority vote.This procedure is called bagging (Breiman 1994). Furthermore the candidate variable for each split of each tree is taken from a random sample of all the available independent variables. This introduces even more variability and makes the trees more diverse. This procedure is called the random subspace method (Ho, 1998). If you were to fit a random forest classifier you can explicitly choose the size of the sample taken from each class to grow each of the many trees. So you could grow each tree from an independant sample with balanced classes. This can work extremely well for your kind of problem as the class diffrerence is not ovewhelming. An example of how to fit this model in R is as follows (supposing you have 3 independant variables): binaryclassifier Another thing that can come in handy here is that randomForest calculates an error measure on the fly. In this example, each tree is built on a 1000 observation bootstrap sample, meaning you left out 6000 observations. These 6000 observations are referred to as the out-of-bag sample and can be used to validate the model as the forest is progressively grown. You can finally even calculate the confusion matrix for the model based on this procedure: binaryclassifier$confusion You could play with sampsize a bit to, maybe, obtain an OOB accuracy you are happy with. Note that the sampsize doesnt have to be perfectly balanced for this to happen.
