[site]: crossvalidated
[post_id]: 475405
[parent_id]: 
[tags]: 
Approximately how many data do we need to train a artificial neural network with 2000 parameters?

My approach is to think about ANN like a common statistical model. 2000 data for 2000 parameter is clearly not enough. However, if we got 10k data points then the training result might start to become meaningful. Is it correct? In my case, the output can be simplify to a simple 0,1 classification. I've recent seen an empirical rule stating that a sample size of number of parameters squared is roughly needed to train a neural network. Are there any sources for this empirical rule?
