[site]: crossvalidated
[post_id]: 126142
[parent_id]: 126056
[tags]: 
In my opinion, Cox-Jaynes interpretation of probability provides a rigorous foundation for Bayesian probability: Cox, Richard T. " Probability, frequency and reasonable expectation ." American Journal of Physics 14.1 (1946): 1–13. Jaynes, Edwin T. Probability theory: the logic of science . Cambridge University Press, 2003. Beck, James L. "Bayesian system identification based on probability logic." Structural Control and Health Monitoring 17.7 (2010): 825–847. The axioms of probability logic derived by Cox are: (P1): $\Pr[b|a]\ge0$ (by convention) (P2): $\Pr[\overline{b}|a]=1-\Pr[b|a]$ (negation function) (P3): $\Pr[b\cap c|a]=\Pr[c|b\cap a]\Pr[b|a]$ (conjunction function) Axioms P1-P3 imply the following [Beck, 2010]: (P4): a) $\Pr[b|b\cap c] = 1$ ; b) $\Pr[\overline{b}|b\cap c] = 0$ ; c) $\Pr[b|c]\in[0,1]$ (P5): a) $\Pr[a|c \cap (a \Rightarrow b)]\le\Pr[b|c\cap(a \Rightarrow b)]$ , b) $\Pr[a|c\cap(a \Leftrightarrow b)] = \Pr[b|c\cap(a \Leftrightarrow b)]$ , where $a \Rightarrow b$ means that $a$ is contained in $c$ , and $a \Leftrightarrow b$ means that $a$ is equivalent to $b$ . (P6): $\Pr[a \cup b|c] = \Pr[a|c]+\Pr[b|c]-\Pr[a\cap b|c]$ (P7): Assuming that proposition $c$ states that one and only one of propositions $b_1,\ldots,b_N$ is true, then: a) Marginalization Theorem: $\Pr[a|c]=\sum_{n=1}^N P[a \cap b_n|c]$ b) Total Probability Theorem: $\Pr[a|c] = \sum_{n=1}^N \Pr[a|b_n\cap c]\Pr[b_n|c]$ c) Bayes' Theorem: For $k=1,\ldots,N$ : $\Pr[b_k|a\cap c] = \frac{\Pr[a|b_k\cap c]\Pr[b_k|c]}{\sum_{n=1}^N \Pr[a|b_n\cap c]\Pr[b_n|c]}$ They imply Kolmogorov's statement of logic, which can be viewed as a special case. In my interpretation of a Bayesian viewpoint, everything is always (implicitly) conditioned on our believes and on our knowledge. The following comparison is taken from Beck [2010]: The Bayesian point of view Probability is a measure of plausibility of a statement based on specified information. Probability distributions represent states of plausible knowledge about systems and phenomena, not inherent properties of them. Probability of a model is a measure of its plausibility relative to other models in a set. Pragmatically quantifies uncertainty due to missing information without any claim that this is due to nature's inherent randomness. The Frequentist point of view Probability is the relative frequency of occurrence of an inherently random event in the long run . Probability distributions are inherent properties of random phenomena. Limited scope, e.g., no meaning for the probability of a model. Inherent randomness is assumed, but cannot be proven. How to derive Kolmogorov's axioms from the axioms above In the following, section 2.2 of [Beck, 2010] is summarized: In the following we use: probability measure $\Pr(A)$ on subset $A$ of a finite set $X$ : [K1] $\Pr(A)\ge 0, \forall A \subset X$ [K2]: $\Pr(X) = 1$ [K3]: $\Pr(A\cup B)=\Pr(A)+\Pr(B), \forall A,B \subset X$ if $A$ and $B$ are disjoint. In order to derive (K1–K3) from the axioms of probability theory, [Beck, 2010] introduced propositon $\pi$ that states $x\in X$ and specifies the probability model for $x$ . [Beck, 2010] furthermore introduces $\Pr(A) = \Pr[x\in A|\pi]$ . P1 implies K1 with $b=\{x\in A\}$ and $c=\pi$ K2 follows from $\Pr[x\in X|\pi]=1$ ; P4(a), and $\pi$ states that $x\in X$ . K3 can be derived from P6: $A$ and $B$ are disjoint means that $x\in A$ and $x\in B$ are mutually exclusive. Therefore, K3: $\Pr(x\in A\cup B|\pi)=\Pr(x\in A|\pi)+\Pr(x\in B|\pi)$
