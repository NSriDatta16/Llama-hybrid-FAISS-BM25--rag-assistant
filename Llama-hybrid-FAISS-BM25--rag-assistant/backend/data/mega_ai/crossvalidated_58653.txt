[site]: crossvalidated
[post_id]: 58653
[parent_id]: 58644
[tags]: 
In the context of multiple linear regression, "adjusting" for a covariate simply means including it as an explanatory variable. There is an equivalent way of understanding multiple linear regression that provides insight into this question. To regress, say, $Z$ on $X$ and $Y$, we may (arbitrarily) select one of the explanatory variables (let it be $X$) and Separately regress $Z$ on $X$ and $Y$ on $X$, producing residuals $Z_X$ and $Y_X$, respectively, then Regress $Z_X$ on $Y_X$. The first step "takes $X$ out of both $Y$ and $Z$." Its interpretation and legitimacy are the same as for any ordinary regression. The generalization to more than two explanatory variables should be clear. After step 1, you do not have to proceed to step 2: you may study the relationships among the residuals in any ways you please, including PCA. The residuals, by construction, will be orthogonal to a constant (they will have zero means) and also orthogonal to the covariates that have been "left out" ("adjusted for"). You can adjust for interactions and nonlinear terms in the same way: create variables for them and include those in the variables that are taken out. I will illustrate with R code. set.seed(17) # # Create independent variables. # n.rows
