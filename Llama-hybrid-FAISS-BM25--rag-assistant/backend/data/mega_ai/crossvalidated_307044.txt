[site]: crossvalidated
[post_id]: 307044
[parent_id]: 307032
[tags]: 
What you're basically trying to do is convert your dataset from a time-like structure to a feature-like one. I once tried to do the same with random forest, i.e. add extra features on top of lagged terms as inputs. I tried several different things, notably ways of detecting seasonality, cyclicity, and trend, and then transforming them into features (I was successful only to some extent). With neural networks, however, you don't need to do any of those things. As described in this book chapter (and as I have experienced myself), neural network autoregression works quite well without adding any extra time-based features on top of lagged variables. As explained in section "neural network autoregression", this method can be efficient without any time-based features because it emulates a nonparametric $AR(p)$ model (or a $ARIMA(p,0,0)$), therefore the time-dependent information is extracted from the lagged terms alone. For a network of $p$ lagged input variables, $m$ hidden neurons in a single hidden layer, and the identity activation function $g(x) = x$, you get a model like this: $$\hat y_t = w_0 + (\sum_i^m w_i ) \sum_k^p w_k y_{t-k} $$ Which is, well, equivalent to an $AR(p)$ or $ARIMA(p,0,0)$ process of the form: $$ \hat y_t = c+ \sum_k^p \phi_k y_{t-k} $$ I should probably also point out that the implementation described in the book applies forecast aggregation (trains multiple networks and averages their prediction). If you want something more "advanced" with a single neural network, you should look into LSTM Networks. With your methods you're increasing computational complexity without any guaranteed payoff, and that's never a good thing.
