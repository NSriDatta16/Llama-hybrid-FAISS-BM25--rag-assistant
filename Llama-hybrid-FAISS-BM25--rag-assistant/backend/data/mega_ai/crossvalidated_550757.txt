[site]: crossvalidated
[post_id]: 550757
[parent_id]: 550731
[tags]: 
In logistic regression we are not 'making the regression linear'. Instead, the model is non-linear. If we assume Bernoulli distribution for the observations with probability $p$ then the model is $$p(x) = \frac{1}{1+e^{-(a+bx)}}$$ There is a linear part $a+b x$ , but we do not 'make the regression linear'. Or at least, we do not make the regression linear in the sense of transforming the outcomes and solving it as a linear equation. This is for instance done by Microsoft's Excel when it fits an exponential curve by transforming the observed $y$ and solves it with a single linear regression. Below is an example of the difference between the linearized fit and a non-linear fit. The linearized fit is optimizing the errors in the right graph. Linearization in generalized linear models To be fair, In logistic regression, a special case of a generalized linear model, there is some form of linearization in another sense. What is that form and why is that done? To get to the solution of the non-linear problem an algorithm is used that approaches the solution iteratively and in each step it solves a 'linearized' form of the equations. The reason to linearize is because it creates a simple way to find a solution. Below we see this illustrated in an image related to gradient descent (from here ). The optimization of the non-linear function is done in steps and the solution is found by following the gradient of the cost function, which gives a linear function instead of a non-linear function. The solution of that linearized function takes us into the direction of the arrow, and by following multiple steps we follow along the path getting closer to the final solution.
