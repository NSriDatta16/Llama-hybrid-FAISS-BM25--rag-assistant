[site]: crossvalidated
[post_id]: 591068
[parent_id]: 591065
[tags]: 
Your datasets sounds at a superficial level reasonable large, so I would normally expect some value from hyperparameter tuning and in small datasets the right amount of regularization can be rather important. The standard settings of sklearn.XGBClassifier can of course also be reasonably decent at times, so it is possible you may sometimes not be able to do much better. I'll assume that you already have a good evaluation set-up and are evaluating whether performance improves in an appropriate manner (e.g. via some appropriate cross-validation, where appropriate is very problem dependent see e.g. this blog ). With that caveat, I have three main candidates for what you could do differently: My first suspicion is whether you are tuning the right hyperparameters. E.g. when you look at a Kaggle masterâ€™s default XGBoost tuning strategy or what the optuna LightGBMTunerCV does (yes, I know that's LightGBM, but there's massive similarities between the algorithms and what hyperparameters matter for them), you can see what they focus on. You'll notice that at least tuning subsample and colsample_bytree (and/or by level) are generally thought to be pretty important to tune. I'd focus on those parameters (note that they tune them in a fixed sequence, which can work pretty well, but you can usually do better by searching without that, it will just take much longer). You can also explore some additional ones that may help e.g. L1 or L2 regularization. My first suspicion is around the n_estimator choice. Setting the learning rate to a fixed low value is usually a good approach and you certainly don't need to tune that, but you then need to make sure you use enough trees (aka estimations). I.e. make sure you look at a wide enough range for n_estimator , where values substantially higher than 1000 may be appropriate (e.g. up to 10000 or even higher). Generally, the lower you make the learning rate, the better the final model (there's drastically diminishing returns, I've never gone below 0.005) and the higher the number of trees that you need. Also make sure to run enough experiments for your optuna search, but there's usually diminishing returns beyond a few thousand experiments.
