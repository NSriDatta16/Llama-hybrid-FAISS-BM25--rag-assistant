[site]: datascience
[post_id]: 38259
[parent_id]: 38258
[tags]: 
As you have mentioned, the output of linear regression is a real value while logistic regression's represents classes(classification). Their main difference is this. The loss function of linear regression is convex which means you always can find the optimal point using customary optimisations while if you use that for logistic regression, you may get stuck in a non-global minimum which is not optimal. Consequently, people take logarithm of that loss and call it cross entropy. For simple logistic regression tasks, it is convex. Another difference that has to be cared about is the non-linearity which is usually applied for different tasks. For logistic regression, it is customary that people use non-linearites like Tanh or Sigmoid after the linear part, inner product of weights and inputs, for specifing the similarity to class 1 or 0 for typical binary classifications. For linear regression, people usually use Linear activation function. There is a point here. The idea of using the linear activation is not due to the need for employing a linear function. It is used because its output is not limited . Consequently, you can use other functions that are one-to-one and are not limited. Consider $y = x^3$ as an example.
