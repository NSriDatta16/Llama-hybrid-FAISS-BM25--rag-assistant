[site]: datascience
[post_id]: 38768
[parent_id]: 38766
[tags]: 
Not a complete answer, but was too long for a comment. I always first try to see how the default parameters perform. Then from the documentation or some reading, you can see what is each parameter global influence (by influence I mean maybe increasing parameter X means complexifying the model, or parameter Y means increasing the convergence speed towards a solution). Depending on the first result you get, pick up one parameter, the one that seem to have the most influence on the model, and make it vary a bit in the way that make sense from your first results. If things improve on the validation set, keep moving the value this way, if not do the opposite. Often times you get good results without tuning every single parameter. This is a method by hand, it is not optimal. But as you precise that you are a beginner in machine learning, I believe it is the best way to learn to "feel" what usually impact the performance of an algorithm as Xgboost and what impacts less and that therefore can be overlooked for a primary coarse tuning. https://xgboost.readthedocs.io/en/latest/parameter.html has some nice pieces of information about what parameter impacts what. Don't hesitate to ask more precise questions about some specific parameters if you need :)
