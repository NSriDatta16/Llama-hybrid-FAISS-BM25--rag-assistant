[site]: crossvalidated
[post_id]: 264028
[parent_id]: 
[tags]: 
Why not a correlation-check with outcome variable to select features for regression

Just an early warning before you read the rest, I am probably missing some theory behind it and I have lack of knowledge that needs to be filled with this question. So, what I will say might be very basic and may sound stupid! As far as I see, in machine learning tasks, a common approach is to generate a bunch of features that could possible tell something about the outcome variable ( y ) and therefore contribute to the prediction power of a regression model, without checking if they indeed significantly correlate with y . I wonder why one would not use correlation to filter some features before including anything in a regression model. In simple logic, if two things are not significantly associated, then why on earth one should predict the other? Or, including a bunch of features is common since many people for example count on some regularization methods that sort of performs an internal feature selection?
