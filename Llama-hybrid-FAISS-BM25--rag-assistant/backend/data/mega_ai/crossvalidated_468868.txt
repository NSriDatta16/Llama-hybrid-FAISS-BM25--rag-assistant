[site]: crossvalidated
[post_id]: 468868
[parent_id]: 466551
[tags]: 
Using PCA to project the data to a lower dimension can yield worse results. This is as true for neural networks as it is for any other model. This is because the projection to a lower dimension is not aware of the outcome. All it does is retain the PCs with the largest variance; if the informative features PCs with lowest variance, then PCA will make the model worse because you're excluding the signal. Examples of PCA where PCs with low variance are "useful" Do PCA affect different classification methods? Is PCA always recommended? On the other hand, if you only use PCA to rotate the data but not project it to lower dimension, PCA can be useful because it improves the optimization dynamics. In "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", Sergey Ioffe and Christian Szegedy suggest that whitening transformation are helpful during the optimization steps. It has been long known (LeCun et al., 1998b; Wiesler & Ney, 2011) that the network training converges faster if its inputs are whitened â€“ i.e., linearly transformed to have zero means and unit variances, and de-correlated. How to select features for inclusion depends on the domain. The best feature selection is to use domain knowledge to eliminate features which are not related to the outcome. For tabular datasets where domain knowledge is unavailable, generic feature selection methods like boruta can be helpful, even though they're not foolproof. There's always some risk that you'll leave out important features, or include irrelevant features. Also, the results obtained from Boruta will depend on the model you use to measure feature importance, which introduces additional knobs to turn. See also: Are dimensionality reduction techniques useful in deep learning
