[site]: crossvalidated
[post_id]: 464052
[parent_id]: 
[tags]: 
Derivative of softmax function as a matrix

I have a generalised n-layer neural network. Currently, I am using it to perform digit classification (on the MNIST dataset), using a softmax + cross-entropy loss setup with simple stochastic gradient descent (for now). $$\text{Terminology: } y\rightarrow\text{label},\, z\rightarrow\text{pre-activation vector}, \, \hat{y}\rightarrow\text{output vector (after applying softmax)} $$ To calculate $\frac{\partial E}{\partial z}$ , I need to find $\frac{\partial E}{\partial \hat{y}}\frac{\partial \hat{y}}{\partial z}$ I am calculating the derivatives of cross-entropy loss and softmax separately. However, the derivative of the softmax function turns out to be a matrix, while the derivatives of my other activation functions, e.g. $\text{tanh}$ , are vectors (in the context of stochastic gradient descent), since in those cases, $\frac{\partial \hat{y} _i}{\partial z_j} = 0$ . This mismatch of dimensions cannot be resolved in my generic implementation. One method I have for circumventing this is to convert the derivatives of all the other activation functions from vectors to diagonal matrices using, say, numpy.diagflat(v) . Then, I can compute $\frac{\partial E}{\partial z}$ as a matrix multiplication between $\frac{\partial E}{\partial \hat{y}}$ and $\frac{\partial \hat{y}}{\partial z}$ . Does this method seem correct? And is the original formula implied to be a matrix multiplication anyway? Note : I know how to compute the derivative of the loss function with respect to $z$ directly , however my implementation cannot support this.
