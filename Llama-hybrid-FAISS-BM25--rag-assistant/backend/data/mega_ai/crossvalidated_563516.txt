[site]: crossvalidated
[post_id]: 563516
[parent_id]: 
[tags]: 
Convergence to a distribution when combining random samples

By running an experiment, I get a set of positive real numbers, from which I can draw a histogram or any other density estimate. I can repeat this experiment multiple times; I expect different yet similar sets due to stochastic variations in the data generation process. By stacking together several sets, I can remove meaningless fluctuations before further treatment, e.g. distribution fitting. For instance, on the figure, I plot ECDFs for three independent realisations (coloured lines) and the "average" ECDF for 60 datasets (black line). I am interested in criteria for establishing the convergence of such a stacking. E.g., does combining 50 original sets yield a significantly different distribution, compared to combining 40 ? Or 20 ? If I already have $n-1$ sets, should I expect a different distribution if I had a $n$ th set? I am thinking of looking at the evolution of the two sample KS statistics, where the samples are combinations of $n$ and $n+1$ sets. It should (and it does) decrease when $n$ increases. Is that a sound approach? Are there more robust ones?
