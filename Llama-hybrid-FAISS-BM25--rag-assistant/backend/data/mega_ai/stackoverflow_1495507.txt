[site]: stackoverflow
[post_id]: 1495507
[parent_id]: 1495363
[tags]: 
The answer to your question is to use Disallow: /?q= The best (currently accessible) source on robots.txt I could find is on Wikipedia . (The supposedly definitive source is http://www.robotstxt.org , but site is down at the moment.) According to the Wikipedia page, the standard defines just two fields; UserAgent: and Disallow:. The Disallow: field does not allow explicit wildcards, but each "disallowed" path is actually a path prefix; i.e. matching any path that starts with the specified value. The Allow: field is a non-standard extension, and any support for explicit wildcards in Disallow would be a non-standard extension. If you use these, you have no right to expect that a (legitimate) web crawler will understand them. This is not a matter of crawlers being "smart" or "dumb": it is all about standards compliance and interoperability. For example, any web crawler that did "smart" things with explicit wildcard characters in a "Disallow:" would be bad for (hypothetical) robots.txt files where those characters were intended to be interpreted literally.
