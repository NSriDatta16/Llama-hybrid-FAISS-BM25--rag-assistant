[site]: crossvalidated
[post_id]: 143128
[parent_id]: 143094
[tags]: 
I hope that understood your situation and the question correctly. Considering your data set's number of distributions, visual exploratory approaches (such as QQ plots, which you mentioned) are not feasible in this case. Therefore, you have to resort to analytical approaches , such as goodness-of-fit (GoF) tests, as some have already mentioned in the comments above. Since you have informed that distribution parameters are estimated from data , I assume that you have used or plan to use one of distribution fitting approaches . One of the most popular fitting approaches (along with least squares , to a lesser degree ) is maximum likelihood estimation (MLE) , which is generally easy to perform, for example, using function fitdistr() from R package MASS . However, depending on your particular data, fitting via fitdistr() might not be so trivial . Some people prefer R package fitdistrplus , as they consider it more advanced or useful . After this straightforward step, you need to validate the estimation results , using one or more of the following GoF tests for continuous data (considering their pros and cons): chi-square (via binning), Kolmogorov-Smirnov (via corrected tables for critical values or Monte Carlo simulation , which I'm listing here just for completeness, as you are trying to avoid this), Anderson-Darling, Lilliefors, Cramér–von Mises and Watson. In terms of performance , the problem gets reduced to performing a relatively large number of non-parametric GoF tests, which IMHO is achievable either via doing it on a more powerful hardware (i.e., renting Amazon EC2 virtual instance ), or via parallelizing code . Returning to the essence of your question, my idea of possible approaches is to aggregate results either via bootstrapping (similarly to the one presented in this excellent answer ), or some kind of averaging approach , similar to ensemble methods (for example, take a look at this research paper ).
