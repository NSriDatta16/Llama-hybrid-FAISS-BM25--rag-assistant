[site]: crossvalidated
[post_id]: 105201
[parent_id]: 
[tags]: 
Estimating a first order plus dead time model

The data generating process is given by the following differential equation: $y(t) = a + b u(t - \theta) + c \frac {dy} {dt}$ Now imagine having as data a long time series for both $y$ and $u$. If $\theta$ is known, this is an easy linear regression. But is there a "correct" procedure when the delay $\theta$ is unknown? Right now I treat it as a model-selection problem, running multiple regressions and choosing the one that predicts with the smallest error. That is clunky though, so if a better method exists, great! To clarify: $u(\cdot)$ is a function. It is an arbitrary stream of inputs $a$,$b$,$c$ and $\theta$ are parameters. Fixed and unknown. You can assume a gaussian $\epsilon$ measurement error whenever $y(t)$ is observed. The only data I have is a long series of daily observations of the pair $(u,y)$, where $u$ is observed perfectly. I feed $y_{t}-y_{t-1}$ for the derivative, which is appropriate since the original model is really a difference equation.
