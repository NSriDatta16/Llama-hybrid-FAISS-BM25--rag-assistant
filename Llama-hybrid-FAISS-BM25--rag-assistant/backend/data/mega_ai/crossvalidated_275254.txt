[site]: crossvalidated
[post_id]: 275254
[parent_id]: 275189
[tags]: 
As you correctly identified this pipe-line introduces over-fitting, this over-fitting is due to data-leakage and thus renders the current form of this particular modelling pipe-line unusable. Let's see why data-leakage happens: When feeding the class probability predictions from the NB model as a feature into the RF model, despite the fact that the NB probabilities where fitted without using features available to the RF, you pass information about the predicted labels to the RF model. This information should be unavailable to the RF at the time of prediction or training. This pipe-line is therefore flawed so no amount of noise will fix it. Unfortunately data-leakage references as sparse, " Leakage in Data Mining: Formulation, Detection, and Avoidance " by Kaufman et al. and " Ten Supplementary Analyses to Improve E-commerce Web Sites " by Kohavi and Parekh are nice reads. In general the works of S. Rosset and R. Kohavi seem to touch upon this matter more seriously. I think you have essentially worked out a basic model-stacking procedure but unfortunately you have mixed up the dataset split. Model stacking is a staple technique for most data science competitors because it allows combining different models together; Kagglers have some very good tutorials eg. here . A canonical reference on stacking is Wolpert's " Stacked generalization "; " Combining Estimates in Regression and Classification " by LeBlanc and Tibshirani is pretty important too. In short, the class probability predictions from a base classier (the NB model here) are treated as meta-features thus allowing the top classifier (the RF model here) to use them directly as highly relevant features. Importantly, these meta-features have to derived without using information (in terms of predictors variables $X$ as well as target values/labels $y$) that will be used in the fitting procedure of the top classier. The simplest way of ensuring data-leakage will be to split the original training dataset to two training datasets; one to be used by the base and one by the top classifier. The overall performance will be estimated as always on a separate test dataset.
