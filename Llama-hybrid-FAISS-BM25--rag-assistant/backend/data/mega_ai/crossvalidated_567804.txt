[site]: crossvalidated
[post_id]: 567804
[parent_id]: 567017
[tags]: 
I assume that by alternating minimization you mean optimizing over one layer while keeping the others fixed, and then repeating this process. This is also called block coordinate descent (coordinate descent being the setting in which we minimize over each variable one at a time). A critical flaw of coordinate descent methods is that they need not converge to a stationary point. See the limitations section of this Wikipedia article on Coordinate Descent . This is worse that the situation for stochastic gradient descent, where you converge to a local minima. This is one reason why coordinate descent isn't used to train neural networks--even in small examples. It only works in special cases, and the gnarly objective functions produced when training neural networks isn't one of those cases.
