[site]: crossvalidated
[post_id]: 96475
[parent_id]: 96420
[tags]: 
In the simplest linear case, a state-space model is composed of two equations, the state equation (or transition equation), $$ \boldsymbol{\theta_t} = \boldsymbol{T_t}\boldsymbol{\theta_{t-1}} + \boldsymbol{R_t}\boldsymbol{\nu_t}$$ and the observation equation, $$\boldsymbol{Y_t} = \boldsymbol{F_t}\boldsymbol{\theta_t} + \boldsymbol{\epsilon_t}.$$ Vector $\boldsymbol{\theta_t}$ is termed the state-vector, and embodies all information that describes the state of the system at moment $t$. It evolves over time following the dynamics given by the state-equation. The state vector is in principle made of latent, unobservable variables; what you do observe is $\boldsymbol{Y_t}$, which are linear combinations of elements of the state vector corrupted by noise $\boldsymbol{\epsilon_t}$. The specification is completed with the distribution of the state and observation noises ($\boldsymbol{\nu_t}$ and $\boldsymbol{\epsilon_t}$), usually gaussian distributions in the linear case. Now, if you consider the linear time-invariant case ($\boldsymbol{T_t} =\boldsymbol{T}, \boldsymbol{F_t} = \boldsymbol{F}$ for all $t$), you have something very close to what you want; you would estimate the needed parameters, in case there are some. The best estimator of $\boldsymbol{Y_t}$ would be $\boldsymbol{F}\boldsymbol{\hat\theta_t}$ and $\boldsymbol{\hat\theta_t}$ can be estimated in terms of $\boldsymbol{Y_{t-1},\ldots}$ using the Kalman filter. This is only a telegraphic account. For a good discussion see for instance Durbin and Koopman or Anderson and Moore among many books on state-space time series methods.
