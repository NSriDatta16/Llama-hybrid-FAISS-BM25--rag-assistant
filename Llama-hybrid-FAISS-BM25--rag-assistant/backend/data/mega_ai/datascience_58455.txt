[site]: datascience
[post_id]: 58455
[parent_id]: 33117
[tags]: 
Features are the information of your model. The more the information, the better will it be able to perform and predict. The lesser of it, the harder to predict values. So the short naser is yes. It is always worth to have as many features as possible. There is always a limit to this though since an information overload too might burn your processor, so be careful of how many features are being engineered. Also, unnecessary features only add to the burnout, so it's always good practise to clean up certain features. The whole data preprocessing phase is about that. The first answer has some good details about it. As far as stopping a cycle is concerned, well there are several measures and factors that you need to be aware of to check where your model has stopped performing better and those are measures like the RMSE. A simple example will be using xgboost regression on your data and specifying the number of cycles. Run the model and you will get the RMSE for each cycle. It will decrease to a limit after which you'll be able to deduce that the model has plateaued after a certain cycle. This is how model tuning and optimisation works.
