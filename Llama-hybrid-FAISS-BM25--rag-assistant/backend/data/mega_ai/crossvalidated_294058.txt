[site]: crossvalidated
[post_id]: 294058
[parent_id]: 
[tags]: 
Stochastic gradient descent and sampling

By sampling, I mean that you have an original dataset $D$ and you extract randomly a sub-dataset $D_p$: you keep each line independantly with probability $p$. You then run the training on $D_p$ instead of $D$. $p$ is often 1/10, 1/100... Sampling may degrade the quality and is done mainly save storage resources, read/transfer time... I'm mainly focused on big datasets (millions of lines or more). Is there any theoretical/heuristic property that relates the appropriate parameters to choose (step size, number of iterations...) to $p$ in stochastic gradient descent (SGD) for regression? Is it possible to understand how the behaviour of the algorithm changes with sampling? I'm open to answers not strictly related to SGD: other cases where the connection between sampling and training is understood.
