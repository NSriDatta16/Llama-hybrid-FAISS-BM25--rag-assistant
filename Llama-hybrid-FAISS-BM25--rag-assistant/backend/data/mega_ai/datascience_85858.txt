[site]: datascience
[post_id]: 85858
[parent_id]: 85855
[tags]: 
First, let's reason why positional embeddings are needed at all: A multi-head attention layer of the Transformer architecture performs computations that are position-independent . This means that, if the same inputs are received at two different positions, the attention heads in the layer would return the same value at the two positions. Note that this is different from LSTMs and other recurrent architectures which, apart from the input, receive the state from the previous time step. The role of positional embeddings is to supply information regarding the position of each token. This allows the attention layer to compute results that are context-dependent, that is, two tokens with the same value in the input sentence would get different representations. Second, let's clarify why having a fixed formula to compute the positional embeddings: Positional embeddings can be handled as "normal" embedding matrixes and therefore can be trained with the rest of the network. These are "trainable positional embeddings". With this kind of positional embeddings, after each training step, the positional embedding matrix is updated together with the rest of the parameters. However, we can obtain the same level of performance (translation quality, perplexity, or whatever other measure being used) if, instead of training the positional embeddings, we used the formula proposed in the original transformer paper. This saves us from having to train a very big embedding matrix. Now, about why using the "embedding size dimension" in the formula: We need different values in each position of the embedded vector. Having the same value in each position of the vector would leave us with an "effective" embedding size of 1, as we are wasting the other $d-1$ positions. In order to compute different values for each position of the embedded vector, we need an independent variable that we use to compute the value at each position based on it. We don't have any other suitable variable but the position itself. That's why it is used in the formula. Old answer: Making the embedding vector independent from the "embedding size dimension" would lead to having the same value in all positions, and this would reduce the effective embedding dimensionality to 1. The formula uses the embedding size dimension to be able to provide different values within each embedded vector.
