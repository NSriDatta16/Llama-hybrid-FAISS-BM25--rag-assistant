[site]: crossvalidated
[post_id]: 469268
[parent_id]: 454633
[tags]: 
Boruta, based on RF, doesn't care about scale. It permutes a column several times and looks at the distribution of permuted importance vs. the non-permuted. It only labels as reject those columns that are more likely in the distribution than not. Things that are questionable are retained, but have an estimated importance value. Boruta looks at nonlinear interactions in its importance estimate. The regularization does not necessarily do that. It can allow you to set columns to zero before applying the regularization, so it acts as if you set a smaller regularization parameter. So I'm borrowing images from this site: https://kevinbinz.com/2019/06/09/regularization/ You can think of regularized regression like this: In the image, the point at the center of the ellipse is the non-regularized fit (metaphor). You can see that the values for the 2 axes are both non-zero. For the L1 regularized, preference is given to values that are along the axis AND closer to zero, aka where the other axes tend to be zero. You can also see that in L2 regularization they only want points whose values are closer to zero. In an exercise of fun they also provide this: Update : Some folks say you can take the power of a random forest and encode it into a weighted linear model ( link , link ) if you use the RF weights in it. You could take Boruta feature importance for retained features, and get an effect like Lasso, where some parameter values are set to zero, and an effect like Ridge where the parameter values are regularized. It should act like a robust hybrid of both. It might be interesting to compare it to a Huber-regularization analog of Ridge( link , link ).
