[site]: crossvalidated
[post_id]: 354578
[parent_id]: 354562
[tags]: 
In order to make the agent knows that the env would behave it own ways regardless what the agent does, what would I do? You can technically use RL to model this problem, but since both immediate reward r and next rewards r' do not depend on agents' actions, there is no chance that state-action transitions will learn to maximize long-term reward R . should x_1, x_2, ... be in the observation space too? If adding these factors contribute to better convergence (learning to maximize R ), then they should be added. It can be that all their information has been summarized to the continuous variable, and then they are abundant. If no, what would I do next beside RNN? What is that?
