[site]: crossvalidated
[post_id]: 467573
[parent_id]: 
[tags]: 
Do random forests offer advantages over elastic net regression for variable selection?

I have a dataset with 13 predictors and 330 observations (if need be, this can be combined with a second data set which we originally meant to use as replication data for a total of 550 observations). In an exploratory analysis, I would like to build a model which includes the predictors that account for a meaningful portion of the variance in my outcome variable. Importantly, this should be a model that is interpretable, that is, while we do want to maximize variance explained, it is also important for us to know which predictors went into the model and what their individual contribution is. Elastic net regression seems like a good choice, but I have also seen approaches which first build random forests and then plug the selected variables into a regression model. I understand that random forests can be advantageous when the data contain non-linear associations and because they can handle multicollinearity better compared to classical regression models. As a beginner to machine learning approaches, is there anything else I should be aware of? Do random forests have other advantages (or disadvantages) over elastic net regression for this kind of a problem?
