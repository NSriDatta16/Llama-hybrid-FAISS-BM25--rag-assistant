[site]: crossvalidated
[post_id]: 339200
[parent_id]: 
[tags]: 
Summation of an infinite series involving a gamma function, in the context of estimating a Dirichlet prior

I have an unknown multinomial distribution $P^*$ over potentially unbounded set $\Sigma=\{1,2,\ldots,L\}$ from which a training set $\{x^1,\ldots,x^N\}$ has been observed. The observations form the set $\Sigma^o\subseteq \Sigma$, and $k^o=|\Sigma^o|$. I need to predict the probability of drawing a symbol $i\in\Sigma$ on the $N+1$th observation. Based on some mathematical derivations in [1], one step in this process involves computing $$\Sigma_{k'\ge k^o} m_k$$ where $$m_k=P(S=k)\frac{k!}{(k-k^o)!}\cdot\frac{\Gamma(k\alpha)}{\Gamma(k\alpha+N)}$$ Here, $S$ is the size of the unknown subset of $\Sigma$ from which all values will actually be observed. $P(S=k)$ is a chosen prior such as $\beta^k$ or $k^{-\beta}$, and $\alpha$ is a hyperparameter related to the Dirichlet distribution. I'll be tuning $\alpha$ and $\beta$ and determining which distribution to use for $P(S=k)$ through experimentation using real data. What I need to know is how to take the above summation. The terms of course decrease until they are negligibly small, and the paper suggests determining when to stop adding new terms by computing ln($m_k$) for values of $k\ge k^o$ until I obtain values that are "signficantly smaller than the largest value beforehand." I'm not sure how to interpret "significantly smaller." For my values of $N$ and $k^o$, which are an order of magnitude greater than those used in the paper, I'm finding that the log term seems to decrease linearly with $k$. So, I'm wondering if there's some intuition for a good heuristic to use here, or if there's an analytic solution for a related summation that may be of use for understanding this sum. [1] Friedman & Singer. "Efficient Bayesian Parameter Estimation in Large Discrete Domains." 1999. http://ai.stanford.edu/users/nir/Papers/FS1.ps
