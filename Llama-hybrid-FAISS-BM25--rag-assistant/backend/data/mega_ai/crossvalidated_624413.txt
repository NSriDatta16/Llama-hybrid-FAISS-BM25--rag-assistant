[site]: crossvalidated
[post_id]: 624413
[parent_id]: 623850
[tags]: 
This is tricky, because it depends in part on (i) what you mean by importance, and (ii) what you want out of such a comparison. To the first point, there are many ways to convert the somewhat vague notion of importance into a metric, and different metrics often provide different rankings. To the second, are you interested comparing the importance between predictors within the fitted models (which is what most importance metrics are designed for) or perhaps the unique contribution to predictive power between models ? If you are interested in comparisons within the fitted models, then the permutation importance method often used for random forests are a good solution that should work for any kind of model. If you are interested in comparing importance (in the sense of 'improves predictive power of the model') between models, a simple alternative could be to compare predictive performance of a full model with all predictors with that of a reduced model with one predictor dropped (do this separately for all model types). The reduction in performance when the predictor is dropped, relative to the full model, tells you how much that predictor contributed uniquely. This value can be compared between the models. Note that either method will lead to strange interpretations if you have multicollinearity (strong correlations between predictors). If your overall model performance is excellent but the predictors are correlated, the procedures above could indicate that none of the predictors is important. This is because the information each predictor contains is also contained within the other predictors, so dropping that predictor will need to negligible loss in overall loss in performance. Permutation importance should be less susceptible to this but is not immune. There are some solutions that have been proposed for this in specific algorithms 1 but I don't know if there is any consensus about what the best general approach to the problem is. So in general - but especially if you have multicollinearity - it's worth carefully thinking through exactly what you want out of any importance metric and from the comparison before choosing your approach. And note Frank Harrell's valuable point in the comment below - any metric you choose will have an associated (and often large) uncertainty, even though this is usually ignored. 1 https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-9-307
