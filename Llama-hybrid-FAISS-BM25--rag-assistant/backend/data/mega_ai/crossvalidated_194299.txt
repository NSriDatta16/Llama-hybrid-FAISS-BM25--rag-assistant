[site]: crossvalidated
[post_id]: 194299
[parent_id]: 194035
[tags]: 
I think one of the biggest questions, as a statistican, you have to ask yourself is whether or not you believe in, or want to adhere to, the likelihood principle. If you don't believe in the likelihood principle then I think the frequentist paradigm to statistics can be extremely powerful, however, if you do believe in the likelihood principle, then (I believe) you most certainly have to espouse the Bayesian paradigm in or to not violate it. In case you are unfamiliar with it, what the likelihood principle tells us is the following: The Likelihood Principle : In making inferences or decisions about $\theta$ after some data $\mathbf{x}$ is observed, all relevant experimental information is contained in the likelihood function : $$\ell(\theta;\mathbf{x})=p(\mathbf{x}|\theta)$$ where $\mathbf{x}$ corresponds to the data observed and is thus fixed. Furthermore, if $\mathbf{x}$ and $\mathbf{y}$ are two sample points such that $\ell(\theta;\mathbf{x})$ is proportional to $\ell(\theta;\mathbf{y})$, that is, there exists a constant $C(\mathbf{x},\mathbf{y})$ such that $$\ell(\theta;\mathbf{x})=C(\mathbf{x},\mathbf{y})\ell(\theta;\mathbf{y})\hspace{.1in}\text{for all }\theta,$$ then the conclusions drawn from $\mathbf{x}$ and $\mathbf{y}$ should be identical.\ Note that the constant $C(\mathbf{x},\mathbf{y})$ above may be different for different $(\mathbf{x},\mathbf{y})$ pairs but $C(\mathbf{x},\mathbf{y})$ does not depend on $\theta$. In the special case of $C(\mathbf{x},\mathbf{y})=1$, the Likelihood Principle states that if two sample points result in the same likelihood function, then they contain the same information about $\theta$. But the Likelihood Principle goes further. It states that even if two sample points have only proportional likelihoods, then they contain equivalent information about $\theta$. Now, one of the draws of Bayesian statistics is that, under proper priors, the Bayesian paradigm never violates the likelihood principle. However, there are very simple scenarios where the frequentist paradigm will violate the likelihood principle. Here is a very simple example based on hypothesis testing. Consider the following: Consider an experiment where 12 Bernoulli trials were run and 3 successes were observed. Depending on the stopping rule we could characterize the data as the following: Binomial Distribution: $X|\theta\sim\text{Bin}(n=12,\theta)$ and Data: $x=3$ Negative Binomial Distribution: $Y|\theta\sim\text{NegBin}(k=3,\theta)$ and Data: $y=12$ And thus we would obtain the following likelihood functions: \begin{align} \ell_1(\theta;x=3)&=\binom{12}{3}\theta^3(1-\theta)^9\\ \ell_2(\theta;y=12)&=\binom{11}{2}\theta^3(1-\theta)^9\\ \end{align} which implies that $$\ell_1(\theta;x)=C(x,y)\ell_2(\theta,y)$$ and thus, by the Likelihood Principle, we should obtain the same inferences about $\theta$ from either likelihood. Now, imagine testing the following hypotheses from the frequentist paradigm $$H_o:\theta\geq\frac{1}{2}\hspace{.2in}\text{versus}\hspace{.2in}H_a:\theta For the Binomial model we have the following: \begin{align} \text{p-value}&=P\left(X\leq 3|\theta=\frac{1}{2}\right)\\ &=\binom{12}{0}\left(\frac{1}{2}\right)^{12}+\binom{12}{1} \left(\frac{1}{2}\right)^{12}+ \binom{12}{2}\left(\frac{1}{2}\right)^{12}+\binom{12}{3}\left(\frac{1}{2}\right)^{12}=0.0723 \end{align} Notice that $\binom{12}{3}\left(\frac{1}{2}\right)^{12}=\ell_1(\frac{1}{2};x=3)$ but the other terms do not satisfy the likelihood principle. For the Negative Binomial model we have the following: \begin{align} \text{p-value}&=P\left(Y\geq 12|\theta\frac{1}{2}\right)\\ &=\binom{11}{2}\left(\frac{1}{2}\right)^{12}+\binom{12}{2}\left(\frac{1}{2}\right)^{12}+ \binom{13}{2}\left(\frac{1}{2}\right)^{12}+...=0.0375 \end{align} From the above p-value calculations we see that in the Binomial model we would fail to reject $H_o$ but using the Negative Binomial model we would reject $H_o$. Thus, even though $\ell_1(\theta;x)\propto\ell_2(\theta;y)$ there p-values, and decisions based on these p-values, do not coincide. This p-value argument is one often used by Bayesians against the use of Frequentist p-values. Now consider again testing the following hypotheses, but from the Bayesian paradigm $$H_o:\theta\geq\frac{1}{2}\hspace{.2in}\text{versus}\hspace{.2in}H_a:\theta For the Binomial model we have the following: \begin{align} P\left(\theta\geq\frac{1}{2}|x\right)=\int_{1/2}^1\pi(\theta|x)dx=\int_{1/2}^1\theta^3(1-\theta)^9\pi(\theta)d\theta \bigg/\int_{0}^1\theta^3(1-\theta)^9\pi(\theta)d\theta \end{align} Similarly, for the Negative Binomial model we have the following: \begin{align} P\left(\theta\geq\frac{1}{2}|y\right)=\int_{1/2}^1\pi(\theta|x)dx=\int_{1/2}^1\theta^3(1-\theta)^9\pi(\theta)d\theta \bigg/\int_{0}^1\theta^3(1-\theta)^9\pi(\theta)d\theta \end{align} Now using Bayesian decision rules, pick $H_o$ if $P(\theta\geq\frac{1}{2}|x)>\frac{1}{2}$ (or some other threshold) and repeat similarly for $y$. However, $P\left(\theta\geq\frac{1}{2}|x\right)=P\left(\theta\geq\frac{1}{2}|y\right)$ and so we arrive at the same conclusion and thus this approach satisfies the likelihood Principle. And so to conclude my ramblings, if you don't care about the likelihood principle then being frequentist is great! (If you can't tell, I'm a Bayesian :) )
