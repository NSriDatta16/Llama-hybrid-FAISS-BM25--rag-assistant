[site]: crossvalidated
[post_id]: 419665
[parent_id]: 
[tags]: 
In what sense does interim monitoring of clinical trials "cost" a Bayesian?

I have read (and will seek a specific reference on the subject) that unlike Frequentist trials, Bayesians can continually monitor data as it accrues. A Frequentist tries to control, and thus minimize, the probability of adopting ineffective drugs. A Bayesian tries to maximize the probability that adopted drugs work. Frequentists control for multiple testing by "spending" their alpha. If one performs two or more comparisons, to preserve the overall 0.05 level of the trial, the alpha level is spent by Bonferonni, Pocock, or O'Brien Flemming formulas to control for multiple tests. It seems, though, that when the Bayesian performs too many "interim looks" there must be some Bayes analogue to multiple testing that you are more likely to make the decision to adopt the drug on the basis of favorable interim results. So what is the real cost to the analysis? How is it correctly reflected in a properly conducted Bayesian trial? Does the Bayesian enforce more stringent decision rules for future analyses based on the result of an interim look should we decide to keep going? Alternately, do we introduce some uncertainty in the posterior that attenuates effect somewhat to account for the fact that we might be observing a stochastic trend for an ineffective drug that may randomly show "favorable" results?
