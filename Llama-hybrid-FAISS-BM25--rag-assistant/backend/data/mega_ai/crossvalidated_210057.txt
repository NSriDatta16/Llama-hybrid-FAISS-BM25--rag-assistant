[site]: crossvalidated
[post_id]: 210057
[parent_id]: 
[tags]: 
Regression on a timeseries signal using RNN, works well but is delayed by 1 time point

I implemented my own NN framework and did a regression on a small dataset that follows pretty good the target only is delayed forward, by 1 time point. This is how the validation data looks: The curios thing is that the training data looks the same: Although the prediction has almost the same shape as the target signal, the 1 time point difference creates a significant difference between the 2 signals. Did anyone encountered this kind of problem and know how to solve it ? For other datasets the regression works very well, also on timeseries, also for classification, using my NN framework. It could be a bug, but why does it work very well on MNIST10 ? and other timeseries datasets ? I used a classic RNN, not a LTSM. Input data is a time lagged input from (t, t-1, ....., t-12). Input data was normalized in [0,1], then sent to hidden layer 1 activated with tanh, then to hidden layer 2 activated with tanh, and then to error layer activated with sigmoid(tried also linear, same thing). Hidden layers each have a context layer that stores the activation of the hidden layer at the previous time t-1 to feed it at time t to the hidden layer. I sent batches of 128 sequences of length 10,13,30,40 and other. The train dataset has about 2950 time points, and the val dataset has 155 time points. The accuracy is: TRAIN Root mean squared error: 0.038273 TRAIN Correlation coefficient: 0.974084 VAL Root mean squared error: 0.054285 VAL Correlation coefficient: 0.861191 Does anyone know how this problem can be solved using RNNs ? I am not interested in using other statistical models or Google frameworks. Many thanks, Viorel
