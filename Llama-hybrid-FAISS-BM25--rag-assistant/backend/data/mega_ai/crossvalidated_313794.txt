[site]: crossvalidated
[post_id]: 313794
[parent_id]: 313775
[tags]: 
In general, variance increases as complexity increases, and it holds for any model---not just a neural network. Variance refers to the variance of the model coefficients/ weights over different training datasets. These training sets differ really essentially with respect to noise alone. A model with more complexity necessarily has higher ability to over-interpret noise in the training set. Hence the coefficients will be more different on one training set relative to another for a comlex model than for a simple model. Hence more complexity leads to more variance in coefficients over different training sets. Suppose we had a neural network that outputs 1 regardless of input. Regardless of the training data, we get 1. So, the "coefficient" (which doesn't really exist) has 0 variance over training sets. Conversely if we memorize a training set because the network has maximum complexity, the coefficients must vary over training sets to a great degree. Again in theory I don't think there is anything surprising you could find along this continuum. However, what if there were a model with n coefficients that happens to be too simple to capture a certain relationship in the data. With n+1 coefficients, it is able to capture that relationship. If the relationship is definitive between the input and response, perhaps then the variance of the coefficients would decrease moving from n to n+1? Consider a sphere dataset. With 2 learnable parameters for a 3D input (fix one at 0), it is not possible to determine whether a point is inside or outside of the sphere; the only points that would be perfectly classified would be ones on the vertical line in the center of the sphere. With 3 parameters for a 3D input, however (we now have 3 parameters that can vary), it should be trivial for the model to learn whether a point is in or out of the sphere. I think the variance of the coefficients of the 2 parameter model might be larger than the variance of the coefficients of the 3 parameter model in this case? Although actually the 2 parameter model might still have lower variance because it simply learns the line each time.
