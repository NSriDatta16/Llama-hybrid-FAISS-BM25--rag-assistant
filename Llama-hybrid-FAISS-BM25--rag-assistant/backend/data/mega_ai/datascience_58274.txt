[site]: datascience
[post_id]: 58274
[parent_id]: 58269
[tags]: 
Bagging and boosting are two methods of implementing ensemble models. Bagging: each model is given the same inputs as every other and they all produce a model Boosting: the first model trains on the training data and then checks which observations it struggled most with, it passes this info to the next algorithm which assigns greater weight to the misclassified data Because of this both bagging and boosting reduce variance. However boosting is better at improving accuracy vs a single model whilst bagging is better at reducing overfitting. I would advise training a single version of the ensemble aka decision tree for random forest and then seeing where improvements can be made. Good article explaining boosting vs bagging
