[site]: crossvalidated
[post_id]: 498858
[parent_id]: 
[tags]: 
Attention is all you need: During training, does decoder generates the whole sentence in 1 shot and not sequentially word by word?

In the famous paper " Attention is all you need ", does the decoder generates the whole output sentence in one shot in parallel. In this case will the final softmax output the probabilities for each word for each position like this: $$\left[(\text{pos 1}: P(w_1), P(w_2), \ldots, P(w_V)), (\text{pos 2}: P(w_1), P(w_2),... P(w_V)), \ldots, (\text{pos $N$}: P(w_1), P(w_2), \ldots, P(w_V) )\right]$$ where $N$ is the sequence length and $V$ the vocab size. Hence, are there $N$ sets of outputs from the softmax ?
