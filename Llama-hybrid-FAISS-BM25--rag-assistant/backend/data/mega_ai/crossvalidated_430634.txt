[site]: crossvalidated
[post_id]: 430634
[parent_id]: 
[tags]: 
Setting some CNN/DNN weights to zero

With weight pruning in CNN/DNN models, you can get rid of non-essential weights (set them to zero) after training. And then you can re-train the remaining weights, to improve any loss in accuracy. This is a common technique used in Model Compression [1,2]. From the above we can intuitively reason that certain weights/neurons are more susceptible to accuracy loss than others. I would like to have more control over which weights in the CNN we can prune out, rather than making the decision after training, based on the trained weight value. Assume I am going to freeze certain weights and training does not change them. e.g., $W = [1,1,2,0,0,3]$ in the above we manually set $W_3, W_4$ to zero during initialization and the rest to random. During training $W_3, W_4$ will remain at zero. Question: How can I control the position of zero valued weights without much loss in accuracy ? Do you find anything clearly wrong with selective zero-initialization ? thanks [1] https://arxiv.org/abs/1510.00149 [2] https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras
