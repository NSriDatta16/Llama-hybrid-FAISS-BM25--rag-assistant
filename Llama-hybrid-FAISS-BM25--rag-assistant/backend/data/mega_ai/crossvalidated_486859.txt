[site]: crossvalidated
[post_id]: 486859
[parent_id]: 483685
[tags]: 
Defining extremeness of test statistic and defining p-value for a two-sided test... I would suggest that an appropriate perspective here is that, when one has the "right" statistic, the statistic itself tells you what "extremeness" means for the test problem at hand---one-sided or two-sided. The more basic question is therefore what the "right" statistic is. Test problems are special cases of optimization problems---you want to maximize power subject to size constraint. So this means defining the "right" solution concept. For example, finding the most powerful test for the test problem with a simple null vs. simple alternative is a special case of a linear program: $$ \sup_{0 \leq \phi \leq 1, \, \\ \\ \int \phi(\omega) f_0(\omega) d\mu \leq \alpha} \int \phi(\omega) f_1(\omega) d\mu. $$ It is a general fact that a solution $\phi^*$ for any such program takes the form $$ \phi^* = \begin{cases} 1 & \text{if } f_1 \geq k f_0 \\ 0 & \text{if } f_1 \geq k f_0, \end{cases} $$ for some $k$ . In the context of a test problem, a natural interpretation is then that one rejects when the likelihood ratio statistic $\frac{f_1}{f_0}$ is larger than $k$ . (It is suggested in the comments that the threshold $k$ is interpreted to be the "shadow price" of the size constraint. Apparently this terminology is borrowed from economics. $k$ is the Kuhn-Tucker-Lagrange multiplier of the problem. For interior solutions, typically one would say that if $\alpha$ ---the budget, in economic problems---is relaxed by $\epsilon$ , the power of the test increases by $k \epsilon$ . This interpretation, however, does not really hold for linear programs in general.) Similarly, finding a most powerful test of composite null vs. simple alternative amounts to solving a linear program. The solution to the corresponding dual program tells us that the most powerful statistic is a likelihood ratio statistic with respect to the least favorable Bayesian prior on the null. (The simple null case is a special case, with trivial prior.) Tests with one-sided alternatives for models with monotone likelihood ratio (MLR) property is of course another example. MLR means the model admits a ranking of likelihood ratios that's invariant with respect to data $\omega$ . So the likelihood ratio test is a most powerful test, almost by assumption. For two-sided alternatives, e.g. $\Gamma_0 = \{\gamma_0\}$ and $\Gamma_1 = (-\infty,\gamma_0)\cup (\gamma_0, \infty)$ for normal densities parametrized by mean $\gamma \in \mathbb{R}$ , the most powerful test does not exist in general. Therefore the right statistic needs to be determined by some other criterion---e.g. one can instead look for a locally most powerful test . A test $\phi^*$ is a locally most powerful test if for any other test $\phi$ , there exists an open neighborhood $N_{\gamma_0, \phi}$ of the null hypothesis such that $\phi^*$ has uniformly higher power than $\phi$ on $N_{\gamma_0, \phi}$ . The corresponding first-order optimality condition gives the criterion $$ \phi^* = \begin{cases} 1 & \text{if } \frac{\partial^2}{\partial \gamma^2}f_{\gamma_0} \geq k_1 \frac{\partial}{\partial \gamma} f_{\gamma_0} + k_2 f_{\gamma_0} \\ 0 & \text{if } \frac{\partial^2}{\partial \gamma^2}f_{\gamma_0} for some $k_1$ and $k_2$ . Substituting the normal density into above expressions, we have that $\phi^*$ rejects when $|x- \gamma_0|$ is large---a two-sided test.
