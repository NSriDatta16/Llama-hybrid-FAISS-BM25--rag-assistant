[site]: crossvalidated
[post_id]: 204695
[parent_id]: 204667
[tags]: 
The last one. You learn a new model on all the data you have. Why did you compute CV? To get an estimate of the generalization error, i.e., of the error you will get on unseen data (coming from the same data generating process, of course). You could just use the MSE on the training set, but that's usually smaller than the error you will get on unseen data (it's an optimistic estimate), and it can be extremely smaller (even zero) if your model is overfit to training data. The CV error, defined as the average of the MSE errors on each fold, is a better estimate of the generalization error. Anyway, often CV is not used to estimate the generalization error, but to choose between different models. It definitely helps, if you're using it to choose between different models. Example: fit different degree polynomials to the same training set (and please, use orthogonal polynomials :). Now, you want to select the model which has the best generalization properties, i.e., which will make better predictions on new, unseen data. If you chose the model with the lowest training set MSE, you would invariably end up selecting the highest order polynomial, which would likely overfit training set data. Instead, the CV error would usually be minimum for a polynomial of intermediate degree. Concerning the class of learners for which it works, there's a theorem but I'm sorry I can't recall it. You should google for papers by Efron, Hastie or Tibshirani, I guess it's in one of them (or at least they may give a reference to it). One important thing to note is that, at least in the version I know, the samples must be i.i.d., which prevents applying CV in its basic form to time series (there are workarounds, though). Well, basically it works for the same reason why a validation set works. If your model is very complex, it will have low bias but high variance. This means that its predictions may have been very different, had the training set been a different one. With cross-validation, you quantify this variability by training it multiple times on different subsets of the original data set, and testing it on the hold-out data. If the model overfits the training subsample, then it will perform poorly on the hold-out data. In the age of Big Data, it's always needed (when the hypotheses for its applicability hold). For example, consider deep neural networks. Imagenet is a deep neural network with 60 milions parameters. Even if you have a huge data set, this is still an incredibly flexible model. Now, you need to learn its parameter. You start training Imagenet with some optimization method, and you have to decide when to stop training. If you just look at the trend of the training set MSE, it will continue decreasing, on average, the longer you keep learning. You're likely going to end up with an horribly overfit model. If instead you use a CV error curve or a test set curve as a guide, they usually will start going up at some point, and you can use that as an indication to stop training your DNN.For a better example, think genomic data. Here the number of predictors is so huge, that even when you have a big data set, you may have more predictors than observations. In this case you need to regularize your regression (ridge regression, LASSO, elastic net, etc.). How to choose the regularization parameter? CV. Because you're basically throwing away data which you could use to train a more complex model. It looks like today the winning approach in statistical learning is to train extremely complex (extremely flexible) models, with very low bias and huge variance, on huge data sets, in order to prevent overfitting. In such a context, the more data you have, the more complex a model you can fit, while still retaining good generalization properties.
