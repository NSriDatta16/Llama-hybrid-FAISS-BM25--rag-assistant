[site]: crossvalidated
[post_id]: 263350
[parent_id]: 
[tags]: 
How to deal with Exponential errors for (minimum quantile) linear regression?

I have data, $(x_i, y_i)$ where $y_i$'s are responses and $x_i$'s are covariates, that appears to follow this model: $y_i = [0.000001^\text{th} \text{ quantile linear regression of } (x_i, y_i)] + e_i$, where $e_i \sim \operatorname{Exponential}(\lambda)$ and $\lambda$ is to be estimated. I know how to fit the minimum, e.g. $0.000001^\text{th}$, quantile linear regression of $(x_i, y_i)$ in R using the quantreg package. So I estimated $\lambda$ by fitting an Exponential distribution for: $$y_i - [0.000001^\text{th} \text{quantile linear regression of } (x_i, y_i)], \text{ for all } i.$$ I ended up getting $\hat{\lambda} = 4$. Then, to see if this is reasonable, I simulated $\hat{e}_i \sim Exponential(4)$ randomly for all $i$ and then computed $$\hat{y}_i = [0.000001^\text{th} \text{ quantile linear regression of } (x_i, y_i)] + \hat{e}_i$$ The plot of $(x_i, \hat{y}_i)$ looks similar to that of $(x_i, y_i)$: Do you have any tips on how this can be elegantly implemented? How do you estimate $\hat{y}_i$?
