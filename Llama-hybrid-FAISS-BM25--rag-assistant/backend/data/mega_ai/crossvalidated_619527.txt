[site]: crossvalidated
[post_id]: 619527
[parent_id]: 449907
[tags]: 
Your conclusion that $H(X|Y)=H(X|YI)$ in this case is in fact correct, because the indicator variable $I$ you have constructed is a function of $Y$ alone. In such situations the "data-processing inequality" $H(X|Y)\geq H(X|YI)$ for conditional entropy is saturated (one way to look at it is that you can simply apply the data-processing argument in the reverse direction as well, because the pair of random variables $YI$ can be generated from $Y$ alone). Another perspective on this is that in general we have a chain rule $H(X|YI)=H(X|Y)-\mathcal{I}(I:X|Y)$ (using $\mathcal{I}$ for mutual info to avoid notation clash with the indicator variable), but since in your case $I$ is a function of $Y$ alone, the $\mathcal{I}(I:X|Y)$ term is equal to zero (e.g. by noting $X \leftrightarrow Y \leftrightarrow I$ form a Markov chain).
