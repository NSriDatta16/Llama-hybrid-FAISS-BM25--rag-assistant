[site]: crossvalidated
[post_id]: 557541
[parent_id]: 554142
[tags]: 
Your proposal is quite different to what we usually do in binary regression problems Your question uses a kind of binary regression model but it appears that your model has only a single regression function (as opposed to a parametric family of such functions). At present it is also unclear exactly what you will use this proposed metric for, so it is difficult to give effective advice on the usefulness (or lack thereof) of your proposal. In any case, I will take your question on its own terms and show how you can simplify this problem and identify the sets of interest to you, but I will also make some comments along the way comparing your approach to what we usually do in binary regression. (Also, there is no need to assume that this is a logistic regression, so I will drop that assumption.) Consider the case of a binary regression model with an assumed regression function $f$ (which may or may not be the logistic regression), giving $\mathbb{P}(X_i=1 | S_i=s) = \mathbb{E}(X_i | S_i=s) = f(s)$ . This regression model has sampling distribution given by: $$p(X_1=x_1,...,X_n=x_n | \mathbf{S}=\mathbf{s}) = \prod_{i=1}^n \text{Bern}(x_i |f(s_i)).$$ Using the observed data points $(x_1,s_1),...,(x_n,s_n)$ we can easily compute the deviations of the actual response from its expected value as: $$r_i \equiv x_i - f(s_i).$$ These give us some crude "residuals" for the model, measuring the deviation of the binary response variable from its expectation under the model. (Since there is only a single regression function in this problem there is no estimated regression function; consequently, there is no distinction in this case between the "error terms" in the model and the "residuals". You could just as easily call these quantities the error terms if you prefer, since there is no distinction in this case.) You can use these quantities to simplify the expressions in your question. To do this, observe for any set $\mathcal{T} \subseteq \{ 1,...,n \}$ (including the full set of values) we have: $^\dagger$ $$\sum_{i \in \mathcal{T}} r_i \approx 0 \quad \quad \quad \iff \quad \quad \quad \sum_{i \in \mathcal{T}} x_i \approx \sum_{i \in \mathcal{T}} f(s_i).$$ Your proposed analysis: In your question, you say that you want to have $\sum_{i \in \mathcal{T}} x_i \approx \sum_{i \in \mathcal{T}} f(s_i)$ as a test to see if the model is unbiased. Since this is equivalent to $\sum_{i \in \mathcal{T}} r_i \approx 0$ , you are effectively looking at whether the sum of residuals is near zero. This is quite a crude way to look for bias in the model --- it is not wrong, but it is only going to detect a narrow set of forms of bias that are systematic across enough of the explanatory values that it manifests in large positive or negative totals for the residuals. Your approach will not generally detect forms of bias where the regression is positively biased in some parts of the function and negatively biased in others (such that it "cancels out" in the residual sum). Here it is worth noting what we usually do in regression analysis when we want to know if the posited regression function is reasonable or not. What we usually do is to look at the residual plot showing the relationship between the residuals and the fitted values. (In the case of a regression with a single explanatory variable, on can also look at the residuals against that explanatory variable.) This type of plot will usually allow us to diagnose systematic bias in the regression function. Moreover, it can be used to see if the regression function is biased in just some parts of the range of the explanatory variables, etc. In the case of binary regression it is a bit tricky, owing to the binary response, but the residual plot is still useful for this purpose if examined with care. A secondary aspect of your approach that is potentially problematic is that you have not really specified how you will tell if the residual sum of interest is "close enough" to zero that you think it is unbiased. To do this you would need to derive the distributional properties of the residuals, so that you can see the distribution of the residual sum (or at least some approximation to this distribution) under the model (i.e., assuming the specified regression function is the correct form). Finally, while you have proposed identifying subsets of values where the residual sum deviates substantially from zero, you have not really described how you would apply this analysis. As you correctly recognise in your question, even if the model is correct (and therefore unbiased) you can certainly cherry-pick subsets of values where the residual sum is substantially different from zero. (I will show you how to do this below.) Consequently, any attempt to use cherry-picked subsets like this to try to identify local bias in the regression is likely to be a very bad procedure; so it is unclear what the point is of identifying these subsets of values. Identifying subsets of values of interest: Notwithstanding the above critique, I will take your question on its own terms and show you how you can compute the subsets of interest. It is simple to use a sorting algorithm to obtain the ordered residuals , and it is also simple to identify where these ordered residuals cross zero. Consequently, it is simple to compute index values $i_1,...,i_n$ and the value $k$ giving the following order: $$r_{i_1} \leqslant \cdots \leqslant r_{i_k} If you want to identify a subset of data points with an extreme negative residual sum, you can take any subset $\mathcal{T} = \{ i_1,...,i_r \}$ for some $r \leqslant k$ . Similarly, if you want to identify a subset of data points with an extreme positive residual sum, you can take any subset $\mathcal{T} = \{ i_{r+1},...,i_n\}$ for some $r \geqslant k$ . In both cases, you start by adding the data point with the most extreme negative/positive residual, then the second-most-extreme, and so on. It is simple to construct an algorithm to do this. I will illustrate this below in R . First, I generate some mock data from a logistic regression model with a fixed regression function. Then I show how to compute subsets giving extreme negative/positive residual sums. #Generate logistic regression data with a fixed regression function set.seed(1) n 0) { SUB.NEG $^\dagger$ Note that the use of the indicator function for $x_i$ in your sums is unecessary, since $x_i$ is already a binary variable . Consequently, this expression does not use indicator functions.
