[site]: crossvalidated
[post_id]: 171377
[parent_id]: 157234
[tags]: 
In this case, the regular discrete entropy of each variable might suffice for your purposes. If an attribute takes value i with probability $p_i$, then entropy is just $H(X) = -\sum_i p_i \log p_i$. If an attribute takes only a single value (all 0's), you get 0. If it takes just two values, then the maximum value will be 1 bit (using log base two). If it takes four values, the maximum possible value will be 2 bits. If it takes all possible values from 0 to 255 the maximum is 8 bits. What will the "most interesting" variables look like for this measure? It will be the ones where each value is equally likely. This is a "smooth" measure in the following sense. If you have an attribute that almost always takes just two values (0 and 255 in your example), but rarely takes other values, the score will still be near to one bit. In other words, rare events will not lead to "interestingness"/high entropy. This measure can also be interpreted in terms of compression. What if you tried to compress this attribute, how many bits on average would be required? Well, if all possibilities are equally likely, 8 bits. If only two values ever occur, then only one bit is required.
