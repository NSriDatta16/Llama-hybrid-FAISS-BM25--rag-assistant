[site]: crossvalidated
[post_id]: 418180
[parent_id]: 418154
[tags]: 
Part of this depends upon what you mean by the word "best." The various information criteria can be derived either from information theory or Bayesian theory. Assuming your likelihood function is correct, the various information criteria are good approximations of the Bayesian posterior density under relatively flat prior densities unless the values are quite close. Your statement regarding heteroskedasticity is not a problem as long as your likelihood function doesn't assume homoskedasticity. If you would assume a normal distribution with a fixed variance, then it is possible that a given criterion could generate an incorrect solution. The value of the information criterion will generally be proportional to the posterior probability found using a Bayesian method. The largest difference is about how parameters are understood. The information criterion is driven by the supremum of the likelihood of the parameter estimates, whereas the Bayesian posterior over the model space integrates out the uncertainty over the entire parameter space. The Bayesian method removes the consequence of making a specific choice for a parameter. In practice, most models are far apart in posterior probability from the most likely model. It does happen that a few models will be close in probability. I have had that happen. I have had the case where one model had about 54% posterior probability; another had about a 46% posterior probability and the remaining seventy-six models had a total posterior probability of around 1/10,000 when their posterior probabilities were added together. I did not have a reason to calculate an information criterion but had I, seventy-six would not be close to the best two, and the best two would be marginally different. What I did instead was to exclude the worst 76 and used model averaging on the two remaining models. Tests against the hypothesis of concern showed that the joint model was better than either apart. As they did not share variables, the difference was substantial. Interestingly, both models made theoretical sense, after the fact, but also made it clear something was missing in the models and that further research was needed. There is an argument, that I do not find persuasive, that using the information criterion could be problematic using combinatoric solutions if none of the models is the true model in nature. In essence, every choice is a misspecified model. The problem with the argument is that you can never validate this state of nature unless someone else comes along and validates via a theoretical construction a model that is not in your set. Nonetheless, you need to work with what you have today. Two other potential issues could happen if there is a variable or set of variables that must be present for theoretical reasons, but removing them is optimal. That can happen with some form of collinearity. The solution is to force mandatory variable into every model. It could also happen if you cannot fully capture causal variables and marks the difference between prediction and inference. Imagine you were looking at readmission rates at a hospital. You found the single best variable was having been in the ICU, for predictive purposes. It would be life-threateningly dangerous to then ban people from going to intensive care. Imagine the true best predictor for readmission was skipping doctor visits six months prior to the hospitalization and non-compliance with medical orders, but you couldn't capture or fully capture this. Imagine a hospital where there are three readmissions per month, all of whom were admitted to the ICU. When you ban admission to the ICU it goes up to eighteen readmissions per month. The presence of the ICU as a variable may be the best single predictor, but it is not valuable for inferential purposes. In that case, an information criterion could give a misleading result. Logic is always your greatest protection against bad models.
