[site]: crossvalidated
[post_id]: 560625
[parent_id]: 560383
[tags]: 
High degree polynomials do not overfit the data This is a common misconception which is nonetheless found in many textbooks. In general, in order to specify a statistical model, it is necessary to specify both a hypothesis class and a fitting procedure. In order to define the Variance of the model ("variance" here in the sense of Bias-Variance Tradeoff), it is necessary to know how to fit the model, so claiming that a hypothesis class (like polynomials) can overfit the data is simply a category error. To illustrate this point, consider that the Lagrange interpolation method is only one of several ways to fit a polynomial to a collection of points. Another method is to use Bernstein polynomials . Given $n$ observation points $x_i=i/n$ and $y_i=f(x_i)+\epsilon_i$ , the Bernstein approximant is, like the Lagrange approximant, a polynomial of degree $n-1$ (i.e., as many degrees of freedom as there are data points). However, we can see that the fitted Bernstein polynomials do not exhibit the wild oscillations that the Lagrange polynomials do. Even more strikingly, the variance of the Bernstein fits actually decreases as we add more points (and thus increase the degree of the polynomials). As you can see, the Bernstein polynomial does not pass through each point. This reflects the fact that it uses a different fitting procedure than the Lagrange polynomial. But both models have the exact same hypothesis class (the set of degree $n-1$ polynomials), which underscores that it is incoherent to talk about statistical properties of a hypothesis class without also specifying the loss function and fitting procedure. As a second example, it is also possible to exactly interpolate the training data using a polynomial without wild oscillations, assuming we are working over complex numbers . If the observation points $x_i$ lie along the unit circle $x_i=e^{2\pi i\sqrt{-1}/N}$ , then fitting a polynomial $y_i=\sum_d c_dx_i^d$ basically comes down to computing the inverse Fourier transform of $y_i$ . Below I plot the complex polynomial interpolant as a function of the complex argument $i/N$ . Moreover, it turns out that the variance of this interpolant is constant as a function of $n$ . Code for these simulations can be found at the end of this answer. In summary, It is simply false that including higher degree polynomials will increase the variance of the model, or make it more prone to overfit As a consequence, any explanation of why Lagrange interpolants do in fact overfit will need to use the actual structure of the model and fitting procedure, and cannot be based on general parameter-counting arguments. Fortunately, it is not too hard to mathematically show why Lagrange interpolants do exhibit this overfitting. What is basically comes down to is that fitting the Lagrange interpolant requires inversion of a Vandermonde matrix, which are often ill-conditioned. More formally, what we want to do is to show that the variance of the Lagrange interpolants grows very quickly as a function of $n$ , the number of data points. It is not too hard to establish the following quantiative bound: proposition Let $x_i=i/n$ for $i=1,\dots,n$ . Let $\epsilon_i\sim N(0,1)$ be iid noise, and let $p_{\epsilon}(x)$ be the interpolating polynomial through the points $(x_i,y_i+\epsilon_i)$ . Then the average variance $\int_0^1 Var_{\epsilon}p_{\epsilon}(x)dx$ grows at least as $O(e^{(1.5-\log 4)n})$ . proof of proposition Let our interpolating polynomial be given by $p(x)=\sum_{d=0}^{n-1}w_d x^d$ . Evidently p must satisfy $y_i=p(x_i)=\sum_{d=0}^{n-1}w_d x_i^d=(Vw)_i$ where the vandermonde matrix $V$ is defined by $V_{ij}=x_i^{j-1}$ . So we have the formula $w=V^{-1}y$ for the coefficients of the interpolating polynomial. As a warmup, we'll first consider the variance of the coefficient vector $w$ , and then generalize the argument to analyze the variance of the polynomial $p(x)$ . coefficient variance We want to compute the variance of $w$ , that is $$Var(w):= E_{\epsilon} \|w-E_{\epsilon} w\|^2$$ If we write $w=V^{-1}y+V^{-1}\epsilon$ , then the first term is constant (does not depend on epsilon), while the second term has zero mean. Because the variance is translation invariant, $$Var(w)=E_{\epsilon} \|V^{-1}\epsilon\|^2=E_{\epsilon} \epsilon^t (V^{-1})^tV^{-1}\epsilon=Tr( (V^{-1})^t V^{-1})=Tr((VV^t)^{-1})$$ Thus the variance of the coefficients is directly related to the eigenvalues of the matrix $VV^t$ - the smaller these eigenvalues, the larger the variance. Now it becomes clear why the conditioning of $V$ is so important. In the above discussion, the values of the $x_i$ s was arbitrary, but let us now assume that they are evenly spaced in the unit interval: $x_i=i/n$ . In this case, we can work out an explicit lower bound for the variance. Firstly, note that $Tr(M)/n\geq det(M)^{1/n}$ for any symmetric positive definite matrix (this follows immediately from the AMGM inequality). In our case, $$Var(w)\geq n |det(V)|^{-2/n}$$ Now, the determinant of a vandermonde matrix has the well-known formula $det(V)=\prod_{i . Under our definition of $x_i$ , this is $$det(V)=\prod_{i (To see the last step, note that there are $n-1$ pairs $i,j$ such that $j-i=1$ , there are $n-2$ pairs such that $j-i=2$ etc. ) The product of factorials is called the \textit{superfactorial}. Similarly to how the factorials are interpolated by the gamma function, the superfactorials are interpolated by an analytic function called the Barnes G function, $G$ . More precisely, we have $G(n+1)= \prod_{i=1}^{n-1} i!$ when $n$ is a positive integer. Furthermore, there is an analogue of Stirling's formula: $$\log G(n+1)= .5*n^2\log n-.75*n^2+n\log \sqrt{2\pi}+O(log(n))$$ So the determinant goes like $$\log |Det(V)|\sim -(n^2-n)/2\log n+(n^2/2)\log n-3n^2/4+n\log \sqrt{2\pi}+...=-3n^2/4-(n/2)\log n+n\log \sqrt{2\pi}+...$$ So by the inequality for the variance, $$\log Var(w)\geq \log n -(2/n)\log |det(V)|=3n/2+2log n-\log 2\pi$$ Or said a bit more succinctly, $Var(w)=O(e^{3n/2})$ , that is the variance grows exponentially. prediction variance To bound the variance of the model predictions, which is what we ultimately care about, it is necessary to modify the above argument only slightly. As a bit of notation, define $pow(x)$ to be the vector $pow(x)_i=x^{i-1}$ . Observing as before that $w$ decomposes into a constant $V^{-1}y$ and a mean-zero term $V^{-1}\epsilon$ , we have that $$Var(x)=Var_{\epsilon} \sum_d (V^{-1}\epsilon)_d x^d=Var_{\epsilon}pow(x)^t (V^{-1}\epsilon)=\|V^{-1}pow(x)\|^2$$ Integrating over $x$ , \begin{eqnarray*} Var&=&\int Var(x)dx=\int Tr(V^{-1}pow(x)pow(x)^t(V^{-1})^t))dx\\ &=& Tr(V^{-1}(\int pow(x)pow(x)^tdx)(V^{-1})^t))\\ & = & Tr(V^{-1}H_n(V^{-1})^t) \end{eqnarray*} where $H_n$ is the Hilbert matrix given by $(H_n)_{ij}=1/(i+j-1)=\int_0^1 x^{i+j-2}dx$ . We want to bound this from below, arguing as before we can reduce this to a determinant calculation: $$Var=Tr(V^{-1}H_n(V^{-1})^t)\geq n det (V^{-1}H_n(V^{-1})^t)^{1/n}=n |det V|^{-2/n}|det H|^{1/n}$$ Now, the Hilbert determinant has a well-known asumptotic expansion: $$\log det (H)\sim -n^2 \log(4)+n\log 2\pi +...$$ Thus the dominant term in $|det(H)|^{1/n}$ will be $e^{-n \log 4}$ . On the other hand, we saw above that $n |det(V)|^{-n/2}$ has dominant term $e^{3n/2}$ . Thus we can conclude that $Var=O(e^{(3/2-\log 4)n})$ . Note crucially that $3/2>\log 4$ , so that the variance indeed increases exponentially. This concludes the proof. Edit: analysis of Bernstein variance As an interesting comparison, it is not too hard to work out the variance for the Bernstein approximator. This polynomial is defined by $\hat{B}(x)=\sum_i b_{i,n}(x)y_i$ , where $b_{i,n}$ are the Bernstein basis polynomials, as defined in the Wikipedia link. Arguing similarly as before, the variance with respect to different samplings of $y$ is $\sum_i b_{i,n}(x)^2$ . As in the simulation, I'll average over $x\in [0,1]$ . Evidently $\int_0^1 b_{i,n}(x)^2dx={n\choose i}^2B(2i+1,2n-2i+1)$ , with $B$ the beta function. We have to sum this over $i$ . I claim the sum comes to: $$\int_0^1 Var(\hat{B}(x))dx={\frac {4^n}{(2n+1){2n\choose n}}}$$ (proof is below) This formula does indeed closely match the empirical results from the simulation (note that for direct comparison, you will have to multiply by $.1^2$ , which was the noise variance used in the simulation). By Stirling's formula, the variance of the Bernstein approximator goes as $O(n^{-.5})$ . Proof of variance formula The first step is to use the identity $B(2i+1,2n-2i+1)=(2i)!(2n-2i)!/(2n)!$ . So we get \begin{eqnarray*} {n\choose i}^2 B(2i+1,2n-2i+1) & = & {\frac {(n!)^2}{(i!)^2(n-i)!^2}}{\frac {(2i)!(2n-2i)!}{(2n+1)!}}\\ & = & {\frac 1 {2n+1}}{2n\choose n}^{-1}{2i\choose i}{2n-2i\choose n-i} \end{eqnarray*} Using the generating function $\sum_{i=0}^{\infty} {2i\choose i}x^i=(1-4x)^{-1/2}$ , we see that $\sum_{i=0}^n {2i\choose i}{2n-2i\choose n-i}$ is the coefficient of $x^n$ in $((1-4x)^{-1/2})^2=(1-4x)^{-1}$ , that is $4^n$ . Summing over $i$ we get the claimed formula for the variance. Edit: effect of distribution of x points on Lagrange variance As shown in the very interesting answer of Robert Mastragostino, the bad behavior of the Lagrange polynomial can be avoided through judicious choice of $x_i$ . This raises the possibility that the lagrange polynomial does uncharacteristically badly for uniformly sampled points. In principle, given iid sample points $x_i\sim \mu$ it is possible that the lagrange polynomials behave sensibly for most choices of $\mu$ , with the uniform distribution being just an ``unlucky choice". However, this is not the case, and it turns out that the Lagrange variance still grows exponentially for any choice of $\mu$ , with the single exception of the arcsine distribution $d\mu(x)=\textbf{1}_{(-1,1)}\pi^{-1}(1-x^2)^{-1/2}dx$ . Note that in Robert Mastragostino's answer, the $x_i$ points are chosen to be Chebyshev nodes. As $n\to\infty$ , the density of these points converges to none other than the arcsine distribution. So in a sense, the situation in his answer is the only exception to the rule of exponential growth of the variance. More precisely,take any continuous distribution $\mu(dx)=p(x)dx$ supported on $(-1,1)$ and consider an infinite sequence of points $\{(x_i,f(x_i)+\epsilon_i)\}_{i=1,\infty}$ where $x_i\sim \mu$ are iid, $f$ is a continuous function and $\epsilon_i$ are iid normal samples. Let $\hat{f_n}(x)$ denote the Lagrange interpolant fitted on the first $n$ points. And let $V_n=\int Var(\hat{f_n})(x)p(x)dx$ denote the corresponding average variance. Claim: Assume that (1): $p(x)>0$ for all $x\in (-1,1)$ and (2): $\int p(x)^2\sqrt{1-x^2}dx . Then $V_n$ grows at least like $O(e^{\epsilon n})$ for some $\epsilon>0$ , unless $p$ is the arcsine distribution . To prove this, note that, as before, $Var(\hat{f_n}(x))=\sum_{i=1}^nl_{i,n}^2$ , where now $l_{i,n}$ denotes the ith Lagrange basis polynomial wrt the first $n$ points. Now, \begin{eqnarray*} \log \int l_{i,n}^2(x)d\mu(x)&\geq& 2\int \log |l_{i,n}(x)|p(x)dx\\ & = & 2\int \sum_{j\neq i, j\leq n} \left(\log |x-x_j|-\log |x_i-x_j|\right)p(x)dx \end{eqnarray*} where we used Jensen's inequality in the first line, and the definition of $l_{i,n}$ in the second. Let us introduce the following notation: \begin{eqnarray*} f_p(x)&=&\int \log|x-y|p(y)dy\\ c_p& = & \int \log |x-y|p(x)p(y)dxdy \end{eqnarray*} Then we get \begin{eqnarray*}\lim\inf n^{-1}\log \int l_{i,n}^2(x)p(x)dx&\geq& 2\lim\inf_n n^{-1}\int \sum_{j\neq i, j\leq n} \left(\log |x-x_j|-\log |x_i-x_j|\right)p(x)dx\\ & \geq & 2 \int \lim\inf_n n^{-1}\sum_{j\neq i, 1,j\leq n} \left(\log |x-x_j|-\log |x_i-x_j|\right)p(x)dx\\ & = & 2\int \left(f_p(x)-f_p(x_i)\right)p(x)dx\\ & = & 2(c_p-f_p(x_i)) \end{eqnarray*} using Fatou's lemma, and then the strong law of large numbers. Note that $f_p'(x)=H_p(x)$ , where $H_p(x)=\int {\frac {p(y)}{x-y}}dy$ is the Hilbert transform of $p$ . In particular, $f_p$ is continuous. Now, suppose it is the case that $f_p(x)$ is non-constant. In this case, there exists $\epsilon>0$ as well as an interval $[a,b]$ such that $\inf_{x'\in [a,b]}c_p-f_p(x')\geq\epsilon/2$ . By assumption (1), $\int_a^b p(x)dx>0$ , so accordingly, $P(\exists i: x_i\in [a,b])=1$ . We can reorder finitely many samples without changing the distribution, so WLOG we may assume that $x_1\in [a,b]$ . As a consequence $2(c_p-f_p(x_1))\geq\epsilon$ . Combining with the above, we get $$n^{-1}\log \int l_{1,n}^2(x)p(x)dx\geq \epsilon$$ for all $n$ sufficiently large. By simple rearrangement, this is equivalent to $\int l_{1,n}^2(x)p(x)dx\geq e^{n\epsilon}$ and since $V_n=\int \sum_{i=1}^n l_{i,n}^2(x)p(x)dx \geq \int l_{1,n}^2(x)p(x)dx$ we get the claimed exponential growth, assuming $f_p$ is non-constant . So the last step is to show that $f_p$ is non-constant if $p$ is not the arcsine distribution, or equivalently, that the hilbert transform $H_p$ is not identically zero. Under assumption (2), this follows from a theorem of Tricomi . Thus we have proved the Claim. Code for Bernstein polynomials from scipy.special import binom import numpy as np import matplotlib.pyplot as plt variances=[] n_range=[5,10,20,50,100] for n in n_range: preds=[] for _ in range(1000): xs=np.linspace(0,1,100) X=np.linspace(0,1,n+1) Y=np.sin(8*X)+np.random.randn(n+1)*.1 nu=np.arange(n+1) bern=binom(n,nu)[:,None]*(xs[None,:])**(nu[:,None])*(1-xs[None,:])**(n-nu[:,None])*Y[:,None] pred=bern.sum(0) preds.append(pred) preds=np.array(preds) variances.append(preds.var(0).mean()) variances=np.array(variances) plt.scatter(n_range,variances) plt.xlabel('n') plt.ylabel('variance') plt.ylim(0,.005) plt.title('bernstein polynomial variance') Code for complex polynomials n=64 xarg=np.linspace(0,2*np.pi,n+1,endpoint=True)[:n] y=np.sin(2.5*xarg)+.25*np.random.randn(n) xcomp=np.exp(1j*xarg) xs=np.linspace(0,2*np.pi,200) ys=np.sin(2.5*xs) X=np.array([xcomp**i for i in range(len(xarg))]) w=np.linalg.solve(X,y) #only need to go up to middle frequency, bc later ones are complex conjugates interp=2*np.dot(w[1:n//2+1],np.array([np.exp(1j*xs*i) for i in range(1,n//2+1)]))+w2[0] plt.plot(xs,interp.real,label='complex polynomial interp') plt.plot(xs,ys,c='gray',linestyle='--',label='true function') plt.scatter(xarg,y) plt.xlabel('complex argument') plt.legend()
