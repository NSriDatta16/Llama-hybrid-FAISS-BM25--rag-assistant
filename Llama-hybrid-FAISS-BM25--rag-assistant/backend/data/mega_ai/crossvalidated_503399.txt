[site]: crossvalidated
[post_id]: 503399
[parent_id]: 
[tags]: 
why in regularisation, we'd assume a dummy basis function?

On p.328 of Introduction to Applied Linear Algebra – Vectors, Matrices, and Least Squares by Stephen Boyd and Lieven Vandenberghe , the authors write that for regularised data fitting, we should minimise $||y-A\theta||^2+\lambda||\theta_{2:p}||^2$ , and we'd assume $f_1$ is the constant function. But I can't seem to understand why we'd want to assume a constant function? I tried to found some explanation from the book and online, and I know the answer may be very obvious, but after some search but I still can't seem to understand why. Here's what I found: On the same page, the book said: There is one exception here: if $f_i(x)$ is constant (for example, the number one), then we should not worry about the size of $θ_i$ , since $f_i(x)$ never varies. But we would like all the others to be small, if possible. On p.138 of Pattern Recognition and Machine Learning, Christopher Bishop writes: It's often convenient to define an additional dummy function $\phi_0(x)=1$ ... Any further explanation would be greatly appreciated.
