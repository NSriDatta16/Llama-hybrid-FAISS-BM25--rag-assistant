[site]: crossvalidated
[post_id]: 530643
[parent_id]: 
[tags]: 
Result of a diagnostic test of a predictive model looking too good

I have created a predictive model that outputs a predictive density. I used 1000 rolling windows to estimate the model and predict one step ahead in each window. I collected the 1000 predictions and compared them to the actual realizations. I used several diagnostic tests, among them Kolmogorov-Smirnov. I saved the $p$ -value of the test. I did the same for multiple time series. Then I looked at all of the $p$ -values from the different series. I found that they are 0.440, 0.579, 0.848, 0.476, 0.753, 0.955, 0.919, 0.498, 0.997 . At first I was quite happy that they are much larger than 0.010 , 0.050 or 0.100 (to use the standard cut-off values). But then a colleague of mine pointed out that the $p$ -values should be distributed as $\text{Uniform}[0,1]$ under the null of correct predictive distribution, and so I should perhaps not be so happy. On the one hand, the colleague must be right; the $p$ -values should ideally be uniformly distributed. On the other hand, I have found that my model predicts "better" than the true model normally would; the discrepancy between the predicted density and the realized density is less than one would normally expect between the true density and the realized density. This could be an indication of overfitting if I were evaluating my model in-sample, but the model has been evaluated out of sample. What does this tell me? Should I be concerned with a diagnostic test's $p$ -values being too high? You could say this is just a small set of $p$ -values (just 8 of them) so anything could happen, and you might be right. However, suppose I have a larger set of $p$ -values that are closer to 1 than uniformly distributed; is that a problem? What does that tell me? Update: I have adapted the code by @jbowman and played with different values of $k$ and $n$ to arrive at the following histograms of $p$ -values from the Kolmogorov-Smirnov test. The relationship between the shape of the histogram and $k$ and $n$ seems nontrivial. For small $n$ (columns on the left), increasing $k$ (going down the rows) appears to push the $p$ -values towards $0$ . For large $n$ (columns on the right), increasing $k$ (going down the rows) appears to push the $p$ -values towards $1$ . For cases in between such as $n=30$ , I do not see a clear trend. (The number of simulation runs is $1000$ for each histogram.) You can experiment for yourself by running the code below: m=1e3 # number of simulation runs n=1e3 # size of training sample (length of rolling window) k=1e3 # size of test sample / number of rolling windows within one simulation run ks_value It is still a puzzle for me why this is happening. I can see that the $p$ -values of observations are autocorrelated across windows (within a simulation run) because of the overlap of the training windows that causes the estimated mean and estimated standard deviation to be autocorrelated. However, I do not see why this should skew their unconditional distribution towards $1$ . Actually, deliberately setting the estimated mean and estimated standard deviation to be fixed across windows (within a simulation run) yields KS test's $p$ -values that look fairly uniform when collected across the simulation runs. This appears to imply that the problem is caused by data leakage, i.e. test observations becoming training observations as the window rolls. But why/how exactly does that cause nonuniformity?
