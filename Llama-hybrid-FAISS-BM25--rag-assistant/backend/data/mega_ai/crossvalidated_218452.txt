[site]: crossvalidated
[post_id]: 218452
[parent_id]: 
[tags]: 
Optimal decision process to estimate Markov chain limiting distribution

Suppose there is a irreducible, reversible Markov chain with known states $1,\ldots,N$ and unknown transition matrix $T_{ij}$ and unknown limiting distribution $\pi_i$. I am able to repeatedly initialize the system in an arbitrary state $s$ then observe a random transition $s\rightarrow t$ where $t$ is a sample from $P(S_{k+1} | S_k=s)$ in the Markov chain. I would like an algorithm to choose the initial states $s_k$ dependent on the previous results $(s_1 \rightarrow t_1), \ldots, (s_{k-1} \rightarrow t_{k-1})$, to optimally estimate $\log \pi_2 - \log \pi_1$. A naive method would be to choose $s_k = t_{k-1}$ to obtain a sample of length $M$ from the Markov chain. Then, the estimator $\log \frac{1+\sum_k (s_k = 2)}{1+M} - \log \frac{1+\sum_k (s_k = 1)}{1+M}$ would converge to $\log \pi_2 - \log \pi_1$ as $M \rightarrow \infty$. This method is inefficient when either $\pi_1$ or $\pi_2$ is very small, which is the case I am interested in. Is there a better, or even optimal, scheme to choose $s_k$ dependent on $(s_1 \rightarrow t_1), \ldots, (s_{k-1} \rightarrow t_{k-1})$ to estimate $\log \pi_2 - \log \pi_1$ for a fixed number of observed transitions $M$?
