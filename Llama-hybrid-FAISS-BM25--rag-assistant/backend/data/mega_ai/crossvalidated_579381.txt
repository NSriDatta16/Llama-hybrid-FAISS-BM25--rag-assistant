[site]: crossvalidated
[post_id]: 579381
[parent_id]: 579368
[tags]: 
Mostly yes. Simple logistics: Symmetric trees were not around when the original XGBoost implementation came out. Nobody using XGBoost is bothered enough to code them as the current performance is deemed adequate. New base-learners are no clear wins. Initially, people thought for example that DART (Dropouts) was also a game-changer and it turned out to be... OKish...? Just to be clear, symmetric trees are not guaranteed to be better. If anything, we empirically know that CatBoost has not dominated any of the other implementations, it is as good as . Also given that we have so many different ways to regularise a GBM already, the ability to have one more regularisation lever is not so groundbreaking. Symmetric trees have a clear advantage when it comes to inference time but for GBM applications, the inference speed usually is a small aspect of the project. Contrary to that, Catboost's grandparent from Yandex, MatrixNet , was a GBM specifically geared towards recommender systems where inference time is crucial, and during a time when GPUs were not so mature; this has carried forward. In contrast, XGBoost, which started as part of the DCML , has moved towards high-performance computing on the GPU work first by cuBLAS and nowadays via Rapids , a far more GPU-first approach. (Side-note: using a GPU doesn't equate to HPC.)
