[site]: datascience
[post_id]: 86459
[parent_id]: 64120
[tags]: 
Luong's attention came after Bahdanau's and is generally considered an advancement over the former even though it has several simplifications. None of the pre-written layers I have seen, entirely implement Luong or Bahdanu's attention in entirety but only implement key pieces of those. It has been shown that major gains are seen in performance with the introduction of Attention in any basic form. The specific implementation does not seem to matter that much though there seems to be strong benefits in passing on the attention weights learnt to subsequent timesteps. Both Bahdanau and Luong and subsequent attention models that came out from 2014-2018 have now been replaced by self-attention in most cases. Self-attention was introduced by Google in 2018 (though they were probably inspired by an earlier paper on intra-attention) Since there are over a dozen flavors of attention and each flavor could have several ways of implementation, this can sometimes be a source of confusion to many. The below link contains a simplified rendering of the evolution of Attention and shows how to implement attention in 6 lines of code: https://towardsdatascience.com/create-your-own-custom-attention-layer-understand-all-flavours-2201b5e8be9e
