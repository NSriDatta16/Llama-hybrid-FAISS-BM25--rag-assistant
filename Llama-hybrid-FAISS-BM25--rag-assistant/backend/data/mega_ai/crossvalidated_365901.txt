[site]: crossvalidated
[post_id]: 365901
[parent_id]: 76191
[tags]: 
Let's say you observe a RV $X$ and you want to predict a second RV $Y$ where your predictor of $Y$ is $g(X)$. Then you can prove that the "best" predictor of $Y$ is $g(X) = E[Y \mid X]$, where "best" is the function $g$ that minimizes $E[(Y-g(X))^2]$. So $E[Y \mid X]$ will always be at least as good as, if not better than $E[Y]$. To prove that $E[Y|X]$ is the "best" predictor possible we just need to show that $$E[(Y-g(X))^2] \ge E[(Y-E[Y \mid X])^2]$$ Let's start off by looking at $E[(Y-g(X))^2 \mid X]$ which is not quite the same as the LHS above but not to worry, at the end we can leverage the property that $E[E[X|Y]] = E[X]$. So, $$E[(Y-g(X))^2 \mid X] = E[(Y - E[Y \mid X] + E[Y \mid X] - g(X))^2 \mid X]$$ $$=E[(Y- E[Y \mid X]))^2 \mid X]$$ $$ + \, E[(E[Y \mid X] - g(X))^2 \mid X]$$ $$ + \, 2E[(Y- E[Y \mid X])(E[Y \mid X] - g(X)) \mid X]$$ Where the above is basically doing $$(x+y)^2 = (x-c+c+y)^2 = (x-c)^2 + (x+c)^2+2(x-c)(x+c)$$ and using the linearity of expectations. Focusing on the third term of the expansion only, we can show that it equals zero. $$2E[(Y- E[Y \mid X])(E[Y \mid X] - g(X)) \mid X]=0$$ This is because given $X$, the term $E[Y \mid X] - g(X)$ is like a constant and can be pulled out of the expectation using the property that $E[aX]=aE[X]$. Letting the constant we pulled out equal $c$ we have $$c \cdot E[Y- E[Y \mid X] \mid X] = c \cdot (E[Y \mid X]- E[E[Y \mid X] \mid X] )$$ $$= c \cdot (E[Y \mid X] - E[Y \mid X]) = 0$$ From the property $E[g(X) \mid X] = g(X)$ we know that $E[E[Y \mid X] \mid X] = E[Y \mid X]$ since $E[Y \mid X]$ is a function of $X$. Therefore going back to the expansion, we have $$E[(Y-g(X))^2 \mid X] = E[(Y- E[Y \mid X]))^2 \mid X] + E[(E[Y \mid X] - g(X))^2 \mid X]$$ Noting that the second term on the RHS must always be positive, we can say $$E[(Y-g(X))^2 \mid X] \ge E[(Y- E[Y \mid X]))^2 \mid X]$$ and then taking the expected value of both sides, leveraging the property that $E[X] = E[E[X \mid Y]]$, we get $$E[(Y-g(X))^2] \ge E[(Y-E[Y \mid X])^2]$$ Which shows that the "best" predictor is $E[Y \mid X]$. Note that it will be as good as $E[Y]$ if $Y$ and $X$ are independent.
