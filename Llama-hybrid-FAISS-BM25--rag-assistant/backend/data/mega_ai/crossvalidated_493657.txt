[site]: crossvalidated
[post_id]: 493657
[parent_id]: 233625
[tags]: 
The classical parametric approach in frequentist statistics would be to model the situation as follows. Suppose all the people who see a certain advertisement are all independent of each other and have the same probability $p$ of clicking on the advertisement. We say these people form a Bernoulli population, i.e. a member $X$ of this population either reacts ( $X=1$ ) to the advertisement or does not ( $X=0$ ) with probabilities $p$ and $1-p$ respectively. Suppose you now take a sample of size $n$ ( $n=1000$ in your case) of the population, so you observe $X_1, \ldots, X_n$ and see how many of them have clicked. In other words, we observe the sample average $$ \bar{X} = \frac{1}{n} \sum_{i=1}^n X_i $$ In your case, you observe $\bar{X}=0.01$ and you are using this sample average as an estimate of the unknown population parameter $p$ , i.e. $\hat{p}=\bar{X}$ . The statistical exercise we have to make now is deciding how 'reliable' or how 'precise' this estimate is. The central question here is: assuming the population fraction is indeed $p$ , then what values of our estimator $\bar{X}$ are we likely to observe in a sample of size $n$ ? This brings probabilities into to the story. Specifically, we know (possibly to be explained further) that the sample sum $n\bar{X}$ is binomially distributed: $$ \text{Prob}[n\bar{X} = i] = \binom{n}{i} p^i(1-p)^{n-i}\,,\qquad i=0,\ldots,n $$ This is the point now where most people will suggest the approximation $\bar{X}\sim \text{N}(p,\frac{p(1-p)}{n})$ which works fine if $n$ is large compared to $np$ . So instead of the discrete binomial distribution, we now use the continuous normal distribution which is somewhat easier to work with. For example, it follows (could also be explained further) that $$ \frac{\bar{X} - p}{\sqrt{\frac{p(1-p)}{n}}} \sim \text{N}(0,1) $$ has the standard normal distribution. Note that this distribution now no longer depends on $p$ or any other population parameter and for that reason, we call this quantity a pivot . Pivots are extremely useful to construct hypothesis tests and/or confidence intervals from. In our case, let $z_{\alpha/2}$ and $z_{1-\alpha/2}$ be the left and right quantiles of the standard normal distribution (in case $\alpha=0.05$ then these are the familiar values -1.96 and 1.96 respectively), then using the known distribution of the pivot: $$ 1-\alpha = \text{Prob}[ z_{\alpha/2} which after some manipulations becomes $$ 1-\alpha = \text{Prob}[ \bar{X} - z_{1-\alpha/2} \sqrt{\frac{p(1-p)}{n}} These bounds form an (unobserved) confidence interval. Note that the left and right bound are not proper statistics. That means that strictly speaking, these left and right bounds can not be computed from a sample because we don't know what $p$ is! Yet another pragmatic approximation is to use the estimate $\hat{p}=\bar{X}$ instead of $p$ in these bounds. That brings us finally to: $$ \text{a } (1-\alpha)\text{-CI for }p\text{ is }[\bar{X} - z_{1-\alpha/2} \sqrt{\frac{\bar{X}(1-\bar{X})}{n}} , \bar{X} - z_{\alpha/2} \sqrt{\frac{\bar{X}(1-\bar{X})}{n}}] $$ This often used way of constructing a CI is the culmination of two approximations: First we use CLT to apprimate the binomial distribution by a normal distribution and then we estimate the standard error by using $\hat{p}=\bar{X}$ . In most cases however, the resulting CI is useful. Sometimes however, the lower bound can be negative or the upper bound larger than 1 which makes no practical sense for a parameter like $p$ . For $\alpha=0.05$ , $n=1000$ and $\bar{X}=0.01$ we have the CI $[0.0038,0.0162]$ .
