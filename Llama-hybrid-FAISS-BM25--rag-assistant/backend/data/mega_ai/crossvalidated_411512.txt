[site]: crossvalidated
[post_id]: 411512
[parent_id]: 
[tags]: 
Is Variational Bayes (VB) and Mean-Field Approximation Useful In practice

I have just had a course in Bayesian Inference, and I am left puzzled about what method should I actually use in practice. Assume I have a multivariate model with multiple parameters $\theta$ , where all priors $p[\theta]$ are normally distributed and the likelihood $P[x|\theta]$ is not a simple function. I want to compute the model evidence $P[x]$ (to do model selection), the maximum a posteriori estimate $\theta_{MAP}$ , as well as make some reasonable approximation of the posterior $P[\theta | x]$ . Methods I have learned: Computing $P[x]$ numerically, e.g. using MCMC , Chib approximation, etc. Can be made to work for any problem given sufficient time. Suffers from curse of dimensionality. Does not require VB Conjugate Priors . Works for most common prior and likelihood pairs, otherwise does not work at all. Does not require VB Laplace's approximation . For simple prior and likelihood, mean and covariance are analytically computable, otherwise can use MCMC except it converges faster because we only care about mean and variance, not the integral. Does not require VB Mean-Field Approximation . Uses VB :). Approximates model evidence with negative free energy. Computes marginal distributions of parameters analytically. Then analytically computes other scary integrals to find relationships between optimal parameter values, which are then finally solved iteratively. We were not clarified what to do if any of the marginalization or optimization integrals can not be done analytically. Questions: Is VB useful in practice? Is the additional analytic effort justified in optimizing total time to do inference (human time + machine time)? Are the mentioned integrals computable for non-basic distributions? If no, does one proceed to compute the integrals numerically? How? Is numerical VB still better than the other mentioned methods?
