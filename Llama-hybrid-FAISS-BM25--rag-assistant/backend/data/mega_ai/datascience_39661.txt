[site]: datascience
[post_id]: 39661
[parent_id]: 
[tags]: 
Validation loss is lower than the training loss

I am using autoencoder for anomaly detection in warranty data. Architecture 1: The plot shows the training vs validation loss based on Architecture 1. As we see in the plot, validation loss is lower than the train loss which is totally weird. Based on the post Validation loss lower than training loss , I understood that it is because of the dropout layer in my model. So I ran the model with dropout layer. Architecture 2: Based on the above architecture, I plotted the training vs validation loss. Now validation loss is a bit higher than the training loss. Clearly it is because of the dropout layer. Now my question is, whether the model based on Architecture 1 is correct or not? If it is not correct, what kind of changes I could do to make it work? Thank you ! Any help is much appreciated!!
