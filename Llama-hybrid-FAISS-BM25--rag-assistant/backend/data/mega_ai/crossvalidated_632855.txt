[site]: crossvalidated
[post_id]: 632855
[parent_id]: 386369
[tags]: 
@ShawnHemelstrand's answer contains much of what is needed to get to an understanding of these issues, but there is something that I can add that will help, particularly with issues like those raised in @FrankHarrel's answer. The Neyman and Pearson sentences often quoted are not always fully explained. (I quote them here by snipping from Shawn's answer. He provides the source.) We are inclined to think that as far as a particular hypothesis is concerned, no test based upon the theory of probability can by itself provide any valuable evidence of the truth or falsehood of that hypothesis. That sentence is their justification for forgoing any treatment of the data as straightforward evidence for or against the null hypothesis. The next paragraph then goes on to set out the framework for an evidence agnostic approach to decision-making. But we may look at the purpose of tests from another view-point. Without hoping to know whether each separate hypothesis is true or false, we may search for rules to govern our behaviour with regard to them, in following which we insure that, in the long run of experience, we shall not be too often wrong. That means that their purpose of the tests is not to extract a useable sort of evidence from the data and statistical model, but to use them to make algorithmic decisions that will not 'too often' be wrong. The method that they go on to describe then provides an algorithm that will work well 'in the long run'. Note that the decision is not much informed by the strength of the evidence! As long as the test statistic falls in the 'critical region' the decision is to discard the null hypothesis. The decision is the same for a test statistic value that is at the inside edge of the critical region as it is for one that falls well inside the region. In p-value terms, that sentence becomes (assuming we are dealing with a predetermined $\alpha$ of 0.03) 'the decision is the same for an observed p-value of 0.029 as it is for a value of 0.00000000001'. Another way to see how the strength of evidence is ignored in the Neyman–Pearsonian method is to consider that p-values of 0.031 and 0.029 would yield the opposite decisions. The Neo-Fisherian responses to those same p-values might be that p=0.029 (and p=0.031) shows only modestly strong evidence against the null, but that evidence is enough to be interesting and so therefore the experiment should be run again or developed further. The result of p=0.00000000001 corresponds to a strength of evidence against the null hypothesis in question to be compelling enough for the question to be considered to be answered. Fisher's approach was to characterise the evidence in the data concerning the particular null hypothesis in question. The Fisherian approach has the data converted to evidence against a 'local' hypothesis and so it is helpful to call that evidence 'local'. That is the idea that the first sentence of quoted Neyman & Pearson speaks against. The Neyman–Pearsonian approach yields a decision. That decision may be correct or incorrect as detailed in the table that Shawn includes, but there is no way to say whether it will be right or wrong for any particular 'local' hypothesis tested. What can be said is the global probability of error associated with the repeated application of the method in the long run. The Neyman–Pearsonian methods are designed with the global error rates in mind, not the properties of any particular decision regarding a local hypothesis. The evidence-ignoring aspects of the Neyman–Pearsonian probably makes many of us so uncomfortable that it needs some justification. What is the benefit of working within that framework? Well, it enables simple accounting. The long run error rates that can be expected from the faithful application of the algorithm can be specified. They can be 'controlled' to the extent that the false positive errors are pre-set in the selection of $\alpha$ and the false negative errors are 'controlled' by choice of sample size and the choice of a most powerful test procedure. Now, finally, to the point raised by Frank Harrel's answer. Is the probability of being wrong a Bayesian probability? Yes, it would be for any decision made on the basis of a neo-Fisherian analysis of the strength of evidence. But if we are talking about the global long run probability of error associated with application of the Neyman–Pearsonian framework that probability is a well-defined frequency determined by the design of the analysis. That frequency comes from the (theoretical) long run application of the method, not from any repeated testing of the local hypothesis of interest. Its expression as a probability needs no prior as it is purely frequentist and non-Bayesian.
