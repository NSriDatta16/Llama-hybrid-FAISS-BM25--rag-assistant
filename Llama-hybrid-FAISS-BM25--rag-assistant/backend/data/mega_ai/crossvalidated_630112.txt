[site]: crossvalidated
[post_id]: 630112
[parent_id]: 
[tags]: 
Feature selection in a traditional regression model to an experiment data

I have an experiment data (total of 96) with 10 predictor and 2 response variables. I want to build a traditional multiple linear regression model to them in R. My aim is to build clearly interpretable (coefficients) and explanatory models for a journal article. Not all variables being significant, I have been trying to reduce the number of variables by checking their t and f table p values as well as the VIF scores to reduce multicollinearity among them. However, rather than this manual selection, I want to use a straightforward and commonly accepted automated feature selection. I have been doing stepwise regression and I stopped after reading all the statistical doubts about it in the forum. I have read many people suggest Lasso models or some more machine learning related sophisticated approach for this. But, I want to keep this process as simple as possible such that the people in my area (engineering) can easily understand it. Does anyone have any other suggestion rather than stepwise regression for this automated process? Note: I have realized some interaction variables play important role in my dataset, so I want to include them as well into this trial (even though the multicollinearity coming with it).
