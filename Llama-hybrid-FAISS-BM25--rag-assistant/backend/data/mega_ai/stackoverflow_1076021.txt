[site]: stackoverflow
[post_id]: 1076021
[parent_id]: 1075250
[tags]: 
FWIW, on the last high-data-volume project I was on, we found filtering, aggregating and pre-classifying data using some heavily-tuned C code was key. All our feeds went into this pre-processor and it took care of the simple data cleansing before passing the bulk of the data to our Java-based system for processing. Basically the pre-processor did just what you're asking: identifying records of interest, verifying they were complete and removing dups and empties. During peak times the pre-processor could eliminate up to 20% of the 8M or so records we'd get per hour (probably not quite the volume I imagine you get from stock market feeds). Our original Java version was lucky to get half that (but it was "elegant", at least!)
