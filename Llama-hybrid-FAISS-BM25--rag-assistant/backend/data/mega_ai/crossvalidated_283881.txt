[site]: crossvalidated
[post_id]: 283881
[parent_id]: 283760
[tags]: 
Horace is right. When you think about OOB, you aren't exactly keeping it out of your model. Each tree may have different samples in-bag and out-of-bag but the forest uses them all. This allows information leakage. The aggregation has its weaknesses. My understanding is that aggregation is accomplished as "weighted average" or "weighted vote". If there are two trees with equal "true" predictive ability over the population of true data, but one of them has higher effective accuracy over the training sample, then it will be given higher weight. The problem with that is, all else being equal, if it has higher accuracy on the training sample then it will have lower accuracy on values outside that sample, that is to say, in what is left of the general population. In this case OOB will have added error by how the bias impacts tree-weight in the aggregation. For a single cross-validation then, imagine the same circumstance: two trees of equal "true" performance on the population, but one has better training results than the other. The holdback portion doesn't inform any of the trees, but it does allow unbiased estimation of performance. The testing error shows that the tree that had better "in-bag" performance will have equal to its counterpart on overall data, as long as the hold-out set is large enough. Classic OOB wouldn't show that. References: http://file.scirp.org/pdf/OJS20110300008_18086118.pdf http://kldavenport.com/wp-content/uploads/2014/05/random_forests.pdf http://appliedpredictivemodeling.com/blog/2014/11/27/08ks7leh0zof45zpf5vqe56d1sahb0
