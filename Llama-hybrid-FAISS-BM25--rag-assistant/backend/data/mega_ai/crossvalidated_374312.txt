[site]: crossvalidated
[post_id]: 374312
[parent_id]: 
[tags]: 
How to turn a time series problem into a cross-section problem

A fairly frequent problem in the world of machine learning and predictions in general are the cases of cross-section and time series predictions. A time series is represented basically by an observation over time measured in constant time spaces. For example the sales of one store per day, where every remark is the sum of the store's sale in one day. Another possibility would be a cross-section which is a photograph of a single moment of time (which may add several moments). For example, each observation can be a different store and each column the value of the sale in a period (Example: a day, a month or other aggregations). Doubt is, how best to transform a time series problem (when we have observations over time) into a cross-section problem. We can simply consider each day of sale as a cross-section even though it is not independent? the traditional algorithms of machine learning (thinking of predicting any target) would normally work in this case?
