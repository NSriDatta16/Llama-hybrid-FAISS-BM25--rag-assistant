[site]: crossvalidated
[post_id]: 485957
[parent_id]: 485932
[tags]: 
Given that the nonlinear transform is invertible, it won't have a visible affect for the random forest because decision trees trained internally will find split points for these features. For example, if with original features the best split point (e.g. in terms of entropy or gini measure) is between 2-3, after the transformation it'll be between 2-4. For neural networks, the scale of the feature changes. This might affect gradient descent performance if the scale becomes too high. For example, the transformation $100^x$ , even after feature standardisation will create data points that seem like outliers.
