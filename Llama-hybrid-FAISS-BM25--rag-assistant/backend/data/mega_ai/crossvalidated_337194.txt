[site]: crossvalidated
[post_id]: 337194
[parent_id]: 
[tags]: 
Need help conceptualizing MLE for stochastic processes?

I recently learned how to perform some Maximum-Likelihood Estimation, and thought I had a fair grasp of it. For example, for the normal distribution where both $\mu$ and $\sigma^2$ are unknown for some dataset, an algorithm to maximize the log-likelihood will simply keep iterating until some satisfactory conditions for the 2 estimates are met. Though this is a great simplification, it is overall accurate. However, for stochastic processes that include a random variable, something like this basic state-space equation, $\begin{equation} y_t = \mu_t*G + \epsilon_t, \ \ \epsilon \sim (0,\sigma_{\epsilon}^{2}) \, , \end{equation}$ the intuition now escapes me, because each "modelling" of a random process for some given parameters produces a different, though similar, path. So if each path produced by such a process is at least slightly different, then how do MLE processes even work? Do they simulate each model with the same "parameter set" multiple times and then take the average log-likelihood, and use that, or do they do something else? I would highly appreciate it if anyone theoretically knowledgeable could shed some light for me. EDIT: It seems I did not clarify myself adequately. This is an example picture of a state-space model fitted to some data: with the "stochastic level" referring to the eq. used, which is of this form, Both of these were taken from Commandeur and Koopman's "State Space Time Series Analysis", pg.s 9-18. The model above is pretty nicely fitted, but my core question is , if an equation of the specified form above was used, then each time it is "run" or used to generate a path, it will generate a unique path because of the irregular elements ($\epsilon_t$ and the other variable). If so, how does Maximum-Likelihood Estimation work if one set of parameters can generate completely different paths, and thus different log-likelihoods?
