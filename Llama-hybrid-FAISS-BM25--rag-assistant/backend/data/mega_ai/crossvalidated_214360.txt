[site]: crossvalidated
[post_id]: 214360
[parent_id]: 
[tags]: 
What are the effects of depth and width in deep neural networks?

How does depth and width in neural networks affect the performance of the network? For example, He et al. introduced very deep residual networks and claimed “We obtain [compelling accuracy] via a simple but essential concept— going deeper.” On the other hand Zagoruyko and Komodakis argues that wide residual networks “are far superior over their commonly used thin and very deep counterparts.” Can someone summarise the current (theoretical) understanding in deep learning about the effects of width and depth in deep neural networks?
