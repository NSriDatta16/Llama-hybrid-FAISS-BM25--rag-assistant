[site]: datascience
[post_id]: 124667
[parent_id]: 
[tags]: 
Custom objective function for xgboost to optimize lift in best decile

I've tried to define a custom objective function for xgboost to optimize the lift for a binary classification problem in the upper decile. The task is simply to concentrate the training effort to the (say) 10% of data points that are the easiest to predict correctly. The problem is also formulated for a practical case here: https://stats.stackexchange.com/questions/202438/optimizing-cumulative-lift-in-classification . I'm not sure if this is a well defined problem, but in any case, my idea is to sort the scores, pick out the best 10% and concentrate the cost function to those. The sorting and cutoff are however non-differentiable, so I use the fast soft sort algorithm ( https://github.com/google-research/fast-soft-sort ) together with a smooth Heaviside function instead. Then I can compute the Jacobian, but it seems perhaps the Hessian is not defined for the soft sort. This is as far as I've got: import numpy as np from fast_soft_sort.numpy_ops import soft_rank def smooth_heaviside(x, smooth=0.3, offset=0.): return 1./(1. + np.exp(-(1./smooth)*(x-offset))) def smooth_heaviside_derivative(x, smooth=0.05, offset=0.): # offset should be set to the fraction above which to optimize the lift f = smooth_heaviside(x, smooth=smooth, offset=offset) return (1./smooth)*f*(1.-f) def compute_cost_and_jacobian(y_pred, y_label, smooth=0.001, frac=0.8, reg_strength=1.): N = y_pred.shape[0] sr = fast_soft_sort.numpy_ops.SoftRank(y_pred, regularization_strength=reg_strength) R = sr.compute() # rank Hs = smooth_heaviside(R/N, smooth, frac) C = -sum(Hs*y_label) Hs_d = smooth_heaviside_derivative(R/N, smooth, frac) Cj = -sr.jvp((1./N)*Hs_d*y_label) return C, Cj def smooth_rank_lift(y_true, y_pred): C, Cj = compute_cost_and_jacobian(y_pred, y_true, smooth=0.2, frac=0.8, reg_strength=0.0001) grad = Cj hess = 0*Cj # ? print(np.mean(grad)) return grad, hess But using this as the argument for 'objective' with the XGBClassifier fails to produce anything but the initial guess with predict_proba (Iâ€™m not very experienced with xgboost). I'm guessing a lot of things may be wrong here, but does this seem like a viable approach? Any other ways of solving this problem?
