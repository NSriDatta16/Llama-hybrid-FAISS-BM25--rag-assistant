[site]: crossvalidated
[post_id]: 22424
[parent_id]: 22422
[tags]: 
I think "error" is best described as "the part of the observations that is unpredtictable given our current information". Trying to think in terms of population vs sample leads to conceptual problems (well it does for me anyway), as does thinking of the errors as "purely random" drawn from some distribution. thinking in terms of prediction and "predictability" makes much more sense to me. I also think the maximum entropy principle provides a neat way to understand why a normal distribution is used. For when modelling we are assigning a distribution to the errors to describe what is known about them. Any joint distribution $p(e_{1},\dots,e_{n})$ could represent a conceivable state of knowledge. However if we specify some structure such as $E(\frac{1}{n}\sum_{i=1}^{n}e_{i}^2)=\sigma^2$ then the most uniform distribution subject to this constraint is the normal distribution with zero mean and constant variance $\sigma^2$. This shows that "independence" and "constant variance" are actually safer than assuming otherwise under this constraint - namely that the average second moment exists and is finite and we expect the general size of the errors to be $\sigma$. So one way to think of this is that we do not necessarily think our assumptions are "correct" but rather "safe" in the sense that we are not injecting a lot of information into the problem (we are imposing just one structural constraint in $n$ dimensions). so we are starting from a safe area - and we can build up from here depending on what specific information we have about the particular case and data set at hand.
