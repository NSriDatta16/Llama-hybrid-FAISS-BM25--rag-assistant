[site]: crossvalidated
[post_id]: 314072
[parent_id]: 183236
[tags]: 
Intuitive relationship of PCA and KMeans Theoretically PCA dimensional analysis (the first K dimension retaining say the 90% of variance...does not need to have direct relationship with K Means cluster), however the value of using PCA came from a) practical consideration given the nature of objects that we analyse tends to naturally cluster around/evolve from ( a certain segment of) their principal components (age, gender..) b) PCA eliminates those low variance dimension (noise), so itself adds value (and form a sense similar to clustering) by focusing on those key dimension In simple terms, it is just like X-Y axis is what help us master any abstract mathematical concept but in a more advance manner. K Means try to minimize overall distance within a cluster for a given K For a set of objects with N dimension parameters, by default similar objects Will have MOST parameters “similar” except a few key difference (eg a group of young IT students, young dancers, humans… will have some highly similar features (low variance) but a few key features still quite diverse and capturing those "key Principal Componenents" essentially capture the majority of variance, eg. color, area of residence.... Hence low distortion if we neglect those features of minor differences, or the conversion to lower PCs will not loss much information It is thus “very likely” and “very natural” that grouping them together to look at the differences (variations) make sense for data evaluation (eg. if you make 1,000 surveys in a week in the main street, clustering them based on ethnic, age, or educational background as PC make sense) Under K Means’ mission, we try to establish a fair number of K so that those group elements (in a cluster) would have overall smallest distance (minimized) between Centroid and whilst the cost to establish and running the K clusters is optimal (each members as a cluster does not make sense as that is too costly to maintain and no value) K Means grouping could be easily “visually inspected” to be optimal, if such K is along the Principal Components (eg. if for people in different age, ethnic / regious clusters they tend to express similar opinions so if you cluster those surveys based on those PCs, then that achieve the minization goal (ref. 1) Also those PCs (ethnic, age, religion..) quite often are orthogonal, hence visually distinct by viewing the PCA However this intuitive deduction lead to a sufficient but not a necessary condition. (Ref 2: However, that PCA is a useful relaxation of k-means clustering was not a new result (see, for example,[35]), and it is straightforward to uncover counterexamples to the statement that the cluster centroid subspace is spanned by the principal directions.[36]) Choosing clusters based on / along the CPs may comfortably lead to comfortable allocation mechanism This one could be an example if x is the first PC along X axis: (...........CC1...............CC2............CC3 X axis) where the X axis say capture over 9X% of variance and say is the only PC Finally PCA is also used to visualize after K Means is done (Ref 4) If the PCA display* our K clustering result to be orthogonal or close to, then it is a sign that our clustering is sound , each of which exhibit unique characteristics (*since by definition PCA find out / display those major dimensions (1D to 3D) such that say K (PCA) will capture probably over a vast majority of the variance. So PCA is both useful in visualize and confirmation of a good clustering, as well as an intrinsically useful element in determining K Means clustering - to be used prior to after the K Means. References: https://msdn.microsoft.com/en-us/library/azure/dn905944.aspx https://en.wikipedia.org/wiki/Principal_component_analysis Clustering using principal component analysis: application of elderly people autonomy-disability (Combes & Azema) http://cs229.stanford.edu/notes/cs229-notes10.pdf Andrew Ng
