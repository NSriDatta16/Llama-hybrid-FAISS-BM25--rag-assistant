[site]: stackoverflow
[post_id]: 2121777
[parent_id]: 2026071
[tags]: 
I don't know how the Q20 neural network works, but you can use neural networks to train the connections in a Belief Network. Essentially, every question has a "desire to be asked" which is modified based upon the output of the previously-asked question. In other words, the 3 questions with the highest initial "desire" might be "Is it animal?" "Is it vegetable?" or "Is it mineral?" Let's just say that you split the tie and go with "Is it animal?" If the person answers "Yes", you increase the probability / desire of asking questions such as "Does it have hair?" and decrease the probability of questions such as "Is it a legume?" In this scheme, it would be the parameters of the delta probability function that could be done with a neural net. In other words, the decision as to how much to increase or decrease the probability of next asking "Does it have hair?" could be done based on the success or failure of "Is it animal?"-> TRUE, "Does it have hair->"...[Result] over a very large number of games. Note that while this is possible to do with a NN, it's not at all intuitive that a NN would be efficient -- statistical methods going all the way back to Bayes and up to "Support Vector Machines" would be alternative ways of modifying the linkage between questions to learn better. Based on my experience with AI, I would suspect that you wouldn't have success just throwing all the questions into a pot and completely cross-connecting them, but would rather have to have a series of sub-nets that achieved goals. I don't know for sure, though. One of the things that's striking about the Q20 algorithm is that it doesn't seem to have discrete sub-goals -- I think you can see any question at any time. It's very impressive.
