[site]: crossvalidated
[post_id]: 176908
[parent_id]: 176892
[tags]: 
For a very probabilistic application of ML, take a look at Markov Fields (undirected graphical models) that are used in image processing. For a more stochastic-processes flavor, it would be interesting to study the path-properties of the test vs training MSE for a simple machine learning model applied to gaussian data (e.g., K-nearest neighbors under a gaussian mixture vs single multivariate gaussian). For example: The training error will be a supermartingale wrt model complexity (i.e., fewer neighbors = more complexity) However, the test error will generally be convex or quasi-convex (on average) It would be interesting to examine the stochastic properties of these two paths (i.e., under repeated sampling from the underlying distribution). In particular, how well can one identify the minimum of curve 2? (Compare this to the "1-standard deviation rule", where you find the empirical minimum and then reduce the complexity until the test error curve intersects the line located one standard deviation above the minimum empirical test error). Just some thoughts. Someone just asked a question about Markov Fields here: Modeling a Classification Problem with an Undirected Graphical Model That should give you some more ideas.
