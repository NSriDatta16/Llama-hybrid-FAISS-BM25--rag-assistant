[site]: crossvalidated
[post_id]: 384964
[parent_id]: 384924
[tags]: 
It works well but only if the features are properly prepared so that the order of the lines is not important anymore. E.g. for a univariate time series $y_i$ , you would use $y_i$ as response and e.g. the following features: Lagged versions $y_{i-1}$ , $y_{i-2}$ , $y_{i-3}$ etc. Differences of appropriate order, e.g. $y_{i-1} - y_{i-2}$ , $y_{i-1} - y_{i-8}$ (if there is weekly seasonality expected and the observations occur daily) etc. Integer or dummy coded periodic time info such as month in year, week day, hour of day, minute in hour etc. The same approach works for different modelling techniques, including linear regression, neural nets, boosted trees etc. An example is the following (using a binary target "temperature increase" (y/n)): library(tidyverse) library(lubridate) library(ranger) library(MetricsWeighted) # AUC # Import raw % mutate(Date = ymd(Date), y = year(Date), m = month(Date), d = day(Date), increase = 0 + (Temp > lag(Temp))) with(prep, table(y)) summary(prep) # Plot full data -> year as seasonality ggplot(data = prep, aes(x = Date, y = Temp))+ geom_line(color = "#00AFBB", size = 2) + scale_x_date() # No visible within year seasonality prep %>% filter(y == 1987) %>% ggplot(aes(x = Date, y = Temp))+ geom_line(color = "#00AFBB", size = 2) + scale_x_date() # Add some lags and diffs & remove incomplete rows prep % mutate(lag1 = lag(Temp), lag2 = lag(Temp, 2L), lag3 = lag(Temp, 3L), dif1 = lag1 - lag2, dif2 = lag2 - lag3) %>% filter(complete.cases(.)) # Train/valid split in blocks valid % filter(y == 1990) train % filter(y % mutate(residuals = increase - fit_rf$predictions[, 2]) %>% filter(y == 1987, m == 3) ggplot(random_month, aes(x = Date, y = residuals))+ geom_line(color = "#00AFBB", size = 2) + scale_x_date() Replacing variables "y" and "m" by factors would probably improve the logistic regression. But since the question was about random forests, I leave this to the reader.
