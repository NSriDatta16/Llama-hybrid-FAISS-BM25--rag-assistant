[site]: crossvalidated
[post_id]: 476133
[parent_id]: 
[tags]: 
How to benchmark performance of an autoencoder

An Autoencoder is defined as a device that can extract useful features from data, and also use those features to reconstruct initial data. I'm trying to understand what the word "useful" means in a quantitative manner. Most sources I can find (e.g. Hinton paper ) attempt to answer usefulness in a qualitative way. They cluster hidden layer values, color them by some supervised label, and state that the labels look separable, whatever that means. Let's say for simplicity that I want to train a linear single hidden layer autoencoder on ImageNet or MNIST. I can set the number of neurons in my hidden layer to anything between 1 to the number of pixels in the original image, and even beyond. I would expect the reconstruction error to monotonically decrease with hidden layer size. But I don't know explicitly how much of the data is useful features and how much is not. Can I still benefit in any way from knowing the value of the reconstruction error? I could further proceed to train a classifier from a hidden layer to the data label, and evaluate the usefulness of the hidden layer by the performance of that classifier. However, this metric is not necessarily specific to the quality of the representation, as it also depends on for example, (a) the intrinsic performance of the classifier (b) potential sensitivity of the classifier to the number of input parameters. Is there a canonical way to formalize usefulness quantification? My ideas would be to either bypass classification network completely and use something like clustering coefficient within vs across labeled hidden datapoints, or to use some very strictly defined classifier that is somehow guaranteed to be stable.
