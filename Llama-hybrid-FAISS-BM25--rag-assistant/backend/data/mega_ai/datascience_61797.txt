[site]: datascience
[post_id]: 61797
[parent_id]: 61771
[tags]: 
Question 1: Bagging (Random Forest) is just an improvement on Decision Tree; Decision Tree has lot of nice properties, but it suffers from overfitting (high variance), by taking samples and constructing many trees we are reducing variance, with minimal effect on bias. Boosting is a different approach, we start with a simple model that has low variance and high bias, and add new models sequentially to reduce bias. If we used deep trees, we would run high risk of overfitting. Question 2: Gradient Boosting with deeper trees will allow you to fit a very complex relationship; higher variance lower bias. This will reduce error due to bias. Random Forest with shallow trees will have lower variance and higher bias, this will reduce error do to overfitting. It is possible that Random Forest with standard parameters is overfitting, so reducing depth of trees improves the performance.
