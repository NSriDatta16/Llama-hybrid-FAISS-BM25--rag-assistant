[site]: crossvalidated
[post_id]: 464137
[parent_id]: 
[tags]: 
Improve stability for value-based Deep Reinforcement Learning

Background The main difference between Q-Learning and Deep Q-Learning is that we cannot update the Q-value for a single state-action pair using Deep Q-Learning. Every time we update the policy, the whole distribution over the action space changes. Question I am wondering if there are methods to overcome this specific problem for value-based RL, so that we can have the best of Q-Learning (i.e convergence is mathematically guaranteed) and the best of Deep Q L-Learning (i.e. possibility to generalize over a continuous state space). I am familiar with different possible improvements (Double Deep Q-Learning, Dueling Deep Q-learning, the use of target networks, different strategies for experience replay), but I was wondering if there are ways to improve this specific cause of instability, which is inherent in Neural networks.
