[site]: crossvalidated
[post_id]: 258848
[parent_id]: 
[tags]: 
Using random forest/boosting for classification when important features are known

Let's assume that we have a pretty decent idea of which features are important, and let's assume that those features are not a lot, like 2 or 3, some categorical, some numerical. Is there any reason to use random forest or boosting over other classification methods like SVM with regularization for cases like this? I thought random forest or boosting are effective over other methods when there are a lot of features, and we don't know selecting which features would be optimal. In cases like this in which the important features are known and there are not a lot of them (like 2~3), are there methods that generally outperform random forest or boosting?
