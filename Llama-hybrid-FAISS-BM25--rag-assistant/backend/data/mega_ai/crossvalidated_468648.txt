[site]: crossvalidated
[post_id]: 468648
[parent_id]: 226923
[tags]: 
ReLU is probably one of the simplest nonlinear function possible. A step function is simpler. However, a step function has the first derivative (gradient) zero everywhere but in one point, at which it has an infinite gradient. ReLU has a finite derivative (gradient) everywhere. It has an infinite second derivative in one point. The feed forward networks are trained by looking for a zero gradient. The important thing here is that there's a lot of first derivatives to calculate in a large net's backpropagation routine, and it helps that they are fast to compute like ReLU. The second is that unlike step function, ReLU's gradients are always finite and they're not trivial zeros almost everywhere. Finally, we need nonlinear activations for the deep learning net to work well, but that's different subject.
