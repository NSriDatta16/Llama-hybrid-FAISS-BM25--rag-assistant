[site]: crossvalidated
[post_id]: 244581
[parent_id]: 244567
[tags]: 
Firstly, this is aside from your main points but it's worth mentioning. In the medical trial you could have 1000 people testing a drug which can be given to the 10000 people who are sick annually. You might look at that and think "That's being tested on 10% of the population", in fact the population isn't 10000 people, its all future patients so the population size is infinite. 1000 people isn't large compared to the infinite potential users of the drug but these kinds of studies work. It's not important whether you test 10%, 1% or 0.1% of the population; what's important is the absolute size of the sample not how big it is compared to the population. Next, your main point is that there are so many confounding variables which can influence people's voting. You're treating the 22000 districts of California like 22000 variables but really they are just a handful of variables (income and education like you mentioned). You don't need a representative sample from every district, you just need enough samples to cover the variation due to income, education, ect. If you have $k$ confounding variables (age, gender, education, ect) and they all have similar effects then the variance of the vote increases by about $k$ times. If you sample $n$ people then the variance of the sample average decreases by a factor of $n$. Therefore, if the variation from each confounding variable is $\sigma^2$ then your sample average from $n$ people with $k$ confounding variables will be $\frac{k\sigma^2}{n}$. You can probably think of 10 or so confounding variables but the sample size is 1000 so $k$ is a lot smaller than $n$. Therefore the variance of the sample average is quite small. Edit: The above formula was assuming that each confounding variable is equally important. If we want to consider hundreds of things that can add variance to the results then this assumption isn't valid (e.g. maybe twitter users support one candidate more, but we know that twitter use isn't as important as gender). We could list all the confounding variables in order of importance (e.g. gender, age, income, ... , twitter use, ...). Let's assume that each variable is only 90% as important as the previous one. Now if gender adds a variance equal to $\sigma^2$ then age adds a variance equal to $0.9 \sigma^2$ and income adds $0.9^2 \sigma^2 $. If we include an infinite number of confounding variables then the total variability is $\sum_{n=0}^{\infty} \sigma^2 0.9^n = 10 \sigma^2$. With this type of consideration for minor variables we've ended up with a variance with 10 times the variability of gender alone. So with $n$ samples the variation in the sample average is $\frac{10\sigma^2}{n}$. Of course $0.9$ was chosen arbitrarily but this conveys a point about how these infinite number of minor variables should add up to something small
