[site]: crossvalidated
[post_id]: 186725
[parent_id]: 
[tags]: 
Time series analysis to quantify trend when seasonal amplitude is decreasing

Short version: How would one be able to quantify an intervention effect in time-series analysis when the intervention decreases seasonal amplitude variation but doesn't directly effect the median? Here is a link to my raw data. I have a complex time-series of daily incidence numbers for a population over 7 years, totaling 2557 observations. There is a strong weekly and yearly seasonality (high incidence in winter months and low incidence in summer months). There is a baseline negative trend which is orders of magnitude smaller than the seasonality. An intervention was introduced at time = 1700. This intervention should theoretically not cause a level shift. My aim is to detect whether the intervention increases the baseline negative trend. I have attempted to fit a dynamic linear regression with ARIMA errors in R using auto.arima() in the forecast package. I modeled the weekly season using a dummy variable for each weekday and the weekend. I modeled the monthly seasonality with harmonics using fourier() function in the forecast package. An the intervention effect was coded in by specifying the time index and post-intervention times as independent variables using the methods described in Segmented regression analysis of interrupted time series studies in medication use research . With these variables specified auto.arima() suggests an ARMA(7,7) process. The coefficients for baseline trend and post-intervention trend are however non-significant. I am concerned that by using fourier terms to model away the seasonality I am artificially removing any intervention effect, as visual analysis of the time series indicates that the intervention is specifically decreasing incidence during the winter months and therefore reducing the yearly seasonal variability.
