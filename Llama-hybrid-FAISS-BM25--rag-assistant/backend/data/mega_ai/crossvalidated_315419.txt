[site]: crossvalidated
[post_id]: 315419
[parent_id]: 314649
[tags]: 
Preprocessing is quite a must to obtain a good result. It depends on the data type that you have and what you try to achieve. While for some cases like medical imaging, various techniques like special histogram equalization, intensity filtering are being used, in a NLP application, word2vec, vector normalization are being used. All of their purpose are same: Interpret data as meaningful features Enhance features Reduce noise Normalize input In the mathematical side, briefly, think that you have an image with pixel values ranging from 0-255. In that case, even a small change in the early layer weights / biases will have a huge impact on output, which we do not prefer to have. However, if you normalize the image between 0-1 or -1/1, weights can be distributed much more even across the layers and your network will have less overfit.
