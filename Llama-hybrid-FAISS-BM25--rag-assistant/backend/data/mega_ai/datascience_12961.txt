[site]: datascience
[post_id]: 12961
[parent_id]: 
[tags]: 
Kernel on graphs and SVM : a weird interaction.

I have a collection of 80k graphs, half of them are labeled 1 (they represent the cities of France) and the other half is labeled 0 (representing cities in the USA). My goal is to create a classifier that can predict whether a graph (representing a city), using a kernel on graphs. Since nodes in graphs representing cities are generally of degree Article about all connected graphlet kernel . What this kernel does is simple : it represents each graph G by a vector F(G) which is the distribution of the all connected 3, 4 and 5 graphlets in the original graph. There are 29 different graphlets of size 3, 4 and 5 so every graph is now represented by a vector of length 29. The article then defines the kernel between two graphs G and G' by the inner product between the two vectors : K(G,G') = This means that this kernel does not behave like others kernels we are used to work with : we specifically know the feature map and the Hilbert space associated to the kernel. The Hilbert space is R^29 and the feature map is F. Then in order to create the classifier I use SVM. But here's the problem : if I use the feature map F as it is, then compute the gram matrix and apply SVM, all predictions on the test data are 0. That's pretty sad. What I have been noticing is that 3 of the 29 graphlets are much more present than the others in the graphs. So for a typical city, 95% of the distribution is concentrated on theses 3 graphs, which means that when you compute the kernel (the inner product), the gram matrix have really similar values : the standard deviation of the matrix is 0.0003. Naturally, I though : I want to transform F so that the differences between graphs are clearer. So I computed the mean distribution M of 3,4,5 graphlets in my database, I derived the inverse mean distribution M' (I just inverted every element of M), and instead of using F, i used F* which takes a graph G, computes F(G), and then F*(G) by multiplying point by point F(G) and M'. That way, the differences between graphs when computing the kernel are more easy to see. Note that the kernel is still valid since it's still an inner product in R^29. I have no idea how mathematically accurate is this intuition, but when I applied SVM with this other feature map F*, I got a 82% accuracy, and the AUC was 0.88. The ROC curves is beautiful too. So I must have done something right ? My actual question is why does SVM likes the feature map F* more than F ? I mean I didn't add any information using the new feature map, I just modified the old one using my intuition.
