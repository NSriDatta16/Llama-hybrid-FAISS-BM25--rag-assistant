[site]: crossvalidated
[post_id]: 262141
[parent_id]: 262138
[tags]: 
The AIC, the BIC and the $p$ -values all address different questions. For feature selection (variable selection, model selection), only the former two are relevant. See e.g. Rob J. Hyndman's blog posts "Statistical tests for variable selection" and "Facts and fallacies of the AIC" . AIC is best suited for forecasting purposes as it targets maximization of the likelihood of a new observation from the same data generating process (under normality of errors, maximization of likelihood is equivalent to minimization of square loss). BIC is a consistent model selector and as such may be useful in finding which model among a given set is the true model (if the true model is in the set). So depending on your goal (prediction or identification of the data generating process), AIC or BIC may be relevant for you. In this particular application I suspect that AIC is more relevant as the models you are considering probably are not thought to contain any true model among them. Furthermore, model averaging as suggested by @Bj√∂rn might be a good (better) alternative solution.
