[site]: crossvalidated
[post_id]: 548269
[parent_id]: 
[tags]: 
Formal treatment of overfitting on the test set

Assume that I split randomly the data into training and test sets. Suppose that I build a machine learning model using the training set. And suppose that I evaluate the accuracy of the model on the test set and it turned out to be 0.8. I think we can say that the expected accuracy of the model on future (unseen) data is 0.8. But if I try many different algorithms I will begin to overfit on the test set (which is typically the case in many Kaggle competitions). Note that this overfitting will happen even if I use a separate validation set for hyperparameter tuning. Rather somewhat informally, we can say that for finding the accuracy if one uses the test only once then there is no problem. But if one uses it many times then there will be overfitting on the test set and the accuracy result will not be a good estimate of the true error of the model. My question is: is there some formal treatment of this rather informal statement above. For example which says that if you try $n$ different models on the test then the estimated accuracy's confidence will decrease as a function of $n$ . I'm asking this because I cannot see any way to prevent overfitting on the test set in an offline experimental setup where one tries many different algorithms to find the best model. Thanks.
