[site]: crossvalidated
[post_id]: 485317
[parent_id]: 485302
[tags]: 
Depending on the problem at hand, both formulations might be equivalent ( for example in the case of Ridge Regression ). An example of preferring constrained optimization would be the case of Stacked Regressions . Here you have a set of predictions $v$ from different models and apply constrained least squares in order to find a weighted average that minimizes prediction error. It can be formulated as: $min \sum_n(y_{i}-\sum_ka_kv_{kn})^2, s.t. a_k\geq0, \sum a_k=1$ This is a little bit different, but in some applications, such as optimal control or design, we might want to find the least-norm solution of a set of linear equations. The problem here is: $min ||x||, s.t. Ax=b$
