[site]: crossvalidated
[post_id]: 582243
[parent_id]: 
[tags]: 
Validation performance is really problematic; should I give up on increasing the validation performance of my deep learning model?

I am working on a multiclassification problem using time series data. Three datasets are utilized in this study. My deep neural network performs satisfactorily across two different data sets. However, on one dataset, the validation performance is highly problematic; the model progresses in the initial few epochs and then degrades the performance further. The following are screenshots: Regarding my Deep Architecture: I use a hybrid model (such as LSTM, CNN, attention, etc.). I can't show the code here, but I'm confident it's not a code issue because I reran the code of baseline study and saw the same trend, but they reported 90% + performance in their paper. Regarding hyperparameter tuning: I feel that obtaining optimal hyper-parameters may be one of the issues. But I have been experimenting with this model and this dataset for the past six months. I tried the original paper dropouts, optimizer, and activation function as-is, but I could not even achieve an 89% f1-score. In addition, I experimented with the following hyperparameters configurations: Tried different optimizers (ADAM, RMS, SGS) with cross-entropy function and dropout on the fully connected layers. Experimented with batch sizes ranging from 10 to 500, with a learning rate of 0.001 and a decay rate of (10^-9) for every ten epochs (this is exactly what state-of-the-art papers used). Regarding weights initialization : I utilized recurrent_initializer=tf.keras.initializers.Orthogonal() on LSTM layers and the default weight initialization mechanisms in TensorFlow Keras for the rest of the model. Regarding the Activation function : I utilized Relu on all model layers except LSTM. Regarding Preprocessing, I experimented with and without preprocessing but could not notice any significant impact. Here is the code for preprocessing: def normalization(x_train, x_val, x_test): mean_train = np.mean(x_train, axis=0) std_train = np.std(x_train, axis=0) x_train = (x_train - mean_train) / std_train x_val = (x_val - mean_train) / std_train x_test = (x_test - mean_train) / std_train return x_train, x_val, x_test ` My present performance gain is in the range of 83 to 88.5%, whereas my target is at least 90% or 91%. I ran my experiments over a thousand times with a variety of parameter settings, but I was unable to meet my performance goals. Is there anything else I should try or give up from this dataset?
