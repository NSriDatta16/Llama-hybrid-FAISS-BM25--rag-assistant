[site]: crossvalidated
[post_id]: 332158
[parent_id]: 332143
[tags]: 
Your observation is correct, as the Universal AT doesn't account for layer sizes. In real life scenarios however, weight initializations, learning rates and similar parameters can significantly impact the learning. Interestingly enough, for graph-learning-based tasks, two hidden layers appear as the optimal number of layers, yet this is normally not the case for images. Furthermore, it is not possible to have infinitely many units in a single layer, and thus more layers are used to construct higher-order generalizations as it simply works. There remains an open gap in understanding exactly how neural networks, especially deeper ones learn.
