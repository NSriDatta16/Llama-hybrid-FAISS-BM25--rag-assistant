[site]: datascience
[post_id]: 62869
[parent_id]: 62649
[tags]: 
Find this research paper for more info: A Large Dimensional Analysis of Least Squares Support Vector Machines Least squares support vector machine (LS-SVM) is a successful method for classification or regression problems, in which the margin and sum square errors (SSEs) on training samples are simultaneously minimized. However, LS-SVM only considers the SSEs of input variable. Least-squares SVM Linear separation of the datapoints into two classes Implementation and Equations Proximal support vector machines and related approaches (Fung & Mangasarian, 2001; Suykens & Vandewalle, 1999) can be interpreted as ridge regression applied to classification problems (Evgeniou, Pontil, & Poggio, 2000). Extensive computational results have shown the effectiveness of PSVM for two-class classification problems where the separating plane is constructed in time that can be as little as two orders of magnitude shorter than that of conventional support vector machines. When PSVM is applied to problems with more than two classes, the well known one-from-the-rest approach is a natural choice in order to take advantage of its fast performance. However, there is a drawback associated with this one-from-the-rest approach. The resulting two-class problems are often very unbalanced, leading in some cases to poor performance. More Compared to two widely-used variants of support vector machine for regression, namely, least-square support vector machine (LS-SVM) and proximal support vector machine (P-SVM), ELM is subject to fewer and milder optimization constraints. More info Fuzzy Least Squares Twin Support Vector Machines Proximal support vector machine classifiers
