[site]: datascience
[post_id]: 40726
[parent_id]: 39246
[tags]: 
You can see my answer for this question: Does gradient descent always converge to an optimum? You are right that there is no guarantee (no proven theoretical result) for the standard gradient descent method to converge, not mention to converge to a critical point or minimum point. (Except the case where the gradient - i.e. the derivative - of the function is globally Lipschitz continuous. [If these conditions are not satisfied, there are simple counter-examples showing that no convergence result is possible, see the cited paper below for some.]) However, In a very recent joint work (cited in that answer, and for your convenience here ), we showed that: Back tracking gradient descent, when applied to an arbitrary C^1 function , with the mild assumption that it has at most a countable number of critical points (which is the generic case), will always either converge to a critical point or diverge to infinity. (Think about like cos (x), sin (x) or combinations thereof.) Therefore, if you assume that the cost function has compact sub levels, as it is always the case in applications, then convergence to a critical point is guaranteed. We showed that the limit point in the above, in a sense, is rarely a saddle point. We argued that in the long term backtracking gradient descent will become the standard gradient descent method. This gives an explanation to why the standard gradient descent usually works well in practice, and thus answers your question, I believe. Based on the above, we proposed a new method in deep learning, which is on par with current state-of-the-art methods in deep learning, and does not require manual fine-tuning (= errors and trials in your language) of the learning rate. (In a nutshell , the idea is that you run backtracking gradient descent a certain amount of time, until you see that the learning rates, which change with each iteration, become stabilise. We expect this stabilisation, in particular at a critical point which is C^2 and is non-degenerate, because of the convergence result I mentioned in 1. At that point, you switch to the standard gradient descent method. Please see the cited paper for more detail. This method can also be applied to other optimal algorithms, for example the ones you mentioned in your question.) P.S. The above answer (in particular, the one in 4) shows that you can find a good learning rate automatically by backtracking gradient descent method. If you have other parameters, then usually you can state the question in some double optimal problem, and can again use backtracking gradient descent method as above.
