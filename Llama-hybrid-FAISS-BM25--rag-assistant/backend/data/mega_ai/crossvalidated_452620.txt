[site]: crossvalidated
[post_id]: 452620
[parent_id]: 
[tags]: 
Model selection: testing accuracy higher than cross-validation

I trained convnets with different architetures and validated using 5-fold cross-validation. After this, I trained the final version of the convnets using the full data set (training + validation splits) and evaluated the performance on an external test set (i.e., not used before). This test set and the validation splits have almost the same number of samples, 416 vs 427. The thing is the performance on the test set was higher (~10%) than the cross-validation performance. This happened for all architetures and even for a Random forest model I used as base line. I believe this is because I trained on much more data. Do you think this is the case? And how can I make sure things are OK with my validation? And how do I properly select the final algorithm to use, based on CV or testing? I believe it is based on CV, but I still confuse this, otherwise I would be leaking testing information into training.
