[site]: crossvalidated
[post_id]: 152210
[parent_id]: 152107
[tags]: 
On the Generalized Testing of Machine Learning Algorithms: Yes, if there is a known working method, comparing your result to that method over all possible parameters will guarantee that your program is also a known working method. This is usually impossible and always pointless because there already is a known working method. If there isn't a known working method, then in general no, as a counter point consider this "code" that calculates regression coefficients: $\hat{\beta} = (X'X)^{-1}X'y + \delta$ where $\delta = 1000$ when $y[1] = \pi$ else 0. This implementation is right almost all of the time (technically right a.s., but not under IEEE 754), and it is computationally intractable to find it's error. On the Implementations of Methods: Standard practice is the same as in all software development small test cases, and constant validation against something known. E.g. if the model under certain parameters has a known closed form solution or is equivalent to another method check that. Also note that papers's aren't perfect, in most lit reviews I do I usually find a few typo's. Some of these typos actually would cause incorrect results, so always double check your sources. There is a reason emails are on journal papers (HINT: it's so you can contact the authors). Also note that author's don't generally bite if you are nice to them. Just be courteous, and show that you have done some work. But don't expect them to step through your broken code to find a bug. If you have the case where an author won't release the code, or won't respond to you, then don't use that method, more than likely the quality of the code was "good enough to publish" but that's about it. There certainly is no shortage of machine learning algorithms out there. It's also worth while checking who has cited the paper in question and see if they have some code.
