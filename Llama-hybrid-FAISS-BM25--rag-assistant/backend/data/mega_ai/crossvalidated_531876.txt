[site]: crossvalidated
[post_id]: 531876
[parent_id]: 269270
[tags]: 
I was reorgnizing my study notes and came across this old post. I guess there's no harm adding some details to user20160's answer. Assume in a Bayesian setup where you want to make inference about the parameter $\theta = (\theta_1,\theta_2)^T$ , and you know the form of the (probably unnormalized) posterior distribution, say $p(\theta)$ . Your goal is either: get the MAP estimate of $\theta$ by finding $\theta_{MAP} =\arg \max_\theta p(\theta)$ or, draw samples of $\theta$ from the (unnormalized) density function $p(\theta)$ . You can use coordinate ascent for 1 and Gibbs sampling for 2. Coordinate ascent step1: randomly initialize $\theta^{(0)}=(\theta_1^{(0)},\theta_2^{(0)})$ step2: iterate between: $$ \theta_1^{(i)} = \theta_1^{(i-1)} + \alpha \frac{\partial p(\theta_1,\theta_2=\theta_2^{(i-1)})}{\partial \theta_1} \\ \theta_2^{(i)} = \theta_2^{(i-1)} + \alpha \frac{\partial p(\theta_1=\theta_1^{(i)},\theta_2)}{ \partial \theta_2} $$ $\theta^{(n)}$ approaches a single point $\theta_{MAP}$ as $n$ increases. Gibbs sampling step1: randomly initialize $\theta^{(0)}=(\theta_1^{(0)},\theta_2^{(0)})$ step2: iterate between: $$ \theta_1^{(i)} = \text{sample }\theta_1\text{ from }p(\theta_1,\theta_2=\theta_2^{(i-1)}) \\ \theta_2^{(i)} = \text{sample }\theta_2\text{ from }p(\theta_1=\theta_1^{(i)},\theta_2) $$ $\{\theta^{(i)};i=1:n\}$ resemble draws from $p(\theta)$ as $n$ increases. As you can see, Gibbs sampling can somehow be seen as a randomized version of the coordinate ascent. Coordinate ascent is a fixed direction version of Gibbs sampling. If describe from a sampling perspective, coordinate ascent "draws samples" towards the modes of the conditionals in each iteration (the conditionals are $p(\theta_1,\theta_2^{(i-1)})$ and $p(\theta_1^{(i)},\theta_2)$ ) by moving the previous sample in the direction of the gradient, eventually the latest "sample" will approach the mode of the joint distribution.
