nderset {\theta }{\operatorname {arg\,min} }}-\sum _{i}^{T}\log \sum _{j=1}^{J^{(i)}}P(y_{j}^{(i)}|y_{1,j-1}^{(i)},\mathbf {x} ^{(i)})} In practice, this minimization is done iteratively on small subsets (mini-batches) of the training set using stochastic gradient descent. Teacher forcing During inference, auto-regressive decoders use the token generated in the previous step as the input token. However, the vocabulary of target tokens is usually very large. So, at the beginning of the training phase, untrained models will pick the wrong token almost always; and subsequent steps would then have to work with wrong input tokens, which would slow down training considerably. Instead, teacher forcing is used during the training phase: The model (the “student” in the teacher forcing metaphor) is always fed the previous ground-truth tokens as input for the next token, regardless of what it predicted in the previous step. Translation by prompt engineering LLMs As outlined in the history section above, instead of using an NMT system that is trained on parallel text, one can also prompt a generative LLM to translate a text. These models differ from an encoder-decoder NMT system in a number of ways: Generative language models are not trained on the translation task, let alone on a parallel dataset. Instead, they are trained on a language modeling objective, such as predicting the next word in a sequence drawn from a large dataset of text. This dataset can contain documents in many languages, but is in practice dominated by English text. After this pre-training, they are fine-tuned on another task, usually to follow instructions. Since they are not trained on translation, they also do not feature an encoder-decoder architecture. Instead, they just consist of a transformer's decoder. In order to be competitive on the machine translation task, LLMs need to be much larger than other NMT systems. E.g., GPT-3 has 175 billion parameters, while mBART has 680 million and the original transformer-big has “only” 213 million. This means that they are computationally more expensive to train and use. A generative LLM can be prompted in a zero-shot fashion by just asking it to translate a text into another language without giving any further examples in the prompt. Or one can include one or several example translations in the prompt before asking to translate the text in question. This is then called one-shot or few-shot learning, respectively. For example, the following prompts were used by Hendy et al. (2023) for zero-shot and one-shot translation: ### Translate this sentence from [source language] to [target language], Source: [source sentence] ### Target: Translate this into 1. [target language]: [shot 1 source] 1. [shot 1 reference] Translate this into 1. [target language]: [input] 1. Literature Koehn, Philipp (2020). Neural Machine Translation. Cambridge University Press. Stahlberg, Felix (2020). Neural Machine Translation: A Review and Survey. See also Attention (machine learning) Transformer (machine learning model) Seq2seq == References ==