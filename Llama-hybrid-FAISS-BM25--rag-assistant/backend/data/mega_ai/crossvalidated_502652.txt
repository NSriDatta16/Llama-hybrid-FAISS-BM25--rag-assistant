[site]: crossvalidated
[post_id]: 502652
[parent_id]: 502214
[tags]: 
It's not necessary that you compute the loss at each timestep for RNNs. You could compute only for the last timestep $L^{(T)}$ and then use BPTT algorithm to get the gradients. However, computing the loss at each timestep won't hurt for the sentiment analysis use case - as it would give you an idea of how the sentiment is varying across the sentence. Assuming the output is binary, you get a sequence of [+ve, -ve, +ve, +ve, +ve, -ve, ...] as output; which greatly helps in understanding the sentence, at the cost of greater complexity. A comment on your statement ...In BPTT, we would for each timestep (input word) calculate the loss... - that is not entirely true for the case in which you only compute the loss at the last timestep $L^{(T)}$ . This lecture from CS224 2019 has a great explanation on BPTT (from 35:48). Let's say we want to compute gradient updates for parameter $W_h$ . $$\frac{dL^{(T)}}{dW_h} = \frac{dL^{(T)}}{dh^{(T)}} \frac{dh^{(T)}}{dW_h}$$ You can see that the computation kinda blows up - $W_h$ influences each of $h^{(1)}, h^{(2)}, ..., h^{(T)}$ -- making the gradient computation tricky. The trick that the lecture suggests is to consider $W_h$ at each timestep $i$ as a different entity $W_h |_{(i)}$ . From the image; using chain rule, what we get is $$\frac{dL^{(T)}}{dW_h} = \Sigma_{i = 1}^{T} \frac{dL^{(T)}}{dW_h|_{(i)}}$$ Great. Now let's shift focus on the individual entities $\frac{dL^{(T)}}{dW_h|_{(i)}}$ . Note that $W_h|_{(i)}$ only influences $h^{(i)}, h^{(i+1)}, ..., h^{(T)}$ . The equation involving it looks somewhat like $$h^{(i)} = ReLU(W_x|_{(i)}x^{(i)} + W_h|_{(i)}h^{(i-1)})$$ Through the $i^{th}$ hidden state $h^{(i)}$ , the gradient that comes backwards (backprop) is $\frac{dL^{(T)}}{dh^{(i)}}$ . So, $$\frac{dL^{(T)}}{dW_h|_{(i)}} = \frac{dL^{(T)}}{dh^{(i)}} \frac{dh^{(i)}}{dW_h|_{(i)}}$$ You know both quantities on RHS (one from backprop, and the other from the ReLU equation above) - hence, you can compute $\frac{dL^{(T)}}{dW_h|_{(i)}}$ The math looks a bit tricky at first glance but once you write it down it all makes sense. Coding BPTT is not tricky though, you can see the iterative nature from previous step. I recommend you to check Andrej Karpathy's minimal implementation of Char-RNN here - easy to follow BPTT code.
