[site]: datascience
[post_id]: 117588
[parent_id]: 117583
[tags]: 
Not possible in a typical Transformer AFAIK. Most neural networks are non-invertible mappings, so you cannot guaranteed reconstruct their inputs from their hidden layers. For transformers specifically, the positional embedding is added to the token embedding before being fed into self-attention, so there not an easy way to disentangle these that I'm aware of.
