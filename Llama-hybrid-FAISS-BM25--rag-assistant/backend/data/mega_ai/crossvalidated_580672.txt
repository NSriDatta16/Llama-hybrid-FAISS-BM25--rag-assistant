[site]: crossvalidated
[post_id]: 580672
[parent_id]: 580647
[tags]: 
Your explanation is accurate, though I think there is an essential detail it doesn't address. Namely that in the typical confusing situation, the recent outcome does give you information about the new outcome. And regression to the mean is still present. Think about parent's height vs children's height or a sports team's performance in the first segment of a game vs the second segment. There is some underlying factor (genes, team's current fitness) that is relatively constant and influences both outcomes. But there is also some random factor (environment / gene expression, luck in sports) that influences each outcome separately. As long as there is any random factor, regression to the mean is present. Yet people intuitively overlook that if they are thinking about the constant factor. This may be a consequence of people's general tendency to see patterns where none exist and therefore underestimate the random component. In the example with the sports team that performed horrible in the first segment, the trainer will yell at the team in the break, the team will perform better despite the intimidation, and the trainer will think that the yelling was helpful. It's a false pattern the trainer sees and the trainer's belief in it is reinforced by the regression of the performance toward the mean. (In that situation the team's fitness/spirit is to some degree not necessarily a constant / the same in each segment, but it's still not the most important explanation for performance differences.) In case you are interested, here's a toy example. An approximate version of the "Gaussian" die you mention does exist, it's called "throwing two dice and computing the sum". Imagine I would do that and tell you only the sum. Then I would take the die to my right and throw it again and leave the left die untouched. What do you expect the new sum to be? If the old sum was, say, an 11, you'd knew that my dice had shown 5 and 6. So it is reasonable for you to expect a rather high number for the new sum, since one of the dice is left untouched. That is what people often intuitively get right. At the same time it is reasonable for you to expect the new sum to be lower that 11, since one of the dice is thrown again. That is the regression toward the mean that people often miss in more complicated situations. Tangential topic: Bayesian inference As pointed out in Sextus Empiricus' answer , there is a relation to Bayesian inference. The uncertainty about the right die's result (random component) may be called aleatoric , while the uncertainty about the left die (fixed component) may be called epistemic . Since the left die is already determined before the second roll and we can gain knowledge about it from the reported sum in the first roll. This process of gaining/inferring probabilistic knowledge is called Bayesian inference and works roughly as follows in the above example. We know that the number was sampled with uniform probability between 1 and 6 (" prior "). We observe a value of 11 and notice that any number other than 5 and 6 would make it impossible (chance of zero) to observe 11 while both 5 and 6 would produce a chance of 1/6 (" evidence "). We conclude (but usually have to compute) that the left die must show a 5 or a 6 and both have equal probability (" posterior "), since they had equal prior and equal likelihood. From that posterior over the left die and the known distribution over the right die, we can calculate the distribution over the new sum. The following image from this website shows how the quantities in Bayesian inference would look like if we didn't have fair dice but Gaussian "dice". The fact that the posterior is not as far out as the evidence corresponds to regression toward the mean. The fact that it is away from the prior shows that the regression does not go all the way to the (prior) mean. (A child of tall parents will usually still be higher than average.) The amount of the regression depends on the relative influence of the random component (width of evidence curve) and the fixed component (width of prior distribution). This answer on Cross Validated has an image that shows how these widths influence the posterior.
