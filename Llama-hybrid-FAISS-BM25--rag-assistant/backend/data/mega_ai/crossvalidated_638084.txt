[site]: crossvalidated
[post_id]: 638084
[parent_id]: 135702
[tags]: 
When I learned Bayesian statistics at that time, the alternative to model checking was model expansion/averaging. That is, a Bayesian was thought to (ideally) place prior probabilities on models, as well as on parameters within models. Incorporating data would give you posteriors over models, and models that were well-supported by the data would have higher posterior probability. The attractive aspect of this was that nothing was needed except Bayes Theorem, and that model choice and parameter estimation worked the same way. It also fit well with the growing awareness that data-driven model selection was unstable. The disadvantage was that MCMC over models was often a pain, and that specifying priors over large enough sets of models is harder than it sounds.
