[site]: crossvalidated
[post_id]: 579122
[parent_id]: 579022
[tags]: 
If you have a pre-specified model to evaluate on new samples, there really isn't anything different from standard statistical power calculations. Yes, the more new cases the better, but you can't collect an infinite number of new samples so you need to make tradeoffs for sample size. You presumably have a logistic regression model for the probability of condition versus control, with your final "equation" incorporating a linear predictor of the form $\mathbf{X}' \beta$ (the sum of terms of each predictor $X_j$ times its corresponding modeled regression coefficient $\beta_j$ ). That your regression coefficients $\beta$ came from a "machine learning" model doesn't really matter if you intend to use the same coefficients in future work including your model evaluation. You need to specify the magnitude of the effect (e.g., odds ratio) you'd like to detect, the false-positive rate (typically 0.05 in a two-sided test), the desired power (often 80% or 90%), and the distribution of your linear predictor for the condition and control groups. For that last item you could use the distribution of the linear predictors for the two groups in your data set. The free G*Power software provides tools for logistic regression power estimates. This page has an answer that explains how those calculations are done. Simulations can be more straightforward, and ensure that you actually know and specify the underlying assumptions. Simulation has the further advantage of being applicable to arbitrarily complicated situations that might not be handled by standard software. You simulate data based on the data you have, and see how large a sample you need by evaluating multiple new samples at different sample sizes. See the page just linked and this page for hints about implementation.
