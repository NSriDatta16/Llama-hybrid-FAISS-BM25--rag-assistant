[site]: crossvalidated
[post_id]: 140626
[parent_id]: 139066
[tags]: 
In the paper, and in general, (random) variables are everything which is drawn from a probability distribution. Latent (random) variables are the ones you don't directly observe ( $y$ is observed, $\beta$ is not, but both are r.v). From a latent random variable you can get a posterior distribution, which is its probability distribution conditioned to the observed data. On the other hand, a parameter is fixed, even if you don't know its value. Maximum Likelihood Estimation, for instance, gives you the most likely value of your parameter. But it gives you a point, not a full distribution, because fixed things do not have distributions! (You can put a distribution on how sure you are about this value, or in what range you thing this value is, but this is is not the same as the distribution of the value itself, which only exists if the value is actually a random variable) In a Bayesian setting, you can have all of them. Here, parameters are things like the number of clusters; you give this value to the model, and the model considers it a fixed number. $y$ is a random variable because it is drawn from a distribution, and $\beta$ and $w$ are latent random variables because they are drawn from probability distributions as well. The fact that $y$ depends on $\beta$ and $w$ doesn't make them "parameters", it just makes $y$ dependent on two random variables. In the paper they consider that $\beta$ and $w$ are random variables. In this sentence: These update equations need to be run iteratively until all parameters and the complete log likelihood converge to steady values in theory they talk about the two parameters, not the ones that are random variables, since in EM this is what you do, optimizing over parameters.
