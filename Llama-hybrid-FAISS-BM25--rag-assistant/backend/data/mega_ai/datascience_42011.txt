[site]: datascience
[post_id]: 42011
[parent_id]: 
[tags]: 
What should be the requirement for training data in order to obtain a good regression model using neural network?

I have made a neural network regression model using the theory for the first time and would like to clarify some basic doubts, whose concrete answers I couldn't find yet. Data:- I have 3000 samples with 7 input features (consisting of 5 normalized continuous variables and 1 binary variable recoded using one-hot encoding). I have 2 outputs per sample. So I have trained my model and I am getting around 95% accuracy. I feel it's too good to be true. Is it? I have normalized my training data and used the mean and standard deviation of the training data to normalize the test data. Right approach, right? I have read that neural network is after all a method to do interpolation, so how would it behave on the data with features with values which are not within the range while it's being trained? I ask this because when I take a $80\%$ random sample out of the available data, it's possible that for a given feature (continuous variable) $X_1$ out of $X_1, X_2, ..., X_7$ , $X_1 \in [a,b]$ , where $a,b$ are some scalars. However, the other $20\%$ test data might have $X_1$ values $ or $>b$ . Is my trained model expected to give correct results then? Should I treat each output independently and create models, $i.e.$ weights separately (2 in my case) or should I find weights with the joint output? (The outputs may have different scales, so should I normalize them too in this case?) How different would be the behaviours of both the models? I believe the questions are preliminary but it would be really helpful to get my basics clear.
