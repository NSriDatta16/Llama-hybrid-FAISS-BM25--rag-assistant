[site]: datascience
[post_id]: 45218
[parent_id]: 45163
[tags]: 
Here's a simple way to deal with the word-count imbalance. You could first convert the representation of the tokens using word embedding. There are two publicly available models that are very popular: Word2Vec and GloVe. Word embeddings can be very useful because they capture latent semantic and lexical information that is not usually available in vanilla BOW models. https://nlp.stanford.edu/projects/glove/ https://radimrehurek.com/gensim/models/word2vec.html Next, using the word embeddings, take a mean over the set for each example in you data set. That will balance the classes in some sense because you are essentially reducing the problem to an average embedding representation for each example. Assuming that the number of examples per class are balance then your balance problem is solved. Also consider doing stop-word filtering to remove useless terms. You can then use that representation as input to you classification model of choice - SVM, Random Forest, Logistic Regression etc... but that is it's own problem to solve. There are tradeoffs to every type of feature engineering that you do. Be aware of that and do your due diligence to evaluate what type of systematic effects (if any) this type of preprocessing will have on your result.
