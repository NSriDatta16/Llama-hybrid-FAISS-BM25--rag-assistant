[site]: crossvalidated
[post_id]: 280758
[parent_id]: 279077
[tags]: 
You can find much more accessible conditions for consistency and asymptotic normality of MLE in Hayashi's Econometrics, ch. 7. ,in the general context of Extremum Estimators and its sub-class, the M-estimators. Hayashi has also references for detailed proofs on the conditions. The MLE with independent observations belongs to this subclass, because it maximizes a "sample average", an average of a real-valued function of the data and the unknown parameters (note that with independent observations the log-likelihood of the sample is certainly a sum, and we can divide it by the sample size without affecting the solution). So (in general notation) $$\hat \theta_{MLE} = \text{argmax}_{\theta} \left\{\frac 1n \sum_{i=1}^n \ell_i(x_i;\theta)\right\}$$ where $\ell_i$ is the log-likelihood of observation $i$. For consistency , there are two-three alternative sets of conditions. Common to all conditions are: 1) The parameters lie in the interior of the parameter space 2) $\ell_i(x_i;\theta)$ is measurable (if it is continuous, it is measurable) 3) The objective function $\frac 1n \sum_{i=1}^n \ell_i(x_i;\theta)$ converges in probability to some function, say $\ell_0(\theta)$ 4) $\ell_0(\theta)$ is uniquely maximized at the true parameter vector (say $\theta_0$) Then moreover: 1st Alternative : if the parameter space is compact, and convergence is uniform, we obtain consistency. 2nd Alternative : if the parameter space is not compact, then if the log-likelihood is concave and convergence is just pointwise, we again obtain consistency. I ' ll leave asymptotic normality for the OP to look up and explore.
