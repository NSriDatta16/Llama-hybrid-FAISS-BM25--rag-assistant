[site]: datascience
[post_id]: 111118
[parent_id]: 111115
[tags]: 
Usually, the aim of K-fold cross-validation is to check how a model performs (both on average and how much it varies across folds) given some hyper-parameters setting. We then pick the "best" set of hyper-parameters. Afterwards, we fix the hyper-parameters and train the model with full dataset to squeeze all the juice. In the case where there is no hyper-parameters to tune e.g. simple linear regression, cross-validation can give you an estimate of how your model will perform. You then train a final model with all data.
