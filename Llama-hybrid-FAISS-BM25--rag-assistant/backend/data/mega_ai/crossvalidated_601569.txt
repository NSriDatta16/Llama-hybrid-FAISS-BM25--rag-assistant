[site]: crossvalidated
[post_id]: 601569
[parent_id]: 595909
[tags]: 
Here is some simple R code demonstrating that bootstrapping the control function estimator has a correct coverage rate. It uses a simple x-y bootstrap with 200 resamples. Not the most efficient code in the world, takes less that two minutes for 200 repetitions of the process (and was quick to write up). I don't use the example you give above because one requires knowledge of the truth to assess coverage rates, and so a simulation is required. set.seed(1234) library(pbapply) # Function for control function estimation, potentially re-sampled one_run $residuals, data = d) ss$ coefficients } # Number of simulations to assess coverage rate n_sims out # Collect and combine results all $covered lower = 1, 1, 0) summary(all) Running the above shows that the truth falls within the bootstrapped confidence intervals at the desired 0.95 rate, which means that the confidence intervals are of the desired size. Worth note is that one may occasionally get bootstrap intervals which are particularly wide or narrow, just like you might get analytic standard errors which are particularly wide or narrow relative to the what you would expect under repeated sampling because of the peculiarities of a given sample. For example, the smallest interval observed in the above simulation was of width 0.29 [0.89, 1.19] while the largest was of width 1.36 [-0.26, 1.09]; this demonstrates the conceptual issue with the approach you took above. While in expectation the bootstrap and analytical standard errors to be approximately equal since they target the same theoretical quantity, the comparison of any individual bootstrap with any individual analytic standard error may be wildly different.
