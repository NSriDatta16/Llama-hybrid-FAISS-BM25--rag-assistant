[site]: crossvalidated
[post_id]: 623067
[parent_id]: 623056
[tags]: 
[Edit: I assumed that $Y$ and $Z$ are independent, but this was not clear from the first version of this question] If $X$ and $Z$ were independent, then indeed the conditional distribution of $Y = X-Z$ given $X$ would be normal. But $X$ and $Z$ are only independent if $Y$ is a normal random variable. Think of a very extreme example: that $Y$ only takes one possible value, say $P(Y=5)=1$ . Then $X\sim N(5, 1)$ but $P(Z= X+5)=1$ , so there is no option for $Z-X$ to vary, and $Z-X=5$ with probability 1. Edits in response to comments and edits: In summary: Your question cannot be decided without assuming a dependence structure. [For clarification: there was a discussion about $Z$ and $Y$ being independent or not. The initial version did not state that $Z$ and $Y$ actually are independent, and there were doubts on that in the comments. I keep this part of the answer because it highlights the importance of specifying the dependence structure] Since you specified the distribution of $Z$ (standard normal) and $Y$ (some distribution) in the start, I assumed that $Z$ and $Y$ are independent, and $X$ is defined by $Z$ and $Y$ . Following @Henry's comments, you stated that you don't assume anything about the dependence. However, conditional distribution of $Y$ given $X$ really depends on how the variables relate to each other - their joint distribution . In my example where $Y$ only takes one possible value, $Y$ would be automatically independent of both $Z$ and $X$ . On the contrary, you can also determine $Y$ by specifying $X$ . For example, you can require that $X$ is constant, say $P(X=10)=1$ . Then you would get that $Y\mid X = 10-Z$ is standard normal (because $-Z\sim Z$ .) There is not only one distribution of $Y$ given $X$ in general , without knowing the dependence. The dependence comes in to the Bayesian formula when you calculate expressions like $P(Y=y, X=x)$ in the denominator, because $$ \begin{aligned} P(Y=y | X=x) &= \frac{P(Y=y \wedge X=x)}{P(X=x)} \quad \small\text{(joint distribution of $X$ and $Y$ needed)} \\&= \frac{P(Y=y \wedge Z=x-y)}{P(Z=x-Y)}\quad \small\text{(joint distribution of $Y$ and $Z$ needed)} \\&= \frac{P(X=x \wedge Z=x-y)}{P(X=x)}\quad \small\text{(joint distribution of $X$ and $Z$ needed)} \end{aligned} $$ [Caveat: This is not a rigorous formula for continuous random variables, just to illustrate the principle, for sake of simplicity] In the numerator of Bayes formula, you always have some joint events, saying something about [ $Y$ and $X$ ] or [ $Y$ and $Z$ ] or [ $X$ and $Z$ ]. If you want to get anywhere solving the equation, you need to know the joint distribution of two of the variables. This is easiest if you assume two of them to be independent (such as I did when I read your first version of this question).
