[site]: crossvalidated
[post_id]: 486908
[parent_id]: 
[tags]: 
In a PCA setting, is there any relationship with the sum of squares of the scores (t1) with the eigenvalue of that principal component?

When one computes the vector of scores (t1) using the principal component (p1) the data is being projected over the direction of biggest variation. One could measure the distance between the point where the data was projected and the origin. If we do the squared sum of squares of those distances (because they could be negative), we won't obtain the eigenvalue of the eigenvector pointing in the direction of the principal component. Why? An example, as requested: The original data: sample = [[1.343730519 , -.160152268 , .186470243], [-.160152268 , .619205620 , -.126684273], [.186470243 , -.126684273 , 1.485549631]] ) Eigenstuff (from the covariance matrix): evalues = [2.22044605e-16, 1.67438287, 2.82561713] evectors.T = [ 0.54061848, 0.65888106, 0.52307496], [ 0.68485977, 0.0164023 , -0.72849026], [ 0.48856807, -0.75206829, 0.44237374]] Score using just the first component (3rd vector from above) t1 = [1.0619562 , -1.93803314, 0.87607695] The following is the part that I don't get The elements in vector t1 are the 'distances' from the origin to the point where the original data was projected in the direction of p1 Since the eigenvalue of p1 is the magnitude of the variance in that direction, I would expect that the sum of squares of the elements in t1 would yield the same result as the plain eigenvalue. Which is not the case, computing the squared sum of squares (SS) over t1 yields SS = 2.3772324776675657 The eigenvalue was: evalue_p1 = 2.82561713 It is very similar yet not the same, why?
