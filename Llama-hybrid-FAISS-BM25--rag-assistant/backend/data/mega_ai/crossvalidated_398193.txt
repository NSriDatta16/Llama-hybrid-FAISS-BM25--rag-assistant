[site]: crossvalidated
[post_id]: 398193
[parent_id]: 66862
[tags]: 
As common in some machine learning algorithms, Boosting is subject to Bias-variance trade-off regarding number of trees. Loosely speaking, this trade-off tells you that: (i) weak models tend to have high bias and low variance: they are too rigid to capture variability in the training dataset, so will not perform well in the test set either (high test error) (ii) very strong models tend to have low bias and high variance: they are too flexible and they overfit the training set, so in the test set (as the datapoints are different from the training set) they will also not perform well (high test error) The concept of Boosting trees is to start with shallow trees (weak models) and keep adding more shallow trees that try to correct previous trees weakenesses. As you do this process, the test error tends to go down (because the overall model gets more flexible/powerful). However, if you add too many of those trees, you start overfitting the training data and therefore test error increases. Cross-validation helps with finding the sweet spot
