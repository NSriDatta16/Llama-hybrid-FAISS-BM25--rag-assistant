[site]: crossvalidated
[post_id]: 384374
[parent_id]: 
[tags]: 
How much bias am I risking by doing model goodness of fit comparison without accounting for clustering?

I am interested in testing whether an interaction term is statistically significant or not in a logistic regression. Data is large and observations are clustered by family and suffer from sparsity for both the outcome and one of the covariates in the interaction term (% of 0s vs. 1s are about 95% and 5%). I first tried (with glmer() in lme4) anova(mixed effects model 1, mixed effects model 2) very simple outcome ~ covar 1 * covar 2 + (1|family) vs. outcome ~ covar 1 + covar 2 + (1|family) Both covar 1 and covar 2 are binary variables. The first model probably ran to the edge of the world after 12 hours and could be found no more. Then I wondered what the cost would be if I ran both models in the same wrong way (i.e., not accounting for the random effect) Here is a publicly available data set I explored-- library(lattice) gm1 p from ANOVA = 0.002152, p from LRT = 0.001613 As I was expecting, the LRT was too optimal, but say if the bias (in terms of p-value) is no more than 200% (or some upper limit), a quick way to do model comparison could be running the LRT and multiplying the too-optimal p-values by a scaling factor. Thank you for your time and insight! Any papers on this would be very helpful, too.
