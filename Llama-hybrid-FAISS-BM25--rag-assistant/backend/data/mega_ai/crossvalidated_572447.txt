[site]: crossvalidated
[post_id]: 572447
[parent_id]: 572293
[tags]: 
As you note, model coefficients in a linear regression will tell you about change in predicted outcome for a one unit change in the predictor. Coefficients, standardized or not, then relate mostly to the the predicted levels/mean of the outcome. By contrast, general dominance statistics (assume that's what is being interpreted) produces a partitioning of the $R^2$ which reflects all of the values of the predictor as translated into values of the predicted outcome. Thus, general dominance statistics reflect all of the predictor's variance at once--not one unit of change at a time. General dominance statistics then relate mostly to the predicted spread/variation of the outcome. These statistics then answer two different questions (though they can sometimes give similar answers). If importance for your research question has more to do with change to levels/mean values given change on predictors, coefficients are to be preferred. This might be the case for an evaluation of a specific intervention. If importance for your research question has more to do with total impact of a predictor across its entire range, dominance is to be preferred. This is usually the case in exploratory research when there may not be specific expectations about effects. It seems, given your description, this might be the situation you are in and that dominance statistics might be preferable. For example, if a variable has a high VIF or p-value, why could it show up as an important variable with dominance analysis? To this question, it is worth noting that dominance analysis is not very good at model selection and will ascribe non-0 shares of a fit statistic like the $R^2$ to predictors that have true non-0 effects when they are correlated with other variables that do (e.g., Budescu, 1993; Grömping, 2007). This is generally why those that have large p-values or VIF values would receive a potentially large dominance statistic when other statistics suggest they should not. That is, they should have been filtered in a previous, model selection, stage. References Budescu, D. V. (1993). Dominance analysis: a new approach to the problem of relative importance of predictors in multiple regression. Psychological bulletin, 114(3), 542. Grömping, U. (2007). Estimators of relative importance in linear regression based on variance decomposition. The American Statistician, 61(2), 139-147. Update Responses to comment. Could I please ask you to elaborate on 'specific expectations about effects'? In short, does the researcher/analyst have a hypothesis about the direction and magnitude of the effect of a specific variable. For example, in experimental research this is often the case and in such cases understanding differences in levels/effect size is most valuable. ... how I could use 'total impact of a predictor' to explain drivers of online sales, and how it is different from 'unit change in predictor variable' Will illustrate with an example: data(mtcars) > lm(mpg ~ vs + cyl + carb, data = mtcars |> datawizard::standardize()) |> coef() |> round(digits = 3) (Intercept) vs cyl carb 0.000 -0.149 -0.883 -0.170 In the above, vs is a binary variable, cyl and carb are both categorical taking on 3 or 4 levels. All are standardized like the original example to facilitate comparison. In this example, a one standard deviation change in vs results in a -.149 standard deviation change in mpg . This information is useful as it tells the researcher "how" to get higher or lower values on the outcome (or explains why they got that way). If understanding how levels of the outcome got the way they are coefficients are most useful. What coefficients do not do, is show how the variable actually behaves when applied to the levels of the predictors in the data. For example, if the vs coefficient is applied to the values observed in the data, we see that its one standard deviation change concept is meaningless as it is a binary variable that takes on values of 0 and 1. It's standard deviation is ~.5 and there are no possible values that could occur at .5 of vs . So, all changes in the data are effectively at 2 standard deviations. Similar cases apply to carb and cyl . Those differences as applied to data are lost with coefficients as that's not what they are meant to convey. By contrast, if we take all the predicted values in the data and correlate those with the outcome (and square it to obtain an $R^2$ ), we are evaluating how the values of the predictors "have translated" the coefficients into predicted values. That vs is a binary variable is necessarily incorporated into the $R^2$ as it is based on predicted values from the data. Dominance analysis is then an approach to ascribe components of the $R^2$ to the different variables to show their impact such as the example below: > domir::domin(mpg ~ vs + cyl + carb, lm, list(performance::r2, "R2"), data = mtcars, complete = F, conditional = F) Overall Fit Statistic: 0.7475552 General Dominance Statistics: General Dominance Standardized Ranks vs 0.1799347 0.2406975 2 cyl 0.4502766 0.6023322 1 carb 0.1173440 0.1569703 3 The ~.1799 value that vs obtains in the dominance analysis suggests that, on average, that is the $R^2$ using predicted values from this model. That value, again, accommodates the levels of the predictor as observed in the data and shows what it does when you take predicted values and correlate them with the outcome--hence, reflects the total effect of the variable across its observed range in the data. This is useful as it shows the impact of the predictor as applied to the data, not abstracted from it as the coefficient is. Again, the two different methods supply different information about the model and data.
