[site]: crossvalidated
[post_id]: 408684
[parent_id]: 408676
[tags]: 
What you can do is something called nested cross validation. Instead of creating one train-test split you create K train-test splits (as you would in k-fold cross validation). For each of your K training sets you perform cross-validation to select hyper-parameters (this is called the inner CV loop) and then test on the test dataset (outer loop). You will end up with K error rates which are the average of the K* error rates you got from each inner loop. You then average these K error rates to obtain a final generalization error. After this is complete it would be a good idea to repeat this process many times and take the average of that value.
