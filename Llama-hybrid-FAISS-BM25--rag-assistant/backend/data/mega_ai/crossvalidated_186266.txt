[site]: crossvalidated
[post_id]: 186266
[parent_id]: 
[tags]: 
Do I have a situation of overfitting in xgboost on this data? How can I reduce it?

I apply the xgboost algorithm for classification. I perform cross-validation in the training data set in order to find parameters (eta, step size shrinkage, = 0.01, maximum depth of a tree: 14, 1400 rounds) for best accuarcy and I get something like 0.9. However on the test data set I get 0.5. Furthermore my prediction in-sample looks like this: Using classical methods (glm for example) the probabilities are much more "unclear" meaning that they cluster around 0.5. In the case of xgboost I get a much more spread-out picture. Is this a sign of overfitting? Which parameters can I calibrate to avoid this? I assume gamma is the one, I use the default 0. What are typical values for gamma?
