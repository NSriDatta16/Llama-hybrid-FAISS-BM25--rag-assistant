[site]: datascience
[post_id]: 109439
[parent_id]: 109431
[tags]: 
The single layer neural network - the Perceptron - was first implemented by Frank Rosenblatt in 1957 using hardware (!!) - that is: resistors, electrical motor, and a potentiometer! The perceptron had one layer of neurons, (not including the output layer), and it could tell the difference between simple shapes and letters. According to Google's N-Gram Viewer it was a long time before the term 'Deep Neural Network' was used in a book (~1980), and much longer (~ late 2000's) before it gained popular usage: So to answer the question, I suspect although it's somewhat subjective , I think the Perceptron is the only true single-layer neural network in your diagram, and therefore the others would be considered 'deep'. Some may consider other 'shallow' neural networks (like 'feed forward netural nets') to not be deep, but it's worth noting that although shallow in structure, the term 'feed forward netual network' didn't come into popular parlance until slightly after the term 'deep neural network' (see here ).
