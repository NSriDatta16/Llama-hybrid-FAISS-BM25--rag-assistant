[site]: datascience
[post_id]: 61724
[parent_id]: 60896
[tags]: 
Yes, of course, this technique exists. In XGBoost, for instance, you can change the objective function to the multi:softprob which in specifics does: multi:softprob: same as softmax, but output a vector of ndata * nclass, which can be further reshaped to ndata * nclass matrix. The result contains predicted probability of each data point belonging to each class. From my memory, if it answers your question I think it rounds up to 1.
