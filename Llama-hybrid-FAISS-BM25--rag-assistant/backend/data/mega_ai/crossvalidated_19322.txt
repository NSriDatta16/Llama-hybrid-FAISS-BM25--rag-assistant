[site]: crossvalidated
[post_id]: 19322
[parent_id]: 
[tags]: 
Why the infrequent use of machine learning techniques in translational biomedicine?

This is in followup to a previous question. here : Neural network model to predict treatment outcome and might be considered to refer to a different aspect of this question: Application of machine learning techniques in small sample clinical studies Thanks to Zach who suggested reposting. I've put some fairly serious reading in now on CART, randomForest, Neural Networks and machine learning in general, learned about WEKA and the R packages, seen and followed the Stanford engineering lectures http://www.ml-class.org/course/class/index , I'm 3 chapters into Hastie. Given the kind of data we see regularly in clinically oriented research - loads of clinical parameters + loads of biochemical parameters + pen and paper test data +/- neuroimaging data with smallish numbers, I get the feeling that I'm missing something. I'm not regularly reading about ML techniques being applied in the research literature. My question is: have I just latched on to something which is dubious and therefore regarded with justified suspicion by researching clinicians and biostatisticians who are well aware of it, or are these techniques genuinely overlooked or feared outside of "business analytics"? What keeps it "niche"?
