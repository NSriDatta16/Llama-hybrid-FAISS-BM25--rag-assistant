[site]: crossvalidated
[post_id]: 432278
[parent_id]: 432268
[tags]: 
From a regression perspective, sever multicollinearity between features can cause your standard errors to become unstable. For continuous features, one usually computes the Pearson correlation and excludes predictors above some prespecified cutoff. There are a number of cutoffs that you can use; there's no hard and fast rule for what value to use as a cutoff. For predictive modelling, the variable which is the least predictive is removed. To keep it simple, you could just look at univariate measures of the predictive strength a variable has with the response. That said, if predictive performance is your goal (as is generally the case with machine learning), using PCA to condense your predictor set seems like a reasonable idea. I believe the literature you're referring to means the magnitude of the correlation. That is, the absolute value of the Pearson correlation. That's why they never mention 'negative correlation', as you say.
