[site]: crossvalidated
[post_id]: 340073
[parent_id]: 340025
[tags]: 
The key idea here is that RNN steps cannot be parallelized because they are connected: each one needs the result of the previous one to run. If they were not connected, RNN would not be particularly more expensive than other models. However, if you collect all the results from you RNN (with $N$ hidden layers) at each time step, you can then apply a standard multi-perceptron with $M$ layers to all these outputs in parallel . Thus, the computation of your $M$ layers is much faster than the $N$ ones of the RNN if $N$ and $M$ are "quite close"...
