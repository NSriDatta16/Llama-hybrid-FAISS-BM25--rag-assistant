[site]: crossvalidated
[post_id]: 151836
[parent_id]: 151827
[tags]: 
I'd suspect something is wrong with how you created the hold out set. Ie either it wasn't randomized or needed to be done with stratified sampling. This sort of thing can also happen if you did feature selection or engineering outside of the CV loop as you'll produce features that look good across the cv folds but don't generalize. It can also happen if you tuned model hyperparameters using the CV loop. How did the other model's do on the hold out set? SVMs can be quite prone to this while RF's tend to be less prone so i'd wonder about that in particular. Other strategies for dealing with this include an inner CV loop and fixing the regularization parameters to limit the amount of overfitting you can do. Finally if this is highly dimensional data with lots of variance unrelated to the target (like genetic data) it is quite possible there exist models that works well in the cv data but not the hold out data by random chance. This phenomena has been described as "anti learning", again RF's tend to be more robust against this (because of the internal bagging) though it begins to effect them as well as the dimensionality grows.
