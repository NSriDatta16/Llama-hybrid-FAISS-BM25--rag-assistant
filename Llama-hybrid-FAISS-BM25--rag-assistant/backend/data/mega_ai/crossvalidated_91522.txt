[site]: crossvalidated
[post_id]: 91522
[parent_id]: 91484
[tags]: 
A generative model is typically overfitting less because it allows the user to put in more side information in the form of class conditionals. Consider a generative model $p(c|x) = p(c)p(x|c)$. If the class conditionals are mulitvariate normals with shared covariance, this will have a linear decision boundary. Thus, the model by itself is just as powerful as a linear SVM or logistic regression. However, a discriminative classifier is much more free in the choice of decision function: it just has to find an appropriate hyperplane. The generative classifier however will need much less samples to find good parameters if the assumptions are valid. Sorry, this is rather handwavy and there is no hard math behind it. But it is an intuition.
