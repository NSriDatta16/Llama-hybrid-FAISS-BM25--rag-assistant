[site]: crossvalidated
[post_id]: 130832
[parent_id]: 
[tags]: 
Explanation that the prior predictive (marginal) distribution follows from prior and sampling distributions

While I have a vague intuition that this makes sense, I am interested in the formal demonstration that the prior predictive distribution in Bayesian inference is equal to the integral over $\theta$ of the product of the prior distribution $p(\theta)$ and the sampling distribution $p(y|\theta)$, such that: $$p(y) = \int_{\theta} p(\theta) p(y|\theta)\text{d}\theta.$$ Could one say that the integral makes the distribution unconditional (i.e. it removes the conditionality) by integrating over all possible parameters? If so, is there a more formal explanation?
