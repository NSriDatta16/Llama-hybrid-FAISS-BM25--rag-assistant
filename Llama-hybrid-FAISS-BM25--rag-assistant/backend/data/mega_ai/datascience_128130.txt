[site]: datascience
[post_id]: 128130
[parent_id]: 
[tags]: 
Decomposition and scaling and their effect on interpretability?

I have time series GDP growth rate data that I use as my Y and other X variables that I put into neural networks to make predictions. The two questions that I have are: When I decompose my GDP_growth variable I get a detrended variable that I model and make predictions of. If I have train and test data until 2023 and want to make predictions for 2024 them they will likely be way different than the real values as when I removed the trend term I got smaller numbers for the new y. So how to deal with this? When I get predictions of 0.0034 because the data it is fed (after it being decomposed) is between -0.05 and 0.05 interpretability is impossible as the real values are generally bigger than 0.05 and smaller than -0.05. This is not the networks fault as this is all the data that it sees still the interpretability is awful. When I scale my X's and put new data in (for example the year is 2025 and want to use the same model trained up to 2023 to make predictions) should I train a scaler upto 2023 and apply it to the 2025 data? If so, any unseen data for 2025 will be above the max or below the min of the min-max scaler.
