[site]: crossvalidated
[post_id]: 177796
[parent_id]: 
[tags]: 
Feature selection for clustering problems

I am trying to make group together different datasets using unsupervised algorithms (clustering). The problem is that I have many features (~500) and a small amount of cases (200-300). So far I used to do only classification problems for which I always had labeled data as training sets. There I used some criterion (i.e. random.forest.importance or information.gain) for preselection of the features and then I used a sequential forward selection for different learners to find the relevant features. Now I see that in case of unsupervised learning I have neither any criterion for preselection nor can I use the sequential forward selection (at least not in the mlr package). I was wondering if I could do a principal component analysis before to find a small number of features to fead to my clustering algorithm. Or do you have any other idea? Thanks edit: Ok, so after some research online I can update my question a bit: First of all I have read some articles that discourage the use of PCA before clustering algorithms, due to two reasons: The PCs are functions of all features so it is hard to relate the result to the inital data set and thus it is harder to interpret Moreover, if you have the problem that in truth only a very small fraction of your features are helpful to do the clustering, it is not said that these features are also describing the largest variance among the samples (which is what the PCs do) So PCA is off the table... Now I am back to my initial idea to do a sequential forward selection for clustering. What performance measure would you recommend? (I thought about the Dunn-Index) Which clustering algorithm would lead to clusters of more or less the same size? (for hierarchical clustering I usually get one cluster with a single outlier and another with all the rest -> so I would need something that somehow protects against outliers) Hope you guys can help me...
