[site]: crossvalidated
[post_id]: 475168
[parent_id]: 435782
[tags]: 
The MAP idea is to find the set of parameters $\hat{\theta}_{\mathrm{MAP}}$ which maximizes the posterior probability $p(\theta|X,y)$ , in other words, the most likely parameter vector. Using Bayes' rule and (for the last step) assuming statistical independence between individual samples, one obtains $$ \hat{\theta}_{\mathrm{MAP}} = \arg\max_\theta p(\theta|X,y) = \arg\max_\theta \frac{p(y|X,\theta) p(\theta)}{p(y|X)} \\ = \arg\max_\theta p(y|X,\theta) p(\theta) = \arg\max_\theta \log p(y|X,\theta) p(\theta) \\ = \arg\max_\theta \log p(\theta) + \sum_{i=1}^N \log p(y_i|x_i,\theta) $$ Now the first term represents the influence of the prior, and the second term the influence of the data. Obviously, the influence of the data grow with $N$ , whereas the influence of the prior stays constant. Therefore, in most instances, MAP estimates converge towards ML estimates, which is a consequence of the Bernstein-von Mises theorem . There are, of course, counterexamples in which the theorem does not hold and things go wrong, see, e.g., Diagonis and Freedman, "On the consistency of Bayes estimates" . This is probably true for the example you gave (I have not verified that explicitly). If you're not interested in all the technical details and just want something to cite, Murphy's Machine Learning: A probabilistic perspective , p.68-70 would be a good choice. I still recommend understanding the details, though. :)
