[site]: datascience
[post_id]: 122192
[parent_id]: 122191
[tags]: 
The main differences between BERT, GPT, and ELMo are the underlying model architecture and pre-training objective. BERT learns embeddings with Transformer (Vaswani et al., 2017) and its objective is Masked Language Modeling (MLM), GPT learns embeddings with Transformer and its objective is Causal Language Modeling. However, ELMo uses bi-directional LSTM (with two layers), and the objective of forward and backward LSTMs is also next-word prediction. In the simplest case, ELMo just selects the top layer activations as contextual embeddings, pools them, and uses it for a task-specific model.
