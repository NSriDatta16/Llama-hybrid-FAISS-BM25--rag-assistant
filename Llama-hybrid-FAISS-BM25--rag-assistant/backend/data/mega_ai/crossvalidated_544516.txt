[site]: crossvalidated
[post_id]: 544516
[parent_id]: 422430
[tags]: 
If you look at seq2seq models and transformers, the way this is usually done is by outputting a one-hot encoded vector using a softmax layer. Basically, the output will be a vector whose length is the size of the vocabulary.
