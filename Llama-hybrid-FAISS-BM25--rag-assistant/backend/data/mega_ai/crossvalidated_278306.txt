[site]: crossvalidated
[post_id]: 278306
[parent_id]: 
[tags]: 
Structural risk minimization in SVMs

SVM with gaussian kernel (RBF kernel) have infinite VC-dimension and the VC-dimension for SVM with polynomial kernels is very big too. Thus, I wonder how is possible that SVM have good generalization performances. In SRM (structural risk minimization) as larger is the VC dimension as larger is the risk. I know that SVM algorithm selects the hyperplane with minimum VC dimension (namely with minimum margin) however if the VC-dimension is infinite (like in gaussian kernels) this minimum is infinite and the risk will be likely high. How is it possible? (the question is more specific than an other already existing)
