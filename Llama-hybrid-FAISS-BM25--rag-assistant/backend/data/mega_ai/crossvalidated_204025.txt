[site]: crossvalidated
[post_id]: 204025
[parent_id]: 203919
[tags]: 
The effect of the SVM C-Parameter While the first textbook description of an SVM always speaks of "maximizing the margin", but this is only the first step. If your data is not perfectly separable there will points on the wrong side of the separating hyperplane. To allow for such points slack variables were introduced (= soft-margin SVM ). They include the problematic points into the equation and weight them using the C-Parameter. This parameter is a tradeoff between maximizing the margin and minimizing the error. Why this? Imagine (or draw on a paper) a perfectly separable 2D dataset with a plot similar to the above. Imagine a suitable hyperplane. Image you have a hard margin svm which does not allow for such misclassified points. Now imagine you will break the rules and place a document intentionally on the other side of the hyperplane. The hyperplane will probably change a lot and will be worse than before. If you had used a soft-margin SVM instead the old solution would still be a better one. Your example Increasing the value of the C-Parameter $\iff$ Weight of misclassified points is increased $\iff$ Margin gets smaller And i think that is what Hastie and Tibshirani meant in terms of stable: In other words closer to the hard-margin SVM.
