[site]: crossvalidated
[post_id]: 266859
[parent_id]: 
[tags]: 
Confusion when going from index notation to matrix notation

In Andrew Ng's lectures on machine learning, on page 4 of his first lecture notes , he presents the cost function for the ordinary least squares regression model as: $$ \begin{align} J(\theta) = \frac{1}{2} \sum_{i=1}^{m} (\theta^T x^{(i)} - y^{i}))^2 \end{align} $$ He then shows that the partial derivative can be shown as: $$ \begin{align} \frac{\partial}{\partial \theta_j} J(\theta) = \sum_{i=1}^{m} (\theta^T x^{(i)} - y^{(i)}) x_j^{(i)} \end{align} $$ However in the solution set he gives the gradient of $J(\theta)$ as: $$ \begin{align} \nabla_\theta J(\theta) = X^TX\theta - X^Ty \end{align} $$ My question is, how do I systematically go from the index form to the matrix form? As of right now, all I am doing is checking the dimensions of the matrices and making sure that they all come out correctly, but this seems to be inadequate. Yet I haven't found a good explanation as to how to convert from one form to the other. Help with this would be greatly appreciated.
