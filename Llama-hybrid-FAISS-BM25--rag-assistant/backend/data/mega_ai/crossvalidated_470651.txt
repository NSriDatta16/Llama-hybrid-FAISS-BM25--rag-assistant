[site]: crossvalidated
[post_id]: 470651
[parent_id]: 470625
[tags]: 
The essential difference is that an algebraic equation has no random (stochastic) component. However, for a regression model, for example: $$Y = \beta_0 + \beta_1 X_0 + \beta_2 X_1 + \beta_2 X_2 + \epsilon$$ Where the explanatory variables are presumed to be fixed and known (non-stochastic in nature) together with a single random error term. In the special case of a Least-Absolute Deviation (LAD) regression model, the error term is presumed to follow a Laplace distribution, while for classic Least-Squares regression, the error term is presumed to be a Normal random deviate with a mean value of zero. Wikipedia on the Logistic regression further comments: The logistic regression can be understood simply as finding the $\beta $ parameters that best fit: $y = 1 \text{ } \beta_0 + \beta_1 x + \epsilon > 0 $ $\text{ } = 0$ elsewhere $\epsilon $ is an error distributed by the standard logistic distribution. (If the standard normal distribution is used instead, it is a probit model.)
