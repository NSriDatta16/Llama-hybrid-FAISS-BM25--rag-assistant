[site]: crossvalidated
[post_id]: 429702
[parent_id]: 429668
[tags]: 
Your questions mostly concern the implementation of Fasttext rather than the underlying statistical concepts. I couldn't find clear documentation on how sentence embeddings are calculated from the words embeddings, but looking at the C code provided some hints. The doc string for get_sentence_vector [here][ https://github.com/facebookresearch/fastText/blob/2bef44b36fe6fb900e20fdb5828e7602484a5d29/python/fasttext_module/fasttext/FastText.py] says: Given a string, get a single vector representation. This function assumes to be given a single line of text. We split words on whitespace (space, newline, tab, vertical tab) and the control characters carriage return, formfeed and the null character. I understand this to mean that hyphens are not split and words with hyphens are therefore considered a single token to be learned. Looking at getSentenceVector here , I think that the vector for each word is normalized by its sum and the normalized vectors are summed up. I suspect this method overemphasizes the occurrence of 'laptop' in your example. No, as we know from 2., fasttext sums up the meanings for each word. If you want to 'understand' the sentence including negations, then actual language models that model not only the words but the sentence as a structure will be more helpful. For example, look at recurrent neural network architectures or transformer architectures. Recent language models relax the bag-of-words assumption and process the words in order. RNNs will process the embedded word input one word at a time and aggregate the processed content into a latent state. At some point, the hidden state is typically passed on to a fully connected layer to make a prediction used for training. However, you could load well-optimized weights, use the recurrent layers to calculate the latent state for a phrase while ignoring the later layers, and then compare the hidden states of phrases/sentences. Recent architecture for which pre-trained weights and tutorials are available are ULMfit or ELMo . For both these models, common use would be to not train the model on your data at all (although finetuning is possible), but to download a set of weights trained on a large corpus.
