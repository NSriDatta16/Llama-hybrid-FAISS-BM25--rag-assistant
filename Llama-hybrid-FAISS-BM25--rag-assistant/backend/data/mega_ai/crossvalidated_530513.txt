[site]: crossvalidated
[post_id]: 530513
[parent_id]: 
[tags]: 
NLP - train a model

I have a large set of large texts (around 60K texts, each one having between 100 and 30K words). Each text has 5 corresponding values (the famous big5 traits). The task is the following: for a new unseen text, predict the following 5 values. My approach : Tokenize text with word_tokenize , stem each text with PorterStemmer().stem . Run TfidfVectorizer(lowercase=True, max_features=10000) on the texts, then reduce the 10K dimension to 700 features with PCA(n_components=700) . So basically, now we have training data X (with size 60Kx1K and y with size 60Kx5 ). Fit this data into a neural network with 2 hidden layers (256 and 128 neurons, respectively). Predict Error metric : a sample is classified correctly, if all of its 5 values are with no more than 10 difference than the original one. Expected model accuracy: 70% Current model accuracy: 36% My question is: what methods could I use/what should I change in my current methods, in order to increase the accuracy? I am thinking that the fact that some of the texts have around 500-600 words maybe actually confuses the model because I have set it up to work on really large texts (because of the max_features and PCA). A possible idea: Split each long text into subtexts of size 512 - for each subtext, label it with the original text labels. Then encode all the data using BERT - it accepts the maximum word length of 512 . Then we have augmented the data this way. The problem is - having so short texts may be not enough to capture the "big idea", that is, the 5 values perhaps can't be predicted by so short texts. The classification will be: split long texts into subtexts, classify each one, and average the classifications. Thank you in advance.
