[site]: datascience
[post_id]: 69325
[parent_id]: 35861
[tags]: 
Alternatively to the accepted answer, another way to estimate the uncertainty of a specific prediction is to combine the probabilities returned by the model for each class using a certain function. This is a common practice in "Active learning", where given a trained model you select a subset of unlabelled instances to label (to augment the initial training dataset) based on some sort of uncertainty estimation. The three most common functions used (called sampling strategies [1] in the literature) are: Shannon entropy: you simply apply Shannon entropy to the probabilities returned by the model for each class. The highest the entropy the highest the uncertainty. Least confident: you simply look at the highest probability returned by the model among all classes. Intuitively the certainty level is lower for a test instance with a 'low' highest probability (e.g. [.6, .35, .05] --> .6) compared to a 'high' highest probability (eg. [.9, .05, .05] --> .9). Margin Sampling: you subtract form the highest probability the second-highest probability (e.g. [.6, .35, .05] --> .6-.35=.25). It is conceptually similar to the least confident strategy, but a bit more reliable since you're looking at the distance between two probabilities rather than a single raw value. Also, in this case, a small difference means a high uncertainty level. Another more interesting way to estimate the uncertainty level for a test instance that is applicable to deep models with dropout layers is instead deep active learning [2]. Basically, by leaving dropout active while doing predictions you can bootstrap a set of different outcomes (in terms of probabilities for each class) from which you can estimate mean and variance. The variance, in this case, tells you how much the model is uncertain about that instance. Anyway, consider that these are just crude approximations, using a model that specifically estimates the uncertainty of a particular prediction as suggested in the accepted answer is surely the best option. Nevertheless, these estimations can be useful because they are potentially applicable to every model that returns probabilities (and there are also adaptations for models like SVM). [1] http://www.robotics.stanford.edu/~stong/papers/tong_thesis.pdf [2] https://arxiv.org/abs/1808.05697
