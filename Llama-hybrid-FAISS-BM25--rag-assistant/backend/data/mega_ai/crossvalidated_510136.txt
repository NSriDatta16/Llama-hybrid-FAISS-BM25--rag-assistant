[site]: crossvalidated
[post_id]: 510136
[parent_id]: 510002
[tags]: 
An average seems a sensible way of comparing the scores. The only thing to take into account is that if a student has got fewer reviews, then the average score of that student will be more sensitive to the presence of a very strict or a very generous reviewer, while students with a higher number of reviewers will have a more fair average score. Coming back to your questions: "does the fact that Anna had more reviews than Billy change the statistical relevance of that simple calculation?" The higher the number of reviewers, the more statistically relevant the score is. The lower the number, the more subject to chance it is. Specifically, if the standard deviation of the reviewer's scores is $ \sigma_r $ , then the variability $\sigma_s$ of the student's average score is $ \sigma_s \approx \sigma_r / \sqrt{N}$ , where $N$ is the number of reviewers. You want $\sigma_s$ small, so you can reduce $\sigma_r$ (that is, having very good reviewers) or increase $N$ (a higher number of reviewers, even if they have less quality as e.g. if they have lower expertise). "Is there something more that should be done to account for the variation in number of reviews?" You could set a minimum number of reviewers to reduce the chances of a student getting values that are not representative, a consequence of some extreme or biased reviews. Also, you could at the end check if there are students that performed exceedingly well or bad, but they happen to be among those with the lowest amount of reviews. For those cases, maybe further reviewing is advisable as a sanity check, to ensure that their average score is actually representative of the student's performance.
