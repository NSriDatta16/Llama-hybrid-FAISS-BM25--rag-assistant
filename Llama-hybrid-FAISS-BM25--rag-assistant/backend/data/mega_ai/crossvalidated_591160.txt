[site]: crossvalidated
[post_id]: 591160
[parent_id]: 591151
[tags]: 
From a purely logical perspective: Regularization aims at controlling variance. If you are trying to predict anything at times out into the future, further points are inclined to vary more than nearer points in time Thus, the prediction of where a vehicle will be 10-seconds from last known location based on some set of variables, is likely to vary more than the prediction 1-second out. So, if you were trying to estimate further into the future, you would want more regularization to help constrain that variance. This is really no different than a traditional time series forecast, where will some point be at some future time. And you can see in forecasting that variance increases further into the future. So, 10-seconds wins the stronger regularization challenge! The second part of the question is a lot more challenging and might have something to do with what type of neural network model you were building, what sorts of data you were using and what information you have to inform the model in deployment. However, one thing Neural Networks suck at is Extrapolation. They are great a defining potential values within a data space defined by training data(interpolation) and very poor predicting values outside the training space (Extrapolations). If the goal is to reduce variance in the model at 10-seconds without regularizing the best way to do that is to be sure that you had plenty of data up to, including and well past the 10-second mark for each of the training observations. The more data you have describing the potential evolutions of travel at that 10-second time period and beyond, relative to the actions of the vehicle & driver, the more completely the model can learn in that space and the less variance you will see in the predictions, provided your data is reflective of all potential outcomes.
