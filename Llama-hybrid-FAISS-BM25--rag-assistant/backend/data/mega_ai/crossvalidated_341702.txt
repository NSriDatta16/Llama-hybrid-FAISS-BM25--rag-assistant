[site]: crossvalidated
[post_id]: 341702
[parent_id]: 
[tags]: 
How constraints about the posterior can be used in bayesian inference?

I'm referring to bayesian inference as described by E. T. Jaynes in "Probability theory: The logic of science": $$P(\theta \mid X, I) = P(\theta\mid I) \cdot \frac{P(X \mid \theta, I)}{P(X \mid I)}$$ Where $\theta$ is the parameter, $X$ the observed data and $I$ the background information we have about the situation. If I know that the posterior distribution for $\theta$ satisfies a given constraint, how can I incorporate it into $I$ so that the formula produces an adequate posterior? Does that question make sense according to Jaynes/Cox objective bayesianism point of view? I.E. can we reasonably have knowledge of some constraint on the posterior? If so, and if there is no way to ensure the produced posterior satisfies the constraint, it this a counter example to this point of view? I'm just trying to get a better grasp of all of this.
