[site]: crossvalidated
[post_id]: 438751
[parent_id]: 438158
[tags]: 
I don't think you can do this easily. Because GMM usually considers correlations in the data, all features will be interconnected. Also, GMM learns a scale of each feature in each cluster, so you can not just argue that feature a has a larger scale, so it has !ore effect (this does work with k-means, but not GMM). You could try the following: randomly "wiggle" each data point along each feature independently by, say, +-0.1 * standard deviation in this feature. Then observe how much the cluster assignment of all points changes on average. You can then argue that features where this changes the most are more important (most sensitive, need the values more precisely). But you can also make a similar argument for these being least important (least stable). It's all about interpretation and you use case.
