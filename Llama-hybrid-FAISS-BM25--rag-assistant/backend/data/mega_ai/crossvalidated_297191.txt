[site]: crossvalidated
[post_id]: 297191
[parent_id]: 295643
[tags]: 
I've hit problems like this, and it was due to hitting numeric instability (floating point numbers getting too big or too small). That lowering the learning rate makes it happen later is supporting evidence for that idea. In every case it was always doing something unreasonable; as soon as you do bring in the validation training set, and early stopping, you never reach enough trees for the algorithm to have this kind of nervous breakdown. (The other place it happens is with deep learning auto encoders, when not using Tanh activation.) If it does still happen, then either you have a very interesting data set and/or you have found a bug in H2O. I'm sure the developers would love it if you could narrow it down to a small reproducible example, as it could be quite a serious bug.
