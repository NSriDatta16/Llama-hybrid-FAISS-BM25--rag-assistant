[site]: datascience
[post_id]: 80500
[parent_id]: 80436
[tags]: 
My first question is: What are the filters? Convention in Deep Learning is to call the weights used in the convolution either filters or synonymously kernels. Other fields make a distinction between these two terms – in my book, the array of weights is the kernel, and the operation that performs a convolution with those weights is the filter. I.e., a filter is a function that takes some input e.g. image and gives you a, well, filtered image. With the understanding that we're talking convolutional filters, training the kernel is equivalent to training the filter, because the filter is completely defined by the weights in the kernel. (Mathematically speaking, convolutional filters are the class of linear time-invariant filters with compact support.) But how are they getting initialized? There's a myriad of ways, see other answers. how does someone backpropagate the filter of the convolutional layer That's where it does pay off for understanding to make a distinction between filters and kernels. What you're actually doing is passing two arguments to the convolution operation: the kernel and the input . $$ f(k,x) = k\star x $$ The filter is $f$ partially applied to the kernel: $$ f_k = \backslash x \mapsto f(k,x) $$ That is what you're eventually interested in; after training the kernel will be fixed so the filter is only a function of $x$ . But you can't really backpropagate the filter (at least in the common frameworks) because it's a function. What you backpropagate is the kernel , and that works the same way as you'd backpropagate any other parameters: you evaluate $f$ together with its derivatives (of both arguments!) for one particular $x$ in the forward pass, and then send through a dual vector in the backwards pass that tells you the gradient contributions in both $x$ and $k$ . The one in $x$ you back-pass further to the preceding layer, the one in $k$ you use for the learning update. Secondly, I have noticed that I can add an activation function to the convolutional layer in Keras Yes, but the activation isn't really part of the convolution operation. It's best understood as a separate layer, but because it doesn't have any parameters and because CNNs typically contain a Relu after each and every convolution, Keras has a shortcut for this. $$ g(k,x) = \operatorname{Relu}(f_k(x)) $$ $$ g_k = \bigl(\backslash x\mapsto \operatorname{Relu}(f_k(x))\bigr) = \operatorname{Relu} \circ f_k $$ To backpropagate this, you first pull the backwards pass through the activation's derivative before getting to the convolution. Last but not least, does a convolutional layer have weight and biases like a dense layer? Yes, the weights are in the kernel and typically you'll add biases too, which works in exactly the same way as it would for a fully-connected architecture. One thing that is important in convolutional architectures, and often not really explained very well, is that one such layer isn't actually just a single convolutional filter but a whole “bank” of such filters, each with its own kernel. Thus for every input you get a whole family of outputs, which are called channels : $$ f_i(\mathbf{k},x) = f(k_i,x) $$ Then, you pass all of those to the next layer, i.e. the layers also need to accept multiple channels – which is again handled by having multiple kernels, i.e. you have a whole matrix of kernels . A mathematical way of looking at this is that the signal flow contains not vectors in the space $I$ of images, but in a tensor-product space $\mathbb{R}^m\otimes I$ , where $m$ is the number of channels. And on the “channel space”, you're actually performing fully-connected. (When people talk about $1\times1$ convolutional layers, it means they're not actually performing a meanigful convolution at all, but just a fully-connected matrix between the channels.)
