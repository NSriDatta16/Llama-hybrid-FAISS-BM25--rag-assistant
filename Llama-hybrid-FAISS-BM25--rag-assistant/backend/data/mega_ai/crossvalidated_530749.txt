[site]: crossvalidated
[post_id]: 530749
[parent_id]: 530745
[tags]: 
It is a decision tree parameter : maxBins : Number of bins used when discretizing continuous features. Increasing maxBins allows the algorithm to consider more split candidates and make fine-grained split decisions. However, it also increases computation and communication. Note that the maxBins parameter must be at least the maximum number of categories $M$ for any categorical feature. If you have a categorical variable with $K$ categories, then If $K > M$ , what you should do for this to make sense, is to cluster similar categories together. Likely, Spark's implementation does deterministic splits. Categorical variable codes categories as arbitrary numbers, so packing together categories 1 and 2 only because the numbers are close, does not have to mean that such grouping makes any sense at all. If you would like to reduce the dimensionality, you would need to do some kind of clustering to pack similar categories together. If $K = M$ it would be an identify function, you are not doing any binning since each bin has only one value. If $K you would be packing the same values into different bins. From a feature engineering point of view, it doesn't make sense. Likely they are recommending this because of some implementations details of how they do the distributed training.
