[site]: crossvalidated
[post_id]: 197525
[parent_id]: 193824
[tags]: 
Yes, the problem you're describing can be modeled as a Markov decision process, particularly one with a continuous domain. (Meaning, a continuous state space.) Your room example would have a four-dimensional state space, corresponding to each room's temperature and humidity. A standard example of this type of problem is the mountain car, described in Sutton and Barto's reinforcement learning text , which might help you understand the problem in fewer dimensions. This tutorial may also help, particularly if you can follow Java. To your question about the reward function, you can answer this by asking what costs are associated with dipping below this range. This doesn't need to be exact to lead to the correct solution. Optimal policies are invariant to scaling and shifting operations performed on the reward function. Meaning, if the cost of going outside the parameters is uniform, a reward of $-1$ in the out-of-bounds states should suffice. (In other applications, the reward function is just an arbitrary means of encoding a goal. E.g., the goal state has a reward of, say, $100$, with other rewards uniformly $0$ or a small negative number.
