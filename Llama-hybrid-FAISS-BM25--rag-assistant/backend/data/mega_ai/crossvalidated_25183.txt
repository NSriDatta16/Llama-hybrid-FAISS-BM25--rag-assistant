[site]: crossvalidated
[post_id]: 25183
[parent_id]: 24705
[tags]: 
Yes you can, and in fact this is precisely what the R package GLMNET does for multinomial logistic regression. Writing the log-likelihood function as: $$LogL=\sum_i\sum_cn_{ic}\log(p_{ic})$$ Where $i$ denotes observations and $c$ denotes the multinomial categories $n_{ic}$ is the observed count for observation $i$ in category $c$. The observations are defined by their unique covariate combinations - or alternatively we can allow duplicates and set each $n_{ic}=1$ so that we have categorical "binary" data (....don't know what the plural of binary is....). For logistic regression the probabilities are defined as: $$p_{ic}=\frac{\exp\left(x_{i}^T\beta_{c}\right)}{\sum_{c'}\exp\left(x_{i}^T\beta_{c'}\right)}$$ This is a less than full rank parameterisation and can be useful if you are using penalised likelihood (such as GLMNET). We could in principle use IRLS/newton rhapson on the full beta matrix $(\beta_1,\dots,\beta_{C})$, however you end up with non-diagonal weight matrices. Alternatively we can optimise "Gibbs-style" by fixing all categories betas except for one, and then optimising just over that category. Then proceed to the next category, and so on. You can see that because the probabilities have the form $$p_{ic}=\frac{\exp\left(x_{i}^T\beta_{c}\right)}{\exp\left(x_{i}^T\beta_{c}\right)+A}\text{ where }\frac{\partial A}{\partial \beta_c}=0$$ $$p_{ic'}=\frac{B}{\exp\left(x_{i}^T\beta_{c}\right)+A}\text{ where }\frac{\partial B}{\partial \beta_c}=0$$ That the quadratic expansion about $\beta_c$ will have the same form as for logistic regression, but with the IRLS weights calculated differently - although we still have $W_{ii,c}=n_{ic}p_{ic}(1-p_{ic})$ in the usual $(X^TWX)^{-1}X^TWY$ update of beta.
