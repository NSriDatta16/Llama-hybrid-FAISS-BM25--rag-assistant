[site]: crossvalidated
[post_id]: 435342
[parent_id]: 
[tags]: 
How to transform $P[k_1\leq (x_i-\mu - \sigma\cdot Z)^2 \leq k_2]$ to $P[k_1\leq \frac{(x_i-\mu)^2}{\sigma^2}+e \leq k_2]$?

Taste estimation As an example consider an experiment conducted to determine the optimal concentration of salt in popcorn. The concentration of salt in sample $i$ is denoted by ${x_i}$ . The subject in this experiment is asked to rate samples of popcorn containing different but known concentrations of salt by stating their preference as either negative,neutral or positive. These responses are encoded by response variable ${y}$ as ${0,1,2}$ respectively. The goal of this experiment is to determine the properties of the latent optimal salt concentration denoted by $x^*$ Intuitively if there is either much more or much less salt present than the subject likes a negative preference will be stated ( $y = 0$ ), if the concentration of salt is close to the latent optimum the subjects response will be positive ( $y=2$ ) and neutral ( $y=1$ ) if it is in between. Let us formalise this intuition as: the subject rates the samples according to whether the distance between the sample's salt concentration ( $x_i$ ) and the preferred concentration ( $x^*$ ) measured by some latent distance function $d$ meets some threshold ( $k_j$ ). Which is expressed by the following equation (eq. 1): $$y= \begin{cases} 2 & d(x_i,x^*) Due to random measurement errors on the part of the subject, $x^*$ is assumed to follow some distribution, for example a normal distribution as follows: $x^* = \mu + \sigma*Z$ , where $Z \sim N(0,1)$ and $\sigma \geq 0$ . The following squared distance function $d$ is considered: $$d(x_i,x^*) = (x_i-x^*)^2 = (x_i-\mu - \sigma*Z)^2 \tag{2}$$ Such that the probability of stating a neutral preference can be expressed as: $$\Bbb{P}[y=1] = \Bbb{P}[k_1\leq d(x_i,x^*)\leq k_2] = \Bbb{P}[k_1\leq (x_i-\mu - \sigma*Z)^2 \leq k_2] \tag{3}$$ In the context of ordinal logistic regression one would like to express this as a function of $x_i$ , parameters( $\mu,\sigma$ ) and an error ( $e$ ) term as $f(x_i,\mu,\sigma)+e$ . Where $e$ follows a distribution with an analytical CDF (or its CDF can be approximated as such). One would like to transform $d(x_i,x^*)$ to $f(x_i,\mu,\sigma)$ , where: $$ f(x_i,\mu,\sigma) =\frac{(x_i-\mu)^2}{\sigma^2} \tag{4} $$ Such that one arrives at something closely reassembling the (squared) Mahalanobis distance. $$y= \begin{cases} 2 & \frac{(x_i-\mu)^2}{\sigma^2} + e And one can write eg.3 as $$\Bbb{P}[y=1] = \Bbb{P}[k_1\leq d(x_i,x^*)\leq k_2] = \Bbb{P}[k_1\leq f(x_i,\mu,\sigma)+e \leq k_2] = \Bbb{P}[e \leq k_2 - \frac{(x_i-\mu)^2}{\sigma^2}] - \Bbb{P}[e \leq -k_1 + \frac{(x_i-\mu)^2}{\sigma^2}] \tag{6} $$ Where for example $e \sim HalfLogistic $ . (Or some other distribution with an analytical CDF). So within this framework $x^*$ represents the set of all salt concentrations the subject might enjoy and the subject states their preference in accordance to the likelihood with which each sample is a member of that set. Now in order to estimate the distributional properties of $x^*$ , maximum likelihood estimation can be employed to estimate $\mu$ and $\sigma$ . By maximizing the log-likelihood function for eq.5. Which, due to the analytical nature of the CDF can optimized using Iterative Re-Weighted Least Squares. Question My question is, which approximation or assumption could one employ such that one could rewrite eq.3 in terms of eq.6? (so eq.2 to eq.4 in the context of eq.3 and eq.5) And which distribution would $e$ follow? Alternatively, one could start based on some behavioural insight by defining eq. 5 assuming some distribution for $e$ , how could one derive the distribution of $x^*$ in eq.2? And which distribution should be assumed for $e$ ?
