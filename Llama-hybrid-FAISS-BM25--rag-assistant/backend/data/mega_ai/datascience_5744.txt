[site]: datascience
[post_id]: 5744
[parent_id]: 77
[tags]: 
There are good/fast ways to model graphs in RDBMS, and dumb/slow ways. Some use clever indexing and Stored Procs, trading CPU load and tuned temp tables on RAM disks for faster graph retrieval speed. Some use precomputed graph paths (this may be less feasible in social network scenario, but in a tree with majority of nodes being leaf nodes, it's a pretty good tradeoff space-for-time Some simply compute in a loop, using un-tuned in-indexed temp table. From the #s thrown in the article, that smells like what they did (30 second- performance on fairly smallish data-set) For example, I have my own tree computation. It is encapsulated in a highly-tuned stored proc While it's running in an enterprise-sized-hardware Sybase ASE15 dataserver, that server is shared with a couple terabytes of data from all other enterprise apps, some much more data hungry than mine; and isn't dedicated solely to executing my queries. I did not have access to the main speedup tool, a temp table on a RAM disk. A representative set of data I was retrieving that seems to somewhat match theirs was getting a 150,000 node subtree out of 2.5M node full forest dataset (unlimited depth of tree, which varies between 5 and 15, but smaller average arity of a given node than the 50 friends listed in the experiment) I tuned it to the point that this query ~30-45 seconds. It most certainly does NOT exhibit the exponential slowdown that the figures in the question seem to indicate on their RDBMS performance, which is extra double strange given there is no exponential growth in the result set (which to me reeks of un-tuned index on a temp table from personal experience). So, this comparison is most likely incorrect and based on poor RDBMS side design, although as the previous answer noted, it is impossible to ascertain without them open sourcing 100% of their code and table definitions.
