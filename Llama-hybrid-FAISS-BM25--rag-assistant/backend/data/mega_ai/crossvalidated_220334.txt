[site]: crossvalidated
[post_id]: 220334
[parent_id]: 
[tags]: 
Does backpropagation of error in multilayer neural network depend on the objective function being optimized?

I know for square error gradient descent is equivalent to back propagation. However, for a general objective function, how can I convince myself that back propagation is equivalent to computing derivatives at every layer.
