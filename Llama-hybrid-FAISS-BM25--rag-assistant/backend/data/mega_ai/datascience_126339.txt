[site]: datascience
[post_id]: 126339
[parent_id]: 126337
[tags]: 
Statisticians typically focus on probabilistic classification. In the way they build their models they are interested in predicting $P(Y|X) = P(X|Y)P(Y)/P(X)$ . Now we find : The predicted probabilities are seriously harmed when altering P(Y) during training. The most extreme case is when you have no predictive variable and consider estimating P(Y) only. Should you fudge with the natural prevalences of the outcomes of Y? No! If you observed Y=1 in 2 percent of the cases your model should estimate $\hat{P}(Y=1)=0.02$ . Done. If you had some predictive variables and wanted to do a binary classification based on $\hat{P}(Y=1|X)=0.02$ do threshold tuning ( don't stick with a threshold of 0.5 which seems to be good mainly for balanced datasets ...). Also note that probabilistic classification is best evaluated with scoring rules https://en.m.wikipedia.org/wiki/Scoring_rule and not with metrics for binarized events (see https://www.fharrell.com/post/class-damage/ ). Some scoring rules like the one related to cross entropy loss are also directly related to maximum likelihood estimation of a data generating process. Data Scientists may be interested in other goals than probabilistic classification. See the following two points: what is a valid reason for being interested in binary classification instead of predicting probabilities? Well sometimes you might have to estimate expected costs based on classification outcomes. If you have a very imbalanced cost for your false positives or false negatives you may be able to reduce the expected costs by trading off between one type of error and another one. You do not necessarily need a calibrated model then. you might wanna do data augmentation for improved generalization (but you should still maintain the class frequencies if you look for a calibrated classifier). Data augmentation may be valid and a great idea depending on the model and depending on the fact if the augmentation adds new information by introduction of systematic real world variation While the statistician task 1) is a correct perspective also points 2-3 are valid (and more present in data science). 2 is itself a valid problem and 3 is a valid approach for adding information to your model. However sometimes people end up wrongly and blindly advocating for upsampling or down sampling (especially when you have a fixed threshold at 0.5). Note that up/down sampling is (more or less) equivalent to assigning class weights to training examples - this is obvious when considering gradients . Point 2 can be expanded to other classification settings: in a typical data science project you might find a multi-class classification setting where simple 1 dimensional threshold tuning will not work anymore to improve the precision/recall tradeoff for all classes equally. (And you might have to make this tradeoff due to a high cost imbalance) How can you solve these multi target optimizations from point 2? You could use oversampling/downsampling/weighting as an indirect way to find different Pareto optimal points. Given different fractions of targets in up- or downsampling you will achieve different values of precision and recall and will move on the Pareto front. (red Pareto front (bigger is better) with x axis e.g. recall and y axis precision. Since there is no total order (but only the Pareto order https://en.m.wikipedia.org/w/index.php?title=Pareto_order ) you cannot decide which point on the Pareto front is better, image by Berklas under CC-by SA) Ways to find a Pareto optimal solution while avoiding to fudge with the class frequencies by upsampling or downsampling: The draft https://arxiv.org/abs/2201.08528v3 ("To SMOTE or not to SMOTE?") shows that for binary classification threshold-tuning is equally good to up or down sampling in strong learners (XGboost etc) and maintains calibration. With an explicit cost assignment you can alternatively do cost-sensitive machine learning and get class assignments which are cost-optimal. This scalarization with a cost function is a more explicit way to deal with the fact that in a multi-objective optimization (detecting multiple classes optimally) there is multiple incomparable Pareto optimal solutions. Update : The above was mainly written with probabilistic classification in mind. The same facts, however, also hold true for regression. A statistician tends to describe the data generating processes (e.g. using maximum likelihood estimation) while a data scientist may have other goals in mind. When describing a data generating process you may want to achieve e.g. calibration (in problems where a data scientist sees the classification task) unbiasedness for the conditional mean (in regression where you use the mean squared error/ Gaussian log likelihood) or a conditional quantile (in quantile regression). Still sometimes also statisticians are interested in biased estimators because they have a lower variance (making a bias-variance tradeoff) Oversampling or downsampling may affect the conditional bias of a regression model (in a similar way like it affects calibration) and the variance of the model: typically increasing the bias (by upsampling) reduces the variance. It may be desirable to modify the bias of your model due to asymmetric costs of overestimation or underestimation. In Cost-sensitive regression a Pareto optimal solution is found by assigning costs for overestimation and underestimation. You can alternatively move along the Pareto front by assigning weights/over- undersampling your training examples. However, this is very indirect and you probably will not know which tradeoff you are going to get before performing an experiment. Summary : When solving a multi-objective optimization problem 2), rather than doing oversampling/undersampling with unknown a-priori outcome , you might better do: Cost-sensitive learning Regularization (for the bias-variance tradeoff) Threshold-tuning ... in order to achieve your pareto-optimal solution in more direct way. If you have a probabilistic classification problem 1), then a statistician would also warn you from binarizing it and perform evaluation of the probability model with metrics like accuracy, precision, recall, etc (see Frank Harrells post). PS : You should not confuse downsampling with rejection sampling which checks for some pattern in your data. In another scenario (not mentioned above) your training data could be drawn not at random , but by a biased sampling strategy . Then rejecting some of these biased samples based on a characteristic of the data might seem like downsampling while in reality it is a form of rejection sampling just correcting the biased sampling
