[site]: crossvalidated
[post_id]: 540997
[parent_id]: 
[tags]: 
Dense layer with more neurons than on input

What is the purpose of having a dense layer with more neurons on the output than it received on input? Let's imagine we have a neural network in which the last layer has the size of 1. Does it make any sense to add a Dense layer to this network with output shape of, say, 128? If so, how does it work? Having a single neuron on input to dense layer intuitively seems nonsense to me, as every neuron in dense layer will only receive this single signal anyway. Are there any advantages of architecture like that?
