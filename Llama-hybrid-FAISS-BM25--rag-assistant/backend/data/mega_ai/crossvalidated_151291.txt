[site]: crossvalidated
[post_id]: 151291
[parent_id]: 151269
[tags]: 
It appears that your predictors share a particular type of multi-colinearity known as suppression. Suppression refers to a pattern of multi-colinearity that results in a larger or reversed effect in multiple regression compared to simple regression. Since the partial correlation of D is larger (and sig.) in the multiple regression case than in the simple regression case we know that the unique effect of D that is not shared with the other predictors was being "suppressed" in the simple regression case. Remember that whereas simple regression presents a "total" effect of your single predictor, multiple regression yields the "unique" effect of each predictor after partialing out any shared explanatory power. For example, if we wanted to estimate children's weights from their heights and ages, height and age likely explain a lot of the same variability in weight because taller children tend to be older. To illustrate suppression, let's say we wanted to predict children's weights using age and number of hours involved in organized sports to predict weight. What we might imagine is that older children tend to be involved in more sports (i.e. age and # hours are positively correlated). We might also reason that children who are involved in more organized sports tend to weigh less than children who are not involved in organized sports. However, since age and involvement in organized sports are positively correlated, some part of the # hours-weight relationship is really driven by the relationship between age and weight (which is positive). Since both this positive correlation and negative correlation are going into the overall correlation between # hours and weight, the total effect may appear smaller than the partial effect. Here is a visual representation of this: Each circle represents one of the variables in the example. The dotted line within the two predictor circles separates out the portion of that predictor that shares explanatory power with the other predictor. The small portion of the # hours circle is the portion of explanatory power that is is shared with age. The dotted lines represent the portion of the total effect of a predictor that is really driven by the other predictor. For example, the dotted line with the + symbol is the part of the # hours simple relationship that is really driven by child age. The solid lines represent the partial correlation of that predictor with the outcome over and above any shared explanatory power of the two predictors. The total effect would be a combination of the dotted and solid lines stemming from the same predictor variable. As we can see, the + and - relationships would sort of wash each other out. Thus, the partial correlation would be stronger than the simple correlation. The way that suppression works with more than 2 predictors gets a bit more complicated, but I bet there is some overall relationship among your predictors. If you calculate (or ask the computer to calculate) tolerance, this should help you know what's going on. Tolerance varies from 0 to 1 and tells you the extent to which your predictors are orthogonal. 1 indidicates that there is no multicollinearity and 0 indicates that there is complete multicolinearity (fun fact, if you regressed D on A, B and C the $R^2$ you would obtain would be 1-tolerance). Since there are more than 2 predictors, the multi collinearity issue is more complex than just, "are there simple relationships among my predictors". As such, I think tolerance would be a better measure of this. In terms of explaining why such suppression exists, I would think about the simple example above and consider whether it would make sense that such a phenomenon is happening with your own data. Hope this is helpful! A tolerance of .67 means that something like 33% of the variability in one of your predictors is explained by the other 3 predictors. This seems, to me, like a non trivial amount of redundancy.
