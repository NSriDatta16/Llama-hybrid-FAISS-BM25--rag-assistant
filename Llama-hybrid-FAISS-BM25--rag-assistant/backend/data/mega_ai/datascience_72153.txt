[site]: datascience
[post_id]: 72153
[parent_id]: 
[tags]: 
On simple 1D dataset, LogisticRegressionCV selects terrible hyperparameters, resulting scores are nonsensical

I am trying to use LogisticRegressionCV to fit a logistic regression model to a simple 1D dataset. Very oddly, when given a choice, it seems to select a tiny C value, which forces my model to select a tiny theta resulting in a useless model. I tried looking at the scores_ provided by the model, but they make no sense. For example, when I tell it to do 3-fold cross validation with 5 choices of C values, it gives me: {1: array([[0.47058824, 0.47058824, 0.47058824, 0.47058824, 0.47058824], [1. , 1. , 1. , 1. , 1. ], [0.63636364, 0.63636364, 0.63636364, 0.63636364, 0.63636364]])} The dataset is not linearly separable, and yet it claims to be getting 100% accuracy regardless of which C values I've given it to try. Example code below: import numpy as np import pandas as pd from sklearn.linear_model import LogisticRegressionCV from sklearn.linear_model import LogisticRegression def gen_y(x): p1 = np.clip(x + 0.5, 0, 1) v = np.random.uniform(0, 1) if v
