esponse to a user, such as "Here is your Wikipedia article on" or "Up to my last training update", the article is typically tagged for speedy deletion. Other signs of AI use include excessive use of em dashes, overuse of the word "moreover", promotional material in articles that describes something as "breathtaking" and formatting issues like using curly quotation marks instead of straight versions. During the discussion on implementing the speedy deletion policy, one user, who is an article reviewer, said that he is "flooded non-stop with horrendous drafts" created using AI. Other users said that AI articles have a large amount of "lies and fake references" and that it takes a significant amount of time to fix the issues. Ilyas Lebleu, founder of WikiProject AI Cleanup, said that they and their fellow editors noticed a pattern of unnatural writing that could be connected to ChatGPT. They added that AI is able to mass-produce content that sounds real while being completely fake, leading to the creation of hoax articles on Wikipedia that they were tasked to delete. Wikipedia created a guide on how to spot signs of AI-generated writing, titled "Signs of AI writing". Hoaxes and malicious AI use In 2023, researchers discovered that ChatGPT frequently fabricates information and makes up fake articles for its users. At that time, a ban on AI was deemed "too harsh" by the community. AI was deliberately used to create various hoax articles on Wikipedia. For example, an in-depth 2,000-word article about an Ottoman fortress that never existed was found by Ilyas Lebleu and their team. Another example showed a user adding AI-generated misinformation to an article on Estola albosignata, a species of beetle. The paragraph seemed normal but referenced an unrelated article. AI has been used on Wikipedia to advocate for certain political viewpoints in articles covered by contentious topic guidelines. One instance showed a banned editor using AI to engage in edit wars and manipulate Albanian history-related articles. Other instances included users generating articles about political movements or weapons, but dedicating the majority of the content to a different subject, such as by pointedly referencing JD Vance or Volodymyr Zelensky. Simple Article Summaries In 2025, Wikimedia started testing a "Simple Article Summaries" feature which would provide AI-generated summaries of Wikipedia articles, similar to Google Search's AI Overviews. The decision was met with immediate and harsh criticism from Wikipedia editors, who called the feature a "ghastly idea" and a "PR hype stunt." They criticized a perceived loss of trust in the site due to AI's tendency to hallucinate and questioned the necessity of the feature. The negative criticism led Wikimedia to halt the rollout of Simple Article Summaries while hinting that they are still interested in how generative AI could be integrated into Wikipedia. Using Wikipedia for artificial intelligence In the development of the Google's Perspective API that identifies toxic comments in online forums, a dataset containing hundreds of thousands of Wikipedia talk page comments with human-labelled toxicity levels was used. Subsets of the Wikipedia corpus are considered the largest well-curated data sets available for AI training. A 2012 paper reported that more than 1,000 academic articles, including those using artificial intelligence, examine Wikipedia, reuse information from Wikipedia, use technical extensions linked to Wikipedia, or research communication about Wikipedia. A 2017 paper described Wikipedia as the mother lode for human-generated text available for machine learning. A 2016 research project called "One Hundred Year Study on Artificial Intelligence" named Wikipedia as a key early project for understanding the interplay between artificial intelligence applications and human engagement. There is a concern about the lack of attribution to Wikipedia articles in large-language models like ChatGPT. While Wiki