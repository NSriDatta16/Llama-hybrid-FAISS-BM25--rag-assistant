[site]: crossvalidated
[post_id]: 605425
[parent_id]: 
[tags]: 
Fewer parameters in dense layer using matrix factorization

Yesterday I was wondering how I "solve" the problem that dense layers in neural networks have a ton of parameters. So, my first idea was to use low rank matrices, which means that I can decompose the matrix without many problems. Therefore, today, I went and wrote some TF code to test it in MNIST Digits, by implementing the following layer: $$ l(i) = \sigma((A @ B)@i + c) $$ where $A,B$ are the factorization of the $W$ matrix, in my case, the first layer that receives the 784 pixels, $A$ is $784x1$ and $B$ is $1x100$ (example), thus their product produces a matrix that is $784x100$ like a dense layer, just that the matrix is low rank. Now, I'm open to criticism wrt this methods, because to me seems almost like building "linear autoencoders" at each step, which takes the input, squash it to 1 dimension, and then project it back to N dimensions, but I'm not good enough in linear algebra to check this However, I'm curious why this works, but have very strage properties as far as generalization. For example, this is one loss (train blue, val orange) history of a network: and about the accuracy, is almost as noisy as this... any idea? it seems that it overfits immediately
