[site]: crossvalidated
[post_id]: 479838
[parent_id]: 
[tags]: 
Visual attention on images in LSTM neural network - how to implement?

I have developed a bi-directional LSTM on images, and I want to get some kind of visual feedback on how the neural network is working, something that I think is called visual attention . I want to achieve something like the following image: i.e. some kind of focus attention on the image itself. The image is just for reference, I want anything that helps identify the focus of the LSTM, even a red square on the pixels of the image or whatever. I have found different papers ( paper1 , article1 , article2 and linked papers ) that explain what is going on under the hood but this doesn't help me in actually implementing it. I'm working on TensorFlow + Keras. There is some kind of repo, tutorial, even questions (I haven't found answered questions on this topic that explain implementation) that could help me? I'm discarding the implementation on text analysis, because I'm working with images, so the input to the model and everything else is too much different for me to start understanding how to do it. I got a suggestion telling me that a Jacobian could help me achieve something like that, but I don't know where to apply that operation and how to get results. It should correspond to the input derivative pointed in paper1 . References to papers and articles: (Paper1) In Techniques for visualizing LSTMs applied to electrocardiograms (2018) (3rd paragraph), are cited different visualization techniques for LSTM: Temporal output score, input derivatives, occlusion, using an input mask. They proceed to test methods on MNIST database, and the result is a focus over the border of the digits, but then the paper focuses on other things and there is very little for someone who is approaching to NN like me and needs code examples (ofc, it's just a paper). (Article1) In Visual Attention Model in Deep Learning are considered 2 databases and 2 models: CNN and LSTM. It's presented the Glimpse Sensor and is applied on RNN. Even if there is a Github Repo linked, it's still difficult to extract the methodologies and to apply them on a generic LSTM model. (Article2) The article Attention in Long Short-Term Memory Recurrent Neural Networks is an exposure of methodologies. The last update to the article is August 2019 and it's stated something that I had guessed/feared, i. e. Keras does not have (yet) some function or library that helps applying attention mechanism.
