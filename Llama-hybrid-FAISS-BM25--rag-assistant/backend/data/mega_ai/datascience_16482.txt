[site]: datascience
[post_id]: 16482
[parent_id]: 16480
[tags]: 
Define $Z_i = 1[Y_i>0]$, i.e., $Z_i = 1$ if $Y_i > 0$, else $Z_i = -1$. Now the data set $(X_i,Z_i)$ defines a two-class boolean classification problem. The solution to that boolean classification problem with highest accuracy solves your original problem (minimizes your loss function). So, you are basically asking how to train a neural network to solve a two-class boolean classification problem. A reasonable approach is to put a softmax layer at the output of the neural network and train it using the cross-entropy loss, as usual. During test time, to pick a class, you pick whichever of the two classes has the higher likelihood output from the softmax stage. In other words, at training time, you look at the actual outputs (continuous-valued) from the softmax layer and use the cross-entropy; at test time, you compare the two outputs and pick whichever is larger to choose the classification (and apart from that ignore their exact values). The shortcoming of this approach is that it doesn't take into account the value of the $Y_i$'s, only their sign. To improve this, use weights on the samples. We're again going to train a boolean classifier on the training set $(X_i,Z_i)$, but this time we'll weight each sample in the training set differently. Put the weight $Y_i$ on the sample $(X_i,Z_i)$ in your training set. In other words, when training the boolean classifier, the loss function will be a weighted sum of the loss for each sample: $$\text{Loss}(\theta) = \sum_i Y_i \cdot \ell(f_\theta(X_i),Z_i)$$ where $\theta$ are the model parameters, $f_\theta(X_i)$ is the output of the classifier on input $X_i$, and $\ell(\cdot,\cdot)$ is the cross-entropy loss. Notice that errors in samples where $Y_i$ is large are penalized more, and errors in samples where $Y_i$ is small are penalized less; this is appropriate, as that's exactly what will happen when you use the classifier $f_\theta$ to compute $L(f_\theta)$. This should improve on the approach you're taking.
