[site]: crossvalidated
[post_id]: 277691
[parent_id]: 277642
[tags]: 
"Averaging results" won't work on small samples in general. Typically MLEs are asymptotically normally distributed, so in very large samples, each estimate based on independent subsets of equal size will be approximately normal with the same mean and variance -- and then you might reasonably average them. A warning : This sort of scheme must be done with care. Consider a biased estimator (outside a few nice cases MLEs are typically biased, but consistent). If you have a large sample of size $N$ (say), the bias might be $O(1/N)$ (as an example consider the MLE for the variance of a normally distributed sample). But if you split your data up into $k=N/m$ samples of size $m$, your bias in each would then be $O(1/m)$ and this will not reduce when you average $k$ of them - the bias will remain the same. So as your sample size grows, you can't just throw more and more processors at the calculation (i.e. holding $m$ constant but increasing $k$) and hope that everything is fine ... eventually the bias will dominate the mean square error and there'll no longer be much value in obtaining more data. You might put up with splitting in 4 and tolerating 4 times the bias and maybe in some situations, splitting into ten pieces and having 10 times the bias ... but would you be willing to split into a thousand pieces? Often, probably not. In MSE terms if the squared bias is small compared to the variance, such a split-and-average scheme will be useful, but only up to a point; some degree of assessment of the value of continuing to split might be made by considering the effect of changing the value of $m$ one where the estimate tends to be.
