[site]: crossvalidated
[post_id]: 327891
[parent_id]: 327850
[tags]: 
Translation whose model scales linearly with the number of languages is an open research area. A complication is that modern translation systems use attention heads, which couples the language representations more tightly perhaps. Bengio's team has created a system whose parameters scale linearly with the number of languages: https://arxiv.org/pdf/1601.01073.pdf Multi-way, multi-lingual neural machine translation with a shared attention system Firat, Cho, Bengio, 2016 Abstract "We propose multi-way, multilingual neural machine translation. The proposed approach enables a single neural translation model to translate between multiple languages, with a number of parameters that grows only lin- early with the number of languages. This is made possible by having a single attention mechanism that is shared across all language pairs. We train the proposed multiway, multilingual model on ten language pairs from WMTâ€™15 simultaneously and observe clear performance improvements over models trained on only one language pair. In particular, we observe that the proposed model significantly improves the translation quality of low-resource language pairs."
