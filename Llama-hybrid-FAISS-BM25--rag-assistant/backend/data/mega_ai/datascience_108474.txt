[site]: datascience
[post_id]: 108474
[parent_id]: 
[tags]: 
Should I use GridSearch CV for hyper-parameter tuning in a data-rich context?

My textbook states that k-fold cross-validation is a resampling technique that is useful for estimating generalization error in a data-poor setting. Ideally, if we had enough data, we would set aside a validation set and use it to assess the performance of our prediction model. Since data are often scarce, this is usually not possible. To finesse the problem, K-fold cross- validation uses part of the available data to fit the model, and a different part to test it. ( Hastie, The Elements of Statistical Learning, Section 7.10, page 241. ) For a machine learning project, I used Scikit-learn's grid-search cv method to find the optimal hyper-parameters for my random forest. The Scikit-learn docs recommend exactly this : It is possible and recommended to search the hyper-parameter space for the best cross validation score. ... Two generic approaches to parameter search are provided in scikit-learn: for given values, GridSearchCV exhaustively considers all parameter combinations. However, my professor disagreed: he asked me why I used cross-validation in a data-rich context (Fashion-MNIST dataset). Was grid-search CV inappropriate in a data-rich problem? I don't know how to resolve this discrepancy, and my professor's feedback was not particularly instructive.
