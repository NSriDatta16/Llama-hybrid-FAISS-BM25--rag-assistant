[site]: crossvalidated
[post_id]: 346561
[parent_id]: 
[tags]: 
Inference while controlling False Discovery rates given known dependence between test statistics

I have a number $n$ of standard normal test statistics $\boldsymbol{\beta}$, each of which belonging to a hypothesis I want to test. So under H$_0$ $$\boldsymbol{\beta} \sim N(\mathbf{0}, \boldsymbol{\Sigma})$$ where $\boldsymbol{\Sigma}$ has been estimated using maximum likelihood theory. How can I test all $n$ hypotheses separately, while controlling the false discovery rate but maintaining maximal power by leveraging my knowledge of $\boldsymbol{\Sigma}$? I've been looking through the literature but can't quite seem to find what I want. What I do not want is: A chi-squared omnibus test A very general correction on the p-values that is too conservative A procedure that assumes $\boldsymbol{\Sigma}$ unknown A procedure based on dependence in the data
