[site]: datascience
[post_id]: 60919
[parent_id]: 51183
[tags]: 
A RNN is supposed to be able to accumulate information across the sequence, as you suggest. Each time it observes a new token, it combines that token with its previous hidden state. It then incorporates information about that token into the hidden state and produces a new hidden state. The idea is that the hidden state summarizes information about all tokens seen so far. The reality, however, is a bias toward recent tokens. LSTMs ameliorate that problem by modeling how much information in the hidden state to keep at each step. Nonetheless they will tend to lose information over long stretches. Note that many applications of RNNs don't just use RNNs, but use RNNs followed by an attention mechanism. The attention sees all hidden states that the RNN produces. It can then "look back" at any hidden state, allowing information about the sequence to be used even if it isn't retained in the RNN's final hidden state.
