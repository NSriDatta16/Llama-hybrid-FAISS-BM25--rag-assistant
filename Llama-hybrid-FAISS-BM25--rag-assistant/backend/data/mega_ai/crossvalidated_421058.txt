[site]: crossvalidated
[post_id]: 421058
[parent_id]: 
[tags]: 
Is this a well-known source of small-sample bias?

Suppose we have an outcome $Y$ which is caused by a binary treatment $T$ and some continuous covariate $X$ , such that: $$E[Y\mid X,T=1] = E[Y^1\mid X] = \beta_0 + \delta + \beta_X X$$ $$E[Y\mid X,T=0] = E[Y^0\mid X] = \beta_0 + \beta_X X$$ where $\beta_0$ is an intercept and $\delta$ is the average treatment effect ( $ATE$ ). Suppose also that we have a sample of two units: A control unit ( $T=0$ ) with $X=5$ A treated unit ( $T=1$ ) with $X=5.5$ Ignoring for simplicity any noise or sampling variance, if we were to estimate the $ATE$ as the difference between the outcomes of those two units, our estimate $\widehat{ATE}$ would be biased: Here the true $ATE$ is the vertical distance between the parallel regression planes, but our $\widehat{ATE}$ is overestimating. Importantly, this bias is present whether or not $X$ is a confounder (i.e. whether or not $X$ also has an effect on $T$ ). What I just noticed is that this bias becomes bigger if the effect of $X$ on $Y$ is stronger (i.e. if the regression planes are steeper): In the language of matching, it seems that the biasing effect of having an imperfect match is stronger when the slope of $X$ is larger, even if $X$ is not a confounder. I realize this bias will vanish in larger samples because the mismatches will average out, but I think it will vanish slower if the slopes are steep and, at least in my work, it could be confused for confounding bias in small samples. Is this a thing?
