[site]: crossvalidated
[post_id]: 175877
[parent_id]: 175860
[tags]: 
This really depends on what kind of a model you have. In frequentist statistics you can use the $l_1$ norm on the parameters to perform regularization in multiple regression which also does variable selection. The effective number of parameters is well approximated by the number of non-zero parameters in the model. If you use the $l_2$ norm you do not truncate the parameters in the model completely to zero, but the effective number of parameters goes down. So in a sense the model adapts to the data and chooses the number of parameters for you! But that is not all the truth. The function that you are optimizing is a weighted combination of the residual sum of squares and the regularization term. So you need to tune the regularization parameter. There are ways to make this automatic, e.g. cross-validation. In a Bayesian setting this is done by incorporating priors and/or creating hiearchical models where the conditional dependence between layers reduces the effective number of parameters. I.e. by adding these extra assumptions or creating your model in this way you create dependence between parameters that reduces the effective number of them. This removes flexibility from your model and is done to account for overfitting among other things. In a Bayesian setting you can also have something called PC priors in your model. PC stands for penalizing complexity. So when you run you Gibbs-sampler the model adapts to the data and "selects" if a parameters should be included in the model or not. This is still relatively new and it is quite hard to construct these priors. So to summarize: Regularization is used to control the effective number of parameters in your model, depending on the norm on the parameters, you either shrink the total length of the parameter vector or you also remove some of the variable from the model.
