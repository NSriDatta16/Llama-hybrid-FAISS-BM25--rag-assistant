[site]: crossvalidated
[post_id]: 488365
[parent_id]: 
[tags]: 
Intuition behind projection matrix

I'm new to machine learning and came across projection matrix . In a random thread it was interpreted as The matrix $X(X^\text{T} X)^{-1} X^\text{T}$ is a projection matrix, as it does precisely that: it is the linear transformation that orthogonally projects a vector onto the span of the vectors comprising $X$ . I'm having hard time to understand the underlying concept behind this and how is this related to linear regression .Some intuition and use cases in machine learning and deep learning would help. And Ik taking inverse is undoing the process but what does this means here?
