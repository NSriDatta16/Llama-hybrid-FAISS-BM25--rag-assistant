[site]: crossvalidated
[post_id]: 226348
[parent_id]: 226337
[tags]: 
You could try some of these approaches: A GLM consists of 3 components - the conditional distribution of the response variable given values of the independent variables, the linear predictor function, and the link function translating the linear predictor to the expected response variable. You've used the default family - Gaussian with default identity link function, this is the same as a multiple linear regression. Check if this is suitable for your target variable mass.loss.a.pct . Evaluate if other alternative family/link functions would be better suited to this dataset. You haven't mentioned the fitted model's null deviance as compared to the fit deviance, that's a good indicator to examine. The leverage plot looks somewhat worrying, it appears you have too many high leverage points which can result in a poor fit. Try using a validation set to see how well the model performs on held-out samples. Since this is a regression model, evaluate the root mean squared error (RMSE), which is the square root of the mean squared error. Since this dataset shows variables with interaction are significant to explain the response variable, you may be able to use regression trees. Try fitting an rpart model on this dataset. Specific answers to your queries are: Meaning of intercept: Yes, you're right, the intercept is supposed to give the expected value of the response when all other variables are 0. If this if too different from the mean you've observed when all variables are 0 then this might imply a poor fit. For this I would recommend checking the deviance and results over held-out validation set. You cant change the intercept, its supposed to balance the equation and the value is obtained by best fitting it to the data. In case the intercept is extremely odd, you could either identify and remove outliers with high leverage. Or, alternatively use regularization to diminish its effects - use glmnet for including regularization in fitting the model. Since these are nominal variables, GLM expands each of these into multiple dummy variables. So, the number of predictors grows to as many as the number of levels of the variable. With interaction effects, these multiply to an even greater number of variables. Your final model is mass.loss.a.pct ~ origin + species + origin:species + 1 . So the number of actual variables used by GLM = Number of unique levels of origin + number of unique levels of species + (number of levels of origins x number of levels of species). Think of each of these dummy variables along with non-dummy variables as a dimension, so if there are 118 dimensions, you're trying to fit a surface through this space. If for any of these dimensions you don't have sufficient values to fit this surface, these coefficients will show up as NA. Also, refer to this help page for guidance on GLM: http://stat.ethz.ch/R-manual/R-patched/library/stats/html/glm.html
