[site]: crossvalidated
[post_id]: 101005
[parent_id]: 101003
[tags]: 
A first approach is to use PCA in order to reduce the dimensionality of the dataset. Try to retain ~97% of the total variance, this may help out quite a bit. Another option is to use something like stochastic gradient descent, this can be a much faster algorithm and able to fit into R's memory. EDIT: One problem with R is that you can only use your RAM so if you only have 8 GB of memory then that is what you are limited to. I have run into many problems with this and have since moved onto using python's scikit-learn which seems to handle bigger datasets much better. A very nice chart which gives some idea of places to start based on your dataset size can be found here: http://3.bp.blogspot.com/-dofu6J0sZ8o/UrctKb69QdI/AAAAAAAADfg/79ewPecn5XU/s1600/scikit-learn-flow-chart.jpg
