[site]: crossvalidated
[post_id]: 100327
[parent_id]: 100282
[tags]: 
I would advise you to have a look at this article [0]. The problem it purports to address seems to fit your description of yours rather well, except that the method proposed by the author is slightly more refined than NN-inputation (although it uses something similar as a starting point). (throughout, I will assume that $\pmb X$, the $n$ by $p$ data matrix has been standardized: each column has been divide by the mad in the pre-processing step of the analysis) The idea of this approach is to obtain a rank $k$ robust PCA decomposition of your data matrix in a way that is resistant to the possible presence of outliers (this is done by using a bounded loss function when estimating the PCA components) and missing values (this is done by using an EM-type imputation method). As I explain below, once you have such a PCA decomposition of your dataset, filling in the missing elements (and assessing the uncertainty around these estimates is pretty straightforward). The first step of each iteration is the data imputation step. This is done as in the EM algorithm: the missing cells are filled by the value which they are expected to have (this is the E-step). In the second part of the two step iterative procedure, one fits a (robust) PCA to the augmented data obtained from the previous step. This results in a spectral decomposition of $\pmb X$ into $\pmb t\in\mathbb{R}^p$ (an estimate of center), a $p$ by $k$ orthogonal matrix $\pmb L$ and a $k$ by $k$ diagonal matrix $\pmb D$ (with $k\leq p$), which is a sort of robustified, PCA based, M-step. To summarize the paper, here is the general algorithm they propose: Set $l=0$. Obtain an estimate $\pmb W^0$ where the missing elements are filled with the initial estimates. For each missing cell, these initial estimates are the averages of the row-wise and the column-wise medians of the non missing elements of $\pmb X$ (the original data matrix). Then, do until convergence: a. do robust PCA on $\pmb W^l$ and obtain the estimates $(\pmb t^l,\pmb L^l,\pmb D^l)$ b. set $l=l+1$ c. use $\pmb Y^{l}=\pmb L^{l-1}(\pmb W^{l-1}-\pmb t^{l-1}) (\pmb L^{l-1})'$ d. fill the missing elements of $\pmb W^{l}$ by what they are expected to be based on the model $\pmb W^{l}\sim\mathcal{N}(\pmb t^{l-1},\pmb L^{l-1} \pmb D^{l-1}(\pmb L^{l-1})')$ (as in the E step of the EM algorithm) and the non missing elements by the corresponding entries of $\pmb Y^{l}$. Iterate (a->c) until $||\pmb W^{l-1}-\pmb W^l||_F$ is smaller than some threshold. The vector of estimated parameter obtained at the final iteration are stored as $(\pmb t,\pmb L,\pmb D)$. The idea, is that at each iteration, the model for the data $(\pmb t^{l-1},\pmb L^{l-1} \pmb D^{l-1})$ moves increasingly further away from the naive, initial estimates while the robust M step prevents the outliers from influencing the fitted parameter. This approach also gives you a host of diagnostic tool to check the quality of the imputation. For example, you could also produce multiple draws from $\mathcal{N}(\pmb t^{l-1},\pmb L\pmb D(\pmb L)')$ but this time for the non missing elements of your data-matrix and see how much the distribution of the generated (counter-factual) data matches the observed value for each of the non missing cells. I don't know of a ready made R implementation for this approach, but one can easily be produced from the sub-components (chiefly a robust PCA algorithm), and these are well implemented in R, see the rrcov package (the paper is quiet informative on this subject). [0] Serneels S. and Verdonck, T. (2008). Principal component analysis for data containing outliers and missing elements. Computational Statistics & Data Analysis vol:52 issue:3 pages:1712-1727.
