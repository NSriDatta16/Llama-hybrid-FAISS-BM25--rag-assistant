[site]: crossvalidated
[post_id]: 199076
[parent_id]: 
[tags]: 
Is it possible that, for the smaller value of the cost parameter, the margin become small, while for the larger cost, the margin become larger?

I have written an R code for soft margin svm using the ipop function of kernlab package. Despite it's working fine, but I still have some doubt whether this code working properly or not. According to the literature large value of cost parameter leads to less classification error and smaller margin and small value of cost leads to bigger margin and more classification error. But this property is not satisfy in my function. For example set.seed(72), the margin for cost=100, 1000 are larger than the cost=1, while it's supposed to be narrower. Any help would be appreciated. softmargin=function(x,y,cost,x.test,y.test) { library(kernlab) #----------------------------------------------------# # Optimization uing ipop() function of kernlab package #----------------------------------------------------# n 1e-6) b = mean(y[pos]-fout[pos]) #b fx=t(w %*% t(as.matrix(x))) + b # the classifiying function f(x)=wx+b fx.test=t(w %*% t(as.matrix(x.test))) + b # the predicted classifying function for the test set data #-----------------------------# # SVM line & support vectors #-----------------------------# plot(x,pch=ifelse(y==1, 1, 3),col=ifelse(y==1, 1, 2),main="Soft margin SVM") abline(-b/w[1,2], -w[1,1]/w[1,2], col="black", lty=1) abline(-(b+1)/w[1,2],-w[1,1]/w[1,2], col="orange", lty=3) abline(-(b-1)/w[1,2], -w[1,1]/w[1,2], col="orange", lty=3) points(x[pos,],col="blue",cex=2) # show the support vectors #--------------------------# # Computation of ACCURACY #--------------------------# pred 0){C[i]=1} else {C[i]=-1} } accuracy=mean(C==lable) result=list(C,accuracy) return(accuracy) } pred(fx.test,y.test) } #---------------------# # DATA # #---------------------# set.seed(72 )#118 train set groupP
