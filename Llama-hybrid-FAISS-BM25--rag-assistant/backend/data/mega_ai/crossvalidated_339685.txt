[site]: crossvalidated
[post_id]: 339685
[parent_id]: 
[tags]: 
Using constrained regression model to get closer to the true posterior when doing Approximate Bayesian Computation

I'm using rejection sampling algorithm to generate a reference table ($\theta$,SS). Where $\theta$ are parameter values of model M1 and SS the summary statistics extracted from the pseudo-data generated under model M1. The SS are used to compute the distance between the observations and pseudo-data. To get closer to the true posterior I'm using multiple linear regression model as proposed by Beaumont et al 2002. Among the paramter values that I want to correct using the regression model I have the constraints that some of the parameters (e.g., $\theta_1$,$\theta_2$,$\theta_3$ and $\theta_4$) sum to 1. In that particular context (getting closer to the true posterior), the explicative variables are the SS and the response variables are the paramters of M1 (i.e., $\theta$). In the next example M1 gets 4 parameters that sums to 1 $\theta_i = b_i+x_{ij}SS_j + .. +x_{iN}SS_N$, where $0 Any idea how elaborate a regression model in a way that the regression model parameters are fitted under the contraints that $\sum_1^4{\theta_i} =1$. The term constrained regression seem to be usually used to define regression with constraints on the coefficients ($x_{ij}$). It seems that there is a missing part to the mulitple regression, where the constraints for $\theta_i$ to sum to 1 could be added. I'm not familiar with neural network, could it be usefull?
