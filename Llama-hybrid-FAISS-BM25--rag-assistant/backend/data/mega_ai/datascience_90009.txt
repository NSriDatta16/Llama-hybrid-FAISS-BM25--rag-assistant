[site]: datascience
[post_id]: 90009
[parent_id]: 
[tags]: 
Is there any problem with the following Python+TF+Keras code for a custom loss function and network?

I am trying to code a custom loss function for variational autoencoder. I am not using mse for reconstruction loss since I am not learning p(x|z) ~ N(mu,I) . Instead of that, I want to learn the sigma as well. So p(x|z)~N(mu,sigma**2I) . If I consider this, then my reconstruction loss function part changes to include additional items: input_dim*ln(2*pi) and (x-mu)**2/(sigma)**2 along with the KL divergence term. (x-mu)**2/(sigma)**2 is equivalent to 0.5*input_dim*mse(input,output)/sigma**2 . My question is, with this modification, is my code valid. def loss(inputs,outputs): # Note log_var = ln(sigma**2) from tensorflow.keras import backend as K log_var = K.Variable(0.0) reconn_loss = 0.5*input_dim*(mse(inputs,outputs)/K.exp(log_var)+log_var+ np.log(2*np.pi)) #kl loss kl_loss = -0.5*K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1) vae_loss = K.sum(reconn_loss + kl_loss) return vae_loss Now the actual encoder-decoder function. Note, since I want to output mu, and sigma from decoder as well, it's architecture is similar to the encoder's. def VAEmodel(params): input_dim = params['input_dim'] latent_dim = 2 intermediate_dim = 32 intermediate_dim_2 = 16 # extra added Delete later # encoder inputs = Input(shape=(input_dim,),name='encoder_input') x = Dense(intermediate_dim, activation='relu')(inputs) x = Dense(intermediate_dim_2, activation='relu')(x) z_mean = Dense(latent_dim, name='z_mean')(x) z_log_var = Dense(latent_dim, name='z_log_var')(x) # reparametrization trick to sample latent code (z) from Q(z|x) z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var]) # instantiate encoder model encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder') #encoder.summary() # decoder latent_inputs = Input(shape=(latent_dim,), name='z_sampling') x = Dense(intermediate_dim_2, activation='relu')(latent_inputs) # Extra layer added,Delete later x = Dense(intermediate_dim, activation='relu')(x) outputs_mean = Dense(input_dim, activation='linear',name='output_mean')(x) outputs_var = Dense(input_dim, activation='linear',name='output_var')(x) # instantiate decoder model decoder = Model(latent_inputs, [outputs_mean,outputs_var], name='decoder') #decoder.summary() # instantiate vae model outputs = decoder(encoder(inputs)[2]) vae = Model(inputs, outputs, name='vae_mlp') #get the loss function vaeloss = loss(inputs,outputs) #compile vae.compile(optimizer= 'adam',loss=vaeloss) return vae,encoder,decoder,outputs Note, I can get the mu, and sigma from outputs[0], and outputs[1] respectively. Is this set up going to work the way I want i,e learn sigma of p(x|z)? Thank you very much. Your inputs will be greatly appreciated.
