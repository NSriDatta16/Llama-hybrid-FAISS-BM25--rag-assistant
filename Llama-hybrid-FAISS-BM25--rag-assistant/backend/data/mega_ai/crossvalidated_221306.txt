[site]: crossvalidated
[post_id]: 221306
[parent_id]: 
[tags]: 
Distribution of the sum of the two dependent bivariate gaussian distributions and related questions

This is something I was thinking about and I decided to modify a question from a mid-term to ask this. Suppose $X_{1}$ and $X_{2}$ are two bivariate gaussian variables, decribed as $$ X_{i}=\begin{bmatrix} \mu_{1}+y\\ \mu_{2}+w\\ \end{bmatrix} $$ where $y$ and $w$ are iid random variables each with mean $0$ and variance $1$. $\mu_{1}$ and $\mu_{2}$ are unknown parameters. The pdf of $X_{i}$ is $N(\mu,\Sigma)$, where $$\mu=\begin{bmatrix} \mu_{1}\\ \mu_{2}\\ \end{bmatrix} $$ and $$\Sigma=\begin{bmatrix} 1&0\\ 0&1\\ \end{bmatrix} $$ So, finding the joint pdf of $X_{1}$ and $X_{2}$, if they are independent, is easy enough, and may be generalised to n iid variables. Firstly, what would the pdf be $X_{1}$ if y and w weren't iid? I should simply replace the off-diagonal entries of $\Sigma$ with the co-variance, correct? Now, suppose $X_{1}$ and $X_{2}$ aren't iid. What would the pdf of $Z=X_{1}+X_{2}$ be (assuming that this sum is also gaussian)? I have read around on stackexchange and there is an answer to this question if $X_{1}$ and $X_{2}$ were univariate, but I don't necessarily see how the maths would work out for my case. Now, I also want to ask how I can put this in the context of a machine learning problem. Does $X_{i}$ being a vector mean that each member of the vector is a feature of the sample $X_{i}$? When are features of a sample dependent? What would dependent $X_{i}$ physically mean, then? Can I model a system which collects samples as this: "If the first sample $X_{1}$, modelled as a Gaussian random variable, is less than 5000, then $X_{2}$ is obtained iid. Otherwise, the mean of $X_{2}$ shifts by an amount $a$." I hope that this question isn't too vague or worded confusingly.
