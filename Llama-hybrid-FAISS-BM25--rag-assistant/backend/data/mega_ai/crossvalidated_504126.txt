[site]: crossvalidated
[post_id]: 504126
[parent_id]: 504118
[tags]: 
Teacher forcing effectively means that instead of using the predictions of your neural network at time step t (i.e the output of your RNN), you are using the ground truth. "Why does teacher forcing speed up training?" Because if you don't use teacher forcing, it is autoregressive, meaning that you need to calculate the labels first before passing it at time step t. "Also why in the Kaggle link are they only doing teacher forcing a percentage of the time?" Because conditioning on the actual predictions might be more beneficial. Suppose that your RNN is unable to learn the input-output mapping to the desired precision. In that case, it is better to condition on its own faulty output so that it has a better chance of mitigating its own mistakes. Related question: Is teacher forcing more accurate than using actual model output or just faster?
