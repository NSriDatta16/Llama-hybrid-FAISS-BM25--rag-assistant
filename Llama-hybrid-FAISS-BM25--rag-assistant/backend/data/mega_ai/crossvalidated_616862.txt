[site]: crossvalidated
[post_id]: 616862
[parent_id]: 
[tags]: 
Sampling from Gaussian Process

I am learning the Gaussian process and feel confused about how three lines were generated in Fig 2.2(A) in the book "Gaussian Process For Machine Learning". As described by the author: "To see this, we can draw samples from the distribution of functions evaluated at any number of points; in detail, we choose a number of input points, $X_âˆ—$ , and write out the corresponding covariance matrix using eq. (2.16) elementwise. Then we generate a random Gaussian vector with this covariance matrix". $$ \operatorname{cov}\big(f(\mathbf{x}_p), f(\mathbf{x}_q) \big) = k(\mathbf{x}_p, \mathbf{x}_q) = \exp\big(-\tfrac{1}{2} |\mathbf{x}_p- \mathbf{x}_q|^2 \big) \tag{2.16} $$ What does it mean by drawing samples from the distribution of functions? Does this function refer to $f(x)=W^Tx$ in the linear Bayesian regression, and we are sampling from prior of p(w)? But if using 2.16 and assuming the mean is zero, we don't need this distribution, isn't it? As shown in Fig-2.2(A), there are three samples corresponding to 3 lines, how each line was generated? Are they the "Gaussian vector" described in the text, in other words, at each time, a vector containing all f(x) values on selected x is generated and that is a line, and then generate another vector using the same joint distribution? Thank you
