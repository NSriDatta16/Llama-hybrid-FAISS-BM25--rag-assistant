[site]: crossvalidated
[post_id]: 616487
[parent_id]: 
[tags]: 
Binary and Censored Data: What are the advantages and disadvantages of considering binary data as a completely censored random variable?

(Full disclosure: I'm trained as a physicist, so maybe the answer to this question exists and I just need to be pointed in the right direction). I have been working with a material properties data set for several years now. The data comes in the form pass/fail binary data. A stimulus of varying (numerical) degrees is applied to the material, and the material either passes or fails. The stimulus is numerically continuous, i.e. There is some sampling scheme that is used to explore the stimulus space which purports to do a pretty good job of sampling the relevant stimulus space. For technical reasons, the pass/fail test can only be conducted once per sample, and only a single stimulus can be applied to a given sample. (I think what I've just described is a pretty general problem...) Historically, the data set has been analyzed using a binary probit analysis. Admittedly, I don't know exactly how the algorithm is conducting the fit (probably MLE), but it is, of course, some how fitting the equation: $\theta^{-1}(y;0,0)=mx+b$ , where $\theta$ is the normal cdf. One can (and we have reason to) convert m/b to $\mu/\sigma$ in order to recover the typical parameterization of the normal distribution. One could (and I increasingly have) plugged this reparameterization into the equation for a normal pdf and create visualizations that could be interpreted as the distribution of failure thresholds for the material. Since I've been working with this dataset, I've learned about censoring. It has occurred to me that under the assumption that there is some hidden threshold random variable that the pass/fail stimulus test is probing, one can view the pass/fail as completely censored data on a continuous random variable. A pass tells you that failure threshold for that particular sample occurred at a stimulus somewhere to the right, and a fail tells you that the failure threshold for that particular sample occurred at a stimulus somewhere to the left. I've found some algorithms that will fit left/right/interval censored data to a normal pdf and tried fitting these data sets using those algorithms. Perhaps unsurprisingly, I get more or less the same answer as the probit fits. (Maybe there is some theorem somewhere that says there is a mathematical equivalence or something like that...) As the title to this question implies, are there any known advantages or disadvantages to viewing the data this way? I guess I should include that my reasoning for even thinking is this direction is two-fold: (1) intuition of the meaning and (2) injecting more information into the analysis. For number (1), as I mentioned at the top, I'm trained as a physicist, so I'm used to dealing with measurements on continuous variables. It seems much easier to recognize changes in the behavior of the material by looking at pdfs, whose mode or width can very easily be visually assessed for a change, rather than cdfs or linearized probits to try to assess if two measurements represent the same behavior. Using the censored continuous variable ontology motivates and in some sense justifies this visualization beyond just a mathematical trick. For number (2), although I expressed pass/fail as right/left censored data, I think one might be able to leverage techniques for interval censored data to potentially inject more information into the analysis. Although properly speaking a pass only tells me that the fail threshold was to the right (i.e. somewhere between the applied stimulus and positive infinity), I can assert that there is some reasonable stimulus beyond which the material definitely would have failed. This is, of course, an assumption, a guess. But one can perhaps reasonably say that although the exact failure threshold is unknown, it is probably less than some finite boundary. In the case of the failure I'm analyzing, one can probably set that boundary relatively close to stimulus at which the measurement is taking place. I've looked around for any discussion on treating binary data as censored data, and haven't found any. This leads me to believe that it is either a stupid idea on the face of it or that it hasn't been suggested. I'm really hoping it is not former.
