[site]: crossvalidated
[post_id]: 230462
[parent_id]: 
[tags]: 
How far is my average from the series values?

I'm sure there's a name for this concept, but I can't find it (I'm not familiar with statistical terms): Suppose I have a series of numbers, and an average of those. I'm wondering how correct this average is to predict future values. Say the average is 5, then if my series all consisted of fives, I would say that the next number in the series has a high probability of being five. But the series could just as well have been half 10:s and half 0:s (still giving an average of 5), and that would give a low probability of the next number in the series being 5. How do I calculate this, or express the distance of my series to the average? int[] series = [5, 5, 5, 5, 5, 5]; //avg is 5, series is close to avg int[] series = [10, 10, 10, 0, 0, 0] //avg is 5, series is far from avg A calculation like this would give some indication of how much the series values differs from the average. The blue box is what I'm looking to calculate or find a name for (it's the average difference expressed as a multiple of the average).
