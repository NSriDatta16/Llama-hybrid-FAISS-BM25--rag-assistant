[site]: crossvalidated
[post_id]: 485100
[parent_id]: 485083
[tags]: 
Feature importance or variable importance is a broad but very important concept in machine learning. Specifically, in terms of RF, your understanding is unfortunately problematic. First, random forest is a parallel ensemble method, you grow trees parallelly using bootstrapped data. That means, having more trees in your forest doesn't necessarily associate to a worse performance, on the contrary, it would usually reduce overfitting. With that said, you might want to do a solid cross validation procedure in order to assure the performances. Second, feature importance in random forest is usually calculated in two ways: impurity importance (mean decrease impurity) and permutation importance (mean decrease accuracy). The impurity importance of each variable is the sum of impurity decrease of all trees when it is selected to split a node. Permutation importance of a variable is the drop of test accuracy when its values are randomly permuted. It seems you interpret important features as having less trees but better performance (if not, you may need to clarify your question). As said before, larger number of trees in forest actually can be more beneficial. Lastly, feature importance is algorithm and data dependent, so it is suggestive. You need to understand how it is computed to actually use it in practice.
