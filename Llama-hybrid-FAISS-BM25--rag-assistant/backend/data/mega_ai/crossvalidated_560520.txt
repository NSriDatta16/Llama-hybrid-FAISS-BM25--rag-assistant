[site]: crossvalidated
[post_id]: 560520
[parent_id]: 
[tags]: 
Calculating the accuracy of an estimate when the true value is unkown

This is the context for the problem I've run into: A program starts counting down in seconds, choosing a random value with a min of 300 and a max of 600. At the end of the countdown event A occurs, a new countdown is chosen, and the cycle repeats. I can measure the amount of times event A occurs, but not the time between occurrences or the total runtime of the program. My goal is to estimate the total runtime and measure the accuracy of this estimate. With the absence of other variables I've assumed the average time between events to be 7.5 minutes. If I multiply this by the number of occurrences I believe I'll have a good estimate of the total runtime, but I'm unsure how I'd represent the estimate's accuracy. My first thought was to find a margin of error for the average rate of occurrence, then multiply it by the number of occurrences to provide a reasonable range for the original estimate's accuracy. This may not be the best method, and after a few hours looking online I've only gotten more confused. I'm aware accuracy will be depend on the number of occurrences, and I have a general understanding of concepts like normal distribution curves, standard distribution, and confidence intervals, but it’s been a few years since I’ve taken a statistics class and I am unsure how to apply these. Most of what I find online references population or something small like a coinflip. Edit: Changed minutes to seconds and revised the context to be clearer.
