[site]: crossvalidated
[post_id]: 57650
[parent_id]: 
[tags]: 
Simple cloud computing to run R + JAGS simulations

I want to simulate the frequentist properties of a Bayesian model. So, for example, I might want to fit a Bayesian model 1,000 times to 50 different configurations each of which takes about 10 seconds to fit on my machine. i.e., total computing time = 1000 * 50 * 10 / 60 second / 60 minutes / 24 hours = 5.7 days on my machine. Running this on my machine is not particularly practical. I could reduce some of the features of my simulation (e.g., fewer simulations per cell of the design; fewer cells in the design; shorter chains for each simulation). However, it would be nice if I could ship this simulation off to the cloud. I could then run it and collect the results when it was finished. The simulation is also highly parallel, such that it could potentially take advantage of using multiple computers to speed up the time it takes to run. Questions: How can I get a ballpark on the costs of such analyses? For example, if I see a figure that computing time costs 10 cents per hour on EC2, can I assume that the performance of the EC2 machine would be similar to a modern desktop machine. Are there any cloud computing systems particularly suited to running occasional R and JAGS simulation jobs? Are there any trade-offs between running the jobs in parallel on perhaps multiple instances versus running the program on just one instance? Are there any tutorials or examples of running R and JAGS in the cloud? Initial discoveries I found this post that provides some useful information about using JAGS on EC2.
