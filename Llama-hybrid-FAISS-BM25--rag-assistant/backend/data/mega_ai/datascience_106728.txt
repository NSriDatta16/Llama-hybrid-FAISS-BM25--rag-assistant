[site]: datascience
[post_id]: 106728
[parent_id]: 106720
[tags]: 
Allright, here's what I think: I would always recommend splitting the training set if the amount of data allows it. Your train/test split in the train data wil then be a train/validation split. The validation set will allow you to test your model for things such as overfitting on the training data. Even though you can get the ROC/AUC score for the test set predictions, you can't use this for tuning your model, as this would be data leakage. This question is a big part of the whole data science process, and too broad to be answered in this context. Try your best to try different configurations and see what works best! :) Also, high train set performance and low test set performance can never be due to overfitting, it is overfitting. Yes, see the train/validation split suggestion in part 1.
