[site]: crossvalidated
[post_id]: 374596
[parent_id]: 374588
[tags]: 
Decision Trees are pretty straight forward to understand. Take for example a famous problem where you have to label each passenger on the Titanic . For each person you have a bunch of info ( sex and age , for example), and the outcome after the disaster, whether they lived or not. The DT , tries to find the best pattern in order to correctly classify a person. This is almost like what you'd do, for example you may think (correctly) that women and children were the first to get into a safeboat, and so sex = Female and age , would be two pretty good first splits of the data. These are good because they let you discriminate well the overall observations (in alive or dead ), because there's a good portion of subjects that are either Female or Children that survided. The DT does this, but with some kind of measure of how good a variable splits the data, the variable that discriminate most is the first, and then it continues, building what looks like a tree. To answer your second question, almost every filed can have an application for DT , at least for more "advanced" types, called Random Forest or Boosting , all you have to know in layman terms is that both try to find the best way to classify observation by averaging a lot of trees. By this I mean that you have trained lots of trees on the same data, and you take for each observation the major label (if most of them said that one has lived the accident, then it probably is safe to say so). This can shed some lights on a lot of applications as I was saying, from anomaly transaction detection in Banks, Medical Diagnose, some Regression problem, and even Handwritten Image Recognition.
