[site]: crossvalidated
[post_id]: 581283
[parent_id]: 580710
[tags]: 
Caveats about the correct terminology aside, you have implemented Algorithm 8.2.3, pp. 364 in " Discrete-Event Simulation " incorrectly. The relevant line is this while predicate: while ((n w * sqrt(n - 1))) where w is the required margin of error (half-width of the confidence interval). In your implementation, you compare the current margin of error to (a function of) the current average: t * sqrt(currentSumVariance / (n - 1)) > c * currentAvg * sqrt(n) The details about the constants are not important (when do we use n -1 vs n ?) What matters is that you have currentSumVariance on the left side and currentAvg on the right side of the inequality. So both sides of the comparison are stochastic. currentAvg is the running sample mean, so effectively, the target precision varies from iteration to iteration, esp. at the start of the simulation. Most likely your simulations happened to exit the while loop too early due to the unintended randomness in the required precision / margin of error. You should keep w fixed. Aside : This method for computing the sample variance one step at a time, rather than with a big sum over the entire sample, is known as Welford accumulator . To verify, I coded the algorithm in python. (For reproducibility, I fixed the seed.) There are 13 outliers in 1,000 simulations; this agrees well with the significance level α = 0.01. [1] L. M. Leemis and S. K. Park. Discrete-Event Simulation: A First Course (2006) import numpy as np import scipy.stats as stats np.random.seed(seed=20220608) def rexp(rate): return stats.expon.rvs(scale=1 / rate, size=1)[0] def qt(p, df): return stats.t.ppf(1 - alpha / 2, df - 1) def t_margin_of_error(n, alpha, sigma2): return qt(1 - alpha / 2, n - 1) * np.sqrt(sigma2 / (n - 1)) def run_simulation(alpha, delta, rate, verbose = False): currentAvg = 0.0 # estimate currentSumVariance = 0.0 # residual sum of squares n = 0 while n delta: diff = rexp(rate) - currentAvg currentAvg += diff / (n + 1) currentSumVariance += diff * diff * n / (n + 1) n += 1 half_width = t_margin_of_error(n, alpha, currentSumVariance / n) if verbose: print(f"CI = {currentAvg:.3f} ± {half_width:.3f}") print(f"Precision = {half_width / currentAvg * 100:.2f}%") print(f"Iterations = {n}") print(f"Population mean = {1/rate:.3f}") print("===========") return np.abs(1 / rate - currentAvg) > half_width alpha = 0.01 delta = 0.3 rate = 0.5 outliers = 0 draws = 1000 for _ in range(draws): outliers += run_simulation(alpha, delta, rate) outliers, draws #> (13, 1000)
