[site]: crossvalidated
[post_id]: 187337
[parent_id]: 66757
[tags]: 
I notice that this is an old question, but I think more should be added. As @Manoel Galdino said in the comments, usually you are interested in predictions on unseen data. But this question is about performance on the training data and the question is why does random forest perform badly on the training data ? The answer highlights an interesting problem with bagged classifiers which has often caused me trouble: regression to the mean. The problem is that bagged classifiers like random forest, which are made by taking bootstrap samples from your data set, tend to perform badly in the extremes. Because there is not much data in the extremes, they tend to get smoothed out. In more detail, recall that a random forest for regression averages the predictions of a large number of classifiers. If you have a single point which is far from the others, many of the classifiers will not see it, and these will essentially be making an out-of-sample prediction, which might not be very good. In fact, these out-of-sample predictions will tend to pull the prediction for the data point towards the overall mean. If you use a single decision tree, you won't have the same problem with extreme values, but the fitted regression won't be very linear either. Here is an illustration in R. Some data is generated in which y is a perfect liner combination of five x variables. Then predictions are made with a linear model and a random forest. Then the values of y on the training data are plotted against the predictions. You can clearly see that random forest is doing badly in the extremes because data points with very large or very small values of y are rare. You will see the same pattern for predictions on unseen data when random forests are used for regression. I am not sure how to avoid it. The randomForest function in R has a crude bias correction option corr.bias which uses linear regression on the bias, but it doesn't really work. Suggestions are welcome! beta
