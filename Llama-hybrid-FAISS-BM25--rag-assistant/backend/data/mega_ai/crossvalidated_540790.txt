[site]: crossvalidated
[post_id]: 540790
[parent_id]: 
[tags]: 
Understanding the probability distributions behind a Monte Carlo experiment

A colleague and I are trying to model the expected maintenance cost/h ( E[C/h] ) of a component A on an aircraft over its life based on its reliability distribution. As the component fail, it's replaced by the same component but new and each component life span are independent from each others and follow the same distribution. A follow a Weibull distribution with 2 parameters: (beta = 2.1 and alpha = 2500). We know (and have observed) that both models converge to the same constant value that we call "Mature Cost / hour" (MCH). MCH = Cost / MTBF , but we are interested in the transition phase leading to that plateau. We took 2 different approaches, that yield 2 different results and we are trying to understand the differences. ----- APPROACH #1: ----- E[C/h(t)] = P(A fail where P(A fail The idea being, the 1st component comes for "free" with the aircraft (no maintenance). Once it has been removed at least once, and the full cost of the component has occurred, you can assume subsequent removals to occur on average every MTBF. -----APPROACH #2:------ Use a Monte Carlo simulation with a fleet of 10,000 components, all starting at the same time, and we count how many removals occurs in a specific interval of time T = (t; t+dt). E[C/h(T)] = Sum of removals in the interval(T) x Cost / (number of hours in the interval x 10,000). -----RESULTS----- The green curve is Approach 1 The blue curve is Approach 2 (each interval is 100 hours) and this represent the expected cost in this interval only. The orange curve is Approach 2 but this time it's the cumulative number of removals and hours since the beginning that we use. -----QUESTION----- Approach 2 (Monte Carlo) constantly over estimate the cost in the beginning compared to the Approach 1. My theory is, by simulating thousands of component with the same distribution a lot of the first removals will tend to occurs around the same value. Hence there is a concentration effect that exist with many component but does not exist when looking at a component in isolation. My colleague theory is that the MC is overshooting because for a specific interval T, it's looking at the possibility of the component failing once but also the probability that, in extreme case, the component could have failed 2 or 3 times at that interval. Hence the MC is more "accurate" because he accounts for this scenarios as well. Is my colleague right in assuming that he can use the results from 10,000 component to asses the behavior of 1 and that the MC is providing additional insight on the component ? Or are both approach modeling different thing altogether ?
