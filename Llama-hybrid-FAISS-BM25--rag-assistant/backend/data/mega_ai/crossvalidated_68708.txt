[site]: crossvalidated
[post_id]: 68708
[parent_id]: 68704
[tags]: 
The biggest problem with simple, classical neural network is their overfitting capabilities. If you do not use any type of regularization (like eg. Tikhonov regularization), then the only parameter which prevents overfitting is small number of hidden neurons. Assuming, that you dit not make any implementation error, it looks like ~30 is in your case kind of equlibrium between expression power (larger hidden layer) and model simplicity and so-smaller chance of overfitting (smaller hidden layer). Smaller number of hidden neurons then output ones also suggest high correlation of your output values (as well as input ones). To sum up: you should double check the code to be sure that everything is ok if it is ok, then there are at least two possible reasons: there is a high correlation in input/output values of your data found size of the layer is kind of equlibrium between model's expresivness and complexity
