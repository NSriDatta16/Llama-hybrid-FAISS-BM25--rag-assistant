[site]: crossvalidated
[post_id]: 111007
[parent_id]: 110994
[tags]: 
You manually impose a decision-tree-like structure by training 20 different classifiers, which is legitimate. However, I would argue that the decision tree would pick up duration as the root node, if the length of the time series is actually the most important factor in class discrimination. So you may be unnecessarily intervening in the training here. Also, I assume the length of each instance is finalised before you classify them , since otherwise you wouldn't be able to choose which of the 20 classifiers to feed a new instance into. An alternative here is to use descriptive statistics of the time series (the dead simple ones being: max, min, median, mean, variance, length, etc.) as features to train a single (central) classifier. The downside is, this will lose information unless you are sure about and careful to extract the specific characteristics of the time series you are interested in for your classifier. Some notes: 10,000 observations per classifier may be plenty or too little, depending on the dimensions of your probability space. As the curse of dimensionality dictates: with a fixed number of training samples, the predictive power of your classifier reduces as the dimensionality of the probability space increases. In other words, an enormous amount of training data may be required to ensure that there are several samples with each combination of values. You note that as a decision tree grows down, the no. of training instances it uses to create a sub-tree reduces which is completely true. If you do not have sufficient training data, you may run out of training instances before you hit a leaf node.
