[site]: crossvalidated
[post_id]: 481832
[parent_id]: 
[tags]: 
val_accuracy not changing but it is very high

My model's validation accuracy doesn't change and I have been trying to fix it for a while, but now the accuracy is very high. I'm not sure if that means my model is good because it has high accuracy or should I be concerned about the fact that the accuracy doesn't change. In addition, every time I run the code each fold has the same accuracy. code: def model_and_print(x, y, Epochs, Batch_Size, loss, opt, class_weight, callback): # fix random seed for reproducibility seed = 7 np.random.seed(seed) # define 10-fold cross validation test harness kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed) # Define per-fold score containers acc_per_fold = [] loss_per_fold = [] # K-fold Cross Validation model evaluation fold_no = 1 for train, test in kfold.split(x, y): print(x[train]) lda = LDA(n_components=1) x[train] = lda.fit_transform(x[train], y[train]) x[test] = lda.transform(x[test]) print("------------------------------------") print(x[train]) # create model model = Sequential() model.add(Dropout(0, input_shape=(len(x[0]),))) model.add(Dense(8, activation='relu', kernel_constraint=maxnorm(3))) model.add(Dropout(0.4)) model.add(Dense(4, activation='relu', kernel_constraint=maxnorm(3))) model.add(Dropout(0.5)) model.add(Dense(1, activation=tf.nn.sigmoid)) # compile model model.compile(optimizer=opt, loss=loss, metrics=['accuracy'] ) # Generate a print print('------------------------------------------------------------------------') print(f'Training for fold {fold_no} ...') # fit model history = model.fit(x[train], y[train], validation_data=(x[test], y[test]), epochs=Epochs, batch_size=Batch_Size, verbose=1, class_weight=class_weight) # , callbacks=[callback] # Generate generalization metrics scores = model.evaluate(x[test], y[test], verbose=1) print( f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1] * 100}%') acc_per_fold.append(scores[1] * 100) loss_per_fold.append(scores[0]) # Increase fold number fold_no = fold_no + 1 # == Provide average scores == print('------------------------------------------------------------------------') print('Score per fold') for i in range(0, len(acc_per_fold)): print('------------------------------------------------------------------------') print(f'> Fold {i + 1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%') print('------------------------------------------------------------------------') print('Average scores for all folds:') print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})') print(f'> Loss: {np.mean(loss_per_fold)}') print('------------------------------------------------------------------------') # summarize history for accuracy plt.plot(history.history['accuracy']) plt.plot(history.history['val_accuracy']) plt.title('model accuracy') plt.ylabel('accuracy') plt.xlabel('epoch') plt.legend(['train', 'test'], loc='upper left') plt.show() # summarize history for loss plt.plot(history.history['loss']) plt.plot(history.history['val_loss']) plt.title('model loss') plt.ylabel('loss') plt.xlabel('epoch') plt.legend(['train', 'test'], loc='upper left') plt.show() def main(): data = ["data.pkl", "data_list.pkl", "data_mean.pkl"] df = pd.read_pickle(data[2]) x, y = data_frame_to_feature_and_target_arrays(df) # hyper meters Epochs = 2000 Batch_Size = 64 learning_rate = 0.3 optimizer = optimizers.Adadelta(learning_rate=learning_rate) loss = "binary_crossentropy" class_weight = compute_class_weight('balanced', np.unique(y), y) class_weight = dict(enumerate(class_weight)) es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3) model_and_print(x, y, Epochs, Batch_Size, loss, optimizer, class_weight, es_callback) if __name__ == "__main__": main() part of out put of last fold and summary of all folds: 64/73 [=========================>....] - ETA: 0s - loss: 0.4697 - accuracy: 0.7812 73/73 [==============================] - 0s 42us/step - loss: 0.4644 - accuracy: 0.7808 - val_loss: 0.2812 - val_accuracy: 1.0000 Epoch 1943/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.4318 - accuracy: 0.7969 73/73 [==============================] - 0s 68us/step - loss: 0.4475 - accuracy: 0.7945 - val_loss: 0.2811 - val_accuracy: 1.0000 Epoch 1944/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.5224 - accuracy: 0.7500 73/73 [==============================] - 0s 68us/step - loss: 0.5245 - accuracy: 0.7397 - val_loss: 0.2812 - val_accuracy: 1.0000 Epoch 1945/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.5170 - accuracy: 0.7812 73/73 [==============================] - 0s 41us/step - loss: 0.4984 - accuracy: 0.7945 - val_loss: 0.2814 - val_accuracy: 1.0000 Epoch 1946/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.5684 - accuracy: 0.7188 73/73 [==============================] - 0s 27us/step - loss: 0.5538 - accuracy: 0.7260 - val_loss: 0.2813 - val_accuracy: 1.0000 Epoch 1947/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.4427 - accuracy: 0.7969 73/73 [==============================] - 0s 27us/step - loss: 0.4470 - accuracy: 0.7945 - val_loss: 0.2812 - val_accuracy: 1.0000 Epoch 1948/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.5000 - accuracy: 0.7344 73/73 [==============================] - 0s 68us/step - loss: 0.4960 - accuracy: 0.7534 - val_loss: 0.2810 - val_accuracy: 1.0000 Epoch 1949/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.4940 - accuracy: 0.7344 73/73 [==============================] - 0s 41us/step - loss: 0.4959 - accuracy: 0.7397 - val_loss: 0.2809 - val_accuracy: 1.0000 Epoch 1950/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.4776 - accuracy: 0.7812 73/73 [==============================] - 0s 37us/step - loss: 0.4751 - accuracy: 0.8082 - val_loss: 0.2810 - val_accuracy: 1.0000 Epoch 1951/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.4514 - accuracy: 0.8125 73/73 [==============================] - 0s 41us/step - loss: 0.4978 - accuracy: 0.7808 - val_loss: 0.2818 - val_accuracy: 1.0000 Epoch 1952/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.5187 - accuracy: 0.7500 73/73 [==============================] - 0s 41us/step - loss: 0.5224 - accuracy: 0.7260 - val_loss: 0.2818 - val_accuracy: 1.0000 Epoch 1953/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.4045 - accuracy: 0.8438 73/73 [==============================] - 0s 41us/step - loss: 0.4094 - accuracy: 0.8356 - val_loss: 0.2816 - val_accuracy: 1.0000 Epoch 1954/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.5283 - accuracy: 0.7344 73/73 [==============================] - 0s 41us/step - loss: 0.4958 - accuracy: 0.7534 - val_loss: 0.2816 - val_accuracy: 1.0000 Epoch 1955/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.5244 - accuracy: 0.7344 73/73 [==============================] - 0s 27us/step - loss: 0.5246 - accuracy: 0.7260 - val_loss: 0.2819 - val_accuracy: 1.0000 Epoch 1956/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.4830 - accuracy: 0.8125 73/73 [==============================] - 0s 41us/step - loss: 0.4899 - accuracy: 0.7945 - val_loss: 0.2820 - val_accuracy: 1.0000 Epoch 1957/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.5231 - accuracy: 0.7344 73/73 [==============================] - 0s 41us/step - loss: 0.5053 - accuracy: 0.7397 - val_loss: 0.2819 - val_accuracy: 1.0000 Epoch 1958/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.4301 - accuracy: 0.8125 73/73 [==============================] - 0s 41us/step - loss: 0.4422 - accuracy: 0.8082 - val_loss: 0.2818 - val_accuracy: 1.0000 Epoch 1959/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.4817 - accuracy: 0.7812 73/73 [==============================] - 0s 41us/step - loss: 0.4824 - accuracy: 0.7808 - val_loss: 0.2818 - val_accuracy: 1.0000 Epoch 1960/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.4598 - accuracy: 0.7812 73/73 [==============================] - 0s 41us/step - loss: 0.4839 - accuracy: 0.7534 - val_loss: 0.2819 - val_accuracy: 1.0000 Epoch 1961/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.4983 - accuracy: 0.7344 73/73 [==============================] - 0s 41us/step - loss: 0.4958 - accuracy: 0.7397 - val_loss: 0.2818 - val_accuracy: 1.0000 Epoch 1962/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.4872 - accuracy: 0.7812 73/73 [==============================] - 0s 41us/step - loss: 0.4844 - accuracy: 0.7671 - val_loss: 0.2819 - val_accuracy: 1.0000 Epoch 1963/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.4871 - accuracy: 0.7812 73/73 [==============================] - 0s 41us/step - loss: 0.4859 - accuracy: 0.7808 - val_loss: 0.2818 - val_accuracy: 1.0000 Epoch 1964/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.4188 - accuracy: 0.8125 73/73 [==============================] - 0s 27us/step - loss: 0.4230 - accuracy: 0.8219 - val_loss: 0.2815 - val_accuracy: 1.0000 Epoch 1965/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.5283 - accuracy: 0.7188 73/73 [==============================] - 0s 27us/step - loss: 0.5309 - accuracy: 0.7123 - val_loss: 0.2814 - val_accuracy: 1.0000 Epoch 1966/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.5033 - accuracy: 0.7500 73/73 [==============================] - 0s 27us/step - loss: 0.4809 - accuracy: 0.7671 - val_loss: 0.2813 - val_accuracy: 1.0000 Epoch 1967/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.4611 - accuracy: 0.7812 73/73 [==============================] - 0s 27us/step - loss: 0.4626 - accuracy: 0.7808 - val_loss: 0.2814 - val_accuracy: 1.0000 Epoch 1968/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.5145 - accuracy: 0.7031 73/73 [==============================] - 0s 27us/step - loss: 0.5024 - accuracy: 0.7260 - val_loss: 0.2813 - val_accuracy: 1.0000 Epoch 1969/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.5231 - accuracy: 0.7031 73/73 [==============================] - 0s 27us/step - loss: 0.5099 - accuracy: 0.7260 - val_loss: 0.2812 - val_accuracy: 1.0000 Epoch 1970/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.4966 - accuracy: 0.7656 73/73 [==============================] - 0s 41us/step - loss: 0.5019 - accuracy: 0.7534 - val_loss: 0.2811 - val_accuracy: 1.0000 Epoch 1971/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.4728 - accuracy: 0.7812 73/73 [==============================] - 0s 41us/step - loss: 0.4818 - accuracy: 0.7671 - val_loss: 0.2810 - val_accuracy: 1.0000 Epoch 1972/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.4607 - accuracy: 0.7656 73/73 [==============================] - 0s 41us/step - loss: 0.4753 - accuracy: 0.7671 - val_loss: 0.2809 - val_accuracy: 1.0000 Epoch 1973/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.5350 - accuracy: 0.7031 73/73 [==============================] - 0s 41us/step - loss: 0.5241 - accuracy: 0.7123 - val_loss: 0.2809 - val_accuracy: 1.0000 Epoch 1974/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.4490 - accuracy: 0.7969 73/73 [==============================] - 0s 41us/step - loss: 0.4625 - accuracy: 0.7808 - val_loss: 0.2809 - val_accuracy: 1.0000 Epoch 1975/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.5022 - accuracy: 0.7344 73/73 [==============================] - 0s 41us/step - loss: 0.4917 - accuracy: 0.7534 - val_loss: 0.2807 - val_accuracy: 1.0000 Epoch 1976/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.5628 - accuracy: 0.7344 73/73 [==============================] - 0s 27us/step - loss: 0.5517 - accuracy: 0.7397 - val_loss: 0.2807 - val_accuracy: 1.0000 Epoch 1977/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.4203 - accuracy: 0.8125 73/73 [==============================] - 0s 27us/step - loss: 0.4283 - accuracy: 0.8082 - val_loss: 0.2805 - val_accuracy: 1.0000 Epoch 1978/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.4796 - accuracy: 0.7656 73/73 [==============================] - 0s 27us/step - loss: 0.4890 - accuracy: 0.7534 - val_loss: 0.2806 - val_accuracy: 1.0000 Epoch 1979/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.4471 - accuracy: 0.7812 73/73 [==============================] - 0s 41us/step - loss: 0.5046 - accuracy: 0.7671 - val_loss: 0.2811 - val_accuracy: 1.0000 Epoch 1980/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.4724 - accuracy: 0.7812 73/73 [==============================] - 0s 55us/step - loss: 0.4743 - accuracy: 0.7671 - val_loss: 0.2810 - val_accuracy: 1.0000 Epoch 1981/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.5025 - accuracy: 0.7812 73/73 [==============================] - 0s 55us/step - loss: 0.4842 - accuracy: 0.8082 - val_loss: 0.2811 - val_accuracy: 1.0000 Epoch 1982/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.5298 - accuracy: 0.7344 73/73 [==============================] - 0s 41us/step - loss: 0.5178 - accuracy: 0.7397 - val_loss: 0.2809 - val_accuracy: 1.0000 Epoch 1983/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.4816 - accuracy: 0.7500 73/73 [==============================] - 0s 55us/step - loss: 0.4959 - accuracy: 0.7534 - val_loss: 0.2806 - val_accuracy: 1.0000 Epoch 1984/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.4713 - accuracy: 0.7812 73/73 [==============================] - 0s 109us/step - loss: 0.4899 - accuracy: 0.7534 - val_loss: 0.2804 - val_accuracy: 1.0000 Epoch 1985/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.3938 - accuracy: 0.8281 73/73 [==============================] - 0s 55us/step - loss: 0.4105 - accuracy: 0.8219 - val_loss: 0.2802 - val_accuracy: 1.0000 Epoch 1986/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.5256 - accuracy: 0.7188 73/73 [==============================] - 0s 41us/step - loss: 0.5156 - accuracy: 0.7260 - val_loss: 0.2801 - val_accuracy: 1.0000 Epoch 1987/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.5261 - accuracy: 0.7188 73/73 [==============================] - 0s 95us/step - loss: 0.5187 - accuracy: 0.7397 - val_loss: 0.2800 - val_accuracy: 1.0000 Epoch 1988/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.5315 - accuracy: 0.7500 73/73 [==============================] - 0s 55us/step - loss: 0.5097 - accuracy: 0.7808 - val_loss: 0.2800 - val_accuracy: 1.0000 Epoch 1989/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.5448 - accuracy: 0.7500 73/73 [==============================] - 0s 27us/step - loss: 0.5441 - accuracy: 0.7397 - val_loss: 0.2801 - val_accuracy: 1.0000 Epoch 1990/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.4220 - accuracy: 0.8125 73/73 [==============================] - 0s 68us/step - loss: 0.4212 - accuracy: 0.8219 - val_loss: 0.2800 - val_accuracy: 1.0000 Epoch 1991/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.4956 - accuracy: 0.7344 73/73 [==============================] - 0s 69us/step - loss: 0.5134 - accuracy: 0.7260 - val_loss: 0.2799 - val_accuracy: 1.0000 Epoch 1992/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.4724 - accuracy: 0.7812 73/73 [==============================] - 0s 68us/step - loss: 0.4784 - accuracy: 0.7534 - val_loss: 0.2803 - val_accuracy: 1.0000 Epoch 1993/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.4280 - accuracy: 0.8125 73/73 [==============================] - 0s 83us/step - loss: 0.4244 - accuracy: 0.8219 - val_loss: 0.2806 - val_accuracy: 1.0000 Epoch 1994/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.4116 - accuracy: 0.8281 73/73 [==============================] - 0s 68us/step - loss: 0.4122 - accuracy: 0.8356 - val_loss: 0.2805 - val_accuracy: 1.0000 Epoch 1995/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.4958 - accuracy: 0.7500 73/73 [==============================] - 0s 55us/step - loss: 0.4934 - accuracy: 0.7534 - val_loss: 0.2804 - val_accuracy: 1.0000 Epoch 1996/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.5281 - accuracy: 0.7500 73/73 [==============================] - 0s 27us/step - loss: 0.5280 - accuracy: 0.7534 - val_loss: 0.2804 - val_accuracy: 1.0000 Epoch 1997/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.4441 - accuracy: 0.8125 73/73 [==============================] - 0s 82us/step - loss: 0.4460 - accuracy: 0.8082 - val_loss: 0.2803 - val_accuracy: 1.0000 Epoch 1998/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.4106 - accuracy: 0.8125 73/73 [==============================] - 0s 55us/step - loss: 0.4056 - accuracy: 0.8356 - val_loss: 0.2800 - val_accuracy: 1.0000 Epoch 1999/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.4229 - accuracy: 0.8281 73/73 [==============================] - 0s 41us/step - loss: 0.4157 - accuracy: 0.8493 - val_loss: 0.2795 - val_accuracy: 1.0000 Epoch 2000/2000 64/73 [=========================>....] - ETA: 0s - loss: 0.4924 - accuracy: 0.7969 73/73 [==============================] - 0s 68us/step - loss: 0.4920 - accuracy: 0.7808 - val_loss: 0.2797 - val_accuracy: 1.0000 8/8 [==============================] - 0s 123us/step Score for fold 10: loss of 0.27967995405197144; accuracy of 100.0% ------------------------------------------------------------------------ Score per fold ------------------------------------------------------------------------ > Fold 1 - Loss: 0.9029330015182495 - Accuracy: 66.66666865348816% ------------------------------------------------------------------------ > Fold 2 - Loss: 0.251650869846344 - Accuracy: 87.5% ------------------------------------------------------------------------ > Fold 3 - Loss: 0.15162397921085358 - Accuracy: 100.0% ------------------------------------------------------------------------ > Fold 4 - Loss: 0.12148290127515793 - Accuracy: 100.0% ------------------------------------------------------------------------ > Fold 5 - Loss: 0.1457768827676773 - Accuracy: 100.0% ------------------------------------------------------------------------ > Fold 6 - Loss: 0.17563441395759583 - Accuracy: 100.0% ------------------------------------------------------------------------ > Fold 7 - Loss: 0.1939687728881836 - Accuracy: 100.0% ------------------------------------------------------------------------ > Fold 8 - Loss: 0.2388959378004074 - Accuracy: 87.5% ------------------------------------------------------------------------ > Fold 9 - Loss: 0.1327996551990509 - Accuracy: 100.0% ------------------------------------------------------------------------ > Fold 10 - Loss: 0.27967995405197144 - Accuracy: 100.0% ------------------------------------------------------------------------ Average scores for all folds: > Accuracy: 94.16666686534882 (+- 10.408329472389672) > Loss: 0.25944463685154917 ------------------------------------------------------------------------
