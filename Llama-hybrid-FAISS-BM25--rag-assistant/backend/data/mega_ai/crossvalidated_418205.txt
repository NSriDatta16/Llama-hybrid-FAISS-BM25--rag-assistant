[site]: crossvalidated
[post_id]: 418205
[parent_id]: 
[tags]: 
A Two-tailed One-sample Test of a Categorical Variable with Multiple Raters

I feel like this should be a Stats 101 question, but I've taught Stats 101, and I can't quite figure it out, so probably not.... I'm using an unsupervised learning technique to create topic models of a corpus of documents, and I want to know which model is best. It turns out that none of the "objective" measures of model quality works very well, a fact that leads pretty directly to a need for human assessments. I am fortunate to have some subject-matter experts to assist me in this. At the moment, I'm preparing to gather data by showing each of my testers the same sample of documents, and asking each tester, for each document, which model produces the better set of topic scores (operationalized as the top 4 topics, so long as the scores are substantially above average for the corpus). For the sake of this question, let's say that if a tester chooses Model A, we score it as a 1, and Model B, as a -1; thus, our null hypothesis is a mean score 0 (both models are equally good), and we're doing a two-tailed test. One additional wrinkle is that, fairly often, the two models produce a list of topic scores for a given document that are basically indistinguishable (the same 4 topics in the same order--I doubt my testers are going to worry much about marginal differences in scores). Helping me is not these experts' main job, and I'm not paying them, and so I'm not going to present them with pointless questions--ergo, I leave those documents out of the testing sample, though I do count them; in effect, they're cases where the measured score is 0. So I'm faced with the following questions: Normally, with an ordered categorical variable having only 3 levels, I'd want to avoid a test designed for continuous variables, but I'm not sure which one I'd use in this case. OTOH, if I average the scores from the testers, I'd arguably have continuous variables... ...or should I just create a polychoric factor model with the individual responses as measures of both a latent variable for the difference between models and a latent for each tester? I'd think that the fact that some documents are preemptively scored as 0--for all testers--would mess with the assumption that we're measuring a latent variable rather than taking direct measurements. And here's the toughest one for me: I have multiple models, and I want to test them against each other, in pairs. All of those tests are not going to be statistically independent, because the same model will show up in multiple tests. How do I deal with that?
