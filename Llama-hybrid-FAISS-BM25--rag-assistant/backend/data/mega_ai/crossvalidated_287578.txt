[site]: crossvalidated
[post_id]: 287578
[parent_id]: 287569
[tags]: 
If you just fix all hyperparameters and do time series cross validation as in the picture in your post, then you do not need a separate test set to evaluate the out-of-sample performance; the forecast error for the validation points is a fair evaluation of that. But if you do tune your model and pick the best tuning values based on validation performance, then the validation error will be an optimistic estimate of test error. Also, the following might be a misunderstanding: Unlike 10-fold cross validation where 9 out of 10 times you have seen the data during the training so the prediction error is underestimated. The underestimation happens only if you do hyperparameter tuning and select the best-tuned model. Otherwise, the validation error on fold $k$ is a fair evaluation of out-of-sample performance for that particular set of tuning parameters (not selected based on performance).
