[site]: crossvalidated
[post_id]: 591761
[parent_id]: 
[tags]: 
When do we train a neural network using full batch training?

It is a trend in deep learning to train models using multi-batches, i.e., to show the model a subset of the entire dataset for each weight update. In some cases, as in continual learning, we see that it is possible to train the network on one datapoint at the time. It is quite rare, on the other hand, to see research papers, or state of the art models, that are trained on a whole dataset simultaneously, i.e., in full-batch training. For a research project, it would be useful for me to come up with a list of applications/models where training a neural network in full batch training is preferable with respect to using multi-batches. An example I have found is the COIN and COIN++ papers, that train on full batch training in order to memorise datapoints. Question: Do you know other applications where performing full-batch training is preferable to use mini-batches? Which ones?
