[site]: crossvalidated
[post_id]: 359044
[parent_id]: 
[tags]: 
In last few layers of a neural network, what is the benefit of connecting several affine layers instead of just one?

It is a question regarding some fundamental structure of the neural network. I now take the famous LENET network for MNIST data as an example. In LENET, the last few layers are the affine layers. In the below image, they are C5, F6 and Output respectively, which are 400×120, 120×84 and 84×10 layers. In this sense, at the output of these layers we are using the vectors of 120 dimensions, 84 dimensions and 10 dimensions to try to represent each MNIST image (or to match the features). My question is, why can't we just use the two layers of 400×84 and 84×10 to replace the above? Or at an extreme, can we even just use a single layer of 400×10 to replace the above all three layers? In the view of parameters, the said two/single layers are lighter than three layers (and are easier to train). For image processing sense, the previous CNN/pooling layers have already extracted the 400-dimensional features from each MNIST image. So, what is the benefit of using the above three layers instead of the said two/single layers? Or to be more general, what is the benefit of connecting several separated affine layers instead of combining the layers? Many thanks!
