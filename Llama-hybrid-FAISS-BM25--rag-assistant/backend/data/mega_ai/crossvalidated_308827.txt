[site]: crossvalidated
[post_id]: 308827
[parent_id]: 226831
[tags]: 
I did some research in the past about online and incremental learning. There are some ideas you need to take into account. Every classifier can 'do' incremental learning, the only problem is that with some is much more harder to do so. There is not an incremental learning algorithm as such, only a way of achieving this buy using the common algorithms. Normally, you would pick one of them and adapt the way you train it and feed the data either in a batch or in an online fashion. You can do this in two ways: a) Retrain the model from scratch each time a new sample (or set of samples) arrive. Obviously this is not ideal, but if your model is not too complex (meaning yo can perform a whole training between instances coming) and you limit your dataset size (discarding old data, new data or random data and keeping a steady number of training instances), it can work in some scenarios. A nice example of this 'pseudo-incremental' learning with support vector machines can be found here . b) Find a way to update your model parameters/weights by only modifying 'a little bit' these parameters when the prediction was wrong. Neural Nets are naturally great for this as you can train a model, save the weights and then retrain with new batches of data as they come. Additionally, you can tweak the learning rate to give more/less relevance to your new inputs. If you can choose any algorithm for your case, this would be my choice. However, there are many other methods: e.g. in Bayesian approaches you can modify the distributions applying numerical increments/decrements to certain parameters (see this for another example.) Do some reading and look for previous approaches that match whatever you want your learning algorithm behaviour. It can seem daunting in the beginning to set everything up for yourself instead of using this or that library, but it becomes supercool when you arrive to the point you feel in charge of all the learning process of your model. Good luck!
