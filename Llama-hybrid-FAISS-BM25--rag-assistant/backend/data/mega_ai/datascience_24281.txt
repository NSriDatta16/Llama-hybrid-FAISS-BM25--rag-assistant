[site]: datascience
[post_id]: 24281
[parent_id]: 24276
[tags]: 
PCA - Principal Component Analysis: I think the best option here would be to perform some dimensionality reduction using principal component analysis (PCA) prior to training your model and prior to predicting once you have a model. PCA will allow you to preserve a percentage of the total variance in the input data while significantly reducing the total number of dimensions. In the toy example that you present, I would expect a high degree of linear correlation between the height per square foot features and a high degree of linear orthogonality between the other features. In such a case, PCA will naturally mash the correlated height terms together into 1 or 2 features while preserving 95-99% of the total original variance. The only downside here, is that you will loose the physical meaning of your input features. I suppose you could decompose the eigenvectors and try to understand them, but loosing the physical meaning is usually not particularly hindering to training a model. There are other methods, like lasso-ridge regression (you mention classification, but then provide a regression example e.g. the continuum variable of home price) or directly tracking the feature importance in a linear regression, but I prefer how PCA is able to project out linear dependence in input features, which seems particularly topical to your problem. Give it a try and let us know how it goes. Hope this helps!
