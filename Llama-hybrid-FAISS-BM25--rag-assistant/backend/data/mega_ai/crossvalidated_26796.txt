[site]: crossvalidated
[post_id]: 26796
[parent_id]: 26757
[tags]: 
Having taken a look at paper cited, it's not quite as bad as I thought -- they basically suggest normality testing as a way to identify extreme outliers that might screw up the analysis, and they say that mixed modeling allows less "aggressive" outlier identification/removal. (They use the terms "minimal trimming" and "mild" vs "aggressive" a priori data screening/outlier removal, which at a quick glance I don't see defined precisely in the paper: perhaps they're defined in the references??) At least pre-screening according to strictly specified rules, based only on the response variable without taking the predictors into account, does not lead to the danger of data snooping (as opposed to, say, running an initial analysis without screening and then going back and screening only if you find the results not to your liking). I still don't like it though, and would probably say that only results that are qualitatively robust to the presence or absence of outliers should be taken completely seriously.) However, my previous answer on r-help still holds: outlier detection etc. should be done on the conditional distributions, not marginal distributions -- given a strong effect of a qualitative predictor, the marginal distribution will be multimodal = not normal at all (I don't see how Baayen and Milin get away with this); as you suggest, 1/21 values of $p I would personally prefer that you skip a priori test-based screening completely (removing physically impossible/very long values as suggested in the paper is perfectly sensible) and use model criticism instead (as suggested by the paper), but you do have to conform to the norms of your community where they are not completely outrageous. If possible, compare your results with screened vs. unscreened data and see that they are qualitatively similar.
