[site]: crossvalidated
[post_id]: 296256
[parent_id]: 296220
[tags]: 
There are several reasons to use a Bayesian method rather than a Frequentist or Likelihoodist method. This is even more true for vector autoregression. First, let me begin with a trivial example of what appear to be "no differences" under a trivial and simple problem. The example comes from Thomas Bayes' original article that solved Bayes theorem. His method involved a billiards table, but let's simplify it to computerized random number generation. His example is nicer in that the physics, the Frequentist interpretation, and the Bayesian interpretation are obviously linked. This is a two player game. The first step is to generate a random number between 0 and 1, denoted $\theta.$ Random numbers will be generated between zero and one for each round of the game. If the random number is less than or equal to $\theta$ then the first player scores, otherwise the second player scores. The first player to six points wins. Now imagine a score of 5-3, what are the odds that player two will win? It is here that Pearson-Neyman Frequentist, Fisherian Likelihoodist and Bayesian methods diverge. For the Frequentist and the Likelihoodist, the estimated probability of winning any one round for player two is $3/8^{ths}$. The probability of player two winning three rounds in a row is $$\frac{3}{8}^3.$$ This is approximately 18:1 odds against player two. For the Bayesian the question is different. First, the posterior probability has to be solved, which is a distribution and not a point estimate. With a flat prior where $p(\theta)\propto{1}$, the posterior probability under a binomial likelihood is $504\theta^3(1-\theta)^5$. Because this is a distribution, we must average over the entire posterior to make a prediction and eliminate the uncertainty regarding $\theta.$ This is solved by calculating $$\int_0^1504\theta^3\theta^3(1-\theta)^5.$$ The extra $\theta^3$ is the probability of winning three in a row, while the rest is the posterior. The resulting calculated odds are 10:1 against the second player winning. That is not trivially different from the Frequentist or Likelihoodist odds. Further, a bookie using null hypothesis methods could be "Dutch booked," or in simpler terms, a gambler or set of gamblers could construct a convex combination of gambles that would create a sure win for themselves due to the calculation differences. A Bayesian prediction does not automatically have the same value as null hypothesis predictions. The non-Bayesian method can never stochastically dominate the Bayesian method, but the Bayesian method can stochastically dominate the null hypothesis method. Which should you use? It depends on what you are doing with the prediction. If you are testing if some monetary regime impacts some measure of output, then you should use a null hypothesis method. If you are going to base a budget or gamble in some manner on the outcome, then you should use the Bayesian method. Bayesian solutions are inherently coherent and admissible, that is to say, that you can gamble on them and that there is no less risky way to construct an estimate. While it is true that if your true model has normally distributed data, then under a flat prior you will get equivalent results when there are three or fewer independent variables, this will not be true for three or more in regression. This is due to Stein's paradox. In that case, you can always construct a Bayesian model that will be superior to a null hypothesis method, though you cannot use a "flat" prior as it will not integrate to unity. Finally, for individuals in macroeconomics and in the capital markets, non-Bayesian methods are not valid solutions. Up until now, everyone has just assumed the distribution of the underlying data into existence. It was usually normal or lognormal. I have written a paper deriving the distribution of the underlying data. I show that there is no sufficient statistic and that least squares creates serious estimation errors. For the capital markets, it overestimates returns by two percent per annum and underestimates risk by four percent per annum. To understand the magnitude of that bias, had a prediction been made one hundred years ago on the current per capita income of people under the British Raj, the error would have put Indian per capita income between Spain and Portugal's. India would be the largest economy in the world. Although the papers are on equity securities, primarily, it applies to any model that has capital in it. You can find those papers at https://papers.ssrn.com/sol3/cf_dev/AbsByAuth.cfm?per_id=1541471 .
