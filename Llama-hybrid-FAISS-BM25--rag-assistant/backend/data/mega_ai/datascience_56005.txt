[site]: datascience
[post_id]: 56005
[parent_id]: 55991
[tags]: 
This usually means that you use a very low learning rate for a set number of training steps (warmup steps). After your warmup steps you use your "regular" learning rate or learning rate scheduler. You can also gradually increase your learning rate over the number of warmup steps. As far as I know, this has the benefit of slowly starting to tune things like attention mechanisms in your network.
