[site]: crossvalidated
[post_id]: 78732
[parent_id]: 78579
[tags]: 
Mathematics deals with idealized abstractions that (almost always) have absolute solutions, or the fact that no such solution exists can generally be described fully. It is the science of discovering complex but necessary consequences from simple axioms. Statistics uses math, but it is not math. It's educated guesswork. It's gambling. Statistics does not deal with idealized abstractions (although it does use some as tools), it deals with real world phenomena. Statistical tools often make simplifying assumptions to reduce the messy real world data to something that fits into the problem domain of a solved mathematical abstraction. This allows us to make educated guesses, but that's really all that statistics is: the art of making very well informed guesses. Consider hypothesis testing with p-values. Let's say we are testing some hypothesis with significance $\alpha = 0.01$, and after gathering data we find a p-value of $0.001$. So we reject the null hypothesis in favor of an alternative hypothesis. But what is this p-value really? What is the significance? Our test statistic was developed such that it conformed to a particular distribution, probably student's t. Under the null hypothesis, the percentile of our observed test statistic is the p-value. In other words, the p-value gives the probability that we would get a value as far from the expectation of the distribution (or farther) as the observed test statistic. The signficance level is a fairly arbitrary rule-of-thumb cutoff: setting it to $0.01$ is equivalent to saying, "it's acceptable if 1 in 100 repetitions of this experiment suggest that we reject the null, even if the null is in fact true." The p-value gives us the probability that we observe the data at hand given that the null is true (or rather, getting a bit more technical, that we observe data under the null hypothesis that gives us at least as extreme a value of the tested statistic as that which we found). If we're going to reject the null, then we want this probability to be small, to approach zero. In our specific example, we found that the probability of observing the data we gathered if the null hypothesis were true was just $0.1\%$, so we rejected the null. This was an educated guess. We never really know for sure that the null hypothesis is false using these methods, we just develop a measurement for how strongly our evidence supports the alternative. Did we use math to calculate the p-value? Sure. But math did not give us our conclusion. Based on the evidence, we formed an educated opinion, but it's still a gamble. We've found these tools to be extremely effective over the last 100 years, but the people of the future may wonder in horror at the fragility of our methods.
