[site]: datascience
[post_id]: 128243
[parent_id]: 128242
[tags]: 
Yes, transformer-based architectures generate contextual token embeddings. In article To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks , we can find the following description of the feature extraction process: For both ELMo and BERT, we extract contextual representations of the words from all layers. During adaptation, we learn a linear weighted combination of the layers (Peters et al., 2018) which is used as input to a task-specific model. When extracting features, it is important to expose the internal layers as they typically encode the most transferable representations. It basically says: Run the model with your input in inference mode. Take the output vectors of the model layers, including the middle ones. In your task classifier, learn a linear combination of the layers you took from the previous model.
