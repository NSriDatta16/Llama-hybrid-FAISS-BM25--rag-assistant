[site]: crossvalidated
[post_id]: 499343
[parent_id]: 
[tags]: 
Is it possible to learn a neural network to predict always zero?

I saw a paper where they learn L based on the following cost function: $$D_1L(q(k-1), q(k)) + D_2L(q(k),q(k+1)) = 0$$ Here $D$ is the derivative with respect to ith argument of L. So the target is always zero independently of the input of L but how does the optimization method knows the real values of L for a given input? I mean as long as the cost function is satisfied with a zero value the model is learning but it can be learning wrong values for L. For instance, if the real value of $D_1L(q(k-1), q(k))$ is 10 then the real value of $D_2L(q(k),q(k+1))$ has to be -10. However the neural network may be learning wrong values for L as long as they are symmetric, for example -2 and 2. Am I missing something or the method in that paper simply doesn't make any sense? Is it possible to learn a neural network in regression task with constant target?
