[site]: crossvalidated
[post_id]: 93881
[parent_id]: 
[tags]: 
How can I evaluate the performance of a system that generates word clusters?

The word2vec tool uses deep learning to compute vector representations of words. They've mentioned that - "The word vectors can be also used for deriving word classes from huge data sets. This is achieved by performing K-means clustering on top of the word vectors." When I execute the code, it seems that the tool computes word vectors from a dataset and clusters it into 200 classes. What if I wanted to use another method of clustering? How would I measure how 'good' these classes are, or how well the clustering algorithm is working? Also, it seems to me that the word vector representation would be a very sparse vector space. Couldn't the clustering be improved by PCA or some method of dimension reduction? Again, how can I test how well the clustering is performing? If I can figure out a way to evaluate, I can change the code and add PCA and see if it does better. EDIT - I found some papers that mention that word clustering can be used in document classification. The idea is to use the word clusters instead of a bag-of-words as features for classification, and this reduces the feature space while preserving 'redundant' features. I'm looking more into this, but it seems like I could use the word clusters on a document classification task and see if it performs better if I change the clustering algorithm. Would this work? A follow-up question - how would I implement this approach of using word clusters as features?
