[site]: crossvalidated
[post_id]: 409275
[parent_id]: 
[tags]: 
What is the relation between expected average reward and single step mean reward for a non-stationary MDP policy?

The expected average reward for a policy $\pi$ is: $$ \rho_\pi = \lim_{T \rightarrow \infty } \frac{1}{T} \sum_{t=1}^{T} r_t$$ where $r_t$ is the reward obtained at time $t$ following policy $\pi$ . For a stationary policy $\pi$ , $$ \rho_\pi + \lambda_\pi(s) = \bar{r}(s, \pi(s)) + \sum_{s'} p(s'|s,a) \cdot \lambda_\pi(s') $$ where $\bar{r}(s,a)$ and $p(\cdot|s,a)$ are the mean rewards and transition probabilities of the MDP respectively and $\lambda_\pi$ is the bias vector of $\pi$ . Does a similar relation exist for a non-stationary policy? Edit- Reference for bias vector: The Handbook of Markov Decision Processes 1 defines bias of a stationary deterministic policy as follows: $\lambda_\pi(s) = \sum_{n = 0}^{\infty} \mathbb{E}[\bar{r}(s_n, \pi(s_n)) - \rho_\pi | s_0 = s]$ where n indicates the current time step and $s_n$ indicates the state at time step $n$ after following policy $\pi$ starting from the initial state $s_0 = s$
