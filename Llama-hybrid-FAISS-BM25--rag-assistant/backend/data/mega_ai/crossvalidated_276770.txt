[site]: crossvalidated
[post_id]: 276770
[parent_id]: 
[tags]: 
Replay memory algorithm in Q-learning with neural network

I read a lot about replay memory, but did not find the material, where this method is described in detail. Tell me, please, do I understand the algorithm correctly? And if not, tell me, please, where I'm wrong. Store the experience in the replay memory (size N), until it is full. When memory is full, fill the buffer (size B) with random samples from memory. Select mini-batches (size M) from the buffer, accumulate the gradient and update weights (when the accumulated gradient = M), until the buffer is empty. That is, to update the weights, we use not all the memory, but only a part (do random sampling from the memory for training after each episode). And when replay memory is full, overwrite the old values.
