[site]: crossvalidated
[post_id]: 631002
[parent_id]: 
[tags]: 
Batches in policy gradient methods â€“ theory vs practice

I am currently trying to understand the implementation of batching in policy gradient / actor-critic methods. My understanding is that these methods in principal work as follows: collect a batch of $N$ trajectories $\{\tau_i\}$ of length $T_i$ and optimise the policy by following the policy gradient: $$ \nabla_\theta J(\theta) \approx \frac1{N}\sum_{i=1}^N \sum_{t=1}^{T_i} \nabla_\theta \log \pi_\theta(a_t|s_t) \hat A(s_t). $$ For example in A2C, $N$ would be the number of threads that simultaneously execute the policy in different environments and $T_i$ is the number of environment steps we perform before updating our policy (related to our method of advantage estimation). However, it seems that in practice most implementations do not actually collect a distinct batch of trajectories; instead they simply keep an experience buffer of tuples $(s_t,a_t,r_t,s_{t+1})$ . Once the desired number of environment steps has been reached, they then update the policy by performing a simple mean over the experience. For example, this is the relevant code in the stable-baselines3 A2C implementation ( link ): # Policy gradient loss policy_loss = -(advantages * log_prob).mean() A similar loss implementation can be found in OpenAI's Spinning Up VPG ( link ). To me this seems like it does not actually compute the proper policy gradient since it is taking the mean over the entire experience, i.e. it instead computes $$ \nabla_\theta J(\theta) \approx \frac1{\sum_{i=1}^N T_i}\sum_{i=1}^N \sum_{t=1}^{T_i} \nabla_\theta \log \pi_\theta(a_t|s_t) \hat A(s_t). $$ Am I correct or am I missing something? If my interpretation is correct, why do these implementations compute the mean over everything? I guess it maybe does not make too much different in practice, since this is simply a rescaled version of the gradient, but on the other hand it seems that when the $T_i$ are very different (for example due to early episode termination) taking the mean over the entire experience is incorrect. I would appreciate any insights or any pointers if I have misunderstood something!
