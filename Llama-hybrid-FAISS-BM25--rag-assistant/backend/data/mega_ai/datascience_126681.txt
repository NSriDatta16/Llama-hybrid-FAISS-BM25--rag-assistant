[site]: datascience
[post_id]: 126681
[parent_id]: 63021
[tags]: 
I don't think you are too far off. Here's Geoff Hinton explaining his motivation for using the phrase 'hidden' in his work on neural nets in the late 80's: The reason hidden units in neural nets are called hidden units is that Peter Brown told me about hidden Markov models. I decided "hidden" was a good name for those extra units, so that's where the name "hidden" comes from. (p. 379) In the same article he also had this to say about the influence of statistical methods on the development of neural nets: There's been a lot more use of the underlying algorithm in hidden Markov models; the EM algorithm is used much more in neural nets now. There's been a lot of transfer of ideas from statistics into neural networks. I think there's also transfer the other way. I think it's a very fruitful interaction, so that's something very good that's happened. (pp. 382-383) Source: https://direct.mit.edu/books/book/4886/chapter/622920/Geoffrey-E-Hinton The context of how "hidden" is used in LSTM specifically is slightly different. In the technical paper describing LSTM, Schmidhuber and Hochreiter repeatedly refer to the "conventional" or "standard" hidden units (see pg. 7, 8)- but notice, not hidden layers . This is used to show the difference between LSTM cells and "conventional" hidden units. Certainly by this point the usage of "hidden" in the neural nets world was rampant to refer to the hidden units, a la Hinton. Schmidhuber and Hochreiter also explicitly state: In contrast to finite state automata or hidden Markov models LSTM does not require an a priori choice of a finite number of states. In principle it can deal with unlimited state numbers. (p. 23) Source: https://www.bioinf.jku.at/publications/older/2604.pdf To summarize, the intuition behind them is specific to LSTM: what is hidden is the nets learned state of what data/sequences matter and what doesn't. A HMM on the other hand assumes further (hidden/latent) Markov models nested inside it. The context is slightly different, although Hinton's comment on the flow between statistics and neural networks is apt.
