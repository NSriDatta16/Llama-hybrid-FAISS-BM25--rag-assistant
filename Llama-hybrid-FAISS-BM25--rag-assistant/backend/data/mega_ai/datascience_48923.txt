[site]: datascience
[post_id]: 48923
[parent_id]: 
[tags]: 
How to calculate Average Precision for Image Segmentation?

If I've understood things correctly, when calculating AP for Object Detection (e.g. VOC, COCO etc) the procedure is: collect up all the detected objects in your dataset sort the detections by their confidence score categorise each detection as True Positive or False Positive, by comparing the Intersection over Union with a Ground Truth object to a pre-set threshold plot Precision $\frac{TP}{n}$ against Recall $\frac{n}{N}$ , where n is the number of objects in the list that have been considered so far, and N is the total number of objects integrate Precision with respect to Recall. (There are various different ways to perform the integration.) When I attempt to replicate these steps for Segmentation, I found that my segmentation CNN didn't provide the confidence as an output. Even if it did, it would presumably be for each individual pixel. So I am stuck at step 2. Calculating AP without sorting by confidence will obviously change the result. But is it still "valid" in some sense? If not, is there a roughly equivalent metric I could use to compare segmentation results? (Or perhaps more generally, a metric for detection where ranking is not possible?) Edit: looking at VOCdevkit , it seems that they use accuracy $\frac{TP}{TP+FP+FN}$ rather than AP as the metric to evaluate segmentation. Is that what I should be doing? It seems to me that AP is the "better" metric, so I would prefer to use something as close to that as possible. Looking at the Berkeley Simultaneous Detection and Segmentation code , and the accompanying paper , they calculate a pixel-wise AP (called $AP^r$ ), but it seems like they have a confidence score for each object.
