[site]: crossvalidated
[post_id]: 518191
[parent_id]: 468606
[tags]: 
Seems that some people are a bit confused by your question because of its title; indeed, RBF neural networks exist, but they are a different architecture than the traditional multi-layer NNs. The body of your question is complete and formulated well (apart from some ambiguity because of the use of MLP ), so to get to the answer let me ask a slightly different question: why are some activation functions more popular than others? Why are there only some activation functions in use? There is an infinite number of functions that are not used as activation functions in multi-layer NNs. One could think about some activation function that is a slight modification of the popular one, for example by squaring the argument, and ask "Why this kind of a function is not widely used as an activation function?" In multi-layer NNs, each layer transforms the space in some (usually non-linear) way. When we use a non-monotonic activation function, we can imagine how a single layer of neurons works. But what would happen if we used another layer with a non-monotonic activation function transforming the outputs of the first layer? What if we added yet another such layer? As you can imagine, the transformation of the original space gets more sophisticated and twisted more quickly compared to if we used a monotonic activation function. The training process may also be more difficult because the error landscape is more complicated. Calculating the derivatives may be less efficient depending on the formula of the activation function. So it is not like you cannot use a Gaussian as an activation function in a multi-layer NN. You can use it, and you can use other unpopular activation functions, and for a particular data and a particular NN topology they may even be more efficient and yield better predictions than those provided by the most popular activation functions. The question is: Why use this function and not the other? Why is this particular activation function beneficial? How is this function better from the other one that works well in a general case, hence it became so popular? In a similar vein, you can use different activation functions (other than the most popular Gaussian) in the RBF NN architecture, and some of them will make more sense (and will perform better) than others. This reminds me of another question that I want to bring up here just for illustration: how many layers in a NN you should use? We know that a single hidden layer is sufficient. So why people use more hidden layers? For efficiency, because of the lower total number of neurons needed when more layers are introduced? For better generalization? This demonstrates that your goal may be achieved in many ways, and it is good to first define some evaluation criteria to be able to measure the performance of different possible approaches and compare them. Coming back to the original question "Why aren't [traditional] neural networks used with RBF activation functions (or other non-monotonic ones)?" â€“ They are, but if one wants to use many such layers, given the potential problems I mentioned above, one should justify this decision by stating "This activation function is better than sigmoid or ReLU because..."
