[site]: crossvalidated
[post_id]: 31441
[parent_id]: 30474
[tags]: 
You are not the first person who does something like this. Researches have done this for image recognition for about 15 to 20 years. An example for this is the MNIST data set of handwritten digits . The data set usually consists of 60,000 training examples. But this number is not sufficient to reach >99,5 % accuracy with multilayer perceptrons. So, people generate more training examples with distortions in each iteration of the optimization algorithm. The algorithm they usually use to train the neural networks is called stochastic gradient descent (or online learning in comparison to batch learning ). There exist variants like "stochastic diagonal Levenberg-Marquardt" that require an approximation of the Hessian. The averaging of weights could produce a classifier that is really bad.
