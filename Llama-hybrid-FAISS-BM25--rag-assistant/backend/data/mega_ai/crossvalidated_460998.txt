[site]: crossvalidated
[post_id]: 460998
[parent_id]: 460993
[tags]: 
The distinction between probability density and probability of an event has little to relate with Bayesian statistics. Given a random variable $X$ , with distribution $P^X$ , this distribution can be characterised by the definition of $P^X(A)$ for all measurable sets $A$ , or equivalently by the definition of a density $p^X$ wrt a measure $\text{d}\mu$ if the distribution is absolutely continuous wrt this measure, the connection being that $$P^X(A) = \int_A p^X(x)\text{d}\mu(x)$$ For instance, if $\text{d}\mu$ is the Lebesgue measure and $P^X$ the Uniform distribution on $(-1,1)$ , $$P^X([0,1])=\int_0^1 \frac{1}{2}\text{d}x=\frac{1}{2}$$ When dealing with conditional distributions , things are pretty much the same, namely that given $m=0$ and $0\le a\le 1$ , $b$ is a random variable with a probability distribution denoted by e.g. $$P^b(\cdot|0\le a\le 1,m=0)$$ which is associated with a probability coverage of all measurable sets $A$ in $[-1,1]$ , $$P^b(A|0\le a\le 1,m=0)$$ and which enjoys a probability density $$p^b(\cdot|0\le a\le 1,m=0)$$ wrt the Lebesgue (uniform) measure on $[-1,1]$ , such that $$P^b(A|0\le a\le 1,m=0)=\int_A p^b(x|0\le a\le 1,m=0)\text{d}x$$ The only part where Bayes has a say is in defining this conditional distribution , since Bayes' theorem states that $$P^b(A|0\le a\le 1,m=0)=\frac{P^b(A,0\le a\le 1|m=0)}{P^b(0\le a\le 1|m=0)}$$
