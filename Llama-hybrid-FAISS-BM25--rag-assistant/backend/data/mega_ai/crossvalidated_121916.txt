[site]: crossvalidated
[post_id]: 121916
[parent_id]: 
[tags]: 
Why are mixed data a problem for euclidean-based clustering algorithms?

Most classical clustering and dimensionality reduction algorithms (hierarchical clustering, principal component analysis, k-means, self-organizing maps...) are designed specifically for numeric data, and their input data are seen as points in a euclidean space. This is a problem of course, as many real-world questions involve data that are mixed: for instance if we study buses, the height and length and motor size will be numbers, but we might also be interested in color (categorical variable: blue/red/green...) and capacity classes (ordered variable: small/medium/large capacity). Specifically, we might want to study these different types of variables simultaneously. There are a number of methods to extend classical clustering algos to mixed data, for instance using a Gower dissimilarity to plug into hierarchical clustering or multidimensional scaling, or other methods that take a distance matrix as input. Or for instance this method, an extension of SOM to mixed data. My question is: why can't we just use the euclidean distance on mixed variables? or why is it bad to do so? Why can't we just dummy-encode the categorical variables, normalize all variables so that they have a similar weight in the distance between observations, and run the usual algos on these matrices? It's really easy, and never done, so I suppose it's very wrong, but can anyone tell me why? And/or give me some refs? Thanks
