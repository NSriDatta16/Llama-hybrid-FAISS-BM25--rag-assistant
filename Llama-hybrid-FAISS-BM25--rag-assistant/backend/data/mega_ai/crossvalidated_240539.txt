[site]: crossvalidated
[post_id]: 240539
[parent_id]: 
[tags]: 
Intuition behind output neuron error signal in backpropagation

I have a potentially obvious question about neural network error backpropagation. In Andrej Karpathy's blog post on neural networks, he goes through an example of backpropagation. When calculating the error gradient at the output of the network (which is then backpropagated to get the derivative of the output error with respect to each weight), he does: dscores = probs dscores[range(num_examples),y] -= 1 dscores /= num_examples, where probs is an array where each row represents an output class probability distribution for a given training observation, num_examples is the number of training observations, and y is the output class (which corresponds to an index of the dscores array. Can anyone briefly explain why he divides each observation by the number of training examples? I'm struggling to understand this.
