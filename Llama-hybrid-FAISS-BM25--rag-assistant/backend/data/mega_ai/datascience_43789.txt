[site]: datascience
[post_id]: 43789
[parent_id]: 43787
[tags]: 
We usually say a model is overfitting, not a layer or a neuron. Basically, there is not a standard way to set the number of convolutional layers. Iff you see your model is not learning well enough, increase the number of filters. Iff it is overfitting, decrease the number of filters or use the common approaches like drop out to overcome the problem. There are standard ways for interpreting the number of neurons is dense layers. Take a look at How to set the number of neurons and layers in neural networks . If by activation normalisation you mean batch normalisation , there is an answer. Batch normalisation is a technique for avoiding covariat shift. It does not allow the distribution of activations to change. It is needed for very deep networks and it does help. If you have a very deep network, use it and you will see that it is very helpful most of the time. Simple networks may be able not to have them. These are usual facts but you may be able not to use them in dense layers, something like AlexNet, and still have a good result.
