[site]: crossvalidated
[post_id]: 608517
[parent_id]: 
[tags]: 
Why is the accuracy of a Decision Tree decreasing whereas the accuracy of an LSTM is increasing when adding augmented data?

I am using sklearn's DecisionTreeClassifier and LSTMs (Keras) for time series classification. To increase the accuracy and robustness of the models I augmented the training data set with jittered, interpolated, and warped data. The accuracy of the decision tree decreases from 89.35% to 88.12% whereas the accuracy of the LSTM increases from 92.61% to 94.32%. Is there a reason for that? The maximum depth of the Decision Tree is 16 and the criterion is entropy tree.DecisionTreeClassifier(criterion='entropy', max_depth=16) and the following LSTM configuration model_2_0 = tf.keras.Sequential([ layers.LSTM(64, activation="relu", return_sequences = True, input_shape= (win_length, num_features)), layers.Dropout(0.2), layers.LSTM(64, activation="relu", return_sequences=True), layers.Dropout(0.2), layers.LSTM(64, activation="relu"), layers.Dropout(0.2), layers.Dense(23, activation="softmax") ], name=model_name) The data sets were really large, to begin with. I am not quite sure how to interpret these results. What could be the reason here?
