[site]: crossvalidated
[post_id]: 123373
[parent_id]: 123080
[tags]: 
Typically parallel versions do some form of stochastic gradient descent, in which SGD steps for different data instances are spread across nodes (map) and then the partial results are summed up (reduce). This simple strategy can be used to parallelize many algorithms. This NIPS paper is a very good read on parallelizing machine learning algorithms (including NN) using MapReduce. Quoting from this paper: By defining a network structure (we use a three layer network with two output neurons classifying the data into two categories), each mapper propagates its set of data through the network. For each training example, the error is back propagated to calculate the partial gradient for each of the weights in the network. The reducer then sums the partial gradient from each mapper and does a batch gradient descent to update the weights of the network.
