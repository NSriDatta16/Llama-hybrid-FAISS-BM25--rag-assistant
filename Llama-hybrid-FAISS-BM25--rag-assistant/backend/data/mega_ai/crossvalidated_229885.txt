[site]: crossvalidated
[post_id]: 229885
[parent_id]: 
[tags]: 
What's the recommended weight initialization strategy when using the ELU activation function?

For deep neural networks using ReLU neurons, the recommended connection weight initialization strategy is to pick a random uniform number between -r and +r with: $r = \sqrt{\dfrac{12}{\text{fan-in} + \text{fan-out}}}$ Where fan-in and fan-out are the number of connections going in and out of the layer being initialized. This is called "He initialization" ( paper ). My question is: what's the recommended weights initialization strategy when using ELU neurons ( paper )? Since ELUs look a lot like ReLUs, I'm tempted to use the same logic, but I'm not sure it's the optimal strategy. Note There is a fairly similar question but this one is more specifically about the ELU activation function (which is not covered by the answers to the other question).
