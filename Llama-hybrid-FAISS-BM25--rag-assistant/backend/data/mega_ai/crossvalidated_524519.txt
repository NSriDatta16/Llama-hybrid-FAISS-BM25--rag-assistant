[site]: crossvalidated
[post_id]: 524519
[parent_id]: 524339
[tags]: 
In my understanding MC-Dropout is for testing only. Arguably, though, its a final step in inference, you do it once right at the very end to generate an ensemble. So, it can be included in inference in a backhand way as the final step. MCMC and variational methods are for inference. The "posterior" of a variable is an abstract theoretical concent that exists in principle, and with deep neural networks we never get to represent it fully. MCMC and variational methods are alternative methods for (approximately) sampling from the posterior or approximating the posterior. Parameter learning is not really something that fits neatly into the Bayesian framework. The best you can do is approximate, sample, estimate. But the results of typical deep learning, because its always stochastic, can be viewed as a sample from the posterior. For this reason, a very simple approach for ensembling is to train 5 networks independently (with different batch presentations). But this is 5 times slower than normal, and why MC-dropout was invented.
