[site]: datascience
[post_id]: 118818
[parent_id]: 118804
[tags]: 
"Interpretable" is not very precise in this context. In the case of deleting a dense layer, the embedding layer is more likely can learn the nontask dependent co-occurrences of words in the dataset. In the second case of adding more data, the embedding layer would learn more signals because there is an increased opportunity to "average out" the noise. In other words, word embeddings are more generalizable by reducing the complexity of the architecture and training on more data.
