[site]: datascience
[post_id]: 65105
[parent_id]: 65089
[tags]: 
I think these common answers are quite right but abstract. In my opinion, pooling "select the most important feature" or "increase the receptive field" by droping nearby information, such as Maxpooling. This may makes sense because in some case like image classification we just need some important feature to classify, hence we could drop redundant local feature. Or by AveragePooling, we can make the feature maps more stable to local change. The last FC layer did select the most important feature to use, however, if every layer's output is more useful for the final task, you can get a more accruate result, we want the former network to learn reliable features, FC layer just map them. As for your second idea, the receptive field could be increased by increasing the kernel size. But if you use larger kernel size, you lose local features. That's why dialated convolution is proposed, it can increase receptive field without losing information as pooling. By the way, many researcher don't use pooling or just use them at the end of the network recent year, as Hinton said: The pooling operation used in convolutional neural networks is a big mistake and the fact that it works so well is a disaster.
