[site]: crossvalidated
[post_id]: 573597
[parent_id]: 
[tags]: 
Validation accuracy does not reflect the actual change in the testing accuracy

When training a CNN classifier for the Fashion-MNIST, I have noticed that there are multiple instances that validation error is not improving even if the tesing accuracy is improving every epoch or vice versa. Indeed, I have epochs that have 90% testing accuracy however the validation accuracy did not improve and the weights of the network was not saved. Another epochs from that one have an 86% testing accuracy but the validation accuracy improved! This discourages me from using the validation accuracy as a callback in the implementation. It seems like the only use of validation accuracy is an upper bound for the testing accuracy. Is there a better way for measuring the change in the testing accuracy? It would be even better if a lower bound exists.
