[site]: crossvalidated
[post_id]: 223250
[parent_id]: 223246
[tags]: 
Your feature selection criterion fisher-ratio > 0.2 has an implicit parameter which you have fixed to a constant. It would be better to view the 0.2 part as a free tuning parameter fisher-ratio > lambda As you say, when you use your criterion for a fixed value of 0.2 , the resulting model is always dominated by a model with less predictors. You should be able to recover this information from a better training procedure. If you think of lambda as a parameter to be tuned in the algorithm, then cross validation will help you determine a good value of lambda . To do this, create a grid of lambda you are willing to consider, say [.1, .2, .3, ...] and then train your algorithm on all your folds for each such value of lambda. Then, the average out of fold error for each such lambda you used is an estimate of the out of sample error of your model when features are selected according to the criterion fisher-ratio > lambda . This will let you (approximately) select the feature selection strength that results in the best model. In your case, if everything works out consistently, this should validate your suspicion that you should select a small number of powerful features.
