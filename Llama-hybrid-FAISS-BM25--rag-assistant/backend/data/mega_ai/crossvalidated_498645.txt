[site]: crossvalidated
[post_id]: 498645
[parent_id]: 498641
[tags]: 
The prior and posterior distributions are distributions over your parameters/latent variables, not your data. So in this case, the prior would be $p(z)$ , and the posterior is $p(z|x)$ . I have never seen the posterior referred to as the evidence. Sometimes, you may see the marginal likelihood $p(x)$ referred to as the model evidence. As mentioned above, denote the log model evidence as the log of the marginal likelihood of the data: $$\log p(x) = \log \int_z p(x, z)dz.$$ We can do some simple algebra and apply Jensen's inequality: \begin{align*} \log p(x) &= \log \int_z p(x, z)dz \\ &= \log \int_z p(x, z)\frac{q(z)}{q(z)}dz \\ &= \log \mathbb{E}_q\left[\frac{p(x, z)}{q(z)}\right] \\ &\geq \mathbb{E}_q\left[\log \frac{p(x, z)}{q(z)}\right] \\ &= \mathbb{E}_q[\log p(x, z)] - \mathbb{E}_q[\log q(z)]. \end{align*} This last term is the ELBO. As you can see, we have $\log p(x) \geq \text{ELBO}$ , meaning that the ELBO lower-bounds the model evidence. Sometimes, the purpose of a variational autoencoder is to compute a low dimensional approximation to our data that still achieves good reconstruction accuracy. If this is your goal, then $p(z|x)$ is the important part, since you don't care about reconstructing the input, you just care about a good approximation. However, variational autoencoders can also be used as generative models to simulate new data that was not part of the original dataset. In this case, you would be more interested in the decoder $p(\tilde{x} | z)$ .
