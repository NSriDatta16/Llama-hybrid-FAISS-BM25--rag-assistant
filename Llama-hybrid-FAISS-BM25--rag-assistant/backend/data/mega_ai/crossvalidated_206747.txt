[site]: crossvalidated
[post_id]: 206747
[parent_id]: 
[tags]: 
2SLS with heterogeneous first-stage

Suppose I have the following model, where $c$ indexes group and $i$ indexes members within a group. The sample is made up of $(i,c)$ pairs. The model is: $$ y_{ic} = \alpha + \beta_x x_i + \epsilon_i \\ x_{ic} = \gamma + \beta_z^c z_i + \nu_i $$ I am interested in obtaining an estimate of $\beta_x$. Suppose that $\mathrm{corr}(x_i,\epsilon_i) \ne 0 , \mathrm{corr}(z_i,\epsilon_i) = \mathrm{corr}(z_i,\nu_i) = 0$ and $\beta_z^c \ne 0$ for all $c$. Then OLS on the first equation is inconsistent. A consistent estimate of $x$ can be obtained by using two-stage least squares, with $z_{ic}$ instrumenting for $x_{ic}$. The effect of $z_{ic}$ on $x_{ic}$ depends on the group $c$. The right thing to do seems to be to estimate the first-stage separately for each group, then estimate the second stage pooling all observations. (1) If I estimate the first stage by pooling all observations, is the second stage estimate $\hat{\beta_x}$ consistent? (2) How is the estimate $\hat{\beta_x}$ affected by weighting schemes in the first stage? If we weight a given observation $(i,c)$ by the inverse number of observations in that group $c$, the pooled estimate $\beta_z^{pool}$ converges in probability to the unweighted average of $\beta_z^c$ across $c$. If we do not weight, it converges to a weighted average, tilted towards $\beta_z^c$ from groups $c$ with many observations $(i,c)$. Since in the second stage, relatively more observations come from those same groups $c$, it seems like this offsets at least some of the damage caused by pooling in the first stage. Maybe all, hence my question (1). (3) In finite samples, provided $\beta_z^c$ does not vary too much across groups $c$, shouldn't pooling in the first stage provide better estimates? Intuitively, if $\beta_z^c$ is relatively constant across groups, observations in one group provide information about the coefficient in the other groups. How should I think about this tradeoff? Thank you.
