[site]: datascience
[post_id]: 97020
[parent_id]: 
[tags]: 
Memorization in deep neural networks, random vs. properly labelled datasets

From about 19:20 in the video here: https://www.youtube.com/watch?v=IHZwWFHWa-w it shows the difference in value of the cost function for randomly labelled data vs. properly labelled data. What do they mean by randomly labelled? That the labels change on every batch or epoch? Or that they are just assigned some random label which are then fixed throughout training? Then how is it possible for a classification network with a softmax layer for the output to do any better on the properly labelled data? They are just labels and meaningless to the network it seems.
