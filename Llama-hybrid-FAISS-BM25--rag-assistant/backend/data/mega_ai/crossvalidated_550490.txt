[site]: crossvalidated
[post_id]: 550490
[parent_id]: 460161
[tags]: 
Fixed length BERT, same as Transformer, use attention as a key feature. The attention as used in those models, has a fixed span as well. Cannot reflect relative distance We assume neural networks to be universal function approximators . If that is the case, why wouldn't it be able to learn building the Fourier terms by itself? Why did they use it? Because it was more flexible then the approach used in Transformer. It is learned, so possibly it can figure out by itself something better--that's the general assumption behind deep learning as a whole. It also simply proved to work better.
