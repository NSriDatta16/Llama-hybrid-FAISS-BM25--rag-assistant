[site]: crossvalidated
[post_id]: 286723
[parent_id]: 
[tags]: 
Is manually tuning learning rate during training redundant with optimization methods like Adam?

I have seen some high-profile deep learning papers where an optimization method like Adam was used, yet the learning rate was manually changed at specific iterations. What is the relationship between the adaptivity provided by adaptive optimization methods and manually tuning the learning rate? Would it still make sense with Adam to, for example, lower the learning rate after not seeing improvement for a number of iterations?
