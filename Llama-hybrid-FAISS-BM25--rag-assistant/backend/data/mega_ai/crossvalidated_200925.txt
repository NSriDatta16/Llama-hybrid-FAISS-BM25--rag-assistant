[site]: crossvalidated
[post_id]: 200925
[parent_id]: 
[tags]: 
How to use a Bayesian parameter estimate?

This might be a naive question, but I still ask it since I couldn't easily find an answer to it. Suppose we have an unknown parameter, $\theta$, and some function $\theta \mapsto f(\theta)$. Suppose also that we have an a priori distribution for the value of $\theta$ Finally, we assume for simplicity that we have not obtained any information yet, so our posterior distribution equals the apriori distribution. So our best estimate of $\theta$ is $\tilde{\theta} = E[\theta]$. As I understand it, the typical thing to do in Bayesian statistics in order to compute $f$, and please correct me if I'm wrong, is now to simply evalute $f(E[\theta]) = f(\tilde{\theta})$. However, wouldn't a more correct thing to do be to actually compute $E[f(\theta)]$? If $f$ is convex for example, it follows that the latter is larger than the former, hence there will be a systematic difference between the two variants. A reason why I'm asking this is that I have a model where computing $f(\theta)$ for given $\theta$ is expensive, so actually computing $E[f(\theta)]$ is far from desirable. Edit: Ok, I just read about posterior predictive distributions and it seemed that I was probably wrong in my assumption that one merely used the expected value as the "true" parameter.
