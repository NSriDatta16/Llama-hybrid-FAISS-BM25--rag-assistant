[site]: crossvalidated
[post_id]: 51748
[parent_id]: 
[tags]: 
Confusion related to convex optimization of a function in a svm paper

I was reading this paper http://www.ist.temple.edu/~vucetic/documents/wang11kdd.pdf related to adaptive multi-hyperplane machine for non linear classification In that paper, they have mentioned about multiclass SVM, with multiple weights for each class. The loss for any classification is $l(x_n,y_n) = max_{i\epsilon y\\\y_n}(0,1 + max g(i,x_n) - g(y_n,x_n))$ where $y_n$ is the label for the nth example and $x_n$ is the features. I have this confusion when they do the training of this algorithm. They call this SVM MM(Multiple Hyperplane). They say the convex-approximated problem is defined as $min_{W}P(W|z) = \frac{\lambda}{2}||W||^2 + \frac{1}{N}\sum_{n=1}^{N}l_{cvx}(W;(x_n,y_n);z_n)$ where they have the concave term $-g(y_n,z_n)$ replaced with the convex term $-w^T_{y_n,z_n}x_n$. I am not sure if I have described it clearly. But I am going to attach the screenshot of the paper as well. The thing is I didn't get what's the difference between $-g(y_n,z_n)$ and $-w^T_{y_n,z_n}x_n$. They seem the same term to me. I might be asking a lot. But can anyone provide some info? I have marked by the red rectangle the part that I didn't understand. I might be asking a lot. But I didn't get that part. Why is it so?
