[site]: crossvalidated
[post_id]: 307219
[parent_id]: 
[tags]: 
Training an agent to move to a goal location

Suppose I have collected some data, where each datum is an interaction of an agent with an environment. From this, I then train a neural network to learn a dynamics model, and to predict the next state $s_{t+1}$ given state $s_t$ and action $u_t$. What I want to do now is, for any given initial state $s_0$, predict the optimum sequence of actions $u_t, ..., u_{t+n}$ that will cause my agent to end up in state $s_{goal}$. How can I do this? I'm not very familiar with this field, but one idea is to formulate it as a Markov Decision Process (MDP). I would give a positive reward depending on the final distance to the goal, and small negative reward for each action, to encourage the agent to reach the goal quickly. I would then solve the MDP using Value Iteration. Would this make sense? Or does this only apply for discrete states? (My state and action spaces are continuous.) Or are there any other ways to solve this? Thanks.
