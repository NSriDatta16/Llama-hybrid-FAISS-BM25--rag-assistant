[site]: crossvalidated
[post_id]: 440686
[parent_id]: 
[tags]: 
Is this kind of stacked ensemble method prone to over-fitting?

I am working on a stacked ensemble method. I trained three classifiers as my first-layer models and one Logistic Regression as my second layer model. I then stacked both the first-layer models and second-layer model and trained it as a Stacked Classifier. Now, I get a perfect score of 1.0 when I measure performance on my test set. This makes me suspicious. Can you tell me, are these kind of models prone to over-fitting? My code looks something like this: # Creating the first-layer models clf_knn =KNeighborsClassifier(n_neighbors= 5, algorithm= 'ball_tree') clf_decision_t = DecisionTreeClassifier(min_samples_leaf = 5, min_samples_split = 15, random_state=500) clf_naive_b = GaussianNB() # Creating second-layer model (meta-model) clf_logistic_r = LogisticRegression() # Creating and fitting the stacked model clf_stack = StackingClassifier(classifiers=[clf_knn, clf_decision_t , clf_naive_b ], meta_classifier=clf_logistic_r ) clf_stack.fit(X_train, y_train) # Evaluate the stacked modelâ€™s performance print("Accuracy: {:0.4f}".format(accuracy_score(y_test, clf_stack.predict(X_test)))) I used scikit and mlxtend libraries
