[site]: crossvalidated
[post_id]: 262774
[parent_id]: 262752
[tags]: 
Recurrent networks can indeed learn features. What these features represent depends on the architecture and the layer being considered. Activations in the recurrent layer at time $t$ are a function of the entire input sequence up to $t$. You could consider them to be features representing this sequence history. But, they aren't features representing the instantaneous input, because they generally can't be expressed as a function of the instantaneous input alone. This also holds for all layers downstream of a recurrent layer, even if they themselves aren't recurrent. Consider an architecture where the input feeds into the recurrent layer via some number of intermediate, feedforward hidden layers. Activations in these first feedforward layers are a function of the current input alone. So, these activations can be considered features of the instantaneous input (but not of the sequence history). RNNs do support unsupervised learning. RNN-based autoencoders are a prominent example, as @Aaron mentioned in the comments. In general, you can implement unsupervised learning by training to minimize a loss function that doesn't depend on any auxiliary labels/known outputs.
