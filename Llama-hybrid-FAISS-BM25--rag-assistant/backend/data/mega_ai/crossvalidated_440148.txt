[site]: crossvalidated
[post_id]: 440148
[parent_id]: 
[tags]: 
Output of xgboost() while optimization is not very intuitive

I am running xgboost() on a data set with a data set with below columns. `data.frame': 24045 obs. of 32 variable $ LanID : Factor w/ 14853 levels "ADV1610403456773583",..: 520 6286 4471 8102 8215 6871 3404 6151 1417 268 ... $ Region : Factor w/ 4 levels "EAST","NORTH",..: 4 4 4 3 3 3 2 3 3 4 ... $ Loan.City : Factor w/ 7 levels "Bangalore","Chennai",..: 5 7 6 1 1 1 3 1 1 5 ... $ Loan.Type : Factor w/ 6 levels "EMI Free Loan",..: 2 2 1 5 2 5 5 3 6 6 ... $ Loan.Scheme : Factor w/ 4 levels "others","Paytm Term Loan",..: 1 1 4 4 3 4 4 4 4 4 ... $ Loan.Status : Factor w/ 2 levels "active","closed": 1 1 1 1 1 1 1 1 1 1 ... $ Last.EMI : num 2512 1479 4500 5875 4992 ... $ All.Dues : num 2692 1479 0 0 67048 ... $ Bullets.Overdue : num 0 0 30000 0 0 0 0 0 0 0 ... $ Loan.Quality : Factor w/ 7 levels "Closed","Doubtful",..: 6 6 6 6 7 6 6 6 6 4 ... $ Tenure : num 8 24 24 23 14 23 24 24 14 8 ... $ Gender : Factor w/ 2 levels "female","male": 1 2 2 2 2 2 2 2 2 2 ... $ Educational.Qualification: Factor w/ 6 levels "graduate","Not Available",..: 2 4 4 4 1 1 1 1 2 2 ... $ Marital.Status : Factor w/ 3 levels "married","single",..: 2 1 1 1 2 2 1 1 2 2 ... $ Employment.Year : Factor w/ 4 levels "less-than-2-years",..: 4 3 3 3 1 2 3 1 4 4 ... $ Age : num 21 33 34 42 33 26 41 41 28 28 ... $ relevant_pos : num 23238 100000 300000 165893 89366 ... $ crif_active_accounts : num 0 10 11 18 9 0 11 13 0 0 ... $ crif_overdue_amt : num 0 0 0 0 0 0 0 0 0 0 ... $ crif_current_outstanding : num 0 3696499 8722449 519849 115283 ... $ cibil_active_accounts : num 0 10 12 0 0 0 12 0 0 0 ... $ cibil_overdue_amt : num 0 0 0 0 0 0 0 0 0 0 ... $ cibil_current_outstanding: num 0 3738994 8738488 0 0 ... $ NACH.status : Factor w/ 6 levels "active","nach-registration-initiated",..: 4 1 1 4 1 1 4 1 4 4 ... $ Tenure.End : Factor w/ 4 levels "Closed","No",..: 2 2 2 2 2 2 2 2 2 2 ... $ LastMonthBnc : Factor w/ 6 levels "1","2","3","4",..: 1 1 2 1 1 2 1 2 2 1 ... $ Last.Month.delinq : Factor w/ 7 levels "1","2","3","4",..: 6 6 6 6 6 6 6 6 6 6 ... $ CIBIL.Bracket : Factor w/ 10 levels "-1","490-600",..: 10 4 6 10 1 5 10 5 10 10 ... $ Salary.Bracket : Factor w/ 10 levels "125K-200K","30k-40k",..: 9 5 7 4 6 2 1 1 5 9 ... $ DELINQ.NON.DELINQ : Factor w/ 2 levels "DELINQ","NON DELINQ": 2 2 2 2 1 2 2 2 2 2 ... $ Month : num 9 9 10 6 9 10 6 10 10 6 ... $ state : Factor w/ 2 levels "0","1": 1 1 1 1 1 1 1 1 1 1 ...` Above i have converted the data(train-test into matrix format(as it is must for xgboost(). library("Matrix") sparse_matrix When i am trying to optimize the algorithm with different values, output is not very intuitive to me. Below are the match of actual and predicted class outputs(predicted class 1 = 203 and actual are also 203). [,1] [,2] [,3] [,4] [,5] [1,] 203 203 203 203 203 for nrounds = 2, 50, 100, 1000, 10000 221,199,203,206,189,178 for max_depth = 1,3,5,7,9,15 And [,1] [,2] [,3] [,4] [,5] [,6] [,7] [1,] 221 221 221 221 221 221 2 for eta = 0.001, 0.01, 0.1, 0.3, 0.5, 0.7, 1 So by Max depth what i could understand that algorithm was easily able to discriminate the classes in initial splits(and so need not to go for high depth) but not sure why with increase in depth correct predictions are going down. Predicted probabilities for 2 classes are also pretty close. table(xgb.pred.class) xgb.pred.class 0.495001137256622 0.495651245117188 0.496811062097549 0.503574728965759 9582 875 41 415 One point to note that on same dataset i tried GLM(), NB() and KNN() for all identification of minority class(like 221 in above case) was extremely poor(could identify only 4-5% of cases correctly for minority class) however with xgboost() it was able to identify almost 85-90% cases correctly(not sure why it is performing so good than others). I want to be sure that i am not making any mistake(or missing some hidden indications by data).
