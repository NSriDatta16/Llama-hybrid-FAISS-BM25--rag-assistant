[site]: crossvalidated
[post_id]: 439517
[parent_id]: 438508
[tags]: 
Answer: Your code seems to be in line with the cited paper, and the incosistency is an artefact of non-standard definition of principal components used in scikit-learn of versions 0.22 (this is mentioned in the deprecation message). You can get meaningful results by setting normalize_components=True dividing the entries of the variance array by the number of samples, 505. This gives you explained variance ratios like 0.90514782, 0.98727812, 0.99406053, 0.99732234, 0.99940307 . and 3. The most immediate way is to check the source files of the sklearn.decomposition on your computer. Details: The code of SparsePCA , as in scikit-learn=0.21.3 , has an unexpected artefact: as is returns a transformation of inputs such that the $QR$ decomposition has $R$ diagonal with non-zero values in $\{-1, 1\}$ (rounding to 5 digits after the comma, easy to check) and therefore is not reflective of the preserved variance as it should be according to Zou, Hastie and Tibshirani. normalize components is key to getting to the meaningful variance ratios. This argument is only present in scikit-learn=0.21.3 and absent in scikit-learn=0.22 . Set to False it induces a normalization of the outputs that hides the information about the explained variance. One can see the details of this normalization in the source code: class SparsePCA(BaseEstimator,TransformerMixin): ... def transform(self, X): ... U = ridge_regression(...) if not self.normalize_components: s = np.sqrt((U ** 2).sum(axis=0)) s[s == 0] = 1 U /= s return U The deprecation warning reads that this normalization is not in line with the common definition of principal components and is removed in version 0.22 . If you are still using an old version like me, this normalization can be reversed as mentioned in the brief version of the answer.
