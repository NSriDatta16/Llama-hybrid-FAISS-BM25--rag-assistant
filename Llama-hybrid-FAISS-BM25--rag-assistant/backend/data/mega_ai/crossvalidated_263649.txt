[site]: crossvalidated
[post_id]: 263649
[parent_id]: 171043
[tags]: 
I know this is an old question, but I use a different method from the ones above. I use the BayesianOptimization function from the Bayesian Optimization package to find optimal parameters. To do this, you first create cross validation folds, then create a function xgb.cv.bayes that has as parameters the boosting hyper parameters you want to change. In this example I am tuning max.depth, min_child_weight, subsample, colsample_bytree, gamma . You then call xgb.cv in that function with the hyper parameters set to in the input parameters of xgb.cv.bayes . Then you call BayesianOptimization with the xgb.cv.bayes and the desired ranges of the boosting hyper parameters. init_points is the number of initial models with hyper parameters taken randomly from the specified ranges, and n_iter is the number of rounds of models after the initial points. The function outputs all boosting parameters and the test AUC. cv_folds
