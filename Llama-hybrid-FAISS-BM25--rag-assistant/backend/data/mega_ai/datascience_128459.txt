[site]: datascience
[post_id]: 128459
[parent_id]: 
[tags]: 
Why is positional encoding preferable over adding additional features for transformer models?

Why is information about the position not added as an additional feature? I read in forums that the only reason would be length-based overfitting, but I couldn't find a reliable source for that. What if the transformer is used for applications where length-based overfitting is not an issue due to consistent sequence length?
