[site]: datascience
[post_id]: 72051
[parent_id]: 71950
[tags]: 
Sammy and Dave have accurately answered the question that Nielsen intended, but based on your comment on Sammy's answer I think you are wondering whether having arbitrarily many slightly-rotated training images would cause a problem. Having arbitrarily many slightly-rotated would not be problematic; indeed, you could simulate this by generating a batch of images with small random linear transformations (rotation is a type of linear transformation) at each training step, and this approach is common. See for example this post To just to add to the other answers here, there is nothing inherently wrong with putting arbitrarily-rotated images into a neural network . What Nielsen is hoping you will realise is the problem this would cause within the specific domain he's talking about (digit recognition): the problem isn't the rotation, the problem is telling the network that a 180-degree-rotated 9 is still a 9 (when actually it's a 6). If you were training a network to classify types of blood cell, arbitrary rotation of the training images would be a great strategy as the orientation of each cell wouldn't carry any special information.
