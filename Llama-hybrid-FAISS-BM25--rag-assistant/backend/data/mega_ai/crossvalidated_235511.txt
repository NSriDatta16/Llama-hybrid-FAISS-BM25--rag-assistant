[site]: crossvalidated
[post_id]: 235511
[parent_id]: 
[tags]: 
Predicting target variables that fit in some real interval [a, b], typically [0, inf)

I would like to be able to predict the magnitude of changes or returns of a set of time series that are correlated. An obvious example would be share price returns in a given industry. Or, the change in temperature of different neighbourhoods of a given city. While I would also like to able to predict direction, it is the very large return movements that are of interest to me. The might be known to happen more likely at certain times, like at nightfall (say 7pm) the temperature over the next 30 minutes may be known to drop quicker. I can sort of capture this behaviour in my prediction models using binary/dummy variables. However, if I think I can predict the magnitude of the returns more easily than the actual return sign (so the domain is [0, infinity), how can I do this effectively using Even though i'm dealing with time series, I want to build a typical sort of prediction model outside of useless prediction modeling techniques like ARIMA, GARCH etc... 1) typical classification techniques (random forests, logistic regressions, convoluted neural nets. I guess I could set "y" (target) class 0 if the value is between [0, b], and y =1 if between [b, infinity). But how can I choose b effectively? I might want to make 3 target classes, in which case I would be looking for [0, a], [a, b], [b, inf). Are there any standardised statistical approaches/tests (papers? links?) that help determine these cut offs? 2) regression approaches (boosting approaches, random forests, linear regression etc). Here I'm not exactly sure what to do. If my y value is real and between [0, infinity), what do I do if the predicted y value that is output is less than 0? that wouldn't made sense. Are there any truncated or sensored prediction type models I could use? I'm aware of modeling approaches like generalised linear models which can restrict the range of the output (binomial, poisson, gamma, etc), but these models from my experience are great as "Structural" models (explaining relationships), but are weak at prediction compared to methods like GBMs and newer neural net type models. In answering 1) and 2) above, again, there is correlation in the "target" value outputs, so if I know one time series has a value close to 0, then related series are probably small in value for that prediction. In answering this question I would also happily accept any links/references to approaches used by data modelers in the public domain (are there kaggle like competitions which had this kind of problem formulation for which I could get inspiration from in discussion forums (or posted approaches by competitors in the competition)? Thanks for any help in advance...
