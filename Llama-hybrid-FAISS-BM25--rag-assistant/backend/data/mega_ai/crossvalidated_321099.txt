[site]: crossvalidated
[post_id]: 321099
[parent_id]: 270546
[tags]: 
I also had the same question and after reading a couple of posts and materials I think I figured out what embedding layer role is. I think this post is also helpful to understand, however, I really find Daniel's answer convenient to digest. But I also got the idea behind it mainly by understanding the embedding words . I believe it's inaccurate to say embedding layers reduce one-hot encoding input down to fewer inputs. After all the one-hot vector is a one-dimensional data and it is indeed turned into 2 dimensions in our case. Better to be said that embedding layer comes up with a relation of the inputs in another dimension Whether it's in 2 dimensions or even higher. I also find a very interesting similarity between word embedding to the Principal Component Analysis. Although the name might look complicated the concept is straightforward. What PCA does is to define a set of data based on some general rules (so-called principle components). So it's like having a data and you want to describe it but using only 2 components. Which in this sense is very similar to word embeddings. They both do the same-alike job in different context. You can find out more here . I hope maybe understanding PCA helps understanding embedding layers through analogy. To wrap up, the answer to the original question of the post that " how does it calculate the value? " would be: Basically, our neural network captures underlying structure of the inputs (our sentences) and puts relation between words in our vocabulary into a higher dimension (let's say 2) by optimization. Deeper understanding would say that the frequency of each word appearing with another word from our vocabulary influences (in a very naive approach we can calculate it by hand) Aforementioned frequency could be one of many underlying structures that NN can capture You can find the intuition on the youtube link explaining the word embeddings
