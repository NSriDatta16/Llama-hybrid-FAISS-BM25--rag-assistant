[site]: crossvalidated
[post_id]: 624140
[parent_id]: 
[tags]: 
Confusing GPT architecture diagram from the paper "Locating and Editing Factual Associations in GPT"

I am fairly new to Transformers thus this schema confuses me a lot. I am familiar with "classic" "vanilla" encoder-decoder transformers. I have figured out that GPT-like models are based on decoder-only transformers, where basically everything is fed to the decoder which then consists of a sequence of "self-attention blocks" (which in turn consist of a self-attention mechanism, followed by layer normalization, followed by an MLP, followed by layer normalization), and finally an MLP which "predicts" the next token. This is then applied auto-regressively until an end-of-sequence token. The paper Locating and Editing Factual Associations in GPT sparked my interest and I decided to read and understand it in depth, but I am really confused about this architecture diagram which they show: Let us focus on (a) for instance, the tokens "The Space Need le is in downtown" is fed to the GPT-like network and it outputs "Seattle". Firstly, I am confused why it is shown as if each token was fed to the network individually. Given this is the input to the network no auto-regressive steps should be executed, right? Thus, shouldn't the whole sequence be just one input? Secondly, what are the red connections going out from each hidden state to the attention block of the hidden state in the same hidden layer of the next input token? As far as I understand all the context a transformer has access to are only the inputs + previous outputs, and they have no access to any previous network internals like attention, activations and so on?
