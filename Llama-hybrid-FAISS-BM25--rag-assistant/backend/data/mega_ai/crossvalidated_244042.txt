[site]: crossvalidated
[post_id]: 244042
[parent_id]: 
[tags]: 
Trend in irregular time series data

I have a dataset of water temperature measurements taken from a large waterbody at irregular intervals over a period of decades. (Galveston Bay, TX if you’re interested) Here’s the head of the data: STATION_ID DATE TIME LATITUDE LONGITUDE YEAR MONTH DAY SEASON MEASUREMENT 1 13296 6/20/91 11:04 29.50889 -94.75806 1991 6 20 Summer 28.0 2 13296 3/17/92 9:30 29.50889 -94.75806 1992 3 17 Spring 20.1 3 13296 9/23/91 11:24 29.50889 -94.75806 1991 9 23 Fall 26.0 4 13296 9/23/91 11:24 29.50889 -94.75806 1991 9 23 Fall 26.0 5 13296 6/20/91 11:04 29.50889 -94.75806 1991 6 20 Summer 28.0 6 13296 12/17/91 10:15 29.50889 -94.75806 1991 12 17 Winter 13.0 (MEASUREMENT is the temperature measurement of interest.) The full set is available here: https://github.com/jscarlton/galvBayData/blob/master/gbtemp.csv I would like to remove the effects of seasonal variation to observe the trend (if any) in the temperature over time. Is a time series decomposition the best way to do this? How do I handle the fact that the measurements were not taken at a regular interval? I'm hoping there is an R package for this type of analysis, though Python or Stata would be fine, too. (Note: for this analysis, I’m choosing to ignore the spatial variability in the measurements. Ideally, I’d account for that as well, but I think that doing so would be hopelessly complex.)
