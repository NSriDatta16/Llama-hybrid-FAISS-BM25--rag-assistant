[site]: datascience
[post_id]: 87199
[parent_id]: 87188
[tags]: 
The Transformer model is a sequence-to-sequence model, that is, it is meant to address problems where the input is a sequence of discrete tokens (i.e. text) and the output is also a sequence of discrete tokens. Therefore, a Transformer is well suited to be trained with a dataset of dialogs where the input is a statement or question and the output is the answer. This is usually called a "chit-chat" chatbot, because they are not backed by a knowledge base. They can just have "small talk".
