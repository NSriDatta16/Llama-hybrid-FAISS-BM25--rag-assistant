[site]: datascience
[post_id]: 122376
[parent_id]: 
[tags]: 
TensorFlow Gradient Tape - LookupError (Tandem Neural Network project)

I am trying to train a network by using a custom lost function, then computing the gradient of the loss, and updating the trainable variables in the reverse network (named: reverse_loaded). For finding the gradient, I am using GradientTape, but not sure if I'm using it correctly. Here is my code: # Define the loss function def custom_loss(y_true, y_pred): loss = tf.square(y_true - y_pred) return tf.reduce_mean(loss, axis=-1) import keras.losses keras.losses.custom_loss = custom_loss #Define function that pass input through the reverse network, and then the forward network, and then find loss values def pass_through(norm_train_X_rev): # Pass the inputs through the reverse network output_rev = reverse_loaded.predict(norm_train_X_rev) # Here is the error I got: LookupError Traceback (most recent call last) in () 34 # Calculate the loss 35 #loss_value = custom_loss(norm_train_X_rev, np_output_forw.transpose()) ---> 36 loss_value = pass_through(norm_train_X_rev) 37 38 # Backward pass and compute the gradients 2 frames in pass_through(norm_train_X_rev) 13 14 # Pass the inputs through the reverse network ---> 15 output_rev = tf.stop_gradient(reverse_loaded.predict(norm_train_X_rev)) 16 np_output_rev = np.squeeze(np.asarray(output_rev)) 17 /usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs) 68 # To get the full stack trace, call: 69 # `tf.debugging.disable_traceback_filtering()` ---> 70 raise e.with_traceback(filtered_tb) from None 71 finally: 72 del filtered_tb /usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph) 637 grad_fn = func_call.python_grad_func 638 else: --> 639 raise LookupError( 640 "No gradient defined for operation" 641 f"'{op.name}' (op type: {op.type}). " LookupError: No gradient defined for operation'IteratorGetNext' (op type: IteratorGetNext). In general every operation must have an associated `@tf.RegisterGradient` for correct autodiff, which this op is lacking. If you want to pretend this operation is a constant in your program, you may insert `tf.stop_gradient`. This can be useful to silence the error in cases where you know gradients are not needed, e.g. the forward pass of tf.custom_gradient. Please see more details in https://www.tensorflow.org/api_docs/python/tf/custom_gradient. Note that the error is raised on the line: output_rev = reverse_loaded.predict(norm_train_X_rev) For troubleshooting, I tried to move the GradientTape to only enclosing the loss calculation, such as: with tf.GradientTape() as tape: tape.watch(reverse_loaded.trainable_variables) loss_value = loss_value = custom_loss(norm_train_X_rev, np_output_forw.transpose()) # Backward pass and compute the gradients gradients = tape.gradient(loss_value, reverse_loaded.trainable_variables) optimizer = tf.keras.optimizers.Adam() optimizer.apply_gradients(zip(gradients, reverse_loaded.trainable_variables)) But then, I got gradients = [None, None, None, None, None, None, None, None] Any suggestions or solutions are greatly appreciated!
