[site]: crossvalidated
[post_id]: 619036
[parent_id]: 
[tags]: 
How to deal with negative values after subtracting the background?

I am working with Luminex data and have a question regarding the treatment of negative values after subtracting the background average. Specifically, I would like to know if these negative values need to be addressed before conducting statistical analyses such as non-parametric tests or correlations. If so, I am curious to learn about the best approach to handle them. I have come across different methods employed by researchers to handle negative values. One approach involves adding a constant value (e.g., X) to the entire dataset, effectively converting the negative values into positive ones. Another approach I have observed is replacing negative values with a specific value, such as 0. In addition, I am planning to transform my data to a log2 scale for visualization purposes in graphs. To achieve this, I have been replacing all negative values with 0.1. Interestingly, when applying the Kruskal-Wallis test, I obtained consistent results between the transformed data (with the negative values replaced by 0.1) and the unmodified data. According to what I found out, this was expected since the Kruskal-Wallis test primarily relies on the order of the data rather than the specific values. The modification I made did not alter the central tendency of the data, as the position of negative values remained the same. It's worth noting that 0.1 is much smaller than any positive value in my dataset. However, when I performed the two-sample Mann-Whitney U test (transitioning from three groups to two groups), the test results differed between the transformed and untransformed data. I would greatly appreciate your guidance on this matter. I am seeking clarification on the best approach to handle such data. Understanding the appropriate methodology will greatly assist me in my analysis. EDIT based on extra comments from the OP Luminex is commonly used to measure analytes in the field of biological sciences, such as cytokines, antibodies, etc. In my case, it was used to measure antibody levels. To clarify your doubts about the blank, it involves averaging the blanks on the plate and subtracting that average from the duplicate averages of the samples. In other words, it's a constant value. There are some samples that naturally yield negative values. In my case, this can indicate either the absence of analyte in that sample or a high background signal. These negative values cannot be excluded from the analysis as it would introduce bias. This situation is similar to when samples fall below the limit of detection, where researchers often use approaches like replacing them with a constant value. However, I have also come across discussions that suggest this may not be the best approach. I'm uncertain whether negative values should be included in the analysis. For the statistical analysis, I am using the actual values, including the negative values. To visualize the data, I have adopted an approach commonly seen in several papers, which involves replacing negative values with a positive value (lower and distinct from the actual positive values) to allow for plotting the log2 of the results. However, I still have doubts about whether it is correct to use negative values or if I should replace them with half the average background value, as advised for the limit of detection. Yet, I have also encountered comments that disagree with this approach. All I know is that I cannot ignore the negative values as they reflect the lack of analyte signal in the samples under the run conditions. The statistical analyses I am conducting are non-parametric, including Wilcoxon tests, Kruskal tests, correlation analysis (Spearman), and PCA. I appreciate your guidance on how to handle these data concerns. END OF EDIT
