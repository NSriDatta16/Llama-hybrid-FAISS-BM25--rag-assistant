[site]: crossvalidated
[post_id]: 331735
[parent_id]: 197321
[tags]: 
A better idea is that instead of thinking about the sampling distribution you use Posterior samples $\tilde{y}$ using MCMC P($\tilde{y}|y)$ = $ \int_\theta P(\tilde{y}|\theta)P(\theta|y)d\theta$ to generate a bunch of Deviance samples $D = -2*\sum_ilog(P(\tilde{y}))$. MCMC methods allow you to generate posterior samples. Since posterior samples encapsulate the uncertainty it is propagated to the deviance and that is what you really want when you make decisions. You use measures like WAIC and LOO which average deviances in very specific ways that are derived from solid theory based in Cross Validation. A really good computational reference is McElreath's new book's Chapter 6 and for theory on this papers like this and this one that explain these concepts. Of course posterior estimation are not easy or possible in many models. But for most Linear, Generalized Linear(especially these) models these methods are easily applicable(Basically anywhere where you have a maximum likelihood that's well defined)
