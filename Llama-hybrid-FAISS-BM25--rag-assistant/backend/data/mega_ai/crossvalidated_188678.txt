[site]: crossvalidated
[post_id]: 188678
[parent_id]: 
[tags]: 
Numerical check of gradient in neural network

I am trying to check if my implementation of backpropogation is correct by checking the calculated gradients with the numeric gradient. I am testing it on a very simple linear network (i.e. no sigmoid on the output activation) as shown below: There are two input neurons and one output neuron. The weights have the values [0.34, 0.56] and the bias is 0.54 The loss function used is the squared loss i.e. $$ \frac{1}{2}(y - output)^2 $$ With an input of [1, 0] , working out the gradients w.r.t the weights ( dw ) and bias ( db ) using the chain rule (backpropogation) gives me: dw = [0.7744, 0] and db = [0.7744] which is exactly what my backprop implementation gives. However, to calculate the numerical gradient my understanding is that I should compute $g(\theta) \approx \frac{J(\theta + \epsilon) - J(\theta - \epsilon)}{2\epsilon}$ where $J$ is the network output. Using the example input of [1,0] and perturbing the 0.34 weight with $+/- \epsilon$ the numeric gradient will always be $1$, regardless of what I use for $\epsilon$. This is quite far away from my analytic gradient. Am I doing something wrong? Thanks
