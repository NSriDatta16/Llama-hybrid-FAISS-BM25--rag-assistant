[site]: crossvalidated
[post_id]: 466255
[parent_id]: 465807
[tags]: 
So is the window size more of a tool for saving memory / computing requirements? Or does it have a big impact on a model? It's both! Imagine you have a long text like War and Peace . Back-propagating from the end of the text to the beginning is a huge effort because the text is so long. Most of the effect of the update will pertain to the most recent time-steps, because the previous words in a sentence are most relevant for what you're predicting (the next word). On the other hand, imagine the most extreme truncation, which only looks back 1 time step. This won't allow the model to learn any long-term dependencies because the model focuses exclusively on the most recent time-step. Picking a good window size is important, but fine-tuning (e.g. choosing between 64 and 65) isn't necessary -- pick a window that's "large enough" to learn longer dependencies, and call it a day. The term of art for truncating the number of time steps in a recurrent neural network is "truncated back-propagation through time."
