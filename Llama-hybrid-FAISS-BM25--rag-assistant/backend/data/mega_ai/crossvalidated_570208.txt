[site]: crossvalidated
[post_id]: 570208
[parent_id]: 569884
[tags]: 
Plot and inspect your data before you jump to a non-parametric test. A parametric model might be more powerful and more informative. A parametric model can provide information about the magnitude and linearity of the trend, not just whether it's "statistically significant." The Mann-Kendall test is just the Kendall rank-correlation test of values against time. As the manual for the Kendall() function in the Kendall package says: It may also be noted that usual Pearson correlation is fairly robust and it usually agrees well in terms of statistical significance with results obtained using Kendallâ€™s rank correlation. The Pearson correlation is the standardized linear regression coefficient . So why not try simple linear regression versus time? Or, better, allow for a flexibly non-linear association of your outcome metric versus time. You can do that with a parametric regression model that includes time and the individual animals as predictors of your outcome metric. Model time flexibly, for example with restricted cubic regression splines. You can then test whether there is evidence of a significant non-linear component to the trend. Choose whether to treat animals as fixed or random effects based on your study design. Either approach will account for "different starting values." You could also allow for different trends among animals beyond their "different starting values" with interaction terms for fixed effects, or with random slopes. If you nevertheless want to do Mann-Kendall tests you might take advantage of the following characteristic of the Kendall rank-correlation, from the above manual page: An advantage of the Kendall rank correlation over the Spearman rank correlation is that the score function S nearly normally distributed for small n and the distribution of S is easier to work with. Do separate Mann-Kendall tests on each of the animals. Get the normalized score value for each animal (S divided by square root of var(S), both provided by the MannKendall() function in the Kendall package). Determine whether the mean of the normalized score value over all animals is different from 0 via a one-sample t-test. In response to revised question: there is no good reason to assume why the data should be normal distributed The data don't have to have a normal distribution. It's ideal if the errors around the model predictions are. Under much weaker assumptions, a linear regression (potentially including non-linear terms like regression splines) can provide very useful models . I'm at the moment mainly interested in testing for trends in the data and i'm not sure weather they are linear Restricting yourself to a trend test might undercut your ability to find the true association between outcome and time. For example, say you have a very rapid initial trend followed by a slow continued trend or no later trend. The Mann-Kendall test might well show no trend at all, depending on the proportion of your data from the late slow/no-trend timepoints. That's why I suggested looking at your data first. A regression spline or a generalized additive model can pick up that type of behavior over time while a simple trend test might not. This [one-sample t-test] works independent to the sample size? Because i only have 4 animals in total. With only 4 animals in your sample you have limited power to detect a true response, but you can still perform a 1-sample t-test to see if the mean normalized score over those 4 is different from the null hypothesis of a 0 mean value. I just read about the seasonal mann Kandell test, this should yield similar results to your proposed method right? I don't have experience with the seasonal test. If each animal has all measurements from a single season, it wouldn't seem to be helpful for you, as it evidently tries to account for seasonal variation within each time series.
