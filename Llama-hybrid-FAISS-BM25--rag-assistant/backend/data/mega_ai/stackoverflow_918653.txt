[site]: stackoverflow
[post_id]: 918653
[parent_id]: 905551
[tags]: 
I see an algorithm that is O(1/n) admittedly to an upper bound: You have a large series of inputs which are changing due to something external to the routine (maybe they reflect hardware or it could even be some other core in the processor doing it.) and you must select a random but valid one. Now, if it wasn't changing you would simply make a list of items, pick one randomly and get O(1) time. However, the dynamic nature of the data precludes making a list, you simply have to probe randomly and test the validity of the probe. (And note that inherently there is no guarantee the answer is still valid when it's returned. This still could have uses--say, the AI for a unit in a game. It could shoot at a target that dropped out of sight while it was pulling the trigger.) This has a worst-case performance of infinity but an average case performance that goes down as the data space fills up.
