[site]: datascience
[post_id]: 82342
[parent_id]: 82337
[tags]: 
Yes; selecting based on the correlation coefficient, which I'll call $r$ , is a valid option. It doesn't necessarily have to be $|r|>0.5$ , but keep in mind that the lower you go, the more likely you are to lose valuable information. You may also decide that you wish to eliminate a certain number of features, $k$ , and choose these based on the $k$ -highest correlation coefficients. If the reason why you want to eliminate variables is because you're worried about redundancy between features harming your predictivity, I would consider eliminating pH and stopping there, since it correlates with so many other variables. If you simply don't want to deal with too many variables, perhaps start eliminating the ones that correlate with pH (but not pH). I would prioritize elimination based on what makes sense in the real world, especially if you do not have a lot of data samples (meaning some of those high $|r|$ could be influenced by small sample size). E.g., I'm guessing you weren't surprised by pH being correlated with fixed acidity, since pH is an indication of how acidic or basic a compound is. As for alternative solutions have you considered ridge regression? This is a supervised approached (considers labels/dependent variable), so it may not be appropriate for your problem. It's nice because it doesn't require as much decision making, although you may not be happy with the number of variables it sets to zero (eliminating them). Sometimes it eliminates too few, or maybe even too many, depending on your data and modeling needs. If you don't need to keep the original variables intact, PCA is a very popular unsupervised option for reducing features/redundancy between features while trying to preserve explained variance in the original data. This, theoretically, means you're less likely to lose predictivity than elimination through unsupervised approaches like the one you've proposed.
