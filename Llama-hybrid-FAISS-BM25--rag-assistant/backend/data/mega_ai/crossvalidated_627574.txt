[site]: crossvalidated
[post_id]: 627574
[parent_id]: 
[tags]: 
Jensen-Shannon Distance - Estimating the discrete probability distribution

In my current research, I am working with a comprehensive dataset that captures human mobility patterns across different locations. My specific focus is on quantifying the similarity between these distributions, using a feature called "median dwell time," which represents the median duration individuals spend in various locations. To accomplish this, I am contemplating the use of the Jensen-Shannon Divergence (JSD) metric, both within different categories of Points of Interest (POIs) and across different cities. However, a crucial prerequisite for computing the JSD metric is the estimation of discrete probabilities. I'm uncertain about the best approach to creating these probabilities. One approach I'm considering is constructing a histograg and a constant values to avoid 0 probabilities # Function to calculate JSD for a pair of cities def calculate_jsd(data1, data2): p1 = np.histogram(data1, 24)[0] + 0.000001 p1 = p1 / np.sum(p1) p2 = np.histogram(data2, 24)[0] + 0.000001 p2 = p2 / np.sum(p2) return distance.jensenshannon(p1, p2) The second approach I'm considering is kernel density estimation (KDE). def calculate_kde_and_normalize_JSD(data1, data2, num_points=24): # Calculate KDE for data1 and data2 kde1 = scipy.stats.gaussian_kde(data1) kde2 = scipy.stats.gaussian_kde(data2) # Create an array of values for the PDF X = np.linspace(min(data1.min(), data2.min()), max(data1.max(), data2.max()), num_points) # Calculate the PDFs using KDE p = kde1(X) q = kde2(X) # Normalize the PDFs to sum to 1 p /= np.sum(p) q /= np.sum(q) return distance.jensenshannon(p, q) I also have the below code example of my calculation if I consider PDF instead, and I want some advice if this is the right approach in statistic: import numpy as np import scipy.stats import matplotlib.pyplot as plt # Assuming my data in x1 and x2 x1 = test[test['city'] == 'Golden Hills']['median_dwell'].tolist() x2 = test[test['city'] == 'Detroit']['median_dwell'].tolist() # Determine the number of bins using Freedman-Diaconis rule num_bins = int(np.ceil(2 * (len(x1) ** (1/3)))) # Determine the range based on my data min_value = min(min(x1), min(x2)) max_value = max(max(x1), max(x2)) # Create histograms with dynamically determined bins and range hist1 = np.histogram(x1, bins=num_bins, range=(min_value, max_value)) hist1_dist = scipy.stats.rv_histogram(hist1) hist2 = np.histogram(x2, bins=num_bins, range=(min_value, max_value)) hist2_dist = scipy.stats.rv_histogram(hist2) # Define evaluation points for PDFs num_points = 100 #assumption X = np.linspace(min_value, max_value, num_points) # Calculate PDFs at the specified evaluation points Y1 = hist1_dist.pdf(X) Y2 = hist2_dist.pdf(X) # Compute Jensen-Shannon Distance M = (Y1 + Y2) / 2 d1 = scipy.stats.entropy(Y1, M, base=2) d2 = scipy.stats.entropy(Y2, M, base=2) js_dv = (d1 + d2) / 2 js_distance = np.sqrt(js_dv) print(f'Jensen-Shannon Distance: {js_distance}') Jensen-Shannon Distance: 0.0945012408944263 However, the KDE calculation would have completely different results: import numpy as np import scipy.stats import matplotlib.pyplot as plt # Assuming your data is in x1 and x2 x1 = test[test['city'] == 'Golden Hills']['median_dwell'].tolist() x2 = test[test['city'] == 'Detroit']['median_dwell'].tolist() # Create KDE estimations for x1 and x2 kde1 = scipy.stats.gaussian_kde(x1) kde2 = scipy.stats.gaussian_kde(x2) # Define evaluation points for PDFs num_points = 100 # You can adjust this number based on your preference min_value = min(min(x1), min(x2)) max_value = max(max(x1), max(x2)) X = np.linspace(min_value, max_value, num_points) # Calculate PDFs using KDE at the specified evaluation points Y1 = kde1(X) Y2 = kde2(X) # Compute Jensen-Shannon Distance M = (Y1 + Y2) / 2 d1 = scipy.stats.entropy(Y1, M, base=2) d2 = scipy.stats.entropy(Y2, M, base=2) js_dv = (d1 + d2) / 2 js_distance = np.sqrt(js_dv) print(f'Jensen-Shannon Distance: {js_distance}')enter code here Jensen-Shannon Distance: 0.3110560464721506 I would greatly appreciate any insights or recommendations on these approaches or any alternative methods that might be more suitable for my analysis. Thank you!
