[site]: crossvalidated
[post_id]: 383510
[parent_id]: 
[tags]: 
How can policy parameterization be simpler than action-value parameterization in function approximation?

In the second edition of the book "Reinforcement Learning: an introduction" by Sutton and Bato page 323 (Policy gradient chapter) it says that: "Perhaps the simplest advantage that policy parameterization may have over action-value parameterization is that the policy may be a simpler function to approximate." Can anyone please explain the reason? Thank you
