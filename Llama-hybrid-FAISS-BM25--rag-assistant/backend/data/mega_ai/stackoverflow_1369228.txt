[site]: stackoverflow
[post_id]: 1369228
[parent_id]: 
[tags]: 
Is neural network's response guaranteed on training data?

I'm trying to train an ANN (I use this library: http://leenissen.dk/fann/ ) and the results are somewhat puzzling - basically if I run the trained network on the same data used for training, the output is not what specified in the training set, but some random number. For example, the first entry in the training file is something like 88.757004 88.757004 104.487999 138.156006 100.556000 86.309998 86.788002 1 with the first line being the input values and the second line is the desired output neuron's value. But when I feed the exact same data to the trained network, I get different results on each train attempt, and they are quite different from 1, e.g.: Max epochs 500000. Desired error: 0.0010000000. Epochs 1. Current error: 0.0686412785. Bit fail 24. Epochs 842. Current error: 0.0008697828. Bit fail 0. my test result -4052122560819626000.000000 and then on another attempt: Max epochs 500000. Desired error: 0.0010000000. Epochs 1. Current error: 0.0610717005. Bit fail 24. Epochs 472. Current error: 0.0009952184. Bit fail 0. my test result -0.001642 I realize that the training set size may be inadequate (I only have about a 100 input/output pairs so far), but shouldn't at least the training data trigger the right output value? The same code works fine for the "getting started" XOR function described at the FANN's website (I've already used up my 1 link limit)
