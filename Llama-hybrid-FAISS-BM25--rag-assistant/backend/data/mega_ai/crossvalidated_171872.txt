[site]: crossvalidated
[post_id]: 171872
[parent_id]: 163015
[tags]: 
I assume that the paper you linked is mostly concerned about memory issues during calculation of the kernel matrix, rather than storing of the matrix itself. This is supported by the memory issues described, e.g., in Shi, Qinfeng, et al. "Hash kernels." International Conference on Artificial Intelligence and Statistics. 2009. In fact, the whole thing may be a confusion of kernel matrix and document-term-matrix , or something similar to that. Regarding kernel matrix storage: Whether or not a kernel matrix needs to be stored (in the sense of: it is needed for prediction of test points) will depend on the employed method. A Support Vector Machine may only require to store the training points $\mathbf{x}_i$ and parameters $\alpha_i$, $b$, since the predictor is typically written as: $$ f_{svm}(\mathbf{x'}) = \sum \alpha_i y_i k(\mathbf{x_i},\mathbf{x'}) +b$$ where $k(\mathbf{x},\mathbf{x'})$ is the kernel function. Actually, only the support vectors have to be stored, i.e., all $\mathbf{x}_i$ with $\alpha_i \neq 0$. On the other hand, methods like Gaussian Process Regression (Kriging) usually require the whole kernel (or covariance) matrix, or rather its inverse, see: $$ f_{gpr}(\mathbf{x'}) = \hat{\mu} + \psi^T \Psi^{-1} (\mathbf{y-\mathbf{1}} \hat{\mu}),$$ where $\hat{\mu}$ is a parameter of the model, $\mathbf{y}$ is the vector of all training observations $y_i$, $\mathbf{1}$ is a vector of ones (same length as $\mathbf{y}$), $\psi$ is the vector containing $k(\mathbf{x_i},\mathbf{x'})$, and $\Psi$ is the kernel matrix containing all $k(\mathbf{x_i},\mathbf{x_j})$.
