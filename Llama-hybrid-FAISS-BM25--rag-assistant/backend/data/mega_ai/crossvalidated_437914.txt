[site]: crossvalidated
[post_id]: 437914
[parent_id]: 
[tags]: 
Does it make sense to combine Early Stopping with k-fold cross validation?

I have a CNN architecture for which I want to optimize the hyperparameters such as learning rate, dropout rate and number of epochs. I am thinking of a combination of k-fold cross validation and Early Stopping for tuning, because I hope to avoid the search for the optimal number of epochs when I use Early Stopping. But I'm very unsecure if this approach make sense (because I have to separate an additional validation set for EarlyStopping). So far, I have not found a definitive statement if this makes sense? Have you already had experience with this approach?
