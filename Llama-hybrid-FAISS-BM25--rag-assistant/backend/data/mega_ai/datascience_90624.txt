[site]: datascience
[post_id]: 90624
[parent_id]: 90609
[tags]: 
A search on Google Scholar yielded NeuralComputation 1991 article That is Kolmogorov's Theorem Is Relevant Abstract We show that Kolmogorov's theorem on representations of continuous functions of n-variables by sums and superpositions of continuous functions of one variable is relevant in the context of neural networks. We give a version of this theorem with all of the one-variable functions approximated arbitrarily well by linear combinations of compositions of affine functions with some given sigmoidal function. We derive an upper estimate of the number of hidden units." This theorem of Kolmogorov is surely the one referred to in the seminar I attended. But I somewhat erred I first thought in using the plural of mathematician. But then I see the terminology "Kolmogorov–Arnold representation theorem (or superposition theorem)" is also employed. Wikipedia gives the following references: Andrey Kolmogorov, "On the representation of continuous functions of several variables by superpositions of continuous functions of a smaller number of variables", Proceedings of the USSR Academy of Sciences, 108 (1956), pp. 179–182; English translation: Amer. Math. Soc. Transl., 17 (1961), pp. 369–373. Vladimir Arnold, "On functions of three variables", Proceedings of the USSR Academy of Sciences, 114 (1957), pp. 679–681; English translation: Amer. Math. Soc. Transl., 28 (1963), pp. 51–54.
