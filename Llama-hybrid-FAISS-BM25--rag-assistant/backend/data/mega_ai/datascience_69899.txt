[site]: datascience
[post_id]: 69899
[parent_id]: 54232
[tags]: 
BERT and ELMo are recent advances in the field. However, there is a fine but major distinction between them and the typical task of word-sense disambiguation: word2vec (and similar algorithms including GloVe and FastText) are distinguished by providing knowledge about the constituents of the language. They provide semantic knowledge, typical about word types (i.e. words in the dictionary), so they can tell you something about the meaning of words like "banana" and "apple". However, this knowledge is at the level of the prototypes, rather than their individual instances in texts (e.g. "apple and banana republic are american brands" vs "apple and banana are popular fruits"). The same embedding will be used for all instances (and all different senses) of the same word type (string). In past years, distributional semantics methods were used to enhance word embeddings to learn several different vectors for each sense of the word, such as Adaptive Skipgram. These methods follow the approach of word embeddings, enumerating the constituents of the language, but just at a higher resolution. You end up with a vocabulary of word-senses, and the same embedding will be applied to all instances of this word-sense. BERT and ELMo represent a different approach. Instead of providing knowledge about the word types, they build a context-dependent, and therefore instance-specific embedding, so the word "apple" will have different embedding in the sentence "apple received negative investment recommendation" vs. "apple reported new record sale". Essentially, there is no embedding for the word "apple", and you cannot query for "the closest words to apple", because there are infinite number of embeddings for each word type. Thus, BERT and ELMo handle the differences between word-senses but will not reveal what are the different senses of the word.
