[site]: crossvalidated
[post_id]: 547762
[parent_id]: 433867
[tags]: 
This was also confusing for me too! And I didn't quite think that the accepted answer or well, the only answer covered the source of confusion completely. The issue here isn't just with softmax, because even though you want to minimize the other logits, the loss will not converge unless the prediction and target vectors are nearly the same. If the target labels contained 1 for each present class, then because of softmax your loss will never converge -- its an impossible problem. But, in the mentioned paper the target labels are divided by k where k is the number of non-zero labels in the target vector. Then our loss function can converge. However, now think about what this target distribution is saying. Given your input, X, it is equally likely that the classification should be any of the k-assigned labels. This is not true since this is not an either-or scenario rather AND! This is where lack of independence comes in and is covered by the first sentence of the second paragraph of the answer above, and I've reposted below. When applying the logit and taking binary cross-entropy loss, we encourage each output component to be independent of each other
