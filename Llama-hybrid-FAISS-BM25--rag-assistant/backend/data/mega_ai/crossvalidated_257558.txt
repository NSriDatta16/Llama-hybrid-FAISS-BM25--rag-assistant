[site]: crossvalidated
[post_id]: 257558
[parent_id]: 
[tags]: 
Random forest regression: Multiple $R^2$ < 1 when data generating model is actually known

Let us assume we happen to know a (complex) data-generating model exactly and generate data according to this model. We then analyze the data using a random forest regression analysis. Multiple $R^2$ turns out to be close but not exactly to one (i.e., .95). This is what happened in a simulation I conducted. From a lecture I once attended I remember that, even if we know the data-generating process exactly, an explained variance of 100% in a random forest analysis can only be achieved if the number of training samples is infinity. My question is: Is there any formal explanation for this assertion? I vaguely remember that something like this can be shown for neural networks, but I haven't seen anything like that for random forests. I could also not find any "proof" or indirect proof in textbooks or papers.
