[site]: crossvalidated
[post_id]: 82728
[parent_id]: 82664
[tags]: 
As a general response, if you're using "least squares" type regression models there really isn't much difference between bayes and ML, unless you use an informative prior for the regression parameters. In response to specifics: 1)$ H_9 $ won't necessarily overfit the data - only when you have close to 9 observations. If you had 100 observations, most of the supposedly "overfitted" coefficients will be close to zero. Also $ H_1 $ would almost always result in "underfitting" - as there would be clear curvature missed 2) This is, not true for "linear" like polynomial expansions ("linear" meaning linear with respect to parameters, not $ x $). ML estimates for least squares are identical to posterior means under non informative priors or large sample sizes. In fact you can show that ML estimates can be thought of as "asymptotic" posterior means under a variety of models. 3) The Bayesian approach can avoid overfitting only for proper priors. This operates in a similar manner to penalty terms you see in some fitting algorithms. For example, L2 penalty = normal prior, L1 penalty = laplace prior.
