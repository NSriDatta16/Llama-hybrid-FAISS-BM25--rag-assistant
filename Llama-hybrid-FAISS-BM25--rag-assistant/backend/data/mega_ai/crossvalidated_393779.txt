[site]: crossvalidated
[post_id]: 393779
[parent_id]: 393740
[tags]: 
In statistics let's say you have some dataset. You want to test whether or not your model performs well in general on this kind of data, so you tweak your model based on some of the data in your dataset and validate the model's performance on the remaining (unseen) data in that dataset. This split is referred to as a train-test split. You'd typically expect a ratio of 80:20 for training and testing data. These sets of data are not mixed - they are wholly separate chunks of your data! But the problem with this approach is that once you test on the test set, you cannot tweak the model anymore, not without being biased towards improving your results on the test set. Maybe you tried tweaking too much for your training data and you got poor generalisation. The solution to this problem is typically using a validation set. So now you divide your data into three chunks: Training data (typically 60% of your data, these percentages can vary) Validation data (typically 30% of your data) Testing data (typically 10% of your data) You train on the training set, get a measure for your generalisation performance on the validation set, and only when you are COMPLETELY done with model tweaking, do you then test your model on the testing data. You hide your testing data under a rock - you touch it only once and only at the very end. It's worth stressing that this approach has some variants (k-fold cross validation), but the broad rationale for it and other techniques is that you don't want to bias your model towards the data you test it on, because otherwise you are effectively cheating and you're not really getting a true measure of generalisation. Hope this makes things clearer - and keep in mind this is true for neural networks as well as any kind of statistical modelling!
