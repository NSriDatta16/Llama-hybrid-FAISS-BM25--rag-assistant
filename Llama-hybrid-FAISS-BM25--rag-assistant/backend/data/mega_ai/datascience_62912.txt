[site]: datascience
[post_id]: 62912
[parent_id]: 62881
[tags]: 
As far as I know, the reason first activations were chosen this way can be traced back to the electrical properties of a neuron. There must be a minimum electrical potential for a neuron to transmit an incoming signal. This can be mathematically expressed by a step function , in which any value above the threshold is equal to one, otherwise its zero. All we need to know is if the the potential is smaller or larger than a given threshold value - if the switch is on or off - So there is no need to know "how much" it is on or off. Sigmoid function is in a sense differentiable version of the step function. And tanh also has the same structure, but just shifted down a little. But modern activations are not limited: ReLU and its variations Leaky ReLU and elu have no upper limit. But they all share a common property that is they are monotonically increasing - they either increase or stay constant with increasing input. So I suppose you will be OK as long as the function you choose shares this property (how fast will it converge is whole another question). Your example of f(x) = x^2 does not have this property, for example -0.5 and 0.5 have the same value, which I suppose will cause problems. This is against the idea of a how a neuron works, but of course we are not bounded by biology in constructing Neural Networks.
