[site]: crossvalidated
[post_id]: 641330
[parent_id]: 
[tags]: 
Why does PyTorch Linear allow multiple output dimensions?

PyTorch has a Linear component that accepts input and output dimension arguments. If you want to implement linear regression, for example, supply n input features and 1 output feature and the model will take the dot product of features (for a given observation) and weights, $xw^T$ . Great! But I'm curious, under what circumstances would the output dimension be greater than 1? Each element would receive all inputs but presumably return different a different value (unless every row of $w$ was identical.) To my knowledge, PyTorch has proper neural network modules that include fully connected layers. So I'm not sure what purpose Linear(n,m) serves if fully connected layers are addressed elsewhere.
