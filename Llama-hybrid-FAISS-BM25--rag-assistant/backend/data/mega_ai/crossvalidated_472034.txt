[site]: crossvalidated
[post_id]: 472034
[parent_id]: 471883
[tags]: 
I am going to give some structure to this problem by taking $X_1,X_2,X_3,... \sim \text{IID Bern}(\theta)$ to be your observable sequence of coin flips. Note that I have assumed exchangeability of the coin flips, which is a neccessary assumption for this kind of problem (which I will explain later). Your statistical problem involves taking a sample of $n$ values and determining whether $\theta = \tfrac{1}{2}$ based on observation of the maximum length of a run of heads in the sample: $$M_n \equiv \max_{1 \leqslant\ell \leqslant n} \sum_{i=\ell}^n \prod_{j = \ell}^{i} X_j.$$ The distribution of the maximum run-length for coin tosses is examined in Schilling (1990) . Since you are interested in inference about a single parameter outcome against an alternative, this is a hypothesis testing problem where you null hypothesis is that the coin is fair. I will show you how to perform a classical hypothesis test for the problem, but bear in mind that you could also apply a Bayesian test if you prefer. This is a somewhat unusual problem, insofar as you do not observed the total number of heads and tails on the coin, so it is a non-standard runs test. Essentially, it involves using the observed number of runs to make an inference about the likely number of heads in the sample, and then use this to infer whether or not the coin is fair. (The intermediate inference is usually only implicit, since the test will go straight from the runs statistic to an inference of the fairness of the coin.) Classical hypothesis test: Under the assumption of an exchangeable sequence of coin flips, it is possible to show that the statistic $M_n$ is stochastically increasing in $\theta$ ---i.e., a larger value of the parameter $\theta$ tends to make the values of $M_n$ larger. (If we are unwilling to assume exchangeability then the parameter $\theta$ is not well-defined or identifiable, so this assumption is a necessity of the test.) This is intuitively sensible, since a larger $\theta$ tends to give more heads, which tends to increase the largest run of heads. To facilitate the test, we will denote the null distribution of this statistic as: $$p_0(m) \equiv \mathbb{P}( M_n = m | \theta = \tfrac{1}{2}).$$ For a two-sided hypothesis test with $n$ data points, the p-value function is: $$\text{p}(m_n) = \sum_{m=0}^n p_0(m) \cdot \mathbb{I}(p_0(m) \leqslant p_0(m_n)).$$ It is possible to derive the exact null distribution of the test statistic (see e.g., Schilling 1990, p. 198) or an asymptotic approximation (pp. 203-4). In the present working I will approximate the null distribution by simulation. Below we code the implementation of the test in R using $k=10^4$ simulations to approximate the mass function. The graph at the end shows the simulated null distribution of the runs statistic, and the estimated p-value for the test with your data. As you can see, the observed value is in the upper tail of the null distribution, and we obtain an estimated p-value of $\hat{\text{p}} = 0.044$ so there is some weak evidence to reject the null hypothesis that this is a fair coin. #Define a function to calculate the runs for an input vector RUNS_HEADS $Freq[match(R, FREQS$ RR)]; p $Freq*(FREQS$ Freq If you wanted to go further than this, you could construct some simulations to estimate the power of the test for different values of $\theta \neq \tfrac{1}{2}$ . It is likely that the test would have low power, since the runs statistic is quite volatile. The above result shows fairly weak evidence to reject the null hypothesis. It is "statistically significant" at the $\alpha=0.05$ level, but not at lower levels.
