[site]: crossvalidated
[post_id]: 355477
[parent_id]: 
[tags]: 
Should we average weight decay loss in neural network?

In a typical neural network, which way is the common way to add regularization? Assuming regression task, regression error loss is Mean-squared-error Then we can have two choice of regularization on weights: $\lambda$ * $\sum ||W||^2$ $\lambda$ * $\textbf{average} ||W||^2$ I have seen most people use the first option, just being curious to ask.
