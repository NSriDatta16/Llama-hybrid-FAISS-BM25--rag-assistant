[site]: crossvalidated
[post_id]: 224670
[parent_id]: 198463
[tags]: 
I reimplemented all of it, now the accuracy on CIFAR-10 test set is at 89.31% . The key points were : -the preprocessing: GCN followed by ZCA-whitening in that order -the mode of the convolutions: most of them are SAME convolutions (padding=1 for kernels of 3x3) except for the last two which are VALID, which leads to an 8x8xd output, which is then averaged. -the initialization: small gaussian or He -the scheduled decrease of learning rate: after 200, 250 and 300 epochs -the size of the output the logits and softmax are of size 16 even if there are only 10 classes I guess it comes from the fact that with more logits softmax is more spread. Nervana neon claims to have attained 89.5% on caffe and nervana but even if I would like it to match exactly this number I consider it to be close enough for my needs. Link towards my github .
