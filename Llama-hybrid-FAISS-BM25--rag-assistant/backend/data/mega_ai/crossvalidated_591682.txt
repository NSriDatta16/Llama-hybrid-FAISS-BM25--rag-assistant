[site]: crossvalidated
[post_id]: 591682
[parent_id]: 
[tags]: 
Is it valid to calculate a transformer neural network loss with respect to one element of a sequence input?

Suppose one sample of my training data consists of a sequence with $n$ elements. My task is to do binary classification on one element in the sequence, and my labels are such that for each sequence in the training set, I only have a label for one element which I'll call the 'target' element. I use self-attention layers to create embeddings for each of the $n$ elements in one sequence. I then want to run the embedding, for the target element only, through a feed-forward network resulting in a binary prediction. The index for the target element can be different for each sample. The target element is determined in advance (it's not a function of the neural network parameters) I want to know if it makes sense to backpropagate on a loss which is calculated from the embedding for one element only? And to get some intuition or theoretical justification for why this approach is valid or not. To give an analogy: my sequences are sentences and my task is to make a binary prediction on one word in each sentence and I have a dataset that determines the target word for each sentence and its corresponding label. The goal of the model is not to predict which word is the target, but rather to assign the correct label to a pre-specified target. The motivation is that I have a pre-trained model which already creates good embeddings for each element, I want to fine-tune the per-element embeddings on a much smaller task-specific dataset and I suspect that trying to aggregate all the embeddings into a single embedding that summarizes the whole sequence will dilute the signal from the target element.
