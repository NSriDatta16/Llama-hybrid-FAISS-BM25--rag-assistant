[site]: crossvalidated
[post_id]: 437337
[parent_id]: 
[tags]: 
Training an LSTM "autoencoder" without a decoder?

Say you take a timeseries. Then you'd train the model to predict a future window, based on a previous window, i.e. learn the temporal dependence in data through forecasting. This would result in both an optimizable loss, as well as a final hidden state that describes the data. Could this in theory work as an autoencoder? Why do you need the second LSTM to unpack the first when it's already a working system on its own? Edit: It turns out that without the unpacking of the decoder, the encoder's final hidden state has no reason to summarize anything, so it just learns to return the final datapoint. This of course results in a low reconstruction error, but the latent representations are useless for anything else.
