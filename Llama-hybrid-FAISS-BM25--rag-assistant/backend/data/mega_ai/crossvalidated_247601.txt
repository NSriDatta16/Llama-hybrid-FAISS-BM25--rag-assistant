[site]: crossvalidated
[post_id]: 247601
[parent_id]: 247572
[tags]: 
This is common in NLP: manipulating very high dimensional feature vectors. Each dimension corresponds to a word (or a bigram) if you are considering a very simple case of text classification. Feature selection, taking again the text classification task, can be done very naturally in Logistic Regression with $l_1$-norm. You control the strength of the parameter and the algorithm automatically prunes (sets their coefficients to 0) away features that don't contribute for the classification task. In Bayesian setting, I believe feature selection is done using special priors like Laplacian. Unfortunately I am not very proficient in this field. Having some custom rules to prune away features is nice and instructive. For a real problem, let a well-tested learning algorithm this do for you.
