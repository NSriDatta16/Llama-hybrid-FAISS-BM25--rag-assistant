[site]: crossvalidated
[post_id]: 166653
[parent_id]: 
[tags]: 
Theoretical properties of Gaussian Process Emulator

I am studying Guassian Prcess Emulator (GPE) to approximate computationally expensive computer models. Basically, we suppose the computer model, or simulator, is denoted by $f(x)$, where $x$ is the vector of input parameters. Then the GPE assumes $f(\cdot)\sim GP(m(\cdot),C(\cdot,\cdot))$ with some appropriate mean function and covariance function. Then given a traning set, $D=\{(x_1,f(x_1)),...,(x_N,f(x_N))\}$, at any input $x_0$, the simulation value $f(x_0)$ can be approximated/predicted/interpolated based on the training set in the sense of a full probabilistic sense. For example usually it is approximated with $E(f(x_0)|D)$. If $x_0=x_j$, for some $j=1,...,N$, then this value is exactly $f(x_j)$, otherwise it is a prediction with some predictive variance. After reading many papers on this topic, I found nearly all of them are talking about practical aspects, e.g. the specification of prior and calculation of posterior (Bayesian), or calculation of MLE (Frequentist), design, etc. I wonder if there are any literatures focusing on the theoretical parts, for example, how well $f(\cdot)$ is approximated with such statistical surrogate, the convergence rate in terms of $N$, the asymptotic properties, etc. Would anyone by chance guide me to some literatures on any of these points? Many thanks!
