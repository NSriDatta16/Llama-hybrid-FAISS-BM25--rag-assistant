[site]: datascience
[post_id]: 91188
[parent_id]: 64899
[tags]: 
See this paper's ( Exact solutions to the nonlinear dynamics of learning in deep linear neural networks ) result: Moreover, we introduce a mathematical condition for faithful backpropagation of error signals, namely dynamical isometry, and show, surprisingly that random scaled Gaussian initializations cannot achieve this condition despite their norm-preserving nature, while greedy pre-training and random orthogonal initialization can, thereby achieving depth independent learning times. Finally, we show that the property of dynamical isometry survives to good approximation even in extremely deep nonlinear random orthogonal networks operating just beyond the edge of chaos. I think this is an answer to your question.
