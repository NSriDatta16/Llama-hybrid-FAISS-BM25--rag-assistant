[site]: stackoverflow
[post_id]: 4741777
[parent_id]: 
[tags]: 
Efficient (both time and space) dictionary database (unique word to uniq id and back)

I'm looking for a solution, which is capable of: storing arbitrary sized unique words, along with their unique 64 bit unsigned integer identifier and a 32 or 64 bit unsigned int reference count accessing the data quickly with these patterns: for a lookup of a word, give back its uint64 identifier for a lookup of an identifier, give back the word inserting new records, preferably with auto incremented identifier and atomically incremented reference count, preferably in batch commits (meaning not word by word, each in a separate transaction, but several words in one committed transaction) atomically deleting records, which has zero reference count (this could be done even with a rate limited full table scan, by iterating through all the records and deleting the ones with 0 refcount in a transaction) storing a high amount of records on traditional spinning rust (hard disks), the record number is somewhere between 100 million and 1000 billion (1000*10^9) the average word size is somewhere between 25-80 bytes it would be good to have a python (for prototyping) and C interface, mainly embeddable, or an efficient "remote" (will be on localhost only) API For example a MySQL schema would be something like this: CREATE TABLE words ( id SERIAL, word MEDIUMTEXT, refcnt INT UNSIGNED, INDEX(word(12)), PRIMARY KEY (id) ) This of course works, but MySQL isn't up to this task, and due to the index needed for word searches, it stores redundant information needlessly. During the search for the most efficient solution, I figured out the following so far: - because the words share a lot of commonality (most of them are plain dictionary words in various languages and character sets), something this: http://www.unixuser.org/~euske/doc/tcdb/index.html would be good - the best I could find so far is Tokyo Cabinet's TDB: packages.python.org/tokyocabinet-python/TDB.html, but I have to evaluate its performance, and possible setups (where to store what and use what kind of index where for best time and space efficiency) Any ideas, algorithms, of even better, ready to use products and setups? Thanks,
