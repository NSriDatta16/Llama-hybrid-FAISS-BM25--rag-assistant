[site]: crossvalidated
[post_id]: 369169
[parent_id]: 
[tags]: 
Does a policy depend on the past?

I am wondering about the mere definition of the word 'policy'. Let us sassume that we have a finite space of states $S$ and a finite set of actions $A$ . People tend to write that it is a 'stochastic function' $$\pi : S \to A$$ i.e. what they mean is that we have a function $\pi^{(d)} : S \times A \to [0,1]$ such that for every $s \in S, \sum_{a \in A} \pi^{(d)}(a) = 1$ and the agent samples its next action according to this distribution, i.e. $P[A_t = a|S_t = s] = \pi^{(d)}(s, a)$ in the Markov process. One of the first examples given in this area is the N-armed bandit. I.e. there are $N$ one-armed bandits $B_1, ..., B_N$ and every single one $B_i$ gives a reward distributed like $\mathcal{N}(\mu_i, \sigma_i)$ when pulled. The underlying probabilistic state machine (or whatever you want to call it) has just one reflexive state and all actions $B_1, ..., B_N$ lead back into that single state. The first strategy ones comes up with is the greedy one: Pull some random arms $a_0, ..., a_w$ , collect rewards $r_0, ..., r_w$ for a while and then at each time $t$ for each bandit $B_j$ compute $$ p_j = \frac{\sum_{\substack{i \in \{0,...,t-1\} \\ a_i = B_j}} r_i}{\sum_{\substack{i \in \{0,...,t-1\} \\ a_i = B_j}} 1}$$ (i.e. compute the mean reward in the past that you received from $B_j$ ) and then pull $B_j$ that has the highest $p_j$ . However, now the action $a_t$ taken at time $t$ does not only depend on the single state in the automata but rather on all the past observations. In fact, that is the whole point in reinforcement learning: For a current state, check which states were similar in the past and check which action the agent took at that past time and from that somehow conclude what would be the smartest thing to do now. So, it appears to me that the policy depends on much more than the current state: it depends on all the past states and all the past rewards ... So how come that one can simplify it like $\pi(s)$ or $\pi(a|s)$ when in fact one should write $\pi(a_t|s_t, ..., s_0, r_{t-1}, ..., r_1)$ ? NB: It seems to me that there is a hidden concept of 'rounds'/matches involved in here. For example: when one wants the computer to learn to play pong using a neural network then one starts with a neural network and fixes that neural network for the whole round. In that sense, it is feeded the current state $s$ and samples an action $a$ from it. Only after the current match is finished one updates the weights of the neural network (multiplied by a factor depending on whether or not the game was won). So somehow a policy is a weird thing that is like $\pi(a|s)$ during one 'match' but the agent is allowed to change after one 'match' but then one needs to specify and formalize 'match' (i.e. start states and end states and how they are connected, etc). This seems a little complicated.
