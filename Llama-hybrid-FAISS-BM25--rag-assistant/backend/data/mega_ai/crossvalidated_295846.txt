[site]: crossvalidated
[post_id]: 295846
[parent_id]: 295822
[tags]: 
Be aware that sometimes rare words provide you very much information. Obviously, if a word appears once in the dataset, then it is useless. In many cases what people do is they first remove the stopwords and then just take some number of the most common words as features to consider in their algorithm. Given the fact that language data has long tails (there is lots of words that appear very rarely), it is more reasonable to take top $n$ words rather then figuring out how to cut the tail. One more thing that you should consider is that, instead of number of occurrences, you should rather look at metrics of importance as TF-IDF . Moreover, instead of ignoring some words you could consider some kind of dimensionality reduction like word2vec or GloVe (see also this blog entry ) and use the transformed data as input for your algorithm. I know cases where this worked remarkably well even after transforming the noisy raw text (without any filtering, cleaning, stemming etc).
