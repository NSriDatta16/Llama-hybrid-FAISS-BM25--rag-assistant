[site]: crossvalidated
[post_id]: 586703
[parent_id]: 
[tags]: 
How to train a language model (like BERT, or any other) to not classify a sentence in a particular class?

I have limited data, around 50 sentences per class and around 70 classes. Due to this reason of having less data, sometimes some sentences get classified in a class, which are similar but not belonging to that class. Is there any architecture or method by which I can further fine-tune the model to not classify a sentence in a particular class? Or is there some better approach for such a classification problem with less data? For more details: The classification problem is to classify legal clauses (which can be sentences or paragraphs) into one of 70 classes (like Confidentiality clause, Governing Law clause, Termination clause, etc.) My current approach is to fine-tune Legal-Bert to perform this classification task. The problem is limited data - around 50 training samples per clause.
