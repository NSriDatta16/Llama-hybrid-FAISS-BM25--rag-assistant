[site]: crossvalidated
[post_id]: 380887
[parent_id]: 379285
[tags]: 
Feature selection is one heck of a job and it is a huge domain. Well, in case of binary classification using SVM (or any other method as a matter of fact), the approach I choose is the use of Probability Density Functions (or PDFs). Let me put out a picture: Why do I use PDFs? Ok. So, the goal of SVM in your case is to classify your data into two classes. How does SVM work? It aims to find a plane to divide your data into two classes (a line in 2D, a plane in 3D and a hyper-plane in n-D). When you trace a normalised PDF of a known data set for the two classes, the two curves tend to either be completely separated, maybe overlap a little or completely overlap. Here are three examples of a data set that I use. These are some examples from ECG signals. I have traced the mean of the data (Plot.1), the mean absolute deviation (Plot.2) and a slope Q-R (Plot.3) from ECG signals. Plot.1 Plot.2 Plot.3 As you can see in Plot.1, the red and blue curves on the right side almost completely overlap. In Plot.2, they are almost separated while in Plot.3, they are totally separated. How is this useful? The dashed line in the three plots above, that is very similar to the SVM margin for classification for data both in feature space and the hyper-space (if you're using kernel functions). So if the separating hyper-plane is not able to separate the data into two classes, no matter what kernel or any feature set you use, the learning step tends to fail resulting in low accuracies (That will in turn fail your testing phase). If you have a known data set, you could trace such PDFs and see how the features appear. And if you're able to find say 5-6 features out of 11 that you have get good separation (as in Plots 2 and 3 above), I guarantee you that the learning step will be a 100% accurate and your testing step too will give you a very high accuracy. This is known as physical analysis of signal features (well, that's what they call it here at least :P). Advantages of using this approach As most work suggests, learning data should be at least 66% of the total data set. By using this approach, I was able to obtain very similar results even with learning data set size as low as 45% of the total data set. PS: The PDF plots were traced using the moving average curve smoothing technique; you could use other techniques too.
