[site]: datascience
[post_id]: 32461
[parent_id]: 32460
[tags]: 
The embedding layer maps your vocabulary index input to a dense vector, so it acts as lookup layer and (if set to trainable) will be influenced on some weights only, by the words occurring in a batch of training data. Having a linear layer, it would be sequentially trained by all the data batches and would not provide a lookup functionality (each word given to the input would share the same weights). Also, you're right considering word2vec differently. When using a custom trainable embedding layer, the dense vectors will be optimized (by SGD) for the task you are considering whereas models like word2vec act like language modeling and find a semantic optimum representation in the embeddings. Therefore, depending on data size, the representation found during training might be better for your task than the one found by a neutral word2vec or other model.
