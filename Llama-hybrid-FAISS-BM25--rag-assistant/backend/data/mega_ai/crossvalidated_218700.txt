[site]: crossvalidated
[post_id]: 218700
[parent_id]: 218602
[tags]: 
Analysis Because this is a conceptual question, for simplicity let's consider the situation in which a $1-\alpha$ confidence interval $$\left[\bar x^{(1)} + Z_{\alpha/2} s^{(1)}/\sqrt{n}, \bar x^{(1)} + Z_{1-\alpha/2} s^{(1)}/\sqrt{n}\right]$$ is constructed for a mean $\mu$ using a random sample $x^{(1)}$ of size $n$ and a second random sample $x^{(2)}$ is taken of size $m$, all from the same Normal$(\mu,\sigma^2)$ distribution. (If you like you may replace the $Z$s by values from the Student $t$ distribution of $n-1$ degrees of freedom; the following analysis will not change.) The chance that the mean of the second sample lies within the CI determined by the first is $$\Pr\left(\bar x^{(1)} + \frac{Z_{\alpha/2}}{\sqrt{n}} s^{(1)} \le \bar x^{(2)} \le \bar x^{(1)} + \frac{Z_{1-\alpha/2}}{\sqrt{n}} s^{(1)}\right) =\Pr\left(\frac{Z_{\alpha/2}}{\sqrt{n}} s^{(1)} \le \bar x^{(2)}-\bar x^{(1)} \le \frac{Z_{1-\alpha/2}}{\sqrt{n}} s^{(1)}\right).$$ Because the first sample mean $\bar x^{(1)}$ is independent of the first sample standard deviation $s^{(1)}$ (this requires normality) and the second sample is independent of the first, the difference in sample means $U = \bar x^{(2)} - \bar x^{(1)}$ is independent of $s^{(1)}$. Moreover, for this symmetric interval $Z_{\alpha/2}=-Z_{1-\alpha/2}$. Therefore, writing $S$ for the random variable $s^{(1)}$ and squaring both inequalities, the probability in question is the same as $$\Pr\left(U^2 \le \left(\frac{Z_{1-\alpha/2}}{\sqrt{n}}\right)^2 S^2\right)= \Pr\left(\frac{U^2}{S^2} \le \left(\frac{Z_{1-\alpha/2}}{\sqrt{n}}\right)^2\right).$$ The laws of expectation imply $U$ has a mean of $0$ and a variance of $$\operatorname{Var}(U) = \operatorname{Var}\left(\bar x^{(2)} - \bar x^{(1)}\right) = \sigma^2\left(\frac{1}{m} + \frac{1}{n}\right).$$ Since $U$ is a linear combination of Normal variables, it also has a Normal distribution. Therefore $U^2$ is $\sigma^2\left(\frac{1}{n} + \frac{1}{m}\right)$ times a $\chi^2(1)$ variable. We already knew that $S^2$ is $\sigma^2/n$ times a $\chi^2(n-1)$ variable. Consequently, $U^2/S^2$ is $1/n + 1/m$ times a variable with an $F(1,n-1)$ distribution. The required probability is given by the F distribution as $$F_{1,n-1}\left(\frac{Z_{1-\alpha/2}^2}{1 + n/m}\right).\tag{1}$$ Discussion An interesting case is when the second sample is the same size as the first, so that $n/m=1$ and only $n$ and $\alpha$ determine the probability. Here are the values of $(1)$ plotted against $\alpha$ for $n=2,5,20,50$. The graphs rise to a limiting value at each $\alpha$ as $n$ increases. The traditional test size $\alpha=0.05$ is marked by a vertical gray line. For largish values of $n=m$, the limiting chance for $\alpha=0.05$ is around $85\%$. By understanding this limit, we will peer past the details of small sample sizes and better understand the crux of the matter. As $n=m$ grows large, the $F$ distribution approaches a $\chi^2(1)$ distribution. In terms of the standard Normal distribution $\Phi$, the probability $(1)$ then approximates $$\Phi\left(\frac{Z_{1-\alpha/2}}{\sqrt{2}}\right) - \Phi\left(\frac{Z_{\alpha/2}}{\sqrt{2}}\right) = 1 - 2\Phi\left(\frac{Z_{\alpha/2}}{\sqrt{2}}\right) .$$ For instance, with $\alpha=0.05$, $Z_{\alpha/2}/\sqrt{2} \approx -1.96/1.41 \approx -1.386$ and $\Phi(-1.386) \approx 0.083$. Consequently the limiting value attained by the curves at $\alpha=0.05$ as $n$ increases will be $1 - 2(0.083) = 1 - 0.166=0.834$. You can see it has almost been reached for $n=50$ (where the chance is $0.8383\ldots$.) For small $\alpha$, the relationship between $\alpha$ and the complementary probability--the risk that the CI does not cover the second mean--is almost perfectly a power law. Another way to express this is that the log complementary probability is almost a linear function of $\log\alpha$. The limiting relationship is approximately $$\log\left(2\Phi\left(\frac{Z_{\alpha/2}}{\sqrt{2}}\right)\right) \approx -1.79712 + 0.557203\log(20 \alpha) + 0.00657704 (\log(20 \alpha))^2 + \cdots$$ In other words, for large $n=m$ and $\alpha$ anywhere near the traditional value of $0.05$, $(1)$ will be close to $$1 - 0.166 (20\alpha)^{0.557}.$$ (This reminds me very much of the analysis of overlapping confidence intervals I posted at https://stats.stackexchange.com/a/18259/919 . Indeed, the magic power there, $1.91$, is very nearly the reciprocal of the magic power here, $0.557$. At this point you should be able to re-interpret that analysis in terms of reproducibility of experiments.) Experimental results These results are confirmed with a straightforwward simulation. The following R code returns the frequency of coverage, the chance as computed with $(1)$, and a Z-score to assess how much they differ. The Z-scores are typically less than $2$ in size, regardless of $n, m, \mu, \sigma, \alpha$ (or even whether a $Z$ or $t$ CI is computed), indicating the correctness of formula $(1)$. n
