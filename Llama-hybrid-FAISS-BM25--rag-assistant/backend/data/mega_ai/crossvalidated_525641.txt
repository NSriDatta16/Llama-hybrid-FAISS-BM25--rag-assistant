[site]: crossvalidated
[post_id]: 525641
[parent_id]: 
[tags]: 
In a VAE, why do we use a sum operation when calculating KL divergence?

Following from a tensorflow guide on VAE's here , I notice the loss function sums over the latent space. def log_normal_pdf(sample, mean, logvar, raxis=1): log2pi = tf.math.log(2. * np.pi) return tf.reduce_sum(-.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi), axis=raxis) Why do we take the sum and not the mean here? Wouldn't this make the scale of the loss dependent on how big our latent space is, coupling our learning rate and model parameters?
