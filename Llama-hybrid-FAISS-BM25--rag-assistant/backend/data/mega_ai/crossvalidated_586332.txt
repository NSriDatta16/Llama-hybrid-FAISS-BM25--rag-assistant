[site]: crossvalidated
[post_id]: 586332
[parent_id]: 276385
[tags]: 
When using model.fit and model.evaluate on different datasets, the result will NEVER be exactly the same. There is a multitude of factors, but usually it's just because X_train and X_test are not exactly alike. So the model inherently will not get the exact same prediction results, thus not the same loss. As you correctly pointed out, you should check whether fit and evaluate on X_train produce a similar loss. They won't be exactly the same (due to how fit vs evaluate works, where one updates the weights after each batch whereas the evaluate uses the same weights throughout the dataset). But they should be pretty close. In your case, loss of 19 vs 16 seems to be different enough that it's an issue......which brings me to a question no one as answered yet -> That is the bigger issue with Keras (and apparently with Time Series) seems to be that fit and evaluate on the SAME dataset (i.e "X_train") can produce radically different results. That's certainly not normal. I've documented in more detail here where you can see a difference in train vs evaluate loss by an order of magntitude!: https://datascience.stackexchange.com/questions/113627/lstm-sequential-val-loss-train-loss-on-same-dataset
