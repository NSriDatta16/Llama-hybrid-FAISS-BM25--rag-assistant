[site]: crossvalidated
[post_id]: 359181
[parent_id]: 294856
[tags]: 
I agree with Sycorax. Consider that what you're actually measuring by doing this is variation in your model rather than variation in your target, and conflating the two is dangerous. Suppose for example that we have a binary classification problem and a model which is a perfect oracle, i.e., it knows the true probabilities for all examples. Our model always predicts the most likely event no matter how many times we run it, but if we average these results we will only get zeros and ones, which obviously should not be interpreted as probabilities.
