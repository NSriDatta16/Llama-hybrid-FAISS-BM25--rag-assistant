[site]: crossvalidated
[post_id]: 447799
[parent_id]: 447006
[tags]: 
The code you wrote seems to treat samples as features. Consider this part: > cov(x) sample1 sample2 sample3 sample1 0.56666667 0.08213672 -0.6488034 sample2 0.08213672 0.35119661 -0.4333333 sample3 -0.64880339 -0.43333333 1.0821367 As you can see the covariance matrix is constructed for samples, not features. Following this your pca$vectors will have 3 dimensions. But the samples from B you are trying to project have 5: > pca$vectors [,1] [,2] [,3] [1,] -0.5064330 0.6404626 0.5773503 [2,] -0.3014404 -0.7588151 0.5773503 [3,] 0.8078733 0.1183525 0.5773503 > B sample4 sample5 obs1 0 0 obs2 1 2 obs3 2 1 obs4 0 1 obs5 2 1 To fix the issue the covariance matrix should be computed on a transpose of x . pca And the obtained projections: > t(y) %*% pca$vectors [,1] [,2] [,3] [,4] [,5] sample4 -0.9933442 -0.78697392 -1.6369351 0.1304668 -0.17519903 sample5 1.5496399 -0.01146507 -0.8184675 0.0652334 -0.08759952 Which are the same with predict : > predict(prcomp(t(x)), t(y)) PC1 PC2 PC3 sample4 0.9933442 0.78697392 0.9869463 sample5 -1.5496399 0.01146507 0.4934732 NOTE 1: The directions of PCA are arbitrary so the signs can be inverted NOTE 2: The "eigen" version has more PCs, but the last 3 are degenerate. This is also why the PC3 can be different. The highest number of PCs you get from a data matrix is the minimum of #columns and #rows minus 1. So in this case 2. You can also check this by looking at pca$values , and note that the eigenvalues for the last 3 vectors are 0: > all.equal(pca $values[1], 0) [1] "Mean relative difference: 1" > all.equal(pca$ values[2], 0) [1] "Mean relative difference: 1" > all.equal(pca $values[3], 0) [1] TRUE > all.equal(pca$ values[4], 0) [1] TRUE > all.equal(pca$values[5], 0) [1] TRUE
