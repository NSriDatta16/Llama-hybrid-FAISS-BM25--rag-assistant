[site]: crossvalidated
[post_id]: 567538
[parent_id]: 567536
[tags]: 
I don't think it is true. The advantage of the SVM is that it focuses on the decision boundary and the solution does not depend on datapoints too far from the decision boundary to provide useful information on where the boundary should be drawn. As fitting a model generally involves some compromise, if we use the logistic loss, then it is possible that the behaviour of the model around the $p = 0.5$ threshold (or some other specific threshold that is appropriate for your application) may be sacrificed to gain a bigger reduction in the loss somewhere in the attribute space that is distant from this boundary. The SVM does not have this problem as it focusses specifically on the region near the decision boundary. So this means that IF you are only interested in accuracy and the misclassification costs are fixed and known at training time and the operational class frequencies are fixed and known at training time and you don't need a "reject" option and ... then the hinge loss/SVM may give better performance than the logistic loss. But this doesn't really depend on the size of the dataset. In general, I prefer the logistic loss (e.g. Kernel Logistic Regression) as it is more flexible in the situations where it is applicable, and often it works just as well as the SVM (or better). If it is a situation where the SVM may work better, try both and evaluate them properly. IMHO, for small datasets, you are better off with a shallow classifier, such as KLR or Gaussian processes, rather than deep learning (unless it is something like image recognition where convolutional NN are likely to be a good approach), and the main difficulty is tuning (or better still marginalising) the hyper-parmeters.
