[site]: crossvalidated
[post_id]: 5380
[parent_id]: 5360
[tags]: 
An important question is "why do why do you want a model with as few variables a possible?". If you want to have as few variables as possible to minimize the cost of data collection for the operational use of your model, then the answers given by whuber and mbq are an excellent start. If predictive performance is what is really important, then you are probably better off not doing any feature selection at all and use regularized logistic regression instead (c.f. ridge regression). In fact if predictive performance was what was of primary importance, I would use bagged regularized logistic regression as a sort of "belt-and-braces" strategy for avoiding over-fitting a small dataset. Millar in his book on subset selection in regression gives pretty much that advice in the appendix, and I have found it to be excellent advice for problems with lots of features and not very many observations. If understanding the data is important, then there is no need for the model used to understand the data to be the same one used to make predictions. In that case, I would resample the data many times and look at the patterns of selected variables across samples to find which variables were informative (as mbq suggests, if feature selection is unstable, a single sample won't give the full picture), but I would still used the bagged regularized logistic regression model ensemble for predictions.
