[site]: crossvalidated
[post_id]: 630329
[parent_id]: 
[tags]: 
Efficiently sample from a limit set given a differential equation?

Given a dynamical system of many variables, described by an ordinary differential equation, is there some way to use machine learning to efficiently sample from the limit set (or maybe more accurately a limit distribution, since states that the system is more likely in should be sampled with a higher probability density), without having to simulate the system for a long time, since some systems may require to be simulated for a very, very , long time before getting close to the limit set? One basic example of a (very simple) dynamical system would be the Lorenz system , which is a set of ordinary differential equations in three dimensions, but which has a limit set that excludes almost every point of the space it is embedded in. (Although I suspect that for this specific system, it would be significantly cheaper to just simulate it for a large number of time steps and take the point that you end up in instead of using machine learning, since the distance between the point—the state of the system—and the attractor—the limit set—will approach zero rather quickly anyway, and I guess the fact that it has a fractal dimension (at just slightly above 2) could complicate things slightly if you use machine learning. But for other systems, the corresponding distance can approach zero much, much slower.) (Note that this is somewhat related to, by still distinct from, thermodynamic equilibrium , which is also a type of state that is approached or eventually reached as the system interacts with its surroundings over a long time. One key distinction is that thermodynamic equilibrium is a stable stationary state. I guess the limit set of a dynamical system could still be considered a kind of equilibrium, though, in the sense that it is something that the state of the system tends to get closer and closer to.)
