[site]: crossvalidated
[post_id]: 14893
[parent_id]: 
[tags]: 
libsvm training very slow on 100K rows, suggestions?

I'm trying to run the libsvm-provided wrapper script easy.py on a training set of 100K rows, each row has ~300 features. The feature data is relatively sparse, say only 1/10th are non-zero values. The script is excruciatingly slow, I'm talking days (or more). I ran the same script on 1% of the data, and it finished in about 20 minutes, with some reasonable looking results, so it looks like the input data / format is correct and there are no obvious issues with it. I found the documentation for libsvm to be somewhat lacking and not very helpful on practical issues like performance. Their FAQ is silent on these matters: http://www.csie.ntu.edu.tw/~cjlin/libsvm/faq.html Has anyone experienced similar issues with SVM training speed? Do you know of more suitable libraries or specific strategies to try out in such cases?
