[site]: crossvalidated
[post_id]: 595266
[parent_id]: 595121
[tags]: 
When we don't have a lot of confidence on information that can be used to construct Bayesian Priors - is it generally better to stick to Frequentist Approaches instead of "guessing" which Priors will better fit the data and the choice of model? In my opinion, the full power of a Bayesian analysis is unleashed when a priori information is available, although how to translate this prior information into a distribution is far from trivial and there may be several different ways to do it. Indeed, building a prior from extra experimental (or expert opinions) data is, to some extent, akin to choosing the statistical model. Thus the inferential task for a subjective Bayesian is heavier than that of a frequentist statistician. The power, however, is that we can, at least in principle, include expert opinion or extra experimental information in our inferential conclusions, information that in a frequentist approach would have been otherwise discarded. In absence of information, however, we also can find several compelling reasons why a Bayesian approach is still useful. In the linked post you find many of them, and I summarise them as lead to inferential procedures with good frequentist properties (probability coverage, unbiasedness of the MAP) default priors tailored to the parameter of the model at hand that are guaranteed to impact the posterior as less as possible. Notice the bold default , which seems to be the most suitable adjective. Indeed, we cannot call them non-informative since "informativeness" is relative to the measure of information used and may depend on the scale of the parameter. Among these compelling reasons, I find two of them that are particularly relevant. Firstly, some of these default priors have been discovered and used by frequentists in order to adjust their inference that otherwise would have been inaccurate or useless. For instance, Firth(1993) showed that the Jeffreys prior in exponential families expressed in the canonical form leads to a maximum a posteriori (MAP) with reduced bias. Furthermore, and more importantly for frequentists, he proposed a weight function for frequentists which solves the problem of perfect separation in logistic regression. However, the most compelling argument in the defence of default priors, such as the Jeffreys, is invariance under one-to-one reparametrizations. (personal communication by J. Berger in an O'Bayes meeting some years ago). And I totally agree with this. Indeed, if we think invariance to parametrization is a good thing, as most frequentists do, then that's the ultimate defence for the use of non-informative or default prior.
