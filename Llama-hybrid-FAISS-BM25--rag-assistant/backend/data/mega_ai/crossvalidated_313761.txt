[site]: crossvalidated
[post_id]: 313761
[parent_id]: 
[tags]: 
How to count precision and recall for multiclass classification which returns top-5 classes per test example

This is how testing looks like: There is 100 target classes The test set consists of 10K elements - each one of them is tagged by one target class The distribution of classes over test set is unbalanced During the testing: for each one of 10K testing elements - my classifier returns the set of top-5 best matched classes, lets say the set of: {c1, c2, c3, c4, c5} only one of them can be true rank on which correct prediction appears doesn't matter So now the qustion : How to count Precision and Recall for that kind of multiclass classification with top-n results returned? I considered here 2 main approaches: Separately for each class(out of 100) I count: True Positive(TP) , True Negative(TN) , False Positive(FP) , False Negative(FN) considering that if one out of 5 returned classes is correct this is: +1 TP similarly I count TN , FP , FN - for each class separately after that I can count Precision and Recall based on micro-avg or macro-avg . In my case (where distribution of classes over test set is unbalanced) I use micro-avg I can count Precision and Recall for each one of 10K examples separately - treating that: there is max only one TP (because only one out of 5 returned classes maybe correct) if it is found: Recall is 1/1 , Precision is 1/5 if it was not found: Recall is 0/1 , Precision is 0/5 After testing all 10K examples - I count avg-Precision and avg-Recall , simply by summing up all precisions and recalls (separetely) and divide them by number of examples (10K) This approach is called Precision at rank and came from Information Retrieval field The second approach seems to me a bit unreasonable so I understand that I should use the first one, but here is the question for you: Did I understood it well and should I go with the first approach?
