[site]: crossvalidated
[post_id]: 65506
[parent_id]: 
[tags]: 
RBM - clamping the visible units during reconstruction

I am currently attempting to code an implementation of Deep NN for image recognition using stacked RBMs (by adding a linear classifier at the "top" of the last RBM in the stack). I am at the very early stage of learning RBMs. I have read several papers and some powerpoint presentation from the Toronto University Machine Learning group. From this material, I understood when computing (vi hj)0 - (vi hj)n (to update weights) I would first set the visible units to a data vector and use that to compute the states of the hidden layer (this would be used for (vi hj)0) then use the newly computed hidden units to create a reconstruction of the visible units and use that reconstruction in a loop to calculate (vi hj)n by updating the hidden layers with the reconstruction, then updating the reconstruction, and continuing n times updating the hidden layer with the newest reconstruction of the visible layer. Recently I came by this : http://halfbakedmaker.org/?p=11748 Here the author clamps the training vector to the visible units not only to calculate the first iteration of updates, but through all the process. His loop is also short, which is fine I guess, I read about such a shortcut in one of HintonÂ´s powerpoint files. My questions are, if there is a correct way of doing this, which one is it? Clamping the visible units for the first iteration only and then using the newest reconstruction or clamping the visible unit through the whole process? If both ways are correct, are there performance (learning) differences?
