[site]: crossvalidated
[post_id]: 536320
[parent_id]: 461918
[tags]: 
I'm certainly not a mind reader as to why or how these recommendations came into being; however, I can at least speculate and share some personal experience in APA. As you observed in your response to @BruceET, the APA guidelines for reporting the test statistic do predate the major position papers on the misuse of p -values. As such, the recommendations also predate the transition to assuming that effect sizes will be reported for every test. Prior to such a requirement, interested readers could at least compute some rough effect measures from the test statistics (e.g., d = t/sqrt(df)). Ultimately, though, I think the short answer is as a matter of transparency. If someone reports their results as t (30) = 1.50, p d = 1.25, then readers (ideally this would be caught by the reviewers/editors) can look at those values and see clearly that this is incorrect. Similarly, if someone were to just report, "The means differed significantly, p p On the note of degrees of freedom, I do think part of that is related to ways of computing effect sizes, but I think it is also a transparency issue as well. Just because the total sample for a study may be large, once you factor in missing observations, the n for specific statistical tests and the N of the sample can be fairly different. Should an author find that their test was non-significant in the entire sample, they may be tempted to start looking at subgroups or portions of the sample, and reporting the degrees of freedom for each of those results helps prevent mis-interpretation. In short, I think in a perfect world where everyone is very responsible in their research and everyone understand the statistics they are using, there wouldn't be any need for reporting the statistics themselves. I think, however, as a matter of pragmatism and good faith effort for transparency in research, reference styles like APA recommend such practices.
