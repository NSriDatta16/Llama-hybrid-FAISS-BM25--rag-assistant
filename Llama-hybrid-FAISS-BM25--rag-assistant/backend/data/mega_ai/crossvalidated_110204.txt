[site]: crossvalidated
[post_id]: 110204
[parent_id]: 108417
[tags]: 
I'll take the alternative approach to @forecaster and suggest you have the option of not treating this as a time series problem. Instead, with A as the response, pre-compute predictor values for each of the lags. That is, before training, add a column for each lagged value to your data frame, say df$p1=B df$p2=lag(B,1) df$p3=lag(B,2) df$p4=C df$p5=lag(C,1) ... and so on. That way you don't have to worry about keeping rows together during sampling and can use bootstrap and k-fold just fine. Your formula becomes A ~. . Depending on which learning technique you use you may find that certain "highly correlated" cities and lags are in fact not good predictors for A . Moreover if your predictors are highly correlated you can get rid of some. I'd suggest using the caret package to simplify your programming task, qualify your predictors, and evaluate your modeling results. [Edit] I'm not clear why you're interested in bootstrapping. You can use the partition creator of caret to make splits easily. If you have plenty of data you may not need bootstrapping, but if you want resampling you can uses caret for that too (see createResample ). Here is a template for how you might use caret to perform a random forest fit with your data. The formula shown is the A~. suggested above which assumes your response is A in the training and testing data frames. SEED=800 require(caret) library('psych') library('corrplot') library('zoo') # set up predictors and responses predictors The caret documentation is excellent (e.g. caret model training ) so you can explore many other options there. You might also use the time series split of caret ?createTimeSlices to use the techniques suggested by @forecaster.
