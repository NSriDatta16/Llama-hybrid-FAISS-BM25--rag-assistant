[site]: crossvalidated
[post_id]: 355329
[parent_id]: 
[tags]: 
Data sparsity becomes a problem

I read this in a paper Developing an approach toward virtual synthesis parameter screening introduces two primary computational challenges: data sparsity and data scarcity. ...Such canonical representations, however, are necessarily sparse as there are many more actions that one might perform during the synthesis of a material, compared to the number of actions actually used. However, I learnt before that data sparsity is a desired property. When the data points are sparsely distributed, there will be a clear pattern that a machine can learn. My understanding of data sparsity becoming a challenge is that if our data matrix looks like this: $$(x_1, x_2, \cdots, x_N) = \begin{pmatrix} 1 &&&\\ & 1 \\ && \ddots\\ &&&1 \end{pmatrix} $$ Clearly, there will be no pattern that a machine could learn because there is data scarcity in each dimension. So, I think the problem of data sparsity is still a problem of data scarcity. Questions : Is my understanding correct? Is there a way for telling when data sparsity poses a challenge? When data sparsity becomes a problem will reasonable dimensionality reduction helps?
