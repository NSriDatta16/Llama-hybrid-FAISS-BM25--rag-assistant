[site]: crossvalidated
[post_id]: 234475
[parent_id]: 
[tags]: 
impact to AUC if swap positive and negative during model training

If I swap positive class and negative class, then train a model again (I tried decision tree, adaboost, svm from scikit-learn built-in package) for a two class classification problem. Sometimes, I can see AUC slightly change (around 1-2%). Anyone have any ideas why there are such changes? For ROC curve, x-axis is false positive rate, and y-axis and true positive rate. When prediction model gives prediction scores, we will order the scores from higher value to lower value, and then choose threshold according to the sorted values and calculate at the specific threshold point, what is the fpr and tpr. AUC is the area under ROC. BTW, for swap, I mean manually assign negative label to be 1 and manually assign positive label as 0. I am asking if I swap, whether area of AUC may change? Edit 1 , here is how adaboost works, confused why it is not converged? From the formula, it should be converged. Refer from this book
