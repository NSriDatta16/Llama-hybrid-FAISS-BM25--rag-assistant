[site]: stackoverflow
[post_id]: 4063605
[parent_id]: 4063423
[tags]: 
There is no single perfect algorithm for tokenization, though your algorithm may suffice for information retrieval purposes. It will be easier to implement using a regular expression: def Tokenize(text): words = re.split(r'[-\.,?!:;_()\[\]\'`"/\t\n\r \x0b\x0c]+', text) return [word.strip() for word in words if word.strip() != ''] It can be improved in various ways, such as handling abbreviations properly: >>> Tokenize('U.S.') ['U', 'S'] And watch out what you do with the dash ( - ). Consider: >>> Tokenize('A-level') ['A', 'level'] If 'A' or 'a' occurs in your stop list, this will be reduced to just level . I suggest you check out Natural Language Processing with Python , chapter 3 , and the NLTK toolkit.
