[site]: crossvalidated
[post_id]: 264810
[parent_id]: 
[tags]: 
Error measure that take the time of prediction into account?

I am doing a machine learning project where I'm attempting to predict the arrival times of buses equipped with GPS at bus stops, and I am very new to statistics/data analysis. I am looking for an error measure that would take into account the time at which the prediction is made. For instance, let's say I have a data point (data point 1) recorded at 3:00pm, with an actual arrival time at a particular bus stop of 3:02pm on the same day, and another data point (data point 2) recorded at 3:00pm, with an actual arrival time at the same bus stop of 3:39pm on the same day. Say I produce a model that predicts the arrival time of data point 1 is 3:05pm on the same day, and 3:42pm for data point 2 on the same day. So the data about this model might look like this in JSON format: [{"time-of-day":"3:00pm","actual-arrival":"3:02pm","predicted-arrival":"3:05pm"}, {"time-of-day":"3:00pm","actual-arrival":"3:39pm","predicted-arrival":"3:42pm"}] So the predictions for data points 1 and 2 have the same mean absolute error (in minutes) of 3. However, it can be argued that the prediction made by the model on data point 2 is better than the prediction made on data point 1, since the prediction was made further ahead in time. Is there a commonly used statistical error measure that takes into account how far ahead in time the prediction is made?
