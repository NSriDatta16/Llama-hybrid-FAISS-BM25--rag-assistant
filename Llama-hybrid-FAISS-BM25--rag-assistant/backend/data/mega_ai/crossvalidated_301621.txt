[site]: crossvalidated
[post_id]: 301621
[parent_id]: 245502
[tags]: 
To try to give another explanation: One of the most powerful things about neural networks is that they can be very complex functions, allowing one to learn very complex relationships between your input and output data. These relationships can include things you would never expect, such as the order in which data is fed in per epoch. If the order of data within each epoch is the same, then the model may use this as a way of reducing the training error, which is a sort of overfitting. With respect to speed: Mini-batch methods rely on stochastic gradient descent (and improvements thereon), which means that they rely on the randomness to find a minimum. Shuffling mini-batches makes the gradients more variable, which can help convergence because it increases the likelihood of hitting a good direction (or at least that is how I understand it).
