[site]: datascience
[post_id]: 36736
[parent_id]: 
[tags]: 
In a neural network with tensor input X, it seems there are times when it will never learn... Why?

import numpy as np import keras.models as km import keras.layers as kl import keras.optimizers as ko import keras.losses as kloss # This will cause no learning np.random.seed(1692585618) def f(x): a = x[0]* 3.141 + x[1] return a; # Create a sample dataset # Input is (*, 2) # Output is (*, ) x_train=np.array([[1,2], [3,4], [5,6]]) y_train=np.array([f(x) for x in x_train]) # These are required by the shape of x_train and y_train in_dim = x_train.shape[1] out_dim = 1 model = km.Sequential() model.add(kl.Dense(units=3, activation='relu', input_shape=(in_dim,))) model.add(kl.Dense(units=out_dim, activation='relu')) model.compile( loss=kloss.mean_squared_error , optimizer=ko.Adam(lr=0.1) ) model.fit(x_train, y_train, epochs=500, batch_size=1, verbose=True) Output: Epoch 1/50 3/3 [==============================] - 1s 221ms/step - loss: 225.9046 Epoch 2/50 3/3 [==============================] - 0s 3ms/step - loss: 225.9046 Epoch 3/50 3/3 [==============================] - 0s 3ms/step - loss: 225.9046 Epoch 4/50 3/3 [==============================] - 0s 2ms/step - loss: 225.9046 ...and so on (hundreds more)
