[site]: datascience
[post_id]: 121440
[parent_id]: 45774
[tags]: 
Not many people have looked into this problem from a theoretical perspective. There's a line of research that focuses on model selection that might be worth considering. Background Model selection is intertwined with data in stochastic learning. Traditionally, there are two main elements that dictate which model should be used given the data, the type of model and complexity. In learning, data drives the model component, while complexity is dictate by the how it is measured and the model architecture. This is all an elegant way of pointing out what is essentially equivalent to the bias-variance trade-off. Therefore, the amount of data that is required typically saturates at some point in the training process and that is inherently because of the bias-variance trade-off principle and its dependence on the complexity of the model. Deep learning One of the observations in deep learning has been that adding more data doesn't seem to saturate. This led some people to revisit the notion of bias-variance trade-off. In fact, it turns out that for deep learning models, it might be true that our traditional view of the bias-variance trade-off no longer holds. I encourage you to read [1] for a complete description, as well as [2]. Conclusion Rather than questioning deep learning as some extraordinary class of model that is somehow data-hungry. It might be good to think about the limitations of our understanding of model selection and its relationship to data. Additional Resources [1] Belkin, M., Hsu, D., Ma, S., & Mandal, S. (2019). Reconciling modern machine-learning practice and the classical biasâ€“variance trade-off. Proceedings of the National Academy of Sciences, 116(32), 15849-15854. [2] Dwivedi, R., Singh, C., Yu, B., & Wainwright, M. J. (2020). Revisiting complexity and the bias-variance tradeoff. arXiv preprint arXiv:2006.10189.
