[site]: crossvalidated
[post_id]: 379208
[parent_id]: 287920
[tags]: 
I wondered about the exact same thing when taking this course, and ended up researching this a bit. I'll give a short answer here, but you can read a more detailed overview in a blog post I wrote about it . I believe that at least part of the reason for those scaling coefficients is that L² regularization probably entered the field of deep learning through the introduction of the related, but not identical, concept of weight decay. The 0.5 factor is then there to get a nice λ-only coefficient for the weight decay in the gradient, and the scaling by m ... well, there are at least 5 different motivations that I have found or came up with: A side-effect of batch gradient descent: When a single iteration of gradient descent is instead formalized over the entire training set, resulting in the algorithm sometimes called batch gradient descent, the scaling factor of 1/m, introduced to make the cost function comparable across different size datasets, gets automatically applied to the weight decay term. Rescale to the weight of a single example: See grez's interesting intuition. Training set representativeness: It makes sense to scale down regularization as the size of the training set grows, as statistically, its representativeness of the overall distribution also grows. Basically, the more data we have, the less regularization is needed. Making λ comparable: By hopefully mitigating the need to change λ when m changes, this scaling makes λ itself comparable across different size datasets. This make λ a more representative estimator of the actual degree of regularization required by a specific model on a specific learning problem. Empirical value: The great notebook by grez demonstrates that this improves performance in practice.
