[site]: crossvalidated
[post_id]: 358177
[parent_id]: 358151
[tags]: 
Multivariate kernel density estimation can be defined in terms of product of univariate kernels $K^P(\mathrm{x}) = \prod_{i=1}^{d} \kappa(x_i)$, as a symmetric kernel $K^S(\mathrm{x}) \propto\kappa\{(\mathrm{x}'\mathrm{x})^{1/2}\}$, or in terms of standalone multivariate kernel, e.g. multivariate Gaussian distribution. There are also different possible choices of bandwidth matrix, it can have equal bandwidth for each of the variables $\mathrm{H} = h^2\mathrm{I}_d$, different for different variables $\mathrm{H} = \mathrm{diag}(h_1^2, h_2^2, \dots, h_d^2)$, or it could be a covariance matrix. The three choices are illustrated by Wand and Jones in their Kernel Smoothing book using the following figure of two-dimensional case. The first choice is "symmetric", it assumes no correlation and equal variances. Second allows for unequal variances. Third allows additionally for correlation between variables. The scipy documentation does not tell us much about the kind of multivariate kernel that they are using. It only tells us that it uses Scotts or Silvermans rules of thumb for selecting the bandwidth, so it estimates some constant $h^2$ and the covariance matrix is either same for all variables or is a scaling factor for covariance matrix (more likely, but you'd need to check the source code). Nonetheless, scipy is using rule of a thumb for choosing the bandwidth, so this does not have to be optimal choice and I'd encourage you to look for packages that implement more sophisticted approaches (R has several, I can't tell for python).
