[site]: crossvalidated
[post_id]: 231997
[parent_id]: 
[tags]: 
Intuition for when to use larger convolutional kernels

Although this question in some form has been asked in many places, I still have yet to read an answer or explanation that grants intuition for when it is appropriate to use larger kernels in a convolutional neural network. It seems like smaller kernels like 3x3 are all the rage these days, and I'll often hear things like, "it allows for more expressive power" or "can pick up on finer details" but this seems opposite to me. Shouldn't larger kernels always have greater expressive power (assuming same number of layers and at a greater computational cost, of course)? For example, let's say I'd like to discriminate between very small features, such as detecting computer typeface. Should this warrant larger or smaller kernels? Edit: As a quick test I built a 1 layer convolutional auto encoder and as far as I can tell, lower loss and better output from visual inspection of the decoded images are highly correlated with larger filters. So this seems to support my intuition that information is lost after pooling that no matter how many 3x3s we stack with the additional non-linear transformations, it will never be able to capture the information we could have learned if we used a larger filter. Would love some confirmation on this, though.
