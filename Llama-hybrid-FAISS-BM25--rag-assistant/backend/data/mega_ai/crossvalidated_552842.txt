[site]: crossvalidated
[post_id]: 552842
[parent_id]: 
[tags]: 
Scoring rules for Dirichlet process mixtures

Let's say I do a sample-based fitting of a Dirichlet process model: $$ \begin{aligned} X_i &\sim f(x_i\mid \theta_i)\\ \theta_i &\sim G\\ G &\sim \text{DP}(\alpha, G_0) \end{aligned} $$ where I'm fitting cluster assignment by MCMC. Let's introduce a latent parameter $\delta_i$ that indicates cluster assignment. $$ P(\delta_i = j) \propto \begin{cases} n_j^{-i}f(x_i\mid\theta_j) & \text{for }j = 1,\ldots,J^{-i}\\ \alpha\int_{\Omega}f(x_i\mid\theta_i)dG_0(\theta_i)d\theta_i &\text{for }j = J^{-i} + 1 \end{cases} $$ Let's say I want to evaluate my fitted model using a scoring rule. Let's say, posterior predictive loss (Gelfand and Ghosh): $$ S = Var(X_i) + \frac{k}{k+1}\left(x_i - \text{E}(X_i)\right)^2 $$ Where both expectations are taken with respect to the posterior predictive distribution. Do I use the posterior predictive distribution assuming the fitted $\delta_i$ 's, or is $\text{E}(X_i)$ with respect to a new $X_i$ ? I've been doing the former, and I'm now having doubts. Take a hypothetical example: binary data, with a mixture on point masses at 1 and 0--if I'm allowed to factor the fitted mixture identifiers into the calculation for score, then I will always result in a perfect model, predicting with 0 error and zero variance. I believe this is wrong. Am I correct in this interpretation?
