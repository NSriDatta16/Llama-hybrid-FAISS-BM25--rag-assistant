[site]: crossvalidated
[post_id]: 95985
[parent_id]: 
[tags]: 
LIBSVM cross-validation - number of folds vs. sample size

I am using LIBSVM for classification and the default RBF Kernel. To get optimal parameters for C and gamma, I am using the grid.py tool included in the LIBSVM package. It is based on a k-fold cross-validation, as far as I know. I have a training and a testing set of 15 451 instances with 5 features. Should I run the cross-validation on the whole training set or split it into e.g. a half and to the validation on it. And is there a rule for how many folds k I should chose in respect to the size of the cross-validation sample? For example, I wanted to do the cross-validation on the whole training set (15451 instances) with k = 50. Is this a good approach for getting the best parameter values?
