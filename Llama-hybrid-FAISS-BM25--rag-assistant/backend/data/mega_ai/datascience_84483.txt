[site]: datascience
[post_id]: 84483
[parent_id]: 84437
[tags]: 
First be careful, looking only at accuracy in a multiclass problem can be misleading: with almost 75% of the data in the majority class, a dummy model which always predict the majority class achieves almost 75%. Measuring performance with micro or macro F1-score would be more informative. Now about designing your experiments: currently you seem to be trying various methods at random, including sampling techniques and classification algorithms. Why not, but in this way you rely entirely on luck to improve performance. In particular what strikes me is that you don't mention anything about the task or the features (btw it's probably the reasons why some people downvoted the question). The type and nature of the features, their number and their relation to the class can be important to understand why certain methods work and others don't. There might be some feature engineering to do. In particular using feature selection methods sometimes brings great improvement. It would also be useful to get an idea of the performance obtained with simple methods (like decision trees, SVM, logistic regression). Finally you could investigate in more detail which kind of cases which get misclassified and/or study how stable the model is with respect to varying the number of instances or features.
