[site]: crossvalidated
[post_id]: 545201
[parent_id]: 
[tags]: 
Understanding probabilistic view of linear regression

I am trying to get into some machine learning and am not very good at math. Could someone please help me with the following queston? I am reading this script that explains the basic ideas behind probabilistic ML: https://www.cl.cam.ac.uk/teaching/1617/MLBayInfer/mlbi-1.pdf On page 41, the author explains how he generated the data for the linear regression examples and tries to show, I think, how this can be viewed as a probabilistic process. I quite simply don't understand how steps 2 and 3 give you $p(y_i|\vec{x_i},\vec{w})$ It says: For the ith example: I sampled $x_i$ according to the uniform density on [0, 3]. So there is a distribution $p(\vec{x})$ . I computed the value $h_{\vec{w}}(\vec{x}_i)$ . I sampled $\epsilon_i ∼ N (0, σ^2)$ with $σ^2 = 0.1$ and formed $y_i = h_{\vec{w}}(\vec{x}_i)+\epsilon_i$ And therfore $p(y_i|\vec{x_i},\vec{w}) = N(h_{\vec{w}}(\vec{x}_i), \sigma^2)$ I kinda understand that the rgression line is the expected value of the normal distribution at that point... but I don't quite understand how the normal distribution arises from steps 2 and 3. EDIT Thanks for the comments. I'll definitely check out the book and youtube videos. I think mhdadk's comment touches on where I have difficulties. So okay, If I add a constant k to a normal distribution $N(0,1)$ , it becomes $N(k,1)$ . But how exactly is $y_i = h_{\vec{w}}(\vec{x}_i)+\epsilon_i$ the cond. distribution of $y$ given $x$ and $w$ ? In this scenario, both $x$ and $e$ are random variables, right? When I see a conditional distribution $P(A|B,C)$ , I think of $\frac{P(A,B,C)} {P(B,C)}$ So does that mean that $N(h_{\vec{w}}(\vec{x}_i), \sigma^2)$ is somehow the result of $\frac{P(y,x,w)}{P(x,w)}$ ?
