[site]: crossvalidated
[post_id]: 16424
[parent_id]: 16305
[tags]: 
There is two methods that come to my mind which you might be interested in. The first is making use of known clusters and is called 'Neighbourhood components analysis' by Goldberger et al . The idea is that you learn a mapping (eg affine) from the higher dimensional space to a visualizable space. (eg $A: \mathbb{R}^n \mapsto \mathbb{R}^2$). This mapping is estimated by maximimizing the average number of correct classification if a variation of k-nearest neighbour classification is used. There are some impressive results obtained: The other one is tSNE , which learns a mapping (eg $A: \mathbb{R}^n \mapsto \mathbb{R}^2$). This mapping does not have any constraints, but the loss optimized (not wrt to some parametrization, but with new points in $\mathbb{R}^2$ itself) is that the new space reflects similar distances to the original space. Similar is rather complicated here, it is based on assuming certain distributions of the points in the space and the corresponding KL-divergence. For the latter, there is matlab code which you can find at the given link. Here is a visualization of the MNIST dataset:
