[site]: datascience
[post_id]: 68680
[parent_id]: 68675
[tags]: 
Here are some general hyperparameter adjustments to increase model capacity: Architecture-related: add additional layers increase the number of hidden units per layer remove dropout or decrease dropout rates Optimizer related: find the optimal learning rate train for more epochs Section 11.4.1 in the Deep Learning Book provides a good overview. The paper "Practical recommendations for gradient-based training of deep architectures" is another good source to understand the effect of different hyperparameters (though it is a bit older). Moreover, there are specific hyperparameters to increase model capacity for different network types, e.g. kernel size for CNNs, sequence length for LSTMs and embedding dimension for embedding layers.
