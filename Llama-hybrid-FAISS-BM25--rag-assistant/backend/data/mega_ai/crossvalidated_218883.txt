[site]: crossvalidated
[post_id]: 218883
[parent_id]: 218842
[tags]: 
One way to deal with a very skewed dataset is to rebalance it via bootstrap. It works like this. Let's say your rare class has label 1, the popular class has label 0. You bootstrap from you original data, whenever you get 1, keep that entry save it in a resample set, and up a counter by one. When it's a 0, check the counter, if you have more 0s than 1s, then discard this sample. Repeat until you have enough data for your model. This way, you'd get a balanced set. BTW, this works for any model, not just random forest. This is known as oversampling/undersampling. Weiss, G. M., McCarthy, K., & Zabar, B. (2007). Cost-sensitive learning vs. sampling: Which is best for handling unbalanced classes with unequal error costs?. DMIN, 7, 35-41. Farquad, M. A. H., & Bose, I. (2012). Preprocessing unbalanced data using support vector machine. Decision Support Systems, 53(1), 226-233 Just found this ref. Turns out there is a paper on exactly this topic: Chen, C., Liaw, A., & Breiman, L. (2004). Using random forest to learn imbalanced data. University of California, Berkeley.
