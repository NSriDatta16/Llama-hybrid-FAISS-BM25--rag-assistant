ractice. Formulation Like natural policy gradient, TRPO iteratively updates the policy parameters θ {\displaystyle \theta } by solving a constrained optimization problem specified coordinate-free: { max θ L ( θ , θ i ) D ¯ K L ( π θ ‖ π θ i ) ≤ ϵ {\displaystyle {\begin{cases}\max _{\theta }L(\theta ,\theta _{i})\\{\bar {D}}_{KL}(\pi _{\theta }\|\pi _{\theta _{i}})\leq \epsilon \end{cases}}} where L ( θ , θ i ) = E s , a ∼ π θ i [ π θ ( a | s ) π θ i ( a | s ) A π θ i ( s , a ) ] {\displaystyle L(\theta ,\theta _{i})=\mathbb {E} _{s,a\sim \pi _{\theta _{i}}}\left[{\frac {\pi _{\theta }(a|s)}{\pi _{\theta _{i}}(a|s)}}A^{\pi _{\theta _{i}}}(s,a)\right]} is the surrogate advantage, measuring the performance of π θ {\displaystyle \pi _{\theta }} relative to the old policy π θ i {\displaystyle \pi _{\theta _{i}}} . ϵ {\displaystyle \epsilon } is the trust region radius. Note that in general, other surrogate advantages are possible: L ( θ , θ i ) = E s , a ∼ π θ i [ π θ ( a | s ) π θ i ( a | s ) Ψ π θ i ( s , a ) ] {\displaystyle L(\theta ,\theta _{i})=\mathbb {E} _{s,a\sim \pi _{\theta _{i}}}\left[{\frac {\pi _{\theta }(a|s)}{\pi _{\theta _{i}}(a|s)}}\Psi ^{\pi _{\theta _{i}}}(s,a)\right]} where Ψ {\displaystyle \Psi } is any linear sum of the previously mentioned type. Indeed, OpenAI recommended using the Generalized Advantage Estimate, instead of the plain advantage A π θ {\displaystyle A^{\pi _{\theta }}} . The surrogate advantage L ( θ , θ t ) {\displaystyle L(\theta ,\theta _{t})} is designed to align with the policy gradient ∇ θ J ( θ ) {\displaystyle \nabla _{\theta }J(\theta )} . Specifically, when θ = θ t {\displaystyle \theta =\theta _{t}} , ∇ θ L ( θ , θ t ) {\displaystyle \nabla _{\theta }L(\theta ,\theta _{t})} equals the policy gradient derived from the advantage function: ∇ θ J ( θ ) = E ( s , a ) ∼ π θ [ ∇ θ ln ⁡ π θ ( a | s ) ⋅ A π θ ( s , a ) ] = ∇ θ L ( θ , θ t ) {\displaystyle \nabla _{\theta }J(\theta )=\mathbb {E} _{(s,a)\sim \pi _{\theta }}\left[\nabla _{\theta }\ln \pi _{\theta }(a|s)\cdot A^{\pi _{\theta }}(s,a)\right]=\nabla _{\theta }L(\theta ,\theta _{t})} However, when θ ≠ θ i {\displaystyle \theta \neq \theta _{i}} , this is not necessarily true. Thus it is a "surrogate" of the real objective. As with natural policy gradient, for small policy updates, TRPO approximates the surrogate advantage and KL divergence using Taylor expansions around θ t {\displaystyle \theta _{t}} : L ( θ , θ i ) ≈ g T ( θ − θ i ) , D ¯ KL ( π θ ‖ π θ i ) ≈ 1 2 ( θ − θ i ) T H ( θ − θ i ) , {\displaystyle {\begin{aligned}L(\theta ,\theta _{i})&\approx g^{T}(\theta -\theta _{i}),\\{\bar {D}}_{\text{KL}}(\pi _{\theta }\|\pi _{\theta _{i}})&\approx {\frac {1}{2}}(\theta -\theta _{i})^{T}H(\theta -\theta _{i}),\end{aligned}}} where: g = ∇ θ L ( θ , θ i ) | θ = θ i {\displaystyle g=\nabla _{\theta }L(\theta ,\theta _{i}){\big |}_{\theta =\theta _{i}}} is the policy gradient. F = ∇ θ 2 D ¯ KL ( π θ ‖ π θ i ) | θ = θ i {\displaystyle F=\nabla _{\theta }^{2}{\bar {D}}_{\text{KL}}(\pi _{\theta }\|\pi _{\theta _{i}}){\big |}_{\theta =\theta _{i}}} is the Fisher information matrix. This reduces the problem to a quadratic optimization, yielding the natural policy gradient update: θ i + 1 = θ i + 2 ϵ g T F − 1 g F − 1 g . {\displaystyle \theta _{i+1}=\theta _{i}+{\sqrt {\frac {2\epsilon }{g^{T}F^{-1}g}}}F^{-1}g.} So far, this is essentially the same as natural gradient method. However, TRPO improves upon it by two modifications: Use conjugate gradient method to solve for x {\displaystyle x} in F x = g {\displaystyle Fx=g} iteratively without explicit matrix inversion. Use backtracking line search to ensure the trust-region constraint is satisfied. Specifically, it backtracks the step size to ensure the KL constraint and policy improvement. That is, it tests each of the following test-solutions θ i + 1 = θ i + 2 ϵ x T F x x , θ i + α 2 ϵ x T F x x , θ i + α 2 2 ϵ x T F x x , … {\displaystyle \theta _{i+1}=\theta _{i}+{\sqrt {\frac {2\epsilon 