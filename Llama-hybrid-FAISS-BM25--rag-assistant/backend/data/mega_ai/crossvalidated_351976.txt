[site]: crossvalidated
[post_id]: 351976
[parent_id]: 307784
[tags]: 
I am no expert, but I see a few things. 1) You are using an astonishing number of linear activation functions in a row, eleven of them. The composite of two linear functions is just another linear function. You aren't actually doing much (in fact, you're just doing a linear regression) until you reach your last layer, which has a sigmoid activation function. In every neural network example I've seen, non-linear activation is used right away, starting at the top layer. The output layer might be linear, but the input layer (almost?) never is. (It appears you're trying to do a classification though, outputting a zero or a one, I think that a sigmoid activation on the output layer is fine for that purpose.) 2) By the time you finally reach the one sigmoid activation function you have, you're on layer 11, and you've also gone through eleven Dropout layers! I have never seen an architecture this deep, with such a small number of filters (you have seven filters per layer) and so much dropout. I calculate that any one path you might choose through your network has only a 5.4% probability of being connected. You're regularizing your (essentially linear) model thoroughly with all that dropout, but after that much dropout -- there's not much of a model left. 3) Finally, is this Keras? I think that you only have to specify the input_dim for the top layer when you use Keras.Sequential. When you call Keras.Sequential.add(), Keras should automatically infer the required input dimension of the new layer from the preceding layer. I don't know what errors you might introduce by overriding Keras' default input_dim. My suggestion would be to start with a much simpler architecture first and compare the results to these. Don't add dropout at first. It may be needed, but you don't know yet. Definitely use a non-linear activation function early. You have a 9-dimensional input. I usually like to start with a shallow network, with one hidden layer, with twice as many filters as I have input dimensions. So, something like this: model = Sequential() model.add(Dense(18, input_dim=X_train.shape[1], activation='relu')) model.add(Dense(1, activation='sigmoid')) model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',precision,recall,f1]) Really, that's all for a start.
