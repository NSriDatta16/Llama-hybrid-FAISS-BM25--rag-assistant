[site]: crossvalidated
[post_id]: 589681
[parent_id]: 
[tags]: 
Does neural network work this way in general?

Note: My knowledge of ML is very limited, but I appreciate detailed technical answers as long as they are wrapped in a more general understanding. I also know this is somewhat of a soft question, and I only appreciate any suggestions (as long as they're specific) to move it to another more appropriate forum, or other tags; this one is the most active and best for ML imo. In a recent video called "But what is a neural network REALLY?" , presumably named after the 3Blue1Brown series , it is claimed that why neural networks actually work comes down to the fact that we are fitting a sum of a bunch of piecewise linear functions (ReLU:s) to the data. More precisely each neuron is represented by a function $n_k=w_{k+2}\max(0,w_kx+b_k)+b_{k+1}$ and we sum more and more of these neurons up fit the data (points in 2d here). Questions: (i) Is this essentially how ANN:s and deep learning works in general and then what more precisely is the mysterious black box? Is it why backpropagation works as well as it does? (but that is essentially through gradient descent right?) (ii) I assume that the parameters $w_i$ are represented as weights in the network, but what is the network interpretation/representation of the $b_i$ :s? The reason of question (i) is that it is often argued that we don't really know why neural networks work and it is referred to as a bit of a black box (examples include the book Artificial Intelligence by Melanie Mitchell, but many more). I've also heard that Naftali Tishby et al.'s work on the Information Bottleneck, which I confess I have to read up more on, is an attempt to "unlock" this box. I also read the first answer to this question , where it reads: "Once we have set the values of the parameters we can try to look at what the neural network is doing by considering those building blocks to understand the inner logic of the system. This is what we sometimes are unable to do [...]". But it seems to me that summing up piecewise linear functions and minimizing the error via backpropagation is an explanation. We don't usually call other regression methods, such as LSE black boxes. Basically, if one were to form research questions about this what would they include? "Why does part ___ of a network of type ___ work as well as it does?" or something similar.
