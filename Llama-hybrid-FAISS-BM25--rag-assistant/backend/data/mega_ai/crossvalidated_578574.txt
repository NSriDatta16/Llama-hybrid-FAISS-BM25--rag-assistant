[site]: crossvalidated
[post_id]: 578574
[parent_id]: 
[tags]: 
How are machine learning diffusion models "learned"?

Let me take as a reference the paper that introduced diffusion models and let's take as a test case a Gaussian diffusion model. The reverse trajectory of a Gaussian Markov chain will have the same distributional form as the forward trajectory, thus only the mean and the covariance of the reverse trajectory need to be learned to specify its behavior. Following the notation given in appendix B the reverse diffusion kernel is given by $$p(x^{(t-1)}| x^{(t)}) = N( x^{(t-1)}; f_{\mu} ( x^{(t)}, t ) , f_{\Sigma} ( x^{(t)}, t ) ) $$ where $f_{\mu} ( x^{(t)}, t ) , f_{\Sigma} ( x^{(t)}, t )$ are two function chosen as multilayer perceptrons (in this case). The model is then "trained" by optimizing a lower bound $K$ of the loglikelihood $L$ where $$L = \int d x^{(0)} q( x^{(0)}) \log p( x^{(0)}) $$ and where $q( x^{(0)})$ is the distribution of the (unaltered) data. Two questions: Why is the model loglikelihood specified as $\int d x^{(0)} q( x^{(0)}) \log p( x^{(0)})$ ? I would have thought that the loglikelihood was only $\log p( x^{(0)})$ . How is this integral form of the loglikelihood recovered? Am I correct that the "training" consists of (only) optimizing $K$ wrt to all the parameters used in the multilayer perceptrons $f_{\mu} ( x^{(t)}, t ) , f_{\Sigma} ( x^{(t)}, t )$ ? There is no training set or any labels, we "simply" run an optimization algo on $K$ wrt the parameters used to define $f_{\mu} ( x^{(t)}, t )$ and $f_{\Sigma} ( x^{(t)}, t )$ ?
