[site]: stackoverflow
[post_id]: 3851441
[parent_id]: 3838785
[tags]: 
A simple solution would be collapse the time aspect of your data and take each timestamp as one instance. In this case, the values of the sensors are considered your feature vector, where each time step is labeled with a class value of category A or B (at least for the labeled training data): sensors | class A B C D E | ------------------------- 1 1 1 0 0 | catA 1 0 0 0 0 | catB 1 1 0 1 0 | catB 1 1 0 0 0 | catA .. This input data is fed to the usual classification algorithms (ANN, SVM, ...), and the goal is to predict the class of unlabeled time series: sensors | class A B C D E | ------------------------- 0 1 1 1 1 | ? 1 1 0 0 0 | ? .. An intermediary step of dimensionality reduction / feature extraction could improve the results. Obviously this may not be as good as modeling the time dynamics of the sequences, especially since techniques such as Hidden Markov Models (HMM) take into account the transitions between the various states. EDIT Based on your comment below, it seems that the best way to get less transitory predictions of the target class is to a apply a post-processing rule at the end of the prediction phase, and treating the classification output as a sequence of consecutive predictions. The way this works is that you would compute the class posterior probabilities (ie: probability distribution that an instance belong to each class label, which in the case of binary SVM are easily derived from the decision function), then given a specified threshold, you check if the probability of the predicted class is above that threshold: if it is we use that class to predict the current timestamp, if not then we keep the previous prediction, and the same goes for future instances. This has the effect of adding a certain inertia to the current prediction.
