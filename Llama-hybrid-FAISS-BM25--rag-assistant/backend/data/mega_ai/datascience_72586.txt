[site]: datascience
[post_id]: 72586
[parent_id]: 
[tags]: 
Decision Trees and Feature Selection

I'm trying to experiment with the performance of different machine learning algorithms before and after applying feature selection. I tested SVM, Random Forest, KNN, Linear Regression, and, Decision Tree and I used Random Forest as a feature selection method by utilizing the function (SelectFromModel) provided by sklearn. All the algorithms improved significantly after applying feature selection, except the Decision Tree. I'm trying to find an explanation of why that happened? Does it mean that Decision Tree needs a large number of features to produce a good model?
