[site]: crossvalidated
[post_id]: 218967
[parent_id]: 218781
[tags]: 
First, note that randomizer seed might be an argument in the implementation of Random Forest you're using, but it is not a parameter of the RF algorithm itself (it just sets the generator of pseudo-random numbers of the machine in a state so that the results are reproducible). Also, the abbreviations you're using ($I$, $K$, etc.) are not standard and seem to be specific to your implementation. Anyhow, regardless of notation issues, the two main parameters of RF are the number of trees grown and the number of predictors randomly tried at each split. What you call depth is sometimes found as the maximum node size, and controls the size of the trees that are grown. In the original implementation of RF, trees are grown to the maximum potential extent so that they reach the lowest possible bias. Then, variance is reduced by growing many trees and averaging them. The reason is that in RF, you can only decrease error by reducing the variance (where $error = bias + variance$), so the bias needs to be as low as possible in the first place. Therefore, in most cases, you don't really need to adjust this parameter (depth or max nodesize). Just make sure the default value allows the trees to grow as deep as possible. Note that in some instances, it seems that slightly reducing the sizes of the trees can help in reducing overfitting (see the comments in that thread ). Finally, for machine learning algorithms such as RF, Boosting, etc., hyper-parameters and parameters are the same things (the proper name would be hyperparameters though). There is a slight semantic difference between the two when dealing with probability distributions. See for instance: What do we mean by hyperparameters? What exactly is a hyperparameter? What's in a name: hyperparameters Also, somehow related: Are Random Forest and Boosting parametric or non-parametric?
