[site]: crossvalidated
[post_id]: 324465
[parent_id]: 283303
[tags]: 
As I understand, the estimated error would be biased in the repeated cross validation example.... However if I am only interested in choosing the best model is nested cross validation really necessary? Short Answer: Your understanding is correct. Nested cross-validation is not necessary to pick the best model (out of the models considered). However, are you sure that this is what you want to do? Long Answer: As discussed here in similar terms, you can simplify your question by thinking of hyperparameter selection and model selection (i.e., Neural Network (NN), KNN, or SVM) as the same question; that is, you can think of selecting a single model from the big set $$\left\{ NN_1, ...., NN_{p_{NN}}, KNN_1,...,KNN_{p_{KNN}}, ... SVM_1,...,SVM_{p_{SVM}} \right\}$$ (where the subscript is used to index the different hyperparameter combinations). This should make it clear why you only need one cross-validation loop to select the best model. As you suspected, nested cross-validation is needed only to obtain an unbiased estimate of error. That said, I think it's very unlikely that you would want to do this in practice (except as a (nested) step in some larger validation procedure), as this leaves you vulnerable to over-fitting (especially if the number of candidate hyperparameters is large relative to the sample size). Regardless of the estimated error, your final model, fit to the whole data, could perform absolutely terribly when confronted with new data -- you would have no way of knowing.
