[site]: crossvalidated
[post_id]: 594266
[parent_id]: 576759
[tags]: 
Yes, it makes sense. Under the null hypothesis, all categories have the same probability, which corresponds to the model that does not even have an intercept (equivalently, a model with an intercept equal to zero). Then the alternative hypothesis allows the categories to have different probabilities, which corresponds to the model with an intercept (equivalent to a model with an intercept allowed to fit the data instead of being specified). The softmax function in multinomial logistic regression makes the math messy, but in regular logistic regression with just two outcome categories, we can think of this as testing if the log-odds is zero (corresponds to 50/50 chance, so equal probability or even odds of each class), versus if the log-odds is not zero (corresponding to something other than equal probability of each class). (Unlike in the two-sample case where I first did this, there is no additional parameter beyond the intercept, since the goal is not to compare two groups.) Like so many hypothesis tests (e.g., ANOVA F-test that compares the sum of squared residuals of nested models), this then becomes a matter of comparing how well the models fit the data. If the intercept provides a considerable improvement in fit, which is quantified by the $\chi^2$ test statistic and resulting p-value, then we declare the added intercept parameter to be significantly different from zero, corresponding to unequal probabilities of class membership. Interestingly, none of this should change if we want to test a different one-sample hypothesis. If we want to test if the probabilities are $(0.25,0.25,0.5)$ , then we can just figure out the parameters giving these values in the intercept of the regression, calculate the likelihood of such a model, allow the intercept to fit the data in an unrestricted model, calculate the likelihood, and compare the likelihood as before to determine if the assumed intercept can be shown to be incorrect, same as we did when we aimed to show an all-zero intercept to be false. Overall, this seems like a completely reasonable approach to one-sample testing of a categorical distribution.
