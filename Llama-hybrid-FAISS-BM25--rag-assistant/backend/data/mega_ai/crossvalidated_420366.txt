[site]: crossvalidated
[post_id]: 420366
[parent_id]: 420177
[tags]: 
You want to identify "variables that are most strongly related to the outcomes of complaints against practitioners in a profession," but not to predict future outcomes of complaints. Presumably, the idea is to generate hypotheses about factors that might be manipulated in future work to reduce undesirable outcomes. Cross-validation to choose a LASSO model, combined with bootstrapping to gauge the stability of the set of selected predictors, provides one useful approach to this type of problem. LASSO is typically combined with cross validation to choose the number of predictor variables maintained in the model, based on optimizing an appropriate measure of cross-validated error; deviance is a good choice of measure for logistic regression. In practice, you might find a model similar to one developed by stepwise selection, but the penalization of regression coefficients in LASSO avoids the overfitting with stepwise selection that sends shudders up the spines of statisticians. If the candidate predictors are correlated, those maintained by any selection scheme can be highly dependent on the particular sample at hand. So it's also important to get a sense of how stable your set of selected predictors might be if you could take more samples from the underlying population. Bootstrapping is the next best thing to taking more samples from the population. Bootstrap sampling with replacement from the data at hand is a reasonable approximation to taking more samples from the population. So you repeat the entire LASSO model-building process, with its inherent cross-validation to choose predictors, on multiple bootstrap samples from your data set. Then you can see how frequently individual predictors are kept or omitted. That will give you an idea about which predictors might deserve the most focused future attention. That process is reasonably simple to automate with simple scripts. An Introduction to Statistical Learning works through using cross-validation to optimize LASSO in Section 6.6.2; that example is for linear regression but the approach is the same for logistic regression, with deviance minimized. Statistical Learning with Sparsity illustrates bootstrapping to evaluate the stability of predictors chosen by LASSO and their coefficient values in Section 6.2. As I noted in a comment, the issue of traditional inference (p-values, confidence intervals, etc) in models that used the data to select predictors is difficult. Chapter 20 of Statistical Learning with Sparsity goes into the problems. As you seem to be primarily interested in using the present data to direct future work, however, that might not be a big issue for you.
