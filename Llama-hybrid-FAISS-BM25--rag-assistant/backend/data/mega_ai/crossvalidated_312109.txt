[site]: crossvalidated
[post_id]: 312109
[parent_id]: 312095
[tags]: 
According to Auto-Encoding Variational Bayes (eq. 3), the loss function of a variational autoencoder The expectation term is usually an intractable integral, so we want to approximate this expected value by drawing samples then computing the average. The random value is added for generating samples from $q_\phi(z|x^{(i)})$, theoretically we should draw a large number of random values for an accurate approximation, but since the training usually takes thousands of iterations, we can use only one random value per input instance. On the other hand, if we use the mean value instead, it'll no longer be the same loss function, as $$E[f(x)]\neq f(E[x]).$$ Update In the original autoencoder models the data likelihood $p_\theta(x)$ as a whole so it's convenient to use mean square error or binary cross entropy to give a likelihood to optimize. In the variational autoencoder we introduced the latent variable $z$, and want to model $q_\phi(z|x)$ (encoder) and $p_\theta(x|z)$ (decoder) jointly. The difficulty in this case is $p(x)$ can no longer be simply computed through one pass of the network as in the original autoencoder. The paper addresses this by maximizing a variational lower bound of $p(x)$ (the above loss function), which contains an intractable expectation term which needs to be approximated using sampling and the "reparameterization trick". I just found a very good Variational Autoencoder course on coursera , it mentions exactly your question in the forth video.
