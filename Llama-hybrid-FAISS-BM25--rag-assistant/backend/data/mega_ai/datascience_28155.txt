[site]: datascience
[post_id]: 28155
[parent_id]: 28120
[tags]: 
Prior to GAP, one would flatten your tensor and then add a few fully connected layers in your model. The problem is that a bunch of parameters in your model end up being attributed to the dense layers and could potentially lead to overfitting. A natural solution was to add dropout to help regulate that. However a few years ago, the idea of having a Global Average Pooling came into play. GAP can be viewed as alternative to the whole flatten FC Dropout paradigm. GAP helps prevent overfitting by doing an extreme form of reduction. Given a H X W X D tensor, GAP will average the H X W features into a single number and reduce the tensor into a 1 X 1 X D tensor. The original paper simply applied GAP and then a softmax. However, it's now common to have GAP followed by a FC layer.
