[site]: crossvalidated
[post_id]: 355931
[parent_id]: 355929
[tags]: 
In general, optimization methods do not care about computational cost when computing the objective function. However, there are some strategies to deal with this case, which usually can be used with any optimization method. For references, you can read some papers: Efficient optimization of computationally expensive objective functions Survey of modeling and optimization strategies to solve high-dimensional design problems with computationally-expensive black-box functions Note that , there are many ways to realize a speedup strategy. For example, with the strategy of approximating the objective function. The first cited paper above uses recursive surrogate objective function. Another common way is Bayesian approximation by Gaussian process, combining it with an exploration-exploitation strategy to guess the optima position (not using gradient information) resulting in Bayesian optimization approach, as suggested by @FabianWerner in comment. For your specific problem, you can choose which speedup strategies and optimization method you want to use and how to realize them. In deep learning, a general strategy is to do massive parallelizing. For example, generating self-play games in AlphaZero is done on multiple GPUs/TPUs. Parameter update is usually only partially synchronized, but it will eventually approximately converge.
