[site]: datascience
[post_id]: 29296
[parent_id]: 29295
[tags]: 
In neural networks applied to natural language processing normally each possible word (or sub-words) is handled as an individual token, and normally the vocabulary (the set of supported tokens) is around 32K. These tokens are typically one-hot encoded. Therefore, there is not inherent problem in having one-hot encoded vectors with thousands of components . However, the amount and variety of training data should support the dimensionality of the model for it not to overfit. Note that in NLP, these one hot encodings are used as expected outputs of the model, over which to compute maximum likelihood (categorical cross-entropy) together with the output of the model (a softmax). Depending on how you are using this portfolio information, another option would be to have an embedding layer , which embeds discrete values into a continuous representation space (whose dimensionality is a hyperparameter of the model). This would be possible if this information is used as input to the model. In those cases, you input an integer number, which is used to index the embedded vector table.
