[site]: crossvalidated
[post_id]: 934
[parent_id]: 886
[tags]: 
If statistics is all about maximizing likelihood, then machine learning is all about minimizing loss. Since you don't know the loss you will incur on future data, you minimize an approximation, ie empirical loss. For instance, if you have a prediction task and are evaluated by the number of misclassifications, you could train parameters so that resulting model produces the smallest number of misclassifications on the training data. "Number of misclassifications" (ie, 0-1 loss) is a hard loss function to work with because it's not differentiable, so you approximate it with a smooth "surrogate". For instance, log loss is an upper bound on 0-1 loss, so you could minimize that instead, and this will turn out to be the same as maximizing conditional likelihood of the data. With parametric model this approach becomes equivalent to logistic regression. In a structured modeling task, and log-loss approximation of 0-1 loss, you get something different from maximum conditional likelihood, you will instead maximize product of (conditional) marginal likelihoods. To get better approximation of loss, people noticed that training model to minimize loss and using that loss as an estimate of future loss is an overly optimistic estimate. So for more accurate (true future loss) minimization they add a bias correction term to empirical loss and minimize that, this is known as structured risk minimization. In practice, figuring out the right bias correction term may be too hard, so you add an expression "in the spirit" of the bias correction term, for instance, sum of squares of parameters. In the end, almost all parametric machine learning supervised classification approaches end up training the model to minimize the following $\sum_{i} L(\textrm{m}(x_i,w),y_i) + P(w)$ where $\textrm{m}$ is your model parametrized by vector $w$, $i$ is taken over all datapoints $\{x_i,y_i\}$, $L$ is some computationally nice approximation of your true loss and $P(w)$ is some bias-correction/regularization term For instance if your $x \in \{-1,1\}^d$, $y \in \{-1,1\}$, a typical approach would be to let $\textrm{m}(x)=\textrm{sign}(w \cdot x)$, $L(\textrm{m}(x),y)=-\log(y \times (x \cdot w))$, $P(w)=q \times (w \cdot w)$, and choose $q$ by cross validation
