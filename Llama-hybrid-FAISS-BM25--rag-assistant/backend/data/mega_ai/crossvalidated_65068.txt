[site]: crossvalidated
[post_id]: 65068
[parent_id]: 65059
[tags]: 
What is more important is neither R Square nor accuracy as you measure it, but that your model is structured correctly. As Frank pointed out when you transform variables, models are not comparable. One thing to watch for is the nature of your dependent variable Y. When you have a dependent variable that consists of time series that grow over time (many do such as macroeconomic variables such as GDP, CPI, etc...); a model using a dependent variable that is a nominal level (GDP size) will always have very high R Square and low Standard Error. However, such models are heteroskedastic by definition. This means you can't measure accurately the statistical significance of any of your variables because the variance around any of their regression coefficients is not stable enough. As a result the estimated Confidence Interval is wrong. Also, such model's residual would not be randomly distributed. They would trend. You could measure that by looking at the Durbin Watson score or simply by measuring the autocorrelation of the residuals (because such residuals are strongly autocorrelated). In the above case, you would have to change your dependent variable from GDP level to % change in GDP from one period to the next. This eliminates heteroskedasticity, autocorrelation of residuals. But, such a model will typically have a far lower R Square, higher (relative) standard error, and overall accuracy as you would measure it than the first model. Yet, this second model is much better because it has a more robust structure and does not break the underlying assumptions of linear regression. Meanwhile, the first model using nominal GDP would appear to be more accurate, have a better fit, etc... Yet, it be all wrong because it does break the underlying assumptions of linear regression (the problems mentioned above). Here is another thought assumming both your models have a robust and comparable structure: test them using Hold Out sample. You could rerun both your models using let's say 10 data points as Hold Out... and doing this several times with different Hold Out samples. Then, look at which model performs best. Performance in Hold Out sample is probably a lot more important than any statistics of fit or accuracy over the learning sample. Given the very low R Square of both of your models, I suspect they will both perform equally badly in Hold Out. But, in any case doing Hold Out is a good way to differentiate between models.
