[site]: datascience
[post_id]: 32330
[parent_id]: 6547
[tags]: 
I recently developed a toolbox: Py thon O utlier D etection toolbox ( PyOD ). See GitHub . It is designed for identifying outlying objects in data with both unsupervised and supervised approaches. PyOD is featured for: Unified APIs, detailed documentation, and interactive examples across various algorithms. Advanced models, including Neural Networks/Deep Learning and Outlier Ensembles. Optimized performance with JIT and parallelization when possible, using numba and joblib. Compatible with both Python 2 & 3 (scikit-learn compatible as well). Here are some important links: Github PyPI Documentation Interactive Jupyter Notebooks If you use PyOD in a scientific publication, we would appreciate citations to the following paper @article{zhao2019pyod, title={PyOD: A Python Toolbox for Scalable Outlier Detection}, author={Zhao, Yue and Nasrullah, Zain and Li, Zheng}, journal={arXiv preprint arXiv:1901.01588}, year={2019}, url={https://arxiv.org/abs/1901.01588} } It is currently under review at JMLR (machine learning open-source software track). See preprint . Quick Introduction PyOD toolkit consists of three major groups of functionalities: (i) outlier detection algorithms; (ii) outlier ensemble frameworks and (iii) outlier detection utility functions. Individual Detection Algorithms : PCA : Principal Component Analysis (the sum of weighted projected distances to the eigenvector hyperplanes) MCD : Minimum Covariance Determinant (use the mahalanobis distances as the outlier scores) OCSVM : One-Class Support Vector Machines LOF : Local Outlier Factor CBLOF : Clustering-Based Local Outlier Factor LOCI : LOCI: Fast outlier detection using the local correlation integral HBOS : Histogram-based Outlier Score kNN : k Nearest Neighbors (use the distance to the kth nearest neighbor as the - **outlier score AvgKNN : Average kNN (use the average distance to k nearest neighbors as the outlier score) MedKNN : Median kNN (use the median distance to k nearest neighbors as the outlier score) ABOD : Angle-Based Outlier Detection FastABOD : Fast Angle-Based Outlier Detection using approximation SOS : Stochastic Outlier Selection IForest : Isolation Forest Feature Bagging LSCP : LSCP: Locally Selective Combination of Parallel Outlier Ensembles XGBOD : Extreme Boosting Based Outlier Detection (Supervised) AutoEncoder : Fully connected AutoEncoder (use reconstruction error as the outlier score) SO_GAAL : Single-Objective Generative Adversarial Active Learning MO_GAAL : Multiple-Objective Generative Adversarial Active Learning Outlier Detector/Scores Combination Frameworks : Feature Bagging LSCP : LSCP: Locally Selective Combination of Parallel Outlier Ensembles Average : Simple combination by averaging the scores Weighted Average : Simple combination by averaging the scores with detector weights Maximization : Simple combination by taking the maximum scores AOM : Average of Maximum MOA : Maximization of Average Utility Functions for Outlier Detection : score_to_lable() : convert raw outlier scores to binary labels precision_n_scores() : one of the popular evaluation metrics for outlier mining (precision @ rank n) generate_data() : generate pseudo data for outlier detection experiment wpearsonr() : weighted pearson is useful in pseudo ground truth generation Comparison of all implemented models are made available below: ( Figure , Code , Jupyter Notebooks ): If you are interested, please check Github for more information.
