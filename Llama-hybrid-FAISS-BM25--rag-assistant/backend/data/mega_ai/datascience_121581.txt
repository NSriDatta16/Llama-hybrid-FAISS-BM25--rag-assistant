[site]: datascience
[post_id]: 121581
[parent_id]: 
[tags]: 
find q-table for discrete action space

I am trying to use q-learning for a discrete observation space that is represented by: buffer: list of 200 integer values in [0,10] discard_counter: list of 200 integer values in [0, 4] capacity: list of 30integer values in [-1,10] I think buffer and discard counter can be combined into one 2D array as for every buffer entry there is a value and a discard counter. So in order to represent all states I use the following method: def obs_to_state(self): #maps obs to int value, because i am unable to think in 6 dimensions obs = self.env.get_obs() arr1 = np.zeros((200, 11, 5)) # array representing buffer and discard_counter for i in range(len(obs['buffer'])): dc = obs['discard_counter'][i] val = obs['buffer'][i] arr1[i, val, dc] = 1 arr1 = list(arr1.flatten()) arr2 = np.zeros((30, 12)) # capacity array for i, v in enumerate(obs['capacity']): arr2[i, v] = 1 arr2 = list(arr2.flatten()) return arr1 + arr2 this method gives me a list of length 200*11*5*30*12=11360. However each of these values can be either 0 or 1 (and most combinations are possible states that can be reached). This results in overall 2**11360 possible states which is a number that definitely is too big for using q-learning with a q-table. In order to create the q-table I have to create an array of size 11360*2 (there are only 2 actions). Am I missing something or is q-learning not a good idea for this task? Here is the description of the task and the code I have so far: A startup wants to run multiple workflows simultaneously. Given the low budget of the company, only one local resource (LR) and some EC2 instances are available to run all the workflow tasks. The scheduler that coordinates the execution wants to learn when a particular task should be executed locally or offloaded to the cloud. Consider the following restrictions: Tasks durations are discrete values between 1 and 10 time slots. LR capacity is 30 time slots. If capacity is not enough, tasks will be discarded. Processing rate of LR is 2 time slots per Q-learning iteration. Offloading to cloud costs 4 time slots. An unlimited buffer can be used to queue tasks until LR is available. Buffered tasks are discarded after 4 time slots. The task arrival frequency is totally up to you. Create a custom Gym environment out of your MDP model. Implement Q-learning and obtain the optimal policy that maximize the cumulative reward. Code: The Agent class is work in progress and num_states needs to be 2**11360 but my Laptop cannot do this. import gym from gym import spaces from gym.envs.registration import register import random from gym.utils.env_checker import check_env import numpy as np from itertools import product # source: https://www.gymlibrary.dev/content/environment_creation/ # source: https://www.gymlibrary.dev/content/basic_usage/ class Env(gym.Env): def __init__(self) -> None: super().__init__() self.N = 200 self.step_cnt = 0 self.schedule_time = 2 self.reward = 0 self.observation_space = spaces.Dict({ "capacity": spaces.Box(low = 0, high = 10, shape=(30,), dtype=int), "buffer": spaces.Box(low = 0, high = 10, shape=(self.N,), dtype=int), "discard_counter": spaces.Box(low = 1, high = self.schedule_time, shape=(self.N,), dtype=int) }) # execute (1) or offload (2) self.action_space = spaces.Discrete(2) def get_obs(self): return({ "capacity": self.capacity, "buffer": self.buffer, "discard_counter": self.discard_counter }) def create_tasks(self): result = [] while self.N > 0: # Generate a random self.N between 1 and 10 (inclusive) value = random.randint(1, 10) value = min(value, self.N) result.append(value) self.N -= value return result def reset(self, seed = None, options = None): #super.reset(seed = seed) super has no reset method... self.capacity = [0 for x in range(30)] self.buffer = [0 for x in range(self.N)] self.discard_counter = [0 for x in range(self.N)] self.tasks = self.create_tasks() observation = self.get_obs() return observation def is_terminated(self): if(sum(self.capacity) + sum(self.buffer) + sum(self.tasks) == 0): return True return False def update_buffer(self): # deal with discarded tasks for i, t in enumerate(self.discard_counter): if t == 0 and self.buffer[i] > 0: self.buffer[i] = 0 self.reward -= 1000 elif self.buffer[i] != 0: self.discard_counter[i] -= 1 # move tasks to next position if possible if sum(self.buffer) != 0: while self.buffer[0] == 0: for i in range(len(self.buffer) - 1): self.buffer[i] = self.buffer[i+1] self.discard_counter[i] = self.discard_counter[i+1] self.buffer[-1] = 0 def update_capacity(self, action): # fill capacity from buffer #while sum(self.buffer) > 0 and sum(self.capacity) len(self.capacity): break for j, x in enumerate(self.capacity): if x == 0: self.capacity[j] = buffer_value self.discard_counter[i] = 0 self.buffer[i] = 0 self.update_buffer() # capacity empty if sum(self.capacity) ```
