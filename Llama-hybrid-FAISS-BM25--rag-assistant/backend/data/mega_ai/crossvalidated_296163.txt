[site]: crossvalidated
[post_id]: 296163
[parent_id]: 
[tags]: 
Bayes optimal decision for logistic regression: Self-study exercise

We want to find the Bayes optimal decision for logistic regression. That means that the goal is to find the actions, which minimize our expected loss (also often called expected cost or risk). Here are the the characteristics: $ \begin{equation} \text{Conditional distribution $P(y|x)$} = \begin{cases} \sigma (\mathbf{w}^T\mathbf{x}), & \text{if}\ y=1 \\ 1-\sigma (\mathbf{w}^T\mathbf{x}), & \text{if y= -1} \end{cases} \end{equation}$ Action set = $\{-1,1,D\}$ $ \begin{equation} \text{Cost-function $C(y,a)$} = \begin{cases} (\mathbf{1}_{y \neq a}), & \text{if a $\in$ {1,-1}}\ \\ c Where $\sigma$ is the Sigmoid-function used in the context of logistic regression. D means that "we don't know". The cost function is written down as follows in our solutions: $(*) \mathbb{E}[C(a,y)|x] = \int C(y,a)P(y|x)dy = \sum_y (\mathbb{1}[y \neq a]\mathbb{1}[a \neq D]+c*\mathbb{1}[D=a])P(y|x)$ My question The way (*) is written, it reads to me as: $ (\mathbb{1}[y_1 \neq a]\mathbb{1}[a \neq D]P(y|x)+c*\mathbb{1}[D=a]P(y|x) + (\mathbb{1}[y_{-1} \neq a]\mathbb{1}[a \neq D]P(y|x)+c*\mathbb{1}[D=a]P(y|x)$ So we are summing up the cost for D twice - is this not wrong?
