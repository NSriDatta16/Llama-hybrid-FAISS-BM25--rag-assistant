[site]: stackoverflow
[post_id]: 4924942
[parent_id]: 4915675
[tags]: 
How big is your database in total? What kind of tables are you using? A big risk with backups using mysqldump has to do with table locking, and updates to tables during the backup process. The mysqldump backup process basically works as follows: For each table { Lock table as Read-Only Dump table to disk Unlock table } The danger is that if you run an INSERT/UPDATE/DELETE query that affects multiple tables while your backup is running, your backup may not capture the results of your query properly. This is a very real risk when your backup takes hours to complete and you're dealing with an active database. Imagine - your code runs a series of queries that update tables A,B, and C. The backup process currently has table B locked. The update to A will not be captured, as this table was already backed up. The update to B will not be captured, as the table is currently locked for writing. The update to C will be captured, because the backup has not reached C yet. This is an easy way to destroy referential integrity in your database. Your backup process needs to be atomic, and transactional. If you can't shut down the entire database to writes during the backup process, you're risking disaster. Also - there must be something wrong here. At a previous company, we were running nightly backups of a 450G Mysql DB (largest table had 150M rows), and it took less than 6 hours for the backup to complete. Two thoughts: Do you have a slave database? Run the backup from there - Stop replication (preventing RW risk), run the backup, restart replication. Are your tables using InnoDB? Consider investing in InnoDBhotbackup , which solves this problem, as the backup process leverages the journaling that is part of the InnoDB storage engine.
