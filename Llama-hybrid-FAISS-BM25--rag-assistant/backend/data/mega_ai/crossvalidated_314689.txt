[site]: crossvalidated
[post_id]: 314689
[parent_id]: 
[tags]: 
What is the purpose of giving a probabilistic interpretation of linear and logistic regression?

I can write the cost functions for both linear regression and logistic regression and minimize them using gradient descent without using any probabilistic notions. In the case of linear regression the cost function is: $$ \displaystyle J(\theta) = \frac{1}{2m} \displaystyle\sum\limits_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})^2 $$ For logistic regression the cost function is: $$ \displaystyle J(\theta) = -\frac{1}{m} \left[\sum\limits_{i=1}^m y^{(i)}log(h_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)})) \right] $$ The intuition for both cost functions is to penalize models which have large errors between the real response and the predicted response. One other important property of these functions is that they are convex which makes it easy to find the global minimums. These cost functions can also be derived using the maximum likelihood method under certain assumptions. But what is the purpose of giving them a probabilistic interpretation? Are there any (theoretical or practical) benefits?
