cientists have also found that hallucinations can serve as a valuable tool for scientific discovery, particularly in fields requiring innovative approaches to complex problems. At the University of Washington, David Baker's lab has used AI hallucinations to design "ten million brand-new" proteins that don't occur in nature, leading to roughly 100 patents and the founding of over 20 biotech companies. This work contributed to Baker receiving the 2024 Nobel Prize in Chemistry, although the committee avoided using the "hallucinations" language. In medical research and device development, hallucinations have enabled practical innovations. At California Institute of Technology, researchers used hallucinations to design a novel catheter geometry that significantly reduces bacterial contamination. The design features sawtooth-like spikes on the inner walls that prevent bacteria from gaining traction, potentially addressing a global health issue that causes millions of urinary tract infections annually. These scientific applications of hallucinations differ fundamentally from chatbot hallucinations, as they are grounded in physical reality and scientific facts rather than ambiguous language or internet data. Anima Anandkumar, a professor at Caltech, emphasizes that these AI models are "taught physics" and their outputs must be validated through rigorous testing. In meteorology, scientists use AI to generate thousands of subtle forecast variations, helping identify unexpected factors that can influence extreme weather events. At Memorial Sloan Kettering Cancer Center, researchers have applied hallucinatory techniques to enhance blurry medical images, while the University of Texas at Austin has utilized them to improve robot navigation systems. These applications demonstrate how hallucinations, when properly constrained by scientific methodology, can accelerate the discovery process from years to days or even minutes. Mitigation methods The hallucination phenomenon is still not completely understood. Researchers have proposed that hallucinations are inevitable and are an innate limitation of large language models. Therefore, there is still ongoing research to try to mitigate its occurrence. Particularly, it was shown that language models not only hallucinate but also amplify hallucinations, even for those which were designed to alleviate this issue. Researchers from OpenAI wrote that hallucinations occur because the training and evaluation of LLMs reward guessing over acknowledging uncertainty, and proposed modifying the scoring of benchmarks. Ji et al. divide common mitigation methods into two categories: data-related methods and modeling and inference methods. Data-related methods include building a faithful dataset, cleaning data automatically, and information augmentation by augmenting the inputs with external information. Model and inference methods include changes in the architecture (either modifying the encoder, attention, or the decoder in various ways); changes in the training process, such as using reinforcement learning; and post-processing methods that can correct hallucinations in the output. Researchers have proposed a variety of mitigation measures, including getting different chatbots to debate one another until they reach consensus on an answer. Another approach proposes to actively validate the correctness corresponding to the low-confidence generation of the model using web search results. They have shown that a generated sentence is hallucinated more often when the model has already hallucinated in its previously generated sentences for the input, and they are instructing the model to create a validation question checking the correctness of the information about the selected concept using Bing search API. An extra layer of logic-based rules was proposed for the web search mitigation method, by using different ranks of web pages as a knowledge base, which differ in hierarchy. When there are no external data sources a