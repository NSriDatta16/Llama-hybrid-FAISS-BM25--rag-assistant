[site]: datascience
[post_id]: 112846
[parent_id]: 112840
[tags]: 
The typical approach for this is follow BERT 's approach: add an extra special token at the beginning of the input sequence (in BERT it is [CLS] ) and only use the output of the network at that position as input to your fully connected layer. The output at the rest of the positions is ignored. You can see a nice illustration of this approach in the notorious blog post The Illustrated BERT , which explains very visually all the details about BERT: In the illustration, you can see the model input at the bottom and how it has been added a special [CLS] token at the beginning and then the output of the model at that position is then used for a classification task. During training, the model will learn to condense the needed information from the whole sentence into the output of the first position. Another alternative, as you pointed out, is to have global average pooling over all the outputs. This was the norm in the LSTM times before Transformers came. I am not aware of any articles comparing the performance of both approaches but, nowadays, with Transformers, everybody uses the BERT approach. Both BERT's approach and the global average/max pooling approach achieve your goal: collapsing the variable length sequence of vectors into a single vector that you can then use for classification.
