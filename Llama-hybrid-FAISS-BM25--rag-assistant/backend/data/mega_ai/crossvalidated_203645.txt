[site]: crossvalidated
[post_id]: 203645
[parent_id]: 
[tags]: 
How to fix this implementation of Bayesian regularization for ANNs?

I have implemented the Levenberg-Marquard algorithm (from Hagan's "Artifical Neural Network Design" -- 2014) for a two layer network with 20 neurons in the hidden layer. This network can beautifully overfit, therefore, I wanted to implement Bayesian regularization. Below I explain what I implemented. However, it does not work for me (I cannot reproduce some plots in the book). Bad result The training stops after about 20 iterations, whereas in the book at about 100 My inital values for $\frac{\alpha}{\beta}$ start at about 5 and only rise to about 10, where they should start with order 0.1 and settle at around 0.01 $\gamma$ rises from 46 to 58, whereas in the book it drops from 61 to 5 Question What am I doing wrong? In the top right plot of the picture below, the red dots are the training points and the blue ones denote the network response at the latest iteration: Manual regularization It is worth noting, that when I "manually" apply the 0.01 as regularization value, i.e. $\frac{\alpha}{\beta}=0.01$ , I observe consistent results. I implement this by using the "old" Jacobian (for $F\left(\bar{\omega}\right)=E_{D}$ ) to determine the direction of the next step $d\bar{\omega}$ . However, to determine the step size such that $F\left(\bar{\omega}+\texttt{d}\bar{\omega}\right) - F\left(\bar{\omega}\right) , I make use of the regularised $F\left(\bar{\omega}\right)=E_{D} + 0.01\,E_{W}$ . The picture below demonstrates this and is satisfactory: Algorithm Notation $\bar{t}_{q}$ : target (vector) of $q$ th input training point $\bar{a}_{q}$ : network output (vector) for $q$ th input training point $E_{D} = \sum\,\left(\bar{t}_{q}-\bar{a}_{q}\right)^{2}$ (sum squared error) $E_{W}$ = $\sum\,\omega_{i}^{2}$ sum of squared network weights $\gamma = n - 2\alpha\,\texttt{trace}\left(H^{-1}\right)$ , this is the "effective number of parameters". $H$ is the (approximated) Hessian matrix of $F\left(\bar{\omega}\right)$ (see below). $\alpha = \frac{\gamma}{2E_{W}}$ $\beta = \frac{N-\gamma}{2E_{D}}$ $F\left(\bar{\omega}\right) = \beta\,E_{D} + \alpha\,E_{W}$ regularised objective function, with penalty for large weights $\bar{\omega}$ Schema I implemented the algorithm as follows: Initialize the network weights $\omega_{i}$ randomly, calculate $E_{D}$ and $E_{W}$ , set $\gamma$ to $n$ and compute $\alpha$ and $\beta$ as above Take a step in the Levenberg-Marquard algorithm towards minimizing $F\left(\bar{\omega}\right)$ . Compute $\gamma$ with the new values for $E_{D}$ and $E_{W}$ (using the newly calculated weights $\bar{\omega}$ ). Compute new values for $\alpha$ and $\beta$ , using the new values for $\gamma$ , $E_{D}$ and $E_{W}$ . Iterate step 2-4 until converge I am trying to apply this algorithm to a function $1+\texttt{sin}\left(\frac{1}{2}\pi\,x\right) + \epsilon$ , where I take only 21 evenly distributed points on $[-2, 2]$ where $\epsilon$ is a random number from a gaussian distribution with mean $0$ and standard deviation $0.25$ (like in the book). Jacobian To calculate the Jacobian for $F\left(\bar{\omega}\right)=\beta\,E_{D}+\alpha\,E_{W}$ (in stead of for $F\left(\bar{\omega}\right)=E_{D}$ ), I made the following modifications to the non-regularised code: I heeded the assumption that for the Levenberg-Marquard algorithm to work, the general shape of $F\left(\bar{\omega}\right)$ must be a sum of squares: $\sum\,v^{2}_{i}$ . So for the $E_{D}$ part, the $v^{\texttt{new}}_{i}$ become $\sqrt{\beta}v^{\texttt{old}}_{i}$ . And for the $E_{W}$ part, the $v_{i}$ become $\sqrt{\alpha}\omega_{i}$ . Hence, the "old" elements of the Jacobian $\left(\frac{\partial\,v_{i}}{\partial\,\omega_{j}}\right)$ are multiplied by $\sqrt{\beta}$ . Also, the Jacobian becomes much bigger now to include the $E_{W}$ part. A row is added for each weight in the network, with terms $\frac{\partial\,v_{i}}{\partial\,\omega_{j}}$ , which are zero when $i=j$ and otherwise $\sqrt{\alpha}$ .
