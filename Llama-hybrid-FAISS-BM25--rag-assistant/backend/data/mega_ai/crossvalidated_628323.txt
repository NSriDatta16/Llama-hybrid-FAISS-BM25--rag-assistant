[site]: crossvalidated
[post_id]: 628323
[parent_id]: 628319
[tags]: 
Standing on PWCs You may find that there are alternative views on pairwise tests, particularly with cases like yours where there are a substantial number of mean comparisons being made. Some dictate that doing so comes at a severe loss of power and thus are not recommended (Nakagawa, 2004), some view that correction is necessary to preventing catastrophically poor decisions in hypothesis testing (Bennett et al., 2009), and others have a by-context view on the matter (Midway et al., 2020). Personally, the more I learn about p-value correction, the more I feel it becomes troublesome with several comparisons. First, it speaks nothing at all to the actual mean differences between groups. Second, it only gives you a decision framework to act upon the precision of the estimate rather than the estimate itself. Third, it isn't really addressing the major theoretical interest you are pursuing, which is likely lost in a maze of comparisons that on their own aren't meaningful but aggregated probably mean something more. Fourth, as noted above, the tests themselves become severely underpowered when many tests are conducted, accomplishing the opposite approach that they are designed for (correcting alpha errors). Alternatives If it was up to me, I think a better option would be to run a linear mixed effects model (Brown, 2021), using the grouping factor here (blood test type), coding the groups as a random intercept, and then seeing what the overall difference in means is based on conditional shifts from this intercept. Running a billion PWCs probably isn't going to get you much unless you group these tests into reduced, meaningful categories. Otherwise I fail to see what an inferential test between 19 different groupings would help inform people related to your question of interest. Another approach, as noted by others and is another from of data reduction, is simply running some kind of PCA on your features if they are highly correlated. But that is assuming they can be reduced in such a way. Edit If you are just interested in differences across all tests but are less concerned with a theoretical effect of interest, you could simply construct a mixed model where only tests are entered as random effects, and if there are no other effects of interest, they can be simply compared in this way. A commonly used example is shown below from the sleepstudy data in R, where here I only enter by-subject differences in reaction time into the model: #### Load Libraries #### library(lmerTest) #### Fit Model #### fit.lmer We can see from the fixed effects in the summary that the conditional mean reaction time is around 300 ms and this can vary considerably by subject (SD is around 35.75): Random effects: Groups Name Variance Std.Dev. Subject (Intercept) 1278 35.75 Residual 1959 44.26 Number of obs: 180, groups: Subject, 18 Fixed effects: Estimate Std. Error df t value Pr(>|t|) (Intercept) 298.51 9.05 17.00 32.98 We can inspect the differences across several groups easily, or in this case subjects, by running the random effects like below: #### Inspect Differences #### ranef(fit.lmer) We see for example that Subject 308 has a conditional mean reaction time that is about 38 ms larger than the average response (aka slow to respond), whereas Subject 309 is a quick responder and thus has an average reaction time that is about 72 ms less than average. Note that there are about 18 subjects here, so this example somewhat resembles the number of levels for your use case: $Subject (Intercept) 308 37.829172 309 -72.209816 310 -58.536727 330 4.087222 331 9.476087 332 7.625658 333 15.305131 334 -2.779868 335 -42.001706 337 66.953479 349 -19.660707 350 13.089079 351 -7.292650 352 33.743025 369 6.526637 370 -5.901763 371 -3.055622 372 16.803368 Doing this, we can easily aggregate the overall variance in effects across many clusters (in your case the blood test types) as well as specific cluster differences by random intercepts. Note: This is all assuming a single dependent variable case. As Dipetkov rightfully states, your distinction about a 19 x 4 outcome is unclear and may require some additional detail for a better answer. I assumed that you meant some type of medical outcome for multiple diseases (aka acquiring a diabetes-related disability). Citations Bennett, C. M., Baird, A. A., Miller, M. B., & Wolford, G. L. (2009). Neural correlates of interspecies perspective taking in the post-mortem Atlantic salmon: An argument for proper multiple comparisons correction. Journal of Serendipitous and Unexpected Results, 1(1), 1–5. Brown, V. A. (2021). An introduction to linear mixed-effects modeling in R. Advances in Methods and Practices in Psychological Science, 4(1), 1–19. https://doi.org/10.1177/2515245920960351 Midway, S., Robertson, M., Flinn, S., & Kaller, M. (2020). Comparing multiple comparisons: Practical guidance for choosing the best multiple comparisons test. PeerJ, 8, e10387. https://doi.org/10.7717/peerj.10387 Nakagawa, S. (2004). A farewell to Bonferroni: The problems of low statistical power and publication bias. Behavioral Ecology, 15(6), 1044–1045. https://doi.org/10.1093/beheco/arh107
