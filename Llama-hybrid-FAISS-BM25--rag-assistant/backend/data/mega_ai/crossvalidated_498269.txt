[site]: crossvalidated
[post_id]: 498269
[parent_id]: 497683
[tags]: 
In HMM, you have a sequence of hidden states that have some transition probabilities and every hidden state produces an output. We can imagine the states to be some word classes (e.g., POS tags) and the outputs word forms. The transition probabilities then capture some simplified syntax and the emission probabilities are probabilities that a word belongs to this class. The typical use at inference time would be, for a sequence of words find out what is the most probable sequence of hidden states. SMT has two main components: a translation model (table with phrase translation and their probabilities) and a language model (that can say how fluent a sentence is or say what is the probability of a next word). Now, if we imagine the target sentence forms the hidden states of an HMM, then the transition probabilities would be a bigram language model, the emission probabilities would capture the probability that a word is translated to a different word (therefore a very simplified phrase table). The translation would then correspond to finding the most probable sequence of hidden states. Theoretically, you could increase the order of the language model by the hidden states corresponding not to single words but to word n-grams and allowing emitting n-grams of different lengths. However, this cannot work well because it assumes monotonic alignment between the source and the target sentence, whereas SMT works with a re-ordering model. Moreover, without rethinking how HMMs are implemented, this would be computationally intractable.
