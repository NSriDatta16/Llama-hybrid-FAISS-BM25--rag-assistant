[site]: crossvalidated
[post_id]: 569471
[parent_id]: 447741
[tags]: 
Given the previous technical answers, one philosophical point may help to clear the ambiguity between ME vs. Deep Learning: the concept of learning . In deep learning, there are multiple layers, each layer is a learning step. In the first step, the data input is 'converted' (or learned) into a synthetic intermediate output (a bit higher abstraction, loosely speaking). Then in each step, the input is progressively learned ( or 'transformed') into higher abstraction features, which may or may not be comprehensible to humans. Loosely speaking, the combination of these layers will approximate the 'formulas' for you, we don't need to specify any model or hypothesis beforehand. This is the same 'learning' concept as in cognitive science: construct higher abstraction from inputs . In ME methodology, there is simply no increase in abstraction. One example is word embedding in NLP, where words are transformed progressively into vectors of numbers with increasing abstraction, such that in the end, the vectors of numbers can actually represent syntactic (grammar) and semantic (logic) meaning. This capacity to deal with ambiguity (in languages, or other use cases) by building abstracted features is one stark distinction of deep learning vs. many other statistical methods.
