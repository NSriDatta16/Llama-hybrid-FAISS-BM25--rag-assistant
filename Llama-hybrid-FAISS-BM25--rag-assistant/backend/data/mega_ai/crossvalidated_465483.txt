[site]: crossvalidated
[post_id]: 465483
[parent_id]: 
[tags]: 
is off-policy Monte Carlo control really off-policy?

I'm reading the "Reinforcement learning: An introduction" by Sutton and Burto ( http://incompleteideas.net/book/bookdraft2017nov5.pdf ) The off-policy MC control algorithm puzzles me, please if anyone could help me to understand it better, I'd appreciate it. tldr, my question: is the off-policy MC control (page 91) really off-policy? (my current understanding, it is not). Remaining post below - elaboration on that question. Policy control commonly has two parts: 1) value estimation and 2) policy update. "off" in the "off-policy" means that we estimate values of one policy $\pi$ by Monte Carlo sampling another policy $b$ . The book first introduces off-policy value estimation algorithm (p. 90). It totally makes to me (you can skip that screenshot below and just keep reading. The important thing that any arbitrary $\pi$ can be estimated by any arbitrary policy $b$ ) then combined with the second step (policy update), the book introduces "policy control algorithm" (page 91). This time however, there is a huge difference: $\pi$ is by design a deterministic policy. The line $b \leftarrow \text{any soft policy}$ in most cases will lead to the instantaneous exit from the loop. The algorithm will work effectively only when the loop is running, meaning that $A_t$ must equal to $\pi(S_t)$ . It puts a lot of limitations on the $b$ . It is not really any soft policy, but the policy, that produces the same actions (at least starting from some time $T$ ) as the policy $\pi$ with high probability. To me that violates the idea behind "off-policy" (that by definition allows to explore a variety of the policies). From my current understanding, that algorithm could be turned to the true "off-policy" control if $\pi$ maintained to be non-deterministic. The concrete implementation in the book, however, puzzles me. It doesn't seem to be "off-policy" at all. Algorithm puzzles me in another way: it is simply super inefficient because the loop will not be running if we allow $b$ be any soft policy. Am I missing anything?
