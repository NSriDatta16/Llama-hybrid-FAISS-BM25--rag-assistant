[site]: crossvalidated
[post_id]: 495877
[parent_id]: 495876
[tags]: 
There is not a formal proof because the assertion is false: autoencoders do not perform non-linear PCA. PCA is defined as a (reversible) linear transformation into a space where variables are now orthogonal that captures maximal variance. Autoencoders do not do that in general. Linear autoencoders with $k$ -dimensional bottlenecks will often converge to the space spanned by the $k$ first principal components. Notice, however, that the orthogonal part is forsaken. Non-linear PCA is mostly Kernel PCA. This entails a specific space: one defined by the kernel of choice. Non-linear autoencoders, most often (I'm sure there are exceptions) do not employ kernels, instead modelling a local space directly. So, in essence, they are not performing non-linear PCA, just non-linear dimension reduction.
