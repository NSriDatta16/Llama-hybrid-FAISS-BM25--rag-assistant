[site]: crossvalidated
[post_id]: 291843
[parent_id]: 
[tags]: 
How to understand / calculate FLOPs of the neural network model?

In the paper on ResNet , authors say, that their 152-layer network has lesser complexity than VGG network with 16 or 19 layers: We construct 101- layer and 152-layer ResNets by using more 3-layer blocks (Table 1). Remarkably, although the depth is significantly increased, the 152-layer ResNet (11.3 billion FLOPs) still has lower complexity than VGG-16/19 nets (15.3/19.6 billion FLOPs) page 7 top. How can it be?
