[site]: crossvalidated
[post_id]: 507349
[parent_id]: 
[tags]: 
Bayesian Probability of Zero?

I've been reading a few different philosophical papers/books which have mentioned a "Bayesian belief". Within these texts I've been basically inferring that within the Bayesian theorem, there's something that says there's always a non-zero chance of something happening. Is there ever a time that the Bayesian belief would assign a probability of exactly 0% that something will not happen? After reading the wiki, I'm getting the sense that my inference was wrong, so now I'm confused on what the authors were attempting to convey. As an example, in a book by Nick Bostrom called Superintelligence, he writes: Unless the AI's motivation system is of a special kind, or there are additional element in its final goal that penalize strategies that have excessively wide-ranging impacts on the world, there is no reason for the AI to cease activity upon achieving its goal. On the contrary: if the AI is a sensible Bayesian agent, it would never assign exactly zero probability to the hypothesis that it has no yet achieved its goal - this, after all, being an empirical hypothesis against which the Ai can have only uncertain perceptual evidence. If within the Bayesian theorem, there is something that says there's a zero chance of something happening, in what circumstance would that be (in layman's terms ) and also in what circumstances is there never a zero probability?
