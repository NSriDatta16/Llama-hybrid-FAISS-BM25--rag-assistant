[site]: datascience
[post_id]: 107596
[parent_id]: 90181
[tags]: 
Agree with the other answer here - but in general BOW is for word encoding and TFIDF to remove common words like "are", "is", "the", etc. which do not lead to intelligence discovery in text. So comparing BOW and TFIDF is not appropriate as they have different uses. In general, I would recommend use pre-trained models such as BERT which is already trained on massive text corpus like Wikipedia and can provide commercial grade accuracy even with limited additional data used for incremental machine learning on top of base BERT model. Handcoding feature extraction on large text corpus with NLTK APIs like BOW or TFIDF or Lemmatizer / Stemmers or CountVectorizer is not going to be able to match commercial grade pre-trained model sophistication such as BERT or Open AI GPT.
