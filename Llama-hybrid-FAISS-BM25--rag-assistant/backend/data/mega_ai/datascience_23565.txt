[site]: datascience
[post_id]: 23565
[parent_id]: 
[tags]: 
Why is it good news that the expected gradient of the loss for a mini batch is equal to the gradient for the whole set?

I recently proved that $\mathbb{E}_I[\nabla L_I(x,y,w)] = \nabla(L_I(x,y,w))$ Where $I$ represents a randomized batch of size $m$. However, I cannot really understand why this is good news. I know that gradient is important for optimizing a model, especially with gradient descent. I also understand that in mini-batch algorithms we average over a whole batch instead of doing it for all the data, saving a lot of computations and reducing the variance.
