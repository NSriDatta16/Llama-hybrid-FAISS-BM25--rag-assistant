[site]: crossvalidated
[post_id]: 624774
[parent_id]: 
[tags]: 
Low CV-RMSE and negative $R^2$ (comparative)

I am trying to predict a numeric variable using XGBoost with optuna for hyperparameter optimization. I defined two objective functions for optuna, one optimized for very small datasets (5 to 17 observations per feature), one optimized for large datasets (from 17 to 6000 observations per feature). This is how I preprocessed my data (for context, steps={step1:5, step2:20, step3:65, step4:2400} : def preprocess_data(manufacturer, model,step1, step2, step3, step4): sql_engine = sql.create_engine(db_connection) event.listen(sql_engine, "before_cursor_execute", add_own_encoders) query = ("select transmission_type, engine_hp, fuel_type, color, manufacturer_year, km_no, collection_date, price from market_data " "where vehicle_manufacturer = '"+manufacturer+"' and vehicle_model = '"+model+"' and collection_date >= date_add(Now(), " "interval - 72 month) and manufacturer_year is not null and km_no is not null and price is not null" ) df = pd.read_sql_query(query, sql_engine) #print(df) #return empty dataframe if reading from sql unsuccessful if len(df) == 0: return pd.DataFrame() #Age more relevant for price prediction than manufacture year df['collection_date'] = pd.to_datetime(df['collection_date'], errors='coerce') df['manufacturer_year'] = pd.to_datetime(df['manufacturer_year'], format='%Y', errors='coerce') df['age'] = ((df['collection_date'] - df['manufacturer_year']).dt.days / 30.44 - 6).astype(int) #-6 because df.manufacturer_year defaults to january, june is more likely #avoid negative age df['age'] = df['age'].apply(lambda x: x if x >= 0 else 0) #add collection year to account for inflation df['collection_year']= df['collection_date'].dt.year.astype(int) #Process dataframe for large samples if len(df.index) > step4: #drop all observations from 2018 (older?) due to lower quality of data df = df[df['collection_date'] >= pd.Timestamp('2019-01-01')] #Set color to lowercase, remove spaces, map colors df.loc[:, "color"] = df.loc[:, "color"].str.lower().str.strip().apply( lambda x: 'missing' if not x or x == '' or x==' ' else "auriu" if re.search(r'auriu|galben', x) else ( "alte_culori" if re.search(r'alt', x) else ( "bej" if re.search(r'bej', x) else ( "argintiu" if re.search(r'argint', x) else x ) ) ) ) #General processessing #Horsepower missing values replaced with median median_hp = df['engine_hp'].median() df['engine_hp'] = df['engine_hp'].fillna(median_hp) #General: keep only columns useful for regression in df df = df[['engine_hp', 'age', 'km_no', 'collection_year','transmission_type', 'fuel_type','color','price']] #drop all all prices outside [1500, 150000] df['price'] = df['price'].apply(lambda x: x if 1500 max, 'price'] = np.nan # Age: Calculate the 75th percentile and IQR for age q75_age = np.nanpercentile(df['age'], 75) iqr_age = q75_age - np.nanpercentile(df['age'], 25) # calculate the upper bound for outliers max_age = q75_age + (2 * iqr_age) # set outliers above this threshold to NaN df.loc[df['age'] > max_age, 'age'] = np.nan #Transmission, Fuel: remove columns if more than 60% of values are missing df.replace(['',None,' '],np.nan, inplace = True) for col in ['transmission_type','fuel_type']: print(f"Percentage of nulls in {col} is {df[col].isnull().mean()}") if df[col].isnull().mean() > 0.6: df.drop(columns = col, inplace = True) #map transmission types if 'transmission_type' in df.columns: df['transmission_type'] = df['transmission_type'].apply(lambda x: 'M' if 'manual' in str(x).lower().strip() else ('A' if 'automat' in str(x).lower().strip() else (x if str(x).upper().strip() in ['M', 'A'] else 'unknown'))) #future if len(df.index) As you can see, I am optimizing for low rmse. The problem I encountered is that for small datasets (between step2 and step3), using objective_small for hyperparameter optimization consistently yields great rmse and cv-rmse (normalized with mean; usually cv-rmse is lower than 5%, which is very good for the purposes of this ML model), but negative or very low r2. However, using objective_large tends to yields double rmse values, but much higher r2 (consistently positive and above 0.3). The latter scenario is more in line with the results outputted by other scripts used on the same dataset (with random search or grid search for hyperparameter optimization). This is the train function corresponding to the scenario described above in my current script: elif step2 The r2 and RMSE are computed inside the objective function passed to Optuna. They are calculated inside each cross-validation fold and appended to a list, like this: params = { #not important for the purposes of this question? } oof_predictions = np.zeros(len(Y)) oof_test = np.zeros(len(Y)) # Use 20-fold cross-validation. 20 not set in stone kf = KFold(n_splits=20, shuffle=True, random_state=42) rmse_scores = [] r2_scores = [] for train_index, val_index in kf.split(X): X_train, X_val = X.iloc[train_index], X.iloc[val_index] Y_train, Y_val = Y.iloc[train_index], Y.iloc[val_index] xgb_regressor = XGBRegressor(**params, early_stopping_rounds=30) xgb_regressor.fit(X_train, Y_train, eval_set=[(X_val, Y_val)], verbose=False) Y_pred = xgb_regressor.predict(X_val) #rmse rmse = np.sqrt(mean_squared_error(Y_val, Y_pred)) rmse_scores.append(rmse) #r2 r2 = r2_score(Y_val, Y_pred) r2_scores.append(r2) #test and predicted values oof_predictions[val_index] = Y_pred oof_test[val_index] = Y_val if save_oof: # Store the predictions and actual values as user attributes trial.set_user_attr("oof_predictions", oof_predictions) trial.set_user_attr("oof_test", oof_test) trial.set_user_attr("r2_scores", r2_scores) trial.set_user_attr("rmse_scores", rmse_scores) The final values for the two metrics represent the mean of all the computed r2 and rmse across the cross-validation folds.My first guess was that there are a few outliers affecting r2 much more than rmse; I plotted the data and used winsorization for small datasets (n between step2 and step3). This did not help. I tested this on 10 different datasets, and the differences in r2 and cv-rmse were reproducible across datasets. My questions are: What is causing this? is my intuition correct that this is due to some outliers not eliminated by my preprocessing? Is the low r2 a problem? From an end-user perspective, I believe cv-rmse is much more relevant, because it expresses the average mistake of the model, with a quadratic penalty for large mistakes. I'm also attaching a few graphs I generated on a representative dataset to diagnoze the issue:
