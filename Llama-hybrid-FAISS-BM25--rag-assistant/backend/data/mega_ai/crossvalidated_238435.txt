[site]: crossvalidated
[post_id]: 238435
[parent_id]: 
[tags]: 
Research on neural nets that learn paths/connections? Is this just the same as setting weight to zero?

Is anyone aware of research on neural networks tuning the existence of paths through their layers as part of their learning algorithm? Traditionally a neural net trains its weights and biases in order to improve performance on the cost function. I'm curious if there exist nets that train the existence of paths or connections - for example changing a layer into a fully connected layer, or subtracting paths to result in a non-fully connected layer. In some sense this might be equivalent to setting weights along edges to 0 (subtracting paths). Since weights can be negative as well, a non-zero value supplies additional information beyond the existence of a path - it provides a magnitude (its absolute value) and a direction (its sign) as to the importance of the evidence it is weighing for the neuron it is feeding into. So I'm curious if researchers have looked at path existence itself as a training parameter. I tried searching google and cross-validated for various permutations of this question but didn't come up with anything. I seem to remember a paper about selectively firing paths and learning connections but can't find it anymore. Pointers appreciated! EDIT: Equivalently, this question might be posed as allowing the addition/subtraction of neurons in various layers (along with their connections). In this light it is not as simple as a reduction to a question of zero/non-zero values for weights.
