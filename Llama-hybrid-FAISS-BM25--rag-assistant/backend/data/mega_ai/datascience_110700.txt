[site]: datascience
[post_id]: 110700
[parent_id]: 110560
[tags]: 
Once you standardize the data, the data falls in the range -3.14 to +3.14 in 99.99% of the cases( 6 Sigma ). The main reason for standardizing the data is achieved as the data falls in a short range and the algorithm can converge faster . Applying normalization on top of it would further scale down the data to -1 to +1 . So, in regular practise one of them is applied based on the scenario. As in the real data, data will be having outliers and the distributon might not be Gaussian, there are other scaling techniques( Robust Scaler can efficiently deal even we have outliers in the data. Quantile transformer Scaler also handles when there are outliers which under the hood uses a CDF to transform the data using that function.)
