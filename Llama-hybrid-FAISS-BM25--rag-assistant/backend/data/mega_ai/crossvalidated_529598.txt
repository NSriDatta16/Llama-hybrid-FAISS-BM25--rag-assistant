[site]: crossvalidated
[post_id]: 529598
[parent_id]: 529597
[tags]: 
Minimizing square loss can be fine when the model is nonlinear, such as doing deep learning with mse as the loss function. Ordinary least squares, however, specifically refers to a linear model with the parameter vector estimated via $\hat{\beta} = (X^TX)^{-1}X^Ty$ . Thus, this is a bit of a tautology: we don't do ordinary least squares for nonlinear models because ordinary least squares is defined as applying to linear models. It's totally fine to try out square loss in a logistic-style regression, however. In that situation, you would describe it as optimizing the so-called Brier score . This is not the standard way to do a logistic-style regression (which uses maximum likelihood, equivalent to minimizing crossentropy loss, not Brier score), but it might work well in some situations. I have even seen Yann LeCun post MNIST examples on his website, using neural netoworks, that optimize square loss ( mse ). Be aware of how broad a linear regression can be, however. It has to do with linearity in the parameters, not just lines and higher-dimension analogues (e.g., planes). For instance, $y=\beta_0 + \beta_1x + \beta_2 x^2$ is a linear model, despite the parabolic curvature from the quadratic term.
