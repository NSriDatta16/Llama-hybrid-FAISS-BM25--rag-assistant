[site]: stackoverflow
[post_id]: 3338428
[parent_id]: 3326145
[tags]: 
Recently (R2010a I believe), MATLAB added new functions for k-Nearest Neighbor (kNN) searching using KD-tree (a spatial indexing method similar to R-tree) to the Statistics Toolbox. Example: load fisheriris % Iris dataset Q = [6 3 4 1 ; 5 4 3 2]; % query points % build kd-tree knnObj = createns(meas, 'NSMethod','kdtree', 'Distance','euclidean'); % find k=5 Nearest Neighbors to Q [idx Dist] = knnsearch(knnObj, Q, 'K',5); Refer to this page for nice description. Also if you have the Image Processing Toolbox, it contains (for a long time now) an implementation of the kd-tree and kNN searching. They are private functions though: [matlabroot '\images\images\private\kdtree.m'] [matlabroot '\images\images\private\nnsearch.m'] To compare your two approaches ( Dynamic Time Warping and Euclidean distance ), you can design a classic problem of classification; given a set of labeled training/testing time series, the task is to predict the label of each test sequenceby finding the most similar ones using kNN then predict the majority class. To evaluate performance, use any of the standard measures of classification like accuracy/error, etc..
