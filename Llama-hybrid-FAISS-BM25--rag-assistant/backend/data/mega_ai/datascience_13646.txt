[site]: datascience
[post_id]: 13646
[parent_id]: 
[tags]: 
Policy network AlphaGo and transferring to other domains

This is not covered very well in their AlphaGo paper but I assume that their policy network has a softmax output layer with a node for all the positions on the board, including illegal ones (the one with a stone on the board already). Maybe the illegal ones are basically zero already, I'm uncertain, but I assume they manually put them to zero and then normalize the probabilities. Please correct me if I'm wrong. To dip my toes into using neural networks for game AI I decided to work on a Backgammon player. I have only implemented a 1D convolutional value network on the board and merged the convolutional layers with the bar and the bearing states. I let random players play a bunch of games, train the model on those game states and let the model play against itself a number of times, tune the model further and iterate this step a number of times. The results are promising so I was looking to add a tree search algorithm similar to AlphaGo. This is where my question comes from. I would like to use a policy network to guide the exploration through the tree, but I don't know how to represent it. If you don't take the current state in mind, you would have a huge amount of potential states in your last layer of which more than 99.99% would be illegal. If you do keep the current state in mind I don't see how I could train a network on this. Is there any way of building a policy network for Backgammon? An alternative method would be to use the value network on the potential states that you are able to go to (the network outputs a probability of winning for the active player) and transform those probabilities into a new distribution for exploration, taking in mind exploration versus exploitation. EDIT: My current representation of actions is the state it would transfer into by doing that action, which means the number of actions over all possible games is enormous.
