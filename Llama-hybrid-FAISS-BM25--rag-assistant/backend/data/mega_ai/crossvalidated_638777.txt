[site]: crossvalidated
[post_id]: 638777
[parent_id]: 638774
[tags]: 
That would be desirable, but it is not guaranteed to make as much sense as we might like. First, you could make an argument that any predicted $p(\mathcal C_k|\mathbf x_i)\in[0,1]$ is a probability in the sense that it can define a binomial or multinomial distribution parameter that governs how often a particular event occurs; the Kolmogorov axioms are satisfied . However, in order to assert $p(\mathcal C_k|\mathbf x_i)$ as a probability, it would be nice for it to be the case that, when you assert event $\mathcal C_k$ to happen with probability $p$ , then $\mathcal C_k$ really does happen $p\times100\%$ of the time. This gets into the assessment of model calibration : if the predicted probabilities align with the reality of how often a particular category occurs. For instance, if you keep predicting event $\mathcal C_k$ to happen with a probability above $0.9$ and only observe $\mathcal C_k$ happening in $60\%$ of those instances, your predictions lack calibration. Many machine learning models lack calibration, and this leads to the caution Madden mentions in the comments. Calibrating their predictions is an open problem. See, for instance Guo et al (ICML 2017) and my question about it. REFERENCES Guo, Chuan, et al. "On calibration of modern neural networks." International Conference on Machine Learning. PMLR, 2017.
