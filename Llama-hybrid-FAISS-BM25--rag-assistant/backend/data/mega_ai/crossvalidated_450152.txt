[site]: crossvalidated
[post_id]: 450152
[parent_id]: 450001
[tags]: 
Because Naive Bayes assumes your features are not correlated, you don't need to provide an explicit a priori causal model - or rather, you already have, just implicitly. A Naive Bayes algorithm for predicting cancer would assume $p(Cancer|Smoking,Tar) = p(Cancer|Smoking)p(Smoking) + p(Cancer|Tar)p(Tar)$ A Hierarchical Bayesian model could specify a model - Tar->Smoking->Cancer , for example, and use it to construct an equation $p(Cancer|Smoking,Tar) = p(Cancer|Smoking)p(Smoking|Tar)p(Tar)$ . Note that this model is A) simple - you only have 3 nodes, and the number of possible vertices grows rapidly, and B) deliberately wrong - it gets the cause and effect between smoking and lung damage backwards. Depending on the interactions in the real world, getting the dependencies wrong may be a far worse problem than just assuming none exist, and even if you do get it right, a hierarchical model is both less straightforward and more computationally intensive to fit. If you have a nice, fairly clean set of independent features Naive Bayes gets around all that, but on the flipside a good Hierarchical Bayes model is really powerful.
