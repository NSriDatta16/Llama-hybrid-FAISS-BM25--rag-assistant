[site]: crossvalidated
[post_id]: 534188
[parent_id]: 534161
[tags]: 
Theoretically rigorous understandings of deep neural networks are still a wide-open area of research, though a multitude of statistical heuristics are in common practice. In this light, I'll try to go through your questions and offer some empirical suggestions. 1. What are the impacts of learning rate on this model? At a high level, iterative optimization techniques, i.e. gradient descent or GD ( Adam , the one that you use, can be thought of as a version of GD with some enhancements), calculate a "descent direction," and then update the model parameters in that descent direction by an amount given by the step size. In the deep learning setting, the best learning rates are often found using hyperparameter search -- i.e. trying many different values and selecting the model with the best validation performance. This is what you are doing. In general, I've seen that these heuristics (which are actually provable in simpler regimes than deep learning) hold fairly well even in the deep learning regime: a. If you set your learning rate too low, your model will converge very slowly. b. If you set your learning rate too high, your model's convergence will be unstable; training loss may bounce around, or even get stuck at a suboptimal level (local minima). I see this in your graphs: at learning rate $10^{-4}$ , I actually think the model is performing quite well from an optimization standpoint; however, there is a small gap in the training and validation performance. Whether or not this is acceptable requires task-specific knowledge. When you bring down the learning rate by a factor of 10, the model learns slower: it's as if you've zoomed in on the left half of the previous graph -- which is very nice expected behavior. In the regime with learning rate $10^{-3}$ , the model pretty much fails to learn, which is potentially attributable to having overly large parameter updates that entirely "skip over" the region of "good" parameter values. Even though you mention your PI wants you to use a constant learning rate, it is now common practice to use learning rate schedules , where the learning rate changes ( usually decreases, excepting the usage of "warmup" -- beginner-friendly guide here ) during training. 2. Why is it overfitting? TL;DR I think you are overtraining a little. You have a 10-layer CNN; this is very likely powerful enough to memorize the entire training dataset of 384x384x3 images. In fact, we see this in some of your graphs, where training accuracy reaches 1. I do indeed observe a small gap between training and validation performance; whether this is of concern depends on the task (i.e. is the validation performance satisfactory to an expert). Decreasing the learning rate doesn't necessarily help overfitting; I could see an argument that in certain regimes, it would slow down the onset of overfitting, but the statistical problem remains. Some techniques that empirically help overfitting: Dropout. You do this already in your final layers. Regularization. The most common is weight decay regularization; it's built into most ML frameworks. Changing the model architecture. In general, models with more parameters tend to overfit more readily, so removing layers or decreasing the layer sizes can help. Early stopping. You do this in the final graph. This is very common practice; you'd set a rule along the lines of "if the validation loss fails to improve for N epochs, end training." A tiny point of correction: you claim that we can see overfitting due to validation loss stagnating. Actually, overfitting is characterized by a large gap between training accuracy (usually 1 or very close to 1 in the overfitted remige) and validation accuracy. Here's a graph from Google Images that shows this: Validation loss stagnation is, however, a common symptom of overfitting.
