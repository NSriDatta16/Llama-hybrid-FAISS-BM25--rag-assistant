[site]: crossvalidated
[post_id]: 514488
[parent_id]: 
[tags]: 
Hyperparameter optimisation for approximate Bayesian computation

I have a simulation model with an intractable likelihood function and would like to use approximate Bayesian computation (ABC) to obtain the posterior density for the simulator's parameters. In running ABC, there are a number of hyperparameters to set, for example the distance threshold $\epsilon$ in (soft-)rejection ABC, and the parameters of the proposal distribution used to generate samples in the MCMC scheme. My question is: is there a systematic way to tune these hyperparameters to give the most accurate posterior distribution possible? Of course, the problem is that I don't know what the true posterior density looks like to be able to verify that the approximation to the posterior is accurate. I know about simulation-based calibration , so that one possible objective function to optimise is possibly a test statistic for the uniformity of rank histograms, but my concern is that this might require an unfeasibly large number of calls to my simulator. My question could also be extended to: Are there any other ways that one can assess the quality of the ABC-approximated posterior, and in that way optimise hyperparameters? Are there any tools other than simulation-based calibration for validating approximate Bayesian inference pipelines?
