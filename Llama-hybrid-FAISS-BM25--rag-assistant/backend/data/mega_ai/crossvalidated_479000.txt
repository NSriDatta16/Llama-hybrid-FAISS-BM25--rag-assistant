[site]: crossvalidated
[post_id]: 479000
[parent_id]: 346680
[tags]: 
I've been wondering about this as well. For some reason, applying batchnorm degrades the performance (accuracy) of NLP benchmarks most of the time. There is a recent paper trying to attribute this to the variance of weights that we are training. We find that there are clear differences in the batch statistics of NLP data versus CV data. In particular, we observe that batch statistics for NLP data have a very large variance throughout training. This variance exists in the corresponding gradients as well. In contrast, CV data exhibits orders of magnitude smaller variance. See Figure 2 and 3 for a comparison of BN in CV and NLP. https://arxiv.org/pdf/2003.07845v1.pdf
