[site]: stackoverflow
[post_id]: 3022116
[parent_id]: 3021921
[tags]: 
I think you have already a clear Idea on how to organize your layers. First of all you would need a Web Framework for your front-end. You have many choices here, Cakephp afaik is a good choice and it is designed to force you to follow the design pattern MVC . Then, you would need to design your database to store what users want to be spidered. Your db will be accessed by your web application to store users requests, by your php script to know what to scrape and finally by your python batch to confirm to the users that the data requested is available. A possible over-simplified scenario: User register to your site User commands to grab a random page from Wikipedia Request is stored though CakePhp application to db Cron php batch starts and checks db for new requests Batch founds new request and scrapes from Wikipedia Batch updates db with a scraped flag Cron python batch starts and checks db for new scraped flag Batch founds new scraped flag and parse Wikipedia to extract some tags Batch updates db with a done flag User founds the requested information on his profile.
