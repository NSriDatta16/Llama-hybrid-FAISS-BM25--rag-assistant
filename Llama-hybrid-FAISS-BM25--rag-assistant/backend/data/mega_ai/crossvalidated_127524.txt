[site]: crossvalidated
[post_id]: 127524
[parent_id]: 
[tags]: 
Adjusting for multiple Kruskal-Wallis tests

I've run 3 different machine learning algorithms on 10 different datasets, generating an accuracy on each one. My hypothesis is that two of the algorithms are consistently better than the third. I've noticed that the accuracies aren't normally distributed and so I'm looking to use non-parametric tests. My initial idea on how to assess any difference is to run Kruskal-Wallis on each dataset, to see if there is a significant difference in the accuracies from each algorithm. As I'd be running K-W 10 times, would I need to account for this with a multiple comparison correction method? If on any of the datasets I get a significant result, I'd run a post-hoc analysis. From what I've seen there aren't many simple (in R) non-parametric post-hoc techniques, and so I would run 3 pairwise Mann-Whitney U-tests between each algorithm's scores. My questions are: Is running 10 K-W tests the correct approach for the first part? If so would I need to correct for multiple tests? Is my post-hoc analysis a good approach?
