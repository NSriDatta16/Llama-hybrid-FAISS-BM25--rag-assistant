[site]: crossvalidated
[post_id]: 614469
[parent_id]: 
[tags]: 
How to prevent Softmax to be overconfidence/getting stuck in one solution

I have a Neural Network (Feed Forward Neural Network) with Softmax as the output. The neural network is maximizing some loss (call it $L$ ), with respect to the input called VAMP2 (there is not True labels!). The optimization process goes by minimizing the loss as much as possible. The loss is bound by $N$ , where $N$ is number of Softmax outputs. So $max\{L\} = N$ The problem is that often Softmax, does maximize the loss only for one or two of its outputs (depending on the random seed). Let's say that the number of outputs $N$ is $N=4$ . Then Softmax optimizes only 2 of the outputs, the other 2 always stay 0 or 1. How to prevent such a scenario ? Are there some tricks with Softmax to prevent it ? It does look like Softmax can't get out of some local minima or get stuck there. Could I use some specific regularization for Softmax ? As Entropy ? Loss definition if one would look at it $$L = -||C_{00}^{-1/2}C_{01}C_{11}^{-1/2}||^2_F$$ where $$C_{00}=Cov(\chi_1, \chi_1); C_{01}=Cov(\chi_1, \chi_2); C_{00}=Cov(\chi_2, \chi_2);$$ and $\chi_1$ and $\chi_2$ are outputs of the neural network shifted by time $\tau$ . The input and output of the neural network is an ordered time-series. So one can shift it.
