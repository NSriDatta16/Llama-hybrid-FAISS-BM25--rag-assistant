[site]: crossvalidated
[post_id]: 309425
[parent_id]: 
[tags]: 
Optimising dual SVM: why do some authors drop constraints?

In Hastie's Elements of Statistical Learning the dual problem is put as $$ \begin{align} \text{arg min}_\alpha \quad &\ \frac{1}{2}\alpha^\top Q\, \alpha_i- \sum_i \alpha_i\\ \text{subject to}\quad &\ \forall i:0\le\alpha_i\le C\\ &\ \sum_i y_i \alpha_i = 0 \end{align} $$ I implemented a dual coordinate descent method based on Hsieh et al.'s A Dual Coordinate Descent Method for Large-scale Linear SVM but they state that the dual problem is $$ \begin{align} \text{arg min}_\alpha \quad &\ \frac{1}{2}\alpha^\top Q\, \alpha_i- \sum_i \alpha_i\\ \text{subject to}\quad &\ \forall i:0\le\alpha_i\le C \end{align} $$ In other words, they dropped the second constraint. I ran the implementation and indeed the constraint is not met after the algorithm finishes. Note that LIBLINEAR uses the same approach with the constraint $\sum_i y_i \alpha_i = 0$ dropped. It doesn't seem legit to me to simply drop constraints, so why are some authors doing it?
