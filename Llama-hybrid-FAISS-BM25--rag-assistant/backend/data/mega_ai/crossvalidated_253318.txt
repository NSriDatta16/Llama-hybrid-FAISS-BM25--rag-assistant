[site]: crossvalidated
[post_id]: 253318
[parent_id]: 253127
[tags]: 
I need to correct a number of mistaken or partly misplaced ideas in the question first (as well as some that aren't in the question but are commonly seen and may be indirectly influencing the way you ask your question), but I will return to the main issue. The answer you link to says: The t-test assumes that the means of the different samples are normally distributed; it does not assume that the population is normally distributed. As one of the comments below the answer points out, this claim is not correct as stated. The assumption that's made in the derivation that the t-statistic has a t-distribution relies on the distribution from which the observations were drawn being normal, not just the means. Note that the t-statistic is not just a numerator, but a ratio of two things. (Such derivations are where the assumptions for tests arise from.) You need the denominator in the t-statistic to be (i) distributed as scaled chi and (ii) independent of the numerator. There's no result I'm aware of that makes the t-statistic have a t-distribution in any other general circumstance of interest. This the 'what was assumed in deriving the null distribution' part ("the assumptions "). In addition, while you can use the CLT plus Slutsky's theorem to argue that a t-statistic will asymptotically approach a standard normal (so that as sample sizes go to infinity you approach a level- $Î±$ test), there's nothing so far that makes it approach the $t$ itself (e.g. at least in some situations, it might always be better approximated by the normal than it is by the t, all along the way as $n$ increases), albeit the t converges to the same place as the statistic. However, under suitable conditions, and in sufficiently large samples, the t test will then have about the right significance level. Which is to say that the t-test is at least moderately level robust to the distributional assumption (the two-sample test is more robust in this sense than the one-sample or paired test). This is not at all the same thing as saying it doesn't make the assumption at all. There's also nothing so far that says that such a test will have good power (i.e. that would make us care to use it). In fact in large samples the relative power at small effect sizes may be quite poor compared to commonly considered alternatives (indeed the ARE of the t-test can go to zero). [One of the comments under that answer points this error out but it has not been addressed by the person who answered.] That said, however, often the t-test does work at least fairly well. It would not do to be overly reliant on it without understanding something of the circumstances with which one was dealing. parametric statistics are using sample data to draw inference about population parameters like the mean, or the difference in means between two populations. I recognize that you're not saying that parametric statistics is only about means there, but I want to take a moment to deal with a not-uncommon idea that they do. Some parametric methods do focus on the mean but parametric methods are not in general about means. (A lot of books that don't go into the theory properly manage to get this completely wrong. The term " parametric " is not about normality and it is not about means.) If the population distribution is not normal, it means that the population mean is a poor representation of the central tendency of the population. One can care about a population mean without remotely caring about whatever "central tendency" might be. Consider the exponential distribution . This distribution is commonly used to model a number of things. A typical example is inter-event times (indeed you can derive it as the inter-event time in a Poisson process ). Let's imagine we're doing just that (using it as a model for inter-event times). It looks like this: Here the labels on the x-axis represent multiples of the population mean (since it's in units of the mean, the population mean is at 1). I'm not even sure we can say that the exponential distribution has a central tendency (I don't discern anything that I'd want to label as clearly the vicinity of a 'center') -- but it makes perfect sense to use its population mean to describe the process, it's still the average inter-event time. It's not the typical inter-event time for a single event, but if I am repeatedly experiencing this process that mean is very much of interest. is the inference gained from the test informative when the population is not normal and is thus poorly described by the mean and variance? Outside some very particular situations, this is not the best way to think about it. It doesn't really matter whether the particular population parameter of interest is typical of the distribution (in whatever sense that 'typical' is intended). It's easily possible for a population mean to be of interest even when the mean is not a 'typical' value (e.g. when the mean and the mode are quite different). Whether a particular population parameter is of interest (might be suitable for some hypothesis) is not so much a question of what the distribution looks like, but of what the question is that you want to answer. For example, if I have to wait for 100 inter-event times in the above example, the population mean on the individual times is an important thing to be thinking about, even though most of the time I'll wait much less than the mean. We can also construct tests based on sample means without assuming normality (this is an unrelated issue to the above one, but you seem as if you might be partly conflating the two issues, so I'd better discuss it). In the above exponential model we have a different parametric assumption, and we can base a suitable test on that assumption (and it can sensibly be based on the sample mean - both answers there are relevant). Or if we don't have a parametric model, we can nevertheless construct a test that doesn't make a parametric assumption but still quite explicitly deals with (say) a difference in population means (such as a permutation test of the difference in sample means). However, in the other hand, just because I may want to test something in relation to a population mean, that does not necessarily imply that the sample mean is a good way to construct a test of it. If I were interested in the difference in population means for two samples from logistic distributions, I would be better to use something other than the sample mean as the basis of the test (indeed, a Wilcoxon-Mann-Whitney test -- used as a test of a shift in population mean under a logistic assumption -- would be an excellent choice in that case, though not quite as powerful as you could get). In summary : let us begin to summarize that by passing back to the title question. Is the inference from a parametric test valid when the population distribution is not normal? It certainly can be. The consideration of validity of a parametric test is not related to whether or not we're dealing with normality. the mean can be a perfectly reasonable population quantity to make hypotheses about, even when the mean is not at all close to the median or the mode. a parametric test needn't be about the mean. It might be about the population minimum, for example, or the population median, or the population interquartile range or any number of other things. Nor is a parametric test necessarily related to normality at all (like a test for the upper limit of a uniform distribution or the shift parameter in a shifted exponential) we must be careful not to conflate considerations of what population quantities we're interested in with what sample quantities we use for making inferences about them. Sometimes it's good to use the sample equivalent to estimate the population quantity and sometimes it's not -- and it's not always intuitive. With a (right skewed) exponential distribution, using the sample mean in inference about the population mean works well. With a symmetric, bell-shaped, not very heavy-tailed distribution like a logistic (and one you're likely to think looks pretty normal-ish in a histogram), not so much. And if it's Laplace even less so. When looking at what to test and how to test it, I think it boils down to answering two main questions -- What question about the population are you interested in answering? (This consideration needn't necessarily relate much to distribution shapes.) Given what you know/assume (or don't know/won't assume) about the distribution, what's a good way to answer that question? (it's here that questions of efficiency and robustness of tests come in, and where possible distribution shapes may be a central consideration)
