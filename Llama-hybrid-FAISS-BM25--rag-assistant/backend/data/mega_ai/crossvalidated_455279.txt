[site]: crossvalidated
[post_id]: 455279
[parent_id]: 
[tags]: 
Neural nets: How to get the gradient of the cost function from the gradient evaluated for each observation?

The gradient of the cost function, $E$ , changes for each input observation. I have taken $E$ to be the sum of least squares error, for example. To see this, note that the partial derivative with respect to the weight connecting neuron $j$ in layer $l-1$ and neuron $i$ in layer $l$ is given by $$ \frac{\partial E}{ \partial w_{ij}^{(l)} } = \frac{\partial E}{\partial z_i^{(l)}} x_j^{(l)}, $$ where $x_j^{(l)}$ is the input to neuron $j$ in layer $l$ . Therefore I will get a weight estimate for each input to the layer if I were to use gradient decent with this gradient. How do extend what I've found to get the best weights for the whole training set, and not only for each individual observation? Is it correct to use gradient descent for each training example separately, then take the average of each weight at the end? Or should I take the average of each gradient at each iteration in gradient decent? Or something else? EDIT: I am NOT making the statement that $E$ changes with each training observation. But it seems that the gradient should.
