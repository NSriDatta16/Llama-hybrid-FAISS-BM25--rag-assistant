[site]: crossvalidated
[post_id]: 417916
[parent_id]: 417894
[tags]: 
Your reviewer is (maybe) right, but he/she kind of cherrypicked his comparison point. First, why a maybe. Even if two data points are in the 95% interval of each other, their difference might be significantly different at a 5% margin. See: https://towardsdatascience.com/why-overlapping-confidence-intervals-mean-nothing-about-statistical-significance-48360559900a To get a correct inference, you have to make the difference between 2017 and all other years, then compute the associated standard errors. Graphically, 1945 will be clearly an outlier compared to most "old" years. Second, I would take averages over ten years, compute the difference between decades, and the associated standard errors. I think they will be lower. At the end, just make a separate test between the mean value over 2000-2017 and the mean value before. Seeing the graph, it will be significant I think.
