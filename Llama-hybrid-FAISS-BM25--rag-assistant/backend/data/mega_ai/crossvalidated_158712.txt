[site]: crossvalidated
[post_id]: 158712
[parent_id]: 
[tags]: 
How to train an SVM via backpropagation?

I was wondering if it was possible to train an SVM (say a linear one, to make things easy) using backpropagation? Currently, I'm at a road block, because I can only think about writing the classifier's output as $$ f(\mathbf{x};\theta,b) = \text{sgn}(\theta\cdot\mathbf{x} - (b+1)) = \text{sgn}(g(\mathbf{x};\theta,b)) $$ Hence, when we try and calculate the "backwards pass" (propagated error) we get $$ \begin{align} \frac{\partial E}{\partial \mathbf{x}} &= \frac{\partial E}{\partial f(\mathbf{x};\theta,b)} \frac{\partial f(\mathbf{x};\theta,b)}{\mathbf{x}} \\ &= \frac{\partial E}{\partial f(\mathbf{x};\theta,b)} \frac{\partial \text{sgn}(g(\mathbf{x};\theta,b))}{\partial g(\mathbf{x};\theta,b)} \frac{\partial g(\mathbf{x};\theta,b)}{\partial \mathbf{x}} \\ &= \delta \, \frac{d \text{sgn}(z)}{dz} \, \theta \\ &= \delta \cdot 0 \cdot \theta \\ &= \mathbf{0} \end{align} $$ since the derivative of $\text{sgn}(x)$ is $$ \frac{d\text{sgn}(x)}{dx} = \begin{cases} 0 &\text{if $x \neq 0$}\\ 2\delta(x) &\text{if $x=0$} \end{cases} $$ Similarly, we find that $\partial E/\partial \theta = \partial E /\partial b = 0$, which means we cannot pass back any information, or perform gradient updates! What gives?
