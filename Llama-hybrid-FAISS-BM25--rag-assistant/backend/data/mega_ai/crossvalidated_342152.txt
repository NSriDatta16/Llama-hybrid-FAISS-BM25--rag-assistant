[site]: crossvalidated
[post_id]: 342152
[parent_id]: 341236
[tags]: 
I'm going to offer a couple of strategies to consider, and they are purposefully "simplistic" in nature, as there is no "statistical" answer that will provide the pragmatic answers you probably are hoping for. Additionally, I, as an applied statistician, would recommend looking at tests 1 thru 3 separately, as they appear to be different analyses (even with some amount of shared data). This is conventional in many disciplines, and I don't think you would encounter much push back (as the pragmatic counter-argument is the alpha as error rate vs. family-wise error rate vs. life-time-wise error rate). Approach #1 (MCP/FDR): Holms-Bonferroni This is the approach that would probably "clear the reviewers". It will mean a lot more low p-valued tests being flagged as n.s. (as you suggested above). Approach #2: Pick an absolute cut-off Using some metric of effect size, say that you will consider any tests that cross this threshold as "reportable" and you will report an unadjusted $p$-value for interpretation purposes of those researchers who wish to use your findings in support of their future studies. If you go this route, the cut-off should be a fairly high hurdle to cross. Approach #3: Just pick a strict $\alpha$ and stick with it Here's the logic for the strict alpha approach: If I'm going to conduct 20,000 tests, I'm not expecting them all to be correct. So, ¿how many are ok to be wrong? The familywise error approach says zero (despite the fact that the very nature of the evidence we are utilizing has a built in uncertainty to it). So, let's say we are ok with 1 error out of 20000 (or in probability terms at most 1 error). Then we could use an $\alpha = 0.000\ 002\ 5$. This is the probability that produces $P(X \le 1) > 95\%$. With this logic, if we are willing to accept at most 2 errors in 20,000 (a 0.01% error rate), $\alpha = 0.000\ 040\ 8$. For at most 5 errors, $\alpha = 0.000\ 130\ 6$. (For at most 20 errors, $\alpha ≈ .0007$.) The rationale is simple: there will be some errors, but there are at most $k$ of them...and it is the job of future researchers to determine which ones, of our the 20,000 tests, are wrong. Approach #4: Just report the results The very idea that statistical/probabilistic assessment provides a dichotomization of evidence into truth-affirming or truth-denying fact is inherently flawed. No one would expect 20,000 different journal articles to contain no errors. But, the users of said 20,000 journal articles operate as such, regardless of the fact that a consistent $\alpha=0.05$ would mean ≈1,000 are actually wrong. This goes against some of the basic tenants of the scientific paradigm. We cannot prove, only disprove. Thus, if we have evidence that is suggestive, (say, god-forbid, a $p=0.051$), then it is worthy of further exploration. If you reported the results without any adjustment, and included the caveat that this information is provided to suggests possible avenues for future research, then the 20,000 tests would add more evidence to our growing knowledge base. True, it will be flawed evidence...but in the end, ¿what evidence isn't flawed in some manner? Not sure if this helps or not, but I'm sure it will spur on some amount of discussion. As always, happy to clarify anything when possible.
