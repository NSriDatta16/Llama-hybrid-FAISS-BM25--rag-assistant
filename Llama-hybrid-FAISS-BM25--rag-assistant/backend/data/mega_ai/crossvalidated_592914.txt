[site]: crossvalidated
[post_id]: 592914
[parent_id]: 589647
[tags]: 
Quick Q&A: "Method of Moments requires you to set each "moment condition" equal to 0. I don't understand why this is necessary." It isn't necessary. It's convenient to make math expressions look pretty. Setting it to a different number also doesn't do add anything. Imagine you have $\mathrm{E}[f(x)] = c$ where $c\neq 0$ . Then you could define $g(x) = f(x) - c$ and you're back to $\mathrm{E}[g(x)] = 0$ . "Our prof mentioned that Generalized Method of Moments is closely related to OLS" You can derive OLS as a special case of GMM. Recall from linear algebra that two vectors are said to be orthogonal if their inner product is zero. Recall that one way to derive OLS is to: Assume that in the population , the error term is orthogonal to the regressors. That is if you have $i = 1, \ldots, k$ regressors, assume $\operatorname{E}[x_i \epsilon] = 0$ . Find regression coefficients $b_i$ such that in your sample , the regressors are orthogonal to residuals. Find $b_i$ such that $\frac{1}{n} \sum_{j=1}^n x_{i,j} e_j = 0$ where $e_j$ is the residual for observation $j$ . This turns out to give you the normal equations which are solved with least squares. In GMM language, your moment conditions are $\operatorname{E}[x_i \epsilon] = 0$ for $i=1,\ldots,k$ and then you find regression coefficients such that statement is true under the empirical measure defined by your sample. Q "but the Expected Value of a Exponential Distribution is "summation n_i/x". Therefore, why is the "Method of Moments" said NOT to depend on a probability distribution, and to be considered more robust than MLE - when it clearly does depend on a probability distribution?" To estimate something by maximum likelihood, you have to write down the likelihood function . To write the likelihood function, you need to know the probability distribution. Also clearly in mathematics, you need to know the probability distribution of a random variable $X$ to calculate its expectation $\operatorname{E}[X] = \int_\Omega X dP$ . On the other hand. Let's say I have 5 independent draws of the random variable $X$ and I get 3, 1, 7, 2, and 12. Do I need to know the distribution of $X$ to calculate the sample mean? No! I just add up and divide to get: $$ \frac{3 + 1 + 7 + 2 + 7}{5} = 4$$ So yeah, in some full philosophical sense that mean of 4 in this sample depends on the probability distribution of $X$ , but in a narrower sense, all you have to do to get the sample average is add those 5 numbers together and divide by 5: it doesn't require the researcher to know the probability distribution at all! Where this goes is that you can reasonably use OLS and rely on its large sample, asymptotic properties in various settings where you as the researcher have deep uncertainties as to what the probability distribution of the error term is. Is the error term normally distributed? T-distributed? log-normal? None of the above? Another possibly pedantic note: MLE can be viewed as a special case of GMM in a variety of conditions If you have a log-concave likelihood function and use the moment condition that the expectation of the score is 0, GMM gives you maximum likelihood estimation. Maximizing the log likelihood function is equivalent to maximizing the likelihood function (since log is a monotonic transformation) If the log likelihood function is concave, the maximum can be found by taking the gradient and setting it to 0. Recall that the score is defined as the gradient of the log likelihood function. At the true parameter values, the expectation of the score is zero. Then the GMM procedure would have you find parameter values such that in your data, the sample average of the score is zero. This is equivalent to maximizing the likelihood function (under certain regularity conditions). Like a lot of areas of mathematics, a problem or procedure in one framework often has an interesting correspondence to a problem or procedure in another framework. Some practical musings: OLS is to statistics as is a hammer or screwdriver to construction: an immensely useful and WIDELY applicable! It is worth knowing OLS well. At a minimum, it's a great starting point for analysis. There are many, many extensions that in some sense build on OLS. MLE is fantastic when you have reasonable beliefs about the likelihood function. Often times in the social science or business setting, that's a big ask.... GMM may get a bit academic or theoretical... it's cool to see how different estimation procedures can be derived as special cases of GMM, but how often is a typical practitioner going to derive their own GMM estimator for a problem?
