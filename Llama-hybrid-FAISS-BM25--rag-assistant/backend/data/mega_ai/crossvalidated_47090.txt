[site]: crossvalidated
[post_id]: 47090
[parent_id]: 
[tags]: 
Increasing the sample size does not help the classification performance

I am training a SVM classifier based on a given document collections. I started from using 500 documents for training, then I add another 500 for training, and so on. In other words, I have three training sets, 500, 1000, 1500. And the smaller training set is a subset of the sequential larger set. I validate the model against the same test set. SampleSize Precision Recall Accuracy AUC 500 79.62% 67.49% 77.65% 0.854 1000 82.49% 77.94% 82.67% 0.890 1500 81.64% 78.08% 82.28% 0.888 The performance gets the best when we use 1000 training set. Looks like the extra 500 training samples for constructing the 1500 training set in fact does harm to the model. How can I explain this observation.
