[site]: crossvalidated
[post_id]: 63478
[parent_id]: 63477
[tags]: 
The logic remains the same for several classes, to wit If a document belonging to A… is classified as A, it's a true positive/true A is classified as B, it's a false positive for B/false B and a false negative for A is classified as C, it's a false positive for C/false C and a false negative for A If a document belonging to B… is classified as A, it's a false positive for A/false A and a false negative for B is classified as B, it's a true positive for B is classified as C, it's a false positive for C/false C and a false negative for B etc. Precision for A is true positives/(true positives + false positives) where “false positives” are the false positives from all other classes (i.e. the B documents classified as A + the C documents classified as A, etc.). Recall for A is true positives/(true positives + false negatives) where “false negatives” are all the A documents not classified as A (i.e. the A documents classified as B + the A documents classified as C, etc.) or, equivalently, the total number of A documents minus the number of true positives. You can also look at all this as a series of confusion matrices with two categories: One with A and non-A (so B and C together), one with B and non-B and finally one with C and non-C. Most informative is to report precision and recall for each category (especially if you have just a few) but I have seen people combine them in a F1 score and average across categories to obtain some sort of overall performance measure.
