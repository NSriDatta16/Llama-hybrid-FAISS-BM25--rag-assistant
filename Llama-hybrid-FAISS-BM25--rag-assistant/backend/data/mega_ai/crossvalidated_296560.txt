[site]: crossvalidated
[post_id]: 296560
[parent_id]: 296110
[tags]: 
I would propose a few potential issues with your current procedure. (2) is more likely the primary issue. Feature engineering / model selection: It may be the case that the models or the features are not well suited to extracting your signal of interest. This question is an empirical one, you should consider if your data can reasonably perform the task. You might also consider dimensionality reduction techniques (word2vec springs to mind) once you have constructed your features, but I would first try and see if you can get decent performance with a lasso using TFIDF features and build out from there. You are, in general, going to get a probability from these classification models. You can adjust the threshold (between 0 and 1) to maximize your performance metric of interest. Sometimes, implementations of these algorithms have a default threshold of 0.5 (everything above becomes a 1, everything below a 0). You don't have to use the 0.5 threshold, you can adjust it to maximize your precision subject to some minimum recall, for example. Sounds you like should try adjusting it upward in your case. Class imbalance: If you have many more examples of one class than another, you may end up with a model that emphasizes performance on one class (for example, if you have 99% positive examples, a model can get 99% accuracy by just classifying everything as positive). If you have class imbalance, consider up or down sampling or weighting upwards of observations of the minority class.
