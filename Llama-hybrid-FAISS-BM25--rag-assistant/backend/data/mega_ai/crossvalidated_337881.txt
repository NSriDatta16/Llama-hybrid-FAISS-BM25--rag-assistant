[site]: crossvalidated
[post_id]: 337881
[parent_id]: 337828
[tags]: 
Each eigenvalue indicates the variance of the data along the direction of the corresponding eigenvector. If the data are jointly Gaussian, then the covariance matrix completely determines the shape of the distribution. In this case, similar eigenvalues indicate a distribution that's closer to spherical. Uneven eigenvalues indicate a more ellipsoidal distribution that's wider along directions corresponding bigger eigenvalues. I'm assuming what you mean by "variance of eigenvalues" is $\frac{1}{d} \sum_{i=1}^d (\lambda_i - \bar{\lambda})^2$, given eigenvalues $\{\lambda_1, \dots, \lambda_d\}$ of a particular covariance matrix, with average $\bar{\lambda}$. This doesn't say much about shape. For example, consider covariance matrix $C_1$ with eigenvalues $[1, .01, .01]$. This corresponds to an elongated Gaussian with variance 100 times greater along the principal direction. Now consider covariance matrix $C_2$ with eigenvalues $[100, 99.01, 99.01]$. The corresponding Gaussian is much closer to spherical, but the variance of the eigenvalues is identical for $C_1$ and $C_2$. The eigenvalues' relative magnitudes say more about shape than their variance. Furthermore, if the data are non-Gaussian, the covariance matrix doesn't uniquely determine the shape of the distribution. Even though the eigenvalues tell us the variance along each direction, this may not be very informative about the shape of the distribution. For example, here are some datasets with unit variance in all directions (each covariance matrix has eigenvalues $[1, 1]$):
