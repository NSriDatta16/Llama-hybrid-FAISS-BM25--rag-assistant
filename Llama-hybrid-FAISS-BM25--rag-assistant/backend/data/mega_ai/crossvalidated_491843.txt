[site]: crossvalidated
[post_id]: 491843
[parent_id]: 
[tags]: 
Maximum Likelihood Estimator : special case of uniform which exclude the upper limit

I would like to understand why we can't get a MLE in this special case of PDF : "Sometimes it is not so easy to find the maximum of the likelihood function as in the examples above and one might have to do it numerically. Also, MLE does not always exist. Here is an example: let us consider uniform distribution $U[0, \theta)$ and define the density by $$ f(x \mid \theta)=\left\{\begin{array}{ll} \frac{1}{\theta}, & 0 \leq x The difference is that we 'excluded' the point $\theta$ by setting $f(\theta \mid \theta)=0 .$ Then the likelihood function is $$ \varphi(\theta)=\prod_{i=1}^{n} f\left(X_{i} \mid \theta\right)=\frac{1}{\theta^{n}} I\left(\max \left(X_{1}, \ldots, X_{n}\right) and the maximum at the point $\hat{\theta}=\max \left(X_{1}, \ldots, X_{n}\right)$ is not achieved. Of course, this is an artificial example that shows that sometimes one needs to be careful." Indeed, it is always possible to take $\hat{\theta}=X_{n}$ since the $\text{$\log\mathcal{L}$}$ is decreasing, isn't it ? Caution, this is slightly different from my previous post since I exclude the upper limit in PDF : $$\frac{1}{\theta} = 0 \leq x What is the trick to conclude about no existent $\hat{\theta}$ given a dataset $(X_1, X_2, ..., X_n)$ ? Any help is welcome EDIT 1: I found this answer particulary well explained for classical uniform case : " The p.d.f for one $x_{i}$ is given as $$ f(x \mid \theta)=\left\{\begin{array}{ll} \frac{1}{\theta} & \text { if } 0 \leq x \leq \theta \\ 0 & \text { otherwise } \end{array}\right. $$ Let's call $\vec{x}=\left(x_{1}, \ldots, x_{n}\right)$ The $n$ observations are i.i.d. so the likelihood of observing the $n$ -vector $\vec{x}=\left(x_{1}, \ldots x_{n}\right)$ is the product of the component-wise probabilities. Ignoring the issue of support for the moment, note that this product can be simply written as a power: $$ f(\vec{x} \mid \theta)=\prod_{i}^{n} \frac{1}{\theta}=\frac{1}{\theta^{n}}=\theta^{-n} $$ Next, we turn our attention to the support of this function. If any single component is outside its interval of support $(0,1 / \theta),$ then its contribution to this equation is a 0 factor, so the product of the whole will be zero. Therefore $f(\vec{x})$ only has support when all components are inside $(0,1 / \theta)$ $$ f(\vec{x} \mid \theta)=\left\{\begin{array}{ll} \theta^{-n} & \text { if } \forall i, 0 \leq x_{i} \leq \theta \\ 0 & \text { otherwise } \end{array}\right. $$ By definition, this is also our likelihood: $$ \mathcal{L}(\theta ; \vec{x})=f(\vec{x} \mid \theta)=\left\{\begin{array}{ll} \theta^{-n} & \text { if } \forall i, 0 \leq x_{i} \leq \theta \\ 0 & \text { otherwise } \end{array}\right. $$ The MLE problem is to maximize $\mathcal{L}$ with respect to $\theta$ . But because $\theta>0$ (given in the title of the problem) then $\theta^{-n}>0$ therefore 0 will never be the maximum. Thus, this is a constrained optimization problem: $$ \hat{\theta}=\operatorname{argmin}_{\theta} \theta^{-n} \text { s.t. } \forall i 0 \leq x_{i} \leq \theta $$ This is easy to solve as a special case so we don't need to talk about the simplex method but can present a more elementary argument. Let $t=\max \left\{x_{1}, \ldots, x_{n}\right\}$ . Suppose we have a candidate solution $\theta_{1}=t-\epsilon .$ Then let $\theta_{2}=t-\epsilon / 2 .$ Clearly both $\theta_{1}$ and $\theta_{2}$ are on the interior of the feasible region. Furthermore we have $\theta_{2}>\theta_{1} \Longrightarrow \theta_{2}^{-n} Therefore $\theta_{1}$ is not at the minimum. We conclude that the minimum cannot be at any interior point and in particular must not be strictly less than $t$ . Yet $t$ itself is in the feasible region, so it must be the minimum. Therefore, $$ \hat{\theta}=\max \left\{x_{1}, \ldots, x_{n}\right\} $$ is the maximum likelihood estimator. Note that if any observed $x_{i}$ is less than $0,$ then $\mathcal{L}$ is a constant 0 and the optimization problem has no unique solution." Now, I have to understand the impossible existence of a MLE when we have strict inequality, i.e : $$f(x \mid \theta)=\left\{\begin{array}{ll} \frac{1}{\theta}, & 0 \leq x From my first track, it seems that we can't have in the same time an estimator $\theta=t-\epsilon$ strictly lower than $\theta$ and respecting : $$\hat{\theta}=t-\epsilon=\operatorname{argmin}_{\theta} \theta^{-n}$$ But it is yet approximative in my mind. EDIT 2: Just a figure to better understand the 2 different cases (upper limit included or not) :
