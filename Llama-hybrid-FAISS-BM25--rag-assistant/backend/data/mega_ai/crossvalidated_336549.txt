[site]: crossvalidated
[post_id]: 336549
[parent_id]: 
[tags]: 
Bishop: EM algorithm, expectation step

I am reading the Bishop Pattern Recognition and Machine Learning and am on section 9.3 An Alternative View on EM: I am confused as to how Bishop obtains the expression $Q(\theta, \theta^{old}) = \sum_{Z}p(Z | C, \theta^{old}) lnp(X , Z | \theta)$ Thank you for insight! ANSWER: In the method of maximum likelihood, we're generally interested in p(X|θ), or equivalently logp(X|θ). We seek a θ that makes this quantity most likely. However, in our model, in addition to observations X we have latent variable(s) Z which we haven't observed but we have some idea of what it could be (we've elsewhere described a probability distribution guessing Z). So, we're now interested in p(X,Z|θ), or equivalently logp(X,Z|θ), instead of just choosing a θ to maximize p(X|θ). Yet it's hard to find the θ that maximizes logp(X,Z|θ) if we don't even know what Z is. Instead, we seek the θ that maximizes logp(X,Z|θ) weighted over our distribution of what Z might be.
