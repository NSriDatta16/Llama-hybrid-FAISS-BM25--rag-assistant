[site]: datascience
[post_id]: 72031
[parent_id]: 
[tags]: 
What does a conservative technique mean in the context of neural networks?

I am reading this article on Layer-wise relevance propagation method and I can't understand this particular paragraph LRP is a conservative technique, meaning the magnitude of any output y is conserved through the backpropagation process and is equal to the sum of the relevance map R of the input layer. This property holds for any consecutive layers j and k, and by transitivity for the input and output layer. What does this even mean? I can't understand what the author is trying to say. I understand what backpropagation is but not what is being said related to backpropagation!
