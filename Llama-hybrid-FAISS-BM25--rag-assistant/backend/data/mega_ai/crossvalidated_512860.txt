[site]: crossvalidated
[post_id]: 512860
[parent_id]: 
[tags]: 
Can dropout negatively impact performance by increasing repetition?

Dropout is the idea that you can drop, i.e set to zero, some of the nodes in a computational neural network. The goal of this is to increase regularization by preventing the model from relying too much on a few overfitted features. However, this seems to raise the possibility of the neural network learning the same features multiple times as backups, in case one of them is set to zero. Here's an extremely over-idealized scenario: In an image recognition neural network, one very important feature may be a wheel, because it strongly suggests an automobile. When dropout (with 50% chance) is applied, this "wheel" feature is dropped 50% of the time. However, instead of causing the neural network to regularize, the neural network creates a second copy of the "wheel" feature, just in case one of them is dropped. In fact, maybe the neural network could relearn this "wheel" feature 3 or 4 times, just in case. Doesn't this reduce the effectiveness of the neural network by essentially wasting nodes by learning the same features multiple times, instead of learning many different useful features? Or is this not really a concern in practice?
