[site]: datascience
[post_id]: 5908
[parent_id]: 5875
[tags]: 
The general approach in feature selection is to get a score of each feature in the data set and select top features. We can run algorithms like GBM or Random Forest on all the variables simply to get a ranking of variable importance. We can also use χ² (chi-squared) statistic with cross validation to select a user-specified percentile of features with the highest scoring. But, the disadvantage with these approaches is not detecting correlations between features. We can also use backward elimination: features are tested one by one and the ones that are not statically significant are deleted from data set. In forward selection that starts with out any variable in dataset and then adds variables that are statically significant. Hope this helps.
