[site]: crossvalidated
[post_id]: 271988
[parent_id]: 
[tags]: 
How can I objectively choose a sampling rate so that the values of the resulting time series are uncorrelated?

Setup I am making measurements that require both large storage space and long acquisition times . Aim In order to understand the underlying process, I need to precisely characterise the random variations of the instantaneous values (currently through its probability density function as I don't know it precisely yet -- and no, it is not normally-distributed). Problem The problem is that with my current sampling rate , the values I obtain are correlated (obvious in a lag/autocorrelation plot). Correlated values carry less statistical weight (one needs many more points to get an accurate description of the underlying distribution). Correlated values can skew the results (here it does since I cannot make as many measurements as I would need to). Correlated values give the wrong idea (in some cases, I have two peaks and I don't know if that's caused by improper sampling or if it represents an underlying truth about my system). What I tried and why it didn't work I tried to use the lag plot to find a proper decimation parameter. It didn't work because it is highly subjective , not computer-friendly , not very precise and I cannot do that for hundreds of lags . I tried to use the autocorrelation plot instead. That didn't work because choosing what correlation magnitude is negligible is subjective and it is not computer-friendly . In short, I need to know if there is a method that a computer could use that could provide me with a decimation parameter and that would be as objective as possible. Trying making it clearer Let's talk about another related (but different) subject. Let's imagine that I have two sets of data and want to know whether or not these sets were taken from the same distribution. One practical way to answer that question is to estimate and compare histograms of each set and, assuming quite a few things, provide an answer. That "histogram" method is akin to the solutions I have found (i.e. lag/autocorrelation plot): in all but the most obvious cases, if you ask two people for an answer, it is probable that they will differ. In short, qualitative estimations based on plots are highly subjective . This is not something I want. Worse than that: the answer is qualitative, so I can just get a feeling of the uncertainty as opposed to estimates. Now, an alternative method would be to compute CFDs instead, estimate ranges to get an idea of the spread and calculate overlaps in order to estimate the probability of being right. That method is not only computer-friendly, it is also more precise and reduces the field of subjectivity to a single number (that can be discussed) as opposed to a "feeling" of what it should be. To get back to our problem: the histogram method is similar to what I was doing (autocorrelation plot). And I am wondering what the second method equivalent is for correlation.
