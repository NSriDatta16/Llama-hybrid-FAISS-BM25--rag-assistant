[site]: crossvalidated
[post_id]: 407283
[parent_id]: 
[tags]: 
Filter-based feature selection for binary classification with unbalanced classes

I have a data set with ~10k observations and ~50 features. Each observation is assigned one of two classes (labeled 0 and 1, say). Approximately 98% of the observations are class 0, and the remaining 2% are class 1. I would like to reduce the number of features to isolate the 5-8 most "relevant" features. Ideally, I want the selected features to be strongly correlated with the class label, but uncorrelated with each other. (Next part might be suspect as I'm learning this on the fly) In essence, I'm just trying to gain insight on the data, so I want to use a filter-based method , which doesn't require introducing a model. For example, I tried penalized logistic regression, but then I have to select a penalty parameter, and I don't see how I can do this without introducing a model, doing CV, etc . I tried using a one-way ANOVA (as implemented in sklearn here ). This seemed to "work", but I'm concerned that the homoscedasticity assumption is violated because of the class imbalance (the class 1 points have 2x-10x the standard deviation of the class 0 points). Is there a go-to approach here?
