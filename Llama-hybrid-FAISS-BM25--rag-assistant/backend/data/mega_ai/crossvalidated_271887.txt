[site]: crossvalidated
[post_id]: 271887
[parent_id]: 259788
[tags]: 
I do not know the literature you referring to in detail, but I think a comprehensive summary of generalization bounds that should be up to date can be found in Boucheron et al. (2004) (Link: https://www.researchgate.net/profile/Olivier_Bousquet/publication/238718428_Advanced_Lectures_on_Machine_Learning_ML_Summer_Schools_2003_Canberra_Australia_February_2-14_2003_Tubingen_Germany_August_4-16_2003_Revised_Lectures/links/02e7e52c5870850311000000/Advanced-Lectures-on-Machine-Learning-ML-Summer-Schools-2003-Canberra-Australia-February-2-14-2003-Tuebingen-Germany-August-4-16-2003-Revised-Lectures.pdf#page=176 ) I will sketch part of the SVM bound in the following, leaving out details and proves. Before elaborating specifically about SVM bound, we need to understand what the generalization bounds are trying to achieve. First let us assume that the true probability $P(Y = +1| X = x)$ is known then the best possible classifier would be the bayes classifier, i.e. \begin{align} g* = \begin{cases} + 1 \ \ if P(Y = 1| X = x) > 0.5 \\ -1 \ \ otherwise \end{cases} \end{align} The goal of statistical learning theory now is to find the difference between a classifier of class $C$ (e.g. SVM) \begin{align} \hat{g}_n = arg \min_{g \in C} L_n(g) \end{align} and the bayes classifier, i.e. \begin{align} L(\hat{g}_n) - L(g*) = L(\hat{g}_n) - L(g^{*}_c) + L(g^{*}_c) - L(g*). \end{align} Note that $L(g) = \mathbb{E}l(g(X),Y)$ is the expected loss given data and $g^{*}_c$ is the best possible classifier in the model class $C$. The term $Z =: L(g*) - L(\hat{g}_n)$ is called estimation error and often the focus because it can be bounded much easier than the approximation error (the other term). I will also omit the approximation error here. The estimation error $Z$ can be further decomposed with \begin{align} Z = Z - \mathbb{E}Z + \mathbb{E}Z. \end{align} Now this can be bounded by two steps: Bound $Z - \mathbb{E}Z$ using McDiarmid inequality Bound $\mathbb{E}Z$ with the Rademacher complexity $R_n(C) = \mathbb{E}sup_{g \in C}|1/n \sum_{i=1}^{n} l(g(X_i),Y_i)|$ Using McDiarmids inequality one can show that if the loss function is ranging in an interval no more than $B$, step one result in a bound of \begin{align} Z - \mathbb{E}Z \leq 2 B \sqrt{\dfrac{ln(1/\delta)}{2n}}, \end{align} where $\delta$ is the confidence level. For the second step we can show that \begin{align} \mathbb{E}Z \leq 2R_n(C), \end{align} If you have a discrete loss-function, i.e. non-Lipschitz such as the 0-1-loss, you would need the VC-Dimension for further bounding the Rademacher Complexity. However, for L-lipschitz functions such as the Hinge-loss this can be further bounded by \begin{align} R_n(C) \leq \lambda L R/\sqrt{n}, \end{align} where $\lambda$ denotes the regularizer. Since for the Hinge-Loss $L = 1$ and $B = 1 + \lambda R$ (prove with Gauchy-Schwartz inequality) this further simplifies. Finally putting all results together, we can a bound of \begin{align} L(\hat{g}_n) - L(g^{*}_c) \leq 2(1 + \lambda R) \sqrt{\dfrac{ln(1/\delta)}{2n}} + 4 \lambda L R/\sqrt{n} \end{align}
