athcal {H}}_{k},} where ⟨ ⋅ , ⋅ ⟩ k {\displaystyle \langle \cdot ,\cdot \rangle _{k}} is the inner product in H k {\displaystyle {\mathcal {H}}_{k}} . 2. Functions in an RKHS are in the closure of the linear combination of the kernel at given points, f ( x ) = ∑ i k ( x i , x ) c i {\displaystyle f(\mathbf {x} )=\sum _{i}k(\mathbf {x} _{i},\mathbf {x} )c_{i}} . This allows the construction in a unified framework of both linear and generalized linear models. 3. The squared norm in an RKHS can be written as ‖ f ‖ k 2 = ∑ i , j k ( x i , x j ) c i c j {\displaystyle \|f\|_{k}^{2}=\sum _{i,j}k(\mathbf {x} _{i},\mathbf {x} _{j})c_{i}c_{j}} and could be viewed as measuring the complexity of the function. The regularized functional The estimator is derived as the minimizer of the regularized functional where f ∈ H k {\displaystyle f\in {\mathcal {H}}_{k}} and ‖ ⋅ ‖ k {\displaystyle \|\cdot \|_{k}} is the norm in H k {\displaystyle {\mathcal {H}}_{k}} . The first term in this functional, which measures the average of the squares of the errors between the f ( x i ) {\displaystyle f(\mathbf {x} _{i})} and the y i {\displaystyle y_{i}} , is called the empirical risk and represents the cost we pay by predicting f ( x i ) {\displaystyle f(\mathbf {x} _{i})} for the true value y i {\displaystyle y_{i}} . The second term in the functional is the squared norm in a RKHS multiplied by a weight λ {\displaystyle \lambda } and serves the purpose of stabilizing the problem as well as of adding a trade-off between fitting and complexity of the estimator. The weight λ {\displaystyle \lambda } , called the regularizer, determines the degree to which instability and complexity of the estimator should be penalized (higher penalty for increasing value of λ {\displaystyle \lambda } ). Derivation of the estimator The explicit form of the estimator in equation (1) is derived in two steps. First, the representer theorem states that the minimizer of the functional (2) can always be written as a linear combination of the kernels centered at the training-set points, for some c ∈ R n {\displaystyle \mathbf {c} \in \mathbb {R} ^{n}} . The explicit form of the coefficients c = [ c 1 , … , c n ] ⊤ {\displaystyle \mathbf {c} =[c_{1},\ldots ,c_{n}]^{\top }} can be found by substituting for f ( ⋅ ) {\displaystyle f(\cdot )} in the functional (2). For a function of the form in equation (3), we have that ‖ f ‖ k 2 = ⟨ f , f ⟩ k , = ⟨ ∑ i = 1 N c i k ( x i , ⋅ ) , ∑ j = 1 N c j k ( x j , ⋅ ) ⟩ k , = ∑ i = 1 N ∑ j = 1 N c i c j ⟨ k ( x i , ⋅ ) , k ( x j , ⋅ ) ⟩ k , = ∑ i = 1 N ∑ j = 1 N c i c j k ( x i , x j ) , = c ⊤ K c . {\displaystyle {\begin{aligned}\|f\|_{k}^{2}&=\langle f,f\rangle _{k},\\&=\left\langle \sum _{i=1}^{N}c_{i}k(\mathbf {x} _{i},\cdot ),\sum _{j=1}^{N}c_{j}k(\mathbf {x} _{j},\cdot )\right\rangle _{k},\\&=\sum _{i=1}^{N}\sum _{j=1}^{N}c_{i}c_{j}\langle k(\mathbf {x} _{i},\cdot ),k(\mathbf {x} _{j},\cdot )\rangle _{k},\\&=\sum _{i=1}^{N}\sum _{j=1}^{N}c_{i}c_{j}k(\mathbf {x} _{i},\mathbf {x} _{j}),\\&=\mathbf {c} ^{\top }\mathbf {K} \mathbf {c} .\end{aligned}}} We can rewrite the functional (2) as 1 n ‖ y − K c ‖ 2 + λ c ⊤ K c . {\displaystyle {\frac {1}{n}}\|\mathbf {y} -\mathbf {K} \mathbf {c} \|^{2}+\lambda \mathbf {c} ^{\top }\mathbf {K} \mathbf {c} .} This functional is convex in c {\displaystyle \mathbf {c} } and therefore we can find its minimum by setting the gradient with respect to c {\displaystyle \mathbf {c} } to zero, − 1 n K ( Y − K c ) + λ K c = 0 , ( K + λ n I ) c = Y , c = ( K + λ n I ) − 1 Y . {\displaystyle {\begin{aligned}-{\frac {1}{n}}\mathbf {K} (\mathbf {Y} -\mathbf {K} \mathbf {c} )+\lambda \mathbf {K} \mathbf {c} &=0,\\(\mathbf {K} +\lambda n\mathbf {I} )\mathbf {c} &=\mathbf {Y} ,\\\mathbf {c} &=(\mathbf {K} +\lambda n\mathbf {I} )^{-1}\mathbf {Y} .\end{aligned}}} Substituting this expression for the coefficients in equation (3), we obtain the estimator stated previously in equation (1), f ^ ( x ′ ) = k ⊤ ( K + λ n I ) − 1 Y . {