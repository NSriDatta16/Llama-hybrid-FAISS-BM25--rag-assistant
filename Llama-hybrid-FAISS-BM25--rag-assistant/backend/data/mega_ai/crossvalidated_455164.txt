[site]: crossvalidated
[post_id]: 455164
[parent_id]: 
[tags]: 
Why is 'c' minimized in sklearns logistic regression implementation?

I understood how the logistic model works and what it represents (log-odds). All the information on how the parameters are fit only evolved around the statistical way of maximizing the log-likelihood. In machine learning, at least how I understood it, the problem is usually tackled from the other side. Meaning we measure how 'wrong' the model is by a loss function. The default objective in sklearns implementation is the log-loss with l2 regularization: $\min_{\mathbf{w},c} \frac{1}{2}\mathbf{w}^T\mathbf{w}+C\sum_{i=1}^n \ln(1+e^{-y_i(\mathbf{x_i}^T\mathbf{w}+c)}).$ I have troble understanding where the small c variable stems from and why it is minimized. Can someone please explain? Best, Jonas
