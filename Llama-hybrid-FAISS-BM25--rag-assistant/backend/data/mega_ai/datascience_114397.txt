[site]: datascience
[post_id]: 114397
[parent_id]: 114372
[tags]: 
Depends on what you want to achieve. If you only care about performance, the straightest way to go is to build as much features as possible then dump them in a lgbm, then maybe build some ensemble with some advanced NNs. You can see this in the last Kaggle Tabular competition for exemple. (AMEX: https://www.kaggle.com/competitions/amex-default-prediction/discussion ) If you want something else (i.e. explainable model) the way to go would be to test some characteristics of features. Start with information value (package optbinning would do it - see amex comp.); in the age of Covid I would add some metrics about relationship to target to check for stability over time. Then you can select the top 100 features with no change in relationship. Logistic regression with L1 reg would help to further select the features down to
