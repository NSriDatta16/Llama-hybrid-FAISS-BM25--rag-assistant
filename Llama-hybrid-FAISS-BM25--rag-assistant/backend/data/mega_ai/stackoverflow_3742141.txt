[site]: stackoverflow
[post_id]: 3742141
[parent_id]: 3740703
[tags]: 
The only way to catch most "automated" requests is to code in logic that spots activity that couldn't possibly be human with a browser. For example, hitting pages too fast, filling out a form too fast, have an external source in the html file (like a fake css file through a php file), and check to see if the requesting IP has downloaded it in the previous stage of your site (kind of like a reverse honeypot), but you would need to exclude certain IP's/user agents from being blocked, otherwise you'll block google's webspiders. etc. This is probably the only way of doing it if curl (or any other automated script) is faking its headers to look like a browser.
