[site]: crossvalidated
[post_id]: 615310
[parent_id]: 615308
[tags]: 
This is the result that you should expect because linear regression is scale-invariant . This means that scaling does not affect its results. Scaling the features would change the parameters, but would not change the results. This is easy to see , take a trivial linear regression model $$ y = \beta_0 + \beta_1 x + \varepsilon $$ Now, if you scaled $x$ by dividing it by some constant $c$ , to get exactly the same (optimal) result as previously, you would need just to have $\beta_1$ be $c$ times larger, so it becomes $(\beta_1 c) (x/c) = \beta_1 x$ . The parameter estimates would adapt to scaling by increasing or decreasing accordingly. In fact, this is the case for many machine learning algorithms as you can learn from threads like Which machine learning algorithms get affected by feature scaling? or other questions tagged as feature-scaling . Scaling could make the difference though if you used regularized regression, or if using other than the OLS algorithm to obtain the results, or for random effects regression, etc.
