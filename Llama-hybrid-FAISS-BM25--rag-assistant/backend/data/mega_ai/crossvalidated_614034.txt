[site]: crossvalidated
[post_id]: 614034
[parent_id]: 184657
[tags]: 
In the way I understood it, hope it helps : On-policy learning updates the policy currently in use while off-policy learning updates a different policy using the data collected from a different policy. On-policy learning is a type of RL that updates the policy being used to take actions as the agent interacts with the environment. Specifically, the agent learns by following the current policy and then updates the policy based on the rewards received from those actions. (It is often used in situations where the agent's exploration of the environment is limited and the learning must be done with the current policy). Off-policy learning, updates a different policy than the one being used to take actions. This approach involves learning from the behavior of an "older" policy (or another one), while simultaneously interacting with the environment using a newer, improved policy (currently learned). (The main advantage of off-policy learning is that it allows for greater exploration of the environment, which can lead to better policies). Technically, the book is called buffer . Example: an agent playing a game of chess. With on-policy learning, the agent would learn by playing the game using its current policy and then update its policy based on the rewards it receives (experience is sampled from the updated policy). But with the off-policy learning, the agent might study a chess book to learn new strategies, and then incorporate those strategies into its policy while still playing games using its original policy (experience is sampled from the "book" policy here). I recommend to read this following article : https://medium.com/@sergey.levine/decisions-from-data-how-offline-reinforcement-learning-will-change-how-we-use-ml-24d98cb069b0
