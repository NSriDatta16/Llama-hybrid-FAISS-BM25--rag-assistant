[site]: datascience
[post_id]: 36285
[parent_id]: 
[tags]: 
What is the best performance metric used in balancing dataset using SMOTE technique

I used smote technique to oversample my dataset and now I have a balanced dataset. The problem I faced is that the performance metrics; precision, recall, f1 measure, accuracy in the imbalanced dataset are better performed than with balanced dataset. Which measurement can I use to show that balancing dataset may improve performance of the model? NB: roc_auc_score is better in balanced datset than roc_auc_score with imbalanced dataset Can it be considered as a good performance measurement? after the explanation i implemented code and i got this results import pandas as pd import numpy as np from sklearn import preprocessing import matplotlib.pyplot as plt plt.rc("font", size=14) from sklearn.svm import LinearSVC from sklearn.svm import SVC from sklearn.cross_validation import train_test_split,StratifiedShuffleSplit,cross_val_score import seaborn as sns from scipy import interp from time import * from sklearn import metrics X=dataCAD.iloc[:,0:71] y= dataCAD['Cardio1'] # Split the dataset in two equal parts X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=0) print(y_test.value_counts()) model=SVC(C=0.001, kernel="rbf",gamma=0.01, probability=True) t0 = time() clf = model.fit(X_train,y_train) y_pred = clf.predict(X_test) t = time() - t0 print("=" * 52) print("time cost: {}".format(t)) print() print("confusion matrix\n", metrics.confusion_matrix( y_test, y_pred)) cf=metrics.confusion_matrix(y_test, y_pred) accuracy=(cf.item((0,0))/50)+(cf.item((1,1))/14) print("model accuracy \n",accuracy/2) print() print("\t\tprecision_score: {}".format(metrics.precision_score( y_test, y_pred, average='macro'))) print() print("\t\trecall_score: {}".format(metrics.recall_score(y_test, y_pred, average='macro'))) print() print("\t\tf1_score: {}".format(metrics.f1_score(y_test, y_pred, average='macro'))) print() print("\t\troc_auc_score: {}".format(metrics.roc_auc_score( y_test, y_pred, average='macro'))) Results: Name: Cardio1, dtype: int64 ==================================================== time cost: 0.012008905410766602 confusion matrix [[50 0] [14 0]] model accuracy 0.5 precision_score: 0.390625 recall_score: 0.5 f1_score: 0.43859649122807015 roc_auc_score: 0.5 For balanced dataset X_train1,y_train1 = sm.fit_sample(X_train, y_train.ravel()) df= pd.DataFrame({'Cardio1': y_train1}) df.groupby('Cardio1').Cardio1.count().plot.bar(ylim=0) plt.show() print(X_train1.shape) print(y_train1.shape) #model=SVC(C=0.001, kernel="rbf",gamma=0.01, probability=True) model=SVC(C=10, kernel="sigmoid",gamma=0.001, probability=True) t0 = time() clf = model.fit(X_train1,y_train1) y_pred = clf.predict(X_test) t = time() - t0 print("=" * 52) print("time cost: {}".format(t)) print() print("confusion matrix\n", metrics.confusion_matrix(y_test, y_pred)) cf=metrics.confusion_matrix(y_test, y_pred) accuracy=(cf.item((0,0))/50)+(cf.item((1,1))/14) print("model accuracy \n",accuracy/2) print() #print("\t\taccuracy: {}".format(metrics.accuracy_score( y_test, y_pred))) print() print("\t\tprecision_score: {}".format(metrics.precision_score( y_test, y_pred, average='macro'))) print() print("\t\trecall_score: {}".format(metrics.recall_score(y_test, y_pred, average='macro'))) print() print("\t\tf1_score: {}".format(metrics.f1_score(y_test, y_pred, average='macro'))) print() print("\t\troc_auc_score: {}".format(metrics.roc_auc_score( y_test, y_pred, average='macro'))) Results: (246, 71) (246,) ==================================================== time cost: 0.05353999137878418 confusion matrix [[ 0 50] [ 0 14]] model accuracy 0.5 precision_score: 0.109375 recall_score: 0.5 f1_score: 0.1794871794871795 roc_auc_score: 0.5 I found no efficient results. Should i implement the model using cross validation?
