[site]: crossvalidated
[post_id]: 630493
[parent_id]: 
[tags]: 
How to understand the definition of Markov Chain $P(X_{n+1}\in B\mid \mathcal{F}_n)=p(X_n,B)$?

The definition of Markov Chain in Durrett ( Probability: Theory and Examples , 2019, Section 5.2) is: $$P(X_{n+1}\in B\mid \mathcal{F}_n)=p(X_n,B), $$ where $p$ is the Markov transition kernel distribution and $\mathcal{F}_n$ is the filtration induced by the sequence $(X_n)_n$ . My earlier exposure to Markov chains is associated with the property $$P(X_{n+1}=x\mid X_n=x_n)=P(X_{n+1}=x\mid X_{n-1}=x_{n-1},X_n=x_n).$$ What is the intuition behind the first definition and are those two definition equivalent?
