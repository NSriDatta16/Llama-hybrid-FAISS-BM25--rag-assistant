[site]: crossvalidated
[post_id]: 486218
[parent_id]: 485910
[tags]: 
The feed-forward layer is weights that is trained during training and the exact same matrix is applied to each respective token position. Since it is applied without any communcation with or inference by other token positions it is a highly parallelizable part of the model. The role and purpose is to process the output from one attention layer in a way to better fit the input for the next attention layer.
