[site]: crossvalidated
[post_id]: 18765
[parent_id]: 4961
[tags]: 
In simple terms, regularization is tuning or selecting the preferred level of model complexity so your models are better at predicting (generalizing). If you don't do this your models may be too complex and overfit or too simple and underfit, either way giving poor predictions. If you least-squares fit a complex model to a small set of training data you will probably overfit, this is the most common situation. The optimal complexity of the model depends on the sort of process you are modeling and the quality of the data, so there is no a-priori correct complexity of a model. To regularize you need 2 things: A way of testing how good your models are at prediction, for example using cross-validation or a set of validation data (you can't use the fitting error for this). A tuning parameter which lets you change the complexity or smoothness of the model, or a selection of models of differing complexity/smoothness. Basically you adjust the complexity parameter (or change the model) and find the value which gives the best model predictions. Note that the optimized regularization error will not be an accurate estimate of the overall prediction error so after regularization you will finally have to use an additional validation dataset or perform some additional statistical analysis to get an unbiased prediction error. An alternative to using (cross-)validation testing is to use Bayesian Priors or other methods to penalize complexity or non-smoothness, but these require more statistical sophistication and knowledge of the problem and model features.
