[site]: datascience
[post_id]: 117967
[parent_id]: 
[tags]: 
Manual computation of the predictions in a convolutional neural network

I am trying to manually compute the predictions of the Keras library for a convolutional neural network. However, I am struggling a lot to match my final result with the ones provided by Keras. I do appreciate it if you could help me with this question. I have a $r\times c$ tensor that includes categorical values. I apply the one-hot encoding method to convert this tensor to zeros and ones, which results in a $r\times c \times m$ tensor (a multi-channel tensor). I am trying to develop a regressor CNN to predict some quantitative values. The model summary is as follows: Model: "sequential" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 27, 3, 10) 550 conv2d_1 (Conv2D) (None, 27, 3, 20) 1820 max_pooling2d (MaxPooling2D (None, 9, 1, 20) 0 ) flatten (Flatten) (None, 180) 0 dense (Dense) (None, 20) 3620 dropout (Dropout) (None, 20) 0 dense_1 (Dense) (None, 10) 210 dense_2 (Dense) (None, 1) 11 ================================================================= Total params: 6,211 Trainable params: 6,211 Non-trainable params: 0 _________________________________________________________________ My understanding is that each channel/kernel has a bias that will be added to all of its elements after the convolution. Is that correct? For the flattening layer, my understanding is that one should start by flattening the first, channel, then the second channel, and so forth. For each channel, they need to flatten by rows (i.e., start from the first row, then the second row, and so forth). Am I correct? To fully grasp how a CNN works, I am using the weights determined by the Keras model and computing the prediction manually, and comparing it with the prediction of Keras. However, my manual calculation is too different than the Keras prediction. Would you please look at my code and let me know where I am making a mistake? To reproduce the result, you may download the sample input file uploaded on dropbox . Please note that this program is written for this specific test and may not be generalizable. import itertools import pickle import numpy as np import tensorflow as tf from tensorflow.keras.utils import to_categorical from keras.models import Sequential from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout from sklearn.model_selection import KFold from tensorflow import keras import keras.backend as K with open('NeuralNet_Inst0.pkl', 'rb') as file: result, sol = pickle.load(file) # sol is the original input tensor cZ1 = to_categorical(sol) # the converted input tensor weight = result['weight'] # weights obtained by Keras nNC = result['nNC'] # number of neurons in convolution layers nNF = result['nNF'] # number of neurons in fully connected layers layers layers = result['layers'] # dimension of layers E = result['kSize'][0] # size of kernal P = result['pSize'][0] # size of pool actC = result['actC'] # activation function of convolution layers actF = result['actF'] # activation function of fully connected layers cRow = layers[0][0] cCol = layers[0][1] ### Convolution Layers ### cZ2 = np.zeros((cRow, cCol, nNC[0]), dtype=np.float32) w2 = weight[0] b2 = weight[1] for n in range(nNC[0]): for t, m in itertools.product(range(cRow), range(cCol)): for npp, e, ep in itertools.product(range(cZ1.shape[2]), range(-1, E-1), range(-1, E-1)): if (t+e >= 0) and (m+ep >= 0) and (t+e = 0) and (m+ep >= 0) and (t+e
