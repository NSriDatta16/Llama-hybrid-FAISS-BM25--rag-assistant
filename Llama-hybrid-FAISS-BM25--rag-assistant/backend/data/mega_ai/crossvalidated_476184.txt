[site]: crossvalidated
[post_id]: 476184
[parent_id]: 475788
[tags]: 
The simplest way is to use a selection criterion such as the Bayesian Information Criterion ( Schwarz, 1978 ). Other methods are available, such as the supF-tests described in section 5.1 of ( Bai and Perron, 2003 ), but using an information criterion is far simpler. The BIC for a model $M$ is given by $$\mathrm{BIC} = k\ln(n)-2\ln(\widehat {L}),$$ where $\hat {L}$ is the maximized value of the likelihood function of the model $M$ , i.e. $\hat {L}=p(x\mid {\widehat {\theta }},M)$ , where $\widehat{\theta}$ are the parameter values that maximize the likelihood function for model $M$ ; $x$ is the observed dataset; $n$ is the number of data points in $x$ ; and $k$ is the number of parameters estimated by the model. Models with low BIC values are preferred because good models: fit the data well (so have low $-2\ln(\hat{L})$ values), and don't have many parameters (so have lower $k \ln(n)$ values). I've simulated some data and done an analysis below: library(segmented) set.seed(1) n = 300 x1 = runif(n/3, min = 0, max = 5) y1 = 1 + 2*x1 + rnorm(n/3) x2 = runif(n/3, min = 5, max = 15) y2 = 6 + 1*x2 + rnorm(n/3) x3 = runif(n/3, min = 15, max = 20) y3 = -1.5 + 1.5*x3 + rnorm(n/3) x = c(x1, x2, x3) y = c(y1, y2, y3) par(mar = c(4.1, 4.1, 0.1, 0.1)) plot(x, y, xlab = "x", ylab = "y", cex = 0.5, pch = 16) Now let's compare the model with just one regression line, versus the model with three groups: lm1 = lm(y ~ x) s1 = segmented(lm1, seg.Z = ~x, npsi = 2) par(mar = c(4.1, 4.1, 0.1, 0.1)) plot(x, y, xlab = "x", ylab = "y", cex = 0.5, pch = 16) abline(lm1, lwd = 2) sss = seq(from = 0, to = 20, length.out = 1000) lines(sss, predict.segmented(s1, newdata = data.frame(x = sss)), lty = 2, lwd = 2) The model estimated by segmented is pretty close to the truth: summary(s1) intercept(s1) gives ***Regression Model with Segmented Relationship(s)*** Call: segmented.lm(obj = lm1, seg.Z = ~x, npsi = 2) Estimated Break-Point(s): Est. St.Err psi1.x 4.959 0.268 psi2.x 15.860 0.422 Meaningful coefficients of the linear terms: Estimate Std. Error t value Pr(>|t|) (Intercept) 0.84810 0.22134 3.832 0.000156 *** x 2.04758 0.07677 26.673 so the change points are estimated well, with guesses of $4.959$ and $15.860$ when the true values are $5$ and $15$ . The intercepts and slopes are also done well, with $(0.85, 2.0)$ estimated for $(1, 2)$ ; $(6.1, 1.0)$ estimated for $(6, 1)$ ; and $(-4.7, 2.7)$ estimated for $(-1.5, 1.5)$ . For a linear regression with normal errors, the BIC can be calculated from the residual sum of squares (RSS) and is given by: $$\mathrm{BIC} = n\ln(\textrm{RSS}/n) + k\ln(n) + C(n),$$ where $C(n)$ does not depend on the model complexity or fit, so we ignore it. For the linear model we calculate the BIC as: n*sum(lm1$residuals^2/n) + 3*log(n) which is roughly $537$ . For the simple model $k = 3$ because the parameters are the intercept, the slope and the variance. For the three groups model we calculate the BIC as: n*sum(s1$residuals^2/n) + 9*log(n) which is roughly $349$ . For this model $k=9$ because we have three sets of intercepts and slopes, two change points and the variance. The difference between the two BICs is roughly $188$ , which is massive evidence in favour of the model with three groups. A difference of BICs of more than $10$ is considered to be very strong evidence in favour of one model according to ( Kass and Raftey, 1995 ). Suppose that the true model is that there is just one slope, and we try to fit a segmented regression to it, the BIC would hopefully show us that the best model is the simplest one set.seed(100) y2 = 3 + x + rnorm(100) lm2 = lm(y2 ~ x) s2 = segmented(lm2, seg.Z = ~x, npsi = 2) n*sum(lm2 $residuals^2/n) + 3*log(n) n*sum(s2$ residuals^2/n) + 9*log(n) In this case the BIC for the simple model is roughly $326$ , and the BIC for the complex model is roughly $356$ . So there is strong evidence that the simple model is better. References Bai, J. and P. Perron (2003). "Computation and analysis of multiple structural change models", Journal of Applied Econometrics 18 (1), 1–22 Kass, Robert E.; Raftery, Adrian E. (1995), "Bayes Factors", Journal of the American Statistical Association, 90 (430): 773–795 Schwarz, Gideon E. (1978). "Estimating the dimension of a model", Annals of Statistics, 6 (2): 461–464
