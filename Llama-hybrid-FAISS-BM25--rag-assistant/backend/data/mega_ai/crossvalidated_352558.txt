[site]: crossvalidated
[post_id]: 352558
[parent_id]: 
[tags]: 
When is there a difference between a normal likelihood loss and a least squares loss?

My understanding is that if the errors follow a normal distribution, then using a maximum likelihood loss or a least squares loss to train a model amounts to the same thing. However, I am looking at some code for a neural network time series prediction model, and it includes as an input argument the option to specify whether a squared loss or a normal likelihood loss is used, along with a comment that if normal likelihood loss is specified, then the covariance term is trained as well. 3 things I don't get about this: Don't a likelihood loss and a squared loss amount to the same thing if the distribution is normal (see here )? What is a covariance term in the case of time series? What does "training the covariance term" mean exactly in the context of neural networks? That both the loss and the covariance are minimized?
