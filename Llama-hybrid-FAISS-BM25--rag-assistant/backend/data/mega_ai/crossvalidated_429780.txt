[site]: crossvalidated
[post_id]: 429780
[parent_id]: 110266
[tags]: 
The problem with directly using the sparse matrix or large dimensional to do the nearest neighbor computation is the curse of dimensionality . The curse of dimensionality is the general term for all the problems related to large dimensions. In your case, the distance function on high correlated (less information) will not give contrast to the distance. Sparsity peeps in as well, which also makes the data not relevant to be used. SVD is helpful in reducing the matrix to a rank of the most relevance. If your data is N x M though it contains only rank r information, the dimensionality reduction using the SVD will give you mostly relevance features up to N x r. A similar effect is achieved using other dimensionality reduction techniques too such as PCA. So, it's better for you to replace the zero with the mean of the data which will be non-extra information added by your pre-processing which anyhow will be removed by SVD. so that we do not lose any other relevant information on the same sample. and then perform KNN (or other) to do an instance-based search of the related samples.
