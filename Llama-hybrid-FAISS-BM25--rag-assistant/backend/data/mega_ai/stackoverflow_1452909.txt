[site]: stackoverflow
[post_id]: 1452909
[parent_id]: 1451626
[tags]: 
Duffymo posted the best refernce in my opinion. I studied the page rank algorithm in my senior undergrad year. Page rank is doing the following: Define the set of current webpages as the states of a finite markov chain. Define the probability of transitioning from site u to v where the there is an outgoing link to v from u to be 1/u_{n} where u_{n} is the number of out going links from u. Assume the markov chain defined above is irreducible (this can be enforced with only a slight degradation of the results) It can be shown every finite irreducible markov chain has a stationary distribution. Define the page rank to be the stationary distribution, that is to say the vector that holds the probability of a random particle to end up at each given site as the number of state transitions goes to infinity. Google uses a slight variation on the power method to find the stationary distribution (the power method finds dominant eigenvalues). Other than that there is nothing to it. Its rather simple and elegant and probably one of the simplest applications of markov chains I can think of, but it is wortha lot of money! So all the pagerank algorithm does is take into account the topology of the web as an indication of whether a website should be important. The more incoming links a site has the greater the probability of a random particle spending its time at the site over an infinite amount of time.
