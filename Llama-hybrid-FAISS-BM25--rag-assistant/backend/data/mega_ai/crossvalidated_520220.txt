[site]: crossvalidated
[post_id]: 520220
[parent_id]: 519860
[tags]: 
I see two problems here. First, as @whuber notes in a comment, you probably have insufficient data to fit a model with this many predictors. To avoid overfitting a logistic regression, you should limit yourself to about 1 predictor per 10-20 members of the minority outcome class. With 39 non-organic outcomes, you would be hard pressed to justify more than 4 predictors in a model, while you have about 10 in your first model. Second, logistic regression has a particular problem when some combination of predictors is completely or nearly completely associated with outcome: (near)-perfect separation . The enormous standard errors for the intercept in the first model suggest that you might be in such a situation, which was alleviated by removing the two predictors to get the second model. Both of those problems in principle can be worked around with penalized methods like ridge regression that reduce coefficient-magnitude estimates to minimize overfitting.
