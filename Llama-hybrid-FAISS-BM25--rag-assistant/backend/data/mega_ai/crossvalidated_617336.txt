[site]: crossvalidated
[post_id]: 617336
[parent_id]: 
[tags]: 
Wrong predictions when using a trained model in deployment

Background: I have trained a deep neural network model with 3 hidden layers of size 32,32,16 for binary classification problem. I have 9 input features and a 0 or 1 as output. My aim is to use this trained model in an application where I provide the 9 features (computed dynamically in the application code), and I get a 0 or 1 to decide my course of action accordingly. The problem I am dealing with is an imbalanced classification where in the dataset the proportion of 1s overpowers the 0s. Hence, I use the AUC ROC metric to assess the performance and the binary cross entropy as the loss function. Below is the learning curve I get using a learning rate of 1e-4, batch size of 32, and using L2 regularization. Methodology and problem faced: From the curve, I assume this trained_model can be accepted as the test performance was also good. So I apply this model in my actual scenario, i.e., the application. In this application, I collect the same 9 features, normalize them using the minimum and maximum used in the original training set, and then use trained_model.predict(). However, I find that it is not performing well in the application i.e., predicting a 0 even for the straightforward cases where it should have been a 1! My analysis: I am confused about where I am going wrong. Because the way the 9 features are generated/computed is the same in the training dataset generation and application codes. So, there arenâ€™t any distribution changes, I believe. For e.g., features 1 and 2 are from a Poisson distribution in both cases, others are computed using some formulas. I generate the synthetic data using a Python script say File_A.py, train and use model in another File_B (the application). This File_B is the same as File_A except that it doesn't collect the dataset but rather applies the trained_model() in its code where a time-intensive operation exists. Ofcourse, the values of the input parameters change in both files but not the distribution they come from. Or is it suggested I add any more features that better affect the outcome? But in any case, keeping in view the learning curve of the model, even this trained_model should not at least perform this badly even for simple straightforward cases. I also did feature importance analysis using XAI Shap and decided on these 9 features. Can anyone throw some light on this behavior and suggest anything?
