[site]: crossvalidated
[post_id]: 359586
[parent_id]: 359233
[tags]: 
Regardless of whether you could , I don't think that you should. There are three problems with this approach: Significance does not correlate with predictive power . For example, two sample means may differ significantly while those samples' probability distributions overlap considerably. In this case, there is no separation that will yield a high prediction accuracy. The $t$-test compares the means of the groups, in relation to their standard error. The $\chi^2$-test compares the observed proportions of categories in either group to their expected proportions (expected if there were no difference). The $p$-values of these tests are evidence against different null-hypotheses . Even if you were to use another way to find the 'best' predictors, you would be using the data twice: Once to select variables and once to estimate the parameters of a model with those variables. This is a form of stepwise regression ( which is a bad idea ) . Lastly, I don't see any mention of training/validation and where you would apply these tests. A significant difference in the groups in your sample need not imply an actual difference in those group's populations. However, just like the problem of multiple testing you mentioned in the comments, solving this will not solve the fundamental problem with this kind of variable selection. So how then should you proceed? As mentioned in the comments and answers of this question , you may want to opt for a regularized regression model instead, which will shrink smaller coefficients to(wards) zero. Moreover, certain models can 'ignore' irrelevant predictors reasonably well, such as tree-based regression with random forests. Entire books have been written on model selection alone, but that Q&A is a very good starting point to better understand why this simply won't work well.
