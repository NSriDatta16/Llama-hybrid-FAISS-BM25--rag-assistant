[site]: crossvalidated
[post_id]: 347063
[parent_id]: 346573
[tags]: 
You fit a model to error free input features. Then you added some error (noise) to your same data and fit the model again. You observed worse prediction on noisy inputs (inputs with error) than on noise free inputs (inputs without error). You expected the model to be just as good as previous model on noise free inputs and better on noisy inputs. You did not add more training data you simply duplicated the same data with noise. Intuitively, a model trained on ALL noise free inputs is going to have more accurate predictions when inputs are also noise free than a model trained on ALL noisy data. Similarly, my intuition is that a model trained with all noisy data will be more accurate when predicting from noisy inputs than a model trained with all noise free data. If you have some mix of noise free and noisy data, then my intuition is you will have better predictions on noisy data than a model trained with only noise free data and better predictions on noise free data than a model trained on only noisy data. This seems consistent with what you observed. EDIT: Basically, overfitting occurs when we mistake noise in the data for signal. I use the term noise in th conceptual sense of useless information or information specific only to training data. If this occurs the model fits the training data well, but does not generalize well. Imagine we have points and the model interpolates all the points. If the points are noisy then this behavior is undesirable. My rudimentary knowledge of data augmentation is that it reduces overfitting because when we add noise to training data the model we fit will tend to balance the error between these nearby points in order to minmize the overall error. This model is better in an average sense in that it has less error when predicting both noisy and noise free data. It will generalize better to data that may be slightly different than training data. However, the model does not distinguish between noisy and noise free data so it has worse performance on noise free data because it mistakes some signal for noise.
