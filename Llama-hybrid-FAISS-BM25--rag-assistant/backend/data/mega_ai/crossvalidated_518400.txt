[site]: crossvalidated
[post_id]: 518400
[parent_id]: 242770
[tags]: 
I will give a shot at a technical answer. Let us assume that all regression assumptions are met and that the predictors have a multivariate normal distribution. In notation, we can summarize this as $Y=\beta^{\top}X+\epsilon, \text{with} \quad X \sim N(\mu_X, \Sigma_X),\quad \epsilon \sim N(0,\sigma_\epsilon^2)$ with $Y$ representing the dependent variable and $X$ the independent variables/predictors, $\epsilon$ the error. When using adjusted-R-squared, what we actually want to estimate is $$\rho^2=1-\frac{\sigma_\epsilon^2}{Var(Y)}.$$ In words: this is the amount of variance of the dependent variable that the best linear function f (as represented by the true regression weights $\beta$ ) explains. On a side note, I wrote a paper [1] that shows that often there are better ways to do this than standard adjusted R-squared. For predicted R-squared, we almost use the same formula, only a different error term. Instead of estimating the irreducible error $\sigma_\epsilon^2$ , we are interested in $$E_{X,Y}([\hat{f}(X)-Y]^2)$$ where $\hat{f}$ is an estimate of the best function $f$ that we got from applying linear regression on a training set $D$ . Thus, the population value of predicted R-squared is $$\rho_c^2=1-\frac{E_{X,Y}([\hat{f}(X)-Y]^2)}{Var(Y)}$$ This is thus the amount of variance that the particular function $\hat{f}$ explains in the population. We can decompose (this is the start of the well-known bias-variance decomposition from machine learning) $$E([\hat{f}(X)-Y]^2)=E_{X,Y}([\hat{f}(X)-f(X)]^2)+E_{X,Y}([\hat{f}(X)-Y]^2)=E_{X,Y}([\hat{f}(X)-f(X)]^2)+\sigma_\epsilon^2.$$ Note that thus $E_{X,Y}((\hat{f}(X)-Y)^2) \geq\sigma_\epsilon^2$ and equality holds if $\hat{f}=f$ , which is generally not the case. In words: The error that the estimated function $\hat{f}$ makes consists of the difference between the estimated function $\hat{f}$ and the true function $f$ and the difference between the true function $f$ and the true value $Y$ . Or, in other words, $\rho^2_c$ is an upper bound for $\rho^2$ , and no function can predict better than $\sigma_\epsilon^2$ . So which measure is better at model selection? It depends. If you want to select the set of predictors that will lead to the most accurate predictions based on the current sample, then the predicted R-squared is better. If you want to select the set of predictors that will lead to the most accurate predictions if you had the whole population available (which is arguably often the question we would like to ask when we do explanatory modeling), then adjusted R-squared is better. One word of warning. Predictive R-squared estimation via cross-validation works with rather minimal assumptions (only independence is needed). In contrast, adjusted R-squared estimation generally relies on all regression assumptions and the assumption that the predictors are multivariate normal. References $[1]$ : Karch J (2020): Improving on adjusted R-squared. Collabra: Psychology (2020) 6 (1): 45. ( link )
