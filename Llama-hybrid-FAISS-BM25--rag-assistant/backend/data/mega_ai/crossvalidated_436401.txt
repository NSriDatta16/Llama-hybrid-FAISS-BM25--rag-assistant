[site]: crossvalidated
[post_id]: 436401
[parent_id]: 436349
[tags]: 
Let's take a look at this with some data N Here, the population average for one of the sexes is 45%. The data sex come from a binomial distribution. First, the GLM Call: glm(formula = cbind(sex, N - sex) ~ 1, family = binomial(link = "logit")) Deviance Residuals: [1] 0 Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) -0.13889 0.05176 -2.683 0.00729 ** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 0 on 0 degrees of freedom Residual deviance: 0 on 0 degrees of freedom AIC: 9.7603 Number of Fisher Scoring iterations: 2 It is important to be clear on what the deviance tells us. I'm not going to get into the hypothesis testing details, but will approach it from a high level. The residual deviance is comparing our model to a saturated model (essentially, a model with one parameter per observation. Not super useful since we only have one observation and no predictors). If our residual deviance is not too big, then we can conclude that there appears to be evidence that our model is nearly as good as a saturated model. That is good because often times the model we have in hand is much simpler than a saturated model, and thus we should prefer the model in hand because it is simpler (thus more parsimonious). The null deviance is tells us the deviance for a model which only has one intercept. If you take the difference between the residual and null deviance and compare that difference to an appropriate chi-square distribution, then that tells us how our model in hand compares to a model with no predictors. Our hypothesis is often that our predictors tell us something about the outcome, and so we want the difference between residual and null deviance to be quite large. In this example, the deviance is uninteresting because a) we have no predictors, and b) we have one observation. So far as the difference between the two tests, they are testing different things. binom.test tests the proportion parameter for your data. The deviance goodness of fit tests are comparing your model to two very different models (a saturated and null model). A similar test to prop.test with the GLM is the wald test for the intercept. Let's go back to our data and use prop.test. 1-sample proportions test without continuity correction data: sex out of N, null probability 0.5 X-squared = 7.2107, df = 1, p-value = 0.007247 alternative hypothesis: true p is not equal to 0.5 95 percent confidence interval: 0.4402119 0.4906319 sample estimates: p 0.4653333 And now let's look at our intercept from our GLM on the appropriate scale >sigmoid sigmoid(coef(model)) (Intercept) 0.4653333 Our intercept returns the same estimate of the population proportion, and the estimate is significant as per the output.
