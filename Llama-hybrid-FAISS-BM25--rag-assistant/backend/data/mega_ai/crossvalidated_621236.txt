[site]: crossvalidated
[post_id]: 621236
[parent_id]: 
[tags]: 
Why would a model combining two pre-trained models not even achieve the performance of the best sub-model?

I have two different CNNs trained on the same dataset. One performs a bit better than the other but I believe each can provide different and useful information. I use pytorch . Each model consists in convolution and pooling layers, followed by a couple of linear layers and a final linear layer that acts as classifier (output size of 2). What I've done is train each model independently. Then, the combined model takes the output of the last linear layer before the classifier of each model, concatenates them, and passes them through a feed-forward network. I assumed that even if it could not improve the performance, the combined model would at least perform as well as the best sub-model. However, I can't even seem to achieve that, the performance always stays midway between the performances of both models. To build each model I am using the optuna framework to optimize the hyper-parameters. Tuning the combined model should be much simpler than tuning the others since the only hyper-parameters I am optimizing about the structure of the net are the amount of hidden layers and their sizes (apart from other parameters such as lr, scheduler or optimizer). Almost from the first trial, the study achieves the max performance and stays there, unable to improve it. I have also tried normalizing each of the models outputa (separately) before concatenating, as well as applying a relu or tanh activation, without any luck. Note: I am supposedly disabling training for the pre-trained models when training the combined one by doing this: for param in model.parameters(): param.requires_grad = False just in case this could be failing and the models could be being trained somehow More notes: I am using AUROC to assess the validation results due to class imbalance and got 0.823 and 0.845 for each sub-model and just tonight managed to go from 0.83 to 0.840 for the combined one, still a bit under 0.845. I still need to look more into how they differ but a quick look revealed their predictions do differ for some of the samples (meaning each of them is able to detect the positive class in some cases the other cannot). Is the difference between 0.845 and 0.840 too small to think there'a a problem at all? Maybe they simply cannot improve the result? I really think they should as I believe each CNN can capture different aspects of the data but I don't know what else to try. More detail, as requested: AUROC Model 1 (best val AUROC is 0.8215): epoch#0 "train: 0.6366974115371704. val: 0.7702255249023438" epoch#1 "train: 0.7658864259719849. val: 0.8044375777244568" epoch#2 "train: 0.7930459976196289. val: 0.8081876039505005" epoch#3 "train: 0.808762788772583. val: 0.8145741820335388" epoch#4 "train: 0.8074648380279541. val: 0.819037914276123" epoch#5 "train: 0.8217006921768188. val: 0.8215399980545044" epoch#6 "train: 0.8180385828018188. val: 0.8036410212516785" epoch#7 "train: 0.8230613470077515. val: 0.8032418489456177" epoch#8 "train: 0.8317129611968994. val: 0.794983983039856" epoch#9 "train: 0.831802248954773. val: 0.7899027466773987" Model 2 (best val AUROC is 0.8459): epoch#0 "train: 0.7058017253875732. val: 0.809760332107544" epoch#1 "train: 0.763457179069519. val: 0.8283499479293823" epoch#2 "train: 0.7822374701499939. val: 0.8328834772109985" epoch#3 "train: 0.792770266532898. val: 0.8357023000717163" epoch#4 "train: 0.8013472557067871. val: 0.8357467651367188" epoch#5 "train: 0.7995779514312744. val: 0.8434610366821289" epoch#6 "train: 0.8074525594711304. val: 0.8365026712417603" epoch#7 "train: 0.8083140254020691. val: 0.8424113988876343" epoch#8 "train: 0.8113613128662109. val: 0.840691089630127" epoch#9 "train: 0.8122062683105469. val: 0.8387175798416138" epoch#10 "train: 0.8135561347007751. val: 0.83888840675354" epoch#11 "train: 0.8170522451400757. val: 0.8417861461639404" epoch#12 "train: 0.8170279264450073. val: 0.8459413051605225" epoch#13 "train: 0.8188912272453308. val: 0.8401663303375244" epoch#14 "train: 0.815981388092041. val: 0.8426406383514404" epoch#15 "train: 0.8164429664611816. val: 0.8418955206871033" epoch#16 "train: 0.81856369972229. val: 0.844316840171814" epoch#17 "train: 0.8204455971717834. val: 0.8412182331085205" epoch#18 "train: 0.8213235139846802. val: 0.8446505069732666" epoch#19 "train: 0.8185620307922363. val: 0.8446468710899353" epoch#20 "train: 0.8193531036376953. val: 0.8453472852706909" epoch#21 "train: 0.824106752872467. val: 0.8401384353637695" epoch#22 "train: 0.8230797052383423. val: 0.8440967798233032" epoch#23 "train: 0.8226444721221924. val: 0.844730794429779" epoch#24 "train: 0.8208979368209839. val: 0.8456006050109863" epoch#25 "train: 0.8220465183258057. val: 0.8424403667449951" epoch#26 "train: 0.8219859600067139. val: 0.8446440696716309" epoch#27 "train: 0.8249871730804443. val: 0.8453245162963867" epoch#28 "train: 0.8226014375686646. val: 0.8424029350280762" epoch#29 "train: 0.8257030248641968. val: 0.844139575958252" Combined model: epoch#0 "train: 0.6831338405609131. val: 0.8345752954483032" epoch#1 "train: 0.6995154619216919. val: 0.8403782248497009" epoch#2 "train: 0.7099341154098511. val: 0.8315378427505493" epoch#3 "train: 0.7238966822624207. val: 0.8292025923728943" The combined model has few epochs because optuna pruned it. The combined model overfits less on the training dataset due to dropout I guess. With the CNNs seems more difficult to prevent as Model 1 does seem to overfit quickly even with quite a lot of dropout. Training data size is around 70k and validation data size is around 24k. The data is highly imbalanced (only around 5% are positives) and I use WeightedRandomSampler for the training set so not all training data is used in all epochs.
