[site]: crossvalidated
[post_id]: 594862
[parent_id]: 594858
[tags]: 
Simulating The Data There are a couple ways to do this. To demonstrate, I'll first simulate some data in R library(tidyverse) set.seed(0) d Note that the true outcome prevalence for control is about 31% and the outcome in the manipulation group is 35%. My recommendation would be to manipulate the data to be in tidy format, as shown below. # A tibble: 1,920 Ã— 4 id trial condition y 1 1 1 control 0 2 1 1 manipulation 0 3 1 2 control 1 4 1 2 manipulation 0 5 1 3 control 1 6 1 3 manipulation 1 7 1 4 control 0 8 1 4 manipulation 0 9 1 5 control 0 10 1 5 manipulation 1 Now, onto approaches for these data. Linear Regression Assuming the marginal outcome rate is not too high or low, a linear regression could work fairly well. lm(y ~ condition, data=d) Call: lm(formula = y ~ condition, data = d) Residuals: Min 1Q Median 3Q Max -0.4156 -0.4156 -0.3583 0.5844 0.6417 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 0.35833 0.01570 22.82 The estimates are fairly close to the actual data, and if we repeat the simulation many times, we see the estimates are unbiased for the true probabilities This approach breaks down when the outcome probabilities are near 0 or 1, in which case you can use... Logistic Regression Standard approach for these kinds of data. Because all subjects have the same amount of observations, logistic regression should be more than enough to estimate the effects. glm(y~condition, data = d, family = binomial()) %>% summary() Call: glm(formula = y ~ condition, family = binomial(), data = d) Deviance Residuals: Min 1Q Median 3Q Max -1.037 -1.037 -0.942 1.325 1.433 Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) -0.58261 0.06731 -8.656 Note that the effects are now on the log odds scale so you have to do some more work to get those into a more interpretable scale. Lastly, you could do... Mixed Effects Logistic Regression This will allow you to estimate each subject's outcome prevalence and then estimate the between subject variability. You can do this with library(lme4) glmer(y~condition + (1|id), data = d, family=binomial()) %>% summary() Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [glmerMod] Family: binomial ( logit ) Formula: y ~ condition + (1 | id) Data: d AIC BIC logLik deviance df.resid 2562.1 2578.8 -1278.0 2556.1 1917 Scaled residuals: Min 1Q Median 3Q Max -0.8433 -0.8433 -0.7473 1.1858 1.3382 Random effects: Groups Name Variance Std.Dev. id (Intercept) 0 0 Number of obs: 1920, groups: id, 80 Fixed effects: Estimate Std. Error z value Pr(>|z|) (Intercept) -0.58261 0.06731 -8.656 However, if you aren't interested in between subject variability, I would avoid this approach. My Recommendation Depending on if you've ever worked with generalized linear models or not, I would first recommend logistic regression then followed by linear regression in the case the outcome is not very rare or very prevalent.
