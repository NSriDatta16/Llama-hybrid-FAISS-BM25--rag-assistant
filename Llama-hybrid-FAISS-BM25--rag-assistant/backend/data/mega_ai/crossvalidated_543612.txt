[site]: crossvalidated
[post_id]: 543612
[parent_id]: 
[tags]: 
What is purpose of multihead in transformers

Why in the attention mechanism do they apply multihead? It's been said that it would lift the amplitude of vector dot production from word that at the moment is analysed, which can be correct, but dividing the embedding vector to 8 arbitrary vectors may cause the next most important word also to be the same head as the word we are analysing right now not to shine properly again. It's better than previous procedure but still.... There may be some misunderstandings by me if so correct me please. And what are other reasons to use multiheads.
