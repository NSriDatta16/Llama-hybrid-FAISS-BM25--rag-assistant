[site]: datascience
[post_id]: 36140
[parent_id]: 
[tags]: 
Deep RL: Proximal policy optimization gradient calculation

Case: Continuous action domain with 4 outputs (control problem) Policy and Value function approximation with fully connected neural networks I understand that the loss function for PPO for the overall policy network is the clipped-loss function plus some stuff [Schulman, 2017]. $L = E[L_{clip}(\theta) + c_1*L_{VF}(\theta) + c_2*S[\pi_{\theta}(s_t)]]$ $g_{common} = E[\nabla_{\theta} log(\pi_{theta}(s|a)A]$ My questions are thus: What is the gradient estimation of loss function with respect to each predicted output? A common gradient estimation is shown above, but it is for a different objective function. The language in the paper is that is a surrogate objective function. How is loss/gradient calculated for non-output-level neural network weights given the clipped-loss function? Can the inner neurons use something like neg_log_policy with respect to target neuron output or mean-square-error or something else? I'm not sure because loss function and similar use Advantage function and reward .
