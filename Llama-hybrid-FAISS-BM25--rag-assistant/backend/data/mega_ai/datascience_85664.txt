[site]: datascience
[post_id]: 85664
[parent_id]: 74503
[tags]: 
There is a slight problem with the semantics that you have used. RNN is a 'recurring' neural net. LSTM is a type of RNN. So when you say " number of LSTMs in the encoder would have to be the same as number of words in the text ", I suppose what you mean to say is that the number of time-steps or number of NN's in the (single) LSTM layer is the same as the number of words in the sentence. If this is what you actually meant, then you are correct. For e.g. If you have a 20 word tweet and a set of 10000 such tweets. You can use a single LSTM layer with num of timesteps=20 to process this. You do NOT use 20 LSTM layers. This should answer your second question also. The decoder too does not have 20 LSTMs. It has probably 1 LSTM layer with 20 time-steps. The decoder output length need not match exactly with the encoder output length. In other words it need not be a word for word translation. The models typically use BOS and EOS markers to start and stop processing.
