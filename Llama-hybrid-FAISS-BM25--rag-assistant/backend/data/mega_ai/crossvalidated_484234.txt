[site]: crossvalidated
[post_id]: 484234
[parent_id]: 
[tags]: 
Model size, expressivity and overfitting - confusion about a statement in Goodfellow et al

In Goodfellow et al. book Deep Learning chapter 12.1.4 they write These large models learn some function f(x), but do so using many more parameters than are necessary for the task. Their size is necessary only due to the limited number of training examples. I am not able to understand this. Large models are expressive but if you train them on few examples they should also overfit. So what do the authors mean by saying large models are necessary precisely because of limited number of training examples. This seems to go against the spirit of using more bias when training data is limited.
