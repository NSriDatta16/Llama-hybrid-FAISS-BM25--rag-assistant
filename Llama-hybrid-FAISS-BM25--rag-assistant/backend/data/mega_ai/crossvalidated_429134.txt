[site]: crossvalidated
[post_id]: 429134
[parent_id]: 
[tags]: 
Hypothesis Space for Machine Learning

I have the following question on a problem set: Considering only linear combinations of monomials $x^k$ for $k=0,...,K$ , describe a good hypothesis space to approximate a continuous function $f(x)$ defined on $(-1,1)$ under each of the following constraints: a. No constraints: $f(x)$ is arbitrary b. $f$ is even: $f(-x) = f(x)$ c. $f$ is odd: $f(-x) = -f(x)$ d. $f(-1) = 0$ and $f(1) = 0$ e. $f(0) = 1$ I know that we want the hypothesis space to: 1) be reasonably small and 2) contain the true hypothesis in order to ensure convergence. So, for part (b) I assume we want the hypothesis space to consist of even monomials (i.e., $x^2,x^4,...$ ), while for part (c) I assume we want odd monomials (i.e., $x^3,x^5,...$ ). I'm not sure about the others though. How should we think about those?
