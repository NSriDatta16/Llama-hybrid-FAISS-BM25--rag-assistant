[site]: crossvalidated
[post_id]: 439824
[parent_id]: 
[tags]: 
Best way to remove multicollinearity and feature selection for binary classification problem?

I am having around 1200 features 20k observations. Objective is to get the not highly correlated best 100-130 features to build binary classification models such as LR, hypertuned ML trees etc. Skipping the traditional procedure- Weight of Evidence (WOE), VARCLUSS from SAS and sorting based on IV as my intention is to use actual values of features and binned WOE: Detail here Currently, as Step 1: I am traversing across correlation matrix to find a highly correlated variable (threshold .75) in the column of the matrix. Then rank the correlated variable based on the relation with the target(correlation coefficient) and retain the top one rejecting others. Pearson correlation is used. (For Dichotomous, Pearson and point biserial are same). Step 2: After removing collinearity I am developing a quick and simple Random Forest and then further select the variables by ordering descending feature importance. I have 4 questions: How to make step 1 more efficient? Can I use AUC or some other metrics to chose the best among highly correlated variables? In total any other efficient and optimized approach for treating multicollinearity? RF does not go well with collinear variables as the variables share the importance ? Is L1 regularised Logistic regression as suggested in feature_selection.SelectFromModel goes well will multicorrelated features? Link1 Is there any function in python similar to Proc Varreduce in SAS? Detail on Varreduce Thank you for your time. I appreciate your help.
