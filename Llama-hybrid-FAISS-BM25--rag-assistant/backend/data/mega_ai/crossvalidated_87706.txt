[site]: crossvalidated
[post_id]: 87706
[parent_id]: 87695
[tags]: 
The basic issue here is true and fairly well known in statistics. However, his interpretation / claim is extreme. There are several issues to be discussed: First, power doesn't change very fast with changes in $N$. (Specifically, it changes as a function of $\sqrt N$, so to halve the standard deviation of your sampling distribution, you need to quadruple your $N$, etc.) However, power is quite sensitive to effect size. Moreover, unless your estimated power is $50\%$, the change in power with a change in effect size isn't symmetrical. If you are trying for $80\%$ power, power will decrease more rapidly with a decrease in Cohen's $d$ than it will increase with an equivalent increase in Cohen's $d$. For example, when starting from $d = .5$ with $N = 128$, if you had 20 fewer observations, power would drop by $\approx 7.9\%$, but if you had 20 more observation, power would increase by $\approx 5.5\%$. On the other hand, if the true effect size were $.1$ lower, then power would be $\approx 16.9\%$ lower, but if it were $.1$ higher, it would only be $\approx 12.6\%$ higher. This asymmetry, and the differing sensitivity, can be seen in the figures below. If you are working from effects estimated from prior research, say a meta-analysis or a pilot study, the solution to this is to incorporate your uncertainty about the true effect size into your power calculation. Ideally, this would involve integrating over the entire distribution of possible effect sizes. This is probably a bridge too far for most applications, but a quick and dirty strategy is to calculate the power at several possible effect sizes, your estimated Cohen's $d$ plus or minus 1 and 2 standard deviations, and then get a weighted average using the probability densities of those quantiles as the weights. If you are conducting a study of something that has never been studied before, this doesn't really matter. You know what effect size you care about. In reality, the effect is either that large (or larger), or it is smaller (even possibly 0). Using the effect size you care about in your power analysis will be valid, and will give you an appropriate test of your hypothesis. If the effect size that you care about is the true value, you will have (say) an $80\%$ chance of 'significance'. If, due to sampling error, the realized effect size in your study is smaller (larger) your result will be less (more) significant, or even non-significant. That is the way it is supposed to work. Second, regarding the broader claim that power analyses (a-priori or otherwise) rely on assumptions, it is not clear what to make of that argument. Of course they do. So does everything else. Not running a power analysis, but just gathering an amount of data based on a number you picked out of a hat, and then analyzing your data, will not improve the situation. Moreover, your resulting analyses will still rely on assumptions, just as all analyses (power or otherwise) always do. If instead you decide that you will continue to collect data and re-analyze them until you get a picture you like or get tired of it, that will be much less valid (and will still entail assumptions that may be invisible to the speaker, but that exist nonetheless). Put simply, there is no way around the fact that assumptions are being made in research and data analysis . You may find these resources of interest: Kraemer, H.C., Mintz, J., Noda, A., Tinklenberg, J., & Yesavage, J.A. (2006). Caution regarding the use of pilot studies to guide power calculations for study proposals , Archives of General Psychiatry, 63 , 5, pp. 484-489. Uebersax, J.A. (2007). Bayesian Unconditional Power Analysis. http://www.john-uebersax.com/stat/bpower.htm
