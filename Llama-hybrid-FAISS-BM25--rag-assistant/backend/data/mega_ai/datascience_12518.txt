[site]: datascience
[post_id]: 12518
[parent_id]: 
[tags]: 
Two ways of optimize the same function?

I'm actually reading this tutorial about deepLearning and in particular about Logistic Regression. I don't get why it first says to optimize logistic regression taking the max Probability and after using the Log loss function ? Sorry you can explain me the point where him will use the Argmax and where Will use the Loss ? you would not need only 1 of this 2 ?
