peratorname {E} _{x\sim \mu _{\text{ref}},T\sim \mu _{\text{trans}}}[\ln D(T(x))]-\operatorname {E} _{x\sim \mu _{G},T\sim \mu _{\text{trans}}}[\ln(1-D(T(x)))]\\\min _{G}L_{G}(D,\mu _{G})=-\operatorname {E} _{x\sim \mu _{G},T\sim \mu _{\text{trans}}}[\ln(1-D(T(x)))]\end{cases}}} The authors demonstrated high-quality generation using just 100-picture-large datasets. The StyleGAN-2-ADA paper points out a further point on data augmentation: it must be invertible. Continue with the example of generating ImageNet pictures. If the data augmentation is "randomly rotate the picture by 0, 90, 180, 270 degrees with equal probability", then there is no way for the generator to know which is the true orientation: Consider two generators G , G ′ {\displaystyle G,G'} , such that for any latent z {\displaystyle z} , the generated image G ( z ) {\displaystyle G(z)} is a 90-degree rotation of G ′ ( z ) {\displaystyle G'(z)} . They would have exactly the same expected loss, and so neither is preferred over the other. The solution is to only use invertible data augmentation: instead of "randomly rotate the picture by 0, 90, 180, 270 degrees with equal probability", use "randomly rotate the picture by 90, 180, 270 degrees with 0.1 probability, and keep the picture as it is with 0.7 probability". This way, the generator is still rewarded to keep images oriented the same way as un-augmented ImageNet pictures. Abstractly, the effect of randomly sampling transformations T : Ω → Ω {\displaystyle T:\Omega \to \Omega } from the distribution μ trans {\displaystyle \mu _{\text{trans}}} is to define a Markov kernel K trans : Ω → P ( Ω ) {\displaystyle K_{\text{trans}}:\Omega \to {\mathcal {P}}(\Omega )} . Then, the data-augmented GAN game pushes the generator to find some μ ^ G ∈ P ( Ω ) {\displaystyle {\hat {\mu }}_{G}\in {\mathcal {P}}(\Omega )} , such that K trans ∗ μ ref = K trans ∗ μ ^ G {\displaystyle K_{\text{trans}}*\mu _{\text{ref}}=K_{\text{trans}}*{\hat {\mu }}_{G}} where ∗ {\displaystyle *} is the Markov kernel convolution. A data-augmentation method is defined to be invertible if its Markov kernel K trans {\displaystyle K_{\text{trans}}} satisfies K trans ∗ μ = K trans ∗ μ ′ ⟹ μ = μ ′ ∀ μ , μ ′ ∈ P ( Ω ) {\displaystyle K_{\text{trans}}*\mu =K_{\text{trans}}*\mu '\implies \mu =\mu '\quad \forall \mu ,\mu '\in {\mathcal {P}}(\Omega )} Immediately by definition, we see that composing multiple invertible data-augmentation methods results in yet another invertible method. Also by definition, if the data-augmentation method is invertible, then using it in a GAN game does not change the optimal strategy μ ^ G {\displaystyle {\hat {\mu }}_{G}} for the generator, which is still μ ref {\displaystyle \mu _{\text{ref}}} . There are two prototypical examples of invertible Markov kernels: Discrete case: Invertible stochastic matrices, when Ω {\displaystyle \Omega } is finite. For example, if Ω = { ↑ , ↓ , ← , → } {\displaystyle \Omega =\{\uparrow ,\downarrow ,\leftarrow ,\rightarrow \}} is the set of four images of an arrow, pointing in 4 directions, and the data augmentation is "randomly rotate the picture by 90, 180, 270 degrees with probability p {\displaystyle p} , and keep the picture as it is with probability ( 1 − 3 p ) {\displaystyle (1-3p)} ", then the Markov kernel K trans {\displaystyle K_{\text{trans}}} can be represented as a stochastic matrix: [ K trans ] = [ ( 1 − 3 p ) p p p p ( 1 − 3 p ) p p p p ( 1 − 3 p ) p p p p ( 1 − 3 p ) ] {\displaystyle [K_{\text{trans}}]={\begin{bmatrix}(1-3p)&p&p&p\\p&(1-3p)&p&p\\p&p&(1-3p)&p\\p&p&p&(1-3p)\end{bmatrix}}} and K trans {\displaystyle K_{\text{trans}}} is an invertible kernel iff [ K trans ] {\displaystyle [K_{\text{trans}}]} is an invertible matrix, that is, p ≠ 1 / 4 {\displaystyle p\neq 1/4} . Continuous case: The gaussian kernel, when Ω = R n {\displaystyle \Omega =\mathbb {R} ^{n}} for some n ≥ 1 {\displaystyle n\geq 1} . For example, if Ω = R 256 2 {\displaystyle \Omega =\mathbb {R} ^{256^{2}}} i