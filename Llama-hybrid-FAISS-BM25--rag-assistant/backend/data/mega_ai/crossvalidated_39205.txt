[site]: crossvalidated
[post_id]: 39205
[parent_id]: 39129
[tags]: 
I think the problem you noted with method (1) is more subtle: I would expect that people would stop visiting a restaurant after they have been exposed to a rude waiter, so a sequence (0,0,1) is plausible, but (1,0,0) is arguably less plausible (1 is for the rude waiter). If that is the case, this would prevent the mis-alignment bias to cancel out, per your second question, as not all permutations and subsamples of the customer history should be considered as equally likely. Method (3) is essentially a population-based case-control study, in which your sampling design depends on the outcome. I don't know how much sampling/survey statistics background you have, but try taking a look at Scott (2006) Waksberg lecture . It is certainly not terribly wrong from a theoretical perspective, but you need to know how to analyze this kind of stuff properly. Note that Alastair Scott talks about a logistic regression in which the dependent variable is the outcome you sampled upon; in your application, this would mean modeling the probability that in a given transaction, a waiter will be rude. Your research question is different, though, so most of what the case-control outcome-dependent sampling would do is to produce a substantial variability in pweights. On your third question, there's been some work on inverse sampling ( Rao, Scott and Benhin 2003 ) and balanced sampling ( Tille 2011 ) as possible ways to reduce the complexity of the problem while retaining some other desirable properties.
