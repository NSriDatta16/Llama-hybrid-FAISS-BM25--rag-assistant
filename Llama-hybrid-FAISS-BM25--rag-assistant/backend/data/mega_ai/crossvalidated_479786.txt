[site]: crossvalidated
[post_id]: 479786
[parent_id]: 479781
[tags]: 
I find it helpful to think about logistic regression as a special case of Generalized Linear Models (GLM). In GLMs, we assume that the conditional distribution of the response belongs to the exponential family of distributions. (I say the "conditional" distribution here because the response follows this distribution only for some fixed value of the independent variables and the parameters). We then further assume that the expected value of the conditional distribution of the response (or the conditional expected value) is given as a function of a linear combination of the independent variables, $h(x^T\theta)$ . The inverse of this function, $h^{-1}$ , is called the link function. In logistic regression, we assume that the response $Y$ is conditionally distributed following a Bernoulli distribution (a binomial distribution with $n=1$ ), where the parameter $p$ (the "success rate") may depend on $x$ and $\theta$ , thus $$Y|x;\theta \sim Bern(p(x,\theta)).$$ We now wish to choose $p(x,\theta)$ in such a way that the conditional expected value of $y$ is equal to $h(x^T\theta)$ , $$E(Y|x,\theta) = h(x^T\theta).$$ Conveniently, in the Bernoulli distribution we have that the expected value is just equal to $p$ , which is equal to the probability that $Y = 1$ . Therefore, $$E(Y|x,\theta) = P(Y=1|x;\theta)= h(x^T\theta).$$ If we define $h$ to be the standard logistic function, $$h(x^T\theta)=\frac{1}{1+e^{-x^T\theta}},$$ your case follows.
