[site]: crossvalidated
[post_id]: 86541
[parent_id]: 86533
[tags]: 
The only thing that came to mind was using $g(x)=1$ No better distributions come to mind that are easy to sample from. Is this a good option? "Good" is relative. Simulation often requires large samples, so you may have to balance ease of calculation and rejection rate. It's a good option if your rejection rate's not too high (I wouldn't use it with $\pi(x)=100x^{99}$, for example). As for better choices, it depends on what you know: are you familiar with the Probability Integral Transform approach to generating random numbers? If so, generating from $g(x)=\alpha x^{\alpha-1}$ might be fast enough (if you can compute roots reasonably quickly) and likely to work well (in terms of a good $c$) for suitable $\alpha$. do you know any way to generate from a triangular distribution ($\pi(x) = kx$, say)? Then you might be slightly better off using that (there's a way to organize it that you can get two triangular values for every two uniforms you generate, for example). It also depends on whether you require independent values, or whether say some negative dependence is acceptable (or even, in some cases, desirable). That said, the uniform will work reasonably well for this example, and I wouldn't ignore it simply because there are 'closer' $g$'s. Depending on the relative speed of various computations on your machine it may actually be faster to just use the uniform. Also two conceptual things - what makes a distribution easy to sample from? Knowing a method by which to sample from it. There are so many sampling algorithms that it's really just limited to what you know, and some particulars of what the sampling situation is (not just $\pi$, but for example whether you are sampling many values at once, or one at a time, whether this is something you - or someone else - will be using again and again, or just on this one thing, and much else besides). [As an illustration of the various ways in which something might be 'easy', one can, for example, construct several different kinds of adaptive forms of rejection sampling that construct simple upper and lower envelopes ("g"-type bounds) on $\pi$ on the fly , in such a way that if you sample a lot, your average number of evaluations of $\pi$ per random variate obtained is quite small. This can lead to very efficient ways to sample from $\pi$s that are very difficult/expensive to evaluate - as long as you need large enough samples to build up good envelopes.] Why do we want to pick c as small as possible. Why not use c=50? Is this for algorithm efficiency? i.e. more of the randomly drawn samples from cg(x) will fall under the curve of Ï€(x)? Yes, for a given $g$, the larger $c$ is the more often you need to sample from $g$ to get one value from $\pi$ (in fact, $c$ times on average). More generally, even with different competing $g$'s, small $c$ is still desirable, but there's a tradeoff between how fast it is to compute a 'better' $g$ (one with a smaller $c$) and how fast it is to sample from an 'easy' $g$. A uniform $g$ is the easiest of all, of course. All things considered, I think your idea of trying the uniform is likely to be reasonable - an acceptance rate of 1/3.75 isn't high but it's not too bad and the calculations will be both fast and simple.
