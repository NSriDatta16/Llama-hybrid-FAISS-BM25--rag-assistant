[site]: crossvalidated
[post_id]: 209411
[parent_id]: 160479
[tags]: 
If you can write a function to to grid search, it's probably even easier to write a function to do random search because you don't have to pre-specify and store the grid up front. Setting that aside, methods like LIPO, particle swarm optimization and Bayesian optimization make intelligent choices about which hyperparameters are likely to be better, so if you need to keep the number of models fit to an absolute minimum (say, because it's expensive to fit a model), these tools are promising options. They're also global optimizers, so they have a high probability of locating the global maximum. Some of the acquisition functions of BO methods have provable regret bounds, which make them even more attractive. More information can be found in these questions: What are some of the disavantage of bayesian hyper parameter optimization? Optimization when Cost Function Slow to Evaluate
