[site]: crossvalidated
[post_id]: 549155
[parent_id]: 
[tags]: 
Derivations of Bayesian Risk Classifier when Posteriori Probability is Unknown

I found two expressions for a Bayesian risk classifier when the posteriori probability is unknown, but I don't understand how and why the derivations were made. For this scenario, assume: $X\in\mathbb{X}=[0,1],Y\in\{ 0,1 \}$ $\pi_y=P(Y=y)=1/2$ for $y\in{0,1}$ Conditional distributions $[X|Y=y]$ characterised by: $f(x|Y=0)=2-2x$ and $f(x|Y=1)=2x$ . Let $\tau_1$ be the posteriori probability and $L(r*)$ be the risk classifier. In the first case, assume $\tau_1\in[0,1]$ is unknown, thus the following expression can be written: $L(r^*)=\int_Xmin\{(1-\tau_1)f(x|Y=0),\tau_1f(x|Y=1\}dx$ I'd like to understand this expression and get an intuitive understanding why it is true? Additionally, if $\tau_0=\tau_1=1/2$ , the following expression can be derived: $L(r*)=1/2-1/4\int_X|f(x|Y=1)-f(x|Y=0)|dx$ How is this connected to the above statement and why is it true?
