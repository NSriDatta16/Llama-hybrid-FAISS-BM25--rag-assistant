[site]: crossvalidated
[post_id]: 543589
[parent_id]: 541483
[tags]: 
1.) We do not know much about your data. We only see feature amount and row size. Keep in mind that data cleaning and preprocessing can vastly impact all of your prediction results. 2.) David is right to the extent that 13 predictors with 75 observations is a little bit gruesome. So Lasso is an obvious preprocessing step. The other question is, if you are only interested in prediction or inference? If you are interested in prediction , than I would specifically look at my last hint. If you are interested in inference , than you have to keep in mind that multicollinearity will distort your feature/permutation importance of your predictiors. So in this case Lasso is extremely recommend, since it will limit the amount of model solutions due to random seeds. And this can limit the order or impact of feature/permutation importance greatly, which is good. A feature importance selection as you did can be already distorted by multicollinearity, I would first do a regularization method. 3.) Getting the feature importance as you did, is ok (even despite not using reg. methods) . Since it will limit the complexity of the model. You have less features, but a little bit less prediction quality. This is not unnormal. This is still in the range of possible outcomes, you are not using every trash (sometimes noise) in he prediction. Your models can still capture a lot of the variance although you are having only 4 predictors instead of 13. So this step is ok, but: 4.) What about your quality insights from confusion matrix. We can not judge if a model you develop should have e.g. high precision and/or recall. Or to to fromulate it more practical: is it ok to have more false positives than false negatives?. You know what I mean 5.) Looking at your classifiers, I would recommend trying something from the gradient boosting family. Most of the time they outperform your methods at least in terms of prediction. These ML methods are: https://rdrr.io/cran/mboost/man/glmboost.html glmboost https://rdrr.io/cran/mboost/man/gamboost.html gamboost (generalized additive models, which have a high explainability) https://interpret.ml/docs/ebm.html Explainable Boosting Machines (EBM), there is an R implementation on github. I dunno how far they are. But the EBM is an advanced implementation of a GA2M (Generalized Additive Model with interaction effects) while the learning will be done with gradient boosting technqiues. You can see how the EBM outperforms on ROC/AUC several other boosting techniques: Paper: https://arxiv.org/abs/1909.09223 However, none of these methods can deal with multicollinearity when it comes to inference. The models are only immune to multicoll. when it comes to prediction: https://datascience.stackexchange.com/questions/12554/does-xgboost-handle-multicollinearity-by-itself If you only care for prediction and you do lasso/ filter out already the best predictors. Than you are on the right track.
