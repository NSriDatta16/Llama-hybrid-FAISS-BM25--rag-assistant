[site]: datascience
[post_id]: 85864
[parent_id]: 85847
[tags]: 
Further details regarding your last question. Your problem is a sequence classification exercise. Decoders aren't needed. You can use a Dense layer to predict the labels. You can even add 'Attention'-like features by using all hidden state outputs form the encoder instead of the last one while predicting the labels See for e.g.: https://stackoverflow.com/questions/63060083/create-an-lstm-layer-with-attention-in-keras-for-multi-label-text-classification/64853996#64853996
