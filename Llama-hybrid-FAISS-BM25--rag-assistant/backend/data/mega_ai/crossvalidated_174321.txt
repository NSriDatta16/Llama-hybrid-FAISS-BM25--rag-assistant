[site]: crossvalidated
[post_id]: 174321
[parent_id]: 
[tags]: 
What to do when LDA over-fits

I'm looking for guidance on ways to improve my test set prediction accuracy when using Linear Discriminant Analysis (LDA). I have a matrix of ~10K rows x 24K columns. Of the 24K features, 4 represent Open, High, Low, Close (OHLC) prices, ~240 represent price movement indicators generated from the OHLC data including % diffs calculated for the past 1-5 days, and the remaining features are generated using scikit-learn's PolyFeatures(2) (with no bias). I've removed linear combinations of features from the dataset, and then scaled and randomly split it into train and test datasets and finally passed the training set into scikit-learn's Linear Discriminant Analysis (LDA) fit function. LDA gives ~95% accuracy on predicting a uniformly distributed 8 class outcome. However, when applying that fitted model to the test set, the accuracy drops to ~20%. So, obviously I'm over-fitting. However, this is the first time after 1,000's of attempts using various approaches where the accuracy on the training data was over 25%. So naturally I'm highly encouraged by the results using LDA and think this path is worth pursuing. I originally used LDA(n_components=2) and got good separation results on 3 of the 8 classes. Then I tried n_components=3 and got this... It's hard to see without being able to rotate the image yourself, but all but the red and yellow had really good separation. So, I interpreted this to mean LDA was doing it's job. However, when applying that LDA model to the test set, I get the following result, where there is almost no separation of features... I then ran a feature reduction process to find a subset of features that improves the test set accuracy, but the best I can get after choosing the best 1900 features is an accuracy of 30% on the test set. A nice improvement, but still dramatically over-fitting the training set. For predicting the outcome, I've tried several algorithms including LDA by itself, as well as LDA + one of the following: RandomForest, LogisticRegression, SGDClassfier, BeroulliNB, DecisionTreeClassifier, XGBClassifier. So, is there a way to improve my prediction and/or the separation of features on the test set using LDA. Or are there other, more appropriate algorithms like LDA e.g. Regularized LDA (rLDA) that would be better suited to my problem? Thanks in advance for any help you can give! Cheers, Eric
