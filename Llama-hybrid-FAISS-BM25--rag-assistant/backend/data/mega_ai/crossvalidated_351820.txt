[site]: crossvalidated
[post_id]: 351820
[parent_id]: 
[tags]: 
Gaussian Process instability with more datapoints

I'm working my way through Rasmussen and Williams' classical work Gaussian Process for Machine Learning, and attempting to implement a lot of their theory in Python. I've attempted to fit a sin(x) function using a GP Regressor using a simple RBF kernel: class Kernel: @staticmethod def get(x_1, x_2): return np.exp(-0.5 * np.subtract.outer(x_1, x_2) ** 2) I intentionally disregarded the hyperparameters within the RBF for the sake of simplicity when first implementing this. I use np.random.choice to randomly select datapoints from X_sin and y_sin to progressively feed into my GPR. Issue #1: Predictions Asymptotically Approach Positive/Negative Infinity The GPR itself works relatively well with only a few (6) data points. The orange dots show the actual sin function data points (plus a small amount of noise), and the red data points show sin function data points that have been exposed to the GPR (observed). However, I'll frequently get fits that look like this with more data points: Is this an issue of hyperparameter tuning with my RBF kernel? How can I improve the stability of my predictions so that they don't fly off to positive / negative infinity at the edges? Issue #2: Singular Matrices for K(X,X) Moreover, I'm also finding that K(X,X)- the covariance matrix of observed training data is often singular, and therefore not invertible: . I am currently not adding $σ^2I$ to my kernel, as the equation states, so my implementation is more like $K(X,X)^{-1}$ instead of $[K(X,X) + σ^{2}I]^{-1}$. Is the solution to my singular matrix issue as simple as simply adding a small amount of noise to this covariance matrix, or is there a more appropriate solution? Rasmussen and Williams actually suggest another method in Equation 3.26: \begin{equation} B = I + W^{\frac{1}{2}}KW^{\frac{1}{2}} \end{equation} $B$ is supposed to be guaranteed to be well-conditioned for lots of covariance functions, but I have no clue what $W$ is supposed to be here. In fact, the authors say themselves Sometimes it is suggested that it can be useful to replace $K$ by $K + Iε$ where is a small constant, to improve the numerical conditioning10 of K. However, by taking care with the implementation details as above this should not be necessary.
