[site]: crossvalidated
[post_id]: 618205
[parent_id]: 618179
[tags]: 
This issue is indeed a bit of a problem in time series forecasting. (And more generally in prediction, if your test sample can be suspected of differing systematically from the training and validation samples.) I would make two points here. First, whether the most recent data is really "most influential" is very much open to debate, and will depend heavily on your use case. If you are forecasting demand for a new product, yes. (But then you would probably be using specialized models, like the Bass and cross-train them on other products - not choose the model based on a holdout set of the focal time series.) But when my forecast consumers ask me to "put more emphasis on recent observations" or similar, I always push back unless they can provide an actual argument for why the data generating process should have evolved or changed recently. (This prior of mine may reflect that I am working in a very mature industry.) Second, if there are actual reasons to suppose the DGP has changed, you should indeed treat the time series differently, and not rely on a holdout validation sample. For instance, you might use specialized models, like the Bass mentioned above. Or you might only use the most recent data, consider this a short time series and use an appropriate method . Or take one model fitted to the entire series and another one fitted only to the last observations and take the average of the two forecasts. Bottom line: you really need to think about the time series you are forecasting. (Or trust in an automatic system and live with potentially lower accuracy - that may well be a rational use of your time.)
