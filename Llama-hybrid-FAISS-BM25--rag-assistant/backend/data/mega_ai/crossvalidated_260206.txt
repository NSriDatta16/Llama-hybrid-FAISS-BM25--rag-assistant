[site]: crossvalidated
[post_id]: 260206
[parent_id]: 
[tags]: 
importance of mean and standard deviation in feature scaling in machine learning

I have just started ML. I have come across feature scaling using stochastic Gradient Descent algorithm that uses 'standardization' for optimal performance. I am not getting what is it. It is calculating mean of the feature vector U and then standard deviation 'sigma'. x' = ( x - U)/sigma where x and x' are sample vectors. after applying this formula on a copy of iris-dataset X I am getting the following result: and the print of the dataset itself is here below: the first pcture is for X_std = np.copy(X) and the second is of X itself. Please tell me how this difference is going to help in improving the performance of classifiers and also the use of mean and standard deviation over the sample feature vector. here is the code: from sklearn import datasets import numpy as np from sklearn.model_selection import train_test_split iris = datasets.load_iris() #print(iris) X = iris.data[:,[2,3]] # petal_length, petal_width y = iris.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) X_std = np.copy(X) X_std[:,0] = (X[:,0] - X[:,0].mean())/X[:,0].std() print(X_std[:10]) #y = np.unique(iris['target']) print(X[:10]) #print(y)
