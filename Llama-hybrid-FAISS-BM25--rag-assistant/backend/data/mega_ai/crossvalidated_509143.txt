[site]: crossvalidated
[post_id]: 509143
[parent_id]: 
[tags]: 
Train and Validation vs. Train, Test, and Validation

I am embarking on a new job that will give me the opportunity to do some cool machine learning stuff. I haven't touched this stuff on a deeper level since graduate school and I wanted to get some clarification on some concepts. The way I was taught ML is that you split up your data (80/20) into training and validation datasets. You fit your model to the 80% training split and get an error rate, loss, etc. through cross validation. Then, you take the fitted model you constructed with the training data and pop in the 20% validation dataset to compare if the error rates, loss, etc are similar. If so, the model is good. I have been doing some research to refresh my knowledge, and I've been noticing 3-way splits now (training/test/validation) where the split is usually (70/20/10). I'm so confused on how this 3-way split is different from the 2-way split I was taught in school. Also, I'm pretty sure I've been interchanging test with validation when referring to the 2-way split methodology. Can someone verify if my understanding of the 2-way split is correct and explain the difference between that and the 3-way split? Thank you!
