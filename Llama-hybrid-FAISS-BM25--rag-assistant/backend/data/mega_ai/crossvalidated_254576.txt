[site]: crossvalidated
[post_id]: 254576
[parent_id]: 254377
[tags]: 
If you want to employ a reinforcement learning algorithm on this problem you'd probably encode the features as state, the weights as actions and the classification loss as reward. The actions should be continuous so a policy search would be better. Take a look at policy gradient as well as gradient free methods such as RL with CMA-ES or Trust region policy optimization. Approaches that are more like black-box optimization are likely better because you don't really have a value function. Which brings me to the second point: reinforcement learning is likely to general as a framework for your problem. Your actions don't influence your state and your reward is observed immediately. Furthermore, you can fully observe the classification loss for all actions in a given state. I'm not an expert on the subject but to me it sounds like contextual online optimization. Take a look at the literature in that field as well. Given the nature of the online optimization community, they'll likely have strong theoretical guarantees if you can find an algorithm that applies. I have only skimmed it very briefly, but [1] seems like a good read. (Context-FTPL in particular). However, the online optimization literature is fairly dense. It also sounds like someone might have done something like this before but I'm not aware of it. [1] Syrgkanis, Vasilis, Akshay Krishnamurthy, and Robert E. Schapire. "Efficient Algorithms for Adversarial Contextual Learning." arXiv preprint arXiv:1602.02454 (2016).
