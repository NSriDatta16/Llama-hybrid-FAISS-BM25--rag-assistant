[site]: crossvalidated
[post_id]: 620897
[parent_id]: 620879
[tags]: 
There are no formal tests to determine if A/B test assignment to the various buckets was "truly random". What we can test is that we have achieved a balanced distribution against (most of) the key covariates behind our samples. That is for example we can use $t$ - or $\chi^2$ -tests to check if continuous or categorical variables $X$ respectively have similar distributional characteristics to a certain extent. (e.g. the means of two groups being different in a statistically significant manner) Note here that there might be a problem with multiple comparisons that we need to account for if we take this path. The above being said, covariate balance in terms of feature $X$ sample is not why A/B tests work. Ultimately, A/B tests allow us to assume that: $(Y_i^A, Y_i^B) \perp D_i|X_i$ , i.e. that whether the $i$ -th individual is treated ( $B$ ) and untreated ( $A$ ) is exchangeable in the sense that the assignment of treatment ( $D$ ) depends only on the measured covariates ( $X$ ) - this is known as the ignorability . As such, the "no-unmeasured confounders" assumption (NUCA) holds and enables us to assume that any unmeasured confounders have cancelled each other out. Bringing this all together now, a more aggressive way to check if the A/B test assignment was "truly random" would be to build a classifier $C$ that uses the features $X$ of our sample (e.g. user age, location, etc.) to predict if someone was assigned to the $A$ or the $B$ arm. If the classifier $C$ 's performance is very bad (e.g. AUC-ROC $C$ cannot make any meaningful distinctions between the two samples, and therefore we can say that our randomisation worked! This is very similar to what we do in propensity score analysis but without aiming to use $C$ 's probabilistic scores further. (As for how to construct $C$ : for starters, something simple (e.g. a random forest) should be good enough.)
