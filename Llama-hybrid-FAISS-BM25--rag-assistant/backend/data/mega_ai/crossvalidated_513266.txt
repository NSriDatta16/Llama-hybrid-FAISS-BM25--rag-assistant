[site]: crossvalidated
[post_id]: 513266
[parent_id]: 512678
[tags]: 
I think that @Nat's answer is good, but understates the importance of the case where $P(\mbox{absence}) \approx P(\mbox{absence} | \mbox{no evidence})$ (or not-quite-but-almost-equivalently $P(\text{no evidence}|\text{absence}) \approx P(\text{no evidence})$ ). The big problem here is that, as a general rule, it is not known to the experimenter a priori if the above equality holds, and it is therefore not safe in the general case to assume that absence of evidence is evidence of absence. Taken in context, it may be possible to interpret absence of evidence as evidence of absence, but one does not necessarily follow from the other, and one should be very very careful when attempting to interpret absence of evidence as evidence of absence. One scenario where this happens is when evidence is hard to gather for some reason or another . For example, suppose that we would like to study the difference in educational outcomes between textbook A and textbook B. We give textbook A to class A and textbook B to class B, and compare their grades afterward. Well, unfortunately, class A and class B are potentially taught by the same instructor, who has opinions on the textbooks which ultimately affect the outcome of the experiment, xor class A and B are taught by different instructors which affects the outcome of the experiment. Also, two dozen other confounding factors with varying degrees of impact on the outcome of the experiment. Ultimately, the data show nothing because of these things, and there is an absence of evidence differentiating the outcomes from textbook A and B. It's not that we didn't look, and although maybe the data were easy to gather, any potential evidence was difficult to gather. I.e. $P(\text{no evidence}) \approx P(\text{no evidence} | \text{absence}) \approx 1$ . In the Bayesian context, if evidence is impossible, then one cannot update one's prior. In the real world, if you're studying gravity in a vacuum, then you maybe don't have to worry about this, but if you're studying just about anything with human subjects, then it is sometimes difficult to know ahead of time whether your experiment even can yield good evidence. In any case, if researchers do an experiment and fail to find an effect, you should seriously contemplate $P(\text{no evidence})$ in the context of the experiment before you decide to interpret an absence of evidence as evidence of absence. As another analogy, you might consider a criminal who covers his tracks well: the detective fails to find evidence not because she didn't look for it, but because the evidence was hard to find. Yet another scenario where this happens very frequently (and somewhat surprisingly) is when $P(\text{absence}) \approx 0$ . Using the same example above, consider testing the hypothesis that $H_1 :=$ textbook A gives different educational outcomes by some chosen metric than textbook B (which is an incredibly common sort of hypothesis to test). The opposite (absence) is that textbook A and textbook B have identical outcomes, which is, for most priors, impossible (it's a single point!). Since $P(\text{absence}) \approx P(\text{absence} | \text{no evidence}) \approx 0$ , then when our study fails to find an effect, we should be very careful in deciding that this presents evidence of a lack of an effect. Of course, if you're using Bayesian techniques to study the problem, then you won't "fail to find an effect", but in a post hoc Bayesian analysis of the experiment that was run (since many scientists don't do the Bayesian thing), we can't interpret absence of evidence as evidence of absence. So, given an arbitrary study that fails to find an effect, you should not interpret this as strong evidence for a lack of an effect. Without careful consideration of the context of the experiment, you shouldn't even consider it weak evidence for a lack of an effect. I think that this is the essence of the adage: "absence of evidence is not [necessarily] evidence of absence".
