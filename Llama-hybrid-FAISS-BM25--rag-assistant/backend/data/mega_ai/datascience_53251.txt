[site]: datascience
[post_id]: 53251
[parent_id]: 52039
[tags]: 
The attention weights themselves do not carry any information about the encoded sentence. It only tells you something like: the $n$ -th and $m$ -th word carry important information for generating the next word, but not what the words actually are. Moreover, it is a variable-length vector (of the same length as the input size), it would be hard to do anything with it in the decoder.
