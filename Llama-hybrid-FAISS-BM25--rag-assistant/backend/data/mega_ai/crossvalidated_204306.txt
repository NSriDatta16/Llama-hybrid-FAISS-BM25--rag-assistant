[site]: crossvalidated
[post_id]: 204306
[parent_id]: 
[tags]: 
Trend and Breakout detection in time series

I am working on several types of system metrics which characterizes several components of an application. The metrics range from system metrics like cpu.utilization to network metrics and database metrics like bytes.out/bytes.in and response-time for apache and haproxy. The assumption of a normal distribution doesn't seem to hold for these metrics because of dependence on the load the distribution skewed for almost constant load. Also, seasonality too might be present in few of the metrics. The objective is to find out if there is a change in the trend in long term or if there was a breakout in the time series of these metrics at a given instant in real time . What are the best approaches to come up with a generic breakout system for detection or do we need different approaches depending on nature of these metrics? For breakout detection I am thinking of using t-test to check if some current window has significant change in means compared to some previous window or long term value of mean. Any guidance on the approaches will be very helpful. Update: Adding link to a few data sets. mysql.bytes_received haproxy.requests
