[site]: crossvalidated
[post_id]: 193023
[parent_id]: 612
[tags]: 
This answer is to present, in a path chart form, things about which @amoeba reasoned in his deep (but slightly complicated) answer on this thread (I'm a kind of agree with it by 95%) and how they appear to me. PCA in its proper, minimal form is the specific orthogonal rotation of correlated data to its uncorrelated form, with the principal components skimming sequentially less and less of the overall variability. If the dimensionality reduction is all we want we usually don't compute loadings and whatever they drag after them. We're happy with the (raw) principal component scores $\bf P$. [Please note that notations on the chart don't precisely follow @amoeba's, - I stick to what I adopt in some of my other answers.] On the chart, I take a simple example of two variables p=2 and use both extracted principal components. Though we usually keep only few first m components, for the theoretical question we're considering ("Is PCA with rotation a PCA or what?") it makes no difference if to keep m or all p of them; at least in my particular answer. The trick of loadings is to pull scale (magnitude, variability, inertia $\bf L$) off the components (raw scores) and onto the coefficients $\bf V$ (eigenvectors) leaving the former to be bare "framework" $\bf P_z$ (standardized pr. component scores) and the latter to be fleshy $\bf A$ (loadings). You restore the data equally well with both: $\bf X=PV'=P_zA'$. But loadings open prospects: (i) to interpret the components; (ii) to be rotated; (iii) to restore correlations/covariances of the variables. This is all due to the fact that the variability of the data has been written in loadings, as their load. And they can return that load back to the data points any time - now or after rotation . If we conceive of an orthogonal rotation such as varimax that means that we want the components to remain uncorrelated after the rotation done. Only data with spherical covariance matrix, when rotated orthogonally, preserves uncorrelatedness. And voila, the standardized principal components (which in machine learning often are called "PCA-whitened data") $\bf P_z$ are that magic data ($\bf P_z$ are actually proportional to the left, i.e. row eigenvectors of the data). While we are in search of the varimax rotation matrix $\bf Q$ to facilitate interpretation of loadings the data points passively await in their chaste sphericity & identity (or "whiteness"). After $\bf Q$ is found, rotation of $\bf P_z$ by it is equivalent to usual way computation of standardized principal component scores via the generalized inverse of the loading matrix, - this time, of the rotated loadings, $\bf A_r$ (see the chart). The resultant varimax-rotated principal components, $\bf C_z$ are uncorrelated, like we wanted it, plus data are restored by them as nicely as before rotation: $\bf X=P_zA'=C_zA_r'$. We may then give them back their scale deposited (and accordingly rotated) in $\bf A_r$ - to unstandardize them: $\bf C$. We should be aware, that "varimax-rotated principal components" are not principal components anymore: I used notation Cz, C, instead of Pz, P, to stress it. They are just "components". Principal components are unique, but components can be many. Rotations other than varimax will yield other new variables also called components and also uncorrelated, besides our $\bf C$ ones. Also to say, varimax-rotated (or otherwise orthogonally rotated) principal components (now just "components"), while remain uncorrelated, orthogonal, do not imply that their loadings are also still orthogonal. Columns of $\bf A$ are mutually orthogonal (as were eigenvectors $\bf V$), but not columns of $\bf A_r$ (see also footnote here ). And finally - rotating raw principal components $\bf P$ with our $\bf Q$ isn't useful action. We'll get some correlated varibles $\bf "C"$ with problematic meaning. $\bf Q$ appeared as to optimize (in some specific way) the configuration of loadings which had absorbed all the scale into them . $\bf Q$ was never trained to rotate data points with all the scale left on them. The rotating $\bf P$ with $\bf Q$ will be equivalent to rotating eigenvectors $\bf V$ with $\bf Q$ (into $\bf V_r$) and then computing the raw component scores as $\bf "C"=XV_r$. These "paths" noted by @amoeba in their Postscriptum. These lastly outlined actions (pointless for the most part) remind us that eigenvectors, not only loadings, could be rotated, in general. For example, varimax procedure could be applied to them to simplify their structure. But since eigenvectors are not as helpful in interpreting the meaning of the components as the loadings are, rotation of eigenvectors is rarely done. So, PCA with subsequent varimax (or other) rotation is still PCA which on the way abandoned principal components for just components that are potentially more (than the PCs) interpretable as "latent traits" but were not modeled satistically as those (PCA is not fair factor analysis) I did not refer to factor analysis in this answer. It seems to me that @amoeba's usage of word "latent space" is a bit risky in the context of the question asked. I will, however, concur that PCA + analytic rotation might be called "FA- style view on PCA".
