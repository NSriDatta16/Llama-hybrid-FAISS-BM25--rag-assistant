[site]: datascience
[post_id]: 47799
[parent_id]: 47797
[tags]: 
Question 1: Why do most CNN models not apply the cross-validation technique? $k$ -fold cross-validation is often used for simple models with few parameters, models with simple hyperparameters and additionally the models are easy to optimize. Typical examples are linear regression, logistic regression, small neural networks and support vector machines. For a convolutional neural network with many parameters (e.g. more than one million) we just have too many possible changes in the architecture. What you can do is to do some experiments with the learning rate, batch size, dropout (amount and position) and batch normalization (position). Training a convolutional neural network with a huge dataset takes quite a long time. Doing hyperparameter optimization would just be total overkill. Often in papers, they try to improve the results of other research papers. It is not the goal to get better results by improving the chosen hyperparameters but rather to come up with new ideas to solve the given task but with better accuracy or less computational effort. Question 2: If I use cross-validation how can I generate confusion matrix? can I split dataset to train/test then do cross-validation on train set as train/validation (i.e. doing cross-validation as train/validation except for the usual train/test) and at last use test set the same way? or how? In order to do $k$ -fold cross validation you will need to split your initial data set into two parts. One dataset for doing the hyperparameter optimization and one for the final validation. Then we take the dataset for the hyperparameter optimization and split it into $k$ (hopefully) equally sized data sets $\mathcal{D}_1,\mathcal{D}_2,\ldots,\mathcal{D}_k$ . For the sake of clarity let us set $k=3$ . Then for each possible hyperparameter combination that we want to test we use $\mathcal{D}_1$ and $\mathcal{D}_2$ to fit our model and we use $\mathcal{D}_3$ to validate our model. Then we do the same with $\mathcal{D}_2$ and $\mathcal{D}_3$ and use $\mathcal{D}_1$ for validation. Then we do the same with $\mathcal{D}_1$ and $\mathcal{D}_3$ and use $\mathcal{D}_2$ for validation. We will get $3$ confusion matrices for every possible hyperparameter configuration. In order to derive a metric from these three results, we take the mean of these confusion matrices. Then we can scan through all averaged confusion matrices so select the hyperparameter configuration that was the best (you have to define what parts of the confusion matrix are important for your problem). Finally, we pick the 'best' hyperparameters and calculate the prediction performance on the final validation set. This performance metrics are the ones that you report.
