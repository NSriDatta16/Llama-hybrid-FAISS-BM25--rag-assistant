[site]: crossvalidated
[post_id]: 357611
[parent_id]: 
[tags]: 
What is the difference between model selection and hyperparameter tuning?

In the context of supervised learning, in most statistics based texts and papers, one reads about model selection. For example Hastie, Tibshirani and Friedman in ESL define it as: Model Selection: estimating the performance in order to choose the best one. Machine Learning literature and papers on the other hand usually talk about hyperparameter optimization or hyperparameter tuning. From Wikipedia: In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. From what I can see, both are trying to achieve the same objective (choose the best model among a set of predefined models) and can use similar techniques (such as cross validation). But at the same time, model selection seems to employe certain concepts that never come up in hyperparamter optimization, such as using the AIC or the BIC to select the best model. Is this just a difference in terminology or is there a conceptual difference between the two that I am missing?
