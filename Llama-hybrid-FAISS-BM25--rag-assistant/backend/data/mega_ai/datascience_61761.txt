[site]: datascience
[post_id]: 61761
[parent_id]: 
[tags]: 
How does XGBoost use softmax as an objective function?

I'm quite used to seeing functions like log-loss, RMSE, cross entropy as objective functions and it's easy to imagine why minimizing these would give us the best model. What's difficult to imagine is how XGBoost uses softmax, a function used to normalize the logits, as a cost function. As mentioned in the docs here . How can a softmax function be minimized?
