[site]: datascience
[post_id]: 122444
[parent_id]: 122443
[tags]: 
I guess the tutorial answered my question, and I quoted the answer, I just need to really wrap my head around it: Sequence models are better when there are a large number of small, dense vectors. This is because embedding relationships are learned in dense space, and this happens best over many samples. The n-gram approach is also called a "bag of words approach", and doesn't communicate anything about word order to the model. With n-gram vector representation, we discard a lot of information about word order and grammar (at best, we can maintain some partial ordering information when n > 1). This is called a bag-of-words approach. Also, the sequence model approach uses a neural network; the neural network contains a hidden layer that will learn "embeddings" ( word similarity ) Sequence models often have such an embedding layer as their first layer So why would a large S/W ratio push us toward a sequence model approach? As a rule I think neural nets do best with large amounts of data; so the larger the numerator (number of samples), the better we can do with a neural net (RNN specifically recommended for the sequence model, vs a MLP recommended for the n-gram/bag-of-words model) And as the tutorial explained, the more "dense" the samples, (aka the smaller the numerator (number of words per sample)), the better chance the neural net has at "understanding word similarity" (embeddings). Intuitively, I guess that makes sense to my naive human brain; these "dense" sentences help me learn word association better... ['Ascend to go up', 'Descend to go down'] But maybe I should consider other things, like the type of document/writing (the examples I gave above are very deliberate & "instructional" in tone) or the size of the vocabulary And maybe more than anything, the answer "why" is simply because "they tested different approaches based on the ratio" (a kind of hyper-hyper tuning!), in other words, they tested a lot, found the ratio is a meaningful "dataset parameter", which eventually passes "threshhold" in the tradeoff of model accuracy + performance time. We ran a large number (~450K) of experiments across problems of different types (especially sentiment analysis and topic classification problems), using 12 datasets, alternating for each dataset between different data preprocessing techniques and different model architectures. This helped us identify dataset parameters that influence optimal choices.
