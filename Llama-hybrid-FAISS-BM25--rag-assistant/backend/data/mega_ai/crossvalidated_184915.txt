[site]: crossvalidated
[post_id]: 184915
[parent_id]: 182734
[tags]: 
As far as I know, what is called Deep Neural Network (DNN) today has nothing fundamentally or philosophically different from the old standard Neural Network (NN). Although, in theory, one can approximate an arbitrary NN using a shallow NN with only one hidden layer, however, this does not mean that the two networks will perform similarly when trained using the same algorithm and training data. In fact there is a growing interest in training shallow networks that perform similarly to deep networks. The way this is done, however, is by training a deep network first, and then training the shallow network to imitate the final output (i.e. the output of the penultimate layer) of the deep network. See, what makes deep architectures favorable is that today's training techniques (back propagation) happen to work better when the neurons are laid out in a hierarchical structure. Another question that may be asked is: why Neural Networks (DNNs in particular) became so popular suddenly. To my understanding, the magic ingredients that made DNNs so popular recently are the following: A. Improved datasets and data processing capabilities 1. Large scale datasets with millions of diverse images became available 2. Fast GPU implementation was made available to public B. Improved training algorithms and network architectures 1. Rectified Linear Units (ReLU) instead of sigmoid or tanh 2. Deep network architectures evolved over the years A-1) Until very recently, at least in Computer Vision, we couldn't train models on millions of labeled images; simply because labeled datasets of that size did not exist. It turns out that, beside the number of images, the granularity of the label set is also a very crucial factor in the success of DNNs (see Figure 8 in this paper , by Azizpour et al.). A-2) A lot of engineering effort has gone into making it possible to train DNNs that work well in practice, most notably, the advent of GPU implementations. One of the first successful GPU implementations of DNNs, runs on two parallel GPUs; yet, it takes about a week to train a DNN on 1.2 million images of 1000 categories using high-end GPUs (see this paper, by Krizhevsky et al.). B-1) The use of simple Rectified Linear Units (ReLU) instead of sigmoid and tanh functions is probably the biggest building block in making training of DNNs possible. Note that both sigmoid and tanh functions have almost zero gradient almost everywhere, depending on how fast they transit from the low activation level to high; in the extreme case, when the transition is sudden, we get a step function that has slope zero everywhere except at one point where the transition happens. B-2) The story of how neural network architectures developed over the years reminds me of how evolution changes an organism's structure in nature. Parameter sharing (e.g. in convolutional layers), dropout regularization, initialization, learning rate schedule, spatial pooling, sub-sampling in the deeper layers, and many other tricks that are now considered standard in training DNNs were developed, evolved, end tailored over the years to make the training of the deep networks possible the way it is today.
