[site]: crossvalidated
[post_id]: 283390
[parent_id]: 
[tags]: 
visualizing closeness in an n-dimensional space? (particularly, regarding embedded sparse features)

In 3 dimensional space, we have a very intuitive picture of the closeness of two vectors: we simply imagine a space with two points in it, and imagine how close they are to eachother. In this way we can imagine different degrees of closeness. Mathematically, we think of closeness simply as the euclidean metric: $$\sqrt{(x_1-x_2)^2+(y_1-y_2)^2+(z_1-z_2)^2}$$ In machine learning, we often have sparse features (e.g. word ID's in natural language processing), and we want to embed them in a dense vector. The resulting embedded feature has a conception of closeness, in the sense that any given word ID (e.g. cat) will have a meaning (represented by an embedding) that is closer to a particular word (e.g. feline), than to some other word (car). We could measure this closeness using the same Euclidean metric on the $n$ entries of the embedded vector: $$\sqrt {\sum _i (x_{i,1}-x_{i,2})^2}$$ How to visualize $n$ dimensional space in a way that shows intuitively closeness of vectors? However, there is no longer a straightforward way to visualize these vectors in a way that shows their closeness, since we cannot visualize $n$ dimensional space. The way I usually mentally visualize $n$ dimensional space , is as a bar chart where each bar represents a coordinate of a particular vector in that space. If $n$ is very large, I usually think of a particular vector as being approximated by a kind of highly oscillatory (real-valued) function (that maps the coordinate index to its output value), kind of like an audio file : The problem with this visualization, is that it does not represent closeness well at all : If we shift this graph by only one unit to the right , it will look almost indistinguishible from the original graph, but for most graphs, its distance to the original will be enormous . (to see why, imagine in 2 dimensional space we have a vector $(x,y)=(1,10)$. If we shift it by one unit, we get the vector $(10,1)$, a mirror around the diagonal line. The larger the original distance from the origin is, the larger the distance between the new and the original vector will be). My question is, do we have a way to visualize $n$ dimensional vectors in a way that intuitively shows the closeness/distance of two vectors? (in particular in the context of embeddings of sparse features for machine learning).
