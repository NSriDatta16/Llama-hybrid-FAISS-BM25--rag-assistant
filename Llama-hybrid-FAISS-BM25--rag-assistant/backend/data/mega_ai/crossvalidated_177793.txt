[site]: crossvalidated
[post_id]: 177793
[parent_id]: 
[tags]: 
Making use of the uncertainty of a sample proportion

I would like to understand more about how to use the uncertainty in a sample proportion. Imagine I need to offer a warranty for a product against individual sold items failing to work as advertised. I would like to estimate the cost of offering the warranty based on the track record of the same products already sold. As I go along, selling a few more products and learning about more failures and paying out on the warranty, I will adjust my model of what the warranty is costing me. Rather than delve into the depths of the various methods that I could use I want to understand more on how to frame the analytical objective. Lets imagine the proportion of failed products todate is 5% and that I have some measure of uncertainty for the proportion, which happens to be 4%. I need a single number answer , i.e. how much I should roll into the cost of the product as a per item warranty cost. I think I can choose to either: take into account the uncertainty as part of the process of establishing the warranty cost, e.g. 5% +- 4% = a range from 1% to 9%, OR assume that for this type of problem the central proportion, 5%, is sufficient description of the liability, given that I will be continutally resetting my calculation as I go along In the first case as the range is so wide it seems rather arbitrary to add the proportion and uncertainty and thus establish the cost of the warranty as 9% and in this case we can see that it nearly doubles the cost compared to the basic proportion. I'd have a similar worry with the basic proportion of the second case or picking any other single number. Yet a single number is what I need and I have a nagging feeling that I am ignoring something important, hence I don't really know how to use the information. It might be that my choice of data collection period for establishng the running proportion and the accounting period for the latest estimate of the cost of the warranty should be linked, but I don't want to pressuppose that. EDIT Additional context: There is no fixed underlying failure rate, it can evolve with time. There is nothing that says that there should be any relation between failure modes and their relative frequencies, solutions and further problems introduced by fixing the previous faults. There is nothing that says that next year should bear any resemblence to last year, nor change smoothly etc .. and yet there is a need to prevent the organisation selling the products from going bust through not making provision for warranties or, equally , going bust from charging too much for its products. I haven't defined many of the terms that statisticians might like to, its deliberate because its all to easy to disappear down a rabbit hole of definitions. If it helps, I've eased away from central "proportion" to "estimate" so as to disengage from the frequentist view. I think the concern about accounting for uncertainty is that this itself could bias (non statisticians meaning of the word "bias") the result and from the simple, non-statistician's view, the uncertainty could go either way and be just as damaging to the business either way. I've realised I am playing the devil's advocate with my own beliefs but am looking for some further convincing. Refined question: Are there any features of this scenario that mean that the central estimate is not the best one?
