[site]: crossvalidated
[post_id]: 171656
[parent_id]: 167088
[tags]: 
Apparently, different authors have different ideas about stochastic gradient descent. Bishop says: On-line gradient descent, also known as sequential gradient descent or stochastic gradient descent, makes an update to the weight vector based on one data point at a timeâ€¦ Whereas, [2] describes that as subgradient descent , and gives a more general definition for stochastic gradient descent: In stochastic gradient descent we do not require the update direction to be based exactly on the gradient. Instead, we allow the direction to be a random vector and only require that its expected value at each iteration will equal the gradient direction. Or, more generally, we require that the expected value of the random vector will be a subgradient of the function at the current vector. Shalev-Shwartz, S., & Ben-David, S. (2014). Understanding Machine Learning: From Theory to Algorithms. Cambridge University Press.
