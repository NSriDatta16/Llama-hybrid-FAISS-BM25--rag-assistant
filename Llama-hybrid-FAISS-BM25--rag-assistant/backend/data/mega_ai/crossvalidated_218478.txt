[site]: crossvalidated
[post_id]: 218478
[parent_id]: 218477
[tags]: 
Yes, I suggest running a random forest classification model via h2o . Compare its results with a gradient boosting model and a neural network and pick which one is best. Random Forest is really good for data of this size. You can also explore dozens of alternatives in caret and try an extreme gradient boosting model but they will probably be significantly slower. When you run your prediction (using h2o.predict(drf,newdata=mynewdata) ) you'll be given both the discreet classification predictions as well as the probability of each row falling into each class of your mutliclass dependent variable. require(h2o) h2o.init(nthreads = -1) trainHex Any of the 180 following models in caret that are "classification" or "dual use" should work for you as viable alternatives, though again I favor random forest. More over, a better specified multinomial logit run with multinom or nnet may perform better than your prior results. With caret and most other packages in R (asides from h2o ) just make sure you specify type = 'p' in your predict statement. As a rule of thumb, when you have this many rows of data gradient descent or advanced optimization solver models will usually perform better than models using normal equations to minimize their cost function. caret 's 180 models: https://topepo.github.io/caret/modelList.html Here's how to do eXtreme gradient boosting in R: # R XGB with infinite replacement library(xgboost) library(Matrix) #pacman::p_load(Romp) set.seed(1234) train 0), j = .name,value =max(DT[!(is.infinite(.name)),.name])))) train 0), j = .name,value =max(DT[!(is.infinite(.name)),.name])))) test
