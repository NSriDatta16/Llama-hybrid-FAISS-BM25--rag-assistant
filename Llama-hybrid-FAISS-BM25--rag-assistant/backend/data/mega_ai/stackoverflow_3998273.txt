[site]: stackoverflow
[post_id]: 3998273
[parent_id]: 3997316
[tags]: 
i can think only in one raeson right now , XRobot they don't trust you . woh they ? they :) when you want to do some crawling or scraping and you see that they don't trust you , you just have to dump them , how is that ? First of all you should know that some web server filter they contain for malicious software like robot (maybe they know you are a robot, hmmm XRobot :) ), how they do that? there is many way to filter : like using captcha in the webpage , filtering by User-Agent ... And because your ICMP ping work ,chrome browser work but not w3m i suggest you change the User-Agent like this: user_agent = 'Mozilla/5.0 (X11; U; Linux x86_64; en-US; rv:1.9.2.10) Gecko/20100915\ Ubuntu/10.04 (lucid) Firefox/3.6.10' request = urllib2.Request('http://www.google.com/') request.add_header('User-agent', user_agent ) opener.open(request) maybe i'm getting paranoia here, but hopefully this can help you :)
