[site]: crossvalidated
[post_id]: 590195
[parent_id]: 589862
[tags]: 
For any support vector $\textbf{x}^{(i)}$ , the following holds: $$ \textbf{w}^T \cdot \textbf{x}^{(i)} + b = y^{(i)}. $$ This is basically the definition of " support vector ": It lies on the margin, either on the positive side (+1) for vectors from the positive class, or on the negative side (-1) for the negative class. So, the simplest way to compute $b$ is simply to take an arbitrary support vector and calculate: $$ b = y^{(i)} - \textbf{w}^T \cdot \textbf{x}^{(i)}, $$ which, depending on your optimisation algorithm used for finding $\textbf{w}$ , can be done even in $O(1)$ : In the optimisation phase, you just need to store a support vector as soon as you encounter one, and retrieve it directly when calculating $b$ . In practice, however, for numerical stability, it is preferred to average over all support vectors: $$ b = \frac{1}{|SV|}\sum_{i \in SV}y^{(i)} - \textbf{w}^T \cdot \textbf{x}^{(i)}, $$ where $SV$ is the set of support vectors. Here, the complexity is obviously at least $O(|SV|)$ , but, unless your optimiser stores all support vectors separately, e.g. in a hash table, you'll probably need to go through the whole set, identifying the support vectors, which would cost you $O(N)$ . Reputable source: Bishop, Pattern Recognition and Machine Learning , p. 330, eq. (7.18) (using the kernel trick and a slightly different notation).
