[site]: crossvalidated
[post_id]: 576916
[parent_id]: 576562
[tags]: 
Is it possible that the high amount of data is given my an excessive amount of significant fixed effects? Yes, your 680000 measurements are not independent but are made within the same cows and the same farms, and possibly also within similar small time-frames (I count on average about 18 data points per cow, but possibly some of the data points can be made close to one another and be effectively a single sample counted twice). You do include random intercepts, but you may have all sorts of additional correlations that you did not account for and these may increase the apparent significance. If you want an assessment of independence that is independent from the statistical model assumptions, then you might want to use part of the data as a test set to find out how well your model performs in making predictions for different cows, farms, and times. Should I be concerned about the non-normality of the residuals, since I just want to check the effect of the THI in milk production? I will not use the model to forecast, for example. The non-normality is not extreme. The qq-plot indicates heavy tails but they are not of a type that makes the estimate of the standard error wrong. There are different type of heavy tails. You have for instance the tails that relate to infinite variance, and that is not your case (since the milk production is bounded). What you have is just a discrepancy from the standard normal approximation. This occurs possibly because the variance of the residuals is not homogeneous. But that does not invalidate the estimates of the regression and neither the estimates of the error in the regression coefficients (those coefficients are a linear sum of your observations and will follow approximately a normal distribution) If yes, how can I tackle this heavy tailed distribution? Should I use a generalized mixed model or should I keep trying with the robust linear mixed model? I wouldn't be so much worried about point 2, but more about point 1. Instead of using 10 levels of DEL I would prefer to treat DEL as a continuous variable and model it with some function (that possibly involves much less than 10 coefficients). If you use a small data set, like one year from a few farms, to explore the data and behavior of the relationships, then you could come up with some sound model. That would reduce potential problems like clipping (a production one day later does not suddenly make a jump according to the model and create weird plots of residuals), and also you could make the model more flexible (maybe you need to include more interaction terms and more random effects). You could also use existing literature to come up with a model and reduce the amount of data that you want to use (waste) on coming up with a potential model. Then fit coefficients based on another part of the data set, and finally measure performance with a test set.
