[site]: crossvalidated
[post_id]: 534510
[parent_id]: 
[tags]: 
Correct way of mean-centering and scaling time series data used as input to an LSTM?

I am training an LSTM network in tensorflow/Keras which takes eight different time series/features as input. The input matrix given to the network has the form ( nSamples x nTimesteps x nFeatures ). My question is: when mean-centering and scaling the data to unit variance before giving it to the network, should one mean value and one standard deviation be calculated for every feature of the dataset, or, should a mean value be calculated for every individual timestep of each feature? Here is a Numpy example of the alternatives I am contemplating: say my data to be preprocessed is stored in a three-dimensional matrix X . One way of mean-center and scaling X would be: mx = np.mean( X, axis = (0, 1) ) stdx = np.std( X, axis = (0, 1) ) X_scaled = ( X - mx ) / stdx But another way would be: mx = np.mean( X, axis = 0 ) stdx = np.std( X, axis = 0 ) X_scaled = ( X - mx ) / stdx In the first method of the example mx and stdx are of shape ( nFeatures ). In the second method mx and stdx are of shape ( nTimesteps x nFeatures ). If all columns of X (all timesteps) represented lagged values of one data source (data from one sensor for example) they would be expected to have the same mean and variance, and thus the first alternative in the example makes more sense which is also recommended here . However, the nature of my input data is such that the timeseries consists of sorted data that is always increasing. For example the content of X[0,:,0] could be: [10, 20, 100, 500, 2500] . Which means that the columns of the X matrix do not share a common mean or standard deviation; the mean value is increasing for each timestep. Does that mean that alternative two, which considers the data of each column separately, is the appropriate way of scaling this type of data? Or is alternative one still the correct way to preprocess the data?
