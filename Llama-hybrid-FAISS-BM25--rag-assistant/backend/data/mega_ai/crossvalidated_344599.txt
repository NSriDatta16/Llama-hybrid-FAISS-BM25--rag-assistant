[site]: crossvalidated
[post_id]: 344599
[parent_id]: 344593
[tags]: 
Some intuitive idea from a mathematics point of view: Computing the error between two scalars or also two finite-dimensional vectors with real or complex values is pretty easy. Actually, all norms on finite-dimensional vector-spaces are equivalent. This is not the case on spaces of functions or probability measures. Compare for instance $L^2$- and $L^\infty$-distance on say a space of continuous functions on a compact interval, or Total variation and Prokhorov distances on spaces of probability measures. Moreover, it is technically true on discrete spaces, but there the typical norm may be replaced by a discrete metric. In Machine Learning, people often discuss real vectors, but what they actually want to assess are functions and probability measures, as well as discrete vectors. And there are many different ways to do that.
