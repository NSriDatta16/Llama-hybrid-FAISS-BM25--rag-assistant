[site]: crossvalidated
[post_id]: 202340
[parent_id]: 
[tags]: 
Significance of variable but low impact on log likelihood?

I'm currently working on a logistic regression, trying to find out how significant some independent variables considering the dichotomous outcome are. I did some test runs for the logistic regression and wondering about the results that raise some questions about the validity of the results. Is it possible that a variable has a strong significance in the model but is barely contributing to reducing the log likelihood value? As I got from Fields Discovering Statistics, a log likelihood somewhat indicates how much unexplained information is there. With my very small knowledge in logistic regression and statistics in general, I would assume that a higher significance of a variable would lead to a lower loglikelihood value and therefore less unexplained information. Or is there nothing wrong about having a significant variable but still a high log likelihood value? As I mentioned before, I'm having trouble to judge whether the results are valid or not (with statsmodels in python): | Dep. Variable: Churner | No. Observations: 107262 | | Model: Logit | Df Residuals: 107258 | | Method: MLE | Df Model: 3 | | Date: Fri, 18 Mar 2016 | Pseudo R-squ.: 0.3798 | | Time: 01:12:22 | Log-Likelihood: -41705. | | converged: True | LL-Null: -67247. | | LLR p-value: 0.000 | coef std err z P>|z| [95.0% Conf. Int.] const -0.5324 0.019 -27.886 0.000 -0.570 -0.495 currentBreak 0.0065 7.42e-05 87.507 0.000 0.006 0.007 Frequency 0.0019 0.000 19.164 0.000 0.002 0.002 numActiveCategories -0.1589 0.007 -24.117 0.000 -0.172 -0.146 Without having knowledge about the data preprocessing (which is a bit too much to explain it here), could you tell if the results are somewhat usual or are at some point absolutely contradictory?
