[site]: crossvalidated
[post_id]: 638284
[parent_id]: 638276
[tags]: 
In statistics, we call a model linear when the outcome is a linear combination of the parameters, meaning that you can write $\hat y_i = \sum_j x_{ij}\hat\beta_j$ (second definition). We could have something like $x_{2j} = \left(x_{1j}\right)^2$ without breaking that, which is how polynomial regression qualifies as a linear model, even though the model could be squaring or cubing terms, leading to a highly nonlinear plot. If you then throw an activation function on there to get $\hat y_j = \dfrac{1}{1 + \exp\left(-\sum_j x_{ij}\hat\beta_j\right)}$ , then $\hat y$ no longer qualifies as being a linear combination of the $\beta$ parameters, and statistics would not consider this a linear model, with the caveat that the nonlinearity can be expressed in a way that qualifies it as a generalized linear model (which some people will consider linear enough to count as somewhat of an honorary linear model). If the machine learning classification people want to consider a classifier to be linear if the decision boundary is a hyperplane, let them give one word multiple meanings. This is math. Words can have multiple distinct meanings in math. Just consider what "normal" can mean in math. Overall, no, those statements are not equivalent. The second and third do not imply each other. Rather, both can be taken as definitions of "linear" depending on the context.
