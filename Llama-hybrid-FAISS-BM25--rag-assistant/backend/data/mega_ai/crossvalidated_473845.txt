[site]: crossvalidated
[post_id]: 473845
[parent_id]: 
[tags]: 
How to sample (without replacement) from a large set by randomly sampling non-overlapping subsets?

I've got a large corpus of video, and I want to randomly sample frames over the whole corpus. A naive way to do this is convert all videos to a giant set of images with ffmpeg then subsample the gigantic set. An alternative is to convert each video, subsampling from its frames. I'd like to take the second approach to avoid having to store more than a few images at a time, but I'd also like the final randomly-sampled distribution to match the first approach. Is there a statistical relationship that would allow me to do this? I can come up with a way to get close to what I'm looking for: When I select from the big set, I'll select some number of frames out of the total: $\frac{n}{N}$ . So if I set the probability that I select any individual frame as $\frac{n}{N}$ , I should get approximately $n$ frames back. It's not exact, but I can do this selection procedure on single examples, which means I can do it on subsets of any size by iterating through frames from any video in the corpus. In my case I don't actually know better than an estimate for $N$ without converting all videos at least once. I could get this number by iteration and not have to sit on a mountain of data at once. Is there a more elegant way?
