[site]: stackoverflow
[post_id]: 1040154
[parent_id]: 1037719
[tags]: 
Thanks for all the quick input. I decided to consider the problems of speed and "naturalness" separately. For the generation of natural-ish text, I have combined a couple ideas. To generate text, I start with a few text files from the project gutenberg catalog, as suggested by Mark Rushakoff. I randomly select and download one document of that subset. I then apply a Markov Process, as suggested by Noldorin , using that downloaded text as input. I wrote a new Markov Chain in C# using Pike's economical Perl implementation as an example. It generates a text one word at a time. For efficiency, rather than use the pure Markov Chain to generate 1gb of text one word at a time, the code generates a random text of ~1mb and then repeatedly takes random segments of that and globs them together. UPDATE : As for the second problem, the speed - I took the approach to eliminate as much IO as possible, this is being done on my poor laptop with a 5400rpm mini-spindle. Which led me to redefine the problem entirely - rather than generating a FILE with random content, what I really want is the random content. Using a Stream wrapped around a Markov Chain, I can generate text in memory and stream it to the compressor, eliminating 8g of write and 8g of read. For this particular test I don't need to verify the compression/decompression round trip, so I don't need to retain the original content. So the streaming approach worked well to speed things up massively. It cut 80% of the time required. I haven't yet figured out how to do the binary generation, but it will likely be something analogous. Thank you all, again, for all the helpful ideas.
