[site]: crossvalidated
[post_id]: 532100
[parent_id]: 
[tags]: 
CIFAR-100 test accuracy maxes out at 67% but validation accuracy hits 90%

I've been experimenting with several different CNN models for CIFAR-100, but no matter what I do I cannot attain a test accuracy above 67%, even though validation accuracy sometimes reaches 90% for some models (where the validation set includes 5000 samples removed from the supplied training set before any augmentation). I've tried modified versions of DenseNet, ResNet, MobileNet, VGG-style nets. I've tried adding auxiliary loss objectives to train intermediate layers using logistic regression or quasi-SVMs. I've tried no augmentation, standard pad+crop+flip augmentation, mixup augmentation. I've tried with various strengths regularization techniques like L2, L1 and dropout, and I've tried without. I've tried SGD both with and without Nesterov momentum. I've tried various learning rate schedules. I've tried activation functions like ReLU, ELU, Swish, Mish. I've tried both a custom training loop and the normal keras fit function. I've tried a keras subclass model, a keras sequential model, a keras functional model. I've made sure the validation and training data contain no cross-contamination. I have run out of things to try, yet nothing seems to yield a test accuracy above 67%. I am starting to believe that Tensorflow 2.5 may have broken the CIFAR-100 dataset in some way, because this 67+/-1% keeps cropping up regardless of which CNN design I try and has persisted across multiple fresh start python scripts. I've attempted to use keras's ResNet and DenseNet implementations to train from scratch on CIFAR-100, but they yield even worse performance likely due to the lack of any dropout layers which small datasets like this tend to benefit from greatly. This question is simply asking two things: firstly, if, using the latest version of tensorflow and the version of the dataset included in keras, it is possible to get a test accuracy above 67% on CIFAR-100 using any network architecture (without pretrained weights); and secondly, how it is possible for a CNN to attain over 90% validation accuracy while achieving such a low test accuracy on a dataset like CIFAR-100 which has been used as one of the standard datasets in countless papers alongside CIFAR-10 and ImageNet and as such is assumed to have a test set representative of the training data.
