[site]: datascience
[post_id]: 106497
[parent_id]: 
[tags]: 
The behavior of the cross validation error and training error in underfitting case is not clear

I currently study the "Machine Learning" course on Coursera.org by Andrew Ng, it comes to a topic that discusses the performance of learning algorithms under different conditions. Here, we discuss the case when the algorithm underfits the training data (high bias). I cannot understand why the cross-validation error (Jcv(theta)) decreases as we increase the training set size(m), and why it then flattens out. And I cannot understand why the training error (Jtrain(theta)) flattens out at some region, I understand why it increases as we increase m. I asked about it in the coursera discussion forum and studied it twice, but it is still doesn't make sense to me. Notice that : The cross-validation error(Jcv(theta)) is a measure of how well the algorithm fits the cross validation set, the lower Jcv(theta) the better the algorithm fits the cross validation set. The training error (Jtrain(theta)) is a measure of how well the algorithm fits the training set, the lower Jtrain(theta) the better the algorithm fits the training set. m is the number of examples in the training set only.
