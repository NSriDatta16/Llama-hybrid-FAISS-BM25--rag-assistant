[site]: crossvalidated
[post_id]: 610450
[parent_id]: 
[tags]: 
Explainable AI - Noise in gradients and embeddings of large language models

I am doing experiments related to explainable AI. I have two BERT models - the standard bert-base-cased and a distilled prajjwal1/bert-mini that has around 14M parameters, so much smaller than the base-size BERT. Both are fine-tuned on the SST dataset. Both are fine-tuned for 5 epochs. Among others, I use vanilla gradients (w.r.t. input embeddings) and gradients x input to generate attributions, which I then evaluate on an annotated dataset. I don't believe the metric I use to measure the alignment between the ground truth and the attributions is important, but if needed I'll add it. The problem I have is that when I evaluate the attributions for the bert-base-cased model, vanilla gradients perform slightly better than gradients x input. For the prajwall1/bert-mini , it's the opposite - vanilla gradients perform worse than gradients x input. I don't have any idea as to why it happens. My intuition is that a larger model will have more noise in the embeddings and the gradients, so when you multiply them, you get more noise and thus worse attributions. But I have not found any paper, article, whatever, to support this intuition. So, I would like to ask if anyone could a) point me to some research which discusses the amount of noise in gradients and embeddings of large language models (or any models at this point, I've tried and haven't found anything) or b) provide an explanation for the different behavior of the attribution methods. Any additional insight is welcome. Also, if anyone knows how to measure the amount of noise in gradients, I'd appreciate that. Intuitively, I'd try to add a small amount of noise to the input embeddings, create some n samples this way, and calculate their gradients. From them, I can calculate a standard deviation which could be an indicator of how much noise they contain, but I'm not completely sure that it's the right approach. Thanks a lot
