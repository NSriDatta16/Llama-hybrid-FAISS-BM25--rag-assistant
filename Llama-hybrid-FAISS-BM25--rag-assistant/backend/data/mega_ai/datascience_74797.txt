[site]: datascience
[post_id]: 74797
[parent_id]: 74794
[tags]: 
Well ... there are many aspects from which one can answer this question ( Like Valentin's answer ... +1!) as Machine Learning and Data Mining are very much about distributions in general. I just mention a few that come to my mind first. Some models assume Gaussian distribution e.g. K-means. Imagine you want to apply K-means on this data without log-transform. K-means will have much difficulties with the original feature but the log-transform makes it pretty proper for K-means as "mean" is a better representative of samples in a Gaussian distribution rather than skewed ones. Some statistical analysis techniques also assume gaussianity. ANOVA works the best with (and actually designed for) normally distributed data (specially in small sample populations). The reason is simply the fact that it is mainly dealing with mean and sample variance to determine the "center" and "variation" of sample populations and it makes sense the most in Gaussian distribution. All in all, computation with adjusted features is more robust than skewed features. Skewed features have uneven range which is an issue specially if their range is huge (like your example). Adjusted (engineered) features are not going to necessarily become Gaussian but having a smaller and more even range for example.
