[site]: crossvalidated
[post_id]: 448133
[parent_id]: 
[tags]: 
Why do variational autoencoders not find the "best" latent variables?

From my understanding: Variational autoencoders sample the latent variables $y$ using a proposal distribution $q$ of the observed variables $x$ . The objective is that the decoder $p$ applied to $y$ defines a distribution whose value at $x$ is supposed to be maximized (reconstruction cost) plus a regularizer term. Why not forget about $q$ , and for every example $x$ , iteratively update $y$ so as to maximize the objective? That is, setting $y$ to $$\arg \max_y P(x | y)$$ rather than sampling it from $q(y \mid x)$ . Is $q$ an approximation motivated by inference speed?
