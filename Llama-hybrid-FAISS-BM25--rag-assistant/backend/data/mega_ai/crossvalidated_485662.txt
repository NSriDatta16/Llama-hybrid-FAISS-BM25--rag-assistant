[site]: crossvalidated
[post_id]: 485662
[parent_id]: 
[tags]: 
If the predicted value of machine learning method is E(y | x), why bother with different cost functions for y | x?

Say we know that $Y$ follows a distribution with density $f$ . It is well known that the mean $E(Y \ | X)$ minimizes the Root Mean Square Error (RMSE). We are generally interested in predicting a value for $y $ given we know $x$ as $E(Y \ | \ X)$ . Then, even if theoretically we might want to model $f$ , if we just want a deterministic prediction, Why even bother with different distributional assumptions on $Y \ | \ X$ , i.e. why even bother with different cost functions like the negative log-likelihood? Examples : Suppose $Y \sim Bernoulli(p)$ . If we use RMSE as cost function this clearly minimizes the mean of $Y$ , which is indeed $p$ , the value we want. Suppose $Y \sim Gamma(k, \theta)$ . Since $E(Y) = k\theta$ , we can just fix e.g. $\theta = 1$ and predict as $E(Y) = k$ . Again, let us use RMSE as cost function and this will obtain the intended value. So, Why use for example the negative log likelihood of the gamma distribution?
