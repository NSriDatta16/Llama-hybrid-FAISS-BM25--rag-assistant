[site]: crossvalidated
[post_id]: 225221
[parent_id]: 225210
[tags]: 
Okay, remember the relation between the $FPR$ (False Positive Rate), $TPR$ (True Positive Rate) and $ACC$ (Accuracy): $$TPR = \frac{\sum \text{True positive}}{\sum \text{Positive cases}}$$ $$FPR = \frac{\sum \text{False positive}}{\sum \text{Negative cases}}$$ $$ACC = \frac{TPR \cdot \sum \text{Positive cases} + (1-FPR) \cdot \sum \text{Negative cases}}{\sum \text{Positive cases} + \sum \text{Negative cases}}$$ So, $ACC$ can be represented as a weighted average of $TPR$ and $FPR$. If the number of negatives and positives is the same: $$ACC = \frac{TPR + 1 - FPR}{2}$$ But what if $N_- \gg N_+$? Then: $$ACC(N_- \gg N_+) \approx 1-FPR$$ So, in this case, maximal $ACC$ occurs at minimal $FPR$ See this example, negatives outnumber positives 1000:1. data = c(rnorm(10L), rnorm(10000L)+1) lab = c(rep(1, 10L), rep(-1, 10000L)) plot(data, lab, col = lab + 3) tresh = c(-10, data[lab == 1], 10) do.call(function(x) abline(v = x, col = "gray"), list(tresh)) pred = lapply(tresh, function (x) ifelse(data res[order(res$acc),] # acc tpr fpr #12 0.000999001 1.0 1.0000 #11 0.189110889 1.0 0.8117 #9 0.500099900 0.9 0.5003 #2 0.757742258 0.8 0.2423 #5 0.763136863 0.7 0.2368 #4 0.792007992 0.6 0.2078 #10 0.807292707 0.5 0.1924 #3 0.884215784 0.4 0.1153 #7 0.890709291 0.3 0.1087 #6 0.903096903 0.2 0.0962 #8 0.971428571 0.1 0.0277 #1 0.999000999 0.0 0.0000 See, when fpr is 0 acc is maximum. And here's the ROC, with accuracy annotated. plot(sort(res$fpr), sort(res$tpr), type = "S", ylab = "TPR", xlab = "FPR") text(sort(res$fpr), sort(res$tpr), pos = 4L, lab = round(res$acc[order(res$fpr)], 3L)) abline(a = 0, b = 1) abline(a = 1, b = -1) The $AUC$ is 1-sum(res$fpr[-12]*0.1) #[1] 0.74608 The bottom line is that you can optimize accuracy in a way resulting in a bogus model ( tpr = 0 in my example). That's because accuracy is not a good metric, dichotomization of the result should be left to the decision-maker. The optimal threshold is said to be the $TPR = 1-FPR$ line because that way both errors have equal weight, even if accuracy is not optimal. When you have imbalanced classes, optimizing accuracy can be trivial (e.g. predict everyone as the majority class). Another thing, you can't translate most $AUC$ measures to an accuracy estimate like that; see these questions: Area under curve of ROC vs. overall accuracy Accuracy and area under ROC curve (AUC) And most important of all: Why is AUC higher for a classifier that is less accurate than for one that is more accurate?
