[site]: crossvalidated
[post_id]: 330571
[parent_id]: 330554
[tags]: 
Since it seems that you lack some basic understanding of the process behind Bayesian modeling work, let me give you a short summary of the usual workflow: You define the likelihood function for your model, for example: you assume the Bernoulli distribution parametrized by probability of success $\pi$ for the binary variable $Y$. So $Y$ is your data, e.g. series of coin flips: [0, 0, 1, 1, 0, 1, 1] . You assume the prior distribution(s) for your parameter(s), say in our case, we assume beta distribution parametrized by hyperparameters $\alpha$ and $\beta$. Let's say that you assume a uniform prior , i.e. $\alpha = \beta = 1$. Now you described the problems in probabilistic terms and defined your model. Notice that we didn't estimate anything yet, we just defined a set of assumptions about our data and the parameters. Next, what we need to do is to estimate the parameter $\pi$. For this, you can do one of the three things: You can use conjugacy and obtain the closed-form posterior distribution by using pure math, You can use some kind of optimization to find the maximum of the posterior distribution (the mode of the posterior distribution), i.e. use maximum a posteriori (MAP) -- this gives you a point estimate for $\pi$, You can simulate draws from the posterior distribution using Markov Chain Monte Carlo (MCMC) algorithms (e.g. Metropolis-Hastings, Gibbs, or NUTS). By doing so you obtain the samples from the posterior distribution (in this case samples from the posterior distribution of the random variable $\pi$). You can use the samples to approximate the posterior distribution, or to estimate it's properties, e.g. calculate the mean from the samples, to estimate the expected value of the parameter of interest $E(\pi | Y)$ as your point estimate of $\pi$. All the approaches have pros and cons. You can use conjugacy only for the very simple problems, but there is no closed-form solutions for things like multivariate logistic regression, so this won't work for many problems. As about MAP, it gives you only the point estimate and it depends on the optimizer you used, it may not always work for complicated problems that are hard to optimize (recall that when using maximum likelihood we often use dedicated optimizers for complicated problems rather then black-box ones). Simulation is the most commonly used approach in Bayesian estimation, it is slower then the two previous approaches, but it gives you the full posterior distribution. MCMC is the approach of choice for most Bayesians. We use priors to quantify our assumptions about the parameters. They let us to put some out-of-data information into the model. On another hand, the more information does your data convey, the less influential are the priors .
