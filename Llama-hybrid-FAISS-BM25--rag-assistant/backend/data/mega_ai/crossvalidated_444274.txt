[site]: crossvalidated
[post_id]: 444274
[parent_id]: 444270
[tags]: 
I'm assuming you're referring to the word2vec models, in which case the the original paper here references the usage of the word "continuous" in the model name: "We denote this model further as CBOW, as unlike standard bag-of-words model, it uses continuous distributed representation of the context". Since word vectors are elements in $\mathbb{R}^n$ , they are inherently continuous, as opposed to the discrete one-hot representations previously used in NLP. The distributed part of the word vector representations comes from the fact that each word is represented by an array of numbers, meaning that the word meaning is "distributed" among each element of the word vector.
