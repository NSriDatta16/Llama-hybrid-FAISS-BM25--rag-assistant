[site]: crossvalidated
[post_id]: 451070
[parent_id]: 444954
[tags]: 
As per our conversation in the comments section of your original post, it is extremely important to note, that the training data has been very carefully curated at great expense, and thus your unlabelled test examples, which contain missing values for some features, can be assumed to have been "sampled from the same distribution as the training set" For this reason, I would suggest the following if computationally feasible. When making a prediction for any particular data point for which you have access to a subset of the features, you train a model using your training data, but only giving yourself access at train time to the exact set of features which are provided in the data point you wish to label. This does, provided there are enough features and your data can be missing in enough different permutations, mean you can't feasibly pre-train all of the models you might need, so you'll have to train a model every time you want to make a prediction. If this isn't computationally feasible, I'd suggest training one model on all features in your training set, and then filling the missing values in the data point you need to make a prediction for. The simplest way to fill these values is with their mean/median (probably best to use mean if you're using an analytic method like linear regression and median if you're using tree-based methods). That said, you will likely get better results if you fill your missing values based on conditional means (i.e. based on the features that you do have, what are the likely values of those that you don't have). There are many ways of doing this based on deep learning and it's an active field of research, but a more simple way of doing this would be to use KNN. A sketch of how this works: For your data point with some missing features, which you wish to make a prediction for: 1: Calculate its Euclidean distance from all points in your training data, based on the features you do have access to 2: Sort your training data by that feature and select the K closest values 3: For each of the missing features, fill with the mean of these K training instances This is clearly not super computationally efficient, having to calculate the distance between a point, and every training example, before being able to make a prediction with it, but it's likely faster than training a model every time you wish to make a prediction as per my first suggestion, so it's a decent half-way house.
