[site]: datascience
[post_id]: 122144
[parent_id]: 122142
[tags]: 
No they are quite different. The sequence length is the number of tokens that are processed by the transformer together. Each token could be a word embedding, character embedding or some other chunk of natural language encoded into numbers, like pairs of consecutive bytes in byte pair encoding(BPE 1 ). The sequence length thus determines the span of text that the model looks at, at once. The hidden state is a representation generated by the model for every token taking into account the token itself and the tokens present around it (up to sequence length). The representation is typically a small vector of size 32 or 64 etc. For example if we have sequence length of five(word tokens) and we have the following, suitably preprocessed, English sentence: [START]Is[SPACE]sequence[SPACE]length[SPACE]and[SPACE]hidden[SPACE]size[SPACE]is[SPACE]the[SPACE]same[SPACE]in[SPACE]Transformer[END] The 5 word sequence [START]Is[SPACE]sequence[SPACE] will be fed to the model that will generate one hidden state vector for each of the tokens it sees. So a total of five vectors each of some predetermined hidden_state_size get generated. Note that in practice each word in the sequence is encoded using an embedding of some sort so [START]Is[SPACE]sequence[SPACE] will be a list/array of five embedding vectors.
