[site]: crossvalidated
[post_id]: 262743
[parent_id]: 262541
[tags]: 
I wonder if this will answer your question... Suppose you have 100 effects of absolute effect sizes $e_1, e_2, \ldots, e_{100}$, and let their order statistics be $e_{(1)} \le e_{(2)} \le \cdots \le e_{(100)}$. Suppose that 20 of them are significant based on an unadjusted test, so the average effect size of those is $\bar e_S^U$, the average of $e_{(81)}$ through $e_{(100)}$. Similarly, the nonsignificant ones have average effect size of $\bar e_N^U$, the average of $e_{(1)}$ through $e_{(80)}$. Now, consider a multiplicity-adjusted test; its critical value is higher so that fewer effects will be found significant. Suppose there are in fact 10 of them. Then the average effect size of the significant ones based on then adjusted test is $\bar e_S^A$, the average of $e_{(91)}$ through $e_{(100)}$; and the average effect size of the nonsignificant ones is $\bar e_N^A$, the average of $e_{(1)}$ through $e_{(90)}$. Note that $\bar e_S^A \ge \bar e_S^U$, because $\bar e_S^A$ excludes the 10 smallest that are included in $\bar e_S^U$. But also, $\bar e_N^A \ge \bar e_N^U$, because $\bar e_N^A$ includes 10 effects that are larger than any in $\bar e_N^U$. So both averages go up when you use a multiplicity adjustment, even though the average of them all, $\bar e$, is unchanged. This seems counter-intuitive, but there you have it.
