[site]: crossvalidated
[post_id]: 18990
[parent_id]: 18916
[tags]: 
In theory, the scale of your inputs are irrelevant to logistic regression. You can "theoretically" multiply $X_1$ by $10^{10^{10^{10}}}$ and the estimate for $\beta_1$ will adjust accordingly. It will be $10^{-10^{10^{10}}}$ times smaller than the original $\beta_1$, due to the invariance property of MLEs. But try getting R to do the above adjusted regression - it will freak out (won't even be able to construct the X matrix). This is a bit like the cholesky decomposition algorithm for calculating a matrix square root. Yes, in exact mathematics , cholesky decomposition never involves taking square root of negative number, but round off errors, and floating point arithmetic may lead to such cases. You can take any linear combination of your X variables, and the predicted values will be the same. If we take @simone's advice, and using the re-scaled X variables for fitting the model . But we can use the invariance property of MLE to get the beta that we want, after using numerically stable input X variables. It may be that the beta on the original scale may be easier to interpret than the beta on @simone's transformed one. So, we have the transformed $x_{ij}$ ($i$th observation for the $j$th variable), call it $\tilde{x}_{ij}$, defined by: $$\tilde{x}_{ij}=a_{j}x_{ij}+b_{j}$$ @simone's choice corresponds to $a_{j}=\frac{1}{x_{[N]j}-x_{[1]j}}$ and $b_j=\frac{\overline{x}_{j}}{x_{[N]j}-x_{[1]j}}$ (using $x_{[i]j}$ to denote the $i$th order statistic of the $j$th variable, i.e. $x_{[N]j}\geq x_{[N-1]j}\geq\dots\geq x_{[1]j}$). The $a_j$ and $b_j$ can be thought of as algorithm parameters (chosen to make the algorithm more stable and/or run faster). We then fit a logistic regression using $\tilde{x}_{ij}$, and get parameter estimates $\tilde{\beta}_j$. Thus we write out the linear predictor: $$z_i = \tilde{\beta}_0 + \sum_j\tilde{x}_{ij}\tilde{\beta}_j$$ Now substitute the equation for $\tilde{x}_{ij}$ and you get: $$z_i = \tilde{\beta}_0 + \sum_j(a_{j}x_{ij}+b_{j})\tilde{\beta}_j=\beta_0+\sum_jx_{ij}\beta_j$$ Where $$\begin{array}{c c}\beta_0=\tilde{\beta}_0+\sum_jb_{j}\tilde{\beta}_j & \;\;\;\;\;\; & \beta_j=a_j\tilde{\beta}_j \end{array}$$ You can see that theoretically, the parameters $a_j,b_j$ make no difference at all: any choice (apart from $a_j=0$) will lead to the same likelihood, because the linear predictor is unchanged. It even works for more complicated linear transforms, such as representing the X matrix by its principal components (which involves rotations). So we can back-transform the results to get the betas that we want for interpretation.
