[site]: crossvalidated
[post_id]: 479093
[parent_id]: 478758
[tags]: 
In real world, many imbalanced class problems have heavy cost on misclassification. The minority class might be rare, but one occurrence of that class will have really great impact. The minority class is oftentimes "the goal/point" to avoid or to obtain, not "some useless noise class". This is enough to justify resampling: you'll want the algorithm to be able to not misclassify the minority class. Algorithm that sees imbalanced class data will have less information on whether should it classify an observation as the minority or not. In the end, it will often just label them the majority class. My point is this is biasing the data if real world data is going to see less of minority class. In training we are biasing the data by making algorithm see more of it than what it would see in real life. The point of having the algorithm is to use its predictive ability . You will want to have the algorithm predicting correctly, that's it. Whether or not the algorithm sees the data as it is in real life is not the point. If it is the point, say goodbye to feature engineering as well. p.s.: We can stretch this and extrapolate to how humans see imbalanced data. Humans also (kind of) do "resampling/weighting", by remembering more intensely things that are "rare but has great impact", and not the "things that happen everyday and boring". It balances out so the human both remembers "the one thing that happen and changed my life" and "the thing I do everyday, generally".
