[site]: crossvalidated
[post_id]: 95280
[parent_id]: 95247
[tags]: 
Let me add some points to @ttnphns nice list: The Bayes prediction of the LDA's posterior class membership probability follows a logistic curve as well. [Efron, B. The efficiency of logistic regression compared to normal discriminant analysis, J Am Stat Assoc, 70, 892-898 (1975).] While that paper shows that the relative efficiency of LDA is superior to LR if the LDA's assumtions are met (Ref: Efron paper above, @tthnps' last point), according to the Elements of Statistical Learning in practice there is hardly any difference. [Hastie, T. and Tibshirani, R. and Friedman, J. The Elements of Statistical Learning; Data mining, Inference andPrediction Springer Verlag, New York, 2009] That hugely increased relative efficiency of LDA mostly happens in asymptotic cases where the absolute error is practically negligible anyways. [ Harrell, F. E. & Lee, K. L. A comparison of the discrimination of discriminant analysis and logistic regression under multivariate normality, Biostatistics: Statistics in Biomedical, Public Health and Environmental Sciences, 333-343 (1985).] Though I have in practice encountered high dimensional small sample size situations where the LDA seems superior (despite both the multivariate normality and the equal covariance matrix assumptions being visibly not met). [ Beleites, C.; Geiger, K.; Kirsch, M.; Sobottka, S. B.; Schackert, G. & Salzer, R. Raman spectroscopic grading of astrocytoma tissues: using soft reference information., Anal Bioanal Chem, 400, 2801-2816 (2011). DOI: 10.1007/s00216-011-4985-4 ] But note that in our paper the LR is possibly struggling with the problem that directions with (near) perfect separability can be found. The LDA on the other hand may be less severely overfitting. The famous assumptions for LDA are only needed to prove optimality. If they are not met, the procedure can still be a good heuristic. A difference that is important for me in practice because the classification problems I work on sometimes/frequently turn out actually not to be that clearly classification problems at all: LR can easily be done with data where the reference has intermediate levels of class membership. After all, it is a regression technique. [see paper linked above] You may say that LR concentrates more than LDA on examples near the class boundary and basically disregards cases at the "backside" of the distributions. This also explains why it is less sensitive to outliers (i.e. those at the back side) than LDA. (support vector machines would be a classifier that goes this direction to the very end: here everything but the cases at the boundary is disregarded)
