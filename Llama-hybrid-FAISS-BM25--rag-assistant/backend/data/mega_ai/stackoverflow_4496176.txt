[site]: stackoverflow
[post_id]: 4496176
[parent_id]: 4495127
[tags]: 
It looks like it's a problem with your initial guesses; something like (1, 1, 1, 1) works fine: You have p_guess=(np.median(x),np.min(y),np.max(y),.01) for the function def _eNegX_(p,x): x0,y0,c,k=p y = (c * np.exp(-k*(x-x0))) + y0 return y So that's test_data_max e^( -.01 (x - test_data_median)) + test_data_min I don't know much about the art of choosing good starting parameters, but I can say a few things. leastsq is finding a local minimum here - the key in choosing these values is to find the right mountain to climb, not to try to cut down on the work that the minimization algorithm has to do. Your initial guess looks like this ( green ): (1.736, 0.85299999999999998, 3.4889999999999999, 0.01) which results in your flat line (blue): (-59.20295956, 1.8562 , 1.03477144, 0.69483784) Greater gains were made in adjusting the height of the line than in increasing the k value. If you know you're fitting to this kind of data, use a larger k. If you don't know, I guess you could try to find a decent k value by sampling your data, or working back from the slope between an average of the first half and the second half, but I wouldn't know how to go about that. Edit: You could also start with several guesses, run the minimization several times, and take the line with the lowest residuals.
