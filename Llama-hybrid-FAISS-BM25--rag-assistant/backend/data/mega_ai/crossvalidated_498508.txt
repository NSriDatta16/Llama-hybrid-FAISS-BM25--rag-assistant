[site]: crossvalidated
[post_id]: 498508
[parent_id]: 
[tags]: 
Why use tanh function at the last layer of generator in GAN?

While studying GAN, I found out that ReLU activation is used at the intermediate layers, and tanh or sigmoid is used at the last layer of the generator. I'm curious about why sigmoid or tanh is used at the last layer of the generator. Can I use ReLU activation at the last layer of the generator?
