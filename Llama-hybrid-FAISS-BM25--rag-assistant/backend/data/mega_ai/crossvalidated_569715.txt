[site]: crossvalidated
[post_id]: 569715
[parent_id]: 569698
[tags]: 
Unlike in linear regression, there are multiple defendable variants of $R^2$ in logistic regression. For instance, the logistic regression can be evaluated using square loss, same as in an OLS linear regression. To calculate this, take the numerator to be the square loss (Brier score) of the predictions from your model, and take the denominator to be the square loss (Brier score) of the naïve model that always guesses the prior probability (proportion of observations belonging to each class). $$ R^2_{\text{SquareLoss}}= 1-\dfrac{ \text{Your Model’s Brier Score }}{ \text{ Naïve Model’s Brier Score }} $$ Similarly, you can do this with a comparison of how your model performs on log loss (the standard loss function in logistic regression, equivalent to maximum likelihood estimation) vs the naïve model that always guesses the prior probabilities. This sometimes goes by “McFadden’s $R^2$ ”. $$ R^2_{McFadden}=1-\dfrac{ \text{Your Model’s Log Loss }}{ \text{Naïve Model’s Log Loss }} $$ UCLA has a page that describes many other options. Your software does not want to commit to one of these definitions, so it omits any notion of $R^2$ . This is common. For instance, the summary of a logistic glm in R does not mention any $R^2$ values, unlike linear regression as implemented in lm . The way around this is to pick a definition (or several, if you want several) and calculate $R^2$ yourself.
