[site]: stackoverflow
[post_id]: 2036293
[parent_id]: 2035654
[tags]: 
You could "instrument" your code in many different ways, everything from start-up/shut-down events to individual machine instruction execution (using a processor emulator). Of all the possibilities, what's worth doing? Don't just do it for the sake of completeness; have a specific goal in mind. A business case if you like, with a benefit you expect to receive. E.g.: Insight into CPU task execution times/patterns to enable optimisation (if you need to improve performance). Insight into other systems to resolve system integration issues (e.g. what messages is your VoIP box sending and receiving when it connects to a particular peer?) Insight into the nature of errors (for field diagnostics) Aid in development Aid in validation testing I imagine that there's no grand unified theory of logging, because what you do would depend on many details: Quantity of data Type of data Events Streamed audio/video Available storage Storage speed Storage capacity Available channels to extract data Bandwidth Cost Availability Internet connected 24Ã—7 Site visit required Need to unlock a rusty gate, climb a ladder onto a roof, to plug in a cable, after filling out OHS documentation Need to wait until the Antarctic winter is over and the ice sheets thaw Random access vs linear access (e.g. if you compress it, do you need to read from the start to decompress and access some random point?) Need to survive error conditions Watchdog reboots Possible data corruption Due to failing power supply Due to unreliable storage media Need to survive a plane crash As for ASCII vs binary, I usually prefer to keep the logging simple, and put any nice presentation in a PC application that decodes the data. It's usually easier to create a user-friendly presentation in PC software (written in e.g. Python) rather than in the embedded system itself.
