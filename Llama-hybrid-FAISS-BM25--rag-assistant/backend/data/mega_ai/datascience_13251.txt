[site]: datascience
[post_id]: 13251
[parent_id]: 13249
[tags]: 
Likelihood-ratio tests are a mainstay of classical hypothesis testing . The idea is to form the likelihoods of the two hypotheses under consideration, and choose the one with the highest likelihood if their ratio is sufficiently large. Hypotheses come in two flavors: simple, and composite. Simple tests are those for which the hypothesis uniquely defines the distribution; e.g., the mean is that, or the variance is that. If it is not simple, it is composite; e.g., the mean is not equal to something, or the variance is less than something. Generalized likelihood ratio tests apply to composite hypotheses, and the goal is to find the distributions out of all possible options in the hypothesis space that maximize the likelihoods, and consider their ratio. In Bayesian statistics one considers the posterior probabilities instead of the likehoods, and the corresponding ratio is termed the Bayes factor . Supervised/unsupervised learning is something else. I suggest you consult a textbook for worked examples; I think my explanation does not do the subject justice.
