[site]: datascience
[post_id]: 54523
[parent_id]: 54327
[tags]: 
Are you familiar with Dropout Layers? Dropout layers try to break dependencies between different Neurons by randomly leaving out Neurons, such that Neurons are (sometimes) encouraged to learn weights / patterns by themselves. This is just a small description of a very good paper I would recommend you to read: Dropout: A Simple Way to Prevent Neural Networks from Overfitting Also in case you are using python, have a look at Tensorflow dropout respectively Keras dropout as they both also offer some sort of documentation as a further reading.
