[site]: crossvalidated
[post_id]: 639686
[parent_id]: 
[tags]: 
PCA: Treating samples rather than variables as the dimensions to be reduced (will something similar to the transpose trick work?)

I have a matrix of data with approximately 200,000 samples and 30 variables. The variables have been standardized because their original units of measurement are arbitrary/irrelevant. I am interested in eventually clustering the variables , not the samples , but want to do PCA first so I don't have 200,000 dimensions when clustering. I have come across the PCA transpose trick , and I understand that it's for situations when it would result in a smaller correlation matrix (and that in my case it would make the correlation matrix larger). I also understand that the transpose trick is not "just transpose the data and do PCA on that" because you would end up centering the samples, not the variables . (And intuitively "centering the samples" doesn't make sense - it would be like trying to center a person by subtracting the mean of their height, weight, age, and income.) But if I take the general idea of the transpose trick and do PCA using the solution described in the second link to avoid incorrect centering, would give me the result I am trying to get - the ability to calculate PC scores for variables? If I am reading that solution correctly, then if $A_c$ is the data matrix with p=30 variables as columns, n=200,000 samples as rows, and the variables centered or standardized, I would do PCA of $\dfrac{1}{n}{A_c}{A_c}^T$ , giving me an enormous eigenvector matrix (but I think all but 30 would have 0 as the eigenvalue). Main question: Could I then use the selected eigenvectors from that enormous matrix and use them to calculate PC scores for the variables? Calling that eigenvector matrix $V_b$ (b for big), the PC scores for variables would be ${A_c}^{T}{V_b}$ ? Are scores calculated in this way appropriate for use in later analyses such as trying to cluster the variables? Secondary question: I don't actually need to make that enormous matrix, right? Using the transpose trick, if I do PCA the usual way with a 30x30 correlation matrix for variables ( $\dfrac{1}{n}{A_c}^T{A_c}$ ), then if $V_s$ (s for small) is the eigenvector matrix from doing that I can use ${A_c}V_s$ to get the first 30 eigenvectors I would have gotten in $V_b$ . And then the PC scores for my variables would be ${A_c}^T{A_c}{V_s}$ ? I think this is different from a biplot - a biplot would give variables scores/coordinates on axes that come from treating the variables as dimensions to be reduced, and this (hopefully) gives the variables scores/coordinates on axes that come from treating the samples as dimensions to be reduced. Edit: I think there is a flaw in my reasoning - my PCs would not be "axes that come from treating the samples as the dimensions to be reduced" if the samples aren't centered. There would be something like an intercept in there to mess things up. Is there a way to fix or work around that, or is this whole approach flawed? To be clear, my question is now: Is there a better way to accomplish what I was trying to do here? Treat samples as the dimensions to be reduced to make it easier to cluster the variables?
