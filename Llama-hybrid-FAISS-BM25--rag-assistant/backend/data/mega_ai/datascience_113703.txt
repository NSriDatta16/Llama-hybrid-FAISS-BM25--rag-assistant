[site]: datascience
[post_id]: 113703
[parent_id]: 113686
[tags]: 
If the state uncertainty affects the decision making then there are possibly two avenues that you can take: Incorporate the uncertainty in the state and formulate the problem as a Partially Observable Markov Decision Process (POMDP). An example of this is the RockSample domain. Some indicative links that could help you in your search: paper , code+visualization , code+examples+solvers Another approach is to try to solve the exploration vs exploitation dilemma. Mainly if there are a number of different tasks (MDPs) and the agent doesn't know which task is facing it can maintain a belief distribution over tasks and use a solver to solve the task it believes is currently facing. This is the Bayes-Adaptive MDP framework. You would need approximate methods to solve the problem. Aside the traditional POMDP/BAMDP solvers out there, there are also Neural Network approaches (usually model free RL) that solve these types of problems. However if you are not familiar with these problem formulations it is better to use the traditional methods first.
