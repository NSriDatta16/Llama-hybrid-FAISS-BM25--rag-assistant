[site]: crossvalidated
[post_id]: 358301
[parent_id]: 
[tags]: 
Hyperparameters tuning on resampled validation set?

Let's suppose we have trained a neural network on some data set, rather than estimating the hyper-parameters using a basic validation error would not it make more sense to: Generate N validation sets from the original validation set by sampling with replacement. Estimate N validation errors(using the same model) and average them to generate a final validation error. I thought that this estimate would give a better estimate of the real error if the validation set is small?
