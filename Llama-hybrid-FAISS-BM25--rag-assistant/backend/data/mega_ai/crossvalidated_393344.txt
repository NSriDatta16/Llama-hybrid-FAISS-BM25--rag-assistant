[site]: crossvalidated
[post_id]: 393344
[parent_id]: 
[tags]: 
What is the motivation to train one's own word embedding model?

I've been using a few big word embedding models like word2vec & FastText, and they work very well on most problems. I am now adressing a new kind of data, on which they perform quite poorly, and I found out that it is possible to train your own model. Why would you do that ? does it make it more domain-specific ? How much data examples would I need to feed the model to achieve a decent result, knowing that my documents are very short (3-4 words max) ? Is it possible to get an estimation of the training time ? say for a million data examples of 3 words each. Bonus : is continuing the training of a pre-trained model a good strategy ? will it be able to capture the "essence" of the new data even if there are way less data examples than during the first, original training ? Thank you so much !
