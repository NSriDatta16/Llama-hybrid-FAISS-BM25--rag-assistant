[site]: crossvalidated
[post_id]: 280794
[parent_id]: 247842
[tags]: 
How can we get a good but fair decoder hidden state $\hat h_i$ to predict word $\hat w_i$ ? How can it encode as much as possible about the history? Once the language model has been trained the $\hat h_i$ can be got from the previous words $w_0, w_1, ..., w_{i-1}$ where $w_0$ represents the start of sentence token. The history information is encoded in the cell updated by all the previous words. We do not use the predicted $\hat w_i$ and hence will not affect the results, and the character we input into the model at each time step is the ground truth. For example, when we calculating the probability of $w_i$ (for i >= 1) the input is $h_{i-1}$ (which is only affected by $w_0$ to $w_{i-2}$ ), $w_{i-1}$ , and we can get a probability distribution for $w_{i}$ and we just index the probabilty( $p(w_i|w_0, ..., w_{i-1})$ ) using $w_i$ and don't do the others(we don't get the word with max probability $\hat w_i$ ) and input ground truth $w_i$ for the next time step as the input. And the probability of each character is indexed by the ground truth character in the softmax output and the mean perplexity is then calculated by this: \begin{align} \text{Perplexity}(w_1, ..., w_n) &= \sqrt[n]{\frac{1}{\prod_{i=1}^{n} p(w_{i})}} \\ &= 2^{\log_{2}{[\prod_{i=1}^{n} p(w_i|w_0, ..., w_{i-1})]}^{-n}} \\ &= 2^{-\frac{1}{n}\log_{2}{[\prod_{i=1}^{n} p(w_i|w_0, ..., w_{i-1})]}} \\ &= 2^{-\frac{1}{n}\sum_{i=1}^{n}\log_{2}{p(w_i|w_0, ..., w_{i-1})}} \end{align} For the detailed derivation please refer to this article: N-gramLanguage Models .
