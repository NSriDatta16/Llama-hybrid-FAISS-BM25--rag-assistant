[site]: crossvalidated
[post_id]: 307315
[parent_id]: 307302
[tags]: 
In theory, there is nothing a neural network can't approximate. In fact, you only need a single hidden layer! https://en.wikipedia.org/wiki/Universal_approximation_theorem So the answer is definitely YES, it's always possible to achieve perfect or near-perfect performance for any training data set. No matter how small or how big. Have you: Actually looked at the training examples that your network failed? Maybe there's a pattern? It's no good randomly adjust neural network paramters. You said your new dataset is small, why don't you work a bit harder, and pull the training sets that your network can't predict/classify? There should be a reason, maybe you have a bug? Maybe those examples are just non-sense or outliers?? Maybe they are just random noise that your network shouldn't work on it anyway? Maybe you just need more iterations? Please look at those failed examples, don't guess. Machine learning is more than just running the same thing again and again.
