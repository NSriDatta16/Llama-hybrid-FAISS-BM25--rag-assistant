[site]: crossvalidated
[post_id]: 385296
[parent_id]: 
[tags]: 
Why does gradient descent work faster with ReLU compared to using with Signoid?

As far as I understand, Signoid function is used for mapping the outputs of neural network to the values between 0 and 1. Why is using rectified linear unit(ReLU) as activation function in deep neural networks, works faster? Can you please explain the mathematical concept behind it?
