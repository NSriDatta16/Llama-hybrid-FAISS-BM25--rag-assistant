[site]: crossvalidated
[post_id]: 160947
[parent_id]: 152359
[tags]: 
Now I will create two distance matrices. First is association matrix derived from co-occurence data represented by OR-SP index. Observed Roost-Sharing Proportion is calculated for each pair of individuals by dividing the number of days two individuals were found together by the number of all possible days they could be together (overlap bewteen first and last recordngs of both individuals). Regarding this step, I think you would be much more successful if you used Agent-based Modeling (ABM). The approach you describe appears convoluted to me, and ABM code will likely be much simpler -- meaning easier to implement, test, refine, and validate -- and also much more justifiable from a theoretical and empirical point of view. The model you are trying to create is somewhat common within the field of Computational Social Science, which includes ABM as a primary modeling/simulation method. I suggest that you use NetLogo ( https://ccl.northwestern.edu/netlogo/ ). Each of your individuals would be agents in NetLogo . Each time step, each agent executes a program that determines its interaction with the environment, other agents, and changes in its internal state. (If you like, you can also add predator agents, but it sounds like you would like to treat mortality rate as a constant probability each time step, or maybe as a function of age and time.) You can program agent movements in a 2D physical space ("patches"), and therefore you can simulate the process of agents meeting other agents and "roosting" together. They can mate and have babies, too, and passing on their genetic material as internal state for the new agents. Along the way, agents form social network ties, by what ever rules you implement. NetLogo has built-in capabilities for dynamic graphs (i.e. social networks) between agents, and also visualization. After you program your simulation, just run it N times with random initial conditions, where N is chosen to be large enough to give you adequate statistical confidence in your final analysis. You can record data (i.e. your "observations") every T steps to simulate yearly data. You only need to use R for statistical analysis at the end the runs. There is a NetLogo extension to import and export data to R here: https://github.com/NetLogo/NetLogo/wiki/Extensions . You might have reservations about this approach since it involves learning a new system and a new language ( NetLogo has it's own scripting language). However, it's quite easy to learn (many beginners and non-programmers learn it and use it successfully). But the primary benefit is that you are modeling the phenomena of interest in a very direct and natural way which greatly simplifies the task and greatly reduces the chances of error along the way. I compare social distance with genetic distance by Mantel test. Regarding this step, it think it is vital that you first establish that the space of possible social distances is a metric space and that it has characteristics that make it comparable to genetic distance within the space of possible genomes. Just because you have both in matrix form is not sufficient, in my view (though I don't have experience with this particular test). For example, genetic distance = zero means identical genomes, right? But what does "social distance = zero" mean? If zero is undefined for social distance, then it fails the definition of a metric (see: https://en.wikipedia.org/wiki/Metric_space#Definition ). Second, I think you should be measuring change in distances. You have some initial conditions that involve both initial social distances and initial genetic distances. After some number of years, though mating, mortality, and geographic/social mixing, your population has a final set of social distances and genetic distances. The ABM approach makes this more visible. Regarding the Mantel test in particular, you might also evaluate Bayesian alternatives. From the Wikipedia entry: "...the Mantel and partial Mantel tests can be flawed in the presence of spatial auto-correlation and return erroneously low p-values See e.g. Guillot and Rousset, 2013 [3])" A Bayesian approach might be able to avoid this problem and also others associated with Null Hypothesis Significance Testing (NHST). However, I don't have a specific suggestion on a Bayesian approach for this test.
