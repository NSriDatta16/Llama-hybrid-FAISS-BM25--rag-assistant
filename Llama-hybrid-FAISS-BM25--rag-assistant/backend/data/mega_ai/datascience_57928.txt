[site]: datascience
[post_id]: 57928
[parent_id]: 
[tags]: 
Why are bigger embedding vectors not necessarily better?

I'm wondering why increasing the dimension of a word dimension vector in NLP doesn't necessarily lead to a better result. For instance, on examples I run, I see sometimes that using a pre-trained 100d GloVe vector performs better than a 300d one. Why is this the case? Intuitively a larger dimension should become almost like a one-hot encoding and be in some more "accurate," no?
