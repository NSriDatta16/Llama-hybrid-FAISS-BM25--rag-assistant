[site]: crossvalidated
[post_id]: 392888
[parent_id]: 391454
[tags]: 
Considering the size of the Dataset Mentioned by you can use Heterogenous Ensemble, Homogenous Ensemble or Edited Nearest Neighbors for Filtering the noise in the Data. The homogeneous ensemble is inspired by Cross-Validated Committees Filter[1]. At noise levels approximately more than 10% Heterogenous Ensemble performs worse than Homogenous Ensemble. So you should use Homogenous Ensemble to filter the noise. You can use Apache-Spark to implement this model. Here is the algorithm for this method: Input: data consisting of labels and features Input: P= the number of partitions Input: n= Trees the number of trees for Random Forest Input: e= error tolerance (0 Output: the filtered data without noise partitions ← kFold(data, P) filteredData ← NULL for all train, test in partitions do rfModel ← random Forest(train, n Trees) rfPred ← predict(rfModel, test) joinedData ← join(zipWithIndex(test), zipWithIndex(rfPred)) markedData ← map original, prediction ∈ joinedData if label(prediction)-(label(prediction)) *e (label(prediction))*e then original else (label = NULL, features(original)) end if end map filteredData ← union(filteredData, markedData) end for return(filter(filteredData, label != NULL)) [1] [C.E. Brodley, M.A. Friedl. Identifying Mislabeled Training Data. Journal of Artificial Intelligence Research 11 (1999) 131-167 doi: 10.1613/jair.606]
