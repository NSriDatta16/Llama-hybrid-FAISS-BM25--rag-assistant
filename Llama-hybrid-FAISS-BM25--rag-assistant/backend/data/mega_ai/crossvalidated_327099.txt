[site]: crossvalidated
[post_id]: 327099
[parent_id]: 279366
[tags]: 
As @Alex R. already explained in his comment, convolution filters are also weights that get updated during the training. There are several ways how the filters are chosen, and there are two aspects to consider: architecture and weight initialization. Architecture of the convolution filters means how many filters does the network have, how large they should be, how many layers of them. This is mostly driven by current rules of thumb, as I described in this answer . Initialization of the convolution filters means how should they look at the start of the training. There are two main streams: Random initializations (notably "xavier" initialization from Glorot and Bengio 2010 paper , explained nicely in this blog ; and "msra" from the ResNet paper by He et al., 2015.) Transfer learning, domain adaptation (using convolution filters from a network trained on some other task in the same input domain). A list of sources of pre-trained networks can be found in this answer . A more thorough overview of possible intialization schemes from the perspective of their regularization abilities is in my article Regularization for Deep Learning: A Taxonomy , Section 7.
