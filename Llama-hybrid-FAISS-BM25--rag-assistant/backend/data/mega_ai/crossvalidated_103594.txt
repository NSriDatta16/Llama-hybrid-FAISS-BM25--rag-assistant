[site]: crossvalidated
[post_id]: 103594
[parent_id]: 103592
[tags]: 
Well, it depends. Firstly, is the coin fair? Secondly, is each flip independent of all other flips? Most of the time this problem is posed, the assumption is that the coin is fair (Bernoulli, p= 0.5) and the flips are iid (independent, identically distributed). If so, the proper response is that the probability of a the next flip being heads is 0.5. The "problem" with person B's assumption is that s/he is assuming that the four flips are not independent, and that prior flips have some way of "communicating themselves" to the last flip. This is not the case in the real world (we think 8-) ), and so person 2 is probably incorrect. A Bayesian would look at this and say, three flips in a row heads? How certain are you that the coin is fair? To be fair (pun intended), three flips doth not a large sample make, but the Bayesian would start with the prior distribution that the coin is fair, take into account that three flips in a row were heads, and arrive at a new posterior for the coin for the fourth flip. Update Just for fun, the Bayesian would probably use the beta-binomial conjugate family here, starting with a Beta(1, 1) prior to represent uniformity. If so, given $y$ successes (heads) out of $n$ observations, the mean posterior distribution would be: $$ E(\theta\vert y) = \frac{\alpha + y}{\alpha + \beta + n} $$ So a Bayesian using this prior/likelihood assumption would say that there is a whopping 80% chance that the next flip is a heads. Update 2 It is easiest to do the math using the beta/binomial convention. In general, the likelihood of the data under the assumption it is binomially distributed with parameter $\theta$ is: $$ p(y\vert\theta) \propto \theta^a\left(1 - \theta\right)^b $$ Where a and b are the instances of success and failure respectively. Now, the prior distribution of the parameter $\theta$, which represents the "success" rate (or probability of heads) will be assumed beta. Once again conveniently being able to ignore the constants that do not depend on the parameter itself, this can be written as: $$ p(\theta) \propto \theta^{\alpha - 1}\left(1 - \theta\right)^{\beta - 1} $$ So, calling $y$ the number of successes and $n$ the total observations (so failures = $n - yy$), we can write the posterior as proportional to: $$ \begin{align} p(\theta\vert y) &\propto p(y\vert\theta)p(\theta)\\ p(\theta\vert y) &\propto \theta^y(1 - \theta)^{n-y}\theta^{\alpha - 1}(1-\theta)^{\beta-1}\\ &\propto \theta^{y + \alpha - 1}(1 - \theta)^{n-y+\beta-1} \end{align} $$ Well, that last is just a beta with parameters $\alpha +y, \beta + n - y$. Here, we started with a very special beta, with $\alpha = \beta = 1$, which is actually the uniform distribution on (0, 1). We also had $y = n = 3$, so we are left with a beta(4, 1), and given the mean of the beta is $\frac{\alpha}{\alpha + \beta}$ we have a mean expectation for $\theta$, which is the probability of heads after three flips, of $\frac{4}{5}$ or 80\%. Of course, three flips isn't a good sample size from which to make far-reaching conclusions, but it helps.
