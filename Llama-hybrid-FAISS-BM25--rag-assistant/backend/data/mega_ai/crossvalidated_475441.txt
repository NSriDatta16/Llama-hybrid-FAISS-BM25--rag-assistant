[site]: crossvalidated
[post_id]: 475441
[parent_id]: 475386
[tags]: 
When we penalize a machine learning algorithm, we penalize the algorithm for fitting a model that fits the training data tightly. Usually this is done by estimating the training error as the sum of squared errors plus some measurement of the strength of the fit. For LASSO and Ridge Regression, we can choose to penalize larger coefficients of our model as the training data may be suspected of being too influential on the model parameters. For example: Training error = SSE + $\lambda*\Sigma^p_{i=1}\beta_i^2$ Here the $\lambda$ is a tuning parameter for how much we want to account for strong coefficients, but hopefully you can see how larger coefficients ( $\beta$ values) would create a larger training error. Our machine learning algorithm will lead us to pick the model that minimizes our training error. In the case you described, $\lambda$ would be assigned a larger value to penalize the model for overfitting to the training data. The idea is to create a model that doesnâ€™t fit training data close so that the model may better predict the test data.
