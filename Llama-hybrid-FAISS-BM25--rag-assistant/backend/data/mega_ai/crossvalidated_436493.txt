[site]: crossvalidated
[post_id]: 436493
[parent_id]: 158348
[tags]: 
If the functional is in the form $$F[f(x)]=\int\limits_a^bf(x)g(x)dx$$ then $g(x)$ can be learned with a linear regression given enough training functions $f_i(x), ~i=0,\dots,M$ and target values $F[f_i(x)]$ . This is done approximating the integral by a trapezoidal rule: $$F[f(x)]= \Delta x\left[\frac{f_0g_0}{2}+f_1g_1+...+f_{N-1}g_{N-1}+\frac{f_Ng_N}{2}\right]$$ that is $$\frac{F[f(x)]}{\Delta x}=y= \frac{f_0g_0}{2}+f_1g_1+...+f_{N-1}g_{N-1}+\frac{f_Ng_N}{2}$$ where $$f_0=a,~f_1=f(x_1),~...,~f_{N-1}=f(x_{N-1}),~f_N=b,$$ $$a Suppose we have $M$ training functions $f_i(x),~i=1,\dots,M$ . For each $i$ we have $$\frac{F[f_i(x)]}{\Delta x}=y_i= \frac{f_{i0}g_0}{2}+f_{i1}g_1+...+f_{i,N-1}g_{N-1}+\frac{f_{iN}g_N}{2}$$ The values $g_0,\dots, g_N$ are then found as a solution of a linear regression problem with a matrix of explanatory variables $$X=\begin{bmatrix} f_{00}/2 & f_{01} & \dots & f_{0,N-1} & f_{0N}/2 \\ f_{10}/2 & f_{11} & \dots & f_{1,N-1} & f_{1N}/2 \\ \dots & \dots & \dots & \dots & \dots\\ f_{M0}/2 & f_{M1} & \dots & f_{M,N-1} & f_{MN}/2 \end{bmatrix}$$ and the target vector $y=[y_0,\dots,y_M]$ . Let's test it for a simple example. Suppose, $g(x)$ is a Gaussian. import numpy as np def Gaussian(x, mu, sigma): return np.exp(-0.5*((x - mu)/sigma)**2) Discretize the domain $x \in [a,b]$ x = np.arange(-1.0, 1.01, 0.01) dx = x[1] - x[0] g = Gaussian(x, 0.25, 0.25) Let's take sines and cosines with different frequencies as our training functions. Calculating the target vector: from math import cos, sin, exp from scipy.integrate import quad freq = np.arange(0.25, 15.25, 0.25) y = [] for k in freq: y.append(quad(lambda x: cos(k*x)*exp(-0.5*((x-0.25)/0.25)**2), -1, 1)[0]) y.append(quad(lambda x: sin(k*x)*exp(-0.5*((x-0.25)/0.25)**2), -1, 1)[0]) y = np.array(y)/dx Now, the regressor matrix: X = np.zeros((y.shape[0], x.shape[0]), dtype=float) print('X',X.shape) for i in range(len(freq)): X[2*i,:] = np.cos(freq[i]*x) X[2*i+1,:] = np.sin(freq[i]*x) X[:,0] = X[:,0]/2 X[:,-1] = X[:,-1]/2 Linear regression: from sklearn.linear_model import LinearRegression reg = LinearRegression().fit(X, y) ghat = reg.coef_ import matplotlib.pyplot as plt plt.scatter(x, g, s=1, marker="s", label='original g(x)') plt.scatter(x, ghat, s=1, marker="s", label='learned $\hat{g}$ (x)') plt.legend() plt.grid() plt.show() The Gaussian function is successfully learned although the data are spread somewhat around the true function. The spread is larger where $g(x)$ is close to zero. This spread can be smoothed with a Savitzky-Golay filter from scipy.signal import savgol_filter ghat_sg = savgol_filter(ghat, 31, 3) # window size, polynomial order plt.scatter(x, g, s=1, marker="s", label='original g(x)') plt.scatter(x, ghat, s=1, marker="s", label='learned $\hat{g}$ (x)') plt.plot(x, ghat_sg, color="red", label='Savitzky-Golay $\hat{g}$ (x)') plt.legend() plt.grid() plt.show() In general, $F[f(x)]$ does not depend linearly on $f(x)$ , that is $$F[f(x)]=\int\limits_a^b\mathcal{L}\left(f(x)\right)dx$$ It is still can be written as a function of $f_0, f_1\dots,f_N$ after discretizing $x$ which is also true for the functionals of the form $$F[f(x)]=\int\limits_a^b\mathcal{L}\left(f(x),f'(x)\right)dx$$ because $f'$ can be approximated by a finite differences of $f_0, f_1\dots,f_N$ . As $\mathcal{L}$ is a non-linear function of $f_0, f_1\dots,f_N$ , one may attempt to learn it with a non-linear method, e.g. neural networks or SVM, although it will probably not be so easy as in the linear case.
