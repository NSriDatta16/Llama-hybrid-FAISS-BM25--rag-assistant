[site]: crossvalidated
[post_id]: 324557
[parent_id]: 323450
[tags]: 
I think you'd call this SGD with multiple restarts (see here for people doing that kind of stuff) . I'm going to assume you're talking about running SGD in the context of training a deep neural network. The problem is doing one run of SGD until it has converged is often sufficiently computationally expensive that you're likely to get some pretty serious diminishing returns. In general if your optimisation problem is not convex then you're not even guaranteed to converge to a minima, just a stationary point. I think there is some evidence that stochastic component of SGD allows it to avoids saddle points. There is also some evidence that local minima for linear deep neural networks are close to the global minima. but I am not aware of any evidence that suggests high dimensionality means you're less likely to get stuck in a local minima for nonconvex problems.
