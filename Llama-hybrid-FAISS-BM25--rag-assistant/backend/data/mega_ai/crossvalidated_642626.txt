[site]: crossvalidated
[post_id]: 642626
[parent_id]: 
[tags]: 
Model choice based on test/train/validation split

My question is very simple, but no matter where I look it up, it seems that I get another answer. Take a simple classification task. Let's say I trained a kNN, LDA and logistic regression on it for simplicity. For kNN I already determined the optimal number of neighbors (hyperparameter) via cross validation. Then what do? Considering the book I am currently reading, I should only touch the test data once. So, would I run each model on the test data once and just pick the one, which does best on it? I mean chances are big that one model does better than the other by accident? So I would be tempted to repeat the processs and taking some kind of average (just like picking the hyperparamter by crossvalidation). But since that would be contradicting to the what I read so far. I did some research on the internet and also watched some videos on this topic. Bur it seems like every other person has another opinion. Can you describe what is the best to be done or point me to some state of the art resource?
