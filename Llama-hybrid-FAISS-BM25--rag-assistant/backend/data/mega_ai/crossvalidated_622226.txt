[site]: crossvalidated
[post_id]: 622226
[parent_id]: 622221
[tags]: 
My interpretation of the statement is the following: If you fix the data, the likelihood $P(B|A)$ tells you the most likely parameters that generated the data. However, once you have estimated the parameters, you have a fully specified distribution from which you can sample synthetic data. In the Bayesian framework, the prior $P(A)$ allows us to treat the parameter $\theta$ as a random variable, injecting information into the estimation which no longer depends only on the observed data (likelihood). So when you have the trained $P(A|B)$ , you have a generative model. If the posterior has a nice closed form, like when you use conjugate priors, you have a new distribution that factors in the data and the prior knowledge, updating the parameters and giving you a distribution of them. Otherwise, you resort to numerical methods (like MCMC) to get the solution. In both cases, when you have the parameters' distribution, you can plug them in and get the data that the modeled process would generate. If everything is correct, the generated and real data should be compatible, indicating that your model reproduces reality correctly. I hope this helps
