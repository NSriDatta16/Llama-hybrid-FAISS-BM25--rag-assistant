[site]: crossvalidated
[post_id]: 413239
[parent_id]: 278623
[tags]: 
The following idea from this awesome Stanford lecture about generative models was very helpful to me in understanding the sample generations in GANs: Deep Neural Networks (DNNs) are great in learning mappings from an input X to an output Y but are deterministic, hence cannot generate new samples. To overcome this, we sample random noise from a simple probability distribution and use a DNN to learn a transformation from the simple noise distribution to the complex (training) data distribution.
