[site]: crossvalidated
[post_id]: 562623
[parent_id]: 
[tags]: 
Continuous Bag of Words NY Time Corpus

I am working to implement the continuous bag of words approach on the New York Times corpus dataset. However, I am getting word embeddings that do not seem very useful based on a few examples of measuring similarities between words and and performing an analogy. But I am only using perhaps 10% of the total corpus as it takes a long time to train. I am not sure what is wrong, perhaps I am not training on enough data? I imagine, based on the way it works, the model will need to see all these words in different context to add more structure to the embedding. So then I tried to train on the whole corpus, but now it seems my hidden dimension (128) is too short (whole corpus has 125k vocab words). I say because the accuracy improvements are slowing down way too much. I don't think it'll improve past 40% prediction accuracy. My main question is how should I think about fitting the CBOW to the NY Times Corpus Dataset to get reasonable embeddings for clustering, similarities, and analogies? Any advice is appreciated (but I am not specifically asking for advice on data or debugging). Thank you.
