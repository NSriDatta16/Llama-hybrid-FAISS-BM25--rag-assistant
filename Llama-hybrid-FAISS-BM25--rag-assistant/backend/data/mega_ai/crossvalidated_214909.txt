[site]: crossvalidated
[post_id]: 214909
[parent_id]: 
[tags]: 
Basic RNN not converging to zero

I'm writing a basic RNN in Java using JBLAS. My function for generating the hidden state is: tanh(xU + sW) Where x is the input vector and s is the previous hidden state vector. U and W are both the parameters that are being adjusted during backprop. For modifying U and W I use: U += dU * expectedValue * stepSize W += dW * expectedValue * stepSize Where stepSize is about 0.01, though I've tested lots of smaller values. The expectedValue is the same for both and it is just the value of the function I am trying to learn for testing. For the cost function to determine how close my estimations are, I am using the mean squared error function: 1/n * (expectedValue^2 - predictedValue^2) My cost function is not converging to zero over 10,000,000 iterations. Am I screwing up some of the math somewhere? I am not using BPTT. I run the backprop every iteration over a single iteration.
