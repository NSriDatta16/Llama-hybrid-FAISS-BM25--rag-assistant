[site]: crossvalidated
[post_id]: 387123
[parent_id]: 387118
[tags]: 
The question itself is asked on very high level of abstraction, so I try to answer it on such a high level of abstraction: You could decrease the size of your vector by decreasing the number of features you use for your machine learning models. If you use PCA/SVD to decrease the dimensionality of your data it will consume less memory. You can also look into feature selection algorithms. You can decrease the size of the training/validation/testing sets. This does not decrease the memory usage but will decrease the runtime even if you have limited usage on sites like kaggle(Although as far as I know you have 6h, while not enough to run very deep NN's or other training intensive models, for learning this is already quite a lot) Use paid webservices like AWS(Amazon web services). This will cost you depending on how you want to use them, but they can scale as big as you need them to be.
