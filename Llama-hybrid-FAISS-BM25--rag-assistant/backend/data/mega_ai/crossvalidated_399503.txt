[site]: crossvalidated
[post_id]: 399503
[parent_id]: 
[tags]: 
Should loss function be defined over output or parameters?

In machine learning loss is usually defined over the actual output and the predicted output $L(Y,\hat{Y}(X))$ , while in statistics it's defined in the parameter space $L(\theta,\hat{\theta}(X))$ . Why? I assume one reason is that we only assume parametric models in statistics in this case while the ML loss is more general and covers both parametric and non-parametric cases. Is there any other reason?
