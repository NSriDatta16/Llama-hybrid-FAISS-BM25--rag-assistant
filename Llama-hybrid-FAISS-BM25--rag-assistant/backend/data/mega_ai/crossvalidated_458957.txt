[site]: crossvalidated
[post_id]: 458957
[parent_id]: 392009
[tags]: 
From a practical point of view I would guess that people realized that presumably different distributions have astonishingly similar properties so that they searched for a 'common root cause' of these properties. Then they started to 'fuse' single pairs of these 'presumably different' distributions and figured out that the 'root cause' for the similarities is that they surprisingly arise from the same family just with different parameters. So they continued in that way and fused and fused until they arrived at the stage today: the exponential family. It is consistent in an important sense: if I do not misinterpret the wikipedia page then it means that the family stays consistent under using the Bayesian framework, i.e. if one chooses the prior correctly then the posterior of a member of the exponential family is again a member of the exponential family... Now this property also holds for smaller families so you might ask: why did people stop here, i.e. why is the exp. family the 'right thing' or: why don't we enlarge it? This hints at the other questions: They maximize the entropy w.r.t. a reference measure dH, i.e. take some measure dH (probably it needs to satisfy mild assumptions) and take some other random variables T_1,...,T_n (also satisfying some mild conditions like that they are integrable and so on) then what is the measure dF such that this crazy integral in 'Maximum entropy derivation' in the wikipedia page with the additional constraint that the expected value of the T_i w.r.t. that measure dF is some fixed value t_i is a member of the exp. family that you construct from this input data. These are the equations that you search for (they are not easy though!)... Maximum Entropy means 'contains the most information' somewhat...
