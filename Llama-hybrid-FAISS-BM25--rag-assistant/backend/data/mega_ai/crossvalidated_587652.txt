[site]: crossvalidated
[post_id]: 587652
[parent_id]: 
[tags]: 
loss function for supervised anomaly detection in time series

I have a supervised anomaly detection problem in a time series data, which the dataset has three columns: datetime value(a float number) label(1 for anomaly, 0 for normal) It's common that the labels are imbalanced and anomalies are much fewer than normal samples. According to explanations said above, how can I predict labels(0/1 outputs) for the test data? What appropriate loss function can I define for it?
