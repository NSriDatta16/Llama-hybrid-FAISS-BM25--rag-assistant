[site]: crossvalidated
[post_id]: 263545
[parent_id]: 261745
[tags]: 
tl;dr: Logistic regression is always biased in the sense that $E[\hat \beta]$ does not exactly equal the true log odds ratio (relevant CV answer ). But, you don't have to worry about aliasing , e.g. one effect completely masking another. Is LR biased here? Yes; the estimates are nonlinear functions of the data and shit hits the fan from there. Check out the question I linked to in the tl;dr. I will continue to pontificate because there is more to your question than that. A smaller model on a subset yields the same results. Why? Your coeffs stay the same because of the particular way that glm() sets up your regression. To spoil the punchline, what you're calling a main effect is arguably not a main effect -- it's the population A effect, even in the full model. To see why, print the model matrix. > my_glm = glm(Y~population+Var1+Var2+population*Var1+population*Var2, data=X, family = binomial) > model.matrix(my_glm) (Intercept) population Var1 Var2 population:Var1 population:Var2 1 0 300 500 0 0 1 0 300 2500 0 0 1 0 500 500 0 0 1 0 500 2500 0 0 1 1 500 500 500 500 1 1 500 2500 500 2500 1 1 1000 500 1000 500 1 1 1000 2500 1000 2500 You'll see that the upper right quadrant is all zeroes, while the bottom right and bottom left quadrants are identical. (EDIT: this is more clear if you move the population column two spaces to the right.) This is a perfect linear-algebraic storm for two reasons: fitted values of the first four observations are produced only from the "main effect" coefficients. even if the main effects help or hinder the fitted values of the last four observations, this can be exactly mimicked (or undone) by adjusting the interaction coefficients. The result is that the "main effects" are free to fit themselves to population A , while the interactions make up the difference between the populations . This annoying sonofabitch is called corner-point parameterization . A simpler version arises when your model says $\hat{Y_1} = \beta_0$ and $\hat{Y_2} = \beta_0 + \beta_1$. In this case, the model matrix is $\begin{bmatrix} 1, 0 \\1, 1\end{bmatrix}$, and $\beta_0$ reflects the first observation only. I personally would rather use parameters with a "grand mean" $\beta_0 + \beta_1/2$ and an "interaction" equaling $\beta_1/2$ that gets subtracted in one case and added for another, but it's your choice about what inference to make. The predictions will stay the same. Are there multiple indistinguishable effects? I'm answering this because I think it's what you mean when you ask about bias. The answer is no. Your covariate matrix has full column rank, so no column is a multiple of the others and no change in one coefficient can be perfectly mimicked by tweaking the others. In other words, your model is identifiable , and there is no aliasing present. > tol = 10^-5 > sum(svd(MM)$d > tol ) == ncol(MM) TRUE
