[site]: crossvalidated
[post_id]: 403110
[parent_id]: 403074
[tags]: 
You are facing the issue that you have a composition of two functions, $f(\cdot)$ being your known part of the neural network and $g(\cdot)$ being the unknown part. The output is then $g(f(x))$ and there is some loss function $\mathcal{L}(g(f(x)), y)$ comparing the network output to the correct answer. To train the network, you need to estimate how much each of the weights contributed to the error, which is done by computing the derivative $\frac{\partial \mathcal L}{\partial w}$ . Since you don't know the analytical form of $g(\cdot)$ , you cannot analytically compute this derivative. That leaves you with two options: Numerical differentiation : instead of computing the derivative analytically, you can perturb a weight $w$ by a small amount and see how it affects $\mathcal L$ . Note that if you have many weights, this approach will be extremely slow. Approximate $g(\cdot)$ : If you can evaluate $g(\cdot)$ , nothing prevents you from creating another neural network $\hat g(\cdot)$ and train it to mimic $g$ simply by feeding random inputs to $g$ and training $\hat g$ to predict the same thing. Then you can use this new neural network as a surrogate for training $f$ which will allow you to evaluate the gradients analytically. Either way is somewhat impractical.
