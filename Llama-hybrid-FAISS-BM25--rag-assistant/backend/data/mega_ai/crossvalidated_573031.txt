[site]: crossvalidated
[post_id]: 573031
[parent_id]: 443457
[tags]: 
"Significant correlation" would usually mean that you tested a null hypothesis that $\rho=0$ . Depending on your sample size, such correlation may still be quite close to zero. Why would you drop variables that have a very small correlation? Moreover, correlation measures a linear relationship between variables. Below you can see examples of correlation coefficients for different variables, including ones with non-linear relations. Say that you have three features $A$ is correlated with $B$ , $B$ is correlated with $C$ , but not with $A$ , where by "is correlated" I mean here that they are higher than some arbitrary threshold. Which ones would you drop? If you drop $C$ first, you'll need to drop also $B$ , but if you drop $B$ first, you will leave $C$ . Or maybe you drop $A$ and $B$ , or $A$ and $C$ ? Say that you want to predict if someone is going to be arrested, you have two features: "has a tattoo" and "spend time in prison" that are correlated, would it be reasonable to drop "spend time in prison" because it is correlated with "has a tattoo"? Obviously, the causal relation is the other way around and having a tattoo is only a proxy for measuring the latter feature. Another problem is that with looking at the correlation matrix you consider only pairs of features. It can be the case that two (or more) features are not enough if you consider each of them separately, but they interact, so taken together they would be meaningful. In simple models like linear regression, you would need to consider interactions explicitly, but many machine learning models would learn the interaction by themselves. Looking at individual correlations you may accidentally drop such features. If you have many features, you can use regularization instead of throwing away data. In some cases, it will be wise to drop some features, but using something like pairwise correlations is an overly simplistic solution that may be harmful.
