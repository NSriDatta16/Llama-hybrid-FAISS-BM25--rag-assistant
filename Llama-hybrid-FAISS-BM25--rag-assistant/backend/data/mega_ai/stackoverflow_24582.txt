[site]: stackoverflow
[post_id]: 24582
[parent_id]: 3255
[tags]: 
Small reminder: the big O notation is used to denote asymptotic complexity (that is, when the size of the problem grows to infinity), and it hides a constant. This means that between an algorithm in O(n) and one in O(n 2 ), the fastest is not always the first one (though there always exists a value of n such that for problems of size >n, the first algorithm is the fastest). Note that the hidden constant very much depends on the implementation! Also, in some cases, the runtime is not a deterministic function of the size n of the input. Take sorting using quick sort for example: the time needed to sort an array of n elements is not a constant but depends on the starting configuration of the array. There are different time complexities: Worst case (usually the simplest to figure out, though not always very meaningful) Average case (usually much harder to figure out...) ... A good introduction is An Introduction to the Analysis of Algorithms by R. Sedgewick and P. Flajolet. As you say, premature optimisation is the root of all evil , and (if possible) profiling really should always be used when optimising code. It can even help you determine the complexity of your algorithms.
