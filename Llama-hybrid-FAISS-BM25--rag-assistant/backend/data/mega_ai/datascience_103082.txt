[site]: datascience
[post_id]: 103082
[parent_id]: 25717
[tags]: 
I answer a similar question here https://stats.stackexchange.com/a/548006/226796 Hope that it helps you to understand the essence better. Below is my answer. The two forms of rewards are equivalent in the following sense. Now suppose you have an offline dataset consist of s1,s2,..,sk with reward r1,r2,...,rk. Based on the data, you can estimate the MDP model with transition probability T(s,a,s') and R(s,a,s'). You can also estimate the MDP model to be T(s,a,s') and R(s,a). Solve these two MDP models theoretically, you should obtain the same results of policy and value. The above is model-based learning. You can also use the Q-learning method by using the offline dataset in an online fashion (assume that you observe sk,rk sequentially). Obviously, the estimated Q-matrix Q^t(s,a) would be different during the learning as the two forms of reward are different. However, you should obtain the same results of Q(s,a) when it converges.
