[site]: crossvalidated
[post_id]: 590058
[parent_id]: 
[tags]: 
Model performance graph is oscillating?

I've seen a fair amount of information online regarding the loss graph of a deep learning model oscillating, but not so much about the performance graph. In general, when I "properly" train a neural network, the performance usually increase and then plateaus. However, I've noticed that in some cases the performance itself is very shaky. What is the cause of such behavior and what could be a fix? One thing that comes to mind is that decreasing the learning rate may help because the variance seems high, but I'm not sure if this is based on any proof. Any tips are appreciated. Thanks. Edit Here's an example of what I'm talking about. I would expect that the macro-F1 score should also display similar patterns to micro-F1 despite being lower. However, it's oscillating quite a bit.
