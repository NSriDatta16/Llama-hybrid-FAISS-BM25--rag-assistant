[site]: crossvalidated
[post_id]: 585712
[parent_id]: 278985
[tags]: 
Assumption of LDA •Each feature/column in the dataset is Gaussian distribution in simple words data points are normally distributed having bell-shaped curves. •Independent variables are normal for each level of the grouping variable. •Predictive power can decrease with an increase in correlation between variables. •All instances are assumed to be randomly sampled and scores on one variable are assumed to be independent. It is observed that linear discriminant analysis is relatively robust to a slight variation on all of the above assumptions. It is sometimes recommended to apply dimensionality reduction before using LDA (I suppose, it concerns PCA as unsupervised, - though it seems strange for me). But LDA itself is a supervised classification technique in Machine Learning as so as -> Under LDA we basically try to address which set of parameters can best describe the association of groups for a class, and what is the best classification model that separates those groups. as a result, starting from population/sample with multiple features, after applying LDA you're getting several groups rather similar by certain feature_values. Nota bene: it is Supervised (targeting classes are known beforehand) as opposed to PCA. -> here under title 'What is the difference between LDA and PCA for dimension reduction' is comparison, reference and reminding " that LDA makes assumptions about normally distributed classes and equal class covariances (at least the multiclass version) " So, yes it can be considered to be dimensionality reduction method, but as always in statistics you should care about the distribution of samples, nature of features (& targets if exists) [numerical or categorical] & purpose of this analysis application . See wiki LDA : continuous independent variables and a categorical dependent variable (i.e. the class label) But ML, in general care little about the shape of distribution - but of course it decreases learning scores (With the Normality of data distribution - you're getting better scores). Also ML do not care (as I've noticed) about Heteroscedasticity -- but this can be explained by the use of maximum-likelihood estimator in ML (that takes into consideration weights - thus not pure OLS used for getting mean, but more resembles WLS-like estimations.Though nevertheless ML often requires scaling/standartization anyway. For such a case better to see manual for the library - though it is not rich for details)
