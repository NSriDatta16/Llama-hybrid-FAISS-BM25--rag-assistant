[site]: crossvalidated
[post_id]: 394232
[parent_id]: 
[tags]: 
What are "good" features for feature selection?

I use sklearn's random forest for classification (two categories). The following code is used for "good" feature selection. classifier = RandomForestClassifier() classifier.fit(feature_vectors, target) model = SelectFromModel(classifier, prefit=True, threshold=0.00001) masks = model.get_support(True).tolist() The features are binary, meaning either 1 (have) or 0 (not have). For the result, I do not quite understand. If any sample has Feature 1, it must be in category A. However, only very limited samples have Feature 1. A lot of samples have Feature 2, both in category A and category B, almost equally. For the result, I noticed that Feature 2 is selected, and Feature 1 is not. For a good feature, shouldn't Feature 1 is better than Feature 2 ?
