[site]: datascience
[post_id]: 74108
[parent_id]: 65968
[tags]: 
I've got the same issue today, and it's a shame your post got no answers. I think this question is not well addressed in the sklearn documentation. I can show you my workaround to this issue: headers = X.columns.values X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) empty_train_columns = [] for col in X_train.columns.values: # all the values for this feature are null if sum(X_train[col].isnull()) == X_train.shape[0]: empty_train_columns.append(col) print(empty_train_columns) The idea is to keep all your column names, and after you split your data check which of them completely empty in your training set. If I'm not wrong the Imputer respects the column order so, for example, you can correlate every feature with its importance if you are using Decision-Tree-based models. I'm not satisfied with this ugly piece of code but I couldn't find a more elegant (and simple) solution.
