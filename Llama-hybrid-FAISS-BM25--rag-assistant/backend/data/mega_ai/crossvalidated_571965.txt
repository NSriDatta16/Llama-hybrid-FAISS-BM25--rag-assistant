[site]: crossvalidated
[post_id]: 571965
[parent_id]: 571804
[tags]: 
For the usual set-up of the No Free Lunch theorem there's less here than meets the eye. The domain ${\cal X}$ is discrete, ${\cal F}$ is the (finite) set of all (deterministic) functions from ${\cal X}$ to $\{0,1\}$ such that $f(x_i)=y_i$ for data in the training set. That is, for every $x$ not in the training set, ${\cal F}$ contains exactly as many functions with $f(x)=1$ and $f(x)=0$ , and that's why better-than-chance prediction is impossible. In the question as given it's not clear what is random in the expectation, but the basic argument will be the same whatever it is. If you want the average performance of $h$ over all possible test data (or, equivalently, over a random sample of test data drawn with equal probability from all possibilities), then you will be accurate exactly half the time with a binary $Y$ . This is true regardless of your algorithm for choosing $h$ : whatever $h$ is, for any given $x$ either $h(x)=1$ or $h(x)=0$ and each option is wrong exactly half the time. The No Free Lunch theorem isn't actually very useful because we never actually care about the average performance with equal weight on all possible test data. It's mainly interesting because it shows you can't possibly have a proof that an algorithm A_1 is uniformly better than another algorithm A_2 (only that it's better on some interesting subclass of problems)
