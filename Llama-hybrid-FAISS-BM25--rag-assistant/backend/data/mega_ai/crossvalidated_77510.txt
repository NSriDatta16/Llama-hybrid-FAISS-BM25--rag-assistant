[site]: crossvalidated
[post_id]: 77510
[parent_id]: 77508
[tags]: 
From a machine learning perspective, 257 dimensions is far from high dimensional. A wide range of problems, including text classification, are solved in thousands of dimensions. Currently, problems that are considered high dimensional are in millions of dimensions. Both Gaussian processes and SVMs are part of a larger class of algorithms called kernel methods . The combination of the kernel trick , the representer theorem and proper use of regularization make these methods robust against the curse of dimensionality. Kernel methods always work on distances between points, regardless of the dimensionality in which this distance is defined. The dimensionality can be infinite, for example with the Gaussian kernel (see e.g. slide 11 of this presentation by Chih-Jen Lin ). From the representer theorem we know that the solution of any kernel method can be written in terms of instances, and is therefore limited in dimensionality and complexity.
