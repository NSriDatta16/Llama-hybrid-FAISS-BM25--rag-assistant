[site]: datascience
[post_id]: 94259
[parent_id]: 93943
[tags]: 
Question 1: Decision trees (and therefore random forests) allow weighted samples: you can provide different positive real-valued weights for each row in your training data, and the loss function cares more about the higher-weighted rows. When you use sample weights, you need to adjust all of your calculations, and so "number of samples" becomes "total sample weight" or "weighted number of samples", etc. (Note that in the formula in question, the impurities are also calculated using sample weights.) Question 2: You've followed this up at Proof that Gini Impurity in a Decision Tree is Monotone Decreasing? Edit and comments: To follow up on the comments tracing the source code: when you get into the actual tree construction, you get into cython and things get harder to track down. But in this case it's easy enough to just test things out; using the iris dataset, without weights, n_node_samples is the same as weighted_n_node_samples : clf = DecisionTreeClassifier(max_leaf_nodes=3, random_state=0) clf.fit(X, y) print(clf.tree_.n_node_samples) # [150 50 100 54 46] print(clf.tree_.weighted_n_node_samples) # [150. 50. 100. 54. 46.] And with weights (in this case using class weights, which is equivalent to sample-weighting the rows of each class separately), nodes get affected depending on what their class mix is like: wclf = DecisionTreeClassifier(max_leaf_nodes=3, random_state=0, class_weight={0: 1, 1: 2, 2: 1}) wclf.fit(X, y) print(wclf.tree_.n_node_samples) # [150 50 100 54 46] print(wclf.tree_.weighted_n_node_samples) # [200. 50. 150. 103. 47.] (Specifically, at the root node there are all 150 rows, but the 50 of those that are class 1 get counted double toward the weighted number of samples. At node 5 there are 46 rows, one of which is class 1 and so gets counted double.
