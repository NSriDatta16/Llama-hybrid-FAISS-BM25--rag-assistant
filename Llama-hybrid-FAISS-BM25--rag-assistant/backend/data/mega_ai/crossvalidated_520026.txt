[site]: crossvalidated
[post_id]: 520026
[parent_id]: 
[tags]: 
What code lengths can optimal prefix codes assign to the symbols in a given probability distribution?

(Notation) Consider a finite alplhabet $\Sigma\equiv \{x_1,...,x_n\}$ , corresponding to a probability distribution $\{p_1,...,p_n\}$ . I want to encode this using a uniquely decodable binary code. Let $\ell_i$ denote the length such code assigns to the $x_i$ . (Context) From Shannon's theorem, we know that the average expected word length, in the ideal case, equals $H(P)\equiv \sum_i p_i \log(1/p_i)$ . More generally, if an encoding is optimal, then the expected word length $\mathbb E[S]$ is such that $$H(P) \le \mathbb E[S] \le H(P) + 1.\tag1$$ We know that Huffman's is an example of an optimal uniquely decodable binary code, and thus must give an expected word length satisfying (1). (Question) I suppose we should therefore expect that, for any given distribution, Huffman's coding assigns lengths such that $\ell_i\sim \log(1/p_i)$ . In general, however, this will not be an identity. Is there a way to make this statement more precise? In other words, can we assign precise bounds to the lengths $\ell_i$ that an optimal code such as Huffman's associates to a given probability distribution? Or even better, is there a way to estimate the lengths $\ell_i$ that are actually assigned by an optimal encoding to a given probability distribution? (More precise formulation of the question) To be more precise, I am asking about whether there are "easily computable" functions $m_i(\vec p),M_i(\vec p)$ such that an optimal coding of the probabilities $\vec p\equiv(p_1,...,p_n)$ produces lengths $\ell_i$ such that: $$m_i(\vec p) \le \ell_i \le M_i(\vec p).$$ What "easily computable" means is left vague, but I'm thinking about nontrivial bounds: e.g. , computing $m_i,M_i$ must not involve passing through the Huffman coding procedure itself, and the bounds should be telling us something useful (something like $m_i(\vec p)> 0$ would be an example of a not-so-useful bound). Not knowing much about the feasibility of this, I'm not sure what more to ask about such bounds. Things that come to mind are: whether there are bounds that only depend on the entropy of the distribution (that would amount to ask for $m_i(H),M_i(H)$ such that $m_i(H(\vec p))\le \ell_i(\vec p)\le M_i(H(\vec p))$ for any $\vec p$ with a given entropy). bounds that only depend on the individual probabilities: some $m_i=m_i(p_i)$ and $M_i=M_i(p_i)$ such that $m_i(p_i)\le \ell_i(\vec p) \le M_i(p_i)$ for any probability distribution $\vec p$ .
