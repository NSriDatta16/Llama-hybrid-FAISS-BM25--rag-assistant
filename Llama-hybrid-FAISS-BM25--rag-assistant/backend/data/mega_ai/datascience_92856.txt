[site]: datascience
[post_id]: 92856
[parent_id]: 
[tags]: 
The reason behind using a pre-trained model?

These last month I have been studying all about word embeddings and the most known pre-trained word embeddings, Word2Vec, GloVe, FastText, etc. I have read many times how important It is to take advantage of pre-trained models when doing a given task however I don't understand how a pre-trained model can adapt to my given corpus. Furthermore, If I have new words not present in the pre-trained model will I be able to use this pre-trained model to learn the embeddings for the new words?
