[site]: datascience
[post_id]: 123998
[parent_id]: 
[tags]: 
Are "textbook backpropagation" still relevant?

The above backpropagation algorithm is taken from Shalev Shwartz and Ben-David's textbook: Understanding Machine Learning. This algorithm is described in the same way as the one in Mostafa's textbook, and as in many other famous machine learning textbooks. These textbooks are used in most major universities and thousands of students around the world learn backpropagation this way. However, whenever I try to look up how backpropagation is done online, virtually none of these references discuss backpropagation as in these textbooks. There are many terminologies such as forward/backward autodiff and jacobian-vector product that are never used in these textbooks. Similarly, things like "sensitivity vectors" are never discussed in these online references. For instance: http://www.cs.cmu.edu/~mgormley/courses/10601-s18/slides/lecture14-backprop.pdf https://people.cs.umass.edu/~domke/courses/sml2011/08autodiff_nnets.pdf https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2022/tutorials/tut01.pdf As you can see, the backpropagation algorithms described in these slides is very different from the backpropagation algorithm described in standard textbooks. I am really confused why there are these two seemingly different philosophies when it comes to describing backpropagation. One is textbook style which I know works (because it provides a way of explicitly calculating the gradient with respect to the weights), another is almost in this hacker-language style which I do not know if it works, or what it actually is. Can someone please help me bridge the gap between how backpropagation is done in textbook versus how it is described in many online resources? Are textbook backpropagations no longer relevant?
