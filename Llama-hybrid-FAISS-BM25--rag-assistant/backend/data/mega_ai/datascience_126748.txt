[site]: datascience
[post_id]: 126748
[parent_id]: 
[tags]: 
How to Yield a Better AUC / Lift Score?

I have a dataset with 200k records and 173 features focused on binary classification. Class proportion is around 98.7:1.3 (1.3% target=1). Currently, I am trying to increase the performance of my model which is yielding an AUC of 73%. Moreover, my cumulative lift for top 2% is 10.41, and for the top 5% is 5.92. Since I will only be targeting the top 2-5% of my positively predicted scores, I am not particularly concerned with the confusion matrix threshold or improving the matrix values (FP, FN). I have performed feature engineering through transformations (interactions, ^2) and manual mathematical calculations. Despite this, having trained a model without the engineered features, the AUC score is around the same, with cumulative lift being slightly higher in the model without my engineered features. I used an automated feature selection tool using RFE and XGBoost to dictate chosen features. I should note that I trained the model having downsampled datasets for 3 periods (40k from each of the 3 periods), to a classification ratio of 93.5:6.5 (6.5% target=1), and used a regular 4th period of data on the validation dataset (original 1.3% tareget=1 rate). I used H20 for training my model (chose XGBoost). How can I go about improving my model scores, and the quality of the model? I know the model training deals with imputation, but should I try a SimpleImputer, IterativeImputer, or/and KNNImputer during my pre-processing/cleaning stages instead? Will this improve my model? I tried re-training multiple models with and without my engineered features, as well as went back to square 1 and created more variables (engineered) in an attempt to help my AUC and lift scores.
