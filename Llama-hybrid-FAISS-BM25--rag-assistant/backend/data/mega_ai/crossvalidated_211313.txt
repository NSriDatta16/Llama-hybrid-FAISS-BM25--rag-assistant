[site]: crossvalidated
[post_id]: 211313
[parent_id]: 
[tags]: 
Differences in GARCH estimation when using R and Matlab?

For my thesis I'm fitting GARCH(1,1) models (standard GARCH, TGARCH and GJRGARCH) to some log returns of financial series using R. Judging from the p-values obtained using robust standard errors, it seems as if only $\beta_1$ is highly significant, whereas $\omega$ and $\alpha_1$ (and the leverage term) are often evaluated as being not significant. Using normal standard errors however generates significant $\alpha_1$ , $\beta_1$ and leverage coefficient. When I asked the supervisor about this, he said that I've probably done something wrong because GARCH models usually have significant coefficients, and that I should replicate my results with another software (he suggested either Matlab or EViews, he doesn't like R very much), and therefore I'm now trying with Matlab. Whereas the estimated coefficients are pretty much the same, standard errors are unreasonably different, and consequently the t-statistics (see output tables below for standard GARCH). The series can be found here (I'm using adj. closing prices): http://real-chart.finance.yahoo.com/table.csv?s=XLB&a=11&b=22&c=1998&d=02&e=31&f=2016&g=d&ignore=.csv (if you're trying to replicate the output I should also mention that I'm using only the first 1766 obs for parameter estimation; both models estimated assuming normal innovations) My questions: 1) What could be the cause for R and Matlab producing (slightly) different estimates? 2) How would you explain the big difference in st. error estimation? 3) If I were to stick with R, should I use the output under "optimal parameters" or "robust SE"? The supervisor told me it's near to impossible that GARCH parameters are insignificant (which would motivate using normal SE), but to me it seems counterintuitive to neglect robust SE.
