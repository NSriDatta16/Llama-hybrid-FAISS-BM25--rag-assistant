[site]: crossvalidated
[post_id]: 496852
[parent_id]: 
[tags]: 
Are radial basis kernels able to model interactions between predictors?

I have been doing research using Support Vector Regression for some time, especially using radial basis kernel, for predicting a response variable from a set of numeric predictors. As a consequence of this work, I wrote an article with my results and submitted it to a journal. Now, one reviewer is asking me to justify if my SVR model with radial basis kernel is modelling somehow interactions between predictors... I know that polynomial kernels include interactions explicitly. I also know that radial basis kernel does not. However, radial basis functions are considered a universal approximator and in this other question ( https://stats.stackexchange.com/a/491484/233634 ) a user (Firebug) stated that any universal approximator can implicitly model interactions between predictors (he/she uses an example based on a neural network). I am not a mathematician and I confess that I am a bit far from truly understanding the properties of universal approximators. Is the radial basis function really able to model interactions between predictors implicitly? How could I check/prove wether my model is doing it? Thanks ;) P.S.: There is an old question about a very similar topic ( SVM, variable interaction and training data fit ), but, even given the great answer by Dikran Marsupial, the specific matter about radial basis and interactions remains unclear, in my opinion.
