[site]: datascience
[post_id]: 42106
[parent_id]: 41725
[tags]: 
One common approach for your problem would be to first learn a low-dimensional representation of your output space in an unsupervised manner, and then a mapping from your inputs to the now-dimensionally-reduced outputs. To put it in a kind of pseudo-SKLearn terms, the overall procedure would look as follows: Model fitting: dr = SomeDimensionalityReducer() Y_dr = dr.fit_transform(Y) m = SomeSupervisedModel() m.fit(X, Y_dr) Model application: dr.inverse(m.predict(X_new)) For the role of SomeSupervisedModel you are free to choose any standard machine learning regression model, remembering that you may need to predict a vector as an output rather than a single number. In some cases (such as the neural network) it is a natural part of the model, in others it means you would need to train a separate model for each of the components in the output. The choice of the dimensionality reduction technique is a bit more tricky, as the inverse operation is not normally part of standard implementations, hence you might need to understand and implement it manually. Your main options are listed in the Wikipedia page on dimensionality reduction . Consider PCA , Kernel-PCA and an Autoencoder as your base choices. PCA would result in a linear mapping and might not be powerful enough to represent the output space adequately in all but the simplest of tasks. However, it is easy to use and understand and is not too prone to overfitting. Kernel-PCA a more flexible nonlinear model, which is still easy to implement, but it has higher memory and computational requirements and may overfit. The Autoencoder route might be the best of the three, but, being a neural network-based method it may be fiddly and require a lot of tuning. There is a whole world of different kinds of autoencoders to choose from. Another possibility, not mentioned in the Wikipedia article above (because it is more of a "dimensionality expansion" rather than reduction method) is the Generative Adversarial Network . Of all the mentioned approaches it may be the most sophisticated and, if you are lucky and have a lot of data, may give the best results. Unfortunately, it is the fussiest of all to work with, so try other things before trying it. Note that you do not need your inputs X to perform the dimensionality reduction, hence you can "help" your method by feeding more samples from the output space without having to also obtain the corresponding inputs.
