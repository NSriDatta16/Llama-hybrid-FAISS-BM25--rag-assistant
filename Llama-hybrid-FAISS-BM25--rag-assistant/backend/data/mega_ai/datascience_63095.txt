[site]: datascience
[post_id]: 63095
[parent_id]: 
[tags]: 
Activation Functions in Neural network

I have a set of questions related to the usage of various activation functions used in neural networks. I would highly appreciate if someone could give explanatory answers. Why is ReLU is used only on hidden layers specifically? Why is Sigmoid not used in multi-class classification? Why do we not use any activation function in regression problems having all negative values? Why do we use average='micro' while calculating the performance metric in multi_class classification? f1-score(y_pred,y_test,average='micro')
