[site]: crossvalidated
[post_id]: 596333
[parent_id]: 596259
[tags]: 
Why would we ever set up and estimate a statistical prediction model if we could just use the conditional distribution instead (given that we have enough data) ? You are assuming here that you know the distribution. This is usually not true. For a start, let's assume that you know the distribution. The expected value is a prediction for $y$ (conditional on $X$ ), the probability distribution tells you about the probability of observing different values of $y$ . To make a prediction you would need to calculate the expected value, mode, or something else. There usually won't be a closed-form solution for finding the value, so even when knowing the distribution you would need something to estimate or approximate the answer from the distribution. But, as said before, usually you don't know the distribution. This means that you would need first to estimate the distribution, then use it to find the approximate value of the estimate from the distribution. This means two layers of approximations (linear regression does this directly). Next problem is that you want to use a nonparametric method to estimate the distribution. Such methods are very flexible, so can easily approximate any distribution, but also can easily overfit to training data, not generalizing well beyond it. To avoid this, you need much more data compared to simpler models. Since we are talking about multivariate distribution, because of the curse of dimensionality, this means much more data. Finally, you want to use kernel density estimation. You are saying that this “only” needs finding the bandwidth. In fact, it is a very hard problem, even harder for multivariate distributions. Because of this, people are usually using rules of thumb to find the bandwidth, which do not guarantee optimal results. The kernel density estimator is very sensitive to the choice. Taking this all together, kernel density may give you a very rough approximation of the true distribution. Another problem is that vanilla kernel density does not scale well with data size, so with more data, you usually need methods that approximate it, leading to the the the third layer of approximation. So you are correct that if you knew the full distribution, it would give you much richer information than the point estimate alone. This is one of the upsides of Bayesian models that aim to find distributions for the predictions rather than point estimates. The problem is that you usually don't know the distribution and estimating it is a non-trivial problem, much harder than finding the expected value. So it's like asking “should I go there with Lamborghini or by bike?” Sure, if you have Lamborghini, use it, but do you? It also has its costs like fuel, maintenance, etc, so in some cases bike would still be just enough.
