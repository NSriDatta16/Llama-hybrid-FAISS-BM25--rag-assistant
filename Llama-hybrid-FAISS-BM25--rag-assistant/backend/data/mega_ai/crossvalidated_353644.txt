[site]: crossvalidated
[post_id]: 353644
[parent_id]: 353642
[tags]: 
Why [do] we need normal distribution in a data sample? In general, we don't. There are plenty of models for non-normal data: generalized linear models and other forms of maximum likelihood come to mind. The most well known model(s) that reference a so-called normality assumption are OLS, the t-test, and ANOVA; the latter two are actually special cases of the OLS. OLS does not in fact require normality: and by this I mean normally distributed residuals. Since the regression parameters are weighted averages of the data, the central limit theorem applies, and simulations have shown that in practical scenarios, even very small samples of non-normal data (like binary data), the statistical tests have good properties. The benefit of normal residuals with OLS models is that the F-test is of exact size. Otherwise use the "asymptotic" $\chi^2$ test and you will find that the power and the size of the test is relatively well conserved. But is there anything else that we earn from a normal distribution sample Ironically, the only scenario in which normality tends to help us is in predictions and forecasts where one desires to construct a prediction interval for a specific observation . Here we must be sure of its sampling distribution; if it is known, simple model summaries can produce valid PIs. Otherwise, more sophisticated methods must be used like density-smoothers, or parametric tests. Also is normality fit for all data samples or there are exceptions ? Normality is not "fit": it is intrinsic to the data, and to complicate things further, it is neither knowable nor can it be established. Real-life data is never normal. Better to have methods like OLS that are robust to normality assumptions. There are other methods that have varying levels of reliance on the distributional assumptions of those data.
