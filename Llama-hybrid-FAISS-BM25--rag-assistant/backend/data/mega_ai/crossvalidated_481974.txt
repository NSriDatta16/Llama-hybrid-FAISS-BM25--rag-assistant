[site]: crossvalidated
[post_id]: 481974
[parent_id]: 
[tags]: 
Confidence/Prediction Interval on binary/multiclass problems

I've recently fine-tuned a deep learning framework/model BERT for a sentiment classification task. I'd like to look at the confidence/prediction interval of the predicted sentiment scores (class 1 and 5). Assume that I have already chosen the best model using F1/precision/recall/MCC metrics. The motivation behind this is to quantify how much the predicted mean sentiment scores differ from the actual mean sentiment scores . So for example, I'd like to be able to say that I am 95% confident that my model predicts the mean sentiment scores between +/- y% of the true sentiment mean scores. I have 4 questions. Does it make sense to get a confidence interval of the predicted sentiment scores by the model? I've tried looking for something similar online but there are nothing on this. The closest I got is getting confidence interval for accuracy/F1/precision/recall metric. My strategy to get my confidence interval of the predicted sentiment scores is as follows. I created N bootstrap samples, (N=1000) where each bootstrap sample is resampled with replacement from my test data. I'll run the model to predict the sentiment scores for a total of N bootstrap samples * len(test data) = 1000x1000 = 1,000,000 records. I'll calculate my confidence interval using this data. Is this a sound strategy? For resampling with replacement for my bootstrap samples, should I take a stratified sample with replacement instead? This is because there is a class imbalance of 15:1. The idea I have is that my bootstrap samples should represent the population as closely as possible and so, stratifying to account for class imbalance for each bootstrap sample makes sense. Is this right? If my strategy is flawed / does not make sense. What is a sensible statistical approach to quantify how much the predicted mean sentiment scores differ from the actual mean sentiment scores?
