[site]: datascience
[post_id]: 103555
[parent_id]: 
[tags]: 
How to identify precision, recall, IoU, and mAP in these results for my trained Tensorflow model?

I have trained a Single Shot Detector model (using Tensorflow), and have run the evaluation metrics. However, I am not entirely sure what to make of them. Doing a computer vision literature search, common evaluation metrics reported on are precision, recall, intersect-over-union (IoU), and mean average precision (mAP). Being new to object detection, I am trying to obtain these results, but am struggling to understand which metric to choose for precision, recall, mAP, etc. Algorithms such as YOLOv5 will automatically compute average precision/recall, mAP @ 0.5, etc., but those results are a little different than what Tensorflow provides. (Note: I trained the model with a Ubuntu 18.04 machine in the method described in this tutorial ). Here are the results: INFO:tensorflow:DONE (t=0.02s) I1027 11:06:36.038156 139881781995328 coco_tools.py:138] DONE (t=0.02s) creating index... index created! Running per image evaluation... Evaluate annotation type *bbox* DONE (t=5.55s). Accumulating evaluation results... DONE (t=0.41s). Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.375 Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.684 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.376 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.159 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.363 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.382 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.148 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.454 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.577 Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.250 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.541 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.585 INFO:tensorflow:Eval metrics at step 100000 I1027 11:06:42.064121 139881781995328 model_lib_v2.py:1007] Eval metrics at step 100000 INFO:tensorflow: + DetectionBoxes_Precision/mAP: 0.375002 I1027 11:06:42.090729 139881781995328 model_lib_v2.py:1010] + DetectionBoxes_Precision/mAP: 0.375002 INFO:tensorflow: + DetectionBoxes_Precision/mAP@.50IOU: 0.684183 I1027 11:06:42.092986 139881781995328 model_lib_v2.py:1010] + DetectionBoxes_Precision/mAP@.50IOU: 0.684183 INFO:tensorflow: + DetectionBoxes_Precision/mAP@.75IOU: 0.376293 I1027 11:06:42.094577 139881781995328 model_lib_v2.py:1010] + DetectionBoxes_Precision/mAP@.75IOU: 0.376293 INFO:tensorflow: + DetectionBoxes_Precision/mAP (small): 0.159241 I1027 11:06:42.096016 139881781995328 model_lib_v2.py:1010] + DetectionBoxes_Precision/mAP (small): 0.159241 INFO:tensorflow: + DetectionBoxes_Precision/mAP (medium): 0.362854 I1027 11:06:42.097296 139881781995328 model_lib_v2.py:1010] + DetectionBoxes_Precision/mAP (medium): 0.362854 INFO:tensorflow: + DetectionBoxes_Precision/mAP (large): 0.381929 I1027 11:06:42.098785 139881781995328 model_lib_v2.py:1010] + DetectionBoxes_Precision/mAP (large): 0.381929 INFO:tensorflow: + DetectionBoxes_Recall/AR@1: 0.147922 I1027 11:06:42.100110 139881781995328 model_lib_v2.py:1010] + DetectionBoxes_Recall/AR@1: 0.147922 INFO:tensorflow: + DetectionBoxes_Recall/AR@10: 0.454406 I1027 11:06:42.101474 139881781995328 model_lib_v2.py:1010] + DetectionBoxes_Recall/AR@10: 0.454406 INFO:tensorflow: + DetectionBoxes_Recall/AR@100: 0.576815 I1027 11:06:42.102799 139881781995328 model_lib_v2.py:1010] + DetectionBoxes_Recall/AR@100: 0.576815 INFO:tensorflow: + DetectionBoxes_Recall/AR@100 (small): 0.250000 I1027 11:06:42.104048 139881781995328 model_lib_v2.py:1010] + DetectionBoxes_Recall/AR@100 (small): 0.250000 INFO:tensorflow: + DetectionBoxes_Recall/AR@100 (medium): 0.541326 I1027 11:06:42.105308 139881781995328 model_lib_v2.py:1010] + DetectionBoxes_Recall/AR@100 (medium): 0.541326 INFO:tensorflow: + DetectionBoxes_Recall/AR@100 (large): 0.584876 I1027 11:06:42.106697 139881781995328 model_lib_v2.py:1010] + DetectionBoxes_Recall/AR@100 (large): 0.584876 INFO:tensorflow: + Loss/localization_loss: 0.212005 I1027 11:06:42.107778 139881781995328 model_lib_v2.py:1010] + Loss/localization_loss: 0.212005 INFO:tensorflow: + Loss/classification_loss: 0.288771 I1027 11:06:42.108886 139881781995328 model_lib_v2.py:1010] + Loss/classification_loss: 0.288771 INFO:tensorflow: + Loss/regularization_loss: 0.279903 I1027 11:06:42.110019 139881781995328 model_lib_v2.py:1010] + Loss/regularization_loss: 0.279903 INFO:tensorflow: + Loss/total_loss: 0.780679 I1027 11:06:42.111117 139881781995328 model_lib_v2.py:1010] + Loss/total_loss: 0.780679 Again, I'm looking for precision, recall, IoU, and mAP and am wanting to report on them in a similar manner that YOLO povides. Do I need to average all of the precision and recall scores to get those scores, or do I just need to look at one specific output?
