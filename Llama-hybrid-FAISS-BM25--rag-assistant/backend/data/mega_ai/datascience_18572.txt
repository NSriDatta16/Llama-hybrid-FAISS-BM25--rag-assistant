[site]: datascience
[post_id]: 18572
[parent_id]: 
[tags]: 
Machine learning learn to work well on future data distribution?

This is based on my limited machine learning scope and experience, so correct me if I'm wrong. Many of the currently used machine learning models (SVMs, boosted trees, DNNs) work under the assumption that the training, validation and test data sets share the same distribution. They can work to some extent if the distributions differ but not by a lot. Here "can work" means that they work sub-optimally (i.e. can work better if the distributions are the same), not that their theory behind is supposed to deal w/ the distribution difference and can handle them like "nailing it". Hence my question: is there work on predicting based on the assumption that the data sets are actually moving through a series of distribution changes? A crazy thought would be to observe the distribution difference between training and validation sets, and assume that the same diff will exist between validation and test sets and learn to predict well on test set. This will work great on time series where the nature of the data might change over time.
