[site]: crossvalidated
[post_id]: 510470
[parent_id]: 510356
[tags]: 
You have correctly quoted a relevant Bellman equation for action values. The state transition function is required there. In that context, the reward function $R(s,a,s')$ is also part of the model, so required to evaluate the expression. Model-free methods in value-based Reinforcement Learning (RL) are based on the Bellman equation, but do not use it directly. For instance, in basic (single step tabular) Q learning, the value update step is: $$Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha(r_{t+1} + \gamma\text{max}_{a'}Q(s_{t+1}, a_{t+1}) - Q(s_t,a_t))$$ There are no mentions of $P(s'|s,a)$ or $R(s,a,s')$ in that update step. That is because in order to establish the values of $s_{t+1}$ and $r_{t+1}$ to use, these key values have been sampled from the environment. This sampling is done by taking a step in a real or simulated environment. It is this environment that expresses the transition probabilities and reward function, the agent and learning algorithm do not have access to it or use it. It is an important detail to note that sampling is occurring. In stochastic environments, it means that the agent needs to experience the same state and action multiple times, so that it can observe the multiple different ways that the environment can respond, and use the corresponding $s_{t+1}$ and $r_{t+1}$ values to perform updates to the action value estimate.
