[site]: crossvalidated
[post_id]: 611423
[parent_id]: 
[tags]: 
What are the differences and common points, if any, between oversampling as a survey design method and oversampling in a machine learning context?

I've seen the term "oversampling" used in a survey design methodology context and in a machine learning context (e.g. methods like SMOTE). I'm intrigued by the differences between the two. So far, here's what I understood: The purpose of oversampling in survey design is to reduce the variance for a target sub-population with a rare but interesting feature, and to cut costs related to sampling the population, as explained here: https://ajph.aphapublications.org/doi/full/10.2105/AJPH.2017.303895 Oversampling is planned beforehand, and is done when we collect the data. Weights are then applied to observations to account for the bias introduced by oversampling. In machine learning, the goal of oversampling is to improve prediction metrics (e.g. accuracy) for so-called imbalanced datasets. Oversampling is done when the data has already been collected, so it involves duplicating observations or creating synthetic observations. In this context, oversampling has been criticized as a solution to a non-problem (as many posts on this website explain well). For example accuracy may be simply the wrong indicator to look at, and other methods than oversampling may be applied to improve predictions. Am I correct as to the differences between the two contexts? Are there other major differences (or common points) I am missing? Bonus question: has oversampling in machine learning been originally inspired by oversampling as a survey design method? If so, why does it not use weights in the same way as in survey design? I don't have any particular practical problem related to any of this, my question is just out of curiosity, as it's (mildly) surprising to have the same term used in two apparently quite different contexts. So I'm afraid I cannot be really more specific than what I ask above. Thanks!
