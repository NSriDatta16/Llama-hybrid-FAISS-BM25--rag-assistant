[site]: crossvalidated
[post_id]: 436856
[parent_id]: 
[tags]: 
Can I use *any* activation function in my Neural network as long as it is non-linear?

I am currently learning about Neural networks and am finding it hard to comprehend the reason why activation functions such as ReLU or sigmoid are favored. In general, I understand the need for the activation functions to be nonlinear to produce nonlinear decision boundaries. However, I am unable to grasp the rationale behind the NN community in choosing certain nonlinear functions over others. In my understanding, I should be able to plug-in say a cubic polynomial and train my NN and expect results that perform more or less to what a sigmoid function shall perform. Can you point out where my understanding might be wrong and refer some literature I can read on this question.
