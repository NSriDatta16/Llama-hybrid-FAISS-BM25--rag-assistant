[site]: crossvalidated
[post_id]: 277756
[parent_id]: 
[tags]: 
Some general questions on Generative Adversarial Networks

Generative Adversarial Networks are a new type of Adversarial Networks, introducing by "Goodfellow" in 2014. Usually implemented via DeepNNs and they are very powerful in generating "Realistic" outputs which can not be distinguished from a "Real" data. They are created from two main parts. a Generative and a Discriminator. The Generative part is trying to create some sample and fool the discriminator to classify it as a "Real" sample. Mathematically they can be represented like this: min max V (D, G) = E x∼p data (x) [log D(x)] + E z∼p z (z) [log(1 − D(G(z)))] There are tons of papers on GANs from noise reduction to image generation. I have some basic questions about them and I think the answer to these questions can be useful for others too. There are some papers using GANs for extracting better images from a sample. for example "Kevin Schawinski et al 2017 - Generative Adversarial Networks recover features in astrophysical images of galaxies beyond the deconvolution limit" as you see they used a GAN to "make" new data based on the sample. but the problem space is very 'Chaotic' (astronomical images and deep space stuff). So isn't a risk here of losing actual data while recreating the sample and fill it with some 'Sampled Patterns' stored in the GAN DeepNN? What is the role of Noise here? in the G part we input some noise(as i understood with the same dimension of the output) to DNN. it uses this noise to create an output to be judged by the D part. I'm not sure but why so many papers decided to use a linear distribution sampling for sampling from the noise. is there any particular reason for that? and is there a study to specifically compared the results of the G part outputs with different noise sampling strategies? what if instead of pure noise we input something else to the G part? what would happen then? In a paper (Han Zhang et al - 2016) they used a two staged GAN. The 1s stage is to produce a 128*128 image from a sentence and noise and in the second stage the output of stage 1 is used to produce a 256*256 more realistic image. it really looks like a 'layered' network. right? so: is it practical to use a deeper GAN with multiple stages? of course it's possible but is it practical? what can be the downsides of it? sorry for so many questions in a single thread. To Editors: please feel free to edit my poor English and help others to understand the questions better. Thanks
