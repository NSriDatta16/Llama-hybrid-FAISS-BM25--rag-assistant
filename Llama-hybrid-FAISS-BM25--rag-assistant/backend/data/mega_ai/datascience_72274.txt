[site]: datascience
[post_id]: 72274
[parent_id]: 72257
[tags]: 
The first thing I would try to do is to simply use one-hot encoding to represent you're features. One-hot encoding consists in representing sentences (sequences of words in your case) as a sparse vector, if you're familiar with python sklearn has this function already implemented, it's called DictVectorizer # the length is the total amount of different words in all your words sequences "dog owner" --> [0, 0, 0, 0, 1, 0, 0, 1, ....] I would then train several models (random forest, naive bayes, multi layer perceptron, support vector machine, etc.) to check what model works best. Usually with sparse features, svm works well, but it does mean that also in your case they would provide the highest results, that's why the only way to know is to train several models. A more advanced technique (not in terms of coding though) would be using embedding vectors and deep learning. You could use pre-trained vectors, GloVe vectors are a standard choice, as an input for a Convolutional Neural Network (this architecture usually works pretty well with short texts and in classification tasks, and it's fast to train). As a final consideration, from what I see on your example it seems that the label is related only to the last word of the sequence. If this is consistent across all dataset, another trick that might avoid completely deep learning could be to still use embedding vectors, but only to compute some semantic similarity scores, like cosine similarity. If the labels are semantically different enough, it might be possible to predict the label only by computing the similarity between the final word of each sequence and each label to then select the label with the highest score.
