[site]: crossvalidated
[post_id]: 470110
[parent_id]: 7519
[tags]: 
This is an old but interesting topic. I recently thought about this and developed a take that I would like to share. First off, the problem with flat priors as uninformative priors is that this idea is rooted in the way we would guess a number; not the way the data guess a number in likelihood-based inference. We can understand this by comparing two binomial random variables: \begin{eqnarray} X &\sim& Bi(x|n=10,\theta=.5)\\ Y &\sim& Bi(y|n=10,\theta=.9) \end{eqnarray} Clearly, E[X]=5 and E[Y]=9. The likelihood of finding E[Y]=9 under the distribution of X is $\approx 0.01$ , while the likelihood of finding E[X]=5 under the distribution of Y is $\approx 0.0015$ . This fact is independent of parametrization (odds, log odds). For example, if $\phi$ are odds, sample odds of .9/.1=9 give not as much evidence against $H_0: \phi=1$ as sample odds of 1 gives against $H_0: \phi=9$ . Hence, finding x=5 is much better at excluding $H_0:\theta=.9$ than finding x=9 is at excluding $H_0:\theta=.5$ (just considering one random variable $X \sim Bi(x|n,\theta)$ from now on). More generally, intermediate $x$ exclude extreme $\theta$ very well, but extreme $x$ do not exclude intermediate $\theta$ so well. This notion is formalized in Fisher's information, which is the expected curvature of the log likelihood given some $\theta$ . The expected curvature of the log likelihood for binomial random variables equals \begin{equation} \frac{-n}{\theta(1-\theta)}. \end{equation} Referring back to the example above, it is readily verified that the curvature is equal to -4n at $\theta=.5$ , but $\approx-11n$ at $\theta=.9$ . More curvature means that fewer values of X are compatible with that value of $\theta$ , so it is easier to find evidence against that value of $\theta$ (implying lower posterior density in the Bayesian setting). Fisher's information is different for different parameterizations, but that's because it is on a different scale: the curvature may be different, but so is the distance between points. The net result is invariance under a transformation. The key point of using Jeffreys' prior then seems to be that if we do not want to help the data making its decision, we should give less weight to points that are hard to find evidence against, and more weight to points that are easy to find evidence against (e.g., it would be unjust to give a lot of weight to $\theta=0.5$ because it is hard to exclude this point from the posterior anyway). We do so by taking the prior distribution proportional to the expected curvature of the likelihood, which is the square root of Fisher information if we parametrize the prior over $\theta$ (since Fisher information runs over $\theta^2$ ). In the Binomial case, this gives a Beta distribution with parameters .5 and .5. This distribution gives less weight to intermediate values of $\theta$ (values close to 0.5, which are hard to throw out of the posterior anyway) and more weight to extreme values of $\theta$ (values close to 0 or 1, which are easy to throw out of the posterior). From here, I see two ways forward. The first is to reject the notion of uninformative priors altogether because the Bayesian posterior is still different from the frequentist likelihood. The second is to say that by using the Jeffreys prior we finally have a method under which all values of $\theta$ are equally likely before we have seen the data (under frequentist likelihood-based inference, they are not). If I read Jeffreys' 1946 paper, it seems to be all about invariance under transformations. I can see how that is a necessary condition for a prior to be uninformative, but I'm not sure about its sufficiency. I'm not aware of Jeffreys wishing to correct a deficiency of likelihood-based frequentist inference (granted, I haven't looked very much), but that does seem to be the corrolary. Take your pick.
