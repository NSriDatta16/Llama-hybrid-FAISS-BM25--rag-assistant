[site]: crossvalidated
[post_id]: 23649
[parent_id]: 22473
[tags]: 
There are a number of things you can do with this sort of data but in the end you have to make some calls about what is acceptable to you - and the old way of saying "is the average error more than five percent?" may not be such a bad way of doing it. Statistics can be useful in showing whether there is bias in the measurements; and in whether any of the properties is systematically mismeasured. And it can help quantify on average how far out the sensor is, and a sense of the spread of that (ie is it mostly right and occasionally way out - or always mediocre). It's your judgement call on whether that's acceptable. Here's what I did with your data (ok, I was having a slow afternoon...), using R and Hadley Wickham's plyr and ggplot2 libraries.. My first instinct (basically also Michelle's suggestion) was to look at the relationship between the actual value and the sensor's value, conditioned on the five properties. That gives me: which is a start. Amongst other things I see you have very few observations on properties four and five, which might complicate things later. And that some of the properties typically have much higher scores than others. Next, I built a similar plot and compared the data to a line with zero intercept and slope of 1 which is what you'd get if the sensor and the actuals were always the same. I added to this locally smoothed lines showing the actual relationship between actual and sensor scores for the first three properties. This gives me the following, which is starting to suggest that property two is maybe scored higher than it should be by the sensor; and properties one and three perhaps the opposite problem. Property three is probably the one most to look out for, judging by this plot. I'm interested in testing this in a model, but there is clear heteroscedasticity ie the variance in sensor scores, unsurprisingly, increases as the scores increase. This would invalidate the simpler models to fit, so I try taking logarithms and this problem goes away: I can then use this as the basis of fitting a linear regression model which finds that yes, there is statistically significant evidence that knowing which property is being measured is helpful in knowing what the sensor's score will be. If the sensor was equally good at measuring all properties you wouldn't get this, so we now know there is a problem here at least (again, whether it matters is up to you). > mod anova(mod) # shows there is a difference in how well properties measured Analysis of Variance Table Response: log(sens) Df Sum Sq Mean Sq F value Pr(>F) log(act) 1 30.15 30.15 3854.54 summary(mod) # as graphics show, property 2 seems higher than properties 1 and 3 ... Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 0.37988 0.35210 1.08 0.284 log(act) 0.95216 0.04094 23.26 Ok, so I've resolved that at least one of the properties is measured differently from the others. Next I wanted to get a feel for how far out in general the measurements are. I did this by plotting a histogram of the average percentage out for each observation of a property - which shows there actually is a pretty big range: I'd intuitively say that already this is enough to indicate you need some more callibration, but we can quantify it a bit. There's not quite evidence the average is significantly different from zero ie of systematic bias, but it's getting close (p value of around 0.08). If it weren't for those couple of big values where the sensor was 20-40 percent too large, you'd say that it seems to generally underestimate the true value. However, perhaps more importantly, the average absolute value of the percentage out is about seven or eight percent (depending on how you interpret "average") which strikes me as too much. But see, we've just come back to that first rule of thumb, albeit after some useful graphical insight... Hope that helps. The code in R that produced this is below. sensor
