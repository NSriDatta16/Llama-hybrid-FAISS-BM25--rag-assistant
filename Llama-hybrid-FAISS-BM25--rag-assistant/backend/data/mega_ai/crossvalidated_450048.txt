[site]: crossvalidated
[post_id]: 450048
[parent_id]: 450043
[tags]: 
The procedure, that you described, has a similar effect to artificially inflating the sample size. It's akin to cloning the observations of the sample. Say, you take each observation and create 9 more copies of it and run the Bayesian update on the 10 times bigger sample. Here's an example from the Wiki article . Suppose there is a school having 60% boys and 40% girls as students. The girls wear trousers or skirts in equal numbers; all boys wear trousers. An observer sees a (random) student from a distance; all the observer can see is that this student is wearing trousers. What is the probability this student is a girl? You get the posterior $p(G|T)=1/4$ of a girl from observing a student in trousers. If you run Bayes theorem once more you get $P^{(2)}(G|T)=1/7$ What do you think will happen if you keep running Bayes over and over plugging the posterior as prior? Basically, you'll make it look like it's certainly a boy: $P(G|T)\to 0$ . This is effectively saying that girls don't wear trousers based on just one observation of a student in trousers.
