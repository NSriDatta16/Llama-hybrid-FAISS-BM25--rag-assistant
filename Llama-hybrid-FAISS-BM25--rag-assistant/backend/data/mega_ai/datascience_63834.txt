[site]: datascience
[post_id]: 63834
[parent_id]: 63828
[tags]: 
When data is presented that way, it is often because all 100 values in the vector should be read as one. For example, those can be a set of 100 measurements of, say, a temperature at different locations of a system. Data in the vector is likely to be correlated and the order of sub-features may be a valuable information (for instance, in my previous examples, if they represent the temperature measurements over a range of positions along a pipe). If data is not correlated at all , or sub-features have nothing relevant in common, you can just expand each sub-feature as a normal feature. In the process, you may get a final dataset with numerous features, this will eventually need to reduce the dimension. If data is deemed "simply" correlated , a good option would be to reduce the dimension of the vector from 100 to a much lower value, then expand it as before. The most straightforward way to do so is PCA. Be sure to perform it only over each particular multi-sub-features feature, at first. If data is correlated and the order of features matters (i.e. $(1, 1, 0, 0, 0, 0)$ is closer to $(0, 1, 1, 0, 0, 0)$ than to $(0, 0, 1, 0, 0, 1)$ , for instance, you will need a more complex approach. You could perform density-based clustering based on a custom distance function such as the Wasserstein metric / earth mover's distance . The 100-dimension feature would thus be reduced to a single feature representing a class among this clustered dataset.
