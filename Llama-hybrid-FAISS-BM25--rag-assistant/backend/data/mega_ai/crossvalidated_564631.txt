[site]: crossvalidated
[post_id]: 564631
[parent_id]: 564274
[tags]: 
TL;DR the variance is computed over hypothetical other randomly sampled training datasets. You can find the technical definition in the other answers, I try to give a bit of background understanding here. Bias and variance (in this usage) ultimately come from a frequentist conceptual framework, which might be a bit counter-intuitive if you were taught in a Bayesian machine learning style (the opposite kind of problem also comes up, and perhaps even more frequently). The main idea in frequentist statistics is to understand/imagine (training) data to be random , not fixed (and parameters fixed, but that's not important now). Many frequentist concepts in the end boil down to counterfactual thought experiments: what would be our result if our input data were different. If you come from a Bayesian perspective this may be strange, since the input data is what it is , and we want to train a model on this data, who cares what would happen if we had different data, right? In a way, yes, but it is still useful to think about whether our learning process is brittle and would output something very different if we had somewhat different training data. For bias and variance, we imagine what would happen if you resampled a new training set, trained the model on that data and made your predictions anew. Rinse and repeat, and record what your outputs are and see how much spread (variance) they have as you vary the training data. Outside of toy models that can be prodded and analyzed arbitrarily, all this is more of a thought experiment. In the real world, your data is fixed, and you can't just sample arbitrarily many independent training sets to measure bias and variance. They are rather theoretical concepts to be aware of and terminology to use in communication about how your model is performing and what the reason for bad performance may be. Cross-validation is a common real-world approximation to analyze if your model is sensitive in this way to the exact training data. But the different CV folds are not independently sampled, so it's just an approximation.
