[site]: crossvalidated
[post_id]: 372292
[parent_id]: 372278
[tags]: 
I think your example behaves differently due to regularization . In the case of a very small dataset, an unregularized model should be able to "memorize" the training set and thus reach zero training error. This is what Andrew Ng talks about. However, if you add some regularization to your model (such as weight decay), especially in the cases of very small dataset, the regularizer will prevent the model to overfit and instead it will push its parameters towards the regularized solution. In a well regularized model, the training and the validation curve should behave similarly. Update: If you encounter this in an unregularized network, there may be two further problems: There is a bug in your code. Reaching zero training error on a tiny dataset (or even a single element) is a good sanity check. See What should I do when my neural network doesn't learn? Your data is inconsistent : There are multiple samples with the same value but different label. You can never reach zero training error on such data.
