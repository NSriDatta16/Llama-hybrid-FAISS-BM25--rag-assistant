[site]: crossvalidated
[post_id]: 359369
[parent_id]: 
[tags]: 
Should a cost function in ML always be written as an average over all training samples?

I'm building a non-kernelized Support Vector Machine classifier . The problem that I need to solve is: $$\min_{w} \left(w^{t}w + \sum_{i=0}^{n}\max\left(0, 1-y_i\left(w^{t}x_{i} + b\right)\right)\right)$$ I want to solve this problem using subgradient descent, so I construct the cost function: $$J_{1}(w,b) = w^{t}w + C\frac{1}{n}\sum_{i=0}^{n}\max(0, 1-y_i(w^{t}x_{i} + b))$$ But, I found one source online(I will provide a link if I manage to find it again) where they wrote the cost function as follows: $$J_{2}(w,b) = \frac{1}{n}\sum_{i=0}^{n}(w^{t}w + C\max(0, 1-y_i(w^{t}x_{i} + b)))$$ I tried both versions of the cost function using their respective gradients, and it seems to me using $J_{2}$ gives a behavior of the SVM that looks most alike to the mental model of the behavior of SVM I have. So my question is: which one of the two versions of the cost function is the correct one (or the better one) in this case, and why? And more generally, should we write cost functions that are as whole written as an average over the training samples (as in $J_{2}$), or is it better to write some components as an average over the training samples and others not as an average (as in $J_{1}$)? By the way, I'm using vanilla subgradient descent (as opposed to Stochastic GD), meaning my implementation makes updates of the weights and the bias using all the training samples per each update.
