[site]: datascience
[post_id]: 73361
[parent_id]: 73359
[tags]: 
How is it that when starting the next epoch, the loss is almost always smaller than the first one? Does this mean that after an epoch the weights of the neural network are not reset? Yes. The network weights are initialized once before the training starts. After every iteration, the weights are updated by backpropagation using the error gradients that you obtain from the batch of data fed to the network at that iteration. Once an epoch is done, the weights are now better optimized to your training data, meaning you get a lower training loss. The next epoch builds on the weights you got after the first epoch to improve the performance further. This is why the loss the will keep decreasing as the network is trained for more epochs (assuming the hyperparameters are set properly). each epoch is not a standalone training process? Yes. An epoch is a part of the training process. You improve the network's performance by training it for as many epochs as necessary to achieve the desired performance.
