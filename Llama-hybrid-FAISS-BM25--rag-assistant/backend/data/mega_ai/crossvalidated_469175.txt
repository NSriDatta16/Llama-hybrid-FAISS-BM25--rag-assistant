[site]: crossvalidated
[post_id]: 469175
[parent_id]: 469172
[tags]: 
1) This length is the usual Euclidean distance, basically the Pythagorean Theorem. $$\vert\vert (x_1, \dots , x_n) \vert\vert = \sqrt{x_1^2+\dots+x_n^2} $$ Be aware that the double vertical bar notation means a norm, which is a specific operation in linear algebra and functional analysis. Without context saying otherwise, it usually would mean this usual Euclidean distance, but it has a more general definition. (That will matter if you want to get into ridge, lasso, or elastic net regularization, or if you explore the MAE loss function.) 2) The norm operation outputs a number, so you are just squaring a number, not a vector. Again, this is very related to Pythagoras. 3) This is not universal, but that’s for mathematical convenience when you take down the derivative and bring down the $2$ . But it doesn’t matter much; what you want to is find the set of parameters that gives the lowest mean squared error, regardless of what the MSE is. (You’ll care about the value of the MSE later for assessing if your mode is useful or better than another model, and then you’ll need to interpret the performance metric and make sure you’re using the same performance metric for each model under consideration.) I will link you to a post of mine where I explain this . I very much prefer the notation used in the question there. That’s a much easier way to understand MSE, which is not a neural net concept. (It comes up in neural nets, but it also comes up in ordinary least squares regression and random forest regression and every other type of regression.) Finally, you may see the denominator of MSE written as $n-p$ , the number of observations minus the number of parameters. This has to do with getting an unbiased estimate of the variance when you do ordinary least squares, which is may not interest you if you’re doing a neural net. (A common assumption in OLS is that the errors have equal variances, which is less of an assumption in neural networks.) However, all denominators one the MSE formula will, except for numerical issues related to doing math on a computer, give the same parameter estimates in your regression. 4) Nothing changes. The $w$ weights and $b$ biases determine the $y(x)$ values, but once you have the $y(x)$ values, the MSE machinery doesn't care how you got there. The weights and biases determine how $y$ acts on $x$ , but then you just have a number (or a vector, as my edit below discusses) to run through the MSE equation. You can apply the MSE equation to predictions from neural net regressions, random forest regressions, linear regressions, elastic net regressions... EDIT Looking at (1) a second time, I want to mention that this is a very general way of writing the MSE that only becomes particularly useful when the response variable is a vector. Most of the time the form that will make sense is the form in the question I linked, which is a special case of the form in your question. Anyway, when the response variable is a vector, your predictions are vectors, so you find the mean squared error by considering each error to be the distance between the predicted vector and the actual vector. Then you add up those squared errors to get the sum of squared errors and divide by $2n$ (or $n$ or whatever) to get the mean squared error. REMARK Do note that if you take the square root of the MSE, even the MSE you get when you divide by $n$ instead of $2n$ , you do not get the average amount by which a given prediction misses the actual value. This is a common, easy misconception, and it is wrong .
