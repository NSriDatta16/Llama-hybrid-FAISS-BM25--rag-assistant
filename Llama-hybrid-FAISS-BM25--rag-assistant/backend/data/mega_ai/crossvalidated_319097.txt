[site]: crossvalidated
[post_id]: 319097
[parent_id]: 
[tags]: 
How do I define the hidden size and network architecture for a Long Short Term Memory (LSTM) Recurrent Neural Network (RNN)?

I am reading the PyTorch documentation on using LSTM to classify names with a character-level RNN and generating names with a character-level RNN . In the examples they show, the network's hidden_size is set to 128 . Why is that? How do they know that 128 makes the learning work? In general, is there a guiding principle (besides trial and error) that we may use to set this parameter's value? Will 128 work in general and transfer to other problem domains? Also, I find that it is not clear why they define the classification and generation networks architectures as so. If we can transform our problem to the ones like they've demonstrated, will the network architectures be reasonable and/or applicable to use? I realized that nearly identical questions ( here and here ) have been asked, but those are in general, and here, I am referring to concrete examples.
