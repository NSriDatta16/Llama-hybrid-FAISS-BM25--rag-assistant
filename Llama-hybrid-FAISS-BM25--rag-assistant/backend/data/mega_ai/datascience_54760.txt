[site]: datascience
[post_id]: 54760
[parent_id]: 54751
[tags]: 
This is only a general answer but in case it helps: In general decision trees tend to be "robust" in the sense that they can work with practically any kind of data, in particular in cases where other methods such as linear or logistic regression might struggle. For example they have no problem in the case of heterogeneous features, e.g. when mixing categorical and numerical features, or mixing completely different ranges of values, etc. Random forests add ensemble learning to the mix, making decision trees even more robust and especially well equipped to deal with noisy data, whereas standard regression methods can get easily confused by noise. Intuitively I see decision trees (random forests included) as the "swiss army knife" of supervised learning: efficient, versatile, easy to use.
