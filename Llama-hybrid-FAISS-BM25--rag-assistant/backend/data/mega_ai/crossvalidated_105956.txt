[site]: crossvalidated
[post_id]: 105956
[parent_id]: 102842
[tags]: 
Question #5: Which value of lambda [of all the λs returned for the different surrogate models] do I choose? Obviously, if the λs are practically the same, there is no difficulty as there is essentially no choice involved. If the λs you find for the different surrogate models bounce all over the place, you are in trouble: that is a symptom that either your sample size is too small to auto-tune λ based on your data set, or that the models are (still) very unstable. In both cases, IMHO you'd need to step back and think again about your modeling approach. I encounter nearly only situations with extremely small sample sizes in my work. Therefore, I'd always decide for the hyperparameter that yields the least complex model (of any kind of regularization). While your situation may be different, the fact that you use the LASSO indicates that there is a problem with model complexity, so I guess that would be a sensible approach for you as well. Question #3: Which value of lambda (lambda.min or lambda.1se) do I want to keep? The same reasoning about model complexity applies. I'd go for lambda.1se . Question #6 [and #4]: In the end, I just want one LASSO logistic regression model, with one unique value for each hyper-parameter ... correct? yes Question #7: If the answer to Question #5 is yes, how do we obtain an estimate for the AUC value that this model will produce? Is this estimate equivalent to the average of the k = 5 AUC values obtain in Step 5? No it is not the AUC from step 5. This is measured by the outer loop of the nested validation. I think it is easiest to think of your model training as including the autotuning of λ. I.e. write a training function that does all that is necessary to auto-tune λ using e.g. [iterated] cross validation and then return a model trained on all data that is handed to the training function. Perform the usual resampling validation for these models. Questions #1 and #2 ... are best answered by reading the code: you work in R, so you can read the code and even work though it step by step. In addition, read up what the Elements of Statistical Learning say about hyperparameter tuning. AFAIK, that book is the origin of the 1SE idea for hyperparameter tuning. Long anwer: The key idea behind the lambda.1se is that the observed error is subject not only to bias but also to random error (variance). Just picking the lowest observed error risks "skimming" variance: the more models you test, the more likely is observing an accidentally good looking model. lambda.1se tries to guard against this. There are (at least) 3 different sources of variance here: finite test set variance: the actual composition of the finite test set for the surrogate model in question ("finite test set error") model instability : the variance of the true performance of the surrogate models around the average performance of models of that training sample size for the given problem variance of the given data set with respect to all possible data sets of size $n$ for the given problem. This last type of variance is important if you want to compare e.g. algorithms, but less so if you want to estimate the performance you can achieve for the data set at hand. The finite test set variance can be overwhelmingly large for small sample size problems. You can get an idea of the order of magnitude by modeling the testing procedure as a Bernoulli trial: you can know the size of this variance as it is tied to the observed performance. You could in principle construct confidence intervals around your observed performance using this, and decide to use the least complex model that cannot reliably be distinguished from the best performace observation you got. This is basically the idea behind lambda.1se . Model instability causes additional variance (which by the way increases with increasing model complexity). But usually, one characteristic of a good model is that it is actually stable. Regarding the λ, stable models (and a data set that is large enough to do the estimation of λ reliably) imply that always the same λ would be returned. The other way round, λs that vary a lot indicate that the optimization of λ was not successful.
