[site]: crossvalidated
[post_id]: 187303
[parent_id]: 
[tags]: 
How can independence be represented efficiently?

Consider a probability distribution over a high dimensional space. We would like to find an encoding describing the distribution, so that we can approximately compute the expected value of most random variables over that space. If the probability distribution is concentrated around a few low dimensional manifolds, a good approach would be to represent the distribution using a random sample from it. Of course, it's possible for a given distribution to construct pathological random variables where an average of the variable over the sample would give a very high variance estimator, but in general, it is a sensible approach. Importantly, this works even if the distribution is merely concentrated around a low dimensional manifold; its support may well be the entire space. In particular, this approach is used in particle filters, which represent the marginals of the filtering distribution using approximate samples. This approach deals well with the case where these is a lot of mutual information between the different coordinates. Can we have such an efficient approach in the case where there is almost none? The case where every coordinate is independent from every other is easy to deal with, it suffices to represent the independent marginal distributions. But that disappears if the space is rotated, or if the independence is only approximate. Intuitively, we should be able to describe distributions exhibiting low mutual information between the coordinates of a parametrisation of the space efficiently, but how?
