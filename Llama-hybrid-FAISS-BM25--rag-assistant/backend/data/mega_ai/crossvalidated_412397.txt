[site]: crossvalidated
[post_id]: 412397
[parent_id]: 412276
[tags]: 
1200 products is the main driver of the dimensionality of your problem. Now you have only 25 periods. This is very little data, insufficient to do any kind of blanket correlation analysis. In other words you don't have data to have a simultaneous forecast of all products without reducing the dimensionality. This pretty much eliminates all VARMA and other nice theoretical models. It's impossible to deal with the coefficients of these models, there's too many of them to estimate. Consider a simple correlation analysis. You'd need (1200x1200 + 1200)/2 cells in the covariance/correlation matrix. You have only 25 data points. The matrix will be rank defficient to enormous degree. What are you going to do? Broadly you have two simple approaches: separate forecasts and factor model. The first approach is obvious: you run each product independently. The variation is to group them by some feature, e.g. sector such as "mens closing". The second approach is to represent the product demand as $d_i=\sum_jF_{j}\beta_{ji}+e_i$ , where $F_j$ is a factor. What are the factors? These could be exogenous factors such as GDP growth rate. Or they could be exogenous factors , e.g. those you obtained with PCA analysis. If it's an exogenous factor, then you'd need to obtain betas by regressing the series on these factors individually. For PCA, you could do a robust PCA and get first few factors with their weights which are you betas. Next, you analyze the factors, and build a forecasting model to produce $\hat F_j$ and plug them back to your model to obtain forecast of product demand. You could run a time series model for each factor, even a vector model such as VARMA for several factors. Now, that the dimensionality of the problem was reduced, ou may have enough data to build time series forecasting.
