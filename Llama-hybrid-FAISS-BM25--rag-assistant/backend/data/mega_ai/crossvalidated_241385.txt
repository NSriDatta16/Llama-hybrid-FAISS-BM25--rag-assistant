[site]: crossvalidated
[post_id]: 241385
[parent_id]: 241380
[tags]: 
$J$ is a function respect to the parameter $\theta$. It is "cost function", where our ultimate goal is trying to reduce this the value function by using appropriate $\theta$. $\theta$ is also called "weights", "coefficients", "decision variables", where machine learning is finding the right value for $\theta$. First Let's look at the first part of the equation. The cost function $J$ comes from two things, the true distribution of data and label $P(x,y)$ and the loss function of $L$. The cost function is integrating all the possible values for the true distribution $P(x,y)$, and the Loss. More about the loss function $L$. I think the name is confusing because we have both "cost function" and "loss function". Here is the differences: $J$ is a cost that for all of the data points . and $L$ is the loss for each predicted value and label . The loss function $L$, has $3$ inputs. $(x, y)$ and $\theta$. In many cases, we use $\hat y=f(x,\theta)$ to represent predicted value, and $L(y,\hat y)$ to represent the loss value. The second part of the equation In real world, we will not know the true distribution of data and label, i.e., $P(x,y)$. What we can do, is using data to "minimize the empirical loss". Which is $\frac 1 m$ and $\sum_{i=1}^m$ part, assuming we have $m$ data points. Everything will be more clear with an numerical example. Here is one (discrete on $x$ and $y$). Suppose our true distribution on data $x$ and label $y$ is (think about $x$ is if it is cloudy, and $y$ is if it will rain.) $$ P(x=0,y=0)=0.3 $$ $$ P(x=0,y=1)=0.1 $$ $$ P(x=1,y=0)=0.2 $$ $$ P(x=1,y=1)=0.4 $$ And the loss function is $L$ is 0-1 loss. The cost would be $$\sum_x\sum_yP(x,y)L(y,\hat y)$$ That is the first part of the equation. For second part: in real world, we will not know the true distribution of $P(x,y)$, but have many data points of $(x_i,y_i)$, what we do is trying to minimize $$\frac 1 m \sum_{i=1}^m L(y_i,\hat y_i)$$
