[site]: datascience
[post_id]: 123204
[parent_id]: 96511
[tags]: 
Transformers typically convert a sequence of vectors of a fixed length, d, to an output sequence of vectors of the same length. How you arrive at the sequence of vectors to feed into the transformer depends on the problem you're trying to solve. For language translation , you embed your word into a d dimensional embedding. For image recognition you learn a mapping from fixed size patches of the image to a d dimensional vector. But sometimes, you do not have an input vector to provide to the model. Example cases taken from the BERT include When the first output is a special case, the classification of the whole sequence. This is the first [CLS] token. When the input token is masked, it's replaced with a [MASK] token. When you want to show the separation between two sentences, you use a [SEP] token. In case of the DeTr paper, the decoder stage does not have any inputs. So, they simply learn a d dimensional embedding at each position, and call these object queries, since these are used to query the learned encoding of the input image to generate the outputs.
