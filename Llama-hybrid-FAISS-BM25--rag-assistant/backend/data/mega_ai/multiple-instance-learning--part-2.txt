nd another application of multiple instance learning to scene classification in machine vision, and devised Diverse Density framework. Given an image, an instance is taken to be one or more fixed-size subimages, and the bag of instances is taken to be the entire image. An image is labeled positive if it contains the target scene - a waterfall, for example - and negative otherwise. Multiple instance learning can be used to learn the properties of the subimages which characterize the target scene. From there on, these frameworks have been applied to a wide spectrum of applications, ranging from image concept learning and text categorization, to stock market prediction. Examples Take image classification for example Amores (2013). Given an image, we want to know its target class based on its visual content. For instance, the target class might be "beach", where the image contains both "sand" and "water". In MIL terms, the image is described as a bag X = { X 1 , . . , X N } {\displaystyle X=\{X_{1},..,X_{N}\}} , where each X i {\displaystyle X_{i}} is the feature vector (called instance) extracted from the corresponding i {\displaystyle i} -th region in the image and N {\displaystyle N} is the total regions (instances) partitioning the image. The bag is labeled positive ("beach") if it contains both "sand" region instances and "water" region instances. Examples of where MIL is applied are: Molecule activity Predicting binding sites of Calmodulin binding proteins Predicting function for alternatively spliced isoforms Li, Menon & et al. (2014),Eksi et al. (2013) Image classification Maron & Ratan (1998) Text or document categorization Kotzias et al. (2015) Predicting functional binding sites of MicroRNA targets Bandyopadhyay, Ghosh & et al. (2015) Medical image classificationZhu et al. (2017)P.J.Sudharshan et al. (2019) Numerous researchers have worked on adapting classical classification techniques, such as support vector machines or boosting, to work within the context of multiple-instance learning. Definitions If the space of instances is X {\displaystyle {\mathcal {X}}} , then the set of bags is the set of functions N X = { B : X → N } {\displaystyle \mathbb {N} ^{\mathcal {X}}=\{B:{\mathcal {X}}\rightarrow \mathbb {N} \}} , which is isomorphic to the set of multi-subsets of X {\displaystyle {\mathcal {X}}} . For each bag B ∈ N X {\displaystyle B\in \mathbb {N} ^{\mathcal {X}}} and each instance x ∈ X {\displaystyle x\in {\mathcal {X}}} , B ( x ) {\displaystyle B(x)} is viewed as the number of times x {\displaystyle x} occurs in B {\displaystyle B} . Let Y {\displaystyle {\mathcal {Y}}} be the space of labels, then a "multiple instance concept" is a map c : N X → Y {\displaystyle c:\mathbb {N} ^{\mathcal {X}}\rightarrow {\mathcal {Y}}} . The goal of MIL is to learn such a concept. The remainder of the article will focus on binary classification, where Y = { 0 , 1 } {\displaystyle {\mathcal {Y}}=\{0,1\}} . Assumptions Most of the work on multiple instance learning, including Dietterich et al. (1997) and Maron & Lozano-Pérez (1997) early papers, make the assumption regarding the relationship between the instances within a bag and the class label of the bag. Because of its importance, that assumption is often called standard MI assumption. Standard assumption The standard assumption takes each instance x ∈ X {\displaystyle x\in {\mathcal {X}}} to have an associated label y ∈ { 0 , 1 } {\displaystyle y\in \{0,1\}} which is hidden to the learner. The pair ( x , y ) {\displaystyle (x,y)} is called an "instance-level concept". A bag is now viewed as a multiset of instance-level concepts, and is labeled positive if at least one of its instances has a positive label, and negative if all of its instances have negative labels. Formally, let B = { ( x 1 , y 1 ) , … , ( x n , y n ) } {\displaystyle B=\{(x_{1},y_{1}),\ldots ,(x_{n},y_{n})\}} be a bag. The label of B {\displaystyle B} is then c ( B ) = 1 − ∏ i = 1 n ( 1 − y i ) {\displaystyle 