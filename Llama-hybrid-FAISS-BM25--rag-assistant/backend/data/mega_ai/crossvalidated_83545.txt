[site]: crossvalidated
[post_id]: 83545
[parent_id]: 83512
[tags]: 
Sounds to me like you want to fit a structural equation model (SEM)! If your exploratory factor analysis (EFA) has given you a good sense of the latent structure of your two Likert scales (this is a big "if"), it would seem to support a reconceptualization of your IV as three latent factors (similar, but subtly different from components in principal components analysis) with 12 manifest indicators (your items with directly measured Likert scale responses), and your DV as two latent factors with nine manifest indicators. (BTW, each of these is a variable unto itself, FWIW.) If you're generally interested in the overall relationship between your 12 IV indicators and your 9 DV indicators, you can use your EFA results to organize two measurement models for the five latent factors, each representing the relationships of the latent factors with their manifest indicators. I'd recommend fitting these measurement models with confirmatory factor analysis to get a sense of what your fit statistics will look like for each before proceeding further. A number of alternative techniques are available for this, including multidimensional item response theory approaches to item factor analysis (similar to ordered probit models, I think) and confirmatory factor analysis of a polychoric correlation matrix. See also: Factor analysis of questionnaires composed of Likert items . Alternatively, you could just go the simplistic "classical test theory" (CTT) route and average the ratings across each subset of items that loads primarily on a given factor and use that as your factor score, but this won't allow you to estimate measurement error or account for the ordinal nature of Likert scale ratings directly. Averages of Likert scale ratings may approximate continuous dimensions reasonably well if responses for each item are close-to-normally distributed across a good number of response options (e.g., a five-point Likert scale might be just about good enough according to Bollen & Barb, 1981 , though more is better until maybe about 20 according to Nunnally, 1978), and if you're averaging enough Likert scale ratings per factor (again, according to a source I can't recall offhand, you might want about five apiece, which is more than you can do with 21 items and five factors). Note also that this method assumes each item estimates the latent factor equally well (unless you use a weighting scheme you've come up with via other means), whereas a latent common factor measurement model would weigh indicators according to their loadings (correlations with the latent factor they indicate). Either way, once you've got your measurement models nailed down, you're ready to move on to SEM (or path analysis if you're using basic CTT measurement models, in which case factor scores are essentially manifest, measured variables). The following picture depicts (poorly) the structural equation model I think you have in mind, which regresses both of your two latent DV factors onto each of your three latent IV factors. Thus fitting this model means estimating six relationships among the latent factors (at least): It's entirely possible that there are more than these six relationships to estimate among the five latent factors. You might want to consider oblique rotations of the latent structures, which would allow the IV factors to correlate with one another (same for the DV factors). If you want to model nonzero correlations among the IV factors (and among the DV factors), you'd also want to draw three bidirectional arrows (symmetric pathways) connecting these to one another (and one connecting the DV factors, optionally). I also haven't drawn the unidirectional arrows (regressive pathways) from the latent factors to the manifest indicators here, because you haven't said how many items load on each factor. Note that to achieve model identification (and avoid underidentification ) and thus ensure unique solutions for your model's path coefficient estimates, each factor should ideally load onto at least three indicators. With two correlated factors, it's still possible to identify a model with two manifest indicators apiece; still, more is often (probably even usually) better. I also haven't drawn error variances or latent factor variances here. If you fit a latent factor model (as via the aforementioned alternatives to CTT), you'll want to either: estimate error variances (uniquenesses) for each manifest indicator, and fix the latent factor variances to unity / $1$, or... estimate the latent factor variances freely, and estimate error variances for all but one indicator per latent factor, for which you'd fix the loading (the pathway from the latent factor to that indicator) to unity / $1$. These are generally represented in path diagrams like the above as bidirectional arrows that loop around so that both ends point to the same variable. If you'd find it helpful, I can edit these in. I can also edit in suggestions of how to do this for free in R (which I've also provided in my answer to the question I linked above ), but I'm a bit out of my element with SEM in SPSS. You may need to use Amos ("analysis of moment structure"), which is an SPSS program for latent factor modeling, if you want to keep all your analyses in the SPSS family. Once you've fit this model to your data, you'll have estimates of at least three kinds (or just the first of the following kinds if it's a path analysis), and some fit statistics. Your estimates of regressive (asymmetric, one-way) pathways leading from your IV factors to your DV factors can be interpreted like multiple regression pathway coefficients ($\beta$s): they are estimates of the strength of the IV factors' contributions to predictive models of the DV factors. Estimates of regressive pathways from latent factors to manifest indicators are factor loadings, i.e., correlations between the latent factor and the items. Estimates of indicators' error variances (each a bidirectional pathway to and from the same item) are also on the scale of $r$, so they're the square root of the unexplained variance in those items—the unique variance that doesn't correspond to the latent factors in the SEM. You can also fit confidence intervals around each of these estimates if, for instance, you're concerned with judging whether these estimates are on the right side of zero, or different enough from zero to be worth estimating at all. Note that loadings and error variances should always be positive; if any of your error variances are negative, you've got a Heywood case (see What is the precise definition of a "Heywood Case"? ), which is bad news for your model. Note also that modifying your model after fitting it once (e.g., to exclude near-zero pathways or variance estimates that are dragging down your model fit statistics) may introduce some epistemic complications and frustrate some statisticians that don't approve of model modification at all. There are even modification indices you can calculate to help decide when modification is justified, but I won't get further into that here yet, since your model seems pretty straightforward so far. There are tons of fit statistics you can calculate for the SEM, which can get pretty tricky to interpret. Basically, they'll tell you how well your model represents the covariance structure of your data. Things can get particularly ugly here, but bear in mind, they're no less ugly if you don't look at them; you're just less aware of the problems with your model. Fit can be particularly bad for measurement models that you've just obtained from EFA on the same data, so you may want to model correlations among your three IV factors and among your two DV factors in any case, and you may want to allow some items to load on multiple factors, as EFA often produces simple structure only when the user squints hard enough at the factor pattern ( Van Prooijen, & Van Der Kloot, 2001 ). Lei and Wu (2007) recommend calculating (and reporting) the confirmatory fit index (CFI; or RFI), the standardized root mean square residual (SRMR), the root mean square error of approximation (RMSEA), and the likelihood ratio chi-square goodness of fit ($\chi^2$ GoF) statistic. I've seen some arguments against considering or reporting much more of them than this. (E.g., they get a bit redundant, as these four cover most of the general classes of fit statistics that behave very differently, AFAIK, and you wouldn't want to practice or encourage "cherry-picking".) Lei and Wu propose that a "RNI (or CFI) $\ge.95$, SRMR $\le.08$, and RMSEA $\le.06$" generally indicate "good model-data fit," but I've also seen CFI $\ge.9$ and RMSEA $\le.05$ in the past; who's right is a rather deep question I haven't explored yet. Regardless, Lei and Wu's article seems to be a great place to start on all these matters, so I'd strongly recommend reading it through if you like where I've gone with this answer. References - Bollen, K. A., & Barb, K. H. (1981). Pearson's $r$ and coarsely categorized measures. American Sociological Review, 46 , 232–239. Retrieved from http://www.statpt.com/correlation/bollen_barb_1981.pdf . - Lei, P. W., & Wu, Q. (2007). Introduction to structural equation modeling: Issues and practical considerations. Educational Measurement: Issues and Practice, 26 (3), 33–43. - Nunnally, J. C. (1978). Psychometric theory (2nd ed.). New York: McGraw-Hill. - Van Prooijen, J. W., & Van Der Kloot, W. A. (2001). Confirmatory analysis of exploratively obtained factor structures. Educational and Psychological Measurement, 61 (5), 777–792. Retrieved from http://pages.gseis.ucla.edu/faculty/muthen/ED231e/RelatedArticles/Van_Prooijen.pdf .
