[site]: datascience
[post_id]: 44797
[parent_id]: 
[tags]: 
Q learning transition matrix trouble

First, apologies if this isn't the right forum for my question -- let me know if there's a better place. I'm trying to implement Q learning on a two-dimensional grid world, where the agent has four actions available -- movement to the left, right, up, or down. A $4 \times 4$ transition matrix $T$ gives the probability the agent moves in the intended direction. So if $T$ is the identity matrix, and the agent intends to move left, it will move left $100 \%$ of the time; but $T$ can be modified such that the agent moves left $90 \%$ of the time, and up $10 \%$ of the time, etc. My implementation does the following: initializes Q values to 0 places agent in a randomized start location $s$ agent chooses an action $a$ according to an epsilon-greedy strategy the transition matrix is used to determine which direction $i$ the agent actually ends up moving agent moves in direction $i$ , ends up in state $s'$ , collects some reward $r$ , and the Q value $Q(s, a)$ is updated repeat steps 3, 4, 5 as necessary When $T$ is the identity matrix, my implementation returns the correct answer. When I change $T$ , my implementation still returns the answer as though $T$ is the identity. I've spent a lot of time reviewing step 4 above and believe my implementation is correct. When I choose $T$ such that the agent moves in the intended direction $90 \%$ of the time, then $a$ and $i$ match $90 \%$ of the time, exactly as intended. Is there anywhere else this issue could be coming from? Thanks!
