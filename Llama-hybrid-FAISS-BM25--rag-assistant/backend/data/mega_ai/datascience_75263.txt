[site]: datascience
[post_id]: 75263
[parent_id]: 75003
[tags]: 
I glanced over the paper, it seems very interesting. It would be truly fascinating to see if this phase transition from pattern recognition too interpolation of the data actually holds true as the authors claim, though I am a bit sceptical. However, I think you are far away from interpolating your data. Your models are not very complex yet. So for your case we should be able to apply the classical principals of model selection. If you look at the image you provided, figure A has an arrow that reads "sweet spot" . You see how before it the gap between the generalization error and the training error increases, but nonetheless the generalization error still decreases. You are around that spot. Probably you are still before that spot. So I would recommend to still increase the complexity of your model and add layers. Better add some dropout and pooling layers once you notice true overfitting. Usually neural networks become more powerful the deeper they are, since this allows them to learn richer representations of the data. Pooling helps keep the complexity of your model down, and therefore allows you to spend that complexity on the depth of your model. Dropout helps with generalization because it has regularizing properties, as it forces the network to rely on multiple neuron connections for a decision. This makes it less susceptible to being stimulated by a sinlge pattern of your input data. You could interpret dropout as multiple neural networks fusioned into one, so it behaves similar to an ensemble of neural networks. Personally I rely on the classical theory of model selection and it has served me well. Since your dataset is not so large, you should maybe employ something like five-fold-crossvalidation to be surer about your results. Hope I could provide some insights.
