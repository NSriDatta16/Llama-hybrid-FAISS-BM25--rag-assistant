[site]: crossvalidated
[post_id]: 43393
[parent_id]: 43392
[tags]: 
If you have unlimited training data, then the optimal training set size depends on computational considerations, rather than statistical ones. From a statistical point of view, there are many classifiers based on universal approximations, so if you trained on an infinite dataset you would get a classifier that approached the Bayes error and could do no better. If the classifier performs worse a size of the training set increases, that would be a rather worrying sign. If it still does this if you average over multiple random samples of training data, I would suspect there is something wrong with the implementation.
