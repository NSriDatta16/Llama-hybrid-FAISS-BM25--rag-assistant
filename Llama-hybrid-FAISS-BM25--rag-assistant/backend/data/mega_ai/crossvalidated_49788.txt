[site]: crossvalidated
[post_id]: 49788
[parent_id]: 49785
[tags]: 
The second approach, which you're calling Least Squares, is essentially a single perceptron (minus the threshold/activation function). Perceptrons can only learn linearly-separable patterns, while a multi-layer neural network can act as a universal approximator. The classic example of this is XOR (eXclusive OR). The XOR function takes two inputs and returns true (i.e., 1) if and only if exactly one of them is true. This cannot be learnt by a perceptron, but you can build a simple neural network with 3 layers that can learn the xor function. There's a diagram of a suitable network on wikipedia.
