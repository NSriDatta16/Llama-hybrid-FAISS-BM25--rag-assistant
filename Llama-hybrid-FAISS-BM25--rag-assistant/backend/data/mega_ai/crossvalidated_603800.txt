[site]: crossvalidated
[post_id]: 603800
[parent_id]: 603048
[tags]: 
My first approach would be to use a multinomial logistic regression and defined a feature that corresponds to the a priori class proportion. For example, as a log-linear model , if $p_0$ is the simplex vector of class proportion, you could use the feature $x = \log(p_0)$ . Then suppose you have a multinomial regression with only this feature, you would have $P(X = i) = \operatorname{softmax}(x * \beta)$ . If $\beta=1$ , you get back $P(X) = p_0$ . Essentially $\beta$ tells you how informative that feature is, e.g. $\beta = 0$ you get back a uniform distribution. So one advantage of this approach is that you can learn $\beta$ . And of course, you can add other features that are predictive of the class. They could be the predictions from your random forest model. An alternative would be to go Bayesian, you could build a similar multinomial logistic regression, e.g.: $$ P(X = i) = \operatorname{softmax}(log(\theta) + X * \beta) $$ Where $\theta$ corresponds to "true" proportions to be inferred, and $X * \beta$ the rest of your regression. The difference is that you would define a prior for $\theta$ based on the proportion that you expect. For example, something like: $$ \theta \sim \operatorname{Dirichlet}(\alpha * p_0) $$ Where $\alpha$ is a predefined pseudo sample size that controls how informative the prior is. You could code such model using a probabilistic programming language such as Stan (or the R package brms , a wrapper of Stan).
