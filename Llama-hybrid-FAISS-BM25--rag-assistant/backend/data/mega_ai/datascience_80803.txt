[site]: datascience
[post_id]: 80803
[parent_id]: 40957
[tags]: 
The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks is a seminal paper from a couple of years ago that demonstrates that there is a much smaller subnetwork within any neural network that is doing most of the work. They describe their approach to pruning a neural network in the paper. However, they are keeping the best weights, not the best nodes. The important factor affecting a neurons output is not the weights, but the balance of weights (i.e. how each input is weighted relative to the other inputs). I suggested that if youâ€™re looking to keep the most important weights, you take the approach in the above paper. If you want to keep the best neurons, look at the relative importance of each neuron by looking at the relative weight assigned to it by neurons in the following layer.
