[site]: datascience
[post_id]: 123882
[parent_id]: 122103
[tags]: 
I know this question was asked months ago, but just wanted to chime in in case this link is useful: https://huggingface.co/learn/nlp-course/chapter7/3?fw=pt Shows how to fine-tune a masked language model. Not sure what your goal is for a downstream task, but I ended up using a base BERT model, then "fine-tuned" it on new domain specific data with a masked language approach. Essentially, I considered this continued training since I had no intention of using the model for masked language tasks. Once you've continued training, you have a better base model you can fine-tune for the task you are actually interested in. I used this new BERT model for sentiment and sentence similarity after fine-tuning for those tasks... Oh, just notice that you are interested in using the model for named entry recognition. BERT models are suited for that task. Another advantage of a base BERT model is that if you want to feed it heaps of data it's pretty easy to do since all you are doing in the continued training is masking x percent of the words in the text.
