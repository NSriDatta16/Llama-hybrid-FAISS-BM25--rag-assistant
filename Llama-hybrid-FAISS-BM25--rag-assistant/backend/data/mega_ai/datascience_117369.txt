[site]: datascience
[post_id]: 117369
[parent_id]: 
[tags]: 
Interpreting Learning Curves of models

I need some help to understand if the models are overfitting and which of these we can consider "the best". On the internet i only find simple examples with learning curves but in these cases i'm not sure to interpret them, so thank you in advance. It's a binary classification problem, the classes in the dataset are quite balanced. The first model is a Random Forest with all the features of the dataset: The second one is a KNN Classifier with all the features: Then I selected only 4 features of the dataset and I applied the models (using gridsearchcv so changing hyperparameters), this is Random Forest again: The last one is KNN Classifier with only 4 features: Do the first 2 models have some problems looking at the learning curves? And looking at the last 2 models, they have improved comparing to the first 2? I see that the accuracy is worst in the second cases but maybe they are more solid models?
