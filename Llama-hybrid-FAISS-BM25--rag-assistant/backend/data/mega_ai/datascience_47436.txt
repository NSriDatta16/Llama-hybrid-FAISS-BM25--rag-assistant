[site]: datascience
[post_id]: 47436
[parent_id]: 
[tags]: 
Why a Random Reward in One-step Dynamics MDP?

I am reading the 2018 book by Sutton & Barto on Reinforcement Learning and I am wondering the benefit of defining the one-step dynamics of an MDP as $$ p(s',r|s,a) = Pr(S_{t+1},R_{t+1}|S_t=s, A_t=a) $$ where $S_t$ is the state and $A_t$ the action at time $t$ . $R_t$ is the reward. This formulation would be useful if we were to allow different rewards when transitioning from $s$ to $s'$ by taking an action $a$ , but this does not make sense. I am used to the definition based on $p(s'|s,a)$ and $r(s,a,s')$ , which of course can be derived from the one-step dynamics above. Clearly, I am missing something. Any enlightenment would be really helpful. Thx!
