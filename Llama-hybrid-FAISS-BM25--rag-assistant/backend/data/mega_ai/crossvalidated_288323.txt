[site]: crossvalidated
[post_id]: 288323
[parent_id]: 288095
[tags]: 
No machine learning algorithm requires one hot encoding. It is one method for dealing with categorical variables. Dummy variables is another. Traditionally, dummy variables was the preferred solution. For example, the R function lm() automatically creates dummy variables for categorical data. If you are using python and scikt-learn then I believe many of it's algos require one-hot encoding of categorical variables. I believe that tensorFlow also requires one-hot encoding. These are choices of how the variable is encoded. There is no reason why dummy variables couldn't be used in the code instead. This all has to deal with the actual code implementation of the algorithm. As hxd1011 points out the issue of describing the 'distance' between categorical variables is a delicate issue. In addition to the distances mentioned there is also Jaccard distance. Some ML methods, particularly SVM's are inappropriate for categorical data and adding categorical variables can/will (either, both, you decide) lead to models with very poor predictive power. Most ensemble models handle categorical data 'as is' and require no pre-processing.
