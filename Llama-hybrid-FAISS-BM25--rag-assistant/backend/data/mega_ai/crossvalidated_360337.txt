[site]: crossvalidated
[post_id]: 360337
[parent_id]: 
[tags]: 
The meaning of coefficients in Multiple Linear Regression

So I am learning about linear regression. The coefficient is the slope of the function, which means how much the dependent variable change due to change of the independent variable. So I make an linear regression first with only one IV. The coefficient is positive 0.2708, we can infer that relationship between the IV and DV is positive. I post the results below. We can see that the R-squared is 0.516, so the IV explain only half of the variance in the DV. I guess this means that the IV is not the best in explaining the DV? Please correct my interpretation if I am wrong. OLS Regression Results ============================================================================== Dep. Variable: total_goal_count R-squared: 0.516 Model: OLS Adj. R-squared: 0.515 Method: Least Squares F-statistic: 404.7 Date: Thu, 02 Aug 2018 Prob (F-statistic): 9.15e-62 Time: 09:25:41 Log-Likelihood: -837.54 No. Observations: 380 AIC: 1677. Df Residuals: 379 BIC: 1681. Df Model: 1 Covariance Type: nonrobust ======================================================================================= coef std err t P>|t| [0.025 0.975] --------------------------------------------------------------------------------------- position_difference 0.2708 0.013 20.118 0.000 0.244 0.297 ============================================================================== Omnibus: 0.714 Durbin-Watson: 1.726 Prob(Omnibus): 0.700 Jarque-Bera (JB): 0.783 Skew: 0.101 Prob(JB): 0.676 Kurtosis: 2.907 Cond. No. 1.00 ============================================================================== So I add another variable to the regression. The results are as follows: OLS Regression Results ============================================================================== Dep. Variable: total_goal_count R-squared: 0.746 Model: OLS Adj. R-squared: 0.745 Method: Least Squares F-statistic: 554.9 Date: Thu, 02 Aug 2018 Prob (F-statistic): 3.43e-113 Time: 09:35:06 Log-Likelihood: -715.26 No. Observations: 380 AIC: 1435. Df Residuals: 378 BIC: 1442. Df Model: 2 Covariance Type: nonrobust ======================================================================================== coef std err t P>|t| [0.025 0.975] ---------------------------------------------------------------------------------------- position_difference -0.0024 0.018 -0.135 0.893 -0.037 0.032 avg_total_goal_count 0.0268 0.001 18.479 0.000 0.024 0.030 ============================================================================== Omnibus: 12.317 Durbin-Watson: 1.993 Prob(Omnibus): 0.002 Jarque-Bera (JB): 12.617 Skew: 0.436 Prob(JB): 0.00182 Kurtosis: 3.191 Cond. No. 22.3 ============================================================================== position_difference now has a negative coefficient when we add another variable, what can we interpret from this? Both attributes are positively correlated with the DV otherwise. But what can we learn from their interaction in the regression. I am studying the effect of different variables on the amount of goals scored in Premier League games if that is of any relevance. So position_difference is the difference in league positions between the teams, the numbers go from 1-19. avg_total_goal_count is the average of the number of goals scored in matches in which the two teams are playing.
