[site]: crossvalidated
[post_id]: 606688
[parent_id]: 606684
[tags]: 
Introduction Since your question is fairly broad, I will try to at least provide a very brief rundown of a normally-distributed (aka Gaussian) regression summary. I use R as an example, but you can generalize this to any stats software. Here I fit a multiple linear regression, which is simply a regression with more than one predictor. #### Fit Model #### fit The formula for this linear regression is additive, and can be formally designated as so: $$ \text{Petal Length} = \beta_0 + \beta_1\text{Petal Width} + \beta_2\text{Sepal Width} + \epsilon $$ For this specific data, we are measuring flower measurements in cm. The length of the petals is our outcome variable or y . The $\beta_0$ part is the intercept and is the conditional mean when all other factors are accounted for (your normal mean of petal length in this data is 3.758, but this will be "split up" by the slopes in the regression). The other $\beta$ terms here are the slopes. You simply multiply the values here with the raw units of the term given. For example, if our intercept is 5 and the slope of $\beta_1$ is .56, a 2 point increase would be $.56 \times 2 = 1.12$ , and added to the intercept would be $5 + 1.12 = 6.12$ , which would mean our predicted petal length would be 6.12 cm. The $\epsilon$ part is our "error" term, which is how off our regression is at predicting the outcome. All of this is more salient when looking at the regression summary, I'm just prefacing it for now. Summary of Regression Anyways, let's check out what these values actually are below with the summary function in R. #### Summarise Model ### summary(fit) The summary is shown below: Call: lm(formula = Petal.Length ~ Petal.Width + Sepal.Width, data = iris) Residuals: Min 1Q Median 3Q Max -1.33753 -0.29251 -0.00989 0.21447 1.24707 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 2.25816 0.31352 7.203 2.84e-11 *** Petal.Width 2.15561 0.05283 40.804 A lot of info is here and may vary from other stats programs but I'll go through each part in turn. Call Call: lm(formula = Petal.Length ~ Petal.Width + Sepal.Width, data = iris) This isn't in every program, but this basically says how you specified the regression. This isn't anything special other than to confirm your regression is correctly specified. Residuals Residuals: Min 1Q Median 3Q Max -1.33753 -0.29251 -0.00989 0.21447 1.24707 This part explains how "off" your data is by specifying how far the predicted data is from the actual raw data. For example, the max value says that the worst prediction above the regression line is about 1.28 cm whereas the worst below the regression line is about 1.34. The median should always be close to zero, as the average of your residuals should be normally distributed and this measure should reflect that. To show you what this looks like practically, below is the regression line with the residuals plotted over it. The lines signify how "off" our model is, and the max/min of that distance is indicated by our residuals section. Coefficients Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 2.25816 0.31352 7.203 2.84e-11 *** Petal.Width 2.15561 0.05283 40.804 These are the terms in our model and arguably the most important section. The intercept is $\beta_0$ we mentioned earlier. Our conditional mean of petal length is around 2.26 cm. Petal.Width and Sepal.Width are our predictors, their slopes equating to $\beta_1 = 2.16$ for Petal.Width and $\beta_2 = -0.36$ for Sepal.Width. Together this means our linear equation is: $$ \text{Petal Length} = 2.26 + (2.16\times\text{Petal Width}) +(-.36\times\text{Sepal Width}) + \epsilon $$ If a flower's petal width is 2 and it's sepal width is 4, our equation should look like this: $$ \text{Petal Length} = 2.26 + (2.16\times{2}) +(-.36\times{4}) $$ Which would simplify to the following solution, our predicted outcome: $$ \text{Petal Length} = 8.02 $$ For the other parts, the standard error indicates how much accurate each slope is (how much error in estimation), which gets used for the t-value, a formal test of the slope to see if it is significant. The p-values use the t-value and SE term to determine if this is the case, with most accepting a value as significant if $p (though this varies by discipline and has controversial interpretations. Additional Output Residual standard error: 0.4574 on 147 degrees of freedom Multiple R-squared: 0.9338, Adjusted R-squared: 0.9329 F-statistic: 1036 on 2 and 147 DF, p-value: This information gives additional insights about the whole regression model. The residual standard error indicates how well a regression model fits a dataset and is essentially the "standard deviation of the residuals." A higher number indicates higher inaccuracy, but this can be somewhat relative. The degrees of freedom of the residuals is formally: $$ df=n−k∗ $$ with $df$ indicating degrees of freedom, $n$ equaling the number of observations, and $k$ equaling the number of betas used. Since we have one intercept and two slopes ( $\beta_0 + \beta_1 + \beta_2$ = 3), our $k=3$ , so $df = 150 - 3 = 147$ . R-squared and adjusted R-squared are similar and will equal the same for a simple regression, but both indicate how much your model "explains" the variation in the outcome. The difference is that adjusted R-squared penalizes overfitting with many terms, so it is generally advised to use this number over the normal R-squared in a multiple linear regression. In our case, the adjusted R-squared equals .93, indicating that almost all of the variance (93%) can be explained by the model. Finally, the f-statistic and associated p-value convey if the model is significant. Summary and References So that is essentially the output of a regression. Some useful books on the subject are: Regression and Others Stories by Hill et al., 2020. Learning Statistics with R by Navarro, 2012 The second book is particularly useful because it has a user-friendly approach to writing about statistics (the section on linear regression is in Chapter 15).
