[site]: datascience
[post_id]: 26840
[parent_id]: 
[tags]: 
What principle is behind semantic segmenation with CNNs?

I have been learning about segmentation models recently and was reading this paper today: Dense Transformer Networks . On page 2, it describes segmentation as: Given the success of deep learning methods on image-related applications, numerous recent attempts have been made to solve dense prediction problems using CNNs. A central idea of these methods is to extract a square patch centered on each pixel and apply CNNs on each of them to compute the label of the center pixel. The efficiency of these approaches can be improved by using fully convolutional or encoder-decoder networks. Specifically, fully convolutional networks [23] replace fully connected layers with convolutional layers, thereby allowing inputs of arbitrary size during both training and test. In contrast, deconvolution networks [25] employ an encoder-decoder architecture. The encoder path extracts high-level representations using convolutional and pooling layers. The decoder path uses deconvolutional and up-pooling layers to recovering the original spatial resolution. In order to transmit information directly from encoder to decoder, the U-Net [28] adds skip connections [14] between the corresponding encoder and decoder layers. A common property of all these methods is that the label of any pixel is determined by a regular (usually square) patch centered on that pixel. So, I know that the models used take a single image as an input and output a segmentation mask for the entire image as an output. Are the authors here writing that the underlying operation being performed by the models is to extract patches and classify them, while this is all done in parallel? I would assume these patches correspond to the receptive field of the network, if that is what they are saying. My overall question is just hoping for an explanation of how this statement is actually what is being performed in a segmentation model.
