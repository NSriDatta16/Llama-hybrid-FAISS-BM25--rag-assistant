[site]: datascience
[post_id]: 115353
[parent_id]: 
[tags]: 
How does BERT produce CLS token? Internally does it do max-pooling or avarage pooling?

I ran experiment to compare max-pooled word tokens vs CLS token for sentence classification and CLS clearly wins. Trying to understand how BERT generates CLS token embedding if its better than max or avg pooling.
