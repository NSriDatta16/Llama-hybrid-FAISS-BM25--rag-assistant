[site]: crossvalidated
[post_id]: 285676
[parent_id]: 
[tags]: 
Interpreting neural network weights

I created a neural network model for a classification task based on 14 variables, 1 hidden layer of size 8. Outputs give 3 possible classes. The weights I got from input (left) and output (right) layers: I struggle to interpret the relevance of weights attributed to the variables (v1, v2, ...) among the neurons (n1, n2,...). The variables v4 and v9 are initially guessed as most relevant for classifying. However, they have very distinct weights among the neurons (17 or 10 to -12). Are negative weights as important as positive weights? In the output layer, v7 always has negative weights, which reduces the importance of the highest weight of v4 on n7. I also did repeated runs of independent training sessions to check the behavior of the model, and then I summed the total positive and negative weights of each variable from the input layer each time. The results showed that v4 and v7 are apparently always top ranked (see below). But considering the output layer, and the distribution of negative weights, I realized that v4 and v7 are perhaps not the best variables for classification.
