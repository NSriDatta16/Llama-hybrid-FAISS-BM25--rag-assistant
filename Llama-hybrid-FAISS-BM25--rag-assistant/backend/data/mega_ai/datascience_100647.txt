[site]: datascience
[post_id]: 100647
[parent_id]: 100643
[tags]: 
If the classes are mutually exclusive then ideally you should use categorical cross entropy for the loss function. Binary loss should still work after a fashion though, since it will still encourage incorrect classes to predict low probability, and correct classes high. You do not appear to be renormalising the value of aiOutPossible before using categorical_crossentropy . You should definitely do so, because without that step, your class probabilities will not sum to 1, which will skew calculations significantly. I think something like: aiOutPossible = aiOutPossible / aiOutPossible.sum() before calling tf.keras.backend.categorical_crossentropy should do the trick in your custom loss function. You may also find that you don't really need this custom loss function during training - the extra benefit of focussing on only relevant outputs may be offset by not penalising high/confident scores for impossible outputs. So you may also consider training without the filter and custom loss, only using the filter when scoring test data and in production. This will allow you to use some built-in gradient calcutions in TensforFlow which are more numerically stable ( use from_logits=true ). I cannot say whether this will actually help with any confidence, but it is worth checking IMO as it will simplify your training process.
