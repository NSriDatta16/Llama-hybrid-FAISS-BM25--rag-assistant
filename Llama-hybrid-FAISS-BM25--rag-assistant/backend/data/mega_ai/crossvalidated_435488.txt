[site]: crossvalidated
[post_id]: 435488
[parent_id]: 
[tags]: 
error vs. deviation vs. difference

What exactly is the difference between the way these terms are used in statistics? As far as I can tell they mean the same thing mathematically, but it's unclear to me whether and how usage depends on context. Are error and deviation interchangeable, or is it that they're mathematically equivalent, but have different usage, such as that error is preferred when comparing an observed (uncertain) value to a known true or theoretical value, whereas deviation is preferred if an observed value is compared to another observed value that's also subject to uncertainty? (The tag wiki for error seems to imply the latter, but there are no tags for deviation or difference.) Are deviation and difference interchangeable, or is deviation a special case of difference, and if so, what specifically is the distinction? Why is deviation sometimes used in the terminology, and difference at other times, when they both refer to the mathematical difference between two values? Discussion and examples These words seem to sometimes be used interchangeably within statistical terms, but other times swapping one for another completely changes the meaning. For example: mean absolute percentage error (MAPE) and mean absolute percentage deviation (MAPD) are mathematically identical, and some references ( such as Wikipedia ) say that MAPE is also called MAPD. I've also seen mean absolute percentage difference , for apparently the same calculation, and I've encountered the term mean absolute relative difference (MARD), which as far as I can tell is the same calculation as MAPE and MAPD*. This article uses both mean absolute percentage difference and ****mean absolute percentage error** in a way that implies that they're interchangeable This one uses both terms in a way that seems to imply that error denotes a difference between an uncertain value and known value, whereas deviation denotes a difference between two uncertain values mean absolute error (MAE) and mean absolute deviation (MAD), on the other hand, have completely different meanings. MAE is the mean of all the absolute errors, whereas MAD is like standard deviation but without squaring the individual deviations before averaging them and then taking the square root. The terms mean signed deviation and mean signed difference , both abbreviated MSD, are mathematically the same. There is also mean error (ME), which apparently has more than one meaning , but one of them is, as far as I can tell, the same as MSD. The Wikipedia entry for MSD expands it as "mean signed difference, deviation, or error", but uses the abbreviation MSD for all of them and doesn't mention ME, and I haven't seen the abbreviation MSE anywhere in statistics. Use case: When evaluating the accuracy of an assay instrument's results by comparing them with results for the same sample from a reference instrument that's known to be reliable, would it be correct/preferred/acceptable to refer to the difference as error, or would it be necessary (or preferable) to refer to the difference as deviation, since there is uncertainty in the reference instrument's results, and reserve the term error for when the assay instrument is evaluated by testing it with a control solution whose exact concentration of the tested analyte is known? * Don't bog down on the term "relative" as opposed to "percentage". MARD does seem to be a misnomer in that sense, since it's always expressed as a percentage rather than a proportion, but that's not my point of interest. I'm focus is on the fact that there are three terms that are mathematically the same and use the terms error, deviation, and difference apparently in the same sense.
