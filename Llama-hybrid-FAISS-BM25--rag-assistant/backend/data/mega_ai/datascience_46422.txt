[site]: datascience
[post_id]: 46422
[parent_id]: 
[tags]: 
Music Generation LSTM not learning (Keras)

I am trying to train a RNN in keras to produce music but I am having difficulty training it. The loss seems to remain fairly high and constant despite me changing the hidden size and number of layers and when it does sometimes train it often outputs the same note for every input, outputting a song of just one note repeated. Is this a problem with my data or how can I fix my model? The notes are extracted from the midi file and then one hot encoded. def read_midi(path,trans='C'): songs = glob(path) notes = [] offsets = [] durations =[] for file in songs: print(file) try: midi = converter.parse(file) except: continue parts = instrument.partitionByInstrument(midi) if parts is not None: notes_to_parse = parts.parts[0].recurse() else: notes_to_parse = midi.flat.notes for element in notes_to_parse: if isinstance(element,note.Note): notes.append(str(element.pitch)) offsets.append(element.offset) durations.append(element.quarterLength) elif isinstance(element,chord.Chord): notes.append('.'.join(str(n) for n in element.normalOrder)) offsets.append(element.offset) durations.append(element.quarterLength) return notes,offsets,durations In this case I am just using the pitch just to see if I can get the RNN to train but I also planned to use the duration and change in offset as another feature by one-hot encoding combinations of the 3. The sequences are generated as follows pitchnames = sorted(list(set(data))) seq_length = 100 classes = len(pitchnames) num_class = {i:ch for i,ch in enumerate(pitchnames)} class_num = {ch:i for i,ch in enumerate(pitchnames)} input_data = [class_num[x] for x in data] input_temp =[] output_temp=[] for i in range(0,len(input_data)-seq_length,1): input_temp.append(input_data[i:i+seq_length]) output_temp.append(input_data[i+seq_length]) sequences = len(output_temp) x = np.reshape(input_temp,(sequences,seq_length,1)) x -= int(np.mean(x)) x = x/classes y = keras.utils.to_categorical(output_temp) assert classes == y[0].shape[0] music_model = Sequential() music_model.add(CuDNNLSTM(512,input_shape=(seq_length,1),return_sequences=True)) music_model.add(Dropout(0.5)) music_model.add(CuDNNLSTM(512)) music_model.add(Dense(256)) music_model.add(Activation('relu')) music_model.add(Dropout(0.4)) music_model.add(Dense(classes)) music_model.add(Activation('softmax')) music_model.compile(loss='categorical_crossentropy',optimizer='RMSprop') filename = './ML/keras_rnn/music_weights.{epoch:02d}-{loss:.2f}.hdf5' callbacks = [ModelCheckpoint(filename,monitor='loss',save_best_only=True)] music_model.fit(x,y,callbacks = callbacks,epochs=1000,batch_size=64) The loss seems to stop decreasing at around 5.03 and doesn't decrease further over the epochs. Any help would be greatly appreciated
