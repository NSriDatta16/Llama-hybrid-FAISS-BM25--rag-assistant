[site]: crossvalidated
[post_id]: 301442
[parent_id]: 
[tags]: 
Classifier accuracy decreases as n of n-gram models increases. Is this expected?

I am trying to tackle a multi-classification problem that requires text processing. The data contains a lot of samples (approximately 100.000 samples) and one of the features I need to work with is a short (sometimes longer) written message. Dividing the dataset into a 70/30 scheme and cross-validating on the 70% for tuning, the model reaches currently 90% accuracy. I am using scikit-learn's TfidfVectorizer to extract features from the messages and RandomForestClassifier for the machine learning per se. What I find peculiar is that as I increase the maximum n for the n-gram models considered, its accuracy decreases slightly : n in [1,2[ -> Accuracy: 91.0% Dimensionality: 126,756 n in [1,2] -> Accuracy: 90.8% Dimensionality: 605,010 n in [1,3] -> Accuracy: 90.5% Dimensionality: 1,408,346 ... -> ... I initially thought that increasing the maximum n could only increase the model's accuracy but that is not reflected in my results. Am I doing something wrong in my approach? Or is this decrease in accuracy something that could potentially happen?
