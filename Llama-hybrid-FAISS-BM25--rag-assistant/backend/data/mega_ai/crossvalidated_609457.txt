[site]: crossvalidated
[post_id]: 609457
[parent_id]: 567071
[tags]: 
With neural networks, you would do this by having multiple outputs: the (logit)-probability of being in each category from a to d (that's the easy bit with not too many choices) AND one of the following options (at least these two make sense): the (logit)-probability of being in each category-subcategory combination (i.e. falling into a1,a2,a3,a4, b1,b2, c1 or d1 ) treating these are a separate categorical problem to the first one: downside: does not enforce that the probability of being in a1 to a4 adds up to the probability of being in a . It also sort of double-counts categories c and d , where there's really no sub-category. It's very straightforward to implement though. the (logit)-probability of being in sub-category 1 to 4 conditional on being in category a and the (logit)-probability of being in sub-category 1 or 2 conditional on being in category b : enforces the constraints (you can also output the outputs of the above, but internally set-up the model like this, it's really the same thing). As a loss function, you would not incur any loss for the first output when the category is b , c or d , but use categorical crossentropy for the choice of 1 to 4 when the category is a . Similarly, the loss function of the second target would only apply a loss function when the category is b . With things other than neural networks, it's hard to do it all in a single model. However, you could have sequential models (i.e. first one that predicts the category and then for categories a and b two models that predict the subcategory). These, would only be trained on the subset of observations in category a and b .
