[site]: crossvalidated
[post_id]: 364981
[parent_id]: 
[tags]: 
Intuitive or quantitative explanation of why we care about mean average precision (mAP) for CNN classifiers?

Consider CNN classifiers applied to some image classification tasks: to fix ideas, let's consider the ImageNet Challenge, where each image belongs to 1 of 1000 nonoverlapping classes, even though the question is more general. Usually, when people summarize the progress in time of CNN performance on ImageNet, the mAP metric (mean Average Precision) is reported. Given the precision-recall curve of the classifier for a specific class , $AP$ (Average Precision) is usually defined as the average of precision $P(R)$ over $N=11$ equally spaced recall values $R_1=0, R_2=0.1,\dots,R_N=1.0$: $$ AP =\frac{1}{N}\sum_{i=1}^{N} P(R_i)$$ mAP is then defined as the average of $AP$ over all classes (e.g., 1000 classes for the ImageNet Challenge). I get the reason for the average over all classes (i.e., the "m" in "mAP"), but I don't see the need for averaging over the precision-recall curve. The precision-recall curve is used to choose a classification threshold for a classifier. However, this doesn't resemble the way CNN classifiers are used in practice. For each input image, a CNN classifier outputs 1000 numbers in $[0,1]$ (one for each class), and it classifies the image to the class for which this number is maximum 1 . It doesn't actually use a threshold at all. And this is the way they're used in practice: nowadays you can even download apps on iOS/Android and see that they classify an object to the maximum probability class. So, why don't just report the average proportion of correctly classified cases over all classes? I think this has to do with the fact that "accuracy is an improper scoring rule". I don't know exactly what it means, but again, I've seen this sentence used in the context of setting a threshold for a classifer. a CNN classifies without any threshold, so why do we need "a proper scoring rule" (whatever that means) to study the progress of CNN classifiers over time? 1 I'm not calling these numbers "estimated probabilities" because it's pretty well known that these estimates are not calibrated at all for modern CNN architectures, unlike for old shallow MLPs. Thus interpreting them as probabilities is highly questionable.
