[site]: stackoverflow
[post_id]: 2886705
[parent_id]: 2878429
[tags]: 
I was actually able to basically get what I was looking for by doing this in pure Ruby. First I create a two dimensional array where each column is a user and each row is an interest. Each value in the array is a 0 or a 1 depending on whether the current user has that interest. This array is stored in memory with functions to add or modify rows and columns. Then when I want to calculate the users with similar interests to the current user I add up all the columns for each row where the column is set to "1" for the current user. This means I need to iterate through 10,000 columns and run an average of 50 addition operations per column followed by a sorting operation at the very end. You might guess that this takes a very long time, but it's actually about 50-70 milliseconds on my machine (Core 2 Duo, 3ghz. Ruby 1.9.1), and about 110 milliseconds on our production servers. The nice thing is that I don't need to even limit the result set. Here is the ruby code I used to test my algorithm. USERS_COUNT = 10_000 INTERESTS_COUNT = 500 users = [] 0.upto(USERS_COUNT) { |u| users[u] = rand(100000)+100000 } a = [] 0.upto(INTERESTS_COUNT) do |r| a[r] = [] 0.upto(USERS_COUNT) do |c| if rand(10) == 0 # 10% chance of picking an interest a[r][c] = 1 else a[r][c] = 0 end end end s = Time.now countable_rows = [] a.each_index { |i| countable_rows x[0] } puts Time.now.to_f - s.to_f The first few lines are used to create a simulated two dimensional array. The rest of the program runs the algorithm as I described above. The algorithm above scales reasonably well for a while. Obviously it would not be suitable for 50,000+ users, but since our product segments communities into smaller groups this method works quite well (And much faster than SQL). Any suggestions on how it could be tuned for even better performance are welcome.
