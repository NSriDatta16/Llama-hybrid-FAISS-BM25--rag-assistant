[site]: crossvalidated
[post_id]: 547658
[parent_id]: 512089
[tags]: 
I think you should look at the Hopfield Networks is All You Need , where they make the case that attention, as used in transformers, works similarly Hopfield networks. Also, they use it in their Modern Hopfield Networks and Attention for Immune Repertoire Classification paper for computational biology and get pretty decent results. Are they the best at something? Well, if they really are equivalent to attention, then well, they are equal to transformers which are currently the best architectures for several language tasks like translation. This is a work in progress, but there are clearly people who are trying to make the case that they at the very least can be the best at something.
