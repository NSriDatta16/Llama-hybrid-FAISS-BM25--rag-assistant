[site]: datascience
[post_id]: 37206
[parent_id]: 
[tags]: 
How does action get selected in a Policy Gradient Method?

As I understood, in Reinforcement-Learning a big difference between a Value-based method and a Policy-gradient method is how the next action is selected. In Q-learning (Value-based method), each possible action gets a score. We then select next action that has a highest score, with a slight possibility to select any other action at random. How does a policy gradient method decide upon an action? We are no longer allowed to select the highest-scored action, as this will leave us with a non-differentiable policy, that can't be improved. The actions still have scores - do we select them at random ...using the "cummulative distribution function"? Also, am I using the right term? In other words, do actions that have a higher score occupy more space and are more likely to be selected? Edit: there is a really awesome lecture about policy grads. Unfortunatelly the video is deliberately unlisted, so that normal people can't really get to it - but I am for free education, so here it is: CS294-112 9/6/17
