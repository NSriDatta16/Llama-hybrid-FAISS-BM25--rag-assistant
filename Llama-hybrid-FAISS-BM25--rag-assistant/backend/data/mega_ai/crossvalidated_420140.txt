[site]: crossvalidated
[post_id]: 420140
[parent_id]: 
[tags]: 
Why do lots of people want to transform skewed data into normal distributed data for machine learning applications?

For image and tabular data, lots of people transform the skewed data into normally distributed data during preprocessing. What does the normal distribution mean in machine learning? Is it an essential assumption of machine learning algorithms? Even the image data, I've seen quantile transformation, which transforms the whole pixels of an image to follow normal or uniform distribution. I can think of one reason: to avoid the influence of outliers. But these transformation distort the original distribution of data. Why is the normal distribution so important to machine learning that lots of preprocessing includes this step?
