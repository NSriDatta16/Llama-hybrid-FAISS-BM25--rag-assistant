[site]: crossvalidated
[post_id]: 617856
[parent_id]: 
[tags]: 
mlr3 : Benchmark different features selection methods

I have a simple question concerning my methodology. I'm building some algorithms of machine learning to predict a binary outcome, using mlr3. I optimized my different learners (svm, ranger, glmnet, xgboost, rpart) on the train + validation sets. Now, I'm trying to understand the influence of the feature selection on the results (does it improve it ?). I focused then on different methods of features selection (I started with 900 features on my native dataset): -Pearson correlation + wilcoxon test : 14 features. -Feature importance with xgboost: I selected the top 20 features. -Information gain with rpart : 1 feature alone... -Embedded method with rpart : 17 features. -I also combined all the previous methods and selected the most picked up features (5 features) But here is the problem : If I compare multiple learners with these methods on the validation set, I suppose the xgboost will perform much better with the feature importance because it selected these features for this set specifically...(for example) It is a huge bias for me... How could I do then ? Compare the methods of feature selection on the test set directly ?
