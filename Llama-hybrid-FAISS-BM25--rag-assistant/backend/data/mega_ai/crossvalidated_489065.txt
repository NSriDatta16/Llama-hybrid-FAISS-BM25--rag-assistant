[site]: crossvalidated
[post_id]: 489065
[parent_id]: 460161
[tags]: 
Here is my current understanding to my own question. It probably related BERT's transfer learning background. The learned-lookup-table indeed increase learning effort in pretrain stage, but the extra effort can be almost ingnored compared to number of the trainable parameters in transformer encoder, it also should be accepted given the pretrain stage one-time effort and meant to be time comsuming. While in the finetune and prediction stages, it's much faster because the sinusoidal positional encoding need to be computed at every position.
