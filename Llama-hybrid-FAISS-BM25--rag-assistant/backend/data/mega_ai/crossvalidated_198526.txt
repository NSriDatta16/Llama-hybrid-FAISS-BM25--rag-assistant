[site]: crossvalidated
[post_id]: 198526
[parent_id]: 198463
[tags]: 
I believe that your last pooling layer does not match the All-CNN article's architecture. They are doing a 6x6 avg pooling, it seems that your h_conv10_pooled is doing a 2x2 non-overlapping avg pooling. A far less important point, but it is worth pointing out nonetheless: it seems that you have initialized all your weight layers using a Gaussian with standard deviation .05. I took a quick look at the All ConvNet article and they do not seem to list their weight initialization scheme. I also did not find any reference to "Glorot" or "He" (also referred to as "MSRA" or "MSR" for Microsoft Research Asia or Microsoft Research, respectively) which are very commonly used initialization schemes. Unless I'm overlooking some small coding error, it may be that you are using a different initialization scheme than the original authors? If fixing the pooling kernel doesn't work, you could email the authors and seeing if they could share the weight initialization scheme or try the aforementioned weight initialization schemes: Xavier Glorot and Yoshua Bengio's scheme in addition the He/MSR initialization that more recently came out; I believe both are readily available in TensorFlow. You may see some improved accuracy and some performance gains. I haven't tried benchmarking the All-CNN so I cannot be certain. But if I had to bet the 6x6 avg pooling kernel may be the source of the inability to reproduce their experimental results.
