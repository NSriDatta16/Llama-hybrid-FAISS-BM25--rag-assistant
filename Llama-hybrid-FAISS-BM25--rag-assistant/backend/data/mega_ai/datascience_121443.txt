[site]: datascience
[post_id]: 121443
[parent_id]: 
[tags]: 
What is the l2-norm of a scalar

What is the meaning of the l2-norm when dealing with scalar values? I'm assuming it would be the same thing as taking the absolute value. For context: I am trying to implement the clustering method presented in the following paper: https://arxiv.org/abs/1909.11832 where I stumbled across the following loss function: $$ L_{E,G}(\phi(t),\theta(t)) = ||x-\hat{x}||_2^2+\lambda||C_\psi(\hat{x}_a)||_2^2 $$ where E and G are neural networks parameterized by $\phi$ and $\theta$ , respectively. These are set up as an autoencoder. C is another neural network parameterized by $\psi$ with a role similar to a discriminator which only outputs values in the interval $[0,1]$ . The overall structure presented in the paper is the following: The loss for network C is defined as $$ L_C(\psi(t)) = ||C_\psi(\hat{x}_a)-\alpha||_2^2+||C_\psi(\gamma x+(1-\gamma)\hat{x})||_2^2 $$ My current implementation does not result in the expected results and I am now wondering whether the reason could be that I have misunderstood what the l2-norm in the loss functions above is supposed to do.
