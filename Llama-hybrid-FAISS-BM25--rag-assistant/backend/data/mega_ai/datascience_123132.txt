[site]: datascience
[post_id]: 123132
[parent_id]: 123036
[tags]: 
One way is to just take the embedding of the CLS token. Another way is to average all the tokens. Key search terms are sentence embeddings and document embeddings . The best models are specifically trained (or fine-tuned) to give good sentence embeddings. I believe InferSent and SBERT are considered current state-of-the-art? There are also multilingual sentence-embedding models, e.g. https://arxiv.org/abs/2007.01852 (and see https://arxiv.org/abs/2302.08387 for a follow-up making them smaller).
