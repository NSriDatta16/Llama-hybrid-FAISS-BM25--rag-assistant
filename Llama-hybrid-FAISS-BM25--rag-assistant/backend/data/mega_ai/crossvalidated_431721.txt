[site]: crossvalidated
[post_id]: 431721
[parent_id]: 
[tags]: 
AutoEncoder Reconstruction error for Anomaly Detection

I'm building a convolutional autoencoder as a means of Anomaly Detection for semiconductor machine sensor data - so every wafer processed is treated like an image (rows are time series values, columns are sensors) then I convolve in 1 dimension down thru time to extract features. I'm confused about the best way to normalise the data for this deep learning ie. if I normalise within each wafer I remove potential trends in the data - but from my trials I see that this results in trending Recon Errors. This ultimately makes it hard to create a detection threshold..? If I normalise across the entire training data set then the trends are present and recon errors can look different over time, but the loss during epochs are huge (like 1e13 instead of 0.xxxx when I normalise within a wafer run). I'm wondering if someone can offer advice on the best way to go with this..? P.
