[site]: crossvalidated
[post_id]: 275877
[parent_id]: 275869
[tags]: 
Perhaps I'm missing the gist of this question, but: if the next sample is really large variance will blow up, making your t-statistic smaller. You can test it with made up data, for example #Test if the average value of the sample c(2, 2.5, 3) is significantly different from zero #> t.test(c(2,2.5,3))$p.value #[1] 0.01307246 #Now add a 9 to the sample #> t.test(c(2,2.5,3,9))$p.value #[1] 0.08627763 Is it possible that p-value will now be statistically insignificant and we can not reject null hypothesis? In other words is there any situation where increase in variance more than offsets change in $\bar{x}$ and thus renders t-test statistically insignificant? I think I answered both questions with the code above (but apparently everyone knew it already), so let's delve into the t-statistic now: $$t={\bar x \over S/\sqrt n}$$ So for the first sample, with size $n_1$: $$t_1={\bar x_1 \over S_1/\sqrt n_1}$$ Now the second one consists of the first one plus another sample, so: $$t_2={\bar x_2 \over S_2/\sqrt n_2}$$ With: $$n_2=n_1+1 \\ \bar x_2 = {n_1\cdot \bar x_1 + x_{n_1+1} \over n_1 + 1} \\ S_2^2 = {n_1-1\over n_1} \cdot S_1^2 + {(x_{n_1+1}-\bar x_1)^2 \over n_1 + 1}={\left(1\over n_1+1\right)}\left( {n_1^2-1\over n_1} \cdot S_1^2 + (x_{n_1+1}-\bar x_1)^2 \right)$$ $$ t_2= {n_1\cdot \bar x_1 + x_{n_1+1} \over \sqrt{{n_1^2-1\over n_1} \cdot S_1^2 + (x_{n_1+1}-\bar x_1)^2 }} $$ EDIT: I actually removed some further steps to avoid implicitly assuming some terms were different from zero. Defining $\delta = x_{n_1+1} - \bar x_1$ $$ t_2= {(n_1+1)\cdot \bar x_1 + \delta \over \sqrt{{n_1^2-1\over n_1} \cdot S_1^2 + \delta^2 }} $$ Assuming $\delta \neq 0$: $$ t_2= {\delta \over |\delta|}\cdot {{(n_1+1)\cdot \bar x_1\over \delta} + 1 \over \sqrt{{n_1^2-1\over n_1} \cdot \left(\frac{S_1}{\delta}\right)^2 + 1 }}=\\ = \text{sign}(\delta)\cdot {{(n_1+1)\cdot \bar x_1\over \delta} + 1 \over \sqrt{{n_1^2-1\over n_1} \cdot \left(\frac{S_1}{\delta}\right)^2 + 1 }} $$ So re-answering In other words is there any situation where increase in variance more than offsets change in $\bar{x}$ and thus renders t-test statistically insignificant? If we make $\delta$ arbitrarily larger than $\bar x_1$ and $S_1$: $$\lim_{\delta\rightarrow\pm\infty} t_2=\text{sign}(\delta)=\pm 1$$ Indeed: #The original sample is random x = rnorm(n = 1000, mean = 1E-1, sd = 2) t.test(x)$st # t #1.544687 t.test(c(x,1E10))$st #t #1 t.test(c(x,-1E10))$st # t #-1 So basically you can always make $t=\pm1$ with a single addition to the sample, and the smallest obtainable p-value under this regime, with the degrees of freedom tending to infinity, becomes: 2*pnorm(1, lower.tail = FALSE) #[1] 0.3173105 We can also visualize this conclusion looking at the following plot: #Our original sample, here a random normal variable x = rnorm(n = 1000, mean = 0, sd = 2) png("ttestparadox.png") plot(0, 0, xlim = c(-10,10), ylim = c(0,1), type = "n", ylab = "p-value", xlab = "Asinh(new_sample)") abline(h = 2*pnorm(1, lower.tail = FALSE), lwd = 2L, col = 2) for(i in seq(-10,10,length.out = 101L)) points(x = i, y = t.test(c(x,sinh(i)))$p., pch = 20L) dev.off() I've picked new samples in a $\sinh$ scale so we get to large values faster. Anyways, we can see that, when the new sample $x_{n_1+1}$ deviates from $H_0$, the t-statistic goes to 1. Finally, an example using $\alpha = 0.05$ (shown in blue) where we go from a statistically significant result (shown as the dashed black line, p = 0.02014321) to non-significant results depending on the scale of the new unit sample. set.seed(1234) #reproducible x = rnorm(n = 1000, mean = 0.2, sd = 2) png("ttestparadox2.png") plot(0, 0, xlim = c(-10,10), ylim = c(0,1), type = "n", ylab = "p-value", xlab = "Asinh(new_sample)") abline(h = 2*pnorm(1, lower.tail = FALSE), lwd = 2L, col = 2) abline(h = 0.05, lwd = 2L, col = 4) abline(h = t.test(x)$p.v, lwd = 1, lty = 2) for(i in seq(-10,10,length.out = 101L)) points(x = i, y = t.test(c(x,sinh(i)))$p., pch = 20L) dev.off()
