[site]: crossvalidated
[post_id]: 191392
[parent_id]: 110088
[tags]: 
I can recommend this article dicussing good CV practice. (A) When simply running one RF model: Yes OOB-CV is a fine estimate of your future prediction performance, given i.i.d. sampling. For many practical instances you don't have time nor need for anything more. A default RF model is simply good enough, you will first start fiddling with the hyper parameters later, if ever. I would spent more time wondering which prediction performance metric(AUC, accuracy, recall etc) best answered my questions. A little fiddling with hyperparameters (mtry,samplesize), does not make your OOB-CV vastly over-optimistic. (B1) When comparing across classifiers (SVM, logistic, RF, etc.): you need to use the same CV regime, thus you cannot use OOB-CV only available for RF. Use e.g. 20-repeated, 10-fold CV, where all models are tested in the same folds(/by same partitions). (B2) When performing gridSearch and variable selection To evaluate the predictive performance of each variant of your model you would probably use OOB-CV or some other CV. To unbiased estimate the overall performance, you need to wrap your model-selection process in a outer cross-validation, called nested-CV.
