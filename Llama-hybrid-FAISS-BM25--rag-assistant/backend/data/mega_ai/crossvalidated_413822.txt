[site]: crossvalidated
[post_id]: 413822
[parent_id]: 411212
[tags]: 
No it is not necessary for weights to decrease in Perceptron Learning Algorithm . It depends solely on the input vector whether weights will decrease or increase. Since, the Perceptron Learning Algorithm employs the signum function at the output, defining a MSE loss might be an indicator of the loss, but useless for any other purpose nonetheless, accuracy will be equal to MSE loss as $(y-t_i)^2$ will be $+1$ for mis-calssification, so total mis-classifications are the MSE loss. As far as your question goes, weights can be multi dimensional, say the classes you want to separate is 2D input vector and the classes lie in 2nd and 4th quadrant respectively. The classifier weights or weight vector $W$ will be something like $[w_1,-w_2]$ or $[-w_1,w_2]$ where $w_1,w_2>0$ clearly you can see that if you multiply any point in 2nd quadrant, $[-x_1^i,x_2^i] $ where $i$ denotes training example (since it is in 2nd quadrant the negative sign), $x_1>0$ (x_axis) and $x_2>0$ (y_axis), multiply it by the weights, for the first case $-w_1x_1^i - w_1x_1^i$ will always be negaitive, and in the second case $w_1x_1^i + w_1x_1^i$ always positive. Similarly, for 4th quadrant co-ordinate $[w_1,-w_2]$ , in the first case $w_1x_1^i + w_1x_1^i$ always positive result, and in second case always negative. So, why are there 2 solutions for weights? Well, clearly the second solution is just $180^o$ rotated version of the first weights. Now, you can exceed this to multidimensional cases. Thus, unless all the input values of an input class i.e. all the $x_i$ of an input $[x_1, x_2....x_n]$ are of the same sign for each of the classes then only all the weight values $w_i$ of $[w_1,...w_n]$ will move in a certain direction, otherwise you cannot define weight increase or decrease. You can learn more about Perceptron Training from these 2 books: Neural Networks - A Systematic Introduction Neural Networks for Applied Sciences and Engineering: From Fundamentals to Complex Pattern Recognition
