[site]: datascience
[post_id]: 31658
[parent_id]: 31657
[tags]: 
It depends entirely on your goal. Student Phase When you're learning about machine learning algorithms, I think it is a really good idea to implement toy examples. I find this process helps with finding what you understood well and what you did not understand as well as you thought. It's doing this work where you'll find deeper understanding of how the algorithm really works and different internal choices you have to make. Professional Phase When you have a project to deliver, you don't necessary want to rewrite a random forest implementation from scratch. Even if you could build one in a reasonable amount of time, there's value in having something like sklearn that is well vetted and robust enough to handle edge cases that you wouldn't even consider. That's the advantage of using pre-built libraries. I need more Phase Eventually, you get the point where you understand the math and know how to use the packages well and you realize that there's a feature lacking. That's when you rip open a framework like xgboost or sklearn and modify existing code or even create you own implementation. The reason you would do this is because methods are cutting edge so there just isn't anything out there or the implementation of the framework is actually a handicap in production (as I tend to find with sklearn). The issue you seem to be facing is lack of accountability for your output. If all you do is clean data push it through a model and get good results, make a chart showing results, I would say you are forgetting the "science" part in data science. The challenging part isn't using the model it is knowing what moves your model and the potential hurdles your model may face in the real world. I've seen this play out often in my career, where a junior member will make an awesome model on training and test dataset, and suddenly production comes along and performance tanks. Why would that happen? Well, because test and training were similar (and often from the same source), but the junior member failed to question the lack of variety in the data source and question if the real world behaved the same way. What I am trying to say is, being a data scientist, a small part of the job is cleaning data, running model and making pretty pictures. The real challenge is asking the why question. Why does this work? Why does the model perform poorly? Why does the model perform well? Why is this feature important?
