[site]: crossvalidated
[post_id]: 443042
[parent_id]: 359121
[tags]: 
A common approach to avoid the need of scoring all user-item combinations is to use approximate nearest neighbor techniques (ANN). With these techniques you can cluster you item latent vectors and only calculate scores with the vectors of some cluster instead of all the item latent vectors. For more details, I can recommend the following blog article by Erik Bernhardsson : Approximate Nearest Neighbours for Recommender Systems Python Libraries that might help you: faiss, annoy, nmslib Assumption and some Background: I assume the question relates to the computational cost of scoring all user item combinations in order to retrieve the top-n recommendations. A little bit more background: the output of standard collaborative filtering based recommender models is two embeddings (latent vectors), one for users and one for items. To get to the top-n recommendations per user one has to do the following: multiply each user vector with each item vector to get the scores for the respective user-item combinations per users sort descending by this score Take top-n recommendations
