[site]: crossvalidated
[post_id]: 235021
[parent_id]: 235007
[tags]: 
1) I would argue that decision trees are not as interpretable as people make them out to be. They look interpretable, since each node is a simple, binary decision. The problem is that as you go down the tree, each node is conditional on every node above it. If your tree is only four or five levels deep, it's still not too difficult to convert one terminal node's path (four or five splits) into something interpretable (e.g. "this node reflects long-term customers who are high-income males with multiple accounts"), but trying to keep track of multiple terminal nodes is difficult. If all you have to do is convince a client that your model is interpretable ("look, each circle here has a simple yes/no decision in it, easy to understand, no?") then I'd keep decision trees in your list. If you want actionable interpretability, I'd suggest they might not make the cut. 2) Another issue is clarifying what you mean by "interpretability of results". I've run into interpretability in four contexts: The client being able to understand the methodology. (Not what you're asking about.) A Random Forest is pretty straightforwardly explainable by analogy, and most clients feel comfortable with it once it's explained simply. Explaining how the methodology fits a model. (I had a client who insisted I explain how a decision tree is fitted because they felt it would help them understand how to use the results more intelligently. After I did a very nice writeup, with lots of nice diagrams, they dropped the subject. It's not helpful to interpreting/understanding at all.) Again, I believe this is not what you're asking about. Once a model is fitted, interpreting what the model "believes" or "says" about the predictors. Here's where a decision tree looks interpretable, but is much more complex than first impressions. Logistic regression is fairly straightforward here. When a particular data point is classified, explaining why that decision was made. Why does your logistic regression say it's an 80% chance of fraud? Why does your decision tree say it's low-risk? If the client is satisfied with printing out the decision nodes leading to the terminal node, this is easy for a decision tree. If "why" needs to be summarized into human speak ("this person is rated a low risk because they are a long-term male customer who has high-income and multiple accounts with our firm"), it's a lot harder. So at one level of interpretability or explainability (#1 with a little #4, above), K-Nearest Neighbor is easy: "this customer was judged to be high risk because 8 out of 10 customers who have been previously evaluated and were most similar to them in terms of X, Y, and Z, were found to be high risk." At actionable, full level #4, it's not so interpretable. (I've thought of actually presenting the other 8 customers to them, but that would require them to drill down into those customers to manually figure out what those customers have in common, and thus what the rated customer has in common with them.) I've read a couple of papers recently about using sensitivity-analysis-like methods to try to come up with automated explanations of type #4. I don't have any at hand, though. Perhaps someone can throw some links into comments?
