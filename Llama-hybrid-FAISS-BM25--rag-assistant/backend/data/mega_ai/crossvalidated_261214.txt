[site]: crossvalidated
[post_id]: 261214
[parent_id]: 261213
[tags]: 
There are typically no "absolutes" when assessing model performance. For a simple and realistic alternative to in-sample measures like BIC, R-squared, etc, try what I call "percent of variation explained," which I define as $1-\frac {\|\epsilon\|}{\|y-m\|}$ where $\epsilon$ are your forecast errors, $y$ is your observed out of sample time series, $\|\cdot\|$ is an appropriately-chosen norm, and $m$ is a measure of center for $y$ according to your choice of norm. This metric will be equal to 1 only if your model perfectly fits $y$. That means to first split your data into two sets, build your model on the first set, check this metric on the second. Or, you can also try what's known as time series cross validation. Googling this topics will complete the picture if it sounds at all vague.
