[site]: datascience
[post_id]: 31358
[parent_id]: 31129
[tags]: 
As I understand it, this option only calculates the loss function differently without training the model with weights (sample importance) so how do I train a Keras model with different importance (weights) for different samples. when the loss function is calculated differently that means the backprop will behave differently (more emphasis to important samples). However the training error is much lower than before If I understand by error you mean the average loss calculated for each batch. the cross-entropy loss function is a proxy (differentiable) to train the model, while the zero-one loss is the one to fine-tune it. This is a similar question xgboost: give more importance to recent samples but I would like an applicable answer to Keras Penalizing neural networks for specific examples means give them a high probability to be included in the batches. there is no correspondence with XGboost here. side note: maybe you could try Training with a Curriculum Gradually transforming the training task, from an easy one (maybe convex) where examples illustrate the simpler concepts, to the target one (with more difficult examples) The basic idea is to start small, learn easier aspects of the task or easier sub-tasks, and then gradually increase the difficulty level. From the point of view of building representations, advocated here, the idea is to learn representations that capture low-level abstractions first, and then exploit them and compose them to learn slightly higher-level abstractions necessary to explain more complex structure in the data. By choosing which examples to present and in which order to present them to the learning system, one can guide training and remarkably increase the speed at which learning can occur. learning deep architectures for ai by yoshua bengio
