[site]: crossvalidated
[post_id]: 164250
[parent_id]: 164048
[tags]: 
A properly executed random forest applied to a problem that is more "random forest appropriate" can work as a filter to remove noise, and make results that are more useful as inputs to other analysis tools. Disclaimers: Is it a "silver bullet"? No way. Mileage will vary. It works where it works, and not elsewhere. Are there ways you can badly wrongly grossly use it and get answers that are in the junk-to-voodoo domain? youbetcha. Like every analytic tool, it has limits. If you lick a frog, will your breath smell like frog? likely. I don't have experience there. I have to give a "shout out" to my "peeps" who made "Spider". ( link ) Their example problem informed my approach. ( link ) I also love Theil-Sen estimators, and wish I could give props to Theil and Sen. My answer isn't about how to get it wrong, but about how it might work if you got it mostly right. While I use "trivial" noise, I want you to think about "non-trivial" or "structured" noise. One of the strengths of a random forest is how well it applies to high-dimensional problems. I can't show 20k columns (aka a 20k dimensional space) in a clean visual way. It is not an easy task. However, if you have a 20k-dimensional problem, a random forest might be a good tool there when most others fall flat on their "faces". This is an example of removing noise from signal using a random forest. #housekeeping rm(list=ls()) #library library(randomForest) #for reproducibility set.seed(08012015) #basic n Let me describe what is going on here. This image below shows training data for class "1". Class "2" is uniform random over the same domain and range. You can see that the "information" of "1" is mostly a spiral, but has been corrupted with material from "2". Having 33% of your data corrupt can be a problem for many fitting tools. Theil-Sen starts to degrade at about 29%. ( link ) Now we separate out the information, only having an idea of what noise is. #Create "B" class of uniform noise x5 Here is the fitting result: I really like this because it can show both strengths and weaknesses of a decent method to a hard problem at the same time. If you look near the center you can see how there is less filtering. The geometric scale of information is small and the random forest is missing that. It says something about number of nodes, number of trees, and sample density for class 2. There is also a "gap" near (-50,-50), and "jets" in several locations. In general, however, the filtering is decent. Compare vs. SVM Here is the code to allow a comparison with SVM: #now to fit to svm fit.svm It results in the following image. This is a decent SVM. The gray is the domain associated with class "1" by the SVM. The blue dots are the samples associated with class "1" by the RF. The RF based filter performs comparably to SVM without an explicitly imposed basis. It can be seen that the "tight data" near the center of the spiral is much more "tightly" resolved by the RF. There are also "islands" toward the "tail" where the RF finds association that the SVM does not. I am entertained. Without having the background, I did one of the early things also done by a very good contributor in the field. The original author used "reference distribution" ( link , link ). EDIT: Apply random FOREST to this model: While user777 has a nice thought about a CART being the element of a random forest, the premise of the random forest is "ensemble aggregation of weak learners". The CART is a known weak learner but it is nothing remotely near an "ensemble". The "ensemble" though in a random forest is intended "in the limit of a large number of samples". The answer of user777, in the scatterplot, uses at least 500 samples and that says something about human readability and sample sizes in this case. The human visual system (itself an ensemble of learners) is an amazing sensor and data processor and it finds that value to be sufficient for ease of processing. If we take even the default settings on a random-forest tool, we can observe the behavior of the classification error increases for the first several trees, and does not reach the one-tree level until there are around 10 trees. Initially error grows reduction of error becomes stable around 60 trees. By stable I mean x Which yields: If instead of looking at the "minimum weak learner" we look at the "minimum weak ensemble" suggested by a very brief heuristic for default setting of the tool the results are somewhat different. Note, I used "lines" to draw the circle indicating the edge over the approximation. You can see that it is imperfect, but much better than the quality of a single learner. The original sampling has 88 "interior" samples. If the sample sizes are increased (allowing ensemble to apply) then the quality of the approximation also improves. The same number of learners with 20,000 samples makes a stunningly better fit. The much higher quality input information also allows evaluation of appropriate number of trees. Inspection of the convergence suggests that 20 trees is the minimum sufficient number in this particular case, to represent the data well.
