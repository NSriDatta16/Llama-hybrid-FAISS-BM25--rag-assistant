[site]: crossvalidated
[post_id]: 167560
[parent_id]: 
[tags]: 
Deviance n.eff in R becoming 1 at high sample n

I've been doing some basic bayesian t-tests in OpenBUGS, and later JAGS (via a friendly biometrician), and in both cases I ran into a bizarre property of the R-2- packages on the output. Specifically, my deviance in an analysis has collapsed to a single value, with n.eff = 1. Puzzled, I went back to my model and checked it out - everything seemed fine. Just to be sure, I went back to some code from Marc Kery's book cat(" model { # Priors mu1 ~ dnorm(0,0.001) mu2 ~ dnorm(0,0.001) tau1 (etc) and it ran well. However, when I tried to jigger the sample size of the underlaying simulated data n1 and bound it up and ran it in Marc's example code. In the end, I see the same issue in the output, Inference for Bugs model at "h.ttest.txt", Current: 3 chains, each with 2000 iterations (first 500 discarded) Cumulative: n.sims = 4500 iterations saved mean sd 2.5% 25% 50% 75% 97.5% Rhat n.eff mu1 105.070 0.357 104.400 104.800 105.100 105.300 105.800 1.002 1200 mu2 77.468 0.032 77.410 77.450 77.470 77.490 77.530 1.001 4500 delta -27.602 0.356 -28.300 -27.830 -27.610 -27.360 -26.895 1.002 1300 sigma1 2.692 0.263 2.237 2.507 2.672 2.853 3.267 1.001 4000 sigma2 2.505 0.022 2.464 2.489 2.505 2.520 2.548 1.001 4500 deviance 30200.918 2.910 30200.000 30200.000 30200.000 30200.000 30210.000 1.000 1 If I peel the data n back a bit to ~1000, I eventually get back to an expected looking deviance deviance 489.528 2.945 485.900 487.400 488.900 490.900 496.900 1.004 630 When I actually inspect the chains for either, nothing looks amiss, making me think this is an R problem. And the fact that when the biometrician tried it in JAGS and found the same thing, it makes this seem all the more reasonable. Does anyone have any idea what's going on here?
