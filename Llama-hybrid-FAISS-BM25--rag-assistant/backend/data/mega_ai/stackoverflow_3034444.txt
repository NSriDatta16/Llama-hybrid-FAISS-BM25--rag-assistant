[site]: stackoverflow
[post_id]: 3034444
[parent_id]: 3034369
[tags]: 
This is how i understand this issue: Google will reach your website if someone submitted your website URL http://www.google.com/addurl/ or there is a link to your website in another already indexed website. When google reach your website it will look at the robots.txt and will see what rules there, if you disallow indexing using code like the following, google will not index your website at the moment. User-agent: * Disallow: / But google will visit your website again after some days may be, and will do the same as the first time, if you didn't find the robots.txt or found that you put rules that allow them to index the website using code like the following, they will start indexing the website pages and content. User-agent: * Allow: / About putting the website online from now or not? if you will disallow google index using robots.txt, there no difference, go for which is better for you. Note: I am not sure 100% from rules i mentioned in this answer as google always change their indexing technics. Also what i said about Google is the same for other search engines such as yahoo and bing, but its not a rule for any search engine, its just a common way, so may be other search engine index all your website links while you have robots.txt disallow indexing. And i used to put a stage version from my websites to test on the live environment before going on the real life version, and used to use the robots.txt and i never found any of these stage links in Google, Bing or Yahoo.
