[site]: crossvalidated
[post_id]: 301192
[parent_id]: 
[tags]: 
Estimation of parameter uncertainty via $K$-fold cross-validation

Goal: Estimate parameter uncertainty for a given model and dataset. Normally, I would compute the posterior of the parameters (e.g., via MCMC). However, suppose I only have access to maximum-likelihood estimates (MLE). Another standard method is to compute the Hessian at the MLE (essentially, using Laplace approximation of the posterior). However, suppose that computing the Hessian is not feasible. What I do have is the MLE for different training folds of a $K$-fold cross-validation. Crucially, the MLE will differ across training folds. Is there a known way (under some assumptions, I guess) that links the variability of these MLEs to the posterior in the full dataset? I haven't done the math yet, but it seems to me that there should be a connection - for example, if we assume that the likelihood function is multivariate normal in each of the $K$ subsamples. (Note that I am not talking about estimating variability/uncertainty in the test error.)
