[site]: datascience
[post_id]: 81956
[parent_id]: 81796
[tags]: 
Non-linear models are very complex so that a single feature importance cannot be derived (in sense if I increase one feature the model will tend to a particular class). So saying if you increase one feature, the model will vote more for one class is not what you can expect, since the model is non-linear. For example, have a look at google playground and consider the dataset with the two circles. What you can do though is to derive the feature importance locally , as you can locally approximate the neural network by a linear function. This can be used to explain the behaviour and the feature importance, but only in small neighborhood around the current position. If you go to another position, the behaviour could be completely different!
