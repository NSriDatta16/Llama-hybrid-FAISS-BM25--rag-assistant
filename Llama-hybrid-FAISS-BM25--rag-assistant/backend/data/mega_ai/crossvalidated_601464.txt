[site]: crossvalidated
[post_id]: 601464
[parent_id]: 
[tags]: 
bos_token for a custom Transformer

I am trying to use a Transformer to solve a time-series problem. I built the model using the Pytorch library . And I am planning to train the model from scratch. The model is looking back last L time-steps values of N data-series and should predict the next time-step ( N values). For this, I figured out the architecture, however, I am a bit puzzeled by what should I add as the output of the decoder. I found, we normaly use a bos_token (beginning of sentence token). So, is it okay if I just use a Vector of all zeros as the bos_token ? Or is there any specific vector?
