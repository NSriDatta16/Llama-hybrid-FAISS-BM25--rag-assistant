[site]: datascience
[post_id]: 60172
[parent_id]: 60170
[tags]: 
The reason you cannot fit non-linear functions (here sums of squares) is simply that your neural network is actually not a proper neural network: it simply resolves to a single linear element. Why is that? Recall from the Keras documentation the Dense layer: keras.layers.Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) where activation : Activation function to use (see activations). If you don't specify anything, no activation is applied (ie. "linear" activation: a(x) = x). Since you don't specify explicitly any activation, you actually use a linear one for all your layers. And it is well-known that a neural network comprised simply of linear units is equivalent to a simple linear unit (check Andrew Ng's lecture Why Non-linear Activation Functions for a detailed explanation); in fact, it is only with non-linear activation functions that neural networks begin to be able to do interesting things. So, you should add activation='relu' in all your layers except the final one, which should remain as is (final layers in regression settings, like here, need linear activation functions). You may also find the discussion in the Stack Overflow thread Is deep learning bad at fitting simple non-linear functions outside training scope (extrapolating)? interesting. UPDATE (after comment): Determining the argument input_dim for any layer other than the first one is meaningless (the input dimension of layer N is simply the output dimension of layer N-1). Remove all input_dim arguments except in the first layer. Start simple : In the linked SO thread, there is a clear demonstration of a much simpler network that can indeed calculate the square of its input. There is a good chance that 500 epochs are not enough for the weights of your (unnecessarily large) network to converge (this was not the problem before because, as said, your network was essentially a simple linear unit). Additionally, you now seem to have an input you never use ( in_pattern[1] ); this can lead to further delay in the convergence of your model. Please keep in mind that there is never a guarantee that any NN model can do a specific job, and experimenting with the architecture and the hyperparameters is always expected.
