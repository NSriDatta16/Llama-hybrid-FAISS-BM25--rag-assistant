[site]: crossvalidated
[post_id]: 561880
[parent_id]: 
[tags]: 
Relationship between "Neural Networks" and the "Universal Approximation Theorem"

I have the following question about the relationship between the Neural Networks and the Universal Approximation Theorem: For a long time, I was always interested in the reasons behind why neural networks "work". We often see the mathematics behind how neural networks are trained (e.g. backpropagation algorithm, weight update formula, gradient descent), and we are also told some general ideas how neural networks were developed using the human brain as an inspiration - but I always interested in knowing "what precisely within neural networks allows them to approximate functions". Doing some research about this topic, I quickly came across something called the " Universal Approximation Theorem" . Apparently, this theorem served as an important basis for neural networks, as it demonstrated that in theory - a 2 layered neural network with a non-polynomial activation function (and likely with an enormous number of neurons) has the ability to approximate any function to an arbitrary level of precision. When this theorem is written in mathematical language, it become much more difficult to "recognize" the neural network amongst the mathematical symbols : I attempted to "decipher" this theorem and "identify" parts of the modern neural network within this theorem: "Sigma" is the activation function "d" is the dimension of the input data (i.e. number of predictor variables) "D" is the dimension of the response variable (e.g. usually "1") "F-epsilon" is the prediction being made by the neural network "F" is the function that the neural network is trying to approximate "Epsilon" is the error produced by the neural network The index of each W ("1" and "2") refer to the number of layers in this network (i.e. in this case, there are specifically 2 layers). However, I am not exactly sure what is meant by "Composable Affine Maps" and "k compact subsets" . If I had to guess the relevance of these: "Composable Affine Maps" could be similar to the "Link Function" used in GLMs? Composable Affine Maps allow you to write the entire neural network in a linear format. Do W1 and W2 contain the network "weights"? (It would make more sense to me if this was written as : W2 * sigma COMPOSITION W1 ) "k compact subsets" could be some "technical detail" that requires the function being approximated by the neural network to be "contained within some finite space" - otherwise, if the function being approximated by the neural network could not be "contained within some finite space", this might make it very difficult to approximate this function (e.g. something similar to "Lipschitz Continuity Condition"). Am I correct about this? Can someone please comment on this? Thanks! References: https://en.wikipedia.org/wiki/Universal_approximation_theorem
