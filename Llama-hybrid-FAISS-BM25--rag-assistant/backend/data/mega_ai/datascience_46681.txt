[site]: datascience
[post_id]: 46681
[parent_id]: 
[tags]: 
Why do people use CrossEntropyLoss and not just a softmax probability as the loss?

I don't understand why one would add additional complexity to log, probabilities for the loss function of a classification Neural Network. What benefit does that have, as opposed to just using the 0-1.0 values(probabilities of a class) you get from the softmax function at the final layer? Does this add extra non-linearity that we don't understand why it does good, but just happens to do good a lot of times since we give the Neural Net some more complexity?
