[site]: crossvalidated
[post_id]: 458939
[parent_id]: 437216
[tags]: 
Fisher's LDA version maximizes a class-separability criterion, The ratio of the within class scatter to between class scatter. Where samples from the same class should be clustered tight together, and different classes should be as far apart from each other as possible. Recent approaches to LDA for classification use a Bayesian framework (like sklearn's implementation); LDA can be used for dimensionality reduction AND for classification. In my original question I was wondering how to compute class probabilities in fisher space, one way to do this is computing the means and covariances of every class and fitting a multivariate distribution to each class, this enables us to get class probabilties of a sample belonging to each class after projection to fisherspace. Now for the relation with the eigenpairs, I'm still uncertain about this, clarification about the relation with these would be awesome. But I have completely switched over to the bayesian framework so there's no rush. I want to refer to this post and encourage you to study the answers posted by that user and this user regarding LDA, they're excellent.
