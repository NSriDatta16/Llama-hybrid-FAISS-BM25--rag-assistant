[site]: datascience
[post_id]: 118554
[parent_id]: 
[tags]: 
Is SVM rotation invariant?

Let's say we have some data X and we want to find a linear separator using soft SVM with l2 regularization, and then we want to solve the same problem after applying some rotation matrix Q to the data set. Should we expect anything to change? The way I think about it, it doesn't make sense for anything to change because rotation matrix preserves norm and the its transpose is also its inverse, so mathematically: So the solution should be identical and solving the two problems should yield the same accuracy and the same loss (having the same loss function). Is something about this reasoning wrong? Is there something I'm missing?
