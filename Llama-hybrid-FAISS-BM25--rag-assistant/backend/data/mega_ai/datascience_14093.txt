[site]: datascience
[post_id]: 14093
[parent_id]: 14092
[tags]: 
In general, more training examples means improvement in learning but you can also get a very good (and nearby to the optimal score) if you just fit a good algorithm on a subset of your data set that has enough training examples. Here are a few things you can do in your current case : Take a subset of the data say about 4-5GB. The only thing you need to consider is that your target labels should be nearly stratified in the subset other wise the model will perform poorly. Apply PCA on your data and try to minimize the number of features. There may be features that are just redundant in your subset of data. Apply the algorithm, best suited for your dataset, on your current subset. You may need to do a bit of research for that. And above all, I suggest one thing. Training such a large dataset on a local machine can be too much of pain. So, it's better if you deploy your model on AWS or Google's Machine Learning platform provided by them on cloud. I hope it helps!!
