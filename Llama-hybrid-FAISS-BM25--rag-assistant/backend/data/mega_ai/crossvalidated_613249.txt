[site]: crossvalidated
[post_id]: 613249
[parent_id]: 
[tags]: 
Sampling from the posterior with a constraint on the posterior mean

Background Under certain assumptions we know that being given the posterior mean and a family of conditional distributions, we can uniquely determine the joint distribution. I quote one of the theorems specifying appropriate conditions below. The theorem comes from this book and provides the conditions for power series distributions $X|Y \sim PSD(Y)$ . Theorem 7.2 Let $(X,Y)$ be a random vector such that either: (a) $S(X) = \{0,1,2,...,n\}$ for some integer $n$ and the cardinality of $S(Y)$ is $\leq n+2$ ; or (b) $S(X)= \{0,1,2,...\}$ and $S(Y) \subseteq \{0,1,2,...\}$ . Assume that the supports $S(X)$ and $S(Y)$ are known and that for any $x \in S(X), y\in S(Y)$ we have $$P(X=x|Y=y) = c(x)y^x/c^*(y),$$ where $c$ and $c^*$ are known. In addition, if $S(Y)$ is not bounded assume that $$\sum_{x \in S(X)} \sqrt[2x]{c(x)} = \infty.$$ Then the distribution of $(X,Y)$ is uniquely determined by $\mathbb{E}(Y|X=x) = \psi(x), x\in S(x)$ . Example Let $X|Y=y \sim Pois(\lambda y)$ . Then $X|Y \sim PSD(Y)$ with $c(x) = \lambda^x/x!$ and $c^*(y) = \exp(\lambda y)$ . $\sum_{x=0}^{\infty} \sqrt[2x]{c(x)} \geq \sqrt{\lambda} \sum_{x=0}^{\infty} \frac{1}{x} = \infty$ , so the conditional expectation of $Y$ given $X$ uniquely determines the joint distribution. We don't know anything about the joint distribution other than it exists and is unique. The proof is not constructive. I would like to find $Y|X$ by simulation methods. In general let's assume we have the following setting - tractable likelihood $X|\theta$ and the functional form of the posterior mean $\mathbb{E}(\theta|X) = \phi(X)$ . Question Is it possible to simulate observations from the posterior and utilize the information about the posterior mean in an MCMC setting?
