[site]: datascience
[post_id]: 57726
[parent_id]: 
[tags]: 
Gradient descent formula

I came across an interesting book about neural network basics, and the formula for gradient descent from one of the first chapters says : Gradient descent: For each layer update the weights according to the rule $w^l \rightarrow w^l-\frac{\eta}{m} \sum_x \delta^{x,l} (a^{x,l-1})^T$ where $w^l$ is the weights matrix in layer $l$ , and $x$ is the index of a specific training example. I don't want to rewrite all formulas from the chapter, but the important part one is BP4 - equation for the rate of change of the cost with respect to any weight in the network: $\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j$ Am I missing something or the first formula is incorrect? Shouldn't we use Hadamard product instead, like this? $w^l \rightarrow w^l-\frac{\eta}{m} \sum_x \delta^{x,l} \odot a^{x,l-1}$ Thanks for help.
