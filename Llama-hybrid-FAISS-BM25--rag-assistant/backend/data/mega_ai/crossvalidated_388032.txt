[site]: crossvalidated
[post_id]: 388032
[parent_id]: 388025
[tags]: 
First of all, "There is no silver bullet". And, I think, one can easily design a simple supervised algorithm without any hyper-parameters. Or, an off the shelf algorithm, Ordinary Least Squares (no regularization, i.e. not Ridge, Lasso or ElasticNet etc. or a cost function choice, polynomial features (which actually changes the model) etc.) doesn't have any hyper-parameters. However, is it really good to have an algorithm without giving you much choice? These hyper-parameters are present because they were needed (e.g. let's add some regularization term to OLS). Hyper-parameters, in general, add more power to the algorithm, enabling us to tweak it to conform our data. This power comes with a price of course, i.e. the effort we spend to tune it. Popular and successful ML algorithms have many hyper-parameters for example, e.g. XGBoost, Neural Nets in general. Real data, in general, is much more complex than data simulated from simpler mathematical models; and this requires further effort for the choice of the algorithm and hyper-parameters. I personally witnessed the comparison of simulated human DNA vs. real DNA, in which the simulated data almost never possess novelty. This makes the hyper-parameter tuning problem even harder, i.e. an algorithm adapting to the data and making choices accordingly. There are tools that try to make your life easier, by trying possible algorithms, hyper-parameters etc. given your data, e.g. AutoML, H2O. But, this could evolve into another discussion I guess.
