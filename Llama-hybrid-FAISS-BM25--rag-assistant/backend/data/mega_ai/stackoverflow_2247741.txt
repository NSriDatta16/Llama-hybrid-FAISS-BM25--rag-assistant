[site]: stackoverflow
[post_id]: 2247741
[parent_id]: 2246924
[tags]: 
10000 files written in 25mins is about 6 files per second. Even though your HD may support xGB/sec, you cannot write X gigs of data in a second in multiple files, there is overhead involved in creating a new file in the FAT index. Imho, the core issue is you're dealing with static files, which is a poor choice in regards to your performance. The smartest solution is to stop using these static files as they obviously don't perform as well as database queries. If something is directly parsing these files, perhaps you should look into using MOD_REWRITE for Apache and instead of writing actual XML files, have the url run a live database query and output the file on demand. This way you don't have to manually generate all the XML files. But if you continue with this sub-optimal method, you will have to create a separate dedicated server/storage for this. By chance, you're not housing the database and the web server on the same box? If so, you have to separate them. You might need a separate server or NAS to store these XML files, probably in a high performance raid 0 setup. In summary, I highly doubt your database is the bottleneck, it's the act of saving all these tiny files.
