[site]: datascience
[post_id]: 94333
[parent_id]: 
[tags]: 
GradientBoostingRegressor Text Classifier

I am working to build a text classifier using a Boosting method from sklearn. It is performing quite well, at around 97% accuracy on my test data. However, the problem I am seeing is that if I input text that clearly does not fall into a predefined category, it will randomly assign it to a certain classification with a high probability score For example: X_train = df_train.text X_test = df_test.text y_train = df_train.label y_test = df_test.label boosting = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('boosting', GradientBoostingClassifier()),]) boosting = boosting.fit(df_train.text, df_train.label) list of categories -> {'A': fruit, 'B': animal, 'C': car, 'D': person, 'E': dessert, 'F': place} docs = ['this is not category'] boosting.predict_proba(docs).tolist() Output: [[0.0016872033185414193, 0.9915417761339475, 0.0016865302624752719, 0.0016921961567993337, 0.0016974399174602914, 0.001694854210776211]] You can see that the second category is receiving a .99 probability when it is clearly not fitting into any of the options. Regardless of what I put through it, could be "fdahsjfkasl" it will return the same probability score for that second category The model works so well for text that could logically fit into a category (not only performing well on test data, but also on new/random text too), but i need a way to handle text that does not, so that it can be labeled "Not a category" or something like it. Does anyone have any suggestions?
