[site]: crossvalidated
[post_id]: 620098
[parent_id]: 
[tags]: 
Pre-processing (standardization and exponential smoothing) in expanding window cross-validation for time series data

I am currently working on a time series forecasting task using an expanding window cross-validation approach. My dataset is created using the sliding window technique (Window: 4, Horizon: 1). During the cross-validation process, I need to perform pre-processing steps such as standardization and exponential smoothing on my data before training and evaluating the models. I have commented out the pre-processing steps (train_x_scaled, val_x_scaled, scaler = standardize_data_sliding_window(train_x_cv, val_x_cv)) as I am unsure about the correct implementation within the cross-validation loop. How can I appropriately pre-process the training and validation data using standardization and exponential smoothing techniques within the expanding window cross-validation framework? Specifically, how can I ensure that the pre-processing is done correctly (avoid leakage) for each fold of the cross-validation? Any insights, suggestions, or code examples would be greatly appreciated. Thank you! Here is the relevant portion of my code: # Time-series expanding window validation # with tf.device('/cpu:0'): tf.random.set_seed(42) eval_scores = [] tscv = TimeSeriesSplit(n_splits=5) def cross_validation(combination, train_x=train_x, train_y=train_y, tscv=tscv): smape_scores = [] mdape_scores = [] # Cross-Validation for train_index, test_index in tscv.split(train_x): train_x_cv, val_x_cv = train_x[train_index], train_x[test_index] train_y_cv, val_y_cv = train_y[train_index], train_y[test_index] # Pre-Processing # train_x_scaled, val_x_scaled, scaler = standardize_data_sliding_window(train_x_cv, val_x_cv) # ... # Model Architecture and Training for i in range(combination.hidden_layers): model_cv.add(tf.keras.layers.Dense(combination.hidden_neurons, activation="relu", kernel_initializer=tf.initializers.HeNormal(), kernel_regularizer=tf.keras.regularizers.l2(combination.regularization))) model_cv.add(tf.keras.layers.Dense(1, activation="linear", kernel_initializer=tf.initializers.HeNormal(), kernel_regularizer=tf.keras.regularizers.l2(combination.regularization))) model_cv.compile(loss=smape_loss, optimizer=tf.keras.optimizers.Adam(learning_rate=combination.learning_rate), metrics=[metric_mdape, "mae", "mse"]) # Backpropagation model_cv.fit(train_x_cv, train_y_cv, epochs=50, batch_size=combination.batch_size, verbose=0) predictions = model_cv.predict(val_x_cv) smape_score, mdape_score = evaluate_pred(val_y_cv, predictions) smape_scores.append(smape_score) mdape_scores.append(mdape_score) mean_smape = np.mean(smape_scores) mean_mdape = np.mean(mdape_scores) hyperparameters = { 'learning_rate': combination.learning_rate, 'batch_size': combination.batch_size, 'regularization': combination.regularization, 'hidden_neurons': combination.hidden_neurons, 'hidden_layers': combination.hidden_layers } print(f"Current mean SMAPE: {mean_smape}, Current hyperparameters: {hyperparameters}") return mean_smape, mean_mdape, hyperparameters random_combinations = random.sample(combinations, 5) results = map(cross_validation, random_combinations) optimal_smape = float('inf') optimal_mdape = float('inf') optimal_hyperparameters = {} for result in results: smape, mdape, hyperparameters = result if smape ```
