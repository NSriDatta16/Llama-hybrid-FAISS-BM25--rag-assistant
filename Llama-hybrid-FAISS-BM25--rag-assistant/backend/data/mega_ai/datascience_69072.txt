[site]: datascience
[post_id]: 69072
[parent_id]: 69061
[tags]: 
This is not necessarily unexpected (or broken). Imagine that users tend to use none or all of the applications. For a specific example, suppose 90 users use no apps at all, and 10 use all (say) 11 of them. Then the average apps used by a user is $(90\cdot0+10\cdot11)/100=1.1$ , but for each app, the average app-usage of a user who uses that app is $(10\cdot11)/10=11$ . (In your case, for a sanity check maybe compute the number of users who use no apps.)
