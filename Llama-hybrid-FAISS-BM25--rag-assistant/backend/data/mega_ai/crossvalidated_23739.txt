[site]: crossvalidated
[post_id]: 23739
[parent_id]: 23737
[tags]: 
Yes, it is the same thing. Most off-the-shelf numerical optimisation tools are designed to minimise rather than maximise, so representing it as a minimisation problem is convenient (hence the - sign). Also the marginal likelihood is the product of a large number of terms, which can run into numerical precision problems, and easy way to deal with that is to take logarithms, which replaces the products with sums, and eases the numerical issues. So rather than maximise the marginal likelihood, we minimise the negative log-likelihood. It achieves the same result, but with better numerical properties. By the way, Gaussian process hype optimisation needs some further work - they are not nearly as popular as they should be! ;o) P.S. The proper Bayesian way to deal with the hyper-parameters is to integrate them out as well using a hyper-prior, but that is computationally expensive. Sadly, optimising the marginal likelihood can result in over-fitting at in model selection and hence is perhaps not as good an idea as it might first appear, see: G. C. Cawley and N. L. C. Talbot, Over-fitting in model selection and subsequent selection bias in performance evaluation, Journal of Machine Learning Research, 2010. Research, vol. 11, pp. 2079-2107, July 2010. (pdf)
