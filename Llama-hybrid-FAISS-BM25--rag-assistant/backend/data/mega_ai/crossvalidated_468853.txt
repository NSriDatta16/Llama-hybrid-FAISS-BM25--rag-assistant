[site]: crossvalidated
[post_id]: 468853
[parent_id]: 467893
[tags]: 
While that is not a direct answer to your question, it might also be interesting to note that the posterior will also behave like a normal distribution in large samples, a result that used to be relevant before MCMC methods became widely available. So, in the sense that asymptotics are always an approximation in the sense that we never have infinitely large samples in practice, the difference may not be that large as we obtain a normal shape in either case when sample size becomes large, and the issue is maybe rather how good that approximation is. Paraphrasing the discussion in Greenberg, Introduction to Bayesian econometrics: Writing the likelihood function of a simple random sample $y=(y_{1},\ldots ,y_{n})$ \begin{eqnarray*} L\left( \theta |y\right) &=&\prod_{i=1}^{n}f\left( y_{i}|\theta \right) \\ &=&\prod_{i=1}^{n}L\left( \theta |y_{i}\right) \end{eqnarray*} Log-likelihood: \begin{eqnarray*} l\left( \theta |y\right) &=& \ln L(\theta|y)\\ &=&\sum_{i=1}^{n}\ln L\left( \theta |y_{i}\right) \\ &=&\sum_{i=1}^{n}l\left( \theta |y_{i}\right) \\ &=&n\bar{l}\left( \theta |y\right) , \end{eqnarray*} where $\bar{l}\left( \theta |y\right) $ is the average contribution to the log-likelihood. Hence, \begin{eqnarray*} \pi \left( \theta |y\right) &\propto &\pi \left( \theta \right) L(\theta|y) \\ &=&\pi \left( \theta \right) \exp \left( n\bar{l}(\theta|y)\right) \end{eqnarray*} Now consider a Taylor series approximation of $l(\theta|y)$ around the maximum likelihood estimator $\hat{\theta}$ \begin{eqnarray*} l(\theta|y) &\approx &l(\hat{\theta}|y) \\ &&+\ l^{\prime}(\hat{\theta}|y)(\theta -\hat{\theta}) \\ &&+\ \frac{1}{2}l^{\prime\prime}(\hat{\theta}|y)(\theta -\hat{\theta})^{2}\\ &=& l(\hat{\theta}|y)-\frac{n}{2v}(\theta -\hat{\theta})^{2} \end{eqnarray*} with $$ v=\left[ -\frac{1}{n}\sum_{i=1}^{n}l^{\prime \prime }\left( \hat{\theta}|y_{i}\right) \right] ^{-1} $$ For large $n$ approximatively \begin{eqnarray*} \pi(\theta|y) &\propto &\pi(\theta) \exp(l(\theta|y)) \\ &\approx&\pi \left( \theta \right) \exp \left( l(\hat{\theta}|y)- \frac{n}{2v}(\theta -\hat{\theta})^{2}\right) \\ &\propto &\pi \left( \theta \right) \exp \left( -\frac{1}{2\left( v/n\right) }(\theta -\hat{\theta})^{2}\right) \end{eqnarray*} Here, we have dropped terms that do not depend on $\theta$ (such as the fixed value of the ML estimate). The exp-term is the (non-normalized) density of a normal distribution with expectation $\hat{\theta}$ and variance $v/n$ . By "likelihood dominance" (by which I mean that the likelihood dominates the prior in large samples) we get kind of a Bayesian analogue to the asymptotic normality of an estimator.
