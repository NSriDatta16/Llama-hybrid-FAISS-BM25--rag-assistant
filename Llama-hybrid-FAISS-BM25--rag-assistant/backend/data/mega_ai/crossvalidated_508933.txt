[site]: crossvalidated
[post_id]: 508933
[parent_id]: 508909
[tags]: 
Your answer to question 2 is correct, provided here are actually individuals with $y \geq 85$ . The easiest way to see this is that this truncation would at the very least decrease the mean value of $y$ , so even if you cooked up the data in such a way that $\beta_1, \beta_2$ from such a regression stayed exactly the same, then $\beta_0$ would still have to decrease to reflect the fact that the average value of the population has shifted down. Your answer to question 1, however is incorrect if the assumptions you have stated are taken to be literally true. In particular, the assumption that $\mathbb E[e | X] = 0$ is equivalent to the assumption that $$\mathbb E[y | X] = \beta_0 + x_1\beta_1 + x_2\beta_2$$ This assumption is quite strong! It's saying that the conditional expectation of $y$ given $X$ is actually linear in $X$ (i.e. not just approximated by a linear function). Put another way, this assumption is saying that extrapolation is valid; how $y$ behaves on average for $X$ in some neighborhood is completely informative about how $y$ behaves for $X$ in some completely different neighborhood. As a result, fitting the model only with $x_1 will typically give consistent estimates of the parameters, provided there is enough variation to identify everything, and provided the (very strong!) linear functional form is literally true. Contrast this, for example, to the weaker assumption that merely $\mathbb E[eX] = 0$ (i.e. the error is uncorrelated with $X$ ). In this case, we can show that the "true" $\beta$ that is being estimated by OLS is the one that solves the population least squares problem: $$\beta_0, \beta_1, \beta_2 = \underset{b_0, b_1, b_2}{\mathrm{argmin}}\ \mathbb E\left[(y - b_0 - x_1 b_1 - x_2 b_2)^2\right]$$ Under this weaker assumption, we have that $\beta$ parameterizes the "best" linear approximation to the conditional expectation function (where "best" means minimizing a mean squared error objective) of $y$ . Notably, the definition of "best" is now dependent on the distribution of $X$ since that expectation is with respect to the entire joint distribution of $Y,X$ . Put another way, this weaker assumption drops the strong condition implied by $\mathbb E[e | X] = 0$ that extrapolation is valid. In that case, you are absolutely right in stating that subsetting to $x_1 would potentially give different results compared to not doing this subsetting.
