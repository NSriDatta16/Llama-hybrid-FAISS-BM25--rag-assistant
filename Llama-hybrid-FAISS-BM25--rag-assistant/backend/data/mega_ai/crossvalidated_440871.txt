[site]: crossvalidated
[post_id]: 440871
[parent_id]: 440843
[tags]: 
What you are describing in the second portion is not empirical Bayesian analysis. An empirical prior is a prior collected from a parameter that you are not interested in, to estimate the parameter you are interested in. For the Beta-Binomial, there are three standard uninformative distributions. The Haldane prior B(0,0) produces a maximum a posteriori estimator equal to the maximum likelihood estimator. It also allows for double-sided coins. The Jeffreys prior, B(.5,.5), produces results that are invariant to monotone transformations. It does violate the strong likelihood principle. The uniform distribution, B(1,1) distribution, has the intuitive property that it assigns an equal prior probability to all choices. The empirical prior would use information in the sample that you are not interested, to improve the estimators of the parameters that you are interested in, such as the grand mean. One example of an empirical prior for this set would be B(182,263). Because the prior would so dominate the posterior and since it would not make sense to consider such a strong prior if the alternative is a Jeffreys prior, an approximately mean preserving prior would be B(4,6). For your first example, there would be no changes, but with a B(4,6), you end up with slightly different credible sets (not confidence intervals). The pro for using an empirical prior is it is a shrinkage prior in the same sense that Stein's method is. The con is that it may not be representative of any county. Imagine county one is in California, county two is in Montana, and county three is in the United Kingdom. The grand mean may add an illogical distortion to the data. It still provides valid shrinkage a la Stein which is a Frequentist tool, but it may offend against the underlying axioms such as Cox's axiomatic construction of probability which could be thought to undergird Bayesian thinking. The pros for a Jeffreys' prior when there is no valid prior knowledge is that it is minimally informative and excludes the extrema of $\pi\in\{0,1\}$ for a true parameter. So observing $\alpha=0$ and $\beta=50$ does not imply $\pi=0$ . The con of a Jeffreys prior is that it potentially offends against the strong likelihood principle. It could be seen to offend against the underlying axiomatic constructions, particularly, de Finetti's axiomatization or Savage's. After all, who actually would hold the belief that they saw half a head and half a tail on a coin toss unless you possibly meant that it could land on its side. Both methods cross over into Frequentist thinking, which is not a bad thing, but you do want to realize that you are doing it and the limitations that such thinking may imply. There are three views on Frequentist ideas you can take. You can hold the idea of "Heaven forbid that Frequentist ideas contaminate my analysis," or, "the prior could be arrived at without using a Frequentist idea so it is okay anyway," or finally, "why am I not using a Pearson-Neyman model and contaminating my results with Bayesian thinking." If you are worried about the information you are adding with your prior so much that you do not want to add anything or only add from the data, then why are you using a Bayesian method?
