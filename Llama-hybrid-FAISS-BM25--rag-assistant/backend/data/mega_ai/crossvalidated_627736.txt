[site]: crossvalidated
[post_id]: 627736
[parent_id]: 627714
[tags]: 
Embedding is a look-up table for efficient representations. For the input embedding, we would like the network to react similarly to synonyms, while in the output embedding, we would like the scores of words that are interchangeable to be similar (Mnih and Teh, 2012) . Given that, and given the fact that learning positional embeddings does not improve the performance (Vaswani et al., 2017) , we have no use of learned embeddings in the case of binary valued data. Moving on, do we need attention mechanisms so much so that we try to find continuous representations for bits? You could experiment and find out, perhaps first training a simpler model to see the baseline performance. See A suitable method to predict next binary outcome based on the patterns of past binary data .
