[site]: crossvalidated
[post_id]: 271703
[parent_id]: 
[tags]: 
Why is sigmoid or tanh better than linear slope for an activation function?

I usually see sigmoid or tanh being used as activation in neural networks. Why not linear slope given that linearity is simpler mathematically?
