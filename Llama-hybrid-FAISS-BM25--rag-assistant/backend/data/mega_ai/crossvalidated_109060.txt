[site]: crossvalidated
[post_id]: 109060
[parent_id]: 109043
[tags]: 
I don't think the "confusables" problem is as big as you think it is. Supervized learning algorithms are designed to find the differences between classes, even if the classes have resembling symbols. For instance, the MNIST data, a standard optical recognition dataset used to test many techniques, contains handwritten digits, and to my knowledge none of the standard methods (random forests, boosting, SVM, MLP, deep networks...) require any specific pre-treatment because of the fact that handwritten "6" resemble "8". Or "1" and "7", for that matter. You might get a problem if your classes are of uneven sizes : if you have much more "7" than "1" in your training data, the model might be tempted to class all 7s and 1s as "7" to reduce error. But that is a different question, not directly related to the problem you raised.
