[site]: crossvalidated
[post_id]: 300807
[parent_id]: 300746
[tags]: 
The way you describe your procedure, you're actually not learning the distribution per se. You created the uniform sample $\pi'$, then gradually replace the members of the set with new observations $\pi$. First, if you keep doing this as you said to infinity, then at some point all original members of $\pi'$ will be replaced with observations $\pi$. In this case why bother about initializing with $\pi$? All you need is to use the new observations $\pi$. Second, even after you replaced the "old" observations with "new" ones, you simply use the modified datase to sample from it. You're not learning the probability distribution from it in a sense of building the distribution. Now, the only reason to replace old with new observations gradually is if you start with a very small set of observations. So that the new data does not overwhelm the prior belief from the get go. Only in this case it would make a sense to try Bayesian kernel density estimation, see Sec 27.5 here for an example. UPDATE . In your case a very simple solution would be the ordinary kernel density estimator (KDE) with shrinking bandwidth. The bandwidth is the most important parameter of KDE. So, you start with a very wide bandwidth, so wide, that effectively it's going to produce you the uniform distribution. For instance, if you put it equal to 10, any kernel will produce nearly uniform distribution. Next, you shrink the bandwidth as the sample grows by some principle. It could be by $\ln n$ or something along those lines. Obviously, you only use the new observations, there's no need to initialize now, because the kernel bandwidth represents your uniform prior. Once the sample grows your distribution will start acquiring a shape that is driven by your observations.
