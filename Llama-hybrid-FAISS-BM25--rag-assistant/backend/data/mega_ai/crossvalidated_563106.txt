[site]: crossvalidated
[post_id]: 563106
[parent_id]: 
[tags]: 
Reporting the best test accuracy in a research paper?

I am running a random forest classifier for binary classification. I have split the dataset into training and testing. From training data, I select 70% of the data for hyperparameter tuning, I used 5 fold cross validation for that. Then I use the hyperparameters that produce high mean cross-validation accuracy (let's call it validation accuracy) and train the model on full training data and check the model on testing data. I get the testing accuracy. My question is- i) Should I report the best validation accuracy or the best testing accuracy? I can use the seed (by proving random_state = xyz in random forest classifier) to fix the samples used for bootstrapping, which gives the best test accuracy. If I need to report both the accuracies, can I select the seeds that give good validation accuracy and a good testing accuracy? What if I use the best test accuracy I got and report that in the paper?
