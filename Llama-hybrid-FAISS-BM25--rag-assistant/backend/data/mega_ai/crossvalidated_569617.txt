[site]: crossvalidated
[post_id]: 569617
[parent_id]: 
[tags]: 
How to perform linear regression when each data item is itself a distribution?

I have two sets of data $R$ and $R^{'}$ , and I would like to perform regression to infer whether for some function $f$ (which I postulate) and some data item $E$ (representing noise), there exists a relationship $R' = f(R,E)$ . Initially I thought I would try some linear function such as $f(R,E) = \alpha R + \beta E = R^{'}$ - clearly a task for linear regression. However, my problem is that each data item in both $R$ and $R^{'}$ is itself a distribution rather than just single values for $R$ and $R^{'}$ , i.e. a tuple of values - specifically these are quantum mechanical measurements of qubit states, where we are performing measurements on a given state vector and count the measurement outcomes, which are either 1s or 0s for a given qubit. For example, when measuring a 2 qubit state we can get (0,0), (1,0), (0,1), or (1,1), and we would thus then have a tuple with 4 elements in, where the values represent either the raw number of counts of each state, or estimates of the probabilities when normalised by the total number of samples. I was wondering therefore whether it's still possible to carry out regression in this case? Or does this require some feature engineering beforehand in order to convert these distributions into metrics which are single valued?
