neral, at time step t {\displaystyle t} (which ranges from 1 to m), AIXI, having previously executed actions a 1 … a t − 1 {\displaystyle a_{1}\dots a_{t-1}} (which is often abbreviated in the literature as a < t {\displaystyle a_{<t}} ) and having observed the history of percepts o 1 r 1 . . . o t − 1 r t − 1 {\displaystyle o_{1}r_{1}...o_{t-1}r_{t-1}} (which can be abbreviated as e < t {\displaystyle e_{<t}} ), chooses and executes in the environment the action, a t {\displaystyle a_{t}} , defined as follows: a t := arg ⁡ max a t ∑ o t r t … max a m ∑ o m r m [ r t + … + r m ] ∑ q : U ( q , a 1 … a m ) = o 1 r 1 … o m r m 2 − length ( q ) {\displaystyle a_{t}:=\arg \max _{a_{t}}\sum _{o_{t}r_{t}}\ldots \max _{a_{m}}\sum _{o_{m}r_{m}}[r_{t}+\ldots +r_{m}]\sum _{q:\;U(q,a_{1}\ldots a_{m})=o_{1}r_{1}\ldots o_{m}r_{m}}2^{-{\textrm {length}}(q)}} or, using parentheses, to disambiguate the precedences a t := arg ⁡ max a t ( ∑ o t r t … ( max a m ∑ o m r m [ r t + … + r m ] ( ∑ q : U ( q , a 1 … a m ) = o 1 r 1 … o m r m 2 − length ( q ) ) ) ) {\displaystyle a_{t}:=\arg \max _{a_{t}}\left(\sum _{o_{t}r_{t}}\ldots \left(\max _{a_{m}}\sum _{o_{m}r_{m}}[r_{t}+\ldots +r_{m}]\left(\sum _{q:\;U(q,a_{1}\ldots a_{m})=o_{1}r_{1}\ldots o_{m}r_{m}}2^{-{\textrm {length}}(q)}\right)\right)\right)} Intuitively, in the definition above, AIXI considers the sum of the total reward over all possible "futures" up to m − t {\displaystyle m-t} time steps ahead (that is, from t {\displaystyle t} to m {\displaystyle m} ), weighs each of them by the complexity of programs q {\displaystyle q} (that is, by 2 − length ( q ) {\displaystyle 2^{-{\textrm {length}}(q)}} ) consistent with the agent's past (that is, the previously executed actions, a < t {\displaystyle a_{<t}} , and received percepts, e < t {\displaystyle e_{<t}} ) that can generate that future, and then picks the action that maximizes expected future rewards. Let us break this definition down in order to attempt to fully understand it. o t r t {\displaystyle o_{t}r_{t}} is the "percept" (which consists of the observation o t {\displaystyle o_{t}} and reward r t {\displaystyle r_{t}} ) received by the AIXI agent at time step t {\displaystyle t} from the environment (which is unknown and stochastic). Similarly, o m r m {\displaystyle o_{m}r_{m}} is the percept received by AIXI at time step m {\displaystyle m} (the last time step where AIXI is active). r t + … + r m {\displaystyle r_{t}+\ldots +r_{m}} is the sum of rewards from time step t {\displaystyle t} to time step m {\displaystyle m} , so AIXI needs to look into the future to choose its action at time step t {\displaystyle t} . U {\displaystyle U} denotes a monotone universal Turing machine, and q {\displaystyle q} ranges over all (deterministic) programs on the universal machine U {\displaystyle U} , which receives as input the program q {\displaystyle q} and the sequence of actions a 1 … a m {\displaystyle a_{1}\dots a_{m}} (that is, all actions), and produces the sequence of percepts o 1 r 1 … o m r m {\displaystyle o_{1}r_{1}\ldots o_{m}r_{m}} . The universal Turing machine U {\displaystyle U} is thus used to "simulate" or compute the environment responses or percepts, given the program q {\displaystyle q} (which "models" the environment) and all actions of the AIXI agent: in this sense, the environment is "computable" (as stated above). Note that, in general, the program which "models" the current and actual environment (where AIXI needs to act) is unknown because the current environment is also unknown. length ( q ) {\displaystyle {\textrm {length}}(q)} is the length of the program q {\displaystyle q} (which is encoded as a string of bits). Note that 2 − length ( q ) = 1 2 length ( q ) {\displaystyle 2^{-{\textrm {length}}(q)}={\frac {1}{2^{{\textrm {length}}(q)}}}} . Hence, in the definition above, ∑ q : U ( q , a 1 … a m ) = o 1 r 1 … o m r m 2 − length ( q ) {\displaystyle \sum _{q:\;U(q,a_{1}\ldots a_{m})=o_{1}r_{1}\ldots o_{m}r_{m}}