[site]: crossvalidated
[post_id]: 167573
[parent_id]: 167561
[tags]: 
If I understand the paper correctly then this is essentially a generalization of the K-means algorithm, with generalized centroids. The technical term is "spherical K-means." See for example, this paper . When you're doing K-means, you're partitioning your data into $K$ clusters $S=\{S_1,S_2,\cdots,S_K\}$ with centroids $\mu_i$ which minimize the overall distances in each cluster. To clarify, once you've chosen a cluster $S_i$ then that fixes $\mu_i$ to be the average of all observations in $S_i$. In the case where your observations are vectors, then your centroids can be represented as vectors as well. Summarizing, for K-means, we are minimizing over all possible clusters: $$\min_{S}\sum_{i=1}^K\sum_{x\in S_i}\|x-\mu_i\|^2.$$ Here each $x$ is a vector as is each $\mu_i$. Write $\mu_i=\|\mu_i\|D_i$, where $D_i$ is a unit vector. If we form a matrix $D$ of centroids, then we can gain access to $\mu_i$ by: $\mu_i=\|\mu_i\| D\delta_i$ where $\delta_i$ is the indicator vector on index $i$: $(\delta_i)_j=0$ for $j\neq i$ and 1 otherwise. In the paper, the job of $s^{(i)}$ is to select the centroid $\mu_k$ for observation $x^{(i)}$. There is one key difference here: the $\mu_i$'s in this optimization are no longer fixed to be the means of each cluster . Furthermore, you are allowed to dialate the centroids to minimize error between each data point in that class. So instead of $K$-means, we're really doing "$K$-centroids with dialation." So the minimization above now also minimizes over $\mu_i$. So when they say "one hot encoder," they really just a particular hot centroid $\mu_i$, although it sounds like their goal is to emphasize the direction rather than the magnitude of $\mu_i$. Why $K$-Means and $K$-Spherical means are different: take three points in $\mathbb{R}^2$ that form an equilateral triangle and let $K=2$. A moment's consideration will reveal that $K$-means will solve this by picking any pair of points, and assigning it to class 1 along with their (mean) centroid on the edge of the triangle, and take the remaining point and assign it to class 2 with it's (mean) centroid equal to the point. On the other hand, spherical $K$-means can do better. To see why, notice that if you were to assign the third point as usual with a centroid on-top of it, you could reverse this vector by multiplying by $-1$, and now it it is on-top of the $K$-means (mean) centroid between the first two points. Then you could take a second centroid vector and just point it at another one of the first two points. So your error should be half of $K$-means in this case.
