[site]: datascience
[post_id]: 33495
[parent_id]: 33489
[tags]: 
When we correct the network using the "high-quality "gradient we are trying to get that one unified weight vector (or mininma of the error surface) which would fit the entire data i.e. all the training examples. The gradient from each separate example (though having different direction on the error surface initially) aims at achieving the minima on error surface. Hence, averaging them makes sense as it will give average direction towards that minima which is better that the individual gradients. I am not an expert in the field but according to me this is the probable answer. Hope it helps. EDIT : As learned from the comments you diagram is wrong as "The error surface is always over model parameters, not over examples". So, if you meant gradients in different directions the above explanation will help you.
