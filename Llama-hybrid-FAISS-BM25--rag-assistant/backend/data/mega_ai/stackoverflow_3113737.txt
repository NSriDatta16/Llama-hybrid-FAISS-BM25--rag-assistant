[site]: stackoverflow
[post_id]: 3113737
[parent_id]: 3113428
[tags]: 
How big (number of words) are your documents? Memory consumption at 150K trainingdocs should not be an issue. Naive Bayes is a good choice especially when you have many categories with only a few training examples or very noisy trainingdata. But in general, linear Support Vector Machines do perform much better. Is your problem multiclass (a document belongs only to one category exclusivly) or multilabel (a document belongs to one or more categories)? Accuracy is a poor choice to judge classifier performance. You should rather use precision vs recall, precision recall breakeven point (prbp), f1, auc and have to look at the precision vs recall curve where recall (x) is plotted against precision (y) based on the value of your confidence-threshold (wether a document belongs to a category or not). Usually you would build one binary classifier per category (positive training examples of one category vs all other trainingexamples which don't belong to your current category). You'll have to choose an optimal confidence threshold per category. If you want to combine those single measures per category into a global performance measure, you'll have to micro (sum up all true positives, false positives, false negatives and true negatives and calc combined scores) or macro (calc score per category and then average those scores over all categories) average. We have a corpus of tens of million documents, millions of training examples and thousands of categories (multilabel). Since we face serious training time problems (the number of documents are new, updated or deleted per day is quite high), we use a modified version of liblinear . But for smaller problems using one of the python wrappers around liblinear ( liblinear2scipy or scikit-learn ) should work fine.
