[site]: crossvalidated
[post_id]: 313652
[parent_id]: 313564
[tags]: 
When you consider specifying a prior vs using a raw frequentist method, sometimes the prediction will be much better even with an apparently wrong prior because you don't need the prior to be precise at all to improve things . A very rough prior helps to rule out what is unrealistic, and this helps even if specified very imprecisely. It's a probabilistic method of restricting the parameter to a subset of possible values. For the joke: in the case of estimating the number of sheep in a field, ruling out values greater than $10^{52}$ is a safe guess. A counter-productive prior is of course theoretically possible: believing firmly something that is totally unreal. But it is most often the consequence of a wrong mathematical understanding in a difficult formalization. This is an example of such a mistake: http://www.nowozin.net/sebastian/blog/estimating-discrete-entropy-part-3.html If the overall formalization is good, wrong numerical information has less consequences. I was convinced by one of the simplest Bayesian methods: $L^2$ regularization in linear regression. The model is $Y=\beta X+\epsilon$. If you have small data compared to features dimension, the frequentist basic estimator (MLE) $\hat\beta$ will most be often be extremely over-fitted and yield very poor predictions because you allow it to consider every possible $\beta$. . It's not rare that it has higher error than a constant predictor like 0 (in a real situation). Now some vague intuition, experience, rumour... tells you that actually $\beta$ is unlikely to have a big norm, that estimations of $\beta$ with a big norm is just an effect of over-fitting. You think the real $\beta$ tends to be reasonably small. You say: my $\beta$ is around 0 with variance... hum... dunno...say 1. Formally, this is a Gaussian prior on $\beta$. 1 is the regularization constant. But if you choose 2 instead of 1, you'll get roughly the same results. And if you choose 1.2, you can't even see the difference. (not giving a general fact here, just for that it's the kind of thing we often observe). Actually there is a very wide range of values that will yield much better result than the non regularized estimator, and the error curve tends to be pretty flat around the optimal choice. I did a few simulations with wrong prior specification in this case: you may assume a very false prior, yet the result are still far better than without regularization. Because a flat prior is worse than the worst misspecification you can reasonably think of. As an hyper-parameter, the regularization coefficient can be chosen loosely without much consequence on the prediction. It tends to be true in many machine learning situations: the more you go to hyper-paramter, hyper-hpyer-paramters... the least it becomes sensitive to wrong specification. (usually and if the methods are good)
