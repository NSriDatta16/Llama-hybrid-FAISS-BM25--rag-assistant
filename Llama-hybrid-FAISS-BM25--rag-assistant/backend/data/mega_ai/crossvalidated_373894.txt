[site]: crossvalidated
[post_id]: 373894
[parent_id]: 
[tags]: 
basic average (median) model outperforms more sophisticated statistical and machine learning models

Let us say I have 3 categorical features with 3 levels each (there are actually more but I just use this as an example): F1 = {F1_L1, F1_L2, F1_L2} F2 = {F2_L1, F2_L2, F2_L2} F3 = {F3_L1, F3_L2, F3_L2} There is also one continuous dependent variable Y. I have a few 100 samples from this scenario. Overall there are 3 * 3 * 3 = 27 bins/combinations given the above categorical feature details. In order to ‘predict’ Y given a ‘new’ sample, I currently simply take the median Y value of the present combinations’ bins (if I do not have the combination I use the global (marginal?) median). Hope that makes sense. This works quite well and could be deemed null model I presume. I have tried to use simple OLS and quantile regression models and more sophisticated models like gradient boosting with hyper parameter search etc. on training data but on unseen data these models are over fitted and my basic ‘null model’ performs OK. I guess complex interactions cannot be captured by my simple null model. Am I missing something or could it be that something so simple works? PS: Some R code to make this hopefully clearer: expand.grid( F1 results in 27 combinatons: F1 F2 F3 1 L1 L1 L1 2 L2 L1 L1 3 L3 L1 L1 4 L1 L2 L1 5 L2 L2 L1 6 L3 L2 L1 7 L1 L3 L1 8 L2 L3 L1 9 L3 L3 L1 10 L1 L1 L2 11 L2 L1 L2 12 L3 L1 L2 13 L1 L2 L2 14 L2 L2 L2 15 L3 L2 L2 16 L1 L3 L2 17 L2 L3 L2 18 L3 L3 L2 19 L1 L1 L3 20 L2 L1 L3 21 L3 L1 L3 22 L1 L2 L3 23 L2 L2 L3 24 L3 L2 L3 25 L1 L3 L3 26 L2 L3 L3 27 L3 L3 L3
