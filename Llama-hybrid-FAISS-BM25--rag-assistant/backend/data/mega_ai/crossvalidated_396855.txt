[site]: crossvalidated
[post_id]: 396855
[parent_id]: 396823
[tags]: 
There are a few places where this can go wrong, but this really stands out: # check Q for convergence if delta It looks like a hold-over from value iteration, where it would be valid. For Q learning this is not correct at all. Under Q learning, you are sampling random episodes, with an emphasis on behaving "close to current best guess at optimal" and the corrections you need to apply to the Q table for any one episode might be close to zero purely by chance. This is very different to value iteration which cycles through all possible states and can assess the corrections it makes using a known transition model. There is no convenient end point for Q learning where it can tell, by internal measure only during training, that it has converged to the action values for an optimal policy. What you can do is every few hundred episodes, assess the agent by running the greedy policy (i.e. set $\epsilon = 0$ in your action choices). As your environment is deterministic and always starts in the same state, you can just take a single run for this test. If the result is optimal and the Q table predicts this accurately, then it has converged. Often with Q learning on more complex real problems, you will not know if the agent has converged to a true optimum. The best you can do is plot performance over time and show: It is no longer improving The Q values it predicts are close to the ones seen. This can be a normal loss function such as mean square error, perhaps averaged over a few hundred episodes in case of stochastic environments. It is as good or better than other agents in the same environment While using value iteration and policy iteration I was able to get the best results possible but with Q-learning it doesn't seem to go well. When you only have a small numbers of states and actions, plus have access to the transition model (which Q-learning does not use), then this is a valid result. A good analogy might be learning the probability distribution of rolling a number of dice and summing them: Value iteration is like a recursive algorithm which you have told how the results from a single die work, so it can effectively build a probability tree and sum up all the options. Q-learning is like rolling the dice multiple times and making a histogram. This is always going to be an approximation to the true result, and take longer than probability-based calculations. However, value iteration reaches limits of applicability in terms of complexity much sooner than Q learning. It is possible to sample from environments where the underlying model is very complex, or unknown. It is also possible to sample from environments where the full state space is too large to loop through completely in reasonable time.
