[site]: datascience
[post_id]: 53141
[parent_id]: 
[tags]: 
LSTM component dimensions and freedom of design

Like many, I'm learning about LSTMs using this rather clear blog post and to some extent this construction of an RNN cell in Pytorch. I've been studying this diagram: and while I understand the motivation behind the design, I'm having some issues working out the dimensions. (Though others have certainly asked LSTM dimension-related questions, I haven't found one that addresses my point of confusion - don't hesitate to direct me elsewhere if you know of a good source.) Suppose each vector $x_t$ in the input sequence has dimension $m$ , the label $y$ for that sequence has dimension $n$ , and there are $T$ terms in the sequence. To my understanding, we would put each $x_t$ through the cell, passing along the output from the cell at $x_t$ as part of the input for the cell at $x_{t + 1}$ and also maintaining the cell state. Then we would take $h_T$ as our output and compare it to the label $y$ . Then $h_t$ must be of dimension $n$ , since it will ultimately be the output, and the cell state has the same dimension as $h_t$ , since $h_t$ is just $\tanh$ of the cell state times the result of the output gate. My question, then, is this: doesn't that mean that the architecture of our LSTM cell is determined the moment we know the dimensions of our inputs and our labels? The forget, input, and output layers in the cell all must take a vector of size $m + n$ ( $x_t$ concatenated with $h_{t - 1}$ ) to the cell state dimension $n$ . This seems bizarre to me, as in the exposure I've had to feed-forward networks previously, choosing the number of layers, number of neurons per layer, and so on has been a fairly important step in model design. Here it seems like we don't have those degrees of freedom. Where have I gone astray?
