[site]: crossvalidated
[post_id]: 495265
[parent_id]: 22
[tags]: 
The simplest and clearest explanation I've seen, from Larry Wasserman's notes on Statistical Machine Learning (with disclaimer: "at the risk of oversimplifying"): Frequentist versus Bayesian Methods In frequentist inference, probabilities are interpreted as long run frequencies. The goal is to create procedures with long run frequency guarantees. In Bayesian inference, probabilities are interpreted as subjective degrees of belief. The goal is to state and analyze your beliefs. What's tricky is that we work with two different interpretations of probability which can get philosophical. For example, if I say "this coin has a 1/2 probability of landing heads", what does that mean? The frequentist viewpoint is that if we performed many coin flips, then the counts ("frequencies") of heads divided by the total number of flips should more or less get closer and closer to 1/2. There is nothing subjective about this which can be viewed as a good thing, however we can't really perform infinite flips and in some cases we can't repeat the experiment at all, so an argument about limits or long run frequencies might be in some ways unsatisfactory. On the other hand, the Bayesian viewpoint is subjective, in that we view probability as some kind of "degree of belief", or "gambling odds" if we specifically use de Finetti's interpretation. For example, two people may come into the coin flipping experiment with different beliefs about what they believe about the coin (prior probability). After the experiment which has collected data/evidence and the people have updated their beliefs in accordance with Bayes' theorem, they leave with different ideas of what the posterior probability of the coin is, and both people can justify their beliefs as "logical"/"rational"/"coherent" (depending on the exact flavor of Bayesian interpretation). In practice, statisticians can use either kind of methods as long as they are careful with their assumptions and conclusions. Nowadays Bayesian methods are becoming increasingly popular with better computers and algorithms like MCMC. Also, in finite dimensional models, Bayesian inference may have same guarantees of consistency and rate of convergence as frequentist models. I don't think there is any way around really understanding Bayesian and frequentist reasoning without confronting (or at least acknowledging) the interpretations of probability.
