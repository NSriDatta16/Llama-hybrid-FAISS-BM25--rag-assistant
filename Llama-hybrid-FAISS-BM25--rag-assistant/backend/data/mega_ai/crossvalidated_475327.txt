[site]: crossvalidated
[post_id]: 475327
[parent_id]: 
[tags]: 
Residual block in temporal convolutional neural network

I'm trying to understand the structure of the temporal convolutional neural network as described in here . Figure 1 of the paper illustrates a residual block. Are the dilated convolutions on the left the same as the layers on the right? i.e., at the bottom layer, dilated convolutions are applied then -> weight normalisation -> ReLU -> dropout -> next dilated convolution layer? If so, why is the 1x1 convolution needed?
