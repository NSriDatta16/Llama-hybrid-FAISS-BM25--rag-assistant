[site]: crossvalidated
[post_id]: 438973
[parent_id]: 
[tags]: 
Why does gradient descent HAVE to find the minimum as oppose to a change in the opposite direction

I have a question about the gradient descent step in neural networks. I fully understand the derivative step and taking the steps required to move in the direction that reduces the loss (finding the minima). What i am unsure about is the direction in which we are moving. When calculating the derivative of the the cost with respect to the weights, why is it that we move in the direction of minimizing this cost? why does this optimize the model and cause weight adjustment that produces better predictions. Is the local minima directly linked to the loss function? Is it intuitive that the loss function creates error metric 'x' and that finding the local maximum through gradient descent for example would simply increase the error and thus weight adjustment moves in the wrong direction?
