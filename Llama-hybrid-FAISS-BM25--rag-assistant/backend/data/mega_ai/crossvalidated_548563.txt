[site]: crossvalidated
[post_id]: 548563
[parent_id]: 
[tags]: 
Manually Creating "Groups" from the Data and Testing their Statistical Significance

I am working on the following problem: Suppose a hospital has a dataset containing the weight of an individual patient and the number of days that this patient spent in the hospital (i.e. each row contains information on an individual patient). The hospital believes that heavier patients spend on average more time in the hospital. Using the R programming language, I tried to simulate a realistic looking dataset) : set.seed(123) weight_of_patient A plot of the data would look like this: library(ggplot2) ggplot(final_data, aes(x=final_data $weight_of_patient, y=final_data$ days_spent_in_hospital)) + geom_point() + xlab("Weight (lbs) ") + ylab("Number of Days Spent in Hospital") + ggtitle("Number of Days Spent in Hospital vs. Weight of Patient") Problem: The hospital is interested in creating 3 ("logical") groups of patients based on their weight and how long they spend in the hospital. These groups will be used for planning purposes, communicating with the patient's family, etc. These groups must be easily explainable to the patient's family (i.e. no random forest, no neural network, no gradient boosting, no regression models, etc.). When the analysis is finished, the groups should look something like this: Group 1 : Weight Group 2 : Weight >180 and Group 3 : Weight > 200 lbs: AT LEAST 80% of the patients in Group 3 spend less than 120 days in the hospital On some level, this problem is easier than the common types of supervised classification problems. If patient in the weight class from Group 2 spends less than 50 days in the hospital (the time designated for patients in Group 1) - this is not considered as a "problem". But if a patient from the weight class from Group 1 spends more than 100 days in the hospital (the time designated for patients in "higher weight groups") - this is considered a problem. That is, it's always better to overestimate the time someone will spend in the hospital instead of underestimating. I tried to draw this cost-misclassification table below (for some arbitrary group criteria): The most natural way that this kind of problem is solved is by using "clustering algorithms" (e.g. K-Means Clustering): kmeans_clustering $cluster = kmeans_clustering$ cluster ggplot(final_data, aes(x=final_data $weight_of_patient, y=final_data$ days_spent_in_hospital)) + geom_point(aes(color = factor(kmeans_clustering$cluster) )) + xlab("Weight (lbs) ") + ylab("Number of Days Spent in Hospital") + labs(color = "Cluster Membership")+ ggtitle("Number of Days Spent in Hospital vs. Weight of Patient") However, the "clustering boundaries" will inevitably be "too irregular" and difficult to explain: Instead, it seems like the following approach would be more suitable for this kind of problem (e.g. pick some arbitrary weight groups (red) and their corresponding 80% percentiles (blue) - note: general drawing): My Question: I would be interested in testing how "statistically reliable" a given set of groups is likely to represent future data. E.g. In the future, can we expect AT LEAST 80% of people who weigh between 180 lbs and 200 lbs to spend less than 100 days in the hospital? To do this, I first arbitrarily chose weight groups (using the same definitions as above) and their corresponding hospital times. I then created a "cross validation" style procedure that takes random subsets of the data (e.g. 70%) and checks how many of the patients within each weight group match their corresponding hospital stay time (taking into account the cost-misclassification). I repeat this process 1000 times: library(data.table) library(dplyr) results $weight_group = as.factor(ifelse(train_i$ weight_of_patient $weight_of_patient > 180 & train_i$ weight_of_patient $predicted_days = ifelse(train_i$ weight_group == "A", 50, ifelse( train_i$weight_group == "B", 100,120)) train_i $accuracy = ifelse(train_i$ days_spent_in_hospital % group_by(weight_group) %>% dplyr::summarize(Mean = mean(accuracy, na.rm=TRUE))) results_tmp$iteration = i results_tmp $total_mean = mean(train_i$ accuracy) results[[i]] Here are the results (each row is an iteration, each column represents the accuracy of each patient group for that iteration): head(final_table) iteration A B C 1: 1 0.8367238 0.8740661 0.9738951 2: 2 0.8325719 0.8721123 0.9728831 3: 3 0.8299191 0.8740135 0.9747227 4: 4 0.8354497 0.8728378 0.9768330 5: 5 0.8297247 0.8756551 0.9750973 6: 6 0.8359587 0.8734632 0.9768683 The above table be used to check on average, how many patients from each weight group spent less than their designated time in the hospital (in the simulation): mean(final_table$A) [1] 0.8329073 mean(final_table$B) [1] 0.8740477 mean(final_table$C) [1] 0.9748849 We can also look at the histograms of these weight groups (A = 1, B = 2, C = 3) throughout the simulation (useful for confidence intervals): par(mfrow=c(1,3)) hist(final_table$A, breaks = 90, main = "Distribution of Patients in Weight Group 1 Who Spent Less than 50 Days") hist(final_table$B, breaks = 90, main = "Distribution of Patients in Weight Group 2 Who Spent Less than 100 Days") hist(final_table$C, breaks = 90, main = "Distribution of Patients in Weight Group 3 Who Spent Less than 120 Days") Conclusion: Is the statistical methodology that I used in this question reasonable? After performing this simulation, can we reasonably expect that (provided our data is well representative of how future data will behave): Group 1 : Weight Group 2 : Weight >180 and Group 3 : Weight > 200 lbs: AT LEAST 80% of the patients in Group 3 spend less than 120 days in the hospital (in the simulation, we saw that on average 97% of patients in Group 3 spent less than their designated time in the hospital) Through simulation, I tried to test how well the "weight and time groups" generalize to unseen samples of data, even to samples which are unfavorable to initial hypothesis (i.e. outliers: heavier patients that spend shorter amounts of time at the hospital and lighter patients that spend longer amounts of time at the hospital). I know this is a unusual problem which has a strange criteria for accuracy/misclassification, where common statistical methods can not be used, and a seemingly arbitrary target is in place (i.e. 80% of patients in each weight group spending less than their designated time in the hospital). I know that these weight/time groups also seem arbitrary (e.g. these weight/time groups could represent established industry standards that an individual hospital wants to see how well they follow) - but from a statistics point of view, is the methodology logical? Thanks
