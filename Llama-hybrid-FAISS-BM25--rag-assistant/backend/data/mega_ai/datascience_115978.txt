[site]: datascience
[post_id]: 115978
[parent_id]: 115977
[tags]: 
It’s actually possible to use accuracy in a regression problem. As usual, your accuracy score is the number of correct predictions divided by the number of total predictions. Unfortunately, even a regression model with strong performance would be expected to get each prediction at least a little wrong, so the accuracy would be zero, but nothing stops you from calculating that. Maybe you’ll get exceptionally lucky and get a few regression predictions perfectly right! $$L(y_i, \hat y_i) = \begin{cases} 0 & y_i \ne \hat y_i \\ 1 & y_i = \hat y_i \end{cases}\\\\ \text{accuracy}(y,\hat y) = \dfrac{100\%}{n}\sum_{i=1}^n L(y_i,\hat y_i) $$ This applies for either regression or classification problems, though its utility is rather limited in regression problems. (Actually, accuracy is of limited utility for "classification" problems, too.) Mean percent error calculates the residual divided by the actual value, calculates the average value of these, and multiplied by $100\%$ to put the value in terms of percent of the observation. $$ MPE(y,\hat y)=\dfrac{100\%}{n}\sum_{i=1}^n \dfrac{ y_i-\hat y_i }{ y_i } $$ Setting aside the exact equations, accuracy measures the percentage of the time where your model made the correct prediction, and MPE measures the average percent from the true observations of your predictions.
