[site]: crossvalidated
[post_id]: 621617
[parent_id]: 620783
[tags]: 
Should the error bound in [1] only be dependent on the GP prior as it is not assumed that complies with the posterior GP? Or does the assumption of being a sample path of the GP prior implicitly imply complies with the posterior GP? The short answer is "yes", regarding the second question. The assumption of $f$ being a sample path of the GP prior implies that $f$ will also be a sample path of the GP posterior. By the GP modelling assumptions, both the vector of observations $\mathbf{y}_t := [y_1, ..., y_t]^\mathtt{T}$ , where $y_i := f(x_i) + \epsilon_i$ , with $\epsilon_i\sim N(0,\sigma^2)$ , for $i \in \{1,\dots, t\}$ , and the value of $f$ at an arbitrary point $x$ in its domain will be jointly Gaussian distributed, as: $$\begin{bmatrix} \mathbf{y}_t\\ f(x) \end{bmatrix} \sim N \left( \begin{bmatrix} \mathbf{0}\\ 0 \end{bmatrix}, \begin{bmatrix} \mathbf{K}_t + \sigma^2\mathbf{I} & \mathbf{k}_t(x)\\ \mathbf{k}_t(x)^\mathtt{T} & k(x,x) \end{bmatrix} \right), $$ where $\mathbf{K}_t := [k(x_i, x_j)]_{i, j = 1}^{t,t} \in \mathbb{R}^{t\times t}$ and $\mathbf{k}_t(x) := [k(x, x_1),\dots, k(x, x_t)]^\mathtt{T} \in \mathbb{R}^t$ . Solving for $f(x)$ , we have the GP predictive equations with $f(x) \sim N(\mu_{t}(x), \sigma_t^2(x))$ , with: $$\begin{align} \mu_t(x) &:= \mathbf{k}_t(x)^\mathtt{T}(\mathbf{K}_t + \sigma^2\mathbf{I})^{-1}\mathbf{y}_t\\ \sigma_t^2(x) &:= k(x,x) - \mathbf{k}_t(x)^\mathtt{T}(\mathbf{K}_t + \sigma^2\mathbf{I})^{-1}\mathbf{k}_t(x). \end{align}$$ For a discrete domain with a finite number of points, we can then apply union bounds to derive the error bound in Lemma 5.1 of Srinivas et al. [1]. For the second assumption, where $f \in H_k$ , with $H_k$ being the reproducing kernel Hilbert space (RKHS) associated with the GP covariance function $k$ , note that this does not imply that $f$ is a sample from a GP. For a continuous domain and most kernels, $f \sim GP(0, k)$ usually implies $f \notin H_k$ . It is possible to show that $f$ is in the RKHS associated with a fractional power of the kernel, though (see Theorem 4.12 in Kanagawa et al. (2018), below). In any case, the toolset to derive error bounds for $f(x)$ in this second assumption is different, and it usually involves analysing Martingale difference sequences and sub-Gaussian noise assumptions. Kanagawa, M., Hennig, P., Sejdinovic, D., & Sriperumbudur, B. K. (2018). Gaussian Processes and Kernel Methods: A Review on Connections and Equivalences. ArXiv. http://arxiv.org/abs/1807.02582
