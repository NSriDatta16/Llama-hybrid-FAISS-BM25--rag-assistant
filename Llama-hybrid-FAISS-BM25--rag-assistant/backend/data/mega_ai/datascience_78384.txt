[site]: datascience
[post_id]: 78384
[parent_id]: 78383
[tags]: 
Training of XGBoost is based on a boosting model, which is a general ensemble method creating a strong model from a number of weak models. This process is performed by building a model from the training dataset, then, creating a second model that attempts to correct the errors from the first model. Models are added until the training set is predicted perfectly or a maximum number of models are added. All machine learning models based on boosting almost follow the above procedure. For example, AdaBoost is one of the boosting algorithms developed for binary classification. Methods like LightGBM and Catboost use this algorithm. Unlike the boosting algorithm, in the bagging algorithm, the models are independent and each model is directly fitted to a subset of the original training data.
