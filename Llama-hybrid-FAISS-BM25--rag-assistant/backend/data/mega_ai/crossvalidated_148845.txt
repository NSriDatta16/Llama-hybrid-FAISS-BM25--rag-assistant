[site]: crossvalidated
[post_id]: 148845
[parent_id]: 120776
[tags]: 
It is not just a matter of "heavier tails" â€” there are plenty of distributions that are bell shaped and have heavy tails. The T distribution is the posterior predictive of the Gaussian model. If you make a Gaussian assumption, but have finite evidence, then the resulting model is necessarily making non-central scaled t-distributed predictions. In the limit, as the amount of evidence you have goes to infinity, you end up with Gaussian predictions since the limit of the t distribution is Gaussian. Why does this happen? Because with a finite amount of evidence, there is uncertainty in the parameters of your model. In the case of the Gaussian model, uncertainty in the mean would merely increase the variance (i.e., the posterior predictive of a Gaussian with known variance is still Gaussian). But uncertainty about the variance is what causes the heavy tails. If the model is trained with unlimited evidence, there is no longer any uncertainty in the variance (or the mean) and you can use your model to make Gaussian predictions. This argument applies for a Gaussian model. It also applies to a parameter that is inferred whose likelihoods are Gaussian. Given finite data, the uncertainty about the parameter is t-distributed. Wherever there are Normal assumptions (with unknown mean and variance), and finite data, there are t-distributed posterior predictives. There are similar posterior predictive distributions for all of the Bayesian models. Gelman is suggesting that we should be using those. His concerns would be mitigated by sufficient evidence.
