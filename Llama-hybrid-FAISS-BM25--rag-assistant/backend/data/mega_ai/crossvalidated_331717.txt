[site]: crossvalidated
[post_id]: 331717
[parent_id]: 331711
[tags]: 
The Bayesian approach to inference provides a natural framework to incorporate the sort of uncertainty you describe. My answer will be expressed in general terms, intended to provide a conceptual framework for thinking about the problem. Consider the "base case" where $x$ is a collection of observations and $\theta$ is a parameter vector. The joint distribution is given by \begin{equation} p(x,\theta) = p(x|\theta)\,p(\theta), \end{equation} where $p(x|\theta)$ is likelihood (once $x$ is observed) and $p(\theta)$ is the prior distribution for $\theta$. The posterior distribution for $\theta$ can be expressed as \begin{equation} p(\theta|x) \propto p(x|\theta)\,p(\theta) . \end{equation} Now suppose that $x$ is not observed directly, but only indirectly via $y$ due to measurement error (for example). The joint distribution is now given by \begin{equation} p(x,y,\theta) = p(y|x)\,p(x|\theta)\,p(\theta). \end{equation} The posterior distribution for the unobserved entities (variable and parameters) is \begin{equation} p(x,\theta|y) \propto p(y|x)\, p(x|\theta)\,p(\theta) . \end{equation} The marginal posterior for $\theta$ is given by \begin{equation} p(\theta|y) = \int p(x,\theta|y)\,dx . \end{equation} The joint posterior distribution for $x$ and $\theta$ can be characterized by the following full conditional distributions: \begin{align} p(\theta|x,y) &\propto p(x|\theta)\,p(\theta) \\ p(x|y,\theta) &\propto p(y|x)\,p(x|\theta) . \end{align} The righthand sides incorporate important simplifications. First, the conditional distribution for $\theta$ does not depend on $y$. Second, the conditional distribution for $x$ does not depend on the prior for $\theta$. The Gibbs sampler makes draws from the joint posterior by alternating between the two conditional posteriors.
