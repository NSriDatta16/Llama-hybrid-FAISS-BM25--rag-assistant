[site]: crossvalidated
[post_id]: 451236
[parent_id]: 
[tags]: 
Reporting the average log-probability the model assigns to some examples

I am currently studying Deep Learning by Goodfellow, Bengio, and Courville. In chapter 5.1.2 The Performance Measure, $P$ , the authors say the following: To evaluate the abilities of a machine learning algorithm, we must design a quantitative measure of its performance. Usually this performance measure $P$ is specific to the task $T$ being carried out by the system. For tasks such as classification, classification with missing inputs, and transcription, we often measure the accuracy of the model. Accuracy is just the proportion of examples for which the model produces the correct output. We can also obtain equivalent information by measuring the error rate , the proportion of examples for which the model produces an incorrect output. We often refer to the error rate as the expected 0-1 loss. The 0-1 loss on a particular example is $0$ if it is correctly classified and $1$ if it is not. For tasks such as density estimation, it does not make sense to measure accuracy, error rate, or any other kind of 0-1 loss. Instead, we must use a different performance metric that gives the model a continuous-values score for each example. The most common approach is to report the average log-probability the model assigns to some examples. It is this last part that I'm curious about: The most common approach is to report the average log-probability the model assigns to some examples. Just so I am clear on this, can someone please give a mathematical illustration of what this process would look like? I would greatly appreciate it if people would please take the time to clarify this.
