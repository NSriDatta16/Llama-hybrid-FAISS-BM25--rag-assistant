[site]: datascience
[post_id]: 56319
[parent_id]: 56307
[tags]: 
You should take into account a few factors: some_value is key here. You should probably choose a value greater than e1_dim and e2_dim . This may be a problem due to the large memory needs for high embedding dimensionalities. if e2_dim is bounded to $(-1,+1)$ , you may want to set a tanh activation in the last layer (i.e. after Dense(e2_dim) ). With that in place, you may want to replace the activation of the hidden layer with a ReLU, to avoid squashing the gradients unnecessarily. If your embeddings are directional, or they are in the unit ball, then you should be fine with cosine_proximity . Maybe a better option would be to use the von Mises-Fischer distance , but I think it's not straightforward. Otherwise, you may try World-Mover's Distance . That being said, a lot of embedded space mapping works out there assume that the embedded spaces are approximately isomorphic and just go ahead with a linear transformation. You may want to have a look at the relevant bilingual embedding mapping literature; you may start with the work by Artetxe et al., 2016 . I don't know if the mentioned assumption is met to any degree in your scenario (skipgram $\leftrightarrow$ node2vec), but you may assess this with the VF2 algorithm , like Artetxe et al. did.
