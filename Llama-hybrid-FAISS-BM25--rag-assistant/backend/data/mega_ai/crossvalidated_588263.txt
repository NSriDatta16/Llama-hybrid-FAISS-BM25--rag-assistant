[site]: crossvalidated
[post_id]: 588263
[parent_id]: 588260
[tags]: 
Transformer neural networks seem like an obvious candidate. They are most famous for being used for text data, where of course the input size is variable and the order is fixed. However, note that transformers for text data had to add an extra component to deal with the ordering of the input (i.e. positional encodings), while a basic transformer without that would treat input as not having any particular order. The other obvious option is to somehow summarize information across the multiple records into a fixed dimensional set of features, which probably would have to be done based on human expertise/judgment. Thereafter, a much wider variety of ML algorithms can be used straightforwardly. However, it may be challenging to find a set of features that does not loose too much information. E.g. it can be very challenging to deal with interactions of features (an example of this could be if an individual column with feature A being high and feature B being high + a second column with both low results in a high probability, but not when one has A high and B low + the other A low and B high).
