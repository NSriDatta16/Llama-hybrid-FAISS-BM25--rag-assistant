[site]: crossvalidated
[post_id]: 534686
[parent_id]: 534680
[tags]: 
To get rid of high cardinality features, you have options Frequency encoding, which encodes the frequencies of the entities instead of their categorical values Mean encoding (beware of possibility of overfitting) because you'll be using target information Hashing and some others. As a first step, I'd go with frequency encoding, it's simple and less risky compared to mean encoding, and more meaningful than hashing. To calculate website frequencies, use your training data (not testing). You may need to preprocess your data to accommodate for Other Website category, since you may have new websites in the test data. You may also want to use existing architectures like doc2vec, or train separate autoencoders for the websites, but I'd first start simple.
