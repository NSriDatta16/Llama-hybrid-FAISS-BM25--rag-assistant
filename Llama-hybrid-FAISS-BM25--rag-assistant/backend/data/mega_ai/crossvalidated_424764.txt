[site]: crossvalidated
[post_id]: 424764
[parent_id]: 424756
[tags]: 
So your estimated model is like: $\Delta_{y_{t+1}}=\beta \epsilon_{t}+\epsilon_{t+1}$ which implies $y_{t+1}=y_{t}+\beta \epsilon_{t}+\epsilon_{t+1}$ where your estimated beta is the value reported in the output of -0.8024. This means that, if you want to predict $y_{t+1}$ given the information set available today at time t, you can do so (as you know the time-t innovation $\epsilon_{t}$ ), and your prediction on $y_{t+1}$ is $y_{t}+\beta \epsilon_{t}$ . In general you can make 1-step-ahead forecasts. Now, what happens if you want to predict a generic $y_{t+k}$ for k>1? You will have no better prediction that $y_{t+1}$ . Indeed $E_{t}(y_{t+2})=E_{t}(y_{t+1}+\beta \epsilon_{t+1}+\epsilon_{t+2})=E_{t}(y_{t+1})=y_{t}+\beta \epsilon_{t}$ . This will hold for any k>=2. Why your auto-specification function is giving you this result? Well, if you have correctly specified the rest of the process distribution and your sample is large enough, then this means that your time series is a I(1) process with very little persistence in the first-difference $\Delta_{y_{t}}$ . We could say that it is a touch more persistent than a random walk whose increments are White Noises (because here you have a MA(1) on first differences). If you want to have a real-world example of similar time-series (i.e. examples of I(1) processes with first diff that are even less persistent than yours on average!), try for example to fit a model like yours on the natural log of daily stock prices. You will see that, for most cases, you will have that even a simple MA(1) on first differences is not significant.
