[site]: crossvalidated
[post_id]: 371352
[parent_id]: 
[tags]: 
Using the Expected value of the log as a score for the anomaly detection instead of just the expected value

While dealing with anomaly detection using a probabilistic model I need to compute the probability of an example coming out of the model I built. More specifically: If $p(X)$ is the model I built and trained from my data, I need to compute the reconstruction probability of an outlier taking into account the latent variable of my model (just for clarity, I'm using a Variational Autoencoder). If $\hat{x}$ is an outlier I expect the reconstruction probability to be low, since it does not come from the model. $reconstruction\_prob = E_{z}[p(x)]$ where $z$ is the latent variable. This idea is taken from this paper: http://dm.snu.ac.kr/static/docs/TR/SNUDM-TR-2015-03.pdf and referenced is this other paper: https://arxiv.org/abs/1802.03903 (section 3.3). All of these papers proceed to say that instead of using $E_{z}[p(x)]$ it is more convenient to use $E_{z}[log(p(x))]$ . This would be great news but I can't prove that: Being $x,y$ two examples, if: $E_z[p(x)] then: $E_z[log(p(x))] which I guess is what I need in order to use $E_z[log(p(x))]$ as a reconstruction "probability" What am I missing?
