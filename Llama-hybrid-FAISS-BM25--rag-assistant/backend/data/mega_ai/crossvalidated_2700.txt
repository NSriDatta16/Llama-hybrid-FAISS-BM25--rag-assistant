[site]: crossvalidated
[post_id]: 2700
[parent_id]: 2691
[tags]: 
The manuscript "A tutorial on Principal Components Analysis" by Lindsay I Smith really helped me grok PCA. I think it's still too complex for explaining to your grandmother, but it's not bad. You should skip first few bits on calculating eigens, etc. Jump down to the example in chapter 3 and look at the graphs. I have some examples where I worked through some toy examples so I could understand PCA vs. OLS linear regression. I'll try to dig those up and post them as well. edit: You didn't really ask about the difference between Ordinary Least Squares (OLS) and PCA but since I dug up my notes I did a blog post about it . The very short version is OLS of y ~ x minimizes error perpendicular to the independent axis like this (yellow lines are examples of two errors): If you were to regress x ~ y (as opposed to y ~ x in the first example) it would minimize error like this: and PCA effectively minimizes error orthogonal to the model itself, like so: More importantly, as others have said, in a situation where you have a WHOLE BUNCH of independent variables, PCA helps you figure out which linear combinations of these variables matter the most. The examples above just help visualize what the first principal component looks like in a really simple case. In my blog post I have the R code for creating the above graphs and for calculating the first principal component. It might be worth playing with to build your intuition around PCA. I tend to not really own something until I write code that reproduces it.
