[site]: crossvalidated
[post_id]: 155767
[parent_id]: 155720
[tags]: 
I understood agenis to say that, in creating the std dev, he wanted to base them on bucketed X values, not Y values. If he were asking about bucketing the Y values, then Matthew Drury would be correct that this would "leak" Y into the predictors. In addition, agenis hasn't said whether or not there is a temporal dimension to the information. We're all assuming that there is not. If there is, then taking lags of Y would be appropriate and an adequate control for the issue of Y's "leakage" into the predictors. In addition, any temporal relationship would open up new classes of modeling options from nonlinear diffusion models to the many flavors of "Box-Jenkins" type methods. There is still lots of room to play around here. Simply bucketing the X values, e.g., creating 10 mutually exclusive groupings on X, amounts to a kind of poor-man's approach to kernel density analysis. Based on the chart and given the convex slope of the curve, one can see that these new groupings rapidly decline in their predictive power across the range of X wrt Y. Given that, it may well be that fitting 2 or 3 splines would provide a better fit than the main effects model as proposed. If one chooses to bucket X, a consideration worth exploring is using the within bucket coefficient of variation instead of the std dev. The CV is the ratio of the std dev to the mean (times 100) and would result in a metric that is invariant and comparable across the levels of X. Why does this matter? Take two stock prices as an example. Stock 1 has an average price of 500 and a std dev of 100 while stock 2 has an average price of 50 and a std dev of 20. Which stock is more volatile? You can't look at the std devs in and of themselves to answer this question since they are scale dependent. The CV for stock 1 is 20 (100/500*100=20) and for stock 2 it is 40. Therefore and despite a smaller std dev, stock 2 has more inherent volatility than stock 1. To me, the advantages of a metric like this over a scale dependent std dev are clear. Another possibility would be not to bucket X and retain its continuously distributed nature with a transformation. For instance, it could be that, again based on the chart, the relationship between X and Y is exponential. Depending on the magnitude or scale of X, exponentiation could quickly result in byte overflow (values so large they don't fit into the numeric formatting). Given that risk, transforming X first with, e.g., a natural log function and then taking the exponent would be a workaround. Other transformations of X that retain its continuous nature and compress its PDF (probability density function, i.e., its tail) are also possible. There are literally dozens, if not more, transformations available in the literature. There is a book devoted to cataloguing mathematical transformations, although I forget the title. All of the suggestions made so far involve linear functions and models based on X. Models nonlinear in the parameters are also possible but could be hairy in terms of both specification and interpretation. At the end of the day, the question becomes one of the relative importance of prediction vs substantive interpretation of the model results. If the focus is simply on prediction, then a "black box" model that fits the data but is opaque in meaning is permissible. If strategic insights into the relationship between X and Y is the goal, then keeping things at the level, not just of the analyst, but the analyst's audience, is imperative. In this latter instance, highly technical solutions are to be avoided since it's almost certainly the case that the audience will be comprised of the technically semi-literate, at best, with a strong skew to technical illiteracy. It's every analyst's worst nightmare to be explaining something to an innumerate audience where they are the only person in the room who understands what they're talking about. Of course, breaking the data into separate test and holdout samples (or k-folds) to evaluate the model fit "out-of-sample" and control for overfitting is mandated.
