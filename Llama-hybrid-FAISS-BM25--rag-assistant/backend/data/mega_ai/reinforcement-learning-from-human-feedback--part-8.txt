(y|x)}}\right)\right]} However, instead of doing the intermediate step of the reward model, DPO directly optimizes for the final policy. First, solve directly for the optimal policy, which can be done by Lagrange multipliers, as usual in statistical mechanics: π ∗ ( y | x ) = π SFT ( y | x ) exp ⁡ ( r ∗ ( x , y ) / β ) Z ( x ) , {\displaystyle \pi ^{*}(y|x)={\frac {\pi ^{\text{SFT}}(y|x)\exp(r^{*}(x,y)/\beta )}{Z(x)}},} where Z ( x ) {\displaystyle Z(x)} is the partition function. This is unfortunately not tractable, since it requires summing over all possible responses: Z ( x ) = ∑ y π SFT ( y | x ) exp ⁡ ( r ∗ ( x , y ) / β ) = E y ∼ π SFT ( ⋅ | x ) [ exp ⁡ ( r ∗ ( x , y ) / β ) ] {\displaystyle Z(x)=\sum _{y}\pi ^{\text{SFT}}(y|x)\exp(r^{*}(x,y)/\beta )=\mathbb {E} _{y\sim \pi ^{\text{SFT}}(\cdot |x)}[\exp(r^{*}(x,y)/\beta )]} Next, invert this relationship to express the reward implicitly in terms of the optimal policy: r ∗ ( x , y ) = β log ⁡ π ∗ ( y | x ) π SFT ( y | x ) + β log ⁡ Z ( x ) . {\displaystyle r^{*}(x,y)=\beta \log {\frac {\pi ^{*}(y|x)}{\pi ^{\text{SFT}}(y|x)}}+\beta \log Z(x).} Finally, plug it back to the maximum likelihood estimator, we obtain π ∗ = arg ⁡ max π E ( x , y 1 , … , y N ) ∼ D [ ln ⁡ ∏ k = 1 N e β log ⁡ π ( y k | x ) π SFT ( y k | x ) ∑ i = k N e β log ⁡ π ( y i | x ) π SFT ( y i | x ) ] {\displaystyle \pi ^{*}=\arg \max _{\pi }\mathbb {E} _{(x,y_{1},\dots ,y_{N})\sim D}\left[\ln \prod _{k=1}^{N}{\frac {e^{\beta \log {\frac {\pi (y_{k}|x)}{\pi ^{\text{SFT}}(y_{k}|x)}}}}{\sum _{i=k}^{N}e^{\beta \log {\frac {\pi (y_{i}|x)}{\pi ^{\text{SFT}}(y_{i}|x)}}}}}\right]} Usually, DPO is used for modeling human preference in pairwise comparisons, so that N = 2 {\displaystyle N=2} . In that case, we have π ∗ = arg ⁡ max π E ( x , y w , y l ) ∼ D [ log ⁡ σ ( β log ⁡ π ( y w | x ) π SFT ( y w | x ) − β log ⁡ π ( y l | x ) π SFT ( y l | x ) ) ] {\displaystyle \pi ^{*}=\arg \max _{\pi }\mathbb {E} _{(x,y_{w},y_{l})\sim D}\left[\log \sigma \left(\beta \log {\frac {\pi (y_{w}|x)}{\pi ^{\text{SFT}}(y_{w}|x)}}-\beta \log {\frac {\pi (y_{l}|x)}{\pi ^{\text{SFT}}(y_{l}|x)}}\right)\right]} DPO eliminates the need for a separate reward model or reinforcement learning loop, treating alignment as a supervised learning problem over preference data. This is simpler to implement and train than RLHF and has been shown to produce comparable and sometimes superior results. Nevertheless, RLHF has also been shown to beat DPO on some datasets, for example, on benchmarks that attempt to measure truthfulness. Therefore, the choice of method may vary depending on the features of the human preference data and the nature of the task. Identity preference optimization Identity preference optimization (IPO) is a modification to the original DPO objective that introduces a regularization term to reduce the chance of overfitting. It remains robust to overtraining by assuming noise in the preference data. Foremost, IPO first applies a non-linear mapping over the probability distribution of preferences Ψ ( q ) = log ⁡ ( q / ( 1 − q ) ) {\displaystyle \Psi (q)=\log(q/(1-q))} instead of the Bradley-Terry assumption to soften the probability of preferences and smooth the labels. Here, Ψ ( q ) {\displaystyle \Psi (q)} denotes the Ψ {\displaystyle \Psi } preference objective separate from the policy objective. This helps avoid the overfitting issue of the assumption that pairwise preferences can be substituted for point-wise rewards, which weakens the KL regularization by heavily skewing the preference distribution. As with DPO, IPO is also formulated as an offline learning objective learned over a human preference dataset D {\displaystyle D} . In particular, the IPO introduces a new objective by applying a mapping Ψ {\displaystyle \Psi } over the preference probability distribution. Practically, Ψ {\displaystyle \Psi } is taken as the identity mapping, which results in IPO. Hence, IPO also directly optimizes for the final policy from the prefer