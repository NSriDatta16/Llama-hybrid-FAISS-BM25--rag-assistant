[site]: crossvalidated
[post_id]: 539881
[parent_id]: 
[tags]: 
At which point does an ML model pick up biases in datasets?

I am currently examining bias in NLP datasets such as CommonsenseQA . I am looking for a reference where researchers possibly claim at which point , i.e., how biased a dataset must be, in order for an ML model to pick it up. Is anyone aware of work going in this direction? I cannot find anything.
