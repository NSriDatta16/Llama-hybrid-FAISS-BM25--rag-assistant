[site]: crossvalidated
[post_id]: 487459
[parent_id]: 
[tags]: 
Metropolis-Hastings for linear regression, prior on sigma?

For the sake of curiosity, I'm trying to build a Metropolis-Hastings sampler for the purposes of Bayesian linear regression. Below, you'll note my script and more specifically, in-line comments that noting to comment in/out various lines in order to change the behavior of the script. As is, the sampler iteratively proposes a change to either b (the slope), or a (the slope intercept.) However, these parameters of the linear function y=bx+a, are not updated simultaneously. It works great! However, as is, the script does not propose changes to s, the standard deviation of the linear function. When I alter the code block as detailed, the sampler fails to change. It simply stagnates at the initial possible value. My questions are: (1) What are the benefits of sampling different sigma values? I get a pretty good understanding of b and a with s constant. (2) Am I proposing changes to s wrong? I understand that it cannot be negative, but it also needs to be sampled from a symmetric distribution, allowing for increases and decreases. I've used absolute value of the current value plus some random change. (-0.15 -> 0.15) (3) Is there a better prior choice for sigma? I'm using inverse gamma. Also, you'll note that none of my distribution functions involve normalizing constants as this generally isn't necessary in MH. My code: import numpy as np import random def normalPDF(x,mu,sigma): num = np.exp((x-mu)**2/-2*sigma**2) return num def invGamma(x,a,b): non_zero = int(x>=0) func = x**(a-1)*np.exp(-x/b) return non_zero*func def lr_mcmc(X,Y,hops=10_000): samples = [] curr_b = 1 curr_a = 1 curr_s = 1 prior_b_curr = normalPDF(x=curr_b,mu=2,sigma=1) prior_a_curr = normalPDF(x=curr_a,mu=1,sigma=1) prior_s_curr = invGamma(x=curr_s, a=2,b=2) log_lik_curr = sum([np.log(normalPDF(x=curr_b*x + curr_a,mu=y,sigma=curr_s)) for x,y in zip(X,Y)]) current_numerator = log_lik_curr + np.log(prior_a_curr) + np.log(prior_b_curr) + np.log(prior_s_curr) count = 0 for i in range(hops): samples.append((curr_b,curr_a,curr_s)) if count == 0: mov_b = curr_b + random.uniform(-0.25,0.25) mov_a = curr_a mov_s = curr_s count += 1 elif count == 1: mov_a = curr_a + random.uniform(-0.25,0.25) mov_b = curr_b mov_s = curr_s # to change behavior: # count += 1 # uncomment line count = 0 # comment line out # to change behavior, uncomment below code block: # else: # mov_s = np.abs(curr_s + random.uniform(-0.25,0.25)) # mov_b = curr_b # mov_a = curr_a # count = 0 prior_b_mov = normalPDF(x=mov_b,mu=2,sigma=1) prior_a_mov = normalPDF(x=mov_a,mu=1,sigma=1) prior_s_mov = invGamma(x=mov_s,a=2,b=2) log_lik_mov = sum([np.log(normalPDF(x=mov_b*x + mov_a,mu=y,sigma=mov_s)) for x,y in zip(X,Y)]) movement_numerator = log_lik_mov + np.log(prior_a_mov) + np.log(prior_b_mov) + np.log(prior_s_mov) ratio = np.exp(movement_numerator - current_numerator) event = random.uniform(0,1) if event My plot when running successfully w/o code block change. x-axis = slope, y-axis = y-intercept. And the error when I change the code /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:57: RuntimeWarning: invalid value encountered in double_scalars /usr/local/lib/python3.6/dist-packages/statsmodels/nonparametric/kernels.py:128: RuntimeWarning: divide by zero encountered in true_divide return (1. / np.sqrt(2 * np.pi)) * np.exp(-(Xi - x)**2 / (h**2 * 2.)) /usr/local/lib/python3.6/dist-packages/statsmodels/nonparametric/kernels.py:128: RuntimeWarning: invalid value encountered in true_divide return (1. / np.sqrt(2 * np.pi)) * np.exp(-(Xi - x)**2 / (h**2 * 2.)) /usr/local/lib/python3.6/dist-packages/matplotlib/contour.py:1483: UserWarning: Warning: converting a masked element to nan. self.zmax = float(z.max()) /usr/local/lib/python3.6/dist-packages/matplotlib/contour.py:1484: UserWarning: Warning: converting a masked element to nan. self.zmin = float(z.min()) /usr/local/lib/python3.6/dist-packages/matplotlib/contour.py:1132: RuntimeWarning: invalid value encountered in less under = np.nonzero(lev self.zmax)[0] And when I look at the samples, it's just one b,a,s combination for all 25,00 ierations. [(1, 1, 1), (1, 1, 1), (1, 1, 1), (1, 1, 1), (1, 1, 1), (1, 1, 1), (1, 1, 1), ... ]
