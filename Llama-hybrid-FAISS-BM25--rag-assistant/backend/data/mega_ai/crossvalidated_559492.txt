[site]: crossvalidated
[post_id]: 559492
[parent_id]: 559284
[tags]: 
Short answer: Probably not Long answer: If you have heteroskedasticity, but the variance of your errors is independent of the included covariates, robust standard errors will be very similar to OLS standard errors. They won't be as close compared to having homoskedastic errors. If the variance of your error terms is higher when included covariates are far from their means, OLS standard errors will tend to be biased down, and robust standard errors will tend to be larger than OLS standard errors. This is usually what you see in practice. When the variance of the error terms is lower when included covariates are far from their means, OLS standard errors will tend to be too large, and robust standard errors will tend to be smaller than OLS standard errors. This is pretty exotic in my personal experience. I am not sure if I have ever seen this in a real-world dataset. Here is Stata output demonstrating the above using regressions on simulated data: . clear . /* (1) Generate Fake Data */ . set obs 10000 Number of observations (_N) was 0, now 10,000. . set seed 1052021 . generate x = rnormal(100,15) . // This looks cryptic, but I just played around with . // the transformations until . // all three errors were mean zero . // and the errors had the same variance . generate e0 = rnormal(0,301) - 2.469 . generate e1 = rnormal(0, (sin(x)+1)*243) + 0.437 . generate e2 = rnormal(0, x^2)/35.75 + 4.81 . generate e3 = rnormal(0, 5e3*normalden(x,100,15))*2.95 - 0.31 . generate y0 = 10 + 5*x + e0 . generate y1 = 10 + 5*x + e1 . generate y2 = 10 + 5*x + e2 . generate y3 = 10 + 5*x + e3 . // Plot the Data . twoway (scatter y0 x), name(y0, replace) ytitle(, orientation(horizontal)) . twoway (scatter y1 x), name(y1, replace) ytitle(, orientation(horizontal)) . twoway (scatter y2 x), name(y2, replace) ytitle(, orientation(horizontal)) . twoway (scatter y3 x), name(y3, replace) ytitle(, orientation(horizontal)) . graph combine y0 y1 y2 y3, xcommon . summarize e* Variable | Obs Mean Std. dev. Min Max -------------+--------------------------------------------------------- e0 | 10,000 .0004878 300.5657 -1251.636 1336.435 e1 | 10,000 .0000975 300.0096 -1626.21 1784.588 e2 | 10,000 .0037573 300.2937 -1446.295 1615.782 e3 | 10,000 .0007922 300.2773 -1873.059 1364.058 . /* (2) Fit OLS With Ordinary and Het-Robust SEs */ . regress y0 x Source | SS df MS Number of obs = 10,000 -------------+---------------------------------- F(1, 9998) = 634.22 Model | 57301350.9 1 57301350.9 Prob > F = 0.0000 Residual | 903307230 9,998 90348.7927 R-squared = 0.0597 -------------+---------------------------------- Adj R-squared = 0.0596 Total | 960608581 9,999 96070.4651 Root MSE = 300.58 ------------------------------------------------------------------------------ y0 | Coefficient Std. err. t P>|t| [95% conf. interval] -------------+---------------------------------------------------------------- x | 4.999955 .1985386 25.18 0.000 4.61078 5.389131 _cons | 10.00496 20.05918 0.50 0.618 -29.31507 49.32499 ------------------------------------------------------------------------------ . regress y0 x, vce(robust) Linear regression Number of obs = 10,000 F(1, 9998) = 639.68 Prob > F = 0.0000 R-squared = 0.0597 Root MSE = 300.58 ------------------------------------------------------------------------------ | Robust y0 | Coefficient std. err. t P>|t| [95% conf. interval] -------------+---------------------------------------------------------------- x | 4.999955 .19769 25.29 0.000 4.612443 5.387467 _cons | 10.00496 20.00716 0.50 0.617 -29.2131 49.22302 ------------------------------------------------------------------------------ . regress y1 x Source | SS df MS Number of obs = 10,000 -------------+---------------------------------- F(1, 9998) = 600.94 Model | 54090825.6 1 54090825.6 Prob > F = 0.0000 Residual | 899921160 9,998 90010.118 R-squared = 0.0567 -------------+---------------------------------- Adj R-squared = 0.0566 Total | 954011985 9,999 95410.7396 Root MSE = 300.02 ------------------------------------------------------------------------------ y1 | Coefficient Std. err. t P>|t| [95% conf. interval] -------------+---------------------------------------------------------------- x | 4.857866 .1981661 24.51 0.000 4.46942 5.246311 _cons | 24.19839 20.02155 1.21 0.227 -15.04787 63.44466 ------------------------------------------------------------------------------ . regress y1 x, vce(robust) Linear regression Number of obs = 10,000 F(1, 9998) = 641.91 Prob > F = 0.0000 R-squared = 0.0567 Root MSE = 300.02 ------------------------------------------------------------------------------ | Robust y1 | Coefficient std. err. t P>|t| [95% conf. interval] -------------+---------------------------------------------------------------- x | 4.857866 .1917376 25.34 0.000 4.482021 5.23371 _cons | 24.19839 19.42984 1.25 0.213 -13.888 62.28479 ------------------------------------------------------------------------------ . regress y2 x Source | SS df MS Number of obs = 10,000 -------------+---------------------------------- F(1, 9998) = 565.06 Model | 50949701 1 50949701 Prob > F = 0.0000 Residual | 901486350 9,998 90166.6683 R-squared = 0.0535 -------------+---------------------------------- Adj R-squared = 0.0534 Total | 952436051 9,999 95253.1304 Root MSE = 300.28 ------------------------------------------------------------------------------ y2 | Coefficient Std. err. t P>|t| [95% conf. interval] -------------+---------------------------------------------------------------- x | 4.714705 .1983384 23.77 0.000 4.325922 5.103488 _cons | 38.50287 20.03895 1.92 0.055 -.7775081 77.78325 ------------------------------------------------------------------------------ . regress y2 x, vce(robust) Linear regression Number of obs = 10,000 F(1, 9998) = 457.00 Prob > F = 0.0000 R-squared = 0.0535 Root MSE = 300.28 ------------------------------------------------------------------------------ | Robust y2 | Coefficient std. err. t P>|t| [95% conf. interval] -------------+---------------------------------------------------------------- x | 4.714705 .2205436 21.38 0.000 4.282395 5.147015 _cons | 38.50287 20.65658 1.86 0.062 -1.988177 78.99392 ------------------------------------------------------------------------------ . regress y3 x Source | SS df MS Number of obs = 10,000 -------------+---------------------------------- F(1, 9998) = 661.37 Model | 59637456.8 1 59637456.8 Prob > F = 0.0000 Residual | 901550824 9,998 90173.117 R-squared = 0.0620 -------------+---------------------------------- Adj R-squared = 0.0620 Total | 961188281 9,999 96128.4409 Root MSE = 300.29 ------------------------------------------------------------------------------ y3 | Coefficient Std. err. t P>|t| [95% conf. interval] -------------+---------------------------------------------------------------- x | 5.100858 .1983454 25.72 0.000 4.712061 5.489655 _cons | -.0742689 20.03967 -0.00 0.997 -39.35605 39.20751 ------------------------------------------------------------------------------ . regress y3 x, vce(robust) Linear regression Number of obs = 10,000 F(1, 9998) = 2024.13 Prob > F = 0.0000 R-squared = 0.0620 Root MSE = 300.29 ------------------------------------------------------------------------------ | Robust y3 | Coefficient std. err. t P>|t| [95% conf. interval] -------------+---------------------------------------------------------------- x | 5.100858 .1133769 44.99 0.000 4.878617 5.3231 _cons | -.0742689 11.75118 -0.01 0.995 -23.10895 22.96041 ------------------------------------------------------------------------------ Here's a graph of the data: y0 is homoskedastic and the ratio of vanilla to robust SE for $x$ is $\frac{.1985386}{.19769} = 1.00$ . y1 is heteroskedastic, but not in a way that depends on $x$ monotonically. Here the error variance is sinusoidal in $x$ . Here the ratio is $\frac{.1981661}{.1917376}=1.03$ . This is still pretty close to 1. y2 is the usual case, where the variance increases with $x$ . Now the ratio is $\frac{.1983384}{.2205436} = .90$ , so the robust errors are 10% larger. y3 is the exotic one, where the error variance is lower at the tails of $x$ . here the ratio is $\frac{.1983454}{.1133769} = 1.74$ , so OLS is a lot bigger. This is all to say if your errors are similar, you cannot distinguish between a situation where your errors are homoskedastic or heteroskedastic. The y0 vs y1 regressions show this. Robust SEs also have small-sample bias and higher sampling variance. Stata implements a correction for the first that works reasonably well in moderate samples, like we have here. But you could still get the robust ones to be too small by chance alone because of the second. This gets more complicated as you add many covariates which may be correlated with each other and have different types of heteroskedasticity. The second lesson here is that it's not helpful to use differences between SEs to diagnose homoskedasticity. There are better tools for that (residual-versus-fitted plot, residual-versus-predictor plot, Breusch–Pagan and Cook–Weisberg tests for heteroskedasticity or various information matrix tests). In practice, some books suggest using whichever SEs are biggest to be conservative (and also using various types of robust errors, HCx). This is advice that performs fairly well in simulation. Stata Code: cls clear /* (1) Generate Fake Data */ set obs 10000 set seed 1052021 generate x = rnormal(100,15) // This looks cryptic, but I just played around with // the transformations until // all three errors were mean zero // and the errors had the same variance generate e0 = rnormal(0,301) - 2.469 generate e1 = rnormal(0, (sin(x)+1)*243) + 0.437 generate e2 = rnormal(0, x^2)/35.75 + 4.81 generate e3 = rnormal(0, 5e3*normalden(x,100,15))*2.95 - 0.31 generate y0 = 10 + 5*x + e0 generate y1 = 10 + 5*x + e1 generate y2 = 10 + 5*x + e2 generate y3 = 10 + 5*x + e3 // Plot the Data twoway (scatter y0 x), name(y0, replace) ytitle(, orientation(horizontal)) twoway (scatter y1 x), name(y1, replace) ytitle(, orientation(horizontal)) twoway (scatter y2 x), name(y2, replace) ytitle(, orientation(horizontal)) twoway (scatter y3 x), name(y3, replace) ytitle(, orientation(horizontal)) graph combine y0 y1 y2 y3, xcommon summarize e* /* (2) Fit OLS With Ordinary and Het-Robust SEs */ regress y0 x regress y0 x, vce(robust) regress y1 x regress y1 x, vce(robust) regress y2 x regress y2 x, vce(robust) regress y3 x regress y3 x, vce(robust)
