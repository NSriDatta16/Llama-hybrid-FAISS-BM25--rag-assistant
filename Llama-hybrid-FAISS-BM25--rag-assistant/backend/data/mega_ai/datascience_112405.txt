[site]: datascience
[post_id]: 112405
[parent_id]: 112402
[tags]: 
There are two valid inputs to MPNet's tokenizer: Union[TextInputSequence, Tuple[InputSequence, InputSequence]] When you give a list of tuples as an input, from each tuple only the first two sentences, i.e. "U.S. stock index futures" and "points to" are used to encode, similar to BERT's next sentence prediction pre-training task. This is tokenized and converted to input_ids with special tokens as follows: [' ', 'u', '.', 's', '.', 'stock', 'index', 'futures', ' ', ' ', 'points', 'to', ' '] indicates start of sentence and indicates end of sentence. I'm not sure about the before 'points', but this is the output from the model's tokenizer. As such NLU models can work with one or two sentences together, these special tokens are added to identify how many sentences are in the model and they help to separate the sentences. Thus, you get two same vectors from model.encode(t) If you want the same vector without using a tuple, the encode function should contain the following input string: "U.S. stock index futures points to"
