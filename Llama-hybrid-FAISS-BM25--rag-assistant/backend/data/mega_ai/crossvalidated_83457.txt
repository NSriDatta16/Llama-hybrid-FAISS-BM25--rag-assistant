[site]: crossvalidated
[post_id]: 83457
[parent_id]: 83449
[tags]: 
This is just a personal opinion, so take it with a grain of salt. There are a few ways to approach model selection. One of the ways is to construct all possible models and select the one that is 'the best' according to some criterion, e.g. AIC. This is model dredging and is frowned upon by some people because it doesn't incorporate any existing knowledge and maximizes the chance to find a significant model by chance alone. This is basically what you're doing. That being said, data dredging is probably fine if you are exploring an unknown area of research where there is very little or no theoretical knowledge. Another way would be to, based on what you know of the phenomenon you're trying to model, construct a few valid hypotheses and then construct a model to test these hypotheses. When you compare these models (for example using AIC), you can say something along the lines of "based on what is already known, out of these hypotheses, this hypothesis (or hypotheses) appears describe my data best". You can also take model parameters of a few best models, average them (weighted average) et voila, you have an "average" model that is a compromise between a few good models. This is called the information-theoretic approach. In biology, a book by Burnham and Anderson (Model selection and multimodel inference) is an advocate of this approach. My memory isn't the best, but I think Gentle introduction to MARK has a chapter on this (with a technical part for this specific program on how to model average).
