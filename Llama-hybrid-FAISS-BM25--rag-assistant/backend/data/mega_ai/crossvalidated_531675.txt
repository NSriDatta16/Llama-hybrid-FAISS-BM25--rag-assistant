[site]: crossvalidated
[post_id]: 531675
[parent_id]: 531663
[tags]: 
You have likely overfit, if you get a training accuracy of 100% on a largish training dataset and clearly lower on the validation set. When I calculate confidence intervals for binomial proportions assuming a 80:20 training validation split, the two are quite clearly separated. Accuracy can be a noisy metric (and loss, or focal-loss could be less noisy alternatives to look at), but the mismatch is just too striking. I'd say that's due to training for too long, when you're potentially doing nothing useful for the validation performance (at least with your specific settings) any more and continue to overfit the training performance. That does not mean training for longer might not be a good idea, just maybe not with the current configuration. How bad this overfitting is in terms of the possibly still useful validation set performance (depends on the application whether that performance is useful, or not), is of course debatable, but you can likely reduce the training-validation gap somewhat with various regularizaiton techniques. Ideas are things like more drop-out, more data augmentation, weight decay, different learning rate/momentum schedule, different loss function such as focal loss etc. (it's a lot less clear whether something like stochastic weight averaging would help). Tuning the regularization and other settings optimally using cross-validation on the training data is the way to go. The things others recommended such as e.g. lowering the learning rate based on some criteria (some alternatives could be schedules like flat-cosine or the one-cycle policy that have a declining learning rate schedule towards the end of training) also sound like a good idea. Another comment: whether 100 or 800 epochs is a lot or not depends on your setting. It seems like a huge number, if you are doing transfer learning and using a pre-trained model from ImageNet (on large Kaggle competitions I'm more used to using single digit or low double-digit epochs in that situation when using modern learning rate schedules). If you are training a neural network from scratch on ImageNet, this would be a really low number to my mind, on ther other hand for MNIST it seems like a huge number. So, what's a low number of epochs really depends.
