[site]: crossvalidated
[post_id]: 418646
[parent_id]: 
[tags]: 
What are appropriate methods for preparing categorical features for recurrent networks to ensure efficient backpropagation?

Given a 1D sequential categorical input variable, e.g. [rainy, sunny, rainy, cloudy, cloudy], with a small domain {rain, sunny, cloudy}, what encoding methods (e.g. one-hot, dummy, binary) and what scaling methods (e.g. standardisation, min-max scaling) are appropriate specifically for use with RNNs such as LSTM and GRU given their logistic activation functions in comparison to other NNs which tend to use ReLU? In Yann A. LeCun et al - Efficient BackProp Chapter in Neural Networks: Tricks of the Trade: Second Edition, pages 9â€“48, LeCunn states that preprocessing of data should be performed in such a way so as to ensure that the activations of the neurons have unit variance and mean zero (which is of course not possible for sigmoid activations) to ensure backprop is efficient, and presumably to ensure that signals to later layers lie approximately in the region [-1, 1] where in the gradient of the logistic functions is greatest. I believe this is of particular significance to LSTM/GRU RNNs since if one uses one-hot encoding (or some other binary style encoding) the distribution of the responses will not be distributed with unit variance (sigmoid and tanh) as per LeCun's directions. addendum - This has nothing to do with the convolution operation (as per the tagged duplicate) in the context of a categorical variable. This is a question about what are the appropriate encoding and scaling methods for GRU/LSTM RNNs which are likely specific to these types of networks due to the logistic type activation functions they use.
