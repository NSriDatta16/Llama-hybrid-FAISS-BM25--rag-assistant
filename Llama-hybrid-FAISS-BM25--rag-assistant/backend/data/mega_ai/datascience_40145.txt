[site]: datascience
[post_id]: 40145
[parent_id]: 40120
[tags]: 
Welcome to the community Sai! Let's assume your problem is a regression problem (i.e. you have continues target). There are some points: First of all there is no written rule for this. Feature Engineering is an EDA kind of thing. There is no final solution. Your model selection strategy will choose some. Just as a reminder, if these are Pearson correlations, be careful that nonlinear correlations might exist which is not captured by linear correlation analysis. Plus the fact that linear correlation analysis always comes with visual inspection . Negative correlations inhibits information. If whenever a variable increases the other decreases, then knowing it tells you about the other one! So take them into account. You better use Mutual Information for checking dependencies. Sparse Linear Models seem to be fruitful here. I suggest letting LASSO or Ridge Regression choose the final set of features. In case you really insist on your current way (e.g. if your supervisor asked for it), use the threshold as the hyperparameter of your model selection and find the optimal. It means, you train and validate your model using different threshold (which results in different sets of features) and according to empirical error (validation error) you choose the best threshold. Hope it helped. Good Luck!
