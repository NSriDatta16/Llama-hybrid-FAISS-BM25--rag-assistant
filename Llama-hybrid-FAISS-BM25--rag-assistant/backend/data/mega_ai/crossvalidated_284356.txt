[site]: crossvalidated
[post_id]: 284356
[parent_id]: 283878
[tags]: 
There is a paper which concretely studies this question in detail ( Statistical Comparisons of Classifiers over Multiple Data Sets ) with pretty sobering conclusions. It is actually very tricky. As you note, your methods are not deterministic and yield a different result each time. That means that in some cases, A might be better than B by pure chance, but if you run the test several times, in average, B might be better than A. Regardless of the random behaviour of your methods, by testing two methods over different databases, sometimes A will be better than B and viceversa by chance. There is no universally better algorithm. Another way to look at it is: it makes no sense to compare tests obtained with the same methods on different data sets if they are not commensurate. The basic takeaway message of the paper is that, if you cannot guarantee that the assumptions made by parametric tests are fulfilled (be it ANOVA or t-test), then it is better to make use of non-parametric tests (Wilcoxon test of Friedman's test). And that seems to be indeed your case. See also these slides , specially #34, for a very nice summary.
