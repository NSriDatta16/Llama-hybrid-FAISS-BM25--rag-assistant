[site]: datascience
[post_id]: 87836
[parent_id]: 
[tags]: 
How to properly do feature selection when comparing different models?

Context: I'm currently crafting and comparing machine learning models to predict housing data. I have around 32000 data points, 42 features, and I'm predicting housing price. I'm comparing Random Forest Regressor, Decision Tree Regressor, and Linear Regression. I can tell there is some overfitting going on, as my initial values vs cross validated values are as follows: RF: 10 Fold R Squared = 0.758, neg RMSE = -540.2 vs unvalidated R Squared of 0.877, RMSE of 505.6 DT: 10 Fold R Squared = 0.711, neg RMSE = -576.4 vs unvalidated R squared of 0.829 and RMSE of 595.8. LR: 10 Fold R squared = 0.695, neg RMSE = -596.5 vs unvalidated R squared of 0.823 and RMSE of 603.7 I have already tuned the hyperparameters for RF and DT, so I was thinking about doing feature selection as a next step to cut down on some of this overfitting (especially since I know my feature importances/coefs). On to my questions: Is feature selection independent of any machine learning algorithms?->I.E., should I be choosing a strict subset of features to apply to all of my machine learning models to keep everything consistent as I'm comparing model performances? Or should I be doing feature selection uniquely to each model(choosing a unique subset of features for each model)? What is generally considered to be too many features? And is using lasso feature selection acceptable in this case (as I have continuous variables)? And as a follow up question, is it okay to be doing feature selection after tuning my hyperparameters? And would I need to tune them once again after selecting my features? Finally I noticed that my validated RMSE for DT and LR are lower than their unvalidated RMSE. I think this is inconclusive, but does anyone have any thoughts?
