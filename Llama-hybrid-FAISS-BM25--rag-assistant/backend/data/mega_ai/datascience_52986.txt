[site]: datascience
[post_id]: 52986
[parent_id]: 
[tags]: 
How to deal with millions or rows of data for analysis/visualization purpose

I have data in 2 tables in Sql server. First table has around 10 million rows and 8 columns. Second table has 6 million rows and 60 columns. I want to import those tables into a Python notebook using pandas ( I am importing in "chunksize") and then merge them and then run analysis on resulting table. I am unable to import the data due to possible hardware constraints. System hardware configuration is as follows: RAM: 4GB Storage: 160 GB, CPU : Dual core CPU. Even if the import goes through (which seems difficult), my resulting table after the merge will have 5 million rows and 40 columns. Is it feasible to perform analysis/visualization in python notebook using pandas, seaborn on columns on the resulting table? Would love to understand: What should be my next steps to solve the problem? Is installation of Anaconda on Windows 2012 server feasible? (This server has more memory)
