[site]: crossvalidated
[post_id]: 637390
[parent_id]: 637387
[tags]: 
As has been thoroughly discussed on this site, Fisher’s “exact” test is not very accurate, the ordinary Pearson $\chi^2$ is more accurate, and you can save a lot of computing time by using it. A more important challenge underlying your question, no matter which associate measure you use to describe the results, is that it’s too easy to commit an “absence of evidence is not evidence of absence” error, i.e., in concluding that $p > 0.05$ means no association. A simple Bayesian analysis would expose the problem. Take a reasonable skeptical prior distribution and analyze the candidate features one-at-a-time. For each one compute the posterior probability that the true unknown odds ratio exceeds 1.2. You’ll see a minority of features for which you can learn something, i.e, that the posterior probabilities are > 0.95 of Look here for simple simulations showing that below a certain sample size there is no correlation between the true odds ratios and estimated odds ratios. Scary, but shows that you need high sample sizes to learn in high dimensions when the learning is not biologically driven. Examples in the link also show how to use the bootstrap to expose the difficulty of the task through confidence intervals on importance measures or their ranks.
