[site]: datascience
[post_id]: 22093
[parent_id]: 
[tags]: 
Why should the initialization of weights and bias be chosen around 0?

I read this: To train our neural network, we will initialize each parameter W(l)ijWij(l) and each b(l)ibi(l) to a small random value near zero (say according to a Normal(0,系2)Normal(0,系2) distribution for some small 系系, say 0.01) from Stanford Deep learning tutorials at the 7th paragraph in the Backpropagation Algorithm What I don't understand is why the initialization of the weight or bias should be around 0 ?
