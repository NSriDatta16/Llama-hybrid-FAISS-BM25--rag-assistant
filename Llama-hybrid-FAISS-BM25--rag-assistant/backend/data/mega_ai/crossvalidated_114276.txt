[site]: crossvalidated
[post_id]: 114276
[parent_id]: 8718
[tags]: 
Guilherme is on the money here. While the other responses are useful, please note that logistic regression (and all nonlinear regression like Poisson, for that matter) are fundamentally different than linear regression. There may be serious issues with the logit scaling factor when running the same analysis on six different data sets and then running that analysis on the combined data set. Changes in coefficients may have nothing to do with meaningful differences (even if statistically significant or substantively important). They could have everything to do with unobserved heterogeneity across the samples. You absolutely have to test for that. Many (if not most) researchers in social and policy science fields ignore this. Guilherme gives the seminal articles on this that I recommend everyone look at. Peters suggestions are practical, but simply coding a dummy variable for the sample the data come from will not address this heterogeneity in the scaling factor. You can do that in linear regression and the heterogeneity shouldn't affect your coefficients, but here it may. One other aspect to the effect of unobserved heterogeneity unique to logit vs. linear regression, is the effect of different regressors in each data set. If you don't have the same variables, or likely if they are measured differently, you have a form of omitted variable bias. Unlike with linear regression, an omitted variable orthogonal to your key regressor can still bias your estimate. As Cramer puts it: Even with orthogonal regressors, then, omitted variables depress $\hat β$ towards zero, relatively to its value in the full equation. In other words, the $\hat β$ of discrete models vary inversely with the extent of unobserved heterogeneity. The practical consequence is that estimates from samples that differ in this respect are not directly comparable. ( http://dare.uva.nl/document/2/96199 ) Cramer also points out though the coefficient estimates are biased downward when omitting a variable, the partial derivatives are not. This is fairly complicated and you should read the article for a more lucid explanation – the overall point is, don't exclusively look at the log-odds or odds ratios. Consider predicted probabilities and derivatives; see the margins command in Stata for more details. JD Long has a paper that goes into detail here. Finally, there are a number of papers you can Google for that discuss interaction terms in logit models. My understanding has been that take the logit coefficient on an interaction as a guide but not definitive, especially if you prefer to see the coefficients as exponentiated odds ratios. Looking at predicted probabilities and average marginal effect is better (again, look up documentation on Stata's margin command for logit, even if you use SPSS this will still be helpful). I'm not familiar enough with SPSS to know how that package can deal with these issues, but I will say this: when you get into deeper statistical issues like this, it is an indication that it is time for you to move to a more flexible, sophisticated package like Stata or R.
