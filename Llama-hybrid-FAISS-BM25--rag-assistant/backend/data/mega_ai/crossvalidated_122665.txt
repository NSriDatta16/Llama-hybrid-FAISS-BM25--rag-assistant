[site]: crossvalidated
[post_id]: 122665
[parent_id]: 89121
[tags]: 
The algebra is simpler than it might first appear. IMHO, there is little profit or insight achieved by belaboring the algebraic manipulations. Instead, a truly simple identity shows why squared differences can be used to express (the usual Pearson) correlation coefficient. Applying this to the special case where the data are ranks produces the result. It exhibits the heretofore mysterious coefficient $$\frac{6}{n(n^2-1)}$$ as being half the reciprocal of the variance of the ranks $1, 2, \ldots, n$. (When ties are present, this coefficient acquires a more complicated formula, but will still be one-half the reciprocal of the variance of the ranks assigned to the data.) Once you have seen and understood this, the formula becomes memorable. Comparable (but more complex) formulas that handle ties, show up in nonparametric statistical tests like the Wilcoxon rank sum test, or appear in spatial statistics (like Moran's I, Geary's C, and others) become instantly understandable. Consider any set of paired data $(X_i,Y_i)$ with means $\bar X$ and $\bar Y$ and variances $s_X^2$ and $s_Y^2$. By recentering the variables at their means $\bar X$ and $\bar Y$ and using their standard deviations $s_X$ and $s_Y$ as units of measurement, the data will be re-expressed in terms of the standardized values $$(x_i, y_i) = \left(\frac{X_i-\bar X}{s_X}, \frac{Y_i-\bar Y}{s_Y}\right).$$ By definition , the Pearson correlation coefficient of the original data is the average product of the standardized values, $$\rho = \frac{1}{n}\sum_{i=1}^n x_i y_i.$$ The Polarization Identity relates products to squares. For two numbers $x$ and $y$ it asserts $$xy = \frac{1}{2}\left(x^2 + y^2 - (x-y)^2\right),$$ which is easily verified. Applying this to each term in the sum gives $$\rho = \frac{1}{n}\sum_{i=1}^n \frac{1}{2}\left(x_i^2 + y_i^2 - (x_i-y_i)^2\right).$$ Because the $x_i$ and $y_i$ have been standardized, their average squares are both unity, whence $$\rho = \frac{1}{2}\left(1 + 1 - \frac{1}{n}\sum_{i=1}^n (x_i-y_i)^2\right) = 1 - \frac{1}{2}\left(\frac{1}{n}\sum_{i=1}^n (x_i-y_i)^2\right).\tag{1}$$ The correlation coefficient differs from its maximum possible value, $1$, by one-half the mean squared difference of the standardized data. This is a universal formula for correlation, valid no matter what the original data were (provided only that both variables have nonzero standard deviations). (Faithful readers of this site will recognize this as being closely related to the geometric characterization of covariance described and illustrated at How would you explain covariance to someone who understands only the mean? .) In the special case where the $X_i$ and $Y_i$ are distinct ranks , each is a permutation of the same sequence of numbers $1,2 , \ldots, n$. Thus $\bar X = \bar Y = (n+1)/2$ and, with a tiny bit of calculation we find $$s_X^2 = s_Y^2 = \frac{1}{n} \sum_{i=1}^n (i - (n+1)/2)^2 = \frac{n^2 - 1}{12}$$ (which, happily, is nonzero whenever $n\gt 1$). Therefore $$(x_i - y_i)^2 = \frac{\left((X_i - (n+1)/2)- (Y_i - (n+1)/2)\right)^2}{(n^2-1)/12} = \frac{12(X_i-Y_i)^2}{n^2-1}.$$ This nice simplification occurred because the $X_i$ and $Y_i$ have the same means and standard deviations: the difference of their means therefore disappeared and the product $s_X s_Y$ became $s_X^2$ which involves no square roots . Plugging this into the formula $(1)$ for $\rho$ gives $$\rho = 1 - \frac{6}{n(n^2-1)}\sum_{i=1}^n (X_i - Y_i)^2.$$
