[site]: crossvalidated
[post_id]: 428039
[parent_id]: 428035
[tags]: 
In situations where the cardinality of a categorical feature is high (looks like your case) a popular approach (besides one-hot encoding which you are trying to avoid) is to compute target statistics (target mean encoding). What one does is replace the category value (eg. a string) with a conditional expectation which is computed as such: $$\mathbb{E}[y|x^i = x^i_k]$$ where $x^i$ is your categorical feature and $x^i_k$ is the $k$ th unique value that feature holds. This works particularly well for tree-based methods eg. boosting, random forest, etc. However, computing these target mean statistics on your whole training set could lead to overfitting so there are lots of variations of it. See the python package categorical-encoding and also the gradient boosting library CatBoost which can perform a very clever and robust version of this scheme. All being said, you should still try some models with one-hot encoding. Please note that this is commonly done for classification tasks, but I have not seen it come up often in regression.
