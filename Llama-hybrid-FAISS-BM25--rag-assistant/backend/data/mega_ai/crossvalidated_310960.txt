[site]: crossvalidated
[post_id]: 310960
[parent_id]: 310953
[tags]: 
You would do the cross validation to compare performance of a few algorithms on the same folds of the same data-set. Then you take the best one* and retrain it on the entire data-set. So your second option. *The best one is typically the one with the highest average performance, but you often have some discretion in choosing. If there aren't statistically significant differences between some of the algorithms, you can also take the least computationally complex one, the most intuitive one or the one with the least parameters to tune. Note also that this procedures only works if all the algorithms you compare either have few or no parameters to tune or, if they have parameters, provide good default values from the literature. If you have to tune model parameters, you need to do a nested cross validation which makes it less straightforward to have a final model to apply to unseen data.
