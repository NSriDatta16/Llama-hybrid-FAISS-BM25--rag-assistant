[site]: crossvalidated
[post_id]: 340765
[parent_id]: 340762
[tags]: 
In statistics, all is uncertain, also the class membership of each pattern, as assigned by a (naive bayes) classifier. Probabilities that are purely 0 and 1, that indicates a deterministic mechanism, for which statistics was not developed. As soon as uncertainty comes into play, determinism is replaced by statistics and probability theory. In general, 0/1 probabilities occur in limited-size datasets, or when the number of possible events becomes very large, compared to the number of variables. When for example using the present-day level of a stock-index as predictive variable $Si$, discretizing this real-number variable into 1000 bins is likely to yield some bins of which the pertaining stock-index value has never occcurred. Hence, a zero probability (It is not recommended to discretize a continuously distributed variable into a large number of bins, but this example serves the explanation given here). Smoothing of discrete probability tables with 0-probabilities has been approached by Bayesian methods. The Dirichlet distribution mixes a set of observations with an expected, prior distribution. This prior distribution is also called a regularization prior . It recommended to review the theory of the Dirichlet distribution. It makes possible a seamless mixing of the observations in a dataset with an (expected) prior distribution over the outcomes. Within the Bayesian paradigm, this prior distribution is given by expert knowledge by someone who is into the domain for which the classifier is being built. Bayesian probabilities are also called subjective probabilities . Using a frequentist approach, it is also possible to use a uniform prior distribution as regularization prior . Basically you can add $n$ counts to each probability outcome, before computing the probability distribution over the bins. If $n=1$, you add solely one observation to each outcome - and the unifom prior has the minimal influence on the outcome of your Naive Bayes classifier.
