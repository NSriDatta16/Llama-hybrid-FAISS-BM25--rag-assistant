[site]: crossvalidated
[post_id]: 142128
[parent_id]: 141929
[tags]: 
L2 is "easy." It's what you get by default if you do standard matrix methods like linear regression, SVD, etc. Until we had computers, L2 was the only game in town for a lot of problems, which is why everyone uses ANOVA, t-tests, etc. It's also easier to get an exact answer using L2 loss with many fancier methods like Gaussian processes than it is to get an exact answer using other loss functions. Relatedly, you can get the L2 loss exactly using a 2nd-order Taylor approximation, which isn't the case for most loss functions (e.g. cross-entropy, ). This makes optimization easy with 2nd-order methods like Newton's method. Lots of methods for dealing with other loss functions still use methods for L2 loss under-the-hood for the same reason (e.g. iteratively reweighted least squares, integrated nested Laplace approximations). L2 is closely related to Gaussian distributions, and the Central Limit Theorem makes Gaussian distributions common. If your data-generating process is (conditionally) Gaussian, then L2 is the most efficient estimator. L2 loss decomposes nicely, because of the law of total variance. That makes certain graphical models with latent variables especially easy to fit. L2 penalizes terrible predictions disproportionately. This can be good or bad, but it's often pretty reasonable. An hour-long wait might be four times as bad as a 30-minute wait, on average, if it causes lots of people to miss their appointments.
