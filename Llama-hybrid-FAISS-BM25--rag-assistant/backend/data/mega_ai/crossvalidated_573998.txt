[site]: crossvalidated
[post_id]: 573998
[parent_id]: 
[tags]: 
Are machine learning algorithms flexible enough to learn changing feature importances?

I have a prediction problem where each row/entity contains data over a range of time and features can change in importance over time even for a single entity. I am wondering if machine learning models are flexible in learning feature importances with respect to time. Example: Let's say we want to predict whether or not a home will sell. The home is the entity and after the home is put on the market, we receive observations of the number of bids and the value of bids. Intuitively, when the home is first on the market, there are no bids and there are many features (such as location, size, asking price for sq footage etc) that are important for prediction. As time passes, the number of bids and the associated prices are important in determining the probability a house will sell (perhaps even more important than the location/size etc). The goal is to build a model so that at any point in time after the home is put on the market, we can make a prediction to if the house will sell or not. Currently, my strategy is to truncate the data at random points in time: for each home, I have $n$ samples, representing the $n$ different time points after the home is put on the market. I then train a tree-based machine learning on this new dataset. But this leads to the conceptual question of are machine learning (boosted tree-based mostly) models flexible enough to capture that a feature maybe be increasingly/decreasingly important as time passes? And, what does it mean if after analyzing my model using SHAP that I find that my model does not see a relationship between feature importance and time? Also very open to other suggestions and techniques.
