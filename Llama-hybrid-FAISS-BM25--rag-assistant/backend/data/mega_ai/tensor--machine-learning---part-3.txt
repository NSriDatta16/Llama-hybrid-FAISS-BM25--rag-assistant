lf a vector, subject-object-verb semantics could be expressed using mode-3 tensors v a × v b × v c ↦ A ∈ R N × N × N . {\displaystyle v_{a}\times v_{b}\times v_{c}\mapsto {\mathcal {A}}\in \mathbb {R} ^{N\times N\times N}.} In practice the neural network designer is primarily concerned with the specification of embeddings, the connection of tensor layers, and the operations performed on them in a network. Modern machine learning frameworks manage the optimization, tensor factorization and backpropagation automatically. As unit values Tensors may be used as the unit values of neural networks which extend the concept of scalar, vector and matrix values to multiple dimensions. The output value of single layer unit y m {\displaystyle y_{m}} is the sum-product of its input units and the connection weights filtered through the activation function f {\displaystyle f} : y m = f ( ∑ n x n u m , n ) , {\displaystyle y_{m}=f\left(\sum _{n}x_{n}u_{m,n}\right),} where y m ∈ R . {\displaystyle y_{m}\in \mathbb {R} .} If each output element of y m {\displaystyle y_{m}} is a scalar, then we have the classical definition of an artificial neural network. By replacing each unit component with a tensor, the network is able to express higher dimensional data such as images or videos: y m ∈ R I 0 × I 1 × . . × I C . {\displaystyle y_{m}\in \mathbb {R} ^{I_{0}\times I_{1}\times ..\times I_{C}}.} This use of tensors to replace unit values is common in convolutional neural networks where each unit might be an image processed through multiple layers. By embedding the data in tensors such network structures enable learning of complex data types. In fully connected layers Tensors may also be used to compute the layers of a fully connected neural network, where the tensor is applied to the entire layer instead of individual unit values. The output value of single layer unit y m {\displaystyle y_{m}} is the sum-product of its input units and the connection weights filtered through the activation function f {\displaystyle f} : y m = f ( ∑ n x n u m , n ) . {\displaystyle y_{m}=f\left(\sum _{n}x_{n}u_{m,n}\right).} The vectors x {\displaystyle x} and y {\displaystyle y} of output values can be expressed as a mode-1 tensors, while the hidden weights can be expressed as a mode-2 tensor. In this example the unit values are scalars while the tensor takes on the dimensions of the network layers: x n ↦ X ∈ R 1 × N , {\displaystyle x_{n}\mapsto {\mathcal {X}}\in \mathbb {R} ^{1\times N},} y n ↦ Y ∈ R M × 1 , {\displaystyle y_{n}\mapsto {\mathcal {Y}}\in \mathbb {R} ^{M\times 1},} u n ↦ U ∈ R N × M . {\displaystyle u_{n}\mapsto {\mathcal {U}}\in \mathbb {R} ^{N\times M}.} In this notation, the output values can be computed as a tensor product of the input and weight tensors: Y = f ( X U ) . {\displaystyle {\mathcal {Y}}=f({\mathcal {X}}{\mathcal {U}}).} which computes the sum-product as a tensor multiplication (similar to matrix multiplication). This formulation of tensors enables the entire layer of a fully connected network to be efficiently computed by mapping the units and weights to tensors. In convolutional layers A different reformulation of neural networks allows tensors to express the convolution layers of a neural network. A convolutional layer has multiple inputs, each of which is a spatial structure such as an image or volume. The inputs are convolved by filtering before being passed to the next layer. A typical use is to perform feature detection or isolation in image recognition. Convolution is often computed as the multiplication of an input signal g {\displaystyle g} with a filter kernel f {\displaystyle f} . In two dimensions the discrete, finite form is: ( f ∗ g ) x , y = ∑ j = − w w ∑ k = − w w f j , k g x + j , y + k , {\displaystyle (f*g)_{x,y}=\sum _{j=-w}^{w}\sum _{k=-w}^{w}f_{j,k}g_{x+j,y+k},} where w {\displaystyle w} is the width of the kernel. This definition can be rephrased as a matrix-vector product in terms of tensors that express the 