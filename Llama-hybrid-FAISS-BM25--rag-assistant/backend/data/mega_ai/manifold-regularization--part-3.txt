 the curse of dimensionality. Luckily, it is possible to leverage expected smoothness of the function to estimate thanks to more advanced functional analysis. This method consists of estimating the Laplacian operator using derivatives of the kernel reading ∂ 1 , j K ( x i , x ) {\displaystyle \partial _{1,j}K(x_{i},x)} where ∂ 1 , j {\displaystyle \partial _{1,j}} denotes the partial derivatives according to the j-th coordinate of the first variable. This second approach to the Laplacian norm is to put in relation with meshfree methods, that contrast with the finite difference method in PDE. Applications Manifold regularization can extend a variety of algorithms that can be expressed using Tikhonov regularization, by choosing an appropriate loss function V {\displaystyle V} and hypothesis space H {\displaystyle {\mathcal {H}}} . Two commonly used examples are the families of support vector machines and regularized least squares algorithms. (Regularized least squares includes the ridge regression algorithm; the related algorithms of LASSO and elastic net regularization can be expressed as support vector machines.) The extended versions of these algorithms are called Laplacian Regularized Least Squares (abbreviated LapRLS) and Laplacian Support Vector Machines (LapSVM), respectively. Laplacian Regularized Least Squares (LapRLS) Regularized least squares (RLS) is a family of regression algorithms: algorithms that predict a value y = f ( x ) {\displaystyle y=f(x)} for its inputs x {\displaystyle x} , with the goal that the predicted values should be close to the true labels for the data. In particular, RLS is designed to minimize the mean squared error between the predicted values and the true labels, subject to regularization. Ridge regression is one form of RLS; in general, RLS is the same as ridge regression combined with the kernel method. The problem statement for RLS results from choosing the loss function V {\displaystyle V} in Tikhonov regularization to be the mean squared error: f ∗ = arg min f ∈ H 1 ℓ ∑ i = 1 ℓ ( f ( x i ) − y i ) 2 + γ ‖ f ‖ K 2 {\displaystyle f^{*}={\underset {f\in {\mathcal {H}}}{\arg \!\min }}{\frac {1}{\ell }}\sum _{i=1}^{\ell }(f(x_{i})-y_{i})^{2}+\gamma \left\|f\right\|_{K}^{2}} Thanks to the representer theorem, the solution can be written as a weighted sum of the kernel evaluated at the data points: f ∗ ( x ) = ∑ i = 1 ℓ α i ∗ K ( x i , x ) {\displaystyle f^{*}(x)=\sum _{i=1}^{\ell }\alpha _{i}^{*}K(x_{i},x)} and solving for α ∗ {\displaystyle \alpha ^{*}} gives: α ∗ = ( K + γ ℓ I ) − 1 Y {\displaystyle \alpha ^{*}=(K+\gamma \ell I)^{-1}Y} where K {\displaystyle K} is defined to be the kernel matrix, with K i j = K ( x i , x j ) {\displaystyle K_{ij}=K(x_{i},x_{j})} , and Y {\displaystyle Y} is the vector of data labels. Adding a Laplacian term for manifold regularization gives the Laplacian RLS statement: f ∗ = arg min f ∈ H 1 ℓ ∑ i = 1 ℓ ( f ( x i ) − y i ) 2 + γ A ‖ f ‖ K 2 + γ I ( ℓ + u ) 2 f T L f {\displaystyle f^{*}={\underset {f\in {\mathcal {H}}}{\arg \!\min }}{\frac {1}{\ell }}\sum _{i=1}^{\ell }(f(x_{i})-y_{i})^{2}+\gamma _{A}\left\|f\right\|_{K}^{2}+{\frac {\gamma _{I}}{(\ell +u)^{2}}}\mathbf {f} ^{\mathrm {T} }L\mathbf {f} } The representer theorem for manifold regularization again gives f ∗ ( x ) = ∑ i = 1 ℓ + u α i ∗ K ( x i , x ) {\displaystyle f^{*}(x)=\sum _{i=1}^{\ell +u}\alpha _{i}^{*}K(x_{i},x)} and this yields an expression for the vector α ∗ {\displaystyle \alpha ^{*}} . Letting K {\displaystyle K} be the kernel matrix as above, Y {\displaystyle Y} be the vector of data labels, and J {\displaystyle J} be the ( ℓ + u ) × ( ℓ + u ) {\displaystyle (\ell +u)\times (\ell +u)} block matrix [ I ℓ 0 0 0 u ] {\displaystyle {\begin{bmatrix}I_{\ell }&0\\0&0_{u}\end{bmatrix}}} : α ∗ = arg min α ∈ R ℓ + u 1 ℓ ( Y − J K α ) T ( Y − J K α ) + γ A α T K α + γ I ( ℓ + u ) 2 α T K L K α {\displaystyle \alpha ^{*}={\underset {\alpha \in \mathbf {R} ^{\ell +u}}{\arg \!\min }}{\frac {1}{\ell }