[site]: crossvalidated
[post_id]: 310618
[parent_id]: 
[tags]: 
Understanding KS Statistic as Model Selection Tool

As a hobbyist learning about predictive modeling and machine learning, I am having some difficulty finding clarity regarding the KS statistic as a method for model selection. My mentor has been steadfast in selecting the model with the largest KS value (So long as the KS between Training and Validation data sets do not drop significantly) for determining our final model. The underlying assumption I believe is that if there is a large drop in KS then the model has over-fit. I have begun to investigate using an additional Test data set for trying the final model, but i have instances of my previously 'overfitting' models now performing better on Test than Validation (For instance: KS of .40, .32, .37 on Train/Val/Test). What is more frustrating (And intriguing) is that I have tried many, many candidate models that seem to hit a ceiling KS on validation of the aforementioned .32, but now i see on Test these models perform stronger. Is there an explanation for what is happening here? Ultimately, is KS (And how I am using it) the appropriate measure for model performance? I appreciate any assistance you can give, or literature you can point me to, so that i can continue learning about this fascinating area.
