[site]: datascience
[post_id]: 27999
[parent_id]: 27939
[tags]: 
Based on how much i got to know i propose the correct scheme would be third one i.e. Take the first batch of data as first S (Number of time steps) (S1, S2, .., Ss) words from each article (for the sake of simplicity let us assume batch size = m) Set the initial hidden state H0 = [0,0,..,0] Calculate loss and gradient on this batch and update the parameters We move s words forward to next non overlapping s words in each article and initialize H0 to Hs from last iteration Do this to the end Why we wont take any but Scheme 1 and Scheme 3 is because: H0 should only be initialized with a vector of zero when there is no context available (if you have some additional information, why not use it?!). The objective is to maximize the probability of each article and the sentences are not independent. Also, there is no reason to not treat period "." as a step in itself. We do calculate its word embedding too Why not Scheme 1 - Because of catastrophic interference ( As the network in Scheme 1 fed in same input again and again ) combined with slow learning it would result in
