[site]: crossvalidated
[post_id]: 437798
[parent_id]: 104713
[tags]: 
I'm aware this question is old but I landed here from Google anyway and the accepted answer isn't very pleasing as no one needs to programming CV themselves as this is handled by according libraries. For a good answer first the scope terms must be defined. My answer focuses on machine learning ("classical" as in regression, random forest, etc... and not deep learning). The hold-out set or test set is part of the labeled data set, that is split of at the beginning of the model building process. (And the best way to split in my opinion is by acquisition date of the data with newest data being the hold-out set because that exactly mimics future use of the model) A crucial aspect to consider that your model isn't just the used algorithm and parameters but the whole process you use to build it from feature selection to parameter optimization. That is why the hold-out set gets split off at the start so that in above definition the model has never seen that data in any way. k-fold cross-validation is used within your model (reminder: model = your whole pipeline) for example within parameter optimization or feature selection. You need to use CV here because else you optimize your model for 1 specific data split instead of a more general optimization you get with CV. At the end of this pipeline you can also do another CV with the final model settings for an approximate guess of the models performance but be aware that this will almost always be better than the truth because the model during the model building process has already seen the data. It still gives you a rough estimate and especially a hint on the variance. After you have your model you apply it to this hold-out set which if done correctly is 100% new to the model. This should give you a correct indication about your models performance and this as said above will almost always be worse than what you get with CV.
