[site]: datascience
[post_id]: 118114
[parent_id]: 
[tags]: 
GAN Generator Backpropagation Gradient Shape Doesn't Match

In the TensorFlow example ( https://www.tensorflow.org/tutorials/generative/dcgan#the_discriminator ) the discriminator has a single output neuron (assume batch_size=1). Then over in the training loop the generator's BinaryCrossentropy loss is calculated using the discriminator's output which has the shape [1]. It then calculates the loss gradients by plugging in the prediction and label into the d BinaryCrossentropy derivative whose resultant shape is also [1]. How is this [1] shaped gradient fed backward into the generator's layers when its shape doesn't match and the Conv2DTranspose layer expects gradients whose shape matches its output? $\frac{dL}{dZ} = \frac{dL}{dA}*\frac{dA}{dZ}$ Conv2DTranspose output shape, can't do hadamard product. How does backpropagation still work?
