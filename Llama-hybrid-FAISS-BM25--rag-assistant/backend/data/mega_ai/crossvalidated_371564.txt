[site]: crossvalidated
[post_id]: 371564
[parent_id]: 
[tags]: 
Intriguing properties of neural networks

In a paper by Ian GoodFellow 1 , on page 3, what is meant by: Our experiments show that any random direction $v ∈ \mathbb R^n$ gives rise to similarly interpretable semantic properties. More formally, we find that images $x′$ are semantically related to each other, for many $x′$ such that $$x′ =\underset{x∈I}{\arg\max}\big\langleφ(x),v\big\rangle$$ This suggests that the natural basis is not better than a random basis for inspecting the properties of $φ(x)$ . This puts into question the notion that neural networks disentangle variation factors across coordinates. In particular, what is meant by 'random basis' and 'direction' in this context? Are they finding what activates different orientations of the same feature? Any comments on the equation would help as well. 1. Szegedy, C., et al. (2014). Intriguing properties of neural networks ( pds )
