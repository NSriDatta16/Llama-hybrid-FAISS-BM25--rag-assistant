[site]: crossvalidated
[post_id]: 346818
[parent_id]: 
[tags]: 
A kernel $K_1$ maps from $R^n$ to $R^m (m>n)$ and is linearly separable. Given a different kernel $K=K_1+K_2$ will we still find a linear classifier?

Just started learning about SVM, kernels and all those things. Got stuck on this question (homework question): Consider a kernel $K_1$ and its corresponding mapping $φ_1$ that maps from the lower space $R^n$ to a higher space $R^m (m>n)$. We know that the data in the higher space $R^m$ is separable by a linear classifier with the weights vector $w$. Given a different kernel $K_2$ and its corresponding mapping $φ_2$, we create a kernel $K = K_1 + K_2$. Can you find a linear classifier in the higher space to which $φ$, the mapping corresponding to the kernel $K$, is mapping? If yes, find the linear classifier weight vector. If no, prove why not. My intuition says no, because $K_2$ may mess my data up. But it makes more sense that it is true, because if the data is already linearly separable in $R^m$, when we will perform $K$ we will arrive at space $p$ that is $p>m$, and if the data is linearly separable in $R^m$, so probably in $R^p$ it will also be linearly separable. I really don't know how to start proving it. $K$ will probably be $\phi_1(x)\phi_1(y) + \phi_2(x)\phi_2(y)$. Still don't know how to proceed from here. I'm trying somehow to reset all values from $\phi_2$, because I know with $\phi_1$ my data is already linearly separable.
