[site]: crossvalidated
[post_id]: 405166
[parent_id]: 
[tags]: 
Where to learn the theory behind common statistical techniques

I'm a college student and pursuing (in part, at least) a statistics and data science track. Much of my coursework beyond the introductory statistics sequence has involved topics like multiple regression, logistic regression (and probit and tobit), discriminant analysis, clustering techniques, etc. However, the issue with these courses is that they tend to introduce the motivation behind each topic and then move straight into implementation and interpretation. The last time I actually understood the mathematics behind a methodology was linear least squares, and I want to be able to derive the equations for the coefficients for logistic regression or for discriminant analysis. Where can I learn to do this? Is there a good statistics manual or "encyclopedia" of sorts that collects reference material on all sorts of statistical methods? Ideally, I'd also envision some source that I can look to years down the road, when I'm rusty and need a refresher on how a certain estimator is derived or can quickly look at what the assumptions for a model are without having to sift through paragraphs of explanation. I've looked through Introduction to Statistical Learning , but it's fairly surface-level (and also pretty focused on how to use R, and not even a very modern approach to R); I'm working through Elements of Statistical Learning and it seems to be closer to what I want, but it still doesn't seem to have the treatment I want of certain topics (mostly topics that fall under the realm of traditional statistics, such as MANOVA, rather than modern data mining/machine learning). For specificity purposes, let's say that right now, I want to learn the underlying theory behind logistic and probit regression. What's the best place for me to do that?
