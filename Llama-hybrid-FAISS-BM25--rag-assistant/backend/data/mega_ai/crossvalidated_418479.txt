[site]: crossvalidated
[post_id]: 418479
[parent_id]: 418476
[tags]: 
Fixing the seed means that the random initializations for the weights are fixed between the two models. However, changing the order of the features changes which feature is assigned to which weights. This means that even though the weight matrices are the same, you have two different models, and the update procedures will be different. Let's consider an example. Suppose that in both models, a weight matrix is initialized as $$ \begin{bmatrix} -0.13 & 0.12 \\ 0.29 & -0.37 \end{bmatrix} $$ For the first model, the output of the affine transformation for that unit is given by $$ \begin{bmatrix} -0.13 & 0.12 \\ 0.29 & -0.37 \end{bmatrix} \begin{bmatrix} \text{density}\\ \text{residual sugar} \end{bmatrix} + \text{bias}. $$ However, for the second model the output of the affine transformation is $$ \begin{bmatrix} -0.13 & 0.12 \\ 0.29 & -0.37 \end{bmatrix} \begin{bmatrix} \text{residual sugar} \\ \text{density} \end{bmatrix} + \text{bias}. $$ which is clearly a different result whenever $\text{residual sugar} \neq \text{density}$ . This difference in result means that the backpropagation will produce different results between the two models, and therefore the weight updates will be different between the two models. Neural networks are well-known to be non-convex optimization problems, so it's not surprising that the optimization does not recover the same solutions between the two models. Because of the non-convexity of the model, there are many weight configurations which will have the same loss value.
