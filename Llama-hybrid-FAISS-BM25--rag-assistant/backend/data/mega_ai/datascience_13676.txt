[site]: datascience
[post_id]: 13676
[parent_id]: 
[tags]: 
Max-pooling vs. zero padding: Loosing spatial information

When it comes to convolutional neural networks there are normally many papers recommending different strategies. I have heard people say that it is an absolute must to add padding to the images before a convolution, otherwise to much spatial information is lost. On the other hand they are happy to use pooling, normally max-pooling, to reduce the size of the images. I guess the thought here is that max pooling reduces the spatial information but also reduces the sensitivity to relative positions, so it is a trade-off? I have heard other people saying that zero-padding does not keep more information, just more empty data. This is because by adding zeros you will not get a reaction from your kernel anyway when part of the information is missing. I can imagine that zero-padding works if you have big kernels with "scrap values" in the edges and the source of activation centered in a smaller region of the kernel? I would be happy to read some papers about the effect of down-sampling using pooling contra not using padding, but I can't find much about it. Any good recommendations or thoughts? Figure: Spatial down-sampling using convolution contra pooling (Researchgate)
