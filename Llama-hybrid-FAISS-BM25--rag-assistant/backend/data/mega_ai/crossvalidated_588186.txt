[site]: crossvalidated
[post_id]: 588186
[parent_id]: 
[tags]: 
Does the Augmented Dickey-Fuller test only consider first-order effects?

The original Dickey-Fuller test provides a test for the H0: no unit root, so $\gamma \neq 0$ in a simple AR(1) model: $$X_t = \rho X_{t-1} + \varepsilon_t$$ $$ \Delta X_t = \gamma X_{t-1} + \varepsilon_t$$ where $\gamma = \rho - 1$ . They also include models with linear and quadratic (deterministic) trends (i.e., functions of time), because Dickey-Fuller sampling distribution for $\gamma$ depends on whether those effects are in the true model. In Enders "Applied Economic Time Series Analysis", p.132, I found a flow-chart that actually shows how to use these tests to come to a conclusion about whether the process has a unit root. My question is about the the Augmented Dickey-Fuller (ADF). Here, additional higher-order lags are included, I guess similar to the deterministic trends because in case of misspecifications we get biased sampling distributions. The test statistic is $$\gamma_{ADF} = -(1-\sum_{i=1}^p \phi_i)$$ where $\phi_i$ are the coefficients of lags up to order $p$ , and the null hypothesis is $H0: \gamma_{ADF} (see Enders, p.116, eq. 4.19). However, Enders does not give any intuition where this $\gamma_{ADF}$ is coming from, and consequently I don't understand what is actually tested here. Here are my somewhat overlapping questions: (1) What is the intuition behind $\gamma_{ADF}$ ? (2) Does the ADF assume that higher-order lags have no unit roots? That is, are we still testing the unit root for only the first lag, and are adding the additional terms only to avoid biasing the corresponding sampling distribution? (3) If we reject $\gamma_{ADF} , what can we conclude? Only that there is a unit root in the first lag, or that there is at least one unit root, which could be due to any (combination) of the included lags? Any help would be greatly appreciated!
