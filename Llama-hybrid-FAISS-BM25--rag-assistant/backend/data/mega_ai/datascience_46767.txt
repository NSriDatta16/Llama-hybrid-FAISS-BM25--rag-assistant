[site]: datascience
[post_id]: 46767
[parent_id]: 46740
[tags]: 
I am not going to much of your code as i can see you have only imported all the libraries: You are facing with Over-fitting issue, The below are few of things when i come accross the same situation: Build multiple models and check Goodness of fit and then implement. Cross-validation is something which you should look into to make sure you have choosen the right model. How to deal with it: 1. Train with more data:(It wonâ€™t work every time, but training with more data can help algorithms detect the signal better.) 2. Remove features.(because every variable will have variance so even if it is not significant it will try to explain the variance of an dependent variable during training but in test, it will fail because it is not significant enough) 3. Early stopping: (Early stopping refers stopping the training process before the learner passes that point.) 4. Regularization:(It is a way to get a stable model) 5. Ensembling:(My favorite) Ensembles are machine learning methods for combining predictions from multiple separate models. There are a few different methods for ensembling, but the two most common are: Bagging attempts to reduce the chance of overfitting complex models. It trains a large number of "strong" learners in parallel. A strong learner is a model that's relatively unconstrained. Bagging then combines all the strong learners together in order to "smooth out" their predictions. Boosting attempts to improve the predictive flexibility of simple models. It trains a large number of "weak" learners in sequence. A weak learner is a constrained model (i.e. you could limit the max depth of each decision tree). Each one in the sequence focuses on learning from the mistakes of the one before it. Boosting then combines all the weak learners into a single strong learner. While bagging and boosting are both ensemble methods, they approach the problem from opposite directions. Bagging uses complex base models and tries to "smooth out" their predictions while boosting uses simple base models and tries to "boost" their aggregate complexity.
