[site]: datascience
[post_id]: 80854
[parent_id]: 
[tags]: 
Imagining a Linear Regression model with more than 3 dimensions

I'm just getting started with Machine Learning and this is really bugging me now. Assuming we could use more than 2 feature variables to train a Multiple Linear Regression model, how can we imagine the geometry of the model? Let me elaborate. For a linear regression model with a feature variable and a response variable, we come up with an equation of a line (y = mx + c) and for a multivariate linear regression model with 2 predictor variables, we can figure out the equation of a plane (a(x-x1)+ b(y-y1) + c (z-z1) = 0). How can we imagine the model in 4 dimensions or more, since we are humans and as a layman, we cannot imagine higher dimensions like physicists?
