[site]: crossvalidated
[post_id]: 341095
[parent_id]: 
[tags]: 
Can Regularization by achieved using Relative Sensitivity?

In a Mathematical Model we measure the sensitivity of the output with respect to the parameters and it is desirable that a small change in a parameter doesn't lead to wild fluctuations in the output of the model. When a model's inputs use different dimensions or units we are advised to use the 'relative sensitivity'. So where $ y = f(x) = (x_1, x_2, x_3 ....,x_n) $, the relative sensitivity of $y$ with respect to $x_i$ for infinitesimally small changes in $x_i$, is given by $ \frac{x_i}{y} \frac{\partial y}{\partial x_i} $ If $y$ is the error function of an ANN, and $x_i$ is one of its parameters such as a weight, then this equation looks similar to backprop (which is just the chain rule) except for the term: $ \frac{x_i}{y}$ For example, imagine we have some error value $E$ and it changes with respect to some weight $W_i$. If we apply relative sensitivity we seem to get an error function which says: "Scale my rate of change of error according to the relative sizes of the error and the weight in question" $$ \frac{\frac{\partial E}{E}}{\frac{\partial W_i}{W}} = \frac{W_i}{E}\frac{\partial E}{\partial W_i}$$ In a neural network this is how weight updates are made using a standard regularization term: \begin{equation} w_i \leftarrow w_i-\eta\frac{\partial E}{\partial w_i}-\eta\lambda w_i. \end{equation} The proposed approach would look like this: \begin{equation} w_i \leftarrow w_i- \eta\frac{W_i}{E}\frac{\partial E}{\partial W_i} \end{equation} This looks like an elegant way of doing regularization? Am i correct?
