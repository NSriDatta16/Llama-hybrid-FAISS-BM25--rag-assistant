[site]: datascience
[post_id]: 16168
[parent_id]: 16165
[tags]: 
(edited) answer of /u/mostly_reasonable on reddit The thing to note here is that $F(x)$ can refer to the functioning of more than one layer. The paper's authors use '$H(x)$' to mean something like 'the function we want to learn in some (possibly more than one) consecutive layers of a neural network', see their statement [...] hoping each few stacked layers directly [...] Then '$F(x)$' is then that same possibly multi layer function, minus the residuals. The author of course hypothesizes that $F(x)$ is easier to learn than $H(x)$. So I think that in the figure the $F(x)$ is supposed to refer to everything in the Figure besides the ('$+ x$') part. Note how the F(x) symbol is centered with respect the the network, not attached to either layer. Then $F(x) + x$ references the entire $F(x)$ two layer network above combined with the skip connections.
