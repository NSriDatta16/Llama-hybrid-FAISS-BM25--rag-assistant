[site]: crossvalidated
[post_id]: 345443
[parent_id]: 343881
[tags]: 
If the training time is an issue, a typical strategy is to create mini-batches in which sequences have approximately the same length. E.g., from {1}: In order to minimize the zero padding for each batch, during the process of loading the data and creating the mini-batches, the samples are sorted with respect to the number of frames. By doing this, each mini-batch has a minimum variation in number of time steps and the network does not have to process as much silence corresponding to the padded zeros. This technique is called bucketing. See {2} for some comparison shuffle vs. create mini-batches in which sequences have approximately the same length. References: {1} Ramos, M.V., 2016. Voice Conversion with Deep Learning. Tecnico Lisboa Masterâ€™s thesis. {2} Morishita, Makoto, Yusuke Oda, Graham Neubig, Koichiro Yoshino, Katsuhito Sudoh, and Satoshi Nakamura. "An empirical study of mini-batch creation strategies for neural machine translation." arXiv preprint arXiv:1706.05765 (2017). https://arxiv.org/pdf/1706.05765.pdf
