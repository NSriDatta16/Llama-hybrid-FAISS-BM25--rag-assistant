[site]: crossvalidated
[post_id]: 592343
[parent_id]: 592244
[tags]: 
Standard Logistic Regression All of the answers here are great. I just wanted to add a visual example because it is often illustrative of why this can matter. Using R as an example in case you may want to look at this yourself, we can load these libraries and data: #### Load Libraries #### library(datarium) library(tidyverse) library(lmerTest) #### Load Data #### hdp % mutate(DID = factor(DID)) %>% filter(DID %in% 1:20) %>% as_tibble() hdp Printing the data will look like this, information on many metrics predicting cancer remission: # A tibble: 454 × 27 tumorsize co2 pain wound mobil…¹ ntumors nmorp…² remis…³ lungc…⁴ 1 68.0 1.53 4 4 2 0 0 0 0.801 2 64.7 1.68 2 3 2 0 0 0 0.326 3 51.6 1.53 6 3 2 0 0 0 0.565 4 86.4 1.45 3 3 2 0 0 0 0.848 5 53.4 1.57 3 4 2 0 0 0 0.886 6 51.7 1.42 4 5 2 0 0 0 0.701 7 78.9 1.71 3 4 2 0 0 0 0.891 8 69.8 1.53 3 3 3 0 0 0 0.661 9 62.9 1.54 4 4 3 2 0 0 0.909 10 71.8 1.59 5 4 3 0 0 0 0.959 If we fit a regular logistic regression for the overall effect of a patient's length of stay at a hospital and its predictiveness of cancer remission: #### Logistic Regression #### glm.fit We will see that there is a general effect of cancer remission decrease based on length of stay, though its impact is fairly weak: Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) -0.3546 0.5584 -0.635 0.525 LengthofStay -0.1376 0.1012 -1.359 0.174 This can be visualized with the following plot. A plot that looks like an "S" in its curve is typically strong, whereas a flat or semi-flat line is less predictive of the outcome: #### Plot Overall Trend #### hdp %>% ggplot(aes(x=LengthofStay, y=remission))+ geom_point()+ stat_smooth(method="glm", se=FALSE, method.args = list(family=binomial), color = "steelblue", size = 2)+ labs(x="Length of Stay", y="Cancer Remission", title = "Overal Trend of Cancer Remission by Length of Stay") GLMM Logistic Regression What if there is annoying noise related to which doctor was seeing each patient? Does it impact the outcome? We can fit a model with a doctor's identifier (DID) as a random intercept to see if there is some variance in outcomes based on this "random noise" from each doctor. We can also fit a random slope for length of stay per doctor, as patients may be with doctors for various lengths of time that differ between doctors, so we can tease this random variance out as well. glmm.fit From the summary, we can in fact see some differences...the outcome varies by doctor by more than 2.5 standard deviations and there is a negative intercept-slope correlation. We also see that the effect of the predictor is slightly stronger, though still not statistically significant: Random effects: Groups Name Variance Std.Dev. Corr DID (Intercept) 6.7623 2.6004 LengthofStay 0.0423 0.2057 -0.64 Number of obs: 454, groups: DID, 20 Fixed effects: Estimate Std. Error z value Pr(>|z|) (Intercept) -0.6443 1.0467 -0.616 0.538 LengthofStay -0.1722 0.1630 -1.056 0.291 If we plot this, we can see some dramatic differences between doctors. Some have had no patients go into remission, some have very little impact on the outcome, some have a strong impact on the outcome (indicated by a strong S shape in the curves), and others had all their patients go into remission (gasp). #### Plot by Doctor Trend #### hdp %>% ggplot(aes(x=LengthofStay, y=remission))+ geom_point()+ stat_smooth(method="glm", se=FALSE, method.args = list(family=binomial), size = 2, color = "steelblue")+ facet_wrap(~DID)+ labs(x="Length of Stay", y="Cancer Remission", title = "By-Doctor Cancer Remission Based on Length of Stay") By teasing apart the random variance here, we have removed this randomness and figured out more of where the fixed effect is going. Since you are likely learning R for mixed models, you can play around with the other variables in this data and see if its useful for learning. I recommend using the full dataset for that purpose, as I have made some modifications for this example. A Final Caveat All of this is to say that GLMMs can provide interesting and better predictions, but that doesn't always mean you should use them or they are always better ways of modeling regressions. There is a great summary article on the potential pitfalls of GLMMs that can be read through here: On the unnecessary ubiquity of hierarchical linear modeling Psychol Methods . 2017 Mar;22(1):114-140. doi: 10.1037/met0000078. Epub 2016 May 5. PMID: 27149401 DOI: 10.1037/met0000078
