[site]: datascience
[post_id]: 32995
[parent_id]: 31452
[tags]: 
Actually it's controversial and it can differ from one problem to another. the best thing to do is to search the hyperparamteres space for an optimal result. For Question 1 Here I quot the father of CNN (Yann Lecun) : " Training with large minibatches is bad for your health. More importantly, it's bad for your test error. Friends dont let friends use minibatches larger than 32. Let's face it: the only people have switched to minibatch sizes larger than one since 2012 is because GPUs are inefficient for batch sizes smaller than 32. That's a terrible reason. It just means our hardware sucks. What's worse is that the easiest way to parallelize training is to make the minibatch even larger and distribute it across multiple GPUs and multiple nodes. Minibatch sizes over 1024 aren't just bad for your health. They cause brain tumors. They learn quickly, but the wrong thing. ... Update2: I'm pointing to this particular paper as a way to make a general point. But I'm not necessarily endorsing the particular results nor the methodology in this paper (some folks have pointed out a few flaws in the methodology). Still, the main point is that the only reason to use a batch size larger than 1 is the limitations of the limitations of the hardware at our disposal." In this paper a value for batches between 2 and 32 is recommended For Questions 2 & 3: Usually an early stopping technique is used by setting the number of epochs to a very large number and when the generalization error gets worse we just stop. please see For more details and explanations please see At the end I would recommend using "ReLu" activation function for the hidden layers rather than the "sigmoid". it learns much faster
