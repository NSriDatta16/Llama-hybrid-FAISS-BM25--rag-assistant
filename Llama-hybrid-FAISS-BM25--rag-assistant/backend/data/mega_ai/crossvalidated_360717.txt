[site]: crossvalidated
[post_id]: 360717
[parent_id]: 360692
[tags]: 
I don't think there's a rigorous answer today: you may either try different numbers of neurons , and see which number gives you the best test error -or you could just go for the usual approach of the "old-school" CNNs and simply use the same number of neurons as the length of the vector obtained by flattening out the last layer before the fully connected layer (i.e., $N=384$ in this case). Note that the latter is the choice made by the authors of the paper you cite. This is confirmed by the footnote where they confirm that the fully connected part of their neural network (the part in subfigure d. dropout , i.e., the two topmost layers) contribute an extra $384\times 384\times 3$ weights to the overall number of weights of the network. BTW, bit of a crappy architecture. It's practically a dumbed-down LeNet5, where instead of three RGB channels we have 3 window lengths, and the convolutions are 1D rather than 2D. They didn't spend a whole lot of time optimizing their architecture for generalization error, apparently.
