[site]: crossvalidated
[post_id]: 203197
[parent_id]: 
[tags]: 
How to compare posterior distributions for different observed data? KL-divergence?

So I'm solving an inverse problem with the Bayesian approach $p(u | y) \propto p(y| u )p(u)$. Assuming I have two datasets $y_1$ and $y_2$, what can be said about the difference in the posteriors $p(u | y_1)$ and $p(u | y_2)$? Can I use Kullback-Leibler divergence for this? (the datasets are assumed to be samples from the same probability distribution) Also, is there a name for this kind of error type in Bayesian modeling? I quess this problem must be explained somewhere but it seems I'm not searching for the right expressions...
