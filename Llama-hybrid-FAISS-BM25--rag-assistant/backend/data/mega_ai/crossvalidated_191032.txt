[site]: crossvalidated
[post_id]: 191032
[parent_id]: 
[tags]: 
Bayesian interpretation of the repeated sampling principle

My question is philosophical rather than practical, and I will try to explain it through an example: Consider a Kaggle competition. All these contests have a similar structure: A "train dataset" is given which is composed by a "target" variable and (usually many) covariates. A "test dataset" is given as well, which does not include the target variable. Using the "train dataset", competitors estimate a model. Using the "test dataset", they provide a prediction for the unknown target variable. Prediction's goodness is based on some loss function (Mean Squared Error, AUC index, etc), and a winner is awarded. From a frequentist perspective, this setting is absolutely legitimate. There is an unknown "true" relationship between the "target variable" and the "covariates". For istance, let $y$ be the continuous target variable and $\hat{y}$ the prediction, then the MSE $$MSE(y, \hat{y}) = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2,$$ consistently estimates the discrepancy between predictions and real data. Either implicitly or explicitly, we are making use of the principle of repeated sampling , since the discrepancy is assessed on the test dataset. Now, let's go Bayesian. I still want to partecipate to Kaggle, but I reject the principle of repeated sampling. Conditionally to the "train dataset", together with my prior beliefs, I provide a posterior prediction. But how should I interpret the MSE score? Tools like cross-validation are meaningless? If one consider the Bayesian prediction as it were a frequentist estimator, the problem is avoided. Instead, can I be truly Bayesian and still give a meaning to the test set?
