[site]: crossvalidated
[post_id]: 233913
[parent_id]: 
[tags]: 
Scalable machine learning for bigger data

I am aware of the theory of stochastic gradient descent, which is a faster way of developing linear regression. Through this we can have an ' optimized implementation ' of linear regression. There are similar techniques for non-parametric methods as well, which allows you to converge faster keeping in mind cost function. I need suggestion for a book which has worked out implementations or examples of these type of optimized models with R/Python code or pseudo code. So that i can run sophisticated machine learning algorithms faster, without increasing my hardware further. I am open about increasing hardware though. What interests me is a faster implementation of techniques, so that i can use scalable implementations of machine learning algorithms for bigger data. Thanks!!
