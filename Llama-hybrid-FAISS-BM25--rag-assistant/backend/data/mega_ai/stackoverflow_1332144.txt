[site]: stackoverflow
[post_id]: 1332144
[parent_id]: 1332104
[tags]: 
This is a classic use case for a Bloom filter . A Bloom filter is a probabilistic data structure which is optimized for membership tests ("is X a member of this collection?"), and provides O(1) lookup . In exchange, you introduce an arbitrarily small probability of a false positive -- that is, the filter will say a particular word is present, but it is actually not there. The more memory you use, the smaller you can make this probability. However, the probability of false negatives is zero: the filter will never say that a word is absent if it is actually present. In your specific case, with 8 billion bits (1 GB) to work with, you can get a false positive rate a little better than 1 in every 1,000,000,000 trials. That's an extremely low false positive rate. If you looked up 200 million random strings, the probability that you'd never hit a single false positive is about 82%. This does not require the dictionary to be sorted, is highly space-efficient, and does not need a database or other ancillary storage structure. Overall, it's probably a good choice for your needs.
