[site]: crossvalidated
[post_id]: 618683
[parent_id]: 
[tags]: 
Wondering if averaging datapoints by a set number of observations is a valid way to aggregate data in a scatterplot

So essentially the problem I have is this: I've created an economics simulation which, among other things, tracks the profit rates and capital intensity of all the firms over the course of the simulation . The trouble is, which you can see from the scatterplot of these two variables, is how dense and difficult to read it is. A solution I came up for this was to sort the variables according to increasing capital intensity, divide them up into 100 equal sized buckets, and get the average of each variable for each bucket of observations. Which produces this much more readable graph and much more obvious negative linear regression the literature has. My question: is this a valid way of aggregating the data? Does this misrepresent the distribution of the data in some crucial way? EDIT: for reference here's the R code I used to aggregate it dfPC $capIn)/divSize)) divCount divCount $capIn)] mPC divCount==dC) } plot(mPC[,2], mPC[,3], xlab = "Capital Intensity", ylab = "Profit Rate") abline(lm(mPC[,3] ~ mPC[,2]))
