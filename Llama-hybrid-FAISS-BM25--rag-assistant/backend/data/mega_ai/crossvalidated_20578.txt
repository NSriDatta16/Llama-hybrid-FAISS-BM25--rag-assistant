[site]: crossvalidated
[post_id]: 20578
[parent_id]: 20558
[tags]: 
(The original answer is dated 2012). This is a difficult question to answer. The number of people who truly do both is still very limited. Hard core Bayesians despise the users of mainstream statistics for their use of $p$ -values, a nonsensical, internally inconsistent statistic for Bayesians; and the mainstream statisticians just do not know Bayesian methods well enough to comment on them. In the light of this, you will see a lot of criticism of the null hypothesis significance testing in Bayesian literature (ranging as far as nearly pure biology or pure psychology journals), with little to no response from mainstreamers. There are conflicting manifestation as to "who won the debate" in statistics profession. On one hand, the the composition of an average statistics department is that in most places, you will find 10-15 mainstreamers vs. 1-2 Bayesians, although some departments are purely Bayesian, with no mainstreamers at all, except probably for consulting positions that have the job responsibility of producing experiment designs for biologists. Harvard, Duke, Carnegie Mellon, British Columbia, Montreal come to mind in North America; I am less familiar with European scene. On the other hand, you will see that in journals like JASA or JRSS, probably 25-30% of papers are Bayesian. In a way, the Bayesian renaissance may be something like the burst of ANOVA papers in the 1950s: back then, people thought that pretty much any statistics problem can be framed as an ANOVA problem; right now, people think that pretty much anything can be solved with the right MCMC. My feeling is that applied areas don't bother figuring out the philosophical details, and just go with whatever is easier to work with. Bayesian methodology is just too damn complicated: on top of statistics, you also need to learn the art of computation (setting up the sampler, blocking, convergence diagnostics, blah-blah-blah) and be prepared to defend your priors (should you use objective priors, or should you use informative priors if the field has pretty much settled on the speed of light being 3e8 m/s, or even whether the choice of the prior affects whether your posterior will be proper or not). So in most medical or psychology or economics applications, you will see mainstream approaches in the papers written by substantive researchers, although you can also see occasional glimpse of a Bayesian paper -- to have been written by more sophisticated methodologists, or in collaboration with a Bayesian statistician, just because that was the person available at a local department to do this collaborative work. One area where, I think, Bayesian framework is still coming short is model diagnostics -- and that is an important area for practitioners. In Bayesian world, to diagnose a model, you need to build a more complicated one and choose whichever has a better fit by Bayesian factor or BIC. So if you don't like the normality assumption for your linear regression, you can build a regression with Student errors, and let the data generate an estimate of the degrees of freedom, or you can become all fancy and have a Dirichlet process for your error terms and do some M-H jumps between different models. The mainstream approach would be to build a Q-Q plot of studentized residuals and remove outliers, and this is, again, so much simpler. I edited a chapter in a book on this -- see http://onlinelibrary.wiley.com/doi/10.1002/9780470583333.ch5/summary . It is a very archetypal paper, in that gave about 80 references on this debate, all supporting the Bayesian point of view. (I asked the author to extend it in a revised version, which says a lot about it :) ). Jim Berger from Duke, one of the leading Bayesian theorists, gave a number of lectures, and wrote a number of very thoughtful articles on the topic. P.S. (edit in June 2020): the "blah-blah-blah" part of computation has been significantly simplified in the recent years with Stan ( https://mc-stan.org/ ). The NUTS sampler has fewer parameters to tweak, while offering additional diagnostics that make convergence failures more obvious. Model diagnostics has seen improvements, too, with posterior predictive checks and simulation based calibration .
