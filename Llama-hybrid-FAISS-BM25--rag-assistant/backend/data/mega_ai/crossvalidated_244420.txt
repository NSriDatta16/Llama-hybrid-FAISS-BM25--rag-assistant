[site]: crossvalidated
[post_id]: 244420
[parent_id]: 244413
[tags]: 
This problem isn't simple to approach. The reason that backpropagation works well for neural networks is because we can pass the inputs through the network once, record the activation value of each neuron, and use them to compute the derivatives of each neuron. When you integrate over $z$ you are changing the activation value of a neuron which will change every neuron in every subsequent layer. This is very unlike backpropagation where we get the partial derivatives for a single activation value for each neuron. There is no reason to expect that this would have a nice solution so my honest advice to you would be to give up.
