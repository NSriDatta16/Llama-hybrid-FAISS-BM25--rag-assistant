[site]: crossvalidated
[post_id]: 328551
[parent_id]: 328397
[tags]: 
My answer is similar to Martin's. Let's look at an example: Ridge (linear) regression :say for $y=\beta X+\epsilon$. The motivation is to reduce the variance: avoid over-fitting. Imagine input and output are normalized so that what I'm going to say is not sensitive to the scale. The prior on $\beta$ is $N(0,b^2I)$? The assumption is "$\beta$ can't have a big norm" . How much is "big"? By definition, that's the regularization parameter $b$. Since the data is normalized "big norm" means both a lot of big positives and big negative coefficients. The choice of $b$ does not matter much. The method is not much sensitive to the precise value of $b$ but just needs to be in "right range". The curve $b\rightarrow error$ have most often a big flat plateau. The range is often determined with a validation set. The unregularized estimate is over-fitted and produces big (and inaccurate) $\beta$s. Two questions: why and how much? I once did a few simulations in willingly extreme scenarios where the regularized version works very well but the norm of the unregularized estimate happens to be thousands or millions of time the true value's with a high probability. The theoretical answer to "how much" is the variance of the raw estimator which can be calculated explicitly as $\sigma^2(X'X)^{-1}$: see wikipedia . Typically near-collinear inputs make this value explode. The more features, the smaller data, the more near-colinearity is likely to happen randomly. why big $\beta$s could not be the "true" parameter? Can ruling them out make you miss the true value? Why real life data is often like this?. I don't know. Theoretically everything is possible, but... this is very unlikely to happen. And even if it was possible, you could not approximate the value with your data anyway in a satisfying way. It's a bit like a Pascal's Wager : if $\beta$ has a big norm, you can't find any useful approximation of it, so that you'd better assume it has a small norm, and use a good estimator for this case. So rather than prior knowledge, the prior is more like a bet. Note: A purely non Bayesian justification of regularization is something I'm still looking for. There are ways to find the best (from some point of view) value of $b$ explicitly: Determination of Tikhonov factor . I'm not sure, but even if not explicitly Bayesian, a Bayesian assumption may be subtly hidden in it.
