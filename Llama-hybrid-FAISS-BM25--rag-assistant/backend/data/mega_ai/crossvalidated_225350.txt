[site]: crossvalidated
[post_id]: 225350
[parent_id]: 23566
[tags]: 
I worked for several years with Jim Ramsay on FDA, so I can perhaps add a few clarifications to @amoeba's answer. I think on a practical level, @amoeba is basically right. At least, that's the conclusion I finally reached after studying FDA. However, the FDA framework gives an interesting theoretical insight into why smoothing the eigenvectors is more than just a kludge. It turns out that optmization in the function space, subject to an inner product that contains a smoothness penalty, gives a finite dimensional solution of basis splines. FDA uses the infinite dimensional function space, but the analysis does not require an infinite number of dimensions. It's like the kernel trick in Gaussian processes or SVM's. It's a lot like the kernel trick, actually. Ramsay's original work dealt with situations where the main story in the data is obvious: the functions are more or less linear, or more or less periodic. The dominant eigenvectors of standard PCA will just reflect the overall level of the functions and the linear trend (or sine functions), basically telling us what we already know. The interesting features lie in the residuals, which are now several eigenvectors from the top of the list. And since each subsequent eigenvector has to be orthogonal to the previous ones, these constructs depend more and more on artifacts of the analysis and less on relevant features of the data. In factor analysis, oblique factor rotation aims to resolve this problem. Ramsay's idea was not to rotate the components, but rather to change the definition of orthogonality in a way that would better reflect the needs of the analysis. This meant that if you were concerned with periodic components, you would smooth on the basis of $D^3-D$, which eliminates sines and consines. If you wanted to remove a linear trend, you would smooth on the basis of $D^2$ which gives standard cubic splines. One might object that it would be simpler to remove the trend with OLS and examine the residuals of that operation. I was never convinced that the value add of FDA was worth the enormous complexity of the method. But from a theoretical standpoint, it is worth considering the issues involved. Everything we do to the data messes things up. The residuals of OLS are correlated, even when the original data were independent. Smoothing a time series introduces autocorrelations that were not in the raw series. The idea of FDA was to ensure that the residuals we got from initial detrending were suited to the analysis of interest. You have to remember that FDA originated in the early 80's when spline functions were under active study - think of Grace Wahba and her team. Many approaches to multivariate data have emerged since then - like SEM, growth curve analysis, Gaussian processes, further developments in stochastic process theory, and many more. I'm not sure that FDA remains the best approach to the questions it addresses. On the other hand, when I see applications of what purports to be FDA, I often wonder if the authors really understand what FDA was trying to do.
