[site]: crossvalidated
[post_id]: 344703
[parent_id]: 
[tags]: 
normalization operation before feeding the input sequence to the deep learning model

before training a network, I normalize each input sequence, by having this function, def norm(x): return (x - np.mean(x)) / (np.std(x) + 1e-7) datamatrix[row_id,:]=norm(datamatrix[row_id,:]) Here, datamatrix[row_id,:] represents a single input sequence. For each normalization operation, I plot the following figure, where the dashed line represented the normalized sequence, while the solid line represents the original sequence. It seems that the normalization operation even enlarges the range of original sequence, i.e., the max and min
