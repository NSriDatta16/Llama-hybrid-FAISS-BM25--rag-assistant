[site]: crossvalidated
[post_id]: 421325
[parent_id]: 421221
[tags]: 
There is quite a Bayesian literature on "not using the data twice", starting from it being a probability non-sense [e.g., what happens to the dominating measure?] to biasing the inference towards over-fitting. There are however a few positive proponents: Murray Aitkin defends this approach as a way to bypass paradoxes (Lindley's) and difficulties related with improper priors, especially in testing hypotheses. He calls this integrated likelihood and has a book and a Read Paper on the subject. The discussion at the end of the paper is illuminating on why this is incoherent, if ferocious at times. We also wrote a critical analysis of the book for Statistical Science. If embracing the notion that feeding the prior with a power of the likelihood concentrates on the MLE, this may become a way to derive the MLE. I proposed this method in 1993 under the name of prior feedback , with an improved version (with Arnaud Doucet and Simon Godsill] under the acronym of SAME and it has been rediscovered since then under other names, like data cloning .
