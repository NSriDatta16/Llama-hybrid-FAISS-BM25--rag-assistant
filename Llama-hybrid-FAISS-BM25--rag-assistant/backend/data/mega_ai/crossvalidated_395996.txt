[site]: crossvalidated
[post_id]: 395996
[parent_id]: 
[tags]: 
Estimating maximum entropy distribution given first n moments

Is there a good way to estimate the pdf, pdf up to a constant multiple, cdf, or quantile function of a distribution given the first n moments? A closed form for one of those functions in terms of the moments would be ideal, but I suspect that one does not exist in general. The practical motivation here is that I'm trying to write a library for estimating various properties of a time series after the fact. Collections of moments have nice algebraic properties (because expectation is linear) which makes parallel processing easier. A normal distribution is the maximum entropy distribution among distributions constrained to have mean $\mu$ and variance $\sigma^2$ . According to this question and the Wikipedia article on variance , the mean (102) and variance (103) can be defined in terms of raw moments ( $\mu_n$ where $n$ is the power). $$ \mu_n \stackrel{\small{\text{def}}}{=} \mathop{\mathbb{E}} \left[ x^n \right] \tag{101} $$ $$ \mu \stackrel{\small{\text{def}}}{=} \mu_1 \tag{102} $$ $$ \sigma^2 \stackrel{\small{\text{def}}}{=} \mu_2 - \mu_1^2 \tag{103} $$ The normal distribution is naturally parameterized by $\mu$ and $\sigma^2$ , but can also be thought of as being parameterized by $\mu$ and $\mu_2$ . The answer by Glen_b references a theorem by Ludwig Boltzmann which suggests to me that the pdf in a situation where the first $n$ moments are known will be proportional to (104). $$ \exp \circ \left( \text{some $n$ degree polynomial with no constant term} \right) \tag{104} $$ Assuming I haven't horribly misinterpreted the answer, is there a computationally efficient way to estimate these coefficients? Moreover, is there a way to do an online estimate of the coefficients? Ideally I'd like to be able to update the coefficients of the polynomial in (104) at the same time that I update the moments when "merging in" new observations.
