[site]: crossvalidated
[post_id]: 193890
[parent_id]: 
[tags]: 
Algorithm to determine a point in time series data, after which probability of increase in value is very low

I am working with dataset which contains number of movie tickets sold per day. This is basically a count of total number of tickets sold, for a particular movie, for each day after its release date. I want to find out what should be the ideal duration (number of days after release) for a theater to keep the show running. In other words, I want to find out the day after which there is low probability of increase in ticket sales. So, that theaters can stop the show time for that movie and release a new movie. I would like to know an ALGORITHM which is suited to study/analyze the graph or dataset, and provide as a result, after how many days (starting from release date of the movie), it is ideal to remove it. Few examples: Movie 157: Day Tickets Sold 1 800 2 1200 3 1330 4 1300 5 1400 6 700 7 300 8 150 9 100 10 50 As we can determine after 6th and 7th day, movie sales are not increasing, so it is good to remove the movie after 7th day However, the challenge is with non-uniform ticketing trends, like example below: Movie 158: Day Tickets Sold 1 800 2 900 3 600 4 900 5 1200 6 700 7 1500 8 800 9 700 10 550 Movie 159: Day Tickets Sold 1 1300 2 900 3 600 4 400 5 300 6 500 7 800 8 1200 9 1100 10 1250 Now, I want to use an algorithm which can be used for both uniform as well as non-uniform trends.
