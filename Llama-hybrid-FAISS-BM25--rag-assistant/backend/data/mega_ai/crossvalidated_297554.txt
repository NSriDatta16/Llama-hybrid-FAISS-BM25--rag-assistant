[site]: crossvalidated
[post_id]: 297554
[parent_id]: 297380
[tags]: 
What typically happens when there is no mathematical consistency (atleast in this case of neural networks)...when its not giving results as desired, on the test set, your boss will come back and say...Hey why don't you try Drop out (which weights,which layer, how many is your headache as there isn't mathematical way to determine), so after you try and hopefully got a marginal improvement but not the desired, your boss will come back and say, why not try weight decay(what factor?)? and later, why don't you try ReLU or some other activation on some layers, and still not, why not try 'max pooling'? still not, why not try batch normalization, still not, or atleast convergence, but not desired result, Oh you are in a local minimum, try different learning rate schedule, just change the network architecture? and repeat all above in different combinations! Keep it in a loop until you succeed! On the other hand, when you try a consistent SVM, after convergence, if the result is not good, then okay, the linear kernel we are using is not good enough as the data may not be linear, use a different shaped kernel, try a different shaped kernel if you have any hunch, if still not, just leave it, its a limitation of SVM. What I am saying is ,the neural networks being so inconsistent, that it is not even wrong! It never accepts its defeat! The engineer/designer takes the burden, in case it does not work as desired.
