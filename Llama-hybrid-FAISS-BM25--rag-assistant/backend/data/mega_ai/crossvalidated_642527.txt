[site]: crossvalidated
[post_id]: 642527
[parent_id]: 
[tags]: 
Lasso regression test MSE lower than train MSE

Im currently using Lasso to build a predictive model for numeric variable . Before scaling the features I split the data for train test and validation . I have a feature named 'year' and i wanted the data to be split in a way that each value of 'year' will be in train test and validation ( Im dropping the 'year' feature later in the code as I converted it to a dummy variable and Im using the original column of 'year' only for making sure that for the split every dataset has every value of year): from sklearn.model_selection import train_test_split import xgboost as xgb from sklearn.metrics import accuracy_score, classification_report train_dfs, test_dfs,validation_dfs = [], [],[] # Unique years in the 'year' column unique_years = df_encoded['year'].unique() # Iterate through each unique year fo the train test split for year in unique_years: # Extract data for the current year year_data = df_encoded[df_encoded['year'] == year] # Split the data for the current year train_data, test_data = train_test_split(year_data, test_size=0.3,random_state=42) test_data, validation_data = train_test_split(test_data, test_size=0.3, random_state=42) # Append to the lists train_dfs.append(train_data) test_dfs.append(test_data) validation_dfs.append(validation_data) #stratified sampling for validation # Concatenate the dataframes for the final train and test sets train_df = pd.concat(train_dfs) test_df = pd.concat(test_dfs) valid_df=pd.concat(validation_dfs) train_df=train_df.drop(columns=['year']) test_df=test_df.drop(columns=['year']) valid_df=valid_df.drop(columns=['year']) X_train, y_train = train_df.drop(columns=['childbor']), train_df['childbor'] X_test, y_test = test_df.drop(columns=['childbor']), test_df['childbor'] X_valid,y_valid=valid_df.drop(columns=['childbor']), valid_df['childbor'] After that I used StandardScaler() from sklearn.preprocessing import MinMaxScaler from sklearn.preprocessing import StandardScaler # Create an instance of standart sacaler scaler =StandardScaler() # Fit the scaler on the training data and transform the training data train_X_scaled = scaler.fit_transform(X_train) # Transform the test and validation data using the scaler fitted on the training data test_X_scaled = scaler.transform(X_test) validation_X_scaled = scaler.transform(X_valid) Then I wanted to find the best regularization parameter for my model so I used this code : alphas = np.logspace(-2, 2, 300) errors_coefs = [] error_train=[] error_test=[] coefs = [] for a in alphas: clf = linear_model.Lasso(alpha=a,fit_intercept=True) clf.fit(train_X_scaled, y_train) coefs.append(clf.coef_) pred = clf.predict(validation_X_scaled) pred_train=clf.predict(train_X_scaled) pred_test=clf.predict(test_X_scaled) errors_coefs.append(mean_squared_error(pred, y_valid)) error_train.append(mean_squared_error(pred_train,y_train)) error_test.append(mean_squared_error(pred_test,y_test)) plt.figure(figsize=(8, 6)) plt.plot(alphas, coefs) plt.xscale("log") plt.xlabel("alpha") plt.ylabel("Coefficients") plt.title("Lasso Coefficients as a Function of Regularization") #plt.xlim(plt.xlim()[::-1]) # Reverse axis plt.tight_layout() plt.show() and I got this plot And I was wondering how could that be that for small values of alpha the test MSE is lower than train MSE / or equal to it ?Could it be that for lower values of alpha it is capable to "learn" the noise and there for preform better in the test ?
