[site]: crossvalidated
[post_id]: 331775
[parent_id]: 
[tags]: 
Averaging between two sparse PCA models

I am trying to construct a sparse PCA model for a large amount of data that I obtain as batches over time. This means e.g. that I can not draw the gloabal mean beforehand, since the the data is simply not there. I also do not want to store the whole data and recalculate it. The idea is to keep only the vectors and the scores of the old PCA. With a new batch coming, I'd like to calculate a PCA model for the batch, and then recompute the new eigenvectors and scores basing on the information contained in the old model and the batch, with regard to the number of points that were used to create the two. I do understand that there is a classical method for achieving this, and it should be even simpler than the PCA for point data, but I just don't know where to seek for a reference. Will be thankful for any help. Addendum: More generally, the problem is how to calculate PCA not for point clouds, but rather for collections of weighted voluminous objects, e.g. $m$-dimensional ellipsoids in $n\ge m$-dimensional space, under the condition that the ellipsoid sizes may not be neglected.
