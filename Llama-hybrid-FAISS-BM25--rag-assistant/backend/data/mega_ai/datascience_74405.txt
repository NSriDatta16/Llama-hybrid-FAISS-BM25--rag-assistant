[site]: datascience
[post_id]: 74405
[parent_id]: 64120
[tags]: 
In tensorflow-tutorials-for-text they are implementing bahdanau attention layer to generate context vector by giving encoder inputs, decoder hidden states and decoder inputs. Encoder class is simply passing the encoder inputs from Embedding layer to GRU layer along with encoder_states and returns encoder_outputs and ecoder_states. If we use LSTM instead of GRU then states would be state_h, state_c of size (batch_size, units). In the Decoder part, they are passing decoder_inputs, encoder_outputs and states (to initialize use encoder_states for remaining use deocder_states). When we give the above three parameters in the Decoder, BahdanauAttention Layer will calculate contex_vector and weights using encoder_outputs and states. We can also use AdditiveAttention-Layer it is Bahdanau-style attention. In which query is our decoder_states and value is our encoder_outputs. It is one of the nice tutorials for attention in Keras using TF backend that I came across.
