[site]: crossvalidated
[post_id]: 446619
[parent_id]: 
[tags]: 
Can a neural network be trained repeatedly on subsets of data to fight overfitting?

I've used random forests for years but I'm less experienced building neural networks. Overfitting is obviously a concern for both decision trees and neural networks. Statistically, random forests are much more robust than decision trees. They use subsets of the data (with replacement) and then average the results (there's a lot more going on in random forests, but that'll do for our purposes here). I'm familiar with MC dropout , which applies a random approach for regularizing neural networks. Here's my question. Why don't neural networks train on multiple subsets of data (with replacement) and then combine their results like random forests? It seems like a good way to fight overfitting but it seems like this isn't a common practice. Is there a mathematical reason that this doesn't work?
