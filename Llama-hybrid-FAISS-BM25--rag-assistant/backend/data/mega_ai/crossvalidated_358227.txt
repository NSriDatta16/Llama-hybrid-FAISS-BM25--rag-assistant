[site]: crossvalidated
[post_id]: 358227
[parent_id]: 
[tags]: 
What is the correct answer for cross entropy in this case?

I have two neural networks: NN1 and NN2, predicting Dog and Cat. The probabilities are below: NN1 NN2 Dog 1 0.9 0.6 Cat 0 0.1 0.4 Dog 0 0.1 0.3 Cat 1 0.9 0.7 Dog 1 0.4 0.1 Cat 0 0.6 0.9 I calculated Cross Entropy for NN1 as below: -1*ln(0.9) -1*ln(0.9) -1*ln(0.4) = 1.12 But the answer is 0.38 for NN1 and 1.06 for NN2. How come?
