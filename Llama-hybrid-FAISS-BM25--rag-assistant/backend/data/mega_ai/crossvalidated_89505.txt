[site]: crossvalidated
[post_id]: 89505
[parent_id]: 
[tags]: 
Sizing of training and validation sets in machine learning: Is there a proven optimum, or merely heuristics?

When I watch presentations where machine learning algorithms were used, the amount of data put in the training and validation sets seems to be somewhat arbitrary. Sometimes it's 80-20, sometimes it's 90-10. My completely naive approach would be 50-50 (because, hey, that sounds like a fair division, right?) Is there any actual math out there which demonstrates the optimal way to size the sets, or is it all based on, "This seems to work okay most of the time"?
