Singularitarianism is a movement defined by the belief that a technological singularity—the creation of superintelligence—will likely happen in the medium future, and that deliberate action ought to be taken to ensure that the singularity benefits humans. Singularitarians are distinguished from other futurists who speculate on a technological singularity by their belief that the singularity is not only possible, but desirable if guided prudently. Accordingly, they may sometimes dedicate their lives to acting in ways they believe will contribute to its rapid yet safe realization. American news magazine Time describes the worldview of Singularitarians by saying "even though it sounds like science fiction, it isn't, no more than a weather forecast is science fiction. It's not a fringe idea; it's a serious hypothesis about the future of life on Earth. There's an intellectual gag reflex that kicks in anytime you try to swallow an idea that involves super-intelligent immortal cyborgs, but... while the Singularity appears to be, on the face of it, preposterous, it's an idea that rewards sober, careful evaluation". Definition The term "Singularitarian" was originally defined by Extropian thinker Mark Plus (Mark Potts) in 1991 to mean "one who believes the concept of a Singularity". This term has since been redefined to mean "Singularity activist" or "friend of the Singularity"; that is, one who acts so as to bring about the singularity. Singularitarianism can also be thought of as an orientation or an outlook that prefers the enhancement of human intelligence as a specific transhumanist goal instead of focusing on specific technologies such as A.I. There are also definitions that identify a singularitarian as an activist or a friend of the concept of singularity, that is, one who acts so as to bring about a singularity. Some sources described it as a moral philosophy that advocates deliberate action to bring about and steer the development of a superintelligence that will lead to a theoretical future point that emerges during a time of accelerated change. Inventor and futurist Ray Kurzweil, author of the 2005 book The Singularity Is Near: When Humans Transcend Biology, defines a Singularitarian as someone "who understands the Singularity and who has reflected on its implications for his or her own life" and estimates the singularity will occur around 2045. History An early singularitarian articulation that history is making progress toward a point of superhuman intelligence is found in Hegel's work The Phenomenology of Spirit. In 1993, mathematician, computer scientist, and science fiction author Vernor Vinge hypothesized that the moment might come when technology will allow "creation of entities with greater than human intelligence" and used the term "the Singularity" to describe this moment. He suggested that the singularity may pose an existential risk for humanity, and that it could happen through one of four means: The development of computers that are "awake" and superhumanly intelligent. Large computer networks (and their associated users) may "wake up" as a superhumanly intelligent entity. Computer/human interfaces may become so intimate that users may reasonably be considered superhumanly intelligent. Biological science may find ways to improve upon the natural human intellect. Singularitarianism coalesced into a coherent ideology in 2000, when artificial intelligence (AI) researcher Eliezer Yudkowsky wrote The Singularitarian Principles, in which he states that a Singularitarian believes that the singularity is a secular, non-mystical event that is possible, beneficial to the world, and worked toward by its adherents. Yudkowsky's definition is inclusive of various interpretations. Theorists such as Michael Anissimov argue for a strict definition that refers only to the advocacy of the development of superintelligence. In June 2000, Yudkowsky, with the support of Internet entrepreneurs Brian Atkins and Sabine Atkins, founded 