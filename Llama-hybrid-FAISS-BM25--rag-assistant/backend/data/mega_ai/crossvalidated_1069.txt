[site]: crossvalidated
[post_id]: 1069
[parent_id]: 1062
[tags]: 
This is a good, if underspecified question. Simply put, obtaining an orthogonal parametrization allows for parameters of interest to be conveniently related to other parameters, particularly in establishing needed minimizations. Whether or not this is useful depends on what you are trying to do (in the case of some physics problems, for instance, orthogonal parametrization may obscure the symmetries of interest). In the case of statistical inference, orthogonal parametrization can allow the use of statistics by way of minimization (or its dual) on orthogonal parameters. For instance, Cox and Reid use the orthogonality of nuisance parameters (and their appropriately applied maximum likelihood estimates) to construct a generalization of a liklihood ratio statistic for a parameter of interest. To see how orthogonality allows for this requires an understanding of the properties of commonly used mathematical spaces and the construction of estimators, which is essentially an issue of information geometry . See Information Geometry, Bayesian Inference, Ideal Estimates and Error Decomposition for a lucid, but technical description of orthogonality and its role in statistical inference.
