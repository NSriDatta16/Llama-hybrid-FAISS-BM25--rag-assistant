[site]: crossvalidated
[post_id]: 49322
[parent_id]: 
[tags]: 
Data sufficiency -- determining from aggregate

I am working with a dataset which looks essentially like the following: Name Average Score -------- -------------- John 34.9381 Susan 22.2718 Stu 29.1009 ... The assumption is that each average score is composed of integer-value single scores. I would like to know if there are any standard techniques out there for determining the quantity of data underlying the aggregates. For the purposes of simplification, we can assume that number of records is consistent among names. I've tried a few hacked-together techniques for guessing, but the brute-force method of trying n=2, then 3, then 4, etc. of every possible value is computationally miserable for anything other than test cases. I've also attempted to shave it down by assuming first value = round(average), then adding next value to get closest possible on n=2, etc. (e.g., 2.666: first is 3; for second, subtract 3/2 from 2.666, round to nearest 0.5, multiply by 2: result = 2, so best average=2.5; etc.). This works for trivially simple numbers -- rather, single floats. Run until minimized, and it gives a rough idea for a small n. But this may be theoretically full of holes, which I'd like to know. Worse, for many decimal places, large datasets, and lots of rounding problems, this ugly method isn't great. And I feel like I must be solving an already-solved problem, but I don't have the vocabulary even to search for the solution. What is this problem called, or even what field is this in? And does anyone have any bright ideas, R libraries, etc.? Thanks very much.
