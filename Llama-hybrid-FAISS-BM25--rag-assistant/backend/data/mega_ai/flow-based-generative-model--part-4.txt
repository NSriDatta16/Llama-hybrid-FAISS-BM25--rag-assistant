 be the latent variable with distribution p ( z 0 ) {\displaystyle p(z_{0})} . Map this latent variable to data space with the following flow function: x = F ( z 0 ) = z T = z 0 + ∫ 0 T f ( z t , t ) d t {\displaystyle x=F(z_{0})=z_{T}=z_{0}+\int _{0}^{T}f(z_{t},t)dt} where f {\displaystyle f} is an arbitrary function and can be modeled with e.g. neural networks. The inverse function is then naturally: z 0 = F − 1 ( x ) = z T + ∫ T 0 f ( z t , t ) d t = z T − ∫ 0 T f ( z t , t ) d t {\displaystyle z_{0}=F^{-1}(x)=z_{T}+\int _{T}^{0}f(z_{t},t)dt=z_{T}-\int _{0}^{T}f(z_{t},t)dt} And the log-likelihood of x {\displaystyle x} can be found as: log ⁡ ( p ( x ) ) = log ⁡ ( p ( z 0 ) ) − ∫ 0 T Tr [ ∂ f ∂ z t ] d t {\displaystyle \log(p(x))=\log(p(z_{0}))-\int _{0}^{T}{\text{Tr}}\left[{\frac {\partial f}{\partial z_{t}}}\right]dt} Since the trace depends only on the diagonal of the Jacobian ∂ z t f {\displaystyle \partial _{z_{t}}f} , this allows "free-form" Jacobian. Here, "free-form" means that there is no restriction on the Jacobian's form. It is contrasted with previous discrete models of normalizing flow, where the Jacobian is carefully designed to be only upper- or lower-diagonal, so that the Jacobian can be evaluated efficiently. The trace can be estimated by "Hutchinson's trick":Given any matrix W ∈ R n × n {\displaystyle W\in \mathbb {R} ^{n\times n}} , and any random u ∈ R n {\displaystyle u\in \mathbb {R} ^{n}} with E [ u u T ] = I {\displaystyle E[uu^{T}]=I} , we have E [ u T W u ] = t r ( W ) {\displaystyle E[u^{T}Wu]=tr(W)} . (Proof: expand the expectation directly.)Usually, the random vector is sampled from N ( 0 , I ) {\displaystyle N(0,I)} (normal distribution) or { ± n − 1 / 2 } n {\displaystyle \{\pm n^{-1/2}\}^{n}} (Rademacher distribution). When f {\displaystyle f} is implemented as a neural network, neural ODE methods would be needed. Indeed, CNF was first proposed in the same paper that proposed neural ODE. There are two main deficiencies of CNF, one is that a continuous flow must be a homeomorphism, thus preserve orientation and ambient isotopy (for example, it's impossible to flip a left-hand to a right-hand by continuous deforming of space, and it's impossible to turn a sphere inside out, or undo a knot), and the other is that the learned flow f {\displaystyle f} might be ill-behaved, due to degeneracy (that is, there are an infinite number of possible f {\displaystyle f} that all solve the same problem). By adding extra dimensions, the CNF gains enough freedom to reverse orientation and go beyond ambient isotopy (just like how one can pick up a polygon from a desk and flip it around in 3-space, or unknot a knot in 4-space), yielding the "augmented neural ODE". Any homeomorphism of R n {\displaystyle \mathbb {R} ^{n}} can be approximated by a neural ODE operating on R 2 n + 1 {\displaystyle \mathbb {R} ^{2n+1}} , proved by combining Whitney embedding theorem for manifolds and the universal approximation theorem for neural networks. To regularize the flow f {\displaystyle f} , one can impose regularization losses. The paper proposed the following regularization loss based on optimal transport theory: λ K ∫ 0 T ‖ f ( z t , t ) ‖ 2 d t + λ J ∫ 0 T ‖ ∇ z f ( z t , t ) ‖ F 2 d t {\displaystyle \lambda _{K}\int _{0}^{T}\left\|f(z_{t},t)\right\|^{2}dt+\lambda _{J}\int _{0}^{T}\left\|\nabla _{z}f(z_{t},t)\right\|_{F}^{2}dt} where λ K , λ J > 0 {\displaystyle \lambda _{K},\lambda _{J}>0} are hyperparameters. The first term punishes the model for oscillating the flow field over time, and the second term punishes it for oscillating the flow field over space. Both terms together guide the model into a flow that is smooth (not "bumpy") over space and time. Flows on manifolds When a probabilistic flow transforms a distribution on an m {\displaystyle m} -dimensional smooth manifold embedded in R n {\displaystyle \mathbb {R} ^{n}} , where m < n {\displaystyle m<n} , and where the transformation is specified as a function, R n