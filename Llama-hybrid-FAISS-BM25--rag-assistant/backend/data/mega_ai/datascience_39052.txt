[site]: datascience
[post_id]: 39052
[parent_id]: 
[tags]: 
Reinforcement learning: easily learnable state representation

I have created a simple OpenAI Gym environment, which consists of: A continuous 2D world with x and y in range [0.0, 1.0] A rabbit which slowly moves randomly in the world with a constant speed A 'wolf' controlled by the agent. The wolf moves at a constant speed The actions are [turn left by a constant angle, turn right by a constant angle, do nothing (continue straight)] The state is [agent_x, agent_y, agent_bearing, rabbit_x, rabbit_y, rabbit_bearning]. Bearings are in radians [0.0, 2*pi]. All values are floating point numbers. The reward is 30 for catching the rabbit (catching means the agent getting sufficiently close to the rabbit). -0.1 for each timestep without catching the rabbit. Maximum timesteps 260 I am having trouble solving this environment. Agents trained in it get scores only slightly better than a random agent even after long training sessions. I have tried Deep Q-learning (with experience replay, target network) REINFORCE (with and without baseline) and PPO. Conceptually, the problem is quite simple. The agent just needs to learn to turn towards the rabbit. However, it occurs to me that the state representation might make the problem more difficult, since only one of the six variables is directly under the agent's control, and three of them (the rabbit state) are completely random. Does state representation generally affect how difficult a problem is? Is this a poor state representation? Are there rules of thumb for how to design states? Would it help to reformulating the state, eg. to [distance_from_agent_to_rabbit, angle_between_agent_and_rabbit]? Environment source code here .
