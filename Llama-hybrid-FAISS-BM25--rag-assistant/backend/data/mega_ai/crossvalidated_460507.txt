[site]: crossvalidated
[post_id]: 460507
[parent_id]: 459261
[tags]: 
Summing up the AIC is the same as "stacking up" your individual models, akin to having an interaction term in linear regression. For example, if the model for subject 1 is \begin{equation} y_1 = \alpha_11 + \beta_1 x_1 + \epsilon_1, \quad \epsilon_1 \sim \mathcal{N}(0, \sigma_1^2I) \end{equation} and the model for subject 2 is \begin{equation} y_2 = \alpha_21 + \beta_2 x_2 + \epsilon_2, \quad \epsilon_2 \sim \mathcal{N}(0, \sigma_2^2I) \end{equation} You can fit a joint model \begin{equation} \begin{pmatrix} y_1 \\ y_2 \end{pmatrix} = \begin{pmatrix} \alpha_11 \\ \alpha_21 \end{pmatrix} + \begin{pmatrix} \beta_1x_1 \\ \beta_2x_2 \end{pmatrix} + \begin{pmatrix} \epsilon_1 \\ \epsilon_2 \end{pmatrix}, \quad \begin{pmatrix} \epsilon_1 \\ \epsilon_2 \end{pmatrix} \sim \mathcal{N}\left (0, \begin{pmatrix} \sigma_1^2I & 0 \\ 0 & \sigma_2^2I \end{pmatrix} \right) \end{equation} and your AIC would be the sum of your two sub-models, since the log likelihood and the number of parameters of the joint model are simply the sum of the submodels' log likelihood and number of parameters. Of course, in practice, we usually assume the models share something. In the above example, we usually assume $\sigma_1^2=\sigma_2^2$ . Moreover, if there are many subjects, we may assume $\alpha_i$ come from some distribution, and have a random effects model. I suppose the models you use are more complicated than linear regression, but the principle is the same. Summing the AIC up is basically equivalent to a "fixed effects" overall model, where the sub-models don't share anything. When the sub models are grossly different, I suppose the approach by Stephan et al (2009) is akin to having a "model of models". Importantly, their approach supposes you can specify a prior $\text{Dirichlet}(\alpha)$ for the different models. The AIC approach is not Bayesian, so I am not sure you can easily adapt their procedure for AIC.
