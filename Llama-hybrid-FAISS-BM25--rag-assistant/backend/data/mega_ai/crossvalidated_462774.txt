[site]: crossvalidated
[post_id]: 462774
[parent_id]: 
[tags]: 
Selecting training images for object verification(siamese network), different number of examples per object

I'm trying to build a model for object verification (my first not tutorial-guided project of this kind). I saw an approach using a siamese network in the coursera deep learning course by Andrew Ng. The method used there (for face verification) is to train a CNN to extract a feature vector from images. The difference between the vectors calculated for two images is then used to determine whether they show the same object/person or not. I am about to start a project where I would like to use this approach and I got a data set that seems to be well fit to the purpose but before starting, I'd be grateful for some advice from experienced ML-engineers/DS on how to best build my training/validation/test sets from the dataset I have. The question I'm most uncertain about is how many images I should choose from those available for each object in the dataset. The number varies vastly! Here's an overview, sorted by the number of available pictures per object: 2-4 imgs: 100 objects 5-10 imgs: 200 objects 11-20 imgs: 200 objects 21-30 imgs: 150 objects 31-40 imgs: 100 objects 41-50 imgs: 70 objects 51-60 imgs: 60 objects 61-70 imgs: 50 objects 71-80 imgs: 30 objects 81-90 imgs: 80 objects 91-100 imgs: 20 objects 101-130 imgs: 40 objects 131-200 imgs: 60 objects 201-300 imgs: 40 objects 301-400 imgs: 20 objects 201-300 imgs: 40 objects 201-300 imgs: 40 objects 401+ imgs: 25 objects Total: ~ 77.000 images for 1200 objects Here are my thoughts so far: if I just used all images (e.g. 5 for one object, 400 for another) that would lead to my model doing much better for objects similar to those with a large number of training pictures. So it would overfit to those and generalize badly to unseen data with different properties. Min./max. 20 - 50: I could select objects with a minimum of 20 pictures only, and from all with more than that select a maximum of 50 pictures. What I don't like about this idea is that the number of different objects is reduced by quite some number. I would end up with about 30.000 images for 700 objects, ignoring/wasting 500 objects in the dataset. That's also a large number of images for just 700 objects and I somehow feel that this might lead to overfitting and bad generalization as well. Min./max. 5 - 10: perhaps I should change the limits: min. 5, max. 10 pictures per object. That would give me about 18446 pictures for 1105 objects, with a maximum difference of just 5 pictures between the objects. That should do a better job in terms of avoiding overfitting, right? The latter approach would also allow for much larger validation and test sets, which I guess would be good as well. Most of the objects in the dataset would be used, too. I hope it's clear from this description what causes my uncertainty about how to choose from the data. My intuition is that I somehow have too balance between overfitting to those objects with a large amount of images overfitting in general and wasting the chance to use all the objects in my dataset Are these (beginner's) thoughts completely false or am I at least on the right track in some ways? Thanks in advance for any of your insights, experiences and advice! Best regards, Chris
