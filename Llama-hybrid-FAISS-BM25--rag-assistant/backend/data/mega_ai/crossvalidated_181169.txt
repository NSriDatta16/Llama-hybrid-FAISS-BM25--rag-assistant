[site]: crossvalidated
[post_id]: 181169
[parent_id]: 
[tags]: 
Training for Regression with Multiple Outputs per Input Data

I want to use a neural network (or any other method, for that matter) to perform regression from a high-dimensional space (10k dimensions) to a low-dimensional space (3 dimensions). To train this, I have many, many input-output training pairs $(x, y)$. So to train for regression, I could just send each training data $x$ through the network, compute the L2 error between its predicted value of $y$ and the ground truth value over my 3 output nodes, and back propagate the derivative of that error. However, my problem arises because, for each training pair, $(x, y)$, there exist several other training pairs with exactly the same value of $x$. So, my list of training pairs looks like: $(x1, y1a), (x1, y1b), (x1, y1c) ...., (x2, y2a), (x2, y2b), (x2, y2c) ...., (x3, y3a), (x3, y3b), (x3, y3c) ...., .....)$. This is because every value of $x$ has multiple "plausible" values of $y$, which are all equally important. Therefore, my network will be trained to output different values for the same input. My question is: How will my network cope with this problem? If I were to pass $x1$ through the network, will it end up outputting just to $y1a$? Or will it average the values of $y1a, y2a, y3a, ...$? Or will it do something else? And is there an alternative way I can pose this problem? I thought of doing it as a classification, but I would have to discretise my output space, which I want to avoid. And in any case, this would have the same problem as above...
