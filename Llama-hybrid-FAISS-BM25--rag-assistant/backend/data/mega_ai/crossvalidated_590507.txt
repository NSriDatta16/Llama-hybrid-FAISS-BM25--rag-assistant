[site]: crossvalidated
[post_id]: 590507
[parent_id]: 590494
[tags]: 
I am going to slightly rephrase your question: we assume you have $N$ samples $\{x_i\}_{1 \leq i \leq N}$ which were generated from a ground-truth model $d_1$ with parameters $\theta_1 \in \Theta_1$ (where $\Theta_1$ is the set of possible parameters for $d_1$ ). You know neither the ground-truth model $d_1$ nor its parameters $\theta_1$ . You are going to fit a model $d_2$ (which is different from the ground-truth $d_1$ : "all models are wrong" ) and estimate its parameters $\theta_2 \in \Theta_2$ , for instance via maximum likelihood estimation: $$ \hat{\theta}_2 = argmax_{\theta_2} \ \ p(\{x_i\}|\theta_2,d_2) $$ If you were to know the ground-truth $d_1$ and $\theta_1$ , would you necessarily have $p(\{x_i\}|\theta_1,d_1) \geq p(\{x_i\}|\hat{\theta}_2,d_2)$ (i.e. a higher model evidence for the ground truth model)? Well, no. There is no formal and systematic link between the model evidences for $d_1$ and $d_2$ , since their ratio is going to depend on: Their relative complexities (i.e. number of free parameters, as measured by $|\Theta_1|$ and $|\Theta_2|$ ). If $|\Theta_2| , i.e. if $d_2$ is simpler than $d_1$ , it might not be able to explain the observations, and hence have a low likelihood. However, if $|\Theta_1| , then the evidence for model $d_2$ will be penalized by its higher number of free parameters. This is nicely explained in chapter 28 of the following textbook: MacKay, D. J., & Mac Kay, D. J. (2003). Information theory, inference and learning algorithms. Cambridge university press. The observations $\{x_i\}_{1 \leq i \leq N}$ . If $N$ is small, or if the set $\{x_i\}$ is an outlier that does not represent the average output of $d_1$ , then the model evidence for $d_1$ will be small. This is a case of non-identifiability, which we discuss in the following paper: Gontier, C., & Pfister, J. P. (2020). Identifiability of a binomial synapse. Frontiers in computational neuroscience, 14, 558477. I also proposed a solution for the case were $|\Theta_1| in the following question: Formal proof of Occam's razor for nested models
