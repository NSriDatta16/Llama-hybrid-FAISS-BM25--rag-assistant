[site]: crossvalidated
[post_id]: 118486
[parent_id]: 118427
[tags]: 
The application most concerned with tiny deviations from correlation is cryptography. The ability to predict a pseudo-random value with better than ordinary accuracy can translate into superior abilities to break encryption schemes. This can be illustrated with a somewhat artificial example designed to help the intuition. It shows how a truly dramatic change in predictability can be incurred by an arbitrarily tiny serial correlation. Let $X_1, X_2, \ldots, X_n$ be iid standard Normal variables. Let $\bar X = (X_1+X_2+\cdots+X_n)/n$ be their mean and designate $$Y_i = X_i - \bar X$$ as their residuals. It is elementary to establish that (1) the $Y_i$ have identical Normal distributions but (2) the correlation among $Y_i$ and $Y_j$ (for $i\ne j$) is $-1/(n-1)$. For large $n$ this is negligible, right? Consider the task of predicting $Y_n$ from $Y_1, Y_2, \ldots, Y_{n-1}$. Under the assumption of independence, the best possible predictor is their mean. In fact, though, the construction of the $Y_i$ guarantees that their sum is zero, whence $$\hat{Y_n} = -\sum_{i=1}^{n-1}Y_i$$ is a perfect (error-free) predictor of $Y_n$. This shows how even a tiny bit of correlation can enormously improve predictability. In cryptanalysis the details will differ--one studies streams of bits, not Normal variates, and the serial correlations can be tiny indeed--but the potential for equally dramatic results nevertheless exists. For those who like to play with things, the following simulation in R replicates this hypothetical situation and summarizes the mean and standard deviations of the prediction errors. It also summarizes the first-order serial correlation coefficients of the simulated $Y_i$. For reference, it does the same for the $X_i$. predict.mean As written, this will take a good fraction of a minute to run: it performs $1000$ simulations of the case $n=10^5$. The timing is proportional to the product of these numbers, so reducing that by an order of magnitude will give satisfactory turnaround for interactive experiments. In this case, because such a large number of simulations were performed, the results will be pretty accurate, and they are Mean method Exploit Mean (reference) Exploit (reference) Autocorrelation -0.064697 0.000000 -0.064697 5.502087 -0.000046 Mean method Exploit Mean (reference) Exploit (reference) Autocorrelation 1.0235 0.0000 1.0235 321.8314 0.0031 On average, estimating $Y_n$ with the mean of the preceding values made an error of $-0.06$, but the standard deviation of those errors was close to $1$. The exploit offered by recognition of the correlation was absolutely perfect: it always got the prediction right. However, when applied to the truly independent values $(X_i)$, the exploit performed terribly, with a standard deviation of $321.8$ (essentially equal to $\sqrt{n}$). This trade-off between assumptions and performance is instructive!
