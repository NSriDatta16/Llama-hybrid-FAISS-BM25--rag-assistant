[site]: datascience
[post_id]: 22360
[parent_id]: 
[tags]: 
What are some good models to test the speed of a data science machine?

I'm writing a battery of tests (in Python) for the purpose of measuring the speed of my company's different computational instances. The goal is to see how fast different AWS EC2 instances are at running different ML models or common data science tasks. I'm reaching out to ask if anyone knows of any time-consuming (but realistic) models that can be built using only the standard Anaconda packages and either the built in datasets, or a popular dataset known in the industry? The goal would be to give end users a sense of how much computational power they need, and I think using some popular data sets or models to compare relative time to completion would be the best way for them to choose the right amount of power for their needs.
