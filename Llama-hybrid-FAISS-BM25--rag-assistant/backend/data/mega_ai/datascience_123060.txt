[site]: datascience
[post_id]: 123060
[parent_id]: 123053
[tags]: 
There are many contributing factors to the abundance of research based on BERT vs the research based on Llama: Age : BERT has been around for far longer than Llama (2018 vs 2023), so it has more traction with researchers because it has been applied to many many things, so people know they work and has probably already been applied to a problem similar to yours. Computational resources : BERT is lightweight compared to Llama. Anyone can train BERT on a single medium-range GPU. To use Llama for inference you need a lot of very powerful GPUs, let alone training it. Most research groups have modest computational resources. Appropriateness for downstream tasks : BERT is easily applied to text classification because it has the output at the [CLS] token position, which can be directly attached a classification head. Llama is an autoregressive language model, which makes it less obvious how to use it for classification. Of course, you can just approach the task at the natural language level and ask Llama to classify the input text, but the reliability of this kind of approach is not 100% and the model may just answer with their diatribe about why it's not Ok to proceed with your request unless you explicitly constrain its output (e.g. with grammars ). On the other hand, BERT is not meant as a generative model, you you'd better not use it for a generative task.
