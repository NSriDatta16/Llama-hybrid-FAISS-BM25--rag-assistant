[site]: crossvalidated
[post_id]: 125348
[parent_id]: 125344
[tags]: 
What you observe is to be expected with stepwise selection. Stepwise model reduction has a long list of well-documented undesirable effects. One of those is that p-values become artificially small. They're essentially meaningless; you don't know how big they should be, only that the number you have is far smaller than it ought to be. Other effects include that parameter estimates are biased (the ones still in the model are biased away from 0); standard errors are much too small. Measures of 'fit' ($R^2$ and similar measures) are inflated. Confidence intervals don't achieve anything like their desired coverage. Out of sample performance is usually very bad. (These problems tend to occur with pretty much any kind of model selection - on any kind of model - that doesn't in some way split samples so that model selection, estimation and evaluation aren't done on the same data. But stepwise has some additional problems as well.) Numerous answers on this site discuss these problems at length. I highly recommend Chapter 4 of Frank Harrell's Regression Modeling Strategies for reading. I always ask any research students I have to familiarize themselves with it. (Well, read the rest of it, too - it's an excellent book, but Chapter 4 speaks to this issue)
