[site]: crossvalidated
[post_id]: 463724
[parent_id]: 463716
[tags]: 
I think the key point is that: when you combine multiple linear boundaries, you get a nonlinear boundary For example, if I want to separate the grey circles from the black circles (below), I could draw a single boundary (which is what logistic regression would be akin to): But a single boundary doesnt do the best job. Alternatively, I could combine multiple decision boundaries, to do an even better job: In a neural network, you can sort of think of each hidden node as a linear-like decision boundary; the network can combine them to form very nonlinear boundaries (for example, a network with 2 hidden nodes might produce the following): And you can combine as many hidden nodes as you like; here's an example of a network with 5 sigmoid hidden nodes solving an XOR-like problem: Shikhar Sharma has a very useful blog post that does a good job of explaining to.
