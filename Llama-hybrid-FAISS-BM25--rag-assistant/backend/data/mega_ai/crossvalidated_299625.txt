[site]: crossvalidated
[post_id]: 299625
[parent_id]: 
[tags]: 
How does one deal with the issue where a group of parameters moves at a good pace, while others are stalled during training in Deep Learning?

I was noticing that some of my parameters where training faster than others. However, I was not sure of the source of this nor how to deal with it. The only thing that occurred to me is to have a different learning rate per parameters but that seems very hacky and difficult to scale (i.e. a human has to intervene for every change in the network and data set etc). Has the Deep Learning community developed a better tool, algorithm etc to deal with such a problem?
