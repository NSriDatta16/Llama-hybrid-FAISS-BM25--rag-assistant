[site]: datascience
[post_id]: 17173
[parent_id]: 17147
[tags]: 
You can use any base learner for boosting (Adaboost requires sample weighting though). Keep in mind however that the original idea is to use weak learners with strong bias and reducing that bias through boosting. If it is a classification problem, usually logarithmic loss is used to calculating the residual/gradient for boosting. For Python, there is a nice AdaBoost wrapper in scikit-learn (AdaBoostClassifier) which can take for example a Random Forest as base learner.
