[site]: crossvalidated
[post_id]: 393191
[parent_id]: 393147
[tags]: 
You are correct in the connection between Learning theory and Glivenko-Cantelli theorem. Note that original Glivenko-Cantelli theorem was only for half-bounded intervals. Indeed, if we take $\mathcal{H}$ to be the set of half-bounded intervals, then Glivenko-Cantelli theorem is equivalent to $$ \lim_{m\to \infty} \sup_{h \in \mathcal{H}} | L_{\tau_m}(h) - L_{\mathcal{D}}(h) | \to 0 $$ This notion of uniform convergence over half-intervals was then generalized to $\mathcal{H}$ having finite VC-dimension. For example, one can obtain a similar convergence in multiple dimensions by choosing the hypothesis class $\mathcal{H}$ as all half-cuboids which has finite VC Dimension. One may wonder what if you include a lot of functions in $\mathcal{H}$ . If $\mathcal{H}$ is allowed to be large, then one can come up with an example where the uniform convergence doesn't hold. Following results give lower bounds on the estimation error, necessating the need of choosing $\mathcal{H}$ wisely. (1) No Free Lunch theorem in the book, Theorem 5.1. (2) Fundamental theorem of Learning, Theorem 6.8. To quote the authors from Chapter 5 - "Learning theory studies how rich we can make $\mathcal{H}$ while still maintaining reasonable estimation error." The book referenced is "Understanding Machine Learning" by Shai Ben-David and Shai Shalev-Shwartz.
