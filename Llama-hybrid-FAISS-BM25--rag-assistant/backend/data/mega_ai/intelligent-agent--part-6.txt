roposed in April 2025 to use AI agents to automate the work of about 70,000 United States federal government employees, as part of a startup with funding from OpenAI and a partnership agreement with Palantir. This proposal was criticized by experts for its impracticality, if not impossibility, and the lack of corresponding widespread adoption by businesses. Proposed benefits Proponents argue that AI agents can increase personal and economic productivity, foster greater innovation, and liberate users from monotonous tasks. A Bloomberg opinion piece by Parmy Olson argued that agents are best suited for narrow, repetitive tasks with low risk. Conversely, researchers suggest that agents could be applied to web accessibility for people who have disabilities, and researchers at Hugging Face propose that agents could be used for coordinating resources such as during disaster response. The R&D Advisory Team of the BBC views AI agents as being most useful when their assigned goal is uncertain. Concerns Concerns include potential issues of liability, an increased risk of cybercrime, ethical challenges, as well as problems related to AI safety and AI alignment. Other issues involve data privacy, weakened human oversight, a lack of guaranteed repeatability, reward hacking, algorithmic bias, compounding software errors, lack of explainability of agents' decisions, security vulnerabilities, problems with underemployment, job displacement, and the potential for user manipulation, misinformation or malinformation. They may also complicate legal frameworks and risk assessments, foster hallucinations, hinder countermeasures against rogue agents, and suffer from the lack of standardized evaluation methods. They have also been criticized for being expensive and having a negative impact on internet traffic, and potentially on the environment due to high energy usage. There is also the risk of increased concentration of power by political leaders, as AI agents may not question instructions in the same way that humans would. Journalists have described AI agents as part of a push by Big Tech companies to "automate everything". Several CEOs of those companies have stated in early 2025 that they expect AI agents to eventually "join the workforce". However, in a non-peer-reviewed study, Carnegie Mellon University researchers tested the behavior of agents in a simulated software company and found that none of the agents could complete a majority of the assigned tasks. Other researchers had similar findings with Devin AI. Yoshua Bengio warned at the 2025 World Economic Forum that "all of the catastrophic scenarios with AGI or superintelligence happen if we have agents". In March 2025, Scale AI signed a contract with the United States Department of Defense to work with them, in collaboration with Anduril Industries and Microsoft, to develop and deploy AI agents for the purpose of assisting the military with "operational decision-making". Researchers have expressed concerns that agents and the large language models they are based on could be biased towards aggressive foreign policy decisions. Research-focused agents have the risk of consensus bias and coverage bias due to collecting information available on the public Internet. NY Mag unfavorably compared the user workflow of agent-based web browsers to Amazon Alexa, which was "software talking to software, not humans talking to software pretending to be humans to use software." Agents have been linked to the dead Internet theory due to their ability to both publish and engage with online content. Agents may get stuck in infinite loops. Since many inter-agent protocols are being developed by large technology companies, there are concerns that those companies could use these protocols for self-benefit. Possible mitigation Zico Kolter noted the possibility of emergent behavior as a result of interactions between agents, and proposed research in game theory to model the risks of these interactions. Guardrails,