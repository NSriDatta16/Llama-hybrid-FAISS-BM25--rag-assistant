[site]: crossvalidated
[post_id]: 394783
[parent_id]: 394118
[tags]: 
I would argue the performance is not that different as you might expect, but you ask a great question (see the last paragraph). As you mention transfer learning: To compare apples with apples we have to look how many pictures in total and how many pictures of the class of interest a human / neural net "sees". 1. How many pictures does a human look at? HumanÂ´s eye movement takes around 200ms which could be seen as kind of an "biological photo". See the talk by computer vision expert Fei-Fei Li: https://www.ted.com/talks/fei_fei_li_how_we_re_teaching_computers_to_understand_pictures#t-362785 . She adds: So by age 3 a child would have seen hundreds of millions of pictures. In ImageNet, the leading database for object detection, there are ~14million labeled pictures. So a neural network being trained on ImageNet would have seen as many pictures as a 14000000/5/60/60/24*2 ~ 64 days old baby, so two months old (assuming the baby is awake half of her life). To be fair its hard to tell how many of this pictures are labeled. Moreover, the pictures, a baby sees, are not that diverse like in ImageNet. (Probably the baby sees her mother have of the time,... ;). However, i think its fair to say that your son will have seen hundreds of millions of pictures (and then applies transfer learning). So how many pictures do we need to learn a new category given a solid base of related pictures that can be (transfer) learned from? First blog post i found was this: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html . They use 1000 examples per class. I could imagine 2.5 years later even way less is required. However, 1000 pictures can be seen by a human in 1000/5/60 in 3.3 minutes. You wrote: A human child at age 2 needs around 5 instances of a car to be able to identify it with reasonable accuracy regardless of color, make, etc. That would be equivilant to forty seconds per instance (with various angles of that object to make it comparable). To sum up: As i mentioned, I had to make a few assumptions. But i think, one can see that the performance is not that different as one might expect. However, i believe you ask a great question and here is why: 2. Would neural network perform better/different if they would work more like brains? (Geoffrey Hinton says yes). In an interview https://www.wired.com/story/googles-ai-guru-computers-think-more-like-brains/ , in late 2018, he compares the current implementations of neural networks with the brain. He mentions, in terms of weights, the artificial neural networks are smaller than the brain by a factor of 10.000. Therefore, the brain needs way less iterations of trainings to learn. In order to enable artificial neural networks, to work more like our brains, he follows another trend in hardware, a UK based startup called Graphcore. It reduces the calculation time by a smart way of storing the weights of a neural network. Therefore, more weights can be used and the training time of the artificial neural networks might get reduced.
