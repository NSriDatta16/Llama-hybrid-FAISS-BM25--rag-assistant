[site]: datascience
[post_id]: 60603
[parent_id]: 
[tags]: 
Workaround for word embeddings that do not "see" antonyms

Most word embeddings do not "see" antonyms. For instance, among many words they will place vectors for "dependent" and "independent" (as an example) quite close, - actually as close as with synonyms such as "independent" and "autonomous". So it is easy to identify synonyms as close vectors, but how to identify antonyms, or generally work with antonyms? There are some few rare papers that try to develop embedding algorithms "aware" of antonyms (just web-search word-embedding antonyms). But I am working with standard very powerful and already trained on massive data embedding libraries. Is there a workaround to somehow work with STANDARD embeddings but to make them not "blind" to antonyms? Thanks!
