[site]: datascience
[post_id]: 122880
[parent_id]: 122825
[tags]: 
There are several ways word-embeddings are trained, however most of them require a ton of data. They usually involve learning vector representations that are useful for some self-supervised objective, which all tend to be pretty data-hungry. word2vec (and variants) learn representations by training a model to use those representations to predict adjacent words Approaches like ELMo and BERT use intermediate representations from a language model, which are pretrained on large text corpora If you have a large enough dataset you could train new domain-specific embeddings from scratch, but it probably be more effective and much easier to finetune existing embeddings (i.e., initialize on models/representations and train on your domain data). See: this post for finetuning word2vec, for example.
