[site]: crossvalidated
[post_id]: 33008
[parent_id]: 
[tags]: 
How to find 1-year population average with intermittent series data

I have performance review data and scores (ranging from 1 to 4) for employees of a company. I need to show the company average over the past year. However, the employees were only ever reviewed for a few weeks in a row at a time. For example: Roger was reviewed from Jan 1 through March 1 Steve was reviewed from August 15 through September 15 ...etc... Right now, I'm taking all of the reviews over the year (Jan 1 through Dec 31) and for each day I'm computing the average for that day. I then take all of those days and plot the average score for that day on a line graph with time on the X axis and score on the Y axis. The problems I'm having are: there are gaps of time in these reviews (no one was reviewed in February, 20 people were reviewed in March) the higher performing employees may have been reviewed at one part of the year, and the lower performing employees at another when I plot this data the average produces a zigzag looking line chart. for example, day 1 the average is 3.4, but day 2 is 1.3. over time this looks like a lot of spikes. So, where I'm at now is thinking of taking each employee as a "series" (where some series (employees) have data ranging for a few weeks, and some have data ranging a few months) and normalizing them together and computing the average of that normalization. I'd greatly appreciate any help on this and the best way to present this data!
