[site]: crossvalidated
[post_id]: 639979
[parent_id]: 639974
[tags]: 
This doesn't strike me as hopeless or deeply flawed. I however think you'd want to account for the pairwise structure of your data. I.e. any error/noise in measuring $f_i$ is going to affect all pairs including $f_i$ in the same way. One way to account for this is to include a random effect for each measurement, i.e. something like: $$ \Delta F_{i,j} \sim N(\mu_{i,j}, \sigma)\\ \mu_{i,j} = \alpha + \beta \widehat{\Delta F_{i,j}} + \gamma_i - \gamma_j \\ \gamma_i \sim N(0, \tau) $$ Where $\gamma_i$ represents the error in measuring $f_i$ . This is a type of multiple-membership model (see e.g. https://github.com/jvparidon/lmerMultiMember for a package discussing and implementing those). I've seen a similar (though not identical!) model used in the context of ecology ( https://www.mdpi.com/1424-2818/14/10/858 ) - more generally, ecologist have a vast literature on modelling pairwise distances between samples and the linked paper provides an IMHO reasonable entry point into that literature. I think using absolute values would pose more problems (now you have a lower bound at zero) than it solves - if your outcome distribution is symmetric around 0 (e.g. normal) and you have chosen the order of pairs randomly I think you are safe. Hope that clarifies more than confuses.
