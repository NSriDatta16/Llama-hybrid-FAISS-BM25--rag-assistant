[site]: crossvalidated
[post_id]: 213292
[parent_id]: 
[tags]: 
Performance measure for calibration for binary classification problems?

Consider a binary classification problem, where the goal is to use training data $(x_i,y_i)_{i=1}^n$ to fit a classifier $f: \mathbb{R}^d \rightarrow [0,1]$ that outputs a conditional probability estimate (e.g. $f$ could be a logistic regression model). The general way to check if the predicted probabilities match the true probabilities (i.e., are ``well-calibrated") seems to be a reliability plot . This plots the probabilities on the x-axis, and the observed probabilities on the y-axis. I am looking for a performance metric that could be used instead of the reliability plot? Ideally, I'd like to find a metric that is used in the statistics or ML literature.
