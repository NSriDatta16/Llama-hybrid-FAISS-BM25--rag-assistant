[site]: crossvalidated
[post_id]: 437166
[parent_id]: 
[tags]: 
Show that bias term involving an indicator function convergences to zero

Assume that we have $N$ observations of i.i.d. data $(Y_i,X_i)_{i=1}^{N}$ . We want to learn the model given by $Y=f(X)+\epsilon$ . We use the data to estimate $\hat{f}$ using any machine learning algorithm, e.g. random forests or neural nets. We assume that our method is consistent, i.e. $\hat{f}\xrightarrow{p}f$ , but we do not assume anything about the convergence rate. In particular, we do not assume that $\hat{f}$ is root- N consistent. We consider a bias term that arises from a particular estimator (I don't want to bore you with the details). The bias term reads $bias=\sqrt{N}\mathbb{E}\left[Y_i\left(1_{\left\{ f\left(X_{i}\right) where $c$ is a known constant. I want to prove that $bias=o_p(1)$ , i.e. as $N\xrightarrow{}\infty$ , we have $bias\xrightarrow{p}0$ . We can assume that $X$ has a density. If we didn't have the indicator functions, it would be impossible to show without assume root- N consistency, but I wonder whether we can show this due to the indicator function. For instance, we may assume that $\text{Pr}\left(f\left(X\right)=c\right)=0$ . Question : How would you prove $bias=o_p(1)$ (if it is at all)?
