[site]: datascience
[post_id]: 126209
[parent_id]: 126199
[tags]: 
You aren't missing anything. The negative log-likelihood of any distribution can be negative. In the case of the negative log-likelihood for a Gaussian random variable, this occurs when the function is evaluated at a particular $y_i$ that is highly probable given $\mu(x_i)$ and $\sigma(x_i)$ , i.e. when $\mu(x_i)$ is reasonably close to $y_i$ and $\sigma(x_i)$ is sufficiently small. Positive values occur in the opposite case, when the (fitted) model described by $\mu(x_i)$ and $\sigma(x_i)$ gives low density to $y_i$ (i.e. a poor probabilistic prediction). Note that in maximum likelihood estimation, the goal is to maximize the likelihood function, not the negative likelihood function. If you throw a negative sign in front, of course this switches the objective to minimizing, in which case, highly negative values of the negative log-likelihood are preferred. Your observation about the function possibly tending to infinity is also highly relevant; in fact, this is why the log scoring rule (often called ELPD in Bayesian statistics) can behave quite unstable in practice.
