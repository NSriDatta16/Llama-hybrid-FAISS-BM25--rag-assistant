[site]: crossvalidated
[post_id]: 23172
[parent_id]: 22920
[tags]: 
As @Anony-Mousse commented, k-mean relate to minimizing entropy in its clustering algorithm (which uses the euclidean distance), as does the EM algorithm. The k-means is also based on the distance between data points as you would like. Data points are usually in some system of coordinates which you are already working with, and the euclidean distance gives you the distance of the shortest path between them as a measure for differentiating homogeneity between clusters. But the algorithm which uses the measure is what assures completeness of the cluster, the measure it works with is assumed in the representation of the space of datapoints. You could just say, that the distance between points is $k \log( d_{i,j} + 1 )$ where $d_{i,j}$ is the distance between points so that it resembles entropy measures. But since this is strictly monotonically increasing as a function the clustering will not change, but can affect the inferred number of clusters. It penalizes large distances from creating large effects with the logarithmic increase on the distance. I believe that conditional entropy will be applicable only under a constrained space of datapoints. You would have then probabilities of points in clusters based maybe on a bootsrap measure or some Bayesian view with priors on placements. It would get complicated and is a separate question in itself. Without the constrained space to place probabilities on the distribution on points in relation to each other would need some normalization and therefor a constrained space unless some other definition can apply. The paper you link to for the V-measure uses this conditional entropy without euclidean distances it seems and more based on labeling. You could replace the conditional probability measures there to have distances rather than the ratios of class membership in equation (1) of the paper so that the inverse sum of distances is there.
