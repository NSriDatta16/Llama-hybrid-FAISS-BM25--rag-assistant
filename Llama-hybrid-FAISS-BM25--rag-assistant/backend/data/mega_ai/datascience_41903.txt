[site]: datascience
[post_id]: 41903
[parent_id]: 41758
[tags]: 
The process of selecting model is as follows: 1] Choose the appropriate algorithm to suite your data set. 2] Create the model, in this process we will fit the algorithm with training data along with the few other machine learning techniques like grid search and cross validation.If you are using deep learning then you might need to split the data into training, validation and testing data. In machine learning if you are using cross validation you do not need the validation data as cross validation it self will split data for validation. 3] Now, depending on the outcome of the result of the above step, you tune hyper parameters and check again the results again. You can repeat the process until you get the optimal model with best metrics. In this way you are selecting the best model by hyper parameter tuning. One more thing about selecting the model, you can change the algorithm in the first step and continue with step2 and step3. Now, you will have different models with their own best optimal model with best hyperparameters. You can then select best model that suits your needs. For Example 1] First you select the decision tree to create the model and then select the best optimal model with hyper parameter tuning using step2 and step3. 2] Second, you change the algrithm to randomforest to create the model and then again select the best optimal model with hyper parameter tuning using step2 and step3. 3] Now we will have different models, each of this model was selected as the best model using the hyper parameter tuning. In this way you can try creating the few models and compare them and then finally select the best model. So, there are actually 2 scenarios when selecting the model. 1] You create the model with an algorithm and then select the best optimal model with hyperparameter tuning. 2] You create different models with different algorithms each with their own best optimal model and select the best model out of these. However, i've also read that model selection shoud be done before tuning the parameters. I'm getting confused. Which one must be done before the other ? Sometimes, we might get confused with creating the model and selecting the model. When we create the model, we have to select the appropriate algorithm, sometimes this step(where we select the algorithm) can also be referred to as selecting the model. So, hyper parameter helps in selecting the best model, therefore hyper parameter tuning comes before selecting the model. Is the validation set used for tuning ? Validation set is used to evaluate the model performance before testing on the testing data. Once we evaluate the model performance and if it is not as good as you expected, then you change the hyper parameters and check the performance again. So, yes we will use the validation data set to tune hyper parameters. You can think of validation data as doing unit testing while developing the software applications. You develop the application as a developer and do the unit testing. If the unit testing fails, then again you have to make changes as the developer to fix the unit testing bugs and then only it will be tested by testing team. Once the testing team approves, then the application will be deployed in the production. Similarly, in machine learning you develop the model with training data and do the first testing on validation data. Then you do hyper parameter tuning and select the best model. This model will be tested on the testing data set. Once the results on the testing data set are satisfying, you will deploy the model in the production.
