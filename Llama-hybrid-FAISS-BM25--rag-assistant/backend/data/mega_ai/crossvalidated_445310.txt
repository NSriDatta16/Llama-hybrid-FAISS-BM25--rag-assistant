[site]: crossvalidated
[post_id]: 445310
[parent_id]: 
[tags]: 
Calculating sample size with standard deviation?

Okay so I have an exam coming up pretty soon and I really can't get the hang of how to calculate sample size when all you are given is standard deviation. For instance: A normal population has a standard deviation of 15. How large a sample should be drawn to estimate with 95% confidence the population mean to within 1.5? Or: A statistician wants to estimate the mean weekly family expenditure on clothes. He believes that the largest weekly expenditure is $650 and the lowest is \$150. a. Determine with 99% confidence the number of families that must be sampled to estimate the mean weekly family expenditure on clothes to within \$15. Or: A social scientist claims that the average adult watches less than 26 hours of television per week. He collects data on 25 individuals’ television viewing habits and finds that the mean number of hours that the 25 people spent watching television was 22.4 hours. If the population standard deviation is known to be eight hours, can we conclude at the 1% significance level that he is right? Or: Domino’s Pizza in Big Rapids, Michigan, advertises that they deliver your pizza within 15 minutes of placing an order or it is free. A sample of 25 customers is selected at random. The average delivery time in the sample was 13 minutes with a sample standard deviation of 4 minutes. a) Test to determine if we can infer at the 5% significance level that the population mean is less than 15 minutes. A solution to any of these would probably help me figure out the rest of them, but solutions to all would be kindly appreciated.
