[site]: crossvalidated
[post_id]: 642828
[parent_id]: 642702
[tags]: 
The question might have been badly stated. The answer to the question 'when to choose the more indirect method?' is that this choice is to be made when the conditions as stated in the example of the question are not right. If the conditional distribution of the categories is indeed a normal distribution $X|Y∼N(\mu_Y,\sigma)$ , then this is equivalent to a binomial model with logit link $Y|X∼\text{Bernoulli}(p = (1+e^{-\alpha+ -\beta X})^{-1})$ . However, the other way around is not true. The logistic model can be related to more different situations, and also when $X|Y$ is not normal distribution See below an example. In the first case we have two normal distributions for the categories 1 and 2. This leads to $Y|X$ being following a logistic model $$X|Y=1 \sim N(1,1) \\ X|Y=2 \sim N(3,1) \\$$ In the second case we also have a logistic model, but this time it has a different process behind the data generation; By drawing $X$ from a uniform distribution, and determine $Y$ based on a Bernoulli distribution $$X \sim U(-1.5,4.5) \\ Y|X∼\text{Bernoulli}(p = (1+e^{-2(X-2)})^{-1})$$ So, which model to choose, the generative Gaussian classifier, or the discriminative logistic model, depends on our believes about the data generating process. If X is dependent on the category and conditional on that category more or less normal distributed, then the Gaussian classifier can be preferred. But not all cases with a logistic model are like that. On the other hand if the two classes are normal distributed, but with different variance $\sigma^2$ (in the example equal variance was assumed), then logistic regression might be a bad choice. Code for the image: layout(matrix(c(1,2))) par(mar = c(3,4,2,2), mgp = c(2,1,0)) fit = function(z1,z2, title) { r = length(z1)/(length(z2)+length(z1)) d1 = density(z1, bw = 0.3) x1 = d1 $x y1 = d1$ y d2 = density(z2, bw = 0.3) x2 = d2 $x y2 = d2$ y plot(-1,-1, xlim = c(-2,5), ylim = c(0,0.3), xlab = "x", ylab = "density of predictor x", main = title, cex.main = 1) polygon(c(x1,rev(x1)), c(y1*r,y1*0), col = rgb(1,0,0,0.3)) polygon(c(x2,rev(x2)), c(y2*(1-r),y2*0), col = rgb(0,0,1,0.25)) lines(x1,y1*r, col = 2) lines(x2,y2*(1-r), col = 4) z = c(z1,z2) u = c(rep(1,length(z1)),rep(2,length(z2))) w = u-1 mod = glm(w ~ z, family = binomial) plot(w ~ z, col = u*2, xlab = "x", ylab = "data and estimated probability", xlim = c(-2,5), pch = "|") zs = seq(-3,7,0.01) pu = predict(mod, type = "response", newdata = list(w = zs, z = zs)) lines(zs,pu) } set.seed(1) n = 500 x = rnorm(n,1) y = rnorm(n,3) fit(x,y, "1st case: Gaussian distributed X") z = runif(n*2,-1.5,4.5) pu = (1+exp(-2*(z-2)))^{-1} u = rbinom(n*2,1,pu) x = z[u == 0] y = z[u == 1] fit(x,y, "2nd case: non Gaussian distributed X")
