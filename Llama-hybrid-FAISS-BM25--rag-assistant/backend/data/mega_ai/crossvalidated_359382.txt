[site]: crossvalidated
[post_id]: 359382
[parent_id]: 359369
[tags]: 
which one of the two versions of the cost function is the correct one (or the better one) in this case, and why? I suspect you were intending to take the average, in which case the sum would typically be written from $i=1$ to $n$. If that's the case, then your expressions $J_1$ and $J_2$ for the cost function would be equivalent (you can rearrange each to get the other). And more generally, should we write cost functions that are as whole written as an average over the training samples (as in $J_2$), or is it better to write some components as an average over the training samples and others not as an average (as in $J_1$)? In my experience, the form of $J_1$ is more typical. It clearly expresses the cost function as a sum of two terms: 1) the loss (hinge loss in this case), which is summed/averaged over data points, and 2) the penalty/regularization term ($\ell_2$ penalty in this case), which depends only on the weights/parameters. Not all cost functions can be written as a sum/average over data points. For example, some might involve pairs of data points. Or, an operation other than a sum might be of interest. Or, considering cost functions for general optimization problems, there may be no data at all. Edit: Proof that $J_1$ and $J_2$ are equivalent I'll assume here that the sums should be from $1$ to $n$. Start with the expression for $J_2$: $$\frac{1}{n} \sum_{i=1}^n \left ( w^T w + C \max(0, 1-y_i(w^T x_i + b)) \right )$$ Split the sum into two separate sums: $$= \frac{1}{n} \sum_{i=1}^n w^T w + \frac{1}{n} \sum_{i=1}^n C \max(0, 1-y_i(w^T x_i + b))$$ Factor out terms that don't depend on $i$: $$= \frac{1}{n} w^T w \sum_{i=1}^n 1 + C \frac{1}{n} \sum_{i=1}^n \max(0, 1-y_i(w^T x_i + b))$$ Note that $\sum_{i=1}^n 1 = n$: $$= w^T w + C \frac{1}{n} \sum_{i=1}^n \max(0, 1-y_i(w^T x_i + b))$$ This is now the expression for $J_1$.
