[site]: crossvalidated
[post_id]: 622459
[parent_id]: 
[tags]: 
Why convolving a function with a Gaussian kernel is the same as adding a Gaussian noise to the input?

I am implementing accelerated Langevin Dynamics (LD) for posterior estimation with prior presented with deep autoregressive network from paper [1]. I have a question about the prior smoothing operation. The paper says that for the acceleration of LD, we need progressively to smooth the pdf, starting from high noise values and decreasing the noise until we arrive at the original distribution. From the paper, I understood that the operation of convolving the prior function $p(x)*\omega$ (where $\omega$ is a Gaussian function with $0$ mean and std $\sigma$ ) is equivalent to $p(x+\epsilon)$ (where $\epsilon$ is a Gaussian noise $\mathcal{N}(0;\sigma^2)$ ). I am interested in whether there is a mathematical explanation that justifies this claim. [1] Jayaram, Vivek, and John Thickstun. "Parallel and flexible sampling from autoregressive models via Langevin dynamics." International Conference on Machine Learning. PMLR, 2021. https://proceedings.mlr.press/v139/jayaram21b.html P.S. I apologize if I put the wrong tags, it is my first post, and , frankly, I don't know what exactly I am looking for.
