[site]: datascience
[post_id]: 126774
[parent_id]: 
[tags]: 
CS 224N Back Propagation and Margin Loss in Neural Networks

I was going through Stanford CS 224 lecture notes on Back propagation . Page 5 states: We can see from the max-margin loss that: ∂J /∂s = − ∂J/∂s(c) = −1 I'm not sure I understand why this is the case. My thought process : If we're maximizing loss, the derivative of the cost (J) must go to 0. Since s and s(c) aren't necessarily independent, I'm struggling to understand why / how the above equation is correct, since using the chain rule, the result should depend on ∂s(c) /∂s. What am I missing here ?
