[site]: crossvalidated
[post_id]: 558172
[parent_id]: 341138
[tags]: 
(My answer will mostly focus on tabular data as it has proven the hardest to synthesize owning to its heterogeneity and general arbitrariness) Yes, we can get a generative adversarial network (GAN) to generate synthetic data. An exceptional resource on this is: the Synthetic Data Vault initiative where a number of different approaches regarding synthetic data generation are included. As you correctly assess, GANs can be used for synthetic data generation, a number of approaches are implemented in the accompanying sdv package. I will note here that actually variational auto-encoders (VAEs) seem to be a very competitive alternative to GANs for this task. The last couple of years there have been quite a good papers on the subject, some obvious picks would be: Modeling Tabular data using Conditional GAN (2019) by Xu et al. Data Synthesis based on Generative Adversarial Networks (2018) by Park et al. Learning vine copula models for synthetic data generation (2018) by Sun et al. From personal experience: Synthetic data generation can be exceptionally hard to get "arbitrarily correct" but it can be possible to get reasonable mileage out it. For example: Making a synthetic dataset where a classifier cannot distinguish between original and synthetic data is very hard . Making a synthetic dataset to pass to collaborators for them to train some classifier such that it performs adequately in unseen real data is just hard . Making a synthetic dataset to pass to a collaborator to make some EDA and presentation material is usually doable . As such we need to consider why we need the synthetic data to begin with: is it for a privacy preserving task? is it a data-scarcity issue? is it for model-testing requirements? (common in financial applications - eg. stress testing in banking ) (Other, more exotic use cases, also exist: complicated data-sharing agreements, adversarial learning applications, agent-based modelling, etc. but they are rarer beasts.) These points are not particular to tabular data but rather extend to all types of data. For example, one healthcare trust might want/have to avoid sharing real patient MRI scans (imaging data / privacy preserving scenario), similarly a tech start-up might need to train its voice recognition software on multiple speakers (speech data / data-scarcity scenario). Notably even GANs models with strong theoretical foundations ( Wassenstein GAN (2017) Arjovsky et al.) might not be fully appropriate, for example financial time-series data have long-term correlations, volatility clustering and asymmetries (e.g. many small positive moves but few large negative ones) that we require us to use specialised metrics to assess their representativeness. To recap: using deep generative models (GANs, VAEs, EBM s) for synthetic data generation is an extremely fruitful area of ML research. They are already proofs of concepts from research groups as well as some early industrial products; their usefulness will vary wildly among different applications both in terms of data types (images, speech, tabular, etc.) as well as application fields (healthcare, retail, etc.). Therefore we have no established way to generally quantify their impact on the performance of a model. That said, we will have a clearer view in a couple of years; GDPR/CCPA are powerful motivators for certain multi-billion dollar companies to convince legislators about the benignity of their data usage - synthetic data generation is a piece of that puzzle.
