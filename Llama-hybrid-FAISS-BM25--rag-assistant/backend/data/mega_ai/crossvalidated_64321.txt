[site]: crossvalidated
[post_id]: 64321
[parent_id]: 64317
[tags]: 
With generalized linear modeling the mathematical measure that is minimized is called the "deviance" (-2*log-likelihood). There are several sorts of residuals that can be developed. The "deviance residuals" are the individual terms in a modestly complex expression. I think these a most understandable when applied to categorical variables. For a categorical variable using logistic regression these are just the differences between the log-odds(model) and log-odds(data), but for continuous variables they are somewhat more complex. Deviance residuals are what are minimized in the iterative process. See this description at the UCLA website for some nice plots of deviance residuals. It looks to me that analysis of "lift" is done on the scale of probabilities, rather than on the log-odds or odds scale or likelihoods. I see that Frank Harrell has offered some advice and any perceived dispute between Frank and I should be resolved by massive weighting of Frank's opinion. (My advice would be to buy Frank's RMS book.) I'm surprised he didn't offer advice to consider penalized methods and that he didn't issue a caution against over-fitting. I would think that choosing a transformation simply because it maximized "lift" would be akin to choosing models that maximized "accuracy". I know he doesn't endorse that strategy.
