[site]: crossvalidated
[post_id]: 289341
[parent_id]: 179496
[tags]: 
The advice given verbatim from Aurélien Géron' "Hands-On Machine Learning with Scikit-Learn and TensorFlow" on DNN Architecture: - Initialization: He initialization - Activation function: ELU - Normalization: Batch Normalization - Regularization: Dropout - Optimizer: Adam - Learning rate schedule: None You may tinker with this parameters depending on the size of your NN and if the speed or accuracy, whatever you define it, is your objective. As far as batch size is concerned, you might be interested in this excellent discussion supported by links to academic papers at the CV question Tradeoff batch size vs. number of iterations to train a neural network
